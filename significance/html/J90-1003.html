<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
J90-1003 <div class="dstPaperTitle">Word Association Norms, Mutual Information, And Lexicography</div><div class="dstPaperAuthors">Church, Kenneth Ward;Hanks, Patrick;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P91-1017
Two Languages Are More Informative Than One
Dagan, Ido;Itai, Alon;Schwall, Ulrike;"></td>
	<td class="line x" title="1:208	Two Languages Are More Informative Than One * Ido Dagan Computer Science Department Technion, Haifa, Israel and IBM Scientific Center Haifa, Israel dagan@cs.technion.ac.il Alon Itai Computer Science Department Technion, Haifa, Israel itai~cs.technion.ac.il Ulrike Schwall IBM Scientific Center Institute for Knowledge Based Systems Heidelberg, Germany schwall@dhdibml Abstract This paper presents a new approach for resolving lexical ambiguities in one language using statistical data on lexical relations in another language." ></td>
	<td class="line x" title="2:208	This approach exploits the differences between mappings of words to senses in different languages." ></td>
	<td class="line x" title="3:208	We concentrate on the problem of target word selection in machine translation, for which the approach is directly applicable, and employ a statistical model for the selection mechanism." ></td>
	<td class="line x" title="4:208	The model was evaluated using two sets of Hebrew and German examples and was found to be very useful for disambiguation." ></td>
	<td class="line x" title="5:208	1 Introduction The resolution of hxical ambiguities in non-restricted text is one of the most difficult tasks of natural language processing." ></td>
	<td class="line x" title="6:208	A related task in machine translation is target word selection the task of deciding which target language word is the most appropriate equivalent of a source language word in context." ></td>
	<td class="line x" title="7:208	In addition to the alternatives introduced from the different word senses of the source language word, the target language may specify additional alternatives that differ mainly in their usages." ></td>
	<td class="line x" title="8:208	Traditionally various linguistic levels were used to deal with this problem: syntactic, semantic and pragmatic." ></td>
	<td class="line x" title="9:208	Computationally the syntactic methods are the easiest, but are of no avail in the frequent situation when the different senses of the word show *This research was partially supported by grant number 120-741 of the Iarael Council for Research and Development the same syntactic behavior, having the same part of speech and even the same subcategorization frame." ></td>
	<td class="line x" title="10:208	Substantial application of semantic or pragmatic knowledge about the word and its context for broad domains requires compiling huge amounts of knowledge, whose usefulness for practical applications has not yet been proven (Lenat et al. , 1990; Nirenburg et al. , 1988; Chodorow et al. , 1985)." ></td>
	<td class="line x" title="11:208	Moreover, such methods fail to reflect word usages." ></td>
	<td class="line x" title="12:208	It is known for many years that the use of a word in the language provides information about its meaning (Wittgenstein, 1953)." ></td>
	<td class="line x" title="13:208	Also, statistical approaches which were popular few decades ago have recently reawakened and were found useful for computational linguistics." ></td>
	<td class="line x" title="14:208	Consequently, a possible (though partial) alternative to using manually constructed knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora." ></td>
	<td class="line oc" title="15:208	The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks, 1990; Zernik and Jacobs, 1990; Hindle, 1990)." ></td>
	<td class="line x" title="16:208	More specifically, two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of PP-attachment (Hindle and Rooth, 1990) and pronoun references (Dagan and Itai, 1990a; Dagan and Itai, 1990b)." ></td>
	<td class="line x" title="17:208	Clearly, statistical methods can be useful also for target word selection." ></td>
	<td class="line x" title="18:208	Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Haaretz, September 1990 (transcripted to Latin letters)." ></td>
	<td class="line x" title="19:208	130 (1) Nose ze maria' mi-shtei ha-mdinot mi-lahtom 'al hoze shalom." ></td>
	<td class="line x" title="20:208	This sentence would translate into English as: (2) That issue prevented the two countries from signing a peace treaty." ></td>
	<td class="line x" title="21:208	The verb 'lab_tom' has four word senses: 'sign', 'seal', 'finish' and 'close'." ></td>
	<td class="line x" title="22:208	Whereas the noun 'hose' means both 'contract' and 'treaty'." ></td>
	<td class="line x" title="23:208	Here the difference is not in the meaning, but in usage." ></td>
	<td class="line x" title="24:208	One possible solution is to consult a Hebrew corpus tagged with word senses, from which we would probably learn that the sense 'sign' of 'lahtom' appears more frequently with 'hoze' as its object than all the other senses." ></td>
	<td class="line x" title="25:208	Thus we should prefer that sense." ></td>
	<td class="line x" title="26:208	However, the size of corpora required to identify lexical relations in a broad domain is huge (tens of millions of words) and therefore it is usually not feasible to have such corpora manually tagged with word senses." ></td>
	<td class="line x" title="27:208	The problem of choosing between 'treaty' and 'contract' cannot be solved using only information on Hebrew, because Hebrew does not distinguish between them." ></td>
	<td class="line x" title="28:208	The solution suggested in this paper is to identify the lexical relationships in corpora of the target language, instead of the source language." ></td>
	<td class="line x" title="29:208	Consulting English corpora of 150 million words, yields the following statistics on single word frequencies: 'sign' appeared 28674 times, 'seal' 2771 times, 'finish' appeared 15595 times, 'close' 38291 times, 'treaty' 7331 times and 'contract' 30757 times." ></td>
	<td class="line x" title="30:208	Using a naive approach of choosing the most frequent word yields (3) *That issue prevented the two countries from closing a peace contract." ></td>
	<td class="line x" title="31:208	This may be improved upon if we use lexical relations." ></td>
	<td class="line x" title="32:208	We consider word combinations and count how often they appeared in the same syntactic relation as in the ambiguous sentence." ></td>
	<td class="line x" title="33:208	For the above example, among the successfully parsed sentences of the corpus, the noun compound 'peace treaty' appeared 49 times, whereas the compound 'peace contract' did not appear at all; 'to sign a treaty' appeared 79 times while none of the other three alternatives appeared more than twice." ></td>
	<td class="line x" title="34:208	Thus we first prefer 'treaty' to 'contract' because of the noun compound 'peace treaty' and then proceed to prefer 'sign' since it appears most frequently having the object 'treaty' (the order of selection is explained in section 3)." ></td>
	<td class="line x" title="35:208	Thus in this case our method yielded the correct translation." ></td>
	<td class="line x" title="36:208	Using this method, we take the point of view that some ambiguity problems are easier to solve at the level of the target language instead of the source language." ></td>
	<td class="line x" title="37:208	The source language sentences are considered as a noisy source for target language sentences, and our task is to devise a target language model that prefers the most reasonable translation." ></td>
	<td class="line x" title="38:208	Machine translation (MT) is thus viewed in part as a recognition problem, and the statistical model we use specifically for target word selection may be compared with other language models in recognition tasks (e.g. Katz (1985) for speech recognition)." ></td>
	<td class="line x" title="39:208	In contrast to this view, previous approaches in MT typically resolved examples like (1) by stating various constraints in terms of the source language (Nirenburg, 1987)." ></td>
	<td class="line x" title="40:208	As explained before, such constraints cannot be acquired automatically and therefore are usually limited in their coverage." ></td>
	<td class="line x" title="41:208	The experiment conducted to test the statistical model clearly shows that the statistics on lexical relations are very useful for disambiguation." ></td>
	<td class="line x" title="42:208	Most notable is the result for the set of examples for Hebrew to English translation, which was picked randomly from foreign news sections in Israeli press." ></td>
	<td class="line x" title="43:208	For this set, the statistical model was applicable for 70% of the ambiguous words, and its selection was then correct for 92% of the cases." ></td>
	<td class="line x" title="44:208	These results for target word selection in machine translation suggest to use a similar mechanism even if we are interested only in word sense disambiguation within a single language!" ></td>
	<td class="line x" title="45:208	In order to select the right sense of a word, in a broad coverage application, it is useful to identify lexical relations between word senses." ></td>
	<td class="line x" title="46:208	However, within corpora of a single language it is possible to identify automatically only relations at the word level, which are of course not useful for selecting word senses in that language." ></td>
	<td class="line x" title="47:208	This is where other languages can supply the solution, exploiting the fact that the mapping between words and word senses varies significantly among different languages." ></td>
	<td class="line x" title="48:208	For instance, the English words 'sign' and 'seal' correspond to a very large extent to two distinct senses of the Hebrew word 'lab_tom' (from example (1))." ></td>
	<td class="line x" title="49:208	These senses should be distinguished by most applications of Hebrew understanding programs." ></td>
	<td class="line x" title="50:208	To make this distinction, it is possible to do the same process that is performed for target word selection, by producing all the English alternatives for the lexical relations involving 'lahtom'." ></td>
	<td class="line x" title="51:208	Then the Hebrew sense which corresponds to the most plausible English lexical relations is preferred." ></td>
	<td class="line x" title="52:208	This process requires a bilingual lexicon which maps each Hebrew sense separately into its possible translations, similar 131 to a Hebrew-Hebrew-English lexicon (like the Oxford English-English-Hebrew dictionary (Hornby et al. , 1980))." ></td>
	<td class="line x" title="53:208	In some cases, different senses of a Hebrew word map to the same word also in English." ></td>
	<td class="line x" title="54:208	In these cases, the lexical relations of each sense cannot be identified in an English corpus, and a third language is required to distinguish among these senses." ></td>
	<td class="line x" title="55:208	As a long term vision, one can imagine a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses." ></td>
	<td class="line x" title="56:208	As explained above, this knowledge would be crucial for lexical disambiguation, and will also help to refine other types of knowledge acquired from large corpora 1." ></td>
	<td class="line x" title="57:208	2 The Linguistic Model The ambiguity of a word is determined by the number of distinct, non-equivalent representations into which the word can be mapped (Van Eynde et al. , 1982)." ></td>
	<td class="line x" title="58:208	In the case of machine translation the ambiguity of a source word is thus given by the number of target representations for that word in the bilingual lexicon of the translation system." ></td>
	<td class="line x" title="59:208	Given a specific syntactic context the ambiguity can be reduced to the number of alternatives which may appear in that context." ></td>
	<td class="line x" title="60:208	For instance, if a certain translation of a verb corresponds to an intransitive occurrence of that verb, then this possibility is eliminated when the verb occurs with a direct object." ></td>
	<td class="line x" title="61:208	In this work we are interested only in those ambiguities that are left after applying all the deterministic syntactic constraints." ></td>
	<td class="line x" title="62:208	For example, consider the following Hebrew sentence, taken from the daily Haaretz, September 1990: (4) Diplomatim svurim ki hitztarrfuto shell Hon Sun magdila et ha.sikkuyim l-hassagat hitqaddmut ba-sihot." ></td>
	<td class="line x" title="63:208	Here, the ambiguous words in translation to English are 'magdila', 'hitqaddmut' and 'sih_ot'." ></td>
	<td class="line x" title="64:208	To facilitate the reading, we give the translation of the sentence to English, and in each case of an ambiguous selection all the alternatives are listed within curly brackets, the first alternative being the correct one." ></td>
	<td class="line x" title="65:208	1For inatanoe, Hindie (1990) indicates the need to distlnguhsh among aeaases of polysemic words for his statistical c\]~Hic~tlon method." ></td>
	<td class="line x" title="66:208	132 (5) Diplomats believe that the joining of Hon Sun { increases I enlarges I magnifies } the chances for achieving { progress \[ advance I advancement } in the { talks I conversations I calls }." ></td>
	<td class="line x" title="67:208	We use the term a lezical relation to denote the cooccurrence relation of two (or possibly more) specific words in a sentence, having a certain syntactic relationship between them." ></td>
	<td class="line x" title="68:208	Typical relations are between verbs and their subjects, objects, complements, adverbs and modifying prepositional phrases." ></td>
	<td class="line x" title="69:208	Similarly, nouns are related also with their objects, with their modifying nouns in compounds and with their modifying adjectives and prepositional phrases." ></td>
	<td class="line x" title="70:208	The relational representation of a sentence is simply the list of all lexical relations that occur in the sentence." ></td>
	<td class="line x" title="71:208	For our purpose, the relational representation contains only those relations that involve at least one ambiguous word." ></td>
	<td class="line x" title="72:208	The relational representation for example (4) is given in (6) (for readability we represent the Hebrew word by its English equivalent, prefixed by 'H' to denote the fact that it is a Hebrew word): (6) a." ></td>
	<td class="line x" title="73:208	(subj-verb: H-joining H-increase) b." ></td>
	<td class="line x" title="74:208	(verb-obj: H-increase H-chance) c." ></td>
	<td class="line x" title="75:208	(verb-obj: H-achieve H-progress) d." ></td>
	<td class="line x" title="76:208	(noun-pp: H-progress H-in H-talks) The relational representation of a source sentence is reflected also in its translation to a target sentence." ></td>
	<td class="line x" title="77:208	In some cases the relational representation of the target sentence is completely equivalent to that of the source sentence, and can be achieved just by substituting the source words with target words." ></td>
	<td class="line x" title="78:208	In other cases, the mapping between source and target relations is more complicated, as is the case for the following German example: (7) Der Tisch gefaellt mir." ></td>
	<td class="line x" title="79:208	-I like the table." ></td>
	<td class="line x" title="80:208	Here, the original subject of the source sentence becomes the object in the target sentence." ></td>
	<td class="line x" title="81:208	This kind of mapping usually influences the translation process and is therefore encoded in components of the translation program, either explicitly or implicitly, especially in transfer based systems." ></td>
	<td class="line x" title="82:208	Our model assumes that such a mapping of source language relations to target language relations is possible, an assumption that is valid for many practical cases." ></td>
	<td class="line x" title="83:208	When applying the mapping of relations on one lexicai relation of the source sentence we get several alternatives for a target relation." ></td>
	<td class="line x" title="84:208	For instance, applying the mapping to example (6-c) we get three alternatives for the relation in the target sentence: (8) (verb-obj: achieve progress) (verb-obj: achieve advance) (verb-obj: achieve advancement) For example (6-d) we get 9 alternatives, since both 'H-progress' and 'H-talks' have three alternative translations." ></td>
	<td class="line x" title="85:208	In order to decide which alternative is the most probable, we count the frequencies of all the alternative target relations in very large corpora." ></td>
	<td class="line x" title="86:208	For example (8) we got the counts 20, 5 and 1 respectively." ></td>
	<td class="line x" title="87:208	Similarly, the target relation 'to increase chance' was counted 20 times, while the other alternatives were not observed at all." ></td>
	<td class="line x" title="88:208	These counts are given as input to the statistical model described in the next section, which performs the actual target word selection." ></td>
	<td class="line x" title="89:208	3 The Statistical Model Our selection algorithm is based on the following statistical model." ></td>
	<td class="line x" title="90:208	Consider first a single relation." ></td>
	<td class="line x" title="91:208	The linguistic model provides us with several alternatives as in example (8)." ></td>
	<td class="line x" title="92:208	We assume that each alternative has a theoretical probability Pi to be appropriate for this case." ></td>
	<td class="line x" title="93:208	We wish to select the alternative for which Pi is maximal, provided that it is significantly larger than the others." ></td>
	<td class="line x" title="94:208	We have decided to measure this significance by the odds ratio of the two most probable alternatives P = Pl/P2." ></td>
	<td class="line x" title="95:208	However, we do not know the theoretical probabilities, therefore we get a bound for p using the frequencies of the alternatives in the corpus." ></td>
	<td class="line x" title="96:208	Let/3 i be the probabilities as observed in the corpus (101 = ni/n, where ni is the number of times that alternative i appeared in the corpus and n is the total number of times that all the alternatives for the relation appeared in the corpus)." ></td>
	<td class="line x" title="97:208	For mathematical convenience we bound In p instead of p. Assuming that samples of the alternative relations are distributed normally, we get the following bound with confidence 1 a: where Z is the eonfidenee coefficient." ></td>
	<td class="line x" title="98:208	We approximate the variance by the delta method (e.g. Johnson and Wichern (1982)): =,n _ 1 p~(1 p~) -) 1 p~(1 p~) + 2 P*P~ p~ n p~ n npxI~ 1 1 1 1 1 1 = + ~, + =~+~." ></td>
	<td class="line x" title="99:208	npa nI~ n~x n~ nl n2 Therefore we get that with probability at least 1--or, In _> In Zl-a + We denote the right hand side (the bound) by B~,(nl, n2)." ></td>
	<td class="line x" title="100:208	In sentences with several relations, we consider the best two alternatives for each relation, and take the relation for which B,, is largest." ></td>
	<td class="line x" title="101:208	If this Ba is less than a specified threshold then we do not choose between the alternatives." ></td>
	<td class="line x" title="102:208	Otherwise, we choose the most frequent alternative to this relation and select the target words appearing in this alternative." ></td>
	<td class="line x" title="103:208	We then eliminate all the other alternative translations for the selected words, and accordingly eliminate all the alternatives for the remaining relations which involve these translations." ></td>
	<td class="line x" title="104:208	In addition we update the observed probabilities for the remaining relations, and consequently the remaining Ba's." ></td>
	<td class="line x" title="105:208	This procedure is repeated until all target words have been determined or the maximal Ba is below the threshold." ></td>
	<td class="line x" title="106:208	The actual parameters we have used so far were c~ = 0.05 and the bound for Bawas -0.5." ></td>
	<td class="line x" title="107:208	To illustrate the selection algorithm, we give the details for example (6)." ></td>
	<td class="line x" title="108:208	The highest bound for the odds ratio (Ba = 1.36) was received for the relation 'increase-chance', thus selecting the translation 'increase' for 'H-increase'." ></td>
	<td class="line x" title="109:208	The second was Ba = 0.96, 133 for 'achieve-progress'." ></td>
	<td class="line x" title="110:208	This selected the translations 'achieve' and 'progress', while eliminating the other senses of 'H-progress' in the remaining relations." ></td>
	<td class="line x" title="111:208	Then, for the relation 'progress-in-talks' we got Ba = 0.3, thus selecting the appropriate translation for 'H-talks'." ></td>
	<td class="line x" title="112:208	4 The Experiment An experiment was conducted to test the performance of the statistical model in translation from Hebrew and German to English." ></td>
	<td class="line x" title="113:208	Two sets of paragraphs were extracted randomly from current Hebrew and German press." ></td>
	<td class="line x" title="114:208	The Hebrew set contained 10 paragraphs taken from foreign news sections, while the German set contained 12 paragraphs of text not restricted to a specific topic." ></td>
	<td class="line x" title="115:208	Within these paragraphs we have (manually) identified the target word selection ambiguities, using a bilingual dictionary." ></td>
	<td class="line x" title="116:208	Some of the alternative translations in the dictionary were omitted if it was judged that they will not be considered by an actual component of a machine translation program." ></td>
	<td class="line x" title="117:208	These cases included very rare or archaic translations (that would not be contained in an MT lexicon) and alternatives that could be eliminated using syntactic knowledge (as explained in section 2) 2 . For each of the remaining alternatives, it was judged if it can serve as an acceptable translation in the given context." ></td>
	<td class="line x" title="118:208	This a priori judgment was used later to decide whether the selection of the automatic procedure is correct." ></td>
	<td class="line x" title="119:208	As a result of this process, the Hebrew set contained 105 ambiguous words (which had at least one unacceptable translation) and the German set 54 ambiguous words." ></td>
	<td class="line x" title="120:208	Now it was necessary to identify the lexical relations within each of the sentences." ></td>
	<td class="line x" title="121:208	As explained before, this should be done using a source language parser, and then mapping the source relations to the target relations." ></td>
	<td class="line x" title="122:208	At this stage of the research, we still do not have the necessary resources to perform the entire process automatically s, therefore we have approximated it by translating the sentences into English and extracting the lexical relations using the English Slot Grammar (ESG) parser (mc2Due to some technicalities, we have also restricted the experiment to cases in which all the relevant translations of a word consists exactly one English word, which is the most frequent situaticm." ></td>
	<td class="line x" title="123:208	awe are currently integrating this process within GSG (German Slot Gr~nmm') and LMT-GE (the Germs~a to English MT prototype)." ></td>
	<td class="line x" title="124:208	Cord, 1989) 4." ></td>
	<td class="line x" title="125:208	Using this parser we have classified the lexical relations to rather general classes of syntactic relations, based on the slot structure of ESG." ></td>
	<td class="line x" title="126:208	The important syntactic relations used were between a verb and its arguments and modifiers (counting as one class all objects, indirect objects, complements and nouns in modifying prepositional phrases) and between a noun and its arguments and modifiers (counting as one class all noun objects, modifying nouns in compounds and nouns in modifying prepositional phrases)." ></td>
	<td class="line x" title="127:208	The success of using this general level of syntactic relations indicates that even a rough mapping of source to target language relations would be useful for the statistical model." ></td>
	<td class="line x" title="128:208	The statistics for the alternative English relations in each sentence were extracted from three corpora: The Washington Post articles (about 40 million words), Associated Press news wire (24 million) and the Hansard corpus of the proceedings of the Canadian Parliament (85 million words)." ></td>
	<td class="line x" title="129:208	The statistics were extracted only from sentences of up to 25 words (to facilitate parsing) which contained altogether about 55 million words." ></td>
	<td class="line x" title="130:208	The lexical relations in the corpora were extracted by ESG, in the same way they were extracted for the English version of the example sentences (see Dagan and Itai (1990a) for a discussion on using an automatic parser for extracting lexical relations from a corpus, and for the technique of acquiring the statistics)." ></td>
	<td class="line x" title="131:208	The parser failed to produce any parse for about 35% of the sentences, which further reduced the actual size of the corpora which was used." ></td>
	<td class="line x" title="132:208	5 Evaluation Two measurements, applicability and precision, are used to evaluate the performance of the statistical model." ></td>
	<td class="line x" title="133:208	The applicability denotes the proportion of cases for which the model performed a selection, i.e. those cases for which the bound Bapassed the threshold." ></td>
	<td class="line x" title="134:208	The precision denotes the proportion of cases for which the model performed a correct selection out of all the applicable cases." ></td>
	<td class="line x" title="135:208	We compare the precision of the model to that of the 'word frequencies' procedure, which always selects the most frequent target word." ></td>
	<td class="line x" title="136:208	This naive 'straw-man' is less sophisticated than other methods suggested in the literature but it is useful as a common benchmark (e.g. Sadler (1989)) since it can 4The parsing process was controlled manually to make sure that we do not get wrong relational representation of the exo amp\]es due to parsing errors." ></td>
	<td class="line x" title="137:208	134 be easily implemented." ></td>
	<td class="line x" title="138:208	The success rate of the 'word frequencies' procedure can serve as a measure for the degree of lexical ambiguity in a given set of examples, and thus different methods can be partly compared by their degree of success relative to this procedure." ></td>
	<td class="line x" title="139:208	Out of the 105 ambiguous Hebrew words, for 32 the bound Badid not pass the threshold (applicability of 70%)." ></td>
	<td class="line x" title="140:208	The remaining 73 examples were distributed according to the following table: \[ Hebrew-Engiish \]\] Word Frequencies correct I incorrect I Relations Statistics \[ correct Thus the precision of the statistical model was 92% (67/73) 5 while relying just on word frequencies yields 64% (47/73)." ></td>
	<td class="line x" title="141:208	Out of the 54 ambiguous German words, for 22 the bound Badid not pass the threshold (applicability of 59%)." ></td>
	<td class="line x" title="142:208	The remaining 32 examples were distributed according to the following table: Oerm English II Word equeoci I,, correct \] incorrect Relations Statistics \[ correct 8 inrre ' \[\[ I 0 I Thus the precision of the statistical model was 75% (24/32), while relying just on word frequencies yields 53% (18/32)." ></td>
	<td class="line x" title="143:208	We attribute the lower success rate for the German examples to the fact that they were not restricted to topics that are well represented in the corpus." ></td>
	<td class="line x" title="144:208	Statistical analysis for the larger set of Hebrew examples shows that with 95% confidence our method succeeds in at least 86% of the applicable examples (using the parameters of the distribution of proportions)." ></td>
	<td class="line x" title="145:208	With the same confidence, our method improves the word frequency method by at least 18% (using confidence interval for the difference of proportions in multinomial distribution, where the four cells of the multinomial correspond to the four entries in the result table)." ></td>
	<td class="line x" title="146:208	In the examples that were treated correctly by our 5An a posteriorl observation showed that in three of the six errors the selection of the model was actually acceptable, and the a priori judgment of the hnman translator was too severe." ></td>
	<td class="line x" title="147:208	For example, in one of these cases the statistics selected the expression 'to begin talks' while the human translator regarded this expression as incorrect and selected 'to start talks'." ></td>
	<td class="line x" title="148:208	If we consider these cases as correct then there are only three selection errors, getting a 96% precision." ></td>
	<td class="line x" title="149:208	method, such as the examples in the previous sections, the statistics succeeded to capture two major types of disambiguating data." ></td>
	<td class="line x" title="150:208	In preferring 'signtreaty' upon 'seal-treaty', the statistics reflect the relevant semantic constraint." ></td>
	<td class="line x" title="151:208	In preferring 'peacetreaty' upon 'peace-contract', the statistics reflect the hxical usage of 'treaty' in English which differs from the usage of 'h_oze' in Hebrew." ></td>
	<td class="line x" title="152:208	6 Failures and Possible Improvements A detailed analysis of the failures of the method is most important, as it both suggests possible improvements for the model and indicates its limitations." ></td>
	<td class="line x" title="153:208	As described above, these failures include either the cases for which the method was not applicable (no selection) or the cases in which it made an incorrect selection." ></td>
	<td class="line x" title="154:208	The following paragraphs list the various reasons for both types." ></td>
	<td class="line x" title="155:208	6.1 Inapplicability Insufficient data." ></td>
	<td class="line x" title="156:208	This was the reason for nearly all the cases of inapplicability." ></td>
	<td class="line x" title="157:208	For instance, none of the alternative relations 'an investigator of corruption' (the correct one) or 'researcher of corruption' (the incorrect one) was observed in the parsed corpus." ></td>
	<td class="line oc" title="158:208	In this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of 'corruption' with either 'investigator' or 'researcher', without looking for any syntactic relation (as in Church and Hanks (1990))." ></td>
	<td class="line x" title="159:208	The use of this statistic is a subject for further research, but our initial data suggests that it can substantially increase the applicability of the statistical method with just a little decrease in its precision." ></td>
	<td class="line x" title="160:208	Another way to deal with the lack of statistical data for the specific words in question is to use statistics about similar words." ></td>
	<td class="line x" title="161:208	This is the basis for Sadler's Analogical Semantics (1989) which has not yet proved effective." ></td>
	<td class="line x" title="162:208	His results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words (such as in (Hindle, 1990))." ></td>
	<td class="line x" title="163:208	Conflicting data." ></td>
	<td class="line x" title="164:208	In very few cases two alternatives were supported equally by the statistical data, thus preventing a selection." ></td>
	<td class="line x" title="165:208	In such cases, both alternatives are valid at the independent level of the lexical relation, but may be inappropriate for the specific context." ></td>
	<td class="line x" title="166:208	For instance, the two alternatives of 'to take 135 a job' or 'to take a position' appeared in one of the examples, but since the general context concerned with the position of a prime minister only the latter was appropriate." ></td>
	<td class="line x" title="167:208	In order to resolve such examples it may be useful to consider also cooccurrences of the ambiguous word with other words in the broader context." ></td>
	<td class="line x" title="168:208	For instance, the word 'minister' seems to cooccur in the same context more frequently with 'position' than with 'job'." ></td>
	<td class="line x" title="169:208	In another example both alternatives were appropriate also for the specific context." ></td>
	<td class="line x" title="170:208	This happened with the German verb 'werfen', which may be translated (among other options) as 'throw', 'cast' or 'score'." ></td>
	<td class="line x" title="171:208	In our example 'werfen' appeared in the context of 'to throw/cast light' and these two correct alternatives had equal frequencies in the corpus ('score' was successfully eliminated)." ></td>
	<td class="line x" title="172:208	In such situations any selection between the alternatives will be appropriate and therefore any algorithm that handles conflicting data will work properly." ></td>
	<td class="line x" title="173:208	6.2 Incorrect Selection Using the inappropriate relation." ></td>
	<td class="line x" title="174:208	One of the examples contained the Hebrew word 'matzav', which two of its possible translations are 'state' and 'position'." ></td>
	<td class="line x" title="175:208	The phrase which contained this word was: 'to put an end to the {state I position} of war  '." ></td>
	<td class="line x" title="176:208	The ambiguous word is involved in two syntactic relations, being a complement of 'put' and also modified by 'war'." ></td>
	<td class="line x" title="177:208	The corresponding frequencies were: (9) verb-comp: put-position 320 verb-comp: put-state 18 noun-nob j: state-war 13 noun-nob j: position-war 2 The bound of the odds ration (Ba) for the first relation was higher than for the second, and therefore this relation determined the translation as 'position'." ></td>
	<td class="line x" title="178:208	However, the correct translation should be 'state', as determined by the second relation." ></td>
	<td class="line x" title="179:208	This example suggests that while ordering the involved relations (or using any other weighting mechanism) it may be necessary to give different weights to the different types of syntactic relations." ></td>
	<td class="line x" title="180:208	For instance, it seems reasonable that the object of a noun should receive greater weight in selecting the noun's sense than the verb for which this noun serves as a complement." ></td>
	<td class="line x" title="181:208	Confusing senses." ></td>
	<td class="line x" title="182:208	In another example, the Hebrew word 'qatann', which two of its meanings are 'small' and 'young', modified the word 'sikkuy', which means 'prospect' or 'chance'." ></td>
	<td class="line x" title="183:208	In this context, the correct sense is necessarily 'small'." ></td>
	<td class="line x" title="184:208	However, the relation that was observed in the corpus was 'young prospect', relating to the human sense of 'prospect' which appeared in sport articles (a promising young person)." ></td>
	<td class="line x" title="185:208	This borrowed sense of 'prospect' is necessarily inappropriate, since in Hebrew it is represented by the equivalent of 'hope' ('tiqva'), and not by 'sikkuy'." ></td>
	<td class="line x" title="186:208	The reason for this problem is that after producing the possible target alternatives, our model ignores the source language input as it uses only a monolingual target corpus." ></td>
	<td class="line x" title="187:208	This can be solved if we use an aligned bilingual corpus, as suggested by Sadler (1989) and Brown et al.(1990)." ></td>
	<td class="line x" title="189:208	In such a corpus the occurrences of the relation 'young prospect' will be aligned to the corresponding occurrences of the Hebrew word 'tiqva', and will not be used when the Hebrew word 'sikkuy' is involved." ></td>
	<td class="line x" title="190:208	Yet, it should be brought in mind that an aligned corpus is the result of manual translation, which can be viewed as a manual tagging of the words with their equivalent senses in the other language." ></td>
	<td class="line x" title="191:208	This resource is much more expensive and less available than the untagged monolingual corpus, while it seems to be necessary only for relatively rare situations." ></td>
	<td class="line x" title="192:208	Lack of deep understanding." ></td>
	<td class="line x" title="193:208	By their nature, statistical methods rely on large quantities of shallow information." ></td>
	<td class="line x" title="194:208	Thus, they are doomed to fail when disambiguation can rely only on deep understanding of the text and no other surface cues are available." ></td>
	<td class="line x" title="195:208	This happened in one of the Hebrew examples, where the two alternatives were either 'emigration law' or 'immigration law' (the Hebrew word 'hagira' is used for both subsenses)." ></td>
	<td class="line x" title="196:208	While the context indicated that the first alternative is correct, the statistics preferred the second alternative." ></td>
	<td class="line x" title="197:208	It seems that such cases are quiet rare, but only further evaluation will show the extent to which deep understanding is really needed." ></td>
	<td class="line x" title="198:208	7 Conclusions The method presented takes advantage of two linguistic phenomena: the different usage of words and word senses among different languages and the importance of lexical cooccurrences within syntactic relations." ></td>
	<td class="line x" title="199:208	The experiment shows that these phenomena are indeed useful for practical disambiguation." ></td>
	<td class="line x" title="200:208	We suggest that the high precision received in the experiment relies on two characteristics of the am136 biguity phenomena, namely the sparseness and redundancy of the disambiguating data." ></td>
	<td class="line x" title="201:208	By sparseness we mean that within the large space of alternative interpretations produced by ambiguous utterances, only a small portion is commonly used." ></td>
	<td class="line x" title="202:208	Therefore the chance of an inappropriate interpretation to be observed in the corpus (in other contexts) is low." ></td>
	<td class="line x" title="203:208	Redundancy relates to the fact that different informants (such as different lexical relations or deep understanding) tend to support rather than contradict one another, and therefore the chance of picking a 'wrong' informant is low." ></td>
	<td class="line x" title="204:208	The examination of the failures suggests that future research may improve both the applicability and precision of the model." ></td>
	<td class="line x" title="205:208	Our next goal is to handle inapplicable cases by using cooccurrence data regardless of syntactic relations and similarities between words." ></td>
	<td class="line x" title="206:208	We expect that increasing the applicability will lead to some decrease in precision, similar to the tradeoff between recall and precision in information retrieval." ></td>
	<td class="line x" title="207:208	Pursuing this tradeoff will improve the performance of the method and reveal its limitations." ></td>
	<td class="line x" title="208:208	8 Acknowledgments We would like to thank Mori Rimon, Peter Brown, Ayala Cohen, Ulrike Rackow, Herb Leass and Hans Karlgren for their help and comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P91-1019
Subject-Dependent Co-Occurrence And Word Sense Disambiguation
Guthrie, Joe A.;Guthrie, Louise;Aidinejad, Homa;Wilks, Yorick;"></td>
	<td class="line x" title="1:140	SUBJECT-DEPENDENT CO-OCCURRENCE AND WORD SENSE DISAMBIGUATION Joe A. Guthrie,* Louise Guthrie, Yorick Wilks, and Homa Aidinejad Computing Research LabomtoD, Box 30001 New Mexico State University Las Cruces, NM 88003-0001 ABSTRACT We describe a method for obtaining subject-dependent word sets relative to some (subjecO domain." ></td>
	<td class="line x" title="2:140	Using the subject classifications given in the machine-readable version of Longman's Dictionary of Contemporary English, we established subject-dependent cooccurrence links between words of the defining vocabulary to construct these 'neighborhoods'." ></td>
	<td class="line x" title="3:140	Here, we describe the application of these neighborhoods to information retrieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work." ></td>
	<td class="line oc" title="4:140	INTRODUCTION Word associations have been studied for some time in the fields of psycholinguistics (by testing human subjects on words), linguistics (where meaning is often based on how words co-occur with each other), and more recently, by researchers in natural language processing (Church and Hanks, 1990; Hindle and Rooth, 1990; Dagan, 1990; McDonald et al. , 1990; Wilks et al. , 1990) using statistical measures to identify sets of associated words for use in various natural language processing tasks." ></td>
	<td class="line x" title="5:140	One of the tasks where the statistical data on associated words has been used with some success is lexical disambiguation." ></td>
	<td class="line x" title="6:140	However, associated word sets gathered * Present address: Mathematics Department, University of Texas at k-:l Paso, El Paso, Tx 79968 from a general corpus may contain words that are associated with many different senses." ></td>
	<td class="line x" title="7:140	For example, vocabulary associated with the word 'bank' includes 'money', 'rob', 'river' and 'sand'." ></td>
	<td class="line x" title="8:140	In this paper, we describe a method for obtaining subjectdependent associated word sets, or 'neighborhoods' of a given word, relative to a particular (subject) domain." ></td>
	<td class="line x" title="9:140	Using the subject classifications of Longman's Dictionary of Contemporary English (LDOCE), we have established subject-dependent co-occurrence finks between words of the defining vocabulary to construct these neighborhoods." ></td>
	<td class="line x" title="10:140	We will describe the application of these neighborhoods to information reuieval, and present a method of word sense disambiguation based on these co-occurrences, an extension of previous work." ></td>
	<td class="line x" title="11:140	CO-OCCURRENCE NEIGHBORHOODS Words which occur frequently with a given word may be thought of as forming a 'neighborhood' of that word." ></td>
	<td class="line x" title="12:140	If we can determine which words (i.e. spelling forms) co-occur frequently with each word sense, we can use these neighborhoods to disambiguate the word in a given text." ></td>
	<td class="line x" title="13:140	Assume that we know of only two of the classic senses of the word bank: 1) A repository for money, and 2) A pile of earth on the edge of a river." ></td>
	<td class="line x" title="14:140	We can expect the 'money' sense of bank to co-occur frequently with such words 146 as 'money', 'loan', and 'robber', while the 'fiver' sense would be more frequently associated with 'river', 'bridge', and 'earth'." ></td>
	<td class="line x" title="15:140	In order to disambiguate 'bank' in a text, we would produce neighborhoods for each sense, and intersect them with the text, our assumption being that the neighborhood which shared more words with the text would determine the correct sense." ></td>
	<td class="line x" title="16:140	Variations of this idea appear in (l. ,esk, 1986; McDonald, et al. , 1990; Wilks, 1987; 1990; Veronis and Ide, 1990)." ></td>
	<td class="line x" title="17:140	Previously, McDonald and Plate (McDonald et al. , 1990; Schvaneveldt, 1990) used the LDOCE definitions as their text, in order to generate co-occurrence data for the 2,187 words in the LDOCE control (defining) vocabulary." ></td>
	<td class="line x" title="18:140	They used various methods to apply this data to the problem of disambiguating control vocabulary words as they appear in the LDOCE example sentences." ></td>
	<td class="line x" title="19:140	In every case however, the neighborhood of a given word was a co-occurrence neighborhood for its spelling form over all the definitions in the dictionary." ></td>
	<td class="line x" title="20:140	Distinct neighborhoods corresponding to distinct senses had to be obtained by using the words in the sense definition as a core for the neighborhood, and expanding it by combining it with additional words from the cooccurrence neighborhoods of the core words." ></td>
	<td class="line x" title="21:140	SUBJECT-DEPENDENT NEIGHBORHOODS The study of word co-occurrence in a text is based on the cliche that 'one (a word) is known by the company one keeps'." ></td>
	<td class="line x" title="22:140	We hold that it also makes a difference where that company is kept: since a word may occur with different sets of words in different contexts, we construct word neighborhoods which depend on the subject of the text in question." ></td>
	<td class="line x" title="23:140	We call these, naturally enough, 'subject-dependent neighborhoods'." ></td>
	<td class="line x" title="24:140	A unique feature of the electronic version of LDOCE is that many of the word sense definitions are marked with a subject field code which tells us which subject area the sense pertains to." ></td>
	<td class="line x" title="25:140	For example, the 'money'-related senses of bank are marked EC (Economics), and for each such main subject heading, we consider the subset of LDOCE definitions that consists of those sense definitions which sham that subject code." ></td>
	<td class="line x" title="26:140	These definitions are then collected into one file, and co-occurrence data for their defining vocabulary is generated." ></td>
	<td class="line x" title="27:140	Word x is said to co-occur with word y if x and y appear in the same sense definition; the total number of times they co-occur is denoted as We then construct a 2,187 x 2,187 matrix in which each row and column corresponds to one word of the defining vocabulary, and the entry in the xth row and yth column represents the number of times the xth word co-occurred with the yth word." ></td>
	<td class="line x" title="28:140	(This is a symmetric matrix, and therefore it is only necessary to maintain half of it)." ></td>
	<td class="line x" title="29:140	We denote by f, the total number of times word x appeared." ></td>
	<td class="line x" title="30:140	While many statistics may be used to measure the relatedness of words x and y, we used the function r (x,y ) = f x~." ></td>
	<td class="line x" title="31:140	in this study." ></td>
	<td class="line x" title="32:140	We choose a co-occurrence neighborhood of a word x from a set of closely related words." ></td>
	<td class="line x" title="33:140	We may choose the ten words with the highest relatedness statistic, for instance." ></td>
	<td class="line x" title="34:140	Neighborhoods of the word 'metal' in the category 'Economics' and 'Business' are presented below: Table 1." ></td>
	<td class="line x" title="35:140	Economics neighborhood of metal Subject Code EC ffi Economics metal idea coin them silver w, al should pocket gold well him Table 2." ></td>
	<td class="line x" title="36:140	Business neighborhood of recta/ Subject Code BU = Business metal bear apparatus mouth spring entrance plate tight sheet inside brags 147 In this example, the ~ghborhoods reflect a fundamental difference between the two subject areas." ></td>
	<td class="line x" title="37:140	Economics is a more theoretical subject, and therefore its neighborhood contains words like 'idea', 'gold', 'silver', and 'real', while in the more practical domain of Business, we find the words 'brass', 'apparatus', 'spring', and 'plate'." ></td>
	<td class="line x" title="38:140	We can expect the contrast between subject neighborhoods to be especially great for words with senses that fall into different subject areas." ></td>
	<td class="line x" title="39:140	Consider the actual neighborhoods of our original example, bank." ></td>
	<td class="line x" title="40:140	Table 3." ></td>
	<td class="line x" title="41:140	Economics neighborhood of bank bank Subject Code EC = Economies account cheque money by into have keep order out pay at put from draw an busy more supply it safe Table 4." ></td>
	<td class="line x" title="42:140	Engineering neighborhood of bank bank Subject Code EG = Engineering river wall flood thick earth prevent opposite chair hurry paste spread overflow walk help we throw clay then wide level Notice that even though we included the twenty most closely related words in each neighborhood, they are still unrelated or disjoint, although many of the words which appear in the lists are indeed suggestive of the sense or senses which fall under that subject category." ></td>
	<td class="line x" title="43:140	In LDOCE, three of the eleven senses of bank are marked with the code EC for Economics, and these represent the 'money' senses of the word." ></td>
	<td class="line x" title="44:140	It is a quirk of the classification in LDOCE that the 'river' senses of bank are not marked with a subject code." ></td>
	<td class="line x" title="45:140	This lack of a subject code for a word sense in LDOCE is not uncommon, however, and as was the case with bank, some word senses may have subject codes, while others do not." ></td>
	<td class="line x" title="46:140	We label this lack of a subject code the 'null code', and form a neighborhood of this type of sense by using all sense definitions without code as text." ></td>
	<td class="line x" title="47:140	This 'null code neighborhood' can reveal the common, or 'generic' sense of the word." ></td>
	<td class="line x" title="48:140	The twenty most frequently occurring words with bank in definitions with the null subject code form the following neighborhood: Table 5." ></td>
	<td class="line x" title="49:140	Null Code neighborhood of bank Subject Code NULL = no code assigned bank rob river account lend overflow flood money criminal lake flow snow cliff police shore heap thief borrow along steep earth It is obvious that approximately half of these words are associated with our two main senses of bank-but a new element has crept in: the appearance of four out of eight words which refer to the money sense ('rob', 'criminal', 'police', and 'thief') reveal a sense of bank which did not appear in the EC neighborhood." ></td>
	<td class="line x" title="50:140	In the null code definitions, there are quite a few references to the potential for a bank to be robbed." ></td>
	<td class="line x" title="51:140	Finally, for comparison, consider a neighborhood for bank which uses all the LDOCE definitions (see McDonald et al. , 1990; Schvaneveldt, 1990; Wilks et al. , 1990): Table 6." ></td>
	<td class="line x" title="52:140	Unrestricted neighborhood of bank Subject Code All bank account bank busy cheque criminal earn flood flow interest lake lend money overflow pay river rob safes and thief wall Only four of these words ('bank', 'cam', 'sand', and 'thief') are not found in 148 the other three neighborhoods, and the number of words in the intersection of this neighborhood with the Economics, Engineering, and Null neighborhoods are: six, four, and eleven, respectively." ></td>
	<td class="line x" title="53:140	Recalling that the Economics and Engineering neighborhoods are disjoint, this data supports our hypothesis that the subject-dependent neighborhoods help us to distinguish senses more easily than neighborhoods which are extracted from the whole dictionary." ></td>
	<td class="line x" title="54:140	There are over a hundred main subject field codes in LDOCE, and over threehundred sub-divisions within these." ></td>
	<td class="line x" title="55:140	For example, 'medicine-and-biology' is a main subject field (coded 'MD'), and has twentytwo sub-divisions such as 'anatomy' and 'biochemistry'." ></td>
	<td class="line x" title="56:140	These main codes and their sub-divisions constitute the only two levels in the LDOCE subject code hierarchy, and main codes such as 'golf' and 'sports' are not related to each other." ></td>
	<td class="line x" title="57:140	Cknrently, we use only the main codes when we are constructing a subject-dependent neighborhood." ></td>
	<td class="line x" title="58:140	But even this division of the definition text is fine enough so that, given a word and a subject code, the word may not appear in the definitions which have that subject code at all." ></td>
	<td class="line x" title="59:140	To overcome this problem, we have adopted a restructured hierarchy of the subject codes, as developed b~y Slator (1988)." ></td>
	<td class="line x" title="60:140	This tree structure has a node at the top, representing all the definitions." ></td>
	<td class="line x" title="61:140	At the next level are six fundamental categories such as 'science' and 'transportation', as well as the null code." ></td>
	<td class="line x" title="62:140	These clusters are further subdivided so that some main codes become sub-divisions of others ('golf' becomes a sub-division of 'sports', etc.)." ></td>
	<td class="line x" title="63:140	The maximum depth of this tree is five levels." ></td>
	<td class="line x" title="64:140	If the word for which we want to produce a neighborhood appears too infrequently in definitions with a given code, we travel up the hierarchy and expand the text under consideration until we have reached a point where the word appears frequently enough to allow the neighborhood to be constructed." ></td>
	<td class="line x" title="65:140	The worst case scenario would be one in which we had traveled all the way to the top of the hierarchy and used all the definitions as the text, only to wind up with the same co-occurrence neighborhoods as did McDonald and Plate (Schvaneveldt, 1990; Wilks et al. , 1990)!" ></td>
	<td class="line x" title="66:140	There are certain drawbacks in using LDOCE to construct the subject-dependent neighborhoods, however, the amount of text in LDOCE about any one subject area is rather limited, is comprised of a control vocabulary for dictionary definitions only, and uses sample sentences which were concocted with non-native English speakers in mind." ></td>
	<td class="line x" title="67:140	In the next phase of our research, large corpora consisting of actual documents from a given subject area will be used, in order to obtain neighborhoods which more accurately reflect the sorts of texts which will be used in applications." ></td>
	<td class="line x" title="68:140	In the future, these neighborhoods may replace those constructed from LDOCE, while leaving the subject code hierarchy and various applications intact." ></td>
	<td class="line x" title="69:140	WORD SENSE DISAMBIGUATION In this section, we describe an application of subject-dependent co-occurrence neighborhoods to the problem of word sense disambiguation." ></td>
	<td class="line x" title="70:140	The subject-dependent cooccurrence neighborhoods are used as building blocks for the neighborhoods used in disambiguation." ></td>
	<td class="line x" title="71:140	For each of the subject codes (including the null code) which appear with a word sense to be disambiguated, we intersect the corresponding subjectdependent co-occurrence neighborhood with the text being considered (the size of text can vary from a sentence to a paragraph)." ></td>
	<td class="line x" title="72:140	The intersection must contain a pre-selected minimum number of words to be considered." ></td>
	<td class="line x" title="73:140	But if none of the neighborhoods intersect at greater than this threshold level, we replace the neighborhood N by the neighborhood N(1), which consists of N together with the first word from each neighborhood of words in N, using the same subject code." ></td>
	<td class="line x" title="74:140	If necessary, we add the second most strongly associated word for each of the words in the original neighborhood N, forming the neighbor149 hood N(2)." ></td>
	<td class="line x" title="75:140	We continue this process until a subject-dependent co-occurrence neighborhood has intersection above the threshold level." ></td>
	<td class="line x" title="76:140	Then, the sense or senses with this subject code is selected." ></td>
	<td class="line x" title="77:140	If more than one sense has the selected code, we use their definitions as cores to build distinguishing neighborhoods for them." ></td>
	<td class="line x" title="78:140	These are again intersected with the text to determine the correct sense." ></td>
	<td class="line x" title="79:140	The following two examples illustrate this method." ></td>
	<td class="line x" title="80:140	Note that some of the neighborhoods differ from those given earlier since the text used to construct these neighborhoods includes any example sentences which may occur in the sense definitions." ></td>
	<td class="line x" title="81:140	Those neighborhoods presented earlier ignored the example sentences." ></td>
	<td class="line x" title="82:140	In each example, we attempt to disambiguate the word 'bank' in a sentence which appears as an example sentence in the Collins COBUILD English Language Dictionary." ></td>
	<td class="line x" title="83:140	The disambiguation consists of choosing the correct sense of 'bank' from among the thirteen senses given in LDOCE." ></td>
	<td class="line x" title="84:140	These senses are summarized below." ></td>
	<td class="line x" title="85:140	bank(l) : \[ \] : land along the side of a fiver, lake, etc. bank(2) : \[ \] : earth which is heaped up in a field or garden." ></td>
	<td class="line x" title="86:140	bank(3) : \[ \] : a mass of snow, clouds, mud, etc. bank(4) : \[AU\] : a slope made at bends in a road or race-track." ></td>
	<td class="line x" title="87:140	bank(5) : \[ \] : a sandbank in a river, etc. bank(6) : \[ALl\] : to move a ear or aircraft with one side higher than the other." ></td>
	<td class="line x" title="88:140	bank('/) : \[ \] : a row, especially of oars in an ancient boat or keys on a typewriter." ></td>
	<td class="line x" title="89:140	bank(8) : \[EC\] : a place in which money is kept and paid out on demand." ></td>
	<td class="line x" title="90:140	bank(9) : \[MD\] : a place where something is held ready for use, such as blood." ></td>
	<td class="line x" title="91:140	bank(10) : \[GB\] : (a person who keeps) a supply of money or pieces for payment in a gambling game." ></td>
	<td class="line x" title="92:140	bank(ll) : \[ \] : break the bank is to win all the money in bank(10)." ></td>
	<td class="line x" title="93:140	bank(12) : \[EC\] : to put or keep (money) in a bank." ></td>
	<td class="line x" title="94:140	bank(13) : \[EC\] : to keep ones money in a bank." ></td>
	<td class="line x" title="95:140	Example 1." ></td>
	<td class="line x" title="96:140	The sentence is 'Whe aircraft turned, banking slightly'." ></td>
	<td class="line x" title="97:140	The neighborhoods of 'bank' for the five relevant subject codes are given below." ></td>
	<td class="line x" title="98:140	Table 7." ></td>
	<td class="line x" title="99:140	Automotive neighborhood of bank Subject Code ALl = Automotive bank make go up move so they high also round car side turn road aircraft slope bend safe Table 8." ></td>
	<td class="line x" title="100:140	Economics neighborhood of bank bank Subject Code EC = Economics have it person out into take money put write keep pay order another paper draw supply account safe sum cheque Table 9." ></td>
	<td class="line x" title="101:140	Gambling neighborhood of bank bank Subject Code GB = Gambling person use money piece play keep pay game various supply chance Table 10." ></td>
	<td class="line x" title="102:140	Medical neighborhood of bank Subject Code MD Medicine and Biology bank something use place hold medicine ready blood human origin organ store hospital tream~ent product comb 150 Table 11." ></td>
	<td class="line x" title="103:140	Null Code neighborhood of bank bank Subject Code NULL = No code assigned game earth stone boat fiver bar snow lake sand shore mud framework flood cliff heap harbor ocean parallel overflow clerk The AU neighborhood contains two words, 'aircraft' and 'turn', which also appear in the sentence." ></td>
	<td class="line x" title="104:140	Note that we consider all forms of tum (tumed, tuming, etc)." ></td>
	<td class="line x" title="105:140	to match 'turn'." ></td>
	<td class="line x" title="106:140	Since none of the other neighborhoods have any words in common with the sentence, and since our threshold value for this short sentence is 2, AU is selected as the subject code." ></td>
	<td class="line x" title="107:140	We must now decide between the two senses which have this code." ></td>
	<td class="line x" title="108:140	At this point we remove the function words from the sense definitions and replace each remaining word by its root form." ></td>
	<td class="line x" title="109:140	We obtain the following neighborhoods." ></td>
	<td class="line x" title="110:140	Table 12." ></td>
	<td class="line x" title="111:140	Words in sense 4 of bank Definition bank(4) slope make bend road so they safe car go round Table 13." ></td>
	<td class="line x" title="112:140	Words in sense 6 of bank Definition bank(6) car aircraft move side high make turn Since bank(4) has no words in common with the sentence, and bank(6) has two Ctum' and 'aircraft'), bank(6) is selected." ></td>
	<td class="line x" title="113:140	This is indeed the sense of 'bank' used in the sentence." ></td>
	<td class="line x" title="114:140	Example 2." ></td>
	<td class="line x" title="115:140	The sentence is 'We got a bank loan to buy a car'." ></td>
	<td class="line x" title="116:140	The original neighborhoods of 'bank' are, of course, the same as in Example 1." ></td>
	<td class="line x" title="117:140	The threshold is again 2." ></td>
	<td class="line x" title="118:140	None of the neighborhoods has more than one word in common with the sentence, so the iterative process of enlarging the neighborhoods is used." ></td>
	<td class="line x" title="119:140	The AU neighborhood is expanded to include 'engine' since it is the first word in the AU neighborhood of 'make'." ></td>
	<td class="line x" title="120:140	The first word in the AU neighborhood of 'up' is 'increase', so 'increase' is added to the neighborhood." ></td>
	<td class="line x" title="121:140	If the word to be added already appears in the neighborhood of 'bank', no word is added." ></td>
	<td class="line x" title="122:140	On the fifteenth iteration, the EC neighborhood contains 'get' and 'buy'." ></td>
	<td class="line x" title="123:140	None of the other neighborhoods have more than one word in common with the sentence, so EC is selected as the subject code." ></td>
	<td class="line x" title="124:140	Definitions 8, 12, and 13 of bank all have the EC subject code, so their definitions are used as cores to build neighborhoods to allow us to choose one of them." ></td>
	<td class="line x" title="125:140	After twenty-three iterations, bank(8) is selected." ></td>
	<td class="line x" title="126:140	Experiments are underway to test this method and variations of it on large numbers of sentences so that its effectiveness may be compared with other disambiguation techniques." ></td>
	<td class="line x" title="127:140	Results of these experiments will be reported elsewhere." ></td>
	<td class="line x" title="128:140	FURTHER APPUCATIONS Several applications of subjectdependent neighborhoods in addition to word-sense disambiguation are being pursued, as well." ></td>
	<td class="line x" title="129:140	For information retrieval, previously constructed neighborhoods relevant to the subject area can be used to expand a query and the target (titles, key words, etc)." ></td>
	<td class="line x" title="130:140	to include more words in the intersection, and improve both recall and precision." ></td>
	<td class="line x" title="131:140	Another application is the determination of the subject area of a text." ></td>
	<td class="line x" title="132:140	Since the effectiveness of searching for key words to determine the topic of a text is limited by the choice of the particular list of key words, and the fact that the text may use synonyms or refer to the concept the key word represents without using it (for example by using a pronoun in its place), we could look for word associations (thereby involving more words in the process and making it less vulnerable to the above problems), 151 rather than simply searching for key words indicative of a topic." ></td>
	<td class="line x" title="133:140	Neighborhoods of words in the text could be constructed for each of the six fundamental categories, and intersected with the surrounding words in the text." ></td>
	<td class="line x" title="134:140	After choosing the category with the greatest intersection, we would then traverse the subject code tree downward to arrive at a more specific code, stopping at any point where there is not enough data to allow us to choose one code over the others at that level." ></td>
	<td class="line x" title="135:140	Once a subject code is selected for a text, it could be used as a context for word-sense disambiguation." ></td>
	<td class="line x" title="136:140	CONCLUSION Although the words in the LDOCE definitions constitute a small text (almost one million words, compared with the mega-texts used in other co-occurrence studies), the unique feature of subject codes which can be used to distinguish many definitions, and LDOCE's small control vocabulary (2,187 words) make it a useful corpus for obtaining co-occurrence data." ></td>
	<td class="line x" title="137:140	The development of techniques for information retrieval and word-sense disambiguation based on these subject-dependent cooccurrence neighborhoods is very promising indeed." ></td>
	<td class="line x" title="138:140	ACKNOWLEDGEMENTS This research was supported by the New Mexico State University Computing Research Laboratory through NSF Grant No." ></td>
	<td class="line x" title="139:140	IRI-8811108." ></td>
	<td class="line x" title="140:140	Grateful acknowledgement is accorded to all the members of the CRL Natural Language Group for their comments and suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P91-1027
Automatic Acquisition Of Subcategorization Frames From Untagged Text
Brent, Michael R.;"></td>
	<td class="line x" title="1:189	AUTOMATIC ACQUISITION OF SUBCATEGORIZATION FRAMES FROM UNTAGGED TEXT Michael R. Brent MIT AI Lab 545 Technology Square Cambridge, Massachusetts 02139 michael@ai.mit.edu ABSTRACT This paper describes an implemented program that takes a raw, untagged text corpus as its only input (no open-class dictionary) and generates a partial list of verbs occurring in the text and the subcategorization frames (SFs) in which they occur." ></td>
	<td class="line x" title="2:189	Verbs are detected by a novel technique based on the Case Filter of Rouvret and Vergnaud (1980)." ></td>
	<td class="line x" title="3:189	The completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus." ></td>
	<td class="line x" title="4:189	False positive rates are one to three percent of observations." ></td>
	<td class="line x" title="5:189	Five SFs are currently detected and more are planned." ></td>
	<td class="line x" title="6:189	Ultimately, I expect to provide a large SF dictionary to the NLP community and to train dictionaries for specific corpora." ></td>
	<td class="line x" title="7:189	1 INTRODUCTION This paper describes an implemented program that takes an untagged text corpus and generates a partial list of verbs occurring in it and the subcategorization frames (SFs) in which they occur." ></td>
	<td class="line x" title="8:189	So far, it detects the five SFs shown in Table 1." ></td>
	<td class="line x" title="9:189	SF Good Example Bad Example Description direct object direct object & clause direct object & infinitive clause infinitive greet them tell him he's a fool want him to attend know I'll attend hope to attend *arrive them *hope him he's a fool *hope him to attend *want I'll attend *greet to attend Table 1: The five subcategorization frames (SFs) detected so far The SF acquisition program has been tested on a corpus of 2.6 million words of the Wall Street Journal (kindly provided by the Penn Tree Bank project)." ></td>
	<td class="line x" title="10:189	On this corpus, it makes 5101 observations about 2258 orthographically distinct verbs." ></td>
	<td class="line x" title="11:189	False positive rates vary from one to three percent of observations, depending on the SF." ></td>
	<td class="line x" title="12:189	1.1 WHY IT MATTERS Accurate parsing requires knowing the subcategorization frames of verbs, as shown by (1)." ></td>
	<td class="line x" title="13:189	(1) a. I expected \[nv the man who smoked NP\] to eat ice-cream h. I doubted \[NP the man who liked to eat ice-cream NP\] Current high-coverage parsers tend to use either custom, hand-generated lists of subcategorization frames (e.g. , Hindle, 1983), or published, handgenerated lists like the Ozford Advanced Learner's Dictionary of Contemporary English, Hornby and Covey (1973) (e.g. , DeMarcken, 1990)." ></td>
	<td class="line x" title="14:189	In either case, such lists are expensive to build and to maintain in the face of evolving usage." ></td>
	<td class="line x" title="15:189	In addition, they tend not to include rare usages or specialized vocabularies like financial or military jargon." ></td>
	<td class="line x" title="16:189	Further, they are often incomplete in arbitrary ways." ></td>
	<td class="line x" title="17:189	For example, Webster's Ninth New Collegiate Dictionary lists the sense of strike meaning 'go occur to', as in 'it struck him that ', but it does not list that same sense of hit." ></td>
	<td class="line x" title="18:189	(My program discovered both)." ></td>
	<td class="line x" title="19:189	1.2 WHY IT'S HARD The initial priorities in this research were:." ></td>
	<td class="line x" title="20:189	Generality (e.g. , minimal assumptions about the text) . Accuracy in identifying SF occurrences  Simplicity of design and speed Efficient use of the available text was not a high priority, since it was felt that plenty of text was available even for an inefficient learner, assuming sufficient speed to make use of it." ></td>
	<td class="line x" title="21:189	These priorities 209 had a substantial influence on the approach taken." ></td>
	<td class="line x" title="22:189	They are evaluated in retrospect in Section 4." ></td>
	<td class="line x" title="23:189	The first step in finding a subcategorization frame is finding a verb." ></td>
	<td class="line x" title="24:189	Because of widespread and productive noun/verb ambiguity, dictionaries are not much use -they do not reliably exclude the possibility oflexical ambiguity." ></td>
	<td class="line x" title="25:189	Even if they did, a program that could only learn SFs for unambiguous verbs would be of limited value." ></td>
	<td class="line x" title="26:189	Statistical disambiguators make dictionaries more useful, but they have a fairly high error rate, and degrade in the presence of many unfamiliar words." ></td>
	<td class="line x" title="27:189	Further, it is often difficult to understand where the error is coming from or how to correct it." ></td>
	<td class="line x" title="28:189	So finding verbs poses a serious challenge for the design of an accurate, general-purpose algorithm for detecting SFs." ></td>
	<td class="line x" title="29:189	In fact, finding main verbs is more difficult than it might seem." ></td>
	<td class="line x" title="30:189	One problem is distinguishing participles from adjectives and nouns, as shown below." ></td>
	<td class="line x" title="31:189	(2) a. John has \[~p rented furniture\] (comp.: John has often rented apartments) b. John was smashed (drunk) last night (comp.: John was kissed last night) c. John's favorite activity is watching TV (comp.: John's favorite child is watching TV) In each case the main verb is have or be in a context where most parsers (and statistical disambiguators) would mistake it for an auxiliary and mistake the following word for a participial main verb." ></td>
	<td class="line x" title="32:189	A second challenge to accuracy is determining which verb to associate a given complement with." ></td>
	<td class="line x" title="33:189	Paradoxically, example (1) shows that in general it isn't possible to do this without already knowing the SF." ></td>
	<td class="line x" title="34:189	One obvious strategy would be to wait for sentences where there is only one candidate verb; unfortunately, it is very difficult to know for certain how many verbs occur in a sentence." ></td>
	<td class="line x" title="35:189	Finding some of the verbs in a text reliably is hard enough; finding all of them reliably is well beyond the scope of this work." ></td>
	<td class="line x" title="36:189	Finally, any system applied to real input, no matter how carefully designed, will occasionally make errors in finding the verb and determining its subcategorizatiou frame." ></td>
	<td class="line x" title="37:189	The more times a given verb appears in the corpus, the more likely it is that one of those occurrences will cause an erroneous judgment." ></td>
	<td class="line x" title="38:189	For that reason any learning system that gets only positive examples and makes a permanent judgment on a single example will always degrade as the number of occurrences increases." ></td>
	<td class="line x" title="39:189	In fact, making a judgment based on any fixed number of examples with any finite error rate will always lead to degradation with corpussize." ></td>
	<td class="line x" title="40:189	A better approach is to require a fixed percentage of the total occurrences of any given verb to appear with a given SF before concluding that random error is not responsible for these observations." ></td>
	<td class="line x" title="41:189	Unfortunately, determining the cutoff percentage requires human intervention and sampling error makes classification unstable for verbs with few occurrences in the input." ></td>
	<td class="line x" title="42:189	The sampling error can be dealt with (Brent, 1991) but predetermined cutoff percentages stir require eye-bailing the data." ></td>
	<td class="line x" title="43:189	Thus robust, unsupervised judgments in the face of error pose the third challenge to developing an accurate learning system." ></td>
	<td class="line x" title="44:189	1.3 HOW IT'S DONE The architecture of the system, and that of this paper, directly reflects the three challenges described above." ></td>
	<td class="line x" title="45:189	The system consists of three modules: 1." ></td>
	<td class="line x" title="46:189	Verb detection: Finds some occurrences of verbs using the Case Filter (Rouvret and Vergnaud, 1980), a proposed rule of grammar. 2." ></td>
	<td class="line x" title="47:189	SF detection: Finds some occurrences of five subcategorization frames using a simple, finite-state grammar for a fragment of English." ></td>
	<td class="line x" title="48:189	3." ></td>
	<td class="line x" title="49:189	SF decision: Determines whether a verb is genuinely associated with a given SF, or whether instead its apparent occurrences in that SF are due to error." ></td>
	<td class="line x" title="50:189	This is done using statistical models of the frequency distributions." ></td>
	<td class="line x" title="51:189	The following two sections describe and evaluate the verb detection module and the SF detection module, respectively; the decision module, which is still being refined, will be described in a subsequent paper." ></td>
	<td class="line x" title="52:189	The final two sections provide a brief comparison to related work and draw conclusions." ></td>
	<td class="line x" title="53:189	2 VERB DETECTION The technique I developed for finding verbs is based on the Case Filter of Rouvret and Verguaud (1980)." ></td>
	<td class="line x" title="54:189	The Case Filter is a proposed rule of grammar which, as it applies to English, says that every noun-phrase must appear either immediately to the left of a tensed verb, immediately to the right of a preposition, or immediately to the right of a main verb." ></td>
	<td class="line x" title="55:189	Adverbs and adverbial phrases (including days and dates) are ignored for the purposes of case adjacency." ></td>
	<td class="line x" title="56:189	A noun-phrase that satisfies the Case Filter is said to 'get case' or 'have case', while one that violates it is said to 'lack case'." ></td>
	<td class="line x" title="57:189	The program judges an open-class word to be a main verb if it is adjacent to a pronoun or proper name that would otherwise lack case." ></td>
	<td class="line x" title="58:189	Such a pronoun or proper name is either the subject or 210 the direct object of the verb." ></td>
	<td class="line x" title="59:189	Other noun phrases are not used because it is too difficult to determine their right boundaries accurately." ></td>
	<td class="line x" title="60:189	The two criteria for evaluating the performance of the main-verb detection technique are efficiency and accuracy." ></td>
	<td class="line x" title="61:189	Both were measured using a 2.6 million word corpus for which the Penn Treebank project provides hand-verified tags." ></td>
	<td class="line x" title="62:189	Efficiency of verb detection was assessed by running the SF detection module in the normal mode, where verbs were detected using the Case Filter technique, and then running it again with the Penn Tags substituted for the verb detection module." ></td>
	<td class="line x" title="63:189	The results are shown in Table 2." ></td>
	<td class="line x" title="64:189	Note SF direct object direct object &: clause direct object & infinitive clause infinitive Occurrences Found 3,591 94 310 739 367 Control 8,606 381 3,597 14,144 11,880 Efficiency 40% 25% 8% 5% 3% Table 2: Efficiency of verb detection for each of the five SFs, as tested on 2.6 million words of the Wall Street Journal and controlled by the Penn Treehank's hand-verified tagging the substantial variation among the SFs: for the SFs 'direct object' and 'direct object & clause' efficiency is roughly 40% and 25%, respectively; for 'direct object & infinitive' it drops to about 8%; and for the intransitive SFs it is under 5%." ></td>
	<td class="line x" title="65:189	The reason that the transitive SFs fare better is that the direct object gets case from the preceding verb and hence reveals its presence -intransitive verbs are harder to find." ></td>
	<td class="line x" title="66:189	Likewise, clauses fare better than infinitives because their subjects get case from the main verb and hence reveal it, whereas infinitives lack overt subjects." ></td>
	<td class="line x" title="67:189	Another obvious factor is that, for every SF listed above except 'direct object' two verbs need to be found -the matrix verb and the complement verb -if either one is not detected then no observation is recorded." ></td>
	<td class="line x" title="68:189	Accuracy was measured by looking at the Penn tag for every word that the system judged to be a verb." ></td>
	<td class="line x" title="69:189	Of approximately 5000 verb tokens found by the Case Filter technique, there were 28 disagreements with the hand-verified tags." ></td>
	<td class="line x" title="70:189	My program was right in 8 of these cases and wrong in 20, for a 0.24% error-rate beyond the rate using hand-verified tags." ></td>
	<td class="line x" title="71:189	Typical disagreements in which my system was right involved verbs that are ambiguous with much more frequent nouns, like mold in 'The Soviet Communist Party has the power to shape corporate development and mold it into a body dependent upon it '." ></td>
	<td class="line x" title="72:189	There were several systematic constructions in which the Penn tags were right and my system was wrong, including constructions like 'We consumers are' and pseudo-clefts like '~vhat you then do is you make them think  (These examples are actual text from the Penn corpus)." ></td>
	<td class="line x" title="73:189	The extraordinary accuracy of verb detection -within a tiny fraction of the rate achieved by trained human taggers -and it's relatively low efficiency are consistent with the priorities laid out in Section 1.2." ></td>
	<td class="line x" title="74:189	2.1 SF DETECTION The obvious approach to finding SFs like 'V NP to V' and 'V to V' is to look for occurrences of just those patterns in the training corpus; but the obvious approach fails to address the attachment problem illustrated by example (1) above." ></td>
	<td class="line x" title="75:189	The solution is based on the following insights:  Some examples are clear and unambiguous." ></td>
	<td class="line x" title="76:189	 Observations made in clear cases generalize to all cases." ></td>
	<td class="line x" title="77:189	 It is possible to distinguish the clear cases from the ambiguous ones with reasonable accuracy." ></td>
	<td class="line x" title="78:189	 With enough examples, it pays to wait for the clear cases." ></td>
	<td class="line x" title="79:189	Rather than take the obvious approach of looking for 'V NP to V', my approach is to wait for clear cases like 'V PRONOUN to V'." ></td>
	<td class="line x" title="80:189	The advantages can be seen by contrasting (3) with (1)." ></td>
	<td class="line x" title="81:189	(3) a. OK I expected him to eat ice-cream b. * I doubted him to eat ice-cream More generally, the system recognizes linguistic structure using a small finite-state grammar that describes only that fragment of English that is most useful for recognizing SFs." ></td>
	<td class="line x" title="82:189	The grammar relies exclusively on closed-class lexical items such as pronouns, prepositions, determiners, and auxiliary verbs." ></td>
	<td class="line x" title="83:189	The grammar for detecting SFs needs to distinguish three types of complements: direct objects, infinitives, and clauses." ></td>
	<td class="line x" title="84:189	The grammars for each of these are presented in Figure 1." ></td>
	<td class="line x" title="85:189	Any open-class word judged to he a verb (see Section 2) and followed immediately by matches for <DO>, <clause>, <infinitives, <DO><clanse>, or <DO><inf> is assigned the corresponding SF." ></td>
	<td class="line x" title="86:189	Any word ending in 'ly' or 211 <clause> := that?" ></td>
	<td class="line x" title="87:189	(<subj-pron> I <subj-obj-pron> <tensed-verb> <subj-pron> := I J he \[ she \[ I \[ they <subj-obj-pron> := you, it, yours, hers, ours, theirs <DO> := <obj-pron> <obj-pron> := me \[ him \[ us \[ them <infinitive> := to <previously-noted-uninflected-verb> I his I <proper-name>) Figure 1: A non-recursive (finite-state) grammar for detecting certain verbal complements." ></td>
	<td class="line x" title="88:189	indicates an optional element." ></td>
	<td class="line x" title="89:189	Any verb followed immediately expressions matching <DO>, <clause>, <infinitive>, <DO> <clause>, or <DO> <infinitive> is assigned the corresponding SF." ></td>
	<td class="line x" title="90:189	belonging to a list of 25 irregular adverbs is ignored for purposes of adjacency." ></td>
	<td class="line x" title="91:189	The notation 'T' follows optional expressions." ></td>
	<td class="line x" title="92:189	The category previously-noted-uninflected-verb is special in that it is not fixed in advance -open-class nonadverbs are added to it when they occur following an unambiguous modal." ></td>
	<td class="line x" title="93:189	I This is the only case in which the program makes use of earlier decisions -literally bootstrapping." ></td>
	<td class="line x" title="94:189	Note, however, that ambiguity is possible between mass nouns and uninflected verbs, as in to fish." ></td>
	<td class="line x" title="95:189	Like the verb detection algorithm, the SF detection algorithm is evaluated in terms of efficiency and accuracy." ></td>
	<td class="line x" title="96:189	The most useful estimate of efficiency is simply the density of observations in the corpus, shown in the first column of Table 3." ></td>
	<td class="line x" title="97:189	The SF direct object direct object & clause direct object & infinitive clause infinitive occurrences found 3,591 94 310 739 367 % error 1.5% 2.0% 1.5% 0.5% 3.0% Table 3: SF detector error rates as tested on 2.6 million words of the Wall Street Journal accuracy of SF detection is shown in the second 1If there were room to store an unlimited number of uninflected verbs for later reference then the grammar formalism would not be finite-state." ></td>
	<td class="line x" title="98:189	In fact, a fixed amount of storage, sufficient to store all the verbs in the language, is allocated." ></td>
	<td class="line x" title="99:189	This question is purely academic, however -a hash-table gives constant-time average performance." ></td>
	<td class="line x" title="100:189	column of Table 3." ></td>
	<td class="line x" title="101:189	2 The most common source of error was purpose adjuncts, as in 'John quit to pursue a career in finance,' which comes from omitting the in order from 'John quit in order to pursue a career in finance'." ></td>
	<td class="line x" title="102:189	These purpose adjuncts were mistaken for infinitival complements." ></td>
	<td class="line x" title="103:189	The other errors were more sporadic in nature, many coming from unusual extrapositions or other relatively rare phenomena." ></td>
	<td class="line x" title="104:189	Once again, the high accuracy and low efficiency are consistent with the priorities of Section 1.2." ></td>
	<td class="line x" title="105:189	The throughput rate is currently about ten-thousand words per second on a Sparcstation 2, which is also consistent with the initial priorities." ></td>
	<td class="line x" title="106:189	Furthermore, at ten-thousand words per second the current density of observations is not problematic." ></td>
	<td class="line x" title="107:189	3 RELATED WORK Interest in extracting lexical and especially collocational information from text has risen dramatically in the last two years, as sufficiently large corpora and sufficiently cheap computation have become available." ></td>
	<td class="line oc" title="108:189	Three recent papers in this area are Church and Hanks (1990), Hindle (1990), and Smadja and McKeown (1990)." ></td>
	<td class="line x" title="109:189	The latter two are concerned exclusively with collocation relations between open-class words and not with grammatical properties." ></td>
	<td class="line o" title="110:189	Church is also interested primarily in open-class collocations, but he does discuss verbs that tend to be followed by infinitives within his mutual information framework." ></td>
	<td class="line o" title="111:189	Mutual information, as applied by Church, is a measure of the tendency of two items to appear near one-another -their observed frequency in nearby positions is divided by the expectation of that frequency if their positions were random and independent." ></td>
	<td class="line o" title="112:189	To measure the tendency of a verb to be followed within a few words by an infinitive, Church uses his statistical disambiguator 2Error rates computed by hand verification of 200 examples for each SF using the tagged mode." ></td>
	<td class="line x" title="113:189	These are estimated independently of the error rates for verb detection." ></td>
	<td class="line x" title="114:189	212 (Church, 1988) to distinguish between to as an infinitive marker and to as a preposition." ></td>
	<td class="line x" title="115:189	Then he measures the mutual information between occurrences of the verb and occurrences of infinitives following within a certain number of words." ></td>
	<td class="line x" title="116:189	Unlike our system, Church's approach does not aim to decide whether or not a verb occurs with an infinitival complement -example (1) showed that being followed by an infinitive is not the same as taking an infinitival complement." ></td>
	<td class="line x" title="117:189	It might be interesting to try building a verb categorization scheme based on Church's mutual information measure, but to the best of our knowledge no such work has been reported." ></td>
	<td class="line x" title="118:189	4 CONCLUSIONS The ultimate goal of this work is to provide the NLP community with a substantially complete, automatically updated dictionary of subcategorization frames." ></td>
	<td class="line x" title="119:189	The methods described above solve several important problems that had stood in the way of that goal." ></td>
	<td class="line x" title="120:189	Moreover, the results obtained with those methods are quite encouraging." ></td>
	<td class="line x" title="121:189	Nonetheless, two obvious barriers still stand on the path to a fully automated SF dictionary: a decision algorithm that can handle random error, and techniques for detecting many more types of SFs." ></td>
	<td class="line x" title="122:189	Algorithms are currently being developed to resolve raw SF observations into genuine lexical properties and random error." ></td>
	<td class="line x" title="123:189	The idea is to automatically generate statistical models of the sources of error." ></td>
	<td class="line x" title="124:189	For example, purpose adjuncts like 'John quit to pursue a career in finance' are quite rare, accounting for only two percent of the apparent infinitival complements." ></td>
	<td class="line x" title="125:189	Furthermore, they are distributed across a much larger set of matrix verbs than the true infinitival complements, so any given verb should occur with a purpose adjunct extremely rarely." ></td>
	<td class="line x" title="126:189	In a histogram sorting verbs by their apparent frequency of occurrence with infinitival complements, those that in fact have appeared with purpose adjuncts and not true subcategorized infinitives will be clustered at the low frequencies." ></td>
	<td class="line x" title="127:189	The distributions of such clusters can be modeled automatically and the models used for identifying false positives." ></td>
	<td class="line x" title="128:189	The second requirement for automatically generating a full-scale dictionary is the ability to detect many more types of SFs." ></td>
	<td class="line x" title="129:189	SFs involving certain prepositional phrases are particularly chal: lenging." ></td>
	<td class="line x" title="130:189	For example, while purpose adjuncts (mistaken for infinitival complements) are relatively rare, instrumental adjuncts as in 'John hit the nail with a hammer' are more common." ></td>
	<td class="line x" title="131:189	The problem, of course, is how to distinguish them from genuine, subcategorized PPs headed by with, as in 'John sprayed the lawn with distilled water'." ></td>
	<td class="line x" title="132:189	The hope is that a frequency analysis like the one planned for purpose adjuncts will work here as well, but how successful it will be, and if successful how large a sample size it will require, remain to be seen." ></td>
	<td class="line x" title="133:189	The question of sample size leads back to an evaluation of the initial priorities, which favored simplicity, speed, and accuracy, over efficient use of the corpus." ></td>
	<td class="line x" title="134:189	There are various ways in which the high-priority criteria can be traded off against efficiency." ></td>
	<td class="line x" title="135:189	For example, consider (2c): one might expect that the overwhelming majority of occurrences of 'is V-ing' are genuine progressives, while a tiny minority are cases copula." ></td>
	<td class="line x" title="136:189	One might also expect that the occasional copula constructions are not concentrated around any one present participle but rather distributed randomly among a large population." ></td>
	<td class="line x" title="137:189	If those expectations are true then a frequency-modeling mechanism like the one being developed for adjuncts ought to prevent the mistaken copula from doing any harm." ></td>
	<td class="line x" title="138:189	In that case it might be worthwhile to admit 'is V-ing', where V is known to be a (possibly ambiguous) verb root, as a verb, independent of the Case Filter mechanism." ></td>
	<td class="line x" title="139:189	ACKNOWLEDGMENTS Thanks to Don Hindle, Lila Gleitman, and Jane Grimshaw for useful and encouraging conversations." ></td>
	<td class="line x" title="140:189	Thanks also to Mark Liberman, Mitch Marcus and the Penn Treebank project at the University of Pennsylvania for supplying tagged text." ></td>
	<td class="line x" title="141:189	This work was supported in part by National Science Foundation grant DCR-85552543 under a Presidential Young Investigator Award to Professor Robert C. Berwick." ></td>
	<td class="line x" title="142:189	References \[Brent, 1991\] M. Brent." ></td>
	<td class="line x" title="143:189	Semantic Classification of Verbs from their Syntactic Contexts: An Implemented Classifier for Stativity." ></td>
	<td class="line x" title="144:189	In Proceedings of the 5th European A CL Conference." ></td>
	<td class="line x" title="145:189	Association for Computational Linguistics, 1991." ></td>
	<td class="line xc" title="146:189	\[Church and Hanks, 1990\] K. Church and P. Hanks." ></td>
	<td class="line x" title="147:189	Word association norms, mutual information, and lexicography." ></td>
	<td class="line x" title="148:189	Comp." ></td>
	<td class="line x" title="149:189	Ling., 16, 1990." ></td>
	<td class="line x" title="151:189	\[Church, 1988\] K. Church." ></td>
	<td class="line x" title="152:189	A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text." ></td>
	<td class="line x" title="153:189	In Proceedings of the 2nd ACL Conference on Applied NLP." ></td>
	<td class="line x" title="154:189	ACL, 1988." ></td>
	<td class="line x" title="155:189	\[DeMarcken, 1990\] C. DeMarcken." ></td>
	<td class="line x" title="156:189	Parsing the LOB Corpus." ></td>
	<td class="line x" title="157:189	In Proceedings of the A CL. Assocation for Comp." ></td>
	<td class="line x" title="158:189	Ling., 1990." ></td>
	<td class="line x" title="159:189	\[Gleitman, 1990\] L. Gleitman." ></td>
	<td class="line x" title="160:189	The structural sources of verb meanings." ></td>
	<td class="line x" title="161:189	Language Acquisition, 1(1):3-56, 1990." ></td>
	<td class="line x" title="162:189	\[Hindle, 1983\] D. Hindle." ></td>
	<td class="line x" title="163:189	User Manual for Fidditch, a Deterministic Parser." ></td>
	<td class="line x" title="164:189	Technical Report 7590-142, Naval Research Laboratory, 1983." ></td>
	<td class="line x" title="165:189	\[Hindle, 1990\] D. Hindle." ></td>
	<td class="line x" title="166:189	Noun cl~sification from predicate argument structures." ></td>
	<td class="line x" title="167:189	In Proceedings of the 28th Annual Meeting of the ACL, pages 268-275." ></td>
	<td class="line x" title="168:189	ACL, 1990." ></td>
	<td class="line x" title="169:189	\[Hornby and Covey, 1973\] A. Hornby and A. Covey." ></td>
	<td class="line x" title="170:189	Ozford Advanced Learner's Dictionary of Contemporary English." ></td>
	<td class="line x" title="171:189	Oxford University Press, Oxford, 1973." ></td>
	<td class="line x" title="172:189	\[Levin, 1989\] B. Levin." ></td>
	<td class="line x" title="173:189	English Verbal Diathesis." ></td>
	<td class="line x" title="174:189	Lexicon Project orking Papers no. 32, MIT Center for Cognitive Science, MIT, Cambridge, MA., 1989." ></td>
	<td class="line x" title="176:189	\[Pinker, 1989\] S. Pinker." ></td>
	<td class="line x" title="177:189	Learnability and Cognition: The Acquisition of Argument Structure." ></td>
	<td class="line x" title="178:189	MIT Press, Cambridge, MA, 1989." ></td>
	<td class="line x" title="179:189	\[Rouvret and Vergnaud, 1980\] A. Rouvret and JR Vergnaud." ></td>
	<td class="line x" title="180:189	Specifying Reference to the Subject." ></td>
	<td class="line x" title="181:189	Linguistic Inquiry, 11(1), 1980." ></td>
	<td class="line x" title="182:189	\[Smadja and McKeown, 1990\] F. Smadja and K. McKeown." ></td>
	<td class="line x" title="183:189	Automatically extracting and representing collocations for language generation." ></td>
	<td class="line x" title="184:189	In 28th Anneal Meeting of the Association for Comp." ></td>
	<td class="line x" title="185:189	Ling., pages 252-259." ></td>
	<td class="line x" title="186:189	ACL, 1990." ></td>
	<td class="line x" title="187:189	\[Zwicky, 1970\] A. Zwicky." ></td>
	<td class="line x" title="188:189	In a Manner of Speaking." ></td>
	<td class="line x" title="189:189	Linguistic Inquiry, 2:223-233, 1970 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W91-0211
In So Many Words Knowledge As A Lexical Phenomenon
Meijs, Willem;Vossen, Piek;"></td>
	<td class="line x" title="1:227	IN SO MANY WORDS KNOWLEDGE AS A LEXICAL PHENOMENON Willem Meijs & Piek Vossen Amsterdam Dictionary Research Group English Department, Amsterdam University Spuistraat 210 1012 VT Amsterdam The Netherlands LINKS@ALF.!" ></td>
	<td class="line x" title="2:227	~.T.UVA.NL Abstract Lexical knowledge is knowledge that can be expressed in words." ></td>
	<td class="line x" title="3:227	Circular though this may seem, we think it provides a perfectly reasonable point of departure, for, in line with a long-standing philosophical tradition it posits communicability as the most characteristic aspect of lexical knowledge." ></td>
	<td class="line x" title="4:227	Knowledge representation systems should be designed so as to fit lexical data rather than the other way round." ></td>
	<td class="line oc" title="5:227	A broad view of the possible scope of lexical semantics would thus be one which tries to chart out the systematic, generalizable aspects of word meanings, and of the relations between words, drawing on readily accessible sources of lexical knowledge, such as machine readable dictionaries, encyclopedias, and representative corpora, coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources, for instance custom-built parsers to cope with dictionary definitions (Vossen 1990), statistical programs to deal with the distributional properties of lexical items in large corpora (Church & Hanks 1990) etc. At the same time this kind of massive data-acquisition should be made sensitive to the borders between perceptual experience, lexical knowledge and expert knowledge." ></td>
	<td class="line x" title="6:227	1 Introduction Many psychologists are quite content to declare that intelligence is what you measure by means of an intelligence test." ></td>
	<td class="line x" title="7:227	In a somewhat similar way one could say that lexical knowledge is the kind of knowledge that can be expressed in words." ></td>
	<td class="line x" title="8:227	An advantage of this position is that you can turn it round: knowledge that cannot be expressed in words is not lexical knowledge." ></td>
	<td class="line x" title="9:227	A further advantage is that it automatically relates lexical knowledge to something that lends itself to intersubjective inspection: linguistic utterances." ></td>
	<td class="line x" title="10:227	And yet another advantage is that it tells us how and where to look for typical expressions of lexical knowledge within the welter of linguistic utterances." ></td>
	<td class="line x" title="11:227	In ever widening concentric circles, we can inspect: general dictionaries as sources of the lexical knowledge that is associated with the 'common core' vocabulary; encyclopedias for additional knowledge associated with many of the items contained in the dictionaries and for lexically expressed information about all kinds of facts and events; specialiTed dictionaries for more expert knowledge in specific domains; manuals, treatises, newspaper articles, speeches, broadcasts and so forth for yet more details on anything and everything that humans find worth-while communicating about." ></td>
	<td class="line x" title="12:227	113 In this paper we want to defend this point of view by making clear that all knowledge is necessarily dependent on representation." ></td>
	<td class="line x" title="13:227	According to Dik (1986, 1989) this representation takes the form of either perceptual images or verbal structures (section 2)." ></td>
	<td class="line x" title="14:227	In section 3, we want to demonstrate that knowledge encoded in verbal structures is reducible to grammatical elements and lexical items (grammatical knowledge and lexical knowledge respectively)." ></td>
	<td class="line x" title="15:227	The insights gained from our work with machine readable dictionaries (MRDs) the Longman Dictionary of Contemporary English, henceforth LDOCE, and the Van Dale dictionary of contemporary Dutch 'Groot Woordenboek van het Hedendaags Nederlands', henceforth Van Dale, lead to the conclusion that lexical knowledge in the end is perceptual (in the form of primitive perceptual images) and grammatical (in the form of grammatical primitives)." ></td>
	<td class="line x" title="16:227	This makes all linguistically expressed knowledge ultimatily reducible to these primitives." ></td>
	<td class="line x" title="17:227	Since only linguistic knowledge is communicable, part of all knowledge is thus private (at least the knowledge corresponding to perceptual images)." ></td>
	<td class="line x" title="18:227	The structure of lexical knowledge as it emerges from our work, suggests a view on lexical meaning as not defining the object a word stands for but as representing a typical (not necessarily unique) conceptualization of that object (section 4)." ></td>
	<td class="line x" title="19:227	This hypothesis about lexical knowledge leads to the conclusion (section 5) that there are differences between lexical knowledge and expert-domain knowledge, in that the objective of the former is the use of a word as a symbol for a conceptualization of an object, and the objective of the latter is to have knowledge on (all and any imaginable) properties of the object itself." ></td>
	<td class="line x" title="20:227	2 The representation of knowledge Knowledge, of any kind, has to be stored in memory." ></td>
	<td class="line x" title="21:227	What is memoriT~hle can be distinguished in perceptual information and conceptual knowledge." ></td>
	<td class="line x" title="22:227	What we perceive is of course influenced by the concepts we have and, the other way round, concepts are partially based on our perception." ></td>
	<td class="line x" title="23:227	Still there is a clear difference between perceptual information and conceptual representation in that the former is tied to the capacities of the sensory-apparatus and the latter to cognitive fimitations to process information." ></td>
	<td class="line x" title="24:227	In order to reduce the cognitive load of continuously processing and storing an enormous amount of data, the incoming information from the sensory apparatus is categorized in terms of more general concepts which as much as possible abstract from irrelevant details." ></td>
	<td class="line x" title="25:227	By storing clusters of perceived features in terms of their conceptual categories all redundant information can be 'forgotten'." ></td>
	<td class="line x" title="26:227	This process of categorization has been described by Rosch (1977) in terms of two contrastive principles: i* ii." ></td>
	<td class="line x" title="27:227	a concept aims at capturing as many features as possible shared by its instantiations; in other words to be as specific as possible." ></td>
	<td class="line x" title="28:227	a concept aims at capturing as many instances as possible in order to reduce the number of needed categories; in other words to be as general as possible." ></td>
	<td class="line x" title="29:227	Ideal categories are in balance with regard to both principles, and hence provide the most efficient unit to store information." ></td>
	<td class="line x" title="30:227	The cognitive system will therefore probably represent incoming information as much as possible in terms of these ideal categories." ></td>
	<td class="line x" title="31:227	These conceptual categories thus constitute the building blocks of accumulating knowledge shaping, and being shaped, by the ungoing processing of information." ></td>
	<td class="line x" title="32:227	Any abstraction from actual experience, however, implies that there is some kind of representation system in which the concept is expressed." ></td>
	<td class="line x" title="33:227	Without representation knowledge cannot exist because there is no way to generalize over the individual experiences." ></td>
	<td class="line x" title="34:227	The category that represents all prototypical instances has to be 'named' somehow to be able to attach information to it that applies to all these instances." ></td>
	<td class="line x" title="35:227	This dependence of knowledge or 'symbolization' has important consequences." ></td>
	<td class="line x" title="36:227	As far as the above perceptual features are concerned there is evidence 114 that they are at least stored in terms of so-called prototype images (this also is intended to hold for perceptions as 'smell', 'emotions', 'touch', 'taste', 'hearing','movement')." ></td>
	<td class="line x" title="37:227	These are a kind of mental pictures which can 'be seen or experienced' through 'the mind's eye' and from which even inferences can be drawn (e.g. mental rotation tasks of three-dimensional objects having a twodimensional representation, Shepard and Metzler 1977)." ></td>
	<td class="line x" title="38:227	Not all knowledge, however, deals with perceptual features and this non-perceptual knowledge is therefore not bound to the perceptual apparatus of people." ></td>
	<td class="line x" title="39:227	Whatever perceptual features are stored by some prototypical image or mental picture of a 'bird', many of the corresponding concepts, for instance the fact that it is an 'animal' and therefore shares the feature 'living' with all other 'animals', cannot be captured by a purely perceptual image." ></td>
	<td class="line x" title="40:227	Having a prototypical image of an 'animal' probably does not even make sense (although there may be a stereotypical best example of it, probably some kind of mammal), because instances of perceived 'animals' are more likely categorized in terms of more specific (and therefore more informative) categories." ></td>
	<td class="line x" title="41:227	Non-l.rerceptual facts, generalizing over individual cases, have to be represented by some other conceptual language than perceptual images (other examples of non-perceptual knowledge are e.g. 'social status', 'kinship relations', 'skills', 'capacities')." ></td>
	<td class="line x" title="42:227	Dik (1986, 1989, see also Weigand 1989, Meijs 1989) argues that any representation language for non-perceptual knowledge, in addition to perceptual images, must be verbal for the following reasons: io ii." ></td>
	<td class="line x" title="43:227	.." ></td>
	<td class="line x" title="44:227	111." ></td>
	<td class="line x" title="45:227	iv." ></td>
	<td class="line x" title="46:227	If there is a common conceptual language for all rnarddnd, why are the actual natural languages so different from each other7 How can we explain the great difficulties in translating if human beings are supposed to have a common universal conceptual language?" ></td>
	<td class="line x" title="47:227	Why should speakers of natural languages have two systems to represent their information, a universal conceptual language and a natural language, when almost any conceivable content can be expressed in natural language?" ></td>
	<td class="line x" title="48:227	In practice, any actual description of conceptual structures uses words of some specific natural language." ></td>
	<td class="line x" title="49:227	If there is a language independent representation system then what are its symbols?" ></td>
	<td class="line x" title="50:227	Although the symbols of such a knowledge representation are tied to the actual content words of languages, Dik claims that the representation structure in which the content words are captured is more abstract than the actual linguistic expressions of any one language." ></td>
	<td class="line x" title="51:227	Such a structure should represent all the functional communicative and compositional aspects of linguistic expressions while at the same time it should abstrzct away from all language-dependent details of grammatical expression that are irrelevant from a conceptual point of view." ></td>
	<td class="line x" title="52:227	For instance, the fact that the expression of information has a passive or active form is irrelevant because it does not affect the relation between entities designated by the expression." ></td>
	<td class="line x" title="53:227	This point is nicely illustrated by the following two definitions in one and the same dictionary: Entry Word Dictionary Definition a. bray 2 0 n. b. hee-haw 0 1 n. the sound that a donkey makes the sound made by a donkey (Examples from LDOCE, 1978) Similarly, once the semantic interpretation of function words such as determiners, relative pronouns, prepositions, and of inflection information has been established in terms of argument positions, functions and operators in the grammar, these dements can be omitted from the underlying structure." ></td>
	<td class="line x" title="54:227	The following, for instance, is a Functional Grammar representation for both definitions: 115 a. \[ d, s (xl) : soUndN (xl) : { maker \[ i, s (x2) : donkeyN (x2) \]Ag Subj \[ R (xl) \] GoObj } (xl) \] b.\[d,s(xl):soUndN(xl):{makev\[i,s(x2):donkeyN(x2)\]AgObj \[ (xl)\]GoSubj}(xl)\] The structure as a whole designates a term with an entity reference index 'xl '." ></td>
	<td class="line x" title="55:227	The 'd' and 's' before the first occurrence of 'xl' stand for term operaators expressing definiteness and singularity respectively." ></td>
	<td class="line x" title="56:227	Of 'xl' two properties are stated: 'donkey' and a complex property between braces." ></td>
	<td class="line x" title="57:227	The complex property consists of an involvement of 'x 1' (expressed by the eo-referentiality) in an event designated by the main predicate 'make'." ></td>
	<td class="line x" title="58:227	The 'xl' fills the second argument slot of both structures having the semantic function 'Go' (Goal), whereas 'x2' (classified as a 'donkey') fills the first argument position of both structures ('i' and 's' are again term operators, 'i' standing for indefinite), having the semantic function 'Ag' (Agent)." ></td>
	<td class="line x" title="59:227	The only differences between the two structures are the assigment of the syntactic functions subject and object ('Subj' and 'Obj') and the relative pronoun marker 'R'." ></td>
	<td class="line x" title="60:227	These differences lead to the two different surface structures, but they do not affect the semantic interpretation of the structures, which is the same for both." ></td>
	<td class="line x" title="61:227	Note that this view of knowledge allows concepts to be represented by complex structures containing several word senses." ></td>
	<td class="line x" title="62:227	Given the fact that the repertory of expressions in natural language is theoretically unlimited, there is also no theoretical limitation on the possibility of representing some concept in the underlying structure." ></td>
	<td class="line x" title="63:227	3 The cyclic nature of linguistically expressed knowledge The advantage of taking a underlying linguistic-conceptual structure, which is claimed to be universal in Dik's theory of Functional Grammar (1989), as the format of representation is that it brings knowledge within the scope of so-called linguistic knowledge: i.e. knowledge is taken to be stored in verbal form." ></td>
	<td class="line x" title="64:227	Any knowledge, even expert knowledge or specialized symbolic systems such as the 'language' of mathematics or logic, has to have some interface with natural language to make it communicable." ></td>
	<td class="line x" title="65:227	To make the same point Weigand (1989) refers to the work of Gardner (1987), who has studied knowledge representation in the legal domain." ></td>
	<td class="line x" title="66:227	She admits that despite the rather sharply defined technical terms, many items, at some point, have to be grounded in ordinary language." ></td>
	<td class="line x" title="67:227	A law on 'dogs' may define the term 'own' but it does not define the word 'dog' (the so-called open texture problem; Hart 1961)." ></td>
	<td class="line x" title="68:227	The only solution is to relate expert or domain knowledge to general natural language." ></td>
	<td class="line x" title="69:227	Once the tools for analyzing natural language in terms of its semantic content are available it is possible to squeeze knowledge out of any linguistic utterance regardless of whether it is uttered as expert knowledge, broadcasts, speeches, newspaper articles, treatises, or in manuals." ></td>
	<td class="line x" title="70:227	For any utterance it will hold that its meaning can be explained in terms of the underlying linguistic-conceptual structure and the meaning of the content words that it contains: Meaning (expression) Meaning (underlying linguistic-conceptual structure) + Meaning (words) This means that expressions of expert knowledge in some natural language are subject to the same rules and restrictions imposed by the grammar on expressions of the language as a whole." ></td>
	<td class="line x" title="71:227	If we look at the following example of the entry 'water' in the Encyclopaedia Brittanica (1977) the language used to describe the knowledge is ordinary English, e.g.: water 19:633, a familiar substance composed of the chemical elements hydrogen and oxygen and existing in vapour, liquid, and several solid forms." ></td>
	<td class="line x" title="72:227	Water is essential to terrestrial life, participating 116 in virtuaUy every process that occurs in plant and animal organisms." ></td>
	<td class="line x" title="73:227	Although the molecules of water are simple in structure, the physical and chemical properties of the compound are extraordinarily complicated etc  Many words are familiar and common English words used in their non-expert meanings." ></td>
	<td class="line x" title="74:227	The expert words used, such as 'hydrogen' and 'oxygen', can be looked up again, mostly in the same l'l~Ource: hydrogen (from Greek hydroand genes, 'water former'), a colourless, odourless, tasteless, flammable, gaseous substance, the simplest member of the family of chemical elements;  Schematically, all knowledge is thus necessarily related to linguistic knowledge in the following way: Meaning (Expert word) = Meaning (Expert expression) Meaning (Expert expression) Meaning (underlying linguistic-conceptual structure) + Meaning (Expert words) + Meaning (Ordinary words) The meaning of the structure is defined in the grammar." ></td>
	<td class="line x" title="75:227	This definition takes care of very basic notions and presuppositions such as 'entity', 'event', 'state', 'change of state', 'causality', 'control', 'location', 'time', 'semantic roles', 'quantification', 'countability', etc. The rest of the content is based on the meaning of the ordinary words to be found in the lexicon as lexical knowledge." ></td>
	<td class="line x" title="76:227	Thus, to profit from the wealthy electronic resource of linguistically expressed knowledge that is accumulating every day, it will be necessary to get a hold on the semantics of linguistic utterances; that is to develop a grammar and provide a lexicon." ></td>
	<td class="line x" title="77:227	After that everything deserving of the description 'knowledge' (except perceptual or picturized knowledge) is fair game for lexical semantic hunters." ></td>
	<td class="line x" title="78:227	In view of the essential character of knowledge as outlined above, it is not surprising that lexical knowledge is stored in the form of expressions in natural language as well in dictionaries (although some dictionaries additionally also use pictures)." ></td>
	<td class="line x" title="79:227	The same concentric arrangement as described above can thus be applied to the lexical knowledge contained in dictionaries: Meaning (Ordinary word) = Meaning (Dictionary Expression) Meaning (Dictionary Expression) Meaning (underlying linguistic-conceptual stl'ucture) '6 Meaning (Ordinary words) The combined explanations and illustrations can thus be thought of as constituting a huge relational network or grid which must by necessity have a non-trivial correspondence with the knowledge that is being expressed." ></td>
	<td class="line x" title="80:227	Furthermore most explanations foUow a canonical format, so the relational network is also structured to a fairly high degree." ></td>
	<td class="line x" title="81:227	The study of machine-readable dictionaries in the past decade or so has made it possible to trace the relational network inherent in dictionary-definitions in great detail." ></td>
	<td class="line x" title="82:227	While these kind of studies have shown that the inherent network-organization is naturally far from perfect, they have also brought out that the overall contours are nevertheless clearly recognizable." ></td>
	<td class="line x" title="83:227	When, for instance, this concentric principle is systematically applied to the genus words of dictionary definitions the words of a language thus 117 form hierarchical chains like the following examples ffi'om LDOCE: Entry Word Sense Dictionary definition blue-bell 1 flower 1 2 plant 1 thing 1 object 1 any of various blue bell-shapedflowers the part of a plant, often beautiful and coloured, that produces seeds or fruit a p/ant that is grown for the beauty of this part a living thing that has leaves and roots, and grows usu." ></td>
	<td class="line x" title="84:227	in earth, esp. the kind smaller than trees any material object a thing The entry words are decomposed via the genus terms in ever more general classes until, necessarily, a circularity occurs." ></td>
	<td class="line x" title="85:227	Such chains, which have been generated for LDOCE nouns and verbs and Van Dale nouns, tied together form a semantic classification of the vocabulary of a language in terms of a small set of circularily defined words." ></td>
	<td class="line x" title="86:227	At least as far as nouns are concerned the circular top of this hierarchy can be described in terms of a semantic typology (Vossen fc.)." ></td>
	<td class="line x" title="87:227	This means that all nouns finally end up in a few typological primitives (collectives, individuals, masses), defined in the grammar." ></td>
	<td class="line x" title="88:227	This is of course not surprising in a dictionary like LDOCE (the machine-readable version of which we studied in the LINKS-project), which uses a restricted controlled vocabulary and starts from the stated aim of explaining the words which the user-learner may not know in terms of words which he does know." ></td>
	<td class="line x" title="89:227	However, our investigation of the Van Dale Dictionary of Contemporary Dutch in the context of the ESPRIT-project 'ACQUII.EX' has shown that the inherent organization of this dictionary shows a broadly similar pattern, and we have reason to assume that basically the same goes for any good general dictionary (of eg Amsler 1980)." ></td>
	<td class="line x" title="90:227	The relevance of these chains is not so much to get at the typological status of words but to be able to get at the properties or knowledge specified in the differentiae of the more general words in these chains, so that they can be inherited for the more specific words." ></td>
	<td class="line x" title="91:227	The fact that a 'blue-bell' is 'animate', for instance, is inherited via 'flower' from the properties described in the definition of 'plant'." ></td>
	<td class="line x" title="92:227	At the highest level in these hierarchies the definitions become rather void and no new properties are added." ></td>
	<td class="line x" title="93:227	At this point lexical knowledge has been dissolved in all the properties specified by the differentiae along the way, and the semantic primitive at the end." ></td>
	<td class="line x" title="94:227	If we look at the differentiae then they can basically be divided into stated properties and involvements in events." ></td>
	<td class="line x" title="95:227	The latter facts can be represented in the underlying linguistic-conceptual sU'ucture as the fillers of particular slots of the predicate designating that event (in the above examples second argument of 'produce', and 'grow', first argument of 'have')." ></td>
	<td class="line x" title="96:227	These slots can have various functions or roles, all of which are defined in terms of the semantics of the grammar (see section 2 for an example of the underlying structure)." ></td>
	<td class="line x" title="97:227	If, on the other hand, we trace down the static properties that are expressed in the differentiae by looking up their definitions in LDOCE then it appears that they often designate perceptual features: Properties: blue 1 colour 1 2 having the colour blue the quality which allows the eyes to see the difference between (for example) a red flower and a blue flower when both are the same size and shape red, blue, green, black, brown, yellow, white, etc. bell-shaped bell n . 1 not an entry in LDOCE a round hollow metal vessel, which makes a nnging sound when struck 118 round 1 2 circular 1 hoUow 1 shape n. 1 form 1 appearance 2 ~.rcul~r shaped like a ball round having an empty space inside the appearance or form of something that is seen shape; appearance that which can be seen live v . 1 alive 1 life 1 to be alive having life the active force that makes those forms of matter (animals and plants) that grow through feeding and produce new young forms like themselves, different from all other matter (stones, machines, objects, etc)." ></td>
	<td class="line x" title="98:227	mater/a/adj." ></td>
	<td class="line x" title="99:227	1 substance 1 matter 1 material n. 1 of or concerning matter or substance a material the material which makes up the world and everything in space which can be seen or touched, as opposed to thought or mind anything from which something is or may be made The oddity of defining perceptual knowledge in terms of words is apparent from obvious circularities such as 'blue = the colour blue', 'colour = red, blue, green, black, brown, yellow, white, etc', 'round = circular = round' (sic!)." ></td>
	<td class="line x" title="100:227	These properties at least justify a way of representing knowledge separate from the lexical means." ></td>
	<td class="line x" title="101:227	The verbal representation of this knowledge can only be seen as a pointer to some non-verbal perceptual image." ></td>
	<td class="line x" title="102:227	In the end all conceptual knowledge is thus reducible to relations between words (contained in a underlying linguistic.conceptual structure) which can in their turn be related to grammatical primitives and perceptual images." ></td>
	<td class="line x" title="103:227	Figure 1 represents the elaboration of knowledge from elementary primitive concepts defined in the grammar and perceptual images, to lexical primitives, to all lexical items, to layman knowledge upto expert knowledge: 119 I Expert Knowledge I Layman Knowledge !Lexical Knowledge I Lexical primitives I I Grammag." ></td>
	<td class="line x" title="104:227	primitives I I  I I I perceptual & motor I I I I experience I I I  I  ! !" ></td>
	<td class="line x" title="105:227	Figure i. Layers of knowledge The lexical primitives that end up in sensory-motor experiences represented by perceptual images and grammatical primitives, can be used to describe the higher order concepts designated by the other words of the lexicon." ></td>
	<td class="line x" title="106:227	The vocabulary as a whole together with the grammar provides the equipment to represent any conceptual content within layman knowledge." ></td>
	<td class="line x" title="107:227	Along the same lines, although more strict and systematic, expert knowledge is in its turn anchored in basic layman concepts." ></td>
	<td class="line x" title="108:227	Given the fact that only lexical knowledge is communicable, we have to conclude that some knowledge is private knowledge." ></td>
	<td class="line x" title="109:227	This seems to be very reasonable since, although we can talk quite happily about our visual, auditory and tactile (etc)." ></td>
	<td class="line x" title="110:227	experiences, it is ultimately impossible to actually communicate the essence of this kind of sensory-motor experience by lexical means." ></td>
	<td class="line x" title="111:227	Instead, that requires exchanging the actual experience in some way or other (pointing to a colour, playing a tune, giving a blow, etc.)." ></td>
	<td class="line x" title="112:227	When it comes to the crunch, there is a soft centre at the heart of any kind of lexical definition." ></td>
	<td class="line x" title="113:227	That soft centre is 'private meaning'." ></td>
	<td class="line x" title="114:227	However tight we weave the web of words, we can never be certain that the concept one language-user associates with a partictdar word is exactly the same as that associated with it by another language-user." ></td>
	<td class="line x" title="115:227	Dictionary definitions, of any kind, can thus only be approxirnative, never exhaustive." ></td>
	<td class="line x" title="116:227	4 The structure of lexical knowledge Since all knowledge, in the end, reduces to relations between words, structural constraints on these relations entailed by the lexicon are relevant to any kind of knowledge." ></td>
	<td class="line x" title="117:227	Each layer of knowledge elaborates on what is given by the inner area." ></td>
	<td class="line x" title="118:227	Given the hypothesis that languages can express anything, there is no restriction in expressing any piece of knowledge so that the anchoring in lexical knowledge only leaves room for a very weak version of the Sapir & Whorf hypothesis (that language may influence thought and therefore knowlege)." ></td>
	<td class="line x" title="119:227	Despite that, we have evidence that the synchronic results of the diachronic process of knowledge acquisition and lexicaliTation illustrate that there tendencies have played a role in the development of knowledge between lexical and 120 expert knowledge." ></td>
	<td class="line x" title="120:227	In a series of projects at the Univerisity of Amsterdam (ASCOT, LINKS) the syntactic and semantic information in LDOCE (and more recently in the framework of the Esprit project Acquflex, also the information contained in the Van Dale dictionaries of Contemporary Dutch) has been analysed and parsed (Akkerman e.a. 1988, Vossen 1990), and subsequently stored in a database, so that it is now possible to have systematic access to this knowledge." ></td>
	<td class="line x" title="121:227	From studying the classificational structures (see previous section) that arise from it on a large scale and cross-linguistically (Vossen 1991, Vossen fc)." ></td>
	<td class="line x" title="122:227	several conclusions on the structure of lexical knowledge can be drawn." ></td>
	<td class="line x" title="123:227	The main point to be made is that the way in which words are classified can vary, as illustrated in figure 2, in which some of the relations at the top of the noun-hierarchy from LDOCE are given: obj act  thing I I I I I I fruit something plant being I I I I I I I I material covering product vegetable person I I I I I I I I I substance skin soap spice body I I I PART I PART I I I I I I leather curry-powder organ bone I I I I I I liquid food flesh curd I I I I I I cream soup meat Figure 2." ></td>
	<td class="line x" title="124:227	Hierarchical relations in LDOCE Not all 'substances' are also found below 'substance': 'soap', 'spice' and 'curry-powder' are classified as 'products' instead." ></td>
	<td class="line x" title="125:227	A similar thing holds for 'skin' and 'flesh' which are primarily not defined as 'bodypart' but as 'covering' and 'substance' respectively." ></td>
	<td class="line x" title="126:227	As for 'foods' what is 'eatable' is also stored as 'fruit', 'vegetable', 'Liquid', 'flesh', or directly as 'substance', which do not directly relate to the node 'food'." ></td>
	<td class="line x" title="127:227	These differences in classifying words via their definition can be mainly traced back to two different modes of defining in dictionaries." ></td>
	<td class="line x" title="128:227	Either the involvement of the designated object in some event is stressed, resulting in classes such as 'food', 'covering', 'product' (in most cases of concrete non-animate objects its function), or a constitutional view is taken, as is the case with genus terms such as 'substance', 'Liquid', 'PARTOF body', 'plant','fruit', etc If we compare the classifications in LDOCE with their equivalents (equivalent according to the bilingual Dutch-English dictionary of Van Dale) in the hierarchy based on the Van Dale definitions, then these differences become even more striking." ></td>
	<td class="line x" title="129:227	Many EngLish words that are defined as some 'substance' in LDOCE (e.g. 'chocolate') have equivalent counterparts in Dutch (e.g. 'chocolade') which are primarily defined in terms of their function Cgenotmiddel' best translatable as 'means of enjoying by ingesting')." ></td>
	<td class="line x" title="130:227	The word 'middel' (equivalent to English 'means'), functions as a major node in the Dutch taxonomy, below which there are major compounds such as 'vervoerrniddel' ('means of transport'), 'hulpmiddel' ('means of help' or 'instrument'), 'medicijnmiddel' ('means of curing' or 'medicine'), 'voedingsmiddel' ('means of 121 nutrition' or 'food')." ></td>
	<td class="line x" title="131:227	In English the word 'means' has only been used for a very small set of words such as 'mnemonics', 'access', 'approach' (the total number of word-senses related to it is 3 I, whereas thousands of senses are related to the Dutch 'middeI'), and the kind of compounds which naturally lead to 'middel' in Dutch do not exist in English." ></td>
	<td class="line x" title="132:227	Instead of that English has different words that do not so ostensibly lead to a functional classification." ></td>
	<td class="line x" title="133:227	Figure 3 illustrates the difference between the two hierarchies: Dutch Inglish Taxonomy Taxonomy middel < means > / / / / / / \ \ \ \ \ \ Ichocolade\[ I I I I lautol I__MATCH I VIA BILINGUAL DICT." ></td>
	<td class="line x" title="134:227	MATCH VIA BILINGUAL DICT." ></td>
	<td class="line x" title="135:227	substance vehicle / \ / \ / \ instrument / \ means /\ / \ /\ / \ / \/\/ .\I oar I Ichocolatel I I I I \ \ \ \ \ \ \ Figure 3." ></td>
	<td class="line x" title="136:227	Hierarchical differences between LDOCE and Van Dale Although both languages are culturally and historically closely related, large parts of the Dutch and English hierarchies differ considerably because the lexicographer's equipment to describe the concepts i.e. the vocabularies of the two languages and hence the conceptualizations enforced by them differ in crucial respects." ></td>
	<td class="line x" title="137:227	The Dutch lexicographers have probably been triggered by the 'middel' compounds to relate them to the functional node 'rniddel'." ></td>
	<td class="line x" title="138:227	English lexicographers, on the other hand, were faced with words (such as 'vehicle', 'substance', 'medicine', 'food') that do not evoke 'means' as a word for classification." ></td>
	<td class="line x" title="139:227	There are two ways of dealing with this variation in defining." ></td>
	<td class="line x" title="140:227	Either the lexicographers have been inconsistent and one way of defining should be preferred over the other (although this may be more difficult to formulate in some cases than in others), or the work of lexicographers, in principle, is to be taken seriously and the fact that these words are differently classified is meaningful (in practice, examples of both interpretations can be found)." ></td>
	<td class="line x" title="141:227	The latter view is most relevant for our discussion because it implies that lexical meaning is not always the same as equivalence according to a bilingual dictionary." ></td>
	<td class="line x" title="142:227	Although Dutch 'chocolade' and English 'chocolate' are equivalent translation pairs, they still have different lexical meanings." ></td>
	<td class="line x" title="143:227	For translation purposes, substitution of words in all contexts is a sufficient criterion." ></td>
	<td class="line x" title="144:227	Words, however, can have the same referential meaning but still stand for different concepts (Frege's Sinn und Bedeutung)." ></td>
	<td class="line x" title="145:227	In this view the lexical meaning as described in dictionary definitions would be taken to stand for a particular conceptualization of a possible set of referents, whereas that same set of referents can also be conceptualized in many other ways." ></td>
	<td class="line x" title="146:227	In this sense dictionary definitions do not 'define' the object of reference, that is give the List of properties that uniquely discriminate it from all other objects, but merely tell you that using this word means that you have to look at the 122 object from this perspective (conceptualize it in such and such way)." ></td>
	<td class="line x" title="147:227	Within such a view of lexical meaning, variation in classification as in the follwing examples makes sense: Entry Word Dictionary Definition armour blanket carpet daub strong protective covering on fighting vehicles, a thick, esp. woollen covering heavy woven often woollen mater/a/for covering floors or stairs (a) soft sticky mazeria/for covering surfaces like walls (Examples from LDOCE, 1978) When using the word 'blanket' or 'armour' to refer to some object you are told to look at it primarily as a 'covering' and secondarily as some kind of 'material', whereas 'carpet' and 'daub', as defined here, stress the constitution of the object instead of its function 'covering'." ></td>
	<td class="line x" title="148:227	5 Different objectives of lexical and expert-domain knowledge The view on general lexical knowledge developed above makes it to a certain degree different from other types of knowledge." ></td>
	<td class="line x" title="149:227	Expert knowledge, and to some extent also part of the layman knowledge deals with knowledge about the object, whereas in case of general lexical knowledge not knowledge about the object but knowing that a particular word stands for a particular conceptualization (that can be used to refer to a corresponding objec0 is the goal." ></td>
	<td class="line x" title="150:227	This perfectly fits in with the lexicographer's tradition of describing the semantic properties of words, in which the 'use of words' has always prevailed and not the knowledge about the object." ></td>
	<td class="line x" title="151:227	A word as such is no more than a vehicle of information within a particular context." ></td>
	<td class="line x" title="152:227	This does not necessarily imply that the 'meaning of a word' (the information it carries over) equals all the knowledge people have on the object or class of objects that is designated by that word, as is obvious (at least since Galileo) from the following examples: Entry Word Dictionary Definition sunrise sunset the time when the sun is seen to appear after the night the time when the sun is seen to disappear as night begins (Examples from LDOCE, 1978) sunrise sunset the apparent rising of the sun above the horizon the apparent descent of the sun below the horizon (Examples from Webster, 1974) zonsopgang (sunrise) zonsondergang (sunset) opgaan van de zon (rising-above of the sun) bet ondergaan van de zon (the descending-below of the sun) (Examples from Van Dale, 1984)." ></td>
	<td class="line x" title="153:227	The words 'sunrise' and 'sunset' will probably be used in collocations that trigger the conceptualization associated with the words and not the knowledge we have about 'stars', 'planets' and their 'orbits'." ></td>
	<td class="line x" title="154:227	Exactly these collocations form the material for lexicographers to extract the lexical meanings of words." ></td>
	<td class="line x" title="155:227	Furthermore, words not only give different conceptualizations of the same object but can 123 also function as a vehicle of attitudinal and social information, as in the following synonymous variants found in LDOCE on the basis of similar definitions: Entry Word Dictionary Definition bobby b~ ~pper flatfoot peeler pig c~ infml BrE a policeman sl, esp. AmE a policeman infml a policeman sl a policeman BrE oM sl a policeman sl policeman infml policeman (Examples from LDOCE, 1978) All these words designate the same set of potential referents, but they all carry a different attitudinal or social signal along expressed by the labels 'infml' (=informal), 'sl' (=slang), 'AmE' (=American English), 'BrE' ( Bfittish English), 'old' (=obselete), which tell us in what kind of context the words can be found (or used) but which has little to do with the 'knowledge we have of policemen'." ></td>
	<td class="line x" title="156:227	As a consequence of these different objectives of lexical and expert-domain knowledge, the connection between the different layers of knowledge, as represented in figure 1, is not allways that smooth." ></td>
	<td class="line x" title="157:227	For some domains in which the process of lexicalizafion has clearly been triggered by the straightforward acquisition of knowledge through science and education, such as animal and plant names, there may be a rather regular connection between lexical, layman and expert knowledge in which more and more detailed information is added." ></td>
	<td class="line x" title="158:227	As such part of the expert knowledge has become general lexical knowledge as well over time (perhaps a process in parallel to that of 'Gesunkenes Kulturgut')." ></td>
	<td class="line x" title="159:227	However in many domains, and even in animal and plant names (e.g. compare the functional 'watchdog', 'sheepdog', 'pet' with the different species of animal in terms of their constitution), other conceptual~tions of objects cross-classify with e.g. expert knowledge." ></td>
	<td class="line x" title="160:227	Lexical knowledge thus gives you a wider range of views than expert knowledge." ></td>
	<td class="line x" title="161:227	Something in the external world can be looked upon from all kinds of perspectives correlating with different words." ></td>
	<td class="line x" title="162:227	For instance, while in Dutch we can use either 'theewater' (literally 'tea water') or 'koffiewater' (literally 'coffee water') to refer to the water that we are putting on to boil depending on its intended use, it would still be H20 to the expert." ></td>
	<td class="line x" title="163:227	Although the expert's knowledge of 'water' is more specific, the linguistic equipment to refer to 'water' illustrates a greater variation in possible perspectives." ></td>
	<td class="line x" title="164:227	In this sense, expert knowledge can be seen as more demilled and systematic subtype of lexical knowledge from a more resu'icted perspective." ></td>
	<td class="line x" title="165:227	Where we expect that expert knowledge is exhaustive and defining, lexical knowledge thus is typical and associative but hardly ever exhaustive." ></td>
	<td class="line x" title="166:227	Only those properties are described which typically mark the conceptualization that is associated with it." ></td>
	<td class="line x" title="167:227	124 References Akkerman E. , H. Voogt-van Zutpben, and W. Meijs, 'A computerized lexicon for word-level tagging', Ascot report No 2." ></td>
	<td class="line x" title="168:227	In: J. Aarts and W. Meijs (eds) Language and Computers." ></td>
	<td class="line x" title="169:227	No 1, Rodopi, Amsterdam, 1988." ></td>
	<td class="line x" title="170:227	Amsler tLA., 'The structure of the Merriam-Webster Pocket Dictionary', Ph.D. Thesis Texas University, 1980." ></td>
	<td class="line x" title="172:227	Berlin B. , 'Speculations on the growth of etnobotanical nomenclature', In: Language in Society, I, 1972, pp." ></td>
	<td class="line x" title="173:227	51-86." ></td>
	<td class="line x" title="174:227	Copestake A. , 'An approach to building the hierarchical element of a lexical knowledge base from a machine readable dictionary', Computer Laboratory, University of Cambridge, 1990." ></td>
	<td class="line oc" title="175:227	Church K.W., and P. Hanks, 'Word association norms, mutual information, and lexicography', Computational Linguistics,16/l, 1990, pp." ></td>
	<td class="line x" title="176:227	22-29." ></td>
	<td class="line x" title="177:227	Cruse D.A., 'Lexical Semantics', Cambridge, CUP, 1986." ></td>
	<td class="line x" title="178:227	Davidson D. , 'The method of truth in metaphysics', In K.Baynes, J. Bohman & Th." ></td>
	<td class="line x" title="179:227	McCarthy (eds), After Philosophy: End or Transformation?, Cambridge Mass.: MIT Press, 1987." ></td>
	<td class="line x" title="180:227	Dik S.C., 'Stepwise Lexical Decomposition', Lisse: Peter de Ridder Press, 1978 \[available through Working Papers in FG\]." ></td>
	<td class="line x" title="181:227	Dik S.C., 'Linguistically Motivated Knowledge Representation', Working Papers in Functional Grammar, No. 9." ></td>
	<td class="line x" title="182:227	Amsterdam, 1986." ></td>
	<td class="line x" title="183:227	Dik S.C., 'The theory of functional grammar." ></td>
	<td class="line x" title="184:227	Part I: The structure of the clause', Functional grammar series 9, Foris, Dordrecht, 1989." ></td>
	<td class="line x" title="185:227	Dummett M. , 'Can analytic philosophy be systematic?', In K. Baynes, J. Bohman & Th." ></td>
	<td class="line x" title="186:227	McCarthy (eds), After Philosophy: End or Transformation?, Cambridge Mass.: M1T Press, 1987." ></td>
	<td class="line x" title="187:227	'Encyclopaedia Britannica', Micropaedia, 15th edition,1977." ></td>
	<td class="line x" title="188:227	Frawley W. , 'Discussion: in defense of the dictionary: a response to Haiman', In: Lingua 55, 1981, pp.53-61." ></td>
	<td class="line x" title="189:227	Frege G. , 'Ueber Sinn and Bedeutung', In: G. Potzi Funktion, Begriff, Bedeumng, Gottingen, 1962, pp." ></td>
	<td class="line x" title="190:227	40-65." ></td>
	<td class="line x" title="191:227	Gardner, A. von der Lieth, 'An artificial intelligence approach to legal reasoning', MIT Press, cambridge, MA, 1977." ></td>
	<td class="line x" title="192:227	Grice H.P., 'Logic and conversation', In: P. Cole & J.L. Morgan (eds) Syntax and Semantics 3." ></td>
	<td class="line x" title="193:227	Speech Acts, New York/London, 1975, pp." ></td>
	<td class="line x" title="194:227	41-58." ></td>
	<td class="line x" title="195:227	Haiman J. , 'Dictionaries and Encyclopedias', In: Lingua 50, 1980, pp." ></td>
	<td class="line x" title="196:227	329-357." ></td>
	<td class="line x" title="197:227	Haiman J. 'Discussion: Dictionaries and Encyclopedias again', In: Lingua 56, 1982, pp." ></td>
	<td class="line x" title="198:227	353-355." ></td>
	<td class="line x" title="199:227	Hart H.L.A, 'The concept of law', Claredon Press, Oxford, 1961." ></td>
	<td class="line x" title="200:227	Proctor P." ></td>
	<td class="line x" title="201:227	(ed), ''nae Longman dictionary of contemporary English', London, Longman, 1978." ></td>
	<td class="line x" title="202:227	Martin W. , and G.AJ." ></td>
	<td class="line x" title="203:227	Tops, 'Groot Woordenboek Engels-Nederlands, VanDale Lexicografie', Utrecht, 1984." ></td>
	<td class="line x" title="204:227	Martin W. , and G.A.J. Tops, 'Groot Woordenboek Nederlands-Engels', VanDale Lexicografie, Utrecht, 1984." ></td>
	<td class="line x" title="205:227	Medin D.L., and E.E. Smith, 'Concepts and concept formation', In: Annual Review Psychology 35, 1984, pp." ></td>
	<td class="line x" title="206:227	113-138." ></td>
	<td class="line x" title="207:227	Meijs WJ., 'Spreading the word: knowledge-activation in a functional perspective', In J.H. CormoUy & S.C. Dik (eds),Functional Grammar and the Computer, Dordrecht, Foils, 1989." ></td>
	<td class="line x" title="209:227	Rosch E. , 'Classification of real world objects: origins and representation in cognition', In: P.N. Johnson-Laird and P.C. Wason, Thinking: Readings in cognitive science." ></td>
	<td class="line x" title="210:227	Cambridge, 1977, pp." ></td>
	<td class="line x" title="211:227	212-222." ></td>
	<td class="line x" title="212:227	Rosch E. , 'Linguistic Relativity', In: P.N. Johnson-Laird and P.C. Wason, Thinking: Readings in cognitive science." ></td>
	<td class="line x" title="213:227	Cambridge, 1977, pp." ></td>
	<td class="line x" title="214:227	501-52 I. Shepard RAN., and J. Metzler, 'Mental rotation of three-dimensional objects', In: P.N. Johnson-Laird and P.C. Wason, Thinking: Readings in cognitive science." ></td>
	<td class="line x" title="216:227	Cambridge, 1977/, pp." ></td>
	<td class="line x" title="217:227	532-536." ></td>
	<td class="line x" title="218:227	Sterkenburg J. van, and WJJ." ></td>
	<td class="line x" title="219:227	Pijnenburg, 'Groot woordenboek van hedendaags Nederlands', Van Dale Lexicografie, Utrecht, 1984." ></td>
	<td class="line x" title="220:227	Vossen P. , ' A Parser-grammar for the Meaning Descriptions of the Longman Dictionary of Contemporary English', Technical Report NWO, project no. 300-169-007, University of Amsterdam, 1990." ></td>
	<td class="line x" title="221:227	Vossen P. , 'The end of the chain: where does stepwise lexical decomposition lead us eventually?', In: Proceedings of the 4th Functional Grammar Conference, June 1990, Kopenhagen, Denmark, fc., \[also available as Acquilex Working Paper No. 010, July 1990, Esprit BRA3030, University of Amsterdam\]." ></td>
	<td class="line x" title="223:227	Vossen P. 'Comparing noun-taxonomies cross-linguistically', Acquilex Working Paper No. 014, ESPRIT BRA-3030, University of Amsterdam, May 1991." ></td>
	<td class="line x" title="224:227	Weigand H. , 'Linguistically Motivated Principles of Knowledge Base Systems', Dordrecht, Foils, 1989." ></td>
	<td class="line x" title="225:227	Wilks Y. , Fass D. , Guo C. , McDonald E. , Plate T. , Slator B. , 'Machine tractable dictionaries as tools and resources for natural language processing', In: Proceedings of the 12th Conference on Computational Linguistics, Budapest, 1988, pp." ></td>
	<td class="line x" title="226:227	750-755." ></td>
	<td class="line x" title="227:227	Woolf H.B. 'The Merriam-Webster Dictionary', G.&C. Merilam Co. , New York, 1974 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A92-1013
Computational Lexicons: The Neat Examples And The Odd Exemplars
Basili, Roberto;Pazienza, Maria Teresa;Velardi, Paola;"></td>
	<td class="line x" title="1:225	Computational Lexicons: the Neat Examples and the Odd Exemplars Roberto Basili, Maria Teresa Pazienza Dip." ></td>
	<td class="line x" title="2:225	di Ingegneria Elettronica, Universita' 'Tor Vergata', Roma, Italy Paola Velardi Ist." ></td>
	<td class="line x" title="3:225	di Informatica, Universita' di Ancona, Ancona, Italy Abstract When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with." ></td>
	<td class="line x" title="4:225	Words relate to each other in many different, often queer, ways: this information is rarely found in dictionaries, and it is quite hard to be invented a priori, despite the imagination that linguists exhibit at inventing esoteric examples." ></td>
	<td class="line x" title="5:225	In this paper we present the results of an experiment in learning from corpora the frequent selectional restrictions holding between content words." ></td>
	<td class="line x" title="6:225	The method is based on the analysis of word associations augmented with syntactic markers and semantic tags." ></td>
	<td class="line x" title="7:225	Word pairs are extracted by a morphosyntactic analyzer and clustered according to their semantic tags." ></td>
	<td class="line x" title="8:225	A statistical measure is applied to the data to evaluate the significance of a detected relation." ></td>
	<td class="line x" title="9:225	Clustered association data render the study of word associations more interesting with several respects: data are more reliable even for smaller corpora, more easy to interpret, and have many practical applications in NLP." ></td>
	<td class="line x" title="10:225	1." ></td>
	<td class="line x" title="11:225	Introduction One of the fundamental property of computational lexicons is an account of the relations between verbs and its arguments." ></td>
	<td class="line x" title="12:225	Arguments are identified by their position in a predicate-argument structure, or by conceptual relations names (e.g. agent, purpose, location, etc)." ></td>
	<td class="line x" title="13:225	Arguments are annotated with selectional restrictions, that impose type constraints on the set of content words that may fill a relation." ></td>
	<td class="line x" title="14:225	Selectional restrictions often do not provide all the semantic information that is necessary in NLP systems, however they are at the basis of the majority of computational approaches to syntactic and semantic disambiguation." ></td>
	<td class="line x" title="15:225	It has been noticed that representing only the semantics of verbs may be inadequate (Velardi et al. 1988; Boguraev 1991; Macpherson 1991)." ></td>
	<td class="line x" title="16:225	The notion of spreading the semantic load supports the idea that every content word should be represented in the lexicon as the union of all the situations in which it could potentially participate." ></td>
	<td class="line x" title="17:225	Unfortunately, hand writing selectional restrictions is not an easy matter, because it is time consuming and it is hard to keep consistency among the data when the lexicon has several hundred or thousand words." ></td>
	<td class="line x" title="18:225	However the major difficulty is that words relate to each other in many different, often domain dependent ways." ></td>
	<td class="line x" title="19:225	The nowadays vast literature on computational lexicons is filled with neat examples of the eat(animate,food) flavour, but in practice in many language domains selectional constraints between words are quite odd." ></td>
	<td class="line x" title="20:225	It is not just a matter of violating the semantic expectations, such as in 'kill the process' or 'my car drinks gasoline', neither it is that kind of fancifulness that linguists exhibit at finding queer sentences." ></td>
	<td class="line x" title="21:225	Rather, there exist statistically relevant linguistic relations that are hard to imagine a-priori, almost never found in dictionaries, and even harder to assign to the appropriate slot in the whatever conceptual structure adopted for lexical representation." ></td>
	<td class="line x" title="22:225	Several examples of such relations are shown throughout this paper." ></td>
	<td class="line x" title="23:225	Ideally, knowledge on word relations should be acquired directly from massive amounts of texts~ rather than from hand-crafted rules." ></td>
	<td class="line x" title="24:225	This idea is a!" ></td>
	<td class="line x" title="25:225	the basis of many recent studies on word associations." ></td>
	<td class="line pc" title="26:225	The results of these studies have important applications in lexicography, to detect lexicosyntactic regularities (Church and Hanks, 19901 (Calzolari and Bindi,1990), such as, for example~ support verbs (e.g. 'make-decision') prepositional verbs (e.g. 'rely-upon') idioms, semantic relations (e.g. 'part_of') and fixed expressions (e.g. 'kick the bucket')." ></td>
	<td class="line x" title="27:225	In (Hindle,1990; Zernik, 1989; Webster el Marcus, 1989) cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification." ></td>
	<td class="line x" title="28:225	All these studies are based on th~ (strong) assumption that syntactic similarity in wor(~ patterns implies semantic similarity." ></td>
	<td class="line x" title="29:225	In (Guthrie el al. , 1991), sets of consistently contiguous word~, ('neighbourhood') are extracted from machinereadable dictionaries, to help semantic disambiguation in information retrieval." ></td>
	<td class="line x" title="30:225	In (Smadj~ and McKeown, 1990) statistically collectec associations provide pragmatic cues for lexical choic( in sentence generation." ></td>
	<td class="line x" title="31:225	For example, we can learr that 'make decision' is a better choice than, say 96 'have decision' or 'take decision'." ></td>
	<td class="line x" title="32:225	(Hindle and Rooths, 1991) proposes that a syntactic disambiguation criterion can be gathered by comparing the probability of occurrence of nounpreposition and verb-preposition pairs in V NP PP structures." ></td>
	<td class="line x" title="33:225	In general word associations are collected by extracting word pairs in a +-5 window." ></td>
	<td class="line oc" title="34:225	In (Calzolari and Bindi, 1990), (Church and Hanks, 1990) the significance of an association (x,y) is measured by the mutual information I(x,y), i.e. the probability of observing x and y together, compared with the probability of observing x and y independently." ></td>
	<td class="line x" title="35:225	In (Smadja, 1989), (Zernik and Jacobs, 1990), the associations are filtered by selecting the word pairs (x,y) whose frequency of occurrence is above f+ks, where f is the average appearance, s is the standard deviation, and k is an empirically determined factor." ></td>
	<td class="line x" title="36:225	(Hindle, 1990; Hindle and Rooths,1991) and (Smadja, 1991) use syntactic markers to increase the significance of the data." ></td>
	<td class="line x" title="37:225	(Guthrie et al. , 1991\] uses the subject classification given in machine-readable dictionaries (e.g. economics, engineering, etc)." ></td>
	<td class="line x" title="38:225	to reinforce cooccurence links." ></td>
	<td class="line n" title="39:225	Despite the use of these methods to add evidence to the data, the major problem with word-pairs collections is that reliable results are obtained only for a small subset of high-frequency words on very large corpora, otherwise the association ratio becomes unstable." ></td>
	<td class="line o" title="40:225	For example, Church run his experiment on a corpus with over 20-30 millions words, and Hindle reports 6 millions words as not being an adequate corpus." ></td>
	<td class="line x" title="41:225	In many practical NLP/IR applications corpora are not so large, and typically span from 500,000 to a few million words." ></td>
	<td class="line x" title="42:225	The analysis of associations could be done on wider domains, but a part for very general words, it is much more desirable to collect data from the application corpus." ></td>
	<td class="line x" title="43:225	Information collected from other sources could add noise rather than strengthening the data, because in most applications jargon, technical words, and domain-dependent associations are the norm." ></td>
	<td class="line x" title="44:225	In (Smadja, 1989b) it is shown a table of operational pairs like adjective-noun and verb-object, from which clearly emerges the very different nature of the two source domains (Unix Usenet and Jerusalem Post)." ></td>
	<td class="line x" title="45:225	For example, the noun-noun pairs with 'tree' include associations such as 'parse, grammar, decision' and 'olive, Christmas'." ></td>
	<td class="line x" title="46:225	If the NLP/IR application is about the computer world, associations such as 'olive tree' or 'Christmas tree' are (at best) useless." ></td>
	<td class="line x" title="47:225	A second problem with statistically collected word pairs is that an analysis based simply on surface distribution may produce data at a level of granularity too fine." ></td>
	<td class="line x" title="48:225	For example, a purely distributional analysis for word classification, such as those cited above, might place two verbs into distinct classes because one is used primarily with an object olive and the other with the object grape." ></td>
	<td class="line x" title="49:225	This may not be appropriate given the application." ></td>
	<td class="line x" title="50:225	Abstraction via semantic classes (e.g. VEGETABLE), would ensure that the ontology found is appropriate for the domain." ></td>
	<td class="line x" title="51:225	The model of prepositional attachment preference proposed by Hindle is also too weak if applied only to verb-preposition and nounpreposition pairs." ></td>
	<td class="line x" title="52:225	A preposition may or may not be related to a verb, even if it frequently cooccurs with it, depending upon the underlying semantic relation." ></td>
	<td class="line x" title="53:225	It is the semantic category of the noun following a preposition that determines the nature of the semantic link (e.g. for+ HUMAN ENTITY = beneficiary, for+ACTION = purpose), and ultimately influences the choice of the proper attachment." ></td>
	<td class="line x" title="54:225	Semantic abstraction also renders the data more readable." ></td>
	<td class="line x" title="55:225	Millions of simple word cooccurrences let the experimenter sink in an ocean of data, without providing much insight of the conceptual nature of the detected associations." ></td>
	<td class="line x" title="56:225	In this paper, we present a study on word associations augmented with syntactic markers and semantic tagging." ></td>
	<td class="line x" title="57:225	We call these data clustered associations." ></td>
	<td class="line x" title="58:225	Clustered association data are syntactic pairs or triples (e.g. N_V(\]ohn,go) V prep_N(go, to,Boston), N_prepN(Boston,by,bus) 1) in which one or both content words are replaced by their semantic tag (e.g. V_prep_N(PHYSICAL ACT-toPLACE),N_prep_N(PLACE-by-MACHINE) etc.)." ></td>
	<td class="line x" title="59:225	Semantic tags are very high-level in order to reduce the cost of hand-tagging." ></td>
	<td class="line x" title="60:225	Clustered association data have several advantages: First, statistically meaningful data can be gathered from (relatively) small corpora; Second, data are presented in a compact form and are much more readable; Third, clustered association data are useful for many interesting NLP applications, such as conceptual clustering, syntactic and semantic disambiguation, and semi-automatic learning of the relevant selectional restrictions in a given language domain." ></td>
	<td class="line x" title="61:225	In this paper we discuss the results of an experiment in learning selectional restrictions, to provide support for the design of computational lexicons." ></td>
	<td class="line x" title="62:225	Other results are presented in (Basili et al. , 1991; Fabrizi et al. , forthcomingl." ></td>
	<td class="line x" title="63:225	1 We did not want to schock the reader with queer examples since the introduction." ></td>
	<td class="line x" title="64:225	97 The method is applied to a corpus of economic enterprise descriptions, registered at the Chambers of Commerce in Italy." ></td>
	<td class="line x" title="65:225	The database of these descriptions (in total over 1,000,000 descriptions, each spanning from 1 to 100-200 words) is managed in Italy by the Company CERVED." ></td>
	<td class="line x" title="66:225	Sentences describe one or several commercial enterprises carried out by a given Company." ></td>
	<td class="line x" title="67:225	Examples of these descriptions are provided throughout the text." ></td>
	<td class="line x" title="68:225	In our experiment, we used only 25,000 descriptions, including about 500,000 words." ></td>
	<td class="line x" title="69:225	A second experiment on a legal corpus is under preparation and will be ready shortly." ></td>
	<td class="line x" title="70:225	2 Acquiring syntactic associations Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs." ></td>
	<td class="line x" title="71:225	Combining statistical and parsing methods has been done by (Hindle, 1990; Hindle and Rooths,1991) and (Smadja and McKewon, 1990; Smadja,1991)." ></td>
	<td class="line x" title="72:225	The novel aspect of our study is that we collect not only operational pairs, but triples, such as N_prep N, V_prep_N etc. In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words." ></td>
	<td class="line x" title="73:225	By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation (e.g. for=purpose,beneficiary)." ></td>
	<td class="line x" title="74:225	To extract syntactic associations two methods have been adopted in the literature." ></td>
	<td class="line x" title="75:225	Smadja attempts to apply syntactic information to a set of automatically collected collocations (statistics-first)." ></td>
	<td class="line x" title="76:225	Hindle performs syntactic parsing before collocational analysis (syntax-first)." ></td>
	<td class="line x" title="77:225	In our study, we decided to adopt the syntax-first approach, because: as remarked above, it is important to extract not only syntactic pairs, but also triples; statistically collected associations miss some syntactic relation between distant words in coordinate constructions (usually the window in which word pairs are extracted is +-5) and couple many words that are not syntactically related." ></td>
	<td class="line x" title="78:225	Even though (Smadja,1991) reports good performances of his system, it must be noticed that the precision and efficiency figures of the parser apply to a set of data that have been already (statistically) processed." ></td>
	<td class="line x" title="79:225	Thus the actual precision and efficiency in extracting syntactically related words from the source corpus may be lower than expected." ></td>
	<td class="line x" title="80:225	As in other similar works, the syntactic analyzer used in this study does not rely on a complete Italian grammar." ></td>
	<td class="line x" title="81:225	The parser only detects the surface syntactic relations between words." ></td>
	<td class="line x" title="82:225	A full description of the analyzer is outside the scope of this paper (see (Marziali, 1991) for details)." ></td>
	<td class="line x" title="83:225	In short, the parser consists of a segmentation algorithm to cut texts into phrases (NP, PP, VP etc), and a phrase parser that is able to detect the following 15 links: N_V, V_N, N_ADJ, N N, N_prep_N, V_prep_N, N_prep_V, V_prep_V, N_cong_N, ADJ_cong_ADJ, V_ADV, ADV_cong_ADV, V_cong V, N_prep_ADJ, V_prep_ADJ." ></td>
	<td class="line x" title="84:225	The segmentation algorithm is very simple." ></td>
	<td class="line x" title="85:225	If the domain sublanguage is good Italian, sentence cutting is based on the presence of verbs, punctuation, adverbs such as when, if, because, etc: For more jergal domains, such as the economic enterprise corpus, text cutting is based on heuristics such as the detection of a word classified as 'activity' (Fasolo et a1.,1990)." ></td>
	<td class="line x" title="87:225	In fact, this domain is characterized by absence of punctuation, ill formed sentences, long nested coordinate constructions." ></td>
	<td class="line x" title="88:225	The phrase parser is based on DCG (Pereira and Warren,1980), the most complex part of which is the treatment of coordination." ></td>
	<td class="line x" title="89:225	The grammar consists of about 20 rules." ></td>
	<td class="line x" title="90:225	Rather than a parse tree, the output is a 'fiat' set of syntactic relations between content words." ></td>
	<td class="line o" title="91:225	For example, parsing the sentence: fabbrica di scarpe per uomo e per bambino (*manufacture of shoes for man and child) produces the following relations: N_prep N(fabbrica,di,scarpe) N prep_N(fabbrica,per,uomo) N_prep_N(fabbrica,per,bambino) N_prep_N(scarpe,per,uomo) N_prep_N(scarpe,per,bambino) N_cong_N(uomo,e,bambino) Unlike Church and Hindle, we are not interested in collecting binary or ternary relations between words within a sentence, but rather in detecting recurring binary syntactic associations in the corpus." ></td>
	<td class="line x" title="92:225	For this purpose it is unnecessary to retrieve even partial parse trees." ></td>
	<td class="line x" title="93:225	The complexity of the grammar is O(n2), that makes it computationally attractive for parsing large corpora." ></td>
	<td class="line x" title="94:225	In (Marziali,1991) the efficiency and precision of this grammar with respect to the full se!" ></td>
	<td class="line x" title="95:225	of surface syntactic links detectable by a complete DCG grammar are evaluated to be 85% and 90%." ></td>
	<td class="line x" title="96:225	The reference output adopted to perform the evaluation is a syntactic graph (Seo and Simmons,1989)'." ></td>
	<td class="line x" title="97:225	Syntactic graphs include in a unique graph the set of all possible parse trees." ></td>
	<td class="line x" title="98:225	The evaluation was hand-made 98 over a set of 100 sentences belonging to three domains: the economic corpus, the legal corpus, and a novel." ></td>
	<td class="line x" title="99:225	The performances are better for the legal corpus and the novel, due to the ungrammaticality of the economic corpus." ></td>
	<td class="line x" title="100:225	The relatively high efficiency rate, as compared with the figures reported in (Brent, 1991), are due to the fact that Italian morphology is far more complex than English." ></td>
	<td class="line x" title="101:225	Once a good morphologic analyzer is available (the one used in our work is very well tested, and has first described in (Russo,1987)), problems such as verb detection, raised in (Brent, 1991), are negligible." ></td>
	<td class="line x" title="102:225	In addition, the text-cutting algorithm has positive effects on the precision." ></td>
	<td class="line x" title="103:225	Despite this, we verified that about a 35% of the syntactic associations extracted from the economic corpus are semantically unrelated, due to syntactic ambiguity." ></td>
	<td class="line x" title="104:225	As shown in the following sections, semantic clustering in part solves this problem, because semantically unrelated word pairs do not accumulate statistical relevance, except for very rare and unfortunate cases." ></td>
	<td class="line x" title="105:225	In any case, we need more experiments to verify the effect of a more severe sentence cutting algorithm on the precision at detecting semantically related pairs." ></td>
	<td class="line x" title="106:225	This issue is particularly relevant for ungrammatical texts, as in the economic corpus." ></td>
	<td class="line x" title="107:225	3." ></td>
	<td class="line x" title="108:225	Assigning semantic tags The set of syntactic associations extracted by the DCG parser are first clustered according to the cooccurring words and the type of syntactic link." ></td>
	<td class="line x" title="109:225	A further clustering is performed based on the semantic tag associated to the cooccurring words." ></td>
	<td class="line x" title="110:225	Clustering association data through semantic tagging has two important advantages: First, it improves significantly the reliability of association data, even for small corpora; Second, and more importantly, semantic tags make it explicit the semantic nature of word relations." ></td>
	<td class="line x" title="111:225	Manually adding semantic tags to words may appear very expensive, but in fact it is not, if very broad, domain-dependent classes are selected." ></td>
	<td class="line x" title="112:225	In our application, the following 13 categories were adopted: PHYSICAL_ACT (packaging, travel, build, etc)." ></td>
	<td class="line x" title="113:225	MENTAL_ACT(sell, organize, handle, teach, etc)." ></td>
	<td class="line x" title="114:225	HUMAN_ENTITY (shareholder, company, person, farmer, tailor, etc)." ></td>
	<td class="line x" title="115:225	ANIMAL (cow, sheep, etc)." ></td>
	<td class="line x" title="116:225	VEGETABLE (carrots, grape, rubber, coffee, etc)." ></td>
	<td class="line x" title="117:225	MATERIAL (wood, iron, water, cement, etc)." ></td>
	<td class="line x" title="118:225	BUILDING (mill, shop, house, grocery, etc)." ></td>
	<td class="line x" title="119:225	BYPRODUCT (jam, milk, wine, drink, hide, etc)." ></td>
	<td class="line x" title="120:225	ARTIFACT (item, brickwork, toy, table, wears, etc)." ></td>
	<td class="line x" title="121:225	MACHINE (engine, tractor, grindstone,computer, etc)." ></td>
	<td class="line x" title="122:225	PLACE (ground, field, territory, Italy, sea, etc)." ></td>
	<td class="line x" title="123:225	QUALITY (green, chemical, coaxial, flexible, etc)." ></td>
	<td class="line x" title="124:225	MANNER (chemically, by-hand, retail, etc)." ></td>
	<td class="line x" title="125:225	These categories classify well enough the words which are found in the selected sub-corpus as a testbed for our research." ></td>
	<td class="line x" title="126:225	Some words received two tags: for example, there are sentences in which a BUILDING metonymically refers to the commercial ACT held in that building (e.g. 'trade mills for the production'); some word is both a BY_PRODUCT (e.g. 'wood carving') or a MATERIAL (e.g. 'handicrafts in wood')." ></td>
	<td class="line x" title="127:225	Because the domain is very specific, double-tagging i s never due to polisemy." ></td>
	<td class="line x" title="128:225	Once the definition of a semantic class is clearly stated, and with the help of a simple user interface, hand tagging a word is a matter of seconds." ></td>
	<td class="line x" title="129:225	We adopted the policy that, whenever assigning a tag is not obvious, or none of the available tags seems adequate, the word is simply skipped." ></td>
	<td class="line x" title="130:225	Unclassified words are less than 10% in our corpus." ></td>
	<td class="line x" title="131:225	Overall, we classified over 5000 words (lemmata)." ></td>
	<td class="line x" title="132:225	The activity of classification was absolutely negligible in comparison with all the other activities in this research, both on the ground of time and required skill." ></td>
	<td class="line x" title="133:225	Domain-dependent tags render the classification task more simple and ensure that the clustered association data are appropriate given the application." ></td>
	<td class="line x" title="134:225	An obvious drawback is that it is necessary to re-classify many words if the application domain changes significantly." ></td>
	<td class="line x" title="135:225	For example, we are currently preparing a new experiment on a legal corpus." ></td>
	<td class="line x" title="136:225	The domain is semantically more rich, hence we needed 15 classes." ></td>
	<td class="line x" title="137:225	A first estimate revealed that about 30-40% of the words need to be re-classified using more appropriate semantic tags." ></td>
	<td class="line x" title="138:225	4 Acquisition of selectional restrictions Clustered association data are at the basis of our method to detect the important selectional restrictions that hold in a given sublanguage." ></td>
	<td class="line x" title="139:225	The statistical significance of the detected relations is measured by the probability of cooccurrence of two classes C 1 and C 2 in the pattern C 1 synt-rel C2, where synt-rel is one of the syntactic relations detectable by the parser summarized in Section 2." ></td>
	<td class="line x" title="140:225	99 Rather than evaluating the probability Pr(C 1 syntrel C2), we computed the conditioned probability P(C1,C2/synt-rel) estimated by: f(Cl,synt rel,C 2 ) (1) f(synt_rel) The reason for using (1) rather than other measures proposed in the literature, is that what matters here is to detect all the statistically relevant phenomena, not necessarily all the meaningful associations." ></td>
	<td class="line x" title="141:225	Such measures as the mutual information and the t-score (Church et al. , 1991) give emphasis to the infrequent phenomena, because the statistical significance of the coupling between C 1 and C 2 is related to the probability of occurrence of C 1 and C 2 independently from each other." ></td>
	<td class="line x" title="142:225	This would be useful at detecting rare but meaningful relations if one could rely on the correctness of the data." ></td>
	<td class="line x" title="143:225	Unfortunately, due to syntactic ambiguity and errors in parsing, many syntactic associations are not semantically related, i.e. there exists no plausible selectional relations between the to cooccurring words." ></td>
	<td class="line x" title="144:225	Using the mutual information, such relations could accumulate statistical evidence." ></td>
	<td class="line x" title="145:225	The (1) is more conservative, but ensures more reliable results." ></td>
	<td class="line x" title="146:225	In any case, we run several experiments with different statistical measures, without being entirely happy with any of these." ></td>
	<td class="line x" title="147:225	Finding more appropriate statistical methods is one of the future objectives of our work." ></td>
	<td class="line x" title="148:225	Clustered association data are used to build tables, one for each syntactic structure, whose element (x,y) represents the statistical significance in the corpus of a concept pair C 1 C 2." ></td>
	<td class="line x" title="149:225	All the relevant couplings among classes are identified by a human operator who inspects the tables, and labels concept pairs by the appropriate conceptual relation." ></td>
	<td class="line x" title="150:225	Finding an appropriate set of conceptual relations is not an easy task." ></td>
	<td class="line x" title="151:225	In labeling concept pairs, we relied on our preceding work on semantic representation with Conceptual Graph \[Pazienza and Velardi, 1987\]." ></td>
	<td class="line x" title="152:225	Four of these tables are presented in the Appendix." ></td>
	<td class="line x" title="153:225	The data have been collected from a corpus of about 500,000 words." ></td>
	<td class="line x" title="154:225	The morphosyntactic analyzer takes about 6-10 hours on a Spark station." ></td>
	<td class="line x" title="155:225	Clustering the data takes about as much." ></td>
	<td class="line x" title="156:225	At first, we extracted only the V_N, N_prep N and V_prep_N associations, for a total of 52,155 different syntactic associations." ></td>
	<td class="line x" title="157:225	The average is 5 occurrences for each association." ></td>
	<td class="line x" title="158:225	At first glance, the data seem quite odd even to an Italian reader, but it turns out that the tables show exactly what the corpus includes." ></td>
	<td class="line x" title="159:225	Let us briefly go through the 4 tables." ></td>
	<td class="line x" title="160:225	Table 1 summarizes the relations Cl-per-C 2 (per=for)." ></td>
	<td class="line x" title="161:225	Some of the significant associations are: ARTIFACT PHYSICAL_ACT (e.g.: articoli per Io sport (*items for sport), attrezzi per giardinaggio (*tools for gardening)) ARTIFACT BUILDING (e.g. biancheria per la casa (*linens for the house), mobili per negozi (*furnitures for shops)) MACHINE-BUILDING (e.g. macchinari per laboratori (*equipments for laboratories), macine per mulini (*grindstones for mills)) All the above relations subsume the usage (or figurative_destination) relation." ></td>
	<td class="line x" title="162:225	Notice that the 'advertised' beneficiary relation is not very significant in the corpus." ></td>
	<td class="line x" title="163:225	The only statistically relevant beneficiary relations are ARTIFACT-for-HUMAN_ENTITY ( ( e.g. calzature per uomo (*shoes for man), biancheria per signora (*linens for lady)) and HUMAN_ENTITY_for_HUMANENTITY (e.g. parrucchire per signora (*hairdresser for lady)." ></td>
	<td class="line x" title="164:225	It appears that in the considered domain, verbs, except for some, poorly relate with the preposition for (this is the first surprise!)." ></td>
	<td class="line x" title="165:225	Table 2 shows the Cl-in-C 2 relations." ></td>
	<td class="line x" title="166:225	Two relations represent the large majority: ARTIFACT-inBYPRODUCT (e.g. calzature in pelle (*shoes in leather), guarnizioni in gomma (packings in rubber)) ARTIFACT-in-MATERIAL (e.g. oggetti in legnc (*handicrafts in wood) ringhiere in ferro (*banisters in iron)) both subsume a matter relation (this is one of the few 'expected' associations we found)." ></td>
	<td class="line x" title="167:225	Less frequent but interesting are the followin~ relations: MENTAL_ACT-in-MENTAL_ACT (e.g. concedere ir~ appalto (*to grant in bid) acquistare in leasing (*to buy in leasing)) ARTIFACT-in-ARTIFACT (e.g. prodotti in scatok (*products in can)) While the second is a 'reassuring' location relatior (subsumed also by the in-PLACE associations in th( last column), we are less sure about the semanti( interpretation of the first relation." ></td>
	<td class="line x" title="168:225	Tentatively, w( choosed the manner relation." ></td>
	<td class="line x" title="169:225	The same type o: relation is also found in the CI-a-C 2 (a=to,on) tabh (Table 3): MENTAL_ACT-a-MENTAL_ACT (e.g acquistare a credito (*to buy to (=on) credit) abilitare all'ottenimento (*qualifying to th4 attainment), assistenza all'insegnamento (assistenc, to the teaching)) 100 The first example (on credit) clearly subsumes the same relation as for in leasing; the following two seem of a different nature." ></td>
	<td class="line x" title="170:225	We used figurativedestination to label these relations." ></td>
	<td class="line x" title="171:225	This may or may not be the best interpretation: however, what matters here is not so much the human interpretation of the data, but rather the ability of the system at detecting the relevant semantic associations, whatever their name could be." ></td>
	<td class="line x" title="172:225	Once again in this table, notice that the common relations, like recipient (to-HUMAN_ENTITY) and destination (to-PLACE) are less frequent than expected." ></td>
	<td class="line x" title="173:225	Table 4 shows the Cl-da-C 2 relations (da=from,for, to)." ></td>
	<td class="line x" title="174:225	The most frequent relations are: MATERIAL-da-PHYSICAL_ACT (e.g materiali da imballaggio (*material from (=for) packing) legna da ardere (*wood from (=to) burn)) ARTIFACT-da-ARTIFACT (e.g cera da pavimenti (*wax for floors), lenzuola da letto (*sheets for bed)) ARTIFACT-da-PLACE (e.g giocattoli da mare (*toys for sea) abiti da montagna (*wears for mountain) MENTAL_ACT-da-BUILDING (e.g acquistare da fabbrica (*to buy from firms), comprare da oleifici (*to buy from oil-mills)) The first three relations, very frequent in the corpus, all subsume the usage relation." ></td>
	<td class="line x" title="175:225	It is interesting to notice that in Italian 'da+PLACE' commonly subsumes a source relation, just as in English (from+PLACE)." ></td>
	<td class="line x" title="176:225	The table however shows that this is not the case, at least when 'from-PLACE' cooccurs with classes such as ARTIFACT and MATERIAL." ></td>
	<td class="line x" title="177:225	The classical source sense is found in the fourth example." ></td>
	<td class="line x" title="178:225	BUILDINGs here metonymically refer to the human organization that manages an activity in the building." ></td>
	<td class="line x" title="179:225	Currently we are unable to analyze the preposition di (*of), because in the corpus it is used for the large majority to subsume the direct-object syntactic relation (e.g. vendita di frutta *sale of fruit)." ></td>
	<td class="line x" title="180:225	It turns out that the distribution of Cl-di-C 2 is too even to allow an analysis of the data." ></td>
	<td class="line x" title="181:225	Perhaps a less crude parsing strategy could help at ruling out these associations." ></td>
	<td class="line x" title="182:225	A new domain is now under examination (a legal corpus on taxation norms)." ></td>
	<td class="line x" title="183:225	A first analysis of the data shows that even for this corpus, despite it is much less jergal, several unconventional relations hold between content words." ></td>
	<td class="line x" title="184:225	5." ></td>
	<td class="line x" title="185:225	Final Remarks We spent some time at illustrating the tables to make the reader more confident with the data, and to show with several practical examples the thesis of this paper, i.e. that selectional restrictions are more fanciful than what usually appears from the literature on computational lexicons (and from dictionaries as well)." ></td>
	<td class="line x" title="186:225	The reader should not think that we selected for our application the oddest domain we could find: similar (as for fancifulness) data are being extracted from a legal corpus which is currently under examination." ></td>
	<td class="line x" title="187:225	The (semi-)automatic acquisition of selectional restrictions is only one ot the things that can be learned using clustered association data." ></td>
	<td class="line x" title="188:225	In a forthcoming paper (Basili et al. , forthcoming) the same data, clustered only by the right-hand word, are at the basis of a very reliable algorithm for syntactic disambiguation." ></td>
	<td class="line x" title="189:225	We are also experimenting concept formation algorithms for verb and noun classification (Fabrizi et al. , forthcoming)." ></td>
	<td class="line x" title="190:225	In summary, clustered associations in our view greatly improve the reliability and applicability of studies on word associations." ></td>
	<td class="line x" title="191:225	More work is necessary, because of semantic tagging, but there is an evident payoff." ></td>
	<td class="line x" title="192:225	In any case, semantic tagging is not at all the most painful manual activity in association studies." ></td>
	<td class="line x" title="193:225	Acknowledgements." ></td>
	<td class="line x" title="194:225	We thank CERVEDomani for making us available the corpus of enterprise descriptions." ></td>
	<td class="line x" title="195:225	This work has been in part supported by the European Community (PRO-ART and NOMOS ESPRIT 1991 n. 5330)." ></td>
	<td class="line x" title="196:225	Appendix: Examples of acquired conceptual associations per I) 2) 3) 4) 5) 6) 7) 8) 9) i0) ii) i) att mat 2) att ment 3) manufatto 4)entita_umana 5) vegetale 6) costruzione 7) derivato 8) materiale 9) animali i0) macchinario ll) luoghi 0.121 0.075 0.041 0.054 0.094 0.068 0.016 0.024 0.003 0.001 0.061 0.030 0.013 0.004 0.027 0.008 0.000 0.001 0.032 0.036 0.004 0.004 0.000 0.000 0 031 0.022 0.000 0.039 0.002 0.002 0 023 0.018 0.000 0.023 O.OO1 0.000 0 045 0.026 0.001 0.057 0.005 0.005 0 002 0.024 0.000 0.014 0.000 0.001 0 002 0.000 0.000 0 012 0.010 0.013 0.000 0.000 0 006 0.001 0.000 0.004 0.001 0.001 0 011 0.001 0.000 0.015 0.002 0.002 0.000 O.OlO 0.002 O.OO1 0.033 0.001 0.001 0.001 0.000 0.001 0.010 0.005 0.000 0.011 0.001 0.001 0.001 0.001 0.002 0.003 0.004 0.004 0 003 0 004 0 001 0 000 0 001 0 001 0 001 0 000 0 001 0 000 Table 1:Cl-per-C2 102 in 1) 2) 3) 4) 5) 6) 7) 8) 9) 10) 11) I) att_mat 0.019 0.048 0.023 2) att ment 0.033 0.088 0.038 0.050 0.001 0 3) manufatto 0.014 0.006 0.066 0.001 0.007 0 4)entita umana 0.004 0.009 0.000 5) vegetale 0.010 6) costruzione 0.008 0.013 0.017 7) derivato 0.001 0.001 0.006 8) materiale 0.002 0.008 9) animali I0) macchinario 0.001 ii) luoghi 0.001 0.003 0.001 0.013 0.002 0 013 0 041 0.085 0.002 0 0.001 0 0.000 0 0.001 0 0 007 0 007 0 001 0 009 0 004 0 000 0 001 0 016 0 048 092 0 121 004 0 009 006 0 000 009 0 035 005 0 005 002 0 005 0.003 0.001 0.002 Table 2:Cl-in-C2 0.000 0.053 0.004 0.074 0.001 0.009 0.010 0.001 0.020 0.004 0.008 0.001 0.000 0.000 i) 2) 3) 4) 5) 6) 7) 8) 9) I0) II) i) 2) 3) manufatto 0 005 0 4)entita_umana 0 001 0 5) vegetale 0 001 0 6) costruzione 0 007 0 7) derivato 0 000 0 8) materiale 0 002 0 9) animali 0 i0) macchinario 0.004 0 ii) luoghi 0.001 0 attmat att_ment 013 0.019 0.000 009 0\[001 0.002 001 0.002 009 0.002 0.002 001 0.000 0.001 011 000 005 0.002 005 0.007 0.020 0.084 0.032 0.020 0.018 0.002 0.006 0.001 0.025 0.036 0.037 0 351 0.077 0.055 0.008 0.019 0.016 0.005 0.001 0.024 0.049 0.004 0.001 0.002 0.000 0.005 0.005 0.002 0.000 0.002 0.001 0.008 0.001 0.001 0.002 0.001 0.007 0.004 0.004 0.001 0.001 0.004 0.003 0.000 0.001 0.002 0.002 0.003 0.002 0.001 0.017 Table 3:CI-a-C2 da i) 2) 3) 4) 5) 6) 7) 8) 9) i0) ii) I) 2) att_ment 0 3) manufatto 0 ~)entita_umana 0 5) vegetale 0 6) costruzione 0 7) derivato 0 8) materiale 0 9) animali 0 i0) macchinario 0 ii) luoghi 0 art mat 0 046 0.023 0.036 0.033 0.001 0.023 0.010 0.008 022 0.012 0.047 0.052 0.001 0.037 0.001 0.001 023 0.009 0.251 0.009 0.002 0.036 0.007 0.002 003 0.004 0.010 0.004 0.001 0.005 0.002 002 0.001 0.001 0.002 008 0.007 0.036 0.003 0.003 0.023 0.004 0.001 012 0.001 0.010 0.001 0.001 0.001 0.005 012 0.001 0.010 0.001 0.001 0.001 0.005 003 0.001 0.003 0.003 0.008 004 0.001 0.007 0.002 0.001 0.001 003 0.001 0.002 Table 4:Cl-da-C2 0.005 0 021 0.004 0 032 0.007 0 059 0 00~ 0 016 0 025 0 009 0 009 0 006 0.007 0 001 0 001 Legenda: 1) att_mat = PHYSICALACT 2) att_ment = MENTALACT 3) manufatto = ARTIFACT 4) entita umana = HUMAN_ENTITY 5) vegetale = VEGETABLE 6) costruzione = BUILDING 7) derivato = BYPRODUCT 8) materiale = MATTER 9) animali = ANIMALS 10) macchinario = MACHINE 11) luoghi = PLACES References Basili, M. T. Pazienza, P. Velardi, Using word association for syntactic disambiguation, 2nd." ></td>
	<td class="line o" title="197:225	Congress of the Italian Association for Artificial Intelligence, Palermo, 1991 B. Boguraev, Building a Lexicon: the Contribution of Computers, IBM Report, T.J. Watson Research Center, 1991 M. Brent, Automatic Aquisition of Subcategorization frames from Untagged Texts, in (ACL, 1991) N. Calzolari, R. Bindi, Acquisition of Lexical Information from Corpus, in (COLING 1990) K. W. Church, P. Hanks, Word Association Norms, Mutual Information, and Lexicography, Computational Linguistics, vol." ></td>
	<td class="line x" title="198:225	16, n. 1, March 1990." ></td>
	<td class="line x" title="199:225	K. Church, W. Gale, P. Hanks, D. Hindle, Using Statistics in Lexical Analysis, in (Zernik, 1991)." ></td>
	<td class="line x" title="200:225	S. Fabrizi, M.T.Pazienza, P. Velardi, A corpusdriven clustering algorithm for the acquisition of word ontologies, forthcoming." ></td>
	<td class="line x" title="201:225	M.Fasolo, L.Garbuio, N.Guarino, Comprensione di descrizioni di attivita' economicoproduttive espresse in linguaggio naturale, Proc." ></td>
	<td class="line x" title="202:225	of GULP Conference, Padova 1990." ></td>
	<td class="line x" title="203:225	Jo Guthrie, L. Guthrie, Y. Wilks, H. Aidinejad, Subject-dependent Co-occurrence and Word Sense Disambiguation, in (ACL, 1991)." ></td>
	<td class="line x" title="204:225	D. Hindle, Noun classification from predicate argument structures, in (ACL,1990)." ></td>
	<td class="line x" title="205:225	D. Hindle, M. Rooths, Structural Ambiguity and Lexical Relations, in (ACL, 1991)." ></td>
	<td class="line x" title="206:225	'Lexical Semantics and Knowledge Representation' Proc." ></td>
	<td class="line x" title="207:225	of a workshop sponsored by the Special Interest Group on the Lexicon of the ACL, Ed." ></td>
	<td class="line x" title="208:225	J. Pustejovsky, June 1991 M. Macpherson, Redefining the Level of the Word, in (Lexical, 1991)." ></td>
	<td class="line x" title="209:225	A. Marziali, 'Laurea' dissertation, University of Roma II, Dept. of Electrical Engineering, in preparation M.T. Pazienza, P. Velardi, A structured Representation of Word Senses for Semantic Analysis, Third Conference of the European Chapter of ACL, Copenhagen, April 1-3, 1987." ></td>
	<td class="line x" title="210:225	F. Pereira, D. Warren, Definite Clause Grammars for Language Analysis A Survey of the Formalism and a Comparison with Augmented Transition Networks, in Artificial Intelligence, n. 13, 1980." ></td>
	<td class="line x" title="211:225	M. Russo, A generative grammar approach for the morphologic and morphosyntactic analysis of the Italian language, 3rd." ></td>
	<td class="line x" title="212:225	Conf." ></td>
	<td class="line x" title="213:225	of the European Chapter of the ACL, Copenhaghen, April 1-3 1987 J. Seo, R.F. Simmons, Syntactic Graphs a Representation for the Union of All Ambigous Parse Trees, Computational Linguistics, Vol." ></td>
	<td class="line x" title="214:225	15, n.1, March, 1989." ></td>
	<td class="line x" title="215:225	F. A. Smadja, Lexical Co-occurrence The Missing Link, Literary and Linguistic Computing, vol." ></td>
	<td class="line x" title="216:225	4, n.3, 1989." ></td>
	<td class="line x" title="217:225	F. Smadja, Macrocoding the Lexicon with Cooccurrence Knowledge, First Lexical Acquisition Workshop, August 1989, Detroit, and in (Zernik,1991)." ></td>
	<td class="line x" title="218:225	F. Smadja, K. McKewon, Automatically extracting and representing collocations for language generation, in (ACL,1990)." ></td>
	<td class="line x" title="219:225	F. Smadja, From N-Grams to Collocations an evaluation of XTRACT, in (ACL,1991)." ></td>
	<td class="line x" title="220:225	P. Velardi, M.T. Pazienza, M. De Giovanetti 'Conceptual Graphs for the Analysis and Generation of Sentences ', in IBM Journal oi R&D, special issue on language processing, March 1988 M. Webster M. Marcus, Automatic Acquisition ot lexical semantics of verbs from sentence frames, Proc." ></td>
	<td class="line x" title="221:225	of ACL89, Vancouver 1989 U. Zernik, Lexical acquisition Learning from Corpus by capitalizing on Lexical categories Proc." ></td>
	<td class="line x" title="222:225	of IJCAI 1989, Detroit 1989 U. Zernik, P. Jacobs, Tagging for Learning Collecting Thematic Relations from Corpus Proc." ></td>
	<td class="line x" title="223:225	of COLING 90, Helsinki, August 1990." ></td>
	<td class="line x" title="224:225	U. Zernik ed." ></td>
	<td class="line x" title="225:225	'Lexical Acquisition Using On-line Resources to Build a Lexicon', Lawrence Erlbaum Ass.,1991" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C92-1033
TTP: A Fast And Robust Parser For Natural Language
Strzalkowski, Tomek;"></td>
	<td class="line x" title="1:255	TTP: A FAST AND ROBUST PARSER FOR NATURAL LANGUAGE TOMEK STRZALKOWSKI Courant Institute of Mathematical Sciences New York University 715 Broadway, rm 704 New York, NY 10003 tomek@cs.nyu.edu ABSTRACT In this paper we describe TI~, a fast and robust natural language parser which can analyze written text and generate regularized parse structures for sentences and phrases at the speed of approximately 0.5 sec/sentence, or 44 word per second." ></td>
	<td class="line x" title="2:255	The parser is based on a wide coverage grammar for English, developed by the New York University's Linguistic String Project, and it uses the machine-readable version of the Oxford Advanced lw~arner's Dictionary as a source of its basic vocabulary." ></td>
	<td class="line x" title="3:255	The parser operates on stochastically tagged text, and contains a powerful skip-and-fit recovery mechanism that allows it to deal with extra-grammatical input and to operate effectively under a severe time pressure." ></td>
	<td class="line x" title="4:255	Empirical experiments, testing parser's speed and accuracy, were performed on several collections: a collection of technical abstracts (CACM-3204), a corpus of news messages (MUC-3), a selection from ACM Computer Library database, and a collection of Wall Street Journal articles, approximately 50 million words in total." ></td>
	<td class="line x" title="5:255	1." ></td>
	<td class="line x" title="6:255	INTRODUCTION Recently, there has been a growing demand for fast and reliable natural language processing tools, capable of performing reasonably accurate syntactic analysis of large volumes of text within an acceptable time." ></td>
	<td class="line x" title="7:255	A full sentential parser that produces complete mmlysis of input, may be considered reasonably fast if the average parsing time per sentence falls anywhere between 2 and 10 seconds." ></td>
	<td class="line x" title="8:255	A large volume of text, perhaps a gigabyte or more, would contain as many as 7 million sentences." ></td>
	<td class="line x" title="9:255	At the speed of say, 6 sec/sentence, this much text would require well over a year to parse." ></td>
	<td class="line x" title="10:255	While 7 million sentences is a lot of text, this much may easily he contained in a fair-sized text database." ></td>
	<td class="line x" title="11:255	Therefore, the parsing speed would have to be increased by at least a factor of 10 to make such a task manageable." ></td>
	<td class="line x" title="12:255	In this paper we describe a fast and robust natural language parser that can analyze written text and generate regularized parse structures at a speed of below 1 second per sentence." ></td>
	<td class="line x" title="13:255	In the experiments conducted on variety of natural langauge texts, including technical prose, news messages, and newspaper articles, the average parsing time varied between 0.4 sec/sentence and 0.7 see/sentence, or between 1600 and 2600 words per minute, as we tried to find an acceptable compromise between parser's speed and precision.l It has long been assumed that in order to gain speed, one may have to trade in some of the purser's accuracy." ></td>
	<td class="line x" title="14:255	For example, we may have to settle for partial parsing that would recognize only selected grammatical structures (e.g. noun phrases; Ruge et al. , 1991), or would avoid making difficult decisions (e.g. pp-attachment; Hindle, 1983)." ></td>
	<td class="line x" title="15:255	Much of the overhead and inefficiency comes from the fact that the lexical and structural ambiguity of natmal language input can only be dealt with using limited context information available to the parser." ></td>
	<td class="line x" title="16:255	Partial parsing techniques have been used with a considerable success in processing large volumes of text, for example AT&T's Fidditch (Hindle and Rooth, 1991) parsed 13 million words of Associated Press news messages, while MIT's parser (de Marcken, 1990) was used to process the 1 million word Lancaster/Oslo/Bergen (LOB) corpus." ></td>
	<td class="line x" title="17:255	In both cases, the parsers were designed to do partial processing only, that is, they would never attempt a complete analysis of certain constructions, such as the attachment of pp-adjuncts, subordinate clauses, or coordinations." ></td>
	<td class="line x" title="18:255	This kind of partial analysis may be sufficient in some applications because of a relatively high precision of identifying correct syntactic dependencies." ></td>
	<td class="line x" title="19:255	2 However, the ratio at which these dependencies are identified (that is, the recall level) isn't sufficiently high due to the inherently partial character of the parsing process." ></td>
	<td class="line x" title="20:255	The low recall means that many of the important dependencies are lost in parsing, and t These results were obtained on a 21 MIPS SparcStafion ELC." ></td>
	<td class="line x" title="21:255	The experiments were performed within an information retrieval system so that the final recall and precision statistics were used to rnealurc effectiwmess of the panmr." ></td>
	<td class="line oc" title="22:255	a Hindle and Rooth (1991) and Church and Hanks (1990) used partial parses generated by Fidditch to study word ~urrt.nc patterns m syntactic contexts." ></td>
	<td class="line x" title="23:255	ACRES DE COLING-92, NANTES, 23-28 AOr~ 1992 1 9 8 PROC." ></td>
	<td class="line x" title="24:255	OF COL1NG-92." ></td>
	<td class="line x" title="25:255	NANTES, AOO." ></td>
	<td class="line x" title="26:255	23-28, 1992 therelore partial parsing may not be suitable in applications such as information extraction or document retrieval." ></td>
	<td class="line x" title="27:255	The alternative is to create a parser that would attempt to produce a complete parse, and would resort to partial or approxim~ analysis only under exceptional conditions such as an extra-grammatical input or a severe time pressure." ></td>
	<td class="line x" title="28:255	Encountering a construction that it couldn't handle, the parser would first try to produec an approxinmte analysis of the difficult fragment, and then resume normal processing for the rest of the input." ></td>
	<td class="line x" title="29:255	The outcome is a kind of 'fitted' parse, reflecting a compromise between the actual input and grammar-encoded preferences (imposed, mainly, in rule ordering)) 2." ></td>
	<td class="line x" title="30:255	SKIP-ANI)-FIT RECOVERY IN PARSING A robust parser must deal efficiently with difficult input, whether it is an exUa-gmmmatical string, or a string whose complete analysis could be." ></td>
	<td class="line x" title="31:255	considered too costly." ></td>
	<td class="line x" title="32:255	Frequently, these two situations am not distinguishable, estmcially for long and complex sentences found iu free running text." ></td>
	<td class="line x" title="33:255	The parser must be able to analyze such strings quickly and produec at least partiM stractures, imposhlg preferences when necessary, and even removing or inserting small input fragments, if the data-driven processing falters." ></td>
	<td class="line x" title="34:255	For example, in the following sentence, The method is illustrated by the automatic construction of both recursive and iterafive programs operating on natural numbers, lists, and tree.s, ht order to construct a program satisfying certain specifications a theorem induced by those specifu:ations is proved, and the desired program is extracted from the ptooL the italicized part is likely to cause additional complications in parsing this lengthy string, and the parser may be better off ignoring the fragment altogether." ></td>
	<td class="line x" title="35:255	To do so successfully, the parser must close the constituent which is being culrenfly parsed, an(l lYossibly a few of its parent constituents, removing correspumling productions from further consideration, until an appropriate production is rcactivatexl, The parser then jumps over the iutervening inatedal.so as to re.start processing of the remainder of the sentence usiag rite newly reactivated production." ></td>
	<td class="line x" title="36:255	In the example at hand, suppose that the parser has just read the word specifications and is looking at the following article a. Rather than continuing at the present level, the parser reduces the phrase a program satiyfying certain The idea of parse 'fitting' was partly ialspired by the UIM parser (Jen~en et al. , 1983), as well as by the sumdard error mcovely techniques used in shift-reduce parsiug." ></td>
	<td class="line x" title="37:255	specifications to NP, and then traces further reductions: SI --) to V NP; SA -~ SI; S .--) NP V NP SA, until production S --* S and S is reached." ></td>
	<td class="line x" title="38:255	4 Subsequently, the parser skips input to find and, then resumes normal processing." ></td>
	<td class="line x" title="39:255	As may be expected, this kind of action involves a great deal of indeterminacy which, in case of natural language strings, is compounded by the high degree of lexical ambiguity." ></td>
	<td class="line x" title="40:255	If the purpose of this skip-and-fit technique is to get the purser smoothly through even the most complex strings, the amount of additional backtracking caused by the lexical level ambiguity ks certain to defeat it." ></td>
	<td class="line x" title="41:255	Without lexical disambigaation of input, the purser's performance will deteriorate, even if the .skipping is limited only to certain types of adverbial adjuncts." ></td>
	<td class="line x" title="42:255	The most common cases of lexical ambiguity are tho~ of a phwal noun (nns) vs. a singular verb (vbz), a singular noun (nn) vs. a plmal or infinitive verb (vbp,vb), and a past tense verb (vbd) vs. a past participle (vbn), as illusWatod in the following exarnple." ></td>
	<td class="line x" title="43:255	The notation used (vbn or vl~l)?" ></td>
	<td class="line x" title="44:255	explicitly asse.ciates (nns or vbz)?" ></td>
	<td class="line x" title="45:255	a data structure (vb or nn) shared (vbn or vbd)?" ></td>
	<td class="line x" title="46:255	by concun-ent processes (nn.,~ or vbz)?" ></td>
	<td class="line x" title="48:255	wiflt operatimLs defirmd (vbn or vbd)?" ></td>
	<td class="line x" title="49:255	cut it." ></td>
	<td class="line x" title="50:255	3." ></td>
	<td class="line x" title="51:255	PART OF SPEECH TAGGER Oue way of dealing with lexical ambiguity is to use a tagger to preproccss the input marking each wurti with a tags that indicates its syntactic categoriza.tion: a part of speech with selected morphological features such as nunther, tense, mode, case and degree." ></td>
	<td class="line x" title="52:255	The following are tagged sentcoces from the CACM3204 collection: s The(dr) papei'(nn) pre~nts(vbz) a(dt) proposal(on) lor(/n) stmctured(vbn) representation(nn) of(in) multipmgranuning(vbg) in(in) a(dt) high(jj) level(tin) language(nn) .(per) The(tit) notation(nn) used(vbn) explicitly(rb) associates(vbz) ~dt) data0m.v ) structme(nn) shared(vbn) by(in) concmrent(/j) prc~esses(nns) with(in) t)peratit)ns(mJs) defined(vbn) on(in) it(pp) .(per) The tags are underst(xxl as follows: (It determiner, nn singular 1~oan, nns plural noun, in preposition, jj adjective, vbz verb in present tense third person 'lhe decision to force  reducti(m rather than to back up co~ld be triggered by various means." ></td>
	<td class="line x" title="53:255	In clte of TTP parser, it iJ always induced by the thne-citt lignal." ></td>
	<td class="line x" title="54:255	Tagged u~ing the 35-tag Penn 'ft,zebank Tagset cmmed at the University of Pemtsylwmia." ></td>
	<td class="line x" title="55:255	Acq~.s DE COLING-92, NA~'I~, 2328 Ao(rr 1992 1 9 9 PROC." ></td>
	<td class="line x" title="56:255	OF COLlNG-92, NANrF.s, AUo." ></td>
	<td class="line x" title="57:255	23-28, 1992 singular, to particle 'to', vbg present participle, vim past participle, vbd past tense verb, vb infinitive verb, cc coordinate conjunction." ></td>
	<td class="line x" title="58:255	Tagging of the input text substantially reduces the search space of a top-down parser since it resolves most of the lexical level ambiguities." ></td>
	<td class="line x" title="59:255	In the examples ahove, tagging of presents as 'vbz' in the first sentence cuts off a potentially long and cosily 'garden path' with presents as a plural noun followed by a headless relative clause starting with (that) a proposal  In the second sentence, tagging resolves ambiguity of used (vim vs. vbd), and associates (vbz vs. nns)." ></td>
	<td class="line x" title="60:255	Perhaps more imlxmantly, elimination of word-level lexical ambiguity allows the parser to make projection about the input which is yet to be parsed, using a simple lookabead; in particular, phrase boundaries can be determined with a degree of confidence (Church, 1988)." ></td>
	<td class="line x" title="61:255	This latter property is critical for implementing skip-and-fit recovery technique outlined in the previous section." ></td>
	<td class="line x" title="62:255	Tagging of input also helps to reduce the number of parse structures that can be assigned to a sentence, decreases the demand for consulting of the dictionary, and simplifies dealing with unknown words." ></td>
	<td class="line x" title="63:255	Since every item in the sentence is assigned a tag, so are the words for which we have no entry in the lexicon." ></td>
	<td class="line x" title="64:255	Many of these words will be tagged as 'np' (proper noun), however, the surrounding tags may force other selections." ></td>
	<td class="line x" title="65:255	In the following example, chinese, which does not appear in the dictionary, is tagged as 'j.j':~ this(dO papca'(nn) dates(vbz) back(rb) the(d 0 genesis(nn) of(in) binary(j/) conception(nn) circa(/n) 5000(cd) years(nns) ago(rb),(corn) as(rb) derived(vbn) by(m) the(d 0 chinese(if) ancients(nns) .(per) We use a stochastic tagger to process the input text prior to parsing." ></td>
	<td class="line x" title="66:255	The tagger is based upon a bigram model; it selects most likely tag for a word given co-occurrence probabilities computed from a small training SgL 7 4." ></td>
	<td class="line x" title="67:255	PARSING wITH TTP PARSER TTP (Tagged Text Parser) is a top down English parser specifically designed for fast, reliable processing of large amounts of text." ></td>
	<td class="line x" title="68:255	6 We use the machine wadable version of the Oxford Advanced Learner's Dictionary (OALD)." ></td>
	<td class="line x" title="69:255	7 The program, suppfiod to us by Bolt Benmck and Newman, openttes in two almmative modes, either telocting  single most likely tag for each word (best-tag option, the one we use t prcaenO, or supplying t slion tanked list of alternatives (Mercer et al. , 1991)." ></td>
	<td class="line x" title="70:255	TTP is based on the Linguistic String Grammar developed by Sager (1981)." ></td>
	<td class="line x" title="71:255	Written in Quintus Prolog, the parser currently encompasses more than 400 grammar productions, s TIP produces a regularized representation of each lmrsed sentence that reflects the sentence's logical structure." ></td>
	<td class="line x" title="72:255	This representation may differ considerably from a standard Imrse tree, in that the constituents get moved around (e.g. , de." ></td>
	<td class="line x" title="73:255	passivization, de--dativization), and the phrases are organized recursively around their head elements." ></td>
	<td class="line x" title="74:255	An important novel feature of TIP parser is that it is equipped with a time-out mechanism that allows for fast closing of more difficult sub-constituents after a preset amount of time has elapsed without producing a parse." ></td>
	<td class="line x" title="75:255	Although a complete analysis is attempted for each sentence, the parser may occasionally ignore fragments of input to resume 'normal' processing after skipping a few words." ></td>
	<td class="line x" title="76:255	These fragments are latex analyzed separately and attached as incomplete constituents to the main parse tree." ></td>
	<td class="line x" title="77:255	As the parsing ixoceeds, each sentence receives a new slot of time during which its parse is to be returned." ></td>
	<td class="line x" title="78:255	The amount of time allotted to any particular sentence can be regulated to obtain an acceptable compromise between parser's speed and precision." ></td>
	<td class="line x" title="79:255	In our experiments we found that 0.5 see/sentence time slot was appropriate for the CACM abstracts, while 0.7 see/sentence was more appropriate for generally longer sentences in MUC-3 articles." ></td>
	<td class="line x" title="80:255	9 The actual length of the time interval allotted to any one sentence may depend on this sentence's length in words, although this dependency need not be linear." ></td>
	<td class="line x" title="81:255	Such adjustments will have only limited impact on the parser's speed, but they may affect the quality of produced parse trees." ></td>
	<td class="line x" title="82:255	Unfortunately, there is no obvious way to evaluate quality of parsing except by using its results to attain some measurable ends." ></td>
	<td class="line x" title="83:255	We used the parsed CACM collection to generate domain-specific word correlations for query processing in an information retrieval system, and the results were satisfactory." ></td>
	<td class="line x" title="84:255	For other applications, such as information extraction and deep understanding, a more accurate analysis may be required, m * See (Strzalkowski, 1990) for Prolog implementation details." ></td>
	<td class="line x" title="85:255	Giving the parser more time per sentence doesn't always mean that  belmr (more accurate) parse will be obtained." ></td>
	<td class="line x" title="86:255	For complex or extra-grammatical structures we are likely to be better o(f if we do not allow the parser wander around for too long: the molt likely inteq~mtation of an unexpected input is probably the one gcncnlted early (the grammar rule ordering en forces some preferences)." ></td>
	<td class="line x" title="87:255	Jo A qualitative method for par~cr evaluation has he~a pro\[me.ed in (ihrrison et al,, 1990, and it may be used to mike  rdtire comtxtrison of purser's accuracy." ></td>
	<td class="line x" title="88:255	What is not dear is how oeuate a par~er needs to be for may particular apptic.iticct." ></td>
	<td class="line x" title="89:255	ACTES DE COLING-92, NANTES, 23-28 AOt3T 1992 2 0 0 PROC." ></td>
	<td class="line x" title="90:255	OF COLING-92, NANTES, AUG. 23-28, 1992 Initially, a full analysis of each sentence is attempted." ></td>
	<td class="line x" title="91:255	If a parse is not returned before the allotted time elapses, the parser enters the time-out mode." ></td>
	<td class="line x" title="92:255	From this point on, the parser is permitted to skip portions of input to reach a starter terminal for the next constituent to be parsed, and closing the currently opea one (or ones) with whatever partial representation has been generated thus far." ></td>
	<td class="line x" title="93:255	The result is an approximate partial parse, which shows the overall structure of the sentence, from which some of the constituents may be missing." ></td>
	<td class="line x" title="94:255	The fragments skipped in the first pass are not thrown out, instead they are analyzed by a simple phrasal post-processor that looks for noun phrases and relative clauses and then attaches the recovered material to the main parse structure." ></td>
	<td class="line x" title="95:255	The time-out mechanism is implemented using a straightforward parameter passing and is at present limited to only a sub~et of nonterminals used by the grammar." ></td>
	<td class="line x" title="96:255	Suppose that X is such a nonterminal, and that it appears on the right-hand side of a production S ---> X Y Z. The set of 'starters' is computed for Y, which consists of the word tags that can occur as the left-most constituent of Y. This set is passed as a parameter while the parser attempts to recognize X in the input." ></td>
	<td class="line x" title="97:255	If X is recognized successfully within a preset time, then the parser proceeds to parse a Y, and nothing else happens." ></td>
	<td class="line x" title="98:255	On the other hand, if the parser cannot determine whether there is an X in the input or not, that is, it neither succeeds nor fails in parsing X before being timed out, the unfinished X constituent is closed with a partial l~rse, and the parser is restarted at the closest element from the sta~ers set for Y that can be found in the remainder of the input." ></td>
	<td class="line x" title="99:255	If Y rewrites to an empty string, the starters for Z to the right of Y are added to the starters for Y and both sets are passed as a parameter to X. As an example consider the following clauses in the TIP parser: ~1 sentence(P) :assertion(\[\],P)." ></td>
	<td class="line x" title="100:255	assertion (SR, P) : clause(SR,Pl),s coord(SR, PI,P)." ></td>
	<td class="line x" title="101:255	clause (SR, P) :sa ( \[pdt, dr, cd, pp, ppS, J j, Jjr, j Js, nn, nns, np, nps\],PAl), subject ( \[vbd, vbz, vbp\], Tail, P 1 ), verbphrase (SR, Tail, PI, PAl, P), subtail (Tail) . thats (SR, P) :that, assertion (SR, P) . In the clause production above, a (finite) clause n The clauses arc slightly simplified, and some arguments are removed for expository reasons." ></td>
	<td class="line x" title="102:255	rewrites into an (optional) sentence adjunct (SA), a subject, a verbphrase and subject's right adjunct (SUBTAIL, also optional)." ></td>
	<td class="line x" title="103:255	With the exception of subtail, each predicate has a parameter that specifies the list of 'starter' tags for restarting the parser, should the evaluation of this predicate exceed the allotted portion of time." ></td>
	<td class="line x" title="104:255	Thus, in case sa is aborted before its evaluation is complete, the parser will jump over some elemenUs of the unparsed portion of the input looking for a word that could begin a subject phrase (either a predeterminer, a determiner, a count word, a pronoun, an adjective, a noun, or a proper name)." ></td>
	<td class="line x" title="105:255	Likewise, when subject is timed out, the parser will restart with verbphrase at either vbz, vbd or vbp (finite forms of a verb)." ></td>
	<td class="line x" title="106:255	Note that if verbphrase is timed out, then subtail will be ignored, both verbphrase and clause will be closed, and the parser will restart at an element of set SR passed down to clause from assertion." ></td>
	<td class="line x" title="107:255	Note also that in the top-level production for a sentence the starter set for assertion is initialized to be empty: if the failure occurs at this level, no continuation is possible." ></td>
	<td class="line x" title="108:255	When a non-terminal is timed out and the parser jumps over a non-zero length fragment of input, it is assumed that the skipped part was some subconstituent of the closed non-terminal." ></td>
	<td class="line x" title="109:255	Accordingly, a place holder is left in the parse structure under the node dominated by this non-terminal, which will be later filled by some nominal material recovered from the fragment." ></td>
	<td class="line x" title="110:255	The examples given in the Appendix show approximate parse structures generated by TIP." ></td>
	<td class="line x" title="111:255	There are a few caveats in the skip-and-fit parsing strategy just outlined which warrant further explanation." ></td>
	<td class="line x" title="112:255	In particular, the following problems must be resolved to assure parser's effectiveness: how to select starter tags for non-terminals, how to select nonterminals at which to place the starter tags, and finally how to select non-terminals at which input skipping call occur." ></td>
	<td class="line x" title="113:255	Obviotlsly some tags are mote likely to occur at the left-most position of a constituent than others." ></td>
	<td class="line x" title="114:255	~ly, a subject ~ can start with u word tagged with any element from the following fist: Ixlt, dt, cd, ji, jjr, jjs, pp, ppS, nn, nns, np, nps, vbg, vbo, rb, in} 2 In practice, however, we may select only a subset of these, as shown in the clause production above." ></td>
	<td class="line x" title="115:255	Although we now risk missing the left-hand boundary of subject p~rases in some sentences, while skipping an adjunct to their left, most cases are still covered and the chances of making a serious misinterpretation of u Thit list it .ot comphac." ></td>
	<td class="line x" title="116:255	In addition to the tal~ explthled before: pdt \[n~de~trniner, jjt compamtlve *djcctiv, j~ mpcdatire ~.ieO~c, pp pronoun, ppS s~nitiv, rlp, npl p,x~l,er noun." ></td>
	<td class="line x" title="117:255	r'o ~verb." ></td>
	<td class="line x" title="118:255	ACTES DI~; COLING-92." ></td>
	<td class="line x" title="119:255	NANTES." ></td>
	<td class="line x" title="120:255	23-28 nor\]r 1992 2 0 l PROC." ></td>
	<td class="line x" title="121:255	OF COLING-92." ></td>
	<td class="line x" title="122:255	NANTES." ></td>
	<td class="line x" title="123:255	AUG. 23-28, 1992 input are significantly lower." ></td>
	<td class="line x" title="124:255	We also need to decide on how input skipping is to be done." ></td>
	<td class="line x" title="125:255	In a most straightforward design, when a nonterminal X is timed-out, the parser would skip input until it has reached a starter element of a nonterminal Y adjacent to X from the right, according to the top-down predictions, t3 On the other hand, certain adjunct phrases may be of little interest, possibly because of their typically low information contents, and we may choose to ignore them altogether." ></td>
	<td class="line x" title="126:255	Therefore, if X is timed out, and Y is a low contents adjunct phrase, we can make the parser to jump fight to the next nonterminal Z. In the clause production discussed before, subtail is skipped over if verbphrase is timed ouL 14 Finally, it is not an entirely trivial task to select non-terminals at which the input skipping can occur." ></td>
	<td class="line x" title="127:255	If wrong non-terminals are chosen the parser may generate rather uninteresting structures that would be next to useless, or it may become trapped in inadvertently created dead ends, hopelessly trying to fit the parse." ></td>
	<td class="line x" title="128:255	Consider, for example, the following sentence, taken from MUC-3 corpus of news messages: HONDURAN NATIONAL POLICE ON MONDAY PRESENTED TO THE PRESS HONDURAN JUAN BAUTISTA NUNEZ AMADOR AND NICARAGUAN LUIS FERNANDO ORDON\[~ REYES, WHO TOLD REPORTERS THAT COMMANDER AURELIANO WAS ASSASSINATED ON ORDERS FROM JOSE DE JESUS PENA, THE NICARAGUAN EMBASSY CHIEF OF SECURITY." ></td>
	<td class="line x" title="129:255	After reaching the verb PRESENTED, the parser consalts the lexicon and finds that one of the possible subcategorizations of this verb is \[pun,to\], that is, its object suing can be a prepositional phrase with 'to' followed by a noun phrase." ></td>
	<td class="line x" title="130:255	The parser thus begins to look for a prepositional phrase starting at 'TO THE PRESS ', but unfortunately misses the end of the phrase at PRESS (the following word is tagged as a noun), and continues until reaching the end of sentence." ></td>
	<td class="line x" title="131:255	At this point it realizes that it went too far (there is no noun phrase left), and starts backing up." ></td>
	<td class="line x" title="132:255	Before the parser has a chance to back up to the word PRESS and correct the early mistake, however, the time-out mode is turned on, and instead of abandoning the current analysis, the parser now tries hard to fix it by skipping varying portions of input." ></td>
	<td class="line x" title="133:255	This may take a considerable amount time if the skip points are badly i~ Note that the top-down predictions are crucial for the skipping parser, wheahcr the paner's processing is top-down or bouemup." ></td>
	<td class="line x" title="134:255	t4 :mbta//it the remainder of a discontinued subject phrase." ></td>
	<td class="line x" title="135:255	placed." ></td>
	<td class="line x" title="136:255	On the other hand, we wouldn't like to allow an easy exit by accepting an empty noun phrase at the end of the sentenceI \]5 One of the essential properties of the input skipping mechanism is its flexibility to jump over varying-size chunks of the input sUing." ></td>
	<td class="line x" title="137:255	The goal is to fit the input with a closest matching parse structure while leaving the minimum number of words unaccounted for." ></td>
	<td class="line x" title="138:255	In TIP, the skipping mechanism is implemented by adding extra productions for selected nonterminals, and these are always tried fast whenever the nonterminal is to be expanded." ></td>
	<td class="line x" title="139:255	We illustrate this with rn productions covering fight adjuncts to a noun." ></td>
	<td class="line x" title="140:255	rn (SR, P) :timed out, !, skip (SR), store (P) . rn(_, \[\]) :la ( \[ \[pdt, dt, vbz, vbp, vbd, rod, eom, ha, rmr\] \] ), \+is ( \[ \[C0~\], \[wdt,wp,wps\] \] ) . rn(SR,P) :rnI(SR, P)." ></td>
	<td class="line x" title="141:255	In the rn predicate, SR is the list of starter tags and P is the parse tree fragment." ></td>
	<td class="line x" title="142:255	The first production checks if the time-out mode has already been entered, in which case the input is skipped until a starter tag is found, while the skipped words are stored into P to be analyzed later in the purser's second pass." ></td>
	<td class="line x" title="143:255	Note that in this case all other rn productions are cut off; however, should the first skip-and-fit attempt fail to lead to a successful parse, backtracking may eventually force predicate skip(SR) to evaluate again and make a longer leap." ></td>
	<td class="line x" title="144:255	In a top-down left to right parser, each input skipping location becomes potentially a multiple bucktracking point which needs to be controlled in order to avoid a combinatorial explosion of possibilities." ></td>
	<td class="line x" title="145:255	This is accomplished by supplementing top-down predictions with bottom-up, data-driven fragmentation of input, and a limited lookahead." ></td>
	<td class="line x" title="146:255	For example, in the second of the rn productions above, a right adjunct to a noun can be considered empty if the item following the noun is either a period, a semicolon, a comma, or a word tagged as pdt, dt, vbz, vbp, vbd, or md, but not a comma followed by a relative pronoun.~6,2 In the present implementation, when the skipping mode is entered, it will stay on for the balance of the first pass in parsing of the current sentence." ></td>
	<td class="line x" title="147:255	'\[~his way, o~ skip-and-fit attempt may lead to anc4her before any backtracking is considered." ></td>
	<td class="line x" title="148:255	An altemafive is to do time-out on a nonterminal by nonterminal basis, that is, to time out processing of selected nonterminals only and then resume regular parsing, qhis design leads to a far more complex implementation and somewhat inferior performance, but it might be worth comic~ring in the fumre." ></td>
	<td class="line x" title="149:255	t6 md modal veto; vbp plural verb; wdt, wp, wps ttladve pronouns." ></td>
	<td class="line x" title="150:255	ACq'ES DE COLING-92, NANTES, 23-28 AOt3T 1992 2 0 2 PROC." ></td>
	<td class="line x" title="151:255	OF COLING-92, NANTES, AUG. 23-28, 1992 5." ></td>
	<td class="line x" title="152:255	ROBUSTNESS TIP is a robust parser and it will process nearly every sentence or phrase, provided the latter is reasonably correctly tagged." ></td>
	<td class="line x" title="153:255	17 The lmrser robustness is further increased by allowing for a gradual degradation of its performance rather than an outright failure in the face of an unexpected input." ></td>
	<td class="line x" title="154:255	Each sentence or phrase is attempted to be analyzed in up to four ways: (1) as a sentence, (2) as a noun phrase or a preposition phrase with a right adjunct(s), (3) as a gemndive clause, and if all these fail, (4) as a series of simple noun phrases, with each of these attempts allotted a fresh time slice) s The purpose of this extension is to accommodate some infrequent but still important constrnctions, such as dries, itemizations, and lists." ></td>
	<td class="line x" title="155:255	6." ></td>
	<td class="line x" title="156:255	DISCUSSION In this paper we described TIP, a fast and robust parser for natural language." ></td>
	<td class="line x" title="157:255	In the experiments conducted with various text collections of more that 50 million words the average parsing speed recorded was approx." ></td>
	<td class="line x" title="158:255	0.5 sec/sentence." ></td>
	<td class="line x" title="159:255	For example, the total time spent on parsing the CACM-3204 collection was less than 1.5 hours." ></td>
	<td class="line x" title="160:255	In other words, TIP can process 100,000 words in approximately 45 minutes, and it could parse a gigabyte of text (approx." ></td>
	<td class="line x" title="161:255	150 million words) in about 40 days, on a 21 MIPS computer." ></td>
	<td class="line x" title="162:255	The parser is based on a wide coverage grammar for English, and it contains a powerful skip-and-fit recovery mechanism that allows it to deal with unexpected input and to perform effectively under a severe time pressure." ></td>
	<td class="line x" title="163:255	Prior to parsing, the input text is tagged with a stochastic tagger that assigns part-of-speech labels to every word, thus resolving lexical level ambiguity." ></td>
	<td class="line x" title="164:255	TIP has been used as front-end of a natural language processing component to a traditional document-based information retrieval system (Strzalkowski and Vauthey, 1992)." ></td>
	<td class="line x" title="165:255	The parse structures were further analyzed to extract word and phrase dependency relations which were in turn used as input to various statistical and indexing processes." ></td>
	<td class="line x" title="166:255	The results obtained were generally satisfactory: an improvement in both recall and precision of document retrieval have been observed." ></td>
	<td class="line x" title="167:255	At present, we are also conducting experiments with large corpora of technical computer,science texts in order to extract domain-specific,7 Some sentences (1 in 5000) mmy still fail to parse if tagging errors are." ></td>
	<td class="line x" title="168:255	compotmded in In unexpected way." ></td>
	<td class="line x" title="169:255	ts Although parsing of some sentences may now approach four drnes the allotted time limit, we noted that the average parsing tinm per sentence at 0.745 sec." ></td>
	<td class="line x" title="170:255	is only slighdy above the time-out limit." ></td>
	<td class="line x" title="171:255	conceptual taxonomies for an even greater gain in retrieval effectiveness." ></td>
	<td class="line x" title="172:255	7." ></td>
	<td class="line x" title="173:255	ACKNOWLEDGEMENTS We wish to thank Ralph Weischedel and Heidi Fox of BBN for assisting in the use of the part of speech tagger." ></td>
	<td class="line x" title="174:255	ACM has generously provided us with the Computer Library text database." ></td>
	<td class="line x" title="175:255	This paper is based upon work supported by the Defense Advanced Research Project Agency under Contract N00014-90J-1851 from the Office of Naval Research, the National Science Foundation under Grant IRI-8902304, and by the Canadian Institute for Robotics and Intelligent Systems (IRIS)." ></td>
	<td class="line x" title="176:255	APPENDIX: Sample parses A few examples of non-standard output generated by TTP are shown in Figures 1 to 3." ></td>
	<td class="line x" title="177:255	In Figure 1, 'ITP has failed to find the main verb and it had to jump over much of the last phrase such as the LR(k) grammars, partly due to an improper tokenization of LR(k) (note skipped nodes indicating the material ignored in the first pass)." ></td>
	<td class="line x" title="178:255	In Figure 2, the parser has initially assumed that the conjunction in the sentence has the narrow scope, then it realized that something went wrong but, apparently, there was no time left to back up." ></td>
	<td class="line x" title="179:255	Note, however, that little has been lost: a complete strncture of the second half of this sentence following the conjuction and is easily recovered from the parse tree (var points up to the dominating rip)." ></td>
	<td class="line x" title="180:255	Occasionally, sentences may come out substantially truncated, as shown in Figure 3, where although has been mistagged as a preposition." ></td>
	<td class="line x" title="181:255	SENTF~CE: The problem of determining whether an arbitrary context-free grammar is a member of some easily parsed subclass of grammars such as the LR(k) grammars is considered." ></td>
	<td class="line x" title="182:255	APPROXIMATE PARSE: \[ \[verb,\[\] \],\[subject, \[np,\[n,problem\],\[t_pos,the\], \[of,\[\[verb,\[determine\]\],\[subject,anyone\], \[object,\[\[verb,\[be\]\], \[subject,\[np,\[n,grammar\],\[t_pos,an\], \[adj,\[arbitrary\]\],\[adj,\[context free J\]\]\], \[object,\[np,\[n,member\],\[t_pos,a\], \[of,\[np,\[n,subclass\],\[t_pos,some\], \[a pos_v, \[\[verb,\[parse,lady,easily\]I\], \[subject,anyone\], \[object,pro\]\]\], \[of.\[np,\[n,grammar\], \[rn wb,\[\[verb,\[such\]\]." ></td>
	<td class="line x" title="183:255	\[subject,var\]\]\]\]\]\]\]\]\]\] \], \[sub_urd,Ias,\[\[verb,\[be\] \],\[subject,pro\], \[object,\[np,\[n,kl,\[t_pos,the\], \[adj,\['lrC\]\]\]\]\]\]\], \[skipped,\[\[np,\[n,grammar\]\]\]\]\]\]\]\], \[skipped,\[\[is\],\[wh rel,\[\[verb,\[consider\]\], \[sabject,anyone\],\[object,var\]\]\]\]\]\]." ></td>
	<td class="line x" title="184:255	Figure 1." ></td>
	<td class="line x" title="185:255	SENTENCE: The TX-2 computer at MIT Lincoln Laboratory was used for the implementation of such a system and the characteristics of this implementation are reported." ></td>
	<td class="line x" title="186:255	APPROXIMATE PARSE: \[\[bc\],\[\[verb,\[usc\]\], \[subject,anyone\], \[object,\[np,\[n,compster\],\[t_pos,the\],\[adj,\[tx_2\]\]\]\], \[for,\[and, \[np,\[n,implem entation\],\[t._pos,the\], \[of,\[np,\[n,system\],\[t pos,\[such,a\]\]\]\]\], \[np,\[n,characteristics\],\[t_.pos,the\], \[o f,\[np,\[n,implementation\],\[t pos,this\]\]\], \[skipped, \[\[are\],\[w h_rel,\[\[verb,\[report\]\], \[subject,anyone\], \[object,var\]\]\]\]\]\]\]\]\], \[at,\[np,\[n,laboratory\],\[adj,\[mitl\], \[n pos,\[np,\[n,lincoln\]\]\]\]\]\]." ></td>
	<td class="line x" title="187:255	Figure 2." ></td>
	<td class="line x" title="188:255	SENTENCE: In principle, the system can deal with any orthography, although at present it is limited to 4000 Chinese characters and some mathematical symbols." ></td>
	<td class="line x" title="189:255	APPROXIMATE pARSE: \[\[can_anx\],\[\[verb.\[deal\]\]." ></td>
	<td class="line x" title="190:255	\[suhject,\[np,\[n,system\].\[t pos,the\]\]\], \[sub_oral,\[with,\[\[verb,\[limit\]\], \[subject.anyone\], \[object, \[skipped." ></td>
	<td class="line x" title="191:255	\[ \[np.\[n,orthography\],\[t pos,any\]\]." ></td>
	<td class="line x" title="192:255	\[cornS.although.at\]." ></td>
	<td class="line x" title="193:255	\[np.\[n,present\]\], \[np,\[n,it\]\], \[is\]\]\]\], \[to,\[np,\[n,character\], \[counl,\[4000\]\], \[a_pos,\[chinese\]\], \[skipped, \[\[and\], \[np,\[n,symbol\],\[t~pos,some\], \[adj,\[mathematical\]\]\]\]\]\]\]\]\]\]\], \[in,\[np,\[n,principle\]\]\]\]." ></td>
	<td class="line x" title="194:255	Figure 3." ></td>
	<td class="line x" title="195:255	AcrEs DE COLING-92, NANTES, 23-28 hotzr 1992 2 0 4 PROC." ></td>
	<td class="line x" title="196:255	OF COL1NG-92, NANTES, AUG. 23-28, 1992 ACRES DE COLING-92." ></td>
	<td class="line x" title="197:255	NANTES, 23-28 AOI3T 1992 2 0 3 PROC." ></td>
	<td class="line x" title="198:255	OF COLING-92, NANTES, AUG. 23-28, 1992 REFERENCES Church, Kenneth Ward." ></td>
	<td class="line x" title="199:255	1988." ></td>
	<td class="line x" title="200:255	'A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text'." ></td>
	<td class="line x" title="201:255	Proceedings of the Second Conference on Applied Natural Language Processing, pp." ></td>
	<td class="line x" title="202:255	136-143." ></td>
	<td class="line x" title="203:255	Church, Kenneth Ward and Patrick Hanks." ></td>
	<td class="line x" title="204:255	1990." ></td>
	<td class="line x" title="205:255	'Word association norms, mutual information, and lexicography'." ></td>
	<td class="line x" title="206:255	Computational Linguistics, 16(1), MIT Press, pp." ></td>
	<td class="line x" title="207:255	22-29." ></td>
	<td class="line x" title="208:255	De Marcken, Carl G. 1990." ></td>
	<td class="line x" title="209:255	'Parsing the LOB corpus'." ></td>
	<td class="line x" title="210:255	Proceedings of the 28th Meeting of the ACL, Pittsburgh, PA. pp." ></td>
	<td class="line x" title="211:255	243-251." ></td>
	<td class="line x" title="212:255	Harrison, Philip, et al. 1991." ></td>
	<td class="line x" title="213:255	'Evaluating Syntax Performance of Parser/Grammars of English'." ></td>
	<td class="line x" title="214:255	Natural Language Processing Systems Evaluatiou Workshop, Berkeley, CA." ></td>
	<td class="line x" title="215:255	pp." ></td>
	<td class="line x" title="216:255	71-78." ></td>
	<td class="line x" title="217:255	Hindle, Donald." ></td>
	<td class="line x" title="218:255	1983." ></td>
	<td class="line x" title="219:255	'User manual of Fidditch, a deterministic parser'." ></td>
	<td class="line x" title="220:255	Naval Research Laboratory Technical Memorandum 7590-142." ></td>
	<td class="line x" title="221:255	Hindle, Donald and Mats Rooth." ></td>
	<td class="line x" title="222:255	1991." ></td>
	<td class="line x" title="223:255	'Structural Ambiguity and Lexical Relations'." ></td>
	<td class="line x" title="224:255	Proceedings of the 29th Meeting of the ACL, Berkeley, CA." ></td>
	<td class="line x" title="225:255	pp." ></td>
	<td class="line x" title="226:255	229-236." ></td>
	<td class="line x" title="227:255	Jensen, K. , G.E. Heidorn, L.A. Miller, and Y. Ravin." ></td>
	<td class="line x" title="228:255	1983." ></td>
	<td class="line x" title="229:255	'Parse fitting and prose fixing: Getting a hold of ill-formedness'." ></td>
	<td class="line x" title="230:255	Computational Linguistics, 9(3.-4), pp." ></td>
	<td class="line x" title="231:255	147-161." ></td>
	<td class="line x" title="232:255	Meteer, Marie, Richard Schwartz, and Ralph Weischedel." ></td>
	<td class="line x" title="233:255	1991." ></td>
	<td class="line x" title="234:255	'Studies in Part of Speech Labelling'." ></td>
	<td class="line x" title="235:255	Proceedings of the 4th DARPA Speech and Natural Language Workshop, Morgan-Kaufman, San MateD, CA." ></td>
	<td class="line x" title="236:255	pp." ></td>
	<td class="line x" title="237:255	331-336." ></td>
	<td class="line x" title="238:255	Ruge, Gerda, Christoph Schwarz, Amy J. Warner." ></td>
	<td class="line x" title="239:255	1991." ></td>
	<td class="line x" title="240:255	'Effectiveness and Efficiency in Natural Language Processing for Large Amounts of Text'." ></td>
	<td class="line x" title="241:255	Journal of the ASIS, 42(6), pp." ></td>
	<td class="line x" title="242:255	450-456." ></td>
	<td class="line x" title="243:255	Sager, Nanmi." ></td>
	<td class="line x" title="244:255	1981." ></td>
	<td class="line x" title="245:255	Natural Language Information Processing." ></td>
	<td class="line x" title="246:255	Addison-Wesley." ></td>
	<td class="line x" title="247:255	Strzalkowski, Tomek." ></td>
	<td class="line x" title="248:255	1990." ></td>
	<td class="line x" title="249:255	'Reversible logic grammars for natural language parsing and generation''." ></td>
	<td class="line x" title="250:255	Computational Intelligence, 6(3), NRC Canada, pp." ></td>
	<td class="line x" title="251:255	145-171." ></td>
	<td class="line x" title="252:255	Strzalkowski, Tomek and Barbara Vauthey." ></td>
	<td class="line x" title="253:255	1992." ></td>
	<td class="line x" title="254:255	'Information Retrieval Using Robust Natural Language Processing'." ></td>
	<td class="line x" title="255:255	Proceedings of the 30th Annual Meeting of the ACL, Newark, Delaware, June 28 July 2 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H92-1040
Information Retrieval Using Robust Natural Language Processing
Strzalkowski, Tomek;"></td>
	<td class="line x" title="1:140	INFORMATION RETRIEVAL USING ROBUST NATURAL LANGUAGE PROCESSING Tomek Strzalkowski Courant Institute of Mathematical Sciences New York University 715 Broadway, rm." ></td>
	<td class="line x" title="2:140	704 New York, NY 10003 tomek@cs.nyu.edu ABSTRACT We developed a fully automated Information Retrieval System which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval." ></td>
	<td class="line x" title="3:140	In early experiments with the standard CACM-3204 collection of abstracts, the augmented system has displayed capabilities that made it clearly superior to the purely statistical base system." ></td>
	<td class="line x" title="4:140	1." ></td>
	<td class="line x" title="5:140	OVERALL DESIGN Our information retrieval system consists of a traditional statistical backbone (Harman and Candela, 1989) augmented with various natural language processing components that assist the system in database processing (stemming, indexing, word and phrase clustering, selectional restrictions), and translate a user's information request into an effective query." ></td>
	<td class="line x" title="6:140	This design is a careful compromise between purely statistical non-linguistic approaches and those requiring rather accomplished (and expensive) semantic analysis of data, often referred to as 'conceptual retrieval'." ></td>
	<td class="line x" title="7:140	The conceptual retrieval systems, though quite effective, are not yet mature enough to be considered in serious information retrieval applications, the major problems being their extreme inefficiency and the need for manual encoding of domain knowledge (Mauldin, 1991)." ></td>
	<td class="line x" title="8:140	In our system the database text is first processed with a fast syntactic parser." ></td>
	<td class="line x" title="9:140	Subsequently certain types of phrases are extracted from the parse lxees and used as compound indexing terms in addition to single-word terms." ></td>
	<td class="line x" title="10:140	The extracted phrases are statistically analyzed as syntactic contexts in order to discover a variety of similarity links between smaller subphrases and words occurring in them." ></td>
	<td class="line x" title="11:140	A further filtering process maps these similarity links onto semantic relations (generalization, specialization, synonymy, etc)." ></td>
	<td class="line x" title="12:140	after which they are used to transform user's request into a search query." ></td>
	<td class="line x" title="13:140	The user's natural language request is also parsed, and all indexing terms occurring in them are identified." ></td>
	<td class="line x" title="14:140	Next, certain highly ambiguous (usually single-word) terms are dropped, provided that they also occur as elements in some compound terms." ></td>
	<td class="line x" title="15:140	For example, 'natural' is deleted from a query already containing 'natural language' because 206 'natural' occurs in many unrelated contexts: 'natural number', 'natural logarithm', 'natural approach', etc. At the same time, other terms may be added, namely those which are linked to some query term through admissible similarity relations." ></td>
	<td class="line x" title="16:140	For example, 'fortran' is added to a query containing the compound term 'program language' via a specification link." ></td>
	<td class="line x" title="17:140	After the final query is constructed, the database search follows, and a ranked list of documents is returned." ></td>
	<td class="line x" title="18:140	It should be noted that all the processing steps, those performed by the backbone system, and these performed by the natural language processing components, are fully automated, and no human intervention or manual encoding is required." ></td>
	<td class="line x" title="19:140	2." ></td>
	<td class="line x" title="20:140	FAST PARSING WITH TTP TIP (Tagged Text Parser) is based on the Linguistic String Grammar developed by Sager (1981)." ></td>
	<td class="line x" title="21:140	Written in Quintus Prolog, the parser currently encompasses more than 400 grammar productions." ></td>
	<td class="line x" title="22:140	It produces regularized parse tree representations for each sentence that reflect the sentence's logical structure." ></td>
	<td class="line x" title="23:140	The parser is equipped with a powerful skip-and-fit recovery mechanism that allows it to operate effectively in the face of ill-formed input or under a severe time pressure." ></td>
	<td class="line x" title="24:140	In the recent experiments with approximately 6 million words of English texts, 1 the parser's speed averaged between 0.45 and 0.5 seconds per sentence, or up to 2600 words per minute, on a 21 MIPS SparcStation ELC." ></td>
	<td class="line x" title="25:140	Some details of the parser are discussed below." ></td>
	<td class="line x" title="26:140	2 TIP is a full grammar parser, and initially, it attempts to generate a complete analysis for each sentence." ></td>
	<td class="line x" title="27:140	However, unlike an ordinary parser, it has a built-in timer which regulates the amount of time allowed for parsing any one sentence." ></td>
	<td class="line x" title="28:140	If a parse is not returned before the allotted time I These include CACM-3204, MUC-3, and a selection of nearly 6,000 technical articles extracted from Computer Library database (a Zfff Communications Inc. CD-ROM)." ></td>
	<td class="line x" title="29:140	2 A complete description can be found in (Strzalkowski, 1991)." ></td>
	<td class="line x" title="30:140	elapses, the parser enters the skip-and-fit mode in which it will try to 'fit' the parse." ></td>
	<td class="line x" title="31:140	While in the skip-and-fit mode, the parser will attempt to forcibly reduce incomplete constituents, possibly skipping portions of input in order to restart processing at a next unattempted constituent." ></td>
	<td class="line x" title="32:140	In other words, the parser will favor reduction to backtracking while in the skip-and-fit mode." ></td>
	<td class="line x" title="33:140	The result of this strategy is an approximate parse, partially fitted using top-down predictions." ></td>
	<td class="line x" title="34:140	The fragments skipped in the first pass are not thrown out, instead they are analyzed by a simple phrasal parser that looks for noun phrases and relative clauses and then attaches the recovered material to the main parse structure." ></td>
	<td class="line x" title="35:140	As an illustration, consider the following sentence taken from the CACM-3204 corpus: The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees, in order to construct a program satisfying certain specifications a theorem induced by those specifications is proved, and the desired program is extracted from the proof." ></td>
	<td class="line x" title="36:140	The italicized fragment is likely to cause additional complications in parsing this lengthy string, and the parser may be better off ignoring this fragment altogether." ></td>
	<td class="line x" title="37:140	To do so successfully, the parser must close the currently open constituent (i.e. , reduce a program satisfying certain specifications to NP), and possibly a few of its parent constituents, removing corresponding productions from further consideration, until an appropriate production is reactivated." ></td>
	<td class="line x" title="38:140	In this case, TIP may force the following reductions: SI --> to V NP; SA ----> SI; S ---> NP V NP SA, until the production S --> S and S is reached." ></td>
	<td class="line x" title="39:140	Next, the parser skips input to find and, and resumes normal processing." ></td>
	<td class="line x" title="40:140	As may be expected, the skip-and-fit strategy will only be effective if the input skipping can be performed with a degree of determinism." ></td>
	<td class="line x" title="41:140	This means that most of the lexical level ambiguity must be removed from the input text, prior to parsing." ></td>
	<td class="line x" title="42:140	We achieve this using a stochastic parts of speech tagger 3 to preprocess the text." ></td>
	<td class="line x" title="43:140	3." ></td>
	<td class="line x" title="44:140	WORD SUFFIX TRIMMER Word stemming has been an effective way of improving document recall since it reduces words to their common morphological root, thus allowing more successful matches." ></td>
	<td class="line x" title="45:140	On the other hand, stemming tends to decrease retrieval precision, if care is not taken to prevent situations where otherwise unrelated words are reduced to the same stem." ></td>
	<td class="line x" title="46:140	In our system we replaced a traditional morphological stemmer with a conservative dictionary-assisted suffix trimmer." ></td>
	<td class="line x" title="47:140	4 The suffix trimmer performs essentially two tasks: (1) it reduces inflected word forms to their root forms as specified in the dictionary, and (2) it converts nominalized verb 3 Courtesy of Bolt Beranek and Newman." ></td>
	<td class="line x" title="48:140	4 We use Oxford Advanced Leamer's Dictionary (OALD) MRD." ></td>
	<td class="line x" title="49:140	forms (eg." ></td>
	<td class="line x" title="50:140	'implementation', 'storage') to the root forms of corresponding verbs (i.e. , 'implement', 'store')." ></td>
	<td class="line x" title="51:140	This is accomplished by removing a standard suffix, eg." ></td>
	<td class="line x" title="52:140	'stor+age', replacing it with a standard root ending ('+e'), and checking the newly created word against the dictionary, i.e., we check whether the original root ('storage') is defined using the new root ('store')." ></td>
	<td class="line x" title="53:140	This allows reducing 'diversion' to 'diverse' while preventing 'version' to be replaced by 'verse'." ></td>
	<td class="line x" title="54:140	Experiments with CACM-3204 collection show an improvement in retrieval precision by 6% to 8% over the base system equipped with a standard morphological stemmer (the SMART stemmer)." ></td>
	<td class="line x" title="55:140	4." ></td>
	<td class="line x" title="56:140	HEAD-MODIFIER STRUCTURES Syntactic phrases extracted from TTP parse trees are headmodifier pairs: from simple word pairs to complex nested structures." ></td>
	<td class="line x" title="57:140	The head in such a pair is a central element of a phrase (verb, main noun, etc)." ></td>
	<td class="line x" title="58:140	while the modifier is one of the adjunct arguments of the head." ></td>
	<td class="line x" title="59:140	5 For example, the phrase fast algorithm for parsing context-free languages yields the following pairs: algorithm+fast, algorithm+parse, parse+language, language+context_free." ></td>
	<td class="line x" title="60:140	The following types of pairs were considered: (1) a head noun and its left adjective or noun adjunct, (2) a head noun and the head of its right adjunct, (3) the main verb of a clause and the head of its object phrase, and (4) the head of the subject phrase and the main verb, These types of pairs account for most of the syntactic variants for relating two words (or simple phrases) into pairs carrying compatible semantic content." ></td>
	<td class="line x" title="61:140	For example, the pair \[retrieve,information\] is extracted from any of the following fragments: information retrieval system; retrieval of information from databases; and information that can be retrieved by a user-controlled interactive search process." ></td>
	<td class="line x" title="62:140	6 An example is shown in the appendix.7 5." ></td>
	<td class="line x" title="63:140	TERM CORRELATIONS FROM TEXT Head-modifier pairs form compound terms used in database indexing." ></td>
	<td class="line x" title="64:140	They also serve as occurrence contexts for smaller terms, including single-word terms." ></td>
	<td class="line x" title="65:140	In order to determine whether such pairs signify any important association between terms, we calculate the value of the 5 In the experiments reported here we extracted head-modifier word pairs only." ></td>
	<td class="line x" title="66:140	CACM collection is too small to warrant generation of larger compounds, because of their low frequencies." ></td>
	<td class="line x" title="67:140	To deal with nominal compounds we use frequency information about the pairs generated from the entire corpus to form preferences in ambiguous situations, such as natural language processing vs. dynamic information processing." ></td>
	<td class="line x" title="68:140	7 Note that working with the parsed text ensures a high degree of precision in capturing the meaningful phrases, which is especially evident when compared with the results usually obtained from either unprocessed or only partially processed text (Lewis and Croft, 1990)." ></td>
	<td class="line x" title="69:140	207 Informational Contribution (IC) function for each element in a pair." ></td>
	<td class="line x" title="70:140	Higher values indicate stronger association, and the element having the largest value is considered semantically dominant." ></td>
	<td class="line oc" title="71:140	IC function is a derivative of Fano's mutual information formula recently used by Church and Hanks (1990) to compute word co-occurrence patterns in a 44 million word corpus of Associated Press news stories." ></td>
	<td class="line n" title="72:140	They noted that while generally satisfactory, the mutual information formula often produces counterintuitive results for low-frequency data." ></td>
	<td class="line o" title="73:140	This is particularly worrisome for relatively smaller IR collections since many important indexing terms would be eliminated from consideration." ></td>
	<td class="line x" title="74:140	Therefore, following suggestions in Wilks et al.(1990), we adopted a revised formula that displays a more stable behavior even on very low counts." ></td>
	<td class="line x" title="76:140	This new formula IC (x,\[x,y \]) is'based on (an estimate o0 the conditional probability of seeing a word y to the right of the word x, modified with a dispersion parameter for x. fx~r lC (x,\[x,y \]) n,, + d,, -1 where fx~, is the frequency of \[x,y \] in the corpus, n x is the number of pairs in which x occurs at the same position as in Ix,y\], and d(x) is the dispersion parameter understood as the number of distinct words with which x is paired." ></td>
	<td class="line x" title="77:140	When IC(x,\[x,y\])=O, x and y never occur together (i.e. , fx,y = 0); when IC(x,\[x,y\]) = 1, x occurs only with y (i.e. , fx,y =n, and d~ = 1)." ></td>
	<td class="line x" title="78:140	Selected examples generated from CACM-3204 corpus are given in Table 2 at the end of the paper." ></td>
	<td class="line x" title="79:140	IC values for terms become the basis for calculating term-to-term similarity coefficients." ></td>
	<td class="line x" title="80:140	If two terms tend to be modified with a number of common modifiers and otherwise appear in few distinct contexts, we assign them a similarity coefficient, a real number between 0 and 1." ></td>
	<td class="line x" title="81:140	The similarity is determined by comparing distribution characteristics for both terms within the corpus: how much information contents do they carry, do their information contribution over contexts vary greatly, are the common contexts in which these terms occur specific enough?" ></td>
	<td class="line x" title="82:140	In general we will credit high-contents terms appearing in identical contexts, especially if these contexts are not too commonplace." ></td>
	<td class="line x" title="83:140	8 The relative similarity between two words xl and x z is obtained using the following formula (a is a large constant): SIM (x 1,x 2) = log (a ~ sim~ (x 1,x 9) where simy (x l,x z) = MIN (I C (x 1,\[x l,y \]) j C (x 2,\[x 2,y \])) * MIN(IC(y,\[xl,y\])JC(y,\[x2,y\])) The similarity function is further normalized with respect to 8 It would not be appropriate to predict similarity between language and logarithm on the basis of their co-occurrence with natural." ></td>
	<td class="line x" title="84:140	SIM(xl,xl)." ></td>
	<td class="line x" title="85:140	It may be worth pointing out that the similarities are calculated using term co-occurrences in syntactic rather than in document-size contexts, the latter being the usual practice in non-linguistic clustering (eg." ></td>
	<td class="line x" title="86:140	Sparck Jones and Barber, 1971; Crouch, 1988; Lewis and Croft, 1990)." ></td>
	<td class="line x" title="87:140	Although the two methods of term clustering may be considered mutually complementary in certain situations, we befieve that more and slxonger associations can be obtained through syntactic-context clustering, given sufficient amount of data and a reasonably accurate syntactic parser." ></td>
	<td class="line x" title="88:140	9 6." ></td>
	<td class="line x" title="89:140	QUERY EXPANSION Similarity relations are used to expand user queries with new terms, in an attempt to make the final search query more comprehensive (adding synonyms) and/or more pointed (adding specializations)." ></td>
	<td class="line x" title="90:140	1 It follows that not all similarity relations will be equally useful in query expansion, for instance, complementary relations like the one between algol and fortran may actually harm system's performance, since we may end up retrieving many irrelevant documents." ></td>
	<td class="line x" title="91:140	Similarly, the effectiveness of a query containing fortran is likely to diminish if we add a similar but far more general term such as language." ></td>
	<td class="line x" title="92:140	On the other hand, database search is likely to miss relevant documents if we overlook the fact that fortran is a programming language, or that interpolate is a specification of approximate." ></td>
	<td class="line x" title="93:140	We noted that an average set of similarities generated from a text corpus contains about as many 'good' relations (synonymy, speciafization) as 'bad' relations (antonymy, complementation, generalization), as seen from the query expansion viewpoint." ></td>
	<td class="line x" title="94:140	Therefore any attempt to separate these two classes and to increase the proportion of 'good' relations should result in improved retrieval." ></td>
	<td class="line x" title="95:140	This has indeed been confirmed in our experiments where a relatively crude filter has visibly increased retrieval precision." ></td>
	<td class="line x" title="96:140	In order to create an appropriate filter, we expanded the IC function into a global specificity measure called the cumulative informational contribution function (ICW)." ></td>
	<td class="line x" title="97:140	ICW is calculated for each term across all contexts in which it occurs." ></td>
	<td class="line x" title="98:140	The general philosophy here is that a more specific word/phrase would have a more limited use, i.e., would appear in fewer distinct contexts." ></td>
	<td class="line x" title="99:140	ICW is similar to the standard inverted document frequency (idj) measure except that term frequency is measured over syntactic units rather than 9 Non-syntactic contexts cross sentence boundaries with no fuss, which is helpful with short, succinct documents (such as CACM abstracts), but less so with longer texts." ></td>
	<td class="line x" title="100:140	to Query expansion (in the sense considered here, though not quite in the same way) has been used in information retfeval research before (eg." ></td>
	<td class="line x" title="101:140	Sparek Jones and Tait, 1984; Harman, 1988), usually with mixed results." ></td>
	<td class="line x" title="102:140	An alternative is to use term clusters to create new terms, 'metaterms', and use them to index the database instead (eg." ></td>
	<td class="line x" title="103:140	Crouch, 1988; Lewis and Croft, 1990)." ></td>
	<td class="line x" title="104:140	We found that the query expansion approach gives the system more flexibiUty, for instance, by making room for hypertext-style topic exploration via user feedback." ></td>
	<td class="line x" title="105:140	208 document size units." ></td>
	<td class="line x" title="106:140	11 Terms with higher ICW values are generally considered more specific, but the specificity comparison is only meaningful for terms which are already known to be similar." ></td>
	<td class="line x" title="107:140	The new function is calculated according to the following formula: 12 ICW(w) =ICL(w) * ICR (w) where (with nw, dw > 0): ICL (W) = Ic (\[w,_ \]) = n~ aw(nw+aw-1) and analogously for IC R (w )." ></td>
	<td class="line x" title="108:140	For any two terms w 1 and w 2, and a constant ~i > 1, if ICW(w2)>_~* ICW(wl) then w 2 is considered more specific than w 1." ></td>
	<td class="line x" title="109:140	In addition, if SIM,~,~(Wl,Wz)=~> O, where 0 is an empirically established threshold, then w 2 can be added to the query containing term w 1 with weight o. 13 In the CACM-3204 collection: ICW (algol) = 0.0020923 ICW (language) = 0.0000145 ICW (approximate) = 0.0000218 ICW (interpolate) = 0.0042410 Therefore interpolate can be used to specialize approximate, while language cannot be used to expand algol." ></td>
	<td class="line x" title="110:140	Note that if 8 is well chosen (we used 5=10), then the above filter will also help to reject antonymous and complementary relations, such as SIM~orm (pl_i,cobol)=0.685 with ICW (pl_i)=O.O 175 and ICW (cobol)=0.0289." ></td>
	<td class="line x" title="111:140	We continue working to develop more effective filters." ></td>
	<td class="line x" title="112:140	Examples of filtered similarity relations obtained from CACM-3204 corpus are given in Table 3." ></td>
	<td class="line x" title="113:140	7. SUMMARY OF RESULTS The preliminary series of experiments with the CACM3204 collection of computer science abstracts showed a consistent improvement in performance: the average precision increased from 32.8% to 37.1% (a 13% increase), while the normalized recall went from 74.3% to 84.5% (a 14% increase), in comparison with the statistics of the base system." ></td>
	<td class="line x" title="114:140	This improvement is a combined effect of the new stemmer, compound terms, term selection in queries, and query expansion using filtered similarity relations." ></td>
	<td class="line x" title="115:140	The choice of similarity relation filter has beeen found critical in improving retrieval precision through query expansion." ></td>
	<td class="line x" title="116:140	It should also be pointed out that only about 1.5% of all ' We believe that measuring term specificity over document-size contexts (eg." ></td>
	<td class="line x" title="117:140	Sparck Jones, 1972) may not be appropriate in this case." ></td>
	<td class="line x" title="118:140	In particular, syntax-based contexts allow for processing texts without any intemal document structure." ></td>
	<td class="line x" title="119:140	m Slightly simplified here." ></td>
	<td class="line x" title="120:140	13 The filter was most effective at cr = 0.57." ></td>
	<td class="line x" title="121:140	similarity relations originally generated from CACM-3204 were found admissible after filtering, contributing only 1.2 expansion on average per query." ></td>
	<td class="line x" title="122:140	It is quite evident significantly larger corpora are required to produce more dramatic results." ></td>
	<td class="line x" title="123:140	14 15 A detailed summary is given in Table 1 below." ></td>
	<td class="line x" title="124:140	These results, while modest by IR standards, are significant for another reason as well." ></td>
	<td class="line x" title="125:140	They were obtained without any manual intervention into the database or queries, and without using any other information about the database except for the text of the documents (i.e. , not even the hand generated keyword fields enclosed with most documents were used)." ></td>
	<td class="line x" title="126:140	Lewis and Croft (1990), and Croft et al.(1991) report results similar to ours but they take advantage of Computer Reviews categories manually assigned to some documents." ></td>
	<td class="line x" title="128:140	The purpose of this research is to explore the potential of automated NLP in dealing with large scale IR problems, and not necessarily to obtain the best possible results on any particular data collection." ></td>
	<td class="line x" title="129:140	One of our goals is to point a feasible direction for integrating NLP into the traditional IR (Strzalkowski and Vauthey, 1991; Grishman Tests org.system suf~trimmer query exp. Recall Precision 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 Avg." ></td>
	<td class="line x" title="130:140	Prec." ></td>
	<td class="line x" title="131:140	% change Norm Rec." ></td>
	<td class="line x" title="132:140	Queries 0.764 0.775 0.674 0.547 0.449 0.387 0.329 0.273 0.198 0.146 0.093 0.079 0.328 0.743 50 0.793 0.688 0.700 0.547 0.573 0.479 0.486 0.421 0.421 0.356 0.372 0.280 0.304 0.222 0.226 0.170 0.174 0.112 0.114 0.087 0.090 0.356 0.371 8.3 13.1 0.841 0.842 50 50 Table 1." ></td>
	<td class="line x" title="133:140	Recall/precision statistics for CACM-3204 14 KL Kwok (private communication) has suggested that the low percentage of admissible relations might be similar to the phenomenon of 'tight dusters' which while meaningful are so few that their impact is small." ></td>
	<td class="line x" title="134:140	15 A sufficiently large text corpus is 20 million words or more." ></td>
	<td class="line x" title="135:140	This has been partially confirmed by experiments performed at the University of Massachussetts (B. Croft, private communication)." ></td>
	<td class="line x" title="136:140	209 and Strzalkowski, 1991)." ></td>
	<td class="line x" title="137:140	ACKNOWLEDGEMENTS We would like to thank Donna Harman of NIST for making her IR system available to us." ></td>
	<td class="line x" title="138:140	We would also like to thank Ralph Weischedel and Marie Meteer of BBN for providing and assisting in the use of the part of speech tagger." ></td>
	<td class="line x" title="139:140	KL Kwok has offered many helpful comments on an earlier draft of this paper." ></td>
	<td class="line x" title="140:140	In addition, ACM has generously provided us with text data from the Computer Library database distributed by Ziff Communications Inc. This paper is based upon work supported by the Defense Advanced Research Project Agency under Contract N00014-90-J1851 from the Office of Naval Research, and the National Science Foundation under Grant IRI-89-02304." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H92-1047
The Acquisition Of Lexical Semantic Knowledge From Large Corpora
Pustejovsky, James D.;"></td>
	<td class="line x" title="1:143	The Acquisition of Lexical Semantic Knowledge from Large Corpora James Pustejovsky Computer Science Department Brandeis University Waltham, MA 02254 ABSTRACT Machine-readable dictionaries provide the raw material from which to construct computationaily useful representations of the generic vocabulary contained within it." ></td>
	<td class="line x" title="2:143	Many sublanguages, however, are poorly represented in on-line dictionaries, ff represented at all." ></td>
	<td class="line x" title="3:143	Vocabularies geared to specialized domains are necessary for many applications, such as text categorization and information retrieval." ></td>
	<td class="line x" title="4:143	In this paper I describe research devoted to developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of a generative lexicon." ></td>
	<td class="line x" title="5:143	1." ></td>
	<td class="line x" title="6:143	Introduction Machine-readable dictionaries provide the raw material from which to construct computationally useful representations of the generic vocabulary contained within it." ></td>
	<td class="line x" title="7:143	Many sublanguages, however, are poorly represented in on-line dictionaries, if represented at all (cf.Grishman et al (1986))." ></td>
	<td class="line x" title="9:143	Yet vocabularies geared to specialized domains are necessary for many applications, such as text categorization and information retrieval." ></td>
	<td class="line x" title="10:143	In this paper I describe research devoted to developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of a generative theory of the lexicon (Pustejovsky 1991)." ></td>
	<td class="line x" title="11:143	Unlike with purely statistical collocational analyses, the framework of a lexical semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems." ></td>
	<td class="line x" title="12:143	I illustrate the approach for the acquisition of lexical information for several lexical classes, and how such techniques can fine tune the lexical structures acquired from an initial seeding of a machine-readable dictionary, i.e. the machine-tractable version of the LDOCE (Wilks et al (1991))." ></td>
	<td class="line x" title="13:143	The aim of our research is to discover what kinds of knowledge can be reliably acquired through the use of these methods, exploiting, as they do, general linguistic knowledge rather than domain knowledge." ></td>
	<td class="line x" title="14:143	In this respect, our program is similar to Zernik (1989) and Zernik and Jacobs (1990), working on extracting verb semantics from corpora using lexical categories." ></td>
	<td class="line x" title="15:143	Our research, however, differs in two respects: first, we employ a more expressive lexical semantics; secondly, our focus is on all major categories in the language, and not just verbs." ></td>
	<td class="line x" title="16:143	This is important since for fulltext information retrieval, information about nominals is paramount, as most queries tend to be expressed as conjunctions of nouns." ></td>
	<td class="line x" title="17:143	From a theoretical perspective, I believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected, relative to that of verbs (cf.Pustejovsky and Anick (1988), Bogutaev and Pustejovsky (1990))." ></td>
	<td class="line x" title="19:143	Therefore, where others present ambiguity and metonymy as a potential obstacle to effective corpus analysis, we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus." ></td>
	<td class="line x" title="20:143	To demonstrate these points, I describe experiments performed within the DIDEROT Tipster Extraction project (of Brandeis University and New Mexico State University), over a corpus of joint venture articles." ></td>
	<td class="line x" title="21:143	2." ></td>
	<td class="line x" title="22:143	Projecting Syntactic Behavior from Deep Semantic Types The purpose of the research is to experiment with automatic acquisition of semantic tags for words in a sublanguage, tags which are well beyond that available from the seeding of MRDs." ></td>
	<td class="line x" title="23:143	The identification of semantic tags for a word associated with particular lexical forms (i.e. semantic collocations) can be represented as that part of the lexical structure of a word called the projective conclusion space (Pustejovsky (1991)))." ></td>
	<td class="line x" title="24:143	For this work, we will need to define several semantic notions." ></td>
	<td class="line x" title="25:143	These include: type coercion, where a lexical item requires a specific type specification for its argument, and the argument is able to change type accordingly --this explains the behavior of logical metonymy and the syntactic variation seen in complements to verbs and nominals; cospecification, a semantic tagging of what collocationM patterns the lexical item may enter into; and contextual opacity/transparency, which characterizes of a 243 word just how it is used in particular contexts." ></td>
	<td class="line x" title="26:143	Formally, we will identify this property with specific cospecification values for the lexical item (cf.Pustejovsky (forthcoming))." ></td>
	<td class="line x" title="28:143	Metonymy, in this view, can be seen as a case of the 'licensed violation' of selectional restrictions." ></td>
	<td class="line x" title="29:143	For example, while the verb announce selects for a human subject, sentences like The Dow Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr. Dow arr." ></td>
	<td class="line x" title="30:143	announced third quarter losses for Dow Corp, but they are the preferred form in the corpora being examined (i.e. the ACL-DCI WSJ and TIPSTER Corpora)." ></td>
	<td class="line x" title="31:143	This is an example of subject type coercion, where the semantics for Dow Corp. as a company must specify that there is a human typically associated with such official pronouncements (Bergler (forthcoming))." ></td>
	<td class="line x" title="32:143	2.1." ></td>
	<td class="line x" title="33:143	Coercive Environments in Corpora Another example of type coercion is that seen in the complements of verbs such as begin, enjoy, finish, etc. That is, in sentences such as 'John began the book', the normal complement expected is an action or event of some sort, most often expressed by a gerundive or infinitival phrase: 'John began reading the book', 'John began to read the book'." ></td>
	<td class="line x" title="34:143	In Pustejovsky (1991) it is argued that in such cases, the verb need not have multiple subcategorizations, but only one deep semantic type, in this case, an event." ></td>
	<td class="line x" title="35:143	Thus, the verb 'coerces' its complement (e.g. 'the book') into an event related to that object." ></td>
	<td class="line x" title="36:143	Such information can be represented by means of a representational schema called qualia structure, which, among other things, specifies the relations associated with objects." ></td>
	<td class="line x" title="37:143	In related work being carried out with Mats Rooth of ATT, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora." ></td>
	<td class="line x" title="38:143	Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be." ></td>
	<td class="line oc" title="39:143	Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), below are some examples of the most frequent V-O pairs from the AP corpus." ></td>
	<td class="line x" title="40:143	Counts for 'objects' of begin/V: 205 begin/V career/O 176 begin/V day/O 159 begin/V work/O 140 begin/V talk/O 120 begin/V campaign/O 113 begin/V investigation/O 106 begin/V process/O 92 begin/V program/O 8S begin/V operation/O 86 begin/V negotiation/O 66 begin/V strike/O 64 begin/V production/O 59 begin/V meeting/O 89 begin/V term/O 50 begin/V visit/O 45 begin/V test/O 39 begin/V construction/O 31 begin/V debate/O 29 begin/V trial/O Corpus studies confirm similar results for 'weakly intensional contexts' (Pustejovsky (1991)) such as the complement of coercive verbs such as veto." ></td>
	<td class="line x" title="41:143	These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of 'the proposal to', thereby clothing the complement within an intensional context." ></td>
	<td class="line x" title="42:143	The examples below with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; secondly, that almost anything can be vetoed, but that the most frequently occurring objects are closest to the type selected by the verb." ></td>
	<td class="line x" title="43:143	303 veto/V bi11/O 84 veto/V legislation/O 58 veto/V measure/O 36 veto/V resolution/O 21 veto/V law/O 14 veto/V item/O 12 veto/V decision/O 9 veto/V proposal/O 9 veto/V plan/O 7 veto/V package/O 6 veto/V increase/O 5 veto/V sanction/O 6 veto/V penalty/O 4 veto/V notice/O 4 veto/V idea/O 4 veto/V appropriation/O 4 veto/V mission/O 4 veto/V attempt/O 3 veto/V search/O 3 veto/V cut/O 3 veto/V deal/O I veto/V expedition/O What these data 'show is that the highest count complement types match the type required by the verb; namely, that one vetoes a bill or proposal to do something, not 244 the thing itself." ></td>
	<td class="line x" title="44:143	These nouns can therefore be used with some predictive certainty for inducing the semantic type in coercive environments such as 'veto the expedition'." ></td>
	<td class="line x" title="45:143	This work is still preliminary, however, and requires further examination (Pustejovsky and Rooth (in preparation))." ></td>
	<td class="line x" title="46:143	3." ></td>
	<td class="line x" title="47:143	Implications for Natural Language Processing The framework proposed here is attractive for NLP, for at least two reasons." ></td>
	<td class="line x" title="48:143	First, it can be formalized, and thus make the basis for a computational procedure for word interpretation in context." ></td>
	<td class="line x" title="49:143	Second, it does not require the notion of exhaustive enumeration of all the different ways in which a word can behave, in particular in collocations with other words." ></td>
	<td class="line x" title="50:143	Consequently, the framework can naturally cope with the 'creative' use of language; that is, the open-ended nature of word combinations and their associated meanings." ></td>
	<td class="line x" title="51:143	The method of fine-grained characterization of lexical entries, as proposed here, effectively allows us to conflate different word senses (in the traditional meaning of this term) into a single meta-entry, thereby offering great potential not only for systematically encoding regularities of word behavior dependent on context, but also for greatly reducing the size of the lexicon." ></td>
	<td class="line x" title="52:143	Following Pustejovsky and Anick (1988), we call such meta-entries lexical conceptuM paradigms (LCPs)." ></td>
	<td class="line x" title="53:143	The theoretical claim here is that such a characterization constrains what a possible word meaning can be, through the mechanism of logically well-formed semantic expressions." ></td>
	<td class="line x" title="54:143	The expressive power of a KR formalism can then be viewed as simply a tool which gives substance to this claim." ></td>
	<td class="line x" title="55:143	The notion of a meta-entry turns out to be very useful for capturing the systematic ambiguities which are so pervasive throughout language." ></td>
	<td class="line x" title="56:143	Among the alternations captured by LCPs are the following (see Pustejovsky (forthcoming) and Levin (1989)): 1." ></td>
	<td class="line x" title="57:143	Count/Mass alternations; e.g. sheep." ></td>
	<td class="line x" title="58:143	2." ></td>
	<td class="line x" title="59:143	Container/Containee alternations; e.g. bottle." ></td>
	<td class="line x" title="60:143	3." ></td>
	<td class="line x" title="61:143	Figure/Ground Reversals; e.g. door, window." ></td>
	<td class="line x" title="62:143	4." ></td>
	<td class="line x" title="63:143	Product/Producer diathesis; e.g. newspaper, IBM, Ford." ></td>
	<td class="line x" title="64:143	For example, an apparently unambiguous noun such as newspaper can appear in many semantically distinct contexts." ></td>
	<td class="line x" title="65:143	1." ></td>
	<td class="line x" title="66:143	The coffee cup is on top of the newspaper." ></td>
	<td class="line x" title="67:143	2." ></td>
	<td class="line x" title="68:143	The article is in the newspaper." ></td>
	<td class="line x" title="69:143	3." ></td>
	<td class="line x" title="70:143	The newspaper attacked the senator from Massachusetts." ></td>
	<td class="line x" title="71:143	4." ></td>
	<td class="line x" title="72:143	The newspaper is hoping to fire its editor next month." ></td>
	<td class="line x" title="73:143	This noun falls into a particular specialization of the Product/Producer paradigm, where the noun can logically denote either the organization or the product produced by the organization." ></td>
	<td class="line x" title="74:143	This is another example of logical polysemy and is represented in the lexical structure for newspaper explicitly (Pustejovsky (1991))." ></td>
	<td class="line x" title="75:143	Another class of logically polysemous nominals is a specialization of the process/result nominals such as merger, joint venture, consolidation, etc. Examples of how these nominals pattern syntactically in text are given below: 1." ></td>
	<td class="line x" title="76:143	Trustcorp Inc. will become Society Bank 8J Trust when its merger is completed with Society Corp. of Cleveland, the bank said." ></td>
	<td class="line x" title="77:143	2." ></td>
	<td class="line x" title="78:143	Shareholders must approve the merger at general meetings of the two companies in late November." ></td>
	<td class="line x" title="79:143	3." ></td>
	<td class="line x" title="80:143	But Mr. Rey brought about a merger in the next few years between the country's major producers." ></td>
	<td class="line x" title="81:143	4." ></td>
	<td class="line x" title="82:143	A pharmaceutical joint venture of Johnson ~4 Johnson and Merck agreed in principle to buy the U.S. over-the-counter drug business of ICI Americas for over $450 million." ></td>
	<td class="line x" title="83:143	5." ></td>
	<td class="line x" title="84:143	The four-year-old business is turning a small profit and the entrepreneurs are about to sign a joint venture agreement with a Moscow cooperative to export the yarn to the Soviet Union." ></td>
	<td class="line x" title="85:143	Because of their semantic type, these nominals enter into an LCP which generates a set of structural templates predicted for that noun in the language." ></td>
	<td class="line x" title="86:143	For example, the LCP in this case is the union concept, and has the following lexical structure associated with it: 5." ></td>
	<td class="line x" title="87:143	Plant/Food alternations; e.g. fig, apple." ></td>
	<td class="line x" title="88:143	6." ></td>
	<td class="line x" title="89:143	Process/Result diathesis; e.g. ezamination, combination." ></td>
	<td class="line x" title="90:143	7." ></td>
	<td class="line x" title="91:143	Place/People diathesis; e.g. city, New York." ></td>
	<td class="line x" title="92:143	LCP: type: union \[ Const: >2x:entity(x) \] \[ Form: exist(ly) \[entity(y)\] \] \[ Agent: type:event  join(x) \] \[ Telic: nil \] 245 This states that a union is an event which brings about one entity from two or more, and comes about by a joining ewmt." ></td>
	<td class="line x" title="93:143	The lexical structure for the nominal merger is inherited from this paradigm." ></td>
	<td class="line x" title="94:143	=erger(*x*) \[ Const: ({w}>2) \[compamy(w) or firm(w)\] \] \[ Form: exists(y) \[company(y)\] \] \[ Agent: event(*x*): join(*x*,{~}) \] \[ Telie: contextual\] It is interesting to note that all synonyms for this word (or, alternatively, viewed as clustered under this concept) will share in the same LCP behavior: e.g. merging, unification, coalition, combination, consolidation, etc. With this LCP there are associated syntactic realization patterns for how the word and its arguments are realized in text." ></td>
	<td class="line x" title="95:143	Such a paradigm is a very generic, domain independent set of schemas, which is a significant point for multi-domain and multi-task NLP applications." ></td>
	<td class="line x" title="96:143	For the particular LCP of union, the syntactic schemas include the following: LCP schemas: \[where N=UNION; X=argl; Y=arg2\] N of X and Y X's N with Y Y's N with X N between X and Y N of Z (Z=X+Y) N between Z EXAMPLE: merger of x and y x's merger with y y's merger with x merger between x and y merger of the two companies merger between two companies There are several things to note here." ></td>
	<td class="line x" title="97:143	First, such paradigmatic behavior is extremely regular for nouns in a language, and as a result, the members of such paradigms can be found using knowledge acquisition techniques from large corpora (cf.Anick and Pustejovsky (1990) for one such algorithm)." ></td>
	<td class="line x" title="99:143	Secondly, because these are very common nominal patterns for nouns such as merger, it is significant when the noun appears without all arguments explicitly expressed." ></td>
	<td class="line x" title="100:143	For example, in (5) below, presuppositions from the lexical structure combine with discourse clues in the form of definite reference in the noun phrase (the merger) to suggest that the other partner in the merger was mentioned previously in the text." ></td>
	<td class="line x" title="101:143	5." ></td>
	<td class="line x" title="102:143	Florida National said yesterday that it remains committed to the merger." ></td>
	<td class="line x" title="103:143	Similarly powerful inferences can be made from an indefinite nominal when introduced into the discourse as in (6)." ></td>
	<td class="line x" title="104:143	Here, there is a strong presupposition that both partners in the merger are mentioned someplace in the immediately local context, e.g. as a coordinate subject, since the NP is a newly mentioned entity." ></td>
	<td class="line x" title="105:143	6." ></td>
	<td class="line x" title="106:143	Orkem and Coates said last Wednesday that the two were considering a merger, through Orkem's British subsidiary, Orkem Coatings U.K. Lid." ></td>
	<td class="line x" title="107:143	Thus, the lexical structures provide a rich set of schemas for argument mapping and semantic inferencing, as well as directed presuppositions for discontinuous semantic relations." ></td>
	<td class="line x" title="108:143	One final and important note about lexical structures and paradigmatic behavior." ></td>
	<td class="line x" title="109:143	The seed information for these structures is largely derivable from machinereadable dictionaries." ></td>
	<td class="line x" title="110:143	For example, a dictionary definition for merger (from the Longman Dictionary of Contemporary English is 'the joining of 2 or more companies or firms' with subject code FINANCE." ></td>
	<td class="line x" title="111:143	This makes the task of automatic construction of a robust lexicon for NLP applications a very realizable goal (cf.Boguraev (1991) and Wilks et ai." ></td>
	<td class="line x" title="113:143	(1991))." ></td>
	<td class="line x" title="114:143	4." ></td>
	<td class="line x" title="115:143	Induction of Semantic Relations from Syntactic Forms From discussion in the previous section, it should be clear that such paradigmatic information would be helpful if available." ></td>
	<td class="line x" title="116:143	In this section, we present preliminary results indicating the feasability of learning LCPs from corpora, both tagged and untagged." ></td>
	<td class="line x" title="117:143	Imagine being able to take the V-O pairs such as those given in section 2.1, and then applying semantic tags to the verbs which are appropriate to the role they play for that object (i.e. induction of the qualia roles for that noun)." ></td>
	<td class="line x" title="118:143	This is in fact the type of experiment reported on in Anick and Pustejovsky (1990)." ></td>
	<td class="line x" title="119:143	Here we apply a similar technique to a much larger corpus, in order to induce the agentive role for nouns." ></td>
	<td class="line x" title="120:143	That is, the semantic predicate associated with bringing about the object." ></td>
	<td class="line x" title="121:143	In this example we look at the behavior of noun phrases 246 and the prepositional phrases that follow them." ></td>
	<td class="line x" title="122:143	In particular, we look at the co-occurrence of nominals with between, with, and to." ></td>
	<td class="line x" title="123:143	Table 1 shows results of the conflating verb/noun plus preposition patterns." ></td>
	<td class="line x" title="124:143	The percentage shown indicates the ratio of the particular collocation to the key word." ></td>
	<td class="line x" title="125:143	Mutual information (MI) statistics for the two words in collocation are also shown." ></td>
	<td class="line x" title="126:143	What these results indicate is that induction of semantic type from conflating syntactic patterns is possible." ></td>
	<td class="line x" title="127:143	Based on the semantic types for these prepositions, the syntactic evidence suggests that there is a symmetric relation between the arguments in the following two patterns: a. Z with y = Ax3Rz, y\[Rz(x, y) A Rz(y, x)\] b. Z between x and y = 3Rz, x, y\[Rz(x, y) ^ Rz(y, x)\] We then take these results and, for those nouns where the association ratios for N with and N between are similar, we pair them with the set of verbs governing these 'NP PP' combinations in corpus, effectively partitioning the original V-O set into \[+agentive\] predicates and \[-agentive\] predicates." ></td>
	<td class="line x" title="128:143	If our hypothesis is correct, we expect that verbs governing nominals collocated with a with-phrase will be mostly those predicates referring to the agentive quale of the nominal." ></td>
	<td class="line x" title="129:143	This is because the with-phrase is unsaturated as a predicate, and acts to identify the agent of the verb as its argument." ></td>
	<td class="line x" title="130:143	This is confirmed by our data, shown below." ></td>
	<td class="line x" title="131:143	Verb-Object Pairs with Prep = to 19 form/V venture/O 3 announce/V venture/O 3 enter/V venture/O 2 discuss/V venture/O 1 be/V venture/O 1 abandon/V venture/O I begin/V venture/O I omplete/V venture/O I negotiate/V venture/O 1 start/V venture/O 1 expect/V venture/O Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already." ></td>
	<td class="line x" title="132:143	Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/Y venture/0." ></td>
	<td class="line x" title="133:143	Thus, week semantic types can be induced on the basis of syntactic behavior." ></td>
	<td class="line x" title="134:143	In Pustejovsky et al (1991), we discuss how this general technique compares to somewhat different but related approaches described in Smadja (1991) and Zernik and Jacobs (1991)." ></td>
	<td class="line x" title="135:143	5." ></td>
	<td class="line x" title="136:143	Conclusion We contend that using lexical semantic methods to guide lexical knowledge acquisition from corpora can yield structured thesaurus-like information in a form amenable for use within information retrieval applications." ></td>
	<td class="line x" title="137:143	The work reported here illustrates the applicability of this approach for several important classes of nominals." ></td>
	<td class="line x" title="138:143	Future work includes refining the discovery procedures to reduce misses and false alarms and extending the coverage of the lexical semantics component to allow the testing of such techniques on a greater range of terms." ></td>
	<td class="line x" title="139:143	Finally, we are applying the results of the analysis within the context of data extraction for IR, to test their effectiveness as indexing and retrieval aids." ></td>
	<td class="line x" title="140:143	Much of what we have outlined is still programmatic, but we believe that the approach to extracting information from corpora making use of lexical semantic information is a fruitful one and an area definitely worth exploring." ></td>
	<td class="line x" title="141:143	Acknowledgement This research was supported by DARPA contract MDAg04-91-C-9328." ></td>
	<td class="line x" title="142:143	I would like to thank Scott Waterman for his assistance in preparing the statistics used here." ></td>
	<td class="line x" title="143:143	I would also like to thank Scott Waterman, Federica Busa, Peter Anick, and Sabine Bergler for useful discussion." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J92-1001
Using Multiple Knowledge Sources For Word Sense Discrimination
McRoy, Susan W.;"></td>
	<td class="line x" title="1:480	Using Multiple Knowledge Sources for Word Sense Discrimination Susan W. McRoy  Artificial Intelligence Program GE Research and Development Center This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences." ></td>
	<td class="line x" title="2:480	To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions." ></td>
	<td class="line x" title="3:480	However, current approaches make use of only small subsets of this information." ></td>
	<td class="line x" title="4:480	Here we will describe how to use the whole range of information." ></td>
	<td class="line x" title="5:480	Our discussion will include how the preference cues relate to general lexical and conceptual knowledge and to more specialized knowledge of collocations and contexts." ></td>
	<td class="line x" title="6:480	We will describe a method of combining cues on the basis of their individual specificity, rather than a fixed ranking among cue-types." ></td>
	<td class="line x" title="7:480	We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences." ></td>
	<td class="line x" title="8:480	1." ></td>
	<td class="line x" title="9:480	Introduction Many problems in applied natural language processing -including information retrieval, database generation from text, and machine translation -hinge on relating words to other words that are similar in meaning." ></td>
	<td class="line x" title="10:480	Current approaches to these applications are often word-based -that is, they treat words in the input as strings, mapping them directly to other words." ></td>
	<td class="line x" title="11:480	However, the fact that many words have multiple senses and different words often have similar meanings limits the accuracy of such systems." ></td>
	<td class="line x" title="12:480	An alternative is to use a knowledge representation, or interlingua, to reflect text content, thereby separating text representation from the individual words." ></td>
	<td class="line x" title="13:480	These approaches can, in principle, be more accurate than word-based approaches, but have not been sufficiently robust to perform any practical text processing task." ></td>
	<td class="line x" title="14:480	Their lack of robustness is generally due to the difficulty in building knowledge bases that are sufficient for broad-scale processing." ></td>
	<td class="line x" title="15:480	But a synthesis is possible." ></td>
	<td class="line x" title="16:480	Applications can achieve greater accuracy by working at the level of word senses instead of word strings." ></td>
	<td class="line x" title="17:480	That is, they would operate on text in which each word has been tagged with its sense." ></td>
	<td class="line x" title="18:480	Robustness need not be sacrificed, however, because this tagging does not require a full-blown semantic analysis." ></td>
	<td class="line x" title="19:480	Demonstrating this claim is one of the goals of this paper." ></td>
	<td class="line x" title="20:480	Here is an example of the level of analysis a sense tagger would provide to an application program." ></td>
	<td class="line x" title="21:480	Suppose that the input is (1): Correspondence should be addressed to the author at Department of Computer Science, University of Toronto, Toronto, Canada M5S lA4 or mcroy@ai.toronto.edu." ></td>
	<td class="line x" title="22:480	(~) 1992 Association for Computational Linguistics Computational Linguistics Volume 18, Number 1 Example 1 The agreement reached by the state and the EPA provides for the safe storage of the waste." ></td>
	<td class="line x" title="23:480	The analysis would provide an application with the following information." ></td>
	<td class="line x" title="24:480	 agreement refers to a state resulting from concurrence, rather than an act, object, or state of being equivalent." ></td>
	<td class="line x" title="25:480	 reach is intended to mean 'achieve,' rather than 'extend an arm'." ></td>
	<td class="line x" title="26:480	 state refers to a government body, rather than an abstract state of existence." ></td>
	<td class="line x" title="27:480	safe in this context is an adjective corresponding to 'secure,' rather than a noun corresponding to a container for valuables." ></td>
	<td class="line x" title="28:480	The EPA and the state were co-agents in completing some agreement that is instrumental in supplying a secure place to keep garbage, rather than there was some equivalence that extended its arm around the state while the EPA was busy filling safes with trash." ></td>
	<td class="line x" title="29:480	Preliminary e;cidence suggests that having access to a sense tagging of the text improves the performance of information retrieval systems (Krovetz 1989)." ></td>
	<td class="line x" title="30:480	The primary goal of this paper, then, is to describe in detail methods and knowledge that will enable a language analyzer to tag each word with its sense." ></td>
	<td class="line x" title="31:480	To demonstrate that the approach is sufficiently robust for practical tasks, the article will also discuss the incorporation of the approach into an existing system, TRUMP (Jacobs 1986, 1987, 1989), and the application of it to unrestricted texts." ></td>
	<td class="line x" title="32:480	The principles that make up the approach are completely general, however, and not just specific to TRUMP." ></td>
	<td class="line x" title="33:480	An analyzer whose tasks include word-sense tagging must be able to take an input text, determine the concept that each word or phrase denotes, and identify the role relationships that link these concepts." ></td>
	<td class="line x" title="34:480	Because determining this information accurately is knowledge-intensive, the analyzer should be as flexible as possible, requiring a minimum amount of customization for different domains." ></td>
	<td class="line x" title="35:480	One way to gain such flexibility is give the system enough generic information about word senses and semantic relations so that it will be able to handle texts spanning more than a single domain." ></td>
	<td class="line x" title="36:480	While having an extensive grammar and lexicon is essential for any system's domain independence, this increased flexibility also introduces degrees of ambiguity not frequently addressed by current NLP work." ></td>
	<td class="line x" title="37:480	Typically, the system will have to choose from several senses for each word." ></td>
	<td class="line x" title="38:480	For example, we found that TRUMP's base of nearly 10,000 root senses and 10,000 derivations provides an average of approximately four senses for each word of a sentence taken from the Wall Street Journal." ></td>
	<td class="line x" title="39:480	The potential for combinatoric explosion resulting from such ambiguity makes it critical to resolve ambiguities quickly and reliably." ></td>
	<td class="line x" title="40:480	It is unrealistic to assume that word sense discrimination can be left until parsing is complete, as suggested, for example, by Dahlgren, McDowell, and Stabler (1989) and Janssen (1990)." ></td>
	<td class="line x" title="41:480	No simple recipe can resolve the general problem of lexical ambiguity." ></td>
	<td class="line x" title="42:480	Although semantic context and selectional restrictions provide good cues to disambiguation, they are neither reliable enough, nor available quickly enough, to be used alone." ></td>
	<td class="line x" title="43:480	The approach to disambiguation that we will take below combines many different, strong Susan W. McRoy Using Multiple Knowledge Sources sources of information: syntactic tags, word frequencies, collocations, semantic context (clusters), selectional restrictions, and syntactic cues." ></td>
	<td class="line x" title="44:480	The approach incorporates a number of innovations, including:  a hybridization of several lexicons to help control which senses are considered: a static generic lexicon a lexicon linked to collocations w a lexicon linked to concretions (i.e. , specializations of abstract senses of words) m lexicons linked to specialized conceptual domains;  a separate processing phase, prior to parsing, that eliminates some ambiguities and identifies baseline semantic preferences;  a preference combination mechanism, applied during parsing and semantic interpretation, that uses dynamic measures of strength based on specificity, instead of a fixed, ordered set of rules." ></td>
	<td class="line x" title="45:480	Although improvements to our system are ongoing, it already interprets arbitrary text and makes coarse word sense selections reasonably well." ></td>
	<td class="line x" title="46:480	(Section 6 will give some quantitative assessments)." ></td>
	<td class="line x" title="47:480	No other system, to our knowledge, has been as successful." ></td>
	<td class="line x" title="48:480	We Will now review word sense discrimination and the determination of role relations." ></td>
	<td class="line x" title="49:480	In Section 3, we discuss some sources of knowledge relevant to solving these problems, and, in Section 4, how TRUMP's semantic interpreter uses this knowledge to identify sense preferences." ></td>
	<td class="line x" title="50:480	Section 5 describes how it combines the preference information to select senses." ></td>
	<td class="line x" title="51:480	Afterward, we will discuss the results of our methods and the avenues for improvement that remain." ></td>
	<td class="line x" title="52:480	2." ></td>
	<td class="line x" title="53:480	Cues to Word Sense Discrimination The problem of word sense discrimination is to choose, for a particular word in a particular context, which of its possible senses is the 'correct' one for the context." ></td>
	<td class="line x" title="54:480	Information about senses can come from a wide variety of sources:  the analysis of each word into its root and affixes, that is, its morphology;  the contextually appropriate part or parts of speech of each word, that is, its syntactic tag or tags;  for each sense of the word, whether the sense is preferred or deprecated -either in general, because of its frequency, or in the context, because it is the expected one for a domain;  whether a word is part of a common expression, or collocation, such as a nominal compound (e.g. , soda cracker) or a predicative relation (e.g. , take action);  whether a word sense is supported by the semantic context -for example, by its association with other senses in the context sharing a semantic category, a situation, or a topic;  whether the input satisfies the expectations created by syntactic cues (e.g. , some senses only take arguments of a particular syntactic type); 3 Computational Linguistics Volume 18, Number 1 whether it satisfies role-related expectations (i.e. , expectations regarding the semantic relations that link syntactically attached objects); whether the input refers to something already active in the discourse focus." ></td>
	<td class="line x" title="55:480	Of course, not all these cues will be equally useful." ></td>
	<td class="line x" title="56:480	We have found that, in general, the most important sources of information for word sense discrimination are syntactic tags, morphology, collocations, and word associations." ></td>
	<td class="line x" title="57:480	Role-related expectations are also important, but to a slightly lesser degree." ></td>
	<td class="line x" title="58:480	Syntactic tags are very important, because knowing the intended part of speech is often enough to identify the correct sense." ></td>
	<td class="line x" title="59:480	For example, according to our lexicon, when safe is used as an adjective (as in Example 1), it always denotes the sense related to security, whereas safe used as a noun always denotes a type of container for storing valuables." ></td>
	<td class="line x" title="60:480	Morphology is also a strong cue to discrimination because certain sense-affix combinations are preferred, deprecated, or forbidden." ></td>
	<td class="line x" title="61:480	Consider the word agreement." ></td>
	<td class="line x" title="62:480	The verb agree can mean either 'concur,' 'benefit,' or 'be equivalent' and, in general, adding the affix -ment to a verb creates a noun corresponding either to an act, or to its result, its object, or its associated state." ></td>
	<td class="line x" title="63:480	However, of the twelve possible combinations of root sense and affix sense, in practice only four occur: agreement can refer only to the act, object, or result in the case of the 'concur' sense of agree or the state in the case of the 'equivalence' sense of agree." ></td>
	<td class="line x" title="64:480	Furthermore, the last of these combinations is deprecated." ></td>
	<td class="line x" title="65:480	Collocations and word associations are also important sources of information because they are usually 'dead giveaways,' that is, they make immediate and obvious sense selections." ></td>
	<td class="line x" title="66:480	For example, when paired with increase, the preposition in clearly denotes a patient rather than a temporal or spatial location, or a direction." ></td>
	<td class="line x" title="67:480	Word associations such as bank~money similarly create a bias for the related senses." ></td>
	<td class="line x" title="68:480	Despite their apparent strength, however, the preferences created by these cues are not absolute, as other cues may defeat them." ></td>
	<td class="line x" title="69:480	For example, although normally the collocation wait on means 'serve' (Mary waited on John), the failure of a role-related expectation, such as that the BENEFICIARY be animate, can override this preference (Mary waited on the steps)." ></td>
	<td class="line x" title="70:480	Thus, collocations and word associations are strong sources of information that an understander must weigh against other cues, and not just treat as rules for sense-filtering (as in Hirst 1987 or Dahlgren, McDowell, and Stabler 1989)." ></td>
	<td class="line x" title="71:480	The selection of a role relationship can both influence and be influenced by the selection of word senses, because preferences partially constrain the various combinations of a role, its holder, and the filler." ></td>
	<td class="line x" title="72:480	For example, the preposition from prefers referring to the SOURCE role; transfers, such as give, prefer to have a DESTINATION role; and instances of colors, such as red, prefer to fill a COLOR role." ></td>
	<td class="line x" title="73:480	Approaches based on the word disambiguation model tend to apply constraint satisfaction techniques to combine these role preferences (Hirst 1987)." ></td>
	<td class="line x" title="74:480	Preferences based on role-related expectations are often only a weak cue because they are primarily for verbs and not normally very restrictive." ></td>
	<td class="line x" title="75:480	Although generally a weak cue, role-related preferences are quite valuable for the disambiguation of prepositions." ></td>
	<td class="line x" title="76:480	In our view, prepositions should be treated essentially the same as other words in the lexicon." ></td>
	<td class="line x" title="77:480	The meaning of a preposition either names a relation directly, as one of its core senses (Hirst \[1987\] also allows this), or indirectly, as a specialized sense triggered, for example, by a collocation or concretion." ></td>
	<td class="line x" title="78:480	Because the meaning of a preposition actually names a relation, relation-based cues are a good source of information for disambiguating them." ></td>
	<td class="line x" title="79:480	(References to objects in the discourse 4 Susan W. McRoy Using Multiple Knowledge Sources focus can also be a strong cue for disambiguating prepositions, but this cue appears fairly infrequently \[Whittemore, Ferrara, and Brunner 1990\])." ></td>
	<td class="line x" title="80:480	The problem of determining role relationships entangles word sense discrimination with the problem of syntactic attachment." ></td>
	<td class="line x" title="81:480	The attachment problem is a direct result of the ambiguity in determining whether a concept is related to an adjacent object, or to some enveloping structure that incorporates the adjacent object." ></td>
	<td class="line x" title="82:480	Most proposed solutions to this problem specify a fixed set of ordered rules that a system applies until a unique, satisfactory attachment is found (Fodor and Frazier 1980; Wilks, Huang, and Fass 1985; Shieber 1983; Hirst 1987; Dahlgren, McDowell, and Stabler 1989)." ></td>
	<td class="line x" title="83:480	Such rules can be either syntactic, semantic, or pragmatic." ></td>
	<td class="line x" title="84:480	Syntactic rules attempt to solve the attachment problem independent of the sense discrimination problem." ></td>
	<td class="line x" title="85:480	For example, a rule for Right Association (also known as Late Closure) says to prefer attaching a new word to the lowest nonterminal node on the rightmost branch of the current structure (i.e. , in the same structure as the last word processed) (Kimball 1973)." ></td>
	<td class="line x" title="86:480	Semantic rules, by contrast, intertwine the problems of discrimination and attachment; one must examine all combinations of senses and attachments to locate the semantically best one." ></td>
	<td class="line x" title="87:480	Such rules normally also collapse the attachment problem into the conceptual role filling problem." ></td>
	<td class="line x" title="88:480	For example, a lexical preference rule specifies that the preference for a particular attachment depends on how strongly or weakly the verb of the clause prefers its possible arguments (Fodor 1978; Ford, Bresnan, and Kaplan 1982)." ></td>
	<td class="line x" title="89:480	Pragmatic rules also intermingle sense discrimination and attachment, but consider the context of the utterance." ></td>
	<td class="line x" title="90:480	For example, one suggested rule says to prefer to build structures describing objects just mentioned (Crain and Steedman 1985; Altmann and Steedman 1988)." ></td>
	<td class="line x" title="91:480	The accuracy of systems with fixed-order rules is limited by the fact that it is not always possible to strictly order a set of rules independent of the context." ></td>
	<td class="line x" title="92:480	For example, Dahlgren, McDowell, and Stabler (1989) propose the rule 'If the object of the preposition is an expression of time, then S-attach the PP' to explain the preference for assuming that 'in the afternoon' modifies adjourn in Example 2: Example 2 The judge adjourned the hearing in the afternoon." ></td>
	<td class="line x" title="93:480	Although they admit this rule would fail for a sentence like John described the meeting on January 20th, where the NP has a lexical preference for a time modifier, lexical preferences are not always the determining factor either." ></td>
	<td class="line x" title="94:480	The existence of a conceptually similar object in the context (such as 'the morning trial') can also create an expectation for the grouping 'hearing in the afternoon,' as in Example 3 below." ></td>
	<td class="line x" title="95:480	Example 3 The judge had to leave town for the day." ></td>
	<td class="line x" title="96:480	He found a replacement to take over his morning trial, but couldn't find anyone else that was available." ></td>
	<td class="line x" title="97:480	He called the courthouse and cancelled the hearing in the afternoon." ></td>
	<td class="line x" title="98:480	Moreover, pragmatic effects are not always the determining factor either, leading many people to judge the following sentence as silly (Hirst 1987)." ></td>
	<td class="line x" title="99:480	Example 4 The landlord painted all the walls with cracks (Rayner, Carlson, and Frazier 1983)." ></td>
	<td class="line x" title="100:480	Computational Linguistics Volume 18, Number 1 The presence of different lexical items or different objects in the discourse focus may strengthen or weaken the information provided by an individual rule." ></td>
	<td class="line x" title="101:480	Another possibility we will discuss in Section 5 is to weigh all preference information dynamically (cf.Schubert 1986; McRoy and Hirst 1990)." ></td>
	<td class="line x" title="103:480	The system we will be describing in Section 4 will use many of the cues described above, including syntactic tags, morphology, word associations, and role-related expectations." ></td>
	<td class="line x" title="104:480	But first, we need to discuss the sources of knowledge that enable a system to identify these cues." ></td>
	<td class="line x" title="105:480	3." ></td>
	<td class="line x" title="106:480	Sources of Knowledge To identify preference cues such as morphology, word frequency, collocations, semantic contexts, syntactic expectations, and conceptual relations in unrestricted texts, a system needs a large amount of knowledge in each category." ></td>
	<td class="line x" title="107:480	In most cases, this just means that the understander's lexicon and conceptual hierarchy must include preference information, although processing concerns suggest moving some information out of these structures and into data modules specific to a particular process, such as identifying collocations." ></td>
	<td class="line x" title="108:480	TRUMP obtains the necessary knowledge from a moderately sized lexicon (8,775 unique roots), specifically designed for use in language understanding, and a hierarchy of nearly 1,000 higher-level concepts, overlaid with approximately 40 concept-cluster definitions." ></td>
	<td class="line x" title="109:480	It also uses a library of over 1,400 collocational patterns." ></td>
	<td class="line x" title="110:480	We will consider each in turn." ></td>
	<td class="line x" title="111:480	3.1 The Lexicon Development of TRUMP's current lexicon followed an experiment with a moderatelysized, commercially available lexicon (10,000 unique roots), which demonstrated many substantive problems in applying lexical resources to text processing." ></td>
	<td class="line x" title="112:480	Although the lexicon had good morphological and grammatical coverage, as well as a thesaurus-based semantic representation of word meanings, it lacked reasonable information for discriminating senses." ></td>
	<td class="line x" title="113:480	The current lexicon, although roughly the same size as the earlier one, has been designed to better meet the needs of producing semantic representations of text." ></td>
	<td class="line x" title="114:480	The lexicon features a hierarchy of 1,000 parent concepts for encoding semantic preferences and restrictions, sense-based morphology and subcategorization, a distinction between primary and secondary senses and senses that require particular 'triggers' or appear only in specific contexts, and a broad range of collocational information." ></td>
	<td class="line x" title="115:480	(An alternative would have been to give up discriminating senses that the lexicon does not distinguish; cf.Janssen \[1990\])." ></td>
	<td class="line x" title="117:480	At this time, the lexicon contains about 13,000 senses and 10,000 explicit derivations." ></td>
	<td class="line x" title="118:480	Each lexical entry provides information about the morphological preferences, sense preferences, and syntactic cues associated with a root, its senses, and their possible derivations." ></td>
	<td class="line x" title="119:480	An entry also links words to the conceptual hierarchy by naming the conceptual parent of each sense." ></td>
	<td class="line x" title="120:480	If necessary, an entry can also specify the composition of common phrases, such as collocations, that have the root as their head." ></td>
	<td class="line x" title="121:480	TRUMP's lexicon combines a core lexicon with dynamic lexicons linked to specialized conceptual domains, collocations, and concretions." ></td>
	<td class="line x" title="122:480	The core lexicon contains the generic, or context-independent, senses of each word." ></td>
	<td class="line x" title="123:480	The system considers these senses whenever a word appears in the input." ></td>
	<td class="line x" title="124:480	The dynamic lexicons contain word senses that normally appear only within a particular context; these senses are considered only when that context is active." ></td>
	<td class="line x" title="125:480	This distinction is a product of experience; it is conceivable that a formerly dynamic sense may become static, as when military terms creep into everyday language." ></td>
	<td class="line x" title="126:480	The partitioning of the lexicon into static and 6 Susan W. McRoy Using Multiple Knowledge Sources dynamic components reduces the number of senses the system must consider in situations where the context does not trigger some dynamic sense." ></td>
	<td class="line x" title="127:480	Although the idea of using dynamic lexicons is not new (see Schank and Abelson \[1977\], for example), our approach is much more flexible than previous ones because TRUMP's lexicon does not link all senses to a domain." ></td>
	<td class="line x" title="128:480	As a result, the lexical retrieval mechanism never forces the system to use a sense just because the domain has preselected it." ></td>
	<td class="line x" title="129:480	3.1.1 The Core Lexicon." ></td>
	<td class="line x" title="130:480	The core lexicon, by design, includes only coarse distinctions between word senses." ></td>
	<td class="line x" title="131:480	This means that, for a task such as generating databases from text, task-specific processing or inference must augment the core lexical knowledge, but problems of considering many nuances of meaning or low-frequency senses are avoided." ></td>
	<td class="line x" title="132:480	For example, the financial sense of issue (e.g. , a new security) falls under the same core sense as the latest issue of a magazine." ></td>
	<td class="line x" title="133:480	The 'progeny' and 'exit' senses of issue are omitted from the lexicon." ></td>
	<td class="line x" title="134:480	The idea is to preserve in the core lexicon only the common, coarse distinctions among senses (cf.Frazier and Rayner 1990)." ></td>
	<td class="line x" title="136:480	Figure 1 shows the lexical entries for the word issue." ></td>
	<td class="line x" title="137:480	Each entry has a part of speech, :POS, and a set of core senses, :SENSES." ></td>
	<td class="line x" title="138:480	Each sense has a :TYPE field that indicates *primary* for a preferred (primary) sense and *secondary* for a deprecated (secondary) sense." ></td>
	<td class="line x" title="139:480	The general rule for determining the :TYPE of a sense is that secondary senses are those that the semantic interpreter should not select without specific contextual information, such as the failure of some selectional restriction pertaining to the primary sense." ></td>
	<td class="line x" title="140:480	For example, the word yard can mean an enclosed area, a workplace, or a unit of measure, but in the empty context, the enclosed-area sense is assumed." ></td>
	<td class="line x" title="141:480	This classification makes clear the relative frequency of the senses." ></td>
	<td class="line x" title="142:480	This is in contrast to just listing them in historical order, the approach of many lexicons (such as the Longman Dictionary of Contemporary English \[Procter 1978\]) that have been used in computational applications." ></td>
	<td class="line x" title="143:480	The :PaR field links each word sense to its immediate parent in the semantic hierarchy." ></td>
	<td class="line x" title="144:480	(See Section 3.2)." ></td>
	<td class="line x" title="145:480	The parents and siblings of the two noun senses of issue, which are listed in Figure 2, give an idea of the coverage of the lexicon." ></td>
	<td class="line x" title="146:480	In the figure, word senses are given as a root followed by a sense number; conceptual categories are designated by atoms beginning with c-." ></td>
	<td class="line x" title="147:480	Explicit derivations, such as 'period-ic-al-x,' are indicated by roots followed by endings and additional type specifiers." ></td>
	<td class="line x" title="148:480	These derivative lexical entries do 'double duty' in the lexicon: an application program can use the derivation as well as the semantics of the derivative form." ></td>
	<td class="line x" title="149:480	The :ASS0C field, not currently used in processing, includes the lexicographer's choice of synonym or closely related words for each sense." ></td>
	<td class="line x" title="150:480	The :SYNTAX field encodes syntactic constraints and subcategorizations for each sense." ></td>
	<td class="line x" title="151:480	When senses share constraints (not the case in this example), they can be encoded at the level of the word entry." ></td>
	<td class="line x" title="152:480	When the syntactic constraints (such as io-rec, one-obj, and no-oh j) influence semantic preferences, they are attached to the sense entry." ></td>
	<td class="line x" title="153:480	For example, in this case, issue used as an intransitive verb (no-oh j) would favor 'passive moving' even though it is a secondary sense." ></td>
	<td class="line x" title="154:480	The io-rec subcategorization in the first two senses means indirect object as recipient: the ditransitive form will fill the RECIPIENT role." ></td>
	<td class="line x" title="155:480	The grammatical knowledge base of the system relates these subcategories to semantic roles." ></td>
	<td class="line x" title="156:480	The :G-DERIV and :S-DERIV fields mark morphological derivations." ></td>
	<td class="line x" title="157:480	The former, which is NIL in the case of issue to indicate no derivations, encodes the derivations at the word root level, while the latter encodes them at the sense preference level." ></td>
	<td class="line x" title="158:480	For example, the :S-DERIV constraint allows issuance to derive from either of the first two senses of the verb, with issuer and issuable deriving only from the 'giving' sense." ></td>
	<td class="line x" title="159:480	7 Computational Linguistics Volume 18, Number 1 ( issue :POS noun :SENSES (( issue1 :EXAMPLE (address important issues) :TYPE *primary* :PAR (c-concern) :ASS0C (subject) ) ( issue2 :EXAMPLE (is that the october issue)?" ></td>
	<td class="line x" title="160:480	:TYPE *secondary* :PAR (c-published-document) :ASSOC (edition) ))) ( issue :POS verb :G-DERIV nil :SENSES (( issuel :SYNTAX (one-obj io-rec) :EXAMPLE (the stockroom issues supplies) :TYPE *primary* :PAR (c-giving) :ASS0C (supply) :S-DERIV ((-able adj tr_ability) (-ance noun tr_act) (-er noun tr_actor)) ) ( issue2 :SYNTAX (one-obj io-rec) :EXAMPLE (I issued instructions) :TYPE *primary* :PAR (c-informing) :ASSOC (produce) :S-DERIV ((-ance noun tr_act)) ) ( issue3 :SYNTAX (one-obj no-obj) :EXAMPLE (good smells issue from the cake) :TYPE *secondary* :PAR (c-passive-moving) ))) Figure 1 The lexical entries for issue." ></td>
	<td class="line x" title="161:480	The derivation triples encode the form of each affix, the resulting syntactic category (usually redundant), and the 'semantic transformation' that applies between the core sense and the resulting sense." ></td>
	<td class="line x" title="162:480	For example, the triple (-er noun tr_actor) in the entry for issue says that an issuer plays the ACTOR role of the first sense of the verb issue." ></td>
	<td class="line x" title="163:480	Because derivations often apply to multiple senses and often result in different semantic transformations (for example, the ending -ion can indicate the act of performing some action, the object of the action, or the result of the action), a lexical entry can mark certain interpretations of a morphological derivation as primary or secondary." ></td>
	<td class="line x" title="164:480	Susan W. McRoy Using Multiple Knowledge Sources NOUN_ISSUE1: PARENT CHAIN: c-concern c-mental-obj c-obj c-entity something SIBLINGS (all nouns): regardl realm2 puzzlel province2 premonitionl pityl pet2 parameterl ground3 goodwilll feeling2 enigmal draw2 department2 concernl cause2 carel business3 baby2 apprehend-ion-x NOUN_ISSUE2: PARENT CHAIN: c-published-document c-document c-phys-obj c-obj c-entity something SIBLINGS (all nouns): week-ly-x volumel transcriptl tragedy2 tomel thesaurusl supplement2 strip4 source2 softwarel seriall scripturel romance2 publication2 profile2 period-ic-al-x paperbackl paper3 paper2 pamphletl omnibusl obituaryl novell notice2 month-ly-x memoirl mapl manuall magazinel libraryl journall handbookl guidel grammarl gazettel fictionl feature4 facsimilel epicl encyclopedial dissertationl directoryl digestl dictionaryl copy2 constitute-ion-xl comicl column2 columnl cataloguel calendarl bulletinl brochurel bookl blurbl biographyl bibliographyl biblel atlasl articlel anthologyl Figure 2 The parents and siblings of two senses of issue." ></td>
	<td class="line x" title="165:480	3.1.2 The Dynamic Lexicons." ></td>
	<td class="line x" title="166:480	Unlike the core lexicon, which lists senses active in all situations, the dynamic lexicons contain senses that are active only in a particular context." ></td>
	<td class="line x" title="167:480	Although these senses require triggers, a sense and its trigger may occur just as frequently as a core sense." ></td>
	<td class="line x" title="168:480	Thus, the dynamic-static distinction is orthogonal to the distinction between primary and secondary senses made in the core lexicon." ></td>
	<td class="line x" title="169:480	Currently, TRUMP has lexicons linked to domains, collocations, and concretions." ></td>
	<td class="line x" title="170:480	For example, TRUMP's military lexicon contains a sense of engage that means 'attack'." ></td>
	<td class="line x" title="171:480	However, the system does not consider this sense unless the military domain is active." ></td>
	<td class="line x" title="172:480	Similarly, the collocational lexicon contains senses triggered by well-known patterns of words; for example, the sequence take effect activates a sense of take meaning 'transpire'." ></td>
	<td class="line x" title="173:480	(Section 3.3 discusses collocations and their representation in more detail)." ></td>
	<td class="line x" title="174:480	Concretions activate specializations of the abstract sense of a word when it occurs with an object of a specific type." ></td>
	<td class="line x" title="175:480	For example, in the core lexicon, the verb project has the abstract sense 'transfer'; however, if its object is a sound, the system activates a sense corresponding Computational Linguistics Volume 18, Number 1 to a 'communication event,' as in She projected her voice." ></td>
	<td class="line x" title="176:480	Encoding these specializations in the core lexicon would be problematic, because then a system would be forced to resolve such nuances of meaning even when there was not enough information to do so." ></td>
	<td class="line x" title="177:480	Dynamic lexicons can provide much finer distinctions among senses than the core lexicon, because they do not increase the amount of ambiguity when their triggering context is inactive." ></td>
	<td class="line x" title="178:480	Together, the core and dynamic lexicons provide the information necessary to recognize morphological preferences, sense preferences, and syntactic cues." ></td>
	<td class="line x" title="179:480	They also provide some of the information required to verify and interpret collocations." ></td>
	<td class="line x" title="180:480	Sections 3.2, 3.3, and 3.4, below, describe sources of information that enable a system to recognize role-based preferences, collocations, and the semantic context." ></td>
	<td class="line x" title="181:480	3.2 The Concept Hierarchy The concept hierarchy serves several purposes." ></td>
	<td class="line x" title="182:480	First, it associates word senses that are siblings or otherwise closely related in the hierarchy, thus providing a thesaurus for information retrieval and other tasks (cf.Fox et al. 1988)." ></td>
	<td class="line x" title="184:480	In a sense tagging system, these associations can help determine the semantic context." ></td>
	<td class="line x" title="185:480	Second, it supplies the basic ontology to which domain knowledge can be associated, so that each new domain requires only incremental knowledge engineering." ></td>
	<td class="line x" title="186:480	Third, it allows role-based preferences, wherever possible, to apply to groups of word senses rather than just individual lexical entries." ></td>
	<td class="line x" title="187:480	To see howthe hierarchy's concept definitions establish the basic ontology, consider Figure 3, the definition of the concept c-recording, c-recording is the parent concept for activities involving the storage of information, namely, the following verb senses: book2 cataloguel clock1 compile1 date3 documentl enter3 indexl inputl keyl logl recordl In a concept definition, the : PAR fields link the concept to its immediate parents in the hierarch~ The :ASSOC field links the derived instances of the given concept to their places in the hierarchy." ></td>
	<td class="line x" title="188:480	For example, according to Figure 3, the object form derived (c-ent Figure 3 The conceptual c-recording :DESC (the storing of information) :PAR (c-action) :PAR (c-simple-occurrence :ROLE-PLAY (r-object r-patient)) :ASSOC ((r-object c-information)) :PREF ((r-patient c-information))) definition of c-recording." ></td>
	<td class="line x" title="189:480	(c-ent c-clothing :DESC (cloth materials for wearing) :PAR (c-phys-obj) :RELS ( (*modified-by* (c-fabric-material c-made-of-rel) ) ) ) Figure 4 The conceptual definition of c-clothing." ></td>
	<td class="line x" title="190:480	10 Susan W. McRoy Using Multiple Knowledge Sources (c-ent c-color-qual :DESC (qualities of the color of an entity) :PAR (c-phys-prop-qual) :RELS ((*transform* (c-state c-color-rel)) (*modifier-of* (c-phys-obj c-color-rel)))) Figure 5 The conceptual definition ofc-color-qual." ></td>
	<td class="line x" title="191:480	(c-ent c-made-of-rel :DESC (a relationship between an object and what it is made of) :PAR (c-phys-prop-rel) :PREF ((r-statevalue c-phys-obj) (r-stateholder (or c-phys-obj c-whole))) :RELS ((*held-by* c-phys-obj) )) Figure 6 The conceptual definition of c-made-of-rel." ></td>
	<td class="line x" title="192:480	from enter3 (i.e. , entry) has the parent c-information." ></td>
	<td class="line x" title="193:480	The :ROLE-PLAY fields mark specializations of a parent's roles (or introduce new roles)." ></td>
	<td class="line x" title="194:480	Each :ROLE-PLAY indicates the parent's name for a role along with the concept's specialization of it." ></td>
	<td class="line x" title="195:480	For example, c-recording specializes its inherited OBJECT role as PATIENT." ></td>
	<td class="line x" title="196:480	The : REL8 and : PREF fields identify which combinations of concept, role, and filler an understander should expect (and hence prefer)." ></td>
	<td class="line x" title="197:480	For example, the definition in Figure 4 expresses that fabric materials are common modifiers of clothing (e.g. , wool suit) and fill the clothing's MADE-OF role." ></td>
	<td class="line x" title="198:480	TRUMP's hierarchy also allows the specification of such preferences from the perspective of the filler, where they can be made more general." ></td>
	<td class="line x" title="199:480	For example, although colors are also common modifiers of clothing (e.g. , blue suit), it is better to associate this preference with the filler (c-color-qual) because colors prefer to fill the COLOR role of any physical object." ></td>
	<td class="line x" title="200:480	(Figure 5 shows an encoding of this preference)." ></td>
	<td class="line x" title="201:480	The hierarchy also permits the specification of such preferences from the perspective of the relation underlying a role." ></td>
	<td class="line x" title="202:480	For example, the relation c-made-of in Figure 6 indicates (in its :RELS) that physical objects normally have a MADE-OF role and (in its : PREF) that the role is normally filled by some physical object." ></td>
	<td class="line x" title="203:480	Figure 7 gives a complete account of the use of the :RELS and :PREF fields and how they permit the expression of role-related preferences from any perspective." ></td>
	<td class="line x" title="204:480	3.3 Collocational Patterns Collocation is the relationship among any group of words that tend to co-occur in a predictable configuration." ></td>
	<td class="line x" title="205:480	Although collocations seem to have a semantic basis, many collocations are best recognized by their syntactic form." ></td>
	<td class="line x" title="206:480	Thus, for current purposes, we limit the use of the term 'collocation' to sense preferences that result from these well-defined syntactic constructions} For example, the particle combination pick up 1 Traditionally many of these expressions have been categorized as idioms (see Cowie and Mackin 1975; Cowie, Mackin, and McCraig 1983), but as most are at least partly compositional and can be processed by normal parsing methods, we prefer to use the more general term 'collocation'." ></td>
	<td class="line x" title="207:480	This categorization thus happily encompasses both the obvious idioms and the compositional expressions whose status as idioms is highly debatable." ></td>
	<td class="line x" title="208:480	Our use of the term is thus similar to that of Smadja and McKeown, who partition collocations into open compounds, predicative relations, and idiomatic expressions (Smadja and McKeown 1990)." ></td>
	<td class="line x" title="209:480	11 Computational Linguistics Volume 18, Number 1 Preference Perspective holder filler relation holder NA :RELS ((*modifier-of* filler (holder) (relation))) relation : RELS ( ( *held-by* (holder))) :PREF ((r-stateholder (filler))) :RELS ((*modifled-by* (filler) (relation))) :PREF (((role) (filler)):) NA : PREF ((r-statevalue (holder))) :RELS ((*holder-of* (role))) :PREF (((role) (finer))) :RELS ((*modifier-of* (holder) (relation))) NA Figure 7 The use of :PREF and :RELS." ></td>
	<td class="line x" title="210:480	1." ></td>
	<td class="line x" title="211:480	249 profit take 2." ></td>
	<td class="line x" title="212:480	205 take place 3." ></td>
	<td class="line x" title="213:480	157 take act 4." ></td>
	<td class="line x" title="214:480	113 say take 5." ></td>
	<td class="line x" title="215:480	113 act take 6." ></td>
	<td class="line x" title="216:480	99 take advantage 7." ></td>
	<td class="line x" title="217:480	94 take effect 8." ></td>
	<td class="line x" title="218:480	88 take profit 9." ></td>
	<td class="line x" title="219:480	77 take step 10." ></td>
	<td class="line x" title="220:480	76 take account Figure 8 The top ten co-occurences with take." ></td>
	<td class="line x" title="221:480	and the verb-complement combination make the team are both collocation-inducing expressions." ></td>
	<td class="line x" title="222:480	Excluded from this classification are unstructured associations among senses that establish the general semantic context, for example, courtroom~defendant." ></td>
	<td class="line x" title="223:480	(We will discuss this type of association in the next section)." ></td>
	<td class="line x" title="224:480	Collocations often introduce dynamic word senses, i.e., ones that behave compositionally, but occur only in the context of the expression, making it inappropriate for the system to consider them outside that context." ></td>
	<td class="line x" title="225:480	For example, the collocation hang from triggers a sense of from that marks an INSTRUMENT." ></td>
	<td class="line x" title="226:480	In other cases, a collocation simply creates preferences for selected core senses, as in the pairing of the 'opportunity' sense of break with the 'cause-to-have' sense of give in give her a break." ></td>
	<td class="line x" title="227:480	There is also a class of collocations that introduce a noncompositional sense for the entire expression, for example, the collocation take place invokes a sense 'transpire'." ></td>
	<td class="line x" title="228:480	To recognize collocations during preprocessing, TRUMP uses a set of patterns, each of which lists the root words or syntactic categories that make up the collocation." ></td>
	<td class="line x" title="229:480	For example, the pattern (TAKE (A) (ADd) BATH) matches the clauses take a hot bath and takes hot baths." ></td>
	<td class="line x" title="230:480	In a pattern, parentheses indicate optionality; the system encodes the repeatability of a category, such as adjectives, procedurally." ></td>
	<td class="line x" title="231:480	Currently, there are patterns for verb-particle, verb-preposition, and verb-object collocations, as well as compound nouns." ></td>
	<td class="line oc" title="232:480	Initially, we acquired patterns for verb-object collocations by analyzing lists of root word pairs that were weighted for relative co-occurrence in a corpus of articles 12 Susan W. McRoy Using Multiple Knowledge Sources from the Dow Jones News Service (cf.Church and Hanks 1990; Smadja and McKeown 1990)." ></td>
	<td class="line x" title="234:480	As an example of the kind of data that we derived, Figure 8 shows the ten most frequent co-occurrences involving the root 'take'." ></td>
	<td class="line x" title="235:480	Note that the collocation 'take action' appears both in its active form (third in the list), as well as its passive, actions were taken (fifth in the list)." ></td>
	<td class="line x" title="236:480	From an examination of these lists and the contexts in which the pairs appeared in the corpus, we constructed the patterns used by TRUMP to identify collocations." ></td>
	<td class="line x" title="237:480	Then, using the patterns as a guide, we added lexical entries for each collocation." ></td>
	<td class="line x" title="238:480	(Figure 9 lists some of the entries for the compositional collocations associated with the verb take; the entries pair a dynamic sense of take with a sense occurring as its complement)." ></td>
	<td class="line x" title="239:480	These entries link the collocations to the semantic hierarchy, and, where appropriate, provide syntactic constraints that the parser can use to verify the presence of a collocation." ></td>
	<td class="line x" title="240:480	For example, Figure 10 shows the entry for the noncompositional collocation take place, which requires that the object (t-*tail*) be singular and determinerless." ></td>
	<td class="line x" title="241:480	These entries differ from similar representations of collocations or idioms in Smadja and McKeown (1990) and Stock (1989), in that they are sense-based rather than wordbased." ></td>
	<td class="line x" title="242:480	That is, instead of expressing collocations as word-templates, the lexicon groups together collocations that combine the same sense of the head verb with particular senses or higher-level concepts (cf.Dyer and Zernik 1986)." ></td>
	<td class="line x" title="244:480	This approach better addresses the fact that collocations do have a semantic basis, capturing general forms such as give him or her <some temporal object>, which underlies the collocations give month, give minute, and give time." ></td>
	<td class="line x" title="245:480	Currently, the system has entries for over 1700 such collocations." ></td>
	<td class="line x" title="246:480	3.4 Cluster Definitions The last source of sense preferences we need to consider is the semantic context." ></td>
	<td class="line x" title="247:480	Work on lexical cohesion suggests that people use words that repeat a conceptual category or that have a semantic association to each other to create unity in text (Morris 1988; Morris and Hirst 1991; Halliday and Hasan 1976)." ></td>
	<td class="line x" title="248:480	These associations can be thought of as a class of collocations that lack the predictable syntactic structure of, say, collocations arising from verb-particle or compound noun constructions." ></td>
	<td class="line x" title="249:480	Since language producers select senses that group together semantically, a language analyzer should prefer senses that share a semantic association." ></td>
	<td class="line x" title="250:480	However, it is unclear whether the benefit of knowing the exact nature of an association would justify the cost of determining it." ></td>
	<td class="line x" title="251:480	Thus, our system provides a cluster mechanism for representing and identifying groups of senses that are associated in some unspecified way." ></td>
	<td class="line x" title="252:480	A cluster is a set of the senses associated with some central concept." ></td>
	<td class="line x" title="253:480	The definition of a cluster includes a name suggesting the central concept and a list of the cluster's members, as in Figure 11." ></td>
	<td class="line x" title="254:480	A cluster may contain concepts or other clusters." ></td>
	<td class="line x" title="255:480	TRUMP's knowledge base contains three types of clusters: categorial, functional, and situational." ></td>
	<td class="line x" title="256:480	The simplest type of cluster is the categorial cluster." ></td>
	<td class="line x" title="257:480	These clusters consist of the sets of all senses sharing a particular conceptual parent." ></td>
	<td class="line x" title="258:480	Since the conceptual hierarchy already encodes these clusters implicitly, we need not write formal cluster definitions for them." ></td>
	<td class="line x" title="259:480	Obviously, a sense will belong to a number of categorial clusters, one for each element of its parent chain." ></td>
	<td class="line x" title="260:480	The second type of cluster is the functional cluster." ></td>
	<td class="line x" title="261:480	These consist of the sets of all senses sharing a specified functional relationship." ></td>
	<td class="line x" title="262:480	For example, our system has a small number of part-whole clusters that list the parts associated with the object named by the cluster." ></td>
	<td class="line x" title="263:480	Figure 12 shows the part-whole cluster cl-egg for parts of an egg." ></td>
	<td class="line x" title="264:480	The third type of cluster, the situational cluster, encodes general relationships among senses on the basis of their being associated with a common setting, event, 13 Computational Linguistics Volume 18, Number 1 ( take :POS verb :SPECIAL (( take50 :S-COMPOUNDS ((vc (or (member c-verb_advise2-obj c-act-of-verb_blamel c-act-of-verb_losel noun_profit2) c-giving))) :EXAMPLE (take delivery) :PAR (c-receiving) ) ( take51 :S-COMPOUNDS ((vc (or (member noun_effort1) c-temporal-obj c-energy))) :EXAMPLE (the job takes up time)) :PAR (c-require-tel) ) ( take52 :S-COMPOUNDS ((vc (member noun_news1 noun_burden1 noun_load2 noun_pressure3 noun_pressure2 noun_stress1 noun stress2 c-act-of-verb_strain1))) :EXAMPLE (he couldn't take the presssure) :PAR (c-managing) ) ( take58 :S-COMPOUNDS ((vc (or (member noun_office2 noun_advantagel noun_charge1 c-act-of-verb_control1 noun_command2 noun_responsibility1) c-structure-tel c-shape-tel))) :EXAMPLE (they took advantage of the situation) :PAR (c-contracting) ) ( ts_ke59 :S-COMPOUNDS ((vc (member noun_effect1))) :EXAMPLE (the new rules take effect today) :PAR (c-transpire) ) ( take60 :S-COMPOUNDS ((vc (or c-task))) :EXAMPLE (he took the assignment) :PAR (c-deciding) )) Figure 9 Some compositional collocations involving take." ></td>
	<td class="line x" title="265:480	( take-place1 : CTYPE vc :TAIL noun_place :PREF ((r-*tail* (and (fillerp number singular) (fillerp limit null)))) : PAR (c-transpire) ) Figure 10 The entry for the noncompositional phrase take place from the collocational entry for take." ></td>
	<td class="line x" title="266:480	14 Susan W. McRoy Using Multiple Knowledge Sources (c-ent cl-business :PAR c-cluster-obj :CLUSTERS (c-business-group c-business-manager-human c-business-org c-business-qual c-business-human c-profession c-employment-action cl-financial) ) Figure 11 The definition of the cluster cl-business." ></td>
	<td class="line x" title="267:480	(c-ent cl-egg :PAR c-cluster-obj :CLUSTERS (noun_albuminl ) Figure 12 The definition of the cluster cl-egg." ></td>
	<td class="line x" title="268:480	noun_white4 noun_eggl noun_yolkl)) (c-ent cl-courtroom :PAR c-cluster-obj :CLUSTERS (c-law-action c-law-obj verb_judgel noun_juryl verb_defendl noun_lawyerl noun_attorneyl noun_crimel noun_plaintiffl noun_justicel noun_justice2 verb_prosecutel noun_baill noun_pleal verb_objectl noun_finel noun_jaill noun_prisonl noun_courtl noun testimonyl verb_testifyl verb_try3 verb_swear2 noun_oathl noun_truthl noun_bench2 verb perjurel)) Figure 13 The definition of the cluster cl-courtroom." ></td>
	<td class="line x" title="269:480	or purpose." ></td>
	<td class="line x" title="270:480	Since a cluster's usefulness is inversely proportional to its size, these clusters normally include only senses that do not occur outside the clustered context or that strongly suggest the clustered context when they occur with some other member of the cluster." ></td>
	<td class="line x" title="271:480	Thus, situational clusters are centered upon fairly specific ideas and may correspondingly be very specific with respect to their elements." ></td>
	<td class="line x" title="272:480	It is not unusual for a word to be contained in a cluster while its synonyms are not." ></td>
	<td class="line x" title="273:480	For example, the cluster cl-courtroom shown in Figure 13 contains sense verb_testifyl, but not verb_assertl." ></td>
	<td class="line x" title="274:480	Situational clusters capture the associations found in generic descriptions (cf.Dahlgren, McDowell, and Stabler 1989) or dictionary examples (cf.Janssen 1990), but are more compact because clusters may include whole categories of objects (such as c-law-action) as members and need not specify relationships between the members." ></td>
	<td class="line x" title="277:480	(As mentioned above, the conceptual hierarchy is the best place for encoding known role-related expectations)." ></td>
	<td class="line x" title="278:480	The use of clusters for sense discrimination is also comparable to approaches that favor senses linked by marked paths in a semantic network (Hirst 1987)." ></td>
	<td class="line x" title="279:480	In fact, clusters capture most of the useful associations found in scripts or semantic networks, but lack many of the disadvantages of using networks." ></td>
	<td class="line x" title="280:480	For example, because clusters do not specify what the exact nature of any association is, learning new clusters from previously processed sentences would be fairly straightforward, in contrast to learning new fragments of network." ></td>
	<td class="line x" title="281:480	Using clusters also avoids the major problem associated with marker-passing approaches, namely how to prevent the production of stupid paths (or remove them from consideration after they have been produced) (Charniak 15 Computational Linguistics Volume 18, Number 1 1983)." ></td>
	<td class="line x" title="282:480	The relevant difference is that a cluster is cautious because it must explicitly specify all its elements." ></td>
	<td class="line x" title="283:480	A marker passer takes the opposite stance, however, considering all paths up, down, and across the network unless it is explicitly constrained." ></td>
	<td class="line x" title="284:480	Thus a marker passer might find the following dubious path from the 'written object' sense of book to the 'part-of-a-plant' sense of leaf: \[book made-of paper\] \[paper made-from wood\] \[tree made-of wood\] \[tre~e has-part leaf\] whereas no cluster would link these entities, unless there had been some prior evidence of a connection." ></td>
	<td class="line x" title="285:480	(The recommended solution to the production of such paths by a marker passer is to prevent the passing of marks through certain kinds of nodes \[Hirst 1987; Hendler 1987\])." ></td>
	<td class="line x" title="286:480	From the lexical entries, the underlying concept hierarchy, and the specialized entries for collocation and clusters just described, a language analyzer can extract the information that establishes preferences among senses." ></td>
	<td class="line x" title="287:480	In the next section, we will describe how a semantic interpreter can apply knowledge from such a wide variety of sources." ></td>
	<td class="line x" title="288:480	4." ></td>
	<td class="line x" title="289:480	Using Knowledge to Identify Sense Preferences There is a wide variety of information about which sense is the correct one, and the challenge is to decide when and how to use this information." ></td>
	<td class="line x" title="290:480	The danger of a combinatorial explosion of possibilities makes it advantageous to try to resolve ambiguities as early as possible." ></td>
	<td class="line x" title="291:480	Indeed, efficient preprocessing of texts can elicit a number of cues for word senses, set up preferences, and help control the parse." ></td>
	<td class="line x" title="292:480	Then, the parse and semantic interpretation of the text will provide the cues necessary to complete the task of resolution." ></td>
	<td class="line x" title="293:480	Without actually parsing a text, a preprocessor can identify for each word its morphology, 2 its syntactic tag or tags, 3 and whether it is part of a collocation; for each sense, it can identify whether the sense is preferred or deprecated and whether it is supported by a cluster." ></td>
	<td class="line x" title="294:480	These properties are all either retrievable directly from a knowledge base or computable from short sequences of words." ></td>
	<td class="line x" title="295:480	To identify whether the input satisfies the expectations created by syntactic cues or whether it satisfies role-related expectations, the system must first perform some syntactic analysis of the input." ></td>
	<td class="line x" title="296:480	Identifying these properties must come after parsing, because recognizing them requires both the structural cues provided by parsing and a semantic analysis of the text." ></td>
	<td class="line x" title="297:480	In our system, processing occurs in three phases: morphology, preprocessing, and parsing and semantic interpretation." ></td>
	<td class="line x" title="298:480	(See Figure 14)." ></td>
	<td class="line x" title="299:480	Analysis of a text begins with the identification of the morphological features of each word and the retrieval of the (core) senses of each word." ></td>
	<td class="line x" title="300:480	Then, the input passes through a special preprocessor that identifies parse-independent semantic preferences (i.e. , syntactic tags, collocations, and clusters) and makes a preliminary selection of word senses." ></td>
	<td class="line x" title="301:480	This selection process eliminates those core senses that are obviously inappropriate and triggers certain 2 This is at least true for English, although whether it is possible for morphologically complex or agglutinative languages such as Finnish remains to be seen." ></td>
	<td class="line x" title="302:480	3 A similar caveat applies here." ></td>
	<td class="line xc" title="303:480	(See Church \[1988\] or Zernik \[1990\] for statistical approaches to tagging English words)." ></td>
	<td class="line x" title="304:480	16 Susan W. McRoy Using Multiple Knowledge Sources Preprocessor \]~ Identification \[Ildentification of of ~-~ clusters collocations Tagging '~-~ Parser I_ > Semantic interpreter Figure 14 The system architecture." ></td>
	<td class="line x" title="305:480	specialized senses." ></td>
	<td class="line x" title="306:480	In the third phase, TRUMP attempts to parse the input and at the same time produce a 'preferred' semantic interpretation for it." ></td>
	<td class="line x" title="307:480	Since the preferred interpretation also fixes the preferred sense of each word, it is at this point that the text can be given semantic tags, thus allowing sense-based information retrieval." ></td>
	<td class="line x" title="308:480	In the next few subsections we will describe in greater detail the processes that enable the system to identify semantic preferences: morphological analysis, tagging, collocation identification, cluster matching, and semantic interpretation." ></td>
	<td class="line x" title="309:480	Afterward we will discuss how the system combines the preferences it identifies." ></td>
	<td class="line x" title="310:480	4.1 Morphological Analysis and Lexical Retrieval The first step in processing an input text is to determine the root, syntactic features, and affixes of each word." ></td>
	<td class="line x" title="311:480	This information is necessary both for retrieving the word's lexical entries and for the syntactic tagging of the text during preprocessing." ></td>
	<td class="line x" title="312:480	Morphological analysis not only reduces the number of words and senses that must be in the lexicon, but it also enables a system to make reasonable guesses about the syntactic and semantic identity of unknown words so that they do not prevent parsing (see Rau, Jacobs, and Zernik 1989)." ></td>
	<td class="line x" title="313:480	Once morphological analysis of a word is complete, the system retrieves (or derives) the corresponding senses and establishes initial semantic preferences for the primary senses." ></td>
	<td class="line x" title="314:480	For example, by default, the sense of agree meaning 'to concur' (agreed is preferred over its other senses." ></td>
	<td class="line x" title="315:480	The lexical entry for agree marks this preference by giving it :TYPE *primary* (see Figure 15)." ></td>
	<td class="line x" title="316:480	The entry also says that derivations (listed in the :S-DERIV field) agreel+ment and agree2+able are preferred, derivations agreel+able and agree3+ment are deprecated, and all other sense-affix combinations (excepting inflections) have been disallowed." ></td>
	<td class="line x" title="317:480	During morphological analysis, the system retrieves only the most general senses." ></td>
	<td class="line x" title="318:480	It waits until the preprocessor or the parser identifies supporting evidence before it retrieves word senses specific to a context, such as a domain, a situation, or a collocation." ></td>
	<td class="line x" title="319:480	In most cases this approach helps reduce the amount of ambiguity." ></td>
	<td class="line x" title="320:480	The approach is compatible with evidence discussed by Simpson and Burgess (1988) that 17 Computational Linguistics Volume 18, Number 1 ( agree :POS verb :G-DERIV nil :SENSES (( agreel :SYNTAX (one-obj no-obj thatcomp comp subj-equi) :EXAMPLE (she agrees with me  they agreed to use force they agreed on 3 percent  they agreed that he was right I agree it is true) :TYPE ~primary~ :PAK (c-agreeing) :ASSOC (concur believe) :S-DERIV ((-ment preferred noun tr_act tr_object tr_result) (-able secondary adj tr_ability)) ) ( agree2 :SYNTAX (one-obj) :EXAMPLE (winter agrees with me) :TYPE ~secondary~ :PAK (c-abstract-relation) :ASSOC (benefit) :S-DERIV ((-able preferred adj tr ability)) ) ( agree3 :SYNTAX (no-obj) :EXAMPLE (the two accounts do not agree) :TYPE ~secondary~ :PAR (c-equivalence-rel) :ASSOC (correspond) :S-DEKIV ((-ment secondary noun tr_state)) ) ) ) Figure 15 The lexical entry for the verb agree." ></td>
	<td class="line x" title="321:480	'multiple meanings are activated in frequency-coded order' and that low-frequency senses are handled by a second retrieval process that accumulates evidence for those senses and activates them as necessary." ></td>
	<td class="line x" title="322:480	4.2 Tagging Once the system determines the morphological analysis of each word, the next step in preprocessing is to try to determine the correct part of speech for the word." ></td>
	<td class="line x" title="323:480	Our system uses a tagging program, written by Uri Zernik (1990), that takes information about the root, affix, and possible syntactic category for each word and applies stochastic techniques to select a syntactic tag for each word." ></td>
	<td class="line x" title="324:480	Stochastic taggers look at small groups of words and pick the most likely assignment of tags, determined by the frequency of alternative syntactic patterns in similar texts." ></td>
	<td class="line xc" title="325:480	Although it may not be possible to completely disambiguate all words prior to parsing, approaches based on 18 Susan W. McRoy Using Multiple Knowledge Sources stochastic information have been quite successful (Church 1988; Garside, Leech, and Sampson 1987; de Marcken 1990)." ></td>
	<td class="line x" title="326:480	4 To allow for the fact that the tagger may err, as part of the tagging process the system makes a second pass through the text to remove some systematic errors that result from biases common to statistical approaches." ></td>
	<td class="line x" title="327:480	For example, they tend to prefer modifiers over nouns and nouns over verbs; for instance, in Example 5, the tagger erroneously marks the word need as a noun." ></td>
	<td class="line x" title="328:480	Example 5 You really need the Campbell Soups of the world to be interested in your magazine." ></td>
	<td class="line x" title="329:480	In this second pass, the system applies a few rules derived from our grammar and resets the tags where necessary." ></td>
	<td class="line x" title="330:480	For example, to correct for the noun versus verb overgeneralization, whenever a word that can be either a noun or a verb gets tagged as just a noun, the corrector lets it remain ambiguous unless it is immediately preceded by a determiner (a good clue for nouns), or it is immediately preceded by a plural noun or a preposition, or is immediately followed by a determiner (three clues that suggest a word may be a verb)." ></td>
	<td class="line x" title="331:480	The system is able to correct for all the systematic errors we have identified thus far using just nine rules of this sort." ></td>
	<td class="line x" title="332:480	After tagging, the preprocessor eliminates all senses corresponding to unselected parts of speech." ></td>
	<td class="line x" title="333:480	4.3 Identification of Collocations Following the syntactic filtering of senses, TRUMP's preprocessor identifies collocations and establishes semantic preferences for the senses associated with them." ></td>
	<td class="line x" title="334:480	In this stage of preprocessing, the system recognizes the following types of collocations:  verb+particle pairs such as take on;  verb+preposition pairs such as invest in;  verb+particle+preposition combinations such as break in on;  verb+complement clauses such as take a bath, their passives, as in actions were taken, and hyphenated nominals, such as profit-taking;  compound noun phrases such as investment bank." ></td>
	<td class="line x" title="335:480	To recognize a collocation, the preprocessor relies on a set of simple patterns, which match the general syntactic context in which the collocation occurs." ></td>
	<td class="line x" title="336:480	For example, the system recognizes the collocation 'take profit' found in Example 6 with the pattern (TAKE (DET) PROFIT)." ></td>
	<td class="line x" title="337:480	Example 6 A number of stocks that have spearheaded the market's recent rally bore the brunt of isolated profit-taking Tuesday." ></td>
	<td class="line x" title="338:480	The preprocessor's strategy for locating a collocation is to first scan the text for trigger words, and if it finds the necessary triggers, then to try to match the complete pattern." ></td>
	<td class="line x" title="339:480	(Triggers typically correspond to the phrasal head of a collocation, but for 4 Magerman and Marcus (1990) do complete stochastic N-gram parsing." ></td>
	<td class="line x" title="340:480	19 Computational Linguistics Volume 18, Number 1 more complex patterns, such as verb-complement clauses, both parts of the collocation must be present)." ></td>
	<td class="line x" title="341:480	The system's matching procedures allow for punctuation and verb-complement inversion." ></td>
	<td class="line x" title="342:480	If the triggers are found and the match is successful, the preprocessor has a choice of subsequent actions, depending on how cautious it is supposed to be." ></td>
	<td class="line x" title="343:480	In its aggressive mode, it updates the representations of the matched words, adding any triggered senses and preferences for the collocated senses." ></td>
	<td class="line x" title="344:480	It also deletes any unsupported, deprecated senses." ></td>
	<td class="line x" title="345:480	In its cautious mode, it just adds the word senses associated with the pattern to a dynamic store." ></td>
	<td class="line x" title="346:480	Once stored, these senses are then available for the parser to use after it verifies the syntactic constraints of the collocation; if it is successful, it will add preferences for the appropriate senses." ></td>
	<td class="line x" title="347:480	Early identification of triggered senses enables the system to use them for cluster matching in the next stage." ></td>
	<td class="line x" title="348:480	4.4 Identification of Clusters After the syntactic filtering of senses and the activation of senses triggered by collocations, the next step of preprocessing identifies preferences for senses that invoke currently active clusters (see Section 3.4)." ></td>
	<td class="line x" title="349:480	A cluster is active if it contains any of the senses under consideration for other words in the current paragraph." ></td>
	<td class="line x" title="350:480	The system may also activate certain clusters to represent the general topic of the text." ></td>
	<td class="line x" title="351:480	The preprocessor's strategy for assessing cluster-based preferences is to take the set of cluster names invoked by each sense of each content word in the sentence and locate all intersections between it and the names of other active clusters." ></td>
	<td class="line x" title="352:480	(For purposes of cluster matching, the sense list for each word will include all the special and noncompositional senses activated during the previous stage of preprocessing, as well as any domain-specific senses that are not yet active)." ></td>
	<td class="line x" title="353:480	For each intersection the preprocessor finds, it adds preferences for the senses that are supported by the cluster match." ></td>
	<td class="line x" title="354:480	Then, the preprocessor activates any previously inactive senses it found to be supported by a cluster match." ></td>
	<td class="line x" title="355:480	This triggering of senses on the basis of conceptual context forms the final step of the preprocessing phase." ></td>
	<td class="line x" title="356:480	4.5 Semantic Interpretation Once preprocessing is complete, the parsing phase begins." ></td>
	<td class="line x" title="357:480	In this phase, TRUMP attempts to build syntactic structures, while calling on the semantic interpreter to build and rate alternative interpretations for each structure proposed." ></td>
	<td class="line x" title="358:480	These semantic evaluations then guide the parser's evaluation of syntactic structures." ></td>
	<td class="line x" title="359:480	They may also influence the actual progression of the parse." ></td>
	<td class="line x" title="360:480	For example, if a structure is found to have incoherent semantics, the parser immediately eliminates it (and all structures that might contain it) from further consideration." ></td>
	<td class="line x" title="361:480	Also, whenever the semantics of a parse becomes sufficiently better than that of its competitors, the system prunes the semantically inferior parses, reducing the number of ambiguities even further, s As suggested above, the system builds semantic interpretations incrementally." ></td>
	<td class="line x" title="362:480	For each proposed combination of syntactic structures, there is a corresponding combination of semantic structures." ></td>
	<td class="line x" title="363:480	It is the job of the semantic interpreter to identify the possible relations that link the structures being combined, identify the preferences associated with each possible combination of head, role (relation), and filler (the argument or modifier), and then rank competing semantic interpretations." ></td>
	<td class="line x" title="364:480	5 A similar approach has been taken by Gibson (1990) and is supported by the psychological experiments of Kurtzman (1984)." ></td>
	<td class="line x" title="365:480	20 Susan W. McRoy Using Multiple Knowledge Sources For each proposed combination, knowledge sources may contribute the following preferences:  preferences directly associated with the head or the filler, determined recursively from their components, beginning with preferences identified during preprocessing." ></td>
	<td class="line x" title="366:480	 preferences associated with syntactic cues, such as the satisfaction of restrictions listed in the lexicon." ></td>
	<td class="line x" title="367:480	For example, a word may allow only modifiers of a particular syntactic form, or a modifier may modify only a certain syntactic form." ></td>
	<td class="line x" title="368:480	(For example, the sense meaning 'to care for,' in She tends plants or She tends to plants occurs with an NP or PP object, whereas the sense of tend meaning 'to have a tendency' as in She tends to lose things requires a clausal object)." ></td>
	<td class="line x" title="369:480	 preferences associated with the semantic 'fit' between any two of the head, the role, and the filler, for example: filler and role e.g., foods make good fillers for the PATIENT role of eating activities; filler and head e.g., colors make good modifiers of physical objects; head and role e.g., monetary objects expect to be qualified by some QUANTITY." ></td>
	<td class="line x" title="370:480	The conceptual hierarchy and the lexicon contain the information that encodes these preferences." ></td>
	<td class="line x" title="371:480	 preferences triggered by reference resolution." ></td>
	<td class="line x" title="372:480	(Currently, our system does not make use of these preferences, but see Crain and Steedman \[1985\]; Altmann and Steedman \[1988\]; Hirst \[1987\])." ></td>
	<td class="line x" title="373:480	How the semantic interpreter combines these preferences is the subject of the next section." ></td>
	<td class="line x" title="374:480	5." ></td>
	<td class="line x" title="375:480	Combining Preferences to Select Senses Given the number of preference cues available for discriminating word senses, an understander must face the question of what to do if they conflict." ></td>
	<td class="line x" title="376:480	For example, in the sentence Mary took a picture to Bob, the fact that photography does not normally have a destination (negative role-related information) should override the support for the 'photograph' interpretation of took a picture given by collocation analysis." ></td>
	<td class="line x" title="377:480	A particular source of information may also support more than one possible interpretation, but to different degrees." ></td>
	<td class="line x" title="378:480	For example, cigarette filter may correspond either to something that filters out cigarettes or to something that is part of a cigarette, but the latter relation is more likely." ></td>
	<td class="line x" title="379:480	Our strategy for combining the preferences described in the preceding sections is to rate most highly the sense with the strongest combination of supporting cues." ></td>
	<td class="line x" title="380:480	The system assigns each preference cue a strength, an integer value between +10 and -10, and then sums these strengths to find the sense with the highest rating." ></td>
	<td class="line x" title="381:480	The strength of a particular cue depends on its type and on the degree to which the expectations underlying it are satisfied." ></td>
	<td class="line x" title="382:480	For cues that are polar -for example, a sense is either low or high frequency -a value must be chosen experimentally, depending on the strength of the cue compared with others." ></td>
	<td class="line x" title="383:480	For example, the system assigns frequency information (the primary-secondary distinction) a score close to 21 Computational Linguistics Volume 18, Number 1 zero because this information tends to be significant only when other preferences are inconclusive." ></td>
	<td class="line x" title="384:480	For cues that have an inherent extent -for example, the conceptual category specified by a role preference subsumes a set of elements that can be counted -the cue strength is a function of the magnitude of the extent, that is, its specificity." ></td>
	<td class="line x" title="385:480	TRUMP's specificity function maps the number of elements subsumed by the concept onto the range 0 to +10." ></td>
	<td class="line x" title="386:480	The function assigns concepts with few members a high value and concepts with many members a low w~lue." ></td>
	<td class="line x" title="387:480	For example, the concept c-object, which subsumes roughly half the knowledge base, has a low specificity value (1)." ></td>
	<td class="line x" title="388:480	In contrast, the concept noun&after1, which subsumes only a single entity, has a high specificity value (10)." ></td>
	<td class="line x" title="389:480	Concept strength is inversely proportional to concept size because a preference for a very general (large) concept often indicates that either there is no strong expectation at all or there is a gap in the system's knowledge." ></td>
	<td class="line x" title="390:480	In either case, a concept that subsumes only a few senses is stronger information than a concept that subsumes more." ></td>
	<td class="line x" title="391:480	The preference score for a complex concept, formed by combining simpler concepts with the connectives AND, OR, and NOT, is a function of the number of senses subsumed by both, either, or neither concept, respectively." ></td>
	<td class="line x" title="392:480	Similarly, the score for a cluster is the specificity of that cluster (as defined in Section 3.4)." ></td>
	<td class="line x" title="393:480	(If a sense belongs to more than one active cluster, then only the most specific one is considered)." ></td>
	<td class="line x" title="394:480	The exact details of the function (i.e. , the range of magnitudes corresponding to each specificity class) necessarily depend on the size and organization of one's concept hierarchy." ></td>
	<td class="line x" title="395:480	For example, one would assign specificity value 1 to any concept with more members than any immediate specialization of the most abstract concept." ></td>
	<td class="line x" title="396:480	When a preference cue matches the input, the cue strength is its specificity value; when a concept fails to match the input, the strength is a negative value whose magnitude is usually the specificity of the concept, but it is not always this straightforward." ></td>
	<td class="line x" title="397:480	Rating the evidence associated with a preference failure is a subtle problem, because there are different types of preference failure to take into account." ></td>
	<td class="line x" title="398:480	Failure to meet a general preference is always significant, whereas failure to meet a very specific preference is only strong information when a slight relaxation of the preference does not eliminate the failure." ></td>
	<td class="line x" title="399:480	This presents a bit of a paradox: the greater the specificity of a concept, the more information there is about it, but the less information there may be about a corresponding preference." ></td>
	<td class="line x" title="400:480	The paradox arises because the failure of a very specific preference introduces significant uncertainty as to why the preference failed." ></td>
	<td class="line x" title="401:480	Failing to meet a very general preference is always strong information because, in practice, the purpose of such preferences is to eliminate the grossly inappropriate -such as trying to use a relation with a physical object when it should only be applied to events." ></td>
	<td class="line x" title="402:480	The specificity function in this case returns a value whose magnitude is the same as the specificity of the complement of the concept (i.e. , the positive specificity less the maximum specificity, 10)." ></td>
	<td class="line x" title="403:480	The result is a negative number whose absolute value is greater than it would be by default." ></td>
	<td class="line x" title="404:480	For example, if a preference is for the concept c-object, which has a positive specificity of 1, and this concept fails to match the input, then the preference value for the cue will be -9." ></td>
	<td class="line x" title="405:480	On the other hand, a very specific preference usually pinpoints the expected entity, i.e., the dead giveaway pairings of role and filler." ></td>
	<td class="line x" title="406:480	Thus, it is quite common for these preferences to overspecify the underlying constraint; for example, cut may expect a tool as an INSTRUMENT, but almost any physical object will suffice." ></td>
	<td class="line x" title="407:480	When a slight relaxation of the preference is satisfiable, a system should take the cautious route, and assume it has a case of overspecification and is at worst a weak failure." ></td>
	<td class="line x" title="408:480	Again, the specificity function returns a negative value with magnitude equivalent to the specificity of the complement of the concept, but this time the result will be a negative number whose 22 Susan W. McRoy Using Multiple Knowledge Sources absolute value is less than it would be by defaulL When this approach fails, a system can safely assume that the entity under consideration is 'obviously inappropriate' for a relatively strong expectation, and return the default value." ></td>
	<td class="line x" title="409:480	The default value for a concept that is neither especially general nor specific and that fails to match the input is just -1 times the positive specificity of the concept." ></td>
	<td class="line x" title="410:480	The strategy of favoring the most specific information has several advantages." ></td>
	<td class="line x" title="411:480	This approach best addresses the concerns of an expanding knowledge base where one must be concerned not only with competition between preferences but also with the inevitable gaps in knowledge." ></td>
	<td class="line x" title="412:480	Generally, the more specific information there is, the more complete, and hence more trustworthy, the information is. Thus, when there is a clear semantic distinction between the senses and the system has the information necessary to identify it, a clear distinction usually emerges in the ratings." ></td>
	<td class="line x" title="413:480	When there is no strong semantic distinction, or there is very little information, preference scores are usually very close, so that the parser must fall back on syntactic preferences, such as Right Association." ></td>
	<td class="line x" title="414:480	This result provides a simple, sensible means of balancing syntactic and semantic preferences." ></td>
	<td class="line x" title="415:480	To see how the cue strengths of frequency information, morphological preferences, collocations, clusters, syntactic preferences, and role-related preferences interact with one another to produce the final ranking of senses, consider the problem of deciding the correct sense of reached in Example 1 (repeated below): Example 1 The agreement reached by the state and the EPA provides for the safe storage of the waste." ></td>
	<td class="line x" title="416:480	According to the system's lexicon, reached has four possible verb senses:  reach1, as in reach a destination, which has conceptual parents c-dest-occur ('destination occurrence') and c-arriving;  reach2, as in reach for a cookie, which has conceptual parent c-bodypart-act ion;  reach3, as in reach her by telephone, which has conceptual parent c-comm-event ('communication event'); and  reach4, as in reach a conclusion, which has conceptual parent c-cause-to-event-change." ></td>
	<td class="line x" title="417:480	Figure 16 shows a tabulation of cue strengths for each of these interpretations of reach in Example 1, when just information in the VP reached by the state and the EPA is considered." ></td>
	<td class="line x" title="418:480	The sense reach3 has the highest total score." ></td>
	<td class="line x" title="419:480	From the table, we see that, at this point in the parse, the only strong source of preferences is the role information (line 6 of Figure 16)." ></td>
	<td class="line x" title="420:480	The derivation of these numbers is shown in Figures 17, 18, and 19, which list the role preferences associated with the possible interpretations of the preposition by for reach3, and its two nearest competitors, reach1 and reach4." ></td>
	<td class="line x" title="421:480	Together, the data in the tables reveal the following sources of preference strength: The 'arrival' sense (reachl) gains support from the fact that there is a sense of by meaning AGENT, which is a role that arrivals expect (line 3 of column 3 of Figure 17), and the state and the EPA make reasonably good agents (line 5 of column 3 of Figure 17)." ></td>
	<td class="line x" title="422:480	23 Computational Linguistics Volume 18, Number 1 Cue Strength Cue Type reach1 reach2 reach3 reach4 c-dest-occur c-bodypart-action c-comm-event c~cause-to-event-change Frequency Morphology Collocation Cluster Syntax Roles 1 0 0 0 0 41 1 0 0 0 0 38 -1 0 0 0 0 46 1 0 0 0 0 41 TotM 42 39 45 42 Figure 16 Score tabulations for reached in the VP." ></td>
	<td class="line x" title="423:480	reach1 Role Preference Strength Preference Type by1 SPATIAL-PROXIMITY by3 DIRECTION Relation-Filler 0 0 Filler-Holder 0 0 Relation-Holder 0 0 Syntax 1 1 Strength of PP 37 35 Total 38 36 by4 by5 AGENT INSTRUMENT 0 -2 0 0 5 0 1 1 35 35 141 \[ 34 Figure 17 Role-related preferences of reacht for the preposition by." ></td>
	<td class="line x" title="424:480	 The 'communication' sense (reach3) gains support from the fact that there is a sense of by corresponding to the expected role COMMUNICATOR (line 3 of column 3 of Figure 18) and the state and the EPA make very good agents of communication events (communicators), in particular (line 1 of column 3 of Figure 18), as well as being good agents in general (line 5 of column 3 of Figure 18); however, reach3 is disfavored by frequency information (line 1 of column 3 of Figure 16)." ></td>
	<td class="line x" title="425:480	 The 'event change' (conclude) sense (reach4) gains support from the fact that there is a sense of by corresponding to the expected role CAUSE (line 3 of column 3 of Figure 19) and from the fact that the state and the EPA make good agents (line 5 of column 3 of Figure 19)." ></td>
	<td class="line x" title="426:480	Although the system favors the 'communication' sense of reach in the VP, for the final result, it must balance this information with that provided by the relationship between agreement and the verb phrase." ></td>
	<td class="line x" title="427:480	By the end of the parse, the 'event-change' sense comes to take precedence: * The system completely eliminates the 'destination' sense from consideration because it is significantly weaker than all its competitors." ></td>
	<td class="line x" title="428:480	24 Susan W. McRoy Using Multiple Knowledge Sources reach3 Role Preference Strength Preference Type byl by3 by4 by5 SPATIAL-PROXIMITY DIRECTION COMMUNICATOR INSTRUMENT Relation-Filler 0 0 5 0 Filler-Holder 0 0 0 0 Relation-Holder 0 0 5 0 Syntax 1 1 1 1 Strength of PP 37 35 35 35 Total 38 36 46 36 Figure 18 Role-related preferences of reach3 for the preposition by." ></td>
	<td class="line x" title="429:480	reach4 Role Preference Strength Preference Type by1 by3 by4 by5 SPATIAL-PROXIMITY DIRECTION CAUSE (AGENT) INSTRUMENT Relation-Filler 0 0 0 0 Filler-Holder 0 0 0 0 Relation-Holder 0 0 5 0 Syntax 1 1 1 1 Strength of PP 37 35 35 35 Total 38 36 41 36 Figure 19 Role-related preferences of reach4 for the preposition by." ></td>
	<td class="line x" title="430:480	The main cause of this weakness is that (in our system) the role that agreement would fill, DESTINATION, has no special preference for being associated with a c-dest-event -many events allow a DESTINATION role." ></td>
	<td class="line x" title="431:480	The 'communication' sense loses favor because it does not gain much support from having agreement as either PATIENT or RECIPIENT." ></td>
	<td class="line x" title="432:480	The final score of this sense is 52." ></td>
	<td class="line x" title="433:480	The 'event-change' sense gains support from having agreement fill its AFFECTED role, enough that the final strength of the 'event-change' sense, 55, ultimately surpasses the final strength of the 'communication' sense." ></td>
	<td class="line x" title="434:480	By summing the cue strengths of each possible interpretation in this way and selecting the one with the highest total score, the system decides which sense is the 'correct' one for the context." ></td>
	<td class="line x" title="435:480	The strengths of individual components of each interpretation contribute to, but do not determine, the strength of the final interpretation, because there are also strengths associated with how well the individual components fit together." ></td>
	<td class="line x" title="436:480	No additional weights are necessary, because the specificity values the system uses are a direct measure of strength." ></td>
	<td class="line x" title="437:480	25 Computational Linguistics Volume 18, Number 1 6." ></td>
	<td class="line x" title="438:480	Results and Discussion Our goal has been a natural language system that can effectively analyze an arbitrary input at least to the level of word sense tagging." ></td>
	<td class="line x" title="439:480	Although we have not yet fully accomplished this goal, our results are quite encouraging." ></td>
	<td class="line x" title="440:480	Using a lexicon of approximately 10,000 roots and 10,000 derivations, the system shows excellent lexical and morphological coverage." ></td>
	<td class="line x" title="441:480	When tested on a sample of 25,000 words of text from the Wall Street Journal, the system covered 98% of non-proper noun, non-abbreviated word occurrences (and 91% of all words)." ></td>
	<td class="line x" title="442:480	Twelve percent of the senses the system selected were derivatives." ></td>
	<td class="line x" title="443:480	The semantic interpreter is able to discriminate senses even when the parser cannot produce a single correct parse." ></td>
	<td class="line x" title="444:480	Figure 20 gives an example of the sense tagging that the system gives to the following segment of Wall Street Journal text: Example 7 The network also is changing its halftime show to include viewer participation, in an attempt to hold on to its audience through halftime and into the second halves of games." ></td>
	<td class="line x" title="445:480	One show will ask viewers to vote on their favorite all-time players through telephone polls." ></td>
	<td class="line x" title="446:480	Each word is tagged with its part of speech and sense number along with a parent concept." ></td>
	<td class="line x" title="447:480	For example, the tag \[changing verb_3 (c-replacing)\] shows that the input word is changing, the preferred sense is number 3 of the verb, and this sense falls under the concept c-replacing in the hierarchy." ></td>
	<td class="line x" title="448:480	This tagging was produced even though the parser was unable to construct a complete and correct syntactic representation of the text." ></td>
	<td class="line x" title="449:480	In fact, when tested on the Wall Street Journal texts (for which there has been no adaptation or customization aside from processing by a company-name recognizer \[Rau 1991\]), the system rarely produces a single correct parse; however, the partial parses produced generally cover most of the text at the clause level." ></td>
	<td class="line x" title="450:480	Since most semantic preferences appear at this level (and those that do not, do not depend on syntactic analysis), the results of this tagging are encouraging." ></td>
	<td class="line x" title="451:480	This example also shows some of the limitations of our system in practice." ></td>
	<td class="line x" title="452:480	The system is unable to recognize the collocation 'hold on to' in the first sentence, because it lacks a pattern for it." ></td>
	<td class="line x" title="453:480	The system also lacks patterns for the collocations 'vote on' and 'alMime players' that occur in the second sentence, and as a result, mistakenly tags on as c-temporal-proximty-rel rather than something more appropriate, such as c-purpose-tel." ></td>
	<td class="line x" title="454:480	These difficulties point out the need for even more knowledge." ></td>
	<td class="line x" title="455:480	It is encouraging to note that, even if our encoding scheme is not entirely 'correct' according to human intuition, as long as it is consistent, in theory it should lead to capabilities that are no worse, with zero customization, than word-based methods for information retrieval." ></td>
	<td class="line x" title="456:480	However, having access to sense tags allows for easy improvement by more knowledge-intensive methods." ></td>
	<td class="line x" title="457:480	Although this theory is still untested, there is some preliminary evidence that word sense tagging can improve information retrieval system performance (Krovetz 1989)." ></td>
	<td class="line x" title="458:480	To date we have been unable to get a meaningful quantitative assessment of the accuracy of the system's sense tagging." ></td>
	<td class="line x" title="459:480	We made an unsuccessful attempt at evaluating the accuracy of sense-tagging over a corpus." ></td>
	<td class="line x" title="460:480	First, we discovered that a human 'expert' had great difficulty identifying each sense, and that this task was far more tedious than manual part-of-speech tagging or bracketing." ></td>
	<td class="line x" title="461:480	Second, we questioned what we would learn from the evaluation of these partial results, and have since turned our 26 Susan W. McRoy Using Multiple Knowledge Sources \[the det_l (c-definite-qual) \] \[network noun_2 (c-entertainment-obj c-business-org c-system) \] \[also adv_1 (c-numeric-qual) \] \[is *aux* \] \[changing verb_3 (c-replacing) \] \[its ppnoun_l (c-obj) \] \[halftime noun_l (c-entity) \] \[show c-act-of-verb_showl (c-manifesting) \] \[to ~infl~ \] \[include verb_2 (c-grouping) \] \[viewer c-verb_view2-er (c-entity) \] \[participation c-result-of-being-verb_participatel (c-causal-state) \] \[~comma* *punct~ \] \[in prep_2Z (c-group-part) \] \[an det_l (c-definite-qual) \] \[attempt c-act-of-verb_attemptl (c-attempting) \] \[to *infl~ \] \[hold verb_4 (c-positioning) \] \[on adv_l (c-range-qual c-continuity-qual) \] \[to prep_l (c-destination-rel) \] \[its ppnoun_l (c-obj) \] \[audience noun_l (c-human-group) \] \[through prep_1 (c-course-rel) \] \[halftime noun_l (c-entity) \] \[and coordconj_l (c-conjunction) \] \[into prep_5 (e-engage-in) \] \[the det_l (c-definite-qual) \] \[second c-numword_twol-th (c-order-qual) \] \[halves noun_l (c-portion-part) \] \[of prep_8 (c-stateobject-rel) \] \[games noun_l (c-recreation-obj) \] \[*period~ ~punct~ \] \[one noun_1 (c-entity) \] \[show c-act-of-verb_showl (c-manifesting) \] \[will ~aux~ \] \[ask verb_2 (c-asking) \] \[viewers c-verb_view2-er (c-entity) \] \[to ~infl~ \] \[vote verb_1 (c-selecting) \] \[on prep_4 (c-temporal-proximity-rel) \] \[their ppnoun_1 (c-obj) \] \[favorite adj_l (c-importance-qual c-superiority-qual) \] \[all det_1 (c-quantifier) \] \[~hyphen~ ~punct~ \] \[time noun_1 (c-indef-time-period) \] \[players c-verb_playl-er (c-entity) \] \[through prep_l (c-course-rel) \] \[telephone noun_1 (c-machine) \] \[polls c-act-of-verb_poll1 (c-asking) \] \[~period~ *punct* \] Figure 20 A sample sense coding." ></td>
	<td class="line x" title="462:480	attention back to evaluating the system with respect to some task, such as information retrieval." ></td>
	<td class="line x" title="463:480	Improving the quality of our sense tagging requires a fair amount of straightforward but time-consuming work." ></td>
	<td class="line x" title="464:480	This needed work includes filling a number of 27 Computational Linguistics Volume 18, Number 1 gaps in our knowledge sources." ></td>
	<td class="line x" title="465:480	For example, the system needs much more information about role-related preferences and specialized semantic contexts." ></td>
	<td class="line x" title="466:480	At present all this information is collected and coded by hand, although recent work by Ravin (1990) and Dahlgren, McDowell, and Stabler (1989) suggests that the collection of role-related information may be automatable." ></td>
	<td class="line x" title="467:480	Our next step is to evaluate the effect of text coding on an information retrieval task, by applying traditional term-weighted statistical retrieval methods to the recoded text." ></td>
	<td class="line x" title="468:480	One intriguing aspect of this approach is that errors in distinguishing sense preferences should not be too costly in this task, so long as the program is fairly consistent in its disambiguation of terms in both the source texts and the input queries." ></td>
	<td class="line x" title="469:480	7." ></td>
	<td class="line x" title="470:480	Conclusion Having access to a large amount of information and being able to use it effectively are essential for understanding unrestricted texts, such as newspaper articles." ></td>
	<td class="line x" title="471:480	We have developed a substantial knowledge base for text processing, including a word sensebased lexicon that contains both core senses and dynamically triggered entries." ></td>
	<td class="line x" title="472:480	We have also created a number of concept-cluster definitions describing common semantic contexts and a conceptual hierarchy that acts as a sense-disambiguated thesaurus." ></td>
	<td class="line x" title="473:480	Our approach to word sense discrimination uses information drawn from the knowledge base and the structure of the text, combining the strongest, most obvious sense preferences created by syntactic tags, word frequencies, collocations, semantic context (clusters), selectional restrictions, and syntactic cues." ></td>
	<td class="line x" title="474:480	To apply this information most efficiently, the approach introduces a preprocessing phase that uses preference information available prior to parsing to eliminate some of the lexical ambiguity and establish baseline preferences." ></td>
	<td class="line x" title="475:480	Then, during parsing, the system combines the baseline preferences with preferences created by selectional restrictions and syntactic cues to identify preferred interpretations." ></td>
	<td class="line x" title="476:480	The preference combination mechanism of the system uses dynamic measures of strength based on specificity, rather than relying on some fixed, ordered set of rules." ></td>
	<td class="line x" title="477:480	There are some encouraging results from applying the system to sense tagging of arbitrary text." ></td>
	<td class="line x" title="478:480	We expect to evaluate our approach on tasks in information retrieval, and, later, machine translation, to determine the likelihood of achieving substantive improvements through sense-based semantic analysis." ></td>
	<td class="line x" title="479:480	Acknowledgments I am grateful to Paul Jacobs for his comments and his encouragement of my work on natural language processing at GE; to George Krupka for helping me integrate my work with TRUMP, and for continuing to improve the system; to Graeme Hirst for his many comments and suggestions on this article; and to Jan Wiebe and Evan Steeg for their comments on earlier drafts." ></td>
	<td class="line x" title="480:480	I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P92-1014
Information Retrieval Using Robust Natural Language Processing
Strzalkowski, Tomek;Vauthey, Barbara;"></td>
	<td class="line x" title="1:196	INFORMATION RETRIEVAL USING ROBUST NATURAL LANGUAGE PROCESSING Tomek Strzalkowski and Barbara Vauthey1' Courant Institute of Mathematical Sciences New York University 715 Broadway, rm." ></td>
	<td class="line x" title="2:196	704 New York, NY 10003 tomek@cs.nyu.edu ABSTRACT We developed a prototype information retrieval system which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval." ></td>
	<td class="line x" title="3:196	The backbone of our system is a statistical retrieval engine which performs automated indexing of documents, then search and ranking in response to user queries." ></td>
	<td class="line x" title="4:196	This core architecture is augmented with advanced natural language processing tools which are both robust and efficient." ></td>
	<td class="line x" title="5:196	In early experiments, the augmented system has displayed capabilities that appear to make it superior to the purely statistical base." ></td>
	<td class="line x" title="6:196	INTRODUCTION A typical information retrieval fiR) task is to select documents from a database in response to a user's query, and rank these documents according to relevance." ></td>
	<td class="line x" title="7:196	This has been usually accomplished using statistical methods (often coupled with manual encoding), but it is now widely believed that these traditional methods have reached their limits." ></td>
	<td class="line x" title="8:196	1 These limits are particularly acute for text databases, where natural language processing (NLP) has long been considered necessary for further progress." ></td>
	<td class="line x" title="9:196	Unfortunately, the difficulties encountered in applying computational linguistics technologies to text processing have contributed to a wide-spread belief that automated NLP may not be suitable in IR." ></td>
	<td class="line x" title="10:196	These difficulties included inefficiency, limited coverage, and prohibitive cost of manual effort required to build lexicons and knowledge bases for each new text domain." ></td>
	<td class="line x" title="11:196	On the other hand, while numerous experiments did not establish the usefulness of NLP, they cannot be considered conclusive because of their very limited scale." ></td>
	<td class="line x" title="12:196	Another reason is the limited scale at which NLP was used." ></td>
	<td class="line x" title="13:196	Syntactic parsing of the database contents, for example, has been attempted in order to extract linguistically motivated 'syntactic phrases', which presumably were better indicators of contents than 'statistical phrases' where words were grouped solely on the basis of physical proximity (eg." ></td>
	<td class="line x" title="14:196	'college junior' is not the same as 'junior college')." ></td>
	<td class="line x" title="15:196	These intuitions, however, were not confirmed by experiments; worse still, statistical phrases regularly outperformed syntactic phrases (Fagan, 1987)." ></td>
	<td class="line x" title="16:196	Attempts to overcome the poor statistical behavior of syntactic phrases has led to various clustering techniques that grouped synonymous or near synonymous phrases into 'clusters' and replaced these by single 'metaterms'." ></td>
	<td class="line x" title="17:196	Clustering techniques were somewhat successful in upgrading overall system performance, but their effectiveness was diminished by frequently poor quality of syntactic analysis." ></td>
	<td class="line x" title="18:196	Since full-analysis wide-coverage syntactic parsers were either unavailable or inefficient, various partial parsing methods have been used." ></td>
	<td class="line x" title="19:196	Partial parsing was usually fast enough, but it also generated noisy data_' as many as 50% of all generated phrases could be incorrect (Lewis and Croft, 1990)." ></td>
	<td class="line x" title="20:196	Other efforts concentrated on processing of user queries (eg." ></td>
	<td class="line x" title="21:196	Spack Jones and Tait, 1984; Smeaton and van Rijsbergen, 1988)." ></td>
	<td class="line x" title="22:196	Since queries were usually short and few, even relatively inefficient NLP techniques could be of benefit to the system." ></td>
	<td class="line x" title="23:196	None of these attempts proved conclusive, and some were never properly evaluated either." ></td>
	<td class="line x" title="24:196	t Current address: Laboratoire d'lnformatique, Unlversite de Fribourg, ch." ></td>
	<td class="line x" title="25:196	du Musee 3, 1700 Fribourg, Switzerland; vauthey@cfmniSl.bitnet." ></td>
	<td class="line x" title="26:196	i As far as the aut~natic document retrieval is concerned." ></td>
	<td class="line x" title="27:196	Techniques involving various forms of relevance feedback are usually far more effective, but they require user's manual intervention in the retrieval process." ></td>
	<td class="line x" title="28:196	In this paper, we are concerned with fully automated retrieval only." ></td>
	<td class="line x" title="29:196	2 Standard IR benchmark collections are statistically too small and the experiments can easily produce counterintuitive results." ></td>
	<td class="line x" title="30:196	For example, Cranfield collection is only approx." ></td>
	<td class="line x" title="31:196	180,000 English words, while CACM-3204 collection used in the present experiments is approx." ></td>
	<td class="line x" title="32:196	200,000 words." ></td>
	<td class="line x" title="33:196	104 We believe that linguistic processing of both the database and the user's queries need to be done for a maximum benefit, and moreover, the two processes must be appropriately coordinated." ></td>
	<td class="line x" title="34:196	This prognosis is supported by the experiments performed by the NYU group (Strzalkowski and Vauthey, 1991; Grishman and Strzalkowski, 1991), and by the group at the University of Massachussetts (Croft et al. , 1991)." ></td>
	<td class="line x" title="35:196	We explore this possibility further in this paper." ></td>
	<td class="line x" title="36:196	OVERALL DESIGN Our information retrieval system consists of a traditional statistical backbone (Harman and Candela, 1989) augmented with various natural language processing components that assist the system in database processing (stemming, indexing, word and phrase clustering, selectional restrictions), and translate a user's information request into an effective query." ></td>
	<td class="line x" title="37:196	This design is a careful compromise between purely statistical non-linguistic approaches and those requiring rather accomplished (and expensive) semantic analysis of data, often referred to as 'conceptual retrieval'." ></td>
	<td class="line x" title="38:196	The conceptual retrieval systems, though quite effective, are not yet mature enough to be considered in serious information retrieval applications, the major problems being their extreme inefficiency and the need for manual encoding of domain knowledge (Mauldin, 1991)." ></td>
	<td class="line x" title="39:196	In our system the database text is first processed with a fast syntactic parser." ></td>
	<td class="line x" title="40:196	Subsequently certain types of phrases are extracted from the parse trees and used as compound indexing terms in addition to single-word terms." ></td>
	<td class="line x" title="41:196	The extracted phrases are statistically analyzed as syntactic contexts in order to discover a variety of similarity links between smaller subphrases and words occurring in them." ></td>
	<td class="line x" title="42:196	A further filtering process maps these similarity links onto semantic relations (generalization, specialization, synonymy, etc)." ></td>
	<td class="line x" title="43:196	after which they are used to transform user's request into a search query." ></td>
	<td class="line x" title="44:196	The user's natural language request is also parsed, and all indexing terms occurring in them are identified." ></td>
	<td class="line x" title="45:196	Next, certain highly ambiguous (usually single-word) terms are dropped, provided that they also occur as elements in some compound terms." ></td>
	<td class="line x" title="46:196	For example, 'natural' is deleted from a query already containing 'natural language' because 'natural' occurs in many unrelated contexts: 'natural number', 'natural logarithm', 'natural approach', etc. At the same time, other terms may be added, namely those which are linked to some query term through admissible similarity relations." ></td>
	<td class="line x" title="47:196	For example, 'fortran' is added to a query containing the compound term 'program language' via a specification link." ></td>
	<td class="line x" title="48:196	After the final query is constructed, the database search follows, and a ranked list of documents is returned." ></td>
	<td class="line x" title="49:196	It should be noted that all the processing steps, those performed by the backbone system, and these performed by the natural language processing components, are fully automated, and no human intervention or manual encoding is required." ></td>
	<td class="line x" title="50:196	FAST PARSING WITH TI'P PARSER TIP flagged Text Parser) is based on the Linguistic String Grammar developed by Sager (1981)." ></td>
	<td class="line x" title="51:196	Written in Quintus Prolog, the parser currently encompasses more than 400 grammar productions." ></td>
	<td class="line x" title="52:196	It produces regularized parse tree representations for each sentence that reflect the sentence's logical structure." ></td>
	<td class="line x" title="53:196	The parser is equipped with a powerful skip-and-fit recovery mechanism that allows it to operate effectively in the face of illformed input or under a severe time pressure." ></td>
	<td class="line x" title="54:196	In the recent experiments with approximately 6 million words of English texts, 3 the parser's speed averaged between 0.45 and 0.5 seconds per sentence, or up to 2600 words per minute, on a 21 MIPS SparcStation ELC." ></td>
	<td class="line x" title="55:196	Some details of the parser are discussed below.4 TIP is a full grammar parser, and initially, it attempts to generate a complete analysis for each sentence." ></td>
	<td class="line x" title="56:196	However, unlike an ordinary parser, it has a built-in timer which regulates the amount of time allowed for parsing any one sentence." ></td>
	<td class="line x" title="57:196	If a parse is not returned before the allotted time elapses, the parser enters the skip-and-fit mode in which it will try to 'fit' the parse." ></td>
	<td class="line x" title="58:196	While in the skip-and-fit mode, the parser will attempt to forcibly reduce incomplete constituents, possibly skipping portions of input in order to restart processing at a next unattempted constituent." ></td>
	<td class="line x" title="59:196	In other words, the parser will favor reduction to backtracking while in the skip-and-fit mode." ></td>
	<td class="line x" title="60:196	The result of this strategy is an approximate parse, partially fitted using top-down predictions." ></td>
	<td class="line x" title="61:196	The flagments skipped in the first pass are not thrown out, instead they are analyzed by a simple phrasal parser that looks for noun phrases and relative clauses and then attaches the recovered material to the main parse structure." ></td>
	<td class="line x" title="62:196	As an illustration, consider the following sentence taken from the CACM-3204 corpus: 3 These include CACM-3204, MUC-3, and a selection of nearly 6,000 technical articles extracted from Computer Library database (a Ziff Communications Inc. CD-ROM)." ></td>
	<td class="line x" title="63:196	4 A complete description can be found in (Strzalkowski, 1992)." ></td>
	<td class="line x" title="64:196	105 The method is illustrated by the automatic construction of beth recursive and iterative programs opera~-tg on natural numbers, lists, and trees, in order to construct a program satisfying certain specifications a theorem induced by those specifications is proved, and the desired program is extracted from the proof." ></td>
	<td class="line x" title="65:196	The italicized fragment is likely to cause additional complications in parsing this lengthy string, and the parser may be better off ignoring this fragment altogether." ></td>
	<td class="line x" title="66:196	To do so successfully, the parser must close the currently open constituent (i.e. , reduce a program satisfying certain specifications to NP), and possibly a few of its parent constituents, removing corresponding productions from further consideration, until an appropriate production is reactivated." ></td>
	<td class="line x" title="67:196	In this case, TIP may force the following reductions: SI ---> to V NP; SA --~ SI; S -~ NP V NP SA, until the production S --+ S and S is reached." ></td>
	<td class="line x" title="68:196	Next, the parser skips input to lind and, and resumes normal processing." ></td>
	<td class="line x" title="69:196	As may be expected, the skip-and-fit strategy will only be effective if the input skipping can be performed with a degree of determinism." ></td>
	<td class="line x" title="70:196	This means that most of the lexical level ambiguity must be removed from the input text, prior to parsing." ></td>
	<td class="line x" title="71:196	We achieve this using a stochastic parts of speech tagger 5 to preprocess the text." ></td>
	<td class="line x" title="72:196	WORD SUFFIX TRIMMER Word stemming has been an effective way of improving document recall since it reduces words to their common morphological root, thus allowing more successful matches." ></td>
	<td class="line x" title="73:196	On the other hand, stemming tends to decrease retrieval precision, if care is not taken to prevent situations where otherwise unrelated words are reduced to the same stem." ></td>
	<td class="line x" title="74:196	In our system we replaced a traditional morphological stemmer with a conservative dictionary-assisted suffix trimmer." ></td>
	<td class="line x" title="75:196	6 The suffix trimmer performs essentially two tasks: (1) it reduces inflected word forms to their root forms as specified in the dictionary, and (2) it converts nominalized verb forms (eg." ></td>
	<td class="line x" title="76:196	'implementation', 'storage') to the root forms of corresponding verbs (i.e. , 'implement', 'store')." ></td>
	<td class="line x" title="77:196	This is accomplished by removing a standard suffix, eg." ></td>
	<td class="line x" title="78:196	'stor+age', replacing it with a standard root ending ('+e'), and checking the newly created word against the dictionary, i.e., we check whether the new root ('store') is indeed a legal word, and whether the original root ('storage') s Courtesy of Bolt Beranek and Newman." ></td>
	<td class="line x" title="79:196	We use Oxford Advanced Learner's Dictionary (OALD)." ></td>
	<td class="line x" title="80:196	is defined using the new root ('store') or one of its standard inflexional forms (e.g. , 'storing')." ></td>
	<td class="line x" title="81:196	For example, the following definitions are excerpted from the Oxford Advanced Learner's Dictionary (OALD): storage n \[U\] (space used for, money paid for) the storing of goods  diversion n \[U\] diverting  procession n \[C\] number of persons, vehicles, ete moving forward and following each other in an orderly way." ></td>
	<td class="line x" title="82:196	Therefore, we can reduce 'diversion' to 'divert' by removing the suffix '+sion' and adding root form suffix '+t'." ></td>
	<td class="line x" title="83:196	On the other hand, 'process+ion' is not reduced to 'process'." ></td>
	<td class="line x" title="84:196	Experiments with CACM-3204 collection show an improvement in retrieval precision by 6% to 8% over the base system equipped with a standard morphological stemmer (in our case, the SMART stemmer)." ></td>
	<td class="line x" title="85:196	HEAD-MODIFIER STRUCTURES Syntactic phrases extracted from TIP parse trees are head-modifier pairs: from simple word pairs to complex nested structures." ></td>
	<td class="line x" title="86:196	The head in such a pair is a central element of a phrase (verb, main noun, etc)." ></td>
	<td class="line x" title="87:196	while the modifier is one of the adjunct arguments of the head." ></td>
	<td class="line x" title="88:196	7 For example, the phrase fast algorithm for parsing context-free languages yields the following pairs: algorithm+fast, algorithm+parse, parse+language, language+context.free." ></td>
	<td class="line x" title="89:196	The following types of pairs were considered: (1) a head noun and its left adjective or noun adjunct, (2) a head noun and the head of its right adjunct, (3) the main verb of a clause and the head of its object phrase, and (4) the head of the subject phrase and the main verb, These types of pairs account for most of the syntactic variants for relating two words (or simple phrases) into pairs carrying compatible semantic content." ></td>
	<td class="line x" title="90:196	For example, the pair retrieve+information is extracted from any of the following fragments: information retrieval system; retrieval of information from databases; and information that can be retrieved by a user-controlled interactive search process." ></td>
	<td class="line x" title="91:196	An example is shown in Figure 1." ></td>
	<td class="line x" title="92:196	g One difficulty in obtaining head-modifier 7 In the experiments reported here we extracted headmodifier word pairs only." ></td>
	<td class="line x" title="93:196	CACM collection is too small to warrant generation of larger compounds, because of their low frequencies." ></td>
	<td class="line x" title="94:196	s Note that working with the parsed text ensures a high degree of precision in capturing the meaningful phrases, which is especially evident when compared with the results usually obtained from either unprocessed or only partially processed text (Lewis and Croft, 1990)." ></td>
	<td class="line x" title="95:196	106 SENTENCE: The techniques are discussed and related to a general tape manipulation routine." ></td>
	<td class="line x" title="96:196	PARSE STRUCTURE: \[\[be\], \[\[verb,\[and,\[discuss\],\[relate\]\]\], \[subject,anyone\], \[object,\[np,\[n,technique\],\[tpos,the\]\]\], \[to,\[np,\[n,routine\],\[t_pos,a\],\[adj,\[general\]\], \[n__pos,\[np,\[n,manipulation\]\] \], \[n._pos,\[np,\[n,tape\]\]\]\]\]\]\]." ></td>
	<td class="line x" title="97:196	EXTRACTED PAIRS: \[discuss,technique\], \[relate,technique\], \[routine,general\], \[routine,manipulate\], \[manipulate,tape\] Figure 1." ></td>
	<td class="line x" title="98:196	Extraction of syntactic pairs." ></td>
	<td class="line x" title="99:196	pairs of highest accuracy is the notorious ambiguity of nominal compounds." ></td>
	<td class="line x" title="100:196	For example, the phrase natural language processing should generate language+natural and processing+language, while dynamic information processing is expected to yield processing+dynamic and processing+information." ></td>
	<td class="line x" title="101:196	Since our parser has no knowledge about the text domain, and uses no semantic preferences, it does not attempt to guess any internal associations within such phrases." ></td>
	<td class="line x" title="102:196	Instead, this task is passed to the pair extractor module which processes ambiguous parse structures in two phases." ></td>
	<td class="line x" title="103:196	In phase one, all and only unambiguous head-modifier pairs are extracted, and frequencies of their occurrence are recorded." ></td>
	<td class="line x" title="104:196	In phase two, frequency information of pairs generated in the first pass is used to form associations from ambiguous structures." ></td>
	<td class="line x" title="105:196	For example, if language+natural has occurred unambiguously a number times in contexts such as parser for natural language, while processing+natural has occurred significantly fewer times or perhaps none at all, then we will prefer the former association as valid." ></td>
	<td class="line x" title="106:196	TERM CORRELATIONS FROM TEXT Head-modifier pairs form compound terms used in database indexing." ></td>
	<td class="line x" title="107:196	They also serve as occurrence contexts for smaller terms, including single-word terms." ></td>
	<td class="line x" title="108:196	In order to determine whether such pairs signify any important association between terms, we calculate the value of the Informational Contribution (IC) function for each element in a pair." ></td>
	<td class="line x" title="109:196	Higher values indicate stronger association, and the element having the largest value is considered semantically dominant." ></td>
	<td class="line x" title="110:196	107 The connection between the terms cooccurrences and the information they are transmitting (or otherwise, their meaning) was established and discussed in detail by Harris (1968, 1982, 1991) as fundamental for his mathematical theory of language." ></td>
	<td class="line x" title="111:196	This theory is related to mathematical information theory, which formalizes the dependencies between the information and the probability distribution of the given code (alphabet or language)." ></td>
	<td class="line x" title="112:196	As stated by Shannon (1948), information is measured by entropy which gives the capacity of the given code, in terms of the probabilities of its particular signs, to transmit information." ></td>
	<td class="line x" title="113:196	It should be emphasized that, according to the information theory, there is no direct relation between information and meaning, entropy giving only a measure of what possible choices of messages are offered by a particular language." ></td>
	<td class="line x" title="114:196	However, it offers theoretic foundations of the correlation between the probability of an event and transmitted information, and it can be further developed in order to capture the meaning of a message." ></td>
	<td class="line x" title="115:196	There is indeed an inverse relation between information contributed by a word and its probability of occurrence p, that is, rare words carry more information than common ones." ></td>
	<td class="line x" title="116:196	This relation can be given by the function -log p (x) which corresponds to information which a single word is contributing to the entropy of the entire language." ></td>
	<td class="line x" title="117:196	In contrast to information theory, the goal of the present study is not to calculate informational capacities of a language, but to measure the relative strength of connection between the words in syntactic pairs." ></td>
	<td class="line x" title="118:196	This connection corresponds to Harris' likelihood constraint, where the likelihood of an operator with respect to its argument words (or of an argument word in respect to different operators) is defined using word-combination frequencies within the linguistic dependency structures." ></td>
	<td class="line x" title="119:196	Further, the likelihood of a given word being paired with another word, within one operator-argument structure, can be expressed in statistical terms as a conditional probability." ></td>
	<td class="line x" title="120:196	In our present approach, the required measure had to be uniform for all word occurrences, covering a number of different operator-argument structures." ></td>
	<td class="line x" title="121:196	This is reflected by an additional dispersion parameter, introduced to evaluate the heterogeneity of word associations." ></td>
	<td class="line x" title="122:196	The resulting new formula IC (x, \[x,y \]) is based on (an estimate of) the conditional probability of seeing a word y to the right of the word x, modified with a dispersion parameter for x. lC(x, \[x,y \]) f~'Y nx + dz -1 where f~,y is the frequency of \[x,y \] in the corpus, n~ is the number of pairs in which x occurs at the same position as in \[x,y\], and d(x) is the dispersion parameter understood as the number of distinct words with which x is paired." ></td>
	<td class="line x" title="123:196	When IC(x, \[x,y \]) = 0, x and y never occur together (i.e. , f~.y=0); when IC(x, \[x,y \]) = 1, x occurs only with y (i.e. , fx,y = n~ and dx = 1)." ></td>
	<td class="line x" title="124:196	So defined, IC function is asymmetric, a properry found desirable by Wilks et al.(1990) in their study of word co-occurrences in the Longman dictionary." ></td>
	<td class="line oc" title="126:196	In addition, IC is stable even for relatively low frequency words, which can be contrasted with Fano's mutual information formula recently used by Church and Hanks (1990) to compute word cooccurrence patterns in a 44 million word corpus of Associated Press news stories." ></td>
	<td class="line o" title="127:196	They noted that while generally satisfactory, the mutual information formula often produces counterintuitive results for lowfrequency data." ></td>
	<td class="line x" title="128:196	This is particularly worrisome for relatively smaller IR collections since many important indexing terms would be eliminated from consideration." ></td>
	<td class="line x" title="129:196	A few examples obtained from CACM3204 corpus are listed in Table 1." ></td>
	<td class="line x" title="130:196	IC values for terms become the basis for calculating term-to-term similarity coefficients." ></td>
	<td class="line x" title="131:196	If two terms tend to be modified with a number of common modifiers and otherwise appear in few distinct contexts, we assign them a similarity coefficient, a real number between 0 and 1." ></td>
	<td class="line x" title="132:196	The similarity is determined by comparing distribution characteristics for both terms within the corpus: how much information contents do they carry, do their information contribution over contexts vary greatly, are the common contexts in which these terms occur specific enough?" ></td>
	<td class="line x" title="133:196	In general we will credit high-contents terms appearing in identical contexts, especially if these contexts are not too commonplace." ></td>
	<td class="line x" title="134:196	9 The relative similarity between two words Xl and x2 is obtained using the following formula (a is a large constant): l0 SIM (x l,x2) = log (or ~, simy(x t,x2)) y where simy(x 1,x2) = MIN (IC (x 1, \[x I,Y \]),IC (x2, \[x 2,Y \])) * (IC(y, \[xt,y\]) +IC(,y, \[x2,y\])) The similarity function is further normalized with respect to SIM(xl,xl)." ></td>
	<td class="line x" title="135:196	It may be worth pointing out that the similarities are calculated using term co9 It would not be appropriate to predict similarity between language and logarithm on the basis of their co-occurrence with naturaL to This is inspired by a formula used by Hindie (1990), and subsequently modified to take into account the asymmetry of IC meab-'ure." ></td>
	<td class="line x" title="136:196	word head+modifier IC coeff." ></td>
	<td class="line x" title="137:196	distribute normal minimum relative retrieve inform size medium editor text system parallel read character implicate legal system distribute make recommend infer deductive share resource distribute+normal distribute+normal minimum+relative minimum+relative retrieve +inform retrieve+inform size +medium size+medium editor+text editor+text system+parallel system+parallel read+character read+character implicate+legal implicate+legal system+distribute system+distribute make+recommend make+recommend infer+deductive infer+deductive share +resource share+resource 0.040 0.115 0.200 0.016 0.086 0.004 0.009 0.250 0.142 0.025 0.001 0.014 0.023 0.007 0.035 0.083 0.002 0.037 0.024 0.142 0.095 0.142 0.054 0.042 Table 1." ></td>
	<td class="line x" title="138:196	IC coefficients obtained from CACM-3204 occurrences in syntactic rather than in document-size contexts, the latter being the usual practice in nonlinguistic clustering (eg." ></td>
	<td class="line x" title="139:196	Sparck Jones and Barber, 1971; Crouch, 1988; Lewis and Croft, 1990)." ></td>
	<td class="line x" title="140:196	Although the two methods of term clustering may be considered mutually complementary in certain situations, we believe that more and stronger associations can be obtained through syntactic-context clustering, given sufficient amount of data and a reasonably accurate syntactic parser." ></td>
	<td class="line x" title="141:196	~ QUERY EXPANSION Similarity relations are used to expand user queries with new terms, in an attempt to make the n Non-syntactic contexts cross sentence boundaries with no fuss, which is helpful with short, succinct documc~nts (such as CACM abstracts), but less so with longer texts; sec also (Grishman et al. , 1986)." ></td>
	<td class="line x" title="142:196	108 final search query more comprehensive (adding synonyms) and/or more pointed (adding specializations)." ></td>
	<td class="line x" title="143:196	12 It follows that not all similarity relations will be equally useful in query expansion, for instance, complementary relations like the one between algol and fortran may actually harm system's performance, since we may end up retrieving many irrelevant documents." ></td>
	<td class="line x" title="144:196	Similarly, the effectiveness of a query containing fortran is likely to diminish if we add a similar but far more general term such as language." ></td>
	<td class="line x" title="145:196	On the other hand, database search is likely to miss relevant documents if we overlook the fact that for." ></td>
	<td class="line x" title="146:196	tran is a programming language, or that interpolate is a specification of approximate." ></td>
	<td class="line x" title="147:196	We noted that an average set of similarities generated from a text corpus contains about as many 'good' relations (synonymy, specialization) as 'bad' relations (antonymy, complementation, generalization), as seen from the query expansion viewpoint." ></td>
	<td class="line x" title="148:196	Therefore any attempt to separate these two classes and to increase the proportion of 'good' relations should result in improved retrieval." ></td>
	<td class="line x" title="149:196	This has indeed been confirmed in our experiments where a relatively crude filter has visibly increased retrieval precision." ></td>
	<td class="line x" title="150:196	In order to create an appropriate filter, we expanded the IC function into a global specificity measure called the cumulative informational contribution function (ICW)." ></td>
	<td class="line x" title="151:196	ICW is calculated for each term across all contexts in which it occurs." ></td>
	<td class="line x" title="152:196	The general philosophy here is that a more specific word/phrase would have a more limited use, i.e., would appear in fewer distinct contexts." ></td>
	<td class="line x" title="153:196	ICW is similar to the standard inverted document frequency (idf) measure except that term frequency is measured over syntactic units rather than document size units./3 Terms with higher ICW values are generally considered more specific, but the specificity comparison is only meaningful for terms which are already known to be similar." ></td>
	<td class="line x" title="154:196	The new function is calculated according to the following formula: ICt.(w) if both exist ICR(w) ICW(w)=I~R(w) otherwiseifnly ICR(w)exists n Query expansion (in the sense considered here, though not quite in the same way) has been used in information retrieval research before (eg." ></td>
	<td class="line x" title="155:196	Sparck Jones and Tait, 1984; Harman, 1988), usually with mixed results." ></td>
	<td class="line x" title="156:196	An alternative is to use tenm clusters to create new terms, 'metaterms', and use them to index the database instead (eg." ></td>
	<td class="line x" title="157:196	Crouch, 1988; Lewis and Croft, 1990)." ></td>
	<td class="line x" title="158:196	We found that the query expansion approach gives the system more flexibility, for instance, by making room for hypertext-style topic exploration via user feedback." ></td>
	<td class="line x" title="159:196	t3 We believe that measuring term specificity over document-size contexts (eg." ></td>
	<td class="line x" title="160:196	Sparck Jones, 1972) may not be appropriate in this case." ></td>
	<td class="line x" title="161:196	In particular, syntax-based contexts allow for where (with n~, d~ > 0): 14 n~ ICL(W) = IC (\[w,_ \]) d~(n~+d~-l) n~ ICR(w) = IC (\[_,w \]) = d~(n~+d~-l) For any two terms wl and w2, and a constant 8 > 1, if ICW(w2)>8* ICW(wl) then w2 is considered more specific than ' wl." ></td>
	<td class="line x" title="162:196	In addition, if SIMno,,(wl,w2)=~> O, where 0 is an empirically established threshold, then w2 can be added to the query containing term wl with weight ~.14 In the CACM-3204 collection: ICW (algol) = 0.0020923 ICW(language) = 0.0000145 ICW(approximate) = 0.0000218 ICW (interpolate) = 0.0042410 Therefore interpolate can be used to specialize approximate, while language cannot be used to expand algol." ></td>
	<td class="line x" title="163:196	Note that if 8 is well chosen (we used 8=10), then the above filter will also help to reject antonymous and complementary relations, such as SIM~o,~(pl_i, cobol)=0.685 with ICW (pl_i)=O.O175 and ICW(cobol)=O.0289." ></td>
	<td class="line x" title="164:196	We continue working to develop more effective filters." ></td>
	<td class="line x" title="165:196	Examples of filtered similarity relations obtained from CACM-3204 corpus (and their sim values): abstract graphical 0.612; approximate interpolate 0.655; linear ordinary 0.743; program translate 0.596; storage buffer 0.622." ></td>
	<td class="line x" title="166:196	Some (apparent)?" ></td>
	<td class="line x" title="167:196	failures: active digital 0.633; efficient new 0.580; gamma beta 0.720." ></td>
	<td class="line x" title="168:196	More similarities are listed in Table 2." ></td>
	<td class="line x" title="169:196	SUMMARY OF RESULTS The preliminary series of experiments with the CACM-3204 collection of computer science abstracts showed a consistent improvement in performance: the average precision increased from 32.8% to 37.1% (a 13% increase), while the normalized recall went from 74.3% to 84.5% (a 14% increase), in comparison with the statistics of the base NIST system." ></td>
	<td class="line x" title="170:196	This improvement is a combined effect of the new stemmer, compound terms, term selection in queries, and query expansion using filtered similarity relations." ></td>
	<td class="line x" title="171:196	The choice of similarity relation filter has been found critical in improving retrieval precision through query expansion." ></td>
	<td class="line x" title="172:196	It should also be pointed out that only about 1.5% of all similarity relations originally generated from CACM-3204 were found processing texts without any internal document structure." ></td>
	<td class="line x" title="173:196	14 The filter was most effective at o = 0.57." ></td>
	<td class="line x" title="174:196	109 wordl word2 SIMnorm *aim algorithm *adjacency *algebraic *american assert *buddy committee critical best-fit * duplex earlier encase give incomplete lead mean method memory match lower progress range round-off remote purpose method pair symbol standard infer time-share *symposium fmal first-fit reliable previous minimum-area present miss *trail *standard technique storage recognize upper *trend variety truncate teletype 0.434 0.529 0.499 0.514 0.719 0.783 0.622 0.469 0.680 0.871 0.437 0.550 0.991 0.458 0.850 0.890 0.634 0.571 0.613 0.563 0.841 0.444 0.600 0.918 0.509 Table 2." ></td>
	<td class="line x" title="175:196	Filtered word similarities (* indicates the more specific term)." ></td>
	<td class="line x" title="176:196	admissible after filtering, contributing only 1.2 expansion on average per query." ></td>
	<td class="line x" title="177:196	It is quite evident significantly larger corpora are required to produce more dramatic results." ></td>
	<td class="line x" title="178:196	15 ~6 A detailed summary is given in Table 3 below." ></td>
	<td class="line x" title="179:196	These results, while quite modest by IR stundards, are significant for another reason as well." ></td>
	<td class="line x" title="180:196	They were obtained without any manual intervention into the database or queries, and without using any other ts KL Kwok (private communication) has suggested that the low percentage of admissible relations might be similar to the phenomenon of 'tight dusters' which while meaningful are so few that their impact is small." ></td>
	<td class="line x" title="181:196	:s A sufficiently large text corpus is 20 million words or more." ></td>
	<td class="line x" title="182:196	This has been paRially confirmed by experiments performed at the University of Massachussetts (B. Croft, private comrnunicadon)." ></td>
	<td class="line x" title="183:196	110 base surf.trim Tests query exp. Recall Precision 0.764 0.674 0.547 0.449 0.387 0.329 0.273 0.198 0.146 0.093 0.079 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 0.775 0.688 0.547 0.479 0A21 0.356 0.280 0.222 0.170 0.112 0.087 0.793 0.700 0.573 0.486 0.421 0.372 0.304 0.226 0.174 0.114 0.090 Avg." ></td>
	<td class="line x" title="184:196	Prec." ></td>
	<td class="line x" title="185:196	0.328 0.356 0.371 % change 8.3 13.1 Norm Rec." ></td>
	<td class="line x" title="186:196	0.743 0.841 0.842 Queries 50 50 50 Table 3." ></td>
	<td class="line x" title="187:196	Recall/precision statistics for CACM-3204 information about the database except for the text of the documents (i.e. , not even the hand generated keyword fields enclosed with most documents were used)." ></td>
	<td class="line x" title="188:196	Lewis and Croft (1990), and Croft et al.(1991) report results similar to ours but they take advantage of Computer Reviews categories manually assigned to some documents." ></td>
	<td class="line x" title="190:196	The purpose of this research is to explore the potential of automated NLP in dealing with large scale IR problems, and not necessarily to obtain the best possible results on any particular data collection." ></td>
	<td class="line x" title="191:196	One of our goals is to point a feasible direction for integrating NLP into the traditional IR." ></td>
	<td class="line x" title="192:196	ACKNOWLEDGEMENTS We would like to thank Donna Harman of NIST for making her IR system available to us." ></td>
	<td class="line x" title="193:196	We would also like to thank Ralph Weischedel, Marie Meteer and Heidi Fox of BBN for providing and assisting in the use of the part of speech tagger." ></td>
	<td class="line x" title="194:196	KL Kwok has offered many helpful comments on an earlier draft of this paper." ></td>
	<td class="line x" title="195:196	In addition, ACM has generously provided us with text data from the Computer Library database distributed by Ziff Communications Inc. This paper is based upon work suppened by the Defense Advanced Research Project Agency under Contract N00014-90-J-1851 from the Office of Naval Research, the National Science Foundation under Grant 1RI-89-02304, and a grant from the Swiss National Foundation for Scientific Research." ></td>
	<td class="line x" title="196:196	We also acknowledge a support from Canadian Institute for Robotics and Intelligent Systems (IRIS)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P92-1052
SEXTANT: Exploring Unexplored Contexts For Semantic Extraction From Syntactic Analysis
Grefenstette, Gregory;"></td>
	<td class="line x" title="1:57	SEXTANT: EXPLORING UNEXPLORED CONTEXTS FOR SEMANTIC EXTRACTION FROM SYNTACTIC ANALYSIS Gregory Grefenstette Computer Science Department, University of Pittsburgh, Pittsburgh, PA 15260 grefen@cs.pitt.edu Abstract For a very long time, it has been considered that the only way of automatically extracting similar groups of words from a text collection for which no semantic information exists is to use document co-occurrence data." ></td>
	<td class="line x" title="2:57	But, with robust syntactic parsers that are becoming more frequently available, syntactically recognizable phenomena about word usage can be confidently noted in large collections of texts." ></td>
	<td class="line x" title="3:57	We present here a new system called SEXTANT which uses these parsers and the finer-grained contexts they produce to judge word similarity." ></td>
	<td class="line x" title="4:57	BACKGROUND Many machine-based approaches to term similarity, such as found in TItUMP (Jacobs and Zernick 1988) and FERRET (Mauldin 1991), can be characterized as knowledge-rich in that they presuppose that known lexical items possess Conceptual Dependence(CD)like descriptions." ></td>
	<td class="line x" title="5:57	Such an approach necessitates a great amount of manual encoding of semantic information and suffers from the drawbacks of cost (in terms of initial coding, coherence checking, maintenance after modifications, and costs derivable from a host of other software engineering concern); of domain dependence (a semantic structure developed for one domain would not be applicable to another." ></td>
	<td class="line x" title="6:57	For example, sugar would have very different semantic relations in a medical domain than in a commodities exchange domain); and of rigidity (even within wellestablished domain, new subdomains spring up, e.g. AIDS." ></td>
	<td class="line x" title="7:57	Can hand-coded systems keep up with new discoveries and new relations with an acceptable latency)?" ></td>
	<td class="line x" title="8:57	In the Information Retrieval community." ></td>
	<td class="line x" title="9:57	researchers have consistently considered that 324 'the linguistic apparatus required for effective domain-independent analysis is not yet at hand,' and have concentrated on counting document co-occurrence statistics (Peat and Willet 1991), based on the idea that words appearing in the same document must share some semantic similarity." ></td>
	<td class="line x" title="10:57	But document cooccurrence suffers from two problems: granulaxity (every word in the document is considered potentially related to every other word, no matter what the distance between them) and co-occurrence (for two words to be seen as similar they must physically appear in the same document." ></td>
	<td class="line x" title="11:57	As an illustration, consider the words tumor and turnout." ></td>
	<td class="line x" title="12:57	These words certainly share the same contexts, but would never appear in the same document)." ></td>
	<td class="line x" title="13:57	In general different words used to describe similar concepts might not be used in the same document, and are missed by these methods." ></td>
	<td class="line x" title="14:57	Recently, a middle ground between these two approaches has begun to be broken." ></td>
	<td class="line oc" title="15:57	Researchers such as (Evans et al. 1991) and (Church and Hanks 1990) have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb, verb-object pairs." ></td>
	<td class="line x" title="16:57	(Hearst 1992) has shown that certain lexical-syntactic templates can reliably extract hyponym relations from text." ></td>
	<td class="line x" title="17:57	(Ruge 1991) shows that modifier-head relations in noun phrases extracted from a large corpus provide a useful context for extracting similar words." ></td>
	<td class="line x" title="18:57	The common thread of all these techniques is that they require no hand-coded domain knowledge, but they examine more cleanly defined contexts than simple document co-occurrence methods." ></td>
	<td class="line x" title="19:57	Similarly, our SEXTANT 1 uses finegrained syntactically derived contexts, but derives its measures of similarity from considerI Semantic EXtraction from Text via Analyzed Networks of Terms ing not the co-occurrence of two words in the same context, but rather the overlapping of all the contexts associated with words over an entire corpus." ></td>
	<td class="line x" title="20:57	Calculation of the amount of shared weighted contexts produces a similarity measure between two words." ></td>
	<td class="line x" title="21:57	SEXTANT SEXTANT can be run on any English text, without any pre-coding of domain knowledge or manual editing of the text." ></td>
	<td class="line x" title="22:57	The input text passes through the following steps: (I) Morphological analysis." ></td>
	<td class="line x" title="23:57	Each word is morphologically analyzed and looked up in a 100,000 word dictionary to find its possible parts of speech." ></td>
	<td class="line x" title="24:57	(II) Grammatical Disambiguation." ></td>
	<td class="line x" title="25:57	A stochastic parser assigns one grammatical category to each word in the text." ></td>
	<td class="line x" title="26:57	These first two steps use CLARIT programs (Evans et al. 1991)." ></td>
	<td class="line x" title="27:57	(III) Noun and Verb Phrase Splitting." ></td>
	<td class="line x" title="28:57	Each sentence is divided into verb and noun phrases by a simple regular grammar." ></td>
	<td class="line x" title="29:57	(IV) Syntagmatic Relation Extraction." ></td>
	<td class="line x" title="30:57	A fourpass algorithm attaches modifiers to nouns, noun phrases to noun phrases and verbs to noun phrases." ></td>
	<td class="line x" title="31:57	(Grefenstette 1992a) (V) Context Isolation." ></td>
	<td class="line x" title="32:57	The modifying words attached to each word in the text are isolated for all nouns." ></td>
	<td class="line x" title="33:57	Thus the context of each noun is given by all the words with which it is associated throughout the corpus." ></td>
	<td class="line x" title="34:57	(VI) Similarity matching." ></td>
	<td class="line x" title="35:57	Contexts are compared by using similarity measures developed in the Social Sciences, such as a weighted Jaccard measure." ></td>
	<td class="line x" title="36:57	As an example, consider the following sentence extracted from a medical corpus." ></td>
	<td class="line x" title="37:57	Cyclophosphamide markedly prolonged induction time and suppressed peak titer irrespective of the time of antigen administration." ></td>
	<td class="line x" title="38:57	Each word is looked up in a online dictionary." ></td>
	<td class="line x" title="39:57	After grammatical ambiguities are removed by the stochastic parser, the phrase is divided into noun phrases(NP) and verb phrases(VP), giving, NP cyclophosphamide (sn) -markedly (adv) VP prolong (vt-past) NP induction (sn) time (sn) -and (cnj) VP suppress (vt-past) NP peak (sn) titer (sn) irrespective-of (prep) the (d) time (sn) of (prep) antigen (en) administration (sn) Once each sentence in the text is divided into phrases, intraand inter-phrase structural relations are extracted." ></td>
	<td class="line x" title="40:57	First noun phrases are scanned from left to right(NPLR), hooking up articles, adjectives and modifier nouns to their head nouns." ></td>
	<td class="line x" title="41:57	Then, noun phrases are scanned right to left(NPttL), connecting nouns over prepositions." ></td>
	<td class="line x" title="42:57	Then, starting from verb phrases, phrases are scanned before the verb phrase for an unconnected head which becomes the subject(VPRL), and likewise to the right of the verb for objects(VPLtt), producing for the example: VPRL cyclophosphamide, prolong < SUBJ NPRL time, induction < NN VPLR prolong, time < DOBJ VPRL cyclophosphamide, suppress < SUBJ NPRL titer, peak < NN VPLR suppress, titer < DOBJ NPLR titer, time < NNPREP NPRL administration, antigen < NN Next SEXTANT extracts a user specified set of relations that are considered as each word's context for similarity calculations." ></td>
	<td class="line x" title="43:57	For example, one set of relations extracted by SEXTANT for the above sentence can be cyclophosphamide prolong-SUBJ time induction time prolong-DOBJ cyclophosphamide suppress-SUBJ titer peak titer suppress-DOBJ titer time administration antigen time administration In this example, the word time is found modified by the words induction, prolong-DOBJ and administration, while administration is only considered by this set of relations to be modified by antigen." ></td>
	<td class="line x" title="44:57	Over the whole corpus of 160,000 words, one can consider what modifies administration." ></td>
	<td class="line x" title="45:57	Isolating these modifiers gives a list such as administration androgen administration antigen administration aortic administration examine administration associate-DOBJ administration aseociate-SUBJ administration azathioprine administration carbon-dioxide administration case administration cause-SUBJ At this point SEXTANT compares all the other words in the corpus, using a userspecified similarity measure such the Jaccard measure, to find which words are most similar to which others." ></td>
	<td class="line x" title="46:57	For example, the words found as most similar to administration in this medical corpus were the following words in order of most to least similar: 325 administration injection, treatment, therapy, infusion, dose, response,  As can be seen, the sense of administration as in the 'administration of drugs and medicines' is clearly extracted here, since administration in this corpus is most similarly used as other words such as injection and therapy having to do with dispensing drugs and medicines." ></td>
	<td class="line x" title="47:57	One of the interesting aspects of this approach, contrary to the coarse-grained document co-occurrence approach, is that administration and injection need never appear in the same document for them to be recognized as semantically similar." ></td>
	<td class="line x" title="48:57	In the case of this corpus, administration and injection were considered similar because they shared the following modifiers: acid follow-DOBJ growth prior produce-IOBJ dose extract increase-SUBJ intravenous treat-IOBJ associate-SUSJ associate-DOBJ rapid cause-SUBJ antigen adrenalectomy aortic hormone subside-IOBJ alter-IOBJ folio-acid amd folate It is hard to select any one word which would indicate that these two words were similar, but the fact that they do share so many words, and more so than other words, indicates that these words share close semantic characteristics in this corpus." ></td>
	<td class="line x" title="49:57	When the same procedure is run over a corpus of library science abstracts, administration is recognized as closest to administration graduate, office, campus, education, director,  Similarly circulation was found to be closest to flow in the medical corpus and to date in the library corpus." ></td>
	<td class="line x" title="50:57	Cause was found to be closest to etiology in the medical corpus and to determinant in the library corpus." ></td>
	<td class="line x" title="51:57	Frequently occurring words, possessing enough context, are generally ranked by SEXTANT with words intuitively related within the defining corpus." ></td>
	<td class="line x" title="52:57	DISCUSSION While finding similar words in a corpus without any domain knowledge is interesting in itself, such a tool is practically useful in a number of areas." ></td>
	<td class="line x" title="53:57	A lexicographer building a domain-specific dictionary would find such a tool invaluable, given a large corpus of representative text for that domain." ></td>
	<td class="line x" title="54:57	Similarly, a Knowledge Engineer creating a natural language interface to an expert system could use this system to cull similar terminology in a field." ></td>
	<td class="line x" title="55:57	We have shown elsewhere (Grefenstette 1992b), in an Information itetrieval setting, that expanding queries using the closest terms to query terms derived by SEXTANT can improve recall and precision." ></td>
	<td class="line x" title="56:57	We find that one of the most interesting results from a linguistic point of view, is the possibility automaticaUy creating corpus defined thesauri, as can be seen above in the differences between relations extracted from medical and from information science corpora." ></td>
	<td class="line x" title="57:57	In conclusion, we feel that this fine grained approach to context extraction from large corpora, and similarity calculation employing those contexts, even using imperfect syntactic analysis tools, shows much promise for the future." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H93-1049
Hypothesizing Word Association From Untagged Text
Matsukawa, Tomoyoshi;"></td>
	<td class="line x" title="1:159	HYPOTHESIZING UNTAGGED TEXT WORD ASSOCIATION FROM Tomoyoshi Matsukawa BBN Systems and Technologies 70 Fawcett St. Cambridge, MA 02138 ABSTRACT This paper reports a new method for suggesting word associations, based on a greedy algorithm that employs Chisquare statistics on joint frequencies of pairs of word groups compared against chance co-occurrence." ></td>
	<td class="line x" title="2:159	The benefits of this new approach are: 1) we can consider even low frequency words and word pairs, and 2) word groups and word associations can be automatically generated." ></td>
	<td class="line x" title="3:159	The method provided 87% accuracy in hypothesizing word associations for unobserved combinations of words in Japanese text." ></td>
	<td class="line x" title="4:159	1." ></td>
	<td class="line pc" title="5:159	INTRODUCTION Using mutual information for measuring word association has become popular since \[Church and Hanks, 1990\] defined word association ratio as mutual information between two words." ></td>
	<td class="line n" title="6:159	Word association ratios are a promising tool for lexicography, but there seem to be at least two limitations to the method: 1) much data with low frequency words or word pairs cannot be used and 2) generalization of word usage still depends totally on lexicographers." ></td>
	<td class="line o" title="7:159	In this paper, we propose an alternative (or extended) method for suggesting word associations using Chi-square statistics, which can be viewed as an approximation to mutual information." ></td>
	<td class="line oc" title="8:159	Rather than considering significance of joint frequencies of word pairs as \[Church and Hanks, 1990\] did, our algorithm uses joint frequencies of pairs of word groups instead." ></td>
	<td class="line x" title="9:159	The algorithm employs a hillclimbing search for a pair of word groups that occur significantly frequently." ></td>
	<td class="line x" title="10:159	The benefits of this new approach are: 1) that we can consider even low frequency words and word pairs, and 2) that word groups or word associations can be automatically generated,.namely automatic hypothesis of word associations, which can later be reviewed by a lexicographer." ></td>
	<td class="line x" title="11:159	3) word associations can be used in parsing and understanding natural language, as well as in natural language generation \[Smadja and McKeown, 1990\]." ></td>
	<td class="line x" title="12:159	Our method proved to be 87% accurate in hypothesizing word associations for unobserved combinations of words in Japanese text, where accuracy was tested by human verification of a random sample of hypothesized word pairs." ></td>
	<td class="line x" title="13:159	We extracted 14,407 observations of word co-occurrences, involving 3,195 nouns and 4,365 verb/argument pairs." ></td>
	<td class="line x" title="14:159	Out of this we hypothesized 7,050 word associations." ></td>
	<td class="line x" title="15:159	The corpus size was 280,000 words." ></td>
	<td class="line x" title="16:159	We would like to apply the same approach to English." ></td>
	<td class="line x" title="17:159	2." ></td>
	<td class="line x" title="18:159	RELATED WORK Some previous work (e.g. , \[Weischedel, et al. , 1990\]) found verb-argument associations from bracketed text, such as that in TREEBANK; however, this paper, and related work has hypothesized word associations from untagged text." ></td>
	<td class="line x" title="19:159	\[Hindle 1990\] confirmed that word association ratios can be used for measuring similarity between nouns." ></td>
	<td class="line x" title="20:159	For example, 'ship', 'plane', 'bus', etc. , were automatically ranked as similar to 'boat'." ></td>
	<td class="line x" title="21:159	\[Resnik 1992\] reported a word association ratio for identifying noun classes from a preexisting hierarchy as selectional constraints on the object of a verb." ></td>
	<td class="line x" title="22:159	\[Brown et.al. 1992\] proves that, under the assumption of a bi-gram class model, the perplexity of a corpus is minimized when the average mutual information between word classes is maximized." ></td>
	<td class="line x" title="23:159	Based on that fact, they cluster words via a greedy search algorithm which finds a local maximum in average mutual information." ></td>
	<td class="line o" title="24:159	Our algorithm considers joint frequencies of pairs of word groups (as \[Brown et." ></td>
	<td class="line oc" title="25:159	al. 1992\] does) in contrast to joint frequencies of word pairs as in \[Church and Hanks, 1990\] and \[Hindle 1990\]." ></td>
	<td class="line x" title="26:159	Here a word group means any subset of the whole set of words." ></td>
	<td class="line x" title="27:159	For example, 'ship,' 'plane,' 'boat' and 'car' may be a word group." ></td>
	<td class="line x" title="28:159	The algorithm will find pairs of such word groups." ></td>
	<td class="line x" title="29:159	Another similarity to \[Brown et." ></td>
	<td class="line x" title="30:159	al. 1992\]'s clustering algorithm is the use of greedy search for a pair of word groups that occur significantly frequently, using an evaluation function based on mutual information between classes." ></td>
	<td class="line x" title="31:159	On the other hand, unlike \[Brown et." ></td>
	<td class="line x" title="32:159	al. 1992\], we assume some automatic syntactic analysis of the corpus, namely part-of-speech analysis and at least finite-state approximations to syntactic dependencies." ></td>
	<td class="line x" title="33:159	Moreover, the clustering is done depth first, not breadth first as \[Brown et." ></td>
	<td class="line x" title="34:159	248 al. 1992\], i.e., clusters are hypothesized one by one, not in parallel." ></td>
	<td class="line x" title="35:159	3." ></td>
	<td class="line x" title="36:159	OVERVIEW OF THE METHOD The method consists of three phases: 1) Automatic part of speech tagging of text." ></td>
	<td class="line x" title="37:159	First, texts are labeled by our probabilistic part of speech tagger (POST) which has been extended for Japanese morphological processing \[Matsukawa et." ></td>
	<td class="line x" title="38:159	al. 1993\]." ></td>
	<td class="line x" title="39:159	This is fully automatic; human review is not necessary under the assumption that the tagger has previously been trained on appropriate text \[Meteer et." ></td>
	<td class="line x" title="40:159	al. 1991\] 1 2) Finite state pattern matching." ></td>
	<td class="line x" title="41:159	Second, a finite-state pattern matcher with patterns representing possible grammatical relations, such as verb/argument pairs, nominal compounds, etc. is run over the sample text to suggest word pairs which will be considered candidates for word associations." ></td>
	<td class="line x" title="42:159	As a result, we get a word cooccurrence matrix." ></td>
	<td class="line x" title="43:159	Again, no human review of the pattern matching is assumed." ></td>
	<td class="line x" title="44:159	3) Filtering/Generalization of word associations via Chi-square." ></td>
	<td class="line x" title="45:159	Third, given the word co-occurrence matrix, the program starts from an initial pair of word groups (or a submatrix in the matrix), incrementally adding into the submatrix a word which locally gives the highest Chisquare score to the submatrix." ></td>
	<td class="line x" title="46:159	Finally, words are removed which give a higher Chisquare score by their removal." ></td>
	<td class="line x" title="47:159	By adding and removing words until reaching an appropriate significance level, we get a submatrix as a hypothesis of word associations between the cluster of words represented as rows in the submatrix and the cluster of words represented as columns in the submatrix." ></td>
	<td class="line x" title="48:159	4 WORD SEGMENTATION AND PART OF SPEECH LABELING 1 In our experience thus far in three domains and in both Japanese and English, while retraining POST on domainspecific data would reduce the error rate, the effect on overall performance of the system in data extraction from text has been small enough to make retraining unnecessary." ></td>
	<td class="line x" title="49:159	The effect of domain-specific lexical entries (e.g. , DRAM is a noun in microelectronics) often mitigates the need to retrain." ></td>
	<td class="line x" title="50:159	Since in Japanese word separators such as spaces are not present, words must be segmented before we assign part of speech to words." ></td>
	<td class="line x" title="51:159	To do this, we use JUMAN from Kyoto University to segment Japanese text into words, AMED, an example-based segmentation corrector, and a Hidden Markov Model (POST) \[Matsukawa, et." ></td>
	<td class="line x" title="52:159	al. 1993\]." ></td>
	<td class="line x" title="53:159	For example, POST processes an input text such as the following: and produces tagged text such as: 2 --~j/CONJ, /TT ~/CN ~/CN ~,~/rM ~_:~)~:~.,/PN O/NCM ~ ~/SN ::\[~,~/CN '~/CM L./ADV." ></td>
	<td class="line x" title="55:159	/IT ~/CN IJCN ~/NCM ~'~#2/ADJ ~jCN Q)/NCM .Jzff/./CN ~\[Iii~/CN I/~'T/CN ~/NCM ~_ ~/FN '~/PT, \[FT ~ 2\[s;/PN ~TPT ~/NCM ~/CM ~\]~t-~ ~/VB ~,~$/VSUF  /KT 5." ></td>
	<td class="line x" title="56:159	FINITE STATE PATTERN MATCHING We use the following finite state patterns for extracting possible Japanese verb/argument word co-occurrences from automatically segmented and tagged Japanese text." ></td>
	<td class="line x" title="57:159	Completely different patterns would be used for English." ></td>
	<td class="line x" title="58:159	PN PT ''' SN SN where CN = common noun PN = proper name SN = Sa-inflection noun (nominal verb) CM = case marker (-nom/-acc argument) PT = particle (other arguments) VB = verb Here, the first part (CN, PN or SN) represents a noun." ></td>
	<td class="line x" title="59:159	Since in Japanese the head noun of a noun phrase is always at the right end of the phrase, this part should always match a head noun." ></td>
	<td class="line x" title="60:159	The second part (CM or PT) represents a postposition which identifies an argument of a verb." ></td>
	<td class="line x" title="61:159	The final pattern element (VB or SN) represents a verb." ></td>
	<td class="line x" title="62:159	Sainflection nouns (SN) are nominalized verbs which form a verb phrase with the morpheme 'suru'." ></td>
	<td class="line x" title="63:159	2 CONJ = conjunction; Tr = Japanese comma; CN = common noun; TM = Topic marker; PN proper noun; etc. 249 Distance 0 1 2 4 Matched Text g~|/CN \]j-t~/CN ~:'~_/CN k/PT ~J~/'SN xJ-&/VB ~ ~/FN '~TPT  7 9 MX/PN 0)/NCM ~_~/CN ~/PT ~a~/CN ~i~}~t~/SN ~/CM L~NB  ~I\]Z~/ADJ ~:~t/CN ~/'PT '~'/~--b/CN \]J/CM ~i~l~/SN L~NB  ~-/CN -~/CN ~/CN ~/PT ~/CN ~/NNSU~--F/CN ~/NCM ~j #/ON ~ig~/SN ~/NCM Figure 1: Examples of Pattern Matches with Skipping over Words." ></td>
	<td class="line x" title="64:159	Since argument structure in Japanese is marked by postpositions, i.e., case markers (i.e. , 'o,' 'ga') and partic?,es (e.g. , 'ni,' 'kara,' . . .), word combinations matched with the patterns will represent associations between a noun filling a particular argument type (e.g. , 'o') and a verb." ></td>
	<td class="line x" title="65:159	Note that topic markers (TM; i.e., 'wa') and toritate markers (TTM; e.g.'mo', 'sae', ) are not included in the pattern since these do not uniquely identify the case of the argument." ></td>
	<td class="line x" title="66:159	Just as in English, the arguments of a verb in Japanese may be quite distant from the verb; adverbial phrases and scrambling are two cases that may separate a verb from its argument(s)." ></td>
	<td class="line x" title="67:159	We approximate this in a finite state machine by allowing words to be skipped." ></td>
	<td class="line x" title="68:159	In our experiment, up to four words could be skipped." ></td>
	<td class="line x" title="69:159	As shown in Figure 1, matching an argument structure varies from distance 0 to 4." ></td>
	<td class="line x" title="70:159	By limiting the algorithm to a maximum of four word gaps, and by not considering the ambiguous cases of topic markers and taritate markers, we have chosen to limit the cases considered in favor of high accuracy in automatically hypothesizing word associations." ></td>
	<td class="line x" title="71:159	\[Brent, 1991\] similarly limited what his algorithm could learn in favor of high accuracy." ></td>
	<td class="line x" title="72:159	6." ></td>
	<td class="line x" title="73:159	FILTERING AND GENERALIZATION VIA CHI-SQUARE Word combinations found via the finite state patterns include a noun, postposition, and a verb." ></td>
	<td class="line x" title="74:159	A two dimensional matrix (a word co-occurrence matrix) is formed, where the columns are nouns, and the rows are pairs of a verb plus postposifion." ></td>
	<td class="line x" title="75:159	The cells of the matrix are the frequency of the noun (column element) cooccurring in the given case with that verb (row element)." ></td>
	<td class="line x" title="76:159	Starting from a submatrix, the algorithm successively adds to the submatfix the word with the largest Chi-square score among all words outside the submatrix." ></td>
	<td class="line x" title="77:159	Words are added until a local maximum is reached." ></td>
	<td class="line x" title="78:159	Finally, the appropriateness of the submatrix as a hypothesis of word associations is checked with heuristic criteria based on the sizes of the row and the column of the submatrix." ></td>
	<td class="line x" title="79:159	Currently, we use the following criteria for appropriateness of a submatrix: LET 1 : size of row of submatfix m : size of column of submatrix C1, C2, C3 : parameters IF 1 > C1, and m > C1, and 1 > C2 or m/l < C3, and m > C2 or l/m < C3 THEN the submatrix is appropriate." ></td>
	<td class="line x" title="80:159	For any submatrix found, the co-occurrence observations for the clustered words are removed from the word cooccurrence matrix and treated as a single column of clustered nouns and a single row of clustered verb plus case pairs." ></td>
	<td class="line x" title="81:159	Currently, we use the following values for the parameters: C1=2, C2=10, and C3=10." ></td>
	<td class="line x" title="82:159	Table 1." ></td>
	<td class="line x" title="83:159	shows an example of clustering starting from the initial submatrix shown in Figure 2." ></td>
	<td class="line x" title="84:159	The words in Figure 2 were manually selected as words meaning 'organization'." ></td>
	<td class="line x" title="85:159	In Table 1, the first (leftmost) column indicates the word which was added to the submatfix at each step." ></td>
	<td class="line x" title="86:159	The second column gives an English gloss of the word." ></td>
	<td class="line x" title="87:159	The third column reports fix,Y), the frequency of the co-occurrences between the word and the words that co-occur with it." ></td>
	<td class="line x" title="88:159	For example, the first line of the table shows that the word '~/~L' (establish/-acc) co-occurred with the 'organization' words 26 times." ></td>
	<td class="line x" title="89:159	The rightmost column specifies I(X,Y), the scaled mutual information between the rows and columns of the submatrix." ></td>
	<td class="line x" title="90:159	As the clustering proceeds, I(X,Y) gets larger." ></td>
	<td class="line x" title="91:159	~_~(company), ;~k:~l\](head quarter), mS(organization), ~(coorporation), iitij:~.J:(both companies), ~(school), ~:~t\](the company), zj~:~i(child company), ~l~(bank), ~/~(department store), ~t~t~\]~(agency), ~n0(coop.), ~j:~IXbusiness company), ~.~(city bank), ~)~(stand), ~-~ ~\[~l~(trust bank), 3~/~(branch), ~-~ ~(credit association), :~k)~(head store), ~--~--(university), :~-:~(each company), --~\] ~-~ (department store), JAR(agriculture cooperative), --~ --(maker), :~:)~(book store), if' L,' I~')-~j(TV station), 7~ :Y' ~ ~ ~ M(agency), X --)'~(superrnarket), ~\[~tXjoint-stock corporation), ~(doctoFs office), )~(all stores) Figure 2: The initial word group (submatrix) for the clustering shown in Table 1." ></td>
	<td class="line x" title="92:159	250 Word added ~/~ ~/~ ~/~ ~/~ ~/~I~< ~ ~/~ ~/~ ~/~ ~/~ ~/~A ~/~ fi/~ ~z/ ~ z~ ~z/~ ~/~ ~/~ ~/~ ~z/~ ~/~\]~ ~A Z~Z ATT Gloss establish/-acc tie-up/with tie-up/-nom unite/with cooperate/-nom possess/-nom unite/-nom advance/-nom in succession proceed/-nom purchase/-acc entrust/-acc produce/-nom develop/-nom invest/-nom expand/with develop/-nom publish/-nom agree/-nom demand/from invest/in sell/-nom purchase/-nom open/-acc introduce/from create/-nom utilize/at limit/to treat/-nom connect/-nom do/-nom exclude/-acc oppose/to sign/-copula sell/to participate/in corporation major Japan Nisho-Iwai three parties Drug Company Sony dealer Institution Honda Mitsubishi AT&T Air Line respectively Honda Freq 26 25 18 11 7 8 7 6 5 4 5 6 6 7 6 3 3 4 3 3 5 7 3 4 3 3 3 3 3 3 5 3 3 3 4 4 9 5 5 4 3 4 5 5 5 4 3 3 4 3 3 I 0.11 0.19 0.25 0.29 0.32 0.35 0.38 0.40 0.43 0.44 0.46 0.47 0.49 0.51 0.52 0.54 0.55 0.56 0.58 0.59 0.60 0.61 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.69 0.70 0.71 0.71 0.72 0.72 0.72 0.74 0.75 0.77 0.78 0.79 0.80 0.81 0.81 0.82 0.83 0.83 0.84 0.84 0.85 0.85 ~t~ Bank 7 0.85 ~j~:~ Air Line 6 0.85 ~-~.~ Trust Company 4 0.85 ~I~ Steel Company 4 0.85 Table 1: Example of Clustering 7." ></td>
	<td class="line x" title="93:159	EVALUATION Using 280,000 words of Japanese source text from the TIPSTER joint ventures domain, we tried several variations of the initial submatrices (word groups) from which the search in step three of the method starts: a) complete bipartite subgraphs, b) pre-classified noun groups and c) significantly frequent word pairs." ></td>
	<td class="line x" title="94:159	Based on the results of the experiments, we concluded that alternative (b) gives both the most accurate word associations and the highest coverage of word associations." ></td>
	<td class="line x" title="95:159	This technique is practical because classification of nouns is generally much simpler than that of verbs." ></td>
	<td class="line x" title="96:159	We don't propose any automatic algorithm to accomplish noun classification, but instead note that we were able to manually classify nouns in less than ten categories at about 500 words/hour." ></td>
	<td class="line x" title="97:159	That productivity was achieved using our new tool for manual word classification, which is partially inspired by EDR's way of classifying their semantic lexical data \[Matsukawa and Yokota, 1991 \]." ></td>
	<td class="line x" title="98:159	Based on a corpus of 280,000 words in the TIPSTER joint ventures domain, the most frequently occurring Japanese nouns, proper nouns, and verbs were automatically identified." ></td>
	<td class="line x" title="99:159	Then, a student classified the frequently occurring nouns into one of the twelve categories in (1) below, and each frequently occurring proper noun into one of the four categories in (2) below, using a menu-based tool, we were able to categorize 3,195 lexical entries in 12 person-hours." ></td>
	<td class="line x" title="100:159	3 These categories were then used as input to the word co-occurrence algorithm." ></td>
	<td class="line x" title="101:159	1." ></td>
	<td class="line x" title="102:159	Common noun categories 1 a. Organization CORPORATION GOVERNMENT UNDETERMINED-CORPORATION OTHER-ORGANIZATION 1 b. Location CITY COUNTRY PROVINCE 3 We divided the process of classifying common nouns into two phases; classification into the four categories la, lb, lc and ld, and further classification into the twelve categories." ></td>
	<td class="line x" title="103:159	As a result, each word was checked twice." ></td>
	<td class="line x" title="104:159	We found that using two phases generally improves both overall productivity and consistency." ></td>
	<td class="line x" title="105:159	251 OTHER-LOCATION 1 c. Person ENTITY-OFFICER TrlLE OTHER-PERSON 1 d. Other 2." ></td>
	<td class="line x" title="106:159	Proper noun categories ORGANIZATION LOCATION PERSON OTHER Using the 280,000 word joint venture corpus, we collected 14,407 word co-occurrences, involving 3,195 nouns and 4,365 verb/argument pairs, by the finite state pattern given in Section 5." ></td>
	<td class="line x" title="107:159	16 submatrices were clustered, grouping 810 observed word co-occurrences and 6,240 unobserved (or hypothesized) word co-occurrences." ></td>
	<td class="line x" title="108:159	We evaluated the accuracy of the system by manual review of a random sample of 500 hypothesized word co-occurrences." ></td>
	<td class="line x" title="109:159	Of these, 435, or 87% were judged reasonable." ></td>
	<td class="line x" title="110:159	This ratio is fine compared with a random sample of 500 arbitrary word cooccurrences between the 3,195 nouns and the 4,365 verb/argument pairs, of which only 153 (44%) were judged reasonable." ></td>
	<td class="line x" title="111:159	Table 2 below shows some examples judged reasonable; questionable examples are marked by '?'; unreasonable hypotheses are marked with an asterisk." ></td>
	<td class="line x" title="112:159	With a small corpus (280,000 words) such as ours, considering small frequency co-occurrences is critical." ></td>
	<td class="line nc" title="113:159	Looking at Table 3 below, if we had to ignore cooccurrences with frequency less than five (as \[Church and Hanks 1990\] did), there would be very little data." ></td>
	<td class="line x" title="114:159	With our method, as long as the frequency of co-occurrence of the word being considered with the set is greater than two, the statistic is stable." ></td>
	<td class="line x" title="115:159	Frequency Number of Word Pairs 0 6240 1 631 2 113 3 36 4 18 5 4 6 2 7 3 9 1 10 1 16 1 Table 3: Pair Frequencies 8." ></td>
	<td class="line x" title="116:159	CONCLUSION Our method achieved fully automatic hypothesis of word associations, starting from untagged text and generalizing to unobserved word associations." ></td>
	<td class="line x" title="117:159	As a result of human review 87% of the hypotheses were judged to be reasonable." ></td>
	<td class="line x" title="118:159	Because the technique considers low frequency cases, most of the data was used in making generalizations." ></td>
	<td class="line x" title="119:159	It remains to be determined how well this method will work for English, but with appropriate finite state patterns, similar results may be achieved." ></td>
	<td class="line x" title="120:159	(owner) (take office/as) A T T ~' 6/~X (AT&T) (introduceA~rom) ~$\[\] ~:/~ (melropolitan) (build/at) (personnel) (dispatch/-acc) (Commitee) (unite/with) (library) (sell/-nom) (Company) (organize/-acc) ~r~ ~/~ (agency) (publish/-nom) (post office) (tie-up/with) ~t ~:/~ (State) (developAo) (Cannon) (enter/-acc) ~ ~:/~ (doctor's office) (limit/to) (nations) (haveAn) ~ ~/~ (Nomura) (prroduce/-nom) ~ ~/~ (station employee) (take office/-nom) D R A M ~/~ (DRAM) (unite/-nom) (Switzerland) (see/-nom) ~ ~:/~ (director) (announce/to) Table 2: Examples of reasonable hypothesized cooccurrences ACKNOWLEDGMENTS The author wishes to thank Madeleine Bates, Ralph Weischedel and Sean Boisen for significant contributions to this paper." ></td>
	<td class="line x" title="121:159	252 1." ></td>
	<td class="line x" title="122:159	2. 3." ></td>
	<td class="line x" title="123:159	4. 5." ></td>
	<td class="line x" title="124:159	6. 7." ></td>
	<td class="line x" title="125:159	8. 9." ></td>
	<td class="line x" title="126:159	10." ></td>
	<td class="line x" title="127:159	REFERENCES Brent, M.R., (1991) 'Automatic Acquisition of Subcategorization Frames from Untagged Text,' Proceedings of the 29th annual Meeting of the ACL, pp." ></td>
	<td class="line x" title="128:159	209-214." ></td>
	<td class="line x" title="129:159	Brown, P.F., et." ></td>
	<td class="line x" title="130:159	al., (1992) 'Class-based N-gram Models of Natural Language,' Computational Linguistics Vol." ></td>
	<td class="line x" title="131:159	18 (4), pp." ></td>
	<td class="line x" title="132:159	467-479." ></td>
	<td class="line xc" title="133:159	Church, K. and Hanks, P. , (1990) 'Word Association Norms, Mutual Information, and Lexicography,' Computational Linguistics Vol." ></td>
	<td class="line x" title="134:159	16 (1), pp.22-29." ></td>
	<td class="line x" title="135:159	Hindle, D. , (1990) 'Noun Classification from Predicate-Argument Structures,' Proceedings of the 28th Annual Meeting of the ACL, pp." ></td>
	<td class="line x" title="136:159	268-275." ></td>
	<td class="line x" title="137:159	Hoel P. G., (1971): Introduction to Mathematical Statistics, Chapter 9." ></td>
	<td class="line x" title="138:159	2. Resnik, P. , (1992) 'A Class-based Approach to Lexical Discovery,' Proceedings of the 30th Annual Meeting of the ACL, pp." ></td>
	<td class="line x" title="139:159	327-329." ></td>
	<td class="line x" title="140:159	Smadja F.A. and McKeown, K.R., (1990) 'Automatically Extracting and Representing Collocations for Language Generation,' Proceedings of the 28th Annual Meeting of the ACL, pp." ></td>
	<td class="line x" title="141:159	252-259." ></td>
	<td class="line x" title="142:159	Matsukawa T. , Miller S. and Weischedel R." ></td>
	<td class="line x" title="143:159	(1993) 'Example-based Correction of Word Segmentation and Part of Speech Labelling,' Proceedings of DARPA Human Language Technologies Workshop." ></td>
	<td class="line x" title="144:159	Matsukawa, T. and Yokota, E." ></td>
	<td class="line x" title="145:159	(1991) 'Development of the Concept Dictionary Implementation of Lexical Knowledge,' Proc." ></td>
	<td class="line x" title="146:159	of pre-conference workshop sponsored by the special Interest Group on the Lexicon (SIGLEX) of the Association for Computational Linguistics, 1991." ></td>
	<td class="line x" title="147:159	Weischedel, R. et al.(1991) 'Partial Parsing: A Report on Work in Progress,' Proceedings of the Workshop on Speech and Natural Language, pp." ></td>
	<td class="line x" title="149:159	204210." ></td>
	<td class="line x" title="150:159	APPENDIX: JUSTIFICATION OF CHI SQUARE Chi-square score is given by the following formula : I(X, Y)= ~ I(X, Y) p(X, Y) E p(X, Y' Io = / gp-~-p~) (0) where .,~ Y= columns and rows of a word co-occurrence matrix X, Y = subsets of X~ Y, respectively (i.e. word classes at the columns and the rows) This can be justified as follows." ></td>
	<td class="line x" title="151:159	According to \[Hoel 1971\], the likelihood ratio LAMBDA for a test of the hypothesis: p(i) = po(i) (i = 1, 2  k), where p(i) is the probability of case i and po(i) is a hypothesized probability of it, when observations are independent of each other, is given as: k, n(i) -2 log LAMBDA 2 ~ n(i) (1) = log~ i=l where n(i) is the number of observations of case i, and e(i) is its expectation, i.e., e(i) = n p(i), where n is the total number of observations." ></td>
	<td class="line x" title="152:159	The distribution is chi-square when n is large." ></td>
	<td class="line x" title="153:159	If we assume two word classes, ci and cj, occur independently, then the expected value of the probability of their cooccurrence will be, e(ci, cj)= n p(ci) p(cj) (2) where p(ci) and p(cj) are estimations of the probability of occurrence of ci and cj." ></td>
	<td class="line x" title="154:159	The maximum likelihood estimate of p(ci) and p(cj) is f(ci)/n and f(cj)/n, where f(cj) and f(cj) are the number of observations of words classified in ci and cj." ></td>
	<td class="line x" title="155:159	The maximum likelihood estimate of p(ci, cj), the probability of the co-occurrences of words in ci and cj, is f(ci, cj)/n, where f(ci, cj) is the number of observations of the co-occurrences." ></td>
	<td class="line x" title="156:159	Then the number of the co-occurrences n(ci, cj) (which is the same as f(ci, cj) ) can be represented as, n(ci, cj)= n p(ci, cj) (3) Therefore, given k classes, cl, c2  ck, substituting (2) and (3) into (1)." ></td>
	<td class="line x" title="157:159	k i p(ci, c j) 2 ~ ~ np(ci, cj)log i=0j=0 p(~i) }~(-~j ) (4) If n is large, this will have a chi-square distribution; therefore, we can estimate how unlikely our assumption of independence among word classes is. Since formula (4) gives a scaled average mutual information among the word classes, searching for a partition of words that provides maximum average mutual information among word classes is equivalent to seeking classes where independence among word classes is minimally likely." ></td>
	<td class="line x" title="158:159	The algorithm reported in this paper searches for pairs of word classes which provide a local maximum I(X, Y), a term in the summation of formula (0)." ></td>
	<td class="line x" title="159:159	253" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J93-1006
Text-Translation Alignment
Kay, Martin;Roscheisen, Martin;"></td>
	<td class="line x" title="1:392	Text-Translation Alignment Martin Kay* Xerox Palo Alto Research Center and Stanford University Martin R6scheisen* Xerox Palo Alto Research Center and Technical University of Munich We present an algorithm for aligning texts with their translations that is based only on internal evidence." ></td>
	<td class="line x" title="2:392	The relaxation process rests on a notion of which word in one text corresponds to which word in the other text that is essentially based on the similarity of their distributions." ></td>
	<td class="line x" title="3:392	It exploits a partial alignment of the word level to induce a maximum likelihood alignment of the sentence level, which is in turn used, in the next iteration, to refine the word level estimate." ></td>
	<td class="line x" title="4:392	The algorithm appears to converge to the correct sentence alignment in only a few iterations." ></td>
	<td class="line x" title="5:392	1." ></td>
	<td class="line x" title="6:392	The Problem To align a text with a translation of it in another language is, in the terminology of this paper, to show which of its parts are translated by what parts of the second text." ></td>
	<td class="line x" title="7:392	The result takes the form of a list of pairs of items--words, sentences, paragraphs, or whatever--from the two texts." ></td>
	<td class="line x" title="8:392	A pair (a~ b> is on the list if a is translated, in whole or in part, by b. If (a, b> and (a, c) are on the list, it is because a is translated partly by b, and partly by c. We say that the alignment is partial if only some of the items of the chosen kind from one or other of the texts are represented in the pairs." ></td>
	<td class="line x" title="9:392	Otherwise, it is complete." ></td>
	<td class="line x" title="10:392	It is notoriously difficult to align good translations on the basis of words, because it is often difficult to decide just which words in an original are responsible for a given one in a translation and, in any case, some words apparently translate morphological or syntactic phenomena rather than other words." ></td>
	<td class="line x" title="11:392	However, it is relatively easy to establish correspondences between such words as proper nouns and technical terms, so that partial alignment on the word level is often possible." ></td>
	<td class="line x" title="12:392	On the other hand, it is also easy to align texts and translations on the sentence or paragraph levels, for there is rarely much doubt as to which sentences in a translation contain the material contributed by a given one in the original." ></td>
	<td class="line x" title="13:392	The growing interest in the possibility of automatically aligning large texts is attested to by independent work that has been done on it since the first description of our methods was made available (Kay and R6scheisen 1988)." ></td>
	<td class="line x" title="14:392	In recent years it has been possible for the first time to obtain machine-readable versions of large corpora of text with accompanying translations." ></td>
	<td class="line x" title="15:392	The most striking example is the Canadian 'Hansard,' the transcript of the proceedings of the Canadian parliament." ></td>
	<td class="line x" title="16:392	Such bilingual corpora make it possible to undertake statistical, and other kinds of empirical, studies of translation on a scale that was previously unthinkable." ></td>
	<td class="line x" title="17:392	Alignment makes possible approaches to partially, or completely, automatic translation based on a large corpus of previous translations that have been deemed accept* Xerox PARC, 3333 Coyote Hill Road, Palo Alto, CA 94306." ></td>
	<td class="line x" title="18:392	t Department of Computer Science, Technical University of Munich, 8000 Munich 40, Germany." ></td>
	<td class="line x" title="19:392	(~) 1993 Association for Computational Linguistics Computational Linguisti4s Volume 19, Number 1 able." ></td>
	<td class="line x" title="20:392	Perhaps the best-known example of this approach is to be found in Sato and Nagao (1990)." ></td>
	<td class="line x" title="21:392	The method proposed there requires a database to be maintained of the syntactic structures of sentences together with the structures of the corresponding translations." ></td>
	<td class="line x" title="22:392	This database is searched in the course of making a new translation for examples of previous sentences that are like the current one in ways that are relevant for the method." ></td>
	<td class="line x" title="23:392	Another example is the completely automatic, statistical approach to translation taken by the research group at IBM (Brown et al. 1990), which takes a large corpus of text with aligned translations as its point of departure." ></td>
	<td class="line x" title="24:392	It is widely recognized that one of the most important sources of information to which a translator can have access is a large body of previous translations." ></td>
	<td class="line x" title="25:392	No dictionary or terminology bank can provide information of comparable value on topical matters of possibly intense though only transitory interest, or on recently coined terms in the target language, or on matters relating to house style." ></td>
	<td class="line x" title="26:392	But such a body of data is useful only if, once a relevant example has been found in the source language, the corresponding passage can be quickly located in the translation." ></td>
	<td class="line x" title="27:392	This is simple only if the texts have been previously aligned." ></td>
	<td class="line x" title="28:392	Clearly, what is true of the translator is equally true of others for whom translations are a source of primary data, such as students of translation, the designers of translations systems, and lexicographers." ></td>
	<td class="line x" title="29:392	Alignment would also facilitate the job of checking for consistency in technical and legal texts where consistency constitutes a large part of accuracy." ></td>
	<td class="line x" title="30:392	In this paper, we provide a method for aligning texts and translations based only on internal evidence." ></td>
	<td class="line x" title="31:392	In other words, the method depends on no information about the languages involved beyond what can be derived from the texts themselves." ></td>
	<td class="line x" title="32:392	Furthermore, the computations on which it is based are straightforward and robust." ></td>
	<td class="line x" title="33:392	The plan rests on a relationship between word and sentence alignments arising from the observation that a pair of sentences containing an aligned pair of words must themselves be aligned." ></td>
	<td class="line x" title="34:392	It follows that a partial alignment on the word level could induce a much more complete alignment on the sentence level." ></td>
	<td class="line x" title="35:392	A solution to the alignment problem consists of a subset of the Cartesian product of the sets of source and target sentences." ></td>
	<td class="line x" title="36:392	The process starts from an initial subset excluding pairs whose relative positions in their respective texts is so different that the chance of their being aligned is extremely low." ></td>
	<td class="line x" title="37:392	This potentially alignable set of sentences forms the basis for a relaxation process that proceeds as follows." ></td>
	<td class="line x" title="38:392	An initial set of candidate word alignments is produced by choosing pairs of words that tend to occur in possibly aligned sentences." ></td>
	<td class="line x" title="39:392	The idea is to propose a pair of words for alignment if they have similar distributions in their respective texts." ></td>
	<td class="line x" title="40:392	The distributions of a pair of words are similar if most of the sentences in which the first word occurs are alignable with sentences in which the second occurs, and vice versa." ></td>
	<td class="line x" title="41:392	The most apparently reliable of these word alignments are then used to induce a set of sentence alignments that will be a subset of the eventual result." ></td>
	<td class="line x" title="42:392	A new estimate is now made of what sentences are alignable based on the fact that we are now committed to aligning certain pairs." ></td>
	<td class="line x" title="43:392	Because sentence pairs are never removed from the set of alignments, the process converges to the point when no new ones can be found; then it stops." ></td>
	<td class="line x" title="44:392	In the next section, we describe the algorithm." ></td>
	<td class="line x" title="45:392	In Section 3 we describe additions to the basic technique required to provide for morphology, that is, relatively superficial variations in the forms of words." ></td>
	<td class="line x" title="46:392	In Section 4 we show the results of applying a program that embodies these techniques to an article from Scientific American and its German translation in Spektrum der Wissenschaft." ></td>
	<td class="line x" title="47:392	In Section 5 we discuss other approaches to the alignment problem that were subsequently undertaken by other researchers (Gale and Church 1991; Brown, Lai, and Mercer 1991)." ></td>
	<td class="line x" title="48:392	Finally, in Section 6, we consider ways in which our present methods might be extended and improved." ></td>
	<td class="line x" title="49:392	122 Martin Kay and Martin R6scheisen Text-Translation Alignment 2." ></td>
	<td class="line x" title="50:392	The Alignment Algorithm 2.1 Data Structures The principal data structures used in the algorithm are the following: Word-Sentence Index (WSI)." ></td>
	<td class="line x" title="51:392	One of these is prepared for each of the texts." ></td>
	<td class="line x" title="52:392	It is a table with an entry for each different word in the text showing the sentences in which that word occurs." ></td>
	<td class="line x" title="53:392	For the moment, we may take a word as being simply a distinct sequence of letters." ></td>
	<td class="line x" title="54:392	If a word occurs more than once in a sentence, that sentence occurs on the list once for each occurrence." ></td>
	<td class="line x" title="55:392	Alignable Sentence Table (AST)." ></td>
	<td class="line x" title="56:392	This is a table of pairs of sentences, one from each text." ></td>
	<td class="line x" title="57:392	A pair is included in the table at the beginning of a pass if that pair is a candidate for association by the algorithm in that pass." ></td>
	<td class="line x" title="58:392	Word Alignment Table (WAT)." ></td>
	<td class="line x" title="59:392	This is a list of pairs of words, together with similarities and frequencies in their respective texts, that have been aligned by comparing their distributions in the texts." ></td>
	<td class="line x" title="60:392	Sentence Alignment Table (SAT)." ></td>
	<td class="line x" title="61:392	This is a table that records for each pair of sentences how many times the two sentences were set in correspondence by the algorithm." ></td>
	<td class="line x" title="62:392	Some additional data structures were used to improve performance in our implementation of the algorithm, but they are not essential to an understanding of the method as a whole." ></td>
	<td class="line x" title="63:392	2.2 Outline of the Algorithm At the beginning of each cycle, an AST is produced that is expected to contain the eventual set of alignments, generally amongst others." ></td>
	<td class="line x" title="64:392	It pairs the first and last sentences of the two texts with a small number of sentences from the beginning and end of the other text." ></td>
	<td class="line x" title="65:392	Generally speaking, the closer a sentence is to the middle of the text, the larger the set of sentences in the other text that are possible correspondents for it." ></td>
	<td class="line x" title="66:392	The next step is to hypothesize a set of pairs of words that are assumed to correspond based on similarities between their distributions in the two texts." ></td>
	<td class="line x" title="67:392	For this purpose, a word in the first text is deemed to occur at a position corresponding to a word in the second text if they occur in a pair of sentences that is a member of the AST." ></td>
	<td class="line x" title="68:392	Similarity of distribution is a function of the number of corresponding sentences in which they occur and the total number of occurrences of each." ></td>
	<td class="line x" title="69:392	Pairs of words are entered in the WAT if the association between them is so close that it is not likely to be the result of a random event." ></td>
	<td class="line x" title="70:392	In our algorithm, the closeness of the association is estimated on the basis of the similarity of their distributions and the total number of occurrences." ></td>
	<td class="line x" title="71:392	The next step is to construct the SAT, which, in the last pass, will essentially become the output of the program as a whole." ></td>
	<td class="line x" title="72:392	The idea here is to associate sentences that contain words paired in the WAT, giving preference to those word pairs that appear to be more reliable." ></td>
	<td class="line x" title="73:392	Multiple associations are recorded." ></td>
	<td class="line x" title="74:392	If there are to be further passes of the main body of the algorithm, a new AST is then constructed in light of the associations in the SAT." ></td>
	<td class="line x" title="75:392	Associations that are supported some minimum number of times are treated just as the first and last sentences of the texts were initially; that is, as places at which there is known to be a correspondence." ></td>
	<td class="line x" title="76:392	Possible correspondences are provided for the intervening sentences by 123 Computational Linguistics Volume 19, Number 1 the same interpolation method initially used for all sentences in the middle of the texts." ></td>
	<td class="line x" title="77:392	In preparation for the next pass, a new set of corresponding words is now hypothesized using distributions based on the new AST, and the cycle repeats." ></td>
	<td class="line x" title="78:392	2.3 The Algorithm The main algorithm is a relaxation process that leaves at the end of each pass a new WAT and SAT, each presumably more refined than the one left at the end of the preceding pass." ></td>
	<td class="line x" title="79:392	The input to the whole process consists only of the WSIs of the two texts." ></td>
	<td class="line x" title="80:392	Before the first pass of the relaxation process, an initial AST is computed simply from the lengths of the two texts: Construct Initial AST." ></td>
	<td class="line x" title="81:392	If the texts contain m and n sentences respectively, then the table can be thought of as an m x n array of ones and zeros." ></td>
	<td class="line x" title="82:392	The average number of sentences in the second text corresponding to a given one in the first text is n/m, and the average position of the sentence in the second text corresponding to the i-th sentence in the first text is therefore i. n/m. In other words, the expectation is that the true correspondences will lie close to the diagonal." ></td>
	<td class="line x" title="83:392	Empirically, sentences typically correspond one for one; correspondences of one sentence to two are much rarer, and correspondences of one to three or more, though they doubtless occur, are very rare and were unattested in our data." ></td>
	<td class="line x" title="84:392	The maximum deviation can be stochastically modeled as O(v~), the factor by which the standard deviation of a sum of n independent and identically distributed random variables multiplies." ></td>
	<td class="line x" title="85:392	1 We construct the initial AST using a function that pairs single sentences near the middle of the text with as many as O(v~ff) sentences in the other text; it is generously designed to admit all but the most improbable associations." ></td>
	<td class="line x" title="86:392	Experience shows that because of this policy the results are highly insensitive to the particular function used to build this initial table." ></td>
	<td class="line x" title="87:392	2 The main body of the relaxation process consists of the following steps: Build the WAT." ></td>
	<td class="line x" title="88:392	For all sentences s a in the first text, each word in s a is compared with each word in those sentences s B of the second text that are considered as candidates for correspondence, i.e., for which (s A, s B) EAST." ></td>
	<td class="line x" title="89:392	A pair of words is entered into the WAT if the distributions of the two words in their texts are sufficiently similar and if the total number of occurrences indicates that this pair is unlikely to be the result of a spurious match." ></td>
	<td class="line x" title="90:392	Note that the number of comparisons of the words in two sentences is quadratic only in the number of words in a sentence, which can be assumed to be not a function of the length of the text." ></td>
	<td class="line x" title="91:392	Because of the.constraint on the maximum deviation from the diagonal as outlined above, the computational complexity of the algorithm is bound by O(nx/n) in each pass." ></td>
	<td class="line x" title="92:392	1 In such a model, each random variable would correspond to a translator's choice to move away from the diagonal in the AST by a certain distance (which is assumed to be zero mean, Gaussian distributed)." ></td>
	<td class="line x" title="93:392	However, the specific assumptions about the maximum deviation are not crucial in that the algorithm was observed to be insensitive to such modifications." ></td>
	<td class="line x" title="94:392	2 The final results showed that no sentence alignment is at a distance greater than ten from the diagonal in texts of 255 and 300 sentences." ></td>
	<td class="line x" title="95:392	Clearly, any such prior knowledge could be used for a significant speed-up of the algorithm, but it was our goal to adopt as few prior assumptions as possible." ></td>
	<td class="line x" title="96:392	124 Martin Kay and Martin R6scheisen Text-Translation Alignment Our definition of the similarity between a pair of words is complicated by the fact that the two texts have unequal lengths and that the AST allows more than one correspondence, which means that we cannot simply take the inner product of the vector representations of the word's occurrences." ></td>
	<td class="line x" title="97:392	Instead, we use as a measure of similarity: 3 2c NA (v) + N~ (w) where c is the number of corresponding positions, and Nv(x) is the number of occurrences of the word x in the text T. This is essentially Dice's coefficient (Rijsbergen 1979)." ></td>
	<td class="line x" title="98:392	Technically, the value of c is the cardinality of the largest set of pairs (i, j) such that 1." ></td>
	<td class="line x" title="99:392	(s~(v),s~(w)) c AST, where szr(x) is the sentence in text T that contains the z-th occurrence of word x. 2." ></td>
	<td class="line x" title="100:392	Pairs are non-overlapping in the sense that, if (a, b) and (c, d) are distinct members of the set then they are distinct in both components, that is, a ~ c and b ~ d. Suppose that the word 'dog' occurs in sentences 50, 52, 75, and 200 of the English text, and 'Hund' in sentences 40 and 180 of the German, and that the AST contains the pairs (50, 40), (52, 40), and (200,180), among others, but not (75, 40)." ></td>
	<td class="line x" title="101:392	There are two sets that meet the requirements, namely ~(1, 1), (4,2)} and {(2, 1), (4,2)}." ></td>
	<td class="line x" title="102:392	The set {(1, 1), (2, 1), (4,2)} is excluded on the grounds that (1, 1) and (2, 1) overlap in the above sense--the first occurrence of 'Hund' is represented twice." ></td>
	<td class="line x" title="103:392	In the example, the similarity 2 _ 1 regardless of the ambiguity between would be computed as 4+2-2 -2' (1, 1) and (2, 1)." ></td>
	<td class="line x" title="104:392	The result of the comparisons of the words in all of the sentences of one text with those in the other text is that the word pairs with the highest similarity are located." ></td>
	<td class="line x" title="105:392	Comparing the words in a sentence of one text with those in a sentence of the other text carries with it an amortized cost of constant computational complexity, 4 if the usual memory-processing tradeoff on serial machines is exploited by maintaining redundant data structures such as multiple hash tables and ordered indexed trees." ></td>
	<td class="line x" title="106:392	5 The next task is to determine for each word pair, whether it will actually be entered into the WAT: the WAT is a sorted table where the more reliable pairs are put before less reliable ones." ></td>
	<td class="line x" title="107:392	For this purpose, each entry contains, as well as the pair of words themselves, the frequencies of those words in their respective texts and the similarity between them." ></td>
	<td class="line x" title="108:392	The closeness of the association between two words, and thus their rank in the WAT, is evaluated with respect to their similarity and the total num ~ ber of their occurrences." ></td>
	<td class="line x" title="109:392	To understand why similarity cannot be used 3 Throughout this paper, we use the word similarity to denote this similarity measure, which does not necessarily have to be an indicator of what one would intuitively describe as 'similar' words." ></td>
	<td class="line x" title="110:392	In particular, we will later see that similarity alone, without consideration of the total frequency, is not a good indicator for 'similarity'." ></td>
	<td class="line x" title="111:392	4 The basic idea is this: more processing has to be done to compute the similarity of a high-frequency word to another frequent word, but there are also more places at which this comparison can later be saved." ></td>
	<td class="line x" title="112:392	Recall also that we assume sentence length to be independent of text length." ></td>
	<td class="line x" title="113:392	5 For very large corpora, this might not be feasible." ></td>
	<td class="line x" title="114:392	However, large texts can almost invariably be broken into smaller pieces at natural and reliable places, such as chapter and section headings." ></td>
	<td class="line x" title="115:392	125 Computational Linguistics Volume 19, Number 1 alone, note that there are far more one-frequency words than words of higher frequency." ></td>
	<td class="line x" title="116:392	Thus, a pair of words with a similarity of 1, each of them occurring only once, may well be the result of a random event." ></td>
	<td class="line x" title="117:392	If such a pair was proposed for entry into the WAT, it should only be added with a low priority." ></td>
	<td class="line x" title="118:392	The exact stochastic relation is depicted in Figure 1, where the probability is shown that a word of a frequency k that was aligned with a word in the other text with a certain similarity s is just the result of a random process." ></td>
	<td class="line x" title="119:392	6 Note that, for a high-frequency word that has a high similarity with some other word (right front corner), it is very unlikely (negligible plateau height) that this association has to be attributed to chance." ></td>
	<td class="line x" title="120:392	On the other hand, low similarities (back) can easily be attained by just associating arbitrary words." ></td>
	<td class="line x" title="121:392	Low-frequency words--because there are so many of them in a text--can also achieve a high similarity with some other words without having to be related in an interesting way." ></td>
	<td class="line x" title="122:392	This can be intuitively explained by the fact that the similarity of a high-frequency word is based on a pattern made up of a large number of instances." ></td>
	<td class="line x" title="123:392	It is therefore a pattern that is unlikely to be replicated by chance." ></td>
	<td class="line x" title="124:392	Furthermore, since there are relatively few high-frequency words, and they can only contract high similarities with other high-frequency words, the number of possible correspondents for them is lower, and the chance of spurious associations is therefore less on these grounds also." ></td>
	<td class="line x" title="125:392	Note that low-frequency words with low similarity (back left corner) have also a low probability of being spuriously associated to some other word." ></td>
	<td class="line x" title="126:392	This is because low-frequency words can achieve a low similarity only with words of a high frequency, which in turn are rare in a text, and are therefore unlikely to be associated spuriously." ></td>
	<td class="line x" title="127:392	7 Our algorithm does not use all the detail in Figure 1, but only a simple discrete heuristic: a word pair whose similarity exceeds some threshold is assigned to one of two or three segments of the WAT, depending on the word frequency." ></td>
	<td class="line x" title="128:392	A segment with words of higher frequency is preferred to lower-frequency segments." ></td>
	<td class="line x" title="129:392	Within each segment, the entries are sorted in order of decreasing similarity and, in case of equal similarities, in order of decreasing frequency." ></td>
	<td class="line x" title="130:392	In terms of Figure 1, we take a rectangle from the right front." ></td>
	<td class="line x" title="131:392	We place the left boundary as far to the left as possible, because this is where most of the words are." ></td>
	<td class="line x" title="132:392	Build the SAT." ></td>
	<td class="line x" title="133:392	In this step, the correspondences in the WAT are used to establish a mapping between sentences of the two texts." ></td>
	<td class="line x" title="134:392	In general, these new 6 The basis for this graph is an analytic derivation of the probability that a word with a certain frequency in a 300-sentence text matches some random pattern with a particular similarity." ></td>
	<td class="line x" title="135:392	The analytic formula relies on word-frequency data derived from a large corpus instead of on a stochastic model for word frequency distribution (such as Zipf's law, which states that the frequency with which words occur in a text is indirectly proportional to the number of words with this frequency; for a recent discussion of more accurate models, see also Baayen \[1991\])." ></td>
	<td class="line x" title="136:392	Clearly, the figure is dependent on the state of the AST (e.g. lower similarities become more acceptable as the AST becomes more and more narrow), but the thresholds relevant to our algorithm can be precomputed at compile-time." ></td>
	<td class="line x" title="137:392	The figure shown would be appropriate to pass 3 in our experiment." ></td>
	<td class="line x" title="138:392	In the formula used, there are a few reasonable simplifications concerning the nature of the AST; however, a Monte-Carlo simulation that is exactly in accordance with our algorithm confirmed the depicted figure in every essential detail." ></td>
	<td class="line oc" title="139:392	7 This discussion could also be cast in an information theoretic framework using the notion of 'mutual information' (Fano 1961), estimating the variance of the degree of match in order to find a frequency-threshold (see Church and Hanks 1990)." ></td>
	<td class="line x" title="140:392	126 Martin Kay and Martin R6scheisen Text-Translation Alignment /////JIIfJi ',~1 / \[ / \[ / \[ I \] I I J t I f 5 i iiiii i,, i i iiiiiiiil, i i,~,kkNk~,\:,:,\k, t r i P i i i i iiiiiiii\\\\\\\ 8 t!!!!!!!!\[!!~\\\\\\\\\\ 10 15 20 Frequency Figure 1 Likelihood that a word pair is a spurious match as a function of a word's frequency and its similarity with a word in the other text (maximum 0.94)." ></td>
	<td class="line x" title="141:392	associations are added to the ones inherited from the preceding pass." ></td>
	<td class="line x" title="142:392	It is an obvious requirement of the mapping that lines of association should not cross." ></td>
	<td class="line x" title="143:392	At the beginning of the relaxation process, the SAT is initialized such that the first sentences of the two texts, and the last sentences, are set in correspondence with one another, regardless of any words they may contain." ></td>
	<td class="line x" title="144:392	The process that adds the remaining associations scans the WAT in order and applies a three-part process to each pair Iv, w/." ></td>
	<td class="line x" title="145:392	1." ></td>
	<td class="line x" title="146:392	Construct the correspondence set for/v~ w / using essentially the same procedure as in the calculation of the denominator, c, of word similarities above." ></td>
	<td class="line x" title="147:392	Now, however, we are concerned to avoid ambiguous pairs as characterized above." ></td>
	<td class="line x" title="148:392	The set contains a sentence pair IsiA(v),s~(w)l if (1) IsiA(v)~ s~(w)l EAST, and (2) v occurs in no other sentence h (resp." ></td>
	<td class="line x" title="149:392	w in no g) such that Is~(v), h I (resp." ></td>
	<td class="line x" title="150:392	Ig~ s~(w)l) is also in the AST." ></td>
	<td class="line x" title="151:392	2." ></td>
	<td class="line x" title="152:392	If any sentence pair in the correspondence set crosses any of the associations that have already been added to the SAT, the word pair is rejected as a whole." ></td>
	<td class="line x" title="153:392	In other words, if a given pair of sentences correspond, then sentences preceding the first of them can be associated only with sentences preceding the second." ></td>
	<td class="line x" title="154:392	3." ></td>
	<td class="line x" title="155:392	Add each sentence pair in the correspondence set of the word pair Iv, w / to the SAT." ></td>
	<td class="line x" title="156:392	A count is recorded of the number of times a particular association is supported." ></td>
	<td class="line x" title="157:392	These counts are later thresholded when a new AST is computed or when the process terminates." ></td>
	<td class="line x" title="158:392	Build a New AST." ></td>
	<td class="line x" title="159:392	If there is to be another pass of the relaxation algorithm, a new AST must be constructed as input to it." ></td>
	<td class="line x" title="160:392	This is based on the current SAT and is derived from it by supplying associations for sentences for which it 127 Computational Linguistics Volume 19, Number 1 provides none." ></td>
	<td class="line x" title="161:392	The idea is to fill gaps between associated pairs of sentences in the same manner that the gap between the first and the last sentence was filled before the first pass." ></td>
	<td class="line x" title="162:392	However, only sentence associations that are represented more than some minimum number of times in the SAT are transferred to the AST." ></td>
	<td class="line x" title="163:392	In what follows, we will refer to these sentence pairs as anchors." ></td>
	<td class="line x" title="164:392	As before, it is convenient to think of the AST as a rectangular array, even though it is represented more economically in the program." ></td>
	<td class="line x" title="165:392	Consider a maximal sequence of empty AST entries, that is, a sequence of sentences in one text for which there are no associated sentences in the other, but which is bounded above and below by an anchor." ></td>
	<td class="line x" title="166:392	The new associations that are added lie on and adjacent to the diagonal joining the two anchors." ></td>
	<td class="line x" title="167:392	The distance from the diagonal is a function of the distance of the current candidate sentence pair and the nearest anchor." ></td>
	<td class="line x" title="168:392	The function is the same one used in the construction of the initial AST." ></td>
	<td class="line x" title="169:392	Repeat." ></td>
	<td class="line x" title="170:392	Build a new WAT and continue." ></td>
	<td class="line x" title="171:392	3." ></td>
	<td class="line x" title="172:392	Morphology As we said earlier, the basic alignment algorithm treats words as atoms; that is, it treats strings as instances of the same word if they consist of identical sequences of letters, and otherwise as totally different." ></td>
	<td class="line x" title="173:392	The effect of this is that morphological variants of a word are not seen as related to one another." ></td>
	<td class="line x" title="174:392	This might not be seen as a disadvantage in all circumstances." ></td>
	<td class="line x" title="175:392	For example, nouns and verbs in one text might be expected to map onto nouns with the same number and verbs with the same tense much of the time." ></td>
	<td class="line x" title="176:392	But this is not always the case and, more importantly, some languages make morphological distinctions that are absent in the other." ></td>
	<td class="line x" title="177:392	German, for example, makes a number of case distinctions, especially in adjectives, that are not reflected in the morphology of English." ></td>
	<td class="line x" title="178:392	For these reasons, it seems desirable to allow words to contract associations with other words both in the form in which they actually occur, and in a more normalized form that will throw them together with morphologically related other words in the text." ></td>
	<td class="line x" title="179:392	3.1 The Basic Idea The strategy we adopted was to make entries in the WSI, not only for maximal strings of alphabetic characters occurring in the texts, but also for other strings that could usefully be regarded as normalized forms of these." ></td>
	<td class="line x" title="180:392	Clearly, one way to obtain normalized forms of words is to employ a fully fledged morphological analyzer for each of the languages." ></td>
	<td class="line x" title="181:392	However, we were concerned that our methods should be as independent as possible of any specific facts about the languages being treated, since this would make them more readily usable." ></td>
	<td class="line x" title="182:392	Furthermore, since our methods attend only to very gross features of the texts, it seemed unreasonable that their success should turn on a very fine analysis at any level." ></td>
	<td class="line x" title="183:392	We argue that, by adding a guess as to how a word should be normalized to the WSI, we remove no associations that could have been formed on the basis of the original word, but only introduce the possibility of some additional associations." ></td>
	<td class="line x" title="184:392	Also, it is unlikely that an incorrect normalization will contract any associations at all, especially in view of the fact that these forms, because they normalize several original forms, tend to occur more often." ></td>
	<td class="line x" title="185:392	They will therefore rarely be misleading." ></td>
	<td class="line x" title="186:392	128 Martin Kay and Martin R6scheisen Text-Translation Alignment For us, a normalized form of a word is always an initial or a final substring of that word--no attention is paid to morphographemic or word-internal changes." ></td>
	<td class="line x" title="187:392	A word is broken into two parts, one of which becomes the normalized form, if there is evidence that the resulting prefix and suffix belong to a paradigm." ></td>
	<td class="line x" title="188:392	In particular, both must occur as prefixes and suffixes of other forms." ></td>
	<td class="line x" title="189:392	3.2 The Algorithm The algorithm proceeds in two stages." ></td>
	<td class="line x" title="190:392	First a data structure, called the trie, is constructed in which information about the occurrences of potential prefixes and suffixes in the text is stored." ></td>
	<td class="line x" title="191:392	Second, words are split, where the trie provides evidence for doing so, and one of the resulting parts is chosen as the normalization." ></td>
	<td class="line x" title="192:392	A trie (Knuth 1973; pp." ></td>
	<td class="line x" title="193:392	481--490) is a data structure for associating information with strings of characters." ></td>
	<td class="line x" title="194:392	It is particularly economical in situations where many of the strings of interest are substrings of others in the set." ></td>
	<td class="line x" title="195:392	A trie is in fact a tree, with a branch at the root node for every character that begins a string in the set." ></td>
	<td class="line x" title="196:392	To look up a string, one starts at the root, and follows the branch corresponding to its first character to another node." ></td>
	<td class="line x" title="197:392	From there, the branch for the second character is followed to a third node, and so on, until either the whole string has been matched, or it has been discovered not to be in the set." ></td>
	<td class="line x" title="198:392	If it is in the set, then the node reached after matching its last character contains whatever information the structure contains for it." ></td>
	<td class="line x" title="199:392	The economy of the scheme lies in the fact that a node containing information about a string also serves as a point on the way to longer strings of which the given one is a prefix." ></td>
	<td class="line x" title="200:392	In this application, two items of information are stored with a string, namely the number of textual words in which it occurs as a prefix and as a suffix." ></td>
	<td class="line x" title="201:392	Consider the possibility of breaking an n-letter word before the i-th character of the word (1 < i _< n)." ></td>
	<td class="line x" title="202:392	The conditions for a break are: The number of other words starting with characters 1 ." ></td>
	<td class="line x" title="203:392	 i 1 of the current word must be greater than the number of words starting with characters 1   i because, if the characters 1   i 1 constitute a useful prefix, then this prefix must be followed, in different words, by other suffixes than characters i n. So, consider the word 'wanting,' and suppose that we are considering the possibility of breaking it before the 5th character, 'i'." ></td>
	<td class="line x" title="204:392	For this to be desirable, there must be other words in the text, such as 'wants,' and 'wanted,' that share the first i 1 = 4 characters." ></td>
	<td class="line x" title="205:392	Conversely, there must be more words ending with characters i n of the word than with i 1 -." ></td>
	<td class="line x" title="206:392	 n. So, there must be more words with the suffix 'ing' than with the suffix 'ting'; for example 'seeing' and 'believing'." ></td>
	<td class="line x" title="207:392	There is a function from potential break points in words to numbers whose value is maximized to choose the best point at which to break." ></td>
	<td class="line x" title="208:392	If p and s are the potential prefix and suffix, respectively, and P(p) and S(s) are the number of words in the text in which they occur as such, the value of the function is kP(p)S(s)." ></td>
	<td class="line x" title="209:392	The quantity k is introduced to enable us to prefer certain kinds of breaks over others." ></td>
	<td class="line x" title="210:392	For the English and German texts used in our experiments, k = length(p) so as to favor long prefixes on the grounds that both languages are primarily suffixing." ></td>
	<td class="line x" title="211:392	If 129 Computational Linguistics Volume 19, Number 1 the function has the same value for more than one potential break point, the one farthest to the right is preferred, also for the reason that we prefer to maximize the lengths of prefixes." ></td>
	<td class="line x" title="212:392	Once it has been decided to divide a word, and at what place, one of the two parts is selected as the putative canonical form of the word, namely, whichever is longer, and the prefix if both are of equal length." ></td>
	<td class="line x" title="213:392	Finally, any other words in the same text that share the chosen prefix (suffix) are split at the corresponding place, and so assigned to the same canonical form." ></td>
	<td class="line x" title="214:392	The morphological algorithm treats words that appear hyphenated in the text specially." ></td>
	<td class="line x" title="215:392	The hyphenated word is treated as a unit, just as it appears, and so are the strings that result from breaking the word at the hyphens." ></td>
	<td class="line x" title="216:392	In addition, the analysis procedure described above is applied to these components, and any putative normal forms found are also used." ></td>
	<td class="line x" title="217:392	It is worth pointing out that we received more help from hyphens than one might normally expect in our analysis of the German texts because of a tendency on the part of the Spektrum der Wissenschaft translators, following standard practice for technical writing, of hyphenating compounds." ></td>
	<td class="line x" title="218:392	4." ></td>
	<td class="line x" title="219:392	Experimental Results In this section, we show some of the results of our experiments with these algorithms, and also data produced at some of the intermediate stages." ></td>
	<td class="line x" title="220:392	We applied the methods described here to two pairs of articles from Scientific American and their German translations in Spektrum der Wissenschaft (see references)." ></td>
	<td class="line x" title="221:392	The English and German articles about human-powered flight had 214 and 162 sentences, respectively; the ones about cosmic rays contained 255 and 300 sentences, respectively." ></td>
	<td class="line x" title="222:392	The first pair was primarily used to develop the algorithm and to determine the various parameters of the program." ></td>
	<td class="line x" title="223:392	The performance of the algorithm was finally tested on the latter pair of articles." ></td>
	<td class="line x" title="224:392	We chose these journals because of a general impression that the translations were of very high quality and were sufficiently 'free' to be a substantial challenge for the algorithm." ></td>
	<td class="line x" title="225:392	Furthermore, we expected technical translators to adhere to a narrow view of semantic accuracy in their work, and to rate the importance of this above stylistic considerations." ></td>
	<td class="line x" title="226:392	Later we also give results for another application of our algorithm to a larger text of 1257 sentences that was put together from two days from the French-English Hansard corpus." ></td>
	<td class="line x" title="227:392	Table 1 shows the first 50 entries of the WAT after pass 1 of the algorithm." ></td>
	<td class="line x" title="228:392	It shows part of the first section of the WAT (lines 1-23) and the beginning of the second (lines 24-50)." ></td>
	<td class="line x" title="229:392	The first segment contains words or normalized forms with more than 7 occurrences and a similarity not less than 0.8." ></td>
	<td class="line x" title="230:392	Strings shown with a following hyphen are prefixes arising from the morphological procedure; strings with an initial hyphen are suffixes." ></td>
	<td class="line x" title="231:392	Naturally, some of the word divisions are made in places that do not accurately reflect linguistic facts." ></td>
	<td class="line x" title="232:392	For example, English 'proto-' (1) comes from 'proton' and 'protons'; German '-eilchen' (17) is the normalization for words ending in '-teilchen' and, in the same way, '-eistung' (47) comes from '-leistung'." ></td>
	<td class="line x" title="233:392	Of these 50 word pairs, 42 have essentially the same meanings." ></td>
	<td class="line x" title="234:392	We take it that 'erg' and 'Joule,' in line 4, mean the same, modulo a change in units." ></td>
	<td class="line x" title="235:392	Also, it is not unreasonable to associate pairs like 'primary'/'sekundaren' (26) and 'electric'/'Feld' (43), on the grounds that they tend to be used together." ></td>
	<td class="line x" title="236:392	The pair 'rapid-'/'Pulsare-' (49) is made because a pulsar is a rapidly spinning neutron star and some such phrase 130 Martin Kay and Martin R6scheisen Text-Translation Alignment Table 1 The WAT after pass 1." ></td>
	<td class="line x" title="237:392	English German Eng." ></td>
	<td class="line x" title="238:392	Freq." ></td>
	<td class="line x" title="239:392	Similarity 1 protoProto14 1 2 proton-, Proton13 1 3 interstellar interstellare12 1 4 ergs Joule 10 1 5 electricelektrisch9 1 6 pulsarPulsar17 16/17 7 photoPhoto14 14/15 8 and und 69 11/12 9 per pro 12 11/12 10 relativrelativ11 10/11 11 atmospherAtmosph~ire10 10/11 12 Cygnus Cygnus 63 59/65 13 cosmickosmische81 39/43 14 volts Elektronenvolt 19 19/21 15 telescopeTeleskop9 8/9 16 universUnivers8 7/8 17 particle-eilchen 53 51/59 18 showerLuftschauer20 19/22 19 X-rayR6ntgen19 19/22 20 electrons Elektronen 12 11/13 21 sourceQuelle40 37/45 22 magnetic Magnetfeld 11 9/11 23 ray-Strahlung141 135/167 24 Obs diesem 6 1 ervatory 25 shower Gammaquant 6 1 26 primary sekund~iren 6 1 27 percent Prozent 6 1 28 ~a!axies Galaxien 5 1 29 ~nmean Krim 5 1 30 ultrahighultraho5 1 31 density Dichte 5 1 32 synchrotron Synchrotronstrahlung 5 1 33 activaktiv5 1 34 supernova Supernova-Explosion5 1 35 composition Zusammensetzung 5 1 36 detectors l~rim~ire5 1 37 data Daten7 7/8 38 University Universit7 6/7 39 element-usammensetzung 7 6/7 40 neutron Neutronenstern 7 6/7 41 Cerenkov Cerenkov-Licht7 6/7 42 spinning rotier6 6/7 43 electric Feld 6 5/6 44 lines -inien 6 5/6 45 medium Medium 6 5/6 46 estimateabsch~itz6 5/6 47 output -eistung 6 5/6 48 brightAstronom5 5/6 49 rapidPulsare5 5/6 50 proposed vorgeschlagen 6 5/6 occurs with it five out of six times." ></td>
	<td class="line x" title="240:392	Notice, however, that the association 'pulsar-' 'Pulsar-' is also in table (6)." ></td>
	<td class="line x" title="241:392	Furthermore, the German strings 'Pulsar' and 'Pulsar-' are both given correct associations in the next pass (lines 17 and 20 of Table 2)." ></td>
	<td class="line x" title="242:392	The table shows two interesting effects of the morphological analysis procedure." ></td>
	<td class="line x" title="243:392	The word 'shower' is wrongly associated with the word 'Gammaquant' (25) with a frequency of 6, but the prefix 'shower-' is correctly associated with 'Luftschauer-' 131 Computational Linguistics Volume 19, Number 1 ~J g i  ~  :  _  ~__t1.~,  3 3~3 2 1  1 t  t  1 1 1   '1  ~   -3----~  150 160 170 180 190 200 English Sentence No Figure 2 The SAT after pass 1." ></td>
	<td class="line x" title="244:392	(18) with a frequency of 20." ></td>
	<td class="line x" title="245:392	On the other hand, the incorrect association of 'element' with '-usammensetzung' (39) is on the basis of a normalized form (for words ending in 'Zusarnmensetzung'), whereas 'Zusammensetzung,' unnormalized, is correctly associated with 'composition' (35)." ></td>
	<td class="line x" title="246:392	Totally unrelated words are associated in a few instances, as in 'Observatory'/'diesem' (24), 'detectors'/'prim~ire-' (36), and 'bright-'/'Astronom-' (48)." ></td>
	<td class="line x" title="247:392	Of these only the second remains at the end of the third pass." ></td>
	<td class="line x" title="248:392	The English 'Observatory' is then properly associated with the German word 'Observatorium-'." ></td>
	<td class="line x" title="249:392	At that stage, 'bright-' has no association." ></td>
	<td class="line x" title="250:392	Figure 2 shows part of the SAT at the end of pass I of the relaxation cycle." ></td>
	<td class="line x" title="251:392	Sentences in the English text and in the German text are identified by numbers on the abscissa and the ordinate respectively." ></td>
	<td class="line x" title="252:392	Entries in the array indicate that the sentences are considered to correspond." ></td>
	<td class="line x" title="253:392	The numbers show how often a particular association is supported, which is essentially equivalent to how many word pairs in the WAT support such an association." ></td>
	<td class="line x" title="254:392	If there are no such numbers, then no associations have been found for it at this stage." ></td>
	<td class="line x" title="255:392	For example, the association of English sentence 148 with German sentence 170 is supported by three different word pairs." ></td>
	<td class="line x" title="256:392	It is already very striking how strongly occupied entries in this table constrain the possible entries in the unoccupied slots." ></td>
	<td class="line x" title="257:392	Figure 3 shows part of the AST before pass 2." ></td>
	<td class="line x" title="258:392	This is derived directly from the material illustrated in Figure 2." ></td>
	<td class="line x" title="259:392	The abscissa gives the English sentence number and in direction of the ordinate the associated German sentences are shown (bullet)." ></td>
	<td class="line x" title="260:392	Those sentence pairs in Figure 2 supported by at least three word pairs, namely those shown on lines 148, 192, 194, and 196, are assumed to be reliable, and they are the only associations shown for these sentences in Figure 3." ></td>
	<td class="line x" title="261:392	Candidate associations have been provided for the intervening sentences by the interpolation method described above." ></td>
	<td class="line x" title="262:392	Notice that the greatest number of candidates are shown against sentences occurring midway between a pair assumed to have been reliably connected (English sentence numbers 169 to 171)." ></td>
	<td class="line x" title="263:392	Table 2 shows the first 100 entries of the WAT after pass 3, where the threshold 132 Martin Kay and Martin R6scheisen Text-Translation Alignment Table 2 The WAT after pass 3." ></td>
	<td class="line x" title="264:392	English German Eng." ></td>
	<td class="line x" title="265:392	Freq." ></td>
	<td class="line x" title="266:392	Similarity 1 interstellar interstellare12 1 2 ergs Joule 10 1 3 per plro 12 11/12 4 universUnivers8 7/8 5 protoProto14 13/15 6 X-rayR6ntgen19 19/22 7 protonProton13 6/7 8 volts Elektronenvolt 19 9 / 11 9 photoPhoto14 13/16 10 lightLicht23 21/26 11 earth Erde 9 4/5 12 acceleratebeschleunigt 9 7/9 13 object Objekt 9 7/9 14 Cygnus Cygnus 63 27/35 15 acceleratbeschleunig18 16/21 16 modelModell17 16/21 17 pulsars Pulsare8 3/4 18 cosmickosmische81 35/47 19 galaxy Milchstrat~e 19 17/23 20 pulsarPulsar17 14/19 21 electrons Elektronen 12 5/7 22 magnetic Magnetfeld11 5/7 23 showerLuftschauer20 17/24 24 telescopeTeleskop9 7/10 25 sourceQuelle40 33/49 26 SecondSekund20 2/3 27 lownied9 2/3 28 partTeil59 49/76 29 and und 69 9/14 30 electricelektrisch9 7/11 31 gammaGammastrahl61 27/43 32 gasGas16 5/8 33 relativrelativ11 8/13 34 atmospherAtmosphere10 8/13 35 direction -ichtung 10 3/5 36 years Jahre11 10/17 37 objectObjekt14 10/17 38 periodStunden11 7/12 39 electroelektr83 63 / 109 40 only Nur 18 15/26 41 source -uelle 26 4/7 42 photonPhoton10 4/7 43 high-energy hochenerg13 9/16 44 directions -ichtungen 8 5/9 45 thousandTausend8 5/9 46 stars Sterne11 6/11 47 number Anzahl 8 6/11 48 interactwechselwirk9 6/11 49 signal Signal12 7/13 50 the die496 313/582 51 energy Energie 28 22/41 52 waveWellen13 8 / 15 53 starStern29 9/17 54 sources Quellen 14 11/21 55 nudeAtom19 12/23 56 of ein304 1/2 57 not nicht 30 1/2 58 ray Gammaquant14 1/2 59 arrival Ankunfts9 1/2 133 Computational Linguistics Volume 19, Number 1 Table 2 (continued) The WAT after pass 3." ></td>
	<td class="line x" title="267:392	60 percent Prozent 6 1 61 ultrahighultraho5 1 62 galaxies Galaxien 5 1 63 composition Zusammensetzung 5 1 64 Crimean Krim 5 1 65 supernova Supernova-Explosion5 1 66 activaktiv5 1 67 synchrotron Sy.nchrotronstrahlung 5 1 68 detectors '' 5 1 p rlmare69 muons lvlyonen 4 1 70 massive Masse4 1 71 meteoriteMeteorit4 1 72 Low-energy niederenergetische4 1 73 Fermi Fermi4 1 74 decayZerfall4 1 75 discovery Entdeckung 4 1 76 limit Grenze 4 1 77 ground Erdboden 4 1 78  aa Ta 3 1 79 R~ert Rto~ert 3 1 80 mirrors Spiegel3 1 81 absorption Absorptionslinie3 1 82 David David 3 1 83 average Mittel3 1 84 !i~ht-years Lichtjahre 3 1 85 Neutrons Neutronen 3 1 86 GregoryGregory3 1 87 explosions Supernova-Explosionen 3 1 88 electrically elektrisch 3 1 89 electromagnetic elektromagnetische3 1 90 candidates Kandidaten 3 1 91 data Daten7 7/8 92 University Universit7 6 / 7 93 spinning rotier6 6 / 7 94 neutron Neutronenstern 7 6 / 7 95 p.roposed vorgeschlagen 6 5/6 96 nnes -inien 6 5/6 97 colleagueKollegen 4 4/5 98 interactions Wechselwirkungen 5 4/5 99 PhysicPhysik5 4/5 100 models Modelle4 4/5 for the similarity was lowered to 0.5." ></td>
	<td class="line x" title="268:392	As we pointed out earlier, most of the incorrect associations in Table 1 have been eliminated." ></td>
	<td class="line x" title="269:392	German 'Milchstrafge' (19) is not a translation of the English 'galaxy,' but the Milky Way is indeed a galaxy and 'the galaxy' is sometimes used in place of 'Milky Way' where the reference is clear." ></td>
	<td class="line x" title="270:392	The association between 'period-' and 'Stunden-' (38) is of a similar kind." ></td>
	<td class="line x" title="271:392	The words are strongly associated because of recurring phrases of the form 'in a 4.8-hour period'." ></td>
	<td class="line x" title="272:392	Figure 4 gives the SAT after pass 3." ></td>
	<td class="line x" title="273:392	It is immediately apparent, first, that the majority of the sentences have been associated with probable translations and, second, that many of these associations are very strongly supported." ></td>
	<td class="line x" title="274:392	For example, note that the correspondence between English sentence 190 and German sentence 219 is supported 21 times." ></td>
	<td class="line x" title="275:392	Using this table, it is in fact possible to locate the translation of a given English sentence to within two or three sentences in the German text, and usually more closely than that." ></td>
	<td class="line x" title="276:392	However, some ambiguities remain." ></td>
	<td class="line x" title="277:392	Some of the apparent anomalies come 134 Martin Kay and Martin R6scheisen Text-Translation Alignment 8 GO ~a  i  i  i  ~  i i ii." ></td>
	<td class="line x" title="278:392	t =  i  ::_i_LLi  i i ! i.=!tttt i'i i i E i .:tttt!ttt' i  \]  \]  r  ~ ~l;7-il-l''Jl !i i~  i  i  i =: tt tli ~ i  i  ~ ,.t-,-t.,,-ii.Li-'  i  i  i i!,,i!llli!!ii''i i i  i ;i;iii!-lilll,,'-T'   i  i   i--mti!i=  i  i  i  i ,iilii~'' i i i i i  i  i  l  150 160 170 180 190 200 English Sentence No Figure 3 The AST before pass 2." ></td>
	<td class="line x" title="280:392	from stylistic differences in the way the texts were presented in the two journals." ></td>
	<td class="line x" title="281:392	The practice of Scientific American is to collect sequences of paragraphs into a logical unit by beginning the first of them with an oversized letter." ></td>
	<td class="line x" title="282:392	This is not done in Spektrum der Wissenschafl, which instead provides a subheading at these points." ></td>
	<td class="line x" title="283:392	This therefore appears as an insertion in the translation." ></td>
	<td class="line x" title="284:392	Two such are sentences number 179 and 233, but our procedure has not created incorrect associations for them." ></td>
	<td class="line x" title="285:392	Recall that the alignment problem derives its interest from the fact that single sentences are sometimes translated as sequences of sentences and conversely." ></td>
	<td class="line x" title="286:392	These cases generally stand out strongly in the output that our method delivers." ></td>
	<td class="line x" title="287:392	For example, the English sentence pair (5, 6): Yet whereas many of the most exciting advances in astronomy have come from the detailed analysis of X-ray and radio sources, until recently the source of cosmic rays was largely a matter of speculation." ></td>
	<td class="line x" title="288:392	They seem to come from everywhere, raining down on the earth from all directions at a uniform rate." ></td>
	<td class="line x" title="289:392	is rendered in German by the single sentence (5): Dennoch blieben die Quellen der kosmischen Strahlung, die aus allen Richtungen gleichm~t~ig auf die Erde zu treffen scheint, bis vor kurzem reine Spekulation, w~ihrend einige der aufregendsten Fortschritte in der Astronomie aus dem detaillierten Studium von R6ntgenund Radiowellen herriihrten." ></td>
	<td class="line x" title="290:392	The second English sentence becomes a relative clause in the German." ></td>
	<td class="line x" title="291:392	More complex associations also show up clearly in the results." ></td>
	<td class="line x" title="292:392	For example, English sentences 218 and 219 are translated by German sentences 253, 254, and 255, 135 Computational Linguistics Volume 19, Number 1 o = o E 23 tl  ~  ~-~.!" ></td>
	<td class="line x" title="293:392	839~ 13  i  i   i   ~  6.i;~~.z~---~.Y.  3855  ~.~-~--~  3533,  ~-~-~  ~ ,  s~ 2 4 3 8 4  .4-s~_i  150 160 170 180 190 200 English Sentence No Figure 4 The SAT after pass 3." ></td>
	<td class="line x" title="294:392	where 254 is a translation of the latter part of 218 and the early part of 219: When a proton strikes a gas nucleus, it produces three kinds of pion, of which one kind decays into two gamma rays." ></td>
	<td class="line x" title="295:392	The gamma rays travel close to the original trajectory of the proton, and the model predicts they will be beamed toward the earth at just two points on the pulsars orbit around the companion star." ></td>
	<td class="line x" title="296:392	Trifft ein Proton auf einen Atomkern in dieser Gash/.ille, werden drei Arten von Pionen erzeugt." ></td>
	<td class="line x" title="297:392	Die neutralen Pionen zerfallen in jeweils zwei Gammaquanten, die sich beinahe in dieselbe Richtung wie das ursprfingliche Proton bewegen." ></td>
	<td class="line x" title="298:392	Nach der Modellvorstellung gibt es gerade zwei Positionen im Umlauf des Pulsars um seinen Begleitstern, bei denen die Strahlung in Richtung zum Beobachter auf der Erde ausgesandt wird." ></td>
	<td class="line x" title="299:392	Another example is provided by English sentences 19 and 20, which appear in German as sentences 21 and 22." ></td>
	<td class="line x" title="300:392	However the latter part of English sentence 19 is in fact transferred to sentence 22 in the German." ></td>
	<td class="line x" title="301:392	This is also unmistakable in the final results." ></td>
	<td class="line x" title="302:392	Notice also, in this example, that the definition of 'photon' has become a parenthetical expression at the beginning of the second German sentence, a fact which is not reflected." ></td>
	<td class="line x" title="303:392	The other end of the cosmic-ray energy spectrum is defined somewhat arbitrarily: any quantum greater than 108 electron volts arriving from space is considered a cosmic ray." ></td>
	<td class="line x" title="304:392	The definition encompasses not only particles but also gamma-ray photons, which are quanta of electromagnetic radiation." ></td>
	<td class="line x" title="305:392	136 Martin Kay and Martin R6scheisen Text-Translation Alignment Table 3 Correctness of sentence alignment in the various passes of the algorithm." ></td>
	<td class="line x" title="306:392	Pass Correctness Coverage Constraint in SAT of SAT by AST 1 100 % 12 % 4 % 2 100 % 47 % 17 % 3 100 % 89 % 38 % 4 99.7 % 96 % 41 % Das untere Ende des Spektrums der kosmischen Strahlen ist verh~iltnism~it~ig unscharf definiert." ></td>
	<td class="line x" title="307:392	Jedes Photon (Quant der elektromagnetischen Strahlung) oder Teilchen mit einer Energie von mehr als 10 s Elektronenvolt, das aus dem Weltraum eintrifft, bezeichnet man als kosmischen Strahl." ></td>
	<td class="line x" title="308:392	It frequently occurred in our data that sentences that were separated by colons or semicolons in the original appeared as completely distinct sentences in the German translation." ></td>
	<td class="line x" title="309:392	Indeed, the common usage in the two languages would probably have been better represented if we had treated colons and semicolons as sentence separators, along with periods, question marks, and the like." ></td>
	<td class="line x" title="310:392	There are, of course, situations in English in which these punctuation marks are used in other ways, but they are considerably less frequent and, in any case, it seems that our program would almost always make the right associations." ></td>
	<td class="line x" title="311:392	An example involving the colon is to be found in sentence 142 of the original, translated as sentences 163 and 164: The absorption lines established a lower limit on the distance of Cygnus X-3: it must be more distant than the farthest hydrogen cloud, which is believed to lie about 37,000 light-years away, near the edge of the galaxy." ></td>
	<td class="line x" title="312:392	Aus dieser Absorptionslinie kann man eine untere Grenze der Entfernung von Cygnus X bestimmen." ></td>
	<td class="line x" title="313:392	Die Quelle mud jenseits der am weitesten entfernten Wasserstoff-Wolke sein, also weiter als ungefahr 37000 Lichtjahre entfernt, am Rande der Milchstrat~e. English sentence 197, containing a semicolon, is translated by German sentences 228 and 229: The estimate is conservative; because it is based on the gamma rays observed arriving at the earth, it does not take into account the likelihood that Cygnus X emits cosmic rays in all directions." ></td>
	<td class="line x" title="314:392	Dies ist eine vorsichtige Abschatzung." ></td>
	<td class="line x" title="315:392	Sie ist nur aus den Gammastrahlen-Daten abgeleitet, die auf der Erde gemessen werden; dat~ Cygnus X-3 wahrscheinlich kosmische Strahlung in alle Richtungen aussendet, ist dabei noch nicht ber~icksichtigt." ></td>
	<td class="line x" title="316:392	137 Computational Linguistics Volume 19, Number 1 8 c E $ o oX ee  | . ' oX  ooX  ' X @x' .~X Ix X ' xX  X  . 0 10 20 30 40 English Sentence No 50 Figure 5 Sentence alignment of the first 50 sentences of the test texts: true alignment (dots) and hypothesis of the SAT after the first pass (circles) and after the second pass (crosses)." ></td>
	<td class="line x" title="317:392	Table 3 summarizes the accuracy of the algorithm as a function of the number of passes." ></td>
	<td class="line x" title="318:392	The (thresholded) SAT is evaluated by two criteria: the number of correct alignments divided by the total number of alignments, and--since the SAT does not necessarily give an alignment for every sentence---the coverage, i.e., the number of sentences with at least one entry relative to the total number of sentences." ></td>
	<td class="line x" title="319:392	An alignment is said to be correct if the SAT contains exactly the numbers of the sentences that are complete or partial translations of the original sentence." ></td>
	<td class="line x" title="320:392	The coverage of 96% of the SAT in pass 4 is as much as one would expect, since the remaining nonaligned sentences are one-zero alignments, most of them due to the German subheadings that are not part of the English version." ></td>
	<td class="line x" title="321:392	The table also shows that the AST always provides a significant number of candidates for alignment with each sentence before a pass: the fourth column gives the number of true sentence alignments relative to the total number of candidates in the AST." ></td>
	<td class="line x" title="322:392	Recall that the final alignment is always a subset of the hypotheses in the AST in every preceding pass." ></td>
	<td class="line x" title="323:392	Figure 5 shows the true sentence alignment for the first 50 sentences (dots), and how the algorithm discovered them: in the first pass, only a few sentences are set into correspondence (circles); after the second pass (crosses) already almost half of the correspondences are found." ></td>
	<td class="line x" title="324:392	Note that there are no wrong alignments in the first two passes." ></td>
	<td class="line x" title="325:392	In the third pass, almost all of the remaining alignments are found (for the first 50 sentences in the figure: all), and a final pass usually completes the alignment." ></td>
	<td class="line x" title="326:392	Our algorithm produces very favorable results when allowed to converge gradually." ></td>
	<td class="line x" title="327:392	Processing time in the original LISP implementation was high, typically several hours for each pass." ></td>
	<td class="line x" title="328:392	By trading CPU time for memory massively, the time needed by a C++ implementation on a Sun 4/75 was reduced to 1.7 min for the first pass, 0.8 min for the second, and 0.5 min for the third pass in an application to this pair of articles." ></td>
	<td class="line x" title="329:392	(Initialization, i.e., reading the files and building up the data structures, takes another 0.6 min in the beginning)." ></td>
	<td class="line x" title="330:392	It should be noted that a naive implementation of 138 Martin Kay and Martin R6scheisen Text-Translation Alignment the algorithm without using the appropriate data structures can easily lead to times that are a factor of 30 higher and do not scale up to larger texts." ></td>
	<td class="line x" title="331:392	The application of our method to a text that we put together from the Hansard corpus had essentially no problem in identifying the correct sentence alignment in a process of five passes." ></td>
	<td class="line x" title="332:392	The alignments for the first 1000 sentences of the English text were checked by hand, and seven errors were found; five of them occurred in sentences where sentence boundaries were not correctly identified by the program because of periods that did not mark a sentence boundary and were not identified as such by a very simple preprocessing program." ></td>
	<td class="line x" title="333:392	The other two errors involved two short sentences for which the SAT did not give an alignment." ></td>
	<td class="line x" title="334:392	Processing time increased essentially linearly (per pass): the first pass took 8.3 min, the second 3.2 min, and it further decreased until the last pass, which took 2.1 min." ></td>
	<td class="line x" title="335:392	(Initialization took 4.2 min)." ></td>
	<td class="line x" title="336:392	Note that the error rate depends crucially on the kind of 'annealing schedule' used: if the thresholds that allow a word pair in the WAT to influence the SAT are lowered too fast, only a few passes are needed, but accuracy deteriorates." ></td>
	<td class="line x" title="337:392	For example, in an application where the process terminated after only three passes, the accuracy was only in the eighties (estimated on the basis of the first 120 sentences of the English Hansard text checked by hand)." ></td>
	<td class="line x" title="338:392	Since processing time after the first pass is usually already considerably lower, we have found that a high accuracy can safely be attained when more passes are allowed than are actually necessary." ></td>
	<td class="line x" title="339:392	In order to evaluate the sensitivity of the algorithm to the lengths of the texts that are to be aligned, we applied it to text samples that ranged in length from 10 to 1000 sentences, and examined the accuracy of the WAT after the first pass; that is, more precisely, the number of word pairs in the WAT that are valid translations relative to the total number of word pairs with a similarity of not less than 0.7 (the measurements are cross-validated over different texts)." ></td>
	<td class="line x" title="340:392	The result is that this accuracy increases asymptotically to 1 with the text length, and is already higher than 80% for a text length of 100 sentences (which is sufficient to reach an almost perfect alignment in the end)." ></td>
	<td class="line x" title="341:392	Roughly speaking, the accuracy is almost 1 for texts longer than 150 sentences, and around 0.5 for text length in the lower range from 20 to 60." ></td>
	<td class="line x" title="342:392	In other words, texts of a length of more than 150 sentences are suitable to be processed in this way; text fragments shorter than 80 sentences do not have a high proportion of correct word pairs in the first WAT, but further experiments showed that the final alignment for texts of this length is, on average, again almost perfect: the drawback of a less accurate initial WAT is apparently largely compensated for by the fact that the AST is also narrower for these texts; however, the variance in the alignment accuracies is significantly higher." ></td>
	<td class="line x" title="343:392	5." ></td>
	<td class="line x" title="344:392	Related Work Since we addressed the text translation alignment problem in 1988, a number of researchers, among them Gale and Church (1991) and Brown, Lai, and Mercer (1991), have worked on the problem." ></td>
	<td class="line x" title="345:392	Both methods are based on the observation that the length of text unit is highly correlated to the length of the translation of this unit, no matter whether length is measured in number of words or in number of characters (see Figure 6)." ></td>
	<td class="line x" title="346:392	Consequently, they are both easier to implement than ours, though not necessarily more efficient." ></td>
	<td class="line x" title="347:392	The method of Brown, Lai, and Mercer (1991) is based on a hidden Markov model for the generation of aligned pairs of corpora, whose parameters are estimated from a large text." ></td>
	<td class="line x" title="348:392	For an application of this method to the Canadian Hansard, good results are reported." ></td>
	<td class="line x" title="349:392	However, the problem was also considerably facilitated by the way the implementation made use of Hansard-specific comments 139 Computational Linguistics Volume 19, Number 1  ~  Re  S. d .S 40 60 80 120 English: Length in words 'i f, % o 200 400 600 800 English: Length in chars Figure 6 Lengths of Aligned Paragraphs are Correlated: Robust regression between lengths of aligned paragraphs." ></td>
	<td class="line x" title="350:392	Left: length measured in words." ></td>
	<td class="line x" title="351:392	Right: length measured in characters." ></td>
	<td class="line x" title="352:392	and annotations: these are used in a preprocessing step to find anchors for sentence alignment such that, on average, there are only ten sentences in between." ></td>
	<td class="line x" title="353:392	Moreover, this particular corpus is well known for the near literalness of its translations, and it is therefore unclear to what extent the good results are due to the relative ease of the problem." ></td>
	<td class="line x" title="354:392	This would be an important consideration when comparing various algorithms; when the algorithms are actually applied, it is clearly very desirable to incorporate as much prior knowledge (say, on potential anchors) as possible." ></td>
	<td class="line x" title="355:392	Moreover, long texts can almost always be expected to contain natural anchors, such as chapter and section headings, at which to make an a priori segmentation." ></td>
	<td class="line x" title="356:392	Gale and Church (1991) note that their method performed considerably better when lengths of sentences were measured in number of characters instead of in number of words." ></td>
	<td class="line x" title="357:392	Their method is based on a probabilistic model of the distance between two sentences, and a dynamic programming algorithm is used to minimize the total distance between aligned units." ></td>
	<td class="line x" title="358:392	Their implementation assumes that each character in one language gives rise to, on average, one character in the other language." ></td>
	<td class="line x" title="359:392	8 In our texts, one character in English on average gives rise to somewhat more than 1.2 characters in German, and the correlation between the lengths (in characters) of aligned paragraphs in the two languages was with 0.952 lower than the 0.991 that are mentioned in Gale and Church (1991), which supports our impression that the Scientific American texts we used are hard texts to align, but it is not clear to what extent this would deteriorate the results." ></td>
	<td class="line x" title="360:392	In applications to economic reports from the Union Bank of Switzerland, the method performs very well on simple alignments (one-to-one, oneto-two), but has at the moment problems with complex matches." ></td>
	<td class="line x" title="361:392	The method has the 8 Recall that, in a similar way, we assumed in our implementation that one sentence in one language gives rise to, on average, n/m sentences in the other language (see first footnote in Section 2.3)." ></td>
	<td class="line x" title="362:392	140 Martin Kay and Martin R6scheisen Text-Translation Alignment advantage of associating a score with pairs of sentences so that it is easy to extract a subset for which there is a high likelihood that the alignments are correct." ></td>
	<td class="line x" title="363:392	Given the simplicity of the methods proposed by Brown, Lai, and Mercer and Gale and Church, either of them could be used as a heuristic in the construction of the initial AST in our algorithm." ></td>
	<td class="line x" title="364:392	In the current version, the number of candidate sentence pairs that are considered in the first pass near the middle of a text contributes disproportionally to the cost of the computation." ></td>
	<td class="line x" title="365:392	In fact, as we remarked earlier, the complexity of this step is O(nvFff)." ></td>
	<td class="line x" title="366:392	The proposed modification would effectively make it linear." ></td>
	<td class="line x" title="367:392	6." ></td>
	<td class="line x" title="368:392	Future Work For most practical purposes, the alignment algorithm we have described produces very satisfactory results, even when applied to relatively free translations." ></td>
	<td class="line x" title="369:392	There are doubtless many places in which the algorithm itself could be improved." ></td>
	<td class="line x" title="370:392	For example, it is clear that the present method of building the SAT favors associations between long sentences, and this is not surprising, because there is more information in long sentences." ></td>
	<td class="line x" title="371:392	But we have not investigated the extent of this bias and we do not therefore know it as appropriate." ></td>
	<td class="line x" title="372:392	The present algorithm rests on being able to identify one-to-one associations between certain words, notably technical terms and proper names." ></td>
	<td class="line x" title="373:392	It is clear from a brief inspection of Table 2 that very few correspondences are noticed among everyday words and, when they are, it is usually because those words also have precise technical uses." ></td>
	<td class="line x" title="374:392	The very few exceptions include 'only'/'nur' and 'the' / 'die-'." ></td>
	<td class="line x" title="375:392	The pair 'per' / 'pro' might also qualify, but if the languages afford any example of a scientific preposition, this is surely it." ></td>
	<td class="line x" title="376:392	The most interesting further developments would be in the direction of loosening up this dependence on one-to-one associations both because this would present a very significant challenge and also because we are convinced that our present method identifies essentially all the significant one-to-one associations." ></td>
	<td class="line x" title="377:392	There are two obvious kinds of looser associations that could be investigated." ></td>
	<td class="line x" title="378:392	One would consist of connections between a single vocabulary item in one language and two or more in the other, or even between several items in one language and several in the other." ></td>
	<td class="line x" title="379:392	The other would involve connections--one-one, one-many, or many-many--between phrases or recurring sequences." ></td>
	<td class="line x" title="380:392	We have investigated the first of these enough to satisfy ourselves that there is latent information on one-to-many associations in the text, and that it can be revealed by suitable extensions of our methods." ></td>
	<td class="line x" title="381:392	However, it is clear that the combinatorial problems associated with this approach are severe, and pursuing it would require much fine tuning of the program and designing much more effective ways of indexing the most important data structures." ></td>
	<td class="line x" title="382:392	The key to reducing the combinatorial explosion probably lies in using tables of similarities such as those the present algorithm uses to suggest combinations of items that would be worth considering." ></td>
	<td class="line x" title="383:392	If such an approach could be made efficient enough, it is even possible that it would provide a superior way of solving the problem for which our heuristic methods of morphological analysis were introduced." ></td>
	<td class="line x" title="384:392	Its superiority would come from the fact that it would not depend on words being formed by concatenation, but would also accommodate such phenomena as umlaut, ablaut, vowel harmony, and the nonconcatenative process of Semitic morphology." ></td>
	<td class="line x" title="385:392	The problems of treating recurring sequences are less severe." ></td>
	<td class="line x" title="386:392	Data structures, such as the Patricia tree (Knuth 1973; pp." ></td>
	<td class="line x" title="387:392	490-493) provide efficient means of identifying all such sequences and, once identified, the data they provide could be added to 141 Computational Linguistics Volume 19, Number 1 the WAT much as we now add the results of morphological analysis." ></td>
	<td class="line x" title="388:392	Needless to say, this would only allow for uninterrupted sequences." ></td>
	<td class="line x" title="389:392	Any attempt to deal with discontinuous sequences would doubtless also involve great combinatorial problems." ></td>
	<td class="line x" title="390:392	These avenues for further development are intriguing and would surely lead to interesting results." ></td>
	<td class="line x" title="391:392	But it is unlikely that they would lead to much better sets of associations among sentences than are to be found in the SATs that our present program produces, and it was mainly these results that we were interested in from the outset." ></td>
	<td class="line x" title="392:392	The other avenues we have mentioned concern improvements in the WAT which, for us, was always a secondary interest." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J93-2005
Lexical Semantic Techniques For Corpus Analysis
Pustejovsky, James D.;Anick, Peter G.;Bergler, Sabine;"></td>
	<td class="line x" title="1:423	Lexical Semantic Techniques for Corpus Analysis James Pustejovsky* Brandeis University Peter Anick~ Digital Equipment Corporation Sabine Bergler t Concordia University In this paper we outline a research program for computational linguistics, making extensive use of text corpora." ></td>
	<td class="line x" title="2:423	We demonstrate how a semantic framework for lexical knowledge can suggest richer relationships among words in text beyond that of simple co-occurrence." ></td>
	<td class="line x" title="3:423	The work suggests how linguistic phenomena such as metonymy and polysemy might be exploitable for semantic tagging of lexical items." ></td>
	<td class="line x" title="4:423	Unlike with purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems." ></td>
	<td class="line x" title="5:423	We illustrate the approach for the acquisition oflexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary." ></td>
	<td class="line x" title="6:423	In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools." ></td>
	<td class="line x" title="7:423	Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses." ></td>
	<td class="line x" title="8:423	1." ></td>
	<td class="line x" title="9:423	Introduction The proliferation of on-line textual information poses an interesting challenge to linguistic researchers for several reasons." ></td>
	<td class="line x" title="10:423	First, it provides the linguist with sentence and word usage information that has been difficult to collect and consequently largely ignored by linguists." ></td>
	<td class="line x" title="11:423	Second, it has intensified the search for efficient automated indexing and retrieval techniques." ></td>
	<td class="line x" title="12:423	FulMext indexing, in which all the content words in a document are used as keywords, is one of the most promising of recent automated approaches, yet its mediocre precision and recall characteristics indicate that there is much room for improvement (Croft 1989)." ></td>
	<td class="line x" title="13:423	The use of domain knowledge can enhance the effectiveness of a full-text system by providing related terms that can be used to broaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988; Anick et al. 1989." ></td>
	<td class="line x" title="14:423	Likewise, domain knowledge may be applied at indexing time to do word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs 1991)." ></td>
	<td class="line x" title="15:423	Unfortunately, for many domains, such knowledge, even in the form of a thesaurus, is either not available or is incomplete with respect to the vocabulary of the texts indexed." ></td>
	<td class="line x" title="16:423	* Computer Science Department, Brandeis University, Waltham MA 02254." ></td>
	<td class="line x" title="17:423	t Computer Science Department, Concordia University, Montreal, Quebec H3G 1M8, Canada." ></td>
	<td class="line x" title="18:423	Digital Equipment Corporation, 111 Locke Drive LM02-1/D12, Marlboro MA 01752." ></td>
	<td class="line x" title="19:423	(~) 1993 Association for Computational Linguistics Computational Linguistics Volume 19, Number 2 In this paper we examine how linguistic phenomena such as metonymy and polysemy might be exploited for the semantic tagging of lexical items." ></td>
	<td class="line x" title="20:423	Unlike purely statistical collocational analyses, employing a semantic theory allows for the automatic construction of deeper semantic relationships among words appearing in collocational systems." ></td>
	<td class="line x" title="21:423	We illustrate the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary." ></td>
	<td class="line x" title="22:423	In addition to conventional lexical semantic relations, we show how information concerning lexical presuppositions and preference relations (Wilks 1978) can also be acquired from corpora, when analyzed with the appropriate semantic tools." ></td>
	<td class="line x" title="23:423	Finally, we discuss the potential that corpus studies have for enriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirm linguistic hypotheses." ></td>
	<td class="line x" title="24:423	The aim of our research is to discover what kinds of knowledge can be reliably acquired through the use of these methods, exploiting, as they do, general linguistic knowledge rather than domain knowledge." ></td>
	<td class="line x" title="25:423	In this respect, our program is similar to Zernik's (1989) work on extracting verb semantics from corpora using lexical categories." ></td>
	<td class="line x" title="26:423	Our research, however, differs in two respects: first, we employ a more expressive lexical semantics; second, our focus is on all major categories in the language, and not just verbs." ></td>
	<td class="line x" title="27:423	This is important since for full-text information retrieval, information about nominals is paramount, as most queries tend to be expressed as conjunctions of nouns." ></td>
	<td class="line x" title="28:423	From a theoretical perspective, we believe that the contribution of the lexical semantics of nominals to the overall structure of the lexicon has been somewhat neglected, relative to that of verbs." ></td>
	<td class="line x" title="29:423	While Zernik (1989) presents ambiguity and metonymy as a potential obstacle to effective corpus analysis, we believe that the existence of motivated metonymic structures actually provides valuable clues for semantic analysis of nouns in a corpus." ></td>
	<td class="line x" title="30:423	We will assume, for this paper, the general framework of a generative lexicon as outlined in Pustejovsky (1991)." ></td>
	<td class="line x" title="31:423	In particular, we make use of the principles of type coercion and qualia structure." ></td>
	<td class="line x" title="32:423	This model of semantic knowledge associated with words is based on a system of generative devices that is able to recursively define new word senses for lexical items in the language." ></td>
	<td class="line x" title="33:423	These devices and the associated dictionary make up a generative lexicon, where semantic information is distributed throughout the lexicon to all categories." ></td>
	<td class="line x" title="34:423	The general framework assumes four basic levels of semantic description: argument structure, qualia structure, lexical inheritance structure, and event structure." ></td>
	<td class="line x" title="35:423	Connecting these different levels is a set of generative devices that provide for the compositional interpretation of words in context." ></td>
	<td class="line x" title="36:423	The most important of these devices is a semantic transformation called type coercion--analogous to coercion in programming languages--which captures the semantic relatedness between syntactically distinct expressions." ></td>
	<td class="line x" title="37:423	As an operation on types within a A-calculus, type coercion can be seen as transforming a monomorphic language into one with polymorphic types (cf.Cardelli and Wegner 1985)." ></td>
	<td class="line x" title="39:423	Argument, event, and qualia types must conform to the well-formedness conditions defined by the type system defined by the lexical inheritance structure when undergoing operations of semantic composition." ></td>
	<td class="line x" title="40:423	~ 1 The details of type coercion need not concern us here." ></td>
	<td class="line x" title="41:423	Briefly, however, whenever there exists a grammatical environment where more than one syntactic type satisfies the semantic type selected by the governing element, the governing element can be analyzed as coercing a range of surface types into a single semantic type." ></td>
	<td class="line x" title="42:423	An example of subject type coercion is a causative verb, semantically selecting an event as subject (as in (i)), but syntactically permitting a nonevent denoting NP (as in (ii)): i. The flood killed the grass." ></td>
	<td class="line x" title="43:423	ii." ></td>
	<td class="line x" title="44:423	The herbicide killed the grass." ></td>
	<td class="line x" title="45:423	332 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis One component of this approach, the qualia structure, specifies the different aspects of a word's meaning through the use of subtyping." ></td>
	<td class="line x" title="46:423	These include the subtypes CONSTITUTIVE, FORMAL, TELIC, and AGENTIVE." ></td>
	<td class="line x" title="47:423	To illustrate how these are used, the qualia structure for book is given below." ></td>
	<td class="line x" title="48:423	2  book(x,y)." ></td>
	<td class="line x" title="49:423	\] GONST = information(yl \] FORMAL = physobj(x) \[ TELIC = read(T,w,y) \[ AGENTIVE = write(T,z,y) J This structured representation allows one to use the same lexical entry in different contexts, where the word refers to different qualia of the noun's denotation." ></td>
	<td class="line x" title="50:423	For example, the sentences in (1)-(3) below refer to different aspects (or qualia) of the general meaning of book." ></td>
	<td class="line x" title="51:423	3 Example 1 This book weighs four ounces." ></td>
	<td class="line x" title="52:423	Example 2 John finished a book." ></td>
	<td class="line x" title="53:423	Example 3 This is an interesting book." ></td>
	<td class="line x" title="54:423	Example 1 makes reference to the formal role, while 3 refers to the constitutive role." ></td>
	<td class="line x" title="55:423	Example 2, however, can refer to either the telic or the agentive aspects given above." ></td>
	<td class="line x" title="56:423	The utility of such knowledge for information retrieval is readily apparent." ></td>
	<td class="line x" title="57:423	This theory claims that noun meanings should make reference to related concepts and the relations into which they enter." ></td>
	<td class="line x" title="58:423	The qualia structure, thus, can be viewed as a kind of generic template for structuring this knowledge." ></td>
	<td class="line x" title="59:423	Such information about how nouns relate to other lexical items and their concepts might prove to be much more useful in full-text information retrieval than what has come from standard statistical techniques." ></td>
	<td class="line x" title="60:423	To illustrate how such semantic structuring might be useful, consider the general class of artifact nouns." ></td>
	<td class="line x" title="61:423	A generative view of the lexicon predicts that by classifying an element into a particular category, we can generate many aspects of its semantic structure, and hence, its syntactic behavior." ></td>
	<td class="line x" title="62:423	For example, the representation above for book refers to several word senses, all of which are logically related by the semantic template for an artifactual object." ></td>
	<td class="line x" title="63:423	That is, it contains information, it has a material extension, it serves some function, and it is created by some particular act or event." ></td>
	<td class="line x" title="64:423	2 Briefly, the qualia can be defined as follows:  CONSTITUTIVE: the relation between an object and its constituent parts;  FORMAL: that which distinguishes it within a larger domain;  TELIC: its purpose and function;  AGENTIVE: factors involved in its origin or 'bringing it about'." ></td>
	<td class="line x" title="65:423	In the qualia structures given below, we adopt the convention that \[c~, r\] denotes conjunction of formulas within the feature structure, while \[a; r\] will denote disjunction." ></td>
	<td class="line x" title="66:423	3 A related approach for expressing the different semantic relations of nominals in distinguished contexts is given in Bierwisch (1983)." ></td>
	<td class="line x" title="67:423	333 Computational Linguistics Volume 19, Number 2 Such an analysis allows us to minimally structure objects according to these four qualia." ></td>
	<td class="line x" title="68:423	As an example of how objects cluster according to these dimensions, we will briefly consider three object types: (1) containers (of information), e.g., book, tape, record; (2) instruments, e.g., gun, hammer, paintbrush; and (3) figure-ground objects, e.g., door, room, fireplace." ></td>
	<td class="line x" title="69:423	Because of how their qualia structures differ, these classes appear in vastly different grammatical contexts." ></td>
	<td class="line x" title="70:423	As with containers in general, information containers permit metonymic extensions between the container and the material contained within it." ></td>
	<td class="line x" title="71:423	Collocations such as those in Examples 4 through 7 indicate that this metonymy is grammaticalized through specific and systematic head-PP constructions." ></td>
	<td class="line x" title="72:423	Example 4 read a book Example 5 read a story in a book Example 6 read a tape Example 7 read the information on the tape Instruments, on the other hand, display classic agent-instrument causative alternations, such as those in Examples 8 through 11 (cf.Fillmore 1968; Lakoff 1968, 1970)." ></td>
	<td class="line x" title="74:423	Example 8  smash the vase with the hammer Example 9 The hammer smashed the vase." ></td>
	<td class="line x" title="75:423	Example 10  kill him with a gun Example 11 The gun killed him." ></td>
	<td class="line x" title="76:423	Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspective shifts such as those in Examples 12 through 15." ></td>
	<td class="line x" title="77:423	These are nouns that refer to physical objects as well as the specific enclosure or aperture associated with it." ></td>
	<td class="line x" title="78:423	Example 12 John painted the door." ></td>
	<td class="line x" title="79:423	Example 13 John walked through the door." ></td>
	<td class="line x" title="80:423	334 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Example 14 John is scrubbing the fireplace." ></td>
	<td class="line x" title="81:423	Example 15 The smoke filled the fireplace." ></td>
	<td class="line x" title="82:423	That is, paint and scrub are actions on physical objects while walk through and fill are processes in spaces." ></td>
	<td class="line x" title="83:423	These collocational patterns, we argue, are systematically predictable from the lexical semantics of the noun, and we term such sets of collocated structures lexical conceptual paradigms (LCPs)." ></td>
	<td class="line x" title="84:423	4 To make this point clearer, let us consider a specific example of an LCP from the computer science domain, namely for the noun tape." ></td>
	<td class="line x" title="85:423	Because of the particular metonymy observed for a noun like tape, we will classify it as belonging to the container/containee LCP." ></td>
	<td class="line x" title="86:423	This general class is represented as follows, where P and 0 are predicate variables: 5 container(x,y) \] CONST = P(y) FORMAL = Q(x) TELIC = hold(S,x,y) The LCP is a generic qualia structure that captures not only the semantic relationship between arguments types of a relation, but also, through corpus-tuning, the collocation relations that realize these roles." ></td>
	<td class="line x" title="87:423	The telic function of a container, for example, is the relation hold, but this underspecifies which spatial prepositions would adequately satisfy this semantic relation (e.g. in, on, inside, etc.)." ></td>
	<td class="line x" title="88:423	In this view, a noun such as tape would have the following qualia structure: tape(x,y) CONST = information(y) FORMAL = physobj(x),2-dimen(x) TELIC = contain(S,x,y) AGENTIVE = write(T,z,y) This states that a tape is an 'information container' that is also a two-dimensional physical object, where the information is written onto the object." ></td>
	<td class="line x" title="89:423	6 With such nouns, a logical metonymy exists (as the result of type coercion), when the logical argument of a semantic type, which is selected by a function of some sort, denotes the semantic type itself." ></td>
	<td class="line x" title="90:423	Thus, in this example, the type selected for by a verb such as read refers to the 'information' argument for tape, while a verb such as carry would select for the 'physical object' argument." ></td>
	<td class="line x" title="91:423	They are, however, logically related, since the noun itself denotes a relation." ></td>
	<td class="line x" title="92:423	The representation above simply states that any semantics for tape must logically make reference to the object itself (formal), what it can contain (const), what purpose 4 This relates to Mel'~uk's lexical functions and the syntactic structures they associate with an element." ></td>
	<td class="line x" title="93:423	See Mel'~uk (1988) and references therein." ></td>
	<td class="line x" title="94:423	Cruse (1986, 1992) and Nunberg (1978) discuss the foregrounding and backgrounding of information with respect to similar examples." ></td>
	<td class="line x" title="95:423	5 Within the qualia structure for a term, FORMAL and CONST roles typically refer to the object domain while TELI and ACENTIVE refer to events." ></td>
	<td class="line x" title="96:423	Hence, the first parameter in the latter two roles refers to an event sort, i.e., a state (s), process (p), or transition (T)." ></td>
	<td class="line x" title="97:423	6 The appropriate selection of a surface spatial preposition will follow from its formal type specification as a 2-dimen object." ></td>
	<td class="line x" title="98:423	Cf." ></td>
	<td class="line x" title="99:423	Pustejovsky (in press) for details." ></td>
	<td class="line x" title="100:423	335 Computational Linguistics Volume 19, Number 2 it serves (telic), and how it arises (agentive)." ></td>
	<td class="line x" title="101:423	This provides us with a semantic representation that can capture the multiple perspectives a single lexical item may assume in different contexts." ></td>
	<td class="line x" title="102:423	Yet, the qualia for a lexical item such as tape are not isolated values for that one word, but are integrated into a global knowledge base indicating how these senses relate to other lexical items and their senses." ></td>
	<td class="line x" title="103:423	This is the contribution of inheritance and the hierarchical structuring of knowledge (cf.Evans and Gazdar 1990; Copestake and Briscoe 1992; Russell et al. 1992)." ></td>
	<td class="line x" title="105:423	In Pustejovsky (1991) it is suggested that there are two types of relational structures for lexical knowledge; a fixed inheritance similar to that of an is-a hierarchy (cf.Touretzky 1986); and a dynamic structure that operates generatively from the qualia structure of a lexical item to create a relational structure for ad hoc categories." ></td>
	<td class="line x" title="107:423	7 Reviewing briefly, the basic idea is that semantics allows for the dynamic creation of arbitrary concepts through the application of certain transformations to lexical meanings." ></td>
	<td class="line x" title="108:423	Thus for every predicate, Q, we can generate its opposition, =Q. Similarly, these two predicates can be related temporally to generate the transition events defining this opposition." ></td>
	<td class="line x" title="109:423	These operations include but may not be limited to: -~, negation; _<, temporal precedence; >, temporal succession; =, temporal equivalence; and act, an operator adding agency to an argument." ></td>
	<td class="line x" title="110:423	We will call the concept space generated by these operations the Projective Conclusion Space of a specific quale for a lexical item." ></td>
	<td class="line x" title="111:423	To return to the example of tape above, the predicates read and copy are related to the telic value by just such an operation, while predicates such as mount and dismount--i.e, unmount--are related to the formal role." ></td>
	<td class="line x" title="112:423	Following the previous discussion, with mounted as the predicate Q, successive applications of the negation and temporal precedence operators derives the transition verbs mount and dismount." ></td>
	<td class="line x" title="113:423	8 We return to a discussion of this in Section 3, and to how this space relates to statistically significant collocations in text." ></td>
	<td class="line x" title="114:423	It is our view that the approach outlined above for representing lexical knowledge can be put to use in the service of information retrieval tasks." ></td>
	<td class="line x" title="115:423	In this respect, our proposal can be compared to attempts at object classification in information science." ></td>
	<td class="line x" title="116:423	One approach, known as faceted classification (Vickery 1975) proceeds roughly as follows: collect all terms lying within a field; then group the terms into facets by assigning them to categories." ></td>
	<td class="line x" title="117:423	Typical examples of this are state, property, reaction, and device." ></td>
	<td class="line x" title="118:423	However, each subject area is likely to have its own sets of categories, which makes it difficult to re-use a set of facet classifications." ></td>
	<td class="line x" title="119:423	9 Even if the relational information provided by the qualia structure and inheritance would improve performance in information retrieval tasks, one problem still remains, namely that it would be very time-consuming to hand-code such structures for all nouns in a domain." ></td>
	<td class="line x" title="120:423	Since it is our belief that such representations are generic structures across all domains, it is our long-term goal to develop methods for automatically extracting these relations and values from on-line corpora." ></td>
	<td class="line x" title="121:423	In the sections that follow, we describe several experiments indicating that the qualia structures do, in fact, correlate with well-behaved collocational patterns, thereby allowing us to perform structure-matching operations over corpora to find these relations." ></td>
	<td class="line x" title="122:423	7 This is similar to thesauruslike structures, within the IR community, cf.for example Sparck Jones (1981)." ></td>
	<td class="line x" title="124:423	8 Details of the derivation are as follows." ></td>
	<td class="line x" title="125:423	Let Q be mounted, then ~Q gives ~mounted, and K applied to these two states gives Q < -~Q, which is lexicalized as dismount." ></td>
	<td class="line x" title="126:423	A similar derivation exists for mount." ></td>
	<td class="line x" title="127:423	Cf." ></td>
	<td class="line x" title="128:423	Pustejovsky (1991) for details." ></td>
	<td class="line x" title="129:423	9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automated discovery procedures are aimed at clustering nouns into categories like diagnosis and symptom." ></td>
	<td class="line x" title="130:423	336 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis 2." ></td>
	<td class="line x" title="131:423	Seeding Lexical Structures from MRDs In this section we discuss briefly how a lexical semantic theory can help in extracting information from machine-readable dictionaries (MRDs)." ></td>
	<td class="line x" title="132:423	We describe research on conversion of a machine-tractable dictionary (Wilks et al. 1993) into a usable lexical knowledge base (Boguraev 1991)." ></td>
	<td class="line x" title="133:423	Although the results here are preliminary, it is important to mention the process of converting an MRD into a lexical knowledge base, so that the process of corpus-tuning is put into the proper perspective." ></td>
	<td class="line x" title="134:423	The initial seeding of lexical structures is being done independently both from the Oxford Advanced Learners Dictionary (OALD) and from lexical entries in the Longman Dictionary of Contemporary English (Procter, Ilson, and Ayto 1978)." ></td>
	<td class="line x" title="135:423	These are then automatically adapted to the format of generative lexical structures." ></td>
	<td class="line x" title="136:423	It is these lexical structures that are then statistically tuned against the corpus, following the methods outlined in Anick and Pustejovsky (1990) and Pustejovsky (1992)." ></td>
	<td class="line x" title="137:423	Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn (1985), Byrd et al.(1987), Markowitz, Ahlswede, and Evens (1986), and Nakamura and Nagao (1988) showed that taxonomic information and certain semantic relations can be extracted from MRDs using fairly simple techniques." ></td>
	<td class="line x" title="139:423	Later work by Veronis and Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992) provides us with a number of techniques for transfering information from MRDs to a representation language such as that described in the previous section." ></td>
	<td class="line x" title="140:423	Our goal is to automate, to the extent possible, the initial construction of these structures." ></td>
	<td class="line x" title="141:423	Extensive research has been done on the kind of information needed by natural language programs and on the representation of that information (Wang, Vandendorpe, and Evens 1985; Ahlswede and Evens 1988)." ></td>
	<td class="line x" title="142:423	Following Boguraev et al.(1989) and Wilks et al. of 1989), we believe that much of what is needed for NLP lexicons can be found either explicitly or implicitly in a dictionary, and empirical evidence suggests that this information gives rise to a sufficiently rich lexical representation for use in extracting information from texts." ></td>
	<td class="line x" title="144:423	Techniques for identifying explicit information in machine-readable dictionaries have been developed by many researchers (Boguraev et al. 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al. 1990) and are well understood." ></td>
	<td class="line x" title="145:423	Many properties of a word sense or the semantic relationships between word senses are available in MRDs, but this information can only be identified computationally through some analysis of the definition text of an entry (Atkins 1991)." ></td>
	<td class="line x" title="146:423	Some research has already been done in this area." ></td>
	<td class="line x" title="147:423	Alshawi (1987), Boguraev et al.(1989), Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al.(1992) have made explicit some kinds of implicit information found in MRDs." ></td>
	<td class="line x" title="150:423	Here we propose to refine and merge some of the previous techniques to make explicit the implicit information specified by a theory of generative lexicons." ></td>
	<td class="line x" title="151:423	Given what we described above for the lexical structures for nominals, we can identify these semantic relations in the OALD and LDOCE by pattern matching on the parse trees of definitions." ></td>
	<td class="line x" title="152:423	To illustrate what specific information can be derived by automatic seeding from machine-readable dictionaries, consider the following examples)  For example, the LDOCE definition for book is: 'a collection of sheets of paper fastened together as a thing to be read, or to be written in' 10 The following lexical entries, termed gls's, are taken from the lexical databases derived from the OALD using tools developed by Peter Dilworth, and from LDOCE using a combination of tools developed by Louise Guthrie, Gees Stein, and Pete Dilworth." ></td>
	<td class="line x" title="153:423	337 Computational Linguistics Volume 19, Number 2 while the OALD provides a somewhat different definition: 'number of sheet of papers, either printed or blank, fastened together in a cover'." ></td>
	<td class="line x" title="154:423	Note that both definitions are close to, but not identical to the information structure suggested in the previous section, using a qualia structure for nominals." ></td>
	<td class="line x" title="155:423	LDOCE suggests write in rather than write as the value for the telic role, while the OALD suggests nothing for this role." ></td>
	<td class="line x" title="156:423	Furthermore, although the physical contents of a book as 'a collection of sheets of paper' is mentioned, nowhere is information made reference to in the definition." ></td>
	<td class="line x" title="157:423	When the dictionary fails to provide the value for a semantic role, the information must be either hand-entered or the lexical structure must be tuned against a large corpus, in the hope of extracting such features automatically." ></td>
	<td class="line x" title="158:423	We turn to this issue in the next two sections." ></td>
	<td class="line x" title="159:423	Although the two dictionaries differ in substantial respects, it is remarkable how systematic the definition structures are for extracting semantic information, if there is a clear idea how this information should be structured." ></td>
	<td class="line x" title="160:423	For example, from the following OALD definition for cigarette, cigarette n roll of shredded tobacco enclosed in thin paper for smoking." ></td>
	<td class="line x" title="161:423	the initial lexical structure below is generated." ></td>
	<td class="line x" title="162:423	gls (cigarette, syn( \[type (n), code(C)\] ), qualia ( \[formal( \[roll\] ), telic ( \[smoking\] ), const ( \[tobacco,paper\] ), agent ( \[enclosed\] )\] ), cospec ( \[\] ) ) . Parsing the LDOCE entry for the same noun results in a different lexical structure: cigarette n finely cut shredded tobacco rolled in a narrow tube of thin paper for smoking." ></td>
	<td class="line x" title="163:423	gls (cigarette, syn( \[type (n), code(C), ldoce id(cigarette 0_1)\]), qualia( \[formal( \[tube\] ), telic( \[smoking\] ), const ( \[tobacco,paper\] ), agent ( \[rolled\] )\] ), cospec ( \[\] ) ) . One obvious problem with the above representation is that there is no information indicating how the word being defined binds to the relations in the qualia." ></td>
	<td class="line x" title="164:423	Currently, 338 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis subsequent routines providing for argument binding analyze the relational structure for particular aspects of noun meaning, giving us a lexical structure fairly close to what we need for representation and retrieval purposes, although the result is in no way ideal or uniform over all nominal forms." ></td>
	<td class="line x" title="165:423	(cf.Cowie, Guthrie, and Pustejovsky \[1992\] for details of this operation on LDOCE.): 11 cigarette(x) CONST = tobacco(y),shredded(y),paper(z) FOaMAL = roll(x) TELIC = smoke(T,w,x) AGENTIVE = artifact(x) In a related set of experiments performed while constructing a large lexical database for data extraction purposes, we seeded a lexicon with 6000 verbs from LDOCE." ></td>
	<td class="line x" title="167:423	This process and the corpus tuning for both argument typing and subcategorization acquisition are described in Cowie, Guthrie, and Pustejovsky (1992) and Pustejovsky et al.(1992)." ></td>
	<td class="line x" title="169:423	In summary, based on a theory of lexical semantics, we have discussed how an MRD can be useful as a corpus for automatically seeding lexical structures." ></td>
	<td class="line x" title="170:423	Rather than addressing the specific problems inherent in converting MRDs into useful lexicons, we have emphasized how it provides us, in a sense, with a generic vocabulary from which to begin lexical acquisition over corpora." ></td>
	<td class="line x" title="171:423	In the next section, we will address the problem of taking these initial, and often very incomplete lexical structures, and enriching them with information acquired from corpus analysis." ></td>
	<td class="line x" title="172:423	As mentioned in the previous section, the power of a generative lexicon is that it takes much of the burden of semantic interpretation off of the verbal system by supplying a much richer semantics for nouns and adjectives." ></td>
	<td class="line x" title="173:423	This makes the lexical structures ideal as an initial representation for knowledge acquisition and subsequent information retrieval tasks." ></td>
	<td class="line x" title="174:423	3." ></td>
	<td class="line x" title="175:423	Knowledge Acquisition from Corpora A machine-readable dictionary provides the raw material from which to construct computationally useful representations of the generic vocabulary contained within it." ></td>
	<td class="line x" title="176:423	The lexical structures discussed in the previous section are one example of how such information can be exploited." ></td>
	<td class="line x" title="177:423	Many sublanguages, however, are poorly represented in on-line dictionaries, if represented at all." ></td>
	<td class="line x" title="178:423	Vocabularies geared to specialized domains will be necessary for many applications, such as text categorization and information retrieval." ></td>
	<td class="line x" title="179:423	The second area of our research program that we discuss is aimed at developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of generative lexicon theory." ></td>
	<td class="line x" title="180:423	To understand fully the experiments described in the next two sections, we will refer to several semantic notions introduced in previous sections." ></td>
	<td class="line x" title="181:423	These include type coercion, where a lexical item requires a specific type specification for its argument, and 11 As one reviewer correctly pointed out, more than simple argument binding is involved here." ></td>
	<td class="line x" title="182:423	For example, the model must know that paper can enclose shredded tobacco, but not the reverse." ></td>
	<td class="line x" title="183:423	Such information, typically part of commonsense knowledge, is well outside the domain of lexical semantics, as envisioned here." ></td>
	<td class="line x" title="184:423	One approach to this problem, consistent with our methodology, is to examine the corpus and the collocations that result from training on specific qualia relations." ></td>
	<td class="line x" title="185:423	Further work will hopefully clarify the nature of this problem, and whether it is best treated lexically or not." ></td>
	<td class="line x" title="186:423	339 Computational Linguistics Volume 19, Number 2 the argument is able to change type accordingly--this explains the behavior of logical metonymy and the syntactic variation seen in complements to verbs and nominals; and cospecification, a semantic tagging of what collocational patterns the lexical item may enter into." ></td>
	<td class="line x" title="187:423	Metonymy, in this view, can be seen as a case of the 'licensed violation' of selectional restrictions." ></td>
	<td class="line x" title="188:423	For example, while the verb announce selects for a human subject, sentences like The Dow Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr. Dow Jr. announced third quarter losses for Dow Corp, but they are the preferred form in the corpora being examined." ></td>
	<td class="line x" title="189:423	This is an example of subject type coercion, where the semantics for Dow Corp as a company must specify that there is a human typically associated with such official pronouncements (see Section 5)." ></td>
	<td class="line x" title="190:423	12 For one set of experiments, we used a corpus of approximately 3,000 articles written by Digital Equipment Corporation's Customer Support Specialists for an on-line computer troubleshooting library." ></td>
	<td class="line x" title="191:423	The articles, each oneto two-page long descriptions of a problem and its solution, comprise about I million words." ></td>
	<td class="line x" title="192:423	Our analysis proceeds in two phases." ></td>
	<td class="line x" title="193:423	In the first phase, we pre-process the corpus to build a database of phrasal relationships." ></td>
	<td class="line x" title="194:423	This consists briefly of the following steps: 1." ></td>
	<td class="line x" title="195:423	Perform unknown word resolution to the corpus." ></td>
	<td class="line x" title="196:423	The corpus is searched for strings that are not members of a 25,000 word generic on-line English lexicon." ></td>
	<td class="line x" title="197:423	Morphological analysis is then applied to these unknown strings to identify candidate citation forms and their likely morphological paradigms." ></td>
	<td class="line x" title="198:423	Unless morphological evidence indicates otherwise, we enter unknown words into the lexicon as regular nouns; if there is evidence of some other morphological paradigm, such as verbal or adjectival suffixes, the word is entered into the lexicon accordingly." ></td>
	<td class="line x" title="199:423	2." ></td>
	<td class="line x" title="200:423	Corpus tagging." ></td>
	<td class="line x" title="201:423	Once the lexicon is updated to include the new single word forms in the domain, the corpus is tagged with part-of-speech indicators." ></td>
	<td class="line x" title="202:423	Any words that are ambiguous with respect to category are disambiguated according to a set of several dozen ordered disambiguation heuristics, which choose a category based on the categories of the words immediately preceding and following the ambiguous term." ></td>
	<td class="line x" title="203:423	3." ></td>
	<td class="line x" title="204:423	Partial parsing." ></td>
	<td class="line x" title="205:423	The tagged corpus is then segmented into a fiat sequence of phrasal groupings, using closed class words such as prepositions and determiners, as well as certain part-of-speech transitions, to indicate likely phrase boundaries." ></td>
	<td class="line x" title="206:423	No attempt is made to construct a full parse tree or resolve prepositional phrase attachment, conjunction scoping, etc. A concordance is constructed, identifying, for each word appearing in the corpus, the set of sentences, phrases, and phrase locations in which the word appears." ></td>
	<td class="line x" title="207:423	12 Within the current framework, a distinction is made between logical metonymy, where the metonymic extension or relation is transparent from the lexical semantics of the coerced phrase, and conventional metonymy, where the relation may not be directly calculated from information provided grammatically." ></td>
	<td class="line x" title="208:423	For example, in the sentence 'The Boston office called today,' it is not clear from logical metonymy what relation Boston bears to office other than location; i.e., it is not obvious that it is a branch office." ></td>
	<td class="line x" title="209:423	This is well beyond lexical semantics (cf.Lakoff 1987 and Martin 1990)." ></td>
	<td class="line x" title="211:423	340 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis The database of partially parsed sentences provides the raw material for a number of sublanguage analyses." ></td>
	<td class="line x" title="212:423	This begins the second phase of analysis: 1." ></td>
	<td class="line x" title="213:423	Noun compound recognition and bracketing." ></td>
	<td class="line x" title="214:423	In technical sublanguages, noun compounds are often employed to expand the working vocabulary without the invention of new word forms." ></td>
	<td class="line x" title="215:423	It is therefore useful in applications such as lexicon-assisted full-text information retrieval (Anick 1992) to include such noun compounds as lexical items for both querying and thesaurus browsing." ></td>
	<td class="line x" title="216:423	We construct bracketed noun compounds from our database of partial parses in a two-step process." ></td>
	<td class="line x" title="217:423	The first simply searches the corpus for (recurring) contiguous sequences of nouns." ></td>
	<td class="line x" title="218:423	Then, to bracket each compound that includes more than two nouns, we test whether possible subcomponents of the phrase exist on their own (as complete noun compounds) elsewhere in the corpus." ></td>
	<td class="line x" title="219:423	Sample bracketed compounds derived from the computer troubleshooting database include \[ \[system management\] utility\], \[TK50 \[tape drive\]\], \[\[database management\] system\]." ></td>
	<td class="line x" title="220:423	2." ></td>
	<td class="line x" title="221:423	Generation of taxonomic relationships on the basis of collocational information." ></td>
	<td class="line x" title="222:423	Technical sublanguages often express subclass relationships in noun compounds of the form <instance-name> <class-name>, as in 'Unix operating system' and 'C language'." ></td>
	<td class="line x" title="223:423	Unfortunately, noun compounds are also employed to express numerous other relationships, as in 'Unix kernel' and 'C debugger'." ></td>
	<td class="line x" title="224:423	We have found, however, that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships, using a strategy similar to that employed by Hindle (1990) for detecting synonyms." ></td>
	<td class="line x" title="225:423	Given a term T, we extract from the phrase database those nouns Ni that appear as the head of any phrase in which T is the immediately preceding term." ></td>
	<td class="line x" title="226:423	These nouns represent candidate classes of which T may be a member." ></td>
	<td class="line x" title="227:423	We then generate the set of verbs that take T as direct object and calculate the mutual information value for each verb/T collocation (cf.Hindle 1990)." ></td>
	<td class="line x" title="229:423	We do the same for each noun Ni." ></td>
	<td class="line x" title="230:423	Under the assumption that instance and class nouns are likely to co-occur with the same verbs, we compute a similarity score between T and each noun Ni, by summing the product of the mutual information values for those verbs occurring with both nouns." ></td>
	<td class="line x" title="231:423	(Verbs with negative mutual information values are left out of the calculation)." ></td>
	<td class="line x" title="232:423	The noun with the highest similarity score is often the class of which T is an instance, as illustrated by the sample results in Figure 1." ></td>
	<td class="line x" title="233:423	For each word displayed in Figure 1, its 'class' is the head noun with the highest similarity score." ></td>
	<td class="line x" title="234:423	Other head nouns occurring with the word as modifier are listed as well." ></td>
	<td class="line x" title="235:423	As with all the automated procedures described here, this algorithm yields useful, but imperfect results." ></td>
	<td class="line x" title="236:423	The class chosen for 'VMS,' for example, is incorrect, and may reflect the fact that in a DEC troubleshooting database, authors see no need to further specify VMS as 'VMS operating system'." ></td>
	<td class="line x" title="237:423	A more interesting observation is that, among the collocations associated with the terms, there are often several that might qualify as classes of which the term is an instance, e.g., DECWindows could also be classified as 'software'; TK50 might also qualify as 'tape'." ></td>
	<td class="line x" title="238:423	From a generative lexicon perspective, these alternative classifications reflect multiple inheritance through the noun's 341 Computational Linguistics Volume 19, Number 2 word class score other collocations HSC controller 27.69 BACKUP operation 34.18 RL02 disk 15.93 TK50 cartridge 39.17 ACCVIO error 14.35 VAX product 23.28 VMS support 7.98 upgrade procedure 12.27 DCL level 9.14 CHECKSUM value 4.45 EDT editor 11.58 TPU command 3.62 RTL error 1.58 DECWindows environment 75.46 device, disk, path, message disk, tape, process, saveset media, kit, pack tape, kit, density, format problem configuration, node, editor, hardware product, upgrade, installation phase, option, support, prerequisite command, procedure, access, error character, operation, error session, conversion, search, problem editor, session, function, debugger routine, library image, application, intrinsics, software Figure 1 Classification of nouns from a computer troubleshooting corpus." ></td>
	<td class="line x" title="239:423	qualia." ></td>
	<td class="line x" title="240:423	That is, 'cartridge' is further specifying the formal role of tape for TK50." ></td>
	<td class="line x" title="241:423	DECWindows is functionally an 'environment,' its telic role, while 'software' characterizes its formal quale." ></td>
	<td class="line x" title="242:423	3." ></td>
	<td class="line x" title="243:423	Extraction of information relating to noun's qualia." ></td>
	<td class="line x" title="244:423	Under certain circumstances, it may be possible to elicit information about a noun's qualia from automated procedures on a corpus." ></td>
	<td class="line x" title="245:423	In this line of research, we haved employed the notion of 'lexical conceptual paradigm' described above." ></td>
	<td class="line x" title="246:423	An LCP relates a set of syntactic behaviors to the lexical semantic structures of the participating lexical items." ></td>
	<td class="line x" title="247:423	For example, the set of expressions involving the word 'tape' in the context of its use as a secondary storage device suggests that it fits the container artifact schema of the qualia structure, with 'information' and 'file' as its containees: (a) read information from tape (b) write file to tape (c) read information on tape (d) read tape (e) write tape As mentioned in Section 1, containers tend to appear as objects of the prepositions to, from, in, and on as well as in direct object position, in which case they are typically serving metonymically for the containee." ></td>
	<td class="line x" title="248:423	Thus, the container LCP relates the set of generalized syntactic patterns V i Nj {to, froin~ on} X k vi Nj riNk to the underlying lexical semantic structure given below." ></td>
	<td class="line x" title="249:423	container(x,y) \] CONST = P(y) FORMAL ~Q(x) TELIC = hold(S,x,y) 342 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis verb MI count unload 5.43 5 position 3.92 5 mount 3.77 29 initialize 3.18 10 dismount 2.88 5 read 1.40 7 load 1.18 4 restore 0.80 3 write -0.24 2 copy -2.55 1 Figure 2 Verbs associated with direct object tape as direct object." ></td>
	<td class="line x" title="250:423	This LCP includes a nominal alternation between the container and containee in the object position of verbs." ></td>
	<td class="line x" title="251:423	For tape, this alternation is manifested for verbs that predicate the telic role of data storage but not the formal role of physical object, which refers to the object as a whole regardless of its contents: TELIC : data-storage (a) read (tape/data from tape) (b) write (tape/data on tape) (c) copy (tape/data from tape) FORMAL= physical object (a) mount (tape) (b) dismount (tape) We have explored the use of heuristics to distinguish those predicates that relate to the Telic quale of the noun." ></td>
	<td class="line x" title="252:423	Consider the word tape, which occurs as the direct object in 107 sentences in our corpus." ></td>
	<td class="line x" title="253:423	It appears with a total of 34 different verbs." ></td>
	<td class="line x" title="254:423	By applying the mutual information metric (MI) to the verb-object pairs, we can sort the verbs accordingly, giving us the table of verbs most highly associated with tape, shown in Figure 2." ></td>
	<td class="line x" title="255:423	While the mutual information statistic does a good job of identifying verbs that semantically relate to the word tape, it provides no information about how the verbs relate to the noun's qualia structure." ></td>
	<td class="line x" title="256:423	That is, verbs such as unload, position, and mount are selecting for the formal quale of tape, a physical object that can be physically manipulated with respect to a tape drive." ></td>
	<td class="line x" title="257:423	Read, write, and copy, on the other hand, relate to the telic role, the function of a tape as a medium for storing information." ></td>
	<td class="line x" title="258:423	Our hypothesis was that the nominal alternation can help to distinguish the two sets of verbs." ></td>
	<td class="line x" title="259:423	We reasoned that, if the alternation is based on the container/containee metonymy, then it will be those verbs that apply to the telic role of the direct object that participate in the alternation." ></td>
	<td class="line x" title="260:423	We tested this hypothesis as follows." ></td>
	<td class="line x" title="261:423	We generated a candidate set of containees for tape by identifying all the nouns that appeared in the corpus to the left of the adjunct on tape." ></td>
	<td class="line x" title="262:423	343 Computational Linguistics Volume 19, Number 2 $1 & 81 NS2 Sl-& Verbs with tape as object Verbs with a containee of tape as object {restore, create, write, read, copy, replace} {mount, initialize, unload, position, dismount, load, allocate } 81 82 81 Sl n& -& Verbs with disk as object Verbs with a containee of disk as object {compress, restore, disable, rebuild, modify, recover, search, copy} {mount, initialize, boot, dismount, serve, } Sl 82 Sin& $1 -~2 Verbs with directory as object Verbs with containee of directory as object {create, recreate, delete, store, rename, check} {own, miss, search, review} Figure 3 Intersection and set difference for three container nouns." ></td>
	<td class="line x" title="263:423	Then we took the set of verbs that had one of these containee nouns as a direct object and compared this set to the set of verbs that had the container noun tape as a direct object in the corpus." ></td>
	<td class="line x" title="264:423	According to our hypothesis, verbs applying to the telic role should appear in the intersection of these two sets (as a result of the alternation), while those applying to the formal role will appear in the set difference {verbs with containers as direct object}--{verbs with containees as direct object}." ></td>
	<td class="line x" title="265:423	The difference operation should serve to remove any verbs that co-occur with containee objects." ></td>
	<td class="line x" title="266:423	Figure 3 shows the results of intersection and set difference for three container nouns tape, disk, and directory." ></td>
	<td class="line x" title="267:423	The results indicate that the container LCP is able to differentiate nouns with respect to their telic and formal qualia, for the nouns tape and disk but not for directory." ></td>
	<td class="line x" title="268:423	The poor discrimination in the latter case can be attributed to the fact that a directory is a recursive container." ></td>
	<td class="line x" title="269:423	A directory contains files, and a directory is itself a file." ></td>
	<td class="line x" title="270:423	Therefore, verbs that apply to the formal role of directory are likely to apply to the formal role of objects contained in directories (such as other directories)." ></td>
	<td class="line x" title="271:423	This can be seen as a shortcoming of the container LCP for the task at hand, but may be a useful way of diagnosing when containers contain objects functionally similar to themselves." ></td>
	<td class="line x" title="272:423	The result of this corpus acquisition procedure is a kind of minimal faceted analysis for the noun tape, as illustrated below, showing only the qualia that are relevant to the discussion} 3 tape(x,y) 1 CONST = information(y);file(y) FORMAL = mount(z,x);dismount(z,x) TELIC = read(w,y);write(w,y);copy(w,y);contain(w,y) 13 Because the technique was sensitive to grammatical position of the object NP, the argument can be bound to the appropriate variable in the relation expressed in the qualia." ></td>
	<td class="line x" title="273:423	It should be pointed out that these qualia values do not carry event place variables, since such discrimination was beyond the scope of this experiment." ></td>
	<td class="line x" title="274:423	344 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis What is interesting about the qualia values is how close they are to the concepts in the projective conclusion space of tape, as mentioned in Section 1." ></td>
	<td class="line x" title="275:423	To illustrate this procedure on another semantic category, consider the term mouse in its computer artifact sense." ></td>
	<td class="line x" title="276:423	In our corpus, it appears in the object position of the verb use in a 'use NP to' construction, as well as the object of the preposition with following a transitive verb and its object: 1." ></td>
	<td class="line x" title="277:423	use the mouse to set breakpoints 2." ></td>
	<td class="line x" title="278:423	use the mouse anywhere 3." ></td>
	<td class="line x" title="279:423	move a window with the mouse 4." ></td>
	<td class="line x" title="280:423	click on it with the mouse These constructions are symptomatic of its role as an instrument; and the VP complement of to as well as the VP dominating the with-PP identify the telic predicates for the noun." ></td>
	<td class="line x" title="281:423	Other verbs, for which mouse appears as a direct object are currently defaulted into the formal role, resulting in an entry for mouse as follows: mouse(x) \] CONST = button(y) FORMAL = physobj(x) TELIG = set(x,breakpoint);move(x,window);click-on(x,z) The above experiments have met with limited success, enough to warrant continuing our application of lexical semantic theory to knowledge acquisition from corpora, but not enough to remove the human from the loop." ></td>
	<td class="line x" title="282:423	As they currently exist, the algorithms described here can be used as tools to help the knowledge engineer extract useful information from on-line textual sources, and in some applications (e.g. , a 'related terms' thesaurus for full text information retrieval) may provide a useful way to heuristically organize sublanguage terminology when human resources are unavailable." ></td>
	<td class="line x" title="283:423	4." ></td>
	<td class="line x" title="284:423	Semantic Type Induction from Syntactic Forms The purpose of the research described in this section is to experiment with the automatic acquisition of semantic tags for words in a sublanguage, tags well beyond that available from the seeding of MRDs." ></td>
	<td class="line x" title="285:423	The identification of semantic tags is the result of type coercion on known syntactic forms, to induce a semantic feature, such as \[+event\] or \[+object\]." ></td>
	<td class="line x" title="286:423	4.1 Coercive Environments in Corpora A pervasive example of type coercion is seen in the complements of aspectual verbs such as begin and finish, and verbs such as enjoy." ></td>
	<td class="line x" title="287:423	That is, in sentences such as 'John began the book,' the normal complement expected is an action or event of some sort, most often expressed by a gerundive or infinitival phrase: 'John began reading the book,' 'John began to read the book'." ></td>
	<td class="line x" title="288:423	In Pustejovsky (1991) it was argued that in such cases, the verb need not have multiple subcategorizations, but only one deep semantic type, in this case, an event." ></td>
	<td class="line x" title="289:423	Thus, the verb coerces its complement (e.g. 'the book') into an event related to that object." ></td>
	<td class="line x" title="290:423	Such information can be represented by means of a representational schema called qualia structure, which, among other things, specifies the relations associated with objects." ></td>
	<td class="line x" title="291:423	345 Computational Linguistics Volume 19, Number 2 Figure 4 Counts for objects of begin/V. count verb object 205 begin career 176 begin day 159 begin work 140 begin talk 120 begin campaign 113 begin investigation 106 begin process 92 begin program 85 begin operation 85 begin negotiation 66 begin strike 64 begin production 59 begin meeting 59 begin term 50 begin visit 45 begin test 39 begin construction 31 begin debate 29 begin trial In related work being carried out with Mats Rooth of the University of Stuttgart, we are exploring what the range of coercion types is, and what environments they may appear in, as discovered in corpora." ></td>
	<td class="line x" title="292:423	Some of our initial data suggest that the hypothesis of deep semantic selection may in fact be correct, as well as indicating what the nature of the coercion rules may be." ></td>
	<td class="line oc" title="293:423	Using techniques described in Church and Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4 shows some examples of the most frequent V-O pairs from the AP corpus." ></td>
	<td class="line x" title="294:423	Corpus studies confirm similar results for 'weakly intensional contexts' such as the complement of coercive verbs such as veto." ></td>
	<td class="line x" title="295:423	These are interesting because regardless of the noun type appearing as complement, it is embedded within a semantic interpretation of 'the proposal to,' thereby clothing the complement within an intensional context." ></td>
	<td class="line x" title="296:423	The examples in Figure 5 with the verb veto indicate two things: first, that such coercions are regular and pervasive in corpora; second, that almost anything can be vetoed, but that the most frequently occurring objects are closest to the type selected by the verb." ></td>
	<td class="line x" title="297:423	What these data show is that the highest count complement types match the type required by the verb; namely, that one vetoes a bill or proposal to do something, not the thing itself." ></td>
	<td class="line x" title="298:423	These nouns can therefore be used with some predictive certainty for inducing the semantic type in coercive environments such as 'veto the expedition'." ></td>
	<td class="line x" title="299:423	This work is still preliminary, however, and requires further examination (Pustejovsky and Rooth \[unpublished\])." ></td>
	<td class="line x" title="300:423	4.2 Induction of Semantic Relations from Syntactic Forms In this section, we present another experiment indicating the feasibility of inducing semantic tags for lexical items from Corpora." ></td>
	<td class="line x" title="301:423	14 Imagine being able to take the V-O pairs 14 This section presents an abridged version of material reported on in Pustejovsky (1992)." ></td>
	<td class="line x" title="302:423	346 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Figure 5 Counts for objects of veto/V. count verb object 303 veto bill 84 veto legislation 58 veto measure 35 veto resolution 21 veto law 14 veto item 12 veto decision 9 veto proposal 9 veto plan 7 veto package 6 veto increase 5 veto sanction 5 veto penalty 4 veto notice 4 veto idea 4 veto appropriation 4 veto mission 4 veto attempt 3 veto search 3 veto cut 3 veto deal 1 veto expedition such as those given in Section 4.1, and then applying semantic tags to the verbs that are appropriate to the role they play for that object (i.e. , induction of the qualia roles for that noun)." ></td>
	<td class="line x" title="303:423	This is similar to the experiment reported on in Section 3." ></td>
	<td class="line x" title="304:423	Here we apply a similar technique to a much larger corpus, in order to induce the agentive role for nouns; that is, the semantic predicate associated with bringing about the object." ></td>
	<td class="line x" title="305:423	In this example we look at the behavior of noun phrases and the prepositional phrases that follow them." ></td>
	<td class="line x" title="306:423	In particular, we look at the co-occurrence of nominals with between, with, and to." ></td>
	<td class="line x" title="307:423	Table 1 shows results of the conflating noun plus preposition patterns." ></td>
	<td class="line x" title="308:423	The percentage shown indicates the ratio of the particular collocation to the key word." ></td>
	<td class="line x" title="309:423	Mutual information (MI) statistics for the two words in collocation are also shown." ></td>
	<td class="line x" title="310:423	What these results indicate is that induction of semantic type from conflating syntactic patterns is possible." ></td>
	<td class="line x" title="311:423	Based on the semantic types for these prepositions, the syntactic evidence suggests that there is an equivalence class where each preposition makes reference to a symmetric relation between the arguments in the following two patterns:  Z with y = ARzAx3y\[Rz(x,y) A Rz(y, x)\]  Z between x and y=,XRz3x, y\[Rz(x, y)/x Rz(y, x)\] We then take these results and, for those nouns where the association ratios for N with and N between are similar, we pair them with the set of verbs governing these 'NP PP' combinations in corpus, effectively partitioning the original V-O set into \[+agentive\] predicates and \[-agentive\] predicates." ></td>
	<td class="line x" title="312:423	These are semantic n-grams rather than direct interpretations of the prepositions." ></td>
	<td class="line x" title="313:423	347 Computational Linguistics Volume 19, Number 2 Table 1 Mutual information for noun + preposition patterns." ></td>
	<td class="line x" title="314:423	Word Word Word Word Word Word Word + to + with + between Word + to + with + between (%)/MI (%)/MI (%)/MI (%)/MI (%)/MI (%)/MI agreement .117 .159 .028 expansion .013 .007 0 1.512 3.423 3.954 -.666 .381 n/a announcement .010 .003 0 impasse 0 .064 .096 -.918 -.409 n/a n/a 2.520 5.192 barrier .215 0 .030 interactions 0 0 .250 2.117 n/a 4.046 n/a n/a 6.141 competition .019 .028 .021 market .013 .006 .000 -.269 1.701 3.666 -.637 .240 -.500 confrontation .029 .283 .074 range .005 .002 .020 .141 4.000 4.932 -1.533 -.618 3.663 contest .052 .052 .039 relations .009 .217 .103 .715 2.323 4.301 -1.017 3.736 5.254 contract .066 .060 .002 settlement .013 .091 .012 .947 2.463 1.701 -.626 2.868 3.142 deal .028 .193 .004 talks .029 .218 .030 .086 3.616 2.015 .138 3.740 4.043 dialogue 0 .326 .152 venture .032 .105 .035 n/a 4.140 5.644 .226 3.008 4.185 difference .017 .009 .348 war .010 .041 .015 -.410 .638 6.474 -.937 2.079 3.372 What these expressions in effect indicate is the range of semantic environments they will appear in." ></td>
	<td class="line x" title="315:423	That is, in sentences like those in Example 16, the force of the relational nouns agreement and talks is that they are unsaturated for the predicate bringing about this relation." ></td>
	<td class="line x" title="316:423	In 17, on the other hand, the NPs headed by agreement and talks are saturated in this respect." ></td>
	<td class="line x" title="317:423	Example 16 a. John made an agreement with Mary." ></td>
	<td class="line x" title="318:423	b. Apple opened talks with IBM." ></td>
	<td class="line x" title="319:423	Example 17 a. This is an agreement between John and Mary." ></td>
	<td class="line x" title="320:423	b. Those were the first talks between Apple and IBM." ></td>
	<td class="line x" title="321:423	If our hypothesis is correct, we expect that verbs governing nominals collocated with a with-phrase will be mostly those predicates referring to the agentive quale of the nominal." ></td>
	<td class="line x" title="322:423	This is because the with-phrase is unsaturated as a predicate, and acts to 348 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis count verb object Figure 6 Verb-object pairs with prep = with." ></td>
	<td class="line x" title="323:423	19 form venture 3 announce venture 3 enter venture 2 discuss venture 1 be venture 1 abandon venture 1 begin venture 1 complete venture 1 negotiate venture 1 start venture 1 expect venture identify the agent of the verb as its argument (cf.Nilsen (1973))." ></td>
	<td class="line x" title="325:423	This is confirmed by our data, shown in Figure 6." ></td>
	<td class="line x" title="326:423	Conversely, verbs governing nominals collocating with a between-phrase will not refer to the agentive since the phrase is saturated already." ></td>
	<td class="line x" title="327:423	Indeed, the only verb occurring in this position with any frequency is the copula be, namely with the following counts: 12 be/V venture/0." ></td>
	<td class="line x" title="328:423	Thus, weak semantic types can be induced on the basis of syntactic behavior." ></td>
	<td class="line x" title="329:423	There is a growing literature on corpus-based acquisition and tuning (Smadja 1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992)." ></td>
	<td class="line x" title="330:423	We share with these researchers a general dependence on well-behaved collocational patterns and distributional structures." ></td>
	<td class="line x" title="331:423	Probably the main distinguishing feature of our approach is its reliance on a fairly well studied semantic framework to aid and guide the semantic induction process itself, whether it involves selectional restrictions or semantic types." ></td>
	<td class="line x" title="332:423	5." ></td>
	<td class="line x" title="333:423	Lexical Presuppositions and Preferences In the previous section we presented algorithms for extracting collocational information from corpora, in order to supplement and fine-tune the lexical structures seeded by a machine-readable dictionary." ></td>
	<td class="line x" title="334:423	In this section we demonstrate that, in addition to conventional lexical semantic relations, it is also possible to acquire information concerning lexical presuppositions and preferences from corpora, when analyzed with the appropriate semantic tools." ></td>
	<td class="line x" title="335:423	In particular, we will discuss a phenomenon we call discourse polarity, and how corpus-based experiments provide clues toward the representation of this phenomenon, as well as information on preference relations." ></td>
	<td class="line x" title="336:423	As we have seen, providing a representational system for lexical semantic relations is a nontrivial task." ></td>
	<td class="line x" title="337:423	Representing presuppositional information, however, is even more daunting." ></td>
	<td class="line x" title="338:423	Nevertheless, there are some systematic semantic generalizations associated with such subtle lexical inferences." ></td>
	<td class="line x" title="339:423	To illustrate this, consider the following examples taken from the Wall Street Journal Corpus, involving the verb insist." ></td>
	<td class="line x" title="340:423	Example 18 But Mr. Fourtou insisted that the restructuring plans hadn't played a role in his decision." ></td>
	<td class="line x" title="341:423	349 Computational Linguistics Volume 19, Number 2 Example 19 But so far, the majority is insisting that a daily paper in the home is an essential educational resource that Mr. Oshry must have, like it or not." ></td>
	<td class="line x" title="342:423	Example 20 But Mr. Nishi insists there is a common theme to his scattered projects: to improve and spread personal computers." ></td>
	<td class="line x" title="343:423	Example 21 'Mister, Djemaa is a crazy place for you,' insists the first of many young men, clutching a visitor's sleeve." ></td>
	<td class="line x" title="344:423	Example 22 But the BNL sources yesterday insisted that the head office was aware of only a small portion of the credits to Iraq made by Atlanta." ></td>
	<td class="line x" title="345:423	Example 23 Mr. Smale, who ordinarily insists on a test market before a national roll-out, told the team to go ahead--although he said he was skeptical that Pringle's could survive, Mr. Tucker says." ></td>
	<td class="line x" title="346:423	Example 24 The Cantonese insist that their fish be 'fresh,' though one whiff of Hong Kong harbor and the visitor may yearn for something shipped from distant seas." ></td>
	<td class="line x" title="347:423	Example 25 Money isn't the issue, Mr. Bush insists." ></td>
	<td class="line x" title="348:423	From analyzing these and similar data, a pattern emerges concerning the use of verbs like insist in discourse; namely, the co-occurrence with discourse markers denoting negative affect, such as although and but, as well as literal negatives, e.g., no and not." ></td>
	<td class="line x" title="349:423	This is reminiscent of the behavior of negative polarity items such as any more and at all." ></td>
	<td class="line x" title="350:423	Such lexical items occur only in the context of negatives within a certain structural configuration." ></td>
	<td class="line x" title="351:423	15 In a similar way, verbs such as insist seem to require an overt or implicit negation within the immediate discourse context, rather than within the clause." ></td>
	<td class="line x" title="352:423	For this reason, we will call such verbs discourse polarity items." ></td>
	<td class="line x" title="353:423	For our purposes, the significance of such data is twofold: first, experiments on corpora can test and confirm linguistic intuitions concerning a subtle semantic judgment; second, if such knowledge is in fact so systematic, then it must be at least partially represented in the lexical semantics of the verb." ></td>
	<td class="line x" title="354:423	To test whether the intuitions supported by the above data could be confirmed in corpora, Bergler (1991) derived the statistical co-occurrence of insist with discourse polarity markers in the 7 million-word corpus of Wall Street Journal articles." ></td>
	<td class="line x" title="355:423	She derived the statistics reported in Figure 7." ></td>
	<td class="line x" title="356:423	Let us assume, on the basis of this preliminary data 16 presented in Bergler (1992) that these verbs in fact do behave as discourse polarity items." ></td>
	<td class="line x" title="357:423	The question then 15 There is a rich literature on this topic." ></td>
	<td class="line x" title="358:423	For discussion see Ladusaw (1980) and Linebarger (1980)." ></td>
	<td class="line x" title="359:423	16 Overlap between the categories occurs in less than 35 cases." ></td>
	<td class="line x" title="360:423	350 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis Keywords insist insist on insist & but insist & negation Count 586 109 insist & subjunctive Figure 7 117 186 159 Comments occurrences throughout the corpus these have been cleaned by hand and are actually occurrences of the idiom insist on rather than accidental co-occurrences." ></td>
	<td class="line x" title="361:423	occurrences of both insist and but in the same sentence includes not and n't includes would, could, should, and be Negative markers with insist in WSJC." ></td>
	<td class="line x" title="362:423	immediately arises as to how we represent this type of knowledge." ></td>
	<td class="line x" title="363:423	Using the language of the qualia structure discussed above, we can make explicit reference to the polarity behavior, in the following informal but intuitive representation for the verb insist." ></td>
	<td class="line x" title="364:423	17 insist(x:ind,y:prop) \] FORMAL = REPORTING-VERB--LCP TELIC = say(x,true(y)) & presupposed(~b) & y = ~b This entry states that in the REPORTING-VERB sense of the word, insist is a relation between an individual and a statement that is the negation of a proposition, ~b, presupposed in the context of the utterance." ></td>
	<td class="line x" title="365:423	As argued in Pustejovsky (1991) and Miller and Fellbaum (1991), such simple oppositional predicates form a central part of our lexicalization of concepts." ></td>
	<td class="line x" title="366:423	Semantically motivated collocations such as these extracted from large corpora can provide presuppositional information for words that would otherwise be missing from the lexical semantics of an entry." ></td>
	<td class="line x" title="367:423	While full automatic extraction of semantic collocations is not yet feasible, some recent research in related areas is promising." ></td>
	<td class="line x" title="368:423	Hindle (1990) reports interesting results of this kind based on literal collocations, where he parses the corpus (Hindle 1983) into predicate-argument structures and applies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weigh the association between the predicate and each of its arguments." ></td>
	<td class="line x" title="369:423	For example, as a list of the most frequent objects for the verb drink in his corpus, Hindle found beer, tea, Pepsi, and champagne." ></td>
	<td class="line x" title="370:423	Based on the distributional hypothesis that the degree of shared contexts is a similarity measure for words, he develops a similarity metric for nouns based on their substitutability in certain verb contexts." ></td>
	<td class="line x" title="371:423	Hindle thus finds sets of semantically similar nouns based on syntactic co-occurrence data." ></td>
	<td class="line x" title="372:423	The sets he extracts are promising; for example, the ten most similar nouns to treaty in his corpus are agreement, plan, constitution, contract, proposal, accord, amendment, rule, law, and legislation." ></td>
	<td class="line x" title="373:423	This work is very close in spirit to our own investigation here; the emphasis on syntactic co-occurrence enables Hindle to extract his similarity lists automatically; they 17 For illustration, we use an abbreviated version of the lexical entries under discussion, highlighting only certain qualia for the verbs." ></td>
	<td class="line x" title="374:423	For the most recent representation of verbal semantics in this framework, see Pustejovsky (1993)." ></td>
	<td class="line x" title="375:423	351 Computational Linguistics Volume 19, Number 2 are therefore easy to compile for different corpora, different sublanguages, etc. Here we are attempting to use these techniques together with a model of lexical meaning, to capture deeper lexical semantic collocations; e.g., the generalization that the list of objects occurring for the word drink contains only liquids." ></td>
	<td class="line x" title="376:423	In the final part of this section, we turn to how the analysis of corpora can provide lexical semantic preferences for verb selection." ></td>
	<td class="line oc" title="377:423	As discussed above, there is a growing body of research on deriving collocations from corpora (cf.Church and Hanks 1990; Klavans, Chodorow, and Wacholder 1990; Wilks et al. 1993; Smadja 1991a, 1991b; Calzolari and Bindi 1990)." ></td>
	<td class="line x" title="379:423	Here we employ the tools of semantic analysis from Section 1 to examine the behavior of metonymy with reporting verbs." ></td>
	<td class="line x" title="380:423	We will show, on the basis of corpus analysis, how verbs display marked differences in the ability to license metonymic operations over their arguments." ></td>
	<td class="line x" title="381:423	Such information, we argue, is part of the preference semantics for a sublanguage, as automatically derived from corpus." ></td>
	<td class="line x" title="382:423	Metonymy can be seen as a case of 'licensed violation' of selectional restrictions." ></td>
	<td class="line x" title="383:423	For example, while the verb announce selects for a human subject, sentences like The Phantasie Corporation announced third quarter losses are not only an acceptable paraphrase of the selectionally correct form Mr. Phantasie Jr. announced third quarter losses for Phantasie Corp, but they are the preferred form in the Wall Street Journal)." ></td>
	<td class="line x" title="384:423	This is an example of subject type coercion, as discussed in Section 1." ></td>
	<td class="line x" title="385:423	For example, the qualia structure for a noun such as corporation might be represented as below: corporation(x) CONST = group(y),spokesperson(w),executive(z) FORMAL = organization(x) TELIC = execute(z, decisions) AGENTIVE = incorporate(y,x) The metonymic extension in this example is straightforward: a spokesman, executive, or otherwise legitimate representative 'speaking for' a company or institution can be metonymically replaced by that company or institution." ></td>
	<td class="line x" title="386:423	TM We find that this type of metonymic extension for the subject is natural and indeed very frequent with reporting verbs Bergler (1991), such as announce, report, release, and claim, while it is in general not possible with other verbs selecting human subjects, e.g., the verbs of contemplation (such as contemplate, consider, and think)." ></td>
	<td class="line x" title="387:423	However, there are subtle differences in the occurrence of such metonymies for the different members of the same semantic verb class that arise from corpus analysis." ></td>
	<td class="line x" title="388:423	A reporting verb is an utterance verb that is used to relate the words of a source." ></td>
	<td class="line x" title="389:423	In a careful study of seven reporting verbs on a 250,000-word corpus of Time magazine articles from 1963, we found that the preference for different metonymic extensions varies considerably within this field (Bergler 1991)." ></td>
	<td class="line x" title="390:423	Figure 8 shows the findings for the words insist, deny, admit, claim, announce, said, and told for two metonymic extensions, namely where a group stands for an individual (Analysts said ) and where a company or other institution stands for the individual (IBM announced  ).19 The difference in patterns of metonymic behavior is quite striking: semantically similar verbs seem to pattern similarly over all three categories; admit, insist, and deny show a closer resemblance to each other than to any of the others, while said and 18 Note, however, that the metonymic extension is not quite as simple as extending from any employee to the whole company or institution, but that a form of legitimation has to be involved)." ></td>
	<td class="line x" title="391:423	For more detail see Bergler (1992)." ></td>
	<td class="line x" title="392:423	19 The data for Figure 8 have been screened to ensure that only occurrences that constitute reporting contexts were used." ></td>
	<td class="line x" title="393:423	352 James Pustejovsky et al. Lexical Semantic Techniques for Corpus Analysis admit deny insist announce claim said told person 64% 59% 57% 51% 35% 69% 19% 11% 24% 10% 21% 14% 19% 16% 31% 38% other 2% 11% 3% 8% 6% 8% 16% Figure 8 Preference for different metonymies in subject position." ></td>
	<td class="line x" title="394:423	--t person WSJ 49% TIME 83% group 15% 6% institution other 34% 2% 4% 8% Figure 9 Preference for metonymies for said in a 160,000-word fragment of the Wall Street Journal corpus." ></td>
	<td class="line x" title="395:423	told form a category by themselves." ></td>
	<td class="line x" title="396:423	There may be a purely semantic explanation why said and told seem not to prefer the metonymic use in subject position; e.g., perhaps these verbs relate more closely to the act of uttering, or perhaps they are too informal, stylistically." ></td>
	<td class="line x" title="397:423	Evidence from other corpora, however, suggests that such information is accurately characterized as lexical preference." ></td>
	<td class="line x" title="398:423	An initial experiment on a subset of the Wall Street Journal Corpus, for example, shows that said has a quite different metonymic distribution there, reported in Figure 9." ></td>
	<td class="line x" title="399:423	In this corpus we discovered that subject selection for an individual person appeared in only 50% of the sentences, while a company/institution appeared in 34% of the cases." ></td>
	<td class="line x" title="400:423	This difference could either be attributed to a difference in style between Time magazine and the Wall Street Journal or perhaps to a difference in general usage between 1963 and 1989." ></td>
	<td class="line x" title="401:423	The statistics presented here can of course not determine the reason for the difference, but rather help establish the lexical semantic preferences that exist in a certain corpus and sublanguage." ></td>
	<td class="line x" title="402:423	An important question related to the extraction of preference information is what the corpus should be." ></td>
	<td class="line x" title="403:423	Recent effort has been spent constructing balanced corpora, containing text from different styles and sources, such as novels, newspaper texts, scientific journal articles, etc. The assumption is of course that given a representative mix of samples of language use, we can extract the general properties and usage of words." ></td>
	<td class="line x" title="404:423	But if we gain access to sophisticated automatic corpus analysis tools such as those 353 Computational Linguistics Volume 19, Number 2 discussed above, and indeed if we have specialized algorithms for sublanguage extraction, then homogeneous corpora might provide better data." ></td>
	<td class="line x" title="405:423	The few examples of lexical preference mentioned in this section might not tell us anything conclusive for the definitive usage of a word such as said, if there even exists such a notion." ></td>
	<td class="line x" title="406:423	Nevertheless the statistics provide an important tool for text analysis within the corpus from which they are derived." ></td>
	<td class="line x" title="407:423	Because we can systematically capture the violation of selectional restrictions (as semantically predicted), there is no need for a text analysis system to perform extensive commonsense inferencing." ></td>
	<td class="line x" title="408:423	Thus, such presupposition and preference statistics are vital to efficient processing of real text." ></td>
	<td class="line x" title="409:423	6." ></td>
	<td class="line x" title="410:423	Summary and Discussion In this paper we have presented a particularly directed program of research for how text corpora can contribute to linguistics and computational linguistics." ></td>
	<td class="line x" title="411:423	We first presented a representation language for lexical knowledge, the generative lexicon, and demonstrated how it facilitates the structuring of lexical relations among words, looking in particular at the problems of metonymy and polysemy." ></td>
	<td class="line x" title="412:423	Such a framework for lexical knowledge suggests that there are richer relationships among words in text beyond that of simple co-occurrence that can be extracted automatically." ></td>
	<td class="line x" title="413:423	The work suggests how linguistic phenomena such as metonymy and polysemy might be exploited for knowledge acquisition for lexical items." ></td>
	<td class="line x" title="414:423	Unlike purely statistical collocational analyses, the framework of a semantic theory allows the automatic construction of predictions about deeper semantic relationships among words appearing in collocational systems." ></td>
	<td class="line x" title="415:423	We illustrated the approach for the acquisition of lexical information for several classes of nominals, and how such techniques can fine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary." ></td>
	<td class="line x" title="416:423	In addition to conventional lexical semantic relations, we then showed how information concerning lexical presuppositions and preference relations can also be acquired from corpora, when analyzed with the appropriate semantic tools." ></td>
	<td class="line x" title="417:423	In conclusion, we feel that the application of computational resources to the analysis of text corpora has and will continue to have a profound effect on the direction of linguistic and computational linguistic research." ></td>
	<td class="line x" title="418:423	Unlike previous attempts at corpus research, the current focus is supported and guided by theoretical tools, and not merely statistical techniques." ></td>
	<td class="line x" title="419:423	We should furthermore welcome the ability to expand the data set used for the confirmation of linguistic hypotheses." ></td>
	<td class="line x" title="420:423	At the same time, we must remember that statistical results themselves reveal nothing, and require careful and systematic interpretation by the investigator to become linguistic data." ></td>
	<td class="line x" title="421:423	Acknowledgments This research was supported by DARPA contract MDA904-91-C-9328." ></td>
	<td class="line x" title="422:423	We would like to thank Scott Waterman for his assistance in preparing the statistics." ></td>
	<td class="line x" title="423:423	We would also like to thank Mats Rooth, Scott Waterman, and four anonymous reviewers for useful comments and discussion." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J93-3004
Co-Occurrence Patterns Among Collocations: A Tool For Corpus-Based Lexical Knowledge Acquisition
Biber, Douglas;"></td>
	<td class="line x" title="1:126	Squibs and Discussions Co-occurrence Patterns among Collocations: A Tool for Corpus-Based Lexical Knowledge Acquisition Douglas Biber  Northern Arizona University 1." ></td>
	<td class="line x" title="2:126	Introduction One of the main problems for applied natural language processing is gaps in the lexicon, including missing words and word senses, and inadequate descriptions of word use in context." ></td>
	<td class="line x" title="3:126	Traditional lexicography has similar concerns." ></td>
	<td class="line x" title="4:126	The availability of large, on-line text corpora provides a straightforward tool for enlarging the stock of words included in a lexicon." ></td>
	<td class="line x" title="5:126	The identification of additional word senses and uses is more problematic, however." ></td>
	<td class="line x" title="6:126	Much recent lexicographic work employs concordances generated from text corpora for these purposes." ></td>
	<td class="line x" title="7:126	While this approach provides a more solid empirical basis than traditional lexicographic approaches (which depend on the manual collection and sorting of citation index cards), concordances can actually provide too much data." ></td>
	<td class="line x" title="8:126	For example, a concordance for the word certain produced on an 11.6 million-word subsample of the Longman/Lancaster Corpus generated 3,424 entries; a concordance for the word right from the same subcorpus generated 7,619 entries." ></td>
	<td class="line x" title="9:126	Simply determining the number of different senses in a database of this size is a daunting task; to accurately group different uses or rank them in order of importance is not really feasible without the use of additional tools." ></td>
	<td class="line x" title="10:126	One such tool is to simply sort concordance lines according to their different collocational patterns." ></td>
	<td class="line x" title="11:126	Entries can be sorted according to their collocates on both the left and the right." ></td>
	<td class="line x" title="12:126	Many of these collocational pairs show a strong relation to a particular word sense (e.g. , contrast right ear and right away), and thus analysis of collocational relations has become an important tool for lexical knowledge acquisition (see Sinclair 1991; Smadja 1991; Zernik 1991)." ></td>
	<td class="line o" title="13:126	In addition, there are statistical tools that can help determine the relative strength of collocational relations." ></td>
	<td class="line oc" title="14:126	For example, Church and Hanks (1990) describe the use of the mutual information index for this purpose (cf.Calzolari and Bindi 1990)." ></td>
	<td class="line x" title="16:126	Church et al.(1991) further describe the use of t-scores to assess the extent of the differences between the collocational patterns of nearly synonymous words." ></td>
	<td class="line pc" title="18:126	These tools are important in that the strongest collocational associations often represent different word senses, and thus 'they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags' (Church and Hanks 1990, p. 28)." ></td>
	<td class="line n" title="19:126	However, such tools do not directly characterize word senses or even provide any direct indication of the number of different senses that a word has." ></td>
	<td class="line x" title="20:126	1 Further, these * Dept. of English, Northern Arizona University, P.O. Box 6032, Flagstaff, AZ 86011-6032; biber@nauvax.ucc.nau.edu." ></td>
	<td class="line oc" title="21:126	1 Church and Hanks (1990; Church et al. 1991) thus emphasize the importance of human judgment used in conjunction with these tools." ></td>
	<td class="line x" title="22:126	Computational Linguistics Volume 19, Number 3 tools are not designed to assess the relations among various collocations, addressing the question of which clusters of collocations reflect similar underlying senses." ></td>
	<td class="line x" title="23:126	2 The present paper discusses the use of factor analysis (a multivariate statistical technique) as a tool for such research questions." ></td>
	<td class="line x" title="24:126	In particular, this technique contributes three types of information not provided by other complementary techniques: 1) an indication of the number of major senses and/or uses associated with a word; 2) an indication of the way that various collocational patterns relate to one another in marking word senses and uses; and 3) a fuller analysis of the senses themselves, based on interpretation of the shared bases underlying the groupings of collocations." ></td>
	<td class="line x" title="25:126	2." ></td>
	<td class="line x" title="26:126	Methodology The use of multivariate statistical techniques for lexical knowledge acquisition was first proposed by Bindi et al.(1991)." ></td>
	<td class="line x" title="28:126	In particular, that study illustrates the use of Multidimensional Scaling (MDS) to identify the relations among semantically related word types (e.g. , piccolo, corto, breve) and their collocates (or 'word mates')." ></td>
	<td class="line x" title="29:126	The input data for this approach are mutual information indexes computed over the domain of an entire corpus." ></td>
	<td class="line x" title="30:126	Separate association indexes are computed for each pair of target word types (e.g. , piccolo and corto) and for all of the word types in relation to various 'word-mates' (e.g. , piccolo with bambino)." ></td>
	<td class="line x" title="31:126	This entire matrix is then analyzed by MDS to cluster word types and word-mates along a few underlying dimensions, providing a geometric representation of the semantic domain in question (in this case, 'smallness')." ></td>
	<td class="line x" title="32:126	Word types are clustered together to the extent that they are associated with the same word-mates." ></td>
	<td class="line x" title="33:126	The geometric distance between a word-mate and its word type reflects the strength of the relevant association index, and word-mates are clustered together to the extent that they are associated with the same word types (see Bindi et al. 1991)." ></td>
	<td class="line x" title="34:126	Factor analysis differs from MDS in that the input data are computed over the domain of individual texts, rather than over the domain of the entire corpus." ></td>
	<td class="line x" title="35:126	Further, the application of factor analysis here is to identify basic word senses and uses associated with a single word rather than the relations among a set of semantically related words." ></td>
	<td class="line x" title="36:126	This statistical technique identifies the groupings of collocations that tend to co-occur frequently in texts." ></td>
	<td class="line x" title="37:126	If we assume that texts are topically coherent, it follows that groupings of collocations that co-occur frequently in texts will often reflect different underlying word senses." ></td>
	<td class="line x" title="38:126	This approach is illustrated here through analyses of two words--certain and right--in an 11.66 million-word subsample of the Longman/Lancaster corpus." ></td>
	<td class="line x" title="39:126	This corpus is designed to represent a wide range of text varieties (see Summers 1991), including ten major topical categories (e.g. , natural science, social science, fiction), three mediums (books, periodicals, and ephemera), and three levels (technical/literary, lay/middle, and popular)." ></td>
	<td class="line x" title="40:126	The first step in the analysis is to identify the major collocational patterns for the target word." ></td>
	<td class="line x" title="41:126	In the present analysis, this was done simply by computing the frequency of all collocational pairs over the entire corpus; collocations that occurred more than 30 times were considered to be important word associations and thus included in the subsequent analyses." ></td>
	<td class="line x" title="42:126	3 For certain, 20 left collocates and 14 right collocates met this 2 Church et al.(1991, pp." ></td>
	<td class="line x" title="44:126	150-155) show how word association measures computed over the domain of discourse units (rather than bigrams or SVO triples) can be used to identify topical domains for information retrieval purposes." ></td>
	<td class="line x" title="45:126	3 There are more sophisticated approaches that could be used to identify the major collocational associations." ></td>
	<td class="line x" title="46:126	First, mutual information indexes could be computed to identify strong word associations 532 Douglas Biber Co-occurrence Patterns criterion; for right, there were 24 left collocates and 53 right collocates meeting the criterion." ></td>
	<td class="line x" title="47:126	The next step was to count the frequency of each collocation pair in each text of the corpus." ></td>
	<td class="line x" title="48:126	Only texts longer than 20,000 words were included at this stage, to insure adequate representation of the relevant co-occurring collocations." ></td>
	<td class="line x" title="49:126	Two hundred forty-six texts from the subcorpus met this condition." ></td>
	<td class="line x" title="50:126	Factor analysis was then used to identify the groupings of collocational pairs that tended to co-occur in texts." ></td>
	<td class="line x" title="51:126	Factor analysis builds upon pairwise correlations among the variables to identify a reduced set of underlying constructs, or 'factors'." ></td>
	<td class="line x" title="52:126	4 Each variable has some correlation, or 'loading,' with each factor, but only the larger loadings are important in interpreting the underlying constructs." ></td>
	<td class="line x" title="53:126	In the present case, the variables are the frequency counts for each collocational pair." ></td>
	<td class="line x" title="54:126	The factors thus represent the collocational pairs that tend to co-occur frequently in texts." ></td>
	<td class="line x" title="55:126	Given the following assumptions, it was hypothesized that the factor groupings of collocational pairs would represent different underlying word senses; the required assumptions are: 1) that each collocational pair tends to have a strong relation to a single word sense, and 2) that texts are topically coherent, and that words will therefore tend to be used in a single sense throughout the domain of a text." ></td>
	<td class="line x" title="56:126	To the extent that these assumptions are accurate, collocational pairs should co-occur in texts as reflections of the same underlying word sense." ></td>
	<td class="line x" title="57:126	The pilot analyses presented in the following section indicate that these assumptions do commonly hold in natural discourse." ></td>
	<td class="line x" title="58:126	3." ></td>
	<td class="line x" title="59:126	Co-occurrence Patterns among Collocations for certain and right The factor analyses for the collocates of certain and right are summarized in Tables 1 and 2 respectively." ></td>
	<td class="line x" title="60:126	5 The tables present the factor loadings for each collocational pair on each factor." ></td>
	<td class="line x" title="61:126	Loadings can range from 0.0 to plus or minus 1.0." ></td>
	<td class="line x" title="62:126	A loading of 0.0 that are not necessarily frequent in their overall occurrence." ></td>
	<td class="line x" title="63:126	In addition, word associations at a distance should be included." ></td>
	<td class="line x" title="64:126	4 See Biber (1988) for a fuller discussion of factor analysis and its application to the computational analysis of linguistic variation." ></td>
	<td class="line x" title="65:126	There are three main corpus design considerations required by this use of factor analysis." ></td>
	<td class="line x" title="66:126	First, the analysis requires long, connected texts, to provide ample opportunity for the (co-)occurrence of a range of collocational pairs; the analysis here excludes all texts shorter than 20,000 words." ></td>
	<td class="line x" title="67:126	Shorter texts would often contain few tokens of a few collocational pairs and would thus not adequately represent the correlations among collocations." ></td>
	<td class="line x" title="68:126	Composite texts are not acceptable because they violate the assumption of topical coherence." ></td>
	<td class="line x" title="69:126	Second, the analysis requires a large number of texts." ></td>
	<td class="line x" title="70:126	A general rule of thumb for factor analysis is that there should be five times as many texts as variables (Gorsuch 1983)." ></td>
	<td class="line x" title="71:126	Thus, there should be a minimum of 240 texts included for the final factor analysis of right, which is based on 48 collocational pairs." ></td>
	<td class="line x" title="72:126	(The final factor analysis of certain is based on 25 collocational pairs and would thus require only 150 texts)." ></td>
	<td class="line x" title="73:126	The sub-corpus used here, with 246 coherent texts longer than 20,000 words, meets these criteria." ></td>
	<td class="line x" title="74:126	Finally, the corpus used for analysis must represent the full range of variability with respect to the collocational patterns." ></td>
	<td class="line x" title="75:126	Corpora that are restricted topically or restricted to a few registers are not adequate for analyses of this type, because the reduced variability results in reduced correlations, which in turn result in a skewed and unreliable factorial structure (cf.Biber 1990)." ></td>
	<td class="line x" title="77:126	5 I used a common factor analysis with a Promax rotation." ></td>
	<td class="line x" title="78:126	Scree plots of the eigenvalues and consideration of the two-, three-, and four-factor solutions indicated that the three-factor solution was the most adequate for the analysis of certain, while the four-factor solution was the most adequate for the analysis of right." ></td>
	<td class="line x" title="79:126	Variables that had communalities K.10 were dropped from the final analysis; 25 collocational pairs were included in the final analysis of certain, and 48 pairs in the analysis of right." ></td>
	<td class="line x" title="80:126	In the analysis of certain, the first factor accounts for 21.4% of the shared variance, while the three factors together account for 39.4% of the shared variance." ></td>
	<td class="line x" title="81:126	Factors 1 and 2 have a correlation of .30, while the other inter-factor correlations are negligible." ></td>
	<td class="line x" title="82:126	In the analysis of right, the first factor accounts for 22.1% of the shared variance, while the four factors together account for 51.3% of the shared variance; the only appreciable inter-factor correlation is between Factors 2 and 3 (.43)." ></td>
	<td class="line x" title="83:126	533 Computational Linguistics Volume 19, Number 3 Table 1 Factorial structure of the collocates of certain." ></td>
	<td class="line x" title="84:126	FACTOR1 FACTOR2 Major Factor 1 Collocation FACTOR3 in certain 0.74077 0.07110 -0.05468 certain other 0.68352 -0.09431 -0.05035 of certain 0.61015 0.32266 -0.05660 and certain 0.58373 -0.12996 -0.11671 certain of 0.56773 -0.10960 0.22018 certain 0.49718 0.09231 -0.12760 certain 0.46448 0.10248 0.05560 ! there BE certain 0.32284 -0.03747 0.07349 certain type(s) 0.31556 0.06191 -0.10855 on certain 0.29994 0.27142 -0.01401 with certain 0.29908 0.24981 -0.07999 that certain 0.27814 0.18763 0.01827 Major Factor 2 Collocations certain extent -0.21295 0.91065 0.01578 certain aspect(s) -0.08321 0.90127 0.01704 to certain 0.15364 0.71625 0.00352 under certain 0.02432 0.44979 -0.02374 for certain 0.40779 0.44289 0.05722 a certain 0.01796 0.35752 0.01724 by certain 0.22450 0.33351 0.02019 Major Factor 3 Collocations certain that 0.13828 0.00372 0.87341 certain -0.11735 0.03704 0.46955 it BE certain -0.04318 0.00884 0.42112 make/made certain 0.02137 0.00068 0.32323 I/we BE certain -0.08456 -0.06629 0.29203 quite certain -0.08470 0.04002 0.23607 shows that the variable has no relation to the pool of shared variance accounted for by the factor, while a loading of 1.0 represents a perfect correlation." ></td>
	<td class="line x" title="85:126	Each factor comprises a number of collocational pairs with relatively large loadings, while the remaining collocations have small or negligible associations." ></td>
	<td class="line x" title="86:126	Tables 1 and 2 are organized so that the collocations having large loadings on each of the factors are grouped together." ></td>
	<td class="line x" title="87:126	Consider first the factorial structure for certain, presented in Table 1." ></td>
	<td class="line x" title="88:126	The first 12 collocations listed on the table have large loadings on Factor 1, with generally small loadings on the other two factors (e.g. , in certain has a loading of .74 on Factor 1, but loadings near 0.0 on Factors 2 and 3)." ></td>
	<td class="line x" title="89:126	The second group of 7 collocations have the largest loadings on Factor 2; while the last group of 6 collocations have large loadings on Factor 3." ></td>
	<td class="line x" title="90:126	6 For the purposes of interpretation, each factor can be considered as comprising 6 The collocation for certain has a noteworthy loading of .41 on Factor 1 in addition to its loading of .44 on Factor 2." ></td>
	<td class="line x" title="91:126	534 Douglas Biber Co-occurrence Patterns Table 2 Factorial structure of the collocates of right." ></td>
	<td class="line x" title="92:126	FACTOR1 FACTOR2 FACTOR3 FACTOR4 Factor 1 Collocations right hemisphere 0.98345 -0.01528 -0.01434 -0.00717 right sided 0.98343 -0.01487 -0.01405 -0.00737 right hander(s) 0.98343 -0.01487 -0.01405 -0.00737 right ear 0.98304 -0.01521 -0.00534 -0.00933 and right 0.97245 0.00920 -0.01713 -0.00284 of right 0.94451 -0.02987 -0.01721 -0.00597 the right 0.93976 0.01421 -0.01173 -0.01295 right side 0.87188 0.07561 -0.02721 0.02523 a right 0.84510 0.00909 0.03104 -0.00871 or right 0.80412 -0.01508 -0.03749 0.01806 right hand 0.79835 0.04828 0.04005 -0.02339 to right 0.60848 -0.03983 -0.03164 0.00806 right eye 0.52883 -0.04162 0.14620 -0.00923 that right 0.42500 -0.02100 0.31563 -0.00699 right and 0.31707 0.04224 -0.03306 0.21127 right of 0.29633 -0.06270 -0.10545 0.00798 right as 0.24007 0.06985 0.01180 0.21477 Factor 2 Collocations go/went right -0.00434 0.72062 -0.05389 -0.14694 right there 0.00397 0.65902 -0.00984 0.02282 right back 0.00094 0.59946 -0.16701 -0.01122 right now 0.01400 0.57063 0.06385 0.16459 right out 0.00941 0.56666 -0.03302 0.03100 right on -0.00409 0.54770 -0.00731 0.02745 right away -0.02119 0.48780 0.00373 -0.08896 me/you right -0.01704 0.45494 0.11834 0.03465 right here -0.00095 0.45346 -0.05408 0.23709 right for -0.02277 0.44136 0.13048 -0.11498 right up -0.01647 0.43549 0.12772 -0.00086 right in 0.02250 0.42069 -0.02334 0.11543 BE right 0.00832 0.40980 0.16436 0.05745 it right 0.00461 0.39168 -0.08280 0.08606 right with -0.00674 0.31426 0.12281 -0.14323 Factor 3 Collocations right 0.05717 0.10963 0.90268 0.02683 right, -0.00212 0.03678 0.88339 0.04162 all right -0.01675 0.09288 0.86456 -0.08946 that's right -0.00408 0.08106 0.68647 -0.14901 right then -0.00242 -0.07914 0.66223 0.03945 not right -0.00786 -0.15046 0.62236 -0.03184 . right 0.01465 0.09568 0.56646 0.20960 right a -0.00220 -0.05189 0.35346 0.15038 you're right -0.01759 0.17132 0.19201 0.05199 535 Computational Linguistics Volume 19, Number 3 Table 2 Continued." ></td>
	<td class="line x" title="93:126	FACTOR1 FACTOR2 FACTOR3 FACTOR4 Factor 4 Collocations  right 0.03424 0.02408 -0.12703 0.99315 right you -0.00214 -0.06841 0.10162 0.86214 right so 0.02173 -0.05118 0.08467 0.77789 right she/he/they -0.00184 -0.07307 0.09301 0.59287 right I/we -0.00618 0.08199 0.05781 0.57046 right it 0.01641 0.11691 -0.09122 0.37539 right from -0.01475 -0.00082 -0.03191 0.34669 only those features with large loadings, so that each factor represents a separate grouping of collocations that tend to co-occur frequently in texts." ></td>
	<td class="line x" title="94:126	Factor 3 is the easiest to interpret: all six of the collocational pairs grouped on this factor represent contexts where certain functions to mark certainty." ></td>
	<td class="line x" title="95:126	In all of these cases, certain is a predicative adjective, often taking a that complement clause (e.g. , I am quite certain that )." ></td>
	<td class="line x" title="96:126	In contrast, certain does not have the sense of certainty in any of the collocational pairs grouped on Factors 1 and 2; rather the collocations grouped on these factors function to identify an unspecified (and perhaps unknown) subset of some larger group (e.g. , in certain cases, a certain person)." ></td>
	<td class="line x" title="97:126	7 The difference between Factors 1 and 2 is less obvious." ></td>
	<td class="line x" title="98:126	An examination of the concordance listings for the associated collocations, however, shows that there is an important, systematic difference between the two factors: the collocations on Factor 1 tend to modify concrete, physical or tangible referents, while the collocations on Factor 2 tend to modify abstract referents." ></td>
	<td class="line x" title="99:126	Thus, consider the examples from both factors listed in Table 3." ></td>
	<td class="line x" title="100:126	Factor I collocations tend to be more concrete, often characterizing physical objects (e.g. , towns, adults, trees, raw vegetables)." ></td>
	<td class="line x" title="101:126	In contrast, Factor 2 collocations modify more abstract referents, such as basic truths, general values, and assumptions." ></td>
	<td class="line x" title="102:126	The collocation for certain has a loading of about .40 on both Factor 1 and Factor 2, and it shows both kinds of pattern; for example, it modifies users and speakers, similar to other Factor 1 collocations, and it modifies responsibilities, symptoms, and deficiencies, similar to other Factor 2 collocations." ></td>
	<td class="line x" title="103:126	The factorial structure for right, presented in Table 2, similarly shows a highly systematic patterning among collocations." ></td>
	<td class="line x" title="104:126	Factor I represents the positional or directional use of right; in fact, many of these collocations refer to body parts on the right side." ></td>
	<td class="line x" title="105:126	Factor 2 is equally transparent but represents a word sense that does not receive much attention in most dictionaries: right marking 'immediately,' 'directly,' or 'exactly'." ></td>
	<td class="line x" title="106:126	There are a large number of collocational pairs having this sense (e.g. , right there, right back, right now), and Factor 2 shows that there is a strong tendency for these collocations to co-occur in texts." ></td>
	<td class="line x" title="107:126	On first consideration, the collocation go~went right seems to be an exception to the general pattern underlying Factor 2 (representing the Factor 1 sense of 'to the right' instead)." ></td>
	<td class="line x" title="108:126	Examination of the concordance listings for this collocation shows, however, that it almost always occurs with the Factor 2 sense as in go right through, go right up, he went right away." ></td>
	<td class="line x" title="109:126	7 The collocational pair for certain does sometimes convey a sense of certainty, as in it isn't known for certain whether 536 Douglas Biber Co-occurrence Patterns Table 3 Example contexts for major collocations on Factors 1 and 2 in the analysis of certain." ></td>
	<td class="line x" title="110:126	Factor 1 collocations in certain: ways, cases, instances, towns, districts, directions, areas, cultures, companies certain other: external parasites, parts of the body, adults, statements, mediterranean cultures, crisp greens of certain: monkeys, infants, theologians, particles, sovereigns, people, details, trees, individuals and certain: kinds of cats, kinds of laughing, spices, bits of information, compositions, broiler growers, raw vegetables for certain: users, kinds of addressees, speakers, materials Factor 2 collocations (to) a certain extent (for) certain aspects to certain: deficiencies, stimuli, nonmaterial aspects, very strong drives, basic truths, defects, general values, simplifying principles under certain: assumptions, (economic) conditions for certain: responsibilities, symptoms, deficiencies, types Factor 3 represents a grouping of collocations having the general sense of 'ok' or 'correct'." ></td>
	<td class="line x" title="111:126	All right is commonly used to mark agreement or to mark a discourse juncture." ></td>
	<td class="line x" title="112:126	Right can occur by itself with these same functions (hence the collocations of right with preceding and following punctuation)." ></td>
	<td class="line x" title="113:126	The collocation that's right also typically marks agreement to a previous assertion." ></td>
	<td class="line x" title="114:126	Right then appears to represent a use of the Factor 2 pattern meaning 'immediately,' but this collocation also frequently occurs as a response in the context all right then." ></td>
	<td class="line x" title="115:126	The collocation not right indicates a lack of correctness or normalcy, as in all was not right with the mirror and something was not right close-up." ></td>
	<td class="line x" title="116:126	Finally, Factor 4 seems to represent a stylistically marked use of right at the end of a clause, with no intervening punctuation before the following clause." ></td>
	<td class="line x" title="117:126	Collocations of this type commonly have all right, right, or that's right preceding a clause, as in all right you found the, all right I pushed  that's right I think, right away they walk toward The underpinnings of this factor need further investigation." ></td>
	<td class="line x" title="118:126	4." ></td>
	<td class="line x" title="119:126	Conclusion The two pilot analyses presented here indicate that this approach can be a useful tool for the semi-automatic identification of underlying word senses and uses." ></td>
	<td class="line x" title="120:126	Further, both analyses produced unanticipated but systematic results, indicating that this approach can provide a useful complementary perspective to traditional lexicographic methods." ></td>
	<td class="line x" title="121:126	These analyses could be extended in several ways." ></td>
	<td class="line x" title="122:126	First, statistics such as the mutual information index could be used to help identify the set of relevant collocations to be included in the factor analysis." ></td>
	<td class="line x" title="123:126	Second, the corpus could be pre-processed by a 537 Computational Linguistics Volume 19, Number 3 grammatical tagger, making the collocations sensitive to grammatical category." ></td>
	<td class="line x" title="124:126	Finally, lexical associations at a distance should be included in the analysis." ></td>
	<td class="line x" title="125:126	In this regard, the analyses here have been restricted: they consider only collocations of adjacent words, with no regard for grammatical category, and with the input data identified simply on the basis of absolute frequency." ></td>
	<td class="line x" title="126:126	However, even with these restrictions, factor analysis appears to be a powerful tool for identifying underlying patterns among collocations, reflecting some of the major senses and uses of a word." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P93-1022
Contextual Word Similarity And Estimation From Sparse Data
Dagan, Ido;Marcus, Shaul;Markovitch, Shaul;"></td>
	<td class="line x" title="1:178	CONTEXTUAL WORD SIMILARITY AND ESTIMATION FROM SPARSE DATA Ido Dagan AT T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974 dagan@res earch, art." ></td>
	<td class="line x" title="2:178	tom Shaul Marcus Computer Science Department Technion Haifa 32000, Israel shaul@cs, t echnion, ac." ></td>
	<td class="line x" title="3:178	il $haul Markovitch Computer Science Department Technion Haifa 32000, Israel shaulm@cs, t echnion, ac." ></td>
	<td class="line x" title="4:178	il Abstract In recent years there is much interest in word cooccurrence relations, such as n-grams, verbobject combinations, or cooccurrence within a limited context." ></td>
	<td class="line x" title="5:178	This paper discusses how to estimate the probability of cooccurrences that do not occur in the training data." ></td>
	<td class="line x" title="6:178	We present a method that makes local analogies between each specific unobserved cooccurrence and other cooccurrences that contain similar words, as determined by an appropriate word similarity metric." ></td>
	<td class="line x" title="7:178	Our evaluation suggests that this method performs better than existing smoothing methods, and may provide an alternative to class based models." ></td>
	<td class="line x" title="8:178	1 Introduction Statistical data on word cooccurrence relations play a major role in many corpus based approaches for natural language processing." ></td>
	<td class="line x" title="9:178	Different types of cooccurrence relations are in use, such as cooccurrence within a consecutive sequence of words (n-grams), within syntactic relations (verb-object, adjective-noun, etc)." ></td>
	<td class="line x" title="10:178	or the cooccurrence of two words within a limited distance in the context." ></td>
	<td class="line oc" title="11:178	Statistical data about these various cooccurrence relations is employed for a variety of applications, such as speech recognition (Jelinek, 1990), language generation (Smadja and McKeown, 1990), lexicography (Church and Hanks, 1990), machine translation (Brown et al. , ; Sadler, 1989), information retrieval (Maarek and Smadja, 1989) and various disambiguation tasks (Dagan et al. , 1991; Hindle and Rooth, 1991; Grishman et al. , 1986; Dagan and Itai, 1990)." ></td>
	<td class="line x" title="12:178	A major problem for the above applications is how to estimate the probability of cooccurrences that were not observed in the training corpus." ></td>
	<td class="line x" title="13:178	Due to data sparseness in unrestricted language, the aggregate probability of such cooccurrences is large and can easily get to 25% or more, even for a very large training corpus (Church and Mercer, 1992)." ></td>
	<td class="line x" title="14:178	Since applications often have to compare alternative hypothesized cooccurrences, it is important to distinguish between those unobserved cooccurrences that are likely to occur in a new piece of text and those that are not." ></td>
	<td class="line x" title="15:178	These distinctions ought to be made using the data that do occur in the corpus." ></td>
	<td class="line x" title="16:178	Thus, beyond its own practical importance, the sparse data problem provides an informative touchstone for theories on generalization and analogy in linguistic data." ></td>
	<td class="line x" title="17:178	The literature suggests two major approaches for solving the sparse data problem: smoothing and class based methods." ></td>
	<td class="line x" title="18:178	Smoothing methods estimate the probability of unobserved cooccurrences using frequency information (Good, 1953; Katz, 1987; Jelinek and Mercer, 1985; Church and Gale, 1991)." ></td>
	<td class="line x" title="19:178	Church and Gale (Church and Gale, 1991) show, that for unobserved bigrams, the estimates of several smoothing methods closely agree with the probability that is expected using the frequencies of the two words and assuming that their occurrence is independent ((Church and Gale, 1991), figure 5)." ></td>
	<td class="line x" title="20:178	Furthermore, using held out data they show that this is the probability that should be estimated by a smoothing method that takes into account the frequencies of the individual words." ></td>
	<td class="line x" title="21:178	Relying on this result, we will use frequency based es~imalion (using word frequencies) as representative for smoothing estimates of unobserved cooccurrences, for comparison purposes." ></td>
	<td class="line x" title="22:178	As will be shown later, the problem with smoothing estimates is that they ignore the expected degree of association between the specific words of the cooccurrence." ></td>
	<td class="line x" title="23:178	For example, we would not like to estimate the same probability for two cooccurrences like 'eat bread' and 'eat cars', despite the fact that both 'bread' and 'cars' may have the same frequency." ></td>
	<td class="line x" title="24:178	Class based models (Brown et al. , ; Pereira et al. , 1993; Hirschman, 1986; Resnik, 1992) distinguish between unobserved cooccurrences using classes of 'similar' words." ></td>
	<td class="line x" title="25:178	The probability of a specific cooccurrence is determined using generalized parameters about the probability of class cooccur\] 64 rence." ></td>
	<td class="line x" title="26:178	This approach, which follows long traditions in semantic classification, is very appealing, as it attempts to capture 'typical' properties of classes of words." ></td>
	<td class="line x" title="27:178	However, it is not clear at all that unrestricted language is indeed structured the way it is assumed by class based models." ></td>
	<td class="line x" title="28:178	In particular, it is not clear that word cooccurrence patterns can be structured and generalized to class cooccurrence parameters without losing too much information." ></td>
	<td class="line x" title="29:178	This paper suggests an alternative approach which assumes that class based generalizations should be avoided, and therefore eliminates the intermediate level of word classes." ></td>
	<td class="line x" title="30:178	Like some of the class based models, we use a similarity metric to measure the similarity between cooccurrence patterns of words." ></td>
	<td class="line x" title="31:178	But then, rather than using this metric to construct a set of word classes, we use it to identify the most specific analogies that can he drawn for each specific estimation." ></td>
	<td class="line x" title="32:178	Thus, to estimate the probability of an unobserved cooccurfence of words, we use data about other cooccurfences that were observed in the corpus, and contain words that are similar to the given ones." ></td>
	<td class="line x" title="33:178	For example, to estimate the probability of the unobserved cooccurrence 'negative results', we use cooccurrences such as 'positive results' and 'negative numbers', that do occur in our corpus." ></td>
	<td class="line x" title="34:178	The analogies we make are based on the assumption that similar word cooccurrences have similar values of mutual information." ></td>
	<td class="line x" title="35:178	Accordingly, our similarity metric was developed to capture similarities between vectors of mutual information values." ></td>
	<td class="line x" title="36:178	In addition, we use an efficient search heuristic to identify the most similar words for a given word, thus making the method computationally affordable." ></td>
	<td class="line x" title="37:178	Figure 1 illustrates a portion of the similarity network induced by the similarity metric (only some of the edges, with relatively high values, are shown)." ></td>
	<td class="line x" title="38:178	This network may be found useful for other purposes, independently of the estimation method." ></td>
	<td class="line x" title="39:178	The estimation method was implemented using the relation of cooccurrence of two words within a limited distance in a sentence." ></td>
	<td class="line x" title="40:178	The proposed method, however, is general and is applicable for anY type of lexical cooccurrence." ></td>
	<td class="line x" title="41:178	The method was evaluated in two experiments." ></td>
	<td class="line x" title="42:178	In the first one we achieved a complete scenario of the use of the estimation method, by implementing a variant of the d\[Sambiguation method in (Dagan et al. , 1991), for sense selection in machine translation." ></td>
	<td class="line x" title="43:178	The estimation method was then successfully used to increase the coverage of the disambiguation method by 15%, with an increase of the overall precision compared to a naive, frequency based, method." ></td>
	<td class="line x" title="44:178	In the second experiment we evaluated the estimation method on a data recovery task." ></td>
	<td class="line x" title="45:178	The task simulates a typical scenario in disambiguation, and also relates to theoretical questions about redundancy and idiosyncrasy in cooccurrence data." ></td>
	<td class="line x" title="46:178	In this evaluation, which involved 300 examples, the performance of the estimation method was by 27% better than frequency based estimation." ></td>
	<td class="line x" title="47:178	2 Definitions We use the term cooccurrence pair, written as (x, y), to denote a cooccurrence of two words in a sentence within a distance of no more than d words." ></td>
	<td class="line x" title="48:178	When computing the distance d, we ignore function words such as prepositions and determiners." ></td>
	<td class="line x" title="49:178	In the experiments reported here d = 3." ></td>
	<td class="line x" title="50:178	A cooccurrence pair can be viewed as a generalization of a bigram, where a bigram is a cooccurrence pair with d = 1 (without ignoring function words)." ></td>
	<td class="line x" title="51:178	As with bigrams, a cooccurrence pair is directional, i.e." ></td>
	<td class="line x" title="52:178	(x,y)  (y,x)." ></td>
	<td class="line x" title="53:178	This captures some information about the asymmetry in the linear order of linguistic relations, such as the fact that verbs tend to precede their objects and follow their subjects." ></td>
	<td class="line oc" title="54:178	The mutual information of a cooccurrence pair, which measures the degree of association between the two words (Church and Hanks, 1990), is defined as (Fano, 1961): P(xly) I(x,y) -log 2 P(x,y) _ log 2 (1) P(x)P(y) P(x) = log 2 P(y\[x) P(Y) where P(x) and P(y) are the probabilities of the events x and y (occurrences of words, in our case) and P(x, y) is the probability of the joint event (a cooccurrence pair)." ></td>
	<td class="line x" title="55:178	We estimate mutual information values using the Maximum Likelihood Estimator (MLE): P(x,y) _log~." ></td>
	<td class="line x" title="56:178	N f(x,y) \] I(x, y) = log~ P~x)P--(y) ( -d f(x)f(y) ' (2) where f denotes the frequency of an eyent and N is the length of the corpus." ></td>
	<td class="line x" title="57:178	While better estimates for small probabilities are available (Good, 1953; Church and Gale, 1991), MLE is the simplest to implement and was adequate for the purpose of this study." ></td>
	<td class="line x" title="58:178	Due to the unreliability of measuring negative mutual information values in corpora that are not extremely large, we have considered in this work any negative value to be 0." ></td>
	<td class="line x" title="59:178	We also set/~(x, y) to 0 if f(x, y) = 0." ></td>
	<td class="line x" title="60:178	Thus, we assume in both cases that the association between the two words is as expected by chance." ></td>
	<td class="line x" title="61:178	165 paper articles 14I /\00 1 conference." ></td>
	<td class="line x" title="62:178	0.132." ></td>
	<td class="line x" title="63:178	papers ~ /~,, U. I6 ~, l',, '-,, worksh:p. ,,._ ~0.106 ~ ~ \0.126 0." ></td>
	<td class="line x" title="64:178	4 \  symposmm ~ j book ' ' documentation 0.137 Figure 1: A portion of the similarity network." ></td>
	<td class="line x" title="65:178	3 Estimation for an Unobserved Cooccurrence Assume that we have at our disposal a method for determining similarity between cooccurrence patterns of two words (as described in the next section)." ></td>
	<td class="line x" title="66:178	We say that two cooccurrence pairs, (wl, w2) and (w~, w~), are similar if w~ is similar to wl and w~ is similar to w2." ></td>
	<td class="line x" title="67:178	A special (and stronger) case of similarity is when the two pairs differ only in one of their words (e.g.(wl,w~) and (wl,w2))." ></td>
	<td class="line x" title="69:178	This special case is less susceptible to noise than unrestricted similarity, as we replace only one of the words in the pair." ></td>
	<td class="line x" title="70:178	In our experiments, which involved rather noisy data, we have used only this restricted type of similarity." ></td>
	<td class="line x" title="71:178	The mathematical formulations, though, are presented in terms of the general case." ></td>
	<td class="line x" title="72:178	The question that arises now is what analogies can be drawn between two similar cooccurrence pairs, (wl,w2) and tw' wt~ Their probak 1' 21' bilities cannot be expected to be similar, since the probabilities of the words in each pair can be different." ></td>
	<td class="line x" title="73:178	However, since we assume that wl and w~ have similar cooccurrence patterns, and so do w~ and w~, it is reasonable to assume that the mutual information of the two pairs will be similar (recall that mutual information measures the degree of association between the words of the pair)." ></td>
	<td class="line x" title="74:178	Consider for example the pair (chapter, describes), which does not occur in our corpus 1 . This pair was found to be similar to the pairs (intro1 We used a corpus of about 9 million words of texts in the computer domain, taken from articles posted to the USENET news system." ></td>
	<td class="line x" title="75:178	duction, describes), (book, describes)and (section, describes), that do occur in the corpus." ></td>
	<td class="line x" title="76:178	Since these pairs occur in the corpus, we estimate their mutual information values using equation 2, as shown in Table 1." ></td>
	<td class="line x" title="77:178	We then take the average of these mutual information values as the similarity based estimate for I(chapter, describes), denoted as f(chapter, describes) 2." ></td>
	<td class="line x" title="78:178	This represents the assumption that the word 'describes' is associated with the word 'chapter' to a similar extent as it is associated with the words 'introduction', 'book' and 'section'." ></td>
	<td class="line x" title="79:178	Table 2 demonstrates how the analogy is carried out also for a pair of unassociated words, such as (chapter, knows)." ></td>
	<td class="line x" title="80:178	In our current implementation, we compute i(wl, w2) using up to 6 most similar words to each of wl and w~, and averaging the mutual information values of similar pairs that occur in the corpus (6 is a parameter, tuned for our corpus." ></td>
	<td class="line x" title="81:178	In some cases the similarity method identifies less than 6 similar words)." ></td>
	<td class="line x" title="82:178	Having an estimate for the mutual information of a pair, we can estimate its expected frequency in a corpus of the given size using a variation of equation 2: w2) = d f(wl)f(w2)2I(tl't2) (3) /(wl, In our example, f(chapter) = 395, N = 8,871,126 and d = 3, getting a similarity based estimate of f(chapter, describes)= 3.15." ></td>
	<td class="line x" title="83:178	This value is much 2We use I for similarity based estimates, and reserve i for the traditional maximum fikefihood estimate." ></td>
	<td class="line x" title="84:178	The similarity based estimate will be used for cooccurrence pairs that do not occur in the corpus." ></td>
	<td class="line x" title="85:178	166 i(w, (introduction, describes) 6.85 (book, describes) 6.27 (section, describes) 6.12 f(wl,w2) f(wl) f(w2) 5 464 277 13 1800 277 6 923 277 Average: 6.41 Table 1: The similarity based estimate as an average on similar pairs: \[(chapter, describes) = 6.41 (wl, w2) \[(wl, w=) (introduction, knows) 0 (book, knows) 0 (section, knows) 0 Average: 0 f(wl,w2) f(wl) f(w2) 0 464 928 0 1800 928 0 923 928 Table 2: The similarity based estimate for a pair of unassociated words: I(chapter, knows) = 0 higher than the frequency based estimate (0.037), reflecting the plausibility of the specific combination of words 3." ></td>
	<td class="line x" title="86:178	On the other hand, the similarity based estimate for \](chapter, knows) is 0.124, which is identical to the frequency based estimate, reflecting the fact that there is no expected association between the two words (notice that the frequency based estimate is higher for the second pair, due to the higher frequency of 'knows')." ></td>
	<td class="line x" title="87:178	4 TheSimilarity Metric Assume that we need to determine the degree of similarity between two words, wl and w2." ></td>
	<td class="line x" title="88:178	Recall that if we decide that the two words are similar, then we may infer that they have similar mutual information with some other word, w. This inference would be reasonable if we find that on average wl and w2 indeed have similar mutual information values with other words in the lexicon." ></td>
	<td class="line x" title="89:178	The similarity metric therefore measures the degree of similarity between these mutual information values." ></td>
	<td class="line x" title="90:178	We first define the similarity between the mutual information values of Wl and w2 relative to a single other word, w. Since cooccurrence pairs are directional, we get two measures, defined by the position of w in the pair." ></td>
	<td class="line x" title="91:178	The left context similarity of wl and w2 relative to w, termed simL(Wl, w2, w), is defined as the ratio between the two mutual information values, having the larger value in the denominator: simL(wl, w2, w) = min(I(w, wl), I(w, w2)) (4) max(I(w, wl), I(w, w2)) 3The frequency based estimate for the expected frequency of a cooccurrence pair, assuming independent occurrence of the two words and using their individual frequencies, is -~f(wz)f(w2)." ></td>
	<td class="line x" title="92:178	As mentioned earlier, we use this estimate as representative for smoothing estimates of unobserved cooccurrences." ></td>
	<td class="line x" title="93:178	This way we get a uniform scale between 0 and 1, in which higher values reflect higher similarity." ></td>
	<td class="line x" title="94:178	If both mutual information values are 0, then sirnL(wl,w2, w) is defined to be 0." ></td>
	<td class="line x" title="95:178	The right context similarity, simn(wl, w2, w), is defined equivalently, for I(Wl, w) and I(w2, w) 4." ></td>
	<td class="line x" title="96:178	Using definition 4 for each word w in the lexicon, we get 2  l similarity values for Wl and w2, where I is the size of the lexicon." ></td>
	<td class="line x" title="97:178	The general similarity between Wl and w2, termed sim(wl, w2), is defined as a weighted average of these 2  l values." ></td>
	<td class="line x" title="98:178	It is necessary to use some weighting mechanism, since small values of mutual information tend to be less significant and more vulnerable to noisy data." ></td>
	<td class="line x" title="99:178	We found that the maximal value involved in computing the similarity relative to a specific word provides a useful weight for this word in computing the average." ></td>
	<td class="line x" title="100:178	Thus, the weight for a specific left context similarity value, WL(Wl, W2, W), is defined as: Wt(wl, w) = max(I(w, wl), :(w, (5) (notice that this is the same as the denominator in definition 4)." ></td>
	<td class="line x" title="101:178	This definition provides intuitively appropriate weights, since we would like to give more weight to context words that have a large mutual information value with at least one of Wl and w2." ></td>
	<td class="line x" title="102:178	The mutual information value with the other word may then be large, providing a strong 'vote' for similarity, or may be small, providing a strong 'vote' against similarity." ></td>
	<td class="line x" title="103:178	The weight for a specific right context similarity value is defined equivalently." ></td>
	<td class="line x" title="104:178	Using these weights, we get the weighted average in Figure 2 as the general definition of 4In the case of cooccurrence pairs, a word may be involved in two types of relations, being the left or right argument of the pair." ></td>
	<td class="line x" title="105:178	The definitions can be easily adopted to cases in which there are more types of relations, such as provided by syntactic parsing." ></td>
	<td class="line x" title="106:178	167 sim(wl, w2) = ~toetexicon sirnL(wl, w2, w) . WL(Wl, W2, W) -tsimR(wl, w2, w) . WR(wl, w~, w) _ WL(Wl, w2, w) + WR(wl, w2, w) Y'~,o e,,,,,i~or, min(I(w, wl), I(w, w2) ) + min(I(wl, w), I(w~, w)) ~wetexicon max(I(w, Wl), I(w, w2) ) + max(I(wx, w), I(w2, w) ) (6) Figure 2: The definition of the similarity metric." ></td>
	<td class="line x" title="107:178	Exhaustive Search Approximation similar words sim similar words sim aspects 1.000 topics 0.100 areas 0.088 expert 0.079 issues 0.076 approaches 0.072 aspects 1.000 topics 0.100 areas 0.088 expert 0.079 issues 0.076 concerning 0.069 Table 3: The most tic and exhaustive results." ></td>
	<td class="line x" title="108:178	similar words of aspects: heurissearch produce nearly the same similarity s. The values produced by our metric have an intuitive interpretation, as denoting a 'typical' ratio between the mutual information values of each of the two words with another third word." ></td>
	<td class="line x" title="109:178	The metric is reflexive (sirn(w,w) -1), symmetric (sim(wz, w2) = sirn(w2, wz)), but is not transitive (the values of sire(w1, w2) and sire(w2, w3) do not imply anything on the value of sire(w1, w3))." ></td>
	<td class="line x" title="110:178	The left column of Table 3 lists the six most similar words to the word 'aspects' according to this metric, based on our corpus." ></td>
	<td class="line x" title="111:178	More examples of similarity were shown in Figure 1." ></td>
	<td class="line x" title="112:178	4.1 An efficient search heuristic The estimation method of section 3 requires that we identify the most similar words of a given word w. Doing this by computing the similarity between w and each word in the lexicon is computationally very expensive (O(12), where I is the size of the lexicon, and O(l J) to do this in advance for all the words in the lexicon)." ></td>
	<td class="line x" title="113:178	To account for this problem we developed a simple heuristic that searches for words that are potentially similar to w, using thresholds on mutual information values and frequencies of cooccurrence pairs." ></td>
	<td class="line x" title="114:178	The search is based on the property that when computing sim(wl, w2), words that have high mutual information values 5The nominator in our metric resembles the similarity metric in (Hindle, 1990)." ></td>
	<td class="line x" title="115:178	We found, however, that the difference between the two metrics is important, because the denominator serves as a normalization factor." ></td>
	<td class="line x" title="116:178	with both wl and w2 make the largest contributions to the value of the similarity measure." ></td>
	<td class="line x" title="117:178	Also, high and reliable mutual information values are typically associated with relatively high frequencies of the involved cooccurrence pairs." ></td>
	<td class="line x" title="118:178	We therefore search first for all the 'strong neighbors' of w, which are defined as words whose cooccurrence with w has high mutual information and high frequency, and then search for all their 'strong neighbors'." ></td>
	<td class="line x" title="119:178	The words found this way ('the strong neighbors of the strong neighbors of w') are considered as candidates for being similar words of w, and the similarity value with w is then computed only for these words." ></td>
	<td class="line x" title="120:178	We thus get an approximation for the set of words that are most similar to w. For the example given in Table 3, the exhaustive method required 17 minutes of CPU time on a Sun 4 workstation, while the approximation required only 7 seconds." ></td>
	<td class="line x" title="121:178	This was done using a data base of 1,377,653 cooccurrence pairs that were extracted from the corpus, along with their counts." ></td>
	<td class="line x" title="122:178	5 Evaluations 5.1 Word sense disambiguation in machine translation The purpose of the first evaluation was to test whether the similarity based estimation method can enhance the performance of a disambiguation technique." ></td>
	<td class="line x" title="123:178	Typically in a disambiguation task, different cooccurrences correspond to alternative interpretations of the ambiguous construct." ></td>
	<td class="line x" title="124:178	It is therefore necessary that the probability estimates for the alternative cooccurrences will reflect the relative order between their true probabilities." ></td>
	<td class="line x" title="125:178	However, a consistent bias in the estimate is usually not harmful, as it still preserves the correct relative order between the alternatives." ></td>
	<td class="line x" title="126:178	To carry out the evaluation, we implemented a variant of the disambiguation method of (Dagan et al. , 1991), for sense disambiguation in machine translation." ></td>
	<td class="line x" title="127:178	We term this method as THIS, for Target Word Selection." ></td>
	<td class="line x" title="128:178	Consider for example the Hebrew phrase 'laxtom xoze shalom', which translates as 'to sign a peace treaty'." ></td>
	<td class="line x" title="129:178	The word 'laxtom', however, is ambiguous, and can be translated to either 'sign' or 'seal'." ></td>
	<td class="line x" title="130:178	To resolve the ambiguity, the 168 Precision Applicability TWS 85.5 64.3 Augmented TWS 83.6 79.6 Word Frequency 66.9 100 Table 4: Results of TWS, Augmented TWS and Word Frequency methods TWS method first generates the alternative lexical cooccurrence patterns in the targel language, that correspond to alternative selections of target words." ></td>
	<td class="line x" title="131:178	Then, it prefers those target words that generate more frequent patterns." ></td>
	<td class="line x" title="132:178	In our example, the word 'sign' is preferred upon the word 'seal', since the pattern 'to sign a treaty' is much more frequent than the pattern 'to seal a treaty'." ></td>
	<td class="line x" title="133:178	Similarly, the word 'xoze' is translated to 'treaty' rather than 'contract', due to the high frequency of the pattern 'peace treaty '6." ></td>
	<td class="line x" title="134:178	In our implementation, cooccurrence pairs were used instead of lexical cooccurfence within syntactic relations (as in the original work), to save the need of parsing the corpus." ></td>
	<td class="line x" title="135:178	We randomly selected from a software manual a set of 269 examples of ambiguous Hebrew words in translating Hebrew sentences to English." ></td>
	<td class="line x" title="136:178	The expected success rate of random selection for these examples was 23%." ></td>
	<td class="line x" title="137:178	The similarity based estimation method was used to estimate the expected frequency of unobserved cooccurrence pairs, in cases where none of the alternative pairs occurred in the corpus (each pair corresponds to an alternative target word)." ></td>
	<td class="line x" title="138:178	Using this method, which we term Augmented TWS, 41 additional cases were disambiguated, relative to the original method." ></td>
	<td class="line x" title="139:178	We thus achieved an increase of about 15% in the applicability (coverage) of the TWS method, with a small decrease in the overall precision." ></td>
	<td class="line x" title="140:178	The performance of the Augmented TWS method on these 41 examples was about 15% higher than that of a naive, Word Frequency method, which always selects the most frequent translation." ></td>
	<td class="line x" title="141:178	It should be noted that the Word Frequency method is equivalent to using the frequency based estimate, in which higher word frequencies entail a higher estimate for the corresponding cooccurrence." ></td>
	<td class="line x" title="142:178	The results of the experiment are summarized in Table 4." ></td>
	<td class="line x" title="143:178	5.2 A data recovery task In the second evaluation, the estimation method had to distinguish between members of two sets of 8It should be emphasized that the TWS method uses only a monolingual target corpus, and not a bilingual corpus as in other methods ((Brown et al. , 1991; Gale et al. , 1992))." ></td>
	<td class="line x" title="144:178	The alternative cooccurrence patterns in the target language, which correspond to the alternative translations of the ambiguous source words, are constructed using a bilingual lexicon." ></td>
	<td class="line x" title="145:178	cooccurrence pairs, one of them containing pairs with relatively high probability and the other pairs with low probability." ></td>
	<td class="line x" title="146:178	To a large extent, this task simulates a typical scenario in disambiguation, as demonstrated in the first evaluation." ></td>
	<td class="line x" title="147:178	Ideally, this evaluation should be carried out using a large set of held out data, which would provide good estimates for the true probabilities of the pairs in the test sets." ></td>
	<td class="line x" title="148:178	The estimation method should then use a much smaller training corpus, in which none of the example pairs occur, and then should try to recover the probabilities that are known to us from the held out data." ></td>
	<td class="line x" title="149:178	However, such a setting requires that the held out corpus would be several times larger than the training corpus, while the latter should be large enough for robust application of the estimation method." ></td>
	<td class="line x" title="150:178	This was not feasible with the size of our corpus, and the rather noisy data we had." ></td>
	<td class="line x" title="151:178	To avoid this problem, we obtained the set of pairs with high probability from the training corpus, selecting pairs that occur at least 5 times." ></td>
	<td class="line x" title="152:178	We then deleted these pairs from the data base that is used by the estimation method, forcing the method to recover their probabilities using the other pairs of the corpus." ></td>
	<td class="line x" title="153:178	The second set, of pairs with low probability, was obtained by constructing pairs that do not occur in the corpus." ></td>
	<td class="line x" title="154:178	The two sets, each of them containing 150 pairs, were constructed randomly and were restricted to words with individual frequencies between 500 and 2500." ></td>
	<td class="line x" title="155:178	We term these two sets as the occurring and non-occurring sets." ></td>
	<td class="line x" title="156:178	The task of distinguishing between members of the two sets, without access to the deleted frequency information, is by no means trivial." ></td>
	<td class="line x" title="157:178	Trying to use the individual word frequencies will result in performance close to that of using random selection." ></td>
	<td class="line x" title="158:178	This is because the individual frequencies of all participating words are within the same range of values." ></td>
	<td class="line x" title="159:178	To address the task, we used the following procedure: The frequency of each cooccurrence pair was estimated using the similarity-based estimation method." ></td>
	<td class="line x" title="160:178	If the estimated frequency was above 2.5 (which was set arbitrarily as the average of 5 and 0), the pair was recovered as a member of the occurring set." ></td>
	<td class="line x" title="161:178	Otherwise, it was recovered as a member of the non-occurring set." ></td>
	<td class="line x" title="162:178	Out of the 150 pairs of the occurring set, our method correctly identified 119 (79%)." ></td>
	<td class="line x" title="163:178	For th e non-occurring set, it correctly identified 126 pairs (84%)." ></td>
	<td class="line x" title="164:178	Thus, the method achieved an 0retail accuracy of 81.6%." ></td>
	<td class="line x" title="165:178	Optimal tuning of the threshold, to a value of 2, improves the overall accuracy to 85%, where about 90% of the members of the occurring set and 80% of those in the non-occurring 169 set are identified correctly." ></td>
	<td class="line x" title="166:178	This is contrasted with the optimal discrimination that could be achieved by frequency based estimation, which is 58%." ></td>
	<td class="line x" title="167:178	Figures 3 and 4 illustrate the results of the experiment." ></td>
	<td class="line x" title="168:178	Figure 3 shows the distributions of the expected frequency of the pairs in the two sets, using similarity based and frequency based estimation." ></td>
	<td class="line x" title="169:178	It clearly indicates that the similarity based method gives high estimates mainly to members of the occurring set and low estimates mainly to members of the non-occurring set." ></td>
	<td class="line x" title="170:178	Frequency based estimation, on the other hand, makes a much poorer distinction between the two sets." ></td>
	<td class="line x" title="171:178	Figure 4 plots the two types of estimation for pairs in the occurring set as a function of their true frequency in the corpus." ></td>
	<td class="line x" title="172:178	It can be seen that while the frequency based estimates are always low (by construction) the similarity based estimates are in most cases closer to the true value." ></td>
	<td class="line x" title="173:178	6 Conclusions In both evaluations, similarity based estimation performs better than frequency based estimation." ></td>
	<td class="line x" title="174:178	This indicates that when trying to estimate cooccurrence probabilities, it is useful to consider the cooccurrence patterns of the specific words and not just their frequencies, as smoothing methods do." ></td>
	<td class="line x" title="175:178	Comparing with class based models, our approach suggests the advantage of making the most specific analogies for each word, instead of making analogies with all members of a class, via general class parameters." ></td>
	<td class="line x" title="176:178	This raises the question whether generalizations over word classes, which follow long traditions in semantic classification, indeed provide the best means for inferencing about properties of words." ></td>
	<td class="line x" title="177:178	Acknowledgements We are grateful to Alon Itai for his help in initiating this research." ></td>
	<td class="line x" title="178:178	We would like to thank Ken Church and David Lewis for their helpful comments on earlier drafts of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P93-1043
Raisins, Sultanas, And Currants: Lexical Classification And Abstraction Via Context Priming
Hutches, David J.;"></td>
	<td class="line x" title="1:69	RAISINS, SULTANAS~ AND CURRANTS: LEXICAL CLASSIFICATION AND ABSTRACTION VIA CONTEXT PRIMING David J. Hutches Department of Computer Science and Engineering, Mail Code 0114 University of California, San Diego La Jolla, CA 92093-0114 dhutches@ucsd.edu Abstract In this paper we discuss the results of experiments which use a context, essentially an ordered set of lexical items, as the seed from which to build a network representing statistically important relationships among lexical items in some corpus." ></td>
	<td class="line x" title="2:69	A metric is then applied to the nodes in the network in order to discover those pairs of items related by high indices of similarity." ></td>
	<td class="line x" title="3:69	The goal of this research is to instantiate a class of items corresponding to each item in the priming context." ></td>
	<td class="line x" title="4:69	We believe that this instantiation process is ultimately a special case of abstraction over the entire network; in this abstraction, similar nodes are collapsed into metanodes which may then function as if they were single lexical items." ></td>
	<td class="line x" title="5:69	I. Motivation and Background With respect to the processing of language, one of the tasks at which human beings seem relatively adept is the ability to determine when it is appropriate to make generalizations and when it is appropriate to preserve distinctions." ></td>
	<td class="line x" title="6:69	The process of abstraction and knowing when it might reasonably be used is a necessary tool in reducing the complexity of the task of processing natural language." ></td>
	<td class="line x" title="7:69	Part of our current research is an investigation into how the process of abstraction might be realized using relatively low-level statistical information extracted from large textual corpora." ></td>
	<td class="line x" title="8:69	Our experiments are an attempt to discover a method by which class information about the members of some sequence of lexical items may be obtained using strictly statistical methods." ></td>
	<td class="line x" title="9:69	For our purposes, the class to which a lexical item belongs is defined by its instantiation." ></td>
	<td class="line x" title="10:69	Given some context such as he walked across the room, we would like to be able to instantiate classes of items corresponding to each item in the context (e.g. , the class associated with walked might include items such as paced, stepped, or sauntered)." ></td>
	<td class="line x" title="11:69	The corpora used in our experiments are the Lancaster-Oslo-Bergen (LOB) corpus and a subset of the ACL/DCI Wall Street Journal (WSJ) corpus." ></td>
	<td class="line x" title="12:69	The LOB corpus consists of a total of 1,008,035 words, composed of 49,174 unique words." ></td>
	<td class="line x" title="13:69	The subset of the WSJ corpus that we use has been pre-processed such that all letters are folded to lower case, and numbers have been collapsed to a single token; the subset consists of 18,188,548 total words and 159,713 unique words." ></td>
	<td class="line x" title="14:69	II." ></td>
	<td class="line x" title="15:69	Context Priming It is not an uncommon notion that a word may be defined not rigourously as by the assignment of static syntactic and semantic classes, but dynamically as a function of its usage (Firth 1957, 11)." ></td>
	<td class="line x" title="16:69	Such usage may be derived from cooccurrence information over the course of a large body of text." ></td>
	<td class="line x" title="17:69	For each unique lexical item in a corpus, there exists an 'association neighbourhood' in which that item lives; such a neighbourhood is the probability distribution of the words with which the item has co-occurred." ></td>
	<td class="line x" title="18:69	If one posits that similar lexical items will have similar neighbourhoods, one possible method of instantiating a class of lexical items would be to examine all unique items in a corpus and find those whose neighbourhoods are most similar to the neighbourhood of the item whose class is being instantiated." ></td>
	<td class="line x" title="19:69	However, the potential computational problems of such an approach are clear." ></td>
	<td class="line x" title="20:69	In the context of our approach to this problem, most lexical items in the search space are not even remotely similar to the item for which a class is being instantiated." ></td>
	<td class="line x" title="21:69	Furthermore, a substantial part of a lexical item's association neighbourhood provides only superficial information about that item." ></td>
	<td class="line x" title="22:69	What is required is a process whereby the search space is reduced dramatically." ></td>
	<td class="line x" title="23:69	One method of accomplishing this pruning is via context priming." ></td>
	<td class="line x" title="24:69	In context priming, we view a context as the seed upon which to build a network describing that part of the corpus which is, in some sense, close to the context." ></td>
	<td class="line x" title="25:69	Thus, just as an individual lexical item has associated with it a unique neighbourhood, so too does a context have such a neighbourhood." ></td>
	<td class="line x" title="26:69	The basic process of building a network is straightforward." ></td>
	<td class="line x" title="27:69	Each item in the priming context has associated with it a unique neighbourhood defined in terms of those lexical items with which it has co-occurred." ></td>
	<td class="line x" title="28:69	Similarly, each of these 292 latter items also has a unique association neighbourhood." ></td>
	<td class="line x" title="29:69	Generating a network based on some context consists in simply expanding nodes (lexicM items) further and further away from the context until some threshold, called the depth of the network, is reached." ></td>
	<td class="line x" title="30:69	Just as we prune the total set of unique lexical items by context priming, we also prune the neighbourhood of each node in the network by using a statistical metric which provides some indication of how important the relationship is between each lexical item and the items in its neighbourhood." ></td>
	<td class="line oc" title="31:69	In the results we describe here, we use mutual information (Fano 1961, 27-28; Church and Hanks 1990) as the metric for neighbourhood pruning, pruning which occurs as the network is being generated." ></td>
	<td class="line x" title="32:69	Yet, another parameter controlling the topology of the network is the extent of the 'window' which defines the neighbourhood of a lexical item (e.g. , does the neighbourhood of a lexical item consist of only those items which have cooccurred at a distance of up to 3, 5, 10, or 1000 words from the item)." ></td>
	<td class="line x" title="33:69	III." ></td>
	<td class="line x" title="34:69	Operations on the Network The network primed by a context consists merely of those lexical items which are closely reachable via co-occurrence from the priming context." ></td>
	<td class="line x" title="35:69	Nodes in the network are lexical items; arcs represent co-occurrence relations and carry the value of the statistical metric mentioned above and the distance of co-occurrence." ></td>
	<td class="line x" title="36:69	With such a network we attempt to approximate the statistically relevant neighbourhood in which a particular context might be found." ></td>
	<td class="line x" title="37:69	In the tests performed on the network thus far we use the similarity metric S(x, y) IA n BI 2 IA u BI where x and y are two nodes representing lexical items, the neighbourhoods of which are expressed as the sets of arcs A and B respectively." ></td>
	<td class="line x" title="38:69	The metric S is thus defined in terms of the cardinalities of sets of arcs." ></td>
	<td class="line x" title="39:69	Two arcs are said to be equal if they reference (point to) the same lexical item at the same offset distance." ></td>
	<td class="line x" title="40:69	Our metric is a modification of the Tanimoto coefficient (Bensch and Savitch 1992); the numerator is squared in order to assign a higher index of similarity to those nodes which have a higher percentage of arcs in common." ></td>
	<td class="line x" title="41:69	Our first set of tests concentrated directly on items in the seed context." ></td>
	<td class="line x" title="42:69	Using the metric above, we attempted to instantiate classes of lexical items for each item in the context." ></td>
	<td class="line x" title="43:69	In those cases where there were matches, the results were often encouraging." ></td>
	<td class="line x" title="44:69	For example, in the LOB corpus, using the seed context John walked across the room, a network depth of 6, a mutual information threshold of 6.0 for neighbourhood pruning, and a window of 5, for the item John, we instantiated the class {Edward, David, Charles, Thomas}." ></td>
	<td class="line x" title="45:69	A similar test on the WSJ corpus yielded the following class for john r ichard,paul,thomas,edward,david, donald,daniel,f rank,michael,dennis, j oseph,j im,alan,dan,roger Recall that the subset of the WSJ corpus we use has had all items folded to lower case as part of the pre-processing phase, thus all items in an instantiated class will also be folded to lower case." ></td>
	<td class="line x" title="46:69	In other tests, the instantiated classes were less satisfying, such as the following class generated for wife using the parameters above, the LOB, and the context his wife walked across the room mouth,father,uncle,lordship, } finger s,mother,husband,f ather ' s, shoulder,mother ' s,brother In still other cases, a class could not be instantiated at all, typically for items whose neighbourhoods were too small to provide meaningful matching information." ></td>
	<td class="line x" title="47:69	IV." ></td>
	<td class="line x" title="48:69	Abstraction It is clear that even the most perfectly derived lexical classes will have members in common." ></td>
	<td class="line x" title="49:69	The different senses of bank are often given as the classic example of a lexically ambiguous word." ></td>
	<td class="line x" title="50:69	From our own data, we observed this problem because of our preprocessing of the WSJ corpus; the instantiation of the class associated with mark included some proper names, but also included items such as marks, currencies, yen, and dollar, a confounding of class information that would not have occurred had not case folding taken place." ></td>
	<td class="line x" title="51:69	Ideally, it would be useful if a context could be made to exert a more constraining influence during the course of instantiating classes." ></td>
	<td class="line x" title="52:69	For example, if it is reasonably clear from a context, such as mark loves mary, that the 'mark' in question is the human rather than the financial variety, how may we ensure that the context provides the proper constraining information if loves has never cooccurred with mark in the original corpus?" ></td>
	<td class="line x" title="53:69	293 In the case of the ambiguous mark above, while this item does not appear in the neighbourhood of loves, other lexical items do (e.g. , everyone, who, him, mr), items which may be members of a class associated with mark." ></td>
	<td class="line x" title="54:69	What is proposed, then, is to construct incrementally classes of items over the network, such that these classes may then function as a single item for the purpose of deriving indices of similarity." ></td>
	<td class="line x" title="55:69	In this way, we would not be looking for a specific match between mark and loves, but rather a match among items in the same class as mark; items in the same class as loves, and items in the same class as mary." ></td>
	<td class="line x" title="56:69	With this in mind, our second set of experiments concentrated not specifically on items in the priming context, but on the entire network, searching for candidate items to be collapsed into meta-nodes representing classes of items." ></td>
	<td class="line x" title="57:69	Our initial experiments in the generation of pairs of items which could be collapsed into metanodes were more successful than the tests based on items in the priming context." ></td>
	<td class="line x" title="58:69	Using the LOB corpus, the same parameters as before, and the priming context John walked across the room, the following set of pairs represents some of the good matches over the generated network." ></td>
	<td class="line x" title="59:69	(minut es,days),(three,f ive),(f ew, five), (2,3),(f ig,t able),(days,years),(40,50), (me,him),(three,f ew),(4,5),(50,100), (currants,sultanas),(sultanas,raisins), (currants,raisins), Using the WSJ corpus, again the same parameters, and the context john walked across the room, part of the set of good matches generated was (months,weeks),(rose,f ell),(days,weeks), (s ingle-a-plus,t riple-b-plus), (single-a-minus,t riple-b-plus), (lawsuit,complaint),(analyst,economist) (j ohn,robert), (next,past ), ( s ix,f ive), (lower,higher),(goodyear,f irest one), (prof it,loss),(billion,million), (j llne,march),(concedes,acknowledges), (days,weeks ), (months,years ), It should be noted that the sets given above represent the best good matches." ></td>
	<td class="line x" title="60:69	Empirically, we found that a value of S > 1.0 tends to produce the most meaningful pairings." ></td>
	<td class="line x" title="61:69	At S < 1.0, the amount of 'noisy' pairings increases dramatically." ></td>
	<td class="line x" title="62:69	This is not an absolute threshold, however, as apparently unacceptable pairings do occur at S > 12, such as, for example, the pairs (catching, teamed), (accumulating, rebuffed), and (father, mind)." ></td>
	<td class="line x" title="63:69	V. Future Research The results of our initial experiments in generating classes of lexical items are encouraging, though not conclusive." ></td>
	<td class="line x" title="64:69	We believe that by incrementally collapsing pairs of very similar items into meta-nodes, we may accomplish a kind of abstraction over the network which will ultimately allow the more accurate instantiation of classes for the priming context." ></td>
	<td class="line x" title="65:69	The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in (Brown, et al. 1992)." ></td>
	<td class="line x" title="66:69	The approach taken in the cited work is somewhat different than ours and while our method is no less computationally complex than that of Brown, et al. , we believe that it is somewhat more manageable because of the pruning effect provided by context priming." ></td>
	<td class="line x" title="67:69	On the other hand, unlike the work described by Brown, et al. , we as yet have no clear criterion for stopping the merging process, save an arbitrary threshold." ></td>
	<td class="line x" title="68:69	Finally, it should be noted that our goal is not, strictly speaking, to generate classes over an entire vocabulary, but only that portion of the vocabulary relevant for a particular context." ></td>
	<td class="line x" title="69:69	It is hoped that, by priming with a context, we may be able to effect some manner of word sense disambiguation in those cases where the meaning of a potentially ambiguous item,nay be resolved by hints in the context." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W93-0111
Experiments In Syntactic And Semantic Classification And Disambiguation Using Bootstrapping
Futrelle, Robert P.;Gauch, Susan;"></td>
	<td class="line x" title="1:372	EXPERIMENTS IN SYNTACTIC AND SEMANTIC CLASSIFICATION AND DISAMBIGUATION USING BOOTSTRAPPING* Robert P. Futrelle and Susan Gauch Biological Knowledge Laboratory College of Computer Science Northeastern University Boston, MA 02115 {futrelle, sgauch}@ccs.neu.edu ABSTRACT Bootstrap methods (unsupervised classification) that generate word classes without requiring pretagging have had notable success in the last few years." ></td>
	<td class="line x" title="2:372	The methods described here strengthen these approaches and produce excellent word classes from a 200,000 word corpus." ></td>
	<td class="line x" title="3:372	The method uses mutual information measures plus positional information from the words in the immediate context of a target word to compute similarities." ></td>
	<td class="line x" title="4:372	Using the similarities, classes are built using hierarchical agglomerative clustering." ></td>
	<td class="line x" title="5:372	At the leaves of the classification tree, words are grouped by syntactic and semantic similarity." ></td>
	<td class="line x" title="6:372	Further up the tree, the classes are primarily syntactic." ></td>
	<td class="line x" title="7:372	Once the initial classes are found, they can be used to classify ambiguous words, i.e., part-of-speech tagging." ></td>
	<td class="line x" title="8:372	This is done by expanding each context word of a target instance into a tightly defined class of similar words, a simsct." ></td>
	<td class="line x" title="9:372	The use of simsets is shown to increase the tagging accuracy from 83% to 92% for the forms 'cloned' and 'deduced'." ></td>
	<td class="line x" title="10:372	INTRODUCTION The identification of the syntactic class and the discovery of semantic information for words not contained in any on-line dictionary or thesaurus is an important and challenging * This material is based upon work supported by the National Science Foundation under Grant No." ></td>
	<td class="line x" title="11:372	DIR-8814522." ></td>
	<td class="line x" title="12:372	problem." ></td>
	<td class="line x" title="13:372	Excellent methods have been developed for part-of-speech (POS) tagging using stochastic models trained on partially tagged corpora (Church, 1988; Cutting, Kupiec, Pedersen & Sibun, 1992)." ></td>
	<td class="line x" title="14:372	Semantic issues have been addressed, particularly for sense disambiguation, by using large contexts, e.g., 50 nearby words (Gale, Church & Yarowsky, 1992) or by reference to on-line dictionaries (Krovetz, 1991; Lesk, 1986; Liddy & Paik, 1992; Zernik, 1991)." ></td>
	<td class="line x" title="15:372	More recently, methods to work with entirely untagged corpora have been developed which show great promise (Brill & Marcus, 1992; Finch & Chater, 1992; Myaeng & Li, 1992; Schutze, 1992)." ></td>
	<td class="line x" title="16:372	They are particularly useful for text with specialized vocabularies and word use." ></td>
	<td class="line x" title="17:372	These methods of unsupervised classification typically have clustering algorithms at their heart (Jain & Dubes, 1988)." ></td>
	<td class="line x" title="18:372	They use similarity of contexts (the distribution principle) as a measure of distance in the space of words and then cluster similar words into classes." ></td>
	<td class="line x" title="19:372	This paper demonstrates a particular approach to these classification techniques." ></td>
	<td class="line oc" title="20:372	In our approach, we take into account both the relative positions of the nearby context words as well as the mutual information (Church & Hanks, 1990) associated with the occurrence of a particular context word." ></td>
	<td class="line o" title="21:372	The similarities computed from these measures of the context contain information about both syntactic and semantic relations." ></td>
	<td class="line x" title="22:372	For example, high similarity values are obtained for the two semantically similar nouns, 117 'diameter' and 'length', as well as for the two adjectives 'nonmotile' and 'nonchemotactic'." ></td>
	<td class="line x" title="23:372	We demonstrate the technique on three problems, all using a 200,000 word corpus composed of 1700 abstracts from a specialized field of biology: #1: Generating the full classification tree for the 1,000 most frequent words (covering 80% of all word occurrences)." ></td>
	<td class="line x" title="24:372	#2: The classification of 138 occurrences of the -ed forms, 'cloned' and 'deduced' into four syntactic categories, including improvements by using expanded context information derived in #1." ></td>
	<td class="line x" title="25:372	#3: The classification of 100 words that only occur once in the entire corpus (hapax legomena), again using expanded contexts." ></td>
	<td class="line x" title="26:372	The results described below were obtained using no pretagging or on-line dictionary, but the results compare favorably with methods that do." ></td>
	<td class="line x" title="27:372	The results are discussed in terms of the semantic fields they delineate, the accuracy of the classifications and the nature of the errors that occur." ></td>
	<td class="line x" title="28:372	The results make it clear that this new technology is very promising and should be pursued vigorously." ></td>
	<td class="line o" title="29:372	The power of the approach appears to result from using a focused corpus, using detailed positional information, using mutual information measures and using a clustering method that updates the detailed context information when each new cluster is formed." ></td>
	<td class="line x" title="30:372	Our approach was inspired by the fascinating results achieved by Finch and Chater at Edinburgh and the methods they used (Finch & Chater, 1992)." ></td>
	<td class="line x" title="31:372	THE CORPUS -TECHNICAL, FOCUSED AND SMALL In the Biological Knowledge Laboratory we are pursuing a number of projects to analyze, store and retrieve biological research papers, including working with full text and graphics (Futrelle, Kakadiaris, Alexander, Carriero, Nikolakis & Futrelle, 1992; Gauch & Futrelle, 1993)." ></td>
	<td class="line x" title="32:372	The work is focused on the biological field of bacterial chemotaxis." ></td>
	<td class="line x" title="33:372	A biologist has selected approximately 1,700 documents representing all the work done in this field since its inception in 1965." ></td>
	<td class="line x" title="34:372	Our study uses the titles for all these documents plus all the abstracts available for them." ></td>
	<td class="line x" title="35:372	The resulting corpus contains 227,408 words with 13,309 distinct word forms, including 5,833 words of frequency 1." ></td>
	<td class="line x" title="36:372	There are 1,686 titles plus 8,530 sentences in the corpus." ></td>
	<td class="line x" title="37:372	The sentence identification algorithm requires two factors -contiguous punctuation C.', '!', or '')?" ></td>
	<td class="line x" title="38:372	and capitalization of the following token." ></td>
	<td class="line x" title="39:372	To eliminate abbreviations, the token prior to the punctuation must not be a single capital letter and the capitalized token after the punctuation may not itself be followed by a contiguous '.'." ></td>
	<td class="line x" title="40:372	is, An example of a sentence from the corpus '$pre2$ $prel$ one of the open reading frames was translated into a protein with $pct$ amino acid identity to S. typhimurium Flil and $pct$ identity to the beta subunit of E. coli ATP synthase $posl$ $pos2$' The positional items $pre and $pos have been added to furnish explicit context for sentence initial and sentence final constituents." ></td>
	<td class="line x" title="41:372	Numbers have been converted to three forms corresponding to integers, reals and percentages C$pct$ in the example above)." ></td>
	<td class="line x" title="42:372	The machine-readable version of the corpus uses double quoted items to ease processing by Lisp, our language of choice." ></td>
	<td class="line x" title="43:372	The terminology we will use for describing words is as follows:  Target word: A word to be classified." ></td>
	<td class="line x" title="44:372	Context words: Appearing within some distance of a target word, 'The big ~ cat 9.a the mat'." ></td>
	<td class="line x" title="45:372	 Word class: Any defined set of word forms or labeled instances." ></td>
	<td class="line x" title="46:372	Simset: A word class in which each item, an expansion word, has a similarity greater than some chosen cutoffto a single base word." ></td>
	<td class="line x" title="47:372	 Labeled instances: Forms such as 'cloned48' or 'cloned73VBN', that 118 would replace an occurrence of 'cloned'." ></td>
	<td class="line x" title="48:372	DESCRIBING AND QUANTIFYING WORD CONTI~TS In these experiments, the context of a target word is described by the preceding two context words and the following two context words, Figure 1." ></td>
	<td class="line x" title="49:372	Each position is represented by a 150 element vector corresponding to the occurrence of the 150 highest frequency words in the corpus, giving a 600-dimensional vector describing the four-word context." ></td>
	<td class="line x" title="50:372	Initially, the counts from all instances of a target word form w are summed so that the entry in the corresponding context word position in the vector is the sum of the occurrences of that context word in that position for the corresponding target word form; it is the joint frequency of the context word." ></td>
	<td class="line x" title="51:372	For example, if the word the immediately precedes 10 occurrences of the word gene in the corpus then the element corresponding to the in the -1C context vector of gene is set to 10." ></td>
	<td class="line o" title="52:372	Subsequently, a 600-dimensional vector of mutual information values, MI, is computed from the frequencies as follows, log 2 NZ~ + Ml(cw)= \[.\].j1\] This expresses the mutual information value for the context word c appearing with the target word w. The mutual information is large whenever a context word appears at a much higher frequency, fcw, in the neighborhood of a target word than would be predicted from the overall frequencies in the corpus, fc and fw." ></td>
	<td class="line o" title="53:372	The formula adds 1 to the frequency ratio, so that a 0 (zero) occurrence corresponds to 0 mutual information." ></td>
	<td class="line x" title="54:372	A possibly better strategy (Church, Gale, Hanks & Hindle, 1991) is capable of generating negative mutual information for the nonoccurrence or low-frequency occurrence of a very high-frequency word and has the form,  fN(~ +I)'~ Ml(cw) = log 2 ~." ></td>
	<td class="line x" title="55:372	/ \[ I#." ></td>
	<td class="line x" title="56:372	J In any case, some smoothing is necessary to prevent the mutual information from diverging when fcw= O. SIMILARITY, CLUSTERING AND CLASSIFICATION IN WORD SPACE When the mutual information vectors are computed for a number of words, they can be compared to see which words have similar contexts." ></td>
	<td class="line x" title="57:372	The comparison we chose is the inner product, or cosine measure, which can vary between -1.0 and +1.0 (Myaeng & Li, 1992)." ></td>
	<td class="line x" title="58:372	Once this similarity is computed for all word pairs in a set, various techniques can be used to identify classes of similar words." ></td>
	<td class="line x" title="59:372	The method we chose is hierarchical agglomerative clustering (Jain & Dubes, 1988)." ></td>
	<td class="line x" title="60:372	The two words with the highest similarity are first joined into a two-word cluster." ></td>
	<td class="line x" title="61:372	Word to be classified with context: -2C -1C W +1C +2C Figure 1." ></td>
	<td class="line x" title="62:372	The 600-dimensional context vector around a target word W. Each subvecter describes the frequency and mutual information of the occurrences of the 150 highest frequency words, HFC, in the corpus." ></td>
	<td class="line x" title="63:372	119 A mutual information vector for the cluster is computed and the cluster and remaining words are again compared, choosing the most similar to join, and so on." ></td>
	<td class="line x" title="64:372	(To compute the new mutual information vector, the context frequencies in the vectors for the two words or clusters joined at each step are summed, element-wise)." ></td>
	<td class="line x" title="65:372	In this way, a binary tree is constructed with words at the leaves leading to a single root covering all words." ></td>
	<td class="line x" title="66:372	Each cluster, a node in the binary tree, is described by an integer denoting its position in the sequence of cluster formation, the total number of words, the similarity of the two children that make it up, and its member words." ></td>
	<td class="line x" title="67:372	Here, for example, are the first 15 clusters from the analysis described in Experiment #I in the next section, (0 2 0.73926157 is was) (1 2 0.6988309 were are) (Z 4 0.708@31 (is was) (were are)) (3 2 0.65726656 found shown) (4 Z 0.6216794 the a) (5 Z 0.5913143 s mM) (6 Z 0.59088105 coli typhimurium) (7 2 0.586728 galactose ribose) (8 2 0.58630705 method procedure) (9 2 0.58404166 K-12 K12) (10 2 0.5833811 required necessary) (11 3 0.5793458 rain (s raM)) (12 2 0.5750035 isolated constructed) (13 3 0.56909233 (found shown) used) (14 2 0.$6750214 cells strains) (I5 3 0.5652546 mutants (cells strains)) In this sample it is clear that clusters are sometimes formed by the pairing of two individual words, sometimes by pairing one word and a previous cluster, and sometimes by combining two already formed clusters." ></td>
	<td class="line x" title="68:372	In normal tagging, a word is viewed as a member of one of a small number of classes." ></td>
	<td class="line x" title="69:372	In the classification approach we are using, there can be thousands of classes, from pairs of words up to the root node which contains all words in a single class." ></td>
	<td class="line x" title="70:372	Thus, every class generated is viewed extensionally, it is a structured collection of occurrences in the corpus, with their attendant frequencies and contexts." ></td>
	<td class="line x" title="71:372	The classes so formed will reflect the particular word use in the corpus they are derived from." ></td>
	<td class="line x" title="72:372	EXPERIMENT #1: CLASSIFICATION OF THE 1,000 HIGHEST FREQUENCY WORDS The first experiment classified the 1,000 highest frequency words in the corpus, producing 999 clusters (0-998) during the process." ></td>
	<td class="line x" title="73:372	$pre and Spas words were included in the context set, but not in the target set." ></td>
	<td class="line x" title="74:372	Near the leaves, words clustered by syntax (part of speech) and by semantics." ></td>
	<td class="line x" title="75:372	Later, larger clusters tended to contain words of the same syntactic class, but with less semantic homogeneity." ></td>
	<td class="line x" title="76:372	In each example below, the words listed are the entire contents of the cluster mentioned." ></td>
	<td class="line x" title="77:372	The most striking property of the clusters produced was the classification of words into coherent semantic fields." ></td>
	<td class="line x" title="78:372	Grefenstette has pointed out (Grefenstette, 1992) that the Deese antonyms, such as 'large' and 'small' or 'hot' and 'cold' show up commonly in these analyses." ></td>
	<td class="line x" title="79:372	Our methods discovered entire graded fields, rather than just pairs of opposites." ></td>
	<td class="line x" title="80:372	The following example shows a cluster of seventeen adjectives describing comparative quantity terms, cluster 756, similarity 0.28, decreased, effective, few, greater, high, higher, increased, large, less, low, lower, more, much, no, normal, reduced, short Note that pairs such as 'high' and 'higher' and 'low' and 'lower' appear." ></td>
	<td class="line x" title="81:372	'No', meaning 'none' in this collection, is located at one extreme." ></td>
	<td class="line x" title="82:372	The somewhat marginal item, 'effective', entered the cluster late, at cluster 704." ></td>
	<td class="line x" title="83:372	It appears in collocations, such as 'as effective as' and 'effective than', in which the other terms also appear." ></td>
	<td class="line x" title="84:372	Comparing the cluster to Roger's (Berrey, 1962) we find that all the items are in the Roget category Comparative Quantity except for 'effective' and 'no'." ></td>
	<td class="line x" title="85:372	The cluster item, 'large' is not in this Roget category but the category does include 'big', 'huge' and 'vast', so the omission is clearly an error in Roget's." ></td>
	<td class="line x" title="86:372	With 120 this correction, 88% (15/17) of the items are in the single Roget category." ></td>
	<td class="line x" title="87:372	The classification of technical terms from genetics and biochemistry is of particular interest, because many of these terms do not appear in available dictionaries or thesauri." ></td>
	<td class="line x" title="88:372	Cluster 374, similarity 0.37, contains these 18 items, che, cheA, cheB, cheR, cheY, cheZ, double, fla, flaA, taB, flaE, H2, hag, mot, motB, tar, trg, tsr All of these are abbreviations for specific bacterial mutations, except for 'double'." ></td>
	<td class="line x" title="89:372	Its appearance drives home the point that the classification depends entirely on usage." ></td>
	<td class="line x" title="90:372	20 of the 30 occurrences of 'double' precede the words 'mutant' or 'mutants', as do most of the othermutation terms in this cluster." ></td>
	<td class="line x" title="91:372	Cluster 240, similarity 0.4 contains these termS, microscopy, electrophoresis, chromatography Each of these is a noun describing a common technique used in experiments in this domain." ></td>
	<td class="line x" title="92:372	The standard Linnean nomenclature of Genus followed by species, such as Escherichia coli, is reflected by cluster 414, which contains 22 species names, and cluster 510, which contains 9 genus names." ></td>
	<td class="line x" title="93:372	In scientific research, the determination of causal factors and the discovery of essential elements is a major goal." ></td>
	<td class="line x" title="94:372	Here are six concepts in this semantic field comprising cluster 183, similarity 0.43, required, necessary, involved, responsible, essential, important These terms are used almost interchangeably in our corpus, but they don't fare as well in Roget's because of anthropocentric attachments to concepts such as fame, duty and legal liability." ></td>
	<td class="line x" title="95:372	Discussion of Experiment #1 Given the limited context and modest sized corpus, the classification algorithm is bound to make mistakes, though a study of the text concordance will always tell us why the algorithm failed in any specific case." ></td>
	<td class="line x" title="96:372	For example, as the similarity drops to 0.24 at cluster 824 we see the adverb triple 'greatly', 'rapidly' and 'almost'." ></td>
	<td class="line x" title="97:372	This is still acceptable, but by cluster 836 (similarity 0.24) we see the triple, 'them', 'ring', 'rings'." ></td>
	<td class="line x" title="98:372	At the end there is only a single cluster, 998, which must include all words." ></td>
	<td class="line x" title="99:372	It comes together stubbornly with a negative similarity of-0.51." ></td>
	<td class="line x" title="100:372	One problem encountered in this work was that the later, larger clusters have less coherence than we would hope for, identifying an important research issue." ></td>
	<td class="line x" title="101:372	Experiment #1 took 20 hours to run on a Symbolics XL1200." ></td>
	<td class="line x" title="102:372	A fundamental problem is to devise decision procedures that will tell us which classes are semantically or syntactically homogeneous; procedures that tell us where to cut the tree." ></td>
	<td class="line x" title="103:372	The examples shown earlier broke down soon after, when words or clusters which in our judgment were weakly related began to be added." ></td>
	<td class="line x" title="104:372	We are exploring the numerous methods to refine clusters once formed as well as methods to validate clusters for homogeneity (Jain & Dubes, 1988)." ></td>
	<td class="line x" title="105:372	There are also resampling methods to validate clusters formed by top-down partitioning methods (Jain & Moreau, 1987)." ></td>
	<td class="line x" title="106:372	All of these methods are computationally demanding but they can result in criteria for when to stop clustering." ></td>
	<td class="line x" title="107:372	On the other hand, we mustn't assume that word relations are so simple that we can legitimately insist on finding neatly separated clusters." ></td>
	<td class="line x" title="108:372	Word relations may simply be too complex and graded for this ever to occur." ></td>
	<td class="line x" title="109:372	The semantic fields we discovered were not confined to synonyms." ></td>
	<td class="line x" title="110:372	To understand why this is the case, consider the sentences, 'The temperature is higher today'." ></td>
	<td class="line x" title="111:372	and, 'The temperature is lower today'." ></td>
	<td class="line x" title="112:372	There is no way to tell from the syntax which word to expect." ></td>
	<td class="line x" title="113:372	The choice is dependent on the situation in the world; it represents data from the world." ></td>
	<td class="line x" title="114:372	The 121 utterances are informative for just that reason." ></td>
	<td class="line x" title="115:372	Taking this reasoning a step further, information theory would suggest that for two contrasting words to be maximally informative, they should appear about equally often in discourse." ></td>
	<td class="line x" title="116:372	This is born out in our corpus (fhigher=58, i~ower=46) and for the Brown corpus (fhigher=147, fiower=110)." ></td>
	<td class="line x" title="117:372	The same relations are found for many other contrasting pairs, with some bias towards 'positive' terms." ></td>
	<td class="line x" title="118:372	The most extreme 'positive' bias in our corpus is fpossible=88, fimpossible=0; 'never say never' seems to be the catchphrase here -highly appropriate for the field of biology." ></td>
	<td class="line x" title="119:372	Some of the chemical term clusters that were generated are interesting because they contain class terms such as 'sugar' and 'ion' along with specific members of the classes (hyponyms), such as 'maltose' and 'Na +''." ></td>
	<td class="line x" title="120:372	Comparing these in our KWIC concordance suggests that there may be methodical techniques for identifying some of these generalization hierarchies using machine learning (supervised classification) (Futrelle & Gauch, 1993)." ></td>
	<td class="line x" title="121:372	For another discussion of attempts to generate generalization hierarchies, see (Myaeng & Li, 1992)." ></td>
	<td class="line x" title="122:372	As a corpus grows and new words appear, one way to classify them is to find their similarity to the N words for which context vectors have already been computed." ></td>
	<td class="line x" title="123:372	This requires N comparisons." ></td>
	<td class="line x" title="124:372	A more efficient method which would probably give the same result would be to successively compare the word to clusters in the tree, starting at the root." ></td>
	<td class="line x" title="125:372	At each node, the child which is most similar to the unclassified word is followed." ></td>
	<td class="line x" title="126:372	This is a logarithmic search technique for finding the best matching class which takes only O(log2N) steps." ></td>
	<td class="line x" title="127:372	In such an approach, the hierarchical cluster is being used as a decision tree, which have been much studied in the machine learning literature (Quinlan, 1993)." ></td>
	<td class="line x" title="128:372	This is an alternate view of the classification approach as the unsupervised learning of a decision tree." ></td>
	<td class="line x" title="129:372	EXPERIMENT #2: DISAMBIGUATION OF -ED FORMS The following experiment is interesting because it shows a specific use for the similarity computations." ></td>
	<td class="line x" title="130:372	They are used here to increase the accuracy of term disambiguation which means selecting the best tag or class for a potentially ambiguous word." ></td>
	<td class="line x" title="131:372	Again, this is a bootstrap method; no prior tagging is needed to construct the classes." ></td>
	<td class="line x" title="132:372	But if we do identify the tags for a few items by hand or by using a hand-tagged reference corpus, the tags for all the other items in a cluster can be assumed equal to the known items." ></td>
	<td class="line x" title="133:372	The passive voice is used almost exclusively in the corpus, with some use of the editorial 'We'." ></td>
	<td class="line x" title="134:372	This results in a profusion of participles such as 'detected', 'sequenced' and 'identified'." ></td>
	<td class="line x" title="135:372	But such -ed forms can also be simple past tense forms or adjectives." ></td>
	<td class="line x" title="136:372	In addition, we identified their use in a postmodifying participle clause such as, ' the value ~ from this measurement'." ></td>
	<td class="line x" title="137:372	Each one of the 88 instances of 'cloned' and the 50 instances of 'deduced' was hand tagged and given a unique ID. Then clustering was applied to the resulting collection, giving the result shown in Figure 2A." ></td>
	<td class="line x" title="138:372	Experiments #2 and #3 took about 15 minutes each to run." ></td>
	<td class="line x" title="139:372	The resultant clusters are somewhat complex." ></td>
	<td class="line x" title="140:372	There are four tags and we have shown the top four clusters, but two of the clusters contain adjectives exclusively." ></td>
	<td class="line x" title="141:372	The past participle and postmodifier occur together in the same cluster." ></td>
	<td class="line x" title="142:372	(We studied the children of cluster 4, hoping to find better separation, but they are no better." ></td>
	<td class="line x" title="143:372	) The scoring metric we chose was to associate each cluster with the items that were in the majority in the node and score all other items as errors." ></td>
	<td class="line x" title="144:372	This is a good approximation to a situation in which a 'gold standard' is available to classify the clusters by independent means, such as comparing the clusters to items from a pretagged reference corpus." ></td>
	<td class="line x" title="145:372	122 I I,I 2,,VBO\]  JI i \] JJ = Adjective VBD = Verb, past tense VBN : Verb, past participle VBNP = Participle in postmodifying clause 46 VBN 13 VBNP 1 VBD 1 JJ Figure 2A." ></td>
	<td class="line x" title="146:372	Clustering of 88 occurrence of'cloned' and 50 occurrences of'deduced' into four syntactic categories." ></td>
	<td class="line x" title="147:372	The abbreviations, such as 'JJ', are based on (Francis & Kucera, 1982)." ></td>
	<td class="line x" title="148:372	There is a strong admixture of adjectives in cluster 2 and all the postmodifiers are confounded with the past participles in cluster 4." ></td>
	<td class="line x" title="149:372	The total number of errors (minority classes in a cluster) is 23 for a success rate of(138-23)/138 = 83%." ></td>
	<td class="line x" title="150:372	All minority members of a cluster are counted as errors." ></td>
	<td class="line x" title="151:372	This leads to the 83% error rate quoted in the figure caption." ></td>
	<td class="line x" title="152:372	The results shown in Figure 2A can be improved as follows." ></td>
	<td class="line x" title="153:372	Because we are dealing with single occurrences, only one element, or possibly zero, in each of the four context word vectors is filled, with frequency 1." ></td>
	<td class="line x" title="154:372	The other 149 elements have frequency (and mutual information) 0.0." ></td>
	<td class="line x" title="155:372	These sparse vectors will therefore have little or no overlap with vectors from other occurrences." ></td>
	<td class="line x" title="156:372	In order to try to improve the classification, we expanded the context values in an effort to produce more overlap, using the following strategy: We proceed as if the corpus is far larger so that in addition to the actual context words already seen, there are many occurrences of highly similar words in the same positions." ></td>
	<td class="line x" title="157:372	For each non-zero context in each set of 150, we expand it to an ordered class of similar words in the 150, picking words above a fixed similarity threshold (0.3 for the experiments reported here)." ></td>
	<td class="line x" title="158:372	Such a class is called a simset, made up of a base word and a sequence of expansion words." ></td>
	<td class="line x" title="159:372	As an example of the expansion of context words via simsets, suppose that the occurrence of the frequency 1 word 'cheA-cheB' is immediately preceded by 'few' and the occurrence of the frequency 1 word 'CheA/CheB' is immediately preceded by 'less'." ></td>
	<td class="line x" title="160:372	The -I C context vectors for each will have l's in different positions so there will be no overlap between them." ></td>
	<td class="line x" title="161:372	If we expanded 'few' into a large enough simset, the set would eventually contain, 'less', and vice-versa." ></td>
	<td class="line x" title="162:372	Barring that, each simset might contain a distinct common word such as 'decreased'." ></td>
	<td class="line x" title="163:372	In either case, there would now be some overlap in the context vectors so that the similar use of 'cheA-cheB' and 'CheA/CheB' could be detected." ></td>
	<td class="line x" title="164:372	The apparent frequency of each expansion word is based on its corpus frequency relative to the corpus frequency of the word being expanded." ></td>
	<td class="line x" title="165:372	To expand a single context word instance ci appearing with frequency fik in the context of 1 or more occurrences of center word wk, choose all cj such that cj e {set of high-frequency context words} and the 123 similarity S(ci,cj) _> St, a threshold value." ></td>
	<td class="line x" title="166:372	Set the apparent frequency of each expansion word cj to fjk = S(ci,cj)xfik x fj / fi, where fi and fj are the corpus frequencies of ci and cj." ></td>
	<td class="line x" title="167:372	Normalize the total frequency of the context word plus the apparent frequencies of the expansion words to fik." ></td>
	<td class="line x" title="168:372	For the example being discussed here, fik = 1, St=0.3 and the average number of expansion words was 6." ></td>
	<td class="line x" title="169:372	Recomputing the classification of the -ed forms with the expanded context words results in the improved classification shown in Figure 2B." ></td>
	<td class="line x" title="170:372	The number of classification errors is halved, yielding a success rate of 92%." ></td>
	<td class="line x" title="171:372	This is comparable in performance to many stochastic tagging algorithms." ></td>
	<td class="line x" title="172:372	Discussion of Experiment #2 This analysis is very similar to part-ofspeech tagging." ></td>
	<td class="line x" title="173:372	The simsets of only 6 items are far smaller than the part-of-speech categories conventionally used." ></td>
	<td class="line x" title="174:372	But since we use high frequency words, they represent a substantial portion of the instances." ></td>
	<td class="line x" title="175:372	Also, they have higher specificity than, say, Verb." ></td>
	<td class="line x" title="176:372	Many taggers work sequentially and depend on the left context." ></td>
	<td class="line x" title="177:372	But some words are best classified by their right context." ></td>
	<td class="line x" title="178:372	We supply both." ></td>
	<td class="line x" title="179:372	Clearly this small experiment did not reach the accuracy of the very best taggers, but it performed well." ></td>
	<td class="line x" title="180:372	This experiment has major ramifications for the future." ></td>
	<td class="line x" title="181:372	The initial classifications found merged all identical word forms together, both as targets and contexts." ></td>
	<td class="line x" title="182:372	But disambiguation techniques such as those in Experiment #2 can be used to differentially tag word occurrences with some degree of accuracy." ></td>
	<td class="line x" title="183:372	These newly classified items can in turn be used as new target and context items (if their frequencies are adequate) and the analysis can be repeated." ></td>
	<td class="line x" title="184:372	Iterating the method in this way should be able to refine the classes until a fixed point is reached at which no further improvement in classification occurs." ></td>
	<td class="line x" title="185:372	The major challenge in using this approach will be to keep it computationally tractable." ></td>
	<td class="line x" title="186:372	This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989; Kupiec, 1992; Rabiner, 1989), though our zeroth order solution begins quite close to the desired result, so it should converge very close to a global optimum." ></td>
	<td class="line x" title="187:372	11 1 VBNP 21 I ll VBNP 4 JJ 31 'I 9 VBD 46 VBN 3 JJ 1 JJ 1 VBD 1 VBNP Figure 2B." ></td>
	<td class="line x" title="188:372	Clustering of'cloned' and 'deduced' after expansion of the context words." ></td>
	<td class="line x" title="189:372	The postmodifying form, not isolated before, is fairly well isolated in its own subclass." ></td>
	<td class="line x" title="190:372	The total number of errors is reduced from 23 to 11, for a success rate of 92%." ></td>
	<td class="line x" title="191:372	124 EXPERIMENT #3: CLASSIFICATION OF SINGLE WORD OCCURRENCES When classifying multiple instances of a single word form as we did in Experiment #2, there are numerous collocations that aid the classification." ></td>
	<td class="line x" title="192:372	For example, 16 of the 50 occurrences of the word 'deduced' occur in the phrase, 'of the ~ amino acid sequence'." ></td>
	<td class="line x" title="193:372	But with words of frequency 1, we cannot rely on such similarities." ></td>
	<td class="line x" title="194:372	Nevertheless, we experimented with classifying 100 words of corpus frequency 1 with and without expanding the context words." ></td>
	<td class="line x" title="195:372	Though hand scoring the results is difficult, we estimate that there were 8 reasonable pairs found initially and 26 pairs when expansion was used." ></td>
	<td class="line x" title="196:372	Examples of words that paired well without expansion are 'overlaps' and 'flank' (due to a preceding 'which') and 'malB' and 'cheA-cheB' (due to the context 'the \[malB, cheA-cheB\] region')." ></td>
	<td class="line x" title="197:372	After expansion, pairs such as 'setting', 'resetting' appeared (due in part to the expansion of the preceding 'as' and 'to' context words into simsets which both included 'with', 'in' and 'by')." ></td>
	<td class="line x" title="198:372	Discussion of Experiment #3." ></td>
	<td class="line x" title="199:372	The amount of information available about frequency 1 words can vary from a lot to nothing at all, and most frequently tends to the latter, viz., 'John and Mary looked at the blork'." ></td>
	<td class="line x" title="201:372	Nevertheless, such words are prominent, 44% of our corpus' vocabulary." ></td>
	<td class="line x" title="202:372	About half Of them are non-technical and can therefore be analyzed from other corpora or on-line dictionaries." ></td>
	<td class="line x" title="203:372	Word morphology and Latinate morphology in particular, can be helpful." ></td>
	<td class="line x" title="204:372	Online chemical databases, supplemented with rules for chemical nomenclature will clarify additional items, e.g., '2-epoxypropylphosphonic' or 'phosphoglucomutase-deflcient'." ></td>
	<td class="line x" title="205:372	Furthermore, there are naming conventions for genetic strains and mutants which aid recognition." ></td>
	<td class="line x" title="206:372	The combination of all these methods should lead to a reasonable accuracy in the classification of frequency 1 words." ></td>
	<td class="line x" title="207:372	FURTHER DISCUSSION AND FUTURE DIRECTIONS Our corpus of 220,000 words is much smaller than ones of 40 million words (Finch & Chater, 1992) and certainly of 360 million (Brown, Della Pietra, deSousa, Lai & Mercer, 1992)." ></td>
	<td class="line x" title="208:372	But judging by the results we have presented, especially for the full 1,000 word clustering, our corpus appears to make up in specificity for what it lacks in size." ></td>
	<td class="line x" title="209:372	Extending this work beyond abstracts to full papers will be challenging because our corpus requires SGML markup to deal with Greek characters, superscripts and subscripts, etc.(Futrelle, Dunn, Ellis & Pescitelli, 1991)." ></td>
	<td class="line x" title="211:372	We have over 500,000 words from the bacterial chemotaxis research papers carefully marked up by hand in this way." ></td>
	<td class="line x" title="212:372	The characterization of context can obviously be extended to more context positions or words, and extensions of our word-rooted expansion techniques are potentially very powerful, combining broad coverage with specificity in a 'tunable' way." ></td>
	<td class="line x" title="213:372	Morphology can be added to the context vectors by using the ingenious suggestion of Brill to collect high-frequency tri-letter word endings (Brill & Marcus, 1992)." ></td>
	<td class="line x" title="214:372	One of the more subtle problems of the context specification is that it uses summed frequencies, so it may fail to retain important correlations." ></td>
	<td class="line x" title="215:372	Thus if only AB or CD sequences occurred, or only AD or CB sequences, they would lead to the same (summed) context vector." ></td>
	<td class="line x" title="216:372	The only correlations faithfully retained are those with the target word." ></td>
	<td class="line x" title="217:372	Characterizing context n-grams could help work around this problem, but is a non-trivial task." ></td>
	<td class="line x" title="218:372	ACKNOWLEDGMENTS Thanks to Durstin Selfridge for a careful reading of the drafts and to an anonymous reviewer for pointing out the work of (Phillips, 1985) who builds networks to describe word 125 classes rather than trees as we have." ></td>
	<td class="line x" title="219:372	Thanks to the ERC for providing an excellent working environment." ></td>
	<td class="line x" title="220:372	BIBLIOGRAPHY Berrey, L. V." ></td>
	<td class="line x" title="221:372	(Ed.)." ></td>
	<td class="line x" title="222:372	(1962)." ></td>
	<td class="line x" title="223:372	Roget's International Thesaurus." ></td>
	<td class="line x" title="224:372	New York, NY: Thomas Y. Crowell." ></td>
	<td class="line x" title="225:372	Brill, E. , & Marcus, M." ></td>
	<td class="line x" title="226:372	(1992)." ></td>
	<td class="line x" title="227:372	Tagging an Unfamiliar Text with Minimal Human Supervision." ></td>
	<td class="line x" title="228:372	In AAA/ Fall Symposium Series: Probabilistic Approaches to Natural Language (Working Notes), (pp." ></td>
	<td class="line x" title="229:372	10-16)." ></td>
	<td class="line x" title="230:372	Cambridge, MA." ></td>
	<td class="line x" title="231:372	Brown, P. F., Della Pietra, V. J., deSousa, P. V., Lai, J. C., & Mercer, R. L." ></td>
	<td class="line x" title="232:372	(1992)." ></td>
	<td class="line x" title="233:372	Class-based n-gram models of natural language." ></td>
	<td class="line x" title="234:372	Computational Linguistics, 18(4), 467-479." ></td>
	<td class="line x" title="235:372	Church, K. , Gale, W. , Hanks, P. , & Hindle, D." ></td>
	<td class="line x" title="236:372	(1991)." ></td>
	<td class="line x" title="237:372	Using statistics in lexical analysis." ></td>
	<td class="line x" title="238:372	In U. Zernik (Eds.), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon (pp." ></td>
	<td class="line x" title="239:372	115-164)." ></td>
	<td class="line x" title="240:372	Hillsdale, NJ: Lawrence Erlbaum." ></td>
	<td class="line x" title="241:372	Church, K. W." ></td>
	<td class="line x" title="242:372	(1988)." ></td>
	<td class="line x" title="243:372	A Stochastic Parts Program and Noun Parser for Unrestricted Text." ></td>
	<td class="line x" title="244:372	In Proc." ></td>
	<td class="line x" title="245:372	2nd Conf." ></td>
	<td class="line x" title="246:372	on Applied Nat." ></td>
	<td class="line x" title="247:372	Lang." ></td>
	<td class="line x" title="248:372	Processing, (pp." ></td>
	<td class="line x" title="249:372	136143)." ></td>
	<td class="line x" title="250:372	Austin, TX." ></td>
	<td class="line x" title="251:372	Church, K. W., & Hanks, P." ></td>
	<td class="line x" title="252:372	(1990)." ></td>
	<td class="line x" title="253:372	Word Association Norms, Mutual Information and Lexicography." ></td>
	<td class="line x" title="254:372	Computational Linguistics, 16(1), 22-29." ></td>
	<td class="line x" title="255:372	Cutting, D. , Kupiec, J. , Pedersen, J. , & Sibun, P." ></td>
	<td class="line x" title="256:372	(1992)." ></td>
	<td class="line x" title="257:372	A Practical Part-of-Speech Tagger." ></td>
	<td class="line x" title="258:372	In Proc." ></td>
	<td class="line x" title="259:372	3rd Conf." ></td>
	<td class="line x" title="260:372	on Applied Natural Language Processing, (pp." ></td>
	<td class="line x" title="261:372	133140)." ></td>
	<td class="line x" title="262:372	Finch, S. , & Chater, N." ></td>
	<td class="line x" title="263:372	(1992)." ></td>
	<td class="line x" title="264:372	Bootstrapping Syntactic Categories Using Statistical Methods." ></td>
	<td class="line x" title="265:372	In W. Daelemans & D. Powers (Ed.), Proc." ></td>
	<td class="line x" title="266:372	Ist SHOE Workshop, (pp." ></td>
	<td class="line x" title="267:372	229235)." ></td>
	<td class="line x" title="268:372	Tilburg U. , The Netherlands." ></td>
	<td class="line x" title="269:372	Francis, W. N., & Kucera, H." ></td>
	<td class="line x" title="270:372	(1982)." ></td>
	<td class="line x" title="271:372	Frequency Analysis of English Usage." ></td>
	<td class="line x" title="272:372	Boston, MA: Houghton Mifflin." ></td>
	<td class="line x" title="273:372	Futrelle, R. P., Dunn, C. C., Ellis, D. S., & Pescitelli, M. J., Jr." ></td>
	<td class="line x" title="274:372	(1991)." ></td>
	<td class="line x" title="275:372	Preprocessing and lexicon design for parsing technical text." ></td>
	<td class="line x" title="276:372	In Proc." ></td>
	<td class="line x" title="277:372	2nd Intern'l Workshop on Parsing Technologies, (pp." ></td>
	<td class="line x" title="278:372	31-40): ACL." ></td>
	<td class="line x" title="279:372	Futrelle, R. P., & Gauch, S. E." ></td>
	<td class="line x" title="280:372	(1993)." ></td>
	<td class="line x" title="281:372	Using unsupervised and supervised classification to discover word equivalences and other relations." ></td>
	<td class="line x" title="282:372	In 9th Ann." ></td>
	<td class="line x" title="283:372	Waterloo OED Conf." ></td>
	<td class="line x" title="284:372	Oxford, England (submitted)." ></td>
	<td class="line x" title="285:372	Futrelle, R. P., Kakadiaris, I. A., Alexander, J. , Carriero, C. M., Nikolakis, N. , & Futrelle, J. M." ></td>
	<td class="line x" title="286:372	(1992)." ></td>
	<td class="line x" title="287:372	Understanding Diagrams in Technical Documents." ></td>
	<td class="line x" title="288:372	IEEE Computer, 25(7), 75-78." ></td>
	<td class="line x" title="289:372	Gale, W. A., Church, K. W., & Yarowsky, D." ></td>
	<td class="line x" title="290:372	(1992)." ></td>
	<td class="line x" title="291:372	Work on Statistical Methods for Word Sense Disambiguation." ></td>
	<td class="line x" title="292:372	In AAAIFall Symposium Series: Probabilistic Approaches to Natural Language (Working Notes), (pp." ></td>
	<td class="line x" title="293:372	54-60)." ></td>
	<td class="line x" title="294:372	Cambridge, MA." ></td>
	<td class="line x" title="295:372	Gauch, S. , & Futrelle, R. P." ></td>
	<td class="line x" title="296:372	(1993)." ></td>
	<td class="line x" title="297:372	Broad and Deep Classification Methods for Mining Raw Text." ></td>
	<td class="line x" title="298:372	In CIKM 93." ></td>
	<td class="line x" title="299:372	Washington, DC (submitted)." ></td>
	<td class="line x" title="300:372	Grefenstette, G." ></td>
	<td class="line x" title="301:372	(1992)." ></td>
	<td class="line x" title="302:372	Finding Semantic Similarity in Raw Text: the Deese Antonyms." ></td>
	<td class="line x" title="303:372	In AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language (Working Notes), (pp." ></td>
	<td class="line x" title="304:372	61-66)." ></td>
	<td class="line x" title="305:372	Cambridge, MA." ></td>
	<td class="line x" title="306:372	Jain, A. K., & Dubes, R. C." ></td>
	<td class="line x" title="307:372	(1988)." ></td>
	<td class="line x" title="308:372	Algorithms for Clustering Data." ></td>
	<td class="line x" title="309:372	Englewood Cliffs, NJ: Prentice Hall." ></td>
	<td class="line x" title="310:372	Jain, A. K., & Moreau, J. V." ></td>
	<td class="line x" title="311:372	(1987)." ></td>
	<td class="line x" title="312:372	Bootstrap Technique in Cluster Analysis." ></td>
	<td class="line x" title="313:372	Pattern Recognition, 20(5), 547-568." ></td>
	<td class="line x" title="314:372	Krovetz, R." ></td>
	<td class="line x" title="315:372	(1991)." ></td>
	<td class="line x" title="316:372	Lexical Acquisition and Information Retrieval." ></td>
	<td class="line x" title="317:372	In U. Zernik (Eds.), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon (pp." ></td>
	<td class="line x" title="318:372	45-64)." ></td>
	<td class="line x" title="319:372	Hillsdale, New Jersey: Lawrence Erlbaum Associates, Publishers." ></td>
	<td class="line x" title="320:372	Kupiec, J." ></td>
	<td class="line x" title="321:372	(1989)." ></td>
	<td class="line x" title="322:372	Augmenting a hidden Markov model for phrase dependent word tagging." ></td>
	<td class="line x" title="323:372	In DARPA Speech and Natural Language Workshop, a, (pp." ></td>
	<td class="line x" title="324:372	92-98)." ></td>
	<td class="line x" title="325:372	Cape Cod MA." ></td>
	<td class="line x" title="326:372	Kupiec, J." ></td>
	<td class="line x" title="327:372	(1992)." ></td>
	<td class="line x" title="328:372	Robust part-of-speech tagging using a hidden Markov model." ></td>
	<td class="line x" title="329:372	Computer Speech and Language, 6, 225242." ></td>
	<td class="line x" title="330:372	Lesk, M." ></td>
	<td class="line x" title="331:372	(1986)." ></td>
	<td class="line x" title="332:372	Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from and Ice Cream Cone." ></td>
	<td class="line x" title="333:372	In ACM SIGDOC Conf., (pp." ></td>
	<td class="line x" title="335:372	24-26)." ></td>
	<td class="line x" title="336:372	Toronto, Ont.: ACM Press." ></td>
	<td class="line x" title="337:372	Liddy, E. D., & Paik, W." ></td>
	<td class="line x" title="338:372	(1992)." ></td>
	<td class="line x" title="339:372	StatisticallyGuided Word Sense Disambiguation." ></td>
	<td class="line x" title="340:372	In AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language (Working Notes), (pp." ></td>
	<td class="line x" title="341:372	98-107)." ></td>
	<td class="line x" title="342:372	Cambridge, MA." ></td>
	<td class="line x" title="343:372	Myaeng, S. H., & Li, M." ></td>
	<td class="line x" title="344:372	(1992)." ></td>
	<td class="line x" title="345:372	Building Term Clusters by Acquiring Lexical Semantics from a Corpus." ></td>
	<td class="line x" title="346:372	In Y. Yesha (Ed.), CIKM92, (pp." ></td>
	<td class="line x" title="347:372	130-137)." ></td>
	<td class="line x" title="348:372	Baltimore, MD: ISMM." ></td>
	<td class="line x" title="349:372	Phillips, M." ></td>
	<td class="line x" title="350:372	(1985)." ></td>
	<td class="line x" title="351:372	Aspects of text structure: an investigation of the lexical organisation of text." ></td>
	<td class="line x" title="352:372	New York, NY: Elsevier." ></td>
	<td class="line x" title="353:372	Quinlan, J. R." ></td>
	<td class="line x" title="354:372	(1993)." ></td>
	<td class="line x" title="355:372	C4.5: Programs for Machine Learning." ></td>
	<td class="line x" title="356:372	San Mateo, CA: Morgan Kaufmann." ></td>
	<td class="line x" title="357:372	Rabiner, L. R." ></td>
	<td class="line x" title="358:372	(1989)." ></td>
	<td class="line x" title="359:372	A Tutorial on Hidden Markov Models an Selected Applications in SpeeCh Recognition." ></td>
	<td class="line x" title="360:372	Proceedings of the IEEE, 77(2), 257-286." ></td>
	<td class="line x" title="361:372	Schutze, H." ></td>
	<td class="line x" title="362:372	(1992)." ></td>
	<td class="line x" title="363:372	Context Space." ></td>
	<td class="line x" title="364:372	In AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language (Working Notes), (pp." ></td>
	<td class="line x" title="365:372	113-120)." ></td>
	<td class="line x" title="366:372	Cambridge, MA." ></td>
	<td class="line x" title="367:372	Zernik, U." ></td>
	<td class="line x" title="368:372	(1991)." ></td>
	<td class="line x" title="369:372	Tralnl vs. Train2: Tagging Word Senses in Corpus." ></td>
	<td class="line x" title="370:372	In U. Zernik (Eds.), Lexical Acquisition: Exploiting OnLine Resources to Build a Lexicon (pp." ></td>
	<td class="line x" title="371:372	97112)." ></td>
	<td class="line x" title="372:372	Hillsdale, New Jersey: Lawrence Erlbaum Associates, Publishers ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W93-0310
Computation Of Word Associations Based On Co-Occurrences Of Words In Large Corpora
Wettler, Manfred;Rapp, Reinhard;"></td>
	<td class="line x" title="1:130	Computation of Word Associations Based on the Co-Occurences of Words in Large Corpora I Manfred Wettler & Reinhard Rapp University of Paderborn, Cognitive Psychology Postfach 1621, D-4790 Paderborn, Germany Abstract A statistical model is presented which predicts the strengths of word-associations from the relative frequencies of the common occurrences of words in large bodies of text." ></td>
	<td class="line x" title="2:130	These predictions are compared with the Minnesota association norms for 100 stimulus words." ></td>
	<td class="line x" title="3:130	The average agreement between the predicted and the observed responses is only slightly weaker than the agreement between the responses of an arbitrary subject and the responses of the other subjects." ></td>
	<td class="line x" title="4:130	It is shown that the approach leads to equally good results for both English and German." ></td>
	<td class="line x" title="5:130	1 Introduction In the association experiment first used by Galton (1880) subjects are asked to respond to a stimulus word with the first word that comes to their mind." ></td>
	<td class="line x" title="6:130	These associative responses have been explained in psychology by the principle of learning by contiguity: 'Objects once experienced together tend to become associated in the imagination, so that when any one of them is thought of, the others are likely to be thought of also, in the same order of sequence or coexistence as before." ></td>
	<td class="line x" title="7:130	This statement we may name the law of mental association by contiguity'." ></td>
	<td class="line x" title="8:130	(William James, 1890, p. 561)." ></td>
	<td class="line x" title="9:130	When the association experiment is conducted with many subjects, tables are obtained which list the frequencies of particular responses to the stimulus words." ></td>
	<td class="line x" title="10:130	These tables are called association norms." ></td>
	<td class="line x" title="11:130	Many studies in psychology give evidence that there is a relation between the perception, learning and forgetting of verbal material and the associations between words." ></td>
	<td class="line x" title="12:130	If we assume that word-associations determine language production, then it should be possible to estimate the strength of an associative relation between two words on the basis of the relative frequencies that these words co-occur in texts." ></td>
	<td class="line oc" title="13:130	Church et al.(1989), Wettler & Rapp (1989) and Church & Hanks (1990) describe algorithms which do this." ></td>
	<td class="line n" title="15:130	However, the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects." ></td>
	<td class="line x" title="16:130	This paper describes such a comparison and shows that corpus-based computations of word associations are similar to association norms collected from human subjects." ></td>
	<td class="line x" title="17:130	According to the law of association by contiguity, the association strength between two words should be a function of the relative frequency of the two words being perceived together, i.e. the relative frequency of the two words occuring together." ></td>
	<td class="line x" title="18:130	Further more, the association strength between words should determine word selection during language or speech production: Only those words can be uttered or written down which associativeiy come to mind." ></td>
	<td class="line x" title="19:130	If this assumption holds, then it should be possible to predict word associations from the common occurences of words in texts." ></td>
	<td class="line x" title="20:130	IThis research was supported by the Heinz-Nixdorf-lnstitute (project 536) 84 2 Model According to the law of association by contiguity the learning of associations can be described as follows: If two words i and j occur together, the association strength aid(t) between i and j is increased by a constant fraction of the difference between the maximum and the actual association strength." ></td>
	<td class="line x" title="21:130	This leads for association strengths between 0 and 1 to the following formula: aid (t + 1) = aid(t) + (1 aid (t))." ></td>
	<td class="line x" title="22:130	01 if (i&j) (1) If word i occurs in another context, i. e. not in proximity to word j, the association strength between i and j is decreased by a constant fraction: aid (t + 1) = ai,j(t)." ></td>
	<td class="line x" title="23:130	(1 02) if (i&-~j) (2) Under the assumption that the learning rate 01 and the inhibition rate 02 are of identical size, the expected value aid of the association strength aid(t) from i to j for t ~ o~ is equal to the conditional probability of j given i (compare Foppa, 1965): ai,j = p(jli) (3) From these assumptions it could be expected that a stimulus word i leads to those response j, for which the value of equation 3 is at a maximum." ></td>
	<td class="line x" title="24:130	Rapp & Wettler (1991) compared this with other predictions, where additional assumptions on learning and reproduction were taken into account." ></td>
	<td class="line x" title="25:130	With equation 3, mainly words with high corpus frequencies, e. g. function words, were predicted as associative responses." ></td>
	<td class="line x" title="26:130	The predictions were improved when the following formula was used with an exponent of a = 0.66, and the word with the highest rid was considered to be the associative response." ></td>
	<td class="line x" title="27:130	p(jli) (4) = p(j)'-'7 The introduction of the denominator indicates that in the association experiment less frequent words are used than during language production." ></td>
	<td class="line x" title="28:130	This inhibition of frequent words can be explained by the experimental situation, which furthers responses that are specific to the stimulus word." ></td>
	<td class="line x" title="29:130	The exponential function can be interpreted as the tendency that subjective estimations are often found to be exponential functions of the quantities to be estimated." ></td>
	<td class="line x" title="30:130	3 Association norms used For the comparison between the predicted and the associations of human subjects we have used the association norms coUected by Russet\] ~ Jenkins (Jenkins, 1970)." ></td>
	<td class="line x" title="31:130	They have the advantage that translations of the stimulus words were also given to German subjects (Russell & Meseck, 1959, and RusseLl, 1970) so that our model could be tested for English as well as for German." ></td>
	<td class="line x" title="32:130	The Russell & Jenkins association norms, also referred to as the Minnesota word association norms, were collected in 1952." ></td>
	<td class="line x" title="33:130	The I00 stimulus words from the Kent-Rosanoff word association test (Kent ~ Rosanoff, 1910) were presented to 1008 students of two large introductory psychology classes at the University of Minnesota." ></td>
	<td class="line x" title="34:130	The subjects were instructed, to write after each word 'the first word that it makes you think of'." ></td>
	<td class="line x" title="35:130	Seven years later, Russell Meseck (1959) repeated the same experiment in Germany with a carefully translated list of the stimulus words." ></td>
	<td class="line x" title="36:130	The subjects were 331 students and pupils from the area near W~rzburg." ></td>
	<td class="line x" title="37:130	The quantitative results reported on later will be based on comparisons with these norms." ></td>
	<td class="line x" title="38:130	85 The American as well as the German association norms were collected more than 30 years ago." ></td>
	<td class="line x" title="39:130	The texts Which were used to simulate these associations are more recent." ></td>
	<td class="line x" title="40:130	One might expect therefore that this discrepancy will impair the agreement between the observed and the predicted responses." ></td>
	<td class="line x" title="41:130	Better predictions might be attained if the observed associations had been produced by the same subjects as the texts from which the predictions are computed." ></td>
	<td class="line x" title="42:130	However, such a procedure is hardly realizable, and our results will show that despite these discrepancies associations to common words can be predicted successfully." ></td>
	<td class="line x" title="43:130	4 Text corpora In order to get reliable estimates of the co-occurences of words, large text corpora have to be used." ></td>
	<td class="line x" title="44:130	Since associations of the 'average subject' are to be simulated, the texts should not be specific to a certain domain, but reflect the wide distribution of different types of texts and speech as perceived in every day life." ></td>
	<td class="line x" title="45:130	The following selection of some 33 million words of machine readable English texts used in this study is a modest attempt to achieve this goal:  Brown corpus of present day American English (1 million words)  LOB corpus of present day British English (1 million words)  Belletristic literature from Project Gutenberg (1 million words)  Articles from the New Scientist from Oxford Text Archive (1 million words)  Wall Street Journal from the ACL/DCI (selection of 6 million words)  Hansard Corpus." ></td>
	<td class="line x" title="46:130	Proceedings of the Canadian Parliament (selection of 5 million words from the ACL/DCI-corpus)  Grolier's Electronic Encyclopedia (8 million words)  Psychological Abstracts from PsycLIT (selection of 3.5 million words)  Agricultural abstracts from the Agricola database (3.5 million words)  DOE scientific abstracts from the ACL/DCI (selection of 3 million words) To compute associations for German the following corpora comprising about 21 million words were used:  LIMAS corpus of present-day written German (1.1 million words)  Freiburger Korpus from the Institute for German Language (IDS), Mannheim (0.5 million words of spoken German)  Ma~nheimer Korpus 1 from the IDS (2.2 million words of present-day written German from books and periodicals)  Handbuchkorpora 85, 86 and 87 from the IDS (9.3 million words of newspaper texts)  German abstracts from the psychological database PSYNDEX (8 million words) For technical reasons, not all words occuring in the corpora have been used in the simulation." ></td>
	<td class="line x" title="47:130	The vocabulary used consists of all words which appear more than ten times in the English or German corpus." ></td>
	<td class="line x" title="48:130	It also includes all 100 stimulus words and all responses in the English or German association norms." ></td>
	<td class="line x" title="49:130	This leads to an English vocabulary of about 72000 and a German vocabulary of 65000 words." ></td>
	<td class="line x" title="50:130	Hereby, a word is defined as a string of alpha characters separated by non-alpha characters." ></td>
	<td class="line x" title="51:130	Punctuation marks and special characters axe treated as words." ></td>
	<td class="line x" title="52:130	86 5 Computation of the association strengths The text corpora were read in word by word." ></td>
	<td class="line x" title="53:130	Whenever one of the 100 stimulus words occured, it was determined which other words occured within a distance of twelve words to the left or to the right of the stimulus word, and for every pair a counter was updated." ></td>
	<td class="line x" title="54:130	The so defined frequencies of co-occurence tt(i&j), the frequencies of the single words tt(i) and the total number of words in the corpus Q were stored in tables." ></td>
	<td class="line x" title="55:130	Using these tables, the probabilities in formula (4) can be replaced by relative frequencies: tt(i&j) It(j) a Qo H(i&j) n(i) / Qo = H(j)o (5) In this formula the first term on the right side does not depend on j and therefore has no effect on the prediction of the associative response." ></td>
	<td class="line x" title="56:130	With H(j) in the denominator of the second term, estimation errors have a strong impact on the association strengths for rare words." ></td>
	<td class="line x" title="57:130	Therefore, by modifying formula (5), words with low corpus frequencies had to be weakened." ></td>
	<td class="line x" title="58:130	~ H(i&j)/H(j) ~ fiir H(j) > ~.Q,'i,j = Q) f ir H(j) < Z' Q (6) According to our model the word j with the highest associative strength ~/,./to the stimulus word / should be the associative response." ></td>
	<td class="line x" title="59:130	The best results were observed when parameter a was chosen to be 0.66." ></td>
	<td class="line x" title="60:130	Parameters ~5 and 3' turned out to be relatively uncritical, and therefore to simplify parameter optimization were both set to the same value of 0.00002." ></td>
	<td class="line xc" title="61:130	Ongoing research shows that formula (6) has a number of weaknesses, for example that it does not discriminate words with co-occurence-frequency zero, as discussed by Gale & Church (1990) in a comparable context." ></td>
	<td class="line x" title="62:130	However, since the results reported on later are acceptable, it probably gets the major issues right." ></td>
	<td class="line x" title="63:130	One is, that subjects usually respond with common, i.e. frequent words in the free association task." ></td>
	<td class="line x" title="64:130	The other is, that estimations of co-occurence-frequencies for low-frequency-words are too poor to be useful." ></td>
	<td class="line x" title="65:130	6 Results In table 1 a few sample association lists as predicted by our system are compared to the associative responses as given by the subjects in the Russell ~ Jenkins experiment." ></td>
	<td class="line x" title="66:130	A complete list of the predicted and observed responses is given in table 2." ></td>
	<td class="line x" title="67:130	It shows for all 100 stimulus words used in the association experiment conducted by Russell & Jenkins, a) their corpus frequency, b) the primary response, i.e. the most frequent response given by the subjects, c) the number of subjects who gave the primary response, d) the predicted response and e) the number of subjects who gave the predicted response." ></td>
	<td class="line x" title="68:130	The valuation of the predictions has to take into account that association norms are conglomerates of the answers of different subjects which differ considerably from each other." ></td>
	<td class="line x" title="69:130	A satisfactory prediction would be proven if the difference between the predicted and the observed responses were about equal to the difference between an average subject and the rest of the subjects." ></td>
	<td class="line x" title="70:130	The following interpretations look for such correspondences." ></td>
	<td class="line x" title="71:130	For 17 out of the 100 stimulus words the predicted response is equal to the observed primary response." ></td>
	<td class="line x" title="72:130	Tiffs compares to an average of 37 primary responses given by a subject in the Russell & Jenkins experiment." ></td>
	<td class="line x" title="73:130	A slightly better result is obtained for the correspondence between the predicted and the observed associations when it is considered, how many 87 StimPredicted ri,j Observed No." ></td>
	<td class="line x" title="74:130	ulus Responses Responses Subj." ></td>
	<td class="line x" title="75:130	l~lue green 2.144 sky 175 red 1.128 red 160 yellow 1.000 green 125 white 0.732 color 66 flowers 0.614 yellow 56 sky 0.600 black 49 colors 0.538 white 44 eyes 0.471 water 36 bright 0.457 grey 28 color 0.413 boy 20 butter bread 0.886 bread 637 milk 0.256 yellow 81 eggs 0.197 soft 30 Ib 0.179 fat 24 sugar 0.157 food 22 fat 0.147 knife 20 peanut 0.145 eggs 16 fats 0.138 cream 14 flavor 0.130 milk 13 wheat 0.128 cheese 9 baby mother 0.618 boy 162 foods 0.427 child 142 breast 0.353 cry 113 feeding 0.336 mother 71 infant 0.249 girl 51 birth 0.245 small 43 born 0.242 infant 27 milk 0.208 cute 21 her 0.206 little 18 nursing 0.202 blue 17 cold hot 1.173 hot 348 warm 1.164 snow 218 weather 0.736 warm 168 winter 0.603 winter 66 climate 0.474 ice 29 air 0.424 Minnesota 13 war 0.342 wet 13 wet 0.333 dark 10 water 0.330 sick 9 dry 0.315 heat 8 Table I: Comparison between the ten strongest predicted and the ten most frequent observed responses for four stimulus words, rij was computed according to formula 6." ></td>
	<td class="line x" title="76:130	subjects had given the predicted response: Averaged over all stimulus words and all subjects, a predicted response was given by 12.6% of the subjects." ></td>
	<td class="line x" title="77:130	By comparison, an associative response of an arbitrary subject was given by 21.9% of the remaining subjects." ></td>
	<td class="line x" title="78:130	When only those 27 stimulus words are considered, whose primary response was given by at least 500 subjects, an arbitrary response was given by 45.5% of the subjects on average." ></td>
	<td class="line x" title="79:130	By comparison, the predicted response to one of these 27 stimulus words was given by 32.6% of the subjects." ></td>
	<td class="line x" title="80:130	This means, that for stimulus words where the variation among subjects is small, the predictions improve." ></td>
	<td class="line x" title="81:130	On the other hand, 35 of the predicted responses were given by no subject at all, whereas an average subject gives only 5.9 out of 100 responses that are given by no other subject." ></td>
	<td class="line x" title="82:130	In about half of the cases we attribute this poor performance to the lack of representativity of the corpus." ></td>
	<td class="line x" title="83:130	For example, the predictions combustion to the stimulus bed or brokerage to house can be explained by specific verbal usage in the DOE scientific abstracts respectively in the Wall Street Journal." ></td>
	<td class="line x" title="84:130	In most other cases instead of paradigmatic associations (words that are used in similar contexts) syntagmatic associations (words that are often used together) are predicted." ></td>
	<td class="line x" title="85:130	Examples are the prediction of term to the stimulus long, where most subjects answered with short, or the prediction of folk to music, where most subjects responded with song." ></td>
	<td class="line x" title="86:130	88 stim afraid anger baby bath beautiful bed Bible bitter black blossom blue boy bread butter butterfly cabbage carpet chair cheese child citizen city cold comfort command cottage dark deep doctor dream eagle earth eating foot fruit girl green hammer hand hard head health heavy high house hungry joy justice king 'lamp freq 692 615 1157 244 812 1295 593 541 4250 50 1676 1174 863 426 68 116 138 577 566 8897 525 8125 2003 386 799 137 1695 2418 766 629 92 1429 2823 1169 1841 1096 1686 173 5146 3502 5350 11433 3497 25220 3059 268 246 1314 1983 330 par f (par) fear mad boy clean ugly sleep God sweet white flower sky girl butter bread moth head rug table crackers baby(ies) U.S.(A)." ></td>
	<td class="line x" title="87:130	town hot chair order house light shallow nurse sleep bird round food shoe(s) apple boy grass nail(s) foot(ee) soft hair sickness 'light low home food happy peace queen light 261 351 162 314 209 584 236 652 751 672 175 768 610 637 144 165 460 493 108 159 114 353 348 117 196 298 829 318 238 453 550 130 390 232 378 704 262 537 255 674 129 250 583 675 247 362 209 250 751 633 pred am expression mother hot love combustion Society sweet white flower green girl wheat bread fish potatoes red clock milk care senior pop hot ease army cheese brown sea patient sleep bird rare habits square vegetable boy blue string On hit tail mental ion low brokerage eat fear criminal emperor light f (pred), 0 0 71 10 0 0 0 652 751 672 125 768 4 637 0 0 27 0 47 10 0 0 348 76 102 111 1 77 11 453 550 0 0 0 114 704 122 0 0 1 17 0 0 675 0 174 5 1 1 633 Table 2, part 1." ></td>
	<td class="line x" title="88:130	Observed and predicted associative responses to stimulus words 1 to 50." ></td>
	<td class="line x" title="89:130	The abbreviations in the headline mean: stim = stimulus word; freq = corpus frequency of stimulus word; par = primary associative response; f (pax) = number of subjects who gave the primary associative response; pred -predicted associative response; f (pred) = number of subjects who gave the predicted assocdative response." ></td>
	<td class="line x" title="90:130	89 stim light lion long loud man memory moon mountain music mutton needle ocean priest quiet red religion river rough salt scissors sheep short sickness sleep slow smooth soft soldier sour spider square stem stomach stove street sweet swift table thief thirsty tobacco trouble whiskey whistle white window wish woman working yellow freq 7538 182 16437 230 7472 3230 295 1066 3635 39 208 1066 311 673 3029 1224 1624 457 2158 25 854 7388 207 1843 1858 690 1681 321 154 97 1430 796 501 98 859 7OO 184 2396 63 32 1056 1108 63 77 4807 816 2061 2995 5366 1188 par dark tiger short soft woman(e) mind stars hill(s) song(s) lamb thread water church loud white church water smooth pepper cut wool tall health bed fast rough hard army sweet web round flower food hot avenue sour fast chair steal water smoke bad drink(s) stop black door want man(e) hard blue f (par) 647 261 758 541 767 119 205 266 183 365 464 314 328 348 221 285 246 439 430 671 201 397 376 238 752 328 445 187 568 454 372 402 211 235 190 434 369 84O 286 348 515 89 284 131 617 191 124 646 132 156 pred dark sea term noise woman deficits sun ranges folk beef sharing flOOr Catholic sleep yellow Christianity flows smooth sugar pa~r cattle term motion hrs wave muscle drink army sweet tail root brain cancer kitchen corner potatoes rivers honour catch drink textiles rein beer train black glass I yr class green f(pred) 647 2 0 210 767 0 168 0 0 32 0 6 189 53 19 5 0 439 83 1 15 0 0 0 0 1 10 187 568 0 22 2 1 16 20 0 0 0 2 296 0 0 52 89 617 171 2 0 3 89 MEAN: 2064.78 377.52 I 127.34 Table 2, part 2." ></td>
	<td class="line x" title="91:130	Observed and predicted associative responses to stimulus words 51 to 100." ></td>
	<td class="line x" title="92:130	90 Using the corpora listed in section 4, the same simulation as described above was conducted for German." ></td>
	<td class="line x" title="93:130	For the computation of the associative strengths, again formula 6 was used." ></td>
	<td class="line x" title="94:130	For optimal results, only a small adjustment had to be made to parameter alpha (from 0.66 to 0.68)." ></td>
	<td class="line x" title="95:130	However, a significant change was necessary for parameters/~ and 7, which again for ease of parameter optimization were assumed to be identical." ></td>
	<td class="line x" title="96:130	~ and 7 had to be reduced by a factor of approximately four from a value of 0.00002 to a value of 0.000005." ></td>
	<td class="line x" title="97:130	Apart from these parameters, nothing was changed in the algorithm." ></td>
	<td class="line x" title="98:130	Table 3 compares the quantitative results as given above for both languages." ></td>
	<td class="line x" title="99:130	The figures can be interpreted as follows: With an average of 21.9% of the other subjects giving the same response as an arbitrary subject, the variation among subjects is much smaller in English than it is in German (8.7%)." ></td>
	<td class="line x" title="100:130	This is reflected in the simulation results, where both figures (12.6% and 6.9%) have a similar ratio, however at a lower level." ></td>
	<td class="line x" title="101:130	This observation is confirmed when only stimuli with low variation of the associative responses are considered." ></td>
	<td class="line x" title="102:130	In both languages, the decrease in variation is in about the same order of magnitude for experiment and simulation." ></td>
	<td class="line x" title="103:130	Overall, the simulation results are somewhat better for German than they are for English." ></td>
	<td class="line x" title="104:130	This may be surprising, since with a total of 33 million words the English corpus is larger than the German with 21 million words." ></td>
	<td class="line x" title="105:130	However, if one has a closer look at the texts, it becomes clear, that the German corpus, by incorporating popular newspapers and spoken language, is clearly more representative to everyday language." ></td>
	<td class="line x" title="106:130	Description percentage of subjects who give the predicted associative response percentage of other subjects who give the response of an arbitrary subject percentage of subjects who give the predicted associative response for stimuli with little response variation' percentage of other subjects who give the response of an arbitrary subject for stimuli with little response variation* percentage of cases where the predicted response is identical to the observed primary response percentage of cases where the response of an arbitrary subject is identical to the observed primary response percentage of cases where the predicted response is given by no subject * percentage of cases where the response of an arbitrary subject is given by no other subject** English German 12.6% 6'.9% 21.9% 8.7% 32.6% 15.6% i 45.5% 18a% 17.0% ' 19.0% 37.5~ 22.5% 35.0% 57.0% '5.9% 19.8% Table 3: Comparison of results between simulation and experiment for English and German." ></td>
	<td class="line x" title="107:130	Notes: ') little response variation is defined slightly different for English and German: in the English study, only thoee 27 stimulus words are considered, whose primary response is given by at least 500 out of 1008 subjects." ></td>
	<td class="line x" title="108:130	In the German study, only those 25 stimulus words are taken into account, wh~e primary response is given by at least 100 out of 331 subjects." ></td>
	<td class="line x" title="109:130	**) for comparison of English and German experimental figures, it should be kept in mind, that the American experiment was conducted with 1008, but the German experiment with only 331 subjects." ></td>
	<td class="line x" title="110:130	93." ></td>
	<td class="line x" title="111:130	7 Discussion and conclusion In the simulation results a bias towards syntagmatic associations was found." ></td>
	<td class="line x" title="112:130	Since the associations were computed from co-occurences of words in texts, this preference of syntagmatic associations is not surprising." ></td>
	<td class="line x" title="113:130	It is remarkable, instead, that many associations usually considered to be paradigmatic are predicted correctly." ></td>
	<td class="line x" title="114:130	Examples include man -woman, black ~ white and bitter ~ sweet." ></td>
	<td class="line x" title="115:130	We believe, however, that the tendency to prefer syntagmatic associations can be reduced by not counting co-occurences found within collocations." ></td>
	<td class="line x" title="116:130	Equivalently, the association strength between word pairs always occuring together in a strict formation (separated by a constant number of other words) could be reduced." ></td>
	<td class="line x" title="117:130	When going from English to German, the parameters /~ and '7 in equation 6 needed to be readjusted in such a way, that less frequent words obtained a better chance to be associated." ></td>
	<td class="line x" title="118:130	This reflects the fact, that there is more variation in the associative responses of German than of American subjects, and that American subjects tend to respond with words of higher corpus frequency." ></td>
	<td class="line x" title="119:130	We believe that by considering additional languages this parameter adjustment could be predicted from word-frequency-distribution." ></td>
	<td class="line x" title="120:130	In conclusion, the results show, that free word associations for English and German can be successfully predicted by an almost identical algorithm which is based on the co-occurencefrequencies of words in texts." ></td>
	<td class="line x" title="121:130	Some peculiarities in the associative behavior of the subjects were confirmed in the simulation." ></td>
	<td class="line x" title="122:130	Together, this is a good indication that the learning of word associations is governed by the law of association by contiguity." ></td>
	<td class="line x" title="123:130	Although our simulation results are not perfect, specialized versions of our program have already proved useful in a number of applications:  Information Retrieval: Generation of search terms for document retrieval in bibliographic databases (Wettler & Rapp, 1989, Ferber, Wettler & Rapp, 1993)." ></td>
	<td class="line x" title="124:130	 Marketing: Association norms are useful to predict what effects word usage in advertisements has on people (Wettler & Rapp, 1993)." ></td>
	<td class="line x" title="125:130	Muitilingual assocation norms help to find a global marketing strategy in international markets (Kroeber-Riel, 1992)." ></td>
	<td class="line x" title="126:130	 Machine Translation: In an experimental prototype we have shown that associations derived from context are useful to find the correct translations for semantically ambiguous words." ></td>
	<td class="line x" title="127:130	The successful prediction of different types of verbal behavior on the basis of co-occurrences of words in texts is a direct application of the classical contiguity-theory, or, in more modern neurophysiological terms, of Hebb's learning rule." ></td>
	<td class="line x" title="128:130	Cognitive psychology has severely criticized contiguity-theory with the arguments that association theory did not produce useful results (Jenkins, 1974), and that associations are not the result of associative learning but of underlying semantic processes (Clark, 1970)." ></td>
	<td class="line x" title="129:130	Both arguments need a critical revision." ></td>
	<td class="line x" title="130:130	Recent work with large corpora as well as a large number of connectionist studies have yielded very useful results in different psychological domains, and the high predictive power of the associationist approach makes that the intuitive appeal of cognitivist explanations is fading rapidly." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A94-1006
Termight: Identifying And Translating Technical Terminology
Dagan, Ido;Church, Kenneth Ward;"></td>
	<td class="line x" title="1:178	Termight: Identifying and Translating Technical Terminology Ido Dagan* Ken Church AT&T Bell Laboratories 600 Mountain Ave. Murray Hill, NJ 07974, USA dagan~bimacs, cs." ></td>
	<td class="line x" title="2:178	biu." ></td>
	<td class="line x" title="3:178	ac." ></td>
	<td class="line x" title="4:178	il kwc@research, att." ></td>
	<td class="line x" title="5:178	com Abstract We propose a semi-automatic tool, termight, that helps professional translators and terminologists identify technical terms and their translations." ></td>
	<td class="line x" title="6:178	The tool makes use of part-of-speech tagging and wordalignment programs to extract candidate terms and their translations." ></td>
	<td class="line x" title="7:178	Although the extraction programs are far from perfect, it isn't too hard for the user to filter out the wheat from the chaff." ></td>
	<td class="line x" title="8:178	The extraction algorithms emphasize completeness." ></td>
	<td class="line x" title="9:178	Alternative proposals are likely to miss important but infrequent terms/translations." ></td>
	<td class="line x" title="10:178	To reduce the burden on the user during the filtering phase, candidates are presented in a convenient order, along with some useful concordance evidence, in an interface that is designed to minimize keystrokes." ></td>
	<td class="line x" title="11:178	Termight is currently being used by the translators at AT T Business Translation Services (formerly AT&T Language Line Services)." ></td>
	<td class="line x" title="12:178	1 Terminology: An Application for Natural Language Technology The statistical corpus-based renaissance in computational linguistics has produced a number of interesting technologies, including part-of-speech tagging and bilingual word alignment." ></td>
	<td class="line x" title="13:178	Unfortunately, these technologies are still not as widely deployed in practical applications as they might be." ></td>
	<td class="line x" title="14:178	Part-ofspeech taggers are used in a few applications, such as speech synthesis (Sproat et al. , 1992) and question answering (Kupiec, 1993b)." ></td>
	<td class="line x" title="15:178	Word alignment is newer, found only in a few places (Gale and Church, 1991a; Brown et al. , 1993; Dagan et al. , 1993)." ></td>
	<td class="line x" title="16:178	It is used at IBM for estimating parameters of their statistical machine translation prototype (Brown et *Author's current address: Dept. of Mathematics and Computer Science, Bar Ilan University, Ramat Gan 52900, Israel." ></td>
	<td class="line x" title="17:178	al., 1993)." ></td>
	<td class="line x" title="18:178	We suggest that part of speech tagging and word alignment could have an important role in glossary construction for translation." ></td>
	<td class="line x" title="19:178	Glossaries are extremely important for translation." ></td>
	<td class="line x" title="20:178	How would Microsoft, or some other software vendor, want the term 'Character menu' to be translated in their manuals?" ></td>
	<td class="line x" title="21:178	Technical terms are difficult for translators because they are generally not as familiar with the subject domain as either the author of the source text or the reader of the target text." ></td>
	<td class="line x" title="22:178	In many cases, there may be a number of acceptable translations, but it is important for the sake of consistency to standardize on a single one." ></td>
	<td class="line x" title="23:178	It would be unacceptable for a manual to use a variety of synonyms for a particular menu or button." ></td>
	<td class="line x" title="24:178	Customarily, translation houses make extensive job-specific glossaries to ensure consistency and correctness of technical terminology for large jobs." ></td>
	<td class="line x" title="25:178	A glossary is a list of terms and their translations." ></td>
	<td class="line x" title="26:178	1 We will subdivide the task of constructing a glossary into two subtasks: (1) generating a list of terms, and (2) finding the translation equivalents." ></td>
	<td class="line x" title="27:178	The first task will be referred to as the monolingual task and the second as the bilingual task." ></td>
	<td class="line x" title="28:178	How should a glossary be constructed?" ></td>
	<td class="line x" title="29:178	Translation schools teach their students to read as much background material as possible in both the source and target languages, an extremely time-consuming process, as the introduction to Hann's (1992, p. 8) text on technical translation indicates: Contrary to popular opinion, the job of a technical translator has little in common with other linguistic professions, such as literature translation, foreign correspondence or interpreting." ></td>
	<td class="line x" title="30:178	Apart from an expert knowledge of both languages, all that is required for the latter professions is a few general dictionaries, whereas a technical translator needs a whole library of specialized dictionaries, encyclopedias and 1The source and target fields are standard, though many other fields can also be found, e.g., usage notes, part of speech constraints, comments, etc. 34 technical literature in both languages; he is more concerned with the exact meanings of terms than with stylistic considerations and his profession requires certain 'detective' skills as well as linguistic and literary ones." ></td>
	<td class="line x" title="31:178	Beginners in this profession have an especially hard time This book attempts to meet this requirement." ></td>
	<td class="line x" title="32:178	Unfortunately, the academic prescriptions are often too expensive for commercial practice." ></td>
	<td class="line x" title="33:178	Translators need just-in-time glossaries." ></td>
	<td class="line x" title="34:178	They cannot afford to do a lot of background reading and 'detective' work when they are being paid by the word." ></td>
	<td class="line x" title="35:178	They need something more practical." ></td>
	<td class="line x" title="36:178	We propose a tool, termight, that automates some of the more tedious and laborious aspects of terminology research." ></td>
	<td class="line x" title="37:178	The tool relies on part-of-speech tagging and word-alignment technologies to extract candidate terms and translations." ></td>
	<td class="line x" title="38:178	It then sorts the extracted candidates and presents them to the user along with reference concordance lines, supporting efficient construction of glossaries." ></td>
	<td class="line x" title="39:178	The tool is currently being used by the translators at AT&T Business Translation Services (formerly AT~T Language Line Services)." ></td>
	<td class="line x" title="40:178	Termight may prove useful in contexts other than human-based translation." ></td>
	<td class="line x" title="41:178	Primarily, it can support customization of machine translation (MT) lexicons to a new domain." ></td>
	<td class="line x" title="42:178	In fact, the arguments for constructing a job-specific glossary for human-based translation may hold equally well for an MT-based process, emphasizing the need for a productivity tool." ></td>
	<td class="line x" title="43:178	The monolingual component of termigM can be used to construct terminology lists in other applications, such as technical writing, book indexing, hypertext linking, natural language interfaces, text categorization and indexing in digital libraries and information retrieval (Salton, 1988; Cherry, 1990; Harding, 1982; Bourigault, 1992; Damerau, 1993), while the bilingual component can be useful for information retrieval in multilingual text collections (Landauer and Littman, 1990)." ></td>
	<td class="line x" title="44:178	2 Monolingual Task: An Application for Part-of-Speech Tagging Although part-of-speech taggers have been around for a while, there are relatively few practical applications of this technology." ></td>
	<td class="line x" title="45:178	The monolingual task appears to be an excellent candidate." ></td>
	<td class="line x" title="46:178	As has been noticed elsewhere (Bourigault, 1992; Justeson and Katz, 1993), most technical terms can be found by looking for multiword noun phrases that satisfy a rather restricted set of syntactic patterns." ></td>
	<td class="line x" title="47:178	We follow Justeson and Katz (1993) who emphasize the importance of term frequency in selecting good candidate terms." ></td>
	<td class="line x" title="48:178	An expert terminologist can then skim the list of candidates to weed out spurious candidates and cliches." ></td>
	<td class="line x" title="49:178	Very simple procedures of this kind have been remarkably successful." ></td>
	<td class="line x" title="50:178	They can save an enormous amount of time over the current practice of reading the document to be translated, focusing on tables, figures, index, table of contents and so on, and writing down terms that happen to catch the translator's eye." ></td>
	<td class="line x" title="51:178	This current practice is very laborious and runs the risk of missing many important terms." ></td>
	<td class="line x" title="52:178	Termight uses a part of speech tagger (Church, 1988) to identify a list of candidate terms which is then filtered by a manual pass." ></td>
	<td class="line x" title="53:178	We have found, however, that the manual pass dominates the cost of the monolingual task, and consequently, we have tried to design an interactive user interface (see Figure 1) that minimizes the burden on the expert terminologist." ></td>
	<td class="line x" title="54:178	The terminologist is presented with a list of candidate terms, and corrects the list with a minimum number of key strokes." ></td>
	<td class="line x" title="55:178	The interface is designed to make it easy for the expert to pull up evidence from relevant concordance lines to help identify incorrect candidates as well as terms that are missing from the list." ></td>
	<td class="line x" title="56:178	A single key-press copies the current candidate term, or the content of any marked emacs region, into the upper-left screen." ></td>
	<td class="line x" title="57:178	The candidates are sorted so that the better ones are found near the top of the list, and so that related candidates appear near one another." ></td>
	<td class="line x" title="58:178	2.1 Candidate terms and associated concordance lines Candidate terms." ></td>
	<td class="line x" title="59:178	The list of candidate terms contains both multi-word noun phrases and single words." ></td>
	<td class="line x" title="60:178	The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech (Church, 1988)." ></td>
	<td class="line x" title="61:178	The set of syntactic patterns is considered as a parameter and can be adopted to a specific domain by the user." ></td>
	<td class="line x" title="62:178	Currently our patterns match only sequences of nouns, which seem to yield the best hit rate in our environment." ></td>
	<td class="line x" title="63:178	Single-word candidates are defined by taking the list of all words that occur in the document and do not appear in a standard stop-list of 'noise' words." ></td>
	<td class="line x" title="64:178	Grouping and sorting of terms." ></td>
	<td class="line x" title="65:178	The list of candidate terms is sorted to group together all noun phrase terms that have the same head word (as in Figure 1), which is simply the last word of the term for our current set of noun phrase patterns." ></td>
	<td class="line x" title="66:178	The order of the groups in the list is determined by decreasing frequency of the head word in the document, which usually correlates with the likelihood that this head word is used in technical terms." ></td>
	<td class="line x" title="67:178	Sorting within groups." ></td>
	<td class="line x" title="68:178	Under each head word the terms are sorted alphabetically according to reversed order of the words." ></td>
	<td class="line x" title="69:178	Sorting in this order reflects the order of modification in simple English noun phrases and groups together terms that denote different modifications of a more general term (see 35 File Edit Buffers Help software settings settings font slze de@ault paper size paper size anchor point decimal point end point hgphenatlon point insertion point software settings specific settings settings change slze ~cnt size default paper size paper size umn size size anchor point decimal point end point hgphenatlon point lllssrtlon point ~neertlon point point 5sttlng Advanced Options Ineertlon point 117709: ourtd in gout Write document." ></td>
	<td class="line x" title="70:178	To move the_ insertion I17894: Choose OK. The dialog box dleappears, and the Ineertlon 66058: Chepter 2 Application Baslce__2." ></td>
	<td class="line x" title="71:178	Place the Ineertlon 122717: Inden\[atlon of a paragraph:_ I. Place the insertion 122811: aragreph bw ualng the Ruler:_ I. Place the insertion 122909: _ To create a hanslng Indent:_l." ></td>
	<td class="line x" title="72:178	Place the insertion i17546: Intlng Inelde the window and_ where the Inaertlon 36195: nd date to ang Hotep6d document:_ Move the insertion 35870: search for text in flotepad:_ 1." ></td>
	<td class="line x" title="73:178	Move the insertion 46478: destination_ document are vlslble._3." ></td>
	<td class="line x" title="74:178	Move the insertion 32436: into which gou want to insert the text Move the insertion 67442: ext llne." ></td>
	<td class="line x" title="75:178	Press the 5tACEBAR to_ move the insertion 44476: card._ If gou are using Write." ></td>
	<td class="line x" title="76:178	move the insertion 67667: first character gou want to select and drag the insertion 35932: tch Case check box_ 3." ></td>
	<td class="line x" title="77:178	To search ?lom the insertion 35957: default setting is Oown." ></td>
	<td class="line x" title="78:178	This searches from the 12092g: insert a manual page break:_ Position the ~olnt." ></td>
	<td class="line x" title="79:178	point to a location in the docume ~olnt moves to the_ selected pa ~olnt at the place you want the Informer ~olnt inside the paragraph you 'want to ~olnt inside the paragraph @ou want to c ~olnt Inelde the paragraph in 'which gou oolnt will move to if gou click a locatl oolnt to the place gou want the time and 0olnt to the place gou want to start the 0olnt to the place gou want the package 0olnt_ to the place gou want the text oolnt one space to the right._ To oolnt to the place gou want the object_ 3olnt to the last_ character gou oolnt to the besinrtln S of the fde." ></td>
	<td class="line x" title="80:178	sele insertion-point to the end of the file,_ insertion point where you want the page break and Figure 1: The monolingual user interface consists of three screens: (1) the input list of candidate terms (upper right), (2) the output list of terms, as constructed by the user (upper left), and (3) the concordance lines associated with the current term, as indicated by the cursor position in screen 1." ></td>
	<td class="line x" title="81:178	Typos are due to OCR errors." ></td>
	<td class="line x" title="82:178	Underscores denote line breaks." ></td>
	<td class="line x" title="83:178	for example the terms default paper size, paper size and size in Figure 1)." ></td>
	<td class="line x" title="84:178	Concordance lines." ></td>
	<td class="line x" title="85:178	To decide whether a candidate term is indeed a term, and to identify multiword terms that are missing from the candidate list, one must view relevant lines of the document." ></td>
	<td class="line x" title="86:178	For this purpose we present a concordance line for each occurrence of a term (a text line centered around the term)." ></td>
	<td class="line x" title="87:178	If, however, a term, tl, (like 'point') is contained in a longer term, $2, (like 'insertion point' or 'decimal point') then occurrences of t2 are not displayed for tl." ></td>
	<td class="line x" title="88:178	This way, the occurrences of a general term (or a head word) are classified into disjoint sets corresponding to more specific terms, leaving only unclassified occurrences under the general term." ></td>
	<td class="line x" title="89:178	In the case of 'point', for example, five specific terms are identified that account for 61 occurrences of 'point', and accordingly, for 61 concordance lines." ></td>
	<td class="line x" title="90:178	Only 20 concordance lines are displayed for the word 'point' itself, and it is easy to identify in them 5 occurrences of the term 'starting point', which is missing from the candidate list (because 'starting' is tagged as a verb)." ></td>
	<td class="line x" title="91:178	To facilitate scanning, concordance lines are sorted so that all occurrences of identical preceding contexts of the head word, like 'starting', are grouped together." ></td>
	<td class="line x" title="92:178	Since all the words of the document, except for stop list words, appear in the candidate list as single-word terms it is guaranteed that every term that was missed by the automatic procedure will appear in the concordance lines." ></td>
	<td class="line x" title="93:178	In summary, our algorithm performs the following steps:  Extract multi-word and single-word candidate terms." ></td>
	<td class="line x" title="94:178	 Group terms by head word and sort groups by head-word frequency." ></td>
	<td class="line x" title="95:178	 Sort terms within each group alphabetically in reverse word order." ></td>
	<td class="line x" title="96:178	 Associate concordance lines with each term." ></td>
	<td class="line x" title="97:178	An occurrence of a multi-word term t is not associated with any other term whose words are found in t.  Sort concordance lines of a term alphabetically according to preceding context." ></td>
	<td class="line x" title="98:178	2.2 Evaluation Using the monolingual component, a terminologist at AT&T Business Translation Services constructs terminology lists at the impressive rate of 150-200 terms per hour." ></td>
	<td class="line x" title="99:178	For example, it took about 10 hours to construct a list of 1700 terms extracted from a 300,000 word document." ></td>
	<td class="line x" title="100:178	The tool has at least doubled the rate of constructing terminology lists, which was previously performed by simpler lexicographic tools." ></td>
	<td class="line x" title="101:178	36 2.3 Comparison with related work Alternative proposals are likely to miss important but infrequent terms/translations such as 'Format Disk dialog box' and 'Label Disk dialog box' which occur just once." ></td>
	<td class="line oc" title="102:178	In particular, mutual information (Church and Hanks, 1990; Wu and Su, 1993) and other statistical methods such as (Smadja, 1993) and frequency-based methods such as (Justeson and Katz, 1993) exclude infrequent phrases because they tend to introduce too much noise." ></td>
	<td class="line x" title="103:178	We have found that frequent head words are likely to generate a number of terms, and are therefore more important for the glossary (a 'productivity' criterion)." ></td>
	<td class="line x" title="104:178	Consider the frequent head word box." ></td>
	<td class="line x" title="105:178	In the Microsoft Windows manual, for example, almost any type of box is a technical term." ></td>
	<td class="line x" title="106:178	By sorting on the frequency of the headword, we have been able to find many infrequent terms, and have not had too much of a problem with noise (at least for common headwords)." ></td>
	<td class="line x" title="107:178	Another characteristic of previous work is that each candidate term is scored independently of other terms." ></td>
	<td class="line x" title="108:178	We score a group of related terms rather than each term at a time." ></td>
	<td class="line x" title="109:178	Future work may enhance our simple head-word frequency score and may take into account additional relationships between terms, including common words in modifying positions." ></td>
	<td class="line x" title="110:178	Termight uses a part-of-speech tagger to identify candidate noun phrases." ></td>
	<td class="line x" title="111:178	Justeson and Katz (1993) only consult a lexicon and consider all the possible parts of speech of a word." ></td>
	<td class="line x" title="112:178	In particular, every word that can be a noun according to the lexicon is considered as a noun in each of its occurrences." ></td>
	<td class="line x" title="113:178	Their method thus yields some incorrect noun phrases that will not be proposed by a tagger, but on the other hand does not miss noun phrases that may be missed due to tagging errors." ></td>
	<td class="line x" title="114:178	3 Bilingual Task: An Application for Word Alignment 3.1 Sentence and word alignment Bilingual alignment methods (Warwick et al. , 1990; Brown et al. , 1991a; Brown et al. , 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al. , 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al. , 1993; Dagan et al. , 1993)." ></td>
	<td class="line x" title="115:178	have been used in statistical machine translation (Brown et al. , 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al. , 1991b; Gale et al. , 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990)." ></td>
	<td class="line x" title="116:178	Most alignment work was concerned with alignment at the sentence level." ></td>
	<td class="line x" title="117:178	Algorithms for the more difficult task of word alignment were proposed in (Gale and Church, 1991a; Brown et al. , 1993; Dagan et al. , 1993) and were applied for parameter estimation in the IBM statistical machine translation system (Brown et al. , 1993)." ></td>
	<td class="line x" title="118:178	Previously translated texts provide a major source of information about technical terms." ></td>
	<td class="line x" title="119:178	As Isabelle (1992) argues, 'Existing translations contain more solutions to more translation problems than any other existing resource'." ></td>
	<td class="line x" title="120:178	Even if other resources, such as general technical dictionaries, are available it is important to verify the translation of terms in previously translated documents of the same customer (or domain) to ensure consistency across documents." ></td>
	<td class="line x" title="121:178	Several translation workstations provide sentence alignment and allow the user to search interactively for term translations in aligned archives (e.g.(Ogden and Gonzales, 1993))." ></td>
	<td class="line x" title="123:178	Some methods use sentence alignment and additional statistics to find candidate translations of terms (Smadja, 1992; van der Eijk, 1993)." ></td>
	<td class="line x" title="124:178	We suggest that word level alignment is better suitable for term translation." ></td>
	<td class="line x" title="125:178	The bilingual component of termight gets as input a list of source terms and a bilingual corpus aligned at the word level." ></td>
	<td class="line x" title="126:178	We have been using the output of word_align, a robust alignment program that proved useful for bilingual concordancing of noisy texts (Dagan et al. , 1993)." ></td>
	<td class="line x" title="127:178	Word_align produces a partial mapping between the words of the two texts, skipping words that cannot be aligned at a given confidence level (see Figure 2)." ></td>
	<td class="line x" title="128:178	3.2 Candidate translations and associated concordance lines For each occurrence of a source term, termight identifies a candidate translation based on the alignment of its words." ></td>
	<td class="line x" title="129:178	The candidate translation is defined as the sequence of words between the first and last target positions that are aligned with any of the words of the source term." ></td>
	<td class="line x" title="130:178	In the example of Figure 2 the candidate translation of Optional Parameters box is zone Parametres optionnels, since zone and optionnels are the first and last French words that are aligned with the words of the English term." ></td>
	<td class="line x" title="131:178	Notice that in this case the candidate translation is correct even though the word Parameters is aligned incorrectly." ></td>
	<td class="line x" title="132:178	In other cases alignment errors may lead to an incorrect candidate translation for a specific occurrence of the term." ></td>
	<td class="line x" title="133:178	It is quite likely, however, that the correct translation, or at least a string that overlaps with it, will be identified in some occurrences of the term." ></td>
	<td class="line x" title="134:178	Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order." ></td>
	<td class="line x" title="135:178	The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3)." ></td>
	<td class="line x" title="136:178	The user views the concordances to verify correct candidates or to find translations that are 37 You can type application parameters in the Optional Parameters box." ></td>
	<td class="line x" title="137:178	Vous pouvez tapez les parametres d'une application dans la zone Parametres optionnels." ></td>
	<td class="line x" title="138:178	Figure 2: An example of word_align's output for the English and French versions of the Microsoft Windows manual." ></td>
	<td class="line x" title="139:178	The alignment of Parameters to optionnels is an error." ></td>
	<td class="line x" title="140:178	missing from the candidate list." ></td>
	<td class="line x" title="141:178	The latter task becomes especially easy when a candidate overlaps with the correct translation, directing the attention of the user to the concordance lines of this particular candidate, which are likely to be aligned correctly." ></td>
	<td class="line x" title="142:178	A single key-stroke copies a verified candidate translation, or a translation identified as a marked emacs region in a concordance line, into the appropriate place in the glossary." ></td>
	<td class="line x" title="143:178	3.3 Evaluation We evaluated the bilingual component of termight in translating a glossary of 192 terms found in the English and German versions of a technical manual." ></td>
	<td class="line x" title="144:178	The correct answer was often the first choice (40%) or the second choice (7%) in the candidate list." ></td>
	<td class="line x" title="145:178	For the remaining 53% of the terms, the correct answer was always somewhere in the concordances." ></td>
	<td class="line x" title="146:178	Using the interface, the glossary was translated at a rate of about 100 terms per hour." ></td>
	<td class="line x" title="147:178	3.4 Related work and issues for future research Smadja (1992) and van der Eijk (1993) describe term translation methods that use bilingual texts that were aligned at the sentence level." ></td>
	<td class="line x" title="148:178	Their methods find likely translations by computing statistics on term cooccurrence within aligned sentences and selecting source-target pairs with statistically significant associations." ></td>
	<td class="line x" title="149:178	We found that explicit word alignments enabled us to identify translations of infrequent terms that would not otherwise meet statistical significance criteria." ></td>
	<td class="line x" title="150:178	If the words of a term occur at least several times in the document (regardless of the term frequency) then word_align is likely to align them correctly and termight will identify the correct translation." ></td>
	<td class="line x" title="151:178	If only some of the words of a term are frequent then termight is likely to identify a translation that overlaps with the correct one, directing the user quickly to correctly aligned concordance lines." ></td>
	<td class="line x" title="152:178	Even if all the words of the term were not Migned by word_align it is still likely that most concordance lines are aligned correctly based on other words in the near context." ></td>
	<td class="line x" title="153:178	Termight motivates future improvements in word alignment quality that will increase recall and precision of the candidate list." ></td>
	<td class="line x" title="154:178	In particular, taking into account local syntactic structures and phrase boundaries will impose more restrictions on alignments of complete terms." ></td>
	<td class="line x" title="155:178	Finally, termight can be extended for verifying translation consistency at the proofreading (editing) step of a translation job, after the document has been translated." ></td>
	<td class="line x" title="156:178	For example, in an English-German document pair the tool identified the translation of the term Controls menu as Menu Steuerung in 4 out of 5 occurrences." ></td>
	<td class="line x" title="157:178	In the fifth occurrence word_align failed to align the term correctly because another translation, Steuermenu, was uniquely used, violating the consistency requirement." ></td>
	<td class="line x" title="158:178	Termight, or a similar tool, can thus be helpful in identifying inconsistent translations." ></td>
	<td class="line x" title="159:178	4 Conclusions We have shown that terminology research provides a good application for robust natural language technology, in particular for part-of-speech tagging and word-alignment algorithms." ></td>
	<td class="line x" title="160:178	Although the output of these algorithms is far from perfect, it is possible to extract from it useful information that is later corrected and augmented by a user." ></td>
	<td class="line x" title="161:178	Our extraction algorithms emphasize completeness, and identify also infrequent candidates that may not meet some of the statistical significance criteria proposed in the literature." ></td>
	<td class="line x" title="162:178	To make the entire process efficient, however, it is necessary to analyze the user's work process and provide interfaces that support it." ></td>
	<td class="line x" title="163:178	In many cases, improving the way information is presented to the user may have a larger effect on productivity than improvements in the underlying natural language technology." ></td>
	<td class="line x" title="164:178	In particular, we have found the following to be very effective:  Grouping linguistically related terms, making it easier to judge their validity." ></td>
	<td class="line x" title="165:178	 Sorting candidates such that the better ones are found near the top of the list." ></td>
	<td class="line x" title="166:178	With this sorting one's time is efficiently spent by simply going down the list as far as time limitations permit." ></td>
	<td class="line x" title="167:178	 Providing quick access to relevant concordance lines to help identify incorrect candidates as well as terms or translations that are missing from the candidate list." ></td>
	<td class="line x" title="168:178	38 Character menu menu Caracteres 4 2 Itallque 2 3 No translations available 1 menu Caracteres 4 121163: Formatting characters 135073: ~orme des caracteres The commands on the Character menu control how you ?ormat the Lee commandes du menu Caracteree permettent de determiner la pr 121294: 135188: : Chooee the stgle gou want to use ?rom the Character menu, Tgpe gout tcxt." ></td>
	<td class="line x" title="169:178	The CholsIssez le stgle voulu dans le menu Caracteres." ></td>
	<td class="line x" title="170:178	Tapez votre texte." ></td>
	<td class="line x" title="171:178	Le texte S: Card menu T: menu Fiche S: Character menu T: menu Caracteres S: Control menu T: S: Disk menu T: Figure 3: The bilingual user interface consists of two screens." ></td>
	<td class="line x" title="172:178	The lower screen contains the constructed glossary." ></td>
	<td class="line x" title="173:178	The upper screen presents the current term, candidate translations with their frequencies and a bilingual concordance for each candidate." ></td>
	<td class="line x" title="174:178	Typos are due to OCR errors." ></td>
	<td class="line x" title="175:178	 Minimizing the number of required key-strokes." ></td>
	<td class="line x" title="176:178	As the need for efficient knowledge acquisition tools becomes widely recognized, we hope that this experience with termight will be found useful for other text-related systems as well." ></td>
	<td class="line x" title="177:178	Acknowledgements We would like to thank Pat Callow from AT&T Buiseness Translation Services (formerly AT&T Language Line Services) for her indispensable role in designing and testing termight." ></td>
	<td class="line x" title="178:178	We would also like to thank Bala Satish and Jon Helfman for their part in the project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-1074
A 'Not-So-Shallow' Parser For Collocational Analysis
Basili, Roberto;Pazienza, Maria Teresa;Velardi, Paola;"></td>
	<td class="line x" title="1:270	A 'not-so-shallow' parser for collocational analysis Basili R.(*), M.T. Pazienza (*), P. Velardi () (*) Dipartimento Ingegneria Elettronica, Universit,~ di Roma,Tor Vergata \[rbas, pazienza}@tovvxl, ccd.utow:m, it: () Istituto di Informatica, Universitk di Ancona vela@anvax2, cinec~." ></td>
	<td class="line x" title="2:270	J_t Abstract." ></td>
	<td class="line x" title="3:270	Collocational analysis is the basis of many studies on lexical acquisition." ></td>
	<td class="line x" title="4:270	Collocations are extracted from corpora using more or less shallow processing techniques, that span from purely statistical methods to partial parsers." ></td>
	<td class="line x" title="5:270	Our point is that, despite one of tile objectives of collocational analysis is to acquire high-coverage lexical data at low human cost, this is often not the case." ></td>
	<td class="line x" title="6:270	Human work is in fact required for the initial training of most statistically based methods." ></td>
	<td class="line x" title="7:270	A more serious problem is that shallow processing techniques produce a noise that is not acceptable for a fully automated system." ></td>
	<td class="line x" title="8:270	We propose in this paper a not-so-shallow parsing strategy that reliably detects binary and ternary relations among words." ></td>
	<td class="line x" title="9:270	We show that adding more syntactic knowledge to the." ></td>
	<td class="line x" title="10:270	recipe significantly improves the recall and precision of tile detected collocations, regardless of any subsequent statistical computation, while still nleeting the cornputational requi,'ements of corpus parsers." ></td>
	<td class="line x" title="11:270	1." ></td>
	<td class="line x" title="12:270	Week methods for the analysis of collocations In the past few years there has been a flourishing of interest in the study of word collocations." ></td>
	<td class="line x" title="13:270	A common method to extract collocations is using windowing techniques for the extraction of word associations." ></td>
	<td class="line oc" title="14:270	In (Zernik 1990; Calzolari and Bindi 1990; Smadja 1989; Church and Hanks 1990) associations are detected in a 5 window." ></td>
	<td class="line x" title="15:270	A wider window ( tO0 words) is used in (Gale et al. 1992)." ></td>
	<td class="line x" title="16:270	Windowing techniques are also used in (Jelinek et al, 1990), where it is proposed a trigram model to automatically derive, and refine, context-free rules of the grammar (Fujisaki et al, 1991)." ></td>
	<td class="line x" title="17:270	Windowing techniques weekly model tile locality of language as well as other lexical information." ></td>
	<td class="line x" title="18:270	The reliability of the acquired information depends upon tile window size." ></td>
	<td class="line x" title="19:270	A small window fails to detect many important word relations, while enlarging tile window affects the tractability of tile statistical model (especially for markovian n-gram models)." ></td>
	<td class="line x" title="20:270	Finally, window-based collocations provide limited information when dealing with a variety of lexical phenomena." ></td>
	<td class="line x" title="21:270	For example, the simple observation of word cooccurrences is not a suitable marker of lexical subcategorization." ></td>
	<td class="line x" title="22:270	Another popular al;proach is usinl,r a partial parser, augmented with statistical parameters." ></td>
	<td class="line x" title="23:270	Tile reciprocal contribution of syntax and statistics has been outlined in (Zemik 119911) to have an important role for automatic lexicaI acquisition." ></td>
	<td class="line x" title="24:270	The syntactic relations are usually derived by preq)rocessing the target corpus with a part-of-speech tagger or with a simplified parser." ></td>
	<td class="line x" title="25:270	Syntactic markers are applied to elementary links among words or to more structurecl contexts." ></td>
	<td class="line x" title="26:270	The pa,'tial character of the different parsers described in literature makes it possible to process large corpora at a 'reasonable' computational effort." ></td>
	<td class="line x" title="27:270	Most syntax-based statistical approaches use deterministic parsing, derived from Marcus' work on PARSIF'AI." ></td>
	<td class="line x" title="28:270	parser (Marcus, 1980)." ></td>
	<td class="line x" title="29:270	I'ARS1FAL is a deterministic parser with lookahead cat)abilities, that enables partial analyses." ></td>
	<td class="line x" title="30:270	One of the PARSIFAL emanations, the Fidditch parser by I lindle, is used in (Flindle 1990) to detect subject-w~rb-object (SVO) triples." ></td>
	<td class="line x" title="31:270	SVO triples are allowed to be incomplete, i.e. the subject or the object can be missing." ></td>
	<td class="line x" title="32:270	Noisy data (i.e. words that are neither syntactically nor semantically,elated) are reduced by the use of statistical measures, sucl-t as the Itllltllal information (Church et al, 1991), as defined in information tlmory." ></td>
	<td class="line x" title="33:270	The Fidditch parser requires a lexicon including informatkm about base word fornls atld syntactic constraints (e.g,." ></td>
	<td class="line x" title="34:270	tile complement structure of verbs)." ></td>
	<td class="line x" title="35:270	Non-trivial preliminary work is tllus necessary in tuning the lexicon for the different domains and sublanguages." ></td>
	<td class="line x" title="36:270	A second problem with the Fidditch parser is poor performances: tilt_' recall and precision at detecting word collocations are declared to be as low as 50%, I-iowever it is unclear if this value applies only to SVO triples, and how it has been derived." ></td>
	<td class="line x" title="37:270	The recall is low because tile Fidditch parser, as other partial parsers (Sekine et al, 1992; Resnik and Hearst, i993), only detect links between adjacent or near-adjacent words." ></td>
	<td class="line x" title="38:270	Thougll a 50'/,, precision and recall might be 447 reasonable for human assisted tasks, like in lexicography, supervised translation, etc. , it is not 'fair enough' if collocational analysis must serve a fully automated system." ></td>
	<td class="line x" title="39:270	In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques." ></td>
	<td class="line x" title="40:270	Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991, 1993a; Hindle and Rooths 1991,1993; Sekine 1992) (Bogges et al. 1992), sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b, 1993b; Utsuro et al. 1993), lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c), etc. In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a 'large enough' number of words." ></td>
	<td class="line x" title="41:270	In addition, most papers produce a performance evaluation of their methods but do not provide a measure of the coverage, i.e. the percentage of cases for which their method actually provides a (right or wrong) solution." ></td>
	<td class="line x" title="42:270	It is quite common that results are discussed only for 10-20 cases." ></td>
	<td class="line x" title="43:270	In our previous papers, we used semantic tagging to further reduce the noise and gain evidence of recurrent phenomena even with small corpora." ></td>
	<td class="line x" title="44:270	However, no accurate or shallow method can resume valid information that has been lost in previous steps (i.e. in extracting collocations)." ></td>
	<td class="line x" title="45:270	We believe that a higher precision and recall of the input collocational data is desirable to ensure a good coverage to the whatever lexical learning algorithm." ></td>
	<td class="line x" title="46:270	In this paper we describe a not-so-shallow, multi-step, parsing strategy that allows it to detect long distance syntactic relations while keeping the temporal complexity compatible with the computational requirements of largescale parsers." ></td>
	<td class="line x" title="47:270	We demonstrate that a bit more syntax can be added to the recipe, with a significant improvement over existing partial parsers." ></td>
	<td class="line x" title="48:270	We do not discuss of any subsequent processing (statistically or/and knowledge based) that may be applied to further improve the quality of collocational data, since this is outside the scope of this presentation." ></td>
	<td class="line x" title="49:270	The interested reader may refer to our previous works on the matter." ></td>
	<td class="line x" title="50:270	2." ></td>
	<td class="line x" title="51:270	A 'not-so-shallow' parsing technique Our syntactic analyzer (hereafter SSA) extracts partial syntactic structures from corpora." ></td>
	<td class="line x" title="52:270	The analyzer, based on discontinuous grammar (Dahl,1989), is able to detect binary and ternary syntactic relations among words, that we call elementary slmtactic lil~k,~ (esl), The framework of discontinuous grammars has several advantages: it allows a simple notation, and exhibits portability among different logic programming styles." ></td>
	<td class="line x" title="53:270	The presence of skip rules makes it possible to detect long distance dependencies between co-occurring words." ></td>
	<td class="line x" title="54:270	This is particularly important in many texts, for the presence of long coordinate constructions, nested clauses, lists, parenthesised clauses." ></td>
	<td class="line x" title="55:270	The partial parsing strategy described hereafter requires in input few more than a morphologic lexicon (section 2.1)." ></td>
	<td class="line x" title="56:270	Post morphologic processing, as described in section 2.2, is not strictly required, though obviously it increases the reliability of the detected word relations." ></td>
	<td class="line x" title="57:270	The lexicon used is purely morphologic, unlike for the Fidditch parser, neither it requires training, like in n-gram based models." ></td>
	<td class="line x" title="58:270	This means that the shallow analyzer is portable by minimum changes over different domains." ></td>
	<td class="line x" title="59:270	This is not the case with the deterministic partial parsing used in similar works." ></td>
	<td class="line x" title="60:270	Furthermore the grammar rules are easy to tune to different linguistic subdomains." ></td>
	<td class="line x" title="61:270	The analyzer enables the detection of different types of syntactic links among words: noun-verb, verbnoun, noun-preposition-noun, etc. This information is richer than just SVO triples, in that phrase structures are partitioned in more granular units." ></td>
	<td class="line x" title="62:270	The parsing method has been implemented for different corpora, which exhibit very different linguistic styles: a corpus of commercial activities (CD), in telegraphic style, a legal domain (LD) on taxation norms and lows, and remote sensing (RSD) abstracts." ></td>
	<td class="line x" title="63:270	The latter is in English, while the former two are in Italian." ></td>
	<td class="line x" title="64:270	The English application is rather less developed (a smaller morphologic lexicon, no postmorphology, etc.), however it is useful here to demonstrate that the approach is language independent." ></td>
	<td class="line x" title="65:270	In this paper we use many examples from the RSD." ></td>
	<td class="line x" title="66:270	2.1 Morphology The morphologic analyzer (Marziali, 1992) derives from the work on a generative approach to the Italian morphology (Russo, 1987), first used in DANTE, a NLP system for analysis of short narrative texts in the financial domain (Antonacci et al. 1989)." ></td>
	<td class="line x" title="67:270	Tile analyzer includes over 7000 elementary lemmata (stems without affixes, e.g. flex is the elementary lemma for de448 flex, in-flex, re-fiex) anti has been experimented since now on economic, financial, commercial and legal domains." ></td>
	<td class="line x" title="68:270	Elementary lemmata cover much more than 70(}0 words, since many words have an affix." ></td>
	<td class="line x" title="69:270	An entry in the lexicon is as follows: lexicon(len~na, stem, ending_class, syntactic feature) where l emma iS the elementary lemma (e.g. ancora for ancor-aggio (anchor-age)), stem is the lemma without ending (ancor), ending_class iS one over about 60 types of inflections." ></td>
	<td class="line x" title="70:270	For example, ancora belongs to the class ec cosa, since it inflects like the word cosa (thinq,)." ></td>
	<td class="line x" title="71:270	The Italian morphologic lexicon and grammars are fully general." ></td>
	<td class="line x" title="72:270	This means that the analyzer has a tendency to overgenerate." ></td>
	<td class="line x" title="73:270	For example, the word agente (agent, in the sense of dealer), is interpreted as a i~.oun and as the present participle of the verb agire (to act), though this type of inflected form is never found in both Italian domains." ></td>
	<td class="line x" title="74:270	This problem is less evident in English, that is less inflected." ></td>
	<td class="line x" title="75:270	Overgeneration is a common problem with grammar based approaches to morphology, as opposed to part of speech (pos) taggers." ></td>
	<td class="line x" title="76:270	On the other side, pos taggers need manual work for corpus training every since a new domain is to be analyzed." ></td>
	<td class="line x" title="77:270	To quantitatively evaluate the phenomenon of overgeneration, we conskfered a test set of 25 sentences in the LD, including about 800 words." ></td>
	<td class="line x" title="78:270	Of these 800, there were 546 different nouns, adjectives anti verbs (i.e. potentially ambiguous words)." ></td>
	<td class="line x" title="79:270	The analyzer provided 631 interpretations of the 546 words." ></td>
	<td class="line x" title="80:270	There were 76 ambiguous words." ></td>
	<td class="line x" title="81:270	The overall estimated ambiguity is 76/546:0,139, while the overgeneration ratio is better evaluated by: O = \[631 (546-76)\]/76=161/76:2,11 2.2." ></td>
	<td class="line x" title="82:270	Post morphological processing The purpose of this module is to analyse compound expressions and numbers, such as compound verbs, dates, numeric expressions, and super!atives." ></td>
	<td class="line x" title="83:270	Ad-hoc context free grammar have been defined." ></td>
	<td class="line x" title="84:270	Post morphological processing includes also simple (but generally valid) heuristic rules to reduce certain types of ambiguity." ></td>
	<td class="line x" title="85:270	'Ihere are two group of such rules: (i) Rules to disambiguate ambiguous nounadjective (N/Agg) interpretations (e.g. acid) (ii) Rules to disambiguate ambiguous verb-noun (V/N) interpretations (e.g. study) One example of heuristics for N/Agg is: If N/Agg is neither preceded nor followed by a noun, or N/Agg, before a verb is reached, Then it is a noun." ></td>
	<td class="line x" title="86:270	Ex: and sulphuric ~ was detected Though examples are in English, post morphology has not been developed for the English language at the time we are w,'iting." ></td>
	<td class="line x" title="87:270	After post-morphologic analysis, the 546 nouns, verbs anti adjectives produced only 562 interpretations." ></td>
	<td class="line x" title="88:270	The new overgeneration ratio is then O':(562-(546-76))/76=92/76=1,2 The estimated efficacy of the postrnorphology, is 161/92=1,75, about 50%.'eduction of the initial ambiguity." ></td>
	<td class="line x" title="89:270	2.3." ></td>
	<td class="line x" title="90:270	The parser The SSA syntactic analysis is a rewriting procedure of a single sentence into a set of ~!_1 ~meme~!-y_~y~ i\]jg_jin!~ (esl)." ></td>
	<td class="line x" title="91:270	The SSA is based on a discontinuous grammar, described more formally in (Basili et al. 1992a)." ></td>
	<td class="line x" title="92:270	In tiffs section we provide a qualitative clescription of the rules by which esl's are generated." ></td>
	<td class="line x" title="93:270	Examples of esl's generated by the parser are: N_V (the subject-verb relation), V N (the direct object_verb relation), N P N (noun preposition noun), V P N (verb preposition noun), N_Adj (adjective noun), N N (conq)ound) etc. Overall, we identify over 20 different esl's." ></td>
	<td class="line x" title="94:270	There is a discontinuous grammar rule for each esl." ></td>
	<td class="line x" title="95:270	A description of a rule used to derive N P N links is in Figure 1." ></td>
	<td class="line x" title="96:270	This description applies by straightforward modifications to any other esl type (though some esl rules include a concordance test)." ></td>
	<td class="line x" title="97:270	As remfirked at the beginning of this section, skip rules are the key to extract long distance syntactic relations and to approximate the behaviour of a full parser." ></td>
	<td class="line x" title="98:270	The first predicate LOOK RIGItT of Figure 1 skips over the string X until it finds a preposition (prep(w2))." ></td>
	<td class="line x" title="99:270	The second LOOK_RIG\[ IT skips over Y until it finds a noun (noun(w3))." ></td>
	<td class="line x" title="100:270	Given an initial string NL_segment, BACKTRACK force the system to analyse all the possible solutions of the predicate LOOKRIGHT (i.e. one-step rigth skips) to derive all the N P N groups, headed by the first norm (i.e. wl)." ></td>
	<td class="line x" title="101:270	For example, given the string: low concentrations of acetone and ethyl alchool in acqueous solutions the following N_PN are generated: concentration of acetone, concentration of alchool, concentration in solution, acetone in 449 solution, alchooI in solution, all of which are syntactically correct." ></td>
	<td class="line x" title="102:270	SSA rule( NL segment, N_P_N) BEGIN P, EPIZd~T IFNL_segment is EMPTY 'IIIEN F2KrI'; ELSE BEGIN NL segment=(wl Rest)." ></td>
	<td class="line x" title="103:270	IF (noun(wl) ) THFM BEGIN LOOK_RIGIIT(X, w2, Rest, New_Rest); %Rest=(X w2 NewRest) IF (TEST_ON(X) AND prep(w2) ) 'IIIEN BEG I N LOOK RIGIIT( Y, w2, New_Rest, _); %New_Rest--(Y w3 _) IF ( TEST ON(Y) AND noun(w3) ) 'llIEN ASSERT(esl(N_P_N, wl, w2, w3)); BACKTRACK; END; BACKTRACK; END POPwl FROM Nb_segment; END END." ></td>
	<td class="line x" title="104:270	Figure 1: A description of an N P N rule An uncontrolled application of skip rules would however produce unacceptable noise." ></td>
	<td class="line x" title="105:270	The TEST_ON0 are ad hoc heuristic rules that avoid uncontrolled skips." ></td>
	<td class="line x" title="106:270	For example, TEST2.ON(X) in Figure 1 verifies that the string X does notinclude a verb." ></td>
	<td class="line x" title="107:270	Hence, in the sentence:  the atmospheric code contpared favourably with results  the N P_N(code,with,results) is ~ generated." ></td>
	<td class="line x" title="108:270	In general, there is one-two different heuristic rule for each esl rule." ></td>
	<td class="line x" title="109:270	Heuristic rules are designed to take efficient decisions by exploiting purely syntactic constraints." ></td>
	<td class="line x" title="110:270	Such constraints are simple and require a minimum computational effort (essentialy, unification among simple structures)." ></td>
	<td class="line x" title="111:270	In some case, a lower recall is tolerated to avoid overgeneration." ></td>
	<td class="line x" title="112:270	For example, the second TEST ON(Y) rule of Figure 1 verifies that no more than two prepositions are skipped in the string Y. This rule stems from the observation that words located more than three prepositions apart, are rarely semantically related, though a full syntactic parser would eventually detect a relation." ></td>
	<td class="line x" title="113:270	Hence, in the NL segment: 1% accuracy on the night side of the Earth with stars down to visual magnitude tree the triple (accuracy, to, tree) is la_(gt generated, though syntactically correct." ></td>
	<td class="line x" title="114:270	The derivation of esl's is enabled for non adjacent word by virtue of skip rules." ></td>
	<td class="line x" title="115:270	However, interesting information can be lost in presence of more complex phenomena as nested relative clauses or coordination of phrase structures." ></td>
	<td class="line x" title="116:270	To cope with these phenomena, a post syntactic processor has been developed to extract links stemming from coordination among previously detected links." ></td>
	<td class="line x" title="117:270	This processing significantly increases the set of collected esl, and the quality of the derived lexical information." ></td>
	<td class="line x" title="118:270	The contribution of this post syntactic processing device depends heavily on the structure of incoming sentences." ></td>
	<td class="line x" title="119:270	In this phase, simple unification .mechanisms are used, rather than heuristics." ></td>
	<td class="line x" title="120:270	3." ></td>
	<td class="line x" title="121:270	Performance evaluation Recall and Precision M,'my algorithms evaluate their recall and precision against a human reference performer." ></td>
	<td class="line x" title="122:270	This pose many problems, like finding a 'fair' test material, using a large number of judges to render the evaluation less subjective, and finally interpreting the results." ></td>
	<td class="line x" title="123:270	One example of the 450 latter problem is the following: in (Smadja 1993) the nature of a syntactic link between two associated words is detected a posteriori." ></td>
	<td class="line x" title="124:270	The performance of the system, called XTRACT, we evaluated by letting human judges compare their choice against that of the system." ></td>
	<td class="line x" title="125:270	The reported performances are about 80% precision, 90% recall." ></td>
	<td class="line x" title="126:270	One such evaluation experiment is, in our view, questionable, since both the human judges and XTRACT make a decision outside the context of a sentence." ></td>
	<td class="line x" title="127:270	The interpretation of the results then does not take into account how much XTRACT succeeds in identifying syntactic relations as they actually occurred in the test suite." ></td>
	<td class="line x" title="128:270	Another problem is that, a human judge ntay consider not correct a syntactic association on the ground of semantic knowledge 1." ></td>
	<td class="line x" title="129:270	Instead, the performance of a syntactic parser should be evaluated only on a syntactic ground." ></td>
	<td class="line x" title="130:270	We define the linguistic performance of SSA as its ability to approximate the generation of the full set of elementary syntactic links derivable by a complete grammar of the domain." ></td>
	<td class="line x" title="131:270	Given the set I2 of all syntactically valid esl and the set m of esl derived applying SSA, the precision of the system can be defined as the ratio cardinality(f2 m co) / cardinality(Q), while its recall can be expressed by: cardinality(co n ~2) / cardinality(~}), Global evaluations of the precision and recall are estimated by the mean values over the whole corpora." ></td>
	<td class="line x" title="132:270	We designed for testing purposes a full attribute grammar of the Italian legal language, and we selected 150 sentences for which the full grammar was proved correct." ></td>
	<td class="line x" title="133:270	For each parsed sentence, a program automatically computes the esrs globally identified (without repetitions) by the parse trees of each sentence, and compares them with those generated by SSA for the same sentence." ></td>
	<td class="line x" title="134:270	The following Table gives a measure of ~erformance: Esl_type N P N V P N RECALL 69.1 ~Yo' N_V 55 % 67.5 % PRECISION 81.8 % 56 % V_N 86.6 % 59 % 60.5 % To fully appreciate these results, we must consider, first, that the evaluation is on a purely syntactic ground (many collocations detected by 1 It is tmclear whether Smadja considered Otis problem in his evaluation experiment the full grammar and not detected by the SSA are in fact semantically wrong), second, that the domain is particularly complex." ></td>
	<td class="line x" title="135:270	There is an average of 23 trees per sentences in the test set." ></td>
	<td class="line x" title="136:270	In particvlar, the low performances of N_V groups (i.e. the subject relation) is influenced by the very frequent (almost 80'}'0) presence of nested relatives (ex: The income that was perceived during 1988i)is included) and inversions (ex: si considerano esenti da tassei redditi=*it is considered tax-free the income)." ></td>
	<td class="line x" title="137:270	No partial parser could cope with these entangled structttres." ></td>
	<td class="line x" title="138:270	One interesting aspect is that these results seem very stable for the domain." ></td>
	<td class="line x" title="139:270	In fact, incrementally adding new groups of sentences, the perfoemance values do not change significantly." ></td>
	<td class="line x" title="140:270	l'or completeness, we also evaluated the English grammar." ></td>
	<td class="line x" title="141:270	In this case, the evaluation was carried entirely by hand, since no full grammar of English was available to automatically derive the complete set of esl's." ></td>
	<td class="line x" title="142:270	F'irst, a test set of 10 remote sensing abstracts (about 1400 words, 67 sentences) was selected at random." ></td>
	<td class="line x" title="143:270	The results are the following: Esl_type RECALL N _ N 78 % V_N." ></td>
	<td class="line x" title="144:270	81% N_p_N 94 % V pN 87 % N_ V 75 % PRECISION 67 % 58 % 54 '~/o 42 % 57 % Here the recall is rather high, since sentences have a much simple structure." ></td>
	<td class="line x" title="145:270	However, there are many valid long distance pp attachments that for example most existing partial parses would not detect." ></td>
	<td class="line x" title="146:270	The precision is lower because the English parser does not have post morphokGy as yet." ></td>
	<td class="line x" title="147:270	One major source of error at detecting N V pairs are, as expected, comIxmnds." ></td>
	<td class="line x" title="148:270	The most important factors that influence the time complexity are: the number N of sentences (words) of the corpus and the number k of different discontinuous rules (about 20, as we said)." ></td>
	<td class="line x" title="149:270	The global rewriting procedure of SSA depends on the length n of the incoming text segment according to the following expression: *t i=l where e(x) is the cost of the application of a grammar rule, as for in Figure 1, to a segment of 4.51 length x. e(x) is easily seen to depend on: 1." ></td>
	<td class="line x" title="150:270	Predicates that test the syntactic category of a word (e.g. noun(w1)), whose cost is equal to that of a simple unification procedure i.e. 't; 2." ></td>
	<td class="line x" title="151:270	TEST ON predicates, whose cost is not greater than '~*n, where n is the substring length." ></td>
	<td class="line x" title="152:270	We can thus say that the expression e(x) of the complexity of SSA syntactic rules verifies the following inequality: e(n) <3r+ 2'rn = O(n) Hence, the global cost is: N n ~ ke(n i) <~_~3'ck + 2'rk(n -i) = i=1 i=1 = 2'rkn(n + 1) +3'~kn = O(n 2) A significant information is that the processing time needed on a Sun Sparc station by the full grammar to parse the test set of 150 sentences is 6 hours, while SSA takes only 10 minutes." ></td>
	<td class="line x" title="153:270	Portability and scalability These two aspects are obviously related." ></td>
	<td class="line x" title="154:270	The question is: How much, in terms of time and resources, is needed to switch to a different domain, or to update a given domain?" ></td>
	<td class="line x" title="155:270	Since we developed three entirely different applications, we can provide some reliable estimate of these parameters." ></td>
	<td class="line x" title="156:270	The estimate of course is strongly dependent upon the specific system we implemented, however we will frame our evaluation in a way that broadly applies to any system that uses similar techniques." ></td>
	<td class="line x" title="157:270	Morphology: Our experience when switching front the commercial to the legal domain was that, when running the analyzer over the new corpus, about 30,000 words could not be analyzed." ></td>
	<td class="line x" title="158:270	This required the insertion of about 1,500 new elementary lemmata." ></td>
	<td class="line x" title="159:270	Accounting for a new word requires entering the stem without affixes, the elementary lemma of the word and the ending class (see section 2.1)." ></td>
	<td class="line x" title="160:270	Entering a new word takes about 5-10 minutes when the linguist is provided with some onqine help, for example a list of ending classes, browsing and testing facilities, etc. With these facilities, updating the lexicon is a relatively easy job, that does not require a specialized linguist to be performed." ></td>
	<td class="line x" title="161:270	Clearly, when implementing several applications, the global updating effort tends to zero." ></td>
	<td class="line x" title="162:270	This is not the case for statistically based part of speech taggers, that require always a fixed effort to train on a new corpus." ></td>
	<td class="line x" title="163:270	On the long run, it seems that grammar based approaches to morphology have an advantage over pos taggers, in terms of portability." ></td>
	<td class="line x" title="164:270	Our experience is that adding a new rule takes about one-two man days." ></td>
	<td class="line x" title="165:270	First, one must detect the linguistic pattern that is not accounted for in the grammar, and verify whether it can be reasonably accounted for, given the intrinsic limitations of the parsing mechanism adopted." ></td>
	<td class="line x" title="166:270	If the linguist decides that, indeed, adding a new rule is necessary and feasible, he/she implements the rule and test its effects." ></td>
	<td class="line x" title="167:270	Grammar modifications are required to: * Select the esl types of interests; * Define the heuristic rules (TEST ON), as discussed in Section 2.3." ></td>
	<td class="line x" title="168:270	One positive aspect of SSA is that its complexity is O(k) with respect to the number k of grammar rules." ></td>
	<td class="line x" title="169:270	Hence adding new rules does not affect the complexity class of the method." ></td>
	<td class="line x" title="170:270	In summary, portability is an essential feature of SSA." ></td>
	<td class="line x" title="171:270	While other parsers need a non trivial effort to be tuned on clifferent linguistic domains, we need only minimal adjustment to ensure the required coverage of the morphologic lexicon." ></td>
	<td class="line x" title="172:270	However, the activity of lexical extension is needed with every approach." ></td>
	<td class="line x" title="173:270	Portability is also guarantied by the modularity of the apl)roach." ></td>
	<td class="line x" title="174:270	4." ></td>
	<td class="line x" title="175:270	Conclusions." ></td>
	<td class="line x" title="176:270	Shallow methods for corpus analysis claim to have several desirable features, such as limited manual work and high coverage." ></td>
	<td class="line x" title="177:270	Our point is that this is not entirely true." ></td>
	<td class="line x" title="178:270	Fully statistical methods require initial training over the corpus to estimate parameters, and this is not trivial." ></td>
	<td class="line x" title="179:270	Most of all, the effort is exactly the same every since the domain changes." ></td>
	<td class="line x" title="180:270	In addition, a lot of noisy data are collected unless some shallow level of linguistic analysis is added to increase performance." ></td>
	<td class="line x" title="181:270	But even then, reliable data are collected only for a fragment of the corpus." ></td>
	<td class="line x" title="182:270	And what about high coverage?" ></td>
	<td class="line x" title="183:270	On tl'te other side, we wouldn't be here, had traditional NLP techniques had any chance to become truly scalable." ></td>
	<td class="line x" title="184:270	This paper showed, if not else, that a bit more syntax can be added to the recipe, while still meeting important requirements, such as computational complexity and portability." ></td>
	<td class="line x" title="185:270	In media stat virtus: ql'ds could be the moral of this paper, and in general of our research on lexical acquisition." ></td>
	<td class="line x" title="186:270	Of course, we don't know where exactly the perfect balance is, we just seek for a better balance." ></td>
	<td class="line x" title="187:270	452 References." ></td>
	<td class="line x" title="188:270	(Antonacci 1989), F. Antonacci, M.T. l'azienza, M. Russo, P. Velardi, (1989), A Logic based system for text analysis and lexical knowledge acquisitio,l, in Data and Knowledge Engineering, vol 4." ></td>
	<td class="line x" title="189:270	(Basili et al. 1991), R. Basili, M. T. Pazienza, P. Velardi, (1991), Using word association for syntactic disambiguation, in Trends in Artificial Intelligence, E. Ardizzone et al. , Eds., I.NAI n. 549, Springer-Verlag." ></td>
	<td class="line x" title="191:270	(Basili et al. 1992 a) R. Basili, M. T. Pazieuza, P. Velardi, (19921, A shallow Syntax to extract word associations from corpora', in Literary and Linguistic Computiug, vol." ></td>
	<td class="line x" title="192:270	2." ></td>
	<td class="line x" title="193:270	(Basili et al. 1992 b) R. Basili, M. T. Pazienza, P. Velardi, (1992), Computational Lexicons: the neat examples and the odd exemplars, Prec." ></td>
	<td class="line x" title="194:270	of 3rd." ></td>
	<td class="line x" title="195:270	Conf." ></td>
	<td class="line x" title="196:270	on Applied NLP." ></td>
	<td class="line x" title="197:270	(Basili et al.1993a), Basili, R. , M.T. Pazienza, P. Velardi, (19931." ></td>
	<td class="line x" title="198:270	Semi-automatic extraction of linguistic information for syntactic disambiguation, Applied Artificial Intelligence, vol." ></td>
	<td class="line x" title="199:270	4, 1993." ></td>
	<td class="line x" title="200:270	(Basill et al.1993b), Basili, R. , M.T. Pazienza, P. Velardi, (19931." ></td>
	<td class="line x" title="201:270	What can be learned from raw texts ?, Journal of Machine Translation, 8:147-173." ></td>
	<td class="line x" title="202:270	(Basili et a1.1993c), Basili, R. , M.T. Pazienza, P. Velardi, (1993)." ></td>
	<td class="line x" title="203:270	llierarchical clustering of verbs, ACLSIGLEX Workshop on Lexical Acquisition, Columbus Ohio, June." ></td>
	<td class="line x" title="204:270	(Bogges,1991), L. Bogges, R. Agarwal, R. Davis, I)isambiguation of prepositional phrases iu automatically labelled technical text (1991)." ></td>
	<td class="line x" title="205:270	Prec." ></td>
	<td class="line oc" title="206:270	of AAAI 1991 (Church and llanks, 19901, K. Church and P. llauks, Word association norm, mutnal information and lexicography, Computational Linguistics, vol." ></td>
	<td class="line x" title="207:270	16, n.1, 1990 (Church et al, 1991), Church, Gale, flanks and Ilindlc, Using statistics in lexicaI analysis, (19911." ></td>
	<td class="line x" title="208:270	Lexical Acquisition, U. Zernik Ed., l.awrence Erlbaum Ass., Publ., 115-164." ></td>
	<td class="line x" title="212:270	(Calzolari and Bindi,1990) N.Calzolari and R. Bindi, Acquisition of lexical informatiou from Corpora, (19901, Prec." ></td>
	<td class="line x" title="213:270	of COLING 90." ></td>
	<td class="line x" title="214:270	(Dahl, 1989), Dahl,V. , 'Discontinous grammars', (1989)." ></td>
	<td class="line x" title="215:270	Computational Intelligence, n. 5, 161-179." ></td>
	<td class="line x" title="216:270	(Fujsaki et a1.,1991) Fujisaki T. , F. Jelinek, J. Cooke, E. Black, T. Nishino, A probabilistic parsing method for sentence disambiguation, (19911." ></td>
	<td class="line x" title="218:270	Cu,'reut trends in Parsing Technology, M. Tomita Ed., Kluwer Ac." ></td>
	<td class="line x" title="220:270	Publ., 1991." ></td>
	<td class="line x" title="221:270	(Ilindle and Rooths,1991) D. llindle, M. l~,ooths, Structural Ambiguity and Lexical P, elatious (1991)." ></td>
	<td class="line x" title="222:270	Prec." ></td>
	<td class="line x" title="223:270	of ACL 1991 (ltindle, 19901, D. llindle, Nouu Classification form predicate-argument structure (199tl)." ></td>
	<td class="line x" title="224:270	Prec." ></td>
	<td class="line x" title="225:270	of AC1." ></td>
	<td class="line x" title="226:270	1990 (Ilindle and Rooths,1991) D. Ilindle, M. Rooths, Structural Ambiguity and Lexical Relations (1991)." ></td>
	<td class="line x" title="227:270	Prec." ></td>
	<td class="line x" title="228:270	of ACL 1991 (llindle and Rooths, 1993) 11." ></td>
	<td class="line x" title="229:270	llindle and M. Rooths, Structural ambiguity and lexical relations (1993)." ></td>
	<td class="line x" title="230:270	(2ompntational Linguistics, vol." ></td>
	<td class="line x" title="231:270	19, n. 1, 1993 (Gale et al, 1992), I!stimating the upper and lower bounds on the performance of word-sense disambiguation progrt, ms, (1992)." ></td>
	<td class="line x" title="232:270	l:'roc, of ACL 1992 (Jelinek et al. , 1990) F. Jelinek, J.l)." ></td>
	<td class="line x" title="233:270	l.al'ferty, F,.I Mecer, Basic methods of probabilistic context f,'ee grammars, (19901." ></td>
	<td class="line x" title="234:270	Research Report R( 216374 IBM YorkTown lleights NY 10598, 1990." ></td>
	<td class="line x" title="235:270	(Marcus, 1980), M. Max'cns, A Theory of Syntactic recoguitiou for Natural I.anguage, MIT Press, 1980 (Marziali,19921, Marziali, A. , 'Robust Methods fro." ></td>
	<td class="line x" title="236:270	parsiug large-scale text archives, I)issertation, Facolth di lugegneria, Univerith 'La Sapieuza' I>,oma, a.a. 1992 ." ></td>
	<td class="line x" title="237:270	(Percira et at." ></td>
	<td class="line x" title="238:270	'1993) F.Pereira, N. Tishby, L. Lee, (19931." ></td>
	<td class="line x" title="239:270	'Distributional Clustering of English Words', in Prec." ></td>
	<td class="line x" title="240:270	of ACI, 93 Columbus, Ohio, June, 1993." ></td>
	<td class="line x" title="241:270	(Rnsso, 1987), M. Russo, 'A generative grammar approach for the morphologic and morphosyntactic analysis of the Italian langnage' (1987)." ></td>
	<td class="line x" title="242:270	3rd." ></td>
	<td class="line x" title="243:270	Conf." ></td>
	<td class="line x" title="244:270	of the F.urol~ean Chapter of the ACI,, Copenhaghen, April 1-3 1987." ></td>
	<td class="line x" title="245:270	(Sekiae et al, 1992) Automatic learuiug for semantic collocations, (19921." ></td>
	<td class="line x" title="246:270	Prec." ></td>
	<td class="line x" title="247:270	of 3rd." ></td>
	<td class="line x" title="248:270	ANLI', 1992 (Smadja,1989), F. Smadja, 'Lexical cooccurences: the missing link', (1989)." ></td>
	<td class="line x" title="249:270	Literary and 1,iuguistic Computing, vol.4, n.3, 1989." ></td>
	<td class="line x" title="250:270	(Smadja,1991), F. Smadja, From N-Grams to collocations: au evaluation of XTRACT, (1991)." ></td>
	<td class="line x" title="251:270	Prec." ></td>
	<td class="line x" title="252:270	of ACI, 199l (Smadja,1990), F. Smadja, K. McKeou, Automatically extracting and rcsprescnting collocations for langultge generation, (1990)." ></td>
	<td class="line x" title="253:270	Prec." ></td>
	<td class="line x" title="254:270	of ACL 1990 (Smadja, 1993), F. Smadja, Retrieving collocations fi'cma text: XTRACT, (1993)." ></td>
	<td class="line x" title="255:270	Computatioual Linguistics, w~l 19, u.l, 1993 (l(esnik and llearst, 1993) P. Resnik, M. llearst, Structural Ambiguity and Conceptual Relations, (1993)." ></td>
	<td class="line x" title="256:270	pt'oc, of the workshop on Very l,arge Corl)ra, Columbus, June 1993 (Iltsuro et a1.,.1993), T. Utsnro, Y. Matsumoto, M. Nagao, verbal case frame acqnisition from bilingual corlx)ra, (1993)." ></td>
	<td class="line x" title="258:270	Prec." ></td>
	<td class="line x" title="259:270	of IJCAI 1993 (Yarowski, 1992) Yarowsky 1)., 'Word-Sense Disambiguation Using Statistical Models of Roger's Categories Trained on Large Corpora', (1992)." ></td>
	<td class="line x" title="261:270	Prec." ></td>
	<td class="line x" title="262:270	of COI,ING-92, Nantes, Aug. 23-28." ></td>
	<td class="line x" title="263:270	(Zernik,1990), tl." ></td>
	<td class="line x" title="264:270	Zernik, P. Jacobs, Tagging for l.earning: Collecting Thematic relations from Corpus (1990)." ></td>
	<td class="line x" title="265:270	Prec." ></td>
	<td class="line x" title="266:270	of COL1NG 1990 (Zeruik,1991), U. Zernik, Ed." ></td>
	<td class="line x" title="267:270	'Lexical Acquisition: Fxploiting on-line resources to build a lexicon', (1991)." ></td>
	<td class="line x" title="268:270	Lawrence Erlbatun Publ., 1991." ></td>
	<td class="line x" title="270:270	453" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C94-1084
Towards Automatic Extraction Of Monolingual And Bilingual Terminology
Daille, Béatrice;Gaussier, Eric;Lange, Jean-Marc;"></td>
	<td class="line x" title="1:125	Toward,q Automatic Extraction of Monolingual and Bilingual q?erminology B(atricc I)AILLI';* l::,'ic OAUSSIEll J(.'ali-Mal(; LANCI;; (*)TALANA Univ. Pa,'is 7 k IBM-France, See 3099, 75592 Paris Cedcx 12 Emaih j ml((.0vnet.ibm.conl Abstract In this paper, we make use of linguistic knowledge to identify certain noun phrases, both in English and French, which are likely to be terms." ></td>
	<td class="line x" title="2:125	We then test and cmnl)are (lifl'e.rent statistical scores to select the 'good' ones among tile candidate terms, and finally propose a statistical method to build correspondences of multi-words units across languages." ></td>
	<td class="line x" title="3:125	Acknowledgement Most of this work was carried out under project EUII.OTP~A ET-10/63, co-sponsored by the European Economic Conmmnity." ></td>
	<td class="line x" title="4:125	1 Introduction A technical lexicon consists of simple as well as complex lexical units." ></td>
	<td class="line x" title="5:125	Complex lexical units are mainly compounds, commonly characterized by a set of morphosyntactic and semantic criteria (see for example \[llenveniste, 1966\] for Dench and \[tlatcher, 1960\] for English)." ></td>
	<td class="line x" title="6:125	The constitution of a list of technical terms from a specific domain is an important issue in NaturM Language Processing, for example in applications such as Machine Translation." ></td>
	<td class="line x" title="7:125	Several methods for the extraction of terms have been proposed, relying on linguistic or statistical approaches." ></td>
	<td class="line x" title="8:125	To build a monolingual terminology bank, \[Bourigault, 1)92\] has implemented a software package which provides it list of likely terminologicM units, based on linguistic specifications." ></td>
	<td class="line oc" title="9:125	Different statistical methods have Mso been tested to extract collocations from large corpora, as \[Church and lianks, 1990, Smadja aim McKeown, 1990\]." ></td>
	<td class="line x" title="10:125	Our work makes use of both linguistic and statistical knowledge." ></td>
	<td class="line x" title="11:125	The outline of this paper is the following: lirst, given linguistic specifh:ations of English and l;'rencli terms, we select noun phrases which are likely ta be terms, and which we will call candidate terms." ></td>
	<td class="line x" title="12:125	To build a monolingual terminology bank, we then apply different statistical scores to select the good ones among the candidate terms." ></td>
	<td class="line x" title="13:125	Finally, we." ></td>
	<td class="line x" title="14:125	test statistical inethods to obtMn ;~ list of bilingual terms, using aligned corpora in l'cench and Ex@ish." ></td>
	<td class="line x" title="15:125	All work and tests are carried out on a parallel bilingual corpus that is tagged by the words' partof-speech and lemma." ></td>
	<td class="line x" title="16:125	This corpus consists of Mmut 20(I,000 words for each language in the tieht of telecommunications." ></td>
	<td class="line x" title="17:125	Simil,~r work can be found in \[Van der Eijk, 1993\]." ></td>
	<td class="line x" title="18:125	2 Linguistic specifications of French and English terms 2.1 Basic multi-words units From an investigation of the cori)us, and from the tenninologists' point of view (for exampie \[Nkwenti-Azeh, 1992\]), it appears that most of tile terms encountered in technical tiehls are noun phrases corresponding to ~t limited numI}er of syntactic patterns." ></td>
	<td class="line x" title="19:125	These patterns detine the morphosyntactic structures of multi-words units (MWU) which are likely to be terms." ></td>
	<td class="line x" title="20:125	We detine the length of a MWU as the number of main items it contains." ></td>
	<td class="line x" title="21:125	A main item is either a noun, an adjective, a verb or an adverb; thus, neither deternliners nor prepositions are considered main items." ></td>
	<td class="line x" title="22:125	MWU of length 1 often correspond to words connected with a hyphen or an apostrophe, and are easy to identify." ></td>
	<td class="line x" title="23:125	We will not deal with them in this paper." ></td>
	<td class="line x" title="24:125	Nor are we concerned with mono-word terms (e.g. telematics), the extraction of which is a different -possibly 515 516 snore complexproblem." ></td>
	<td class="line x" title="25:125	We present below the patterns retained for MWU of length 2, which we will refer to as base MWU." ></td>
	<td class="line x" title="26:125	For each pattern we give an example followed by its translation in parentheses." ></td>
	<td class="line x" title="27:125	The interpretation of the abbreviations is strMgthforward." ></td>
	<td class="line x" title="28:125	French  N Adj orbite gdostationnaire (geostationary orbit)  N1 N2 diode tunnel (tunnel diode)  N1 de (det) N2 bande de frdquenee (frequency band)  N1 prep (det) N2 assignation e't la demande (demand-assignment) English  AdjN multiple access (ace,s multiple)  N1 N2 data transmission (transmission de donndes) 2.2 Operations on base MWU Although MWU of length greater than 2 do exist, they are most of the time built from base MWU by one of the following operations, encountered both in French and English (the base MWU is bracketed in the examples):  overcomposition: rdgdngration des \[lobes latdraux\] \[side lobe\] regrowth  modification: \[station terrienne\] brouilleuse interfering \[earth(-)station\]." ></td>
	<td class="line x" title="29:125	coordination: assemblage et dgsassemblage de paquets packet assembly/disassembly 0vercomposition and modification do not always preserve the base-MWU structure: in French, an adjective modifier is often inserted inside a base-MWU of Igl prop /q2 structure; for example, national (domestic) cast be inserted inside rdseau fi satellites (satellite network) to produce r'dseau national it satellites (domestic satellite network) (notice that in English, the adjective precedes the base MWU most of the time); in l';nglish, the Adj Ig structure is altered when two base-MWU, one of Adj N1 structure and one of N2 N:t structure, are overcomposed: geostationnary satellite and telecommunication satellite yield geostationnary telecommunication satellite." ></td>
	<td class="line x" title="30:125	Coordination, which requires two base-MWU, always breaks the structure of one of them." ></td>
	<td class="line x" title="31:125	These operations are not used with the same frequency, composition and modification being more frequent than coordination." ></td>
	<td class="line x" title="32:125	From the preceding considerations, it seems natural to focus on base MWU." ></td>
	<td class="line x" title="33:125	But, we tat~e into account the cases where base-MWU structure is broken ms well as their variants." ></td>
	<td class="line x" title="34:125	Variants of base-MWU are mainly graphical, orthographical and morphosyntactic (for more details see \[l)aille, 1994\])." ></td>
	<td class="line x" title="35:125	In English, we have also aceel)ted the transformation of a base-MWU of N2 lql structure into a N:I. of lg2 structure." ></td>
	<td class="line x" title="36:125	2.3 Extraction of base MWU To extract and count the occurrences of base MWU (under their various forms) we use their morphosyntactic structures." ></td>
	<td class="line x" title="37:125	IIappily enough, our corpus is tagged: we can rely on tile sequences of part-of-speech to extract the relevant candidates." ></td>
	<td class="line x" title="38:125	For this purpose, we have implemented finite automata associated to regular expressions covering most occurrences of morphosyntactic structures, including modilications clue to the afore mentionned operations." ></td>
	<td class="line x" title="39:125	The output of the program is a list of pairs composed of two lemmas." ></td>
	<td class="line x" title="40:125	Under each pair are stored all occurrences of the base MWU extracted from the corpus: for example, for the pair (interference, level), we get the following sequences: interference level, interference levels, level of interference, levels of interference; for the pair (circuit, num6rique), we get: circuit numdrique, circuits numdriques, ciTvuit enti~rement numdrique, circuits analogiques et numdriques." ></td>
	<td class="line x" title="41:125	Unfortunately, the pairs obtained through this method are not always 'good' terms; we therefore have to introduce additional filters: statistical scores which use the number of occurrences of the pairs as input." ></td>
	<td class="line x" title="42:125	Statistical scores to select good monolingual candidates Since the MWU extracted after the linguistic specifications still contain noise, we use statistical scores as an additional tilter in of der to choose the ~good' ones among the candidate base MWU (or candidates for short)." ></td>
	<td class="line x" title="43:125	These scores apply on candi&~tc lcmma pairs for a given morphosyntactic pattern." ></td>
	<td class="line x" title="44:125	We con> puted different scores: frequencies, associa.tiol~ criteria, Shaflnon diversity and distance mea~ sures." ></td>
	<td class="line x" title="45:125	We discuss frequencies and association criteria below." ></td>
	<td class="line x" title="46:125	Informations provided by Shannon diversity and distance measures are presented in \[Daille, 1994\]." ></td>
	<td class="line x" title="47:125	Most of the association criteria can be found in the classic literature, such as \[Clifford and Stephenson, 1975\], and are based on the so-called 'contingency tables'." ></td>
	<td class="line oc" title="48:125	In the field of eomputationa.1 linguistics, mutual information \[Brown et al. , 1988\], 2 \[Church and Hanks, 1990\], or a likelihood ratio test \[Dunning, 199a\] are suggested." ></td>
	<td class="line x" title="49:125	Our testing method consists in comparing our result list, sorted according to a specified score, with a reference list containing only valid terms." ></td>
	<td class="line x" title="50:125	In order to l)uild the reference list, we augmented an existing terminology database (EURODICAUTOM) with hand work: we selected those candidates for which at least two judges out of three agreed on their goodness, and included them in the reference llst." ></td>
	<td class="line x" title="51:125	A candidate will be considered 'good' when it is found in this reference list." ></td>
	<td class="line x" title="52:125	The program for the extraction of candidates was run on a 240,000 words French corpus, divided into 9,541 sentences." ></td>
	<td class="line x" title="53:125	It yiehled 2,400 candidates that appear at least three times in the text." ></td>
	<td class="line x" title="54:125	After sorting the candidates according to the particular score examined, we build a graphic representation of the prol)ortion of 'good' candidates per range of score w~lues, in the form of a histogram." ></td>
	<td class="line x" title="55:125	Figure 1 shows the histograms obtMned on the French Iqt de 1~2 candidates with two different scores: the likelihood ratio and mutuM information." ></td>
	<td class="line x" title="56:125	The vertical axis, which represents the proportion of good candio dates, is bounded between 0 and 1." ></td>
	<td class="line x" title="57:125	llut, since we do not pretend to have built an exhaustive reference list, we can't actually expect values of more than 0.8." ></td>
	<td class="line x" title="58:125	The horizontM axis grows with the score." ></td>
	<td class="line x" title="59:125	The likelihood ratio test (LOG) (the use of a likelihood ratio test leads to a log likelihood statistics, which explains tile LOG abbrevi:ttion) shows a curve that grows slowly at tirst, and then decidedly." ></td>
	<td class="line x" title="60:125	There is a true ol)position ill the behaviours of small and big v~Llues, the representativity of good candidates approaching 0.8 in that case." ></td>
	<td class="line x" title="61:125	On tile contrary, mutual informat, ion (MI)is quite uniformeiy distributed, and the representativity never exceeds 0.5." ></td>
	<td class="line x" title="62:125	Thus, if one had to choose between these two scores, the first one would undoubtedly be selected." ></td>
	<td class="line x" title="63:125	The outcome of tile study of tile histograms is that very few searing methods will meet our objective." ></td>
	<td class="line x" title="64:125	The most signilicant seem to be tit('." ></td>
	<td class="line x" title="65:125	likelihood ratio test, and the number of occurfences of the pair." ></td>
	<td class="line x" title="66:125	If it were for the sole histograms, one could be tempted to keep only the frequency of the pair, but due to its definition unfrequent terms won't stand out; therefore, we have to keep several methods." ></td>
	<td class="line x" title="67:125	Anyhow, the performance of our scoring functions indicate tile best method to follow: start any task (human post-editing for example) with the highest frequency values on monolingual candidates previously identified via linguistic patterns, thus maximizing the odds that the candidates will indeed be 'good' candidates." ></td>
	<td class="line x" title="68:125	Is the best score a combination of s(:ores?" ></td>
	<td class="line x" title="69:125	An extra data analysis method To complete the evaluation and comparison of the different scores used to select actual ternls from our candidates list, we have applied to these scores specific data analysis methods." ></td>
	<td class="line x" title="70:125	In our case, tile techniques of data anMysis can provide at least two things: a probabillstic study of the correlations between scores, in order to select tile relevant ones, and the possibility of coml)ining scores, in order to improve the results." ></td>
	<td class="line x" title="71:125	We carried out Principal Component Analysis (PCA) with a set of fifteen variables, i.e. tile scores, on a set of 1,230 individuals, i.e. the can517 0.8 0.6 0.4 0.2 0 0.5 0.4 0.5 0.2 0.1 0 t  Figure 1: Itistograms corresponding to the log likelihood (left), and to mntnal information (right) didates." ></td>
	<td class="line x" title="72:125	The first objective of PCA is to reduce the number of variables with as little loss of information as possible." ></td>
	<td class="line x" title="73:125	Doing so shouhl enable us to visualize our data more easily." ></td>
	<td class="line x" title="74:125	From the study of the correlation matrix of variables and their part in the principal axis, it appeared that variables could be divided into four classes." ></td>
	<td class="line x" title="75:125	In each class, we wanted to retain the most significant variable with regard to our task." ></td>
	<td class="line x" title="76:125	This was done by selecting in each class the variable with the best score-histogram." ></td>
	<td class="line x" title="77:125	All other w~riables can be confidently connected to one of these." ></td>
	<td class="line x" title="78:125	With the remaining four variables we tried to investigate the set of candidates." ></td>
	<td class="line x" title="79:125	The difficulty of this task comes from the high nulnber of candidates, so we decided to work on smaller sets." ></td>
	<td class="line x" title="80:125	We divided the whole set of candidates into six subsets depending on the number of occurrences of the candidate (respectively with a number of occurrences in the text equal to 2, 3, 4, 5, between 6 and 10, and over 10)." ></td>
	<td class="line x" title="81:125	This division was used to counterbalance the fact that most scores tend to privilege low frequencies." ></td>
	<td class="line x" title="82:125	We then visualized the candidates graphically (with different tick marks distinguishing terms and non-terms) on several two-dimensional diagrams, the axis of which are linear combinations of the different variables." ></td>
	<td class="line x" title="83:125	The results show series of marks overlapping each other, without any clear separation between terms and non-terms." ></td>
	<td class="line x" title="84:125	The PCA allowed us to go further in the study of the statistical scores, and confirmed that only a few are relevant to our task; moreover, we found no satisfactory combination of variables that could account for termhood." ></td>
	<td class="line x" title="85:125	4 Bilingual candidate term alignment The problenl is now one of finding correspondences between candidates across languages (between English and I'rench in our case)." ></td>
	<td class="line x" title="86:125	As it has been shown in the previous section, it is difllcult to restrict the set of candidates to the 'good' ones." ></td>
	<td class="line x" title="87:125	l'~urthermore, if a candidate is really a term, it shouht be easy to find the equivalent in other languages, or more precisely, in our case, in aligned sentences (supposing that terms are always translated into the same lexical unit)." ></td>
	<td class="line x" title="88:125	For these reasons, we tried to pick out associations between the whole sets of candidates, both l?rench and English." ></td>
	<td class="line x" title="89:125	Sticking to our general philosophy of harmonizing linguistic and statistical contributions, we made experiments with two methods." ></td>
	<td class="line x" title="90:125	4.1 A simple count method, and a possible improvement This method basically counts how often a source candidate and a target candidate occur in aligned sentences; we shall cMl the resulting score bilingual count." ></td>
	<td class="line x" title="91:125	For a given source language candidate, the most frequently aligned target candidates are sorted according to the score; one can then apply the criterion defined in \[Gaussier et al. , 1992\], which consists in extracting a bilingual pair only when the target candidate has no stronger association with any other source candidate." ></td>
	<td class="line x" title="92:125	Such a naive frequency collection gives satisfactory results, providing a sorted list where the top half candidates are 518 mostly good." ></td>
	<td class="line x" title="93:125	We can however try to improve on that result by taking adv,'tntage of available linguistic inforIffation, namely the part-of-speech patterns of the candidates." ></td>
	<td class="line x" title="94:125	We estimated, by counting pattern alignments on a bilingual corpus, pattern aJfinity, which we define as the probability that the pattern of the source candidate gets translated into the pattern of the target candidate (e.g. terms with pattern NdcN in French get translated to terms with pattern NN in English in 81% of the eases)." ></td>
	<td class="line x" title="95:125	Now, instead of simple counts of the occurrences of the source and target candidates, we use counts that are weighted with a function of the pattern aftinity of candidates, as explained in the following formula: p,,o:,~.~(p, t 7,.,) x c(t) C(s, t) = ~,." ></td>
	<td class="line x" title="97:125	~.p,.ol,~4> 1>) x c(>) where C() means 'count of', and p 'pattern of'; probes stands for the probabilities we estimated, and sent represents the set of aligned sentences in which both the source candidate, s, and the target candidate, t, appear." ></td>
	<td class="line x" title="98:125	Results are discussed below." ></td>
	<td class="line x" title="99:125	4.2 Another method: using word alignment scores This method consists essentially in aligning terms using bilingual associations of single words, that have been previously cmnputed through statistical methods (see \[Gaussier et al. , 1992\])." ></td>
	<td class="line x" title="100:125	Let wordcnl and worden2 denote the two words composing an English candidate an(l, similarly, wordfrl and wordfr2 those for a given French candidate." ></td>
	<td class="line x" title="101:125	Each time a source and target candidate occur in aligned sentences, we define the association score between the two candidates as the sum of all the possible single-word-associations between the words composing the candidates: Assoc(wordenl, wordf rj) L)3 This method relics on the assumption that if two terms are translation of one another, then wordfrl is likely to be the translation of, or at least associated to, either wordcnl or wordcn2, and so is wordfr> For example, l)oth French words station and terrienne belong to the lists of single-word translation candidates associated to the English words earth and station." ></td>
	<td class="line x" title="102:125	4.3 It\]valuation and results To evaluate the results, we manually constructed a reference list which contains 1,238 correspondences of MWU2 we believe to be terms (a.ppearing at least twice)." ></td>
	<td class="line x" title="103:125	For any list of candidate pairs to be tested (extracted and sorted according to some scores), we define its precision as the ratio of the number of elements found in the reference list to the total numl)er of elements, and its recall as the ratio of the number of dements found in the reference list to the total number of elements in the reference list." ></td>
	<td class="line x" title="104:125	In so far as these parameters ;u'e affected by the length of the list, we divided each list into units of the same length, considering the first 100 elements, the lirst 2(}0 elements and so on, till we reach the end of one list." ></td>
	<td class="line x" title="105:125	For each unit, we calcula.ted its recall a.nd its precision." ></td>
	<td class="line x" title="106:125	The scores can then be compared in graphs showing the evolution of precision and recall with the number of candidates considered." ></td>
	<td class="line x" title="107:125	Using such graphs, we found out that adding pattern probabilities gives only slightly better or equal results than the silnple frequency method, which does not entirely satisfy our expectations as to their role on the quality of term alignment." ></td>
	<td class="line x" title="108:125	The second method presented above, which does not make use of pattern probabilities, behaves better for candidates of rank 1,00(} and more, hut worse for the first 100 best candidates." ></td>
	<td class="line x" title="109:125	The examination of the top 100 candidates for both methods brings another surprise: only 6 term pairs are cmnmon to the two metL ods." ></td>
	<td class="line x" title="110:125	Of course this could be due to the restriction to the very hest candidates; but, suspecting that there is nmre to it, we combined the candidates from both methods and thus obtained an improvement of precision in all the range of candidates." ></td>
	<td class="line x" title="111:125	The comparison between this combination of methods and the method involving pattern atlinities is shown in figure 2." ></td>
	<td class="line x" title="112:125	The plain line in figure 2 shows precision and recall for the first method involving pattern aftinitles (which is referred to as normal on the figure)." ></td>
	<td class="line x" title="113:125	We think that such results could be even better if we chose the right combination of two -or, possibly, more than twoscoring methods." ></td>
	<td class="line x" title="114:125	All in all, our method provides 70% precision for a list of more than 1,00(} candidates, or 80% if one restricts to the topmost 500." ></td>
	<td class="line x" title="115:125	it gives a good starting point for terminologists 519 100 80 g ~60,q .o '~ 40 20 '~ Precision,1'' J' \[__~_ normal method \] /~a 'j~' \[ _,." ></td>
	<td class="line x" title="116:125	combined melh0dsJ, I, \], I, . I 250 508 750 1000 1250 N' of candidates Figure 2: Precision and Recall when using first method vs. combined methods .520 seeking to extract bilingual terms from texts." ></td>
	<td class="line x" title="117:125	It has to be noted that the remaining 'wrong' alignments are generally somehow relevant, but either incomplete because one of the candidates has a length of more than 2, or trivial because we also extract pairs such as following formula / formule suivante." ></td>
	<td class="line x" title="118:125	5 Conclusion For monolingual terminology extraction, we have first defined the linguistic specifications of multi-word unit terms." ></td>
	<td class="line x" title="119:125	These specifications are used to extract candidate pairs from the corpora." ></td>
	<td class="line x" title="120:125	I~om these pairs and using frequency counts, we have applied a range of statistic~d scores, in order to find the score that would best discriminate between terms and non-terms among the candidates." ></td>
	<td class="line x" title="121:125	The simple frequency count of the pair turns out to be the best score." ></td>
	<td class="line x" title="122:125	We then carried out bilingual terminology extraction using results from the nmnolingum step." ></td>
	<td class="line x" title="123:125	Two scoring methods augmented by the use of prior knowledge about the bilingual correspondences of linguistic patterns were applied to bilingual pairs of monolingua\[ candidate pairs." ></td>
	<td class="line x" title="124:125	In this case, the best way to proceed is derived from the combination of the two methods." ></td>
	<td class="line x" title="125:125	With regard to the future, we will focus on monolingual extraction of MWU of length a and more (based on the identified base MWU and linguistic specifications on term composition), and on the improvement of term alignment in order to deal with terms of heterogeneous length." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J94-4003
Word Sense Disambiguation Using A Second Language Monolingual Corpus
Dagan, Ido;Itai, Alon;"></td>
	<td class="line x" title="1:730	Word Sense Disambiguation Using a Second Language Monolingual Corpus Ido Dagan* AT&T Bell Laboratories Alon Itai t Technion--Israel Institute of Technology This paper presents a new approach for resolving lexical ambiguities in one language using statistical data from a monolingual corpus of another language." ></td>
	<td class="line x" title="2:730	This approach exploits the differences between mappings of words to senses in different languages." ></td>
	<td class="line x" title="3:730	The paper concentrates on the problem of target word selection in machine translation, for which the approach is directly applicable." ></td>
	<td class="line x" title="4:730	The presented algorithm identifies syntactic relations between words, using a source language parser, and maps the alternative interpretations of these relations to the target language, using a bilingual lexicon." ></td>
	<td class="line x" title="5:730	The preferred senses are then selected according to statistics on lexical relations in the target language." ></td>
	<td class="line x" title="6:730	The selection is based on a statistical model and on a constraint propagation algorithm, which simultaneously handles all ambiguities in the sentence." ></td>
	<td class="line x" title="7:730	The method was evaluated using three sets of Hebrew and German examples and was found to be very useful for disambiguation." ></td>
	<td class="line x" title="8:730	The paper includes a detailed comparative analysis of statistical sense disambiguation methods." ></td>
	<td class="line x" title="9:730	1." ></td>
	<td class="line x" title="10:730	Introduction The resolution of lexical ambiguities in nonrestricted text is one of the most difficult tasks of natural language processing." ></td>
	<td class="line x" title="11:730	A related task in machine translation, on which we focus in this paper, is target word selection." ></td>
	<td class="line x" title="12:730	This is the task of deciding which target language word is the most appropriate equivalent of a source language word in context." ></td>
	<td class="line x" title="13:730	In addition to the alternatives introduced by the different word senses of the source language word, the target language may specify additional alternatives that differ mainly in their usage." ></td>
	<td class="line x" title="14:730	Traditionally, several linguistic levels were used to deal with this problem: syntactic, semantic, and pragmatic." ></td>
	<td class="line x" title="15:730	Computationally, the syntactic methods are the most affordable, but are of no avail in the frequent situation when the different senses of the word show the same syntactic behavior, having the same part of speech and even the same subcategorization frame." ></td>
	<td class="line x" title="16:730	Substantial application of semantic or pragmatic knowledge about the word and its context requires compiling huge amounts of knowledge, the usefulness of which for practical applications in broad domains has not yet been proven (e.g. , Lenat et al. 1990; Nirenburg et al. 1988; Chodorow, Byrd, and Heidron 1985)." ></td>
	<td class="line x" title="17:730	Moreover, such methods usually do not reflect word usages." ></td>
	<td class="line x" title="18:730	Statistical approaches, which were popular several decades ago, have recently reawakened and were found to be useful for computational linguistics." ></td>
	<td class="line x" title="19:730	Within this framework, a possible (though partial) alternative to using manually constructed * AT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974, USA." ></td>
	<td class="line x" title="20:730	E-mail: dagan@research.att.com." ></td>
	<td class="line x" title="21:730	The work reported here was done while the author was at the Technion--Israel Institute of Technology." ></td>
	<td class="line x" title="22:730	t Department of Computer Science, Technion--Israel Institute of Technology, Haifa 32000, Israel." ></td>
	<td class="line x" title="23:730	E-maih itai@cs.technion.ac.il." ></td>
	<td class="line x" title="24:730	(~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g. , Grishman, Hirschman, and Nhan 1986)." ></td>
	<td class="line oc" title="25:730	The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993)." ></td>
	<td class="line x" title="26:730	More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991)." ></td>
	<td class="line x" title="27:730	Clearly, statistics on lexical relations can also be useful for target word selection." ></td>
	<td class="line x" title="28:730	Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana' mi-shtei ha-mdinot mi-lahtom 'al hoze shalom." ></td>
	<td class="line x" title="29:730	issue this prevented from-two the-countries from-signing on treaty peace \[ This sentence would translate into English as (2) This issue prevented the two countries from signing a peace treaty." ></td>
	<td class="line x" title="30:730	The verb lahtom has four senses: 'sign,' 'seal,' 'finish,' and 'close'." ></td>
	<td class="line x" title="31:730	The noun hoze means both 'contract' and 'treaty,' where the difference is mainly in usage rather than in the meaning (in Hebrew the word h.oze is used for both sub-senses)." ></td>
	<td class="line x" title="32:730	One possible solution is to consult a Hebrew corpus tagged with word senses, from which we would probably learn that the sense 'sign' of lahtom appears more frequently with hoze as its object than all the other senses." ></td>
	<td class="line x" title="33:730	Thus we should prefer that sense." ></td>
	<td class="line x" title="34:730	However, the size of corpora required to identify lexical relations in a broad domain is very large, and therefore it is usually not feasible to have such corpora manually tagged with word senses) The problem of choosing between 'treaty' and 'contract' cannot be solved using only information on Hebrew, because Hebrew does not distinguish between them." ></td>
	<td class="line x" title="35:730	The solution suggested in this paper is to identify the lexical relations in corpora of the target language, instead of the source language." ></td>
	<td class="line x" title="36:730	We consider word combinations and count how often they appear in the same syntactic relation as in the ambiguous sentence." ></td>
	<td class="line x" title="37:730	For the above example, the noun compound 'peace treaty' appeared 49 times in our corpus (see Section 4.3 for details on our corpus), whereas the compound 'peace contract' did not appear at all; the verb-obj combination 'to sign a treaty' appeared 79 times, whereas none of the other three alternatives appeared more than twice." ></td>
	<td class="line x" title="38:730	Thus, we first prefer 'treaty' to 'contract' because of the noun compound 'peace treaty' and then proceed to prefer 'sign' since it appears most frequently having the object 'treaty'." ></td>
	<td class="line x" title="39:730	The order of selection is determined by a constraint propagation algorithm." ></td>
	<td class="line x" title="40:730	In both cases, the correctly selected word is not the most frequent one: 'close' is more frequent in our corpus than 'sign' and 'contract' is more frequent than 'treaty'." ></td>
	<td class="line x" title="41:730	Also, by using a model of statistical confidence, the algorithm avoids a decision in cases in which no alternative is significantly better than the others." ></td>
	<td class="line x" title="42:730	Our approach can be analyzed from two different points of view." ></td>
	<td class="line x" title="43:730	From that of monolingual sense disambiguation, we exploit the fact that the mapping between words and word senses varies significantly among different languages." ></td>
	<td class="line x" title="44:730	This enables 1 Hearst (1991) suggests a sense disambiguation scheme along this line." ></td>
	<td class="line x" title="45:730	See Section 7 for a comparison of several sense disambiguation methods." ></td>
	<td class="line x" title="46:730	564 Ido Dagan and Alon Itai Word Sense Disambiguation US to map an ambiguous construct from one language to another, obtaining representations in which each sense corresponds to a distinct word." ></td>
	<td class="line x" title="47:730	Now it is possible to collect co-occurrence statistics automatically from a corpus of the other language, without requiring manual tagging of senses." ></td>
	<td class="line x" title="48:730	2 From the point of view of machine translation, we suggest that some ambiguity problems are easier to solve at the level of the target language than the source language." ></td>
	<td class="line x" title="49:730	The source language sentences are considered a noisy source for target language sentences, and our task is to devise a target language model that prefers the most reasonable translation." ></td>
	<td class="line x" title="50:730	Machine translation is thus viewed in part as a recognition problem, and the statistical model we use specifically for target word selection may be compared with other language models in recognition tasks (e.g. , Katz 1987; Jelinek 1990, for speech recognition)." ></td>
	<td class="line x" title="51:730	To a limited extent, this view is shared with the statistical machine translation system of Brown et al.(1990), which employs a target language n-gram model (see Section 8 for a comparison with this system)." ></td>
	<td class="line x" title="53:730	In contrast to this view, previous approaches in machine translation typically resolve examples like (1) by stating various constraints in terms of the source language (Nirenburg 1987)." ></td>
	<td class="line x" title="54:730	As explained above, such constraints cannot be acquired automatically and therefore are usually limited in their coverage." ></td>
	<td class="line x" title="55:730	The experiments we conducted clearly show that statistics on lexical relations are very useful for disambiguation." ></td>
	<td class="line x" title="56:730	Most notable is the result for the set of examples of Hebrew to English translation, which was picked randomly from foreign news sections in the Israeli press." ></td>
	<td class="line x" title="57:730	For this set, the statistical model was applicable for 70% of the ambiguous words, and its selection was then correct for 91% of the cases." ></td>
	<td class="line x" title="58:730	We cite also the results of a later experiment (Dagan, Marcus, and Markovitch 1993) that tested a weaker variant of our method on texts in the computer domain, achieving a precision of 85%." ></td>
	<td class="line x" title="59:730	Both results significantly improve upon a naive method that uses only a priori word probabilities." ></td>
	<td class="line x" title="60:730	These results are comparable to recent reports in the literature (see Section 7)." ></td>
	<td class="line x" title="61:730	It should be emphasized, though, that our results were achieved for a realistic simulation of a broad coverage machine translation system, on randomly selected examples." ></td>
	<td class="line x" title="62:730	We therefore believe that our figures reflect the expected performance of the algorithm in a practical implementation." ></td>
	<td class="line x" title="63:730	On the other hand, most other results relate to a small number of words and senses that were determined by the experimenters." ></td>
	<td class="line x" title="64:730	Section 2 of the paper describes the linguistic model we use, employing a syntactic parser and a bilingual lexicon." ></td>
	<td class="line x" title="65:730	Section 3 presents the statistical model, assuming a multinomial model for a single lexical relation and then using a constraint propagation algorithm to account simultaneously for all relations in the sentence." ></td>
	<td class="line x" title="66:730	Section 4 describes the experimental Setting." ></td>
	<td class="line x" title="67:730	Section 5 presents and analyzes the results of the experiment and cites additional results (Dagan, Marcus, and Markovitch 1993)." ></td>
	<td class="line x" title="68:730	In Section 6 we analyze the limitations of the algorithm in different cases and suggest enhancements to improve it." ></td>
	<td class="line x" title="69:730	We also discuss the possibility of adopting the algorithm for monolingual applications." ></td>
	<td class="line x" title="70:730	Finally, in Section 7 we present a comparative analysis of statistical sense disambiguation methods and then conclude in Section 8." ></td>
	<td class="line x" title="71:730	2 A similar observation underlies the use of parallel bilingual corpora for sense disambiguation (Brown et al. 1991; Gale, Church, and Yarowsky 1992)." ></td>
	<td class="line x" title="72:730	As we explain in Section 7, these corpora are a form of a manually tagged corpus and are more difficult to obtain than monolingual corpora." ></td>
	<td class="line x" title="73:730	Erroneously, the preliminary publication of our method (Dagan, Itai, and Schwall 1991) was cited several times as requiring a parallel bilingual corpus, 565 Computational Linguistics Volume 20, Number 4 2." ></td>
	<td class="line x" title="74:730	The Linguistic Model Our approach is first to use a bilingual lexicon to find all possible translations of each lexically ambiguous word in the source sentence and then use statistical information gathered from target language corpora to choose the most appropriate alternative." ></td>
	<td class="line x" title="75:730	To carry out this task we need the following linguistic tools, which are discussed in detail in the following sections: Section 2.1: Parsers for both the source language and the target language." ></td>
	<td class="line x" title="76:730	These parsers should be capable of locating relevant syntactic relations, such as subj-verb, verb-obj, etc. Section 2.2: A bilingual lexicon that lists alternative translations for each source language word." ></td>
	<td class="line x" title="77:730	If a word belongs to several syntactic categories, there should be a separate list for each one." ></td>
	<td class="line x" title="78:730	Section 2.3: A procedure for mapping the source language syntactic relations to those of the target language." ></td>
	<td class="line x" title="79:730	Such tools have been implemented within the framework of many computational linguistic theories." ></td>
	<td class="line x" title="80:730	We have used McCord's implementation of Slot Grammars (McCord 1990, 1991)." ></td>
	<td class="line x" title="81:730	However, our method could have proceeded just as well using other linguistic models." ></td>
	<td class="line x" title="82:730	The linguistic model will be illustrated by the following Hebrew example, taken from the Ha-Aretz daily newspaper from September, 1990 (transcripted to Latin letters): (3) Diplomatim svurim ki hitztarrfuto shell Hon Sun magdila diplomats believe that the joining of Hon Sun increases et ha-sikkuyim l-hassagat hitqaddmut ba-sihot." ></td>
	<td class="line x" title="83:730	the-chances for-achieving progress in the-talks Here, the ambiguous words in translation to English are magdila, hitqaddmut, and sihot." ></td>
	<td class="line x" title="84:730	To facilitate the reading, we give the translation of the sentence into English, and in each case of an ambiguous selection, all the alternatives are listed within curly brackets, the first alternative being the correct one." ></td>
	<td class="line x" title="85:730	(4) Diplomats believe that the joining of Hon Sun {increases I enlarges I magnifies} the chances for achieving {progress I advance I advancement} in the {talks I conversations I calls}." ></td>
	<td class="line x" title="86:730	The following subsections describe in detail the processing steps of the linguistic model." ></td>
	<td class="line x" title="87:730	These include locating the ambiguous words and the relevant syntactic relations among them in the source language sentence, mapping these relations to alternative relations in the target language, and finally, counting occurrences of these alternatives in a target language corpus." ></td>
	<td class="line x" title="88:730	2.1 Locating the Ambiguous Words in the Source Language Our model defines the different 'senses' of a source word to be all its possible translations to the target language, as listed in a bilingual lexicon." ></td>
	<td class="line x" title="89:730	Some translations can be eliminated by the syntactic environment of the word in the source language." ></td>
	<td class="line x" title="90:730	For example, in the following two sentences the word 'consider' should be translated 566 Ido Dagan and Alon Itai Word Sense Disambiguation differently into Hebrew, owing to the different subcategorization frame in each case: (5) I consider him smart." ></td>
	<td class="line x" title="91:730	(6) I consider going to Japan." ></td>
	<td class="line x" title="92:730	In these examples, the different syntactic subcategorization frames determine two different translations to Hebrew (mah.shiv versus shoqel), thus eliminating some of the ambiguity." ></td>
	<td class="line x" title="93:730	Such syntactic rules that allow us to resolve some of the ambiguities may be encoded in the lexicon (e.g. , Golan, Lappin, and Rimon 1988)." ></td>
	<td class="line x" title="94:730	However, many ambiguities cannot be resolved on syntactic grounds." ></td>
	<td class="line x" title="95:730	The purpose of this work is to resolve the remaining ambiguities using lexical co-occurrence preferences, obtained by statistical methods." ></td>
	<td class="line x" title="96:730	2.2 Locating Syntactic Tuples in Source Language Sentences Our basic concept is the syntactic tuple, which denotes a syntactic relation between two or more words." ></td>
	<td class="line x" title="97:730	It is denoted by the name of the syntactic relation followed by a sequence of words that satisfies the relation, appearing in their base form (without morphological inflections)." ></td>
	<td class="line x" title="98:730	For example (subj-verb: man walk) is a syntactic tuple, which occurs in the sentence 'The man walked home'." ></td>
	<td class="line x" title="99:730	We assume that our parser (or an auxilliary program) can locate the syntactic relation corresponding to a given syntactic tuple in a sentence." ></td>
	<td class="line x" title="100:730	The use of the base form of words is justified by the additional assumption that morphological inflections do not affect the probability of syntactic tuples." ></td>
	<td class="line x" title="101:730	This assumption is not entirely accurate, but it has proven practically useful and reduces the number of distinct tuples." ></td>
	<td class="line x" title="102:730	In our experience, the following syntactic relations proved useful for resolving ambiguities:  Relations between a verb and its subject, complements, and adjuncts, including direct and indirect objects, adverbs, and modifying prepositional phrases." ></td>
	<td class="line x" title="103:730	 Relations between a noun and its complements and adjuncts, including adjectives, modifying nouns in noun compounds, and modifying prepositional phrases." ></td>
	<td class="line x" title="104:730	 Relations between adjectives or adverbs and their modifiers." ></td>
	<td class="line x" title="105:730	4 As mentioned earlier, the full list of syntactic relations depends on the syntactic theory of the parser." ></td>
	<td class="line x" title="106:730	Our model is general and does not depend on any particular list." ></td>
	<td class="line x" title="107:730	However, we have found some desired properties in defining the relevant syntactic relations." ></td>
	<td class="line x" title="108:730	One such property is the use of deep, or canonical, relations, as was already identified by Grishman, Hirschman, and Nhan (1986)." ></td>
	<td class="line x" title="109:730	This property was directly available from the ESG parser (McCord 1990, 1991), which identifies the underlying syntactic function in constructs such as passives and relative clauses." ></td>
	<td class="line x" title="110:730	We have also implemented an additional routine, which modified or filtered some of the relations received from the parser." ></td>
	<td class="line x" title="111:730	This postprocessing routine dealt mainly with function words and prepositional phrases to get a set of more informative relations." ></td>
	<td class="line x" title="112:730	For example, it combined the subject and complement of the verb 'be' (as in 'the man is happy') into a single relation." ></td>
	<td class="line x" title="113:730	Likewise, a verb with its preposition and the head noun of a modifying prepositional phrase (as in sit on the chair) were also combined." ></td>
	<td class="line x" title="114:730	The routine was designed to choose relations that impose considerable restrictions on the possible 567 Computational Linguistics Volume 20, Number 4 (or probable) syntactic tuples." ></td>
	<td class="line x" title="115:730	On the other hand, these relations should not be too specific, to allow statistically meaningful samples." ></td>
	<td class="line x" title="116:730	The first step in resolving an ambiguity is to find all the syntactic tuples containing the ambiguous words." ></td>
	<td class="line x" title="117:730	For (3) we get the following syntactic tuples: (7)." ></td>
	<td class="line x" title="118:730	2." ></td>
	<td class="line x" title="119:730	3. 4." ></td>
	<td class="line x" title="120:730	(subj-verb: hitztarrfut higdil) (verb-obj: higdil sikkuy) (verb-obj: hissig hitqaddmut) (noun-pp: hitqaddmut bsih.a) (these tuples translate as joining-increase, increase-chance, achieve-progress, and progressin-talks)." ></td>
	<td class="line x" title="121:730	In using these tuples, we expect to capture lexical constraints that are imposed by syntactic relations." ></td>
	<td class="line x" title="122:730	2.3 Mapping Syntactic Tuples to the Target Language The set of syntactic tuples in the source language sentence is reflected in its translation to the target language." ></td>
	<td class="line x" title="123:730	As a syntactic tuple is defined by both its syntactic relation and the words that appear in it, we need to map both components to the target language." ></td>
	<td class="line x" title="124:730	By definition, every ambiguous source language word maps to several target language words." ></td>
	<td class="line x" title="125:730	We thus get several alternative target language tuples for each source language tuple that involves an ambiguous word." ></td>
	<td class="line x" title="126:730	For example, for tuple 3 in (7) we obtain three alternatives, corresponding to the three different translations of the word hitqaddmut." ></td>
	<td class="line x" title="127:730	For tuple 4 we obtain nine alternative target tuples, since each of the words hitqaddmut and siha maps to three different English words." ></td>
	<td class="line x" title="128:730	The full mapping of the Hebrew tuples in (7) to English tuples appears in Table 1 (the rightmost column should be ignored for the moment)." ></td>
	<td class="line x" title="129:730	Each of the tuple sets (a-d) in this table denotes the alternatives for translating the corresponding Hebrew tuple." ></td>
	<td class="line x" title="130:730	From a theoretical point of view, the mapping of syntactic relations is more problematic." ></td>
	<td class="line x" title="131:730	There need not be a one-to-one mapping from source language relations to target language ones." ></td>
	<td class="line x" title="132:730	In many cases the mapping depends on the words of the syntactic tuple, as seen in the following example of translating from German to English." ></td>
	<td class="line x" title="133:730	(8) Der Tisch gefaellt mir.--I like the table." ></td>
	<td class="line x" title="134:730	In this example the source language subject (Tisch) becomes the direct object (table) in the target, whereas the direct object (mir) in the source language becomes the subject (I) in the target." ></td>
	<td class="line x" title="135:730	Therefore, the German syntactic tuples (9) (subj-verb: Tisch gefaellt) (verb-obj: gefaellt mir) are mapped to the following English syntactic tuples (10) (verb-obj: like table) (subj-verb: I like) (The Hebrew equivalent is similar to the German structure)." ></td>
	<td class="line x" title="136:730	In practice this is less of a problem." ></td>
	<td class="line x" title="137:730	In most cases, the source language relation has a direct equivalent in the target language." ></td>
	<td class="line x" title="138:730	In many other cases, transformation rules can be encoded, either in the lexicon (if they are word dependent) or as syntactic transformations." ></td>
	<td class="line x" title="139:730	These rules are usually available in machine translation systems that 568 Ido Dagan and Alon Itai Word Sense Disambiguation Table 1 The alternative target syntactic tuples with their counts in the target language corpus Source Tuples Target Tuples Counts a." ></td>
	<td class="line x" title="140:730	(subj-verb: hitztarrfut higdil) (subj-verb: joining increase) 0 (subj-verb: joining enlarge) 0 (subj-verb: joining magnify) 0 b. C. d." ></td>
	<td class="line x" title="141:730	(verb-obj: higdil sikkuy) (verb-obj: hissig hitqaddmut) (noun-pp: hitqaddmut bsih.a) (verb-obj: increase chance) 20 (verb-obj: enlarge chance) 0 (verb-obj: magnify chance) 0 (verb-obj: achieve progress) 29 (verb-obj: achieve advance) 5 (verb-obj: achieve advancement) 1 (noun-pp: progress in talk) 7 (noun-pp: progress in conversation) 0 (noun-pp: progress in call) 0 (noun-pp: advance in talk) 2 (noun-pp: advance in conversation) 0 (noun-pp: advance in call) 2 (noun-pp: advancement in talk) 0 (noun-pp: advancement in conversation) 0 (noun-pp: advancement in call) 0 use the transfer method, as this knowledge is required to generate target language structures." ></td>
	<td class="line x" title="142:730	To facilitate further the mapping of syntactic relations and to avoid errors due to fine distinctions between them, we grouped related syntactic relations into a single 'general class' and mapped this class to the target language." ></td>
	<td class="line x" title="143:730	The important classes used were relations between a verb and its arguments and modifiers (counting as one class all objects, indirect objects, complements, and nouns in modifying prepositional phrases) and between a noun and its arguments and modifiers (counting as one class all modifying nouns in compounds and nouns in modifying prepositional phrases)." ></td>
	<td class="line x" title="144:730	The classification enables us to get more statistical data for each class, as it reduces the number of relations." ></td>
	<td class="line x" title="145:730	The success of using this general level of syntactic relations indicates that even a rough mapping of source to target language relations is useful for the statistical model." ></td>
	<td class="line x" title="146:730	2.4 Counting Syntactic Tuples in the Target Language Corpus We now wish to determine the plausibility of each alternative target word being the translation of an ambiguous source word." ></td>
	<td class="line x" title="147:730	In our model, the plausibility of selecting a target word is determined by the plausibility of the tuples that are obtained from it." ></td>
	<td class="line x" title="148:730	The plausibility of alternative target tuples is in turn determined by their relative frequency in the corpus." ></td>
	<td class="line x" title="149:730	Target syntactic tuples are identified in the corpus similarly to source language tuples, i.e., by a target language parser and a companion routine as described in Section 2.1." ></td>
	<td class="line x" title="150:730	The right column of Table 1 shows the counts obtained for the syntactic tuples of our example in the corpora we used." ></td>
	<td class="line x" title="151:730	The table reveals that the tuples containing the correct target word ('talk,' 'progress,' and 'increase') are indeed more frequent." ></td>
	<td class="line x" title="152:730	569 Computational Linguistics Volume 20, Number 4 However, we still need a decision algorithm to analyze the statistical significance of the data and choose the appropriate word accordingly." ></td>
	<td class="line x" title="153:730	3." ></td>
	<td class="line x" title="154:730	The Statistical Model As seen in the previous section, the linguistic model maps each source language syntactic tuple to several alternative target tuples, in which each alternative corresponds to a different selection of target words." ></td>
	<td class="line x" title="155:730	We wish to select the most plausible target language word for each ambiguous source language word, basing our decision on the counts obtained from the target corpus, as illustrated in Table 1." ></td>
	<td class="line x" title="156:730	To that end, we should define a selection algorithm whose outcome depends on all the syntactic tuples in the sentence." ></td>
	<td class="line x" title="157:730	If the data obtained from the corpus do not substantially support any one of the alternatives, the algorithm should notify the translation system that it cannot reach a statistically meaningful decision." ></td>
	<td class="line x" title="158:730	Our algorithm is based on a statistical model." ></td>
	<td class="line x" title="159:730	However, we wish to point out that we do not see the statistical considerations, as expressed in the model, as fully reflecting the linguistic considerations (syntactic, semantic, or pragmatic) that determine the correct translation." ></td>
	<td class="line x" title="160:730	The model reflects only part of the relevant data and in addition makes statistical assumptions that are only partially satisfied." ></td>
	<td class="line x" title="161:730	Therefore, a statistically based model need not make the correct linguistic choices." ></td>
	<td class="line x" title="162:730	The performance of the model can only be empirically evaluated, the statistical considerations serve only as heuristics." ></td>
	<td class="line x" title="163:730	The role of the statistical considerations is therefore to guide us in constructing heuristics that make use of the linguistic data of the sample (the corpus)." ></td>
	<td class="line x" title="164:730	Our experience shows that the statistical methods are indeed very helpful in establishing and comparing useful decision criteria that reflect various linguistic considerations." ></td>
	<td class="line x" title="165:730	3.1 The Probabilistic Model First we discuss decisions based on a single syntactic tuple (as when only one syntactic tuple in the sentence contains an ambiguous word)." ></td>
	<td class="line x" title="166:730	Denote the source language syntactic tuple T and let there be k alternative target tuples for T, denoted by T1, , Tk." ></td>
	<td class="line x" title="167:730	Let the counts obtained for the target tuples be nl,." ></td>
	<td class="line x" title="168:730	., nk." ></td>
	<td class="line x" title="169:730	For notational convenience, we number the tuples by decreasing frequency, i.e., nl ~ y/2 ~ ''' ~ nkSince our goal is to choose for T one of the target tuples Ti, we can consider T a discrete random variable with multinomial distribution, 3 whose possible values are T1,, Tk." ></td>
	<td class="line x" title="170:730	Let Pi be the probability of obtaining Ti, i.e., the probability that Ti is the correct translation for T. We estimate the probabilities Pi by the counts ni in the obvious way, using the maximum likelihood estimator (Agresti 1990, pp." ></td>
	<td class="line x" title="171:730	40-41)." ></td>
	<td class="line x" title="172:730	The estimator \]9i for Pi is Hi /~i -k ' (1) Y~q=l nj The precision of the estimator depends, of course, on the size of the counts in the computation." ></td>
	<td class="line x" title="173:730	We will incorporate this consideration into the decision algorithm by using confidence intervals." ></td>
	<td class="line x" title="174:730	4 3 A variable that can have one of a finite set of values, each of them having a fixed probability." ></td>
	<td class="line x" title="175:730	4 The maximum likelihood estimator is known to give poor estimates when small counts are involved, and there are several methods to improve it (see Church and Gale 1991, for a presentation and discussion of several methods)." ></td>
	<td class="line x" title="176:730	For our needs this is not necessary in most cases, since we are not going to use the estimate itself, but rather a confidence interval for the ratio between two estimations (see below)." ></td>
	<td class="line x" title="177:730	570 Ido Dagan and Alon Itai Word Sense Disambiguation We now have to establish the criterion for choosing the preferred target language syntactic tuple." ></td>
	<td class="line x" title="178:730	The most reasonable assumption is to choose the tuple with the highest estimated probability, that is Tl--the tuple with the largest observed frequency." ></td>
	<td class="line x" title="179:730	According to the model, the probability that T1 is the right choice is estimated as Pl. This criterion should be subject to the condition that the difference between the alternative probabilities is significant." ></td>
	<td class="line x" title="180:730	For example, if/Yl = 0.51 and/52 = 0.49, the expected success rate in choosing T1 is approximately 0.5." ></td>
	<td class="line x" title="181:730	To prevent the system from making a decision in such cases, we need to impose some conditions on the probabilities Pi." ></td>
	<td class="line x" title="182:730	One possible such condition is that \]Jl exceeds a prespecified threshold (or, as we shall describe below, that the threshold requirement be applied to a confidence interval)." ></td>
	<td class="line x" title="183:730	According to the model, this requirement ensures that the success probability of every decision exceeds the threshold." ></td>
	<td class="line x" title="184:730	Even though this method satisfies the probabilistic model, it is vulnerable to noise in the data, which often causes some relatively small counts to be larger than their true value in the sample." ></td>
	<td class="line x" title="185:730	The noise is introduced in part by inaccuracies in the model and in part because of errors during the automatic collection of the statistical data." ></td>
	<td class="line x" title="186:730	Consequently, the estimated value of Pl may be smaller than its true value, because other counts in Equation 1 are too large, thus, preventing Pl from passing the threshold." ></td>
	<td class="line x" title="187:730	To deal with this problem, we have chosen another criterion for significance--the odds ratio." ></td>
	<td class="line x" title="188:730	We choose the alternative T1 only if all the ratios r;2' exceed a prespecified threshold." ></td>
	<td class="line x" title="189:730	Note that 15i/lfij -ni/nj, and since nl _~ n2 _)  ~_ nk, the ratio tYl/lY2 is less than or equal to all the other ratios." ></td>
	<td class="line x" title="190:730	Therefore, it suffices to check the odds ratio only for ill/P2." ></td>
	<td class="line x" title="191:730	This criterion is less sensitive to noise of the above-mentioned type than/)1, since it depends only on the two largest counts." ></td>
	<td class="line x" title="192:730	3.1.1 Underlying Assumptions." ></td>
	<td class="line x" title="193:730	The use of a probabilistic model necessarily introduces several assumptions on the structure of the corresponding linguistic data." ></td>
	<td class="line x" title="194:730	It is important to point out these assumptions, in order to be aware of possible inconsistencies between the model and the linguistic phenomena for which it is used." ></td>
	<td class="line x" title="195:730	The first assumption is introduced by the use of a multinomial model, which presupposes the following: Assumption 1 The events Ti are mutually disjoint." ></td>
	<td class="line x" title="196:730	This assumption is not entirely valid, since sometimes it is possible to translate a source language word to several target language words, such that all the translations are valid." ></td>
	<td class="line x" title="197:730	For example, consider the Hebrew sentence (from the Ha-Aretz daily newspaper, November 27, 1990) whose English translation is (11) The resignation of Thatcher is not {related I connected} to the negotiations with Damascus." ></td>
	<td class="line x" title="198:730	In this sentence (but not in others), the ambiguous word qshura can equally well be translated to either 'related' or 'connected'." ></td>
	<td class="line x" title="199:730	In terms of the probabilistic model, the two corresponding events, i.e., the two alternative English tuples that contain these words, T1 -(verb-comp: relate to negotiation) and T2 = (verb-comp: connect to negotiation) are 571 Computational Linguistics Volume 20, Number 4 both correct, thus the events T1 and T2 both occur (they are not disjoint)." ></td>
	<td class="line x" title="200:730	However, we have to make this assumption, since the counts we have, ni, from which we estimate the probabilities of the Ti values, count actual occurrences of single syntactic tuples." ></td>
	<td class="line x" title="201:730	In other words, we count the number of times that each of Zl and T2 actually occur, not the number of times in which each of them could occur." ></td>
	<td class="line x" title="202:730	Two additional assumptions are introduced by using counts of the occurrences of syntactic tuples of the target language in order to estimate the translation probabilities of source language tuples: Assumption 2 An occurrence of the source language syntactic tuple T can indeed be translated to one of Zl~~ Tk." ></td>
	<td class="line x" title="203:730	Assumption 3 Every occurrence of the target tuple Ti can be the translation of only the source tuple T. Assumption 2 is an assumption on the completeness of the linguistic model." ></td>
	<td class="line x" title="204:730	It is rather reasonable and depends on the completeness of our bilingual lexicon: if the lexicon gives all possible translations of each ambiguous word, then this assumption will hold, since for each syntactic tuple T we will produce all possible translations3 Assumption 3, which may be viewed as a soundness assumption, does not always hold, since a target language word may be the translation of several source language words." ></td>
	<td class="line x" title="205:730	Consider, for example, the Hebrew tuple T = (verb-obj: heh.ziq lul)." ></td>
	<td class="line x" title="206:730	Lul is ambiguous, meaning either a playpen or a chicken pen." ></td>
	<td class="line x" title="207:730	Accordingly, T can be translated to either T1 = (verb-obj: hold playpen) or T2 = (verb-obj: hold pen)." ></td>
	<td class="line x" title="208:730	In the context of 'hold' the first translation is more likely, and we can therefore expect our model to prefer T1." ></td>
	<td class="line x" title="209:730	However, this might not be the case because Assumption 3 is contradicted." ></td>
	<td class="line x" title="210:730	'Pen' can also be the translation of the Hebrew word 'et (the writing instrument), and thus T2 can be the translation of another Hebrew tuple, T' = (verb~bj: heh.ziq 'et)." ></td>
	<td class="line x" title="211:730	This means that when translating T we are counting occurrences of T2 that correspond to both T and T', 'misleading' the selection criterion." ></td>
	<td class="line x" title="212:730	Section 6.3 illustrates another example in which the assumption is not valid, causing the algorithm to fail to select the correct translation." ></td>
	<td class="line x" title="213:730	We must make this assumption since we use only a target language corpus, which is not related to any source language information." ></td>
	<td class="line x" title="214:730	6 Therefore, when seeing an occurrence of the target language word w, we do not know which source language word is appropriate in the current context." ></td>
	<td class="line x" title="215:730	Consequently, we count its occurrence as a translation of all the source language words for which w is a possible translation." ></td>
	<td class="line x" title="216:730	This implies that sometimes we use inaccurate data, which introduce noise into the statistical model (see Section 6.3 for a discussion of an alternative, but expensive, solution, using a bilingual corpus)." ></td>
	<td class="line x" title="217:730	As we shall see, even though the assumption does not always hold, in most cases this noise does not interfere with the decision algorithm." ></td>
	<td class="line x" title="218:730	5 The problem of constructing a bilingual lexicon that is as complete as possible is beyond the scope of this paper." ></td>
	<td class="line x" title="219:730	A promising approach may be to use aligned bilingual corpora, especially for augmenting existing lexicons with domain-specific terminology (Brown et al. 1993; Dagan, Church, and Gale 1993)." ></td>
	<td class="line x" title="220:730	In any case, it seems that any translation system is limited by the completeness of its bilingual lexicon, which makes our assumption a reasonable one." ></td>
	<td class="line x" title="221:730	6 As explained in the introduction, this is a very important advantage of our method over other methods that use bilingual corpora." ></td>
	<td class="line x" title="222:730	572 Ido Dagan and Alon Itai Word Sense Disambiguation 3.2 Statistical Significance of the Decision Another problem we should address is the statistical significance of the data--what confidence do we have that the data indeed reflect the phenomenon." ></td>
	<td class="line x" title="223:730	If the decision is based on small counts, then the difference in the counts might be due to chance." ></td>
	<td class="line x" title="224:730	For example, we should have more confidence in the odds ratio 151/152 = 3 when nl = 30 and //2 = 10 than when nl = 3 and n2 = 1." ></td>
	<td class="line x" title="225:730	Consequently, we shall use a dynamic threshold for 151/152, which is large when the counts are small and decreases as the counts increase." ></td>
	<td class="line x" title="226:730	A common method for determining the statistical significance of estimates is the use of confidence intervals." ></td>
	<td class="line x" title="227:730	Rather than finding a confidence interval for 151/152, we will bound the log odds ratio, ln(151/152)." ></td>
	<td class="line x" title="228:730	Since the variance Of the log odds ratio is independent of the mean, it converges to the normal distribution faster than the odds ratio itself (Agresti 1990)." ></td>
	<td class="line x" title="229:730	We use a one-tailed interval, as we want only to decide whether ln(151/152) is greater than a specific threshold (i.e. , we need only a lower bound for ln(151/152))." ></td>
	<td class="line x" title="230:730	Using this method, for each desired error probability 0 < ~ < 1, we may determine a value B~ and state that with a probability of at least 1 c~ the true value, ln(pl/p2), is greater than B~." ></td>
	<td class="line x" title="231:730	The confidence interval of a random variable X with normal distribution is ZI-~, where ZI-~ is the confidence coefficient, which may be found in statistical tables, and var is the variance." ></td>
	<td class="line x" title="232:730	In our case, the size of the confidence interval is Z1-~/var\[ln~221' In the appendix we approximate the variance by the following \[ ~22\] 1 1 var in 151 ~ __ _}_ --." ></td>
	<td class="line x" title="233:730	//1 //2 The bound we get is thus 151 //1 Since ~ //2 we get ln(P~2 ) >ln(pP-~)-Zl-~V~n~ + 1 //2 ln/P~) ~>ln/n,~-~)-Zl-c,V/n~+ 1 //2 ;o (2) B~(nl,n2) (or B~ when nl and n2 are understood from the context) is defined to be the right-hand side of Equation 2." ></td>
	<td class="line x" title="234:730	The meaning of the inequality is that for every given pair nl~ n2 we know with confidence 1 c~ that In pl ~ B~ (3) P2 or in other words, B,~ is a lower bound for ln(pl/P2) with this confidence level." ></td>
	<td class="line x" title="235:730	To obtain a decision criterion, we choose a threshold 0, for B~, and decide to choose T1 only if B~ > 0." ></td>
	<td class="line x" title="236:730	(4) 573 Computational Linguistics Volume 20, Number 4 If Equation 4 does not hold, the algorithm makes no decision." ></td>
	<td class="line x" title="237:730	The meaning of this criterion is that only if we know with confidence of at least 1 ~ that ln(pl/p2) > O, will we select the most frequent tuple T1 as the appropriate one." ></td>
	<td class="line x" title="238:730	In terms of statistical decision theory, we say that our null hypothesis is that ln(pl/P2) < 0, and we will make a decision only if we can reject this hypothesis with confidence at least 1 ~." ></td>
	<td class="line x" title="239:730	Note that we cannot compute B~ when one of the counts is zero." ></td>
	<td class="line x" title="240:730	In this case we have used the common correction method of adding 0.5 to each of the counts (Agresti 1990, p. 249)." ></td>
	<td class="line x" title="241:730	7 We shall now demonstrate the use of the decision criterion." ></td>
	<td class="line x" title="242:730	In the experiment we conducted we chose the parameters ~ = 0.1, for which Z~ = 1.282, and 0 = 0.2." ></td>
	<td class="line x" title="243:730	Thus, to choose T1 we require that with confidence level of at least 90% the hypothesis should satisfy ln(pl/P2) > 0.2 (i.e. , Pl/P2 >_ e 02 = 1.22)." ></td>
	<td class="line x" title="244:730	For the alternative translations of tuple c in Table 1 we got nl = 29 and n2 = 5." ></td>
	<td class="line x" title="245:730	For these values Be = 1.137." ></td>
	<td class="line x" title="246:730	In this case Equation 4 is satisfied for 0 = 0.2, and the algorithm selects the word 'progress' as the translation of the Hebrew word hitqaddmut." ></td>
	<td class="line x" title="247:730	In another case we had to translate the Hebrew word ro'sh, which can be translated to either 'top' or 'head,' in the sentence whose translation is (12) Sihanuk stood at the {top \] head} of a coalition of underground groups." ></td>
	<td class="line x" title="248:730	The two alternative syntactic tuples were (a) (verb-pp: standat head) 10 (b) (verb-pp: stand at top) 5 For nl = 10 and n2 = 5, we get Be = -0.009 (a negative value means that it is impossible to ensure with a 90% confidence level that Pl > P2)." ></td>
	<td class="line x" title="249:730	Since Be G 0.2, the algorithm will refrain from making a decision in this case." ></td>
	<td class="line x" title="250:730	This abstention reflects the fact that the difference between the counts is not statistically significant, and choosing the first alternative can be wrong in many of the cases (as seen in the five cases that were observed in the corpus)." ></td>
	<td class="line x" title="251:730	As mentioned above, our motivation was to find a criterion that depends on a dynamic threshold for ~1/\]Y2 (or alternatively nl/n2), so that the threshold will be higher when nl and n2 are smaller." ></td>
	<td class="line x" title="252:730	Our criterion indeed satisfies this requirement." ></td>
	<td class="line x" title="253:730	If we substitute B~ in Equation 4, we get the following equivalent criterion: In nl > 0 + Zl_c~/n~ q1 //2 //2 The above inequality clarifies the roles of the two parameters, ~ and 0:0 specifies a lower bound on In(nl/n2), which is independent of the sample size; c~ reflects the statistical significance." ></td>
	<td class="line x" title="254:730	If c~ is decreased (i.e. , we require more confidence), ZI_~ will increase, and therefore, the component dependent on the sample size will increase." ></td>
	<td class="line x" title="255:730	Since this component is in inverse relation to nl and n2, the penalty for decreasing c~ increases when the sample size decreases." ></td>
	<td class="line x" title="256:730	From this analysis we can derive the criterion for choosing the parameters: if we wish to use small counts, then c~ should be small, and 0 depends on the required ratio between nl and n2." ></td>
	<td class="line x" title="257:730	The optimal values of the parameters should be determined empirically and might depend on the corpora and parsers we use." ></td>
	<td class="line x" title="258:730	7 In this case, smoothing methods (Church and Gale 1991) may improve the correction method." ></td>
	<td class="line x" title="259:730	574 Ido Dagan and Alon Itai Word Sense Disambiguation 3.3 Sentences with Several Syntactic Relations In the previous section, we assumed that the source sentence contains only one ambiguous syntactic tuple." ></td>
	<td class="line x" title="260:730	In general there may be several ambiguous words that appear in several tuples." ></td>
	<td class="line x" title="261:730	We should take advantage of the occurrence patterns of all of the tuples to reach a decision." ></td>
	<td class="line x" title="262:730	Since different relations may favor different translations for an ambiguous word, we should devise a strategy for selecting a consistent translation for all words in the sentence." ></td>
	<td class="line x" title="263:730	We have used the following constraint propagation algorithm, which receives as input the list of all source tuples along with their alternative translations to target tuples: . . ." ></td>
	<td class="line x" title="264:730	Compute B~ of each source tuple." ></td>
	<td class="line x" title="265:730	If the largest B~ is less than the threshold, 8, then stop." ></td>
	<td class="line x" title="266:730	Let T be the source tuple for which B~ is maximal." ></td>
	<td class="line x" title="267:730	Select the translation for the ambiguous words (or word) in T according to T1 (the most frequent target alternative for T)." ></td>
	<td class="line x" title="268:730	Remove T from the list of source tuples." ></td>
	<td class="line x" title="269:730	Propagate the constraint: eliminate target tuples that are inconsistent with this decision." ></td>
	<td class="line x" title="270:730	If now some source tuples become unambiguous, remove them from the list of source tuples." ></td>
	<td class="line x" title="271:730	Repeat this procedure for the remaining list of source tuples, until all ambiguities have been resolved, or the maximal B~ is less than 8." ></td>
	<td class="line x" title="272:730	To illustrate the algorithm, we consider Table 1 using the parameters c~ = 0.1 and 0 = 0.2." ></td>
	<td class="line x" title="273:730	The largest value of B~ occurs for the tuple (verb-obj: higdil sikkuy), for which higdil can be translated to ;increase,' 'magnify,' or 'enlarge'." ></td>
	<td class="line x" title="274:730	The first alternative appeared nl = 20 times, and the other alternatives did not appear at all, (n2 = n3 = 0)." ></td>
	<td class="line x" title="275:730	Adding the correction factor and computing B~ yields B~(nl + 0.5~n2 q-0.5) = B,~(20.5, 0.5) = 1.879 > 0.2 = 8." ></td>
	<td class="line x" title="276:730	Therefore, the word 'increase' was chosen as the translation of higdil." ></td>
	<td class="line x" title="277:730	Since this word appears also in the tuple (subj-verb: hitztarrfut higdil), the' target tuples that include alternative translations of higdil were deleted." ></td>
	<td class="line x" title="278:730	Thus (13) (subj-verb: joining enlarge) (subj-verb: joining magnify) were deleted." ></td>
	<td class="line x" title="279:730	This leaves us with only one alternative (subj-verb: joining increase) as a possible translation of this Hebrew tuple, which is therefore removed from the input list." ></td>
	<td class="line x" title="280:730	We now recompute the values of B~ for the remaining tuples." ></td>
	<td class="line x" title="281:730	The maximal value is obtained for the tuple (14) (verb-obj: hissig hitqaddmut) where B~ (29, 5) = 1.137 > 8." ></td>
	<td class="line x" title="282:730	We, therefore, choose the word 'progress' as a translation for hitqaddmut." ></td>
	<td class="line x" title="283:730	Since this word, hitqaddmut, also appears in the tuple (noun-pp: hitqaddmut bsih.a), we delete the Six target tuples that are inconsistent with the selection of 'progress' (those containing the words 'advance' and 'advancement')." ></td>
	<td class="line x" title="284:730	There now remain only three alternative target tuples for hitqaddmut bsih.a. We now recompute the values of B~." ></td>
	<td class="line x" title="285:730	The maximum value is B~ (7.5~ 0.5) = 0.836 > 0 (note that because tuples inconsistent with the previous decisions were eliminated, 575 Computational Linguistics Volume 20, Number 4 n2 dropped from 2 to 0, thus increasing B~)." ></td>
	<td class="line x" title="286:730	Thus, 'talk' is selected as the translation of siha." ></td>
	<td class="line x" title="287:730	Now all the ambiguities have been resolved and the procedure stops." ></td>
	<td class="line x" title="288:730	In the above example all the ambiguities were resolved since in each stage the value of B~ exceeded the threshold 0 = 0.2." ></td>
	<td class="line x" title="289:730	In some cases not all ambiguities are resolved, though the number of ambiguities may decrease." ></td>
	<td class="line x" title="290:730	It should be noted that other methods may be proposed for combining the statistics of several syntactic relations." ></td>
	<td class="line x" title="291:730	For example, it may make sense to multiply estimates of conditional probabilities of tuples in different relations, in a way that is analogous to n-gram language modeling (Jelinek, Mercer, and Roukos 1992)." ></td>
	<td class="line x" title="292:730	However, such an approach will make it harder to take into account the statistical significance of the estimate (a criterion that is missing in standard n-gram models)." ></td>
	<td class="line x" title="293:730	In our set of examples, the constraint propagation method proved to be successful and did not seem to introduce any errors." ></td>
	<td class="line x" title="294:730	Further experimentation, on much larger data sets, is needed to determine which of the two methods (if any) is substantially superior to the other." ></td>
	<td class="line x" title="295:730	4." ></td>
	<td class="line x" title="296:730	The Experiment To evaluate the proposed disambiguation method, we implemented and tested the method on a random set of examples." ></td>
	<td class="line x" title="297:730	The examples consisted of a set of Hebrew paragraphs and a set of German paragraphs." ></td>
	<td class="line x" title="298:730	In both cases the target language was English." ></td>
	<td class="line x" title="299:730	The Hebrew examples consisted of ten paragraphs picked at random from foreign news sections of the Israeli press." ></td>
	<td class="line x" title="300:730	The paragraphs were selected from several news items and articles that appeared in several daily newspapers." ></td>
	<td class="line x" title="301:730	The target language corpus consisted of American newspaper articles, and the Hansard corpus of the proceedings of the Canadian Parliament." ></td>
	<td class="line x" title="302:730	The domain of foreign news articles was chosen to correspond to some of the topics that appear in the English corpus, s The German examples were chosen at random from the German press, without restricting the topic." ></td>
	<td class="line x" title="303:730	9 Since we did not have a translation system from Hebrew or German to English, we simulated the steps such a system would perform." ></td>
	<td class="line x" title="304:730	Hence, the results we report measure the performance of just the target word selection module and not the performance of a complete translation system." ></td>
	<td class="line x" title="305:730	The latter can be expected to be somewhat lower for a real system, depending on the performance of its other components." ></td>
	<td class="line x" title="306:730	Note, however, that since the disambiguation module is highly immune to noise, it might be more useful in a real system: in such a system some of the alternatives would be totally erroneous." ></td>
	<td class="line x" title="307:730	Since the corresponding syntactic tuples would typically not be found in the corpora, they would be eliminated by our module." ></td>
	<td class="line x" title="308:730	The experiment is described in detail in the following subsections." ></td>
	<td class="line x" title="309:730	It provides an example for a thorough evaluation that is carried out without having a complete system available." ></td>
	<td class="line x" title="310:730	We specifically describe the processing of the Hebrew data, which was performed by a professional translator, supervised by the authors." ></td>
	<td class="line x" title="311:730	The German examples were processed very similarly." ></td>
	<td class="line x" title="312:730	4.1 Locating Ambiguous Words To locate ambiguous words, we simulated a bilingual lexicon and syntactic filters of a translation system." ></td>
	<td class="line x" title="313:730	For every source language word, the translator searched all possible 8 The corpus includes many irrelevant topics as well, which introduce noisy data with respect to the given domain." ></td>
	<td class="line x" title="314:730	9 The German examples were prepared by Ulrike Schwall from the IBM Scientific Center, Heidelberg, Germany." ></td>
	<td class="line x" title="315:730	576 Ido Dagan and Alon Itai Word Sense Disambiguation translations using a Hebrew-English dictionary (Alcalay 1990)." ></td>
	<td class="line x" title="316:730	The list of translations proposed by the dictionary was modified according to the following guidelines, to reflect better the lexicon of a practical translation system: . . 3." ></td>
	<td class="line x" title="317:730	. . Eliminate translations that would be ruled out for syntactic reasons, as explained in Section 2.1." ></td>
	<td class="line x" title="318:730	Consider only content words, ignoring function words and proper nouns." ></td>
	<td class="line x" title="319:730	Assume that multi-word terms, such as 'prime minister,' appear in the lexicon as complete terms." ></td>
	<td class="line x" title="320:730	Thus we did not consider each of their constituents separately." ></td>
	<td class="line x" title="321:730	Also, we did not consider source language words that should be translated to a multi-word target phrase." ></td>
	<td class="line x" title="322:730	Eliminate rare and archaic translations that are not expected in the context of foreign affairs in the current press." ></td>
	<td class="line x" title="323:730	The professional translator added translations that were missing in the dictionary." ></td>
	<td class="line x" title="324:730	In addition, each of the remaining target alternatives for each source word was evaluated as to whether it is a suitable translation in the current context." ></td>
	<td class="line x" title="325:730	This evaluation was later used to judge the selections of the algorithm." ></td>
	<td class="line x" title="326:730	If all the alternatives were considered suitable, then the source word was eliminated from the test set, since any decision for it would have been considered successful." ></td>
	<td class="line x" title="327:730	We ended up with 103 Hebrew and 54 German ambiguous words." ></td>
	<td class="line x" title="328:730	For each Hebrew word we had an average of 3.27 alternative translations and an average of 1.44 correct translations." ></td>
	<td class="line x" title="329:730	The average number of translations of a German word was 3.26, and there were 1.33 correct translations." ></td>
	<td class="line x" title="330:730	4.2 Determining the Syntactic Tuples and Mapping Them to English Since we did not have a Hebrew parser, we have simulated the two steps of determining the source syntactic tuples and mapping them to English by reversing the order of these steps, in the following way: First, the sample sentences were translated manually, as literally as possible, into English." ></td>
	<td class="line x" title="331:730	Then, the resulting English sentences were analyzed, using the ESG parser and the postprocessing routine (see Section 2.2), to identify the relevant syntactic tuples." ></td>
	<td class="line x" title="332:730	The tuples were further classified into 'general classes,' as described in Section 2.3." ></td>
	<td class="line x" title="333:730	The use of these general classes, which was intended to facilitate the mapping of syntactic relations from one language to another, also facilitated our simulation method and caused it to produce realistic output." ></td>
	<td class="line x" title="334:730	At the end of the procedure, we had, for each sample sentence, a data structure similar to Table 1 (without the counts)." ></td>
	<td class="line x" title="335:730	4.3 Acquiring the Statistical Data The statistical data were acquired from the following corpora:  Texts from The Washington Post ~0 million words." ></td>
	<td class="line x" title="336:730	 The Hansard corpus of protocols of the Canadian Parliament--85 million words." ></td>
	<td class="line x" title="337:730	 Associated Press news items--24 million words." ></td>
	<td class="line x" title="338:730	577 Computational Linguistics Volume 20, Number 4 However, the effective size of the corpora was only about 25 million words, owing to two filtering criteria." ></td>
	<td class="line x" title="339:730	First, we considered only sentences whose length did not exceed 25 words, since longer sentences required excessive parse time and contained many parsing errors." ></td>
	<td class="line x" title="340:730	Second, even 35% of the shorter sentences failed to parse and had to be eliminated." ></td>
	<td class="line x" title="341:730	The syntactic tuples were located by the ESG parser and the postprocessing routine mentioned earlier." ></td>
	<td class="line x" title="342:730	For the purpose of evaluation, we gathered only the data required for the given test examples." ></td>
	<td class="line x" title="343:730	Within a practical machine translation system, the disambiguation module would require a database containing all the syntactic tuples of the corpus with their frequency counts." ></td>
	<td class="line x" title="344:730	In the current research project we did not have the computing resources necessary for constructing the complete database (the major cost being parsing time)." ></td>
	<td class="line x" title="345:730	However, such resources are not needed in order to evaluate the proposed method." ></td>
	<td class="line x" title="346:730	Since we evaluated the method only on a relatively small number of random sentences, we first constructed the set of all 'relevant' target tuples, i.e., tuples that should be considered for the test sentences." ></td>
	<td class="line x" title="347:730	Then we scanned the entire corpus and extracted only sentences that contain both words from at least one of the relevant tuples." ></td>
	<td class="line x" title="348:730	Only the extracted sentences were parsed, and their counts were recorded in our database." ></td>
	<td class="line x" title="349:730	Even though this database is much smaller than the full database, for the ambiguous words of the test sentences, both databases provide the same information." ></td>
	<td class="line x" title="350:730	Thus, the success rate for the test sentences is the same for both methods, while requiring a considerably smaller amount of resources at the research phase." ></td>
	<td class="line x" title="351:730	The problem with this method is that for every set of sample sentences the entire corpus has to be scanned." ></td>
	<td class="line x" title="352:730	Thus, a practical system would have to preprocess the corpus to construct a database of the entire corpus." ></td>
	<td class="line x" title="353:730	Then, to resolve ambiguities, only this database need be consulted." ></td>
	<td class="line x" title="354:730	After acquiring all the relevant data, the algorithm of Section 3.3 was executed for each of the test sentences." ></td>
	<td class="line x" title="355:730	5." ></td>
	<td class="line x" title="356:730	Evaluation Two measurements, applicability and precision, are used to evaluate the performance of the algorithm." ></td>
	<td class="line x" title="357:730	The applicability (coverage) denotes the proportion of cases for which the model performed a selection, i.e., those cases for which the bound B~ passed the threshold." ></td>
	<td class="line x" title="358:730	The precision denotes the proportion of cases for which the model performed a correct selection out of all the applicable cases." ></td>
	<td class="line x" title="359:730	We compare the precision of our method, which we term TWS (for Target Word Selection), with that of the Word Frequencies procedure, which always selects the most frequent target word." ></td>
	<td class="line x" title="360:730	In other words, the Word Frequencies method prefers the alternative that has the highest a priori probability of appearing in the target language corpus." ></td>
	<td class="line x" title="361:730	This naive 'straw-man' is less sophisticated than other methods suggested in the literature, but it is useful as a common benchmark since it can be easily implemented." ></td>
	<td class="line x" title="362:730	The success rate of the Word Frequencies procedure can serve as a measure for the degree of lexical ambiguity in a given set of examples, and thus different methods can be partly compared by their degree of success relative to this procedure." ></td>
	<td class="line x" title="363:730	Out of the 103 ambiguous Hebrew words, for 33 the bound B~ did not pass the threshold, achieving an applicability of 68%." ></td>
	<td class="line x" title="364:730	The remaining 70 examples were distributed according to Table 2." ></td>
	<td class="line x" title="365:730	Thus the precision of the statistical model was 91% 578 Ido Dagan and Alon Itai Word Sense Disambiguation Table 2 Hebrew-English translation: Comparison of TWS and Word Frequencies methods for the 70 applicable examples Word Frequencies Correct Incorrect Total Correct 42 22 64 TWS Incorrect 2 4 6 Total 44 26 70 (64/70), 1 whereas relying just on Word Frequencies yields 63% (44/70), providing an improvement of 28%." ></td>
	<td class="line x" title="366:730	The table demonstrates that our algorithm corrects 22 erroneous decisions of the Word Frequencies method, but makes only 2 errors that the Word Frequencies method translates correctly." ></td>
	<td class="line x" title="367:730	This implies that with high confidence our method greatly improves the Word Frequencies method." ></td>
	<td class="line x" title="368:730	The number of Hebrew examples is large enough to permit a meaningful analysis of the statistical significance of the results." ></td>
	<td class="line x" title="369:730	By computing confidence intervals for the distribution of proportions, we claim that with 95% confidence our method succeeds in at least 86% of the applicable examples." ></td>
	<td class="line x" title="370:730	This means that though the figure of 91% might be due to a lucky selection of the random examples, there is only a 5% chance that the real figure is less than 86% (for the given domain and corpus)." ></td>
	<td class="line x" title="371:730	The confidence interval was computed as follows: p~f~_Zl_c~f~) 64 f-~4 . 6 -70 1'65V 7-7-07 0'86' where a = 0.05 and the variance is estimated by \]~(1 f))/n. With the same confidence, our method improves the Word Frequencies method by at least 18% (relative to the actual improvement of 28% in the given test set)." ></td>
	<td class="line x" title="372:730	Let Pl be the proportion of cases for which our method succeeds and the Word Frequencies method fails (Pl = 22/70) and P2 be the proportion of cases for which the Word Frequencies method succeeds and ours fails (P2 = 2/70)." ></td>
	<td class="line x" title="373:730	The confidence interval is for the difference of proportions in multinomial distribution and is computed as follows: Pl -P2 (-lYl -P2 -Zl-o~ v/var(151 152) = ~1 -/~2 Zl_c~ --~ V/~t (1 ~t) + ~2(1 ~2) + 2~1~2 22 2 65 1 4/22." ></td>
	<td class="line x" title="374:730	(70-22)+2.(70-2)+2.22.2 =0.18." ></td>
	<td class="line x" title="375:730	70 70 1." ></td>
	<td class="line x" title="376:730	~V 702 Out of the 54 ambiguous German words, for 27 the bound B~ did not pass the threshold (applicability of 50%)." ></td>
	<td class="line x" title="377:730	The remaining 27 examples were distributed according to Table 3." ></td>
	<td class="line x" title="378:730	Thus, the precision of the statistical model was 78% (21/27), whereas 10 An a posteriori observation showed that in three of the six errors the selection of the model was actually acceptable, and the a priori judgment of the human translator was too restrictive." ></td>
	<td class="line x" title="379:730	For example, in one of these cases the statistics selected the expression 'to begin the talks,' whereas the human translator regarded this expression as incorrect and selected 'to start the talks'." ></td>
	<td class="line x" title="380:730	If we consider these cases as correct, then there are only three selection errors, getting 96% precision." ></td>
	<td class="line x" title="381:730	579 Computational Linguistics Volume 20, Number 4 Table 3 German-English translation: Comparison of TWS and Word Frequencies methods for the 27 applicable examples Word Frequencies Correct Incorrect Total Correct 15 6 21 TWS Incorrect 0 6 6 Total 15 12 27 relying just on Word Frequencies yields 56% (15/27)." ></td>
	<td class="line x" title="382:730	Here our method corrected 6 errors of the Word Frequencies method, without causing any new errors." ></td>
	<td class="line x" title="383:730	We attribute the lower success rate for the German examples to the fact that they were not restricted to topics that are well represented in the corpus." ></td>
	<td class="line x" title="384:730	This poor correspondence between the training and testing texts is reflected also by the low precision of the Word Frequencies method." ></td>
	<td class="line x" title="385:730	This means that the a priori probability of the target words, as estimated from the training corpora, provides a very poor prediction of the correct selection in the test examples." ></td>
	<td class="line x" title="386:730	Relative to the a priori probability, the precision of our method is still 22% higher." ></td>
	<td class="line x" title="387:730	5.1 Additional Results Recently, Dagan, Marcus, and Markovitch have implemented a variant of the disambiguation method of the current paper." ></td>
	<td class="line x" title="388:730	This variant was developed for evaluating a method that estimates the probability of word combinations which do not occur in the training corpus (Dagan, Marcus, and Markovitch 1993)." ></td>
	<td class="line x" title="389:730	In this section we quote their results, providing additional evidence for the effectiveness of the TWS method." ></td>
	<td class="line x" title="390:730	The major difference between the TWS method, as presented in this paper, and the variant described by Dagan, Marcus, and Markovitch (1993), which we term TWS ~, is that the latter does not use any parsing for collecting the statistics from the corpus." ></td>
	<td class="line x" title="391:730	Instead, the counts of syntactic tuples are approximated by counting co-occurrences of the given words of the tuple within a short distance in a sentence." ></td>
	<td class="line x" title="392:730	The approximation takes into account the relative order between the words of the tuple, such that occurrences of a certain syntactic relation are approximated only by word co-occurrences that preserve the most frequent word order for that relation (e.g. , an adjective precedes the noun it modifies)." ></td>
	<td class="line x" title="393:730	The TWS ~ method still assumes that the source sentence to be translated is being parsed, in order to identify the words that are syntactically related to an ambiguous word." ></td>
	<td class="line x" title="394:730	This model is therefore relevant for translation systems that use a parser for the source language, but may not have available a robust target language parser." ></td>
	<td class="line x" title="395:730	The corpus used for evaluating the TWS' method consists of articles posted to the USENET news system." ></td>
	<td class="line x" title="396:730	The articles were collected from news groups that discuss computer-related topics." ></td>
	<td class="line x" title="397:730	The length of the corpus is 8,871,125 words (tokens), and the lexicon size (distinct types, at the string level) is 95,559." ></td>
	<td class="line x" title="398:730	The type of text in this corpus is quite noisy, including short and incomplete sentences as well as much irrelevant information, such as person and device names." ></td>
	<td class="line x" title="399:730	The test set used for the experiment consists of 78 Hebrew sentences that were taken out of a book about computers." ></td>
	<td class="line x" title="400:730	These sentences were processed as described in Section 4, obtaining a set of 269 ambiguous Hebrew words." ></td>
	<td class="line x" title="401:730	The average number of alternative translations per ambiguous word in this set is 5.8, and there are 1.35 correct translations." ></td>
	<td class="line x" title="402:730	580 Ido Dagan and Alon Itai Word Sense Disambiguation Table 4 Comparison of TWS' and Word Frequencies methods for the 173 applicable examples Word Frequencies correct incorrect Total correct 120 28 148 TWS' incorrect 3 22 25 Total 123 50 173 Out of the 269 ambiguous Hebrew words, for 96 the bound B~ did not pass the threshold, achieving an applicability of 64.3%." ></td>
	<td class="line x" title="403:730	The remaining 173 examples were distributed according to Table 4." ></td>
	<td class="line x" title="404:730	For the words that are covered by the TWS' method, the Word Frequencies method has a precision of 71.1% (123/173), whereas the TWS' method has a precision of 85.5%(148/173)." ></td>
	<td class="line x" title="405:730	As can be seen in the table, the TWS' method is correct in almost all the cases it disagrees with the Word Frequencies method (28 out of 31)." ></td>
	<td class="line x" title="406:730	The applicability and precision figures in this experiment are somewhat lower than those achieved for the Hebrew set in our original evaluation of the TWS method (Table 2)." ></td>
	<td class="line x" title="407:730	We attribute this to the fact that the original results were achieved using a parsed corpus, which was about 2.5 times larger and of much higher quality than the one used in the second experiment." ></td>
	<td class="line x" title="408:730	Yet, the new results give additional support for the usefulness of the TWS method, even for noisy data provided by a low quality corpus, without any parsing or tagging, u 6." ></td>
	<td class="line x" title="409:730	Analysis and Possible Enhancements In this section we give a detailed analysis of the selections performed by the algorithm and, in particular, analyze the cases when it failed." ></td>
	<td class="line x" title="410:730	The analysis of these modes suggests possible improvements of the model and indicates its limitations." ></td>
	<td class="line x" title="411:730	As described earlier, the algorithm's failure includes either the cases for which the method was not applicable (no selection), or the cases for which it made an incorrect selection." ></td>
	<td class="line x" title="412:730	The following paragraphs list various reasons for both types." ></td>
	<td class="line x" title="413:730	At the end of the section, we discuss the possibility of adapting our approach to monolingual applications." ></td>
	<td class="line x" title="414:730	6.1 Correct Selection In the cases that were treated correctly by our method, such as the examples given in the previous sections, the statistics succeeded in capturing two major types of disambiguating data." ></td>
	<td class="line x" title="415:730	In preferring 'sign-treaty' upon 'seal-treaty' (in Example 1), the statistics reflect the relevant semantic constraint." ></td>
	<td class="line x" title="416:730	In preferring 'peace-treaty' upon 'peacecontract,' the statistics reflect the lexical usage of 'treaty' in English which differs from the usage of 'contract'." ></td>
	<td class="line x" title="417:730	6.2 Inapplicability 6.2.1 Insufficient Data." ></td>
	<td class="line x" title="418:730	This was the reason for nearly all the cases of inapplicability." ></td>
	<td class="line x" title="419:730	In one of our examples, for instance, none of the alternative relations, 'an investigator of corruption' (the correct one) or 'researcher of corruption' (the incorrect one), 11 It should be mentioned that the work of Dagan, Marcus, and Markovitch (1993) includes further results, evaluating an enhancement of the TWS method using their similarity-based estimation method." ></td>
	<td class="line x" title="420:730	This enhancement is beyond the scope of the current paper and is referred to in the next section." ></td>
	<td class="line x" title="421:730	581 Computational Linguistics Volume 20, Number 4 was observed in the parsed corpus." ></td>
	<td class="line x" title="422:730	In this case it is possible to perform the correct selection if we used only statistics about the co-occurrence of 'corruption' with either 'investigator' or 'researcher' in the same local context, without requiring any syntactic relation." ></td>
	<td class="line oc" title="423:730	Statistics on co-occurrence of words in a local context were used recently for monolingual word sense disambiguation (Gale, Church, and Yarowsky 1992b, 1993; Sch6tze 1992, 1993) (see Section 7 for more details and Church and Hanks 1990; Smadja 1993, for other applications of these statistics)." ></td>
	<td class="line o" title="424:730	It is possible to apply these methods using statistics of the target language and thus incorporate them within the framework proposed here for target word selection." ></td>
	<td class="line x" title="425:730	Finding an optimal way of combining the different methods is a subject for further research." ></td>
	<td class="line x" title="426:730	Our intuition, though, as well as some of our initial data, suggests that statistics on word co-occurrence in the local context can substantially increase the applicability of the selection method." ></td>
	<td class="line x" title="427:730	Another way to deal with the lack of statistical data for the specific words in question is to use statistics about similar words." ></td>
	<td class="line x" title="428:730	This is the basis for Sadler's Analogical Semantics (Sadler 1989), which according to his report has not proved effective." ></td>
	<td class="line x" title="429:730	His results may be improved if more sophisticated methods and larger corpora are used to establish similarity between words (such as in Hindle 1990)." ></td>
	<td class="line x" title="430:730	In particular, an enhancement of our disambiguation method, using similarity-based estimation (Dagan, Marcus, and Markovitch 1993), was evaluated recently." ></td>
	<td class="line x" title="431:730	In this evaluation the applicability of the disambiguation method was increased by 15%, with only a slight decrease in the precision." ></td>
	<td class="line x" title="432:730	The increased applicability was achieved by disambiguating additional cases in which statistical data were not available for any of the alternative tuples, whereas data were available for other tuples containing similar words." ></td>
	<td class="line x" title="433:730	6.2.2 Conflicting Data." ></td>
	<td class="line x" title="434:730	In very few cases two alternatives were supported equally by the statistical data, thus preventing a selection." ></td>
	<td class="line x" title="435:730	In such cases, both alternatives are valid at the independent level of the syntactic relation, but may be inappropriate for the specific context." ></td>
	<td class="line x" title="436:730	For instance, the two alternatives of 'to take a job' or 'to take a position' appeared in one of the examples, but since the general context was about the position of a prime minister, only the latter was appropriate." ></td>
	<td class="line x" title="437:730	To resolve such ambiguities, it may be useful to consider also co-occurrences of the ambiguous word with other words in the broader context (e.g. , Gale, Church, and Yarowsky 1993; Yarkowsky 1992)." ></td>
	<td class="line x" title="438:730	For instance, the word 'minister' seems to co-occur in the same context more frequently with 'position' than with 'job'." ></td>
	<td class="line x" title="439:730	In another example both alternatives were appropriate also for the specific context." ></td>
	<td class="line x" title="440:730	This happened with the German verb werfen, which may be translated (among other options) as 'throw,' 'cast,' or 'score'." ></td>
	<td class="line x" title="441:730	In our example, werfen, appeared in the context of 'to throw/cast light,' and these two correct alternatives had equal frequencies in the corpus ('score' was successfully eliminated): In such situations any selection between the alternatives will be appropriate, and therefore, any algorithm that handles conflicting data would work properly." ></td>
	<td class="line x" title="442:730	However, it is difficult to decide automatically when both alternatives are acceptable and when only one of them is. 6.3 Incorrect Selection 6.3.1 Using an Inappropriate Relation." ></td>
	<td class="line x" title="443:730	One of the examples contained the Hebrew word matzav." ></td>
	<td class="line x" title="444:730	This word has several translations, two of which are 'state' and 'position'." ></td>
	<td class="line x" title="445:730	The phrase that contained this word was 'to put an end to the {statelposition } of war'." ></td>
	<td class="line x" title="446:730	The ambiguous word is involved in two syntactic relations, being a complement of 'put' and also modified by 'war'." ></td>
	<td class="line x" title="447:730	The corresponding frequencies were 582 Ido Dagan and Alon Itai Word Sense Disambiguation (15) (verb-comp: put-position) 320 (verb-comp: put-state) 18 (noun-nobj: state-war) 13 (noun-nobj: position-war) 2 The bound of the odds ratio (B~) for the first relation was higher than for the second, and therefore, this relation determined the translation as 'position'." ></td>
	<td class="line x" title="448:730	However, the correct translation should be 'state', as determined by the second relation." ></td>
	<td class="line x" title="449:730	These data suggest that while ordering the relations (or using any other Weighting mechanism) it may be necessary to give different weights to the different types of syntactic relations." ></td>
	<td class="line x" title="450:730	For instance, it seems reasonable that the object of a noun should receive greater weight in selecting the noun's sense than the verb for which this noun serves as a complement." ></td>
	<td class="line x" title="451:730	Further examination of the example suggests another refinement of our method: it turns out that most of the 320 instances of the tuple (verb-comp: put position) include the preposition 'in,' as part of the common phrase 'put in a position'." ></td>
	<td class="line x" title="452:730	Therefore, these instances should not be considered for the current example, which includes the preposition 'to'." ></td>
	<td class="line x" title="453:730	However, the distinction between different prepositions was lost by our program, as a result of using equivalence classes of syntactic tuples (see Section 2.3)." ></td>
	<td class="line x" title="454:730	This suggests that we should not use an equivalence class when there is enough statistical data for specific tuples." ></td>
	<td class="line x" title="455:730	12 6.3.2 Confusing Senses." ></td>
	<td class="line x" title="456:730	In another example, the Hebrew adjective qatann modified the noun sikkuy, which means 'prospect' or 'chance'." ></td>
	<td class="line x" title="457:730	The word qatann has several translations, two of which are 'small' and 'young'." ></td>
	<td class="line x" title="458:730	In this Hebrew word combination, the correct sense of qatann is necessarily 'small'." ></td>
	<td class="line x" title="459:730	However, the relation that was observed in the corpus was 'young prospect,' relating to the human sense of 'prospect' that appeared in sports articles (a promising young person)." ></td>
	<td class="line x" title="460:730	This borrowed sense of 'prospect' is necessarily inappropriate, since in Hebrew it is represented by the equivalent of 'hope' (tiqwa) and not by sikkuy." ></td>
	<td class="line x" title="461:730	The source of this problem is Assumption 3: a target tuple T might be a translation of several source tuples, and while gathering statistics for T, we cannot distinguish between the different sources, since we use only a target language corpus." ></td>
	<td class="line x" title="462:730	A possible solution is to use an aligned bilingual corpus, as suggested by Sadler (1989), Brown et al.(1991), and Gale et al.(1992a)." ></td>
	<td class="line x" title="465:730	In such a corpus the occurrences of the relation 'young prospect' will be aligned to the corresponding occurrences of the Hebrew word tiqwa and will not be used when the Hebrew word sikkuy is involved." ></td>
	<td class="line x" title="466:730	Yet, it should be brought to mind that an aligned corpus is the result of manual translation, which can be viewed as including a manual tagging of the ambiguous words with their equivalent senses in the target language." ></td>
	<td class="line x" title="467:730	This resource is much more expensive and less available than an untagged monolingual corpus, and it seems to be necessary only for relatively rare situations." ></td>
	<td class="line x" title="468:730	Therefore, considering the trade-off between applicability and precision, it seems better to rely on a significantly larger monolingual corpus than on a smaller bilingual corpus." ></td>
	<td class="line x" title="469:730	An optimal method may exploit both types of corpora, in which the somewhat more accurate, but more expensive, data of a bilingual corpus are augmented by the data of a much larger monolingual corpus." ></td>
	<td class="line x" title="470:730	13 12 We thank the anonymous reviewer for suggesting this point." ></td>
	<td class="line x" title="471:730	13 Even though there are large quantities of translated texts, experience has shown that it is much harder to obtain large bilingual corpora than large monolingual corpora." ></td>
	<td class="line x" title="472:730	As mentioned earlier, a bilingual 583 Computational Linguistics Volume 20, Number 4 6.3.3 Lack of Deep Understanding." ></td>
	<td class="line x" title="473:730	By their nature, statistical methods rely on large quantities of shallow information." ></td>
	<td class="line x" title="474:730	Thus, they are doomed to fail when disambiguation can rely only on deep understanding of the text and no other surface cues are available." ></td>
	<td class="line x" title="475:730	This happened in one of the Hebrew examples, in which the two alternatives were either 'emigration law' or 'immigration law' (the Hebrew word hagira is used for both subsenses)." ></td>
	<td class="line x" title="476:730	While the context indicated that the first alternative is correct (emigration from the Soviet Union), the statistics (which were extracted from texts related to North America) preferred the second alternative." ></td>
	<td class="line x" title="477:730	To translate the above phrase, the program would need deep knowledge, to an extent that seems to far exceed the capabilities of current systems." ></td>
	<td class="line x" title="478:730	Fortunately, our results suggest that such cases are quite rare." ></td>
	<td class="line x" title="479:730	6.4 Monolingual Applications The results of our experiments in the context of machine translation suggest the utility of a similar mechanism even for in word sense disambiguation within a single language." ></td>
	<td class="line x" title="480:730	To select the right sense of a word, in a broad coverage application, it is useful to identify lexical relations between word senses." ></td>
	<td class="line x" title="481:730	However, within corpora of a single language it is possible to identify automatically only relations at the word level, which are, of course, not useful for selecting word senses in that language." ></td>
	<td class="line x" title="482:730	This is where other languages can supply the solution, exploiting the fact that the mapping between words and word senses varies significantly between different languages." ></td>
	<td class="line x" title="483:730	For instance, the English words 'sign' and 'seal' (from Example 1 in the introduction) correspond to two distinct senses of the Hebrew word lahtom." ></td>
	<td class="line x" title="484:730	These senses should be distinguished by most applications of Hebrew understanding programs." ></td>
	<td class="line x" title="485:730	To make this distinction, it is possible to perform the same process that is performed for target word selection, by producing all the English alternatives for the lexical relations involving lahtom." ></td>
	<td class="line x" title="486:730	Then the Hebrew sense that corresponds to the most plausible English lexical relations is preferred." ></td>
	<td class="line x" title="487:730	This process requires a bilingual lexicon that maps each Hebrew sense separately into its possible translations, similar to a Hebrew-Hebrew-English lexicon (analogous to the Oxford English-English-Hebrew dictionary of Hornby et al. \[1986\], which lists the senses of an English word, along with the possible Hebrew translations for each of them)." ></td>
	<td class="line x" title="488:730	In some cases, different senses of a Hebrew word map to the same word also in English." ></td>
	<td class="line x" title="489:730	In these cases, the lexical relations of each sense cannot be identified in an English corpus, and a third language is required to distinguish among these senses." ></td>
	<td class="line x" title="490:730	Alternatively, it is possible to combine our method with other disambiguation methods that have been developed in a monolingual context (see the next section)." ></td>
	<td class="line x" title="491:730	As a long-term vision, one can imagine a multilingual corpora-based environment, which exploits the differences between languages to facilitate the acquisition of knowledge about word senses." ></td>
	<td class="line x" title="492:730	7." ></td>
	<td class="line x" title="493:730	Comparative Analysis of Statistical Sense Disambiguation Methods Until recently, word sense disambiguation seemed to be a problem for which there is no satisfactory solution for broad coverage applications." ></td>
	<td class="line x" title="494:730	Recently, several statistical methods have been developed for solving this problem, suggesting the possibility of robust, yet feasible, disambiguation." ></td>
	<td class="line x" title="495:730	In this section we identify and analyze basic aspects of a statistical sense disambiguation method and compare several proposed corpus of moderate size can be valuable when constructing a bilingual lexicon, thus justifying the effort of maintaining such a corpus." ></td>
	<td class="line x" title="496:730	584 Ido Dagan and Alon Itai Word Sense Disambiguation methods (including ours) along these aspects." ></td>
	<td class="line x" title="497:730	TM This analysis may be useful for future research on sense disambiguation, as well as for the development of sense disambiguation modules in practical systems." ></td>
	<td class="line x" title="498:730	The basic aspects that will be reviewed are . 2." ></td>
	<td class="line x" title="499:730	3. 4." ></td>
	<td class="line x" title="500:730	Information sources used by the disambiguation method." ></td>
	<td class="line x" title="501:730	Acquisition of the required information from training texts." ></td>
	<td class="line x" title="502:730	The computational decision model." ></td>
	<td class="line x" title="503:730	Performance evaluation." ></td>
	<td class="line x" title="504:730	The first three aspects deal with the components of a disambiguation method, as would be implemented for a practical application." ></td>
	<td class="line x" title="505:730	The fourth is a methodological issue, which is relevant for developing, testing, and comparing disambiguation methods." ></td>
	<td class="line x" title="506:730	7.1 Information Sources We identify three major types of information that were used in statistical methods for sense disambiguation: . . 3." ></td>
	<td class="line x" title="507:730	Words appearing in the local, syntactically related, context of the ambiguous word." ></td>
	<td class="line x" title="508:730	Words appearing in the global context of the ambiguous word." ></td>
	<td class="line x" title="509:730	Probabilistic syntactic and morphological characteristics of the ambiguous word." ></td>
	<td class="line x" title="510:730	The first type of information is the one used in the current paper, in which words that are syntactically related to an ambiguous word are used to indicate its most probable sense." ></td>
	<td class="line x" title="511:730	Statistical data on the co-occurrence of syntactically related words with each of the alternative senses reflect semantic and lexical preferences and constraints of these senses." ></td>
	<td class="line x" title="512:730	In addition, these statistics may provide information about the topics of discourse that are typical for each sense." ></td>
	<td class="line x" title="513:730	Ideally, the syntactic relations between words should be identified using a syntactic parser, in both the training and the disambiguation phases." ></td>
	<td class="line x" title="514:730	Since robust syntactic parsers are not widely available, and those that exist are not always accurate, it is possible to use various approximations to identify relevant syntactic relations between words." ></td>
	<td class="line x" title="515:730	Hearst (1991) uses a stochastic part of speech tagger and a simple scheme for partial parsing of short phrases." ></td>
	<td class="line x" title="516:730	The structures achieved by this analysis are used to identify approximated syntactic relations between words." ></td>
	<td class="line x" title="517:730	Brown et al.(1991) make even weaker approximations, using only a stochastic part of speech tagger, and defining relations such as 'the first verb to the right' or 'the first noun to the left'." ></td>
	<td class="line x" title="519:730	Finally, Dagan et al.(1993) (see Section 5.1) assume full parsing at the disambiguation phase, but no preprocessing at the training phase, in which a higher level of noise can be accommodated." ></td>
	<td class="line x" title="521:730	A second type of information is provided by words that occur in the global context of the ambiguous word (Gale, Church, and Yarowsky 1992b, 1993; Yarowsky 1992; Sch6tze 1992)." ></td>
	<td class="line x" title="522:730	Gale et al. and Yarowsky use words that appear within 50 words in each 14 The reader is referred to some of these recent papers for thorough surveys of work on sense disambiguation (Hearst 1991; Gale, Church, and Yarowsky 1992a; Yarowsky 1992)." ></td>
	<td class="line x" title="523:730	585 Computational Linguistics Volume 20, Number 4 direction of the ambiguous word." ></td>
	<td class="line x" title="524:730	is Statistical data are stored about the occurrence of words in the context of each sense and are matched against the context in the disambiguated sentence." ></td>
	<td class="line x" title="525:730	Co-occurrence in the global context provides information about typical topics associated with each sense, in which a topic is represented by words that commonly occur in it." ></td>
	<td class="line x" title="526:730	Schiitze (1992, 1993) uses a variant of this type of information, in which contextvectors are maintained for character four-grams, instead of words." ></td>
	<td class="line x" title="527:730	In addition, the context of an occurrence of an ambiguous word is represented by co-occurrence information of a second order, as a set of context vectors (instead of a set of context words)." ></td>
	<td class="line x" title="528:730	Compared with co-occurrence within syntactic relations, information about the global context is less sensitive to fine semantic and lexical distinctions and is less useful when different senses of a word appear in similar contexts." ></td>
	<td class="line x" title="529:730	On the other hand, the global context contains more words and is therefore more likely to provide enough disambiguating information, in cases in which this distinction can be based ~on the topic of discourse." ></td>
	<td class="line x" title="530:730	From a general perspective, these two types of information represent a common trade-off in statistical language processing: the first type is related to a limited amount of deeper, and more precise linguistic information, whereas the second type provides a large amount of shallow information, which can be applied in a more robust manner." ></td>
	<td class="line x" title="531:730	The two sources of information seem to complement each other and may both be combined in future disambiguation methods." ></td>
	<td class="line x" title="532:730	16 Hearst (1991) incorporates a third type of statistical information to distinguish between different senses of nouns (in addition to the first type discussed above)." ></td>
	<td class="line x" title="533:730	For each occurrence of a sense, several syntactic and morphological characteristics are recorded, such as whether the noun modifies or is modified by another word, whether it is capitalized, and whether it is related to certain prepositional phrases." ></td>
	<td class="line x" title="534:730	Then, in the disambiguation phase, a best match is sought between the information recorded for each sense and the syntactic context of the current occurrence of the noun." ></td>
	<td class="line x" title="535:730	This type of information resembles the information that is defined for lexical items in lexicalist approaches for grammars, such as possible subcategorization frames of a word." ></td>
	<td class="line x" title="536:730	The major difference is that Hearst captures probabilistic preferences of senses for such syntactic constructs." ></td>
	<td class="line x" title="537:730	Grammatical formalisms, on the other hand, usually specify only which constructs are possible and at most distinguish between optional and obligatory ones." ></td>
	<td class="line x" title="538:730	Therefore the information recorded in such grammars cannot distinguish between different senses of a word that potentially have the same subcategorization frames, though in practice each sense might have different probabilistic preferences for different syntactic constructs." ></td>
	<td class="line x" title="539:730	It is clear that each of the different types of information provides some information that is not captured by the others." ></td>
	<td class="line x" title="540:730	However, as the acquisition and manipulation of each type of information requires different tools and resources, it is important to assess the relative contribution, and the 'cost effectiveness,' of each of them." ></td>
	<td class="line x" title="541:730	Such comparative evaluations are not available yet, not even for systems that incorporate several types of data (e.g. , McRoy 1992)." ></td>
	<td class="line x" title="542:730	Further research is therefore needed to com15 The size of the context was determined experimentally, based on evaluations of different sizes of context." ></td>
	<td class="line x" title="543:730	This optimization was performed for the Hansard corpus of the proceedings of the Canadian Parliament." ></td>
	<td class="line x" title="544:730	In general, the size of the global context depends on the corpus and typically consists of a homogeneous unit of discourse." ></td>
	<td class="line x" title="545:730	16 See also Gale, Church, and Yarowsky 1992b (pp." ></td>
	<td class="line x" title="546:730	58-59), and Sch~itze, 1992, 1993, for methods of reducing the number of parameters when using global contexts and Dagan, Marcus, and Markovitch 1993, for increasing the applicability of the use of local context, in cases in which there is no direct statistical evidence." ></td>
	<td class="line x" title="547:730	586 Ido Dagan and Alon Itai Word Sense Disambiguation pare the relative importance of different information types and to find optimal ways of combining them." ></td>
	<td class="line x" title="548:730	7.2 Acquisition of Training Information When training a statistical model for sense disambiguation, it is necessary to associate the acquired statistics with word senses." ></td>
	<td class="line x" title="549:730	This seems to require manual tagging of the training corpus with the appropriate sense for each occurrence of an ambiguous word." ></td>
	<td class="line x" title="550:730	A similar approach is being used for stochastic part of speech taggers and probabilistic parsers, relying on the availability of large, manually tagged (or parsed), corpora for training." ></td>
	<td class="line x" title="551:730	However, this approach is less feasible for sense disambiguation, for two reasons." ></td>
	<td class="line x" title="552:730	First, the size of corpora required to acquire sufficient statistics on lexical cooccurrence is usually much larger than that used for acquiring statistics on syntactic constructs or sequences of parts of speech." ></td>
	<td class="line x" title="553:730	Second, lexical co-occurrence patterns, as well as the definition of senses, may vary a great deal across different domains of discourse." ></td>
	<td class="line x" title="554:730	Consequently, it is usually not sufficient to acquire the statistics from one widely available 'balanced' corpus, as is common for syntactic applications." ></td>
	<td class="line x" title="555:730	A sense disambiguation model should be trained on the same type of texts for which it will be applied, thus increasing the cost of manual tagging." ></td>
	<td class="line x" title="556:730	The need to disambiguate a training corpus before acquiring a statistical model for disambiguation is often termed as the circularity problem." ></td>
	<td class="line x" title="557:730	In the following paragraphs we discuss different methods that were proposed to overcome the circularity problem, without exhaustive manual tagging of the training corpus." ></td>
	<td class="line x" title="558:730	In our opinion, this is the most critical issue in developing feasible sense disambiguation methods." ></td>
	<td class="line x" title="559:730	7.2.1 Bootstrapping." ></td>
	<td class="line x" title="560:730	Bootstrapping, which is a general scheme for reducing the amount of manual tagging, was proposed also for sense disambiguation (Hearst 1991)." ></td>
	<td class="line x" title="561:730	The idea is to tag manually an initial set of occurrences for each sense in the lexicon, acquiring initial training statistics from these instances." ></td>
	<td class="line x" title="562:730	Then, using these statistics, the system tries to disambiguate additional occurrences of ambiguous words." ></td>
	<td class="line x" title="563:730	If such an occurrence can be disambiguated automatically with high confidence, the system acquires additional statistics from this occurrence, as if it were tagged by hand." ></td>
	<td class="line x" title="564:730	Hopefully, the system will incrementally acquire all the relevant statistics, demanding just a small amount of manual tagging." ></td>
	<td class="line x" title="565:730	The results of Hearst (1991) show that at least 10 occurrences of each sense have to be tagged by hand, and in most cases 20-30 occurrences are required to get high precision." ></td>
	<td class="line x" title="566:730	These results, which were achieved for a small set of preselected ambiguous words, suggest that the cost of the bootstrapping approach is still very high." ></td>
	<td class="line x" title="567:730	7.2.2 Clustering Occurrences of an Ambiguous Word." ></td>
	<td class="line x" title="568:730	Sch6tze (1992, 1993) proposes a method that can be viewed as an efficient way of manual tagging." ></td>
	<td class="line x" title="569:730	Instead of presenting all occurrences of an ambiguous word to a human, these occurrences are first clustered using automatic clustering algorithms." ></td>
	<td class="line x" title="570:730	17 Then a human is asked to assign one of the senses of the word to each cluster, by observing several members of the cluster." ></td>
	<td class="line x" title="571:730	Each sense is thus represented by one or more clusters." ></td>
	<td class="line x" title="572:730	At the disambiguation phase, a new occurrence of an ambiguous word is matched against the contexts that were recorded for these clusters, selecting the sense of that cluster which provides the best match." ></td>
	<td class="line x" title="573:730	It is interesting to note that the number of occurrences that had to be observed by a human in the experiments of Sch/itze is of the same order as in the bootstrapping 17 Each occurrence is represented as a context vector, and the vectors are then clustered, 587 Computational Linguistics Volume 20, Number 4 approach: 10-20 members of a cluster were observed, with an average of 2.8 clusters per sense." ></td>
	<td class="line x" title="574:730	As both approaches were tested only on a small number of preselected words, further evaluation is necessary to predict the actual cost of their application to broad domains." ></td>
	<td class="line x" title="575:730	The methods described below, on the other hand, rely on resources that were already available on a large scale, and it is therefore possible to estimate the expected cost of their broad application." ></td>
	<td class="line x" title="576:730	7.2.3 Word Classification." ></td>
	<td class="line x" title="577:730	Yarowsky (1992) proposes a method that completely avoids manual tagging of the training corpus." ></td>
	<td class="line x" title="578:730	This is achieved by estimating parameters for classes of words rather than for individual word senses." ></td>
	<td class="line x" title="579:730	In his work, Yarowsky considered the semantic categories defined in Roget's Thesaurus as classes." ></td>
	<td class="line x" title="580:730	He then mapped (manually) each of the senses of an ambiguous word to one or several of the categories under which this word is listed in the thesaurus." ></td>
	<td class="line x" title="581:730	The task of sense disambiguation thus becomes the task of selecting the appropriate category for each occurrence of an ambiguous word." ></td>
	<td class="line x" title="582:730	18 When estimating the parameters of a category/9 any occurrence of a word that belongs to that category is counted as an occurrence of the category." ></td>
	<td class="line x" title="583:730	This means that each occurrence of an ambiguous word is counted as an occurrence of all the categories to which the word belongs and not just the category that corresponds to the specific occurrence." ></td>
	<td class="line x" title="584:730	A substantial amount of noise is introduced by this training method, which is a consequence of the circularity problem." ></td>
	<td class="line x" title="585:730	To avoid the noise, it would be necessary to tag each occurrence of an ambiguous word with the appropriate category." ></td>
	<td class="line x" title="586:730	As explained by Yarowsky, however, this noise can usually be tolerated." ></td>
	<td class="line x" title="587:730	The 'correct' parameters of a certain class are acquired from all its occurrences, whereas the 'incorrect' parameters are distributed through occurrences of many different classes and usually do not produce statistically significant patterns." ></td>
	<td class="line x" title="588:730	To reduce the noise further, Yarowsky uses a system of weights that assigns lower weights to frequent words, since such words may introduce more noise." ></td>
	<td class="line x" title="589:730	2 The word class method thus overcomes the circularity problem by mapping word senses to classes of words." ></td>
	<td class="line x" title="590:730	However, because of this mapping, the method cannot distinguish between senses that belong to the same class, and it also introduces some level of noise." ></td>
	<td class="line x" title="591:730	7.2.4 A Bilingual Corpus." ></td>
	<td class="line x" title="592:730	Brown et al.(1991) were concerned with sense disambiguation for machine translation." ></td>
	<td class="line x" title="594:730	Having a large aligned bilingual corpus available, they noticed that the target word which corresponds to an occurrence of an ambiguous source word can serve as a tag of the appropriate sense." ></td>
	<td class="line x" title="595:730	This kind of tagging provides sense distinctions when different senses of a source word translate to different target words." ></td>
	<td class="line x" title="596:730	For the purpose of translation, these are exactly the cases for which sense distinction is required." ></td>
	<td class="line x" title="597:730	Conceptually, the use of a bilingual corpus does not eliminate (or reduce) manual tagging of the training corpus." ></td>
	<td class="line x" title="598:730	Such a corpus is a result of manual translation, and it is the translator who provides tagging of senses as a side effect of the translation process." ></td>
	<td class="line x" title="599:730	Practically, whenever a bilingual corpus is available, it pro18 In some cases, the Roget index was found to be incomplete, and a missing category had to be added to the list of possibilities for a word." ></td>
	<td class="line x" title="600:730	19 Yarowsky uses statistics on occurrences of specific words in the global context of the category, but the method can be used to collect other types of statistics, such as the co-occurrence of the category with other categories." ></td>
	<td class="line x" title="601:730	20 The method of acquiring parameters from ambiguous occurrences in a corpus, relying on the 'spreading' of noise, can be used in many contexts." ></td>
	<td class="line x" title="602:730	For example, it was used for acquiring statistics for disambiguating prepositional phrase attachments, counting ambiguous occurrences of prepositional phrases as representing both noun-pp and verb-pp constructs (Hindle and Rooth 1991)." ></td>
	<td class="line x" title="603:730	588 Ido Dagan and Alon Itai Word Sense Disambiguation vides a useful source of a sense tagged corpus." ></td>
	<td class="line x" title="604:730	Gale, Church, and Yarowsky (1992a) have also exploited this resource for achieving large amounts of testing and training materials." ></td>
	<td class="line x" title="605:730	7.2.5 A Bilingual Lexicon and a Monolingual Corpus." ></td>
	<td class="line x" title="606:730	The method of the current paper also exploits the fact that different senses of a word are usually mapped to different words in another language." ></td>
	<td class="line x" title="607:730	However, our work shows that the differences between languages enable us to avoid any form of manual tagging of the corpus (including translation)." ></td>
	<td class="line x" title="608:730	This is achieved by a bilingual lexicon that maps a source language word to all its possible equivalents in the target language." ></td>
	<td class="line x" title="609:730	This approach has practical advantages for the purpose of machine translation, in which a bilingual lexicon needs to be constructed in any case, and very large bilingual corpora are not usually available." ></td>
	<td class="line x" title="610:730	From a theoretical point of view, the difference between the two methods can be made clear if we assume that the bilingual lexicon contains exactly all the different translations of a word which occur in a bilingual corpus." ></td>
	<td class="line x" title="611:730	For a given set of senses that need to be disambiguated, our method requires a bilingual corpus of size k, in which each sense occurs at least once, in order to establish its mapping to a target word." ></td>
	<td class="line x" title="612:730	In addition, a larger monolingual corpus, of size n, is required, to provide enough training examples of typical contexts for each sense." ></td>
	<td class="line x" title="613:730	On the other hand, using a bilingual corpus for training the disambiguation model would require a bilingual corpus of size n, which is significantly larger than k. The savings in resources is achieved since the mapping between the languages is done at the level of single words." ></td>
	<td class="line x" title="614:730	The larger amount of information about word combinations, on the other hand, is acquired from an untagged monolingual corpus, after the mapping has been performed." ></td>
	<td class="line x" title="615:730	Our results show that the precision of the selection algorithm is high despite the additional noise which is introduced by mapping single words independently of their context." ></td>
	<td class="line x" title="616:730	As mentioned in Section 6.3, an optimal method may combine the two methods." ></td>
	<td class="line x" title="617:730	In some sense, the use of a bilingual lexicon resembles the use of a thesaurus in Yarowsky's approach." ></td>
	<td class="line x" title="618:730	Both rely on a manually established mapping of senses to other concepts (classes of words or words in another language) and collect information about the target concepts from an untagged corpus." ></td>
	<td class="line x" title="619:730	In both cases, ambiguous words in the corpus introduce some level of noise: counting an occurrence of a word as an occurrence of all the classes to which it belongs, or counting an occurrence of a target word as an occurrence of all the source words to which it may correspond (a smaller amount of noise is introduced in the latter case, as a mapping to target words is much more finely grained than a mapping to Roget's categories)." ></td>
	<td class="line x" title="620:730	Also, both methods can distinguish only between senses that are distinguished by the mappings they use: either senses that belong to different classes, or senses that correspond to different target words." ></td>
	<td class="line x" title="621:730	An interesting difference, though, relates to the feasibility of implementing the two methods for a new domain of texts (in particular technical domains)." ></td>
	<td class="line x" title="622:730	The construction of a bilingual lexicon for a new domain is relatively straightforward and is often carried out for translation purposes." ></td>
	<td class="line x" title="623:730	The construction of an appropriate classification for the words of a new domain is more complex, and furthermore, it is not clear whether it is possible in every domain to construct a classification that is sufficient for the purpose of sense disambiguation." ></td>
	<td class="line x" title="624:730	7.3 The Computational Decision Model Sense disambiguation methods require a decision model that evaluates the relevant statistics." ></td>
	<td class="line x" title="625:730	Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works." ></td>
	<td class="line x" title="626:730	These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance 589 Computational Linguistics Volume 20, Number 4 metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991); and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper)." ></td>
	<td class="line x" title="627:730	At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others." ></td>
	<td class="line x" title="628:730	21 Yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them." ></td>
	<td class="line x" title="629:730	As will be explained below, many of the differences are correlated with the different information sources employed by these models." ></td>
	<td class="line x" title="630:730	 Combining several informants: The methods described by Brown et al.(1991) and in the current paper combine several informants (i.e. , statistics about several context words) by choosing the informant that seems most indicative for the selection." ></td>
	<td class="line x" title="632:730	The effect of other, less significant, informants is then discarded." ></td>
	<td class="line x" title="633:730	The Bayesian classifier and the vector distance metric combine all informants simultaneously, in a multiplicative or additive manner, possibly assigning a certain weight to each informant." ></td>
	<td class="line x" title="634:730	 Reducing the number of parameters: Since sense disambiguation relies on statistics about lexical co-occurrence, the number of relevant parameters is very high, especially when co-occurrence in the global context is considered." ></td>
	<td class="line x" title="635:730	For this reason, Schiitze uses two compaction methods: First, 5000 'informative' four-grams are used instead of words." ></td>
	<td class="line x" title="636:730	Second, the 5000 dimensions are decomposed to 97 dimensions, using singular value decomposition." ></td>
	<td class="line x" title="637:730	This method reduces the number of parameters significantly, but has the disadvantage that it is impossible to trace the meaning of the entries in the resulting vectors or to associate them directly with the original co-occurrence statistics." ></td>
	<td class="line x" title="638:730	Gale, Church, and Yarowsky (1992b, pp." ></td>
	<td class="line x" title="639:730	58-59) propose another approach and reduce the number of parameters by selecting the most informative context words for each sense." ></td>
	<td class="line x" title="640:730	The selection of context words is based on a theoretically motivated criterion, borrowed from Mosteller and Wallace (1964, pp." ></td>
	<td class="line x" title="641:730	55-56)." ></td>
	<td class="line x" title="642:730	Finally, Yarowsky's method further reduces the number of parameters, as it records co-occurrences between individual words and word classes." ></td>
	<td class="line x" title="643:730	 Statistical significance of the selection: In the current paper, we use confidence intervals to test whether the statistical preference for a certain sense is significant." ></td>
	<td class="line x" title="644:730	In a simple multiplicative preference score, on the other hand, it is not possible to distinguish whether preferences rely on small or large counts." ></td>
	<td class="line x" title="645:730	The method of Gale et al. remedies this problem indirectly (in most cases) by introducing a sophisticated interpolation between the actual counts of the co-occurrence parameters and the frequency counts of the individual words (see Gale, Church, and Yarowsky 1993, for details)." ></td>
	<td class="line x" title="646:730	In Schiitze's method it is not possible to trace the statistical significance of the parameters since they are the result of extensive processing and compaction of the original statistical data." ></td>
	<td class="line x" title="647:730	21 Once the important information sources for sense selection have been identified, it is possible that different decision algorithms would achieve comparable results." ></td>
	<td class="line x" title="648:730	590 Ido Dagan and Alon Itai Word Sense Disambiguation Resolving all ambiguities simultaneously: In the current paper, the selection of a sense for one word affects the selection for another word through a constraint propagation algorithm." ></td>
	<td class="line x" title="649:730	This property is absent in most other methods." ></td>
	<td class="line x" title="650:730	The differences between various disambiguation methods correlate with the difference in information sources, in particular, whether they use local or global context." ></td>
	<td class="line x" title="651:730	When local context is used, only few syntactically related informants may provide reliable information about the selection." ></td>
	<td class="line x" title="652:730	It is therefore reasonable to base the selection on only one, the most informative informant, and it is also important to test the statistical significance of that informant." ></td>
	<td class="line x" title="653:730	The problem of parameter explosion is less severe, and the number of parameters is comparable to that of a bi-gram language model (and even smaller)." ></td>
	<td class="line x" title="654:730	When using the global context, on the other hand, the number of potential parameters is significantly larger, but each of them is usually less informative." ></td>
	<td class="line x" title="655:730	It is therefore important to take into account as many parameters as possible in each ambiguous case, but it is less important to test for detailed statistical significance, or to worry about the mutual effects of sense selections for adjacent words." ></td>
	<td class="line x" title="656:730	7.4 Performance Evaluation In most of the above-mentioned papers, experimental results are reported for a small set of up to 12 preselected words, usually with two or three senses per word." ></td>
	<td class="line x" title="657:730	In the current paper we have evaluated our method using a random set of example sentences, with no a priori selection of the words." ></td>
	<td class="line x" title="658:730	This standard evaluation method, which is commonly used for other natural language processing tasks, provides a direct prediction for the expected success rate of the method when employed in a practical application." ></td>
	<td class="line x" title="659:730	To compare results on different test data, it is useful to compare the precision of the disambiguation method with some a priori figure that reflects the degree of ambiguity in the text." ></td>
	<td class="line x" title="660:730	Reporting the number of senses per example word corresponds to the expected success rate of random selection." ></td>
	<td class="line x" title="661:730	A more informative figure is the success rate of a naive method that always selects the most frequent sense (the Word Frequencies method in our evaluations)." ></td>
	<td class="line x" title="662:730	The success rate of this naive method is higher than that of random selection and thus provides a tighter lower bound for the desired precision of a proposed disambiguation method." ></td>
	<td class="line x" title="663:730	An important practical issue in evaluation is how to get the test examples, which should be tagged with the correct sense." ></td>
	<td class="line x" title="664:730	In most papers (including ours) the tagging of the test data was done by hand, which limits the size of the testing set." ></td>
	<td class="line x" title="665:730	Preparing one test set by hand may still be reasonable, though time consuming." ></td>
	<td class="line x" title="666:730	However, it is useful to have more than one set, such that results will be reported on a new, unseen, set, while another set is used for developing and tuning the system." ></td>
	<td class="line x" title="667:730	One useful source of tagged examples is an aligned bilingual corpus, which can be used for testing any sense disambiguation method, including methods that do not use bilingual material for training." ></td>
	<td class="line x" title="668:730	Gale proposes to use 'pseudo-words' as another practical source of testing examples (Gale, Church, and Yarowsky 1992b) (equivalently, Schfitze \[1992\] uses 'artificial ambiguous words')." ></td>
	<td class="line x" title="669:730	Pseudo-words are constructed artificially as a union of several different words (say, wl, w2, and w3 define three 'senses' of the pseudo-word x)." ></td>
	<td class="line x" title="670:730	The disambiguation method is presented with texts in which all occurrences of wl, w2, and w3 are considered as occurrences of x and should then select the original word (sense) for each occurrence." ></td>
	<td class="line x" title="671:730	Though testing with this method does not provide results for real ambiguities that occur in the text, it can be very useful while develop591 Computational Linguistics Volume 20, Number 4 ing and tuning the method (Gale shows high correlation between the performance of his method on real sense ambiguities and pseudo-words)." ></td>
	<td class="line x" title="672:730	8." ></td>
	<td class="line x" title="673:730	Conclusions The method presented in this paper takes advantage of two linguistic phenomena, both proven to be very useful for sense disambiguation: the different mapping between words and word senses among different languages, and the importance of lexical co-occurrence within syntactic relations." ></td>
	<td class="line x" title="674:730	The first phenomenon provides the solution for the circularity problem in acquiring sense disambiguation data." ></td>
	<td class="line x" title="675:730	Using a bilingual lexicon and a monolingual corpus of the target language, we can acquire statistics on word senses automatically, without manual tagging." ></td>
	<td class="line x" title="676:730	As explained in Section 7, this method has significant practical and theoretical advantages over the use of aligned bilingual corpora." ></td>
	<td class="line x" title="677:730	We pay for these advantages by introducing an additional level of noise, in mapping individual words independently to the other language." ></td>
	<td class="line x" title="678:730	Our results show, however, that the precision of the selection algorithm is high despite this additional noise." ></td>
	<td class="line x" title="679:730	This work also emphasizes the importance of lexical co-occurrence within syntactic relations for the resolution of lexical ambiguity." ></td>
	<td class="line x" title="680:730	Co-occurrences found in a large corpus reflect a huge amount of semantic knowledge, which was traditionally constructed by hand." ></td>
	<td class="line x" title="681:730	Moreover, frequency data for such co-occurrences reflect both linguistic and domain-specific preferences, thus indicating not only what is possible, but also what is probable." ></td>
	<td class="line x" title="682:730	It is important to notice that frequency information on lexical co-occurrence was found to be much more predictive than single word frequency." ></td>
	<td class="line x" title="683:730	In the three experiments we reported, there were 61 cases in which the two types of information contradicted each other, favoring different target words." ></td>
	<td class="line x" title="684:730	In 56 of these cases (92%), it was the most frequent lexical co-occurrence, and not the most frequent word, that predicted the correct translation." ></td>
	<td class="line x" title="685:730	This result may raise relevant hypotheses for psycholinguistic research, which has indicated the relevance of word frequencies to human sense disambiguation (e.g. , Simpson and Burgess 1988)." ></td>
	<td class="line x" title="686:730	We suggest that the high precision achieved in the experiments relies on two characteristics of the ambiguity phenomena, namely the sparseness and redundancy of the disambiguating data." ></td>
	<td class="line x" title="687:730	By sparseness we mean that within the large space of alternative interpretations produced by ambiguous utterances, only a small portion is commonly used." ></td>
	<td class="line x" title="688:730	Therefore, the chance that an inappropriate interpretation is observed in the corpus (in other contexts) is low." ></td>
	<td class="line x" title="689:730	Redundancy relates to the fact that different informants (such as different lexical relations or deep understanding) tend to support rather than contradict one another, and therefore the chance of picking a 'wrong' informant is low." ></td>
	<td class="line x" title="690:730	It is interesting to compare our method with some aspects of the statistical machine translation system of Brown et al.(1990)." ></td>
	<td class="line x" title="692:730	As mentioned in the introduction, this system also incorporates target language statistics in the translation process." ></td>
	<td class="line x" title="693:730	To translate a French sentence, f, they choose the English sentence, e, that maximizes the term Pr(e)  Pr(f I e)." ></td>
	<td class="line x" title="694:730	The first factor in this product, which represents the target language model, may thus affect any aspect of the translation, including target word selection." ></td>
	<td class="line x" title="695:730	It seems, however, that Brown et al. expect that target word selection would be determined mainly by translation probabilities (the second factor in the above term), which should be derived from a bilingual corpus (Brown et al. 1990, p. 79)." ></td>
	<td class="line x" title="696:730	This view is reflected also in their elaborate method for target word selection (Brown et al. 1991), in which better estimates of translation probabilities are achieved as a result of word sense disambiguation." ></td>
	<td class="line x" title="697:730	Our method, on the other hand, incorporates only 592 Ido Dagan and Alon Itai Word Sense Disambiguation target language probabilities and ignores any notion of translation probabilities." ></td>
	<td class="line x" title="698:730	It thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the lack of translation probabilities." ></td>
	<td class="line x" title="699:730	For our system, the more informative statistics are achieved by syntactic analysis of both the source and target languages, instead of the simple tri-gram model used by Brown et al. In a broader sense, this can be viewed as a tradeoff between the different components of a translation system: having better analysis and generation models may reduce some burden from the transfer model." ></td>
	<td class="line x" title="700:730	In our opinion, the method proposed in this paper may have immediate practical value, beyond its theoretical aspects." ></td>
	<td class="line x" title="701:730	As we argue below, we believe that the method is feasible for practical machine translation systems and can provide a cost-effective improvement on target word selection methods." ></td>
	<td class="line x" title="702:730	The identification of syntactic relations in the source sentence is available in any machine translation system that uses some form of syntactic parsing." ></td>
	<td class="line x" title="703:730	Trivially, a bilingual lexicon is available." ></td>
	<td class="line x" title="704:730	A parser for the target language becomes common in many systems that offer bidirectional translation capabilities, requiring parsers for several languages (see Miller 1993, for available language pairs in several commercial machine translation systems)." ></td>
	<td class="line x" title="705:730	If a parser for the target language corpus is not available, it is possible to approximate the statistics using word co-occurrence in a window, as was demonstrated by a variant of our method (Dagan, Marcus, and Markovitch 1993) (see Section 5.1)." ></td>
	<td class="line x" title="706:730	In both cases, the statistical model was shown to handle successfully the noise produced in automatic acquisition of the data." ></td>
	<td class="line x" title="707:730	Substantial effort may be required for collecting a sufficiently large target language corpus." ></td>
	<td class="line x" title="708:730	We have not studied the relation between the corpus size and the performance of the algorithm, but it is our impression that a corpus of several hundred thousand words will prove useful for translation in a well-defined domain." ></td>
	<td class="line x" title="709:730	With current availability of texts in electronic form, = a corpus of this size is feasible in many domains." ></td>
	<td class="line x" title="710:730	The effort of assembling this corpus should be compared with the effort of manually coding sense disambiguation information." ></td>
	<td class="line x" title="711:730	Finally, our method was evaluated by simulating realistic machine translation lexicons, on randomly selected examples, and yielded high performance in two different broad domains (foreign news articles and a software manual)." ></td>
	<td class="line x" title="712:730	It is therefore expected that the results reported here will be reproduced in other domains and systems." ></td>
	<td class="line x" title="713:730	To improve the performance of target word selection further, our method may be combined with other sense disambiguation methods." ></td>
	<td class="line x" title="714:730	As discussed in Section 6.2, it is possible to increase the applicability (coverage) of the selection method by considering word co-occurrence in a limited context and/or by using similarity-based methods that reduce the problem of data sparseness." ></td>
	<td class="line x" title="715:730	To a lesser extent, the use of a bilingual corpus may further increase the precision of the selection (see Section 6.3)." ></td>
	<td class="line x" title="716:730	A practical strategy may be to use a bilingual corpus for enriching the bilingual lexicon, while relying mainly on co-occurrence statistics from a larger monolingual corpus for disambiguation." ></td>
	<td class="line x" title="717:730	In a broader context, this paper promotes the combination of statistical and linguistic models in natural language processing." ></td>
	<td class="line x" title="718:730	It provides an example of how a problem can be first defined in detailed linguistic terms, using an implemented linguistic tool (a syntactic parser, in our case)." ></td>
	<td class="line x" title="719:730	Then, having a well-defined linguistic scenario, we apply a suitable statistical model to highly informative linguistic structures." ></td>
	<td class="line x" title="720:730	According to this view, a complex task, such as machine translation, should be first decomposed 22 Optical character recognition can also be used to acquire relevant texts in electronic form." ></td>
	<td class="line x" title="721:730	In this case, it may be necessary to approximate the statistics using word co-occurrence in a window, since parsing noisy output from optical character recognition is difficult." ></td>
	<td class="line x" title="722:730	593 Computational Linguistics Volume 20, Number 4 on a linguistic basis." ></td>
	<td class="line x" title="723:730	Then, appropriate statistical models can be developed for each sub-problem." ></td>
	<td class="line x" title="724:730	We believe that this approach provides a beneficial compromise between two extremes in natural language processing: either using linguistic models that ignore quantitative information, or using statistical models that are linguistically ignorant." ></td>
	<td class="line x" title="725:730	Appendix Approximatingvar\[ln(~)l To approximate var \[In (~)\], we first approximate In (~)by the first order derivatives (the first term of the Taylor series): (\]91) ~__ ln( pI )__ \[~Xl (X1/\]~22 In ~ ~ '}(Pl --,1 ) In pl,p2 q-(\]92--P2) \[~-~21n(X~22)\]pl,p2 : ln(P~2) qfil--p~lpl \]92--P2p2 : ln(P~2)q-\]91 --\]92'pl P2 (5) We use the following equations (see Agresti 1990): var(x+c) = var(x), var(xl x2) = var(xl) + var(x2) 2." ></td>
	<td class="line x" title="726:730	covariance(xl,x2), var(~) -p(1-p), n (c) var(x) va r C2, covariance(fii, l~j ) PiPj n covariance( x l, x2)  r x1 x2 covamance( ~, ~2 ) = clc2 Using (5) we get var\[ln(~22)\] ~ var\[ln(P~221 +l~lp, ~21 = varI~11-~221 \[\]91\] \[lY2\]_2.covariance\[lYl,tY2 \] = var Pll +var P2 \[pl ~22 _ 1 p1(1 -Pl) + 1 p2(1 -P2) +2 PiP2 p2 n p2 n nplP2 1 1 1 1 1 1 +__~ + =--+--." ></td>
	<td class="line x" title="727:730	npl np2 np~l nl~ nl n2 594 Ido Dagan and Alon Itai Word Sense Disambiguation Acknowledgments Special thanks are due to Ulrike Schwall for her fruitful collaboration." ></td>
	<td class="line x" title="728:730	We are grateful to Mori Rimon, Peter Brown, Ayala Cohen, Ulrike Rackow, Herb Leass, and Bill Gale for their help and comments." ></td>
	<td class="line x" title="729:730	We also thank the anonymous reviewers for their detailed comments, which resulted in additional discussions and clarifications." ></td>
	<td class="line x" title="730:730	This research was partially supported by grant number 120-741 of the Israel Council for Research and Development." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J94-4005
Training And Scaling Preference Functions For Disambiguation
Alshawi, Hiyan;Carter, David M.;"></td>
	<td class="line x" title="1:230	Training and Scaling Preference Functions for Disambiguation Hiyan Alshawi  AT&T Bell Laboratories David Carter t SRI International We present an automatic method for weighting the contributions of preference functions used in disambiguation." ></td>
	<td class="line x" title="2:230	Initial scaling factors are derived as the solution to a least squares minimization problem, and improvements are then made by hill climbing." ></td>
	<td class="line x" title="3:230	The method is applied to disambiguating sentences in the Air Travel Information System corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors." ></td>
	<td class="line x" title="4:230	We then focus on one class of preference function, those based on semantic lexical collocations." ></td>
	<td class="line x" title="5:230	Experimental results are presented showing that such functions vary considerably in selecting correct analyses." ></td>
	<td class="line x" title="6:230	In particular, we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations." ></td>
	<td class="line x" title="7:230	1." ></td>
	<td class="line x" title="8:230	Introduction The importance of good preference functions for ranking competing analyses produced by language-processing systems grows as the coverage of these systems improves." ></td>
	<td class="line x" title="9:230	Increasing coverage usually also increases the number of analyses for sentences previously covered, bringing the danger of lower accuracy for these sentences." ></td>
	<td class="line x" title="10:230	Large scale rule-based analysis systems have therefore tended to employ a collection of functions to produce a score for sorting analyses in a preference order." ></td>
	<td class="line x" title="11:230	In this paper we address two issues relating to the application of preference functions." ></td>
	<td class="line x" title="12:230	1.1 Combining Multiple Preference Functions The first problem we address is that of combining different functions, each of which is supposed to offer some contribution to selecting the best among a set of analyses of a sentence." ></td>
	<td class="line x" title="13:230	Although multiple functions have been used in other systems (for example, McCord 1990; Hobbs and Bear 1990), little is typically said about how the functions are combined to produce the overall score for an analysis, the weights presumably being determined by intuition or trial and error." ></td>
	<td class="line x" title="14:230	McCord (1993) gives very specific information about the weights he uses to combine preference functions, though these weights are chosen by hand." ></td>
	<td class="line x" title="15:230	Selecting weights by hand, however, is a task for experts, which needs to be redone every time the system is applied to a new domain or corpus." ></td>
	<td class="line x" title="16:230	Furthermore, there is no guarantee that the selected weights will achieve optimal or even near-optimal performance." ></td>
	<td class="line x" title="17:230	The speech-processing community, on the other hand, has a longer history of using numerical evaluation functions, and speech researchers have used schemes for scoring recognition hypotheses that are similar to the one proposed here for disambiguation." ></td>
	<td class="line x" title="18:230	* AT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974, USA." ></td>
	<td class="line x" title="19:230	E-maih hiyan@research.at t.com j' SRI International, Cambridge Computer Science Research Centre, 23 Miller Yard, Cambridge CB2 1RQ, UK." ></td>
	<td class="line x" title="20:230	E-mail: dmc@cam.sri.com (~ 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 For example, Ostendorf et al.(1991) improve recognition performance by using a linear combination of several scoring functions." ></td>
	<td class="line x" title="22:230	In their work the weights for the linear combination are chosen to optimize a generalized mean of the rank of the correct word sequence." ></td>
	<td class="line x" title="23:230	In our case, the problem is formulated as follows." ></td>
	<td class="line x" title="24:230	Each preference function is defined as a numerical (possibly real-valued) function on representations corresponding to the sentence analyses." ></td>
	<td class="line x" title="25:230	A weighted sum of these functions is then used as the overall measure to rank the possible analyses of a particular sentence." ></td>
	<td class="line x" title="26:230	We refer to the coefficients, or weights, used in this linear combination as the 'scaling factors' for the functions." ></td>
	<td class="line x" title="27:230	We determine these scaling factors automatically in order both to avoid the need for expert hand tuning and to achieve performance that is at least locally optimal." ></td>
	<td class="line x" title="28:230	We start with the solution to minimizing a squared-error cost function, a well-known technique applied to many optimization and classification problems." ></td>
	<td class="line x" title="29:230	This solution is then enhanced by application of a hill-climbing technique." ></td>
	<td class="line x" title="30:230	1.2 Word Sense Collocation Functions Until recently, the choice of the various functions used in rule-based systems was made mainly according to anecdotal information about the effectiveness of, for example, various attachment preference strategies." ></td>
	<td class="line x" title="31:230	There is now more empirical work comparing such functions, particularly in the case of functions based on statistical information about lexical or semantic collocations." ></td>
	<td class="line oc" title="32:230	Lexical collocation functions, especially those determined statistically, have recently attracted considerable attention in computational linguistics (Calzolari and Bindi 1990; Church and Hanks 1990; Sekine et al. 1992; Hindle and Rooth 1993) mainly, though not exclusively, for use in disambiguation." ></td>
	<td class="line x" title="33:230	These functions are typically derived by observing the occurrences of tuples (usually pairs or triples) that summarize relations present in an analysis of a text, or their surface occurrences." ></td>
	<td class="line x" title="34:230	For example, Hindle and Rooth (1993) and Resnik and Hearst (1993) give experimental results on the effectiveness of functions based on lexical associations, or lexical-class associations, at selecting appropriate prepositional phrase attachments." ></td>
	<td class="line x" title="35:230	We have experimented with a variety of specific functions that make use of collocations between word senses." ></td>
	<td class="line x" title="36:230	The results we present show that these functions vary considerably in disambiguation accuracy, but that the best collocation functions are more effective than a function based on simple estimates of syntactic rule probabilities." ></td>
	<td class="line x" title="37:230	In particular, the best collocation function performs significantly better than a related function that defines collocation strength in terms of mutual information, reducing the error rate in a disambiguation task from approximately 30% to approximately 10%." ></td>
	<td class="line x" title="38:230	We start by describing our experimental context and training data in Section 2." ></td>
	<td class="line x" title="39:230	Then we address the issue of selecting scaling factors by presenting our optimization procedure in Section 3 and a comparison with manual scaling in Section 4." ></td>
	<td class="line x" title="40:230	Finally, we take a close look at a set of semantic collocation functions, defining them in Section 5 and comparing their effectiveness at disambiguation in Section 6." ></td>
	<td class="line x" title="41:230	2. The Experimental Setup Disambiguation Task All the experiments we describe here were done with the Core Language Engine (CLE), a primarily rule-based natural language-processing system (Alshawi 1992)." ></td>
	<td class="line x" title="42:230	More specifically, the work on optimizing preference factors and semantic collocations was done as part of a project on spoken language translation in which the CLE was used for analysis and generation of both English and Swedish (AgnSs et al. 1993)." ></td>
	<td class="line x" title="43:230	636 Hiyan Alshawi and David Carter Training and Scaling Preference Functions The work presented here is all concerned with the English analysis side, though we see no reason why its conclusions should not be applicable to Swedish or other natural languages." ></td>
	<td class="line x" title="44:230	In our experiments we made use of the Air T, avel Information System (ATIS) corpus of transcribed speech sentences." ></td>
	<td class="line x" title="45:230	This application was chosen because the proposed method for automatic derivation of scaling factors requires a corpus of sentences that are representative of the sublanguage, together with some independent measure of the correctness or plausibility of analyses of these sentences." ></td>
	<td class="line x" title="46:230	In addition, we had access to a hand-parsed subcollection of the ATIS corpus built as part of the Penn Treebank project (Marcus, Santorini, and Marcinkiewicz 1993)." ></td>
	<td class="line x" title="47:230	Another reason for choosing ATIS was that it consists of several thousand sentences in a constrained discourse domain, which helped avoid sparseness problems in training collocation functions} In the various experiments, the alternatives we are choosing between are analyses expressed in the version of quasi logical form (QLF) described by Alshawi and Crouch (1992)." ></td>
	<td class="line x" title="48:230	QLFs express semantic content, but are derived compositionally from complete syntactic analyses of a sentence and therefore mirror much syntactic structure as well." ></td>
	<td class="line x" title="49:230	However, the use of QLF analyses is not central to our method: the important thing is that the representation used is rich enough to support a variety of preference functions." ></td>
	<td class="line x" title="50:230	We have experimented with combinations of around 30 different functions and use 20 of them in our spoken language translation system; the others contribute so little to overall performance that their computational cost cannot be justified." ></td>
	<td class="line x" title="51:230	This default set of 20 was used throughout the scaling factor work described in Sections 3 and 4." ></td>
	<td class="line x" title="52:230	It consists of 1 collocation-based function and 19 non-collocation-based ones." ></td>
	<td class="line x" title="53:230	The work described in Section 6 involved substituting single alternative collocation-based functions for the single one in the set of 20." ></td>
	<td class="line x" title="54:230	Many (unscaled) preference functions simply return integers corresponding to counts of particular constructs in the representation, such as the number of expressions corresponding to adjuncts, unresolved ellipses, particular attachment configurations, or balanced conjunctions." ></td>
	<td class="line x" title="55:230	There are also some real-valued functions, including the semantic collocation functions discussed in Section 5." ></td>
	<td class="line x" title="56:230	To illustrate how the system works, consider the ATIS sentence 'Do I get dinner on this flight'?" ></td>
	<td class="line x" title="57:230	The CLE assigns two analyses to this sentence; in one of them, QH, 'on this flight' attaches high to 'get,' and in the other, QL, it attaches low to 'dinner'." ></td>
	<td class="line x" title="58:230	Four functions return non-zero scores on these analyses." ></td>
	<td class="line x" title="59:230	Two of them, Lowl and Low2, prefer low attachment; the difference between them is an implementation detail that can be ignored here." ></td>
	<td class="line x" title="60:230	A third, SynRules, returns an estimate of the log probability of the syntactic rules used to construct the analysis." ></td>
	<td class="line x" title="61:230	A fourth, SemColl, is a semantic collocation function." ></td>
	<td class="line x" title="62:230	The scores, after multiplying by scaling factors, are as shown in Table 1." ></td>
	<td class="line x" title="63:230	The SemColl function is the only one that prefers QH to QL." ></td>
	<td class="line x" title="64:230	Because this function has a relatively large scaling factor, it is able to override the other four, which all prefer QL for syntactic reasons." ></td>
	<td class="line x" title="65:230	2.1 Training Data The Penn Treebank contains around 650 ATIS trees, which we used during initial development of training and optimization software." ></td>
	<td class="line x" title="66:230	Some of the results in these initial trials were encouraging, but most appeared to be below reasonable thresholds of sta1 The hand-parsed sub-corpus was that on the ACL DCI CD-ROM 1 of September 1991." ></td>
	<td class="line x" title="67:230	The larger corpus, used for the bulk of the work reported here, consisted of 4615 class A and D sentences from the ATIS-2 training corpus." ></td>
	<td class="line x" title="68:230	These were all such sentences of up to 15 words that we had access to at the time, excluding a set of randomly selected sentences that were set aside for other testing purposes." ></td>
	<td class="line x" title="69:230	637 Computational Linguistics Volume 20, Number 4 Table 1 Scaled preference scores for 'Do I get dinner on this flight'?" ></td>
	<td class="line x" title="70:230	Function Score Score on QH on QL Lowi -9.08 -4.03 Low2 -2.80 0.00 SynRules -13.08 -12.78 SemColl 24.32 3~38 Total -0.64 -13.34 tistical significance." ></td>
	<td class="line x" title="71:230	So, we concluded that it was worthwhile to produce more training data." ></td>
	<td class="line x" title="72:230	For this purpose, we developed a semiautomatic mechanism for producing skeletal constituent structure trees directly from QLF analyses proposed by our analyser." ></td>
	<td class="line x" title="73:230	To make these trees compatible with the treebank and to make them relatively insensitive to minor changes in semantic analysis, these QLF-induced trees consist simply of nested constituents with two categories, A (argument) and P (predication), corresponding to constituents induced by QLF term and form expressions, respectively." ></td>
	<td class="line x" title="74:230	The tree for the example sentence used above is as follows: (p do (A I) get (A dinner) (P on (A this flight) ) ) The interactive software for producing the trees proposes constituents for confirmation by a user and takes into account answers given, to minimize the number of interactive choices necessary." ></td>
	<td class="line x" title="75:230	Of the 4615 sentences in our training set, the CLE produced an acceptable constituent structure for 4092 (about 89%)." ></td>
	<td class="line x" title="76:230	A skeletal tree for each of these 4092 sentences was created in this way and used in the various experiments whose results are described below." ></td>
	<td class="line x" title="77:230	We do not directly address here the problems of applying preference functions to select the best analysis when none is completely correct; we assume, based on our experience with the spoken language translator, that functions and scaling factors trained on cases for which a completely correct analysis exists will also perform fairly well on cases for which one does not." ></td>
	<td class="line x" title="78:230	2.2 Training Score Employing treebank analyses in the training process required defining a measure of the ''degree of correctness' of a QLF analysis under the assumption that the phrasestructure analysis in the treebank is correct." ></td>
	<td class="line x" title="79:230	At first sight this might appear difficult, in that QLF is a logical formalism, but in fact it preserves much of the geometry of constituent structure." ></td>
	<td class="line x" title="80:230	Specifically, significant (typically BAR-2 level) constituents tend to give rise to term (roughly argument) or form (roughly predication) QLF subexpressions, though the details do not matter here." ></td>
	<td class="line x" title="81:230	It is thus possible to associate segments of the input with such QLF subexpressions and to check whether such a segment is also present as a constituent in the treebank analysis." ></td>
	<td class="line x" title="82:230	The issues raised by measuring 638 Hiyan Alshawi and David Carter Training and Scaling Preference Functions QLF correctness in terms of agreement with structures containing less information than those QLFs are discussed further at the end of Section 4." ></td>
	<td class="line x" title="83:230	The training score functions we considered for a QLF q with respect to a treebank tree t were functions of the form score(q, t) = allQ N T I a21Q \ T\] a31T \ QI, where Q is the set of string segments induced by the term and form expressions of q; T is the set of constituents in t; al, a2, and a3 are positive constants; and the '\' operator denotes set difference." ></td>
	<td class="line x" title="84:230	The idea is to reward the QLF for constituents in common with the treebank and to penalize it for differences." ></td>
	<td class="line x" title="85:230	Trial and error led us to choose a1=1, a2=10, a3=0, which penalizes hallucination of incorrect constituents (modeled by \[Q \ T\]) more heavily than a shortfall in completeness (modeled by IQ n TI)." ></td>
	<td class="line x" title="86:230	These constants were fixed before we carried out the experiments whose results are presented below." ></td>
	<td class="line x" title="87:230	The explanation for setting a3 to 0 was that trees in the Penn Treebank contain many constituents that do not correspond to QLF form or term expressions; we had to avoid penalizing QLF analyses simply because the treebank uses a different kind of linguistic representation." ></td>
	<td class="line x" title="88:230	For QLF-induced trees, in which the correspondence is one to one, it is also reasonable to set a3 to 0 because when IT \ Q I is non-zero, I Q A T I tends to be non-maximal." ></td>
	<td class="line x" title="89:230	Among the 4092 sentences for which skeletal trees were derived, there were only 5 with alternative QLFs for which the training score value was the same with a3 = 0 but would be different if a3 were non-zero." ></td>
	<td class="line x" title="90:230	3." ></td>
	<td class="line x" title="91:230	Computing Scaling Factors When we first implemented a disambiguation mechanism of the kind described above, an initial set of scaling factors was chosen by hand according to knowledge of how the particular raw preference functions were computed and introspection about the 'strength' of the functions as indicators of preference." ></td>
	<td class="line x" title="92:230	These initial scaling factors were subsequently revised according to their observed behavior in ranking analyses, eventually leading to reasonably well-behaved rankings." ></td>
	<td class="line x" title="93:230	However, as suggested earlier, there are a number of disadvantages to manual tuning of scaling factors." ></td>
	<td class="line x" title="94:230	These include the effort spent in maintaining the parameters." ></td>
	<td class="line x" title="95:230	This effort is greater for those with less knowledge of how the raw preference functions are computed, since this increases the effort for trial-and-error tuning." ></td>
	<td class="line x" title="96:230	A point of diminishing returns is also reached, after which further attempts at improvement through hand tuning often turn out to be counterproductive." ></td>
	<td class="line x" title="97:230	Another problem was that it became difficult to detect preference functions that were ineffective, or simply wrong, if they were given sufficiently low scaling factors." ></td>
	<td class="line x" title="98:230	Probably a more serious problem is that the contributions of different preference functions to selecting the most plausible analyses seem to vary from one sublanguage to another." ></td>
	<td class="line x" title="99:230	These disadvantages point to the need for automatic procedures to determine scaling factors that optimize preference function rankings for a particular sublanguage." ></td>
	<td class="line x" title="100:230	In our framework, a numerical 'preference score' is computed for each of the alternative analyses, and the analyses are ranked according to this score." ></td>
	<td class="line x" title="101:230	As mentioned earlier, the preference score is a weighted sum of a set of preference functions: Each preference function f/ takes a complete QLF representation qi as input, returning a numerical score sq, the overall preference score being computed by summing over the 639 Computational Linguistics Volume 20, Number 4 product of function scores with their associated scaling factors cj: ClSil q-." ></td>
	<td class="line x" title="102:230	-}CmSim 3.1 Collection Procedure The training process begins by analyzing the corpus sentences and computing, for each analysis of each sentence, the training score of the analysis with respect to the manually approved skeletal tree and the (unscaled) values of the preference functions applied to that analysis." ></td>
	<td class="line x" title="103:230	One source of variation in the data that we want to ignore in order to derive scaling factors appropriate for selecting QLFs is the fact that preference function values for an analysis often reflect characteristics shared by all analyses of a sentence, as much as the differences between alternative analyses." ></td>
	<td class="line x" title="104:230	For example, a function that counts the occurrences of certain constructs in a QLF will tend to give higher values for QLFs for longer sentences." ></td>
	<td class="line x" title="105:230	In the limit, one can imagine a function ~b that, for an N-word sentence, returned a value of N + G for a QLF with training score G with respect to the skeletal tree." ></td>
	<td class="line x" title="106:230	Such a function, if it existed, would be extremely useful, but (if sentence length were not also considered) would not be a particularly accurate predictor of the QLF training score." ></td>
	<td class="line x" title="107:230	To discount irrelevant intersentence variability, both the training score with respect to the skeletal tree and all the preference function scores are therefore relativized by subtracting from them the corresponding values for the analysis of that sentence which best matches the skeletal tree." ></td>
	<td class="line x" title="108:230	If the best match is shared by several analyses, the average for those analyses is subtracted." ></td>
	<td class="line x" title="109:230	The relativized training score is the distance function with respect to which the first stage of scaling factor calculation takes place." ></td>
	<td class="line x" title="110:230	It can be seen that the relativized results of our hypothetical preference function 6 are a perfect predictor of the relativized training score." ></td>
	<td class="line x" title="111:230	Consider, for example, a six-word sentence with three QLFs, two of which, ql and q2, have completely correct skeletal tree structures and the third of which, q3, does not." ></td>
	<td class="line x" title="112:230	Suppose also that the training scores and the scores assigned by preference functions, G fl, and fz, are as follows: Training ~ fl f2 ql 10 16 8 4 q2 10 16 6 10 q3 4 10 2 12 After relativizing (subtracting the average of the ql and q2 values), we get Training ~ fl f2 ql 0 0 1 -3 q2 0 0 --1 3 q3 --6 --6 --5 5 3.2 Least Squares Calculation An initial set of scaling factors is calculated in a straightforward analytic way by approximating gi, the relativized training score of qi, by ~j cjzij, where cj is the scaling factor for preference function fj and zq is the relativized score assigned to qi by ~." ></td>
	<td class="line x" title="113:230	We vary the values of cj to minimize the sum, over all QLFs for all training sentences, of the squares of the errors in the approximation 2 640 Hiyan Alshawi and David Carter Training and Scaling Preference Functions Defining the error function as a sum of squares of differences in this way means that the minimum error is attained when the derivative with respect to each ck, --2 ~i Zik(gi -y'~q CjZq), is zero." ></td>
	<td class="line x" title="114:230	These linear simultaneous equations, one for each of cl  c,,, can be solved by Gaussian elimination." ></td>
	<td class="line x" title="115:230	(For a full explanation of this standard technique, see Moore and McCabe 1989, pp." ></td>
	<td class="line x" title="116:230	174ff and 680ff)." ></td>
	<td class="line x" title="117:230	This least squares set of scaling factors achieves quite good disambiguation performance (see Section 4) but is not truly optimal because of the inherent nonlinearity of the goal, which is to maximize the proportion of sentences for which a correct QLF is selected, rather than to approximate training scores (even relativized ones)." ></td>
	<td class="line x" title="118:230	Suppose that a function M has a tendency to give high scores to correct QLFs when the contributions of other functions do not clearly favor any QLF, but that M tends to perform much less well when other functions come up with a clear choice." ></td>
	<td class="line x" title="119:230	Then increasing the scaling factor on M from the least squares value will tend to improve system performance even though the sum of squares of errors is increased; M's tendency to perform well just when it is important to do so should be rewarded." ></td>
	<td class="line x" title="120:230	3.3 Iterative Scaling Factor Adjustment The least squares scaling factors are therefore adjusted iteratively by a hill-climbing procedure that directly examines the QLF choices they give rise to on the training corpus." ></td>
	<td class="line x" title="121:230	Scaling factors are altered one at a time in an attempt to locally optimize 2 the number of correct disambiguation decisions, i.e., the number of training sentences for which a QLF with a correct skeletal tree receives the highest score." ></td>
	<td class="line x" title="122:230	A step in the iteration involves calculating the effect of an alteration to each factor in turn." ></td>
	<td class="line x" title="123:230	3 If factors Ck, k ~ j, are held constant, it is easy to find a set (possibly empty) of real-valued intervals \[u/j, viii such that a correct choice will be made on sentence i if uij < cj <_ vii." ></td>
	<td class="line x" title="124:230	By collecting these intervals for all the functions and for all the sentences in the training corpus, one can determine the effect on the number of correct disambiguation decisions of any alteration to any single scaling factor." ></td>
	<td class="line x" title="125:230	The alteration selected is the one that gives the biggest increase in the number of sentences for which a correct choice is made." ></td>
	<td class="line x" title="126:230	When no increase is possible, the procedure terminates." ></td>
	<td class="line x" title="127:230	We found that convergence tends to be fairly rapid, with the number of steps seldom exceeding the number of scaling factors involved (although the process does occasionally change a scaling factor it has previously altered, when intervening changes make this appropriate)." ></td>
	<td class="line x" title="128:230	One of the functions we used shows the limitations of least squares scaling factor optimization, alluded to above, in quite a dramatic way." ></td>
	<td class="line x" title="129:230	The function in question returns the number of temporal modifiers in a QLE Its intended purpose is to favor readings of utterances like 'Atlanta to Boston Tuesday,' in which 'Tuesday' is a temporal modifier of the (elliptical) sentence rather than a compound noun formed with 'Boston'." ></td>
	<td class="line x" title="130:230	Linear scaling always gives this function a negative weight, causing temporal modifications to be downgraded, and in fact the relativized training score of a QLF turns out to be negatively correlated with the number of temporal modifiers it contains." ></td>
	<td class="line x" title="131:230	However, the intuitions that led to the introduction of the function do seem 2 Finding a global optimum would of course be desirable." ></td>
	<td class="line x" title="132:230	However, inspection of the results, over various conditions, of the iterative scheme presented here did not suggest that the introduction of a technique such as simulated annealing, which in general can improve the prospect of finding a more global optimum, would have had much effect on performance." ></td>
	<td class="line x" title="133:230	3 An algorithm based on gradient descent might appear preferable, on the grounds that it would alter all factors simultaneously and have a better chance of locating a global optimum." ></td>
	<td class="line x" title="134:230	However, the objective function, the number of correct disambiguation decisions, varies discontinuously with the scaling factors, so no gradients can be calculated." ></td>
	<td class="line x" title="135:230	641 Computational Linguistics Volume 20, Number 4 Table 2 Performance of scaling factor sets Scaling factor set Number Percentage correct correct (Random baseline) 1949 47.6 Normalized 3549 86.7 Hand tuned 3717 90.8 Least squares 3841 93.9 Hill climbing 3857 94.3 to hold for QLFs that are close to being correct, and therefore iterative adjustment makes the weight positive." ></td>
	<td class="line x" title="136:230	4." ></td>
	<td class="line x" title="137:230	Comparing Scaling Factor Sets The performance of the factors derived from least squares calculation and adjustment by hill climbing was compared with that of various other sets of factors." ></td>
	<td class="line x" title="138:230	The factor sets considered, roughly in increasing order of their expected quality, were the following:  'Normalized' factors: the magnitude of each factor is the inverse of the standard deviation of the preference function in question, making each function contribute equally." ></td>
	<td class="line x" title="139:230	A factor is positive if it correlates positively with training scores; otherwise it is negative." ></td>
	<td class="line x" title="140:230	 Factors chosen and tuned by hand for ATIS sentences before the work described in this paper was done, or, for functions developed during the work described here, without reference to any automatically derived values." ></td>
	<td class="line x" title="141:230	 Factors resulting from least squares calculation, as described in Section 3.2." ></td>
	<td class="line x" title="142:230	 Factors resulting from least squares calculation followed by hill-climbing adjustment (Section 3.3)." ></td>
	<td class="line x" title="143:230	To provide a baseline, performance was also evaluated for the technique of a random selection of a single QLF for each sentence." ></td>
	<td class="line x" title="144:230	The performance of each set of factors was evaluated as follows." ></td>
	<td class="line x" title="145:230	The set of 4092 sentences with skeletal trees was divided into five subsets of roughly equal size." ></td>
	<td class="line x" title="146:230	Each subset was 'held out' in turn: the functions and scaling factors were trained on the other four subsets, and the system was then evaluated on the held-out subset." ></td>
	<td class="line x" title="147:230	The system was deemed to have correctly processed a sentence if the QLF to which it assigned the highest score agreed exactly with the corresponding skeletal tree." ></td>
	<td class="line x" title="148:230	The numbers of correctly processed sentences (i.e. , sentences whose selected QLFs had correct constituent structures) are shown in Table 2; because all the sentences involved were within coverage, the theoretical maximum achievable is 4092 (100%)." ></td>
	<td class="line x" title="149:230	We use a standard statistical method, the sign test (explained in, for example, Dixon and Massey 1968), to assess the significance of the difference between two factor sets, $1 and $2." ></td>
	<td class="line x" title="150:230	Define Fi(x) to be the function that assigns 1 to a sentence x if Si makes the correct choice in disambiguating x and 0 if it makes the wrong choice." ></td>
	<td class="line x" title="151:230	642 Hiyan Alshawi and David Carter Training and Scaling Preference Functions Table 3 Sign test comparisons of scaling factor sets S1 $2 + #SDs Normalized Hand tuned 154 322 7.7 Normalized Least squares 67 359 14.1 Normalized Hill climbing 75 383 14.4 Hand tuned Least squares 78 202 7.4 Hand tuned Hill climbing 76 216 8.2 Least squares Hill climbing 20 36 2.1 The null hypothesis is that F1 (x) and F2(x), treated as random variables over x, have the same distribution, from which we would expect the difference between F1 (x) and F2(x) to be positive as often as it is negative." ></td>
	<td class="line x" title="152:230	Table 3 gives the number of cases in which this difference is positive or negative." ></td>
	<td class="line x" title="153:230	As is usual for the sign test, the cases in which the difference is 0 do not need to be taken into account." ></td>
	<td class="line x" title="154:230	The test is applied to compare six pairs of factor sets." ></td>
	<td class="line x" title="155:230	The '#SDs' column in Table 3 shows the number of standard deviations represented by the difference between the '+' and '-' figures under the null hypothesis; a #SDs value of 1.95 is statistically significant at the 5% level (two tail), and a value of 3.3 is significant at the 0.1% level." ></td>
	<td class="line x" title="156:230	Table 3 shows that, in terms of wrong QLF choices, both sets of machine-optimized factors perform significantly better than the hand-optimized factors, to which considerable skilled effort had been devoted." ></td>
	<td class="line x" title="157:230	It is worth emphasizing that the process of determining the machine-optimized factors does not make use of the knowledge encoded by hand optimization." ></td>
	<td class="line x" title="158:230	The hill-climbing factor set, in turn, performs significantly better than the least squares set from which it is derived." ></td>
	<td class="line x" title="159:230	A possible objection to this analysis is that, because QLFs are much richer structures than constituent trees, it is possible for a QLF to match a tree perfectly but have some other characteristic that makes it incorrect." ></td>
	<td class="line x" title="160:230	In general, the principal source of such discrepancies is a wrong choice of word sense, but pure sense ambiguity (i.e. , different predicates for the same syntactic behavior of the same word) turns out to be extremely rare in the ATIS corpus." ></td>
	<td class="line x" title="161:230	An examination of the selected QLFs for the 20 + 36 = 56 sentences making up the  and values for the comparison between the least squares and hill-climbing factor sets showed that in no case did a QLF have a correct constituent structure but fail to be acceptable on other criteria." ></td>
	<td class="line x" title="162:230	Thus although the absolute percent correctness figures for a set of scaling factors may be very slightly (perhaps up to 1%) overoptimistic, this has no noticeable effect on the differences between factor sets." ></td>
	<td class="line x" title="163:230	5." ></td>
	<td class="line x" title="164:230	Lexical Semantic Collocations In this section we move from the problem of calculating scaling factors to the other main topic of this paper, showing how our experimental framework can be used diagnostically to compare the utility of competing suggestions for preference functions." ></td>
	<td class="line x" title="165:230	We refer to the variant of collocations we used as lexical semantic collocations because (i) they are collocations between word senses rather than lexical items, and (ii) the relations used are often deeper than syntactic relations (for example the relations between a verb and its subject are different for passive and active sentences)." ></td>
	<td class="line x" title="166:230	The semantic collocations extracted from QLF expressions take the form of (H1, R, H2) triples, in which H1 and H2 are the head predicates of phrases in a sentence 643 Computational Linguistics Volume 20, Number 4 and R indicates the relation (e.g. , a preposition or an argument position) between the two phrases in the proposed analysis." ></td>
	<td class="line x" title="167:230	For this purpose, the triple derivation software abstracted away from proper names and some noun and verb predicates when they appeared as heads of phrases, replacing them by hand-coded class predicates." ></td>
	<td class="line x" title="168:230	For example, predicates for names of meals are mapped onto the class name cc SpecificMeal on the grounds that their distributions in unseen sentences are likely to be very similar." ></td>
	<td class="line x" title="169:230	Some of the triples for the high-attachment QLF for 'Do I get dinner on this flight'?" ></td>
	<td class="line x" title="170:230	are as follows: (getAcquire, 2, personal) (getAcquire, 3, cc_Specif icMeal) (get_Acquire, on, f light_AirplaneTrip)." ></td>
	<td class="line x" title="171:230	The first two triples correspond to the agent and theme positions in the predicate for get, whereas the third expresses the vital prepositional phrase attachment." ></td>
	<td class="line x" title="172:230	In the triple set for the other QLF, this triple is replaced by (cc_SpecificMeal, on, flight_AirplaneTrip)." ></td>
	<td class="line x" title="173:230	Data collection for the semantic collocation functions proceeds by deriving a set of triples from each QLF analysis of the sentences in the training set." ></td>
	<td class="line x" title="174:230	This is followed by statistical analysis to produce the following functions of each triple in the observed triple population." ></td>
	<td class="line x" title="175:230	The first two functions have been used in other work on collocation; some authors use simple pairs rather than triples (i.e. , no relation, just two words) when computing collocation strengths, so direct comparisons are a little difficult." ></td>
	<td class="line x" title="176:230	The third function is an original variant of the second; the fourth is original; and the fifth is prompted by the arguments of Dunning (1993)." ></td>
	<td class="line x" title="177:230	 Mutual information: this relates the probability Pl(a)P2(b)P3(c) of the triple (a, b~ c) assuming independence between its three fields, where P~(x) is the probability of observing x in position p, with the probability A estimated from actual observations of triples derived from analyses ranked highest (or joint highest) in training score." ></td>
	<td class="line x" title="178:230	More specifically, we use lna/\[P1 (a)Pa(b)P3(c)\]}." ></td>
	<td class="line x" title="179:230	 X2: compares the expected frequency E of a triple with the square of the difference between E and the observed frequency F of the triple." ></td>
	<td class="line x" title="180:230	Here the observed frequency is in analyses ranked highest (or joint highest) in training score, and the 'expected' frequency assumes independence between triple fields." ></td>
	<td class="line x" title="181:230	More specifically we use IF E\]  (F E)/E. This variant of X 2, in which the numerator is signed, is used so that the function is monotonic, making it more suitable in preference functions." ></td>
	<td class="line x" title="182:230	 X: as X 2, but the quantity used is (F E)/v'E, as large values of F E have a tendency to swamp the X 2 function." ></td>
	<td class="line x" title="183:230	 Mean distance: the average of the relativized training score for all QLF analyses (not necessarily the highest ranked ones) that include the semantic collocation corresponding to the triple." ></td>
	<td class="line x" title="184:230	In other words, the mean distance value for a triple is the mean amount by which a QLF giving rise to that triple falls short of a perfect score." ></td>
	<td class="line x" title="185:230	 Likelihood ratio: for each triple (HI~ R, H2), the ratio of the maximum likelihood of the triple, given the distribution of triples in correct 644 Hiyan Alshawi and David Carter Training and Scaling Preference Functions Table 4 Performance of collocational and syntactic rule functions alone Function Number Percentage correct correct (Random baseline) 1949 47.6 Mutual info 2817 68.9 Syntactic rule cost 2913 71.2 Likelihood ratio 3120 76.3 X 2 3339 81.6 X 3407 83.3 Mean distance 3670 89.7 analyses of the training data, on the assumption that H1 and H2 are independent given R, to the maximum likelihood without that assumption." ></td>
	<td class="line x" title="186:230	(See Dunning 1993, for a fuller explanation of the use of likelihood ratios)." ></td>
	<td class="line x" title="187:230	Computation of the mutual information and 2 functions for triples involves the simple smoothing technique, suggested by Ken Church, of adding 0.5 to actual counts." ></td>
	<td class="line x" title="188:230	From these five functions on triples, we define five semantic collocation preference functions applied to QLFs, in each case by averaging over the result of applying the function to each triple derived from a QLE We refer to these functions by the same names as their underlying functions on triples." ></td>
	<td class="line x" title="189:230	The collocation functions are normalized by multiplying up by the number of words in the sentence to which the function is being applied." ></td>
	<td class="line x" title="190:230	This normalization keeps scores for QLFs in the same sentence comparable, while at the same time ensuring that the triple function scores tend to grow with sentence length in the same way that the non-collocation functions tend to do." ></td>
	<td class="line x" title="191:230	Thus the optimality of a set of scaling factors is relatively insensitive to sentence length." ></td>
	<td class="line x" title="192:230	Our use of the mean distance function was motivated by the desire to take into account additional information from the training material that is not exploited by the other collocation functions." ></td>
	<td class="line x" title="193:230	Specifically, it takes into account all analyses proposed by the system, as well as the magnitude of the training score." ></td>
	<td class="line x" title="194:230	In contrast, the other collocation functions make use only of the training score to select the best analysis of a sentence, discarding the rest." ></td>
	<td class="line x" title="195:230	Another way of putting this is that the mean distance function is making use of negative examples and a measure of the degree of unacceptability of an analysis." ></td>
	<td class="line x" title="196:230	6." ></td>
	<td class="line x" title="197:230	Comparing Semantic Collocation Functions An evaluation of each function acting alone on the five held-out sets of test data yielded the numbers of correctly processed sentences shown in Table 4." ></td>
	<td class="line x" title="198:230	The figures for the random baseline are repeated from Table 2." ></td>
	<td class="line x" title="199:230	We also show, for comparison, the results for a function that scores a QLF according to the sum of the logs of the estimated probabilities of the syntactic rules used in its construction." ></td>
	<td class="line x" title="200:230	4 4 We estimate the probability of occurrence of a syntactic rule R as the number of occurrences of R leading to QLFs with correct skeletal trees, divided by the number of occurrences of all rules leading to such QLFs." ></td>
	<td class="line x" title="201:230	645 Computational Linguistics Volume 20, Number 4 Table 5 Performance of collocational functions with others Function Number Percentage correct correct X 2 3741 91.4 X 3766 92.0 Mean distance 3857 94.3 In cases where a function judged N QLFs equally plausible, of which 0 < G < N were correct, we assigned a fractional count G/N to that sentence; a random choice among the N QLFs would pick a correct one with probability G/N. For significance tests, which require binary data, we took a function as performing correctly only if all the QLFs it selected were correct." ></td>
	<td class="line x" title="202:230	Such ties did not occur at all for the other experiments reported in this paper." ></td>
	<td class="line x" title="203:230	A pairwise comparison of the results shows that all the differences between collocational functions are statistically highly significant." ></td>
	<td class="line x" title="204:230	The syntactic rule cost function is significantly worse than all the collocational functions except the mutual information one, for which the difference is not significant either way." ></td>
	<td class="line x" title="205:230	(There may, of course, exist better syntactic functions than the one we have tried)." ></td>
	<td class="line x" title="206:230	The mean distance function is much superior to all the others when acting alone." ></td>
	<td class="line x" title="207:230	Presumably, this function has an edge over the other functions because it exploits the additional information from negative examples and degree of correctness." ></td>
	<td class="line x" title="208:230	The difference in performance between our syntactic and semantic preference functions is broadly in line with the results presented by Chang, Luo, and Su (1992), who use probabilities of semantic category tuples." ></td>
	<td class="line x" title="209:230	However, this similarity in the results should be taken with some caution, because our syntactic preference function is rather crude, and because our best semantic function (mean distance) uses the additional information mentioned above." ></td>
	<td class="line x" title="210:230	This information is not normally taken into account by direct estimates of tuple probabilities." ></td>
	<td class="line x" title="211:230	When I collocation function is selected to act together with the 19 non-collocationbased functions from the default set (the set defined in Section 2 and used in the experiments on scaling factor calculation), the picture changes slightly." ></td>
	<td class="line x" title="212:230	In this context, when scaling factors are calculated in the usual way, by least squares followed by hill climbing, the results for the best 3 of the above functions are as shown in Table 5." ></td>
	<td class="line x" title="213:230	The difference between the mean distance function and the other 2 functions is still highly significant; therefore this function is chosen to be the only collocational one to be included in the default set of 20 (hence the 'mean distance' condition here is the same as the 'hill-climbing' condition in Section 4)." ></td>
	<td class="line x" title="214:230	However, the difference between the X and X 2 functions is no longer quite so clear cut, and the relative advantage of the mean distance function compared with the X function is less." ></td>
	<td class="line x" title="215:230	It may be that other preference functions make up for some shortfall of the X function that is, at least in part, taken into account by the mean distance function." ></td>
	<td class="line x" title="216:230	7." ></td>
	<td class="line x" title="217:230	Conclusion We have presented a relatively simple analytic technique for automatically determining a set of scaling factors for preference functions used in semantic disambiguation." ></td>
	<td class="line x" title="218:230	The initial scaling factors produced are optimal with respect to a score provided by a 646 Hiyan Alshawi and David Carter Training and Scaling Preference Functions training procedure and are further improved by comparison with instances of the task they are intended to perform." ></td>
	<td class="line x" title="219:230	The experimental results presented indicate that, by using a fairly crude training score measure (comparing only phrase structure trees) with a few thousand training sentences, the method can yield a set of scaling factors that are significantly better than those derived by a labor-intensive hand-tuning effort." ></td>
	<td class="line x" title="220:230	We have also confirmed empirically that considerable differences exist between the effectiveness of differently formulated collocation functions for disambiguation." ></td>
	<td class="line x" title="221:230	The experiments provide a basis for selecting among different collocational functions and suggest that a collocation function must be evaluated in the context of other functions, rather than on its own, if the correct selection is to be made." ></td>
	<td class="line x" title="222:230	It should be possible to extend this work fruitfully in several directions, including the following." ></td>
	<td class="line x" title="223:230	Training with a measure defined directly on semantic representations is likely to lead to a further reduction in the disambiguation error rate." ></td>
	<td class="line x" title="224:230	The method for computing scaling factors described here has more recently been applied to optimizing preference selection for the task of choosing between analyses arising from different word hypotheses in a speech recognition system (Rayner et al. 1994) and is applicable to other problems, such as choosing between possible target representations in a machine translation system." ></td>
	<td class="line x" title="225:230	Finally, it would be interesting to combine the work on semantic collocation functions with that on similarity-based clustering (Pereira, Tishby, and Lee 1993; Dagan, Marcus, and Markovitch 1993), with the aim of overcoming the problem of sparse training data." ></td>
	<td class="line x" title="226:230	If this is successful, it might make these functions suitable for disambiguation in domains with larger vocabularies than ATIS." ></td>
	<td class="line x" title="227:230	Acknowledgments We would like to thank Manny Rayner for many useful suggestions in carrying out this work, for making the selections necessary to create the database of skeletal trees, and for helping with inspection of experimental results." ></td>
	<td class="line x" title="228:230	The paper has also benefited from discussions with Fernando Pereira, Ido Dagan, Michael Collins, Steve Pulman, and Jaan Kaja and from the comments of four anonymous Computational Linguistics referees." ></td>
	<td class="line x" title="229:230	Most of the work reported here was carried out under a Spoken Language Translation project funded by Telia (formerly Televerket/Swedish Telecom)." ></td>
	<td class="line x" title="230:230	Other parts were done under the CLARE project (JFIT project IED4/1/1165), funded by the UK Department of Trade and Industry, SRI International, the Defence Research Agency, British Telecom, British Petroleum, and British Aerospace." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E95-1037
Topic Identification In Discourse
Chen, Kuang-Hua;"></td>
	<td class="line x" title="1:128	Topic Identification in Discourse Kuang-hua Chen Department of Computer Science and Information Engineering National Taiwan University Taipei, Taiwan, R.O.C. khchen@nlg.csie.ntu.edu.tw Abstract This paper proposes a corpus-based language model for topic identification." ></td>
	<td class="line x" title="2:128	We analyze the association of noun-noun and noun-verb pairs in LOB Corpus." ></td>
	<td class="line x" title="3:128	The word association norms are based on three factors: 1) word importance, 2) pair co-occurrence, and 3) distance." ></td>
	<td class="line x" title="4:128	They are trained on the paragraph and sentence levels for noun-noun and nounverb pairs, respectively." ></td>
	<td class="line x" title="5:128	Under the topic coherence postulation, the nouns that have the strongest connectivities with the other nouns and verbs in the discourse form the preferred topic set." ></td>
	<td class="line x" title="6:128	The collocational semantics then is used to identify the topics from paragraphs and to discuss the topic shift phenomenon among paragraphs." ></td>
	<td class="line x" title="7:128	1 Introduction Although only speakers and writers instead of texts have topics (Brown and Yule, 1983, p. 68), natural language researchers always want to identify a topic or a set of possible topics from a discourse for further applications, such as anaphora resolution, information retrieval and so on." ></td>
	<td class="line x" title="8:128	This paper adopts a corpus-based approach to process discourse information." ></td>
	<td class="line x" title="9:128	We postulate that: (1) Topic is coherent and has strong relationships with the events in the discourse." ></td>
	<td class="line x" title="10:128	Now, consider the following example quoted from the Lancaster-Oslo/Bergen (LOB) Corpus (Johansson, 1986)." ></td>
	<td class="line x" title="11:128	The topics in this example are 'problem' and 'dislocation'." ></td>
	<td class="line x" title="12:128	The two words are more strongly related to the verbs ('explain', 'fell', 'placing' and 'suppose') and nouns ('theories', 'explanations', 'roll', 'codex', 'disorder', 'order', 'disturbance' and 'upheaval')." ></td>
	<td class="line x" title="13:128	There is a whole group of theories which attempt to explain the problems of the Fourth Gospel by explanations based on assumed textual dislocations." ></td>
	<td class="line x" title="14:128	The present state of the Gospel is the result of an accident-prone history." ></td>
	<td class="line x" title="15:128	The original was written on a roll, or codex, which fell into disorder or was accidentally damaged." ></td>
	<td class="line x" title="16:128	An editor, who was not the author, made what he could of the chaos by placing the fragments, or sheets, or pages, in order." ></td>
	<td class="line x" title="17:128	Most of those who expound a theory of textual dislocation take it for granted that the Gospel was written entirely by one author before the disturbance took place but a few leave it open to suppose that the original book had been revised even before the upheaval." ></td>
	<td class="line x" title="18:128	We also postulate that (2) Noun-verb is a predicate-argument relationship on the sentence level and noun-noun relationship is associated on discourse level." ></td>
	<td class="line x" title="19:128	The postulation (2) could be also observed from the above example." ></td>
	<td class="line x" title="20:128	These relationships may be represented implicitly by collocational semantics." ></td>
	<td class="line pc" title="21:128	Collocation has been applied successfully to many possible applications (Church et aal., 1989), e.g, lexicography (Church and Hanks, 1990), information retrieval (Salton, 1986a), text input (Yamashina and Obashi, 1988), etc. This paper will touch on its feasibility in topic identification." ></td>
	<td class="line x" title="23:128	This paper is organized as follows." ></td>
	<td class="line x" title="24:128	Section 2 presents a corpus-based language model and discuss how to train this model." ></td>
	<td class="line x" title="25:128	Section 3 touches on topic identification in discourse." ></td>
	<td class="line x" title="26:128	Section 4 shows a series of experiments based on the proposed model and discusses the results." ></td>
	<td class="line x" title="27:128	Section 5 gives short remarks." ></td>
	<td class="line x" title="28:128	2 A Language Model Brown and Yue (1983) pointed out there are two kinds of topics: one is sentence topic and the other is discourse topic." ></td>
	<td class="line x" title="29:128	The discourse topic is usually the form of topic sentence." ></td>
	<td class="line x" title="30:128	We postulate, further, that the noun in the topic sentence play important roles in the whole discourse." ></td>
	<td class="line x" title="31:128	Thus nouns play the core part in the underlying language model." ></td>
	<td class="line x" title="32:128	The associations of a noun with other nouns and verbs are supporting factors for it to be a topic." ></td>
	<td class="line x" title="33:128	267 The importance of a specific verb or noun is defined by Inverse Document Frequency (IDF) (Salton, 1986b): IDF(W) = log((P O(W)) / O(W)) + c (1) where P is the number of documents in LOB Corpus, i.e. 500, O(I4/) is the number of documents with word W, and c is a threshold value." ></td>
	<td class="line x" title="34:128	LOB Corpus is a million-word collection of present-day British English texts." ></td>
	<td class="line x" title="35:128	It contains 500 texts of approximately 2,000 words distributed over 15 text categories (Johansson, 1986)." ></td>
	<td class="line x" title="36:128	These categories include reportage, editorial, reviews, religion, skills, trades, popular lore, belles lettres, biography, essays, learned and scientific writings, fictions, humour, adventure and western fiction, love story, etc. That is to say, LOB Corpus is a balanced corpus." ></td>
	<td class="line x" title="37:128	The tag set of LOB Corpus is based on the philosophy of that of Brown Corpus (Francis and Kucera, 1979), but some modifications are made." ></td>
	<td class="line x" title="38:128	This is to achieve greater delicacy, while preserving comparability with the Brown Corpus." ></td>
	<td class="line x" title="39:128	Those words that appear more than one haft of the documents in LOB Corpus have negative log((P-." ></td>
	<td class="line x" title="40:128	O(W))/O(W)) shown below." ></td>
	<td class="line x" title="41:128	Noun: Verb: time(-3.68) way(-1.92) year(-1.71) man(-1.47) day(-1.12) part(-0.76) people(-0.75) thing(-0.73) hand(-0.54) life(-0.51) fact(-0.40) place(-0.40) work(-0.35) end(-0.12) case(-0.09) point(-0.05) make(-5.01) take(-3.56) give(-2.95) come(-2.45) find(-2.30) see(-2.26) know(-2.20) say(-2.18) go(-2.11) seem(-l.30) show(-l.20) think(-l.18) use(-1.07) get(-l.06) become(-0.95) bring(-0.73) put(-0.68) leave(-0.62) 1ook(-0.48) call(-0.43) tell(-0.41) keep(-0.32) hold(-0.18) ask(-0.23) begin(-0.08) The threshold values for nouns and verbs are set to 0.77 and 2.46 respectively." ></td>
	<td class="line x" title="42:128	The two values are used to screen out the unimportant words, whose 1DF values are negative." ></td>
	<td class="line x" title="43:128	That is, their 1DF values are reset to zero." ></td>
	<td class="line x" title="44:128	The strength of one occurrence of a verb-noun pair or a noun-noun pair is computed by the importance of the words and their distances: SNV(N~,~) = IDF(N~).IDF(V~)I D(N~,Vj) (2) SNN(N~, Nk) = IDF(N~).IDF(Nk) / D(N~, Nk) (3) where SNV denotes the strength of a noun-verb pair, SNN the strength of a noun-noun pair, and D(X,Y) represents the distance between X and Y. When i equals to k, the SNN(Ni,Nk) is set to zero." ></td>
	<td class="line x" title="45:128	Including the distance factor is motivated by the fact that the related events are usually located in the same texthood." ></td>
	<td class="line x" title="46:128	This is the spatial locality of events in a discourse." ></td>
	<td class="line x" title="47:128	The distance is measured by the difference between cardinal numbers of two words." ></td>
	<td class="line x" title="48:128	We assign a cardinal number to each verb and noun in sentences." ></td>
	<td class="line x" title="49:128	The cardinal numbers are kept continuous across sentences in the same paragraph." ></td>
	<td class="line x" title="50:128	For example, With so many problems 1 to solve2, it would be a great helP3 to select 4 some one problem 5 which might be the key 6 to all the others, and begin 7 there." ></td>
	<td class="line x" title="51:128	If there is any such keyproblem 8, then it is undoubtedly the problem 9 of the unitYlo of the Gospelll." ></td>
	<td class="line x" title="52:128	There are three viewsl2 of the Fourth Gospell3 which have been held14." ></td>
	<td class="line x" title="53:128	Therefore, the cardinal number of problems, C(problems), equals to 1 and C(held) is 14." ></td>
	<td class="line x" title="54:128	The distance can be defined to be D(Z,Y) = abs( C(X)-C( Y) ) (4) The association norms of verb-noun and noun-noun pairs are summation of the strengths of all their occurrences in the corpus: ANV(Nj, V~) = Z SNV(Ni' Vs) (5) ANN(Ni, N k) = Z SNN(N~, N k ) (6) where ANV denotes the association norm of a nounverb pair, and ANN the association norm of a nounnoun pair." ></td>
	<td class="line x" title="55:128	The less frequent word has a higher IDF value so that the strength SNV and SNN of one occurrence may be larger." ></td>
	<td class="line x" title="56:128	However, the number of terms to be summed is smaller." ></td>
	<td class="line x" title="57:128	Thus, the formulae IDF and ANV (ANN) are complementary." ></td>
	<td class="line x" title="58:128	LOB Corpus of approximately one million words is used to train the basic association norms." ></td>
	<td class="line x" title="59:128	They are based on different levels: the paragraph and sentence levels for noun-noun and noun-verb pairs respectively." ></td>
	<td class="line x" title="60:128	Table 1 shows the statistics of the training corpus." ></td>
	<td class="line x" title="61:128	The words with tags NC, NNU and NNUS and Ditto tags are not considered." ></td>
	<td class="line x" title="62:128	Here NC means cited words, and NNU (NNUS) denotes abbreviated (plural) unit of measurement unmarked for number." ></td>
	<td class="line x" title="63:128	Ditto tags are those words whose senses in combination differ from the role of the same words in other context." ></td>
	<td class="line x" title="64:128	For example, 'as to', 'each other', and 'so as to' (Johansson, 1986)." ></td>
	<td class="line x" title="65:128	268 Table 1." ></td>
	<td class="line x" title="66:128	Statistics for LOB Corpus number Document 500 Paragraph 18678 Sentences 54297 Nouns 23399 Verbs 4358 N-N Pairs 3476842 V-N Pairs 422945 Under the topic coherence postulation in a paragraph, we compute the connectivities of the nouns in each sentence with the verbs and nouns in the paragraph." ></td>
	<td class="line x" title="67:128	For example, 439 verbs in LOB Corpus have relationships with the word 'problem' in different degrees." ></td>
	<td class="line x" title="68:128	Some of them are listed below in descending order by the strength." ></td>
	<td class="line x" title="69:128	solve(225.21), face(84.64)  specify(16.55)  explain(6.47),  , fal1(2.52)  suppose(1.67)  For the example in Section 1, the word 'problem' and 'dislocation' are coherent with the verbs and nouns in the discourse." ></td>
	<td class="line x" title="70:128	The nouns with the strongest connectivity form the preferred topic set." ></td>
	<td class="line x" title="71:128	Consider the interference effects." ></td>
	<td class="line x" title="72:128	The constituents far apart have less relationship." ></td>
	<td class="line x" title="73:128	Distance D(X,Y) is used to measure such effects." ></td>
	<td class="line x" title="74:128	Assume there are m nouns and n verbs in a paragraph." ></td>
	<td class="line x" title="75:128	The connective strength of a noun Ni (1 < i < m) is defined to be: CSNN(N~) = Z (ANN(N, N k) / D(Ni, Ark) ) (7) k CSNV(N~) = Z (ANN(N~, V k) / D(N,, V k)) (8) k CS(N~) = (PN." ></td>
	<td class="line x" title="76:128	CSNN(N,) + PV." ></td>
	<td class="line x" title="77:128	CSNV(N~)) / c (9) where CS denotes the connective strength, and PAr and PV are parameters for CSNN and CSNV and PN+PV=I. The determination of par and PV is via deleted interpolation (Jelinek, 1985)." ></td>
	<td class="line x" title="78:128	Using equation PN + PV = 1 and equation 9, we could derive PAr and PV as equation 10 and equation 11 show." ></td>
	<td class="line x" title="79:128	CSCSNV PN = (10) CSNN CSNV CS CSNN PV (11) CSNV CSNN LOB corpus are separated into two parts whose volume ratio is 3:1." ></td>
	<td class="line x" title="80:128	Both PN and PV are initialized to 0.5 and then are trained by using the 3/4 corpus." ></td>
	<td class="line x" title="81:128	Alter the first set of parameters is generated, the remain 1/4 LOB corpus is run until par and PV converge using equations 9, 10 and 11." ></td>
	<td class="line x" title="82:128	Finally, the parameters, PN and PV, converge to 0.675844 and 0.324156 respectively." ></td>
	<td class="line x" title="83:128	3 Topic Identification in a Paragraph The test data are selected from the first text of the files LOBT-DI, LOBT-F1, LOBT-G1, LOBT-H1, LOBT-KI, LOBT-M1 and LOBT-N1 of horizontal version of LOB Tagged Corpus for inside test (hereafter, we will use D01, F01, G01, H01, K01, M01, and N01 to represent these texts respectively)." ></td>
	<td class="line x" title="84:128	Category D denotes religion, Category F denotes popular lore, Category G denotes belles lettres, biography and essays, Category H denotes Miscellaneous texts, Category K denotes general fiction, Category M denotes science fiction, and Category N denotes adventure and western fiction." ></td>
	<td class="line x" title="85:128	Each paragraph has predetermined topics (called assumed topics) which are determined by a linguist." ></td>
	<td class="line x" title="86:128	Because a noun with basic form N may appear more than once in the paragraph, say k times, its strength is normalized by the following recursive formula: NCS( N m) ) = CS( N o(~) ) (12) NCS( No(o) = NCS( No(,_,) ) + (1 NCS(No(,_,))).CS(No(o) (13) where NCS represents the net connective strength, o(k) denotes the cardinal number of the k'th occurrence of the same N such that C(NoO)) < C(No(2)) < C(No(3)) < < C(No(k-l)) < C(No(k))." ></td>
	<td class="line x" title="87:128	The possible topic N* has the high probability NCS(N*)." ></td>
	<td class="line x" title="88:128	Here, a topic set whose members are the first 20% of the candidates is formed." ></td>
	<td class="line x" title="89:128	The performance can be measured as the Table 2 shows." ></td>
	<td class="line x" title="90:128	4 The Preliminary Experimental Results According to the language model mentioned in Section 2, we build the ANN and ANV values for each noun-noun pair and noun-verb pair." ></td>
	<td class="line x" title="91:128	Then, we apply recursive formula of NCS shown in equations 12 and 13 to identifying the topic set for test texts." ></td>
	<td class="line x" title="92:128	Table 3 shows experimental results." ></td>
	<td class="line x" title="93:128	Symbols tx and c denotes mean and standard deviation." ></td>
	<td class="line x" title="94:128	(+) denotes correct number, (-) denotes error number and ()?" ></td>
	<td class="line x" title="95:128	denotes undecidable number in topic identification." ></td>
	<td class="line x" title="96:128	The undecidable case is that the assumed topic is a pronoun." ></td>
	<td class="line x" title="97:128	Figure 1 shows correct rate, error rate, and undecidable rate." ></td>
	<td class="line x" title="98:128	Row (1) in Table 3 shows the difficulty in finding topics from many candidates." ></td>
	<td class="line x" title="99:128	In general, there are more than 20 candidates in a paragraph, It is impossible to select topics at random." ></td>
	<td class="line x" title="100:128	Row (2) gives 269 the rank of assumed topic." ></td>
	<td class="line x" title="101:128	The assumed topics are assigned by a linguist." ></td>
	<td class="line x" title="102:128	Comparing row (1) and row (2), the average number of candidates are much larger than the rank of assumed topic." ></td>
	<td class="line x" title="103:128	Since it is impossible to randomly select candidates as topics, we know topic identification is valuable." ></td>
	<td class="line x" title="104:128	Rows (3), (4) and (5) list the frequencies of candidates, assumed topics and computed topic." ></td>
	<td class="line x" title="105:128	The results intensify the viewpoint that the repeated words make persons impressive, and these words are likely to be topics." ></td>
	<td class="line x" title="106:128	Our topic identification algorithm demonstrates the similar behavior (see rows (4) and (5))." ></td>
	<td class="line x" title="107:128	The average frequencies of assumed topics and computed topics are close and both of them are larger than average frequency of candidates." ></td>
	<td class="line x" title="108:128	Figure 2 clearly demonstrates this point." ></td>
	<td class="line x" title="109:128	Row (6) reflects an interesting phenomenon." ></td>
	<td class="line x" title="110:128	The topic shifted by authors from paragraph to paragraph is demonstrated through comparison of data shown in this row and row (2)." ></td>
	<td class="line x" title="111:128	The rank value of previous topics do obviously increase." ></td>
	<td class="line x" title="112:128	Recall that large rank value denotes low rank." ></td>
	<td class="line x" title="113:128	Table 2." ></td>
	<td class="line x" title="114:128	Metrics for Performance 1 average # of candidates 2 average rank of assumed topic 3 frequency of candidates 4 frequency of assumed topic 5 frequency of computed topic 6 average rank of topic in previous paragraph E # of nouns in basic form in paragraph i / # of paragraphs E rank of assumed topic in paragraph i/# of paragraphs y. # of nouns / E # of nouns in basic form in paragraph i E occurrences of assumed topic / # of paragraphs E occurrences of computed topic / # of paragraphs E rank of topic in previous paragraph / (# of paragraph 1) (F, '~) Table 3." ></td>
	<td class="line x" title="115:128	Experimental Results D01 F01 G01 H01 K01 M01 N01 (1) (21.59, 9.96) (10.57, 18.42) (62.43, 18.42) (19.77, 8.39) (31.71, 23.80) (15.22, 6.44) (12.21, 6.73) (2) (4.56, 5.98) (5.25, 5.51) (7.29, 10.35) (4.55, 4.13) (7.08, 16.02) (2.61, 2.11) (3.68, 3.87) (3) (1.32, 0.88) (1.39, 0.89) (1.21, 0.56) (1.33, 0.82) (1.11, 0.39) (1.11, 0.32) (1.06, 0.25) (4) (2.61, 1.60) (1.27, 1.21) (2.57, 1.18) (2.46, 1,62) (1.77, 1.05) (1.50, 0.69) (1.28, 0.60) (5) (3.33, 1.97) (2.39, 1.84) (3.43, 1.40) (2.91, 1.56) (1.86, 0.99) (1.48, 0.50) (1.29, 0.52) (6) (6.29, 7.84) (5.48, 5.09) (19.67, 16.64) (5.71, 6.06) (17.23, 18.51) (7.92, 6.28) (9.36, 6.62) (+) 12 13 6 12 9 13 15 (-) 6 15 1 10 4 5 10 ()?" ></td>
	<td class="line x" title="116:128	0 0 0 0 1 9 9 Text N01 M01 K01 H01 G01 F01 D01 10 2O Number of Paragraphs Figure 1." ></td>
	<td class="line x" title="117:128	The Results of Topic Identification ible ()?" ></td>
	<td class="line x" title="118:128	+) I 3O 40 270 Text N01 M01 K01 H01 G01 F01 D01 )pic l)ic 1 2 Mean Figure 2." ></td>
	<td class="line x" title="119:128	Comparison of Frequency ml 4 5 Concluding Remarks Discourse analysis is a very difficult problem in natural language processing." ></td>
	<td class="line x" title="120:128	This paper proposes a corpus-based language model to tackle topi.c identification." ></td>
	<td class="line x" title="121:128	The word association norms of nounnoun pairs and noun-verb pairs which model the meanings of texts are based on three factors: 1) word importance, 2) pair occurrence, and 3) distance." ></td>
	<td class="line x" title="122:128	The nouns that have the stronger connectivities with other nouns and verbs in a discourse could form a preferred topic set." ></td>
	<td class="line x" title="123:128	Inside test of this proposed algorithm shows 61.07% correct rate (80 of 131 paragraphs)." ></td>
	<td class="line x" title="124:128	Besides topic identification, the algorithm could detect topic shift phenomenon." ></td>
	<td class="line x" title="125:128	The meaning transition from paragraph to paragraph could be detected by the following way." ></td>
	<td class="line x" title="126:128	The connective strengths of the topics in the previous paragraph with the nouns and the verbs in the current paragraph are computed, and compared with the topics in the current paragraph." ></td>
	<td class="line x" title="127:128	As our experiments show, the previous topics have the tendency to decrease their strengths in the current paragraph." ></td>
	<td class="line x" title="128:128	Acknowledgment We are thankful to Yu-Fang Wang and Yue-Shi Lee for their help in this work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W95-0111
Automatic Suggestion Of Significant Terms For A Predefined Topic
Zhou, Joe F.;Dapkus, Pete;"></td>
	<td class="line x" title="1:189	I Automatic Suggestion of Significant Terms for a Predefined Topic Joe Zhou and Pe~Dapkus LEXIS-NEXIS, a Division of Reed Elsevier, Inc. 9555 Springboro Pike Miamisburg, OH 45342 {joez,peted} @ lexis-nexis.com ABSTRACT This paper presents a preliminary experiment in automatically suggesting significant terms for a predefined topic." ></td>
	<td class="line x" title="2:189	The general method is to compare a topically focused sample created around the predefined topic with a larger and more general base sample." ></td>
	<td class="line x" title="3:189	A set of statistical measures are used to identify significant word units in both samples." ></td>
	<td class="line x" title="4:189	Identification of single word terms is based on the notion of word intervals." ></td>
	<td class="line x" title="5:189	Two-word terms are identified through the computation of mutual information, and the extension of mutual information assists in capturing multi-word terms." ></td>
	<td class="line x" title="6:189	Once significant terms of all these three types are identified, a comparison algorithm is applied to differentiate terms across the two data samples." ></td>
	<td class="line x" title="7:189	If significant changes in the values of certain statistical variables are detected, associated terms will selected as being topic-oriented and included in a suggested list." ></td>
	<td class="line x" title="8:189	To check the quality of the suggested terms, we compare them against terms manually determined by the domain expert." ></td>
	<td class="line x" title="9:189	Though overlaps vary, we find that the automatical suggestion provides more terms that are useful for describing the predefined topic." ></td>
	<td class="line x" title="10:189	1." ></td>
	<td class="line x" title="11:189	INTRODUCTION As we are facing the growing amount of on-line text, the use of text analysis techniques to access information from electronic sources has become more popular and, at the same time, more difficult." ></td>
	<td class="line x" title="12:189	Currently, the effectiveness of such techniques is evaluated not only on how easily they can be applied to text sources to extract information and represent it in a systematic format (Walker 1983), but also on whether they can be applied to large text corpora of several tens of thousand of words." ></td>
	<td class="line x" title="13:189	One of the applications of text analysis is to identify and extract significant terminology from running text." ></td>
	<td class="line x" title="14:189	Choueka (1988), for example, describes an experiment for locating interesting collocational expressions from large textual databases." ></td>
	<td class="line x" title="15:189	A collocational expression, as Choueka defines it, is =sequences of words whose unambiguous meaning cannot be dedved from that of their components'." ></td>
	<td class="line oc" title="16:189	Other representative collocation research can be found in Church and Hanks (1990) and Smadja (1993)." ></td>
	<td class="line x" title="17:189	Though all statistically-based, their definitions of collocations are different from one another." ></td>
	<td class="line oc" title="18:189	Unlike Choueka (1988), Church and Hanks (1990) identify as collocations both interrupted and uninterrupted sequences of words." ></td>
	<td class="line nc" title="19:189	Unlike Church and Hanks (1990), Smadja (1993) goes beyond the 'two-word' limitation and deals with 'collocations of arbitrary length'." ></td>
	<td class="line x" title="20:189	131 The primary goal of collocation research is to build a comprehensive lexicographic toolkit, or to assist automatic language generation applications." ></td>
	<td class="line x" title="21:189	Therefore, the focus is on the extraction of all Interesting word pattems without distinction of domain specificity." ></td>
	<td class="line x" title="22:189	Identifying domain-specific terminology is another research effort." ></td>
	<td class="line x" title="23:189	Gierl and Frost (1992) descdbe their approach to extracting terminological knowledge from medical texts." ></td>
	<td class="line oc" title="24:189	Following Church and Hanks (1990), they use mutual information to select significant two-word patterns, but, at the same time, a lexical inductive process is incorporated which, as they claim, can improve the collection of domain-specific terms." ></td>
	<td class="line x" title="25:189	Justeson and Katz (1993) introduce an algorithm by which technical terms in running text can be identified." ></td>
	<td class="line x" title="26:189	Prior to the development of their algorithm, they performed a thorough study on the linguistic properties of technical terminology." ></td>
	<td class="line x" title="27:189	They report that, structurally, technical terms make heavy use of noun compounds." ></td>
	<td class="line x" title="28:189	In technical terminology, word constituents are limited to adjectives, nouns and occasionally prepositions." ></td>
	<td class="line x" title="29:189	Verbs, adverbs, or conjunctions are extremely rare." ></td>
	<td class="line x" title="30:189	At the discourse level, technical terms tend to be repetitive." ></td>
	<td class="line x" title="31:189	With these observations in mind, they developed an algorithm which has proved to be effective and domain independent." ></td>
	<td class="line x" title="32:189	In this paper, a preliminary experiment is presented in automatically suggesting significant terms for a predefined topic." ></td>
	<td class="line x" title="33:189	The general method is to compare a topic focused sample based on the predefined topic with a larger and more general base sample." ></td>
	<td class="line x" title="34:189	A set of statistical measures are used to identify significant word units in both samples." ></td>
	<td class="line x" title="35:189	Identification of single word terms is based on the notion of word intervals." ></td>
	<td class="line x" title="36:189	Two-word terms are identified through the computation of mutual information, and an extension of mutual information assists in capturing multi-word terms." ></td>
	<td class="line x" title="37:189	Once significant terms of all these three types are identified, a comparison algorithm is applied to differentiate terms across the two samples." ></td>
	<td class="line x" title="38:189	If significant changes in the values of certain statistical variables are detected, associated terms are selected from the focused sample as being topic-oriented and included in a suggested list." ></td>
	<td class="line x" title="39:189	To check the quality of the suggested terms, we compare them against terms manually determined by a domain expert." ></td>
	<td class="line x" title="40:189	Though the numbers of matches vary, we find that our automatic suggestion process provides more terms (than the manual process) that are useful for describing the predefined topic." ></td>
	<td class="line x" title="41:189	2." ></td>
	<td class="line x" title="42:189	METHODOLOGY 2.1 Manual versus Automatic Term Suggestion TO manually select significant terms for a predefined topic, the domain expert first creates a topic focused sample from one specific source or a combination of sources." ></td>
	<td class="line x" title="43:189	Then, he or she reads the documents, providing a relevance judgment (i.e. a reader-assigned score) to each document." ></td>
	<td class="line x" title="44:189	By carefully examining relevant documents in the focused sample, a list of terms that are deemed to be significant for the definition of the topic is identified." ></td>
	<td class="line x" title="45:189	In many cases, it is possible that the domain expert would introduce some terms based on his or her own professional knowledge about the topic." ></td>
	<td class="line x" title="46:189	These terms may be highly prominent for the topic, yet may not necessarily occur in the focused sample." ></td>
	<td class="line x" title="47:189	132 For automatic suggestion of topical terms, initial attempts were made using the sample documents the domain expert created." ></td>
	<td class="line x" title="48:189	The results were not impressive." ></td>
	<td class="line x" title="49:189	The statistical Information generated from the sample documents was not rich and sufficient enough for any discriminative judgment." ></td>
	<td class="line x" title="50:189	Our experience showed that, to draw terms that are reflective of a given topic, a much larger and more general base sample is required." ></td>
	<td class="line x" title="51:189	Such a base sample should be randomly sampled from the same source as the focused sample and it should contain an array of different topics." ></td>
	<td class="line x" title="52:189	Once the baseline statistics are generated from both data collections, a meaningful comparison could spot terms that occur with unusual frequency in the focused sample." ></td>
	<td class="line x" title="53:189	These terms would constitute good candidates for topically sensitive terminological units (Steier and Belew 1994)." ></td>
	<td class="line x" title="54:189	2.2 Focused Sample and Base Sample For our experiments of automatic term suggestion, we selected a predefined topic called 'European Politics and Business'." ></td>
	<td class="line x" title="55:189	The focused sample was originally created by the domain expert using the 1988 United Press International (UPI)." ></td>
	<td class="line x" title="56:189	Table 1 presents statistical information about this dataset." ></td>
	<td class="line x" title="57:189	After reading each of the relevant documents found in the focused sample, the domain expert manually determined 347 topical terms." ></td>
	<td class="line x" title="58:189	Table 2 provides the statistical breakdown of these terms." ></td>
	<td class="line x" title="59:189	Table 1: Focused and Base Samples Data File Source/Name Size (bytes) Unique Words Focused Sample Sample from 1988 UPI 1,015200 12,065 (5,045') Base Sample Sample from 27,322,598 73,583 (33,114') 1987,1988,1989 UPI * only words which occur more than 3 times were used in the experiments Table 2: Predefined Topic and its Manually Determined Topical Terms Predefined Topic one-word two.word multi.word total terms i European Politics & Business 276 36 35 347 Since the focused sample was drawn from the source of 1988 UPI, the construction of its corresponding base sample was also initiated from the same source of the same year." ></td>
	<td class="line x" title="60:189	Our experiments demonstrated that, in order to obtain a random assortment of topics to be included in the base sample, it may be meaningful to sample documents from the time pedod before and after the focused documents." ></td>
	<td class="line x" title="61:189	Therefore, the final base sample was created by randomly drawing documents from the years of 1987, 1988 and 1989." ></td>
	<td class="line x" title="62:189	The size of this dataset is about 27 times larger than the sample data file (see Table 1 )." ></td>
	<td class="line x" title="63:189	133 Though the ratio between the focused and base samples was arbitrary, in order to generate meaningful statistics, we felt that the base sample should be at least 20 times larger in size than the focused sample." ></td>
	<td class="line x" title="64:189	(For the sake of discussion, hereafter, we may sometimes refer to the focused sample as 'focused' and the base sample as 'base')." ></td>
	<td class="line x" title="65:189	2.3 Experimental Procedure The general method we adopted is as follows." ></td>
	<td class="line x" title="66:189	First, we identified statistically significant terms from both samples." ></td>
	<td class="line x" title="67:189	Next, a comparison algorithm was applied to these two sets of terms to single out those that were common to both samples, yet whose patterns of occurrences differed between these two samples." ></td>
	<td class="line x" title="68:189	Finally, we analyzed and presented this set of terms as content odented candidates for the predefined topic, in this case 'European Politics and Business'." ></td>
	<td class="line x" title="69:189	The terms suggested are split into three categones: single word terms, two-word terms and multi-word terms (or phrases)." ></td>
	<td class="line x" title="70:189	The following three sections descnbe in detail the methods for generating each of the three categories." ></td>
	<td class="line x" title="71:189	2.4 Suggesting Single Word Terms Automatically suggesting single word terms as being topically oriented has been most challenging." ></td>
	<td class="line x" title="72:189	Our experiments indicated that the ffirst order' statistics, probability and entropy alone, are not sufficient for gathering information about the topicality of a word in running text." ></td>
	<td class="line x" title="73:189	The information in both measurements is essentially equivalent since entropy is just the log inverse of probability." ></td>
	<td class="line x" title="74:189	We found that the 'second-order' statistics, such as vadance or standard deviation of term frequencies across documents, provide greater insight into topicality." ></td>
	<td class="line x" title="75:189	We selected the interval between the occurrences of a word as the basis for analysis." ></td>
	<td class="line x" title="76:189	Our intuitions led us to believe that topical single words should appear more frequently and more regularly, i.e. at approximately even intervals, in the focused sample than in the base sample." ></td>
	<td class="line x" title="77:189	The focused sample represents, more or less, a topical sublanguage set while the base sample a general language set." ></td>
	<td class="line x" title="78:189	Unlike probability and entropy statistics which yield average scores for the whole document, the use of interval makes it possible to get an 'instantaneous' measure at any location in the document." ></td>
	<td class="line x" title="79:189	More specifically, an interval can be measured 'instantaneously' at any point in the text between the occurrences of a particular word." ></td>
	<td class="line x" title="80:189	Though using interval alone might still not be sufficient for identifying word topicality, it allowed us to measure the vadance which would help identify words that were always changing in their rate of occurrences." ></td>
	<td class="line x" title="81:189	Thus, three scores were generated for each word: the mean log interval, the standard deviation of the mean log interval, and the normalized standard deviation of the mean log interval." ></td>
	<td class="line x" title="82:189	The use of a log scale for these measurements is to minimize the effect of unduly large variations in words with long mean intervals." ></td>
	<td class="line x" title="83:189	The normalized standard deviation is produced by simply dividing the raw standard deviation by the mean log interval." ></td>
	<td class="line x" title="84:189	In most cases, raw standard deviation is found to be larger for words having long mean intervals." ></td>
	<td class="line x" title="85:189	In order to compare the standard deviations across words of different intervals, we found this normalization process quite useful." ></td>
	<td class="line x" title="86:189	134 i After scores were generated for all the words in both the focused sample and the base sample, score comparisons between the two samples were carried out in two ways: comparing the intervals and comparing the standard deviations." ></td>
	<td class="line x" title="87:189	To compare the intervals, the =base' mean log interval was subtracted from the 'focused' mean log Interval and divided by the raw standard deviation from the base sample." ></td>
	<td class="line x" title="88:189	The result represents the change of mean log intervals." ></td>
	<td class="line x" title="89:189	More explicitly, it yields the number of standard deviations that the 'focused' mean log interval is different from the =base' mean log interval." ></td>
	<td class="line x" title="90:189	The more negative;the value, the more significant the change, and the more prominent the word would appear in the focused sample." ></td>
	<td class="line x" title="91:189	To compare the standard deviations, the normalized =base' standard deviation was subtracted from the normalized 'focused' standard deviation." ></td>
	<td class="line x" title="92:189	The difference symbolizes how the word is distributed in ~e focused sample." ></td>
	<td class="line x" title="93:189	The more negative the value is, the more 'bursty' the word is distributed, and the more likely it is content oriented since 'content words tend to appear in 'bursts' (Church and Mercer 1993)." ></td>
	<td class="line x" title="94:189	If a single word term is found in both data samples and it receives negative scores from both interval and standard deviation comparisons, it would be included in the suggested list as being topical onented." ></td>
	<td class="line x" title="95:189	2.5 Suggesting Two-Word Terms The method for suggesting two-word terms tumed out to be much simpler than that for single word terms though the same techniques are equally applicable." ></td>
	<td class="line x" title="96:189	Here, the traditional mutual information score was used." ></td>
	<td class="line x" title="97:189	As stated in Church, et al.(1991) and elsewhere, the mutual information measurement can be expressed as:." ></td>
	<td class="line x" title="99:189	(:p(WlW2)) l(WlW2) = lg~,p(wl)p(w2) where p(wlw2) is the frequency in the data collection of the two-word compound (wl,w2); and p(wl) and p(w2) the frequency of the word constituents." ></td>
	<td class="line x" title="100:189	The highest mutual information score indicates that the individual probabilities are low while the two words occur together frequently." ></td>
	<td class="line x" title="101:189	Two steps led to our automatic suggestion of topic-oriented two-word terms." ></td>
	<td class="line x" title="102:189	First, the mutual information score was computed for each pair of words that occur in each of the two samples." ></td>
	<td class="line x" title="103:189	To capture topicality, we were only interested in pairs of words with high mutual information scores." ></td>
	<td class="line x" title="104:189	Therefore, any pair which contained =closed class' words, such as determiners, prepositions, auxiliaries, or single letters, digit numbers, or overly common verbs like 'give', 'take', etc. , were excluded." ></td>
	<td class="line x" title="105:189	Such an exclusion not only helped getting pairs of words with high mutual information scores, but also sped up computation significantly." ></td>
	<td class="line x" title="106:189	A threshold value was also set such that if any two-word unit occurred less than 3 times in the sample or received a mutual information score lower than 6.0, it was eliminated and would not participate in the next comparison measurement." ></td>
	<td class="line x" title="107:189	135 With the mutual information scores in hand, a 'delta' score was generated by subtracting the 'base' mutual information score from the ffocused' mutual information score." ></td>
	<td class="line x" title="108:189	Topically, prominent two-word terms normally have lower scores in the focused sample that is 'keyed' to their topic." ></td>
	<td class="line x" title="109:189	This is because the constituent words distribute in wider range of contexts." ></td>
	<td class="line x" title="110:189	The probability of them occurring separately increases relative to the probability of them occurring together (Steier and Belew 1994)." ></td>
	<td class="line x" title="111:189	Therefore, the more negative the 'delta' score, the more topically sensitive the two-word term is. If a two-word term occurs in both data samples and receives a negative 'delta' score, it would be included in the suggested list as being topically onented." ></td>
	<td class="line x" title="112:189	2.6 Suggesting Multi-Word Terms When automatically suggesting content two-word terms, we looked at the mutual information scores for adjacent words." ></td>
	<td class="line x" title="113:189	For multi-word terms, the mutual information score was calculated for non-adjacent words." ></td>
	<td class="line x" title="114:189	Our intuitions led us to believe that if there is a significant statistical linkage, i.e. a high mutual information score, between such a pair of words, it is highly possible that they belong to a larger linguistic component." ></td>
	<td class="line x" title="115:189	Our first step was to compute mutual information scores for a word unit separated by a distance of two (i.e. having one unspecified word separating them)." ></td>
	<td class="line x" title="116:189	Two cdteda apply when selecting 'interesting' word units." ></td>
	<td class="line x" title="117:189	Their mutual information score must be 10 or greater." ></td>
	<td class="line x" title="118:189	Following the observations by Steier and Belew (Steier and Belew1994), we only selected pairs which received lower mutual information score in the focused sample than in the base sample." ></td>
	<td class="line x" title="119:189	Once an 'interesting' word unit of distance two was selected, a concordance was built of all sentences containing that word unit." ></td>
	<td class="line x" title="120:189	These sentences were compared for matching text." ></td>
	<td class="line x" title="121:189	If a stdng of text was found to include that word unit and, at the same time, occur most frequently in the concordance, its leading and trailing 'closed-set' words (if any) were chopped off." ></td>
	<td class="line x" title="122:189	The remaining text stdng was presented as a suggested multi-word term." ></td>
	<td class="line x" title="123:189	3." ></td>
	<td class="line x" title="124:189	RESULTS and DISCUSSION 3.1 Suggested Single Terms The focused sample drawn from the 1988 UPI data contains 12,065 unique words." ></td>
	<td class="line x" title="125:189	Among them, 5,045 are frequent enough (occurring 3 times or more) to calculate statistics for our experiments (refer to Table 1)." ></td>
	<td class="line x" title="126:189	The comparison algorithm identified 2,010 suggested terms based on the fact that they received negative scores for both 'change of mean log interval' and 'distribution burstiness' comparisons." ></td>
	<td class="line x" title="127:189	These negative scores indicate that these single word terms have shorter intervals and more regular occurrences in the focused sample." ></td>
	<td class="line x" title="128:189	We compared the suggested list against the single word terms manually selected by the domain expert." ></td>
	<td class="line x" title="129:189	The results are summarized in Table 3." ></td>
	<td class="line x" title="130:189	136 Table 3: Statistics of the Suggested Single Word Terms suggested 2,010 Comparison of Suggested and Manual Terms total manual 276 not possible* 129 no statistics* 91 possible* 56 hits 42 percent included 75% * not possible: terms not existing In the focused sample * no statbtics: terms which have less than 3 occurrences in the focused sample * possible: targeted terms Of the 276 topical single terms determined by the domain expert, 129 terms do not exist in the focused sample." ></td>
	<td class="line x" title="131:189	As explained earlier, these are the terms intellectually introduced by the domain expert." ></td>
	<td class="line x" title="132:189	Almost half of these terms are geographical names in Europe, such as albania, albertville, andorra, barcelona, belarus, belorus, bosnia, byelorussia, chancellors, comecon, cp, croatia, erm, eurocurrency, eurofed, europeanization, europeanwide, europeenne, europewide, gaullist, gaullists, gilbraltar, greenland, guemsey, kazakhstan, kirghizia, kirgizia, kyrgystan, kzakhstan, labour, liechtenstein, moldavia, moldova, monaco, nc, nib, nicosia, nuuk, pentagonale, reunify, reykjavik, salzburg, sicily, slovenia, svalbard, tadzhikistan, tajikistan, tajikstan, tirana, tirane, tories, torshavn, turkmenia, turkmenistan, uk, ussr, uzbekistan, vaduz, valletta, weu Of the remaining 147 actually occurring terms, 91 are not frequent enough to be included in our experiments." ></td>
	<td class="line x" title="133:189	They occur in the focused sample two times or less." ></td>
	<td class="line x" title="134:189	Again, some of them are geographical names in Europe." ></td>
	<td class="line x" title="135:189	amsterdam, athens, azerbaljan, bulgaria, estonia, euro, eurodollar, eurodollars, georgia, hamburg, holland, iceland, jersey, latvia, liberals, lithuania, naples, oecd, prague, reunified, rome, russia, serbia, sofia, tory, ukraine, unification These non-existent and under-represented terms left us with a maximum of 56 terms we could catch in the suggested ten'ns list." ></td>
	<td class="line x" title="136:189	Of these, 42 were caught with an accuracy rate of 75% (see Appendix for details)." ></td>
	<td class="line x" title="137:189	Further analysis of the missing 14 terms reveals that they were not found in the suggested list due to the statistical constraints we established for our experiments." ></td>
	<td class="line x" title="138:189	As shown in Table 4, 13 of these terms received negative scores either for 'change of mean log interval' or for 'distribution burstiness', but not for both." ></td>
	<td class="line x" title="139:189	We believe that their inclusion is possible since they represent what we would call 'border-line' suggested terms." ></td>
	<td class="line x" title="140:189	137 Table 4: =Missed' single word terms single.word term dgtl dgt2 dgt3 dgt4 dgt5 dgt6 dgt7 dgt8 portugal 10 13.75 0.26 16.86 0.23 3.83 -0.81 0.03 europeans 23 12.55 0.35 15.64 0.32 4.98 -0.62 0.03 eec 3 15.49 0.39 19.28 0.32 6.21 -0.61 0.06 luxembourg 12 13.49 0.42 17.06 0.36 6.07 -0.59 0.07 !copenhagen 3 15.49 0.47 18.54 0.34 6.23 -0.49 0.14 i ;~ cyprus 6 14.49 0.44 18.28 0.43 7.89 -0.48 0.01 yugoslavia 12 13.49 0.47 15.33 0.37 5.66 -0.32 0.10 finland 10 13.75 0.51 15.52 0.46 7.19 -0.25 0.05 kgb 5 14.75 0.57 16.41 0.44 7.29 .-0.23 0.13 sweden 13 13.38 0.48 14.26 0.44 6.33 -0.14 0.03 turkey 11 13.62 0.53 14.47 0.50 7.25 -0.12 0.03 czechoslovakia 9 13.91 0.09 13.70 0.46 6.29 0.03 -0.36 switzerland 9 13.91 0.21 13.81 0.47 6.48 0.01-0.26 Statistics Measurements (dgt = digit) dgtl: number of occurrences On the focused sample) dgt2: mean log interval (in the focused sample) dgt3: normalized SD of mean log interval (in the focused sample) dgt4: mean log interval (in the base sample) dgtS: normalized SD of mean log interval (in the base sample) dgt6: raw SD of mean log interval (in the base sample) dgt7: ((2nd digit 4th digit) / 6th digit)) dgtS: (3rd digit." ></td>
	<td class="line x" title="141:189	Sth digit) Admittedly, the suggested list with the total of 2,010 terms is a fairly large one." ></td>
	<td class="line x" title="142:189	It obviously contains terms that are not topic oriented." ></td>
	<td class="line x" title="143:189	We followed the observations made by Justeson and Katz (1993) and introduced a =post-editing' process." ></td>
	<td class="line x" title="144:189	As a result, the list was reduced to 886 terms." ></td>
	<td class="line x" title="145:189	Basically, we removed from the original list all the =closed-set' words such as determiners, prepositions, auxiliaries, conjunctions, single letters, etc. , as well as other less semantically laden words such as adverbs and verbs." ></td>
	<td class="line x" title="146:189	3.2 Suggested Two-Word Terms Among 512 =interesting' two-word terms, 170 receive negative =delta' scores." ></td>
	<td class="line x" title="147:189	These 164 terms were presented in our suggested two-word terms (see Appendix for details)." ></td>
	<td class="line x" title="148:189	138 I A total of 36 topical terms were manually determined based on the UPI focused sample." ></td>
	<td class="line x" title="149:189	Of this number, only 26 are actually existent terms, which means that 10 terms were introduced independent of the source material." ></td>
	<td class="line x" title="150:189	Among these 26 terms, 6 were too infrequent to generate meaningful statistics though the mutual information scores are high (see Table 5)." ></td>
	<td class="line x" title="151:189	Five terms, i.e. E C, U K, the Channel, the Continent, and the Wal/failed to participate in statistical screening because they contain 'closed-set' words, i.e. single letters and the determiner the." ></td>
	<td class="line x" title="152:189	Table 5: 'No statistics' two-word terms two-word term digit1 digit2 monte carlo 1 13.61674723 i social democrats 1 9.58432575 coalition govea'nment . 1 7.59034954 supreme soviet 1 5.06985277 J downing street 1 11.75425075 socialist party 2 6.36709503 Statistical measurements digitl: frequency (in the focused sample) digit2: mutual information score Of the remaining catchable15 two-word terms, 8 are included in the suggested list." ></td>
	<td class="line x" title="153:189	Table 6 summarizes the statistics of the suggested two-word terms." ></td>
	<td class="line x" title="154:189	Table 6: Statistics of the Suggested Two-Word Terms Comparison of Suggested and Manual Terms total total not no l percent suggested manual possible* statistics* possible* hits i included \[ 170 36 10 11 15 8 53% * not possible: terms not existing In the focused sample * no statistics: terms which have less than 3 occurrences in the focused sample * possible: targeted terms Further screening revealed that 3 manually selected two-word terms (i.e. cold war, common market, and North Sea) were actually captured in the 512 'interesting' list." ></td>
	<td class="line x" title="155:189	They were not included in the suggested list because they did not receive negative 'delta' scores." ></td>
	<td class="line x" title="156:189	The suggested list fails to include 4 manually selected two-word terms because their mutual information scores go up." ></td>
	<td class="line x" title="157:189	Typically, content oriented two-word terms within the topically related subset of documents are expected to go down." ></td>
	<td class="line x" title="158:189	This might be caused by the individual word probabilities." ></td>
	<td class="line x" title="159:189	To use Steier and Belew's terms (Steier and Belew 1994), these pairs appear more 'opaque', meaning that their constituent words are more probable individually than when they are combined inthe focused sample." ></td>
	<td class="line x" title="160:189	Table 7 lists these 4 two-word terms appearing in both samples." ></td>
	<td class="line x" title="161:189	139 Table 7: 'Missed' two-word terms Sample two-word term frequency MI score 'base' atlantic alliance 11 8.80256520 'focused' atlantic alliance 4 9.36193333 'base' cold war 54 8.04486800 'focused' cold war 11 9.97241419 'base' common market 26 6.86310460 'focused' common market 17 7.84030540 'base' 49 'focused' united kingdom united kingdom 25 7.55353160 7.80705217 Our suggested two-word terms list (see the Appendix) contains quite a number of useful additional terms about the targeted predefined topic 'European Politics and Business'." ></td>
	<td class="line x" title="162:189	The following are some examples: US-European relations/politics: armed forces, diplomatic relations, nuclear missiles, nuclear weapons, trade barriers European Business: bilateral trade, economic reform, market integration, pdvate enterprise, pdvate investment Notable European entities: banca commerciale, berlin wall, bdtish spies, swiss francs, brussels belgium Heads of state: felipe gonzalez, francois mitterrand, mikhail gorbachev 3.3 Suggested Multi-Word Terms A total of 97 multi-word terms were extracted from the focused sample for inclusion in the suggested list (see Appendix)." ></td>
	<td class="line x" title="163:189	Admittedly, some of them are simply sentence fragments instead of real phrases." ></td>
	<td class="line x" title="164:189	Of the 35 multi-word terms manually selected by the domain expert, 26 actually occur in the focused sample." ></td>
	<td class="line x" title="165:189	As with the single word and two-word terms, the other 9 multi-word terms are simply intellectual introductions from the domain expert." ></td>
	<td class="line x" title="166:189	Of the 26 tenns, 22 occur frequently enough to generate meaningful statistics." ></td>
	<td class="line x" title="167:189	Out of these 22 catchable terms, only 5 are included in the suggested list." ></td>
	<td class="line x" title="168:189	Table 8 presents the statistical summary." ></td>
	<td class="line x" title="169:189	140 Table 8: Statistics of the Suggested Multi-Word Terms total suggested 97 Comparison of Suggested and Manual Terms total manual 35 not possible* no statistics* 4 possible*!" ></td>
	<td class="line x" title="170:189	hits 22 5 * not possible: terms not existing in the focused sample * no statistics: terms which have less than 3 occurrences in the focused sample * possible: targeted terms percent included 23% One possible explanation for not being able to match more manual selections is that most of the two-word terms that could have been used to detect these phrases consist of two common words, such as house, lords, fund, system." ></td>
	<td class="line x" title="171:189	These two-word terms typically generate fairly low mutual information scores since the constituent words occur frequently by themselves." ></td>
	<td class="line x" title="172:189	It is important to point out that the suggested list does contain a number of useful multi-word terms that are related to the targeted predefined topic =European Politics and Business'." ></td>
	<td class="line x" title="173:189	For example, US-European relations/politics: short range nuclear missiles, tactical nuclear weapons, conventional arms reduction, multi party system European Business: gross national product, higher interest rates and inflation, Bank of England, North Sea Oil Notable European entities: predominantly Catholic Idsh Republic, three Bdtish hostages, World War II, Roman Catholic Church Heads of state or notable dignitaries: Secretary of State James Baker, Secretary of State George Shultz, French President Francois Mitterrand, West German Chancellor Helmut Kohl, Soviet leader Mikhail Gorbachev, Soviet Foreign Minister Eduard Shevardnadze 141 4." ></td>
	<td class="line x" title="174:189	CONCLUSION This paper presents a preliminary experiment in identifying significant terminological units from running text." ></td>
	<td class="line x" title="175:189	By comparing a focused sample randomly drawn for a predefined topic against a larger and more general base sample, we can automatically suggest topic-oriented terms based on the detection of significant changes in some statistical measurements." ></td>
	<td class="line x" title="176:189	Our experiment on one predefined topic demonstrated that, compared to the manual selection of the topical terms, our suggested lists do contain more useful terms that can be used to descdbe the topic." ></td>
	<td class="line x" title="177:189	We also found that the method is efficient enough for applications to very large textual corpora." ></td>
	<td class="line x" title="178:189	Our next step is to further refine the methods by carrying out more experiments across different topics." ></td>
	<td class="line x" title="179:189	We mentioned a number of times that our methods were developed based on our intuitive assumptions or hypotheses." ></td>
	<td class="line x" title="180:189	More experiments on more topics will prove whether we can obtain positive and consistent results." ></td>
	<td class="line x" title="181:189	Identification of significant terms from running text can be very useful in building intelligent information management systems." ></td>
	<td class="line x" title="182:189	Terms identified are good candidates for key word indexing of electronic sources." ></td>
	<td class="line x" title="183:189	Topic specificity can assist in grouping or clustering on-line documents." ></td>
	<td class="line x" title="184:189	For an information retrieval system, terms identified for a pre-determined subject can be used to develop specialized libraries or files for targeted user groups." ></td>
	<td class="line x" title="185:189	Our experiment demonstrated that the methods described can identify vadous people names, organization enlJties and other proper names." ></td>
	<td class="line x" title="186:189	Those special text tokens are important for constructing text extraction systems." ></td>
	<td class="line x" title="187:189	ACKNOWLEDGEMENTS This research was done while the second author worked at LEXIS-NEXIS dudng the summer of 1994." ></td>
	<td class="line x" title="188:189	The authors would like to thank Dan Pliske, Mark Wasson and Rob Keefer for helpful comments on this paper, and Rita Freese for proofreading the final draft." ></td>
	<td class="line x" title="189:189	The authors were also benefited from numerous conversations with Ken Church at Bell Labs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1055
Role Of Word Sense Disambiguation In Lexical Acquisition: Predicting Semantics From Syntactic Cues
Dorr, Bonnie Jean;Jones, Douglas A.;"></td>
	<td class="line x" title="1:184	Role of Word Sense Disambiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues Bonnie J. Dorr and Doug Jones Department of Computer Science and Institute for Advanced Computer Studies University of Maryland A.V. Williams Building College Park, MD 20742 {bonnie, j ones}~umiacs, umd." ></td>
	<td class="line x" title="2:184	edu ABSTRACT This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources." ></td>
	<td class="line x" title="3:184	We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy." ></td>
	<td class="line x" title="4:184	These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses." ></td>
	<td class="line x" title="5:184	Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources." ></td>
	<td class="line x" title="6:184	1 Introduction This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources." ></td>
	<td class="line x" title="7:184	We describe two experiments: one which ignored wordsense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy." ></td>
	<td class="line x" title="8:184	These experiments were dual purpose: (l) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses." ></td>
	<td class="line x" title="9:184	Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources, in particular, Longman's Dictionary of Contemporary English (LDOCE) (Procter, 1978), Levin's verb classification scheme (Levin, 1993), and WordNet (Miller, 1985)." ></td>
	<td class="line x" title="10:184	We have used these techniques to build a database of 10,000 English verb entries containing semantic information that we are currently porting into languages such as Arabic, Spanish, and Korean for multilingual NLP tasks such as foreign language tutoring and machine translation." ></td>
	<td class="line x" title="11:184	322 2 Automatic Lexical Acquisition for NLP Tasks As machine-readable resources (i.e. , online dictionaries, thesauri, and other knowledge sources) become readily available to NLP researchers, automated acquisition has become increasingly more attractive." ></td>
	<td class="line x" title="12:184	Several researchers have noted that the average time needed to construct a lexical entry can be as much as 30 minutes (see, e.g., (Neff and McCord, 1990; Copestakc et al. , 1995; Walker and Amsler, 1986))." ></td>
	<td class="line x" title="13:184	Given that we are aiming for large-scale lexicons of 20-60,000 words, automation of the acquisition process has become a necessity." ></td>
	<td class="line oc" title="14:184	Previous research in automatic acquisition focuscs primarily on the use of statistical techniques, such as bilingual alignment (Church and Hanks, 1990; Klavans and Tzoukermann, 1996; Wu and Xia, 1995), or extraction of syntactic constructions from online dictionaries and corpora (Brant, 1993; Dorr, Garman, and Weinberg, 1995)." ></td>
	<td class="line x" title="15:184	Others who have taken a more knowledge-based (interlingual) approach (Lonsdale, Mitamura, and Nyberg, 1996) do not provide a means for systematically deriving the relation between surface syntactic structures and their underlying semantic representations." ></td>
	<td class="line x" title="16:184	Those who have taken more argument structures into account, e.g., (Copestake et al. , 1995), do not take full advantage of the systematic relation between syntax and semantics during lexical acquisition." ></td>
	<td class="line x" title="17:184	We adopt the central thesis of Levin (1993), i.e., that the semantic class of a verb and its syntactic behavior are predictably related." ></td>
	<td class="line x" title="18:184	We base our work on a correlation between semantic classes and patterns of grammar codes in the Longman's Dictionary of Contemporary English (LDOCE) (Procter, 1978)." ></td>
	<td class="line x" title="19:184	While the LDOCE has been used previously in automatic cxtraction tasks (Alshawi, 1989; Farwell, Guthrie, and Wilks, 1993; Boguraev and Briscoe, 1989;,Wilks et al. , 1989; Wilks et al. , 1990) these tasks are primarily concerned with the extraction of other types of information including syntactic phrase structure and broad argument restrictions or with the derivation of semantic structures from definition analyses." ></td>
	<td class="line x" title="20:184	The work of Sanfilippo and Poznanski (1992) is more closely related to our approach in that it attempts to recover a syntacticsemantic relation from machine-readable dictionaries." ></td>
	<td class="line x" title="21:184	Itowever, they claim that the semantic classification of verbs based on standard machine-readable dictionaries (e.g. , the LDOCE) is % hopeless pursuit \[since\] standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness'." ></td>
	<td class="line x" title="22:184	Others have also argued that the task of simplifyin K lexical entries on the basis of broad semantic class membership is complex and, perhaps, infeasible (see, e.g., Boguraev and llriscoe (1989))." ></td>
	<td class="line x" title="23:184	tlowever, a number of researchers (l,'ilhnore, 1968; Grimshaw, 1990; Gruber, 1965; Guthrie et al. , 1991; Hearst, 1991; Jackendotr, 1983; Jackendoff, 1990; l,evin, 1993; Pinker, t989; Yarowsky, 1992) have demonstrated conclusively that there is a clear relationship between syntactic context and word senses; it is our aim to exploit this relationship for the acquisition of semantic lexicons." ></td>
	<td class="line x" title="24:184	3 Syntax-Semantics Relation: -Verb Classification Based on Syntactic Behavior The central thesis of (Levin, 1993) is that the semantics of a verb and its syntactic behavior are predictably related." ></td>
	<td class="line x" title="25:184	As a demonstration that such predictable relationships are not confined to an insignificant portion of the vocabulary, Levin surveys 4183 verbs, grouped into 191 semantic classes in Part Two of her book." ></td>
	<td class="line x" title="26:184	The syntactic behavior of these classes is illustrated with 1668 example sentences, an average of 8 sentences per (:lass." ></td>
	<td class="line x" title="27:184	Given the scope of bevin's work, it is not easy to verify the central thesis." ></td>
	<td class="line x" title="28:184	'lb this end, we created a database of Levin's verb classes and example sentences from each class, and wrote a parser to extract, basic syntactic patterns from tire sentences.1 We then characterized each semantic class by a set of syntactic patterns, which we call a syntactic signature, and used the resuiting database as the basis of two experiments, both designed to to discover whether the syntactic signatures tell us anything about the meaning of the verbs." ></td>
	<td class="line x" title="29:184	2 '\['he first experiment, which we label Verb-Based, ignores word-sense distinctions by assigning one syntactic signature to each verb, regardless of whether it occurred in multiple classes." ></td>
	<td class="line x" title="30:184	The second experiment, which we label Class-Based, implicitly takes word-sense distinctions into account by considering each occurrence of a verb individually and assigning it a single syntactic signature according to class membership." ></td>
	<td class="line x" title="31:184	The remainder of this section describes the assignrnent of signatures to semantic cbusses and the two experiments for determining the relation of syntactic information to semantic cbtsses." ></td>
	<td class="line x" title="32:184	We will see that our classitication technique shows a 15-fold improvement in the experiment where we implicitly account for word-sense distinctions." ></td>
	<td class="line x" title="33:184	1Both the database and the parser are encoded in Quintus Prolog." ></td>
	<td class="line x" title="34:184	2The design of this experiment is inspired by the work of (Dubois and Saint-Dizier, 1995)." ></td>
	<td class="line x" title="35:184	In particular, we depart from the alternation-based data in (Levin, 1993), which is primarily binary in that sentences are presented in pairs which constitute an alternation." ></td>
	<td class="line x" title="36:184	Following Saint-Dizier's work, we construct N-ary syntactic characterizations." ></td>
	<td class="line x" title="37:184	The choice is of no empirieM consequence, but it simplifms the experiment by eliminating the problem of naming the syntactic patterns." ></td>
	<td class="line x" title="38:184	Verbs: break, chip, crack, crash, crush, fracture, rip, shatter, slnash, snap, sl)linter, split, tear Example Sentences: Crystal vases break easily." ></td>
	<td class="line x" title="39:184	The hammer broke the window." ></td>
	<td class="line x" title="40:184	The window broke." ></td>
	<td class="line x" title="41:184	q'ony broke her arm." ></td>
	<td class="line x" title="42:184	'l?ony broke his finger." ></td>
	<td class="line x" title="43:184	'lbny broke the crystal vase." ></td>
	<td class="line x" title="44:184	qbny broke the cup against the wall." ></td>
	<td class="line x" title="45:184	q'ony broke the glass to 1)ieces." ></td>
	<td class="line x" title="46:184	Tony broke the piggy bank open." ></td>
	<td class="line x" title="47:184	Tony broke the window with a hanuner." ></td>
	<td class="line x" title="48:184	Tony broke the window." ></td>
	<td class="line x" title="49:184	*Tony broke at tit(; window." ></td>
	<td class="line x" title="50:184	*qbny broke herself on the arm." ></td>
	<td class="line x" title="51:184	*Tony broke himself." ></td>
	<td class="line x" title="52:184	*qbny broke the wall with the cup." ></td>
	<td class="line x" title="53:184	A break." ></td>
	<td class="line x" title="54:184	Derived Syntactic Signature: 1-\[np,v\] 1-\[np,v,np\] 1-\[np,v,np,adjectiw\] 1\[np, v, np,pp(against) \] l-\[np,v,np,pp(to)\] 1\[np, v, np,pp (with) \] 1\[np, v, pess, np\] 1\[np,v,adv(easily)\] l-in\] O-\[np,v,np,pp(with)\] 0\[np,v,self\] O-\[np,v,seH,pp(on)\] 0\[np,v,pp(at)\] Table 1: Syntactic Signatm:e for Change of State break subclass 3.1 Asslgntnent of Signatures For tile first experiment below, we construct a verbbased syntactic signature, while for the second exl)eriment, we constructed a class-based signature." ></td>
	<td class="line x" title="55:184	The first step for constructing a signature is to decide what syntactic information to extract for ttre t)asic syntactic patterns that make up the signature." ></td>
	<td class="line x" title="56:184	It turns out that a very simple strategy works well, namely, flat parses that contain lists of the major categories in the sentence, the verb, and a handfifl of other elements." ></td>
	<td class="line x" title="57:184	The 'parse', then, for the sentence Tony broke the crystal vase is simply the syntactic pattern \[np,v,np\]." ></td>
	<td class="line x" title="58:184	For Tony broke the vase to pieces we get \[np,v,np,pp(to)\]." ></td>
	<td class="line x" title="59:184	Note that the pp node is marked with its head preposition." ></td>
	<td class="line x" title="60:184	Table l shows an example class, the break subclass of the Change of State verbs (45.1), along with example sentences and the derived syntactic signature based on sentence patterns." ></td>
	<td class="line x" title="61:184	Positive example sentences are denoted by the number 1 in the sentence patterns and negative example sentences are denoted by the number 0 (corresponding to sentences marked with a *)." ></td>
	<td class="line x" title="62:184	3.2 Experiment 1: Verb-based Approach In the first experiment, we ignored word sense distinctions and considered each verb only once, regardless of whether it occurred in multiple classes." ></td>
	<td class="line x" title="63:184	In fact;, 46% of the verbs appear more than once." ></td>
	<td class="line x" title="64:184	In some cases, the verb appears to have a related sense even though it appears in different classes." ></td>
	<td class="line x" title="65:184	For example, the verb roll appears in two subclasses of Manner of Motion Verbs that are distinguished on the basis of whether the grammatical subject is animate or inanimate." ></td>
	<td class="line x" title="66:184	In other cases, tile verb may have (largely) unrelated senses." ></td>
	<td class="line x" title="67:184	For example, the verb move is both a Manner of Motion verb 323 and verb of Psychological State." ></td>
	<td class="line x" title="68:184	To compose the syntactic signatures for each verb, we collect all of the syntactic patterns associated with every class a particular verb appears in, regardless of the different classes are semantically related." ></td>
	<td class="line x" title="69:184	A syntactic signature for a verb, by definition, is the union of the frames extracted from every example sentence for each verb." ></td>
	<td class="line x" title="70:184	The outline of the verb-based experiment is as follows: 1." ></td>
	<td class="line x" title="71:184	Automatically extract syntactic information from the example sentences." ></td>
	<td class="line x" title="72:184	2." ></td>
	<td class="line x" title="73:184	Group the verbs according to their syntactic signature." ></td>
	<td class="line x" title="74:184	3." ></td>
	<td class="line x" title="75:184	Determine where the two ways of grouping verbs overlap: (a) the semantic classification given by Levin." ></td>
	<td class="line x" title="76:184	(1)) the syntactic classification based on the derived syntactic signatures." ></td>
	<td class="line x" title="77:184	To return to the Change of State verbs, we now consider the syntactic signature of the verb break, rather than the signature of the semantic class as a unit." ></td>
	<td class="line x" title="78:184	The verb break belongs not only to the Change of State class, but also four other classes: 10.6 Cheat, 23.2 Split, 40.8.3 Hurl, and 48.1.1 Appear." ></td>
	<td class="line x" title="79:184	Each of these classes is characterized syntactically with a set of sentences." ></td>
	<td class="line x" title="80:184	The union of the syntactic patterns corresponding to these sentences forms the syntactic signature for the verb." ></td>
	<td class="line x" title="81:184	So although the signature for the Change of State class has 13 frames, the verb break has 39 frames from the other classes it appears in." ></td>
	<td class="line x" title="82:184	Conceptually, it is helpful to consider the difference between the intension of a function versus its extension." ></td>
	<td class="line x" title="83:184	In this case, we are interested in the functions that group the verbs syntactically and semantically." ></td>
	<td class="line x" title="84:184	Intensionally speaking, the definition of the function that groups verbs semantically would have something to do with the actual meaning of the verbs." ></td>
	<td class="line x" title="85:184	~ Likewise, the intension of the function that groups verbs syntactically would be defined in terms of something strictly syntactic, such as subcategorization frames." ></td>
	<td class="line x" title="86:184	But the intensions of these functions are matters of significant theoretical investigation, and although much has been accomplished in this ~rea, the question of mapping syntax to semantics and vice versa is an open research topic." ></td>
	<td class="line x" title="87:184	Therefore, we can turn to the extensions of the functions: the actual groupings of verbs, based on these two separate criteria." ></td>
	<td class="line x" title="88:184	The semantic extensions are sets of verb tokens, and likewise, the syntactic extensions are sets of verb tokens." ></td>
	<td class="line x" title="89:184	To the extent that these functions map between syntax and semantics intensionally, they will pick out the same verbs extensionally." ></td>
	<td class="line x" title="90:184	So for the verb-based experiment, our technique for establishing the relatedness between the syntactic signatures and the semantic classes, is mediated by the verbs themselves." ></td>
	<td class="line x" title="91:184	We compare the two orthogonal groupings of the inventory of verbs: the semantic classes defined by Levin and the sets of verbs that correspond to each of the derived syntactic signatures." ></td>
	<td class="line x" title="92:184	When these two groupings overlap, we have discovered a mapping from the syntax of the verbs to their semantics, via the verb tokens." ></td>
	<td class="line x" title="93:184	More specifically, we define the overlap index 3An example of the intensional characterization of the Levin classes are the definitions of Lexical Conceptual Structures which correspond to each of Levin's semantic classes." ></td>
	<td class="line x" title="94:184	See (Dorr and Voss, to appear)." ></td>
	<td class="line x" title="95:184	as the number of overlapping verbs divided by the average of the number of verbs in the semantic class and the number of verbs in the syntactic signature." ></td>
	<td class="line x" title="96:184	Thus an overlap index of 1.00 is a complete overlap and an overlap of 0 is completely disjoint." ></td>
	<td class="line x" title="97:184	In this experiment, the sets of verbs with a high overlap index are of interest." ></td>
	<td class="line x" title="98:184	When we parsed the 1668 example sentences in Part Two of Levin's book (including the negative examples), these sentences reduce to 282 unique patterns." ></td>
	<td class="line x" title="99:184	The 191 sets of sentences listed with each of the 191 semantic classes in turn reduces to 748 distinct syntactic signatures." ></td>
	<td class="line x" title="100:184	Since there are far more syntactic signatures than the 191 semantic classes, it is clear that the mapping between signatures and semantic classes is not direct,." ></td>
	<td class="line x" title="101:184	Only 12 mappings have complete overlaps." ></td>
	<td class="line x" title="102:184	That means 6.3% of the 191 semantic classes have a complete overlap with a syntactic signature." ></td>
	<td class="line x" title="103:184	The results of this experiment are shown in Table 2." ></td>
	<td class="line x" title="104:184	Three values are shown for each of the six variations in the experiment: the mean overlap, the median overlap, and the percentage of perfect overlaps (overlaps of value 1.00)." ></td>
	<td class="line x" title="105:184	In every case, the median is higher than the mean." ></td>
	<td class="line x" title="106:184	Put another way, there is always a cluster of good overlaps, but the general tendency is to have fairly poor overlaps." ></td>
	<td class="line x" title="107:184	The six variations of the experiment are as follows." ></td>
	<td class="line x" title="108:184	The first distinction is whether or not to count the negative evidence." ></td>
	<td class="line x" title="109:184	We note that the use of negative examples, i.e., plausible uses of the verb in contexts which are disallowed, was a key component of this experiment." ></td>
	<td class="line x" title="110:184	There are 1082 positive examples and 586 negative examples." ></td>
	<td class="line x" title="111:184	Although this evidence is useful, it is not available in dictionaries, corpora, or other convenient resources that could be used to extend Levin's classification." ></td>
	<td class="line x" title="112:184	Thus, to extend our approach to novel word senses (i.e. , words not occurring in Levin), we would not be able to use negative evidence." ></td>
	<td class="line x" title="113:184	For this reason, we felt it necessary to determine the importance of negative evidence for building uniquely identifying syntactic signatures." ></td>
	<td class="line x" title="114:184	As one might expect, throwing out the negative evidence degrades the usefulness of the signatures across the board." ></td>
	<td class="line x" title="115:184	The results which had the negative evidence are shown in the left-hand column of numbers in Table 2, and the results which had only positive evidence are shown in the right-hand side." ></td>
	<td class="line x" title="116:184	The second, three-way, distinction involves prepositions, and breaks the two previous distinctions involving negative evidence into three sub-cases." ></td>
	<td class="line x" title="117:184	Because we were interested in the role of prepositions in the signatures, we also ran the experiment with two different parse types: ones that ignored the actual prepositions in the pp's, and ones that ignored all information except for the values of the prepositions." ></td>
	<td class="line x" title="118:184	Interestingly, we still got useful results with these impoverished parses, although fewer semantic classes had uniquely-identifying syntactic signatures under these conditions." ></td>
	<td class="line x" title="119:184	These results are shown in the three major rows of Table 2." ></td>
	<td class="line x" title="120:184	The best result, using both positive and negative evidence to identify semantic classes, gives 6.3% of the verbs having perfect overlaps relating semantic classes to syntactic signatures." ></td>
	<td class="line x" title="121:184	See Table 2 for the full results." ></td>
	<td class="line x" title="122:184	3.3 Experiment 2' Class-based Approach In this experiment, we attempt to discover whether each class-based syntactic signature uniquely identifies a sin324 Verb-based Experiment (No Disami)iguation) :ed,~sitions ~i~ ed )sitions qYgfy )sitions Overlap Median Mean Perfect Median Mean Perfect Median Mean Perfect With No Negative Negative Evidence Evidence O.lO 0.09 0.17 0.17 6.3% 5.2% 0.t0 0.09 0.17 O. 16 6.3% 4.2% 0.10 0.09 0.16 0.715 3.1% 3.1% Table 2: Verb-Ba~sed Results Class-based Experiment (Disambiguated Verbs) With No Negative Negative Overlap Evidence Evidence Marked Prepositions ~lgnored Pret)ositions ~3nly Prepositions -TVledian Mean Perfect Median Mean Perfect Median Mean Perfect 1.00 1.00 0.99 0.93 97.9% 88.0% -1.00 1.00 0.96 0.69 87.4% 52.4% 1.00 0.54 0.82 0.57 66.5% 42.9% 'fable 3: Cla~ss-Based lesnlts gle semantic class." ></td>
	<td class="line x" title="123:184	By h)cnsing on the classes, the verbs are implicitly disambiguated: the word sense is by definition the sense of the verb as a member of a given class." ></td>
	<td class="line x" title="124:184	To compare these signatures with the previous verb-based signatures, it may be helpfnl to note that a verb-based signature is the union of all of the class~ based signatures of the semantic classes that the verb appears m. 'Fhe outline for this class-based exl)eriment is as follows: 1." ></td>
	<td class="line x" title="125:184	Automatically extract syntactic information from tile example sentences to yMd the syntactic signatnre for the class." ></td>
	<td class="line x" title="126:184	2." ></td>
	<td class="line x" title="127:184	Determine which semantic classes have uniquelyidentifying syntactic signatures." ></td>
	<td class="line x" title="128:184	If we use the class-based syntactic signatures containing t)rcposition-marked pp's and both positive and negative evidence, the 1668 example sentences reduce to 282 syntactic patterns, just as before." ></td>
	<td class="line x" title="129:184	But now there are 189 class-based syntactic signatures, as compared with 748 verb-based signatures from before." ></td>
	<td class="line x" title="130:184	187 of them mriquely identify a semantic (:lass, meaning that 97.9% of the classes have uniquely identifying syntactic signatures." ></td>
	<td class="line x" title="131:184	Four of the semantic classes do not have enough syntactic information to distinguish them uniquely." ></td>
	<td class="line x" title="132:184	4 Although the effects of the various distinctions were present in the verb-based experiment, these effects are much clearer in the class-based experiments." ></td>
	<td class="line x" title="133:184	The effects of negative and positive evidence, as well as the three ways of handling prepositions show up much clearer here, as is clear in Table 4." ></td>
	<td class="line x" title="134:184	In the class-based experiment, we counted the percentage of semantic classes that had uniquely ide.ntifying signatures." ></td>
	<td class="line x" title="135:184	In the verb-based experiment, we counted the number of perfect overlaps (i.e. , index of 1.00) between the verbs as grouped in the semantic classes and grouped by syntactic signature." ></td>
	<td class="line x" title="136:184	The overall results of the suite of experiments, illustrating tile role of disambiguation, negative evidence, and prepositions, is shown in Table 4." ></td>
	<td class="line x" title="137:184	There were three ways of treating prepositions: (i) mark the pp with the preposition, (ii) ignore the preposition, and (iii) keel) only the prepositions." ></td>
	<td class="line x" title="138:184	For these different strategies, we see the percentage of perfect overlaps, as well as both tire 4Two of these classes correspond to one of the two nonunique signatures, and two (:orrespond to the other nonunique signature." ></td>
	<td class="line x" title="139:184	median and mean overlap ratios for each experiment." ></td>
	<td class="line x" title="140:184	'Fhese data show that the most important factor in the experiments is word-sense disambiguation." ></td>
	<td class="line x" title="141:184	Marked Prepositions ignored Prepositions Only Prepositions ~W~{h Dismnl)lguation Marked Prepositions Ignored Prepositions Only Prepositions With No Negative Negative Evidence Evidence 6.3% 5.2% 6.3% 4.2% 3.1% 3.1% 97.9% 88.{)% 87.4% 52.4% 66.5% 42.9% Table 4: Overall Results 4 Semantic Classification of Novel Words As we saw above, word sense disambiguation is critical to tile success of any \[exical acquisition algorithm." ></td>
	<td class="line x" title="142:184	The Levin-based verbs are already disambiguated by virtue of their membership in different classes." ></td>
	<td class="line x" title="143:184	The difficulty, then, is to disambiguate and classify verbs that do not occur in Levin." ></td>
	<td class="line x" title="144:184	Our current direction is to make use of the results of tire first two experiments, i.e., the relation t)etween syntactic patterns and semantic classes, but to use two additional techniques for disambiguation and classification of non-Levin verbs: (1) extraction of synonym sets provided in WordNet (Miller, 1985), an online lexical database containing thesaurus-like relations such as synonymy; and (2) selection of appropriate synonyms based on correlations between syntactic information in l,ongman's Dictionary of Contemporary English (LDOCF,) (Procter, 1978) and semantic classes in Levin." ></td>
	<td class="line x" title="145:184	'Phe basic idea is to first determine tire most likely candidates for semantic classification of a verb by examining the verb's synonym sets, many of which intersect directly with the verbs classified by Leviu." ></td>
	<td class="line x" title="146:184	The 'closest' synonyms are then selected fl'om these sets by comparing the LDOCE grammar codes of tire unknown word with those associated with each synonym candidate." ></td>
	<td class="line x" title="147:184	The use of LDOCE as a syntactic filter on tire semantics derived from WordNet is tire key to resolving word-sense ambiguity during the acquisition process." ></td>
	<td class="line x" title="148:184	The fldl acquisition algorithm is as follows: 325 Given a verb, check I,evin (:lass." ></td>
	<td class="line x" title="149:184	1." ></td>
	<td class="line x" title="150:184	If in Levitt, classify directly." ></td>
	<td class="line x" title="151:184	2." ></td>
	<td class="line x" title="152:184	if not in Levin, find synonym set from WordNet." ></td>
	<td class="line x" title="153:184	(a) if synonym in Levin, select, the class that has the closest match with canonical LDOCE codes." ></td>
	<td class="line x" title="154:184	(b) If no synonyms in Levin or canonical LDOCE codes are completely mismatched, hypothesize new class." ></td>
	<td class="line x" title="155:184	Note that this algorithm assmnes that there is a 'canonicM' set of LDOCE codes tbr each of Levin's semantic classes." ></td>
	<td class="line x" title="156:184	Table 5 describes the significance of a subset of the syntactic codes in LDOCE." ></td>
	<td class="line x" title="157:184	(The total nmnber of codes is 174)." ></td>
	<td class="line x" title="158:184	We have developed a relation between LDOCE codes and Levin classes, in mnch the same way that we associated syntactic signatures with the semantic classes in the earlier experiments." ></td>
	<td class="line x" title="159:184	These canonical codes are for syntactic filtering (checking for the closest match) in the classification algorithm." ></td>
	<td class="line x" title="160:184	As an example of how the word-sense disambiguation process and classifcation, consider the non-Levin verb attempt." ></td>
	<td class="line x" title="161:184	The LDOCE specification for this verb is: T1 T3 T4 WV5 N. Using the synonymy feature of WordNet, the algorithm automatically extracts tire candidate classes associated with the synonyms of this word: (1) Class 29.6 'Masquerade Verbs' (ace), (2) Class 29.8 'Captain Verbs' (pioneer), (3) Class 31.1 'Amuse Verbs' (try), (4) Class 35.6 'Ferret Verbs' (seek), and (5) Class 55.2 'Complete Verbs' (initiate)." ></td>
	<td class="line x" title="162:184	The synonyms for each of these classes have the following LDOCE encodiugs, respectively: (1) I I-FOIl I-ON I-UPON LI L9 T1 N; (2) L9 T1 N; (3) I T1 T3 T4 WV4 N; (4) ~ bAF'\['EI~ I-FOR T1 T3; and (5) T1 TI-INTO N. The largest intersection with the syntactic codes for attempt occurs with the verb try (TI T3 T4 N)." ></td>
	<td class="line x" title="163:184	However, Levin's class 31.1 is not the correct class for attempt since this sense of try has a 'negative amuse' meaning (e.g. , John's behavior tried my patience." ></td>
	<td class="line x" title="164:184	In fact, the (:odes T1 'l'3 '1'4 are not part of the canonical class-code mapping associated with class 31.1." ></td>
	<td class="line x" title="165:184	Thus, attempt falls under case 2(b) of the algorithm, and a new class is hypothesized." ></td>
	<td class="line x" title="166:184	This is a case where word-sense disambiguation has allowed us to classify a new word and to enhance Levin's verb classification by adding a new class to the word try as well." ></td>
	<td class="line x" title="167:184	In our experiment;s, our algorithm found severM additional non-Levin verbs that fell into this newly hypothesized (;lass, including aspire, attempt, dare, decide, desire, elect, need, and swear." ></td>
	<td class="line x" title="168:184	We have automatically classified 10,000 'unknown' verbs, i.e., those not occurring in the Levin classification, using this technique." ></td>
	<td class="line x" title="169:184	These verbs are taken from i e, translations provided in bilinEnglish 'glosses' ( )." ></td>
	<td class="line x" title="170:184	gual dictionaries for Spanish and Arabic) As a preliminary measure of success, we picked out 84 L1)OCE control vocabulary verbs, (i.e. , primitive words used for defning dictionary entries) and hand-checked our results." ></td>
	<td class="line x" title="171:184	We found that 69 verbs were classifed correctly, SThe Spanish-English dictionary was built at the University of Maryland; The Arabic-English dictionary was produced by Alpnet, a company in Utah that develops translation aids." ></td>
	<td class="line x" title="172:184	We are Mso in the process of developing bilingual dictionaries for Korean and French, and we will be porting our LCS acquisition technology to these languages in the near future." ></td>
	<td class="line x" title="173:184	i.e., 82% accuracy." ></td>
	<td class="line x" title="174:184	5 Summary We have conducted two experiments with the intent of addressing the issue of word-sense ambiguity in extraction from machine-readable resources for the construe tion of large-scale knowledge sources." ></td>
	<td class="line x" title="175:184	In the first experiment, verbs that appeared in different classes collected the syntactic information fl'om each class it appeared in." ></td>
	<td class="line x" title="176:184	Therefore, the syntactic signature was coml)osed from all of the example sentences fi'om every (:lass the verb appeared in." ></td>
	<td class="line x" title="177:184	In some cases, the verbs were seanantically unrelated and consequently the mat)ping from syntax to semantics was muddied." ></td>
	<td class="line x" title="178:184	'\['he second experiment attelnpted to determine a relationship between a semantic class and the syntactic information associated with each class." ></td>
	<td class="line x" title="179:184	Not surprisingly, but not insignificantly, this relationship was very clear, since this experiment avoided the problem of word sense ambiguity." ></td>
	<td class="line x" title="180:184	These experiments served to validate Levin's claim that verb semantics and syntactic behavior are predictably related and also demonstrated that a significant con> ponent of any lexical acquisition program is the ability to perform word-sense disambiguation." ></td>
	<td class="line x" title="181:184	We have used the results of our first two experiments to help in constructing and augmenting online dictionaries for novel verb senses." ></td>
	<td class="line x" title="182:184	We have used the same syntactic signatures to categorize new verbs into Lcvin's classes on the basis of WordNet and 1,1)O(?1!3." ></td>
	<td class="line x" title="183:184	We are currently porting these results to new languages using online bilingual lexicons." ></td>
	<td class="line x" title="184:184	Acknowledgements The research reported herein was supported, in part, by Army l,esearch Office contract I)AAL03-91-C-0034 through Battelle Corporation, NSF NYI IRl-9357731, Alfred P. Sloan Research Fellow Award BR3336, and a General Research Board Semester Award." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1083
Symbolic Word Clustering For Medium-Size Corpora
Habert, Benoit;Naulleau, Elie;Nazarenko, Adeline;"></td>
	<td class="line x" title="1:134	Symbolic word clustering for medium-size corpora Benoit Habert* and Elie Naulleau* ** and Adeline Nazarenko* *Equipe de Linguistique Inform~tique Ecole NorInale Sup&ieure de Fontenay-St Cloud 31 av." ></td>
	<td class="line x" title="2:134	bombart, F-92260 Fontenay-aux-Roses Firstname." ></td>
	<td class="line x" title="3:134	Name@ens-fcl." ></td>
	<td class="line x" title="4:134	fr **Direction des Etudes et Recherches Electricitd de Fra, nce 1, av." ></td>
	<td class="line x" title="5:134	du G ~z de Gaulle, F-92141 Clamart F irstname." ></td>
	<td class="line x" title="6:134	Name@der." ></td>
	<td class="line x" title="7:134	edfgdf, fr Abstract When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low." ></td>
	<td class="line x" title="8:134	We present an alternative method, symbolic, based on the simplification of parse trees." ></td>
	<td class="line x" title="9:134	We discuss the resuits on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company." ></td>
	<td class="line x" title="10:134	We compare our results with Hindle's scores of similarity." ></td>
	<td class="line x" title="11:134	Subjects Clustering, ontology development, robust parsing, knowledge acquisition from corpora, computational terminology 1 Identifying word classes in medium-size corpora In companies with a wide range of activities, such as EDF, the French electricity company, the rapid evolution of technical domains, the huge amount of textual data involved, its variation in length and style imply building or updating numerous terminologies as NLP resources." ></td>
	<td class="line x" title="12:134	In this context, terminology acquisition is defined as a twofold process." ></td>
	<td class="line x" title="13:134	On one hand, a terminologist must identify the essential entities of the domain and their relationships, that is its ontology." ></td>
	<td class="line x" title="14:134	On the other hand, (s)he must relate these entities and relationships to their linguistic realizations, so as to isolate the lexical entries to be considered as certified terms for the domain." ></td>
	<td class="line x" title="15:134	In this paper, we concentrate on the first issue." ></td>
	<td class="line x" title="16:134	Automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage." ></td>
	<td class="line pc" title="17:134	In the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks, 1990; Hindle, 1990; Smadja, 1993; Grei~nstette, 1994; Grishman and Sterling, 1994)." ></td>
	<td class="line n" title="18:134	Most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation." ></td>
	<td class="line x" title="19:134	In other words, it is not always possible to resort to statistical methods." ></td>
	<td class="line x" title="20:134	On the other hand, medium size corpora (between 100,000 and 500,000 words: typically a reference manual) are already too complex and too long to rely on reading only, even with concordances." ></td>
	<td class="line x" title="21:134	For this range of corpora, a pure symbolic approach, which recycles and simplifies analyses produced by robust parsers in order to classify words, offers a viable alternative to statistical methods." ></td>
	<td class="line x" title="22:134	We present this approach in section 2." ></td>
	<td class="line x" title="23:134	Section 3 describes the results on two technical corpora with two different robust parsers." ></td>
	<td class="line x" title="24:134	Section 4 compares our results to Itindle's ones (Hindle, 1990)." ></td>
	<td class="line x" title="25:134	2 Simplifying parse trees to classify words 2.1 The need for normalized syntactic contexts As Hindle's work proves it, among others (Grishman and Sterling, 1994; Grefenstette, 1994:), the mere existence of robust syntactic parsers makes it possible to parse large corpora in order to automate the discovery of syntactic patterns in the spirit of Harris's distributional hypothesis." ></td>
	<td class="line x" title="26:134	Itowever, Harris' methodology implies also to simplify and transform each parse tree 2, so as to obtain so-called 'elementary sentences' exhibiting the main conceptual classes for the domain (Sager lIa'or instance, Hindle (Hindle, 1990) needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures." ></td>
	<td class="line x" title="27:134	2Changing passive into active sentences, using a verb instead of a nominalization, and so on." ></td>
	<td class="line x" title="28:134	490 NP\] NPa AP4 I I Nr As I I stenose serre NPo PP2 Pa NP6 I de D9 NPlo le NPll AP12 t NPla AP14 A15 I I I N~ A~r gauche I I tronc eorninun I?igure 1: Parse tree for stenose serre de le hone commun gauche et al. , 1987)." ></td>
	<td class="line x" title="29:134	In order to ~mtomate this normalization, we propose to post-process parse trees so as to emphasize the dependency relationships among the content words and to infer semantic classes." ></td>
	<td class="line x" title="30:134	Our approach can be opposed to the a prior one which consists in building simplified representations while parsing (Basili et al. , 1994; Metzler and Haas, 1989; Smeaton and Sheridan, 19911)." ></td>
	<td class="line x" title="31:134	2.2 R.ecycling the results of robust parsers For the sake of reusability, we chose to add a generic post-processing treatment to the results of robust parsers." ></td>
	<td class="line x" title="32:134	It ilnplies to transduce the trees resulting fl:om different parsers to a common fornlat." ></td>
	<td class="line x" title="33:134	We experimented so t~r two parsers: Aleth(h:am and I,exl;er, which are being used at DEREDI,' for terminology acquisition and updating." ></td>
	<td class="line x" title="34:134	They both analyze corpora of arbitrary length." ></td>
	<td class="line x" title="35:134	AlethGram has been developped winthin the GIIAAL project a. I,EXrI'ER has been developped at DER-EI)F (Bourigault, 1993)." ></td>
	<td class="line x" title="36:134	In this experinlent, we if)cussed on noun phrases, as they are central in most terminologies." ></td>
	<td class="line x" title="37:134	2.3 The simplification algorithm The objective is then to reduce automatically the numerous and complex nominal phrases provided by AlethGram and LEXTEI to elementary trees, 3The Eureka GRAAL project gathers in France (IC1-F, RLI (prime contractor), EDF, Aerospatiale and lenanlt." ></td>
	<td class="line x" title="38:134	which more readily exhibit the flmdamental binary relations, and to classify words with respect to these simplified trees." ></td>
	<td class="line x" title="39:134	For instance, from the parse tree for slenose serve de le tronc eommun gauche 4 (cf.fig." ></td>
	<td class="line x" title="41:134	2, in which non terminal nodes are indexed for reference purposes), the algorithm 5 yields the set of elementary trees of figure 1." ></td>
	<td class="line x" title="42:134	'l'he trees a and c correspond to contiguous words in the original sequence, whereas b and d only appear after modifier removal (see below)." ></td>
	<td class="line x" title="43:134	Two types of simplifications are applied when possible to a given tree: 11.,5'plitting: Each sub-tree immediately dominated by the root is extracted and possibly further simplified." ></td>
	<td class="line x" title="45:134	For instance, removing node NP0 yields two sub-trees: NP\], which is elementary (see below) and PP2, which needs further simplification." ></td>
	<td class="line x" title="46:134	2." ></td>
	<td class="line x" title="47:134	Modifier removal: Within the whole tree, every phrase which represents a modified constituent is replaced by the corresponding non modified constituent." ></td>
	<td class="line x" title="48:134	For example, in NP0, the adjectival modifier scrrc is removed, as well as the determiner and the adjectives 4 Tight stcnosis of left common mainstem." ></td>
	<td class="line x" title="49:134	In both parsers, {,he accents are removed during tile analysis, the lemmas are used instead of inflected fo,'ms. Additionally, fro' simplitication purposes, a contracted word like du is considered as a prepositiondeterminer sequ_enec." ></td>
	<td class="line x" title="50:134	5See (Habet't el; al. , 1.995) for a detailled presental,ion." ></td>
	<td class="line x" title="51:134	The corresponding software, SYCI,AI)E, has been developped by the tirst author." ></td>
	<td class="line x" title="52:134	491 N|)a, NP AP I I N A I I st, chose Serl?e NPv N P,, Nl)~t NP PP ~ I ~ NP AP NP A P N /' Xl' I I I I I I I N A N /I stc~ms~: de N \[ \[ \] I \[ troIlc COIflfft/l\[l ~Tollc.(\]ditch( ~ tronc l)'igm:e 2: I,\]lcmcntary trees for sl.cnose serve de lc tro,m co'mmm~ (l(mchr." ></td>
	<td class="line x" title="53:134	-_coronarien -._gauche -~------___._ a t t ein t e. de~.~ diametre .de --------'-----I~ tr0nc ~ ~,~;tenose de / coronarJ ell /\ / / '~ m rena\],~ / 5~ / ~ presence .de~ /  coronare -. coronarien / \ / '." ></td>
	<td class="line x" title="54:134	I / --_ell.staLe \ coronarJ en L  clrconrzexe / montre_de \ -de artere -\ -~\]roxlmal z -' -' \] k de artere / ~ dJametre de .-.joroximal \ / / ~dr~n \ / / de intervent-riculaire /_ aorti~,e ~ ~, ~' /  /  / '~corortarzen/\ | \ \[ / coronarien --dkffus / \ I \ / / . ~ circonflexe  / \ / k I / lorpzma coronarien existence de \  'ien~ | / / --dia on~\] ~7--k / injec~q '--de artere ~ ~ ~/ g,c   ~ \ c  rJ  --~' non-slgnlr icaEz / ~,." ></td>
	<td class="line x" title="55:134	'~, de carotide4~ -r '~ ~ / ~ \ N mztra ~ ~ eszaue / ~ \ ~7, S / N. ~ (llagnostlC de -d2 cnat;idetdulaJ !e ~ % c;ne arien'-de-'trOnc <rnarien// I ~ ---'." ></td>
	<td class="line x" title="56:134	X -_ / -' ~./ b atheromateux persist  e deI kde artere /.-_severe-/I. ire / / X-~ /severite de_<coronarien / / ~,~ .// / i~ -severeX OOCtUS~ -Cgrl~arlen --ce;;earzen '~'o~ ~'~  / / X -de artere -coronar\] en coronare de artere ' ' --~ de tronc { --severe coronarlen \ | --de tronc -_im3ortant \\severe -~de_ar Sl~tm~-~----___~_coronazzen de artere --'---'!~ a~nte ~ mab~ 1 ~ coronarien diagnostic de -~ frequence de -,1 a~asdemse Figure 3: Example of a strongly connected component ((\]MC corpus) 492 uiodifying I, ro~.c, which l('~ts (.o elenienl;ary I;ree b. W\[i(;li I;\]ie (:lirrolil." ></td>
	<td class="line x" title="57:134	imee is clc'm.~;nlary, t\]io siniplific~lJon process s~ops." ></td>
	<td class="line x" title="58:134	I~efT)re I)rocessing lill(, s(;L ()f oril>;iuM \])aJ:se l;l:ees, OllC llillS\[; dec/a>re 1.|le l;l:ees which li:i/lsi; iv)l; I)e sinll)lilied a.lly fllrl.\]ier." ></td>
	<td class="line x" title="59:134	Ill I:)iis exl)el:inlent;, a.rc (:om~id(:red as e/el\]i(;il\[,itl'y l,he ilOliiinaJ I;l'(;es which exhil)it, a. binary 1;ela.l;\]Oll I)eI;w(;(;l~ l;wo '('oiil;elil'." ></td>
	<td class="line x" title="60:134	words, \[7)r iliSl.a.ii(:e I)ei;weeN /,wo N in &ll N \]) N SCqll011cc." ></td>
	<td class="line x" title="61:134	2.4: lProni (:h:m(:nl;ary ('ont;(:xts I;o word ( '.\].,}It .',4 S ( ~." ></td>
	<td class="line x" title="63:134	S 'l'lle i:esull;iug collo(:~lJons a,e tout, rolled I)y IJie synl.acl;ic felaJ;ionshit)s sl, rll('t;llrillg l, he l)a.r.'q(, t;l'ees, which is liOl." ></td>
	<td class="line x" title="64:134	t;lie case For wi n(low--t)ased a.i)l)roaches ((Jhurch a.nd lla.liks, 199())~ ev(;n wh(;n lJley use i)a.l:l,-oP-Sl)cech la.I)els (Sniasljn, 1993; I)a.ille, 1994)." ></td>
	<td class="line x" title="65:134	Ill l;he ('Xallll)\]( % .qaitr:hc is llOf i:elaJ,ed 1;o s/,<:ltosc', as il." ></td>
	<td class="line x" title="66:134	does iiol~ liiOtti\[3~ this noun." ></td>
	<td class="line x" title="67:134	'l'hc eleiiieii/a.ry l;rces I<~d 1,o oh>sos of syiiDI.C(;ic COllt;C:;',.:Lq." ></td>
	<td class="line x" title="68:134	I'or hlsl;a, ll(:e, frOl\[l {;h(; /ec' corl'(;s|)(:)ll(\[ill~ i;o s/,v/tosc,~{~'#'~'c, t;wo (:lasses o1' (:Oll(,exl;s aye crca.lxxl." ></td>
	<td class="line x" title="69:134	'l'hc Ih'sl, ()tie, <s/cltos,': ~, iu which sl;a.nds Ior t, he lfiw)t, word, conl.a.ins serf+, whereas l.\[i(; second o11(~> N .'7C;P7'C~ (:Oiii;a.illS SC'ItO.S<." ></td>
	<td class="line x" title="70:134	kl; t;he end o1' l;he SilUl)lilic;tl, iou process, I, ll(;s(; classes ha,re I)(:ei~ cOUll)lelied ~licl olJi(;r oiler; (:rea l;e(I. VVe (-laim t,h~l, th(' s(,inant;i(', similaril,y I)elween two lcxical enl, ries is in i)l:Ol)orl;ioli wii;h I;lie mlillt)er of sha, red (:Olll;(:xl,s, \[,hi: insl;mlc(', in ol,, of ore' ((:orl)ora.,,s/,e~tosu,'.ha r(;s 8 conliel,s wit, h l(szom In order I,o get, ~ glohal vision of the similm:il.ies relyi,g on elenient, ary conl.exD;, a. gi'ad)h is C, Olill)lil;c:(\]." ></td>
	<td class="line x" title="72:134	Tim WOl~(ls CO\[lSl;il;llt;,:; l, hc IIO(Ics." ></td>
	<td class="line x" title="73:134	A link corresl~onds 1.o a. Cel:l;&ili lliiliil)er oF shared c.oni;exl;s (a<:c.ordill~ l,O ~t. chosen I.hreshold)." ></td>
	<td class="line x" title="74:134	The edges are labclle, d wiiJi l, he sha.red coiit;cxls." ></td>
	<td class="line x" title="75:134	The sl;l:oiigly colineclx;(I c.oinponeill~.s ~I a.nd t;hc cliques '/ a.l'('~ conil)ul.ed a.s woll, ~s t.hcy ~re l;he tiiosi; t'(;l(; Va.lll; l)a.rl,s oF {tie gra.i)h ~ oil i,opologica.I ~lX)lilidS, '\['lie un(l('.l:lying illl;liil;ion is l;h a,l; a~ COiliieclA;d (:Olll\])Otleli/; I'C'\]itl,(:',S lil:':igjhl)ori/igj words (llollSC\]/ and Savit;ch, \]9!)5) m~d I, hal, the cliques tend l,o isoIal;c,<dmih~i:il;y cla,ssc's." ></td>
	<td class="line x" title="76:134	An ext;rm::t of a connc'ci;ed conll)onenl;, wil;h 3 as a, threshold, a,l)pears in ligu r(; 3." ></td>
	<td class="line x" title="77:134	s'\]'he sub-graphs hi which l.here is ~t 1)aLh I)cl.ween every pair of (lisl>hicl; liO(1CS." ></td>
	<td class="line x" title="78:134	rThc sul><~ra, phs in wlfich l;here is a palJl I)et;wee\]l each lto(le and eve>r?/ olhcr noch: of l;he graph." ></td>
	<td class="line x" title="79:134	3 Results 3.1." ></td>
	<td class="line x" title="80:134	Two corpora We haw; l.esl;ed olir niei.,hod on I.wo i;echnicM niedium-size col:pora The fii:sl; ()li(;> i;he INII-cleaJ: '\[}x:hliOlogjy (}Ol.\])tls (N'I'C) of EI)I', is of a,I)olll;,52,000 words." ></td>
	<td class="line x" title="82:134	'l'he second one, I,he (k)i;<) n~u;y Medicine (JOrl)US (CM(7), is of a.I)ou, ($(), 000 words." ></td>
	<td class="line x" title="83:134	It was buill; for t, he l,;urol)ca.u M I'\]N El,AS t)rojccl." ></td>
	<td class="line x" title="84:134	(Zweigenl)a, ilni, 19)/I) and is used For 1)ilol." ></td>
	<td class="line x" title="85:134	sl,udies in l;erminology exLra.clk)n s. 3.2 A vlsnal lliap of {:OIICOps lUld relationships I@en if iio onl;ology (:~u/ I)c \['ully aJIl;o\]iiat, R:a.ily derived \[;l:Olil a." ></td>
	<td class="line x" title="86:134	('orl)iis (llaJ)erl; ;tll(\[ Na.zarelll,:o, 1996), IJle,gY( JI,A I)1,\] gra.I)hs ('AI.II I)e iise(I I.o I)()oi.slma I) i.he I)ilihting of l, he onl;olog;y o(' a. dolila.iu." ></td>
	<td class="line x" title="87:134	'l'he SY(',I,AI)I,; ii(fl, work gives a <glol>a,I view over t,hc COrl).S which etmhles {m all, ernal;e i)a, ra,digniaJ;ic a, nd sylil;agul~l;ic exl)lora.l, iou of I,he cont;cxl; o\[' a word." ></td>
	<td class="line x" title="88:134	'l'hc gl;al)h (;nat)les 1;o idenl,ify I;lic concel)t,s, I hcir possit)lc t, yl)icaJ I)rOl)erl, ies, a, lld also t, he rcla, l;ionshil)s b(;I;weCll 1;he selecl,cd COilCel)l,s. Tim cliques I)ring ()ill; sitia.ll i)ara.diglllai.ic scl.s of \['orins which, ill a. tirsl, sl.et) > Ca.ll Im iuLer I)relx;d as onl;ologh:M classes rellecl;ing coliCelfl~." ></td>
	<td class="line x" title="89:134	'l'he a.rc lal)Ns l.ticn help Ix) retilie I.llosc chlsses t)y acldiu S sOlile of the Sllrl'Olilidhl~ words whicli axe li()l, pa.i'l; ()\[' t;lie clkluc bul." ></td>
	<td class="line x" title="90:134	which ileverllie-le~s sha,r(; the iiIOSl; siguifica.nl; or SOllle Siiliila.r <Ollbexl,8." ></td>
	<td class="line x" title="91:134	1@0111 the clique {sl,+e~,o,sc, b.<Uos b obsl, r~ml, ion, altcinb:} (of." ></td>
	<td class="line x" title="92:134	fig." ></td>
	<td class="line x" title="93:134	3), one ca, t\] build l lie cla.ss of all'eel;ions which arc Io(:al;ed in l, he I)odv as {Idam.:, occ1.<~7o., s/~.Js<~, Ic,~7o., <:.l<:{li~:ali<.,, ob,~'l, rl,:l, ion, aZl, c'inl, c}." ></td>
	<td class="line x" title="97:134	Siinila.rly, from l, he gt'al)h o\[' I;he (~'M(7 corpus, Oile (:a.li i(leni,i\['y l.ll~ classes of body' ~ii, cs { artcrc, I.'anchc, rcs+sa~l, 'v<'ntri, ~dc, intc',rve, nlriculairc, cft'roli(l~,}, o(' diseases { 'malmli+, arth, crvsclcrose} and oF chirul'gica/ m:ts { l)o*ll,(~gc, rcvasc.ularisatio'n, angioplastic}." ></td>
	<td class="line x" title="98:134	Olic(; l.\]ieS(; (;Oli(:('pts axe identilied, t, hei r i)rolJei;{,i('~S Ca, fl I)C lisl, e(l, t)y i llt;erl)l:el, ing ~ I;he l ld)cls of I lie links, 'l'he al;t;ri hu I~(, of the localizaJJon of l,h(' aJl'ecl;ions is descril)ed l;\]irough three, kinds oF u:lodiliers (lig." ></td>
	<td class="line x" title="99:134	',/):,io,,n,~ (,-, (t<'." ></td>
	<td class="line x" title="100:134	{ a,'Z,.~,'<~ ', t,','<,,u:)),,,;,i,os <' ~,,:lyrics (~ d(; {ca'rotTd<', #tl, crventrTculaTre} aud a(l-,iectivcs rela, l;cd to ;~ q)('(:ific aa'l;ery (~ {coro#utiru, co'ronaricn, diaqonal, ci'lvonfl<~:(;})." ></td>
	<td class="line x" title="101:134	'l'he a l, i;i:iblll;e (legr(:e of (;lie a, fl'ecl, ion is a, lso reveaJed IJlrougjh {~ si,qm/ical, if, n<m-,siqnificati.l; severe, 7m, l)Orl.a.l. , s<;veriZc} . '(41roupe '\['erniinologie el; lnl;elligence Ari;iticielle, I ~ IC--(_I I )171." ></td>
	<td class="line x" title="102:134	| ul;elligcnce A r@icicltc, (7 NI {S 493 etude~ ~evaluatlon a t~.alyse calcul etudeS5 b N ~ essai analyse Figure 4: Polysemy of etude Last, relationships between concepts can be extracted, such as the'part-of' relation between tronc and artere, and segment and artere (fig." ></td>
	<td class="line x" title="103:134	3)." ></td>
	<td class="line x" title="104:134	3.3 Distinguishing word meanings Polysemy and quasi-synonymy often makes the ontological reading of linguistic data difficult." ></td>
	<td class="line x" title="105:134	However, through cliques and edge labels, the SYCLADE structured and documented map of the words helps to capture the word meaning level." ></td>
	<td class="line x" title="106:134	Among a set of connected words where w is similar to wi and wj, cliques bring out coherent subsets where wi and wj are also similar to each other." ></td>
	<td class="line x" title="107:134	We argue that the various cliques in which a word appears represent different axes of similarity and help to identify the different senses of that word." ></td>
	<td class="line x" title="108:134	For instance, in the whole set of words connected to etude (study) in a strongly connected component of the NTC graph (analyse, evaluation, resultat, presentation, principe, calcul, travail), some subsets form cliques with etude." ></td>
	<td class="line x" title="109:134	Two of those cliques (resp." ></td>
	<td class="line x" title="110:134	a and b in fig." ></td>
	<td class="line x" title="111:134	4 threshold of 7) bring out a concrete and a more theoretical use of etude." ></td>
	<td class="line x" title="112:134	The network also enables to distinguish the uses of quasi-synonyms such as eoronaire and coronarien in the CMC corpus." ></td>
	<td class="line x" title="113:134	Even if they are among the most similar adjectives (7 shared contexts) and if they belong to the same clique {coronaire, eoronarien, diagonal, circonflexe}, the fact that eoronarien alone is connected to evaluation adjectives (severe, signifieatif and important) shows that they cannot always substitute to each other." ></td>
	<td class="line x" title="114:134	4 Towards an adequate similarity esfimatation for the building of ontologies The comparison with the similarity score of (Hindle, 1990) shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning." ></td>
	<td class="line oc" title="115:134	Hindle uses the observed frequencies within a specific syntactic pattern (subject/verb, and verb/object) to derive a cooccu,> rence score which is an estimate of mutual information (Church and Hanks, 1990)." ></td>
	<td class="line x" title="116:134	We adapted this score to noun phrase patterns) However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis." ></td>
	<td class="line x" title="117:134	The subgraph of the chirurgical acts words, which is easy to identify from the SYCLADE graph (fig." ></td>
	<td class="line x" title="118:134	5a), is split in different parts in the similarity graph (fig." ></td>
	<td class="line x" title="119:134	5b)." ></td>
	<td class="line x" title="120:134	This difference stems from the fact that this cooccurrence score overestimates rare events and underlines the collocations specific to each form." ></td>
	<td class="line x" title="121:134	1 For instance, it appears that the relationship between stenose and lesion, which was central in figure 3, with 8 shared contexts, almost diseappears if one considers the number of shared cooccurrences." ></td>
	<td class="line x" title="122:134	Therefore, similarity measures based on cooccurrences and similarity estimation based on shared contexts must not be used in place of each other." ></td>
	<td class="line x" title="123:134	As opposed to Hindle's lists of similar words which are centered on pivot words whose neighbors are all on the same level, in SYCLADE graphs, a word is represented by its role in a whole syntactic and conceptual network." ></td>
	<td class="line x" title="124:134	The graph enables to distinguish the various meanings of words, a crucial feature in the ontological perspective since the meaning level is closer to the concept level than the word level." ></td>
	<td class="line x" title="125:134	In addition, the results are clear and more easily interpretable than those given by a statistical method, because the reader does not have to supply the explanation as to why and how the words are similar." ></td>
	<td class="line x" title="126:134	The building of an ontology, which is a timeconsuming task and which cannot be achieved automatically, can nevertheless be guided." ></td>
	<td class="line x" title="127:134	The SYCLADE graphs based on shared contexts can facilitate this process." ></td>
	<td class="line x" title="128:134	9For instance, for Na PN2 CoocNi,N~ : log 2 ~~ where f(NIPN2) is the fi'equency of noun N1 occurring with N2 in a noun preposition pattern, f(N1) is the frequency of NI as head of any N1PN,~ sequence and f(N2) the frequency of N2 in modifier/argument position of auy N~PN2 sequence and k is the count of NxPN v elementary trees in the corpus." ></td>
	<td class="line x" title="129:134	COOCNAda and CooeAd~N are similarly defined." ></td>
	<td class="line x" title="130:134	1The various cooccurrence scores retrieve sets of collocations which are sharply different fi'om the contexts shown by SYCLADE connected components." ></td>
	<td class="line x" title="131:134	The coll6cations which get the greatest cooccurrence scores seem to characterize medecine phraseology (facteur (de) risque, milieu hospitalier) but not the coronary diseases as such." ></td>
	<td class="line x" title="132:134	494 pontage angioplastie ~ artere \/ revascul~risation pontage angloplastle I her ed~tC~ionl~y!!: sme pont artere stenose l,'igure 5: Similarity among the chirurgical act family Acknowledgments We ~hank (\]hristian 3aequemin (IRIN), Didier Bourigault, Marie-Luce Herviou, JeanDavid Sta (DER EDF), Marie-tl~51~ne Candito (TAI,ANA) and Sophie Aslanides (ELI) for their remarks on a previous version of this l)aper." ></td>
	<td class="line x" title="133:134	We are very gratefid to Serge Heiden (ELI), who has developed G~aphX (ftp://mycroft." ></td>
	<td class="line x" title="134:134	ens-f c:l fr/pub/graphx/), I;hc graph interactive handling software that enabled us to visualize and handle the SYCLADI,3 graphs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-1097
A Statistical Method For Extracting Uninterrupted And Interrupted Collocations From Very Large Corpora
Ikehara, Satoru;Shirai, Satoshi;Uchino, Hajime;"></td>
	<td class="line x" title="1:186	A Statistical Method for Extracting Uninterrupted and Interrupted Collocations from Very Large Corpora Satoru Ikehara, Satoshi Shirai and Hajime Uchino NTT Communication Science Laboratories Take 1-2356, Yokoshuka-shi, Japan (E-mail:{ikehara, shirai, uchino}@nttkb.ntt.jp) Abstract In order to extract rigid expressions with a high frequency of use, new algorithm that can efficiently extract both uninterrupted and interrupted collocations from very large corpora has been proposed." ></td>
	<td class="line x" title="2:186	The statistical method recently proposed for calculating N-gram of m'bitrary N can be applied to the extraction of uninterrupted collocations." ></td>
	<td class="line x" title="3:186	But this method posed problems that so large volumes of fractional and unnecessary expressions are extracted that it was impossible to extract interrupted collocations combining the results." ></td>
	<td class="line x" title="4:186	To solve this problem, this paper proposed a new algorithm that restrains extraction of unnecessary substrings." ></td>
	<td class="line x" title="5:186	This is followed by the proposal of a method that enable to extract interrupted collocations." ></td>
	<td class="line x" title="6:186	The new methods are applied to Japanese newspaper articles involving 8.92 million characters." ></td>
	<td class="line x" title="7:186	In the case of uninterrupted collocations with string length of 2 or mere characters and frequency of appearance 2 or more times, there were 4.4 millions types of expressions (total frequency of 31.2 millions times) extracted by the N-gram method." ></td>
	<td class="line x" title="8:186	In contrast, the new method has reduced this to 0.97 million types (total frequency of 2.6 million times) revealing a substantial reduction in fractional and unnecessary expressions." ></td>
	<td class="line x" title="9:186	In the case of interrupted collocational substring extraction, combining the substring with frequency of 10 times or more extracted by the first method, 6.5 thousand types of pairs of substrings with the total frequency of 21.8 thousands were extracted." ></td>
	<td class="line x" title="10:186	I. Introduction In natural language processing, the importance of large volume corpus has been pointed out together with the need for technology of analyzing these linguistic data." ></td>
	<td class="line x" title="11:186	For example, in machine translation, there are many expressions that are difficult to be translated literally." ></td>
	<td class="line x" title="12:186	Phrase translations or pattern translations based on phrase or pattern dictionaries are considered very useful for the translations of these expressions." ></td>
	<td class="line x" title="13:186	In order to realize these translation, it is required to identify phrases of high frequency and patterns of expressions from the corpora." ></td>
	<td class="line oc" title="14:186	There are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words (Church and Hanks 1990); the distance between words (Smadja and Makeown 1990); and the number of combined words and frequency of appearance (Kita 1993, 1994)." ></td>
	<td class="line n" title="15:186	But it was not easy to identify and extract expressions of arbitrary lengths and high frequency of appearance from very large corpora." ></td>
	<td class="line x" title="16:186	Thus, conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted (Smadja 1993, Shinnou and Isahara 1995)." ></td>
	<td class="line x" title="17:186	Recently, a new method which can calculate arbitrary number of n-gram statistics for very large corpora has been proposed (Nagao and Mori 1994)." ></td>
	<td class="line x" title="18:186	This method has made it possible to automatically and quickly extract and tabulate substrings of any length used in source texts." ></td>
	<td class="line x" title="19:186	Unfortunately, in this method, so many fractional substrings that were grammatically and semantically inconsistent were being extracted that it was difficult to extract combi nations of expressions collocated at separate locations (i.e. interrupted collocation) which requires a search of the source text by combining the strings thus extracted." ></td>
	<td class="line x" title="20:186	Thus, the analyses had to be limited into small texts (Colier 1994)." ></td>
	<td class="line x" title="21:186	To overcome this problems, this paper first, proposes a method that can automatically extract and tabulate uninterrupted collocational substrings and without omission from the corpora in the order of substring length and frequency under the condition that fractional substrings are excluded." ></td>
	<td class="line x" title="22:186	Second, using the results of the first method, it also proposes a method that can automatically extract and tabulate interrupted coUocational substrings." ></td>
	<td class="line x" title="23:186	2." ></td>
	<td class="line x" title="24:186	N-gram Method and the Problem Involved (1) Conditions for Collocational Substring extradtion In order to extract uninterrupted collocation without omission and to minimize extraction of fractional substrings, we will introduce the following three conditions." ></td>
	<td class="line x" title="25:186	1st Condition: Substrings can be extracted in the order of the number of matching character (string length)." ></td>
	<td class="line x" title="26:186	2nd Condition: Substrings can be extracted in the order of frequency of use." ></td>
	<td class="line x" title="27:186	3rd Condition: Substrings should be extracted according to the principle of the longest match." ></td>
	<td class="line x" title="28:186	Fig." ></td>
	<td class="line x" title="29:186	1 Substrings to be Extracted Here, 3rd condition means that when a string (for instance a in Fig.l) is extracted from a certain location within the source text, any substring ( B, T ) that is included within the string ( a ) is not subject to extraction." ></td>
	<td class="line x" title="30:186	But should such substring ( 6 ) be located in a separate or overlap 574 position, it is to be extracted." ></td>
	<td class="line x" title="31:186	(2) Conventional Algorithm for N-gram Statistics Before discussing the algorithm which satisfies the previous conditions for uninterrupted collocational substring, let's consider the Nagao and Mori's algorithm propose for N-gram statistics." ></td>
	<td class="line x" title="32:186	\[Statistical Method for N-gram\] Assume that the total number of characters in a source text (corpus) is N. Procedure 1: Preparation of Pointer Table Prepare PT-0 (Pointer Table-O) of N records of SP (Source Pointer), with the values of 0, 1, 2, i,,N-1." ></td>
	<td class="line x" title="33:186	Here, the value i represents the String-word i which is the substring from position i to the last character (N-1 address) in the source text." ></td>
	<td class="line x" title="34:186	Procedure 2: Pointer Table Sorting The records of.PT-0 are sorted in the order of corresponding String-words to obtain SPT-O (Sorted Pointer Table-0)." ></td>
	<td class="line x" title="35:186	Procedure 3: Counting of Matching Characters The characters of String-word i is compared with that of the next String-word i+1 from the beginning." ></td>
	<td class="line x" title="36:186	The number of matched characters are registered in the field of a NMC (Number of Matching Character) in the record i. Procedure E: Extraction of Substrings Comparing the values of NMCs of record i and that of the record i+1 of the SPT from i=1 to i=N-1, substrings are extracted and their frequency are determined* 1." ></td>
	<td class="line x" title="37:186	(3) Problems of N-gram Statistics Nagao and Mori's method obviously fulfills requirements of Conditions 1 and 2, but not Condition 3." ></td>
	<td class="line x" title="38:186	It is expected that the accurate frequency of any substring a is obtained subtracting the frequency by the frequency of the other substring ~ which is included in substring o~ *2." ></td>
	<td class="line x" title="39:186	Unfortunately, this does not satisfy Condition 3." ></td>
	<td class="line x" title="40:186	At the time when extracted substring list has been compiled, information regarding mutual inter-relationship between the extracted substrings within the original text has been lost rendering calculations impossible." ></td>
	<td class="line x" title="41:186	3." ></td>
	<td class="line x" title="42:186	Extraction of Uninterrupted Collocation 3.1 Invaliditafion of Extracted Substfings (1) Co-relations between Extracted Substrings In order to satisfy the requirement of Condition 3, consider the extraction of n-gram substring after extracting m -gram substring." ></td>
	<td class="line x" title="43:186	The problem arises when there is a certain overlap between them as shown in Fig.1." ></td>
	<td class="line x" title="44:186	The Case of Absorbed Relation (Case 1) can be classified into three sub-cases as shown, but regardless of which situation, the m-gram substfing is absorbed in the substring of n-gram and therefore there is no need to extract such a m-gram substring." ></td>
	<td class="line x" title="45:186	Thus, when extracting n-gram strings, there is a need to invalidate the related record of the SPT so that m-gram strings do not become involved in processes to follow." ></td>
	<td class="line x" title="46:186	Fn g,'am \] ., ram  ! , Coincided Beginning < case 1 1 > II1 gram L'2.L~)2.KZ:I \] I ll grain \] K~!Lg.!!!CZ\] Holy Included Coincided Ending < easel2> <casel3 > <Casel> Absorbed relation t~-n gram \]  l :--I11.~ r alll  : \[-rl gram \] preceded by m gram preceded by tl gram <case2-1 > < case22 > <Case2> Overlapped relation Fig." ></td>
	<td class="line x" title="47:186	2 Relationships between Extracted Substrings The Case of Partially Joint Relation (Case 2) can be further classified into two sub-cases." ></td>
	<td class="line x" title="48:186	But in either situation, the m-gram string and n-gram string merely overlapped and therefore they are need to be extracted separately." ></td>
	<td class="line x" title="49:186	(2) Necessity of Validity Check for String-words When one substring is extracted, in order not to extract the absorbed string from the same part of sotlrce text where the substring was already extracted (Case 1), related records of SPT need to be checked if the record is valid or not before extracting the next substring." ></td>
	<td class="line x" title="50:186	For example, the substring of 6 characters in the String -word 3 shown in Fig." ></td>
	<td class="line x" title="51:186	3 was extracted, the substring of String-words 3,4,5,,8 need to be set as invalid for the length equal or less than 6,5,4,.-.,1 characters from the beginning." ></td>
	<td class="line x" title="53:186	Source Address: 1 2 3 4 5 6 7 8 9 10 11    r6 gram Source Text: A B /C D E F G I\[ I J K.   Addres Invalid Range i /-~String-word 4 < 5 ch I D E F G H I K 5 4oh \[E F C ~-{.,i I K 6 < 3 ch IF G H i \] K 7 < 2 ch G HI l K 8 ~ 1 ch \[HI I K Fig.3 Example of Validity check 1 3.2 Extracting Algorithm Here, we propose an algorithm which satisfy Condition 3 as well as Conditions 1 and 2." ></td>
	<td class="line x" title="55:186	< Preparation > Fields of NSC (Number of Significant Characters) and RN (Record Number) are added to SIT-0 (Sorted Pointer Table) used for N-gram statistics." ></td>
	<td class="line x" title="56:186	<Algorithm (See Fig.4)> Procedure 1 thr_ough 3: Same as the N-gram statistics." ></td>
	<td class="line x" title="57:186	Procedure 4: Significant Character Determination The length of substrings to be extracted are decided from NMC and written in the NSC field of SPT0." ></td>
	<td class="line x" title="58:186	Procedure 5: Preparation of Augmented PT After sorting the SPT-0 in the original order, add a VP (Validity Flag) field to obtain an PT1." ></td>
	<td class="line x" title="59:186	* 1 Extraction is conducted based on the relation between the values of consecutive NMC." ></td>
	<td class="line x" title="60:186	Ddetails are in (Nagao and Mori 1994)." ></td>
	<td class="line x" title="61:186	* 2 Recently, combining the frequencies of related substring, calculation was conducted(Kita, etal 1993) to obtain the frequency which satisfy the Condition 3." ></td>
	<td class="line x" title="62:186	But accurate results cannot be obtained by this method." ></td>
	<td class="line x" title="63:186	575 Procedure 6: Validity Determination According to the method shown in 3.1(2), check the validity of the suhstring pointed by the records of the PT-1 in the order of the record number and write the results in the VF field." ></td>
	<td class="line x" title="64:186	Procedure 7: Resorting of PT-1 Re-sort the PT1 in the order of the values of SP fields to obtain a SPT1." ></td>
	<td class="line x" title="65:186	Procedure 8: Extraction and Tabulation By referring to the SPT-1, the strings to be extracted are determined and their frequencies are calculated." ></td>
	<td class="line x" title="66:186	An example of the algorithm is shown in Fig.4." ></td>
	<td class="line x" title="67:186	In this example, the types of substrings extracted by the conventional algorithm amounted to 24 with the total frequency of 72." ></td>
	<td class="line x" title="68:186	In contrast, in the method proposed in this paper, these numbers have reduced to 5 and 10 respectively." ></td>
	<td class="line x" title="69:186	4." ></td>
	<td class="line x" title="70:186	Extraction of Interrupted Collocation 4.1 Conditions for Extraction Here, let's consider combinations of 2 or more uninterrupted collocational substrings in different locations within a single sentence together with a method of determining the frequency of them." ></td>
	<td class="line x" title="71:186	In this case, boundary conditions of sentences and mutual relationship between the extracted substrings need to be considered." ></td>
	<td class="line x" title="72:186	(1) Boundary Conditions of Sentences When considering the collocation of substrings within a sentence, combinations of expressions spread over borders of sentences need to be excluded." ></td>
	<td class="line x" title="73:186	But when a single sentence includes other sentences, the extraction of the combinations in units of sentences poses complications." ></td>
	<td class="line x" title="74:186	To simplify matters, we first assume that the substrings which have any kinds of punctuation mark as a part of them are not extracted in the procedure of uninterrupted collocation extraction." ></td>
	<td class="line x" title="75:186	This can be easily performed by restraining the comparison procedure after finding a punctuation mark in Procedure 3." ></td>
	<td class="line x" title="76:186	Second, we assume that when a left quote character is found within a sentence, all characters are ignored until the right quote character forming a pair with the former character." ></td>
	<td class="line x" title="77:186	(2) Relationships between Extracted Substrings In extraction of interrupted collocations, substrings that are linked to or partially overlap one another are excluded from the scope of extraction." ></td>
	<td class="line x" title="78:186	Let's consider substrings a and ~0 which have been extracted from the same sentence." ></td>
	<td class="line x" title="79:186	The positioning would be one of the three cases shown in Fig.3." ></td>
	<td class="line x" title="80:186	Case (c) in which substring a and ~0 are separate from one another is a case of extracting interrupted collocations, and Cases (a) and Co) are not*3." ></td>
	<td class="line x" title="81:186	(3) Order of Substring Appearance In the case of extracting interrupted collocations, the order of appearance of substrings should be considered." ></td>
	<td class="line x" title="82:186	Hence, collocational substrings are extracted and counted taking notice of the order of the appearance of each substring." ></td>
	<td class="line x" title="83:186	Beginning \[ot  (a) Connected  \[  i End L-, T  Beginning ! V a\] End (b) Overlapped Beginning \[a ~\] \[' B  i End (c) Separated  a, B, 7' : Extracted Substfing Fig.5 Relations between Extracted two Substrings 4.2 Extraction Algorithm \[Preparation\] Sequential number is given to all of the substrings extracted in Chapter 3 in the order of extractions." ></td>
	<td class="line x" title="84:186	These Number are registered in the NES (Number of Extracted Substrings) field of the respective record in SPT1." ></td>
	<td class="line x" title="85:186	Procedure 9: Re-sorting the SPT-1 The SPT1 is sorted in the original order of the values of ST' fields." ></td>
	<td class="line x" title="86:186	Procedure 10: Numbering of the sentences SN(Sentence Number) field is added for entering the sentence number of original sentence to which one's record belongs." ></td>
	<td class="line x" title="87:186	Procedure !1: Table condensation The table obtained is condensed by procedures shown in the following to obtain a SPT-2'." ></td>
	<td class="line x" title="88:186	(1) All fields other than the four, Sentence Numbers, ESN, NSC and RN are deleted." ></td>
	<td class="line x" title="89:186	(2) All records with no values in the ArES field are deleted." ></td>
	<td class="line x" title="90:186	Procedure 12: Extraction of Interrupted Collocation Here, k is the number of substrings which compose interrupted colocational expressions." ></td>
	<td class="line x" title="91:186	Then, all of the combinations of k NESs for every sentence are written down into a file and sorted." ></td>
	<td class="line x" title="92:186	And the number of the same combination of NES are counted." ></td>
	<td class="line x" title="93:186	Thus, the substring list of interrupted collocations can be obtained." ></td>
	<td class="line x" title="94:186	If the sentence number is given to every combination list of NES, the sentences corresi~onding to the extracted interrupted collocation can easily be identified." ></td>
	<td class="line x" title="95:186	The lower part of Fig.4 shows the application of this method for k=2." ></td>
	<td class="line x" title="96:186	In this case, there are possibility of 25 combinations for 5 types of uninterrupted collocational substrings obtained by chapter 3." ></td>
	<td class="line x" title="97:186	Out of these combinations, 7 combinations were extracted as the combinations which collocate twice or more within the same sentence." ></td>
	<td class="line x" title="98:186	And the total frequency of these amount to 14 times." ></td>
	<td class="line x" title="99:186	5." ></td>
	<td class="line x" title="100:186	Experiments 5.1 Uninterrupted Collocational Substrings Applying the proposed method to the newspaper articles of Nikkei Industrial News for three months (8.92 million characters), uninterrupted and interrupted coUocational substrings were extracted." ></td>
	<td class="line x" title="101:186	In this experiments, XEROX *3 In the case of (a), there would be a combination of substrings which is regarded as a interrupted collocation." ></td>
	<td class="line x" title="102:186	However the frequency of such a pair is limitted to 1." ></td>
	<td class="line x" title="103:186	Then there is no need to consider." ></td>
	<td class="line x" title="104:186	576 'ancient' 'ancient' 'of' 'qtrange' 'ciike' . mukasi mukas~ no oKas\]na oKasl \[Source Text\] it-,'la~b it-,'D~bo) 2Sh~btx ~a~bo l~'~fl%b~ la:t~blat 2SD~bt, a~fa~bo <)~eaning) This is a story of cakes ill very old day." ></td>
	<td class="line x" title="105:186	The story of the cake is strange story." ></td>
	<td class="line x" title="106:186	tY\[O-O (Pointer Table) SP String-Words i 1 tb70, b?~ bl 2 ~ b~5'70~ boo : 3 b~;70, bo)~ ~ 4 t270, b ~s~,: 5 fl~boO~s70~bi Proc." ></td>
	<td class="line x" title="107:186	6 bo_)#D~L2: Sortb 7 o)~70, bt~s \[ 8 a3D, big ~s~),; 9 ~ bta)s~, bl :1 0 btg~s~O, bo : 1 1 ~70, bo ~i l:i12 g870~bo ~S70': 13 ~bo ts'D~bi be $S70~ boo : t5 o :~a70~bogL!ci Prec." ></td>
	<td class="line x" title="108:186	\]G *aTe, blit2: (;ounti t7: ~xbo.)l:tlg b\[ N~C 18 bo)llfg bI,-t: ~ 9 ff~lIt2 b~i:~s\[ 20 late bl~::t-S70',: Prec." ></td>
	<td class="line x" title="109:186	z 21 ta: blJ:fS70~ L I l)eter~ 22 b~;t~370~b/g: NSC 23 la~\]o, btg ~ 24 ~70, btats ~: 28 la~lita b o ', 29 Ilat; bo : 30 limbo 311 be 32 Lo." ></td>
	<td class="line x" title="110:186	! 'cake' 'of' :story' . ~s' 'qtrange' 'story' . okasi no I~allaSl na oKaslna ol~anasl O:Substring S1 W( la ta a5 tg a~ o (, ) J (@." ></td>
	<td class="line x" title="111:186	) I (." ></td>
	<td class="line x" title="112:186	@) I (." ></td>
	<td class="line x" title="113:186	) I Prec." ></td>
	<td class="line x" title="114:186	12 (, ) I Write ((5), _(D) I down (@, ) I< (, ~)) I -ZCI (, @)1 (@, )1 (, ) I case of (, ) I k:2 (, @)1 (, )1 (, @)1 (, @)1 (, )1 (@, 0))1 (@, @)1 (@, @)1 Prec." ></td>
	<td class="line x" title="115:186	12 \[ I Sorting SPT 0 (Sorted Pointer 'l'r, ble) I~T | V N N F S M R N SP C C 1 3 0 30 0 2 2 10 2 0 1 1 17 3 1 3 3 29!4 Prec." ></td>
	<td class="line x" title="116:186	5 1 3 3 85 Re-sorting 0 2 2 14 6 0 1 1 24 7 1 5 5 1 8 044 6 9 0 3 3 12 10 0 2 2 20 11 131 412 0 2 0 11 13 0 1 0 19 14 0 0 0 32 15 \[Prec." ></td>
	<td class="line x" title="117:186	6\] 1 3 3 3 i 6 Validity 1 3 2 9 17 Check(VF) 0 2 1 15:18 0 1 0 25 19 1 3 3 27 20 0 2 2 22 01 1 1622 1 1 1 26 2:3 1 5 3 224 0 4:2 7 25 0 31 13 26 0 21 21 27 1!o o 28 29 0 2 0 2330 0 1 1 18 31 0 0 0 31 32 (@, ) (), 0)) \[(,(b)\] (@, (0) (~, @) l~,~'~, IC@,O))I (@, @) ICCD, (4))1 _((--9, (2)) (~,~1 I(, )1 I(@~@)J J I Prec." ></td>
	<td class="line x" title="118:186	12 I Coasting Interrupted~ollocational Former Substring SN' 2 S N VN N N F F S M R N S C C 1(5) 13030 1 0 2 2 10 1 0 1 1 17 1  1 3 3 29 1  1 3 3 8 1 02214 1 0 1 1 2d 1@155 1 1 0 414 6 I 0 3~3' 12 1 0 22 20 1 ~ 1 3 1 4 1 02011 1 0 1 0 1 9 1 0 0 0 32 2(2) 1 3 3 3 2132 9 2 021 15 2 0 i 0 2@133 2 022 2 0 1 1,@))I 2 1 1 1 ~)1 2@i 5 3,)/ 2 o 42 @~ 2 o 3 1 )1 2 021 ~{~: @1 2 0 1 0 (,) 2 @ 1 3 o 2 020 2 0 1 1 2 000 Pairs of substring SPT 2* s N N N E S S P S C : Proc 11 1  3 1 Condense 1 @ 3 4 I  3 5 3 i6 2 ~);3 17 2@320 2 @ 5 <4) 3 I  : .< !  String~ 8P word Latter Substring a}ld Frequency i i ~i,'D, bi i 2 3 4 5: 6: 7 7 Prec." ></td>
	<td class="line x" title="119:186	g ~70' bi Sorting i0 b tg ~s i *sD, b~ 13 #/~L,o : Proc." ></td>
	<td class="line x" title="120:186	lO 14 (_,o ~S! Numbering i5 o ~d70~', for SN I6 $a~, bl 17 1/~ be.); 18 b o)l~t ! 25 19: 0)t~g: 27 20 latabl 22 2 t tall1: 16 22 blI~Si 26 2:3 I~t~)~: 2 2~ ~70~b! 7 25 ;)~b~: l 3 26 br~i 5 28 a~I~t~i 2 8 29 I;lt~b: 2 3 3o ta bo \[ 18 3t bo 3 1  i j _. <Sentence List>  kJ Sentence list for I./7 each pair of I interrupted / collocation_ _ C4)lta::D ' Ca)~i U:me ::::e SPT 1 ' rOposed Metliod!N-gram Method gram ''~'r eqnency \'ttrequency SU~strn'~ Subs!rhtg 5gram (1)~70, U~2:~2 ffs~o, b tS ~is 2 __~:~ z .L/_Z.L_: dgram,;O~ ba ~3 2 3gram ~70>U 2 ~D,U d = : ~  70, b'ta: 2 bg~a 2 @t~ts b 2 {arab 2 tYDSU 20D, b 2 Id : :,i, ~ ', ;,:, ~ 4 2 gram  ! :\] 70, b 6 < i = b  2 boo 2 }-: i }~ 7'g 7i3 2 I t<2 \[1 2 r :-:: l:t ?2." ></td>
	<td class="line x" title="121:186	2 77 _Z ~'J' 70' 2 5 :  ~ t~ 4 F  o~ 2 : I,'t: 3 10 Total 72 S P : Source Pointer a N : Record Number N M C : Number of matched Characters N S C : Number of Significant Characters V F : Validity flag N E S : Number of Extractc, d Substring S N : Sentence Number Fig." ></td>
	<td class="line x" title="122:186	4 Example of Uninterrupted and Interrupted Colh)cational Substring Extraction 577 ARGOSS 5270 (OS4.1.3) was used." ></td>
	<td class="line x" title="123:186	The memory capacity were 48 MB." ></td>
	<td class="line x" title="124:186	(1) Characteristics of Extracted Substring From the view point of the length and frequency, the number of extracted substrings are compared with those of the N-gram method and summarized in Table 1 and Table 2." ></td>
	<td class="line x" title="125:186	Some examples of extracted substrings are shown in Table 3." ></td>
	<td class="line x" title="126:186	And the examples of substrings with high frequency are also shown in Table 4." ></td>
	<td class="line x" title="127:186	Table 1." ></td>
	<td class="line x" title="128:186	Length and Number of Extracted Substrings t p Proposed Metlgod N-gram Statistics a: Extract b: Total c:Extmct d: Total (;ran Substring Frequency Substring Frequency 2 ~ 970,203 2.613,704 4,374,141 31,178,897 5~ 591,901 1,476,922 2,960,487 10,808,458 10~ 52,214 114,270 673,601 1,550,817 20~ 1,792 3,692 177,298 359,810 Ratio a/c b/d 22.2% 8.38% 20.0%13.7% 7." ></td>
	<td class="line x" title="129:186	75% 7." ></td>
	<td class="line x" title="130:186	37% 1.01% 1.03% Table2." ></td>
	<td class="line x" title="131:186	Frequency and Number of Extracted Substrings mp." ></td>
	<td class="line x" title="132:186	Proposed Method a:Extract b: Total Freq.\ Substring Frequency 2~ 970,203 2,613,704 5~ 67,321 551,441 10~ 12,351 217,934 20~ 2,288 92,804 50~ 285 37,850 100 ~ 76 24,167 200 ~ 20 16,771 N-gram Statistics c:Extmct d: Total Substring Frequency 4,377,087 39,588,291 882,217 31,288,701 372,291 28,050,199 169,375 25,871,964 62,991 22,209,875 30,316 19,961,961 14,363 17,759,432 Ratio a/c b/d 22.2 % B. 60% 7." ></td>
	<td class="line x" title="133:186	63% 1.76% 3." ></td>
	<td class="line x" title="134:186	32% 0." ></td>
	<td class="line x" title="135:186	78% 1.35% 0." ></td>
	<td class="line x" title="136:186	36% 0." ></td>
	<td class="line x" title="137:186	45% 0." ></td>
	<td class="line x" title="138:186	17% 0." ></td>
	<td class="line x" title="139:186	25Z 0." ></td>
	<td class="line x" title="140:186	12% 0." ></td>
	<td class="line x" title="141:186	14% 0." ></td>
	<td class="line x" title="142:186	07% From these results, the following observations can be obtained." ></td>
	<td class="line x" title="143:186	@ Compared with the N-gram method, most of fractional substring has been deleted, and the types m~d the number of the extracted substrings have highly reduced." ></td>
	<td class="line x" title="144:186	For example, in the extraction of substrings with the Table 3 Examples of Extracted Substrings (in the order gram Proposed Method b~Ct,~7~ (436), ~'J'~t~N~')g (277), C 0)?~), (158), (make it that ~ ), (EC), (for this purpose), 5 gram dJ'~r~'~ (141), ~1~ (141), ~/Y~-;Z'(133), (market share), (consider that ~ ), (motors), &~b\]c_(130), C~l<~,~b(126), c,~, (112), (enphasized that ~ ), (on the contrary), (subsequently ~ ), \[ 190,925 types Total 499,653 times \] (to be ~ ing), (second), 10 gram ~C&~Cf319~-9#2(19), 8 2~-~,/$>~'Y~(17), (it seems to do ~ ), (82 Japan shop), b-Cb~za ~b~ (16), 7 -2-'2 b >--)L H~N (14) (wonder if ~ do ~ ), (Washington 19 ), \[ 21,155 types Total 47,336 times\] length of 2 or more and the frequency of 2 times or more, the substring type reduced to 22.2 % and total frequency of them reduced to 8.38 %." ></td>
	<td class="line x" title="145:186	This effect increases as the increase of substring length." ></td>
	<td class="line x" title="146:186	In the case of substrings of 20 or more characters, these number reduced to 1%." ></td>
	<td class="line x" title="147:186	@ Most of substrings extracted by the proposed method forms expressions as syntactic or semantic units and there are few fractional substrings." ></td>
	<td class="line x" title="148:186	(2) Processing Time It took about 40 hours to make SPT-O*4." ></td>
	<td class="line x" title="149:186	But successive processes were performed very quickly (within one hour)." ></td>
	<td class="line x" title="150:186	5.2 Interrupted Collocational Substrings (1) Characteristics of Extracted Substrings Interrupted collocational substrings were extracted for every two substrings which had appeared 10 or more times in the source text*5." ></td>
	<td class="line x" title="151:186	The results are shown in Table 5." ></td>
	<td class="line x" title="152:186	And, examples of substrings with high frequency and with much characters in total are shown in Table 6." ></td>
	<td class="line x" title="153:186	Table 5 Number of Extracted Pairs of Substrings ~----___~_ Results Frequency -~--~ or more times _ 5 or more times 10 or more times 20 or more times No." ></td>
	<td class="line x" title="154:186	of Pair of Substrings 6,544 941 237 61 Total Frequency of Pairs 21,829 9,057 4,556 2,291 From these results, it can also be seen that expressions typical to newspapers have been extracted." ></td>
	<td class="line x" title="155:186	Thus, using the output results, we can easily obtain interrupted collocational expressions as well as uninterrupted ones." ></td>
	<td class="line x" title="156:186	of frequency) (cf)." ></td>
	<td class="line x" title="157:186	  :Fractional substfing N-gram Statistics 7~J:o~C(,~3(3710), ~Cb~'~, (2827), l<&~ &, (2753), (became to be ~ ), (be ~ ing but ~ ), (according to ~ ), \[<O (/~ ~ (2721), ~ ~2~2 b ~ ~ (2334), ;5 C & IVf~ (2286), (speaking about ~ ), (be done),   tv-~2o~b~ (2079), & lj~(~ (1997), ~ t t2@\]~ (1849), , (explain that ~ ), (57 fiscal year), \[ 748,172 types Total 3,793,077 times \] b It & C ;5 ~." ></td>
	<td class="line x" title="158:186	J: ~ &, (273), ~ 7'b IV." ></td>
	<td class="line x" title="159:186	b 7~< & C ~5 ~V. & (223), (from what ~ do),  , t~)J 5 7~ \[:." ></td>
	<td class="line x" title="160:186	b #_ & C 7) IV." ></td>
	<td class="line x" title="161:186	(223), t~ t:." ></td>
	<td class="line x" title="162:186	b ?c_ & C 7~ t< & ~ (222), ~V. b?." ></td>
	<td class="line x" title="163:186	A Y__ 7) tV.ck~ A (222),,~_~NItg~'}~:i~N~ (208), (according to that ~ was), (second research party), \[ 132,865 types Total 345,232 times \] Examples of Substrings (frequency > 200 ) Table 4 Examples of Substrings with High Frequency &b~9(586), &~\]t(512), &b~1,~5(436), ~#_(325) ~35~(324), ~{(315) (to say that), (said that), (set as), (again), (is that), (photogralihy), bT'j~b, (302), &~o 7~< (283), N~ (281), (~i~ (278), ~J'NJL, N# (277), bT)~b (274), N-f > b (269), (but), (said that), (Tokyo), (Price), (EC), (however), (Point), ~&~& (264), ~-}'~,~ (259), ~fc, (236), C~t2(220), ~_T<8) (204), ?<1~, (20I) \[ (one word), (sell term) (mere over) (this is) (for this sake), (yet) *4 Indirect sorting is conducted." ></td>
	<td class="line x" title="164:186	When this process is excuted within a memory by the computer which has a compare instruction with indirect adressing for arbitrary length of fields, sorting time will be extremely shortened." ></td>
	<td class="line x" title="165:186	*5 It is expected that when the frequency of each substring is small, the frequency of their cooccurence is further,small." ></td>
	<td class="line x" title="166:186	578 Table 6 Pairs of Substrings with High Frequency Collocations of Compound Nouns 4fllif~ ~ ~'~l#JlJl(257), qZ'~-g)I/~ :E-,~'--X'(ll7) (price ~ sell time), (General ~ Motors) (Summit ~ ) (EC ~ the European Community) 4 ~)> ~ ~.x,/~5'4 i~lgb'/-' (80) (lran ~ Japan Oil Industry) ~, ~& {Z~do~L3~)~&){~_~'cT~ (9), ~oJ~?tl,gc~,P~&)~#_ (9), (did ~ but said that), (In the answer to ~ said ~ ) (we talke that ~ ), (the contents is such that ~) Collocations of Sentence Patterns (moreover the minister said that), (doing ~ said ~) (the contents is ~ and so), (did ~ also about ~ ) b ~l~l~b ~ (5 bb ~(,1), ::)~ 0 ~:'~'o (4), ~t~ bx~l~ b~ (4) (as if ~ looks -~), (ilamely ~ is ~ ), (either ~ or (2) Processing Time In the case of interrupted collocational substring extraction, processing time depend highly on the number of components of substrings." ></td>
	<td class="line x" title="167:186	In this experiment, the turnaround time was 1 or 2 hours where components of collocations to be extracted was limited to the substrings with the frequency of 10 or more times." ></td>
	<td class="line x" title="168:186	6." ></td>
	<td class="line x" title="169:186	Conclusion The methods of automatically identifying and extracting uninterrupted and interrupted collocations from very large corpora has been proposed." ></td>
	<td class="line x" title="170:186	First, from the view point of collocational expression extraction, the problems of Nagao and Moffs algorithm for calculating arbitrary length of N-gram has been pointed out." ></td>
	<td class="line x" title="171:186	And, under the condition that fractional substrings are restrained to be extract, a new method of automatically extracting and tabulating all of the uninterrupted collocational substrings has been proposed." ></td>
	<td class="line x" title="172:186	Next, using these results, a method for automatically extracting interrupted collocational substrings has been proposed." ></td>
	<td class="line x" title="173:186	In this method, combinations of uninterrupted collocational substrings which collocate at different positions within a sentence are extracted and counted." ></td>
	<td class="line x" title="174:186	The method was applied to newspaper articles involving some 8.92 million characters." ></td>
	<td class="line x" title="175:186	The results for uninterrupted collocations were compared with that of Ngram statistics." ></td>
	<td class="line x" title="176:186	In the case of substring extraction with 2 or more characters, conventional method yielded substring of 4.4 millions types and the total frequency of them amount to 31.2 millions." ></td>
	<td class="line x" title="177:186	In contrast, the method proposed in this paper extracted 0.97 millions types of substrings and a total frequency of them has reduced to 2.6 millions." ></td>
	<td class="line x" title="178:186	In the case of interrupted collocational substring extraction, combining the substring with frequency of 10 times or more extracted by the first method, 6.5 thousand types of pairs of substrinks with the total frequency of 21.8 thousands were extracted." ></td>
	<td class="line x" title="179:186	From these results, it can be said that, viewed from the point of extraction of collocational expressions (as units of syntactic and semantic expressions), substrings obtained by conventional methods include a voluminous amount of fractional substrings." ></td>
	<td class="line x" title="180:186	In contrast, the method proposed in this paper reduces many of such fractional substrings and condensed into a group of substrings that can be regarded as units of expression." ></td>
	<td class="line x" title="181:186	As a result, it has been made possible to easily calculate interrupted collocations and together with phrase templates and other basic data regarding sentence structure." ></td>
	<td class="line x" title="182:186	This paper used Japanese character chains to examine the algorithm." ></td>
	<td class="line x" title="183:186	Yet this algorithm can be applied to arbitrary symbol chains." ></td>
	<td class="line x" title="184:186	Various types of applications are possible, such as word chains, syntactic element chains obtained from results of morphological analysis or semantic attribute chains which consist of each word being converted to semantic attributes." ></td>
	<td class="line x" title="185:186	As shown in this paper, applications for Japanese character chains still involve output of some amount of fractional stings." ></td>
	<td class="line x" title="186:186	But when applications to word chains or syntactic element strings are concerued, further restriction of unnecessary elements are anticipated." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2100
Good Bigrams
Johansson, Christer;"></td>
	<td class="line x" title="1:136	Good Bigrams Christer Johansson Dept. of Linguistics at Lund University Helgonabacken 12 223 62 Lund, Sweden email: Christer.Johansson @ ling.lu, se Abstract A desired property of a measure of connective strength in bigrams is that the measure should be insensitive to corpus size." ></td>
	<td class="line x" title="2:136	This paper investigates the stability of three different measures over text genres and expansion of the corpus." ></td>
	<td class="line x" title="3:136	The measures are (1) the commonly used mutual information, (2) the difference in mutual information, and (3) raw occurrence." ></td>
	<td class="line x" title="4:136	Mutual information is further compared to using knowledge about genres to remove overlap between genres." ></td>
	<td class="line x" title="5:136	This last approach considers the difference between two products of the same process (human text-generation) constrained by different genres." ></td>
	<td class="line x" title="6:136	The cancellation of overlap seems to provide the most specific word pairs for each genre." ></td>
	<td class="line x" title="7:136	1 Introduction Statistical methods have been used to find cohesion between local items of language (such as phonemes, morphemes, or words)." ></td>
	<td class="line x" title="8:136	Early work (Stolz, 1965; Zellig, 1955) was inspired by the advances in information science (Shannon, 1951; Shannon & Weaver, 1963)." ></td>
	<td class="line x" title="9:136	The research benefited from the possibility to store huge amounts of information in computer systems, and the optimism could be overwhelming when the problems were simplified and thought mostly restricted by the size of the corpus." ></td>
	<td class="line x" title="10:136	In this paper the stability of some bigram measures will be investigated." ></td>
	<td class="line x" title="11:136	Bigrams are items (i.e. word forms) that occur frequently together in a specific order." ></td>
	<td class="line x" title="12:136	The meanings of bigrams are not discussed since there is no meaning outside of a context." ></td>
	<td class="line x" title="13:136	Co-occurrence is still interesting because bigrams occur non-randomly, sometimes to such an extent that we discern some structure beyond co-occurrence." ></td>
	<td class="line x" title="14:136	The reason why it should be so is probably that part of the use of words is reflected by the company that words keep." ></td>
	<td class="line oc" title="15:136	Researchers (Church & Hanks, 1990; Kita & al. , 1994, inter al)." ></td>
	<td class="line n" title="16:136	have noted that mutual information tends to be insensitive to high fi'equency patterns, and unstable for low frequency patterns." ></td>
	<td class="line x" title="17:136	Johansson (1994) compared another measure, the difference in mutual information (Ag), of collocational strength with mutual information (g)." ></td>
	<td class="line x" title="18:136	That measure ranked high frequency bigrams higher than other bigrams if the order was consistent, whereas mutual information tended to pick out combinations of low frequency items." ></td>
	<td class="line x" title="19:136	Since low frequency items carry more specific information such bigrams give an illusion of semantic content." ></td>
	<td class="line x" title="20:136	It is usually this semantic illusion that we are interested in, but what says that 'of the' or 'in a' are worse bigrams than 'wooden spades' or 'various pretexts'." ></td>
	<td class="line x" title="21:136	Johansson proposed the test of finding some of the characters in the children's story 'Alice in Wonderland', and showed that a 'new' measure was to some degree 'better' than mutual information." ></td>
	<td class="line x" title="22:136	Unfortunately, some of that result was based on the fact that mutual information is very sensitive to low frequency items." ></td>
	<td class="line x" title="23:136	2 Definitions 2.1 Mutual information In the following p(x) will denote the observed probability as defined by p(x)=F(x)/N where F(x) is the frequency of occurrence of x, and N is the number of observed cases." ></td>
	<td class="line x" title="24:136	N is, in the calculations, equal to the corpus size in words." ></td>
	<td class="line oc" title="25:136	Given this, the mutual information ratio (Church & Hanks, 1990; Church & Mercer, 1993; Steier & Belew, 1991) is expressed by Formula 1." ></td>
	<td class="line o" title="26:136	(Church & Hanks refer to this measure as the association ratio tbr technical reasons)." ></td>
	<td class="line o" title="27:136	592 \]./ --lOg2(RWT)))(~2 ) = (N *Occ(\[wl,w2l)~ Formula 1: The mutual information ratio The instability of statistical measures seems to be a problem in statistical bigralns." ></td>
	<td class="line x" title="28:136	Especially low frequency counts cause instability." ></td>
	<td class="line oc" title="29:136	To avoid this use the rule of thumb that a bigram must occur more than four times (cf.Church & Hanks, 1990:p.24) to be considered as a candidate/br an interesting bigram." ></td>
	<td class="line x" title="31:136	2.2 The difference in mutual information: temporal co-occurrence A reasonable way of using the temporal orde~ ring in word pairs is to consider the opposite ordering of the word pair as negative evidence against the present order." ></td>
	<td class="line x" title="32:136	A reasonable measure would be to use the difference in mutual information between the two orderings, hereafter Ag." ></td>
	<td class="line x" title="33:136	The size of the corpus cancels out and Ag can be calculated by a ratio between frequencies." ></td>
	<td class="line x" title="34:136	This is intuitively correct for a comparison between apples and pears, i.e. you can say that apples (wl w2) occur twice as often as pears (w2 w l) in my fruit bowl (corpus)." ></td>
	<td class="line x" title="35:136	(p is the probability in the fixed corpus (fiN) which is different fi'om the probability in the language." ></td>
	<td class="line x" title="36:136	It is impossible to have a fixed corpus that equals the language since language does not have a fixed number of words or word patterns)." ></td>
	<td class="line x" title="37:136	2.2.1 Handling zero negative evidence In the case that the reversed ordering of a word pair has not been observed in the corpus, the measure becomes undefined." ></td>
	<td class="line x" title="38:136	To relieve this the frequency t is multiplied by a constant (10), and the frequency of the reversed ordering is set to 1." ></td>
	<td class="line x" title="39:136	Subtracting 9 from that value does not add anything to the measure for a single occurrence (log(10-9)=0)." ></td>
	<td class="line x" title="40:136	Other ways of handling zero-frequencies are evaluated in (Gale & Church, 1994), e.g. the Good-Turing method." ></td>
	<td class="line x" title="41:136	Relative frequencies of non-observed word pairs are hard to estimate." ></td>
	<td class="line x" title="42:136	For example, the frequencies of frequencies (X) and frequency (Y) used in the 1 1 will use 'frequency' as equivalent to 'occurrence' in the sample corpus." ></td>
	<td class="line x" title="43:136	Good-Turing method are linearly dependent in a log-log scale, i.e., there is an infinite frequency of non-observed items (which is another way of saying that we cannot expect the unexpected)." ></td>
	<td class="line x" title="44:136	A\].l =~ \] (Occ(\[wl,w2\])~ og 2 :it occ(\[w2,w \])>o log 2 (1 0* Oc'c(\[ Wl, W 2 \]) 9) 'if OCC(\[W2,Wl\]): 0 Formula 2: Handling zero frequencies 3 Illustration The difference between the two measures are perhaps best illustrated with some concrete examples." ></td>
	<td class="line x" title="45:136	In a previous paper (Johansson, 1.994) 'Alice's adventures in Wonderhmd' (AIW) was used as an experimental corpus to compare phrase finding for ~t, and a new measure -A~t. A critique against that corpus is that the corpus is very small." ></td>
	<td class="line x" title="46:136	'Through the Looking Glass' and 'The Hunting of the Snark' extend that corpus to about 63 000 words of which 26 831 occurred more than 4 times." ></td>
	<td class="line x" title="47:136	With the criterion that an interesting bigram occurs more than 4 times 1970 bigram candidates were found in this larger corpus." ></td>
	<td class="line x" title="48:136	Effect of Effect of della nm,.~, -,,,, 215 1883 34 7 48 202 33 204 29 174 160 29 47 136 28 1400 -9 -519 931 932!" ></td>
	<td class="line x" title="49:136	190 bigraln cheshire cat hui~ lookingzlass march hare mock tnrtle red king reA queen the dormouse white king white knight white queen white rabbit \[n the previous table the effect is measured by the number of steps a bigram is moved up compared to a sorted frequency list." ></td>
	<td class="line x" title="50:136	The effect of mutual information under these conditions is higher than the proposed measure for finding most characters in A1W, except for some names defined by definite article + noun, and common adjective + noun." ></td>
	<td class="line x" title="51:136	593 4 Material 6 Results In the rest of this paper, the corpus is the SUSANNE corpus (Sampson, 1994)." ></td>
	<td class="line x" title="52:136	This corpus consists of an extensively tagged and annotated subset from the Brown Corpus of American English." ></td>
	<td class="line x" title="53:136	The corpus is fairly small, but provides information on grammatical roles on the word and phrase level." ></td>
	<td class="line x" title="54:136	This makes the SUSANNE corpus suitable for further research." ></td>
	<td class="line x" title="55:136	The SUSANNE corpus is divided into 4 (approximately equally large) genre subcategones: 'A: press reportage G: belles lettres, biography, memoirs J: learned (mainly scientific and technical) writing N: adventure and Western fiction' (Sampson, 1994:p. 1.74) Each genre has approximately 20,000 unique word pairs 2." ></td>
	<td class="line x" title="56:136	The four genres will be used as one factor in the comparison between different measures." ></td>
	<td class="line x" title="57:136	The question is whether the genre interacts with the ability of the different measures to discover bigrams." ></td>
	<td class="line x" title="58:136	In category A 439 unique bigrams (occurring more than 4 times) were found, in G 486, in J 598, N 620, and 2573 for the used corpus 3." ></td>
	<td class="line x" title="59:136	5 Method The highest ranking bigralns according to the measure are sampled at 5 different levels: the 10, 50, 100, 200 and 400 top collocations." ></td>
	<td class="line x" title="60:136	Samples are sorted and compared for overlap by the UNIX command 'comm -12 SAMPLE1 SAMPLE2 I wc -1', and the percentage of overlap was calculated from the size of the sample." ></td>
	<td class="line x" title="61:136	Stability of bigrams was tested by three different overlaps." ></td>
	<td class="line x" title="62:136	1) The overlap between samples from genres, and samples for the entire corpus for the same measure." ></td>
	<td class="line x" title="63:136	2) The overlap between different measures at the five different levels for the different genres and the entire corpus." ></td>
	<td class="line x" title="64:136	3) The overlap between different genres." ></td>
	<td class="line x" title="65:136	2(A 21198 unique / 29969 total / 5332 unique words; G 22248 / 31006 / 6048; J 19039 / 29484 / 4676; N 20902 / 31959 / 4876; all 74126 / 12242\[ / 13458) 3The last small part of each genre was excluded fi'om the start for future purposes." ></td>
	<td class="line x" title="66:136	6.1 Mutual Information The average overlap between genres and the corpus showed that the J sample was much more stabile than the other genres 4." ></td>
	<td class="line x" title="67:136	The J genre would be the genre that information retrieval applications would be most interested in." ></td>
	<td class="line x" title="68:136	The ranking of the genres according to the stability of the overlap is: JANG." ></td>
	<td class="line x" title="69:136	The highest collocations are most stabile for J, where the other genres show less specificity (i.e. equal or growing percentages as the overlap grows)." ></td>
	<td class="line x" title="70:136	10 150 \[100 1200 \]400 \[mean 20 22 30 27.5 21.5 24.2 0 6 10 14.5 16.7 9.4 60 62 48 36.5 31 47.5 !10 6 7 15 22 12.0 A G J N 6.2 Delta Mutual Information Delta mutual information shows little effect of genre, and sample size." ></td>
	<td class="line x" title="71:136	Growing sample size predicts less overlap." ></td>
	<td class="line x" title="72:136	The ranking of genres is: GANJ." ></td>
	<td class="line x" title="73:136	Delta mutual information seems to rank the less specific genres high." ></td>
	<td class="line x" title="74:136	10 150 I lO0 1200 1400 Imear 70 64 53 47.5 44.2 55.7 60 58 54 58.5 51.5 56.4 60 54 48 43 39.2 48.8 50 52 49 51 45.5 49.5 A G J N A factorial ANOVA on measure and genre shows that there is a significant effect (p<0.001) of measure (Ag or g), genre and interaction between measures." ></td>
	<td class="line x" title="75:136	F(measure, 1df)=136.2, F(genre, 3df)=9.8, F(measure, genre, 1, 3)=15.4, p <0.001." ></td>
	<td class="line x" title="76:136	These two measures are significantly different." ></td>
	<td class="line x" title="77:136	6.3 Occurrence The results for the samples are similar to a m The overlap is generally higher for occurrence than Ag, but the ranking of genres is the same: GANJ." ></td>
	<td class="line x" title="78:136	An ANOVA on measure (Ag and occurrence) and genre show less significant effect on measure, and no significant effect of genre, or interaction (these measures behave in the same direction)." ></td>
	<td class="line x" title="79:136	4In preliminary investigations the J genre was the least stabile genre for mutual information." ></td>
	<td class="line x" title="80:136	This was 'corrected' by the demand that candidate bigrams should occur more than 4 times." ></td>
	<td class="line x" title="81:136	594 10 150 I i00 1200 1400 Imeanl 60 70 65 60.5 51 61.3 60 70 69 65.5 61 65.1 70 62 53 48.5 43.5 55.4 70 64 57 54.5 54.2 59.9 F(measure, ldf) = ll.l p<0.02, F(genre, 3df) = 2.7, p>0.05, F(measure, genre, 1, 3) = 0.218, p>0.8." ></td>
	<td class="line x" title="82:136	Occurrence is significantly more stabile than the other measure, but there is only a small difference of genres (occurrence and Ag react in a similar way to genre -i.e. on high occurrence)." ></td>
	<td class="line x" title="83:136	6.4 Comparison between measures The overlap between measures is calculated for all combinations of measures." ></td>
	<td class="line x" title="84:136	At the higher levels a high overlap can be expected since there is little possibility to fall out (e.g. in A 400 out of 439 is 91% of the sample)." ></td>
	<td class="line x" title="85:136	The results from this test indicate that the overlap between D (Ag) and F (occurrence) is significantly and consistently higher than between the other combinations (especially for the entire corpus)." ></td>
	<td class="line x" title="86:136	10 50 100 200 400 Genre Test mean overlap 0 6 22 44.5 93.2 A(439) 0 6 16 37.5 91.0 A 90 64 74 78.0 91.2 A 0 18 23 45.5 86.0 ~__ 0 14 20 43.0 82.0 G 80 76 78 77.5 84.0 G 0 8 13 34.0 72.2 J(598) 0 4 1 l 28.5 64.0 J 60 84 78 72.5 75.5 J 0 8 22 33.5 70.5 N(620) 0 6 20 28.0 63.7 N 40 68 71 67.0 72.5 N 0 0 1 7.0 15.7 a11(2573) 0 0 1 4.0 13.0 all 40 54 58 58.0 59.5 all M=D 33.1 M=F 30.1 D=F 79.4 M=D 34.5 IM=F 31.8 D=F 79." ></td>
	<td class="line x" title="87:136	I M=D 25.4 M:FI 21.5 D=F 74.0 M=D 26.8 M=F 23.5 D:F 63.7 M=D 4.7 M=F 3.6 D=-F 53.9 6.5 Overlap between genres To estimate the overlap of the genres the number of common bigrams between two genres were found and compared to the size of the smallest genre." ></td>
	<td class="line x" title="88:136	The results indicate an average overlap between the genres of 10%." ></td>
	<td class="line x" title="89:136	Overlap A G N of genres (% of smallest genre) A G J N --i 11.0 -9.4 11.0 10.0 12.0 7.5 6.6 Reduction of the bigrams The bigrams that are rated high by the measures (especially mutual information) are mixed between two different types of bigrams: (1) bigrams with high internal cohesion between low frequency items that may be associated with a specific interpretation (e.g. 'carbon tetrachloride' or 'cheshire cat'), (2) bigrams with high internal cohesion with usually high frequency of both items that may be associated with a 'syntactical' interpretation (e.g. 'in the')." ></td>
	<td class="line x" title="90:136	To separate type l from type 2 some information about the overlap of genres might be used." ></td>
	<td class="line x" title="91:136	The type 2 bigrams are typically found in most genres, whereas type 1 bigrams are specific to a text." ></td>
	<td class="line x" title="92:136	The results above indicate that we can use the genres with least overlap to filter out common bigrams (i.e. A use J, G use J, J use N, N use J)." ></td>
	<td class="line x" title="93:136	In the following table the effect of the genre (column 2) is shown by the number of 'surviving' bigrams from the candidate bigrams (column 1)." ></td>
	<td class="line x" title="94:136	The third column shows the effect of removing the bigrams that occur (more than 4 times) in both directions after common bigrams have been removed (first parenthesis shows actual removed, second shows those that would have been removed (i.e. those bigrams with both orderings in the candidate set)." ></td>
	<td class="line x" title="95:136	The fourth column shows the effect of removing bigrams that contains words that occur more than 4 times in the rest of the corpus (i.e. in A G N for J) after the bigrams have been formed." ></td>
	<td class="line x" title="96:136	The reason for filtering after forming bigrams is that words that are filtered out later work as place holders, and prevent some bigrams to form." ></td>
	<td class="line x" title="97:136	The reduction is most notable for removing bigrams that contain common words between genres: genre G and N contain few good candidates of collocations type 1." ></td>
	<td class="line x" title="98:136	Cand." ></td>
	<td class="line x" title="99:136	Genre Word order filter Freq." ></td>
	<td class="line x" title="100:136	words 439 216 179 (-63) (-80) 12 486 159 119 (-40) (-127) 1 .~8-\] 355 277 (-78)(-131) 37 620 /395 291 (-104)(-159) 0 A G J N 595 The following bigrams survived the harshest condition of removing bigrams containing words of other genres." ></td>
	<td class="line x" title="101:136	(Genre J, later ordered by mutual information)." ></td>
	<td class="line x" title="102:136	Some good candidates were (of course) removed, e.g. 'black body', 'per cent', 'united states'." ></td>
	<td class="line x" title="103:136	12.2 poynting robertson 9.1 pulmonary vein 11.8 indirect coombs 8.9 active agent 11.6 burning arcs 8.9 bronchial artery l 1.4 anionic binding 8.9 liquid phase 11.1 binding capacity 8.8 pulmonary artery 11.0 starting buffer 8.6 anode holder 10.7 antenna beam 8.3 solar radiation 10.6 wave lengths 8.2 reaction tubes 10.3 wave length 8.0 quadric surface 10.1 multiple secant 7.8 brightness temperature 10.0 carbon tetrachloride 7.8 mass flow 9.9 bronchial arteries 7.7 gas phase 9.9 heat transfer 7.7 surface cleaning 9.9 ideal gas 7.1 reaction cells 9.8 agglutinin activity 7.1 surface active 9.5 hydrogen atoms 6.7 artery puhnonary 9.4 multiple secants 5.0 anode surface 9.3 antibody activity 4.7 surface temperature 9.1 particle size In the A genre (News) the following 12 bigrams survived: 12.5 anne m:undel 12.0 rhode island 10.0 grand jury 9.9 rule charter 9.2 austin texas 8.9 sunday sales 8.9 sales tax 8.9 payroll tax 8.2 fulton county 8.0 lbotball league 7.5 kennedy administration 7.3 tax bill Genres G and N contain few candidates for collocations (among the 'best' ones in N were 'gray eyes', 'picked up', 'help me' and 'stared at' which are quite telling about the prototypical western story: 'The gray eyes stared at the villain who picked up his knife, while the girl cried 'help me''." ></td>
	<td class="line x" title="104:136	7 Other approaches The temporal dependencies of an ordered collocation \[wordl, word2\] has been seen as a problem since the theory of mutual information assumes the frequencies of word pairs to be symmetric (i.e. , f(\[wl, w2\]) and f(\[w2, w 1\]) to be equal)." ></td>
	<td class="line x" title="105:136	Delta mutual information relies on this difference in temporal ordering." ></td>
	<td class="line x" title="106:136	'\[\] f(x, y) encodes linear precedence." ></td>
	<td class="line o" title="107:136	\[\] Although we could fix this problem by redefining f(x, y) to be symmetric (by averaging the matrix with its transpose), we have decided not to do so, since order information appears to be very interesting'." ></td>
	<td class="line oc" title="108:136	(Church & Hanks, 1990:p.24) Merkel, Nilsson, & Ahrenberg (1994) have constructed a system that uses frequency of recurrent segments to determine long phrases." ></td>
	<td class="line x" title="109:136	In their approach they have to chunk the text into contiguous segments." ></td>
	<td class="line x" title="110:136	Significant frequency counts are achieved through the use of a very large corpus, and/or a corpus specialised for a specific task." ></td>
	<td class="line x" title="111:136	They report that it was possible for them to divide a large corpus into smaller sub-sections with little loss." ></td>
	<td class="line x" title="112:136	Smadja (1993)finds significant bigrams using an estimate of z-score (deviation from an expected mean)." ></td>
	<td class="line x" title="113:136	Smadja's method seems to require very large corpora, since the method needs to estimate a reliable measure of the variance of the frequencies with which words co-occur." ></td>
	<td class="line x" title="114:136	This makes the method dependent on the corpus size." ></td>
	<td class="line x" title="115:136	Smadja reports the use of a corpus of size 10 million words." ></td>
	<td class="line x" title="116:136	'More precisely, the statistical methods we use do not seem to be effective on low frequency words (fewer than 100 occurrences)'." ></td>
	<td class="line x" title="117:136	(Smadja, 1993:p.168) Kita & al." ></td>
	<td class="line x" title="118:136	(1994) proposed another measure of collocational strength that was based on the notion of a reduction in 'processing cost' if a frequent chunk of text can be processed as one chunk." ></td>
	<td class="line x" title="119:136	Cost reduction tended to extract conventional 'predicate phrase patterns', e.g., 'is that so' and 'thank you very much'." ></td>
	<td class="line x" title="120:136	Steier & Belew (1991) discuss the 'exporting' of phrases into a general vocabulary, where a word pair with high mutual information within a topic tends to have lower mutual information within the collection, and vice versa." ></td>
	<td class="line x" title="121:136	They relate a higher mutual information within a topic than in the collection to a lower value of discrimination." ></td>
	<td class="line x" title="122:136	Church & Gale (1995) have found it useful to compare the distribution of terms across documents." ></td>
	<td class="line x" title="123:136	They showed that a distribution different from what could be expected by a (random) Poisson process indicates interesting terms." ></td>
	<td class="line x" title="124:136	This approach is similar to the use of one genre to find interesting items in 596 another." ></td>
	<td class="line x" title="125:136	However, removal of the overlap needs some knowledge about the genres -apart from checking explicitly for a genre with least overlap." ></td>
	<td class="line x" title="126:136	Cancelling overlap has the advantage that it can cancel out similar underlying causes, while it exaggerates the underlying causes that differ between genres." ></td>
	<td class="line x" title="127:136	Some questions remain: at which level should overlap be formed?" ></td>
	<td class="line x" title="128:136	overlap in words or in bigrams; how many repetitions does it take for a word or bigram to 'belong' to a genre?" ></td>
	<td class="line x" title="129:136	8 Conclusion The question is 'what is gained by using a measure?'." ></td>
	<td class="line x" title="130:136	Mutual infornmtion tends to find combinations of words that are highly co-ordinated with each other, but these bigrams show both interesting bigrams (e.g. 'cheshire cat') and conventional (and uninteresting for keywords) bigrams (e.g. 'in a')." ></td>
	<td class="line x" title="131:136	The stability of interesting bigrams is improved by demanding candidate bigrams to occur more than a fixed number of times." ></td>
	<td class="line x" title="132:136	In this paper it has been shown that genre matters, and can be used to extract items that differ between genres." ></td>
	<td class="line x" title="133:136	Instead of balancing one big corpus, the analysis of one corpus might benefit from finding out how it is different from another corpus." ></td>
	<td class="line x" title="134:136	The bigrams that were formed by using different genres as filters showed interesting characteristics." ></td>
	<td class="line x" title="135:136	However, if we are to deal with larger amounts of data it might be unrealistic to compare differences directly between two large genres without the exclusion of terms that occur by chance." ></td>
	<td class="line x" title="136:136	The method that could be recommended from the results presented in this study is to triangulate a sample by the difference to other gcnres that we have some recta-knowledge about (i.e. we know that Western Fiction and Scientific Writing, at least on the surface, have little vocabulary in common)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2163
Sense Classification Of Verbal Polysemy Based-On Bilingual Class/Class Association
Utsuro, Takehito;"></td>
	<td class="line x" title="1:141	Sense Classification of Verbal Polysemy based-on Bilingual Class/Class Association* Takehito Utsuro Graduate School of Information Science, Nara Institute of Science and Technology 8916-5, Takayama-cho, Ikoma-shi, Nara, 630-01, JAPAN utsuro@is.aist-nara.ac.jp Abstract \[n the field of statistical analysis of natural language data, the measure of word/class association has proved to be quite useful for discovering a meaningtiff sense cluster in an arbitrary level of the thesaurus." ></td>
	<td class="line x" title="2:141	In this paper, we apply its idea to the sense classification of Japanese verbal polysemy in case frame acquisition from Japanese-English parallel corpora." ></td>
	<td class="line x" title="3:141	Measures of bilingual class~class association and bilingual class/frame association are introduced and used for discovering sense clusters in the sense distribution of English predicates and Japanese case element nouns." ></td>
	<td class="line x" title="4:141	In a small experiment, 93.3% of the discovered clusters are correct in that none of them contains examples of more than one hand-classified senses." ></td>
	<td class="line x" title="5:141	1 Introduction In corpus-based NLP, acquisition of lexical knowledge has become one of the major research topics." ></td>
	<td class="line x" title="6:141	Among several research topics in this field, acquisition from parallel corpora is quite attractive (e.g. Dagan et al.(1991))." ></td>
	<td class="line x" title="8:141	The reason is that parallel sentences are useful for resolving both syntactic and lexical ambiguities in the monolingual sentences." ></td>
	<td class="line x" title="9:141	Especially if the two languages have different syntactic structures and word meanings (such as English and Japanese), this approach has proved to be most effective in disambiguation (Matsumoto et al. , 1993; Utsuro et al. , 1993)." ></td>
	<td class="line x" title="10:141	Utsuro et al.(1993) proposed a method for acquiring surface case frames of Japanese verbs from Japanese-English parallel corpora." ></td>
	<td class="line x" title="12:141	In this method, translated English verbs and case labels are used to classify senses of Japanese polysemous verbs." ></td>
	<td class="line x" title="13:141	Clues to sense classification are found using English verbs and case labels, as well as the sense distribution of the Japanese case element *The author would like to thank Prof. Yuji MATSUMOTO for his valuable comments on this research." ></td>
	<td class="line x" title="14:141	This work is partly supported by the Grants from the Ministry of Education, Science, and Culture, Japan, ~07780326." ></td>
	<td class="line x" title="15:141	nouns." ></td>
	<td class="line x" title="16:141	Then, a human instructor judges whether the clues are correct." ></td>
	<td class="line x" title="17:141	One of the major disadvantages of the method is that the use of English information and sense distribution of Japanese case element nouns is restricted." ></td>
	<td class="line x" title="18:141	Only surface forms of English verbs and case labels are used and sense distribution of English verbs is not used." ></td>
	<td class="line x" title="19:141	Also, the threshold of deciding a distinction in the sense distribution of Japanese case element nouns is predetermined on a fixed level in a Japanese thesaurus." ></td>
	<td class="line x" title="20:141	As a result, the human instructor is frequently asked to judge the correctness of the clue." ></td>
	<td class="line oc" title="21:141	In the field of statistical analysis of natural language data, it is common to use measures of lexical association, such as the informationtheoretic measure of mutual information, to extract useful relationships between words (e.g. Church and Hanks (1990))." ></td>
	<td class="line n" title="22:141	Lexical association has its limits, however, since often either the data is insufficient to provide reliable word/word correspondences, or the task requires more abstraction than word/word correspondences permit." ></td>
	<td class="line x" title="23:141	Thus, Resnik (1992) proposed a useful mea~ sure of word/class association by generalizing information-theoretic measure of word/word association." ></td>
	<td class="line x" title="24:141	The proposed measure addresses the limitations of lexical association by facilitating sta~ tistical discovery of facts involving word classes rather than individual words." ></td>
	<td class="line x" title="25:141	We find the measure of word/class association of Resnik (1992) is quite attractive, since it is possible to discover a meaningful sense clubter in an arbitrary level of the thesaurus." ></td>
	<td class="line x" title="26:141	We thus expect that the restrictions of the previous method of Utsuro et al.(1993) can be Overcome by employing the idea of the measure of word/class association." ></td>
	<td class="line x" title="28:141	In this paper, we describe how this idea can be applied to the sense classification of Japanese verbal polysemy in case frame acquisition from Japanese-English parallel corpora." ></td>
	<td class="line x" title="29:141	First, sense distribution of English predicates and Japanese case element nouns is represented using monolingual English and Japanese thesaurus, respectively (sections 2 and 3)." ></td>
	<td class="line x" title="30:141	Then, the measure of the association of classes of English predicates and Japanese case element nouns, i.e., a measure of bilingual class~class association, is introduced, and extended into a measure of bilingual class/frame association (section 4)." ></td>
	<td class="line x" title="31:141	968 Using these measures, sense clusters are discovered in the." ></td>
	<td class="line x" title="32:141	sense distribution of English predicates and,lapanese ease element nouns." ></td>
	<td class="line x" title="33:141	Finally, examples of a Japanese polysemous verb collected from,/apanese-l'\]nglish parallel corpora are clivided into disjoint clusters according to those discovered sense clusters (section 5)." ></td>
	<td class="line x" title="34:141	The results of a small experiment are presented and the proposed measure is evaluated (section 6)." ></td>
	<td class="line x" title="35:141	2 Bilingual Surface Case Structure In the framework of verbal case frame acquisition fi'om parallel corpora, bilingually matched surface case structures (Matsumoto eta\[., 1993) are collected and surface case frames of Japanese verbs are acquired ti'om the collection, in this paper, each bilingually matched surface case structure is (:ailed a bilingual surface case structure, and represented as a feature structure: p'red : va semu : SEMi, pl : sere : SEMal : P'' : sere :,5'EMa,~ vj in(licat(~s the verb in the Japanese sentence, Pl,, P,, denote the Japanese ease markers, and n~l,,nj,, denote the Japanese ease element nouns." ></td>
	<td class="line x" title="37:141	When a.Japanese noun nji tins several senses, it may appear in several leaf classes in the,lapanese thesaurus." ></td>
	<td class="line x" title="38:141	Thus, St';Mai is represented as a. set of' those classes, and is referred to as a semantic label." ></td>
	<td class="line x" title="39:141	St';ML, is a semantic label of the corresponding English predicate, i.e., a set of classes in the English thesaurus:,gl';Mu = {cE1  (:l~k}, SI'\]Mj; = {cal  cal} cI,:t,, :gk and caa,, cjl indicate the classes in the English and Japanese thesaurus, rcspect.ively." ></td>
	<td class="line x" title="40:141	By structurally matching the Japanese-English parallel sentences in Example 1, the following bilingual surface case structure is obtained: Examph'." ></td>
	<td class="line x" title="41:141	:1 J: Watashi-ha uwagi-wo kagi-ni kaketa." ></td>
	<td class="line x" title="42:141	I7'0 P coat-A C;C hookon hung E: I hung my (;oat on the hook." ></td>
	<td class="line x" title="43:141	\])'i' (~ d,S ( : I~L t,; \]l,(Z : 'UIO : 'lti : Fred : watashi \] J pred : uwagi \] sere: {~,,} J pred : kagi \] .gCT/~ : {Ckl,.,., Ck4 } J We use \[,oget's Thesaurus (Roget, 1911) as the English thesaurus and 'Bunrui Goi Hyon'(BGH) (NLRI, 1993) as the Japanese thesaurus." ></td>
	<td class="line x" title="45:141	In Roget's Thesaurus, the verb 'han.q' has four senses." ></td>
	<td class="line x" title="46:141	In BGH, the nouns 'watash, i' and 'uwagi' have only one sense, respectively, and 'kagi' has four senses." ></td>
	<td class="line x" title="47:141	3 Monolingual Thesaurus A thesaurus is regarded as a tree in which each node represents a class." ></td>
	<td class="line x" title="48:141	We introduce ~ as the superordinate-subordinate relation of closes." ></td>
	<td class="line x" title="49:141	In general, c1 _~ e2 means that cl is subordinate to c2." ></td>
	<td class="line x" title="50:141	We define -/ so that a semantic label SEM= {cl,,cn} is subordinate to each class ci: Vc C SEM, SEM ~ c When searching for classes which give maximum association scm'e (section 5), this detinition makes it possible to calculate association score for all the senses in a semantic label and to find senses which give a maximum association score ~." ></td>
	<td class="line x" title="51:141	BGIt has a six-layered abstraction hierarchy and more than 60,000 .Japanese words are assigned at the leaves and its nominal part contains about 45,000 words 2." ></td>
	<td class="line x" title="52:141	Roget's l hesalrus has a sevenlayered abstraction hierarchy and over 100,000 words are allocated at the leaves a. In Roget's Thesaurus, sense classification is preferred to part of speech distinction." ></td>
	<td class="line x" title="53:141	Thus, a noun and a verb which have similar senses are assigned similar classes in the thesanrus." ></td>
	<td class="line x" title="54:141	4 Class-based Association Score 4.1 Word/Class Association Score The measure of word/class association of Resnik (1992) can be illustrated by the problem of finding tile prototypical object classes for verbs." ></td>
	<td class="line x" title="55:141	Let )2 and A/' be the sets of all verbs and norms, respectively." ></td>
	<td class="line x" title="56:141	(liven a verb v(E )2) and a norm class c(C N'), the joint probability of v and c is estimated as ~'2 count(v, r~) 'n~c Pr(v, e) E E v'CV nJc.Af The association score A(v,c) of a verb v and a noun class c is defined as Pr(v, c) Pr(c \[ v) l(v; e) A(v,c) -Pr(c I v) log Pr(v)Pr(c) The association score takes the mutual information between the verb and a noun class, and scales 1This process corresponds to sense disamblguation by maximizing the association score." ></td>
	<td class="line x" title="57:141	2Five classes are allocated at the next level from the root node: abstract-relations, agents-@humanactivities, human-activities, products, and naturalobjectsand-natural-phenomena." ></td>
	<td class="line x" title="58:141	SAt the next level from the root node, it has six classes: abstract-relations, space, matter, intellect, volition, and affections." ></td>
	<td class="line x" title="59:141	969 it according to the likelihood that a member of the class will actually appear as the object of the verb." ></td>
	<td class="line x" title="60:141	The first term of the conditional probability measures the generality of the association, while the second term of the mutual information measures the co-occurrence of the association." ></td>
	<td class="line x" title="61:141	4.2 Bilingual Class/Class Association Score We now apply the word/class association score to the task of measuring the association of classes of English predicates and Japanese case element nouns in the collection of bilingual surface ease structures." ></td>
	<td class="line x" title="62:141	First, we assume that for any polysemous Japanese verb v j, there exists a case marker p which is most effective for sense classification of vj." ></td>
	<td class="line x" title="63:141	Given the collection of bilingual surface case structures for v j, we introduce the bilingual class/class association score for measuring the association of a class cE of English predicates and a class cj of Japanese case element nouns for a case marker p. Let Eg(vg,p) be the set of bilingual surface case structures collected fronl the JapaneseEnglish parallel corpora, each element of which has a Japanese verb vj and a Japanese case marker p. Among the elements of Eg(vj,p), let Eg(vj,p,c~) be the set of those whose semantic label SEME of the English predicate satisfies the class c~, i.e., SEME ~ cE, and Eg(vj,p/cj) be the set of those whose semantic label SEMj of the Japanese case element noun for the case marker p satisfies the class c j, i.e., SEMj cj." ></td>
	<td class="line x" title="64:141	Let l';g(vj,cE,p/cj) be the intersection of Eg(vj, p, c~i) and Eg(vj, p/cj)." ></td>
	<td class="line x" title="65:141	Then, conditional probabilities Pr(cE Ira,p), Pr(cj I va,p), and Pr(cE,cj I vj,p) are defined as the ratios of the numbers of the elements of those sets: \[Eg(vJ,p, cE)\[ pr(c Pr(,:,, I vJ,p) IE (v,p)I Pr(cE,cJ I vj,p) = \]Eg(v,p) l Then, given vj and p, the association score A(c~,., cj Ivj,p) of cE and cj is defined as A(cE,cj \[ vj,p) = Pr(cs, c~ Ivj,p) Pr(e~,cj I v.l,p)log Pr(cE \[vj,p)Pr(cj I vj,p) This definition is slightly different from that of the word/class association score in that it only needs the set Eg(vj,p) for a Japanese verb vy and a Japanese case marker p, but not the whole 3apanese-English parallel corpora." ></td>
	<td class="line x" title="67:141	This is because our task is to discover strong association of an English (:lass and a Japanese class in Eg(vj,p), rather than in the whole Japanese-English parallel corpora." ></td>
	<td class="line x" title="68:141	Besides, as the first term for measnring the generality of the association, we use Pr(cE,cj \[ vj,p) instead of Pr(c,, I vj,p, cl~) or Pr(c~ \[vj,p/cj) below:* IEg/-, I Pr(cz \[ vj,p, cE) = Pr(cE \[ vj,p/cj) = 4.3 Bilingual Class/Frame Association Score In the previous section, we assume that for any polysemous Japanese verb v j, there exists a case marker p which is most effective for sense classification of verbal polysemy vj." ></td>
	<td class="line x" title="69:141	However, it can happen that a combination of more than one ease marker characterizes a sense of the verbal polysenly vj." ></td>
	<td class="line x" title="70:141	Even if there exists exactly one case marker which is most effective for sense classification, it is necessary to select the most effective case marker automatically by some measure." ></td>
	<td class="line x" title="71:141	For example, using some measure, it is desirable to automatically discover the fact that, for the task of sense classification of verbal polysenry, subject nouns are usually nlost effective for intransitive verbs, while object nouns are usually most effective for transitive verbs." ></td>
	<td class="line x" title="72:141	This section generalizes the previous definition of bilingual class/class association score, and introduces the bilingual class/frame association score." ></td>
	<td class="line x" title="73:141	In the new definition, we consider every possible set of pairs of a Japanese case marker p and a Japanese noun class c j, instead of predetermining the most effective case marker." ></td>
	<td class="line x" title="74:141	The bilingual class/frame association score measures the association of an English class c~ and a set of pairs of a Japanese case marker p and a Japanese noun class cs marked by p. By searching for a large association score, it becomes possible to find any combination of case markers which characterizes a sense of the verbal polysemy vs. 4.3.1 Japanese Case-Class Frame First, we introduce a data structure which represents a set of pairs of Japanese case marker p and a Japanese noun class cj marked by p, and call it; Japanese case-class frame." ></td>
	<td class="line x" title="75:141	A Japanese case-class frame can be represented as a feature structure: Pm : CJm 4pr(cd \[ vj,p, cE) and Pr(eE I vj,p/cd) are too large in lower parts of the thesaurus, since we focus oi1 examples which have a Japanese verb v.l and a Japanese case marker p. When we used the average of Pr(ej I vJ,p, cE) and Pr(e~ \[ vj,p/cj) instead of Pr(eE, cj \] vj,p) in the experiment d section 6, most discovered clusters consisted of only one example." ></td>
	<td class="line x" title="76:141	970 4.3.2 Subsumption Relation Next, we introduce subsuraption relation ~/'of ~ a bilingual surface case structure e and a Japanese ease-class frame fa: e ~f f3 iff." ></td>
	<td class="line x" title="77:141	for each case marker p in f's and its noun class c's, there exists the same case marker p in e and its semantic babel SEMj is subordinate to ca, i.e. SEM'S ~ ca This definition can be easily extended into a subsnmption relation of Japanese case-class frames." ></td>
	<td class="line x" title="78:141	4.3.3 Bilingual Class/Prame Association Score Let Eg(va) be the set of bilingual surface case strnctures collected from the Japanese-English parallel corpora, each element of which has a Japanese verb va. Among the elements e of Eg(va), let Eg(va,cE) be the set of those whose semantic label SEME of the English predicate satisfies the class cE, i.e., SEME ~ cE, and Eg(vj, fd) be the set of those which satisfy the Japanese case-class frame fa, i.e., e ~f fj." ></td>
	<td class="line x" title="79:141	Let Eg(vj,cf,;, fa) be the intersection of Eg(va,cE) and Eg(va, fa)." ></td>
	<td class="line x" title="80:141	Then, conditional probabilities 1','(c~: Iv j), Pr(fa ira), and I'r(cm fjiva) are defined as the ratios of the numbers of the elements of those sets: \[)I'(CE I V J) -\]~(Vj)l t',.(S's Iv,) iEv(~'s) I Pr(c~:, f's I~a) = Then, given va, the association score A(cE, fal V'S) of cg and fj is defined as A(cF,, f,, Iv,s) = PI'(CE, f'S I~'s) Pr(es,:, f,, Iva) log Pr(c~ I v's)Pr(f's I v's) As well as the case of the bilingual class/class association score, this definition only needs the set Eg('va) for a Japanese verb va, not the whole Japanese-English parallel corpora." ></td>
	<td class="line x" title="81:141	5 Sense Classification of Verbal Polysemy This section explains how to classify the elements of the set l')g(va) of bilingual surface case structures according to the sense of the verbal polyscaly va, with the bilingual class/frame association score defined in the previous section, hi this classification process, pairs of an English class cz,: and a Japanese case-class frame fj which give large association score A(cE, fa ira) are searched for." ></td>
	<td class="line x" title="82:141	It is desirable that the set Eg(vj) be divided into disjoint subsets by the discovered pairs of cu and fa." ></td>
	<td class="line x" title="83:141	The classification process proceeds according to the following steps: 1." ></td>
	<td class="line x" title="84:141	First, the index i and the set of examples Eg are initialized as i ~\] and Eg *Eg(va)." ></td>
	<td class="line x" title="85:141	2." ></td>
	<td class="line x" title="86:141	For the i-th iteration, let cE and fa be a pair of an English class and a Japanese case-class frame which satisfy the following constraint for all the pairs ofcEj and fjj (l<j<i1.): csu is not subordinate nor superordinate to cEj (i.e. , cF, ~ cEj and cEj ~ cE), or fa is not subordinate nor superordinate to faj (i.e. , fJ 74 1 faj and fjj Zf fa)." ></td>
	<td class="line x" title="87:141	Then, among those pairs of c~ and f j, search for a pair eel and fJi which gives maximum association score max A(cE, fa Iv j), a and collect the elecE,fj ments of Eg which satisfy the restrictions of CEi and fJi into the set Eg(va, eel, f.'i)." ></td>
	<td class="line x" title="88:141	3." ></td>
	<td class="line x" title="89:141	Subtract the set Eg(va, cu~, fJi) from Eg as Eg +-Eg Eg(va, eel, fal)." ></td>
	<td class="line x" title="90:141	If Eg 7~ O, then increment the index i as i +-i + 1 and go to step 2." ></td>
	<td class="line x" title="91:141	Otherwise, set the number k of the subsets as k +i and terminate the class/flea-." ></td>
	<td class="line x" title="92:141	t/on process." ></td>
	<td class="line x" title="93:141	As the result of this classification process, the set Eg(vj) is divided into disjoint subsets Eg(vj, cNl, fdl),  , Eg(v2, cEk, fak)." ></td>
	<td class="line x" title="94:141	6 For example, if a Japanese polysemous verb vg has both intransitive and transitive senses, pairs with the subject case like (era, \[subj : Cjl\]),,, <CEk,, \[s~l.bj : oak,\]) will be discovered for intransitive senses, while pairs with the object case like (cEk,~ l, \[obj: Cdk,,q\]),, (cEk, \[obj :csk\]) will be discovered for transitive senses." ></td>
	<td class="line x" title="95:141	Given tile set Eg(va), the iterations of the association score calculation is O(IEg('oa)t) ~." ></td>
	<td class="line x" title="96:141	Since the classification process can be regarded as sorting the calculated association score, its COnlputational complexity can be O(IEg(vj) I log IEg(~j)l ) if efficient sorting algorithms such as quick sort are employed." ></td>
	<td class="line x" title="97:141	6 Experiment and Evaluation This section gives the results of a small exper5The association score A(CS~,fjIvj) is calculated from the whole set Eg(v's), not Eg." ></td>
	<td class="line x" title="98:141	6Although the classification process itself guarantees the disjointness of Eg(va, eel, f'sl),  , Eg(vj, CEk, fJk), the subordinate-superordinate constraint of c,,~, and fj in the step 2 also guarantees the disjointness of the example sets which satisfy the restrictions of me p~irs (c~,DJ,, (c~, i's~)." ></td>
	<td class="line x" title="99:141	7Let l's, d's~ and ds~." ></td>
	<td class="line x" title="100:141	be the maximum number of Japanese cases in a bilingual surface ease structure, the depths of the Japanese and English thesauri, respectively." ></td>
	<td class="line x" title="101:141	Then, given a bilingual surface case structure e, the number of Japaxiese case-class frames f.s which is superordinate to e (i.e. , e ~f f's) is less than 2  x d~', an(t the mtmber of possible pairs of c~ and f's is less than 2  x dt/ x dF,, which is constant." ></td>
	<td class="line x" title="102:141	971 l IlandCluster Classif." ></td>
	<td class="line x" title="103:141	No. t 2 ---3 4 5 9 10 11 12 3 13 14 I 15 I1 Table 1: Sense Classification of kau English Predicate Class (CE)/ Japanese ~o(ACC) Case Noun Class (~) (Level in the Thesaurus and Example Word) buy(Leaf)/131(Leve13, hon(book)) buy(Leaf)/13220(Leve15, e(picture)) Purchase(Leaf-I, buy, pay)/14(Levd2, Products) treat oneself to(Leaf)/14650-6-80(Leaf, gaisha(foreign car)) treat Oneself to(Leaf)/14280-3-10(Leaf, yubiwa(ring)) purchase(Leaf)~11720-3-lO(Leaf, dis~-bring(Leaf) /14010-4-40(Leaf, m~-get(Leaf)/14570-1-10(Leaf, omocha(toy)) incur(Leaf)/laO(Level3, urami(enmity)) Motive(Leaf-I, rouse)/13020-5-50(Leaf, hankan(antipathy)) disgust( Leaf ) /13O l O-1-50( Leaf, hinshuku( displeasure) ) appreciate(Leaf)/13040-6-aO(Leaf, do,'yoku(effort)) get an opinion of(Leaf)/12040-1-5O(Leaf, otoko(person)) use(Leaf)/la421-6-50(Leaf, shuwan(ability)) win(Leaf)/laOlO-6-2OO(Leaf, kanshin(favor)) II Total I1 Number of Egs." ></td>
	<td class="line x" title="104:141	8 46 Association Score 0.048 0.018 0.149 0.070 0.070 0.083 0.062 0.070 0.185 3 0.169 1 0.083 1 0.083 1 0.083 75 0.08'3 0.083 nese rb ru Table 2: Examples of Intransitive/Transitive Distinction Ll English Predicate Class (ce)/Japanese Case-Class Frame (fa) (Level in the Thesaurus and Example Word) expensive(Leaf)/ ga(NOM):ne(price)(Leaf) Special Sensation(Leaf-a, freeze)/ ga(NOM) :15130-11-10(Leaf, koori(ice)) Acts(Leaf-2, persist,stick to)/ wo(hCC):13040(LevelS, goujou(obstinacy)) Decrease(Leaf-I, subside)/ga(NOM):151(Level3, kouzui(floods)) Results of Reasoning(Leaf-2, catch, have)~ wo(ACC):15860-11 (Level6, kaze(cold)) Number of Egs." ></td>
	<td class="line x" title="105:141	26 Association Score 3 0.299 3 0.237 7 0.459 0.109 0.421 zku Intellect(Levell, open)/ga(NOM):14(Products)(Level2, to(door)) hold(Leaf) / wo( A CC):13510l-CLeve16, kaigou(meeting)) ~au Completion(Leaf-I, ~Level4, negai ~ Quantity(Leaf-3, equal)/ni(DAT):12000-3-10(Leaf, kate(he)) 12 3 0.339 0.114 I 0.460 0.504 iment." ></td>
	<td class="line x" title="106:141	As a Japanese-English parallel corpus, we use a corpus of about 40,000 translation examples extracted h'om a machine readable JapaneseEnglish dictionary (Shimizu and Narita, 1979)." ></td>
	<td class="line x" title="107:141	6.1 Example of kau First, we show the result of classifying 75 examples (represented as bilingual surface ease structures) of the Japanese polysemous verb kau." ></td>
	<td class="line x" title="108:141	As the result of searching for pairs of an English class and a Japanese case-class frame with a large association score, the wo case (the accusative case) is preferred as the most effective case for sense classification." ></td>
	<td class="line x" title="109:141	15 pairs of an English class and a Japanese case-class frame are found and the set of the 75 examples are divided into 15 disjoint clusters (Table 1)." ></td>
	<td class="line x" title="110:141	Each cluster is represented as a pair of the class c~ of the English predicates and the class ca of the Japanese case element nouns of wo case, along with the level of the class in the thesaurus and the example word." ></td>
	<td class="line x" title="111:141	English classes are taken from Roget's Thesaurus and Japanese classes fi'om BGH s. In both thesauri, leaf classes SThe classes of BGH are represented as numercorrespond to one word." ></td>
	<td class="line x" title="112:141	For the evaluation of the results, we handclassified the 15 clusters into four groups, each of which corresponds to only one sense of kau 9." ></td>
	<td class="line x" title="113:141	Most hand-classified clusters for kau consist of more than one clusters found by maximizing the association score." ></td>
	<td class="line x" title="114:141	However, these clusters are cotrect in that none of them contains examples of more than one hand-classified senses of kau." ></td>
	<td class="line x" title="115:141	6.2 Examples of Intransitive/Transitive Distinction For four Japanese verbs haru, hiku, hiraku, and kanau, Table 2 shows examples of classifying intransitive/transitive senses by the proposed sense icM codes, in which each digit denotes the choice of the branch in the thesaurus." ></td>
	<td class="line x" title="116:141	The classes starting with '11', '12', '13', '14', and '15' are subordinate to abstract-relations, a qents-of.human-activities, human-activities, products and natural-objects-andnatural-phenomena, respectively." ></td>
	<td class="line x" title="117:141	9The criterion of this hand-classification is taken from the existing Japanese dictionaries for human use and the hand-compiled Japanese case frame dictionary IPAL (IPA, 1987)." ></td>
	<td class="line x" title="118:141	972 1 2 3 4 5 6 7 8 9,lapanese Verb agaru~--ageru(raise) aku(open, iv) l~aru( spread, ivT~v ) lgku(subside,pull) h irakll.(open, iv7%V f kakeru(hang), o',qorm to) ~au(buy) 'Fable 3: Eva,lnation of Sense Japanese CaseClass Frame fa 9a(NOM)~Tvo A~AC~ wo(ACC) /ni(DAT) wo(A~O3)---Classification 'Pot al-O n--e S~nn se-Clu s ter 41 r---g~.2~;y~-~-~ 30~ 15 54 1.=C~ 50 (92.6%) 1C74_-3--f~g-7-:Yl-~ Average classification method." ></td>
	<td class="line x" title="119:141	Clusters of intransitive senses are discovered with the Japanese case-class frames which contain the .qa case (the nominative ease), while those of transitive senses are discovered with the Japanese case-class frames which contain the w0 case (the accusative c~se) and ni ease (the dative case)." ></td>
	<td class="line x" title="120:141	6.3 Evaluation u,,,,a~-~rTt,%i-d.-7Classif." ></td>
	<td class="line x" title="121:141	Hand:Glassif." ></td>
	<td class="line x" title="122:141	~-2.41 t8 _ 3.00 8 1.50 -11---1.73 2~--1.74 10 :1.50 25 1.80 3 4.(;7 7 Conclusion This pal)er proposed a t)ilingual class-based method for sense classification of verbal i)olysemy, which is based on the maximization of the bilingual class/frame association score." ></td>
	<td class="line x" title="123:141	It achieved fairly high accuracy, although it is necessary to farther merge the clusters so that exactly one clus-ter corresponds to one hand-classified sense." ></td>
	<td class="line x" title="124:141	We are planning to make experiments on sense classification without bilingual information to evaluate the e.lt'ectiveness of such bilingual information." ></td>
	<td class="line x" title="125:141	li'or 9 verl)s, we made an ext)eriment on sense classification of verbal polysemy." ></td>
	<td class="line x" title="126:141	We compared the result with the hand-classification and checked whether each cluster contained examples of only one hand-classitied sense (Table 3)." ></td>
	<td class="line x" title="127:141	In the table, 'CI'." ></td>
	<td class="line x" title="128:141	and 'lEg'." ></td>
	<td class="line x" title="129:141	indicate the numbers of ellis= ters and examples, respectively." ></td>
	<td class="line x" title="130:141	The column 'One Sense (Jluster' means that each cluster contains examples of only one hand-classified sense, and the sub-eohmms 'CI'." ></td>
	<td class="line x" title="131:141	and 'Eg'." ></td>
	<td class="line x" title="132:141	list the number of SlLch (:lusters and the sum of examples contained in such clusters, respectively." ></td>
	<td class="line x" title="133:141	We ewduated the accuracy of the method am the rate of the number of examples contained in one sense clusters as in the 'Eg'." ></td>
	<td class="line x" title="134:141	sub-eohmm." ></td>
	<td class="line x" title="135:141	This achieved 100% accuracy for four verbs out of the 9 verbs, and 93.3% in average." ></td>
	<td class="line x" title="136:141	The coluinn 'Total C1./HandClassif'." ></td>
	<td class="line x" title="137:141	indicates the ratio of the total number of clusters to the number of hand-classified senses, correspoading to the average number of clnsters into which one hand-classified sense is divided." ></td>
	<td class="line x" title="138:141	Its a, verag% median, and standard deviation are 2.46, 1.80, and 1.06, respectively." ></td>
	<td class="line x" title="139:141	The result of the experiment indicated that the t)r<)posed sense classification method has achieved almost pure classification, while the result seems a little liner than hand-elassitieation." ></td>
	<td class="line x" title="140:141	This is mainly cause<l by the Net that clusters which correspond to the same hand-classified sense are separately located in the human-made thesaurus, and it is not easy to find exactly one representative class in the thesaurus (Utsuro, 11995)." ></td>
	<td class="line x" title="141:141	It is necessary to further merge the clusters so that exactly one cluster corresponds to one hand-classified sense." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C96-2208
The Automatic Extraction Of Open Compounds From Text Corpora
Sornlertlamvanich, Virach;Tanaka, Hozumi;"></td>
	<td class="line x" title="1:99	The Automatic Extraction of Open Compounds from Text Corpora Virach Sornlertlamvanich and Hozumi Tanaka Department of Computer Science, Tokyo Institute of Technology 2-12-1, ()okayama, Meguro-ku, ~t~okyo,.Japan 152 {virach, t anaka}0cs, titech, ac." ></td>
	<td class="line x" title="2:99	jp Abstract This paper describes a new method for extracting open compounds (uninterrupted sequences of words) from text corpora of languages, such as Thai, Japanese and Korea that exhibit unexplicit word segmentation." ></td>
	<td class="line x" title="3:99	Without applying word segmentation techniques to the inputted plain text, we generate ngram data from it." ></td>
	<td class="line x" title="4:99	We then count the occurrence of each string and sort them in alphabetical order." ></td>
	<td class="line x" title="5:99	It is significant that the frequency of occurrence of strings de, creases when the window size of observation is extended." ></td>
	<td class="line x" title="6:99	From the statistical point of view, a word is a string with a fixed pattern that is used repeatedly, meaning that it; shouht occur with a higher frequency than a string that is not a word." ></td>
	<td class="line x" title="7:99	We observe the variation of frequency of the sorted n-gram data and extract the strings that experience a significant (:hange in frequency of oc(:urrence when their length is extended." ></td>
	<td class="line x" title="8:99	We apply this occurrence test to both the right and left hand sides of all strings to ensure the accurate detection of both boundaries of the string." ></td>
	<td class="line x" title="9:99	The method returned satisfying results regardless of the size of the input file." ></td>
	<td class="line x" title="10:99	1 Introduction This paper discusses a method automatic extraction of candidates for open compound registration." ></td>
	<td class="line x" title="11:99	An open compound refers to an uninterrupted sequence of words, generally functioning as a single constituent (Smadja and McKcown, 1990)." ></td>
	<td class="line x" title="12:99	We propose a new method of extraction for languages which haw~ no specific use of punctuation to signify word boundaries." ></td>
	<td class="line x" title="13:99	Our method is applied to n-gram text data using statistical observation of the change of frequency of occurrence when the window size of string observation is extended (character) cluster-wise." ></td>
	<td class="line x" title="14:99	We generate both rightward and the leftward sorted n-gram data, then determine the left and right boundaries of a string using the methods of competitive,selection and unified selection." ></td>
	<td class="line x" title="15:99	In this paper, we examine the result of applying our medlod to Thai tex~ corpora and also introduce conventional Thai spelling rules to avoid e, xtracting invalid strings." ></td>
	<td class="line x" title="16:99	Previous work (Nagao et al. , 1994:) has shown all effective way of constructing a sorted file for tile efficient calculation of n-gram data." ></td>
	<td class="line x" title="17:99	However a surprisely large numbe, r of invMid strings were also extracted." ></td>
	<td class="line x" title="18:99	Subsequent work, (Ikehara et al. , 1995) has extended the sorted file to avoid repeating the counting of substrings contained in strings already counted." ></td>
	<td class="line x" title="19:99	This meant the extraction of only the longest strings in the order of frequency of occurrence." ></td>
	<td class="line x" title="20:99	The result of extraction was improved as a result, but the deterinination of longest strings is always made consecutively from left to right." ></td>
	<td class="line x" title="21:99	If an erroneous string is extracte, d, its error directly propagates through the, rest of input." ></td>
	<td class="line x" title="22:99	It is possible that a string with an invalid starting pattern will be extracted because a string too long in character length has been extracted previously." ></td>
	<td class="line x" title="23:99	In the following sections, we firstly describe the necessity for making this statistical ol)servad(m for extracting open comtlounds from Thai text corpora." ></td>
	<td class="line x" title="24:99	Then, the methodology of data preparw tion and open compound extraction is explained, Finally, we discuss the result of an experiment on both large and small test corpora to investigate the effectiveness of our method." ></td>
	<td class="line x" title="25:99	2 Problem Description It is a non-trivial task to identify a word ill the text of a language which has no specific punctuation to mark word boundaries." ></td>
	<td class="line x" title="26:99	Up to the present, lexicographers' efforts have been inhibited by insufficient corpora and limited computational facilities." ></td>
	<td class="line x" title="27:99	Almost all lexicon knowledge bases have been created with reliance oll human intuition." ></td>
	<td class="line x" title="28:99	\]\]1 recent years, a large amount of text corpora haw', become available, and it is now becoming possible to conduct more rigorous experiments on text corpora." ></td>
	<td class="line x" title="29:99	We address the following problems in such 1143 a way that they are able to be solved by the way of statistical' methods." ></td>
	<td class="line x" title="30:99	1." ></td>
	<td class="line x" title="31:99	There is no good evidence to support the itemization of a word in a dictionary." ></td>
	<td class="line x" title="32:99	In traditional dictionary making, lexicographers have had to rely on citations collected by human readers from limited text corpora." ></td>
	<td class="line oc" title="33:99	More rare words rather than common words are found even in standard dictionaries (Church and Hanks, 1990)." ></td>
	<td class="line x" title="34:99	This is the problem in making a lexical entry list in dictionary construction." ></td>
	<td class="line x" title="35:99	2." ></td>
	<td class="line x" title="36:99	It is hard to decide where to segment a string into its component words." ></td>
	<td class="line x" title="37:99	It is also hard to enumerate words from a text, though it is reported that the accuracy of recent word segmentation methods using a dictionary and heuristic methods is higher than 95% in case of Thai (Virach, 1993)." ></td>
	<td class="line x" title="38:99	The accuracy depends mostly on word entries in the dictionary, and the priority for selecting between candidate words when there is more than one solution for word segmentation." ></td>
	<td class="line x" title="39:99	This is the problem in assigning priority information for selection." ></td>
	<td class="line x" title="40:99	3 Word Extraction from Text Corpora We used a window size of 4 to 32 for n-gram data accumulation." ></td>
	<td class="line x" title="41:99	The value is arbitrary but this range has proven sufficient to avoid collecting illegible strings." ></td>
	<td class="line x" title="42:99	3.1 Algorithm Define that, \]a I is the number of clusters ~ in the string 'a', n(a) is the number of occurrences of the string ~&', &ud n(a+l) is the number of occurrences of the string 'a' with one additional cluster added." ></td>
	<td class="line x" title="43:99	As the length of a string increases the number of occurrences of that string will decrease." ></td>
	<td class="line x" title="44:99	Therefore, + 1) < (1) For the string 'a', n(a+l) decreases significantly from n(a) when 'a' is a frequently used string in contrast to 'a+l'." ></td>
	<td class="line x" title="45:99	From this, it can be seen that 'a' is a rigid expression of an open compound when it satisfies the condition 'n(a + 1) << n(a)." ></td>
	<td class="line x" title="46:99	(2) In such a case, 'a' is considered a rigid expression that is used frequently in the text, and 'a+l' is just a string that occurs in limited contexts." ></td>
	<td class="line x" title="47:99	1The smallest stand-alone character unit as by the spelling rules." ></td>
	<td class="line x" title="48:99	Since we count the occurrence of strings generated from an arbitrary position in tile text, with only the above observation, only the right end position of a string can be assumed to determined a rigid expression." ></td>
	<td class="line x" title="49:99	To identify the correct starting position of a string, we apply the same observation to the leftward extension of a string." ></td>
	<td class="line x" title="50:99	Therefore, we have to include the direction to the string observation." ></td>
	<td class="line x" title="51:99	Further define that, +a is the right observation of the string 'a', and -a is the left observation of the string 'a'." ></td>
	<td class="line x" title="52:99	Then, n(-t-a+l) is the number of occurrences of the string 'a' with one cluster added to its right, and n(-a+l) is the number of occurrences of the string 'a' with one cluster added to its left." ></td>
	<td class="line x" title="53:99	Following the same reasoning as above, we will obtain, n(+a + 1) < n(a), and (3) + 1) < (4) A string 'a' is a rigid expression if it satisfies the following conditions, n(+a + 1) << n(a), and (5) n(-a + 1) << n(a)." ></td>
	<td class="line x" title="54:99	(6) 3.2 Data preparation Following are the steps for creating n-gram text data according to the fundamental features of Thai text corpora." ></td>
	<td class="line x" title="55:99	The results are shown in Table 1 and Table 2." ></td>
	<td class="line x" title="56:99	In each table, 'n' is the number of occurrences and 'd' is the difference in occurrence with the next string." ></td>
	<td class="line x" title="57:99	1." ></td>
	<td class="line x" title="58:99	Tokenize the text at locations of spaces, tabs and newline characters." ></td>
	<td class="line x" title="59:99	2." ></td>
	<td class="line x" title="60:99	Produce n-gram strings following Thai spelling rules." ></td>
	<td class="line x" title="61:99	Only strings that have possible boundaries are generated, and their occurrence counted." ></td>
	<td class="line x" title="62:99	For example, shifting a string from 'a+6' to 'a+7' in the Table 1, the string at 'a+7' is '~z~t~ff.~' and not 'fl~g~|~'l.lfll~l\], despite the first character after 'a+6' being '~'." ></td>
	<td class="line x" title="63:99	According to o/ the Thai spelling rules, the character ' ' can never stand by itself." ></td>
	<td class="line x" title="64:99	It needs both of an initial consonant and a final consonant." ></td>
	<td class="line x" title="65:99	We call this three character unit a cluster." ></td>
	<td class="line x" title="66:99	3." ></td>
	<td class="line x" title="67:99	Create both rightward (Table 1) and leftward (Table 2) sorted strings." ></td>
	<td class="line x" title="68:99	The frequency of each string is the same but the strings are lexically reversed and ordered based on this reversed state." ></td>
	<td class="line x" title="69:99	1144 4." ></td>
	<td class="line x" title="70:99	Calculate the diiference between the occurrenee of adjoining strings ill the sorted lists." ></td>
	<td class="line x" title="71:99	Let {t(a) be the difference wdue of the string 'a', then d(~) = ~(~) n(~ + n." ></td>
	<td class="line x" title="72:99	(7) The difference w~lue (d) is generated separately for the rightward and legward sorted string ta.bles." ></td>
	<td class="line x" title="73:99	The occurrences (n) ill both Tal}le 1 anl Table 2 apparently SUl}port the conditions (3) all\[ (4)." ></td>
	<td class="line x" title="74:99	Strin----~gRightward sorted string \[--n-~d-a u~." ></td>
	<td class="line x" title="75:99	\[ 5Ta68 a+l tl~UTl~ \] 445 (} I a+2 i~a~l~\] I 445 0 a+3 i~z~aa I 445 42 a+4 II i g TI ~i l,~il I 1303 0 a+5 ~ig~liflll } 303 22 a-k6 ill g~ll l,~tll~a I 281 0 a+7 ~l~a~l~aa~Igaffa I 281 274 a+8,~uu,aa,~affa6~ I 7 0 Sorted String Table 1: Exami)le of a Rightward 'Pable -b -b+l I -b+2 I -b+3 I -1}+4 I -b+5 I -b+6 I -b+7 I Leftward sorted string -7 \].r2 --ol 172 0 172 421 130 121 I \].21." ></td>
	<td class="line x" title="76:99	7 114 107t '7 01 Table 2: Example of a Leftward Sorted String Table 3.3 Extraction 3.3.1 Competitive selection According to condition (5) the string %' ( a~ un ) in Table 3 is considered an open compound because the difference of betweml n(a) and n(a+l) is as high as 450." ></td>
	<td class="line x" title="77:99	However, 'a~u~l' is an illegible string and cannot be used on as indivilual basis in general text." ></td>
	<td class="line x" title="78:99	Observing tile same string :a' in Table 1, the difference between n(a) and n(a+l) is only 68." ></td>
	<td class="line x" title="79:99	It is not comparably high enough to be selected." ></td>
	<td class="line x" title="80:99	Therefore, we have to determine the minimum wflue of the difference when there is more than one branch extended from a string." ></td>
	<td class="line x" title="81:99	String_ Rightward sorted string 1 _ {t a a~gw 51~ 45\] / Table 3: A Further Example of the Count of a \]lightwm'd Sorted String Tal)le 3.3.2 Unified selection in Figure 1, we obtain the string '~o~ ~ ~1~ a,lnl~a~' 1)y observing the significant change in d just before the next string '~l~u~l~:l.l~l~i~afi' The string could be wrongly selected if we do not observe its behaviour ill the leftward sorted string table, to determine tit(', correct left boundary." ></td>
	<td class="line x" title="82:99	Thus, we (}bserve tile count of string '~itlSg~l~\],ltll~li~,~' when it is extended leftward, as shown ill Figure 2." ></td>
	<td class="line x" title="83:99	0 20 40 60 80 100 120 140 160 180 Figure 1: Rightward Sorted Strings Starting from an Arbitrary String 0 20 40 60 ~ | 4\] 1114~ II 4 88 1013 120 140 160 180 Figm'e 2: Leftward Sorted Strings Starting fl'om an Arbitrary String By unifying the results of both methods of the observation, we iinally obtain tile word 1145 4 Experimental Results We have applied our method to an actual Thai text corpora without word segmentation preprocessing." ></td>
	<td class="line x" title="84:99	4.1 Natural language data We selected 'Thai Revenue Code (1995)', as large as 705,513 bytes, and 'Convention for Avoidance of Double Taxation between Thailand and Japan', which has a smaller size of 40,401 bytes." ></td>
	<td class="line x" title="85:99	The purpose is to show that our method is effective regardless of the size of the data file." ></td>
	<td class="line x" title="86:99	4.2 Results of extraction g o~ g 0~ o 13_ I~ Word E~ Fixed  String| Expression Illegible 100% 8O% 60% 48% 20% 0% 100 90 80 70 60 50 40 30 20 10 Theshold level of the value of Difference (O) Figure 3: Result of Extraction of 'Thai Revenue Code (705,513 bytes)' 100%,~ (\]0% L 60% 4o~20% ~L 0% 3O I l a Word \[\] Find Expressian lllegble Strinq 20 10 8 B 4 Threshold level of the value of Difference (D) Figure 4: Result of Extraction of 'Convention for Thailand-Japan (40,401 bytes)' The results of extraction examined in both large and small file sizes are very satisfactory." ></td>
	<td class="line x" title="87:99	Very few illegible strings are extracted though the threshold of the difference value is set to be as low as 10 in Figure 3, and 4 in Figure 4." ></td>
	<td class="line x" title="88:99	The suitable value of the threshold of difference varies with the size of text corpus file." ></td>
	<td class="line x" title="89:99	To obtain more meaningful strings fl'om a large file, we have to set a relatively high threshold of extraction." ></td>
	<td class="line x" title="90:99	One of the advantages of our method is that there is an inherent trade~off between the quantity and the quality of the extracted strings." ></td>
	<td class="line x" title="91:99	In the case of Figure 3, to limit the amount of illegible strings to not exceed 15% of the total extracted strings, we set the threshold to 30." ></td>
	<td class="line x" title="92:99	As a result, we obtained 154 words, 114 fixed expressions and only 46 illegible strings." ></td>
	<td class="line x" title="93:99	Furthermore, we found that of the 154 words appearing in the text, there were 84 words that were not found in a standard Thai dictionary." ></td>
	<td class="line x" title="94:99	5 Conclusion This paper has shown an algorithm for data preparation and open compound extraction." ></td>
	<td class="line x" title="95:99	The cornpetitive selection and unified selection of rightward and leftward sorted strings play an important role in improving accuracy of the extraction." ></td>
	<td class="line x" title="96:99	In the experiment, we applied Thai spelling rules to restrict the search path for string counts." ></td>
	<td class="line x" title="97:99	Some types of spelling irregularities can be excluded by this process." ></td>
	<td class="line x" title="98:99	By adjusting the value of threshold, we can extract suitable entries for open compound registration regardless of the size of the input file." ></td>
	<td class="line x" title="99:99	Furthermore, our method has ensured the extraction of new words from the text file of the language that has no explicit word boundary, such as Thai." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0103
Hierarchical Clustering Of Words And Application To NLP Tasks
Ushioda, Akira;"></td>
	<td class="line x" title="1:316	Hierarchical Clustering of Words and Application to NLP Tasks Akira Ushioda* Fujitsu Laboratories Ltd. Kawasaki, Japan email: ushioda@flab, fuj su." ></td>
	<td class="line x" title="2:316	co. jp Abstract This paper describes a data-driven method for hierarchical clustering of words and clustering of multiword compounds." ></td>
	<td class="line x" title="3:316	A large vocabulary of English words (70,000 words) is clustered bottom-up, with respect to corpora ranging in size from 5 million to 50 million words, using mutual information as an objective function." ></td>
	<td class="line x" title="4:316	The resulting hierarchical clusters of words are then naturally transformed to a bit-string representation of (i.e. word bits for) all the words in the vocabulary." ></td>
	<td class="line x" title="5:316	Evaluation of the word bits is carried out through the measurement of the error rate of the ATR Decision-Tree Part-Of-Speech Tagger." ></td>
	<td class="line x" title="6:316	The same clustering technique is then applied to the classification of multiword compounds." ></td>
	<td class="line x" title="7:316	In order to avoid the explosion of the number of compounds to be handled, compounds in a small subclass are bundled and treated as a single compound." ></td>
	<td class="line x" title="8:316	Another merit of this approach is that we can avoid the data sparseness problem which is ubiquitous in corpus statistics." ></td>
	<td class="line x" title="9:316	The quality of one of the obtained compound classes is examined and compared to a conventional approach." ></td>
	<td class="line x" title="10:316	1 Introduction One of the fundamental issues concerning corpus-based NLP is that we can never expect to know from the training data all the necessary quantitative information for the words that might occur in the test data if the vocabulary is large enough to cope with a real world domain." ></td>
	<td class="line x" title="11:316	In view of the effectiveness of class-based n-gram language models against the data sparseness problem (Kneser and Ney 1993, Ueberla 1995), it is expected that classes of words are also useful for NLP tasks in such a way that statistics on classes are used whenever statistics on individual words are unavailable or unreliable." ></td>
	<td class="line x" title="12:316	An ideal type of clusters for NLP is the one which guarantees mutual substitutability, in terms of both syntactic and semantic soundness, among words in the same class (Harris 1951, Brill and Marcus 1992)." ></td>
	<td class="line x" title="13:316	Take, for example, the following sentences." ></td>
	<td class="line x" title="14:316	(a) He went to the house by car." ></td>
	<td class="line x" title="15:316	(b) He went to the apartment by bus." ></td>
	<td class="line x" title="16:316	(c) He went to the ? by ?." ></td>
	<td class="line x" title="17:316	(d) He went to the house by the sea." ></td>
	<td class="line x" title="18:316	Suppose that we want to parse sentences using a statistical parser and that sentences (a) and (b) appeared in the training and test data, respectively." ></td>
	<td class="line x" title="19:316	Since (a) is in the training data, we know that the prepositional phrase by car is attached to the main verb went, not to the noun phrase the house." ></td>
	<td class="line x" title="20:316	Sentence (b) is quite similar to (a) in meaning, and identical to (a) in sentence structure." ></td>
	<td class="line x" title="21:316	Now if the words apartment and bus are unknown to the parsing system *A part of this work is done when the author was at ATR Interpreting Telecommunications Research Laboratories, Kyoto, Japan." ></td>
	<td class="line x" title="22:316	28 (i.e. never occurred in the training data), then sentence (b) must look to the system very much like (c), and it will be very hard for the parsing system to tell the difference in sentence structure between (c) and (d)." ></td>
	<td class="line x" title="23:316	However, if the system has access to a predefined set of classes of words, and if car and bus are in the same class, and house and apartme.nt are in another class, it will not be hard for the system to detect the similarity between (a) and (b) and assign the correct sentence structure to (b) without confusing it with (d)." ></td>
	<td class="line x" title="24:316	The same argument holds for an example-based machine translation system." ></td>
	<td class="line x" title="25:316	In that case, an appropriate translation of (b) is expected to be derived with an example translation of (a) if the system has an access to the classes of words." ></td>
	<td class="line x" title="26:316	Therefore, it is desirable that we build clustering of the vocabulary in terms of mutual substitutability." ></td>
	<td class="line x" title="27:316	Furthermore, clustering is much more useful if the clusters are of variable granularity." ></td>
	<td class="line x" title="28:316	Suppose, for example, that we have two sets of clusters, one is finer than the other, and that word-1 and word-2 are in different finer classes." ></td>
	<td class="line x" title="29:316	With finer clusters alone, the amount of information on the association of the two words that the system can obtain from the clusters is minimal." ></td>
	<td class="line x" title="30:316	However, if the system has a capability of falling back and checking if they belong to the same coarser class, and if that is the case, then the system can take advantage of the class information for the two words." ></td>
	<td class="line x" title="31:316	When we extend this notion of two-level word clustering to many levels, we will have a tree representation of all the words in the vocabulary in which the root node represents the whole vocabulary and a leaf node represents a word in the vocabulary." ></td>
	<td class="line x" title="32:316	Also, any set of nodes in the tree constitutes a partition (or clustering) of the vocabulary if there exists one and only one node in the set along the path from the root node to each leaf node." ></td>
	<td class="line x" title="33:316	In the following sections, we will first describe a method of creating a binary tree representation of the vocabulary and present results of evaluating and comparing the quality of the clusters obtained from texts of very different sizes." ></td>
	<td class="line x" title="34:316	Then we will extend the paradigm of clustering from word-based clustering to compound-based clustering." ></td>
	<td class="line x" title="35:316	In the above examples we looked only at the mutual substitutability of words; however, a lot of information can also be gained if we look at the substitutability of word compounds for either other word compounds or single words." ></td>
	<td class="line x" title="36:316	We will introduce the notion of compound-classes, propose a method for constructing them, and present results of our approach." ></td>
	<td class="line x" title="37:316	2 Hierarchical Clustering of Words Several algorithms have been proposed for automatically clustering words based on a large corpus (Jardino and Adda 91, Brown et al. 1992, Kneser and Ney 1993, Martin et al. 1995, Ueberla 1995)." ></td>
	<td class="line x" title="38:316	They are classified into two types." ></td>
	<td class="line x" title="39:316	One type is based on shuffling words from class to class starting from some initial set of classes." ></td>
	<td class="line x" title="40:316	The other type repeats merging classes starting from a set of singleton classes (which contain only one word)." ></td>
	<td class="line x" title="41:316	Both types are driven by some objective function, in most cases by perplexity or average mutual information." ></td>
	<td class="line x" title="42:316	The merit of the second type for the purpose of constructing hierarchical clustering is that we can easily convert the history of the merging process to a tree-structured representation of the vocabulary." ></td>
	<td class="line x" title="43:316	On the other hand, the second type is prone to being trapped by a local minimum." ></td>
	<td class="line x" title="44:316	The first type is more robust to the local minimum problem, but the quality of classes greatly depends on the initial set of classes, and finding an initial set of good quality is itself a very difficult problem." ></td>
	<td class="line x" title="45:316	Moreover, the first approach only provides a means of partitioning the vocabulary and it doesn't provide a way of constructing a hierarchical clustering of words." ></td>
	<td class="line x" title="46:316	In this paper we adopt the merging approach and propose an improved method of constructing hierarchical clustering." ></td>
	<td class="line x" title="47:316	An attempt is also made to combine the two types of clustering and some results will be shown." ></td>
	<td class="line x" title="48:316	The combination is realized by the construction of clusters using the merging method followed by the reshuffling of words from class to class." ></td>
	<td class="line x" title="49:316	Our word bits construction algorithm (Ushioda 1996) is a modification and an extension 29 of the mutual information (MI) clustering algorithm proposed by Brown et al.(1992)." ></td>
	<td class="line x" title="51:316	The reader is referred to (Ushioda 1996) and (Brown et al. 1992) for details of MI clustering, but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm." ></td>
	<td class="line x" title="52:316	2.1 Mutual Information Clustering Algorithm Suppose we have a text of T words, a vocabulary of V words, and a partition 7r of the vocabulary which is a function from the vocabulary V to the set C of classes of words in the vocabulary." ></td>
	<td class="line x" title="53:316	Brown et al. showed that the likelihood L(Tr) of a bigram class model generating the text is given by the following formula." ></td>
	<td class="line x" title="54:316	L(r) --H -4I (1) Here H is the entropy of the 1-gram word distribution, and I is the average mutual information (AMI) of adjacent classes in the text and is given by equation 2." ></td>
	<td class="line x" title="55:316	F (elc2) I= ~ Pr(clc2)log Pr(cl)Pr(c2) (2) Cl ~C2 Since H is independent of r, the partition that maximizes the AMI also maximizes the likelihood L(r) of the text." ></td>
	<td class="line x" title="56:316	Therefore, we can use the AMI as an objective function for the construction of classes of words." ></td>
	<td class="line x" title="57:316	The mutual information clustering method employs a bottum-up merging procedure." ></td>
	<td class="line x" title="58:316	In the initial stage, each word is assigned to its own distinct class." ></td>
	<td class="line x" title="59:316	We then merge two classes if the merging of them induces minimum AMI reduction among all pairs of classes, and we repeat the merging step until the number of the classes is reduced to the predefined number C. Time complexity of this basic algorithm is O(V s) when implemented straightforwardly, but it can be reduced to O(V 3) by storing the result of all the trial merges at the previous merging step." ></td>
	<td class="line x" title="60:316	Even with the O(V 3) algorithm, however, the calculation is not practical for a large vocabulary of order 104 or higher." ></td>
	<td class="line x" title="61:316	Brown et al. proposed the following method, which we also adopted." ></td>
	<td class="line x" title="62:316	We first make V singleton classes out of the V words in the vocabulary and arrange the classes in the descending order of frequency, then define the merging region as the first C + 1 positions in the sequence of classes." ></td>
	<td class="line x" title="63:316	So initially the C + 1 most frequent words are in the merging region." ></td>
	<td class="line x" title="64:316	Then do the following." ></td>
	<td class="line x" title="65:316	I. Merge the pair of classes in the merging region merging of which induces minimum AMI reduction among all the pairs in the merging region." ></td>
	<td class="line x" title="66:316	2." ></td>
	<td class="line x" title="67:316	Put the class in the (C + 2) nd position into the merging region and shift each class after the (C + 2) nd position to its left." ></td>
	<td class="line x" title="68:316	3." ></td>
	<td class="line x" title="69:316	Repeat I. and 2." ></td>
	<td class="line x" title="70:316	until C classes remain." ></td>
	<td class="line x" title="71:316	With this algorithm, the time complexity becomes O(C2V) which is practical for a workstation with V in the order of 100,00O and C up to 1,000." ></td>
	<td class="line x" title="72:316	2.2 Word Bits Construction Algorithm The simplest way to construct a tree-structured representation of words is to construct a dendrogram as a byproduct of the merging process, that is, to keep track of the order of merging and make a binary tree out of the record." ></td>
	<td class="line x" title="73:316	A simple example with a five word vocabulary is shown in Figure 1." ></td>
	<td class="line x" title="74:316	If we apply this method to the above O(C2V) algorithm straightforwardly, however, we obtain for each class an extremely unbalanced, almost left branching subtree." ></td>
	<td class="line x" title="75:316	The 30 Merging History: Merge(A, B -> A) Merge(C, D -> C) Merge(C, E -> C) Merge(A, C -> A) Merge(X,Y->Z) reads 'merge X and Y and name the new class as Z' Dendrogram f F Figure 1: Dendrogram Construction reason is that after classes in the merging region are grown to a certain size, it is much less expensive, in terms of AMI, to merge a singleton class with lower frequency into a higher frequency class than merging two higher frequency classes with substantial sizes." ></td>
	<td class="line x" title="76:316	A new approach we adopted incorporates the following steps." ></td>
	<td class="line x" title="77:316	1." ></td>
	<td class="line x" title="78:316	MI-clustering: Make C classes using the mutual information clustering algorithm with the merging region constraint mentioned in (2.1)." ></td>
	<td class="line x" title="79:316	2." ></td>
	<td class="line x" title="80:316	Outer-clustering: Replace all words in the text with their class token 1 and execute binary merging without the merging region constraint until all the classes are merged into a single class." ></td>
	<td class="line x" title="81:316	Make a dendrogram out of this process." ></td>
	<td class="line x" title="82:316	This dendrogram, Droot, constitutes the upper part of the final tree." ></td>
	<td class="line x" title="83:316	3." ></td>
	<td class="line x" title="84:316	Inner-clustering: Let {C(1), C(2),  , C(C)} be the set of the classes obtained at step 1." ></td>
	<td class="line x" title="85:316	For each i (1 < i < C) do the following." ></td>
	<td class="line x" title="86:316	(a) Replace all words in the text except those in C(i) with their class token." ></td>
	<td class="line x" title="87:316	Define a new vocabulary V' = V1 U V2, where V1 = {all the words in C(i)}, V2 = {C1,C2,  ,Ci-l,Ci+l,Cc}, and Cj is a token for C(j) for 1 < j _< C. Assign each element in V' to its own class and execute binary merging with a merging constraint such that only those classes which only contain elements of V1 can be merged." ></td>
	<td class="line x" title="88:316	This can be done by ordering elements of V' with elements of V1 in the first Ivll positions and keep merging with a merging region whose width is \]Vll initially and decreases by one with each merging step." ></td>
	<td class="line x" title="89:316	(b) Repeat merging until all the elements in V1 are put in a single class." ></td>
	<td class="line x" title="90:316	Make a dendrogram Dsub out of the merging process for each class." ></td>
	<td class="line x" title="91:316	This dendrogram constitutes a subtree for each class with a leaf node representing each word in the class." ></td>
	<td class="line x" title="92:316	4 Combine the dendrograms by substituting each leaf node of Droot with the corresponding D sub . This algorithm produces a balanced binary tree representation of words in which those words which are close in meaning or syntactic feature come close in position." ></td>
	<td class="line x" title="93:316	Figure 2 shows an example of Dsu b for one class out of 500 classes constructed using this algorithm with a vocabulary of the 70,000 most frequently occurring words in the Wall Street Journal Corpus." ></td>
	<td class="line x" title="94:316	Finally, by tracing the path from the root node to a leaf node and assigning a bit to each branch with zero or one representing a left or right branch, respectively, we can assign a bit-string (word bits) to each word in the vocabulary." ></td>
	<td class="line x" title="95:316	1In the actual implementation, we only have to work on the bigram table instead of the whole text." ></td>
	<td class="line x" title="96:316	31 I I I Figure 2: Sample Subtree for One Class 3 Word Clustering Experiments We performed experiments using plain texts from six years of the Wall Street Journal Corpus to create clusters and word bits." ></td>
	<td class="line x" title="97:316	The sizes of the texts are 5 million words (MW), 10MW, 20MW, and 50MW." ></td>
	<td class="line x" title="98:316	The vocabulary is selected as the 70,000 most frequently occurring words in the entire corpus." ></td>
	<td class="line x" title="99:316	We set the number C of classes to 500." ></td>
	<td class="line x" title="100:316	The obtained hierarchical clusters are evaluated via the error rate of the ATR Decision-Tree Part-Of-Speech Tagger." ></td>
	<td class="line x" title="101:316	Then as an attempt to combine the two types of clustering methods discussed in Section 2, we performed an experiment for incorporating a word-reshuffling process into the word bits construction process." ></td>
	<td class="line x" title="102:316	3.1 Decision-Tree Part-Of-Speech Tagging The ATR Decision-Tree Part-Of-Speech Tagger is an integrated module of the ATR DecisionTree Parser which is based on SPATTER (Magerman 1994)." ></td>
	<td class="line x" title="103:316	The tagger employs a set of 441 syntactic tags, which is one order of magnitude larger than that of the University of Pennsylvania Treebank Project." ></td>
	<td class="line x" title="104:316	Training texts, test texts, and held-out texts are all sequences of word-tag pairs." ></td>
	<td class="line x" title="105:316	In the training phase, a set of events are extracted from the training texts." ></td>
	<td class="line x" title="106:316	An event is a set of feature-value pairs or question-answer pairs." ></td>
	<td class="line x" title="107:316	A feature can be any attribute of the context in which the current word word(O) appears; it is conveniently expressed as a question." ></td>
	<td class="line x" title="108:316	Tagging is performed left to right." ></td>
	<td class="line x" title="109:316	Figure 3 shows an example of an event with a current word like." ></td>
	<td class="line x" title="110:316	The last pair in the event is a special item which shows the answer, i.e., the correct tag of the current word." ></td>
	<td class="line x" title="111:316	The first two lines show questions about identity of words around the current word and tags for previous words." ></td>
	<td class="line x" title="112:316	These questions are called basic questions." ></td>
	<td class="line x" title="113:316	The second type of questions, word bits questions, are on clusters and word bits such as is the current word in Class 295?" ></td>
	<td class="line x" title="114:316	or what is the 29th bit of the previous word's word bits?." ></td>
	<td class="line x" title="115:316	The third type of questions are called linguist's questions and these are compiled by an expert grammarian." ></td>
	<td class="line x" title="116:316	Such questions could concern membership relations of words or sets of words, or morphological features of words." ></td>
	<td class="line x" title="117:316	Out of the set of events, a decision tree is constructed." ></td>
	<td class="line x" title="118:316	The root node of the decision tree represents the set of all the events with each event containing the correct tag for the corresponding word." ></td>
	<td class="line x" title="119:316	Probability distribution of tags for the root node can be obtained by calculating relative frequencies of tags in the set." ></td>
	<td class="line x" title="120:316	By asking a value of a specific feature on each event in the set, the set can be split into N subsets where N is the number of possible values for the feature." ></td>
	<td class="line x" title="121:316	We can then calculate conditional probability distribution of tags for 32 Event128: { (word(0), 'like' ) (word(-1), 'flies' ) (word(-2), 'time' } (word(l), 'a~' ) (word(2), 'arrow' ) (tag(-1), 'Verb-3rd-Sg-type3' ) (tag(-2), 'Noun-Sg-typel4' )  (Basic Questions) (Inclass?(word(0), Class295), 'yes' ) (WordBits(Word(-1), 29), '1' ) (\]sMember?(word(-2), Set('and', 'or', 'nor' )), 'no' ) (Tag, 'Prep-typeS' ) } (WordBits Questions) (IsPrefix?(Word(0), 'anti'), 'no' ) (Linguist's Questions) Figure 3: Example of an event tl L. tim g, \[-q 26 24 22 20 18 16 14 0 \[\] WSJ Text  ATR Corpus t i 60 Clustering Text Size (Million Words) Figure 4: Tagging Error Rate each subset, conditioned on the feature value." ></td>
	<td class="line x" title="122:316	After computing for each feature the entropy reduction incurred by splitting the set, we choose the best feature which yields maximum entropy reduction." ></td>
	<td class="line x" title="123:316	By repeating this step and dividing the sets into their subsets we can construct a decision tree whose leaf nodes contain conditional probability distributions of tags." ></td>
	<td class="line x" title="124:316	The obtained probability distributions are then smoothed using the held-out data." ></td>
	<td class="line x" title="125:316	The reader is referred to (Magerman 1994) for the details of smoothing." ></td>
	<td class="line x" title="126:316	In the test phase the system looks up conditional probability distributions of tags for each word in the test text and chooses the most probable tag sequences using beam search." ></td>
	<td class="line x" title="127:316	We used WSJ texts and the ATR corpus for the tagging experiment." ></td>
	<td class="line x" title="128:316	The WSJ texts are re-tagged manually using the ATR syntactic tag set." ></td>
	<td class="line x" title="129:316	The ATR corpus is a comprehensive sampling of Written American English, displaying language use in a very wide range of styles and settings, and compiled from many different domains (Black et al. 1996)." ></td>
	<td class="line x" title="130:316	Since the ATR corpus is still in the process of development, the size of the texts we have at hand for this experiment is rather minimal considering the large size of the tag set." ></td>
	<td class="line x" title="131:316	Table 1 shows the sizes of texts used for the experiment." ></td>
	<td class="line x" title="132:316	Figure 4 shows the tagging error rates plotted against various clustering text sizes." ></td>
	<td class="line x" title="133:316	Out of the three types of questions, basic questions and word bits 33 Text Size (words) Training Test Held-Out WSJ Text 75,139 5,831 6,534 ATR Text 76,132 23,163 6,680 Table 1: Texts for Tagging Experiments o iN 28 2d 24 2~ 2C 18 16 14 WSJ Text  Word,Bit @ LingQuest & Word\]3its m I II 12  0 10 20 30 40 50 60 Clustering Text Size (Million Words) Figure 5: Comparison of WordBits with LingQuest & WordBits questions are used in this experiment." ></td>
	<td class="line x" title="134:316	To see the effect of introducing word bits information into the tagger, we performed a separate experiment in which a randomly generated bit-string is assigned to each word 2 and basic questions and word bits questions are used." ></td>
	<td class="line x" title="135:316	The results are plotted at zero clustering text size." ></td>
	<td class="line x" title="136:316	For both WSJ texts and ATR corpus, the tagging error rate dropped by more than 30% when using word bits information extracted from the 5MW text, and increasing the clustering text size further decreases the error rate." ></td>
	<td class="line x" title="137:316	At 50MW, the error rate drops by 43%." ></td>
	<td class="line x" title="138:316	This shows the improvement of the quality of clusters with increasing size of the clustering text." ></td>
	<td class="line x" title="139:316	Overall high error rates are attributed to the very large tag set and the small training set." ></td>
	<td class="line x" title="140:316	One notable point in this result is that introducing word bits constructed from WSJ texts is as effective for tagging ATtt texts as it is for tagging WS3 texts even though these texts are from very different domains." ></td>
	<td class="line x" title="141:316	To that extent, the obtained hierarchical clusters are considered to be portable across domains." ></td>
	<td class="line x" title="142:316	Figure 5 contrasts the tagging results using only word bits against the results with both word bits and linguistic questions 3 for the WS3 text." ></td>
	<td class="line x" title="143:316	The zero clustering text size again corresponds ~Since a distinctive bit-string is assigned to each word, the tagger also uses the bit-string as an ID number for each word in the process." ></td>
	<td class="line x" title="144:316	In this control experiment bit-strings are assigned in a random way, but no two words are assigned the same word bits." ></td>
	<td class="line x" title="145:316	Random word bits are expected to give no class information to the tagger except for the identity of words." ></td>
	<td class="line x" title="146:316	3The linguistic questions we used here are still in the initial stage of development and are by no means 34 WSJ Text 22q 20 16 I 14i 12 10 0 60 l \[\]  Word.Bits @  LingQuest &WordBits,i i i I i I J r i i 10 20 30 40 50 Clustering Text Size (Million Words) Figure 6: Effects of Reshuffling for Tagging to a randomly generated bit-string." ></td>
	<td class="line x" title="147:316	Introduction of linguistic questions is shown to significantly reduce the error rates for the WSJ corpus." ></td>
	<td class="line x" title="148:316	Note that the dependency of the error rates on the clustering text size is quite similar in the two cases." ></td>
	<td class="line x" title="149:316	This indicates the effectiveness of combining automatically created word bits and hand-crafted linguistic questions in the same platform, i.e., as features." ></td>
	<td class="line x" title="150:316	In Figure 5 the tagging error rates seem to be approaching saturation after the clustering text size of 50MW." ></td>
	<td class="line x" title="151:316	However, whether no further improvement can be obtained by using texts of greater size is still an unsolved question." ></td>
	<td class="line x" title="152:316	3.2 Reshuffling One way to improve the quality of word bits is to introduce a reshuffling process just after step 1 (MI-clustering) of the word bits construction process (cf. 2.2)." ></td>
	<td class="line x" title="154:316	The reshuffling process we adopted is quite simple." ></td>
	<td class="line x" title="155:316	1." ></td>
	<td class="line x" title="156:316	Pick a word from the vocabulary." ></td>
	<td class="line x" title="157:316	Move the word from its current class to another class if that movement increases the AMI most among all the possible movements." ></td>
	<td class="line x" title="158:316	2." ></td>
	<td class="line x" title="159:316	Repeat step 1 starting from the most frequent word through the least frequent word." ></td>
	<td class="line x" title="160:316	This constitutes one round of reshuffling." ></td>
	<td class="line x" title="161:316	After several rounds of reshuffling, the word bits construction process is resumed from step 2 (Outer-clustering)." ></td>
	<td class="line x" title="162:316	Figure 6 shows the tagging error rates with word bits obtained by zero, two and five rounds of reshuffling 4 with a 23MW text." ></td>
	<td class="line x" title="163:316	Tagging results presented in Figure 5 are also shown as a reference." ></td>
	<td class="line x" title="164:316	Although the vocabulary used in this experiment is slightly different from the other comprehensive." ></td>
	<td class="line x" title="165:316	4The vocabulary used for the reshuffling experiment shown in Figure 6 is the one used for a preliminary experiment and its size is 63850." ></td>
	<td class="line x" title="166:316	35 experiments, we can clearly see the effect of reshuffling for both the word-bits-only case and the case with word bits and linguistic questions." ></td>
	<td class="line x" title="167:316	After five rounds of reshuffling, the tagging error rates become much smaller than the error rates using the 50MW clustering text with no reshuffling." ></td>
	<td class="line x" title="168:316	It is yet to be determined if the effect of reshuffling increases with increasing amount of clustering text." ></td>
	<td class="line x" title="169:316	4 From Word Clustering to Compound Clustering We showed in section 3 that the clusters we obtained are useful for Part-Of-Speech tagging." ></td>
	<td class="line x" title="170:316	However, the clusters we have worked on so far have all been clusters of words, and the PartOf-Speech tagging task has been limited to individual words." ></td>
	<td class="line x" title="171:316	For many other NLP tasks, however, similarities among phrases or multiword compounds are more important than those among individual words." ></td>
	<td class="line x" title="172:316	Let's turn back to the motivation of the clustering work discussed in the Introduction." ></td>
	<td class="line x" title="173:316	Consider the following sentences." ></td>
	<td class="line x" title="174:316	(e) The music sent Mary to sleep." ></td>
	<td class="line x" title="175:316	(f) The music sent Professor Frederic K. Thompson to sleep." ></td>
	<td class="line x" title="176:316	Suppose that we want to translate sentence (f) to some language by an example-based machine translation system with example data including sentence (e) and its translation." ></td>
	<td class="line x" title="177:316	In this case, what the system has to detect is that both 'Mary' and 'Professor Frederic K. Thompson' represent a human." ></td>
	<td class="line x" title="178:316	The similarity between 'Mary' and 'Frederic' as being first names doesn't help in this case." ></td>
	<td class="line x" title="179:316	Similarly, the detection of a correspondence between 'CBS Inc'." ></td>
	<td class="line x" title="180:316	and 'American Telephone & Telegraph Co'." ></td>
	<td class="line x" title="181:316	might be necessary in another case." ></td>
	<td class="line x" title="182:316	This observation leads us to construct classes o.f compounds rather than classes of just words." ></td>
	<td class="line x" title="183:316	Individual words can also be in the same class as multiword compounds, but we will generically call such a class a class of compounds in this paper." ></td>
	<td class="line x" title="184:316	While several methods have been proposed to automatically extract compounds (Smadja 1993, Suet al. 1994), we know of no successful attempt to automatically make classes of compounds." ></td>
	<td class="line x" title="185:316	The obvious problem we face when we construct classes of compounds is that the possible number of compounds is too large if we try to handle them individually." ></td>
	<td class="line x" title="186:316	However, if we represent compounds by a series of word-classes 5 instead of a series of words, we can constrain the explosion of the number of compounds." ></td>
	<td class="line x" title="187:316	One way of looking at this approach is to bundle quite similar compounds in a small subclass and treat them as a single compound." ></td>
	<td class="line x" title="188:316	For example, in the experiment described in Section 3, it was found that some word class, say WC129, contains almost exclusively first names, and another class, say WC246, contains almost exclusively family names." ></td>
	<td class="line x" title="189:316	Then the chain of classes 'WC129_WC246' represents one pattern of human names, or one group of two-word compounds representing human names." ></td>
	<td class="line x" title="190:316	There are of course many other patterns, or class chains, of different lengths which represent human names." ></td>
	<td class="line x" title="191:316	Therefore, our aim is to collect all the different class chains which are syntactically and semantically similar and put them in one compound-class." ></td>
	<td class="line x" title="192:316	In the following subsection, we describe one approach to this goal which is completely automatic." ></td>
	<td class="line x" title="193:316	4.1 Compound Clustering Method Our compound clustering method consists of the following three steps." ></td>
	<td class="line x" title="194:316	1." ></td>
	<td class="line x" title="195:316	Identification of Class Chains First, we replace each word in a large text with its word-class." ></td>
	<td class="line x" title="196:316	We then use mutual information as a measure of 'stickiness' of two classes, and identify which class pair 5We use the term word-class for a class of words to make a clear distinction from a compound.class." ></td>
	<td class="line x" title="197:316	36 . . should be chained." ></td>
	<td class="line x" title="198:316	Let MI(C1,C2) be mutual information of adjacent classes C1 and C2 in the text." ></td>
	<td class="line x" title="199:316	Then we form chain 'C1_C2' if Pr(ClC2) > *TH * (3) MI(C1, C2) = log Pr(cl)Pr(c2) for some threshold *TH*." ></td>
	<td class="line x" title="200:316	If it is found, in the series of three classes 'C1 C2 C3' in the text, that (C1,C2) forms a chain and (C2,C3) also forms a chain, then we simply form one large chain C1_C2_C3." ></td>
	<td class="line x" title="201:316	In a similar way we form a chain of maximum length for any series of classes in the text." ></td>
	<td class="line x" title="202:316	Construction of Reduced Text and New Vocabulary Each class chain identified is then replaced in the text with a token which represents the chain." ></td>
	<td class="line x" title="203:316	We call such a token a class chain token." ></td>
	<td class="line x" title="204:316	After the scan through the text with this replacement operation of a class chain with its token, the text is represented by a series of word-classes and class chain tokens." ></td>
	<td class="line x" title="205:316	The word classes remaining in the text are the ones which don't form a chain in their context." ></td>
	<td class="line x" title="206:316	Those word classes are then converted back to their corresponding words in the text 6 The resulting text is the same as the original text except that a multiword compound which matches one of the extracted class chains is represented by a class chain token." ></td>
	<td class="line x" title="207:316	We call this text the reduced text." ></td>
	<td class="line x" title="208:316	Out of the reduced text, a new vocabulary is created as a set of words and tokens whose frequency in the reduced text is more than or equal to some threshold." ></td>
	<td class="line x" title="209:316	Compound Clustering We conduct MI-clustering (step 1 of the word bits construction process) using the reduced text and the new vocabulary." ></td>
	<td class="line x" title="210:316	The classes we obtained, which we call compound-classes, contain words and class chain tokens." ></td>
	<td class="line x" title="211:316	Each class chain token in a compound-class is then expanded." ></td>
	<td class="line x" title="212:316	This means that all the multiword compounds that are represented by the class chain token in the text are put into the compound-class." ></td>
	<td class="line x" title="213:316	After expanding all the tokens, the tokens are removed from the compound-classes." ></td>
	<td class="line x" title="214:316	This results in compound-classes containing words and multiword compounds." ></td>
	<td class="line x" title="215:316	It is also possible to construct hierarchical clustering of compounds if we follow all the steps in the word bits construction process after this step." ></td>
	<td class="line x" title="216:316	4.2 Compound Clustering Experiment We used plain texts from two years (1987 and 1988) of Wall Street Journal Corpus to create compound clusters." ></td>
	<td class="line x" title="217:316	The total volume amounts to about 40 MW of text." ></td>
	<td class="line x" title="218:316	The word-classes used in this experiment are taken from the result of MI clustering with the 50MW text followed by five rounds of reshuffling." ></td>
	<td class="line x" title="219:316	The quality of the compound clusters depends on the threshold *TH* in equation 3." ></td>
	<td class="line oc" title="220:316	We used *TH*=3 following 'a very rough rule of thumb' used for word-based mutual information in (Church and Hanks, 1990)." ></td>
	<td class="line x" title="221:316	Out of the 40MW text, 7621 distinct class chains and 287,656 distinct multiword compounds are extracted." ></td>
	<td class="line x" title="222:316	To construct a new vocabulary, we selected the words and tokens whose frequency in the reduced text is more than four." ></td>
	<td class="line x" title="223:316	The size of the new vocabulary is 60589 and it contains 4685 class chain tokens." ></td>
	<td class="line x" title="224:316	Some of the compound-classes that were obtained are shown in Figure 7." ></td>
	<td class="line x" title="225:316	The compounds are listed in descending order of frequency in each class, and the lists are truncated at an arbitrary point." ></td>
	<td class="line x" title="226:316	6The conversion of a word-class to a word is not a one-to-one mapping, but with the context in the text the conversion is unique." ></td>
	<td class="line x" title="227:316	In the actual implementation, the text is represented by a series of (word, word-class) pairs and no conversion is actually carried out." ></td>
	<td class="line x" title="228:316	3'7 Figure 7: Examples of Compound Classes COMPOUND CLASS 171: President_Reagan Mr._Reagan Mr._Bush Mr._Dukakis Judge_Bork Ronald_Reagan George_Bush Michael_Dukakis Treasury_Secretary_J ames.B aker Mr._Holmes Vice_President_George_Bush Gov._Dukakis Gen._Noriega Mrs._Thatcher someone_who Mrs._Aquino Mr._Roh Gen._Secord Mr._Lawson Adm._Poindexter anyone_who MrDole Lt._Col._North Jimmy_Carter Sen._Dole Mr._Mulroney Mr._Quayle Sen._Bentsen Mr._Chirac Mr._Gephardt Mr._Marcos Vice_President_Bush Sen._Quayle Mr._Carter Mr._Chun Prime_Minister_Margaret_Thatcher Judge_Greene MrBrady President_Carter President_Chun Judge_Kennedy Sen._Proxmire Robert_Bork Rep._Rostenkowski Mr._Kohl Robert." ></td>
	<td class="line x" title="229:316	Holmes Judge_Pollack MrKemp Prime_Minister_Yasuhiro_Nakasone Mr._Kennedy President_Aquino COMPOUND CLASS 179: General_Motors_Corp." ></td>
	<td class="line x" title="230:316	Drexel_Burnham_LambertInc." ></td>
	<td class="line x" title="231:316	Ford.Motor_Co." ></td>
	<td class="line x" title="232:316	InternationalBusiness_Machines_Corp." ></td>
	<td class="line x" title="233:316	General_Electric_Co." ></td>
	<td class="line x" title="234:316	Shearsoniehman_Brothers_Inc." ></td>
	<td class="line x" title="235:316	Chrysler_Corp." ></td>
	<td class="line x" title="236:316	First.Boston_Corp." ></td>
	<td class="line x" title="237:316	Merrill_Lynch_&_Co." ></td>
	<td class="line x" title="238:316	Morgan_Stanley_&_Co." ></td>
	<td class="line x" title="239:316	Shearson_Lehman_Hut tonInc." ></td>
	<td class="line x" title="240:316	News_Corp." ></td>
	<td class="line x" title="241:316	American_Telephone_&_Telegraph_Co." ></td>
	<td class="line x" title="242:316	PaineWebbet_Inc." ></td>
	<td class="line x" title="243:316	PrudentialB ache_SecuritiesAnc." ></td>
	<td class="line x" title="244:316	TexacoAnc." ></td>
	<td class="line x" title="245:316	McDonnell_Douglas_Corp." ></td>
	<td class="line x" title="246:316	Dean_Witter_ReynoldsAnc." ></td>
	<td class="line x" title="247:316	TimeInc." ></td>
	<td class="line x" title="248:316	AMR_Corp." ></td>
	<td class="line x" title="249:316	CB SAnc." ></td>
	<td class="line x" title="250:316	American." ></td>
	<td class="line x" title="251:316	Express_Co." ></td>
	<td class="line x" title="252:316	Campeau_Corp." ></td>
	<td class="line x" title="253:316	BankAmerica_Corp." ></td>
	<td class="line x" title="254:316	Du_Pont_Co." ></td>
	<td class="line x" title="255:316	Allegis_Corp." ></td>
	<td class="line x" title="256:316	General.Dynamics_Corp." ></td>
	<td class="line x" title="257:316	Digital_Equipment_Corp." ></td>
	<td class="line x" title="258:316	Kohlb erg_Kravis_Roberts_&_Co." ></td>
	<td class="line x" title="259:316	Exxon_Corp." ></td>
	<td class="line x" title="260:316	Chase_Manhattan_Corp." ></td>
	<td class="line x" title="261:316	USX_Corp." ></td>
	<td class="line x" title="262:316	Nikko_Securities_Co." ></td>
	<td class="line x" title="263:316	Lockheed_Corp." ></td>
	<td class="line x" title="264:316	COMPOUND CLASS 221: common_stock preferred_stock cash_flow bank_debt long-term_debt foreign_debt subordinated_debt senior_debt balance_sheet short-term_debt balance_sheets cost_overruns corporate_debt debt_load convertible_preferred_stock international_debt debt_outstanding Class_B_stock debt_ratings cumulative_preferred_stock corporateAOUs current_delivery preference_stock ozoneAayer buffer_stock unsecured_debt convertible_preferred external_debt debt_offering current_contract blood_clots Class_B_common cumulative_convertible_preferred_stock corporate_governance Class_B_common_stock unsold_balance secured_debt debt.issue cumulative_preferred municipal_debt convertible_exchangeable_preferred_stock cash.hoard debt.rating 65-day_supply cash_balance senior_subordinated_debt senior_secured_debt COMPOUND CLASS 256: Fed SEC Reagan_administration IRS Pentagon Justice." ></td>
	<td class="line x" title="265:316	Department Navy Commerce.Department FDA Army FCC FDIC Federal.Reserve_Board State_Department Bundesbank EPA FAA IMF Labor_Department Agriculture_Department FBI NASD Defense." ></td>
	<td class="line x" title="266:316	Department Federal_Home." ></td>
	<td class="line x" title="267:316	Loan_Bank_Board Britishgovernment NRC Finance.Ministry Japanese.government FTC UAW Kremlin PRI Transportation._Department PLO Federal_Trade_Commission CFTC Canadian_government NSC GAO Teamsters Carter_Hawley INS GSA Environmental_ProtectionAgency ANC Labor_Party AFL-CIO FASB NFL Federal_Aviation_Administration ACLU 38 Compound-class-171 consists of names with title many of which are politicians' names." ></td>
	<td class="line x" title="268:316	Compound-class-179 contains multiword company names." ></td>
	<td class="line x" title="269:316	Compound-class-221 consists of multiword compound nouns from several specific semantic domains including money, surgery and natural environment, but most of the frequent compounds are money-related terms." ></td>
	<td class="line x" title="270:316	Compound-class-256 is worth special attention in the sense that although single words and multiword compounds are mixed almost evenly, most of the single words are abbreviations of organizations, mostly public organizations, and the multiword compounds also ahnost exclusively represent public organizations." ></td>
	<td class="line x" title="271:316	Another point to note here is that the pattern of case is not uniform in this list." ></td>
	<td class="line x" title="272:316	Although both 'Defense Department' and 'British government' represent political organizations, the former consists of only capitalized words and the latter doesn't." ></td>
	<td class="line x" title="273:316	In order to measure the performance of this compound clustering method, a consistency check is performed for one class." ></td>
	<td class="line x" title="274:316	The objective is to check what proportion of the identified members of the class actually deserves to be included in the class." ></td>
	<td class="line x" title="275:316	Because this kind of judgement is very difficult in general, we must choose a class whose membership is quite clear to identify." ></td>
	<td class="line x" title="276:316	By this criterion we chose compound-class-179 because it is quite easy to decide if some compound is a correct company name or not." ></td>
	<td class="line x" title="277:316	From the 40MW text, we randomly chose 3000 occurrences of multiword compounds which are members of compound-class-179." ></td>
	<td class="line x" title="278:316	By manual anMysis, it was found that 133 identified compounds were wrong." ></td>
	<td class="line x" title="279:316	The precision is therefore 95.6 %." ></td>
	<td class="line x" title="280:316	Most of the errors are due to the truncation of correct company names." ></td>
	<td class="line x" title="281:316	For example, from the string 'North American Philips Corp.', only 'Philips Corp'." ></td>
	<td class="line x" title="282:316	was extracted." ></td>
	<td class="line x" title="283:316	Although 'Philips Corp'." ></td>
	<td class="line x" title="284:316	is itself a correct company name, we treated this instance as an error because our judgement was occurrence-based." ></td>
	<td class="line x" title="285:316	There was only one instance where a compound irrelevant to company names was extracted (a person name)." ></td>
	<td class="line x" title="286:316	For a control experiment which we will describe shortly, all the incorrect compounds are corrected by hand and a standard file is created which contains all the correct 2999 occurrences of company names." ></td>
	<td class="line x" title="287:316	One merit of the current approach is that the identification of a compound-class is carried out in time linear with the text size." ></td>
	<td class="line x" title="288:316	Therefore, by associating a word with its word-class as a feature in the lexicon, and by storing class chain patterns and their membership to compound classes, we can carry out a real time identification of the compound-classes without actually storing the compounds in the lexicon." ></td>
	<td class="line x" title="289:316	As a control experiment to the above experiment, we conducted word-based compound extraction and compared the result with the above result." ></td>
	<td class="line x" title="290:316	Instead of mutual information of adjacent classes, mutual information of adjacent words are calculated for all the bigrams in the text." ></td>
	<td class="line x" title="291:316	Then using various MI threshold values, words are chained in a similar way as described in Section 4.1, and compounds are identified." ></td>
	<td class="line x" title="292:316	We then evaluated how many of the occurrences of company names in the standard file are identified in the word-based compound extraction experiment." ></td>
	<td class="line x" title="293:316	We varied the MI threshold values from 1.0 to 6.0 with a step of 0.5, but the precision of the word-based approach with respect to the standard file was always below 50 %." ></td>
	<td class="line x" title="294:316	The main reason of the superiority of the class-based approach against the word-based one is associated with the data sparseness problem." ></td>
	<td class="line oc" title="295:316	Most of the previously proposed methods to extract compounds or to measure word association using mutual information (MI) either ignore or penalize items with low co-occurrence counts (Church and Hanks 1990, Su, Wu and Chang 1994), because MI becomes unstable when the co-occurrence counts are very small." ></td>
	<td class="line x" title="296:316	Take for example a class chain 'WC129_WC246' discussed above." ></td>
	<td class="line x" title="297:316	Figure 8 shows some examples of compounds matching the pattern 'WC129_WC246' in the 40MW text." ></td>
	<td class="line x" title="298:316	Each column shows, from left to right, word-based MI for the word bigram (WORD-1,WORD-2), co-occurrence frequency of the word bigram, the first word, the second word, class-based MI for the class bigram (CLASS-I, CLASS-2), co-occurrence frequency of the class bigram, the word-class of WORD-l, and the word-class of WORD-2." ></td>
	<td class="line x" title="299:316	Note that the numbers for class-based entries are 39 Figure 8: Examples of Compounds for Names WORD-MI BI-COUNT WORD-1 WORD-2 CLASS-MI CL-BI-COUNT CLASS-I CLASS-2 16.104915 1 Takako Doi 3.941235 52087 129 246 15.881772 3 Mandy Patinkin 3.941235 52087 129 246 14.783159 3 Hideo Sakamaki 3.941235 52087 129 246 12.280086 10 Curt Bradbury 3.941235 52087 129 246 11.048669 3 Matthew Kennelly 3.941235 52087 129 246 9.358209 1 Marsha Gardner 3.941235 52087 129 246 7.994606 7 Ralph Whitehead 3.941235 52087 129 246 5.073718 1 George Hartman 3.941235 52087 129 246 4.328457 1 Daniel Owen 3.941235 52087 129 246 3.914939 3 Charles Walker 3.941235 52087 129 246 3.319351 1 Robert Fischer 3.941235 52087 129 246 2.939145 1 Robert Lucas 3.941235 52087 129 246 2.236354 2 Edward Baker 3.941235 52087 129 246 1.119861 1 Robert Shultz 3.941235 52087 129 246 1.072005 1 Robert Hall 3.941235 52087 129 246 1.069133 1 George Jackson 3.941235 52087 129 246 0.771154 1 Richard Baker 3.941235 52087 129 246 0.218531 1 John Jackson 3.941235 52087 129 246 the same for all the compounds because we collected compounds with the same class chain." ></td>
	<td class="line x" title="300:316	Although all the compounds are compounds of a first name and a family name, the wordbased MI varies considerably." ></td>
	<td class="line x" title="301:316	This is because frequencies of first names and family names vary considerably while frequencies of pairs of first names and family names in the list are very small." ></td>
	<td class="line x" title="302:316	For example, 'John' and 'Jackson' are very common first and second names, but the name 'John Jackson' appeared only once in the text." ></td>
	<td class="line x" title="303:316	Therefore the word-based MI becomes very small." ></td>
	<td class="line x" title="304:316	On the other hand, because 'Takako' and 'Doi' were very rare names in WSJ news articles in 1987 and 1988, the MI becomes very high even though 'Takako Doi' appeared only once in the text." ></td>
	<td class="line x" title="305:316	In contrast, the class-based MI is very stable because the co-occurrence frequency of the two classes is as high as 52087." ></td>
	<td class="line x" title="306:316	When we examined frequencies of all the compounds in the text that match 'WC129_WC246', it turned out that more than 80 % of the compounds appeared less than five times in the text." ></td>
	<td class="line x" title="307:316	This shows how the data sparseness problem is critical for the purpose of compound extraction." ></td>
	<td class="line x" title="308:316	5 Conclusion We presented an algorithm for hierarchical clustering of words, and conducted a clustering experiment using large texts ranging in size from 5MW to 50MW." ></td>
	<td class="line x" title="309:316	High quality of the obtained clusters is confirmed by the effect of introducing word bits into the ATR Decision-Tree Part-OfSpeech Tagger." ></td>
	<td class="line x" title="310:316	The hierarchical clusters obtained from WSJ texts are also shown to be useful for tagging ATR texts which are from quite different domains than WSJ texts." ></td>
	<td class="line x" title="311:316	The wordclasses thus obtained are then used to identify and cluster multiword compounds." ></td>
	<td class="line x" title="312:316	It is shown that by using statistics on classes instead of on words, the data sparseness problem is avoided and the reliability of mutual information is increased." ></td>
	<td class="line x" title="313:316	As a result, class-based compounds identification and extraction becomes more reliable than word-based methods." ></td>
	<td class="line x" title="314:316	This approach 40 also provides a way of automatically clustering compounds, which has rarely been attempted." ></td>
	<td class="line x" title="315:316	Acknowledgements We thank John Lafferty and Christopher Manning for their helpful comments, suggestions and discussion with us." ></td>
	<td class="line x" title="316:316	Special thanks are to Eric Visser for reviewing a draft of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W96-0306
Acquisition Of Semantic Lexicons: Using Word Sense Disambiguation To Improve Precision
Dorr, Bonnie Jean;Jones, Douglas A.;"></td>
	<td class="line x" title="1:123	Acquisition of Semantic Lexicons: Using Word Sense Disambiguation to Improve Precision Bonnie J. Dorr and Doug Jones Department of Computer Science and Institute for Advanced Computer Studies University of Maryland A. V. Williams Building College Park, MD 20742 {bonnie,jones } @umiacs.umd.edu Abstract This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources." ></td>
	<td class="line x" title="2:123	We describe semantic filters designed to reduce the number of incorrect assignments (i.e. , improve precision) made by a purely syntactic technique." ></td>
	<td class="line x" title="3:123	We demonstrate that it is possible to use these filters to build broad-coverage lexicons with minimal effort, at a depth of knowledge that lies at the syntax-semantics interface." ></td>
	<td class="line x" title="4:123	We report on our results of disambiguating the verbs in the semantic filters by adding WordNet 1 sense annotations." ></td>
	<td class="line x" title="5:123	We then show the results of our classification on unknown words and we evaluate these results." ></td>
	<td class="line x" title="6:123	1 Introduction This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources." ></td>
	<td class="line x" title="7:123	We describe semantic filters designed to reduce the number of incorrect assignments (i.e. , improve precision) made by a purely syntactic technique." ></td>
	<td class="line x" title="8:123	We demonstrate that it is possible to use these filters to build broad-coverage lexicons with minimal effort, at a depth of knowledge that lies at the syntax-semantics interface." ></td>
	<td class="line x" title="9:123	We report on our results of disambiguating the verbs in the semantic filters by adding WordNet sense annotations." ></td>
	<td class="line x" title="10:123	We then show the results of our classification on unknown words and we evaluate these results." ></td>
	<td class="line x" title="11:123	As machine-readable resources (i.e. , online dictionaries, thesauri, and other knowledge sources) become readily available to NLP researchers, automated acquisition has become increasingly more attractive." ></td>
	<td class="line x" title="12:123	Several researchers have noted that the average time needed to construct a lexical entry by hand can be as much as 30 minutes (see, e.g., (Neff and McCord, 1990; Copestake et al. , 1995; Walker and Amsler, 1986))." ></td>
	<td class="line x" title="13:123	Given that most large-scale NLP applications require lexicons of 20-60,000 words, automation of the acquisition process has become a necessity." ></td>
	<td class="line oc" title="14:123	Previous research in automatic acquisition focuses primarily on the use of statistical techniques, such as bilingual alignment (Church and Hanks, 1990; Klavans and Tzoukermann, 1996; Wu and Xia, 1995), or extraction of syntactic constructions from online dictionaries and corpora (Brent, 1993; Dorr, Garman, and Weinberg, 1995)." ></td>
	<td class="line x" title="15:123	Others who have taken a more knowledge-based (interlingual) approach (Lonsdale, Mitamura, and Nyberg, 1996) do not provide a means for systematically deriving the relation between surface syntactic structures and their underlying semantic representations." ></td>
	<td class="line x" title="16:123	Those who have taken more argument structures into account, e.g., (Copestake et al. , 1995), do not take full advantage of the systematic relation between syntax and semantics during lexical acquisition." ></td>
	<td class="line x" title="17:123	1We used Version 1.5 of WordNet, available at http://www.cogsci.princeton.edu/~wn." ></td>
	<td class="line x" title="18:123	42 ! We adopt the central thesis of Levin (1993), i.e., that the semantic class of a verb and its syntactic behavior are predictably related." ></td>
	<td class="line x" title="19:123	We base our work on a correlation between semantic classes and patterns of grammar codes in the Longman's Dictionary of Contemporary English (LDOCE) (Procter, 1978)." ></td>
	<td class="line x" title="20:123	We extend this work by coupling the syntax-semantics relation with a pre-defined association between WordNet (Miller, 1985) word senses and Levin's verbs in order to group the full Set of LDOCE verbs into semantic classes." ></td>
	<td class="line x" title="21:123	While the LDOCE has been used previously in automatic extraction tasks (Alshawi, 1989; Farwell, Guthrie, and Wilks, 1993; Boguraev and Briscoe, 1989; Wilks et al. , 1989; Wilks et al. , 1990) these tasks are primarily concerned with the extraction of other types of information including syntactic phrase structure and broad argument restrictions or with the derivation of semantic structures from definition analyses." ></td>
	<td class="line x" title="22:123	The work of Sanfilippo and Poznanski (1992) is more closely related to our approach in that it attempts to recover a syntactic-semantic relation from machine-readable dictionaries." ></td>
	<td class="line x" title="23:123	However, they claim that the semantic classification of verbs based on standard machine-readable dictionaries (e.g. , the LDOCE) is 'a hopeless pursuit \[since\] standard dictionaries are simply not equipped to offer this kind of information with consistency and exhaustiveness'." ></td>
	<td class="line x" title="24:123	Others have also argued that the task of simplifying lexical entries on the basis of broad semantic class membership is complex and, perhaps, infeasible (see, e.g., Boguraev and Briscoe (1989))." ></td>
	<td class="line x" title="25:123	However, a number of researchers (Fillmore, 1968; Grimshaw, 1990; Gruber, 1965; Guthrie et al. , 1991; Hearst, 1991; Jackendoff, 1983; Jackendoff, 1990; Levin, 1993; Pinker, 1989; Yarowsky, 1992) have demonstrated conclusively that there is a clear relationship between syntactic context and word senses; it is our aim to exploit this relationship for the acquisition of semantic lexicons." ></td>
	<td class="line x" title="26:123	We first describe the LDOCE verb classification resulting from a purely syntactic approach to deriving semantic classes." ></td>
	<td class="line x" title="27:123	We then describe a semantic filter designed to reduce the number of incorrect assignments made by the syntactic technique; we show how this filter can be enhanced with a method that accounts for multiple word senses." ></td>
	<td class="line x" title="28:123	Finally we show the results of our classification of unknown verbs, and we evaluate these results." ></td>
	<td class="line x" title="29:123	Our results clearly indicate that the resolution of polysemy is a key component to developing an effective semantic filter." ></td>
	<td class="line x" title="30:123	2 Verb Classification Based on Syntactic Behavior We build on the syntactic filter approach of (Dorr, Garman, and Weinberg, 1995), in which verbs were automatically classified into semantic classes using syntactic encodings in LDOCE." ></td>
	<td class="line x" title="31:123	This earlier approach produced a ranked assignment of verbs to the semantic classes from (Levin, 1993) based on syntactic tests (e.g. , whether a verb occurs in a dative construction such as Mary gave John the book)." ></td>
	<td class="line x" title="32:123	2 The syntactic approach alone was demonstrated to classify Levin verbs with 47% accuracy (i.e. , 1812 correct verb classifications out of 3851 possible assignments)." ></td>
	<td class="line x" title="33:123	The measure of success used in the purely syntactic approach is flawed in that the 'accuracy' factor was based on the number of correct assignments in the five top-ranked assignments produced by their algorithm." ></td>
	<td class="line x" title="34:123	A better measure of the efficacy of the algorithm would be to examine the ratio of correct assignments to the total number of assignments." ></td>
	<td class="line x" title="35:123	The algorithm in (Dorr, Garman, and Weinberg, 1995) is correct only 13% of the time (1812 correct assignments out of 13761 total assignments) if given up to 5 assignments per verb." ></td>
	<td class="line x" title="36:123	If given up to 15 assignments, the situation 2Levin's semantic classes are labeled with numbers ranging from 9 to 57; the actual number of semantic classes is 191 (not 46) due to many class subdivisions under each major class, These 191 classes cover 2813 verbs that occur in the LDOCE." ></td>
	<td class="line x" title="37:123	Since verbs may occur in multiple classes, the number of possible assignments of LDOCE verbs into classes is 3851." ></td>
	<td class="line x" title="38:123	43 would deteriorate further: even though 2607 out of 3851 possible assignments would be correct, these correct assignments constitute only 6.5% of the total number of assignments made by the algorithm." ></td>
	<td class="line x" title="39:123	We borrow terminology from Information Filtering (see, e.g., (Lewis, 1992)) to characterize these results." ></td>
	<td class="line x" title="40:123	In particular, Recall is the number of correct categorizations the algorithm gives divided by the number of correct categorizations already given in the database." ></td>
	<td class="line x" title="41:123	Precision, on the other hand, is the number of correct categorizations that the algorithm gives divided by the total number of categorizations that it gave." ></td>
	<td class="line x" title="42:123	In these terms, the algorithm in (Dorr, Garman, and Weinberg, 1995) achieves a recall of 67.7%, but a precision of 6.5% if given up to 15 semantic class assignments per verb." ></td>
	<td class="line x" title="43:123	In addition to low precision, the purely syntactic filter described above was tested only on verbs that are in (Levin, 1993) and it did not take into account the problem of multiple word senses." ></td>
	<td class="line x" title="44:123	The remainder of this paper describes the formulation and refinement of semantic filters that increases the precision of this earlier experiment, while extending the coverage to novel verbs (i.e. , ones not occurring in (Levin, 1993)) and addressing the polysemy problem." ></td>
	<td class="line x" title="45:123	3 Semantic Filter: Increasing Precision We take as our starting point 7767 LDOCE verbs, approximately 5000 of which do not occur in Levin's classes." ></td>
	<td class="line x" title="46:123	Each of these verbs was assigned up to 15 possible semantic classes, ranked by the degree of likelihood that the verb belongs to that class, giving a total of 113,106 ranked assignments." ></td>
	<td class="line x" title="47:123	As described above, the syntactic filter discovers 2607 of the 3851 assignments of LDOCE verbs found in Levin's semantic classes." ></td>
	<td class="line x" title="48:123	These assignments are particularly interesting because we know they are correct, and we can see how high the program ranks the correct assignments." ></td>
	<td class="line x" title="49:123	To create a semantic filter, we take a semantic class from Levin and extend it with related verbs from WordNet." ></td>
	<td class="line x" title="50:123	We call this extended list a semantic field." ></td>
	<td class="line x" title="51:123	Verbs that do not occur in the semantic field of a particular class fail to pass through the semantic filter for that class, by definition." ></td>
	<td class="line x" title="52:123	We first examined different semantic relations provided by WordNet (synonymy, hyponymy, both synonyms and hyponyms, and synonyms of synonyms) in order to determine which one would be most appropriate for constructing semantic fields for each of Levin's 191 verb classes." ></td>
	<td class="line x" title="53:123	We evaluated the performance of these different relations by examining the degree of class coverage of the relation using a prototypical verb from each class." ></td>
	<td class="line x" title="54:123	3 For example, the Change of State verbs of the break subclass (Class 45.1) contains the verbs break, chip, crack, crash, crush, fracture, rip, shatter, smash, snap, splinter, split, tear." ></td>
	<td class="line x" title="55:123	The full semantic field contains the union of the related verbs for every verb in the original Levin class." ></td>
	<td class="line x" title="56:123	Thus, if we build our semantic field on the basis of the synonymy relation, all synonyms of verbs in a particular class would be legal candidates for membership in that class." ></td>
	<td class="line x" title="57:123	For Class 45.1, using the synonymy relation would result in a field size of 185 (i.e. , there are 185 WordNet synonyms for the 13 verbs in the class); by contrast, the hyponymy relation would yield a field size of 245." ></td>
	<td class="line x" title="58:123	To choose a relation to use for the semantic field, we looked at verbs semantically related to the prototypical verb in each class, and checked how many of the verbs in each class would be included in the filter." ></td>
	<td class="line x" title="59:123	We examined several relations based on combinations of synonymy and hyponymy." ></td>
	<td class="line x" title="60:123	We considered the best candidate to be the one that matched the greatest proportion of the verbs in Levin's semantic classes when given the prototype verb." ></td>
	<td class="line x" title="61:123	The best relation, synonyms of the 3A verb is considered to be prototypical with respect to a class if it conforms to all of Levin's membership tests for that class." ></td>
	<td class="line x" title="62:123	These tests are based on grammaticahty of usage in certain well-defined contexts (e.g. , the dative construction)." ></td>
	<td class="line x" title="63:123	44 All Filtered Total Assignments 40,248 4168 Right Assignments 2,607 2607 Wrong Assignments 37,641 1561 Precision (Right/Total) 6.5% 62.5% Table 1: Increasing Precision with the Semantic Filter prototype verb, matched an average of 20% of the Levin verbs, while having an average size of 11 verbs." ></td>
	<td class="line x" title="64:123	The average size of Levin's semantic classes is 22 verbs." ></td>
	<td class="line x" title="65:123	Let us now:look at the behavior of the synonymy-based semantic filter." ></td>
	<td class="line x" title="66:123	Of the 113,106 assignments of LDOCE verbs to Levin classes given by the syntactic filter, 6029 (19%) pass through the semantic filter." ></td>
	<td class="line x" title="67:123	Clearly, the semantic filter constrains the possible assignments, but the question to ask is whether the constraint improves the accuracy of the assignments." ></td>
	<td class="line x" title="68:123	To answer this, we first examined the 2813 verbs in LDOCE that also appear in Levin to see if they matched Levin's categorization." ></td>
	<td class="line x" title="69:123	Without the semantic filter, the syntactic filter provides up to 15 semantic-class assignments for each of the 2813 verbs, giving 40,248 assignments, as shown in Table 1." ></td>
	<td class="line x" title="70:123	2,607 of these assignments (6.5%) are correct." ></td>
	<td class="line x" title="71:123	When we add the semantic filter, the number of assignments drops to 4168, 10% of the unfiltered assignments." ></td>
	<td class="line x" title="72:123	2607 of these (62.5%) are correct, a twelve-fold improvement over the unfiltered assignments." ></td>
	<td class="line x" title="73:123	By Right Assignments, we mean: cases in which the system assigns a verb to a given Levin class, when that verb appears in that class in Levin's book." ></td>
	<td class="line x" title="74:123	By Wrong Assignments, we mean: cases in which the system assigns a verb to a given Levin class, when that verb does not appear in that class in Levin's book." ></td>
	<td class="line x" title="75:123	It is important to point out that even though the semantic filter is based on words in Levin, it still sometimes categorized the Levin verb incorrectly." ></td>
	<td class="line x" title="76:123	Since the filter is based on synonyms of Levin verbs, in some cases, a synonym of a verb from some other class will appear in the set that does not belong there." ></td>
	<td class="line x" title="77:123	In this case, there are 1561 assignments known to be wrong, out of a total of 4168 assignments." ></td>
	<td class="line x" title="78:123	For example, the verb scatter is a synonym of break in WordNet." ></td>
	<td class="line x" title="79:123	Because the verb break occurs in each of these classes, the semantic filter based on synonyms assigns scatter to classes 10.6 (Cheat Verbs), 23.2 (Split Verbs), 40.8.3 (Hurt Verbs), 45.1 (Break Verbs), 48.1.1 (Appear Verbs)." ></td>
	<td class="line x" title="80:123	But the correct class for scatter is 9.7 (Spray/Load Verbs)." ></td>
	<td class="line x" title="81:123	This illustrates the difficulty of using an approach that does not account for multiple word senses." ></td>
	<td class="line x" title="82:123	We will address this point further in section 3." ></td>
	<td class="line x" title="83:123	Setting aside the polysemy problem, we see that this semantic filter is very useful for reducing the number of incorrect assignments." ></td>
	<td class="line x" title="84:123	4 Performance on Novel Words We now examine how well it performs on unknown words by constructing a semantic filter based on three different proportions of the original 2813 Levin verbs: (a) 50%, (b) 70%, and (c) 90%, chosen randomly." ></td>
	<td class="line x" title="85:123	4 We then checked whether the 'unknown' verbs (those not used to construct 4We chose randomly selected subsets: First we selected a random 90% of the Levin verbs, then we chose 77.7% of those to give 70% of the Levin verbs." ></td>
	<td class="line x" title="86:123	In turn, 71.4% of those give the verbs for the 50% study." ></td>
	<td class="line x" title="87:123	45 Semantic-Filter Assignments to Levin Classes Levin 50% 70% 90% 100% Original Number of 6 Assignments Total \[Wrong known 11282\[ novel 1325 known 1179812628 novel 809 663 known 1234113632 novel 266 271 all known \[2607\[ 4168 \[ Guesses I Right 1752 I 470 I 1282 841 429 412 I 8301 1798 360 303 \[ 1291 I 2341 158 113 1561\] 2607 Ratios I Precision I Recall 73.2% I 100.0% I 49.0% 31.1% 68.4%\] 100.0%\[ 45.7% 37.5% 64.5% I 100.0% I 41.7% 42.5% 62.5% I 100.0%\[ Original Syntactic-Filter Assignments to Levin Classes Levin Original I Number of Assignments Ratios Assignments I Total I Wrong I Right Precision IRecovery 100% Known \]2607\[40248\[ 37641\[ 2607 6.5%\[ 100% Table 2: Undisambiguated Synonyms the semantic filter) were assigned to their correct classes." ></td>
	<td class="line x" title="88:123	Table 2 summarizes the recall and precision results for semantic filtering on these three different proportions of Levin verbs." ></td>
	<td class="line x" title="89:123	Consider the rows that show the behavior of the experiment which uses 50% of Levin's verbs, and tries to guess the remaining verbs using synonymy." ></td>
	<td class="line x" title="90:123	Recall that there are 2607 verbs all together." ></td>
	<td class="line x" title="91:123	In this case, 1282 verbs were chosen at random to use in constructing the filter." ></td>
	<td class="line x" title="92:123	We call these the 'known' verbs." ></td>
	<td class="line x" title="93:123	This leaves 1325 for use in evaluating the semantic filter--we call these the 'novel' verbs." ></td>
	<td class="line x" title="94:123	For the 1282 known verbs, the filter made 1752 assignments to semantic classes." ></td>
	<td class="line x" title="95:123	There were 470 wrong assignments and 1282 right ones, giving a precision rate of 73.2% and recall rate of 100.0%." ></td>
	<td class="line x" title="96:123	5 The Effect of Disambiguation As mentioned previously, the problem with the semantic filter we have defined is that it is not sensitive to multiple word senses of the particular verbs in the semantic classes." ></td>
	<td class="line x" title="97:123	For example, there are 23 senses of the verb break in WordNet." ></td>
	<td class="line x" title="98:123	This includes senses which correspond to the Change of State verbs, such as Sense 9, 'break, bust, cause to break', the synonyms of which are destroy, ruin, bust up, wreck, wrack." ></td>
	<td class="line x" title="99:123	But it also includes irrelevant senses, such as Sense 7, 'break dance', the synonyms of which are dance, do a dance, perform a dance." ></td>
	<td class="line x" title="100:123	Clearly, the semantic filter would behave better if we used word senses in creating the fields." ></td>
	<td class="line x" title="101:123	As an attempt to address the polysemy problem, we conducted an exploratory study in which the verbs in Levin's semantic classes were disambiguated by hand: each verb received as many WordNet senses as were applicable." ></td>
	<td class="line x" title="102:123	The performance of the various filters is shown in Table 3." ></td>
	<td class="line x" title="103:123	To see the effect of disambiguation, compare the difference between undisambiguated and disambiguated synonyms." ></td>
	<td class="line x" title="104:123	Precision has increased from 62.5% to 85.3%." ></td>
	<td class="line x" title="105:123	For novel verbs, in the experiment which uses 50% of the verbs and 46 Undisambiguated Synonyms Known Novel Recall Precision Recall Precision % Levin 100% 90% 70% 5O% 100.0% 62.5% 100.0% 64.5% 100.0% 68.4% 100.0% 73.2% 0.0% 0.0% 42.5% 41.7% 37.5% 45.7% 31.1% 49.0% Disambiguated Synonyms Known Novel Recall Precision Recall Precision % Levin 100% 9o% 70% 5o% 100.0% 85.3% 100.0% 86.2% 100.0% 88.3% 100.0% 91.7% 0.0% 0.0% 29.3% 63.9% 26.1% 68.5% 21.6% 70.8% Disambiguated Hyponyms of Hypernyms % Known Levin Recall Precision 100% 100.0% 37.7% 9O% 100.0% 39.O% 70% 100.0% 41.5% 50% 100.0% 45.8% Novel Recall Precision 0.0% 0.0% 68.8% 29.5% 63.0% 31.1% 58.6% 34.6% Union of Disambiguated Synonyms with Hyponyms of Hypernyms % Levin lOO% 90% 70% 50% Known Novel Recall Precision Recall Precision 100.0% 37.6% 100.0% 38.9% 100.0% 41.4% 100.0% 45.8% o.o% o.o% 69.5% 29.7% 64.4% 31.5% 59.6% 34.9% Table 3: Comparison of Filters 47 tries to guess the rest, the precision increases from 49.0% to 70.8%." ></td>
	<td class="line x" title="106:123	But notice also that the recall decreases: with disambiguation (in the 50% study), recall drops from 31.1% for undisambiguated verbs to 21.6% for disambiguated verbs." ></td>
	<td class="line x" title="107:123	The reason for this is that the undisambiguated filters contain numerous assignments which are correct but are included only accidentally." ></td>
	<td class="line x" title="108:123	Table 3 also shows the performance of two other semantic filters based on hyponyms." ></td>
	<td class="line x" title="109:123	We found that using hyponyms of hypernyms (going up one level in abstraction, and then one level back down) gave much better recall than plain synonymy, although the precision is lower." ></td>
	<td class="line x" title="110:123	We also built a filter based on the union of synonyms with hyponyms of hypernyms." ></td>
	<td class="line x" title="111:123	The effect of the synonyms on this filter was negligible, presumably since synonyms are often hyponyms of hypernyms." ></td>
	<td class="line x" title="112:123	The results for both of these filters are shown in Table 3." ></td>
	<td class="line x" title="113:123	6 Conclusion and Future Work Our main result is that the semantic field substantially reduces the number of incorrect assignments given by the syntactic filter." ></td>
	<td class="line x" title="114:123	One of our goals is to assign new verbs, i.e., all of the verbs in LDOCE, to the semantic classes of Levin." ></td>
	<td class="line x" title="115:123	Since there are 7767 verbs in LDOCE, and there are 191 semantic classes in Levin, there are 1,483,497 potential assignments of verbs to these semantic classes." ></td>
	<td class="line x" title="116:123	The syntactic filter reduces the number of assignments under consideration to 113,106 (7.6% of the number of potential assignments) while preserving 67% of the assignments we know to be correct." ></td>
	<td class="line x" title="117:123	The various semantic filters in turn reduce the number of assignments further." ></td>
	<td class="line x" title="118:123	For example, the broad semantic filter reduced the 113,106 verbs that passed through the syntactic filter down to 6029 assignments, 19% of the number of assignments based on syntax and 0.4% of the potential assignments." ></td>
	<td class="line x" title="119:123	Our goal throughout the acquisition task is to eliminate as many incorrect assignments as possible while preserving the correct assignments, and in this respect we are encouraged by the the behavior of the semantic filter on 'unknown' verbs." ></td>
	<td class="line x" title="120:123	Recall that to assess this behavior, we excluded randomly selected Levin verbs from the semantic filter, and saw how the filter behaved on these verbs." ></td>
	<td class="line x" title="121:123	Acknowledgements The research reported herein was supported, in part, by Army Research Office contract DAAL0391-C-0034 through Battelle Corporation, NSF NYI IRI-9357731, Alfred P. Sloan Research Fellow Award BR3336, and a General Research Board Semester Award." ></td>
	<td class="line x" title="122:123	We would like to thank Julie Dahmer, Charles Lin, and David Woodard for their help in annotating the verbs." ></td>
	<td class="line x" title="123:123	We would also like to thank Karen Kohl for permission to use her WordNet annotations for Part One of Levin's book as hints for WordNet senses for Part Two." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1021
Large-Scale Acquisition Of LCS-Based Lexicons For Foreign Language Tutoring
Dorr, Bonnie Jean;"></td>
	<td class="line x" title="1:163	Large-Scale Acquisition of LCS-Based Lexicons for Foreign Language Tutoring Bonnie J. Dorr Department of Computer Science University of Maryland College Park, MD 20742, USA bonniecs, urad." ></td>
	<td class="line x" title="2:163	edu Abstract We focus on the probleln of building large repositories of le.rical coJtceplual structure (LCS) representations for verbs in multiple languages." ></td>
	<td class="line x" title="3:163	One of the main results of this work is the definition of a relat, ion between broad semantic classes and LCS meaniug components." ></td>
	<td class="line x" title="4:163	Our acquisition program--LEXICALL--takes, as input, the result of previous work on verb classification and thematic grid tagging, and outputs LCS representations for different." ></td>
	<td class="line x" title="5:163	languages." ></td>
	<td class="line x" title="6:163	These representations have been ported into English, Arabic and Spanish lexicons, each containing approximately 9000 verbs." ></td>
	<td class="line x" title="7:163	We are currently using these lexicons in an operational foreign language tutoring and machine translation." ></td>
	<td class="line x" title="8:163	1 Introduction A wide range of new capabilities in NLP applications such as foreign language tutoring (FLT) has been made possible by recent advances in lexica.1 semantics (Carrier and Randall, 1993; Dowty, 1991; Fillmore, 1968; Foley and Van Valin, 1984; Grimshaw, 1990; Gruber, 1965; Hale and Keyser, 1993; Jackendoff, 1983; aackendoff, 1990: Jackendoff, 1996; Levin, 1993; Levin and Rappaport Hovav, To appear; Pesetsky, 1982; Pinker, 1989)." ></td>
	<td class="line x" title="9:163	Many of these researchers adopt the hypothesis that verbs can be grouped into broad classes, each of which corresponds to some combination of basic meaning con> ponents." ></td>
	<td class="line x" title="10:163	This is the basic premise underlying our approach to multilingual lexicon construction." ></td>
	<td class="line x" title="11:163	In particular, we have organized verbs into broad selnantic classes and subsequently designed a set of le,ical conceptual structures (LC, S), for each class." ></td>
	<td class="line x" title="12:163	These representations have been ported into English, Arabic, and Spanish lexicons, each containing approximately 9000 verbs." ></td>
	<td class="line x" title="13:163	An example of a NLP application for which these lexicons are currently in use is an operational foreign language tutoring (FLT) system called Military Language Tutor (MILT)." ></td>
	<td class="line x" title="14:163	This system provides a wide range of lessons for use in language training." ></td>
	<td class="line x" title="15:163	One of the tutoring lessons, the MicroWorld Lesson (see Figure 1) requires the capability of the languagelearner to state domain-specific actions in a variety of different ways." ></td>
	<td class="line x" title="16:163	For example, the language-learner might connnand the agent (pictured at the left in the graphical interface) to take the following action: Walt' to the table and pick up the document." ></td>
	<td class="line x" title="17:163	The same action should be taken if the user says: Go to the table and remove document, Retrieve the document from the table, etc. The LCS representation provides the capability to execute various forms of the same command without hardcoding them as part, of the graphical interface." ></td>
	<td class="line x" title="18:163	In another tutoring lesson, Question-Answering, the student is asked to answer questions about a foreign language text that they have read." ></td>
	<td class="line x" title="19:163	Their answer is converted into an LCS which is matched against a prestored LCS corresponding to an answer typed in by a human instructor (henceforth, called the 'author')." ></td>
	<td class="line x" title="20:163	The prestored LCS is an idealized form of the answer to a question, which can take one of many forms." ></td>
	<td class="line x" title="21:163	Suppose, for example, the question posed to the user is: Where did Jack put the book'?" ></td>
	<td class="line x" title="22:163	(or Addnde paso Jack el libro?" ></td>
	<td class="line x" title="23:163	in Spanish)." ></td>
	<td class="line x" title="24:163	The author's answer, e.g., Jack put the book in the trash, has been stored as an LCS by the tutoring system." ></td>
	<td class="line x" title="25:163	If the student types Jack threw the book in the trash, or Jack moved the book from the table into the trash, the system is able to nautch against the prestored LCS and determine that all three of these responses are semantically appropriate." ></td>
	<td class="line x" title="26:163	We have developed an acquisition program-LEXICALL--that allows us to construct LCS-based lexicons for the FLT system." ></td>
	<td class="line x" title="27:163	This program is designed to be used for multiple languages, and also for other NLP applications (e.g. , machine translation)." ></td>
	<td class="line x" title="28:163	One of the main results of this work is the definition of a relation between broad semantic classes (based on work by Levin (1993)) and LCS meaning components." ></td>
	<td class="line x" title="29:163	We build on previous work, where verbs were classified automatically (Doff and.Jones, 1996: 139 Figure 1: MicroWorld Lesson in MILT x~ #, ~ .~ ~,a ~ ~ ~ ~:': ::: : Dorr, To appear) and tagged with thematic grid information (Dorr, Garman, and Weinberg, 1995)." ></td>
	<td class="line x" title="30:163	We use these pre-assigned classes and thematic grids as input to LEXICALL." ></td>
	<td class="line x" title="31:163	The output is a set of LCS's corresponding to individual verb entries in our lexicon." ></td>
	<td class="line oc" title="32:163	Previous research in automatic acquisition focuses primarily on the use of statistical techniques, such as bilingual alignment (Church and Hanks, 1990; Klavans and Tzoukermann, 1995; Wu and Xia, 1995) or extraction of syntactic constructions from online dictionaries and corpora (Brent, 1993)." ></td>
	<td class="line x" title="33:163	Others have taken a more knowledge-based (interlingual) approach (Lonsdale, Mitamura, and Nyberg, 1995)." ></td>
	<td class="line x" title="34:163	Still others (Copestake et al 1995), use Englishbased grammatical codes for acquisition of lexical representations." ></td>
	<td class="line x" title="35:163	Our approach differs from these in that it exploits certain linguistic constraints that govern the relation between a word's surface behavior and its corresponding semantic class." ></td>
	<td class="line x" title="36:163	We delnonstrate that-by assigning a LCS representatioll to each semantic class--we can produce verb entries on a broad scale; these, in turn, are ported into multiple languages." ></td>
	<td class="line x" title="37:163	We first show how the LCS is used in a FLT system." ></td>
	<td class="line x" title="38:163	We then present an overview of the LCS acquisition process." ></td>
	<td class="line x" title="39:163	Finally, we describe how LEXICALL constructs entries for specific lexical items." ></td>
	<td class="line x" title="40:163	2 Application of the LCS Representation to FLT One of the types of knowledge that must be captured in FLT is linguistic knowledge at the level of the lexicon, which covers a wide range of information types such as verbal subcategorization for events (e.g. , that a transitive verb such as hit occurs with an object noun phrase), featural information (e.g. , that the direct object of a verb such as frighlen is animate), thematic information (e.g. , that Mary is the agent in Mary hie the ball), and lexical-semantic information (e.g. , spatial verbs such as throw are conceptually distinct fi'om verbs of possession such as give)." ></td>
	<td class="line x" title="41:163	By modularizing the lexicon, we treat each information type separately, thus allowing us to vary the degree of dependence on each level so that we can address the question of how much knowledge is necessary for the success of the particular NLP application." ></td>
	<td class="line x" title="42:163	This section describes the use of the LCS representation in a question-answering component of the MILT system (Sains, 1993; Weinberg et al. , 1995)." ></td>
	<td class="line x" title="43:163	As described above, the LCS representation is used as the basis of matching routines for assessing students' answers to free response questions about a short foreign language passage." ></td>
	<td class="line x" title="44:163	In order to inform the student whether a question has been answered 140 Table 1: Correspondence Between NLP Output and Tutor Feedback System Prompt: Where did Jack put the book?" ></td>
	<td class="line x" title="45:163	Student Answer Prestored Answer Matcher Output Feedback Jack threw the book in the trash Jack threw the book in the trash exact match 'That's right' Jack put the book in the trash Jack threw the book in the trash missing MANNER 'How'?" ></td>
	<td class="line x" title="46:163	.Jack threw the book in the trash Jack put the book in the trash extra MANNER 'You're assuming things' .Jack is friendly Jack put the book in the trash mismatch primitive 'Please reread' Jack threw the book Jack put the book in the trash missing argument 'Where'?" ></td>
	<td class="line x" title="47:163	correctly, the author of the lesson must provide the desired response in advance." ></td>
	<td class="line x" title="48:163	The system parses and semantically analyzes the author's response into a corresponding LCS representation which is then prestored in a database of possible responses." ></td>
	<td class="line x" title="49:163	Once the question answering lesson is activated, each of the student's responses is parsed and semantically analyzed into a LCS representation which is checked for a match against the corresponding prestored LCS representation." ></td>
	<td class="line x" title="50:163	The student is then informed as to whether the question has been answered correctly depending on how closely the student's response LCS matches the author's prestored LCS." ></td>
	<td class="line x" title="51:163	Consider what happens in a lesson if the author has specified that a correct answer to the question Addnde paso Jack el libro?" ></td>
	<td class="line x" title="52:163	in Spanish is Jack fir6 el libro a la basura ('Jack threw out the book into the trash')." ></td>
	<td class="line x" title="53:163	This answer is processed by the system to produce the following LCS: (1) \[E,'~nt CAUSE (\[Thing JACK\], \[Evt GOLo~ (\[Thing BOOK\], \[P~th TOLo~ (\[Position ATLoc (\[Thing BOOK\], \[Thing TRASH\])\])\])\], \[M  THROWINGLY\])\] The LCS is stored by the tutor and then later matched against the student's answer." ></td>
	<td class="line x" title="54:163	If the student types Jack movio ' el libro de la mesa a la basura ('Jack moved the book froln the table to the trash'), the system must determine if these two match." ></td>
	<td class="line x" title="55:163	The student's sentence is processed and the following LCS structure is produced: (2) \[E  CAUSE (\[Thing JACK\], \[Event GOLoc (\[Thing BOOK\], \[Path ZOLoc (\[Position ATLo (\[Thing BOOK\], \[Thing TRASH\])\])\], \[Path FROMLo~ (\[Position ATLo~ (\[Thing BOOK\], \[Thin~; TABLE\])\])\])\])\] The matcher compares these two, and produces the following output: Missing: MANNER THROWINGLY Extra: FROM LOC The system identifies the student's response as a match with the prestored answer, but it also recognizes that there is one piece of missing information and one piece of extra information." ></td>
	<td class="line x" title="56:163	The 'Missing' and 'Extra' output is internal to the NLP component of the Tutor, i.e., this is not the final response displayed to the student." ></td>
	<td class="line x" title="57:163	The system must convert, this information into meaningful feedback so that the student knows how to repair the answer that was originally given." ></td>
	<td class="line x" title="58:163	For example, the instructor can program the tutor to notify the student about the omitted information in the form of a 'How' question, or it can choose to ignore it." ></td>
	<td class="line x" title="59:163	The extra information is generally ignored, although it is recorded in case the instructor decides to program the system to notify the student about this as well." ></td>
	<td class="line x" title="60:163	The full range of feedback is not presented here." ></td>
	<td class="line x" title="61:163	Some possibilities are summarized (in English) in Table 1 (adapted from (Holland, 1994))." ></td>
	<td class="line x" title="62:163	Note that." ></td>
	<td class="line x" title="63:163	the main advantage of using the LCS is that it allows the author to type in an answer that is general enough to match any number of additional answers." ></td>
	<td class="line x" title="64:163	3 Overview of LCS Acquisition We use Levin's publicly available online index (Levin, 1993) as a starting point for building LCSbased verb entries." ></td>
	<td class="line x" title="65:163	1 While this index provides a unique and extensive catalog of verb classes, it does not define the underlying meaning components of each class." ></td>
	<td class="line x" title="66:163	One of the main contributions of our work is that it provides a relation between Levin's classes and meaning components as defined in the LCS representation." ></td>
	<td class="line x" title="67:163	Table 2 shows three broad semantic categories and example verbs along with their associated LCS representations." ></td>
	<td class="line x" title="68:163	We have band-constructed a database containing 191 LCS templates, i.e., one for each verb class in (Levin, 1993)." ></td>
	<td class="line x" title="69:163	In addition, we have genera.ted LCS templates for 26 additional classes that are not included in Levin's system." ></td>
	<td class="line x" title="70:163	Several of these correspond to verbs that take sentential complements (e.g. , coerce)." ></td>
	<td class="line x" title="71:163	1We focus on building entries for verbs; however, we have approximately 30,000 non-verb entries per language." ></td>
	<td class="line x" title="72:163	141 Category Verb Location suspend touch Motion abandon float Placement adorn spill Table 2: Sample Templates Stored in the LCS Database Class Grid 9.2,ag_th,loc() 47.8 thloc 51.2 _th,src 51.3.1 th,src(),goal() 9.8 _ag th,raod-poss (with) 9.5, agth Ko\]I \[CAUSE (X, \[BELo (Y, \[ATLo (Y, Z)\])\], \[BY (MANNER)\])\] \[BELo (Y,\[ATLo~ (Y, Z)\], \[BY (MANNER}\])\] \[GOLo~ (Y, \[(DIRECTION)Lo (Y, \[ATLo (Y, Z)\])\])\] \[GOLo (Y, \[BY (MANNER)\])\] \[CAUSE (X, \[GOIdent (Y, \[TOWARDId~t (Y, \[ATIdn~ (Y, \[(STATE)Id~nt (\[(WITH>po~ (*HEAD*, Z)\])\])\])\])\])\] \[CAUSE (X, \[GOLo (Y)\], \[BY (MANNER)\])\] A full entry in the dal:abase includes a semantic class number with a list of possible verbs, a thematic grid, and a LCS template: (3) Class 47.8: adjoin, intersect., meet, touch  Thematic Grid: _th_loc LCS Template: (be loc (thing 2) (at loc (thing 2) (thing 11)) ( ! !" ></td>
	<td class="line x" title="74:163	-ingly 26) ) The semantic class label 47.8 above is taken from Levin's 1993 book (Verbs of Contiguous Location), i.e., the class to which the verb touch has been assigned." ></td>
	<td class="line x" title="75:163	2 A verb, together with its semantic class uniquely identifies the word sense, or LCS template, to which the verb refers." ></td>
	<td class="line x" title="76:163	The thematic grid (_th_loc) indicates that the verb has two obligatory arguments, a theme and a location." ></td>
	<td class="line x" title="77:163	3 The !!" ></td>
	<td class="line x" title="78:163	in the LCS Template acts as a wildcard; it will be filled by a lexeme (i.e. , a root form of the verb)." ></td>
	<td class="line x" title="79:163	The resulting form is called a constant, i.e., the idiosyncratic part of the meaning that distinguishes among members of a verb class (in the spirit of (Grimshaw, 1993; Levin and Rappaport Hovav, To appear; Pinker, 1989; Talmy, 1985))." ></td>
	<td class="line x" title="80:163	4 Three inputs are required for acquisition of verb entries: a semantic class, a thematic grid, and a lexeme, which we will henceforth abbreviate as 'class/grid/lexeme'." ></td>
	<td class="line x" title="81:163	The output is a Lisp-like expression corresponding to the LCS representation." ></td>
	<td class="line x" title="82:163	An example of input/output for our acquisition procedure is shown here: (4) Acquisition of LCS for: touch Input: 47.8: _th_loc; 'touch' 2Verbs not occurring in Levin's book are also assigned to classes using techniques described in {Dorr and Jones, 1996; Dorr, To appear)." ></td>
	<td class="line x" title="83:163	ZAn underscore (_) designates an obligatory role and a comma (,) designates an optional role." ></td>
	<td class="line x" title="84:163	4The ! !" ></td>
	<td class="line x" title="85:163	in the Lisp representation corresponds to the angle-bracketed constants ill Table 2, e.g., ! !-ingly corresponds to (MANNER}." ></td>
	<td class="line x" title="86:163	Output: (be loc (* thing 2) (at loc (thing 2) (* thing 11)) (touchingly 26) ) Language-specific annotations such as the .-,uarker in the LCS Output are added to the templates by processing the components of thematic grid specifications, as we will see in more detail next." ></td>
	<td class="line x" title="87:163	4 Language-Specific Annotations In our on-going example (4), the thematic grid _th loc indicates that the theme and the location are both obligatory (in English) and should be annotated as such in the instantiated LCS." ></td>
	<td class="line x" title="88:163	This is achieved by inserting a *-marker appropriately." ></td>
	<td class="line x" title="89:163	Consider the structural divergence between the following English/Spanish equivalents: (5) Structural Divergence: E: John entered the house." ></td>
	<td class="line x" title="90:163	S: John entr6 a la casa." ></td>
	<td class="line x" title="91:163	'John entered into the house'." ></td>
	<td class="line x" title="92:163	The English sentence differs structurally from the Spanish in that the noun phrase the house corresponds to a prepositional phrase a la casa." ></td>
	<td class="line x" title="93:163	This distinction is characterized by different positionings of the *-marker in the lexical entries produced by LEXICALL: (6) Lexical Entries: enter: (go loc (* thing 2) (toward loc (thing 2) (in loc (thing 2) (* thing 6))) (enteringly 26) ) entrar: (go loc (* thing 2) ((* toward 5) loc (thing 2) (in loc (thing 2) (thing 6))) (enteringly 26) ) The lexicon entries for enter and entrar both mean 'X (= Thing 2) goes into location Y (= Thing 6)'." ></td>
	<td class="line x" title="94:163	Variable positions (designated by numbers, such as 2, 5 and 6) are used in place of the ultimate fillers 142 such as john and house." ></td>
	<td class="line x" title="95:163	The structural divergence of (,5) is a.ccomnaodated as follows: the *-marked leaf node, i.e., (thing 6) in the enter definition, is filled directly, whereas the .-marked non-leaf node, i.e., ((toward 5) loc ) in the enrar definition, is filled in through unification at the internal toward node." ></td>
	<td class="line x" title="96:163	5 Construction of Lexical Entries C.onsider the construction of a lexical entry for the verb adorn." ></td>
	<td class="line x" title="97:163	The LC, S for this verb is in the class of Fill Verbs (9.8): s (7) (cause (thing 1) (go ident (thing 2) (toward ident (thing 2) (at ident (thing 2) (!!-ed 9)))) (with poss (*head*) (thing 16))) This list structure recursively associates logical heads with their arguments and modifiers." ></td>
	<td class="line x" title="98:163	The logical head is represented as a primitive/field Colnbination, e.g., GOIdent is represented as (go ident )." ></td>
	<td class="line x" title="99:163	The arguments for CAUSE are (thing 1) and (go ident )." ></td>
	<td class="line x" title="100:163	The substructure GO itself has two arguments (thing 2) and (toward ident ) and a modifier (with poss ).6 The ! !-ed constant refers to a resulting state, e.g., adorned for the verb adorn." ></td>
	<td class="line x" title="101:163	The LC.S produced by our program for this verb is: (8) (cause (thing 1) (go ident (thing 2) (toward ident (thing 2) (at ident (thing 2) (adorned 9)))) (with poss (*head*) (thing 16))) The variables in the representation map between LCS positions and their corresponding thematic roles." ></td>
	<td class="line x" title="102:163	In the LCS framework, thematic roles provide semantic information about properties of the argument and modifier structures." ></td>
	<td class="line x" title="103:163	In (7) and (8) above, the numbers 1, 2, 9, and 16 correspond to the roles agent (ag), theme (th), predicate (pred), and possessional modifier (mod-poss), respectively." ></td>
	<td class="line x" title="104:163	These numbers enter into the construction of LCS entries: they correspond to argument positions in the LCS template (extracted using the class/grid/lexeme specification), hfformatiou is filled into the LCS template using these numbers, coupled with the thematic grid tag for the particular word being defined." ></td>
	<td class="line x" title="105:163	5.1 Pundmnentals LEXICALL locates the appropriate template in the LCS database using the class/grid pairing as an in5Some of the other 9.8 verbs are: anoint, bandage." ></td>
	<td class="line x" title="106:163	flood, frame, garland, stud, s~@~se, surround, veil." ></td>
	<td class="line x" title="107:163	6The *head* symbol--used for modifiers--is a placeholder that points to the root (cause) of the overall lexicaJ entry." ></td>
	<td class="line x" title="108:163	dex, and then determines the language-specifc annotations to instantiate for that template." ></td>
	<td class="line x" title="109:163	The default position of the .-marker is the left-most occurrence of the LCS node corresponding to a particula.r thematic role." ></td>
	<td class="line x" title="110:163	However, if a preposition occurs in the grid, the .-marker may be placed differently." ></td>
	<td class="line x" title="111:163	In such a. case, a. primitive representation (e.g. , (to loc (at loc))) is extracted from a set of predefined mappings." ></td>
	<td class="line x" title="112:163	If this representation corresponds to a subcomponent of the LCS template, the program recognizes this as a match against the grid, and the .-marker is placed in the template at the level where this match occurs (as in the entry for entrar given in (6) above)." ></td>
	<td class="line x" title="113:163	If a preposition occurs in the grid but there is no matching primitive representation, the preposition is considered to be a. collocation, and it is placed in a special slot--:collocations--which indicates that the LCS already covers the semantics of the verb and the preposition is an idiosyncratic variation (as in learn about, know of, etc.)." ></td>
	<td class="line x" title="114:163	If a preposition is required but it is not specified (i.e. , empty parentheses 0), then the .-marker is positioned at the level dominating the node that corresponds to that role--which indicates that several different prepositions might apply (as in put on, put under, put through, etc.)." ></td>
	<td class="line x" title="115:163	5.2 Examples The input to LEXICALL is a class/grid/lexeme specification, where each piece of information is separated by a hash sign (#): <class>#<grid>#<lexeme># <other semantic information> For example, the input specification for the verb replant (a word not classified by Levin) is: 9.7#_ag_th,mod-poss(with)#replant# !!-ed = planted (manner = again) This input indicates that the class assigned to replant is 9.7 (Levin's Spray/Load verbs) and its grid has a.n obligatory agent (ag), theme (tit), and all optional possessional modifer with preposition with (mod-poss (with) )." ></td>
	<td class="line x" title="116:163	The information following the final # is optional; this information was previously hand-added to the assigned thematic grids." ></td>
	<td class="line x" title="117:163	In the current example, the !!-ed designates the form of the constant planted which, in this case, is a morphological variant of the lexeme replant, r Also, the rThe constant takes one of several forms, including: ! !-ingly for a manner, ! !-er for an instrument, and !!-ed for resulting states." ></td>
	<td class="line x" title="118:163	If this information has not been hand-added to the class/grid/lexeme specification (as is the case with most of the verbs), a default morphological process produces the appropriate form from tile lexeme." ></td>
	<td class="line x" title="119:163	143 manner again is specified as an additional semantic coin ponent." ></td>
	<td class="line x" title="120:163	For presentational purposes, the remainder of this section uses English examples." ></td>
	<td class="line x" title="121:163	However, as we saw in Section 4, the representations used here carry over to other languages a.s well." ></td>
	<td class="line x" title="122:163	In fact, we have used the same acquisition program, without modification, for building our Spanish and Arabic LCS-based lexicons, each of size comparable to our English LCSbased lexicon." ></td>
	<td class="line x" title="123:163	I. Thematic Roles without Prepositions (9) Example: The flower decorated the room." ></td>
	<td class="line x" title="124:163	Input: 9.8#_mod-poss_th#decorate# Template: (be ident (thing 2) (at ident (thing 2) (!!-ed 9)) (with poss (*head*) (thing 16))) Two thematic roles, th and mod-poss, are specified for the above sense of the English verb decorate." ></td>
	<td class="line x" title="125:163	The thematic code numbers--2 and 16, respectively--are .-marked and the constant decorated replaces the wildcard: (10) Output: (be ident (* thing 2) (at ident (thing 2) (decorated 9)) (with poss (*head*) (* thing 16))) II." ></td>
	<td class="line x" title="126:163	Thematic Roles with Unspecified Prepositions (11) Example: We parked the car near the store." ></td>
	<td class="line x" title="127:163	We parked the car in the garage." ></td>
	<td class="line x" title="128:163	Input: 9." ></td>
	<td class="line x" title="129:163	l#_ag_th_goal ( ) #park# Template: (cause (thing 1) (go loc (thing 2) (toward loc (thing 2) (\[at\] loc (thing 2) (thing 6)))) ( ! !" ></td>
	<td class="line x" title="130:163	-ingly 26) ) The input for this example indicates that the goal is headed by an unspecifed preposition." ></td>
	<td class="line x" title="131:163	The thematic roles ag, th, and goal() correspond to code numbers 1, 2, and 6, respectively." ></td>
	<td class="line x" title="132:163	The variable positions for ag and th are .-marked just as in the previous case, whereas goal() requires a different treatment." ></td>
	<td class="line x" title="133:163	When a required preposition is left." ></td>
	<td class="line x" title="134:163	unspecified, the .-marker is associated with a LCS node dominating a generic \[at\] position: (12) Output: (cause (* thing 1) (go loc (* thing 2) ((* toward S) loc (thing 2) (\[at\] loc (thing 2) (thing 6)))) (parkingly 26) ) } III." ></td>
	<td class="line x" title="135:163	Thematic roles with Specified Prepositions (13) Example: We decorated the room with flowers." ></td>
	<td class="line x" title="136:163	Input: 9.8#_ag_th,mod-poss (with) #decorate# Template: (cause (thing 1) (go ident (thing 2) (toward ident (thing 2) (at ident (thing 2) (!!-ed 9)))) (with poss (*head*) (thing 16))) Here, the mod-poss role requires the preposition 'w~th in the modifier position: (14) Output: (cause (* thing 1) (go ident (* thing 2) (toward ident (thing 2) (at ident (thing 2) (decorated 9)))) ((* with 15) poss (*head*) (thing 16))) In order to determine the position of the .-marker for a thematic role with a required preposition, LEXICALL consults a set of predefined mappings between prepositions (or postpositions, in a language like Korean) and their corresponding primitive representations, s In the current case, the preposition with is mapped to the following primitive representation: (with poss)." ></td>
	<td class="line x" title="137:163	Since this matches a sub-component of the LCS template, the program recognizes this as a match against the grid, and the .-marker is placed in the template at the level of with." ></td>
	<td class="line x" title="138:163	6 Limitations and Conclusions We have described techniques for automatic construction of dictionaries for use in large-scale FLT." ></td>
	<td class="line x" title="139:163	The dictionaries are based on a languageindependent representation called lexical conceptual structure (LCS)." ></td>
	<td class="line x" title="140:163	Significant enhancements to LCSbased tutoring could be achieved by combining this representation with a mechanism for handling issues related to discourse and pragmatics." ></td>
	<td class="line x" title="141:163	For example, Mthough the LCS processor is capable of determining that the phrase in the trash partially matches the answer to Where did John put the book?, a pragmatic component would be required to determine that this answer is (perhaps) more appropriate than the full answer, He put the book in the trash." ></td>
	<td class="line x" title="142:163	Representing conversational context and dynamic context updating (Traum et al. , 1996; Haller, 1996; DiEugenio and Webber, 1996) would provide a fl'amework for this type of response 'relaxation'." ></td>
	<td class="line x" title="143:163	Along SWe have defined approximately 100 such mappings per language." ></td>
	<td class="line x" title="144:163	For example, the mapping produces the following primitive representations for the English word to: (to loc (at loc)), (to poss (at poss)), (to temp (at temp)), (toward loc (at loc)), (toward poss (at poss))." ></td>
	<td class="line x" title="145:163	We have similar mappings defined in Arabic and Spanish." ></td>
	<td class="line x" title="146:163	For example, the following primitive representations are produced for the Spanish word a: (at loc), (to loc (at loc)), (to poss (at poss)), (toward loc (at lot))." ></td>
	<td class="line x" title="147:163	144 these same lines, a pragmatic component could provide a mechanism for det, ermining that certain fully matched responses (e.g. , John hurled the book inlo the trash) are not." ></td>
	<td class="line x" title="148:163	as 'realistic sounding' as partially matched alternatives." ></td>
	<td class="line x" title="149:163	Initially, LEXICALL was designed to support the development of LCS's for English only; however, the same techniques can be used for nmltilingual acquisition." ></td>
	<td class="line x" title="150:163	As the lexicon coverage for other languages expands, it, is expected that our acquisition techniques will help further in the cross-linguistic investigation of the relationship between Levin's verb classes and the basic meaning components in the LCS represent, ation." ></td>
	<td class="line x" title="151:163	In addition, it is expected that verbs in the same Levin class may have finer distinctions than what we have specified in the current LCS templates." ></td>
	<td class="line x" title="152:163	We view the importation of LCS's from the English LCS database into Arabic and Spanish as a first, approxin~ation to the development of complete lexicons for these languages." ></td>
	<td class="line x" title="153:163	The results have been hand-checked by native speakers using the class/grid/lexeme format (which is much easier to check than the flfily expanded LCS's)." ></td>
	<td class="line x" title="154:163	The lexical verification process took only two weeks by the native speakers." ></td>
	<td class="line x" title="155:163	We estimate that, it would take at least 6 months to build such a lexicon from scratch (by human recall and data." ></td>
	<td class="line x" title="156:163	entry alone), and in such a case, the potential for error would be a.t least twice as high." ></td>
	<td class="line x" title="157:163	One important benefit of using the Levin classification as the basis of our program is that, once the mapping between verb classes and LCS representations has been established, we can acquire the LCS representation for a new verb (i.e. , one not in Levin) simply by associating it." ></td>
	<td class="line x" title="158:163	with one of the 191 classes." ></td>
	<td class="line x" title="159:163	We see our approach as a first step toward compression of lexical entries in that it allows lexicons to be stored in terms of the more condensed class/grid/lexeme specifications; these can expanded online, as needed, during sentence processing in the NLP application." ></td>
	<td class="line x" title="160:163	We conclude that, while human intervention is necessary for the acquisition of class/grid information, this intervention is virtually eliminated fi'om the LCS construction process because of our provision of a lnapping between semantic classes and primitive meaning components." ></td>
	<td class="line x" title="161:163	Acknowledgements I would like t.o thank Jungshin Park and Mine Ulku Sencan for their aid in the development of certain components of the LEXICALL program." ></td>
	<td class="line x" title="162:163	In addition, comments from five anonymous reviewers greatly enhanced the presentation of this work." ></td>
	<td class="line x" title="163:163	The author has been supported, in part, by Army Research Office contract DAAL03-91-C-0034 through Battelle Corporation, NSF NYI IRI-9357731 and Logos Corporation, NSF CNRS INT-9314583, Advanced Research Projects Agency and ONR contract N00014-92-J-1929, Alfred P. Sloan Research Fellow Award BR3336, Army Research Institute contract MDA-903-92-R-0035 and Microelectronics and Design, Inc. , and the University of Maryland General Research Board." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="A97-1045
Construction And Visualization Of Key Term Hierarchies
Zhou, Joe F.;Tanner, Troy;"></td>
	<td class="line x" title="1:100	Construction and Visualization of Key Term Hierarchies Joe Zhou and Troy Tanner LEXIS-NEXIS, a Division of Reed Elsevier 9555 Springboro Pike Miamisburg, OH 45342 {joez, tit} @ lexis-nexis.com Abstract This paper presents a prototype system for key term manipulation and visualization in a real-world commercial environment." ></td>
	<td class="line x" title="2:100	The system consists of two components." ></td>
	<td class="line x" title="3:100	A preprocessor generates a set of key terms from a text dataset which represents a specific topic." ></td>
	<td class="line x" title="4:100	The generated key terms are organized in a hierarchical structure and fed into a graphic user interface (GUI)." ></td>
	<td class="line x" title="5:100	The friendly and interactive GUI toolkit allows the user to visualize the key terms in context and explore the content of the original dataset." ></td>
	<td class="line x" title="6:100	1." ></td>
	<td class="line x" title="7:100	INTRODUCTION As the amount of on-line text grows at an exponential rate, developing useful text analysis techniques and tools to access information content from various electronic sources is becoming increasingly important." ></td>
	<td class="line x" title="8:100	In this paper we present an applied research prototype system that intends to accomplish two major tasks." ></td>
	<td class="line x" title="9:100	First, a set of key terms, ranging from single word terms to four word terms, are automatically generated and organized in a hierarchical structure out of a text dataset which represents a specific topic." ></td>
	<td class="line x" title="10:100	Second, a graphic user interface (GUI) is established that provides the domain expert or the user with an interactive environment to visualize the key term hierarchy in the context of the original dataset." ></td>
	<td class="line x" title="11:100	2." ></td>
	<td class="line x" title="12:100	SYSTEM DESCRIPTION The ultimate goal of this prototype system is to offer an automated toolkit which allows the domain expert or the user to visualize and examine key terms in a large information collection." ></td>
	<td class="line x" title="13:100	Such a toolkit has proven to be useful in a number of real applications." ></td>
	<td class="line x" title="14:100	For example, it has helped us reduce the time and manual effort needed to develop and maintain our on-line document indexing and classification schemes." ></td>
	<td class="line x" title="15:100	The system consists of two components: a preprocessing component for the automatic construction of key terms and the front-end component for userguided graphic interface." ></td>
	<td class="line x" title="16:100	2.1 Automatic Generation of Key Terms Automatically identifying meaningful terms from naturally running texts has been an important task for information technologists." ></td>
	<td class="line x" title="17:100	It is widely believed that a set of good terms can be used to express the content of the document." ></td>
	<td class="line x" title="18:100	By capturing a set of good terms, for example, relevant documents can be searched and retrieved from a large document collection." ></td>
	<td class="line x" title="19:100	Though what constitutes a good term still remains to be answered, we know that a good term can be a word stem, a single word, a multiple word term (a phrase), or simply a syntactic unit." ></td>
	<td class="line x" title="20:100	Various existing and workable term extraction tools are either statistically driven, or linguistically oriented, or some hybrid of the two." ></td>
	<td class="line x" title="21:100	They all target frequently co-occurring words in running text." ></td>
	<td class="line x" title="22:100	The earlier work of Choueka (1988) proposed a pure frequency approach in which only quantitative selection criteria were established and applied." ></td>
	<td class="line oc" title="23:100	Church and Hanks (1990) introduced a statistical measurement called mutual information for extracting strongly associated or collocated words." ></td>
	<td class="line n" title="24:100	Tools like Xtract (Smadja 1993) were based on the work of Church and others, but made a step forward by incorporating various statistical measurements like z-score and variance of distribution, as well as shallow linguistic techniques like part-of-speech tagging and lemmatization of input data and partial parsing of raw output." ></td>
	<td class="line x" title="25:100	Exemplary linguistic approaches can be found in the work by Str-zalkowsky (1993) where a fast and accurate syntactic parser is the prerequisite for the selection of significant phrasal terms." ></td>
	<td class="line x" title="26:100	Different applications aim at different types of key terms." ></td>
	<td class="line x" title="27:100	For the purpose of generating key terms for our prototype system, we have adopted a =learn data from data' approach." ></td>
	<td class="line x" title="28:100	The novelty of this 307 approach lies in the automatic comparison of two sample datasets, a topic focused dataset based on a predefined topic and a larger and more general base dataset." ></td>
	<td class="line x" title="29:100	The focused dataset is created by the domain expert either through a submission of an on-line search or through a compilation of documents from a specific source." ></td>
	<td class="line x" title="30:100	The construction of the corresponding base dataset is performed by pulling documents out of a number of sources, such as news wires, newspapers, magazines and legal databases." ></td>
	<td class="line x" title="31:100	The intention is to make the resulted corpora cover a much greater variety of topics or domain subjects than the focused dataset." ></td>
	<td class="line x" title="32:100	To identify interesting word patterns in both samples a set of statistical measures are applied." ></td>
	<td class="line x" title="33:100	The identification of single word terms is based on the variation of a t-test." ></td>
	<td class="line x" title="34:100	Two-word terms are captured through the computation of mutual information (Church et al. 1991), and an extension of mutual information assists in extracting three-word and four-word terms." ></td>
	<td class="line x" title="35:100	Once the significant terms of these four types are identified, a comparison algorithm is applied to differentiate terms across the two samples." ></td>
	<td class="line x" title="36:100	If significant changes in the values of certain statistical variables are detected, associated terms are selected from the focused sample and included in the final generated lists." ></td>
	<td class="line x" title="37:100	(For a complete description of the algorithm and preliminary experiments, please refer to Zhou and Dapkus 1995)." ></td>
	<td class="line x" title="38:100	2.2 Graphic User Interface (GUI) We view our prototype system as a means to achieve information visualization." ></td>
	<td class="line x" title="39:100	Analogous to scientific visualization that allows scientists to make sense out of intellectually large data collections, information visualization aims at organizing large information spaces so that information technologists can visualize what is out there and how various parts are related to each other (Robertson et al. 1991)." ></td>
	<td class="line x" title="40:100	The guiding principle for building the GUI component of our prototype system is to automate the manual process of capturing information content out of large document collections." ></td>
	<td class="line x" title="41:100	2.2.1 General Presentation The design of the GUI component relies on a number of well understood elements which include a suggestive graphic design and a direct manipulation metaphor to achieve an easy-to-learn user interface." ></td>
	<td class="line x" title="42:100	The layout of the graphic design is intended to facilitate the quick comprehension of the displayed information." ></td>
	<td class="line x" title="43:100	The GUI component is divided into two main areas, one for interacting with key terms structures and one for browsing targeted document collections." ></td>
	<td class="line x" title="44:100	The following descriptions should be viewed together with the appropriate figures of the GUI component." ></td>
	<td class="line x" title="45:100	Figure 1, attached at the end of the paper, represents the overall GUI picture." ></td>
	<td class="line x" title="46:100	Figures 2 and 3 capture the area where the interaction with the key term structures occurs." ></td>
	<td class="line x" title="47:100	Figures 4 and 5 present the area for document browsing and key terms selection." ></td>
	<td class="line x" title="48:100	The topic illustrated in the figures is the legal topic =Medical Malpractice'." ></td>
	<td class="line x" title="49:100	2.2.2 Term Access Mechanism The left area of the GUI component (see figures 2 and 3) is devoted to selecting, retrieving and operating on the key terms generated by the preprocessing component of the prototype system." ></td>
	<td class="line x" title="50:100	As can be seen, the key terms, ranging from single word terms to four word terms, are organized in a tree structure." ></td>
	<td class="line x" title="51:100	The tree is a two dimensional visualization of the term hierarchy." ></td>
	<td class="line x" title="52:100	Single word terms are represented as root nodes and multiple word terms can be positioned uniformly below the parent node in the term hierarchy." ></td>
	<td class="line x" title="53:100	The goal of the visualization is to present the key term lists in such a way that a high percentage of the hierarchy is visible with minimal scrolling." ></td>
	<td class="line x" title="54:100	Figure 2 308 The user interaction is structured around term retrieval and navigation as the top level user interactions." ></td>
	<td class="line x" title="55:100	The retrieval of the key terms is treated as an iterative process in which the user may select single world terms from the term hierarchy and navigate to multiple word terms accordingly." ></td>
	<td class="line x" title="56:100	The user begins term navigation by selecting from a list of available topics." ></td>
	<td class="line x" title="57:100	In this case, the legal topic 'Medical Malpractice' (i.e. , medmal3) is selected (see figure 2)." ></td>
	<td class="line x" title="58:100	Often data structures are organized linearly by some metric." ></td>
	<td class="line x" title="59:100	Frequency of key term usage is the metric used to organize and partition the term hierarchy in an ascending numerical order." ></td>
	<td class="line x" title="60:100	The partitioning is necessary as it is difficult to accommodate the large ratio of the term hierarchy on the screen." ></td>
	<td class="line x" title="61:100	Currently, each partition contains 100 root nodes (or folders), representing single word terms." ></td>
	<td class="line x" title="62:100	Once a partition has been selected, the corresponding document collection is loaded into the document browser." ></td>
	<td class="line x" title="63:100	The browser provides the user with the ability to quickly navigate through the document collection to locate relevant key terms." ></td>
	<td class="line x" title="64:100	example, when =malpractice' is selected as the root key term, a list of multiple word terms will be displayed including multiple key terms such as 'medical malpractice', 'malpractice cases', 'medical malpractice action', 'medical malpractice claims', 'limitations for medical malpractice', etc.(see figure 3) Functionality to shrink and collapse subtrees is also in place." ></td>
	<td class="line x" title="66:100	When a term is selected from the tree, a corresponding term lookup is conducted on the document collection to locate the selected term within the currently displayed document." ></td>
	<td class="line x" title="67:100	Documents representing the four highest frequencies for the selected term will be displayed first." ></td>
	<td class="line x" title="68:100	Upon location the selected term is always highlighted within the document browser." ></td>
	<td class="line x" title="69:100	2.2.3 Document Browsing Mechanism The right area of the GUI component (see figures 4 and 5) is occupied by the document browser." ></td>
	<td class="line x" title="70:100	The design of the document browser is intended to provide an easy-to-learn interface for the management and manipulation of the document collection." ></td>
	<td class="line x" title="71:100	There are three subwindows: the document identifier window, the document window and the navigation window." ></td>
	<td class="line x" title="72:100	The document identifier window identifies the document that is currently displayed in the document window." ></td>
	<td class="line x" title="73:100	It shows the document id and the total frequency of the selected key term in the document collection." ></td>
	<td class="line x" title="74:100	The document window provides a view of the content of the targeted document (see figure 4)." ></td>
	<td class="line x" title="75:100	Figure 3 The primary interaction with the key term hierarchy is accomplished by direct manipulation of the tree visualization." ></td>
	<td class="line x" title="76:100	The user can select individual nodes in the tree structure by pointing and clicking the corresponding folders." ></td>
	<td class="line x" title="77:100	When selecting nodes with children, the tree will expand, resulting in the display of multiple word terms of the root key term." ></td>
	<td class="line x" title="78:100	For Figure 4 309 The user can move through the document by making use of the scroll bar, document buttons in the navigation window, or by dragging the mouse up and down while depressing the middle mouse button." ></td>
	<td class="line x" title="79:100	The user can copy relevant key terms to a holding area by selecting 'Edit' from the menubar." ></td>
	<td class="line x" title="80:100	The user is presented with a popup dialog for importing the selected key terms (see figure 5)." ></td>
	<td class="line x" title="81:100	The navigation window enables the user to navigate through the documents to view the selected key terms in context." ></td>
	<td class="line x" title="82:100	In addition, the user is provialed with information regarding term frequencies and term relevance ranking scores." ></td>
	<td class="line x" title="83:100	Figure 5 2.2.4 Implementation The GUI component described above is implemented using the C++ programing language and the OSF Motif graphical user interface toolkit." ></td>
	<td class="line x" title="84:100	The user interface consists of a small set of classes that play various roles in the overall architecture." ></td>
	<td class="line x" title="85:100	The two major objects of the user interface interaction model are the ListTree and the Document Store objects." ></td>
	<td class="line x" title="86:100	ListTree is the primary class for implementing the tree visualization." ></td>
	<td class="line x" title="87:100	Operations for growing, shrinking and manipulating the tree visualization have been implemented." ></td>
	<td class="line x" title="88:100	Document Store provides the interface to document collections." ></td>
	<td class="line x" title="89:100	In particular, a document store provides operations to create, modify and navigate document collections." ></td>
	<td class="line x" title="90:100	3." ></td>
	<td class="line x" title="91:100	RESULTS OF USABILITY TESTING The prototype system, despite its prototype mode, has proven to be useful and applicable in the commercial business environment." ></td>
	<td class="line x" title="92:100	Since the system is in place, we have conducted a series of usability testing within our company." ></td>
	<td class="line x" title="93:100	The preliminary results indicate that the system can provide internal specialized library developers, as well as subject indexing domain experts with an ideal automated toolkit to select and examine significant terms from a sample dataset." ></td>
	<td class="line x" title="94:100	A number of general topics have been tested for developing specialized libraries for our on-line search system." ></td>
	<td class="line x" title="95:100	These include four legal topics =State Tax ~, =Medical Malpractice', =Uniform Commercial Code', and =Energy ~, and three news topics =Campaign', =Legislature', and =Executives'." ></td>
	<td class="line x" title="96:100	Specific subject indexing topics that have been tested are =Advertising Expenditure', =lntranet', =Job interview' and =Mutual fund'." ></td>
	<td class="line x" title="97:100	Two sets of questionnaires were filled out by the domain experts who participated in the usability testing." ></td>
	<td class="line x" title="98:100	The overall ranking for the prototype system falls between 'somewhat useful' to =very useful', depending on the topics." ></td>
	<td class="line x" title="99:100	They pointed out that the system is particularly helpful when dealing with a completely new or unfamiliar topic." ></td>
	<td class="line x" title="100:100	It helps spot significant terms which would normally be missed and objectively examine the significance level of certain fuzzy and ambiguous terms." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P97-1007
Combining Unsupervised Lexical Knowledge Methods For Word Sense Disambiguation
Rigau, German;Atserias, Jordi;Agirre, Eneko;"></td>
	<td class="line x" title="1:168	Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation * German Rigau, Jordi Atserias Eneko Agirre Dept. de Llenguatges i Sist." ></td>
	<td class="line x" title="2:168	Informktics Lengoaia eta Sist." ></td>
	<td class="line x" title="3:168	Informatikoak saila Universitat Polit~cnica de Catalunya Euskal Herriko Unibertsitatea Barcelona, Catalonia Donostia, Basque Country {g. rigau, bat alla}@is i. upc." ></td>
	<td class="line x" title="4:168	es j ibagbee~s i. ehu." ></td>
	<td class="line x" title="5:168	es Abstract This paper presents a method to combine a set of unsupervised algorithms that can accurately disambiguate word senses in a large, completely untagged corpus." ></td>
	<td class="line x" title="6:168	Although most of the techniques for word sense resolution have been presented as stand-alone, it is our belief that full-fledged lexical ambiguity resolution should combine several information sources and techniques." ></td>
	<td class="line x" title="7:168	The set of techniques have been applied in a combined way to disambiguate the genus terms of two machine-readable dictionaries (MRD), enabling us to construct complete taxonomies for Spanish and French." ></td>
	<td class="line x" title="8:168	Tested accuracy is above 80% overall and 95% for two-way ambiguous genus terms, showing that taxonomy building is not limited to structured dictionaries such as LDOCE." ></td>
	<td class="line x" title="9:168	1 Introduction While in English the 'lexical bottleneck' problem (Briscoe, 1991) seems to be softened (e.g. WordNet (Miller, 1990), Alvey Lexicon (Grover et al. , 1993), COMLEX (Grishman et al. , 1994), etc)." ></td>
	<td class="line x" title="10:168	there are no available wide range lexicons for natural language processing (NLP) for other languages." ></td>
	<td class="line x" title="11:168	Manual construction of lexicons is the most reliable technique for obtaining structured lexicons but is costly and highly time-consuming." ></td>
	<td class="line x" title="12:168	This is the reason for many researchers having focused on the massive acquisition of lexical knowledge and semantic information from pre-existing structured lexical resources as automatically as possible." ></td>
	<td class="line x" title="13:168	*This research has been partially funded by CICYT TIC96-1243-C03-02 (ITEM project) and the European Comission LE-4003 (EuroWordNet project)." ></td>
	<td class="line x" title="14:168	As dictionaries are special texts whose subject matter is a language (or a pair of languages in the case of bilingual dictionaries) they provide a wide range of information about words by giving definitions of senses of words, and, doing that, supplying knowledge not just about language, but about the world itself." ></td>
	<td class="line x" title="15:168	One of the most important relation to be extracted from machine-readable dictionaries (MRD) is the hyponym/hypernym relation among dictionary senses (e.g.(Amsler, 1981), (Vossen and Serail, 1990) ) not only because of its own importance as the backbone of taxonomies, but also because this relation acts as the support of main inheritance mechanisms helping, thus, the acquisition of other relations and semantic features (Cohen and Loiselle, 1988), providing formal structure and avoiding redundancy in the lexicon (Briscoe et al. , 1990)." ></td>
	<td class="line x" title="17:168	For instance, following the natural chain of dictionary senses described in the Diccionario General Ilustrado de la Lengua Espadola (DGILE, 1987) we can discover that a bonsai is a cultivated plant or bush." ></td>
	<td class="line x" title="18:168	bonsai_l_2 planta y arbusto asi cultivado." ></td>
	<td class="line x" title="19:168	(bonsai, plant and bush cultivated in that way) The hyponym/hypernym relation appears between the entry word (e.g. bonsai) and the genus term, or the core of the phrase (e.g. planta and arbusto)." ></td>
	<td class="line x" title="20:168	Thus, usually a dictionary definition is written to employ a genus term combined with differentia which distinguishes the word being defined from other words with the same genus term 1." ></td>
	<td class="line x" title="21:168	As lexical ambiguity pervades language in texts, the words used in dictionary are themselves lexically ambiguous." ></td>
	<td class="line x" title="22:168	Thus, when constructing complete disambiguated taxonomies, the correct dictionary sense of the genus term must be selected in each dictionary :For other kind of definition patterns not based on genus, a genus-like term was added after studying those patterns." ></td>
	<td class="line x" title="23:168	48 DGILE overall headwords 93,484 senses 168,779 total number of words average length of definition 1,227,380 7.26 nouns 53,799 93,275 903,163 9.68 LPPL overall nouns 15,953 10,506 22,899 13,740 97,778 66,323 3.27 3.82 Table 1: Dictionary Data definition, performing what is usually called Word Sense Disambiguation (WSD) 2." ></td>
	<td class="line x" title="24:168	In the previous example planta has thirteen senses and arbusto only one." ></td>
	<td class="line x" title="25:168	Although a large set of dictionaries have been exploited as lexicM resources, the most widely used monolingual MRD for NLP is LDOCE which was designed for learners of English." ></td>
	<td class="line x" title="26:168	It is clear that different dictionaries do not contain the same explicit information." ></td>
	<td class="line x" title="27:168	The information placed in LDOCE has allowed to extract other implicit information easily, e.g. taxonomies (Bruce et al. , 1992)." ></td>
	<td class="line x" title="28:168	Does it mean that only highly structured dictionaries like LDOCE are suitable to be exploited to provide lexical resources for NLP systems?" ></td>
	<td class="line x" title="29:168	We explored this question probing two disparate dictionaries: Diccionario General Ilustrado de la Lengua Espa~ola (DGILE, 1987) for Spanish, and Le Plus Petit Larousse (LPPL, 1980) for French." ></td>
	<td class="line x" title="30:168	Both are substantially poorer in coded information than LDOCE (LDOCE, 1987) 3." ></td>
	<td class="line x" title="31:168	These dictionaries are very different in number of headwords, polysemy degree, size and length of definitions (c.f. table 1)." ></td>
	<td class="line x" title="32:168	While DGILE is a good example of a large sized dictionary, LPPL shows to what extent the smallest dictionary is useful." ></td>
	<td class="line x" title="33:168	Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques." ></td>
	<td class="line x" title="34:168	This work does not address all the heuristics cited in her paper, but profits from techniques that were at hand, without any claim of them being complete." ></td>
	<td class="line x" title="35:168	In fact we use unsupervised techniques, i.e. those that do not require hand-coding of any kind, that draw knowledge from a variety of sources the source dictionaries, bilingual dictionaries and WordNet in diverse ways." ></td>
	<td class="line x" title="36:168	2Called also Lexical Ambiguity Resolution, Word Sense Discrimination, Word Sense Selection or Word Sense Identification." ></td>
	<td class="line x" title="37:168	3In LDOCE, dictionary senses are explicitly ordered by frequency, 86% dictionary senses have semantic codes and 44% of dictionary senses have pragmatic codes." ></td>
	<td class="line x" title="38:168	This paper tries to proof that using an appropriate method to combine those heuristics we can disambiguate the genus terms with reasonable precision, and thus construct complete taxonomies from any conventional dictionary in any language." ></td>
	<td class="line x" title="39:168	This paper is organized as follows." ></td>
	<td class="line x" title="40:168	After this short introduction, section 2 shows the methods we have applied." ></td>
	<td class="line x" title="41:168	Section 3 describes the test sets and shows the results." ></td>
	<td class="line x" title="42:168	Section 4 explains the construction of the lexical knowledge resources used." ></td>
	<td class="line x" title="43:168	Section 5 discusses previous work, and finally, section 6 faces some conclusions and comments on future work." ></td>
	<td class="line x" title="44:168	2 Heuristics for Genus Sense Disambiguation As the methods described in this paper have been developed for being applied in a combined way, each one must be seen as a container of some part of the knowledge (or heuristic) needed to disambiguate the correct hypernym sense." ></td>
	<td class="line x" title="45:168	Not all the heuristics are suitable to be applied to all definitions." ></td>
	<td class="line x" title="46:168	For combining the heuristics, each heuristic assigns each candidate hypernym sense a normalized weight, i.e. a real number ranging from 0 to 1 (after a scaling process, where maximum score is assigned 1, c.f. section 2.9)." ></td>
	<td class="line x" title="47:168	The heuristics applied range from the simplest (e.g. heuristic 1, 2, 3 and 4) to the most informed ones (e.g. heuristics 5, 6, 7 and 8), and use information present in the entries under study (e.g. heuristics 1, 2, 3 and 4) or extracted from the whole dictionary as a unique lexical knowledge resource (e.g. heuristics 5 and 6) or combining lexical knowledge from several heterogeneous lexical resources (e.g. heuristic 7 and 8)." ></td>
	<td class="line x" title="48:168	2.1 Heuristic 1: Monosemous Genus Term This heuristic is applied when the genus term is monosemous." ></td>
	<td class="line x" title="49:168	As there is only one hypernym sense candidate, the hyponym sense is attached to it." ></td>
	<td class="line x" title="50:168	Only 12% of noun dictionary senses have monosemous genus terms in DGILE, whereas the smaller LPPL reaches 40%." ></td>
	<td class="line x" title="51:168	2.2 Heuristic 2: Entry Sense Ordering This heuristic assumes that senses are ordered in an entry by frequency of usage." ></td>
	<td class="line x" title="52:168	That is, the most used and important senses are placed in the entry before less frequent or less important ones." ></td>
	<td class="line x" title="53:168	This heuristic provides the maximum score to the first sense of the hypernym candidates and decreasing scores to the others." ></td>
	<td class="line x" title="54:168	49 2.3 Heuristic 3: Explicit Semantic Domain This heuristic assigns the maximum score to the hypernym sense which has the same semantic domain tag as the hyponym." ></td>
	<td class="line x" title="55:168	This heuristic is of limited application: LPPL lacks semantic tags, and less than 10% of the definitions in DGILE are marked with one of the 96 different semantic domain tags (e.g. med." ></td>
	<td class="line x" title="56:168	for medicine, or def." ></td>
	<td class="line x" title="57:168	for law, etc.)." ></td>
	<td class="line x" title="58:168	2.4 Heuristic 4: Word Matching This heuristic trusts that related concepts will be expressed using the same content words." ></td>
	<td class="line x" title="59:168	Given two definitions that of the hyponym and that of one candidate hypernym this heuristic computes the total amount of content words shared (including headwords)." ></td>
	<td class="line x" title="60:168	Due to the morphological productivity of Spanish and French, we have considered different variants of this heuristic." ></td>
	<td class="line x" title="61:168	For LPPL the match among lemmas proved most useful, while DGILE yielded better results when matching the first four characters of words." ></td>
	<td class="line x" title="62:168	2.5 Heuristic 5: Simple Cooccurrence This heuristic uses cooccurrence data collected from the whole dictionary (see section 4.1 for more details)." ></td>
	<td class="line oc" title="63:168	Thus, given a hyponym definition (O) and a set of candidate hypernym definitions, this method selects the candidate hypernym definition (E) which returns the maximum score given by formula (1): SC(O, E) : E cw(wi, wj) (I) 'wIEOAwj6E The cooccurrence weight (cw) between two words can be given by Cooccurrence Frequency, Mutual Information (Church and Hanks, 1990) or Association Ratio (Resnik, 1992)." ></td>
	<td class="line x" title="64:168	We tested them using different context window sizes." ></td>
	<td class="line x" title="65:168	Best results were obtained in both dictionaries using the Association Ratio." ></td>
	<td class="line x" title="66:168	In DGILE window size 7 proved the most suitable, whereas in LPPL whole definitions were used." ></td>
	<td class="line x" title="67:168	2.6 Heuristic 6: Cooccurrence Vectors This heuristic is based on the method presented in (Wilks et al. , 1993) which also uses cooccurrence data collected from the whole dictionary (c.f. section 4.1)." ></td>
	<td class="line x" title="68:168	Given a hyponym definition (O) and a set of candidate hypernym definitions, this method selects the candidate hypernym (E) which returns the maximum score following formula (2): CV(O, E) = sim(Vo, VE) (2) The similarity (sim) between two definitions can be measured by the dot product, the cosine function or the Euclidean distance between two vectors (Vo and VE) which represent the contexts of the words presented in the respective definitions following formula (3): t%el = eiv(wd (3) wi6De,f The vector for a definition (VDel) is computed adding the cooccurrence information vectors of the words in the definition (civ(wi))." ></td>
	<td class="line x" title="69:168	The cooccurrence information vector for a word is collected from the whole dictionary using Cooccurrence Frequency, Mutual Information or Association Ratio." ></td>
	<td class="line x" title="70:168	The best combination for each dictionary vary: whereas the dot product, Association Ratio, and window size 7 proved best for DGILE, the cosine, Mutual Information and whole definitions were preferred for LPPL." ></td>
	<td class="line x" title="71:168	2.7 Heuristic 7: Semantic Vectors Because both LPPL and DGILE are poorly semantically coded we decided to enrich the dictionary assigning automatically a semantic tag to each dictionary sense (see section 4.2 for more details)." ></td>
	<td class="line x" title="72:168	Instead of assigning only one tag we can attach to each dictionary sense a vector with weights for each of the 25 semantic tags we considered (which correspond to the 25 lexicographer files of WordNet (Miller, 1990))." ></td>
	<td class="line x" title="73:168	In this case, given an hyponym (O) and a set of possible hypernyms we select the candidate hzypernym (E) which yields maximum similarity among semantic vectors: sv(o, E) = sim(Vo, (4) where sim can be the dot product, cosine or Euclidean Distance, as before." ></td>
	<td class="line x" title="74:168	Each dictionary sense.has been semantically tagged with a vector of semantic weights following formula (5)." ></td>
	<td class="line x" title="75:168	Yogi = sw (w,) (5) wiEDef The salient word vector (swv) for a word contains a saliency weight (Yarowsky, 1992) for each of the 25 semantic tags of WordNet." ></td>
	<td class="line x" title="76:168	Again, the best method differs from one dictionary to the other: each one prefers the method used in the previous section." ></td>
	<td class="line x" title="77:168	2.8 Heuristic 8' Conceptual Distance Conceptual distance provides a basis for determining closeness in meaning among words, taking as reference a structured hierarchical net." ></td>
	<td class="line x" title="78:168	Conceptual distance between two concepts is essentially the length 50 of the shortest path that connects the concepts in the hierarchy." ></td>
	<td class="line x" title="79:168	In order to apply conceptual distance, WordNet was chosen as the hierarchical knowledge base, and bilingual dictionaries were used to link Spanish and French words to the English concepts." ></td>
	<td class="line x" title="80:168	Given a hyponym definition (O) and a set of candidate hypernym definitions, this heuristic chooses the hypernym definition (E) which is closest according to the following formula: CD(O, E) = dist(headwordo, genusE) (6) That is, Conceptual Distance is measured between the headword of the hyponym definition and the genus of the candidate hypernym definitions using formula (7), c.f.(Agirre et al. , 1994)." ></td>
	<td class="line x" title="82:168	To compute the distance between any two words (wl,w2), all the corresponding concepts in WordNet (el,, e2j) are searched via a bilingual dictionary, and the minimum of the summatory for each concept in the path between each possible combination of c1~ and c2~ is returned, as shown below: 1 dist(wl, w2) = rain E depth(ck) Cl i EWl C2j EW2 CkE path(cl~,c2.i ) (7) Formulas (6) and (7) proved the most suitable of several other possibilities for this task, including those which included full definitions in (6) or those using other Conceptual Distance formulas, c.f.(Agirre and Rigau, 1996)." ></td>
	<td class="line x" title="84:168	2.9 Combining the heuristics: Summing As outlined in the beginning of this section, the way to combine all the heuristics in one single decision is simple." ></td>
	<td class="line x" title="85:168	The weights each heuristic assigns to the rivaling senses of one genus are normalized to the interval between 1 (best weight) and 0." ></td>
	<td class="line x" title="86:168	Formula (8) shows the normalized value a given heuristic will give to sense E of the genus, according to the weight assigned to the heuristic to sense E and the maximum weight of all the sense of the genus Ei." ></td>
	<td class="line x" title="87:168	vote(O, E) = weight(O, E) max E, ( weigth( O, Ei ) ) (s) The values thus collected from each heuristic, are added up for each competing sense." ></td>
	<td class="line x" title="88:168	The order in which the heuristics are applied has no relevance at all." ></td>
	<td class="line x" title="89:168	Correct Genus Selected Monosemous Senses per genus idem (polysemous only) Correct senses per genus idem (polysemous only) DGILE LPPL 391 382 (98%) 61 (16%) 115 111 (97%) 40 (36%) 2.75 2.29 3.64 3.02 1.38 1.51 1.05 \[\] Table 2: Test Sets 3 Evaluation 3.1 Test Set In order to test the performance of each heuristic and their combination, we selected two test sets at random (one per dictionary): 391 noun senses for DGILE and 115 noun senses for LPPL, which give confidence rates of 95% and 91% respectively." ></td>
	<td class="line x" title="90:168	From these samples, we retained only those for which the automatic selection process selected the correct genus (more than 97% in both dictionaries)." ></td>
	<td class="line x" title="91:168	Both test sets were disambiguated by hand." ></td>
	<td class="line x" title="92:168	Where necessary multiple correct senses were allowed in both dictionaries." ></td>
	<td class="line x" title="93:168	Table 2 shows the data for the test sets." ></td>
	<td class="line x" title="94:168	3.2 Results Table 3 summarizes the results for polysemous genus." ></td>
	<td class="line x" title="95:168	In general, the results obtained for each heuristic seem to be poor, but always over the random choice baseline (also shown in tables 3 and 4)." ></td>
	<td class="line x" title="96:168	The best heuristics according to the recall in both dictionaries is the sense ordering heuristic (2)." ></td>
	<td class="line x" title="97:168	For the rest, the difference in size of the dictionaries could explain the reason why cooccurrence-based heuristics (5 and 6) are the best for DGILE, and the worst for LPPL." ></td>
	<td class="line x" title="98:168	Semantic distance gives the best precision for LPPL, but chooses an average of 1.25 senses for each genus." ></td>
	<td class="line x" title="99:168	With the combination of the heuristics (Sum) we obtained an improvement over sense ordering (heuristic 2) of 9% (from 70% to 79%) in DGILE, and of 7% (from 66% to 73%) in LPPL, maintaining in both cases a coverage of 100%." ></td>
	<td class="line x" title="100:168	Including monosemous genus in the results (c.f. table 4), the sum is able to correctly disambiguate 83% of the genus in DGILE (8% improvement over sense ordering) and 82% of the genus in LPPL (4% improvement)." ></td>
	<td class="line x" title="101:168	Note that we are adding the results of eight different heuristics with eight different performances, improving the individual performance of each one." ></td>
	<td class="line x" title="102:168	In order to test the contribution of each heuristic to the total knowledge, we tested the sum of all the heuristics, eliminating one of them in turn." ></td>
	<td class="line x" title="103:168	The results are provided in table 5." ></td>
	<td class="line x" title="104:168	51 LPPL recall precision coverage DGILE recall precision coverage LPPL recall precision coverage DGILE recall precision random (1) (2) (3) (4) (5) (6) 36% 66% 8% 11% 22% 36% 66% 66% 44% 61% 100% 100% 12% 25% 36% (7) 11% 57% 19% (8) 50% 76% 66% Sum 73% 73% 100% 30% 70% 1% 44% 57% 60% 57% 47% 79% 30% 70% 100% 72% 57% 60% 58% 49% 79% 100% 100% 1% 61% 100% 100% 99% 95% 100% Table 3: Results for polysemous genus." ></td>
	<td class="line x" title="105:168	coverage random (1) (2) (3) (4) (5) (6) 59% 35% 78% 40% 42% 50% 59% 100% 78% 93% 82% 84% 100% 35% 100% 43% 51% 59% (7) 42% 88% 48% (s) 68% 87% 78% Sum 82% 82% 100% 41% 16% 75% 2% 41% 59% 63% 59% 48% 83% 41% 100% 75% 100% 79% 65% 66% 63% 57% 83% 100% 16% 100% 2% 56% 95% 97% 94% 89% 100% Table 4: Overall results." ></td>
	<td class="line x" title="106:168	LPPL Sum -(1) -(2) -(3) -(4) -(5) -(6) recall 82% 73% 74% 73% 76% 77% precision 82% 73% 75% 73% 76% 77% coverage 100% 100% 99% 100% 100% 100% DGILE recall 83% 79% 72% 81% 81% 81% 81% precision 83% 79% 72% 82% 81% 81% 81% coverage 100% 100% 100% 98% 100% 100% 100% -(7) -(8) 77% 78% 77% 78% lOO% lOO% 81% 77% 81% 77% 100% 100% Table 5: Knowledge provided by each heuristic (overall results)." ></td>
	<td class="line x" title="107:168	(Gale et al. , 1993) estimate that any senseidentification system that does not give the correct sense of polysemous words more than 75% of the time would not be worth serious consideration." ></td>
	<td class="line x" title="108:168	As table 5 shows this is not the case in our system." ></td>
	<td class="line x" title="109:168	For instance, in DGILE heuristic 8 has the worst performance (see table 4, precision 57%), but it has the second larger contribution (see table 5, precision decreases from 83% to 77%)." ></td>
	<td class="line x" title="110:168	That is, even those heuristics with poor performance can contribute with knowledge that other heuristics do not provide." ></td>
	<td class="line x" title="111:168	3.3 Evaluation The difference in performance between the two dictionaries show that quality and size of resources is a key issue." ></td>
	<td class="line x" title="112:168	Apparently the task of disambiguating LPPL seems easier: less polysemy, more monosemous genus and high precision of the sense ordering heuristic." ></td>
	<td class="line x" title="113:168	However, the heuristics that depend only on the size of the data (5, 6) perform poorly on LPPL, while they are powerful methods for DGILE." ></td>
	<td class="line x" title="114:168	The results show that the combination of heuristics is useful, even if the performance of some of the heuristics is low." ></td>
	<td class="line x" title="115:168	The combination performs better than isolated heuristics, and allows to disambiguate all the genus of the test set with a success rate of 83% in DGILE and 82% in LPPL." ></td>
	<td class="line x" title="116:168	All the heuristics except heuristic 3 can readily be applied to any other dictionary." ></td>
	<td class="line x" title="117:168	Minimal parameter adjustment (window size, cooccurrence weigth formula and vector similarity function) should be done to fit the characteristics of the dictionary, but according to our results it does not alter significantly the results after combining the heuristics." ></td>
	<td class="line x" title="118:168	4 Derived Lexical Knowledge Resources 4.1 Cooccurrence Data Following (Wilks et al. , 1993) two words cooccur if they appear in the same definition (word order in definitions are not taken into account)." ></td>
	<td class="line x" title="119:168	For instance, for DGILE, a lexicon of 300,062 cooccurrence pairs among 40,193 word forms was derived (stop words were not taken into account)." ></td>
	<td class="line x" title="120:168	Table 6 shows the first eleven words out of the 360 which cooccur with vino (wine) ordered by Association Ratio." ></td>
	<td class="line x" title="121:168	From left to right, Association Ratio and number of occurrences." ></td>
	<td class="line x" title="122:168	The lexicon (or machine-tractable dictionary, 52 AR #oc." ></td>
	<td class="line x" title="123:168	11.1655 15 tinto (red) 10.0162 23 beber (to drink) 9.6627 14 moso (must) 8.6633 9 jerez (sherry) 8.1051 9 cubas (cask, barrel) 8.0551 16 licor (liquor) 7.2127 17 bebida (drink) 6.9338 12 uva (grape) 6.8436 9 trago (drink, swig) 6.6221 12 sabot (taste) 6.4506 15 pan (bread) Table 6: Example of (wine)." ></td>
	<td class="line x" title="124:168	association ratio for vino MTD) thus produced from the dictionary is used by heuristics 5 and 6." ></td>
	<td class="line x" title="125:168	4.2 Multilingual Data Heuristics 7 and 8 need external knowledge, not present in the dictionaries themselves." ></td>
	<td class="line x" title="126:168	This knowledge is composed of semantic field tags and hierarchical structures, and both were extracted from WordNet." ></td>
	<td class="line x" title="127:168	In order to do this, the gap between our working languages and English was filled with two bilingual dictionaries." ></td>
	<td class="line x" title="128:168	For this purpose, we derived a list of links for each word in Spanish and French as follows." ></td>
	<td class="line x" title="129:168	Firstly, each Spanish or French word was looked up in the bilingual dictionary, and its English translation was found." ></td>
	<td class="line x" title="130:168	For each translation WordNet yielded its senses, in the form of WordNet concepts (synsets)." ></td>
	<td class="line x" title="131:168	The pair made of the original word and each of the concepts linked to it, was included in a file, thus producing a MTD with links between Spanish or French words and WordNet concepts." ></td>
	<td class="line x" title="132:168	Obviously some of this links are not correct, as the translation in the bilingual dictionary may not necessarily be understood in its senses (as listed in WordNet)." ></td>
	<td class="line x" title="133:168	The heuristics using these MTDs are aware of this." ></td>
	<td class="line x" title="134:168	For instance when accessing the semantic fields for vin (French) we get a unique translation, wine, which has two senses in WordNet: <wine,vino> as a beverage, and <wine, wine-coloured> as a kind of color." ></td>
	<td class="line x" title="135:168	In this example two links would be produced (vin, <wine,vino>) and (vin, <wine, wine-coloured>)." ></td>
	<td class="line x" title="136:168	This link allows us to get two possible semantic fields for vin (noun.food, file 13, and noun.attribute, file 7) and the whole structure of the hierarchy in WordNet for each of the concepts." ></td>
	<td class="line x" title="137:168	5 Comparison with Previous Work Several approaches have been proposed for attaching the correct sense (from a set of prescribed ones) of a word in context." ></td>
	<td class="line x" title="138:168	Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al. , 1994), (Resnik, 1995))." ></td>
	<td class="line x" title="139:168	The performance of WSD is reaching a high stance, although usually only small sets of words with clear sense distinctions are selected for disambiguation (e.g.(Yarowsky, 1995) reports a success rate of 96% disambiguating twelve words with two clear sense distinctions each one)." ></td>
	<td class="line x" title="141:168	This paper has presented a general technique for WSD which is a combination of statistical and knowledge based methods, and which has been applied to disambiguate all the genus terms in two dictionaries." ></td>
	<td class="line x" title="142:168	Although this latter task could be seen easier than general WSD 4, genus are usually frequent and general words with high ambiguity ~." ></td>
	<td class="line x" title="143:168	While the average of senses per noun in DGILE is 1.8 the average of senses per noun genus is 2.75 (1.30 and 2.29 respectively for LPPL)." ></td>
	<td class="line x" title="144:168	Furthermore, it is not possible to apply the powerful 'one sense per discourse' property (Yarowsky, 1995) because there is no discourse in dictionaries." ></td>
	<td class="line x" title="145:168	WSD is a very difficult task even for humans 6, but semiautomatic techniques to disambiguate genus have been broadly used (Amsler, 1981) (Vossen and Serail, 1990) (Ageno et ah, 1992) (Artola, 1993) and some attempts to do automatic genus disambiguation have been performed using the semantic codes of the dictionary (Bruce et al. , 1992) or using cooccurrence data extracted from the dictionary itself (Wilks et al. , 1993)." ></td>
	<td class="line x" title="146:168	Selecting the correct sense for LDOCE genus terms, (Bruce et al. , 1992)) report a success rate of 80% (90% after hand coding of ten genus)." ></td>
	<td class="line x" title="147:168	This impressive rate is achieved using the intrinsic char4In contrast to other sense distinctions Dictionary word senses frequently differ in subtle distinctions (only some of which have to do with meaning (Gale et ah, 1993)) producing a large set of closely related dictionary senses (Jacobs, 1991)." ></td>
	<td class="line x" title="148:168	5However, in dictionary definitions the headword and the genus term have to be the same part of speech." ></td>
	<td class="line x" title="149:168	6(Wilks et al. , 1993) disambiguating 197 occurrences of the word bank in LDOCE say 'was not an easy task, as some of the usages of bank did not seem to fit any of the definitions very well'." ></td>
	<td class="line x" title="150:168	Also (Miller et al. , 1994) tagging semantically SemCor by hand, measure an error rate around 10% for polysemous words." ></td>
	<td class="line x" title="151:168	53 acteristics of LDOCE." ></td>
	<td class="line x" title="152:168	Yhrthermore, using only the implicit information contained into the dictionary definitions of LDOCE (Cowie et al. , 1992) report a success rate of 47% at a sense level." ></td>
	<td class="line x" title="153:168	(Wilks et al. , 1993) reports a success rate of 45% disambiguating the word bank (thirteen senses LDOCE) using a technique similar to heuristic 6." ></td>
	<td class="line x" title="154:168	In our case, combining informed heuristics and without explicit semantic tags, the success rates are 83% and 82% overall, and 95% and 75% for two-way ambiguous genus (DGILE and LPPL data, respectively)." ></td>
	<td class="line x" title="155:168	Moreover, 93% and 92% of times the real solution is between the first and second proposed solution." ></td>
	<td class="line x" title="156:168	6 Conclusion and Future Work The results show that computer aided construction of taxonomies using lexical resources is not limited to highly-structured dictionaries as LDOCE, but has been succesfully achieved with two very different dictionaries." ></td>
	<td class="line x" title="157:168	All the heuristics used are unsupervised, in the sense that they do not need hand-codding of any kind, and the proposed method can be adapted to any dictionary with minimal parameter setting." ></td>
	<td class="line x" title="158:168	Nevertheless, quality and size of the lexical knowledge resources are important." ></td>
	<td class="line x" title="159:168	As the results for LPPL show, small dictionaries with short definitions can not profit from raw corpus techniques (heuristics 5, 6), and consequently the improvement of precision over the random baseline or first-sense heuristic is lower than in DGILE." ></td>
	<td class="line x" title="160:168	We have also shown that such a simple technique as just summing is a useful way to combine knowledge from several unsupervised WSD methods, allowing to raise the performance of each one in isolation (coverage and/or precision)." ></td>
	<td class="line x" title="161:168	Furthermore, even those heuristics with apparently poor results provide knowledge to the final result not provided by the rest of heuristics." ></td>
	<td class="line x" title="162:168	Thus, adding new heuristics with different methodologies and different knowledge (e.g. from corpora) as they become available will certainly improve the results." ></td>
	<td class="line x" title="163:168	Needless to say, several improvements can be done both in individual heuristic and also in the method to combine them." ></td>
	<td class="line x" title="164:168	For instance, the cooccurfence heuristics have been applied quite indiscriminately, even in low frequency conditions." ></td>
	<td class="line x" title="165:168	Significance tests or association coefficients could be used in order to discard low confidence decisions." ></td>
	<td class="line x" title="166:168	Also, instead of just summing, more clever combinations can be tried, such as training classifiers which use the heuristics as predictor variables." ></td>
	<td class="line x" title="167:168	Although we used these techniques for genus disambiguation we expect similar results (or even better taken the 'one sense per discourse' property and lexical knowledge acquired from corpora) for the WSD problem." ></td>
	<td class="line x" title="168:168	7 Acknowledgments This work would not be possible without the collaboration of our colleagues, specially Jose Mari Arriola, Xabier Artola, Arantza Diaz de Ilarraza, Kepa Sarasola and Aitor Soroa in the Basque Country and Horacio Rodr~guez in Catalonia." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P97-1024
Independence Assumptions Considered Harmful
Franz, Alexander M.;"></td>
	<td class="line x" title="1:230	Independence Assumptions Considered Harmful Alexander Franz Sony Computer Science Laboratory &: D21 Laboratory Sony Corporation 6-7-35 Kitashinagawa Shinagawa-ku, Tokyo 141, Japan amIcsl, sony." ></td>
	<td class="line x" title="2:230	co. jp Abstract Many current approaches to statistical language modeling rely on independence a.~sumptions 1)etween the different explanatory variables." ></td>
	<td class="line x" title="3:230	This results in models which are computationally simple, but which only model the main effects of the explanatory variables oil the response variable." ></td>
	<td class="line x" title="4:230	This paper presents an argmnent in favor of a statistical approach that also models the interactions between the explanatory variables." ></td>
	<td class="line x" title="5:230	The argument rests on empirical evidence from two series of experiments concerning automatic ambiguity resolution." ></td>
	<td class="line x" title="6:230	1 Introduction In this paper, we present an empirical argument in favor of a certain approach to statistical natural language modeling: we advocate statistical natural language models that account for the interactions between the explanatory statistical variables, rather than relying on independence a~ssumptions." ></td>
	<td class="line x" title="7:230	Such models are able to perform prediction on the basis of estimated probability distributions that are properly conditioned on the combinations of the individual values of the explanatory variables." ></td>
	<td class="line x" title="8:230	After describing one type of statistical model that is particularly well-suited to modeling natural language data, called a loglinear model, we present einpirical evidence fi'om a series of experiments on different ambiguity resolution tasks that show that the performance of the loglinear models outranks the performance of other models described in the literature that a~ssume independence between the explanatory variables." ></td>
	<td class="line x" title="9:230	2 Statistical Language Modeling By 'statistical language model', we refer to a mathematical object that 'imitates the properties' of some respects of naturM language, and in turn makes predictions that are useful from a scientific or engineering point of view." ></td>
	<td class="line x" title="10:230	Much recent work in this flamework hm~ used written and spoken natural language data to estimate parameters for statisticM models that were characterized by serious limitations: models were either limited to a single explanatory variable or." ></td>
	<td class="line x" title="11:230	if more than one explanatory variable wa~s considered, the variables were assumed to be independent." ></td>
	<td class="line x" title="12:230	In this section, we describe a method for statistical language modeling that transcends these limitations." ></td>
	<td class="line x" title="13:230	2.1 Categorical Data Analysis Categorical data analysis is the area of statistics that addresses categorical statistical variable: variables whose values are one of a set of categories." ></td>
	<td class="line x" title="14:230	An exampie of such a linguistic variable is PART-OF-SPEECH, whose possible values might include nou.n, verb, determiner, preposition, etc. We distinguish between a set of explanatory variames." ></td>
	<td class="line x" title="15:230	and one response variable." ></td>
	<td class="line x" title="16:230	A statistical model can be used to perforin prediction in the following manner: Given the values of the explanatory variables, what is the probability distribution for the response variable, i.e what are the probabilities for the different possible values of the response variable?" ></td>
	<td class="line x" title="17:230	2.2 The Contingency Table Tile ba,sic tool used in categorical data analysis is the contingency table (sometimes called the 'crossclassified table of counts')." ></td>
	<td class="line x" title="18:230	A contingency table is a matrix with one dimension for each variable, including the response variable." ></td>
	<td class="line x" title="19:230	Each cell ill the contingency table records the frequency of data with the appropriate characteristics." ></td>
	<td class="line x" title="20:230	Since each cell concerns a specific combination of feat.ures, this provides a way to estimate probabilities of specific feature combinations from the observed frequencies, ms the cell counts can easily be converted to probabilities." ></td>
	<td class="line x" title="21:230	Prediction is achieved by determining the value of the response variable given the values of the explanatory variables." ></td>
	<td class="line x" title="22:230	182 2.3 The Loglinear Model A loglinear model is a statistical model of the effect of a set of categorical variables and their combinations on the cell counts in a contingency table." ></td>
	<td class="line x" title="23:230	It can be used to address the problem of sparse data." ></td>
	<td class="line x" title="24:230	since it can act a.s a 'snmothing device, used to obtain cell estimates for every cell in a sparse array, even if the observed count is zero' (Bishop, Fienberg, and Holland." ></td>
	<td class="line x" title="25:230	1975)." ></td>
	<td class="line x" title="26:230	Marginal totals (sums for all values of some variables) of the observed counts are used to estimate the parameters of the loglinear model; the model in turn delivers estimated expected cell counts, which are smoother than the original cell counts." ></td>
	<td class="line x" title="27:230	The mathematical form of a loglinear model is a,s follows." ></td>
	<td class="line x" title="28:230	Let mi5~ be the expected cell count for cell (i.j. k ) in the contingency table." ></td>
	<td class="line x" title="29:230	The general form of a loglinear model is ms follows: logm/j~ = u.-{-ltlti).-~lt2(j)-~-U3(k)-~lZl2(ij)-~." ></td>
	<td class="line x" title="30:230	(1) In this formula, u denotes the mean of the logarithms of all the expected counts, u+ul(1) denotes the mean of the logarithms of the expected counts with value i of the first variable, u + u2(j) denotes the mean of the logarithms of the expected counts with value j of the second variable, u + ux~_(ii) denotes the mean of the logarithms of the expected counts with value i of the first veriable and value j of the second variable, and so on." ></td>
	<td class="line x" title="31:230	Thus." ></td>
	<td class="line x" title="32:230	the term uzii) denotes the deviation of the mean of the expected cell counts with value i of the first variable from the grand mean u. Similarly, the term Ul2(ij) denotes the deviation of the mean of the expected cell counts with value i of the first variable and value j of the second variable from the grand mean u. In other words, ttl2(ij) represents the combined effect of the values i and j for the first and second variables on the logarithms of the expected cell counts." ></td>
	<td class="line x" title="33:230	In this way, a loglinear model provides a way to estimate expected cell counts that depend not only on the main effects of the variables, but also on the interactions between variables." ></td>
	<td class="line x" title="34:230	This is achieved by adding 'interaction terms' such a.s Ul2(ij ) to the nmdel." ></td>
	<td class="line x" title="35:230	For further details, see (Fienberg, 1980)." ></td>
	<td class="line x" title="36:230	2.4 The Iterative Estimation Procedure For some loglinear models, it is possible to obtain closed forms for the expected cell counts." ></td>
	<td class="line x" title="37:230	For more complicated models, the iterative proportional fitting algorithm for hierarchical loglinear models (Denting and Stephan, 1940) can be used." ></td>
	<td class="line x" title="38:230	Briefly, this procedure works ms follows." ></td>
	<td class="line x" title="39:230	Let the values for the expected cell counts that are estimated by the model be represented by the symbol 7hljk  The interaction terms in the loglinear nmdels represent constraints on the estimated expected marginal totals." ></td>
	<td class="line x" title="40:230	Each of these marginal constraints translates into an adjustment scaling factor for the cell entries." ></td>
	<td class="line x" title="41:230	The iterative procedure has the following steps: 1." ></td>
	<td class="line x" title="42:230	Start with initial estimates for the estimated expected cell counts." ></td>
	<td class="line x" title="43:230	For example, set all 7hijal = 1.0." ></td>
	<td class="line x" title="44:230	2. Adjust each cell entry by multiplying it by the scaling factors." ></td>
	<td class="line x" title="45:230	This moves the cell entries towards satisfaction of the marginal constraints specified by the nmdel." ></td>
	<td class="line x" title="46:230	3." ></td>
	<td class="line x" title="47:230	Iterate through the adjustment steps until the maximum difference e between the marginal totals observed in the sample and the estimated marginal totals reaches a certain minimum threshold, e.g. e = 0.1." ></td>
	<td class="line x" title="48:230	After each cycle, the estimates satisfy the constraints specified in the model, and the estimated expected marginal totals come closer to matching the observed totals." ></td>
	<td class="line x" title="49:230	Thus." ></td>
	<td class="line x" title="50:230	the process converges." ></td>
	<td class="line x" title="51:230	This results in Maximum Likelihood estimates for both multinomial and independent Poisson sampling schemes (Agresti, 1990)." ></td>
	<td class="line x" title="52:230	2.5 Modeling Interactions For natural language classification and prediction tasks, the aim is to estimate a conditional probability distribution P(H\[E) over the possible values of the hypothesis H, where the evidence E consists of a number of linguistic features el, e2  Much of the previous work in this area assumes independence between the linguistic features: P(/-/le~.ej  ) ~ P(Hlel) x P(Hlej) x  (2) For example, a model to predict Part-of-Speech of a word on the basis of its morphological affix and its capitalization might a.ssume independence between the two explanatory variables a,s follows: P(POSIAFFIX, CAPITALIZATION),,~ (3) P(POSIAFFIX ) x P(POSICAPITALIZATION ) This results ill a considerable computational simplification of the model but, as we shall see below." ></td>
	<td class="line x" title="53:230	leads to a considerable loss of information and concomitant decrease in prediction accuracy." ></td>
	<td class="line x" title="54:230	With a loglinear model, on the other hand." ></td>
	<td class="line x" title="55:230	such independence assumptions are not necessary." ></td>
	<td class="line x" title="56:230	The loglinear model provides a posterior distribution that is properly conditioned on the evidence, and maximizing the conditional probability P(HIE ) leads to minimum error rate classification (Duda and Hart." ></td>
	<td class="line x" title="57:230	1973)." ></td>
	<td class="line x" title="58:230	183 s 3 Predicting Part-of-Speech We will now turn to the empirical evidence supporting the argument against independence assumptions." ></td>
	<td class="line x" title="59:230	~ In this section, we will compare two models for pree ~ dicting the Part-of-Speech of an unknown word: A ~ simple model that treats the various explanatory variables ms independent, and a model using loglinear smoothing of a contingency table that takes into account the interactions between the explanatory variables." ></td>
	<td class="line x" title="60:230	3.1 Constructing the Model The model wa~s constructed in the following way." ></td>
	<td class="line x" title="61:230	First, features that could be used to guess the PUS of a word were determined by examining the training portion of a text corpus." ></td>
	<td class="line x" title="62:230	The initial set of features consisted of the following:  INCLUDES-NUMBER." ></td>
	<td class="line x" title="63:230	Does the word include a nunlber?" ></td>
	<td class="line x" title="64:230	 CAPITALIZED." ></td>
	<td class="line x" title="65:230	Is the word in sentence-initial position and capitalized, in any other position and capitalized, or in lower ca~e?  INCLUDES-PERIOD." ></td>
	<td class="line x" title="66:230	Does the word include a period?" ></td>
	<td class="line x" title="67:230	 INCLUDES-COMMA." ></td>
	<td class="line x" title="68:230	Does the word include a colnlna?" ></td>
	<td class="line x" title="69:230	 FINAL-PERIOD." ></td>
	<td class="line x" title="70:230	Is the last character of the word a period?" ></td>
	<td class="line x" title="71:230	 INCLUDES-HYPHEN." ></td>
	<td class="line x" title="72:230	Does the word include a hyphen?" ></td>
	<td class="line x" title="73:230	 ALL-UPPER-CASE." ></td>
	<td class="line x" title="74:230	Is the word in all upper case?" ></td>
	<td class="line x" title="75:230	 SHORT." ></td>
	<td class="line x" title="76:230	Is the length of the word three characters or less?" ></td>
	<td class="line x" title="77:230	 INFLECTION." ></td>
	<td class="line x" title="78:230	Does the word carry one of the English inflectional suffixes?" ></td>
	<td class="line x" title="79:230	 PREFIX." ></td>
	<td class="line x" title="80:230	Does the word carry one of a list of frequently occurring prefixes?" ></td>
	<td class="line x" title="81:230	 SUFFIX." ></td>
	<td class="line x" title="82:230	Does the word carry one of a list of frequently occurring suffixes?" ></td>
	<td class="line x" title="83:230	Next, exploratory data analysis was perfornled in order to determine relevant features and their values, and to approximate which features interact." ></td>
	<td class="line x" title="84:230	Each word of the training data was then turned into a feature vector, and the feature vectors were crossclassified in a contingency table." ></td>
	<td class="line x" title="85:230	The contingency table was smoothed using a loglinear models." ></td>
	<td class="line x" title="86:230	3.2 Data Training and evaluation data was obtained from the Penn Treebank Brown corpus (Marcus, Santorini, and Marcinkiewicz, 1993)." ></td>
	<td class="line x" title="87:230	The characteristics of ''rare' words that might show up ms unknown words differ fi'om the characteristics of words in general." ></td>
	<td class="line x" title="88:230	so a two-step procedure wa~ employed a first time Overall Accuracy i. __,,o_ 4 L~hnem F~tgf~ 9 L~llnQ& ~Oatu~ 8 . F=0.4 Set Accuracy  4 maeo,tnaom Flalu,~ \[ i 4 LOgL'/~III ~omtur~ j i 9 l.~Jl~ar vulu,u Figure 1: Performance of Different Models to obtain a set of ''rare' words ms training data, and again a second time to obtain a separate set of ''rare*' words ms evMuation data." ></td>
	<td class="line x" title="89:230	There were 17,000 words in the training data, and 21,000 words in the evaluation data." ></td>
	<td class="line x" title="90:230	Ambiguity resolution accuracy was evaluated for the ''overall accuracy' (Percentage that the most likely PUS tag is correct), and ''cutoff factor accuracy' (accuracy of the answer set consisting of all PUS tags whose probability lies within a factor F of the most likely PUS (de Marcken, 1990))." ></td>
	<td class="line x" title="91:230	3.3 Accuracy Results (Weischedel et al. , 1993) describe a model for unknown words that uses four features, but treats the features ms independent." ></td>
	<td class="line x" title="92:230	We reimplemented this model by using four features: POS, INFLECTION, CAPITALIZED, and HYPHENATED, In Figures i 2, the results for this model are labeled 4 Independent Features." ></td>
	<td class="line x" title="93:230	For comparison, we created a loglinear model with the same four features: the results for this model are labeled 4 Loglinear Features." ></td>
	<td class="line x" title="94:230	The highest accuracy was obtained by the loglinear model that includes all two-way interactions and consists of two contingency tM)les with the following features: POS, ALL-UPPER-CASE." ></td>
	<td class="line x" title="95:230	HYPHENATED, INCLUDES-NUMBER, CAPITALIZED, INFLECTION, SHORT." ></td>
	<td class="line x" title="96:230	PREFIX, and SUFFIX." ></td>
	<td class="line x" title="97:230	The results for this model are lM)eled 9 Loglinear Features." ></td>
	<td class="line x" title="98:230	The parameters for all three unknown word models were estimated from the training data." ></td>
	<td class="line x" title="99:230	and the models were evaluated on the evaluation data." ></td>
	<td class="line x" title="100:230	The accuracy of the different models in a.ssigning the most likely POSs to words is summarized in Figure 1." ></td>
	<td class="line x" title="101:230	In the left diagram, the two barcharts show two different accuracy memsures: Percent correct (Overall Accuracy), and percent correct within the F=0.4 cutoff factor answer set (F=0.4 Set Accuracy)." ></td>
	<td class="line x" title="102:230	In both cruses, the loglinear model with four features obtains higher accuracy than the method that assumes independence between the same four features." ></td>
	<td class="line x" title="103:230	The loglinear model with nine 184 o o o o  .- ~  o o  o   -Lglmea'wlt F~t~e= \] 1 2 3 4 5 6 7 N~ol Features Figure 2: Effect of Number of Features on Accuracy $ o Uregmm Pro~exe~ kog~r Mce.~ Figure 3: Error Rate on Unknown Words features further improves this score." ></td>
	<td class="line x" title="104:230	3.4 Effect of Number of Features on Accuracy The performance of the loglinear model can be improved by adding more features, but this is not possible with the simpler nmdel that assumes independence between the features." ></td>
	<td class="line x" title="105:230	Figure 2 shows the performance of the two types of nmdels with fenture sets that ranged from a single feature to nine features." ></td>
	<td class="line x" title="106:230	As the diagram shows, the accuracies for both methods rise with the first few features, but then the two methods show a clear divergence." ></td>
	<td class="line x" title="107:230	The accuracy of the simpler method levels off around at around 50-55%, while the loglinear model reaches an accuracy of 70-75%." ></td>
	<td class="line x" title="108:230	This shows that the loglinear model is able to tolerate redundant features and use information from more features than the simpler method, and therefore achieves better results at ambiguity resolution." ></td>
	<td class="line x" title="109:230	3.5 Adding Context to the Model Next, we added of a stochastic POS tagger (Charniak et al. , 1993) to provide a model of context." ></td>
	<td class="line x" title="110:230	A stochastic POS tagger assigns POS labels to words in a sentence by using two parameters:  Lexical Probabilities: P(wlt ) -the probability of observing word w given that the tag t occurred." ></td>
	<td class="line x" title="111:230	 Contextual Probabilities: P(ti\[ti-1, t~_2) -the probability of observing tag ti given that the two previous tags ti-1, t,i--2 occurred." ></td>
	<td class="line x" title="112:230	The tagger maximizes the probability of the tag sequence T = t.l,t, 2 ,t. ,, given the word sequence W = wz,w2,,,w,,, which is approximated a.s follows: I'L P(TIW) ~ II P(wdt~)P(tdt~_~, ti_=) (4) i= 1 The accuracy of the combination of the loglinear model for local features and the stochastic POS tagger for contextual features was evaluated empirically by comparing three methods of handling unknown words:  Unigram: Using the prior probability distribution P(t) of the POS tags for rare words." ></td>
	<td class="line x" title="113:230	 ProbabUistic UWM: Using the probabilistic model that assumes independence between the features." ></td>
	<td class="line x" title="114:230	 Classifier UWM: Using the loglinear model for unknown words." ></td>
	<td class="line x" title="115:230	Separate sets of training and evaluation data for the tagger were obtained from from the Penn Treebank Wall Street corpus." ></td>
	<td class="line x" title="116:230	Evaluation of the combined syst.em was performed on different configurations of the POS tagger on 30-40 different samples containing 4,000 words each." ></td>
	<td class="line x" title="117:230	Since the tagger displays considerable variance in its accuracy in assigning POS to unknown words in context, we use boxplots to display the results." ></td>
	<td class="line x" title="118:230	Figure 3 compares the tagging error rate on unknown words for the unigram method (left) and the loglinear method with nine features (labeled statistical classifier) at right." ></td>
	<td class="line x" title="119:230	This shows that the Ioglinear model significantly improves the Part-of-Speech tagging accuracy of a stochastic tagger on unknown words." ></td>
	<td class="line x" title="120:230	The median error rate is lowered considerably, and samples with error rates over 32% are eliminated entirely." ></td>
	<td class="line x" title="121:230	185 o = ==  PmO~ UWM  Logli~e= UWM o u, *=*    =a  o  08 0 S tO 15 2Q 25 30 35 40 4S 50 SS 60 Peeclntage ol Unknown WO~= Figure 4: Effect of Proportion of Unknown Words on Overall Tagging Error Rate 3.6 Effect of Proportion of Unknown Words Since most of the lexical ambiguity resolution power of stochastic PUS tagging comes from the lexical probabilities, unknown words represent a significant source of error." ></td>
	<td class="line x" title="122:230	Therefore, we investigated the effect of different types of models for unknown words on the error rate for tagging text with different proportions of unknown words." ></td>
	<td class="line x" title="123:230	Samples of text that contained different proportions of unknown words were tagged using the three different methods for handling unknown words described above." ></td>
	<td class="line x" title="124:230	The overall tagging error rate increases significantly as the proportion of new words increases." ></td>
	<td class="line x" title="125:230	Figure 4 shows a graph of overall tagging accuracy versus percentage of unknown words in the text." ></td>
	<td class="line x" title="126:230	The graph compares the three different methods of handling unknown words." ></td>
	<td class="line x" title="127:230	The diagram shows that the loglinear model leads to better overall tagging performance than the simpler methods, with a clear separation of all samples whose proportion of new words is above approximately 10%." ></td>
	<td class="line x" title="128:230	4 Predicting PP Attachment In the second series of experiments, we compare the performance of different statistical models on the task of predicting Prepositional Phrase (PP) attachment." ></td>
	<td class="line x" title="129:230	4.1 Features for PP Attachment First, an initial set of linguistic features that could be useful for predicting PP attachment was determined." ></td>
	<td class="line x" title="130:230	The initial set included the following features:  PREPOSITION." ></td>
	<td class="line x" title="131:230	Possible values of this feature include one of the more frequent prepositions in the training set, or the value other-prep." ></td>
	<td class="line x" title="132:230	* VERB-LEVEL." ></td>
	<td class="line x" title="133:230	Lexical association strength between the verb and the preposition." ></td>
	<td class="line x" title="134:230	 NOUN-LEVEL." ></td>
	<td class="line x" title="135:230	Lexical association strength between the noun and the preposition." ></td>
	<td class="line x" title="136:230	 NOUN-TAG." ></td>
	<td class="line x" title="137:230	Part-of-Speech of the nominal attachment site." ></td>
	<td class="line x" title="138:230	This is included to account for correlations between attachment and syntactic category of the nominal attachment site, such as 'PPs disfavor attachment to proper nouns'." ></td>
	<td class="line x" title="139:230	 NOUN-DEFINITENESS." ></td>
	<td class="line x" title="140:230	Does the nominal attachment site include a definite determiner?" ></td>
	<td class="line x" title="141:230	This feature is included to account for a possible correlation between PP attachment to the nominal site and definiteness, which was derived by (Hirst, 1986) from the principle of presupposition minimization of (Craln and Steedman, 1985)." ></td>
	<td class="line x" title="142:230	 PP-OBJECT-TAG." ></td>
	<td class="line x" title="143:230	Part-of-speech of the object of the PP." ></td>
	<td class="line x" title="144:230	Certain types of PP objects favor attachment to the verbal or nominal site." ></td>
	<td class="line x" title="145:230	For example, temporal PPs, such as 'in 1959', where the prepositional object is tagged CD (cardinal), favor attachment to the VP, because tile VP is more likely to have a temporal dimension." ></td>
	<td class="line x" title="146:230	The association strengths for VERB-LEVEL and NOUN-LEVEL were measured using the Mutual Information between the noun or verb, and the preposition." ></td>
	<td class="line x" title="147:230	1 The probabilities were derived ms Maximum Likelihood estimates from all PP cases in the training data." ></td>
	<td class="line x" title="148:230	The Mutual Information values were ordered by rank." ></td>
	<td class="line x" title="149:230	Then, the a~ssociation strengths were categorized into eight levels (A-H), depending on percentile in the ranked Mutual Information values." ></td>
	<td class="line x" title="150:230	4.2 Experimental Data and Evaluation Training and evaluation data was prepared from the Penn treebank." ></td>
	<td class="line x" title="151:230	All 1.1 million words of parsed text in the Brown Corpus, and 2.6 million words of parsed WSJ articles, were used." ></td>
	<td class="line x" title="152:230	All instances of PPs that are attached to VPs and NPs were extracted." ></td>
	<td class="line x" title="153:230	This resulted in 82,000 PP cases from the Brown Corpus, and 89,000 PP cases from the WS.\] articles." ></td>
	<td class="line x" title="154:230	Verbs and nouns were lemmatized to their root forms if the root forms were attested in the corpus." ></td>
	<td class="line x" title="155:230	If the root form did not occur in the corpus, then the inflected form was used." ></td>
	<td class="line x" title="156:230	All the PP cases from the Brown Curl)us, and 50,000 of the WSJ cases, were reserved ms training data." ></td>
	<td class="line x" title="157:230	The remaining 39,00 WSJ PP cases formed the evaluation pool." ></td>
	<td class="line oc" title="158:230	In each experiment, performance IMutu',d Information provides an estimate of the magnitude of the ratio t)ctw(.(-n the joint prol)ability P(verb/noun,1)reposition), and the joint probability a.~suming indcpendcnce P(verb/noun)P(prcl)osition ) s(:(, (Church and Hanks, 1990)." ></td>
	<td class="line x" title="159:230	186 o 1 | u R~m A~jllon Hfr,3~ & Roolh kog~eaw ~ak~r 1 ! o o ol t I i o!" ></td>
	<td class="line x" title="160:230	l l o Figure 5: Results for Two Attachment Sites Figure 6: Three Attachment Sites: Right Association and Lexical Association was evaluated oil a series of 25 random samples of 100 PP cases fi'om the evaluation pool." ></td>
	<td class="line x" title="161:230	in order to provide a characterization of the error variance." ></td>
	<td class="line x" title="162:230	4.3 Experimental Results: Two Attachments Sites Previous work oll automatic PP attachment disambiguation has only considered the pattern of a verb phrase containing an object, and a final PP." ></td>
	<td class="line x" title="163:230	This lends to two possible attachment sites, the verb and the object of the verb." ></td>
	<td class="line x" title="164:230	The pattern is usually further simplified by considering only the heads of the possible attachment sites, corresponding to the sequence 'Verb Noun1 Preposition Noun2'." ></td>
	<td class="line x" title="165:230	The first set of experiments concerns this pattern." ></td>
	<td class="line x" title="166:230	There are 53,000 such cases in the training data." ></td>
	<td class="line x" title="167:230	and 16,000 such cases in the evaluation pool." ></td>
	<td class="line x" title="168:230	A number of methods were evaluated on this pattern according to the 25-sample scheme described above." ></td>
	<td class="line x" title="169:230	The results are shown in Figure 5." ></td>
	<td class="line x" title="170:230	4.3.1 Baseline: Right Association Prepositional phrases exhibit a tendency to attach to the most recent possible attachment site; this is referred to ms the principle of ''Right Association'." ></td>
	<td class="line x" title="171:230	For the 'V NP PP'' pattern, this means preferring attachment to the noun phra~se." ></td>
	<td class="line x" title="172:230	On the evaluation samples, a median of 65% of the PP cases were attached to the noun." ></td>
	<td class="line x" title="173:230	4.3.2 Results of Lexical Association (Hindle and R ooth." ></td>
	<td class="line x" title="174:230	1993) described a method for obtaining estimates of lexical a.ssociation strengths between nouns or verbs and prepositions, and then using lexical association strength to predict." ></td>
	<td class="line x" title="175:230	PP attachment." ></td>
	<td class="line x" title="176:230	In our reimplementation of this lnethod." ></td>
	<td class="line x" title="177:230	the probabilities were estimated fi'om all the PP cases in the training set." ></td>
	<td class="line x" title="178:230	Since our training data are bracketed, it was possible to estimate tile lexical associations with much less noise than Hindle & R ooth, who were working with unparsed text." ></td>
	<td class="line x" title="179:230	The median accuracy for our reimplementation of Hindle & Rooth's method was 81%." ></td>
	<td class="line x" title="180:230	This is labeled 'Hindle & Rooth'' in Figure 5." ></td>
	<td class="line x" title="181:230	4.3.3 Results of the Loglinear Model The loglinear model for this task used the features PREPOSITION." ></td>
	<td class="line x" title="182:230	VERB-LEVEL, NOUN-LEVEL, and NOUN-DEFINITENESS, and it included all secondorder interaction terms." ></td>
	<td class="line x" title="183:230	This model achieved a median accuracy of 82%." ></td>
	<td class="line x" title="184:230	Hindle & Rooth's lexical association strategy only uses one feature (lexical aasociation) to predict PP attachment, but." ></td>
	<td class="line x" title="185:230	ms the boxplot shows, the results from the loglinear model for the 'V NP PP' pattern do not show any significant improvement." ></td>
	<td class="line x" title="186:230	4.4 Experimental Results: Three Attachment Sites As suggested by (Gibson and Pearlmutter." ></td>
	<td class="line x" title="187:230	1994), PP attachment for the ''Verb NP PP' pattern is relatively easy to predict because the two possible attachment sites differ in syntactic category, and therefore have very different kinds of lexical preferences." ></td>
	<td class="line x" title="188:230	For example, most PPs with of attach to nouns, and most PPs with f,o and by attach to verbs." ></td>
	<td class="line x" title="189:230	In actual texts, there are often more than two possible attachment sites for a PP." ></td>
	<td class="line x" title="190:230	Thus, a second, more realistic series of experiments was perforlned that investigated different PP attachment strategies for the pattern ''Verb Noun1 Noun2 Preposition Noun3'' that includes more than two possible attachment sites that are not syntactically heterogeneous." ></td>
	<td class="line x" title="191:230	There were 28,000 such cases in the training data." ></td>
	<td class="line x" title="192:230	and 8000 ca,~es in the evaluation pool." ></td>
	<td class="line x" title="193:230	187 '5 o RIgN AUCCUII~ Split HinOle & Rooln Lo~l~ur M0~el Figure 7: Summary of Results for Three Attachment Sites 4.4.1 Baseline: Right Association As in the first set of experiments, a number of methods were evaluated an the three attachment site pattern with 25 samples of 100 random PP cases." ></td>
	<td class="line x" title="194:230	The results are shown in Figures 6-7." ></td>
	<td class="line x" title="195:230	The baseline is again provided by attachment according to the principle of 'Right Attachment'; to the nmst recent possible site, i.e. attaclunent to Noun2." ></td>
	<td class="line x" title="196:230	A median of 69% of the PP cases were attached to Noun2." ></td>
	<td class="line x" title="197:230	4.4.2 Results of Lexical Association Next, the lexical association method was evaluated on this pattern." ></td>
	<td class="line x" title="198:230	First." ></td>
	<td class="line x" title="199:230	the method described by Hindle & Rooth was reimplemented by using the lexical association strengths estimated from all PP cases." ></td>
	<td class="line x" title="200:230	The results for this strategy are labeled 'Basic Lexical Association' in Figure 6." ></td>
	<td class="line x" title="201:230	This method only achieved a median accuracy of 59%, which is worse than always choosing the rightmost attachment site." ></td>
	<td class="line x" title="202:230	These results suggest that Hindle & R.ooth's scoring function worked well in the ''Verb Noun1 Preposition Noun2'' case not only because it was an accurate estimator of lexical associations between individual verbs/nouns and prepositions which determine PP attachment, but also because it accurately predicted the general verb-noun skew of prepositions." ></td>
	<td class="line x" title="203:230	4.4.3 Results of Enhanced Lexical Association It seems natural that this pattern calls for a combination of a structural feature with lexical association strength." ></td>
	<td class="line x" title="204:230	To implement this, we modified Hindle & Rooth's method to estimate attachments to the verb, first noun." ></td>
	<td class="line x" title="205:230	and second noun separately." ></td>
	<td class="line x" title="206:230	This resulted in estimates that combine the structural feature directly with the lexical association strength." ></td>
	<td class="line x" title="207:230	The modified method performed better than the original lexical association scoring function, but it still only obtained a median accuracy of 72%." ></td>
	<td class="line x" title="208:230	This is labeled 'Split Hindle & Rooth' in Figure 7." ></td>
	<td class="line x" title="209:230	4.4.4 Results of Loglinear Model To create a model that combines various structural and lexical features without independence assumptions, we implemented a loglinear model that includes the variables VERB-LEVEL FIRST-NOUN-LEVEL." ></td>
	<td class="line x" title="210:230	and SECOND-NOUN-LEVEL." ></td>
	<td class="line x" title="211:230	2 The loglinear model also includes the variables PREPOSITION and PP-OBJECT-TAG." ></td>
	<td class="line x" title="212:230	It, was smoothed with a loglinear model that includes all second-order interactions." ></td>
	<td class="line x" title="213:230	This method obtained a median accuracy of 79%; this is labeled 'Loglinear Model' in Figure 7." ></td>
	<td class="line x" title="214:230	As the boxplot shows, it performs significantly better than the methods that only use estimates of lexical a,~soclarion." ></td>
	<td class="line x" title="215:230	Compared with the ''Split Hindle Sz Rooth'' method, the samples are a little less spread out, and there is no overlap at all between the central 50% of the samples from the two methods." ></td>
	<td class="line x" title="216:230	4.5 Discussion The simpler 'V NP PP' pattern with two syntactically different attachment sites yielded a null result: The loglinear method did not perform significantly better than the lexical association method." ></td>
	<td class="line x" title="217:230	This could mean that the results of the lexical association method can not be improved by adding other features, but it is also possible that the features that could result in improved accuracy were not identified." ></td>
	<td class="line x" title="218:230	The lexical association strategy does not perform well on the more difficult pattern with three possible attachment sites." ></td>
	<td class="line x" title="219:230	The loglinear model, on the other hand, predicts attachment with significantly higher accuracy, achieving a clear separation of the central 50% of the evaluation samples." ></td>
	<td class="line x" title="220:230	5 Conclusions We have contrasted two types of statistical language models: A model that derives a probability distribution over the response variable that is properly conditioned on the combination of the explanatory variable, and a simpler model that treats the explanatory variables as independent, and therefore models the response variable simply a~s the addition of the individual main effects of the explanatory variables." ></td>
	<td class="line x" title="221:230	2These features use tile s~unc Mutual Informationba.~ed measure of lcxic',d a.sso(:iation a.s tim prc.vious loglinear model for two possibh~' attachment sites, which wcrc estimated from all nomin'M azt(l vcrhal PP att~t(:hments in the corpus." ></td>
	<td class="line x" title="222:230	The features FIRST-NOUN-LEVEL aaM SECOND-NOUN-LEVEL use the same estimates: in other words, in contrm~t to the 'split Lexi(:al Association' method, they were not estimated sepaxatcly for the two different nominaJ, attachment sites." ></td>
	<td class="line x" title="223:230	188 The experimental results show that, with the same feature set, inodeling feature interactions yields better performance: such nmdels achieves higher accuracy, and its accura~,y can be raised with additional features." ></td>
	<td class="line x" title="224:230	It is interesting to note that modeling variable interactions yields a higher perforlnanee gain than including additional explanatory variables." ></td>
	<td class="line x" title="225:230	While these results do not prove that modeling feature interactions is necessary, we believe that they provide a strong indication." ></td>
	<td class="line x" title="226:230	This suggests a mlmber of avenues for filrther research." ></td>
	<td class="line x" title="227:230	First, we could attempt to improve the specific models that were presented by incorporating additional features, and perhal)S by taking into account higher-order features." ></td>
	<td class="line x" title="228:230	This might help to address the performance gap between our models and human subjects that ha,s been documented in the literature, z A more ambitious idea would be to use a statistical model to rank overall parse quality for entire sentences." ></td>
	<td class="line x" title="229:230	This would be an improvement over schemes that a,ssnlne independence between a number of individual scoring fimctions, such ms (Alshawi and Carter, 1994)." ></td>
	<td class="line x" title="230:230	If such a model were to include only a few general variables to account for such features a.~ lexical a.ssociation and recency preference for syntactic attachment, it might even be worthwhile to investigate it a.s an approximation to the human parsing mechanism." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-0205
A Lexicon For Underspecified Semantic Tagging
Buitelaar, Paul;"></td>
	<td class="line x" title="1:166	A Lexicon for Underspecified Semantic Tagging Paul Buitelaar Dept. of Computer Science Brandeis University Waltham, MA 02254-9110, USA paulb@cs, brandeis, edu Abstract The paper defends the notion that semantic tagging should be viewed as more than disambiguation between senses." ></td>
	<td class="line x" title="2:166	Instead, semantic tagging should be a first step in the interpretation process by assigning each lexJ.cal item a representation of all of its sy=stematically related senses, from which fuxther semantic processing steps can derive discourse dependent interpretations." ></td>
	<td class="line x" title="3:166	This leads to a new type of semantic lexicon (CoRv.Lzx) that supports underspecified semantic tagging through a design based on systematic polysemous classes and a class-based acquisition of lexical knowledge for specific domains." ></td>
	<td class="line x" title="4:166	1 Underspecified semantic tagging Semantic tagging has mostly been considered as nothing more than disambiguation to be performed along the same lines as part-of-speech tagging: given n lexical items each with m senses apply linguistic heuristics and/or statistical measures to pick the most likely sense for each lexical item (see eg: (Yarowsky, 1Q92) (Stevenson and Wilks, 1997))." ></td>
	<td class="line x" title="5:166	I do not believe this to be the right approach because it blurs the distinction between 'related' (systematic polysemy) and 'unrelated' senses (homonymy : bank bank)." ></td>
	<td class="line x" title="6:166	Although homonyms need to be tagged with a disambiguated sense, this is not necessarily so in the case of systematic polysemy." ></td>
	<td class="line x" title="7:166	There are two reasons for this that I will discuss briefly here." ></td>
	<td class="line x" title="8:166	First, the problem of multiple reference." ></td>
	<td class="line x" title="9:166	Consider this example from the BROWN corpus: \[A long book heavily weighted with milltary technlcalities\]Np, in this edition it is neither so long nor so technical as it was originally." ></td>
	<td class="line x" title="10:166	The discourse marker (it) refers back to an NP that expresses more than one interpretation at the same time." ></td>
	<td class="line x" title="11:166	The head of the NP (book) has a number of systematically related senses that are being expressed simultaneously." ></td>
	<td class="line x" title="12:166	The meaning of book in this sentence cannot be disambiguated between the number of interpretations that are implied: the informational content of the book (military technicalities), its physical appearance (heavily weighted) and the events that are involved in its construction and use (long)." ></td>
	<td class="line x" title="13:166	The example illustrates the fact that disambiguation between related senses is not always possible, which leads to the further question if a discrete distinction between such senses is desirable at all." ></td>
	<td class="line x" title="14:166	A number of researchers have answered this question negatively (see eg: (Pustejovsky, 1995) (Killgariff, 1992))." ></td>
	<td class="line x" title="15:166	Consider these examples from BROWN: (1) fast run-up (of the stock) (2) fast action (by the city government) (3) fast footwork (by Washington) (4) fast weight gaining (5) fast condition (of the track) (6) fast response time (7) fast people (8) fast ball Each use of the adjective 'fast' in these examples has a slightly different interpretation that could be captured in a number of senses, reflecting the different syntactic and semantic patterns." ></td>
	<td class="line x" title="16:166	For instance: 1." ></td>
	<td class="line x" title="17:166	'a fast action' (1, 2, 3, 4) 2." ></td>
	<td class="line x" title="18:166	'a fast state of affairs' (5, 6) 3." ></td>
	<td class="line x" title="19:166	'a fast object' (7, 8) 25 On the other hand all of the interpretations have something in common also, namely the idea of 'speed'." ></td>
	<td class="line x" title="20:166	It seems therefore useful to underspecify the lexical meaning of 'fast' to a representation that captures this primary semantic aspect and gives a general structure for its combination with other lexical items, both locally (in compositional semantics) and globally (in discourse structure)." ></td>
	<td class="line x" title="21:166	Both the multiple reference and the sense enumeration problem show that lexical items mostly have an indefinite number of related but highly discourse dependent interpretations, between which cannot be distinguished by semantic tagging alone." ></td>
	<td class="line x" title="22:166	Instead, semantic tagging should be a first step in the interpretation process by assigning each lexical item a representation of all of its systematically related 'senses'." ></td>
	<td class="line x" title="23:166	Further semantic processing steps derive discourse dependent interpretations from this representation." ></td>
	<td class="line x" title="24:166	Semantic tags are therefore more like pointers to complex knowledge representations, which can be seen as underspecified lexical meanings." ></td>
	<td class="line x" title="25:166	2 CORELEX: A Semantic Lexicon with Systematic Polysemous Classes In this section I describe the structure and content of a lexicon (CORELEX) that builds on the assumptions about lexical semantics and discourse outlined above." ></td>
	<td class="line x" title="26:166	More specifically, it is to be 'structured in such a way that it reflects the lexical semantics of a language in systematic and predictable ways' (Pustejovsky, Boguraev, and Johnston, 1995)." ></td>
	<td class="line x" title="27:166	This assumption is fundamentally different from the design philosophies behind existing lexical semantic resources like WORDNET that do not account for any regularities between senses." ></td>
	<td class="line x" title="28:166	For instance, WORDNET assigns to the noun book the following senses: the content that is being communicated (communicatiofl) and the medium of communication (artifact)." ></td>
	<td class="line x" title="29:166	More accurately, book should be assigned a qualia structure which implies both of these interpretations and connects them to each of the more specific senses that WORDNET assigns: that is, facts, drama and a journal can be part-of the content of a book; a section is part-of both the content and the medium; publication, production and recording are all events in which both the content and the medium aspects of a book can be involved." ></td>
	<td class="line x" title="30:166	An important advantage of the CORELEX approach is more consistency among the assignments of lexical semantic structure." ></td>
	<td class="line x" title="31:166	Consider the senses that WORDNET assigns to door, gate and window: door movable_barrier -,~ artifact entrance ~-~ opening access ~* cognition, knowledge house ~-, ??" ></td>
	<td class="line x" title="32:166	room ~-~ ??" ></td>
	<td class="line x" title="33:166	gate movable_barrier -,~ artifact computer_circult -,~ opening grossAncome -,~ opening window opening -~ opening panel --~ artifact display ~-* cognition, knowledge publication product, production fact dramatic_composltion, dramatic_work record section, subdivision journal Figure I: WORDNET senses for the noun book At the top of the WORDNET hierarchy these seven senses can be reduced to two unrelated 'basic senses': 26 Figure 2: WORDNET senses for the nouns door, window and gate Obviously these are similar words, something which is not expressed in the WORDNET sense assignments." ></td>
	<td class="line x" title="34:166	In the CORELEX approach, these nouns are given the same semantic type, which is underspecifled for any specific 'sense' but assigns them consistently with the same basic lexical semantic structure that expresses the regularities between all of their interpretations." ></td>
	<td class="line x" title="35:166	However, despite its shortcomings WORDNET is a vast resource of lexical semantic knowledge that can m m m mm m \[\] m m n \[\] m m n m m m m m n m m be mined, restructured and extended, which makes it a good starting point for the construction of CORELEX." ></td>
	<td class="line x" title="36:166	The next sections describe how systematic polysem0us classes and underspecified semantic types can be derived from WORDNET." ></td>
	<td class="line x" title="37:166	In this paper I only consider classes of noun,s, but the process described here can also be applied to other parts of speech." ></td>
	<td class="line x" title="38:166	2.1 Systematic polysemous classes We can arrive at classes of systematically polysemous lexical items by investigating which items share the same senses and are thus polysemous in the same way." ></td>
	<td class="line x" title="39:166	This comparison is done at the top levels of the WORDNET hierarchy." ></td>
	<td class="line x" title="40:166	WORDNET does not have an explicit level structure, but for the purpose of this research one can distinguish a set of 32 =basic senses' that partly coincides with, but is not based directly on WORDNET'S list of 26 'top types': act (act), agent (agt), animal (~.m), artifact (art), attribute (air), blunder (bln), cell (cel), chemical (chm), communication (corn), event (evl;), food (rod), form (frm), group_biological (grb), group (grp), group_social (grs), h-m~n (hum), llnear_measure (1me), location (loc), 1ocation_geographical (log), measure (mea), natural_object (nat), phenomenon (p\]m), plant (plt), possession (pos), part (prt), psychological (psy), quantity_definite (qud), quantity_indefinite (qui), relation (re1), space (spc), state (sta), time (tree) Figure 3 shows their distribution among noun stems in the BROWN corpus." ></td>
	<td class="line x" title="41:166	For instance there are 2550 different noun stems (with 49,824 instances) that have each 2 out of the 32 'basic senses' assigned to them in 238 different combinations (a subset of 322 = 1024 possible combinations)." ></td>
	<td class="line x" title="42:166	We now reduce all of WORDNET'S sense assignments to these basic senses." ></td>
	<td class="line x" title="43:166	For instance, the seven different senses that WORDNET assigns to the lexical item book (see Figure I above) can be reduced to the two basic senses: 'art corn'." ></td>
	<td class="line x" title="44:166	We do this for each lexical item and then group them into classes according to their assignments." ></td>
	<td class="line x" title="45:166	From these one can filter out those classes that have only one member because they obviously do not represent a systematically polysemous class." ></td>
	<td class="line x" title="46:166	The lexical items in those classes have a highly idiosyncratic behavior and are most likely homonyms." ></td>
	<td class="line x" title="47:166	This leaves 27 senses comb's stems instances 2 238 2550 49824 3 379 936 35608 4 268 347 22543 5 148 154 15345 6 52 52 5915 7 27 27 5073 8 10 10 3273 9 3 3 1450 I0 1 1 483 11 2 2 959 12 1 1 441 1161 10797 140914 Figure 3: Polysemy of nouns in BROWN a set of 442 polysemous classes, of which Figure 4 gives a selection: act art evt rel act art log act evt nat chin sta com prt frm sta line qud loc psy log pos sta phm pos tel sta click modification reverse berth habitation mooring ascent climb grease ptomaine appendix brickbat index solid vacancy void em fathom fthm inch mil bourn bourne demarcation fairyland rubicon trend vertex barony province accretion usance wastage baronetcy connectedness context efficiency inclusion liquid relationship Figure 4: A selection of polysemous classes Not all of the 442 classes are systematically polysemous." ></td>
	<td class="line x" title="48:166	Consider for example the following classes: Some of these classes are collections of homonyms that are ambigtzotz,s in similar ways, but do not lead to any kind of predictable polysemous behavior, for instance the class 'act anm art' with the lexical items: drill ruff solitaire stud." ></td>
	<td class="line x" title="49:166	Other classes consist of both homonyms and systematically polysemous lexical items like the class act log, which includes caliphate, clearing, emirate, prefecture, repair, wheeling vs. bolivia, charleston, chicago, michigan." ></td>
	<td class="line x" title="50:166	m m act ~nm art act log act plt axt rod loc chmpsy rod hum plt drill ruff solitaire stud bolivia caliphate charleston chicago clearing emirate michigan prefecture repair santiago wheeling chess grapevine rape pike port complex incense mandarin sage swede Figure 5: A selection of ambiguous classes Whereas the first group of nouns express two separated but related meanings (the act of clearing, repair, etc. takes place at a certain location), the second group expresses two meanings that are not related (the charleston dance which was named after the town by the same name)." ></td>
	<td class="line x" title="51:166	The ambiguous classes need to be removed altogether, while the ones with mixed ambiguous and polllsemous lexical items are to be weeded out carefully." ></td>
	<td class="line x" title="52:166	2.2 Underspecified semantic types The next step in the research is to organize the remaining classes into knowledge representations that relate their senses to each other." ></td>
	<td class="line x" title="53:166	These representations are based on Generative Lexicon theory (G), using qualia roles and (dotted) types (Pustejovsky, 19os)." ></td>
	<td class="line x" title="54:166	Qualia roles distinguish different semantic aspects: FORMAL indicates semantic type; CONSTITUTIVE part-whole information; AGENTIVE and TELIC associated events (the first dealing with the origin of the object, the second with its purpose)." ></td>
	<td class="line x" title="55:166	Each role is typed to a specific class of lexical items." ></td>
	<td class="line x" title="56:166	Types are either simple (human, artifact,) or complex (e.g. , information.physical)." ></td>
	<td class="line x" title="57:166	Complex types are called dotted types after the 'dots' that are used as type constructors." ></td>
	<td class="line x" title="58:166	Here I introduce two kinds of dots: Closed clots ''." ></td>
	<td class="line x" title="59:166	connect systematically related types that are always interpreted simultaneonsly." ></td>
	<td class="line x" title="60:166	Open dots 'o' connect systematically related types that are not (normally) interpreted simultaneously." ></td>
	<td class="line x" title="61:166	Both '#*~' and 'aor' denote sets of pairs of objects (a, b), a an object of type ~ and b an object of type ~'." ></td>
	<td class="line x" title="62:166	A condition aRb restricts this set of pairs to only those for which some relation R holds, where R denotes a subset of the Cartesian product of the sets of type ~ objects and type r objects." ></td>
	<td class="line x" title="63:166	The difference between types '#or' and 'cot' is in the nature of the objects they denote." ></td>
	<td class="line x" title="64:166	The type 'aer' denotes sets of pairs of objects where each pair behaves as a complex object in discourse structure." ></td>
	<td class="line x" title="65:166	For instance, the pairs of objects that are introduced by the type informationephysical (book, journal, scoreboard ) are addressed as the complex objects (x:information, y:physical) in discourse." ></td>
	<td class="line x" title="66:166	On the other hand, the type '#or' denotes simply a set of pairs of objects that do not occur together in discourse structure." ></td>
	<td class="line x" title="67:166	For instance, the pairs of objects that are introduced by the type form.artifact (door, gate, window  ) are not (normally) addressed simultaneously in discourse, rather one side of the object is picked out in a particular context." ></td>
	<td class="line x" title="68:166	Nevertheless, the pair as a whole remains active during processing." ></td>
	<td class="line x" title="69:166	The resulting representations can be seen as underspecified lexical meanings and are therefore referred to as underspecified semantic types." ></td>
	<td class="line x" title="70:166	CORELEX currently covers 104 underspecified semantic types." ></td>
	<td class="line x" title="71:166	This section presents a number of examples, for a complete overview see the CORELEX webpage: http://~, ca." ></td>
	<td class="line x" title="72:166	brandeis, edu/'paulb/Cor eLex/corelex, html Closed Dots Consider the underspecified representation for the semantic type actorelation: FORMAL = Q:act.relation CONSTITUTIVE = X:act V Y:relation V Z:act,relation TELIC --P:event (acterelation) A act (R1) A relation(R2,Rs) Figure 6: Representation for type: actorelation The representation introduces a number of objects that are of a certain type." ></td>
	<td class="line x" title="73:166	The FORMAL role introduces an object Q of type actorelation." ></td>
	<td class="line x" title="74:166	The CONSTITUTIVE introduces objects that are in a partwhole relationship with Q. These are either of the same type actorelation or of the simple types act or relation." ></td>
	<td class="line x" title="75:166	The TELIC expresses the event P that can be associated with an object of type acterelation." ></td>
	<td class="line x" title="76:166	For instance, the event of increase as in 'increasing the communication between member states' implies 'increasing' both the act of communicating an object 28 m m m m m m m m m \[\] m mm m m m m m m m m m m mm mm m m m m m RI and the communication relation between two objects R2 and Rs." ></td>
	<td class="line x" title="77:166	All these objects are introduced on the semantic level and correspond to a number of objects that will be realized in syntax." ></td>
	<td class="line x" title="78:166	However, not all semantic objects will be realized in syntax." ></td>
	<td class="line x" title="79:166	(See Section 3.4 for more on the syntax-semantics interface)." ></td>
	<td class="line x" title="80:166	The instances for the type act*relation are given in Figure 7, covering three different systematic polysemous classes." ></td>
	<td class="line x" title="81:166	We could have chosen to include only the instances of the 'act rel' class, but the nouns in the other two classes seem similar enough to describe all of them with the same type." ></td>
	<td class="line x" title="82:166	generative the lexicon should be and if one allows overgeneration of semantic objects." ></td>
	<td class="line x" title="83:166	.nm rod bluepoint capon clam cockle crawdad crawfish crayfish duckling fowl grub hen lamb langouste limpet lobster monkfish mussel octopus panfish partridge pheasant pigeon poultry prawn pullet quail saki scallop scollop shellfish shrimp snail squid whelk whitebait whitefish winkle act evt rel act rel act rel s~a blend competition flux transformation acceleration communication dealings designation discourse gait glide likening negation neologism neology prevention qualifying sharing synchronisation synchronization synchronizing coordination gradation involvement Figure 7: Instances for the type: act*relation Open Dots The type act.relation describes interpretations that can not be separated from each other (the act and relation aspects are intimately connected)." ></td>
	<td class="line x" title="84:166	The following representation for type -nimalofood describes interpretations that can not occur simultaneously but are however related ~." ></td>
	<td class="line x" title="85:166	It therefore uses a 'o' instead of a ''." ></td>
	<td class="line x" title="86:166	as a type constructor: FORMAL = Q:animalofood CONSTITUTIVE = X:an~mal V Y:food TELIC = Pz :act (Rz,'n|mal) V P2 :act (animal,R2) v P3:act(R3,food) Figure 8: Representation for type: animalofood The instances for this type only cover the class ' ~,m rod'." ></td>
	<td class="line x" title="87:166	A case could be made for including also every instance of the class c~-m' because in principal every animal could be eaten." ></td>
	<td class="line x" title="88:166	This is a question of how 1See the literature on animal grinding, for instance (Copestake and Briscoe, 1992) 29 Figure 9: Instances for the type: animalofood 2.3 Homonyms CORELEX is designed around the idea of systematic polysemons classes that exclude homonyms." ></td>
	<td class="line x" title="89:166	Traditionally a lot of research in lexical semantics has been occupied with the problem of ambiguity in homonyms." ></td>
	<td class="line x" title="90:166	Our research shows however that homonyms only make up a fraction of the whole of the lexicon of a language." ></td>
	<td class="line x" title="91:166	Out of the 37,793 noun stems that were derived from WORDNET 1637 are to be viewed as true homonyms because they have two or more unrelated senses, less than 5%." ></td>
	<td class="line x" title="92:166	The remaining 95% are nouns that do have (an indefinite number of) different interpretations, hut all of these are somehow related and should be inferred from a common knowledge representation." ></td>
	<td class="line x" title="93:166	These numbers suggest a stronger emphasis in research on systematic polysemy and less on homonyms, an approach that is advocated here (see also (Killgariff, 1992))." ></td>
	<td class="line x" title="94:166	In CORZLEX homonyms are simply assigned two or more underspecified semantic types, that need to be disambiguated in a traditional way." ></td>
	<td class="line x" title="95:166	There is however an added value also here because each disambiguated type can generate any number of context dependent interpretations." ></td>
	<td class="line x" title="96:166	3 Adapting CORELEx to Domain Specific Corpora The underspectfied semantic type that CORELEX assigns to a noun provides a basic lexical semantic structure that can be seen as the class-wide backbone semantic description on top of which specific information for each lexical item is to be defined." ></td>
	<td class="line x" title="97:166	That is, doors and gates are both artifacts but they have different appearances." ></td>
	<td class="line x" title="98:166	Gates are typically open constructions, whereas doors tend to be solid." ></td>
	<td class="line x" title="99:166	This kind of information however is corpus specific and therefore needs to be adapted specifically to and on the basis of that particular corpus of texts." ></td>
	<td class="line x" title="100:166	This process involves a number of consecutive steps that includes the probabilistic classification of unknown lexical items: 1." ></td>
	<td class="line x" title="101:166	Assignment of underspecified semantic tags to those nouns that are in CORELEX 2." ></td>
	<td class="line x" title="102:166	Running class-sensitive patterns over the (partly) tagged corpus 3." ></td>
	<td class="line x" title="103:166	(a) Constructing a probabilistic classifier from the data obtained in step 2." ></td>
	<td class="line x" title="104:166	(b) Probabilistically tag nouns that are not in CORELEX according to this classifier 4." ></td>
	<td class="line x" title="105:166	Relating the data obtained in step 2." ></td>
	<td class="line x" title="106:166	to one or more qualia roles Step 1." ></td>
	<td class="line x" title="107:166	is trivial, but steps 2." ></td>
	<td class="line x" title="108:166	through 4." ></td>
	<td class="line x" title="109:166	form a complex process of constructing a corpus specific semantic lexicon that is to be used in additional processing for knowledge intensive reasoning steps (i.e. abduction (Hobbs et al. , 1993)) that would solve metaphoric, metonymic and other non-literal use of language." ></td>
	<td class="line x" title="110:166	3.1 Assignment of CORELEX Tags The first step in analyzing a new corpus involves tagging each noun that is in CORELEX with an underspecified semantic tag." ></td>
	<td class="line x" title="111:166	This tag represents the following information: a definition of the type of the noun (FORMAL); a definition of types of possible nouns it can stand in a part-whole relationship with (CONSTITUTIVE); a definition of types of possible verbs it can occur with and their argument structures (AGENTIVE / TELIC)." ></td>
	<td class="line x" title="112:166	CORELEX is implemented as a database of associative arrays, which allows a fast lookup of this information in pattern matching." ></td>
	<td class="line x" title="113:166	3.2 Class-Sensitive Pattern Matching The pattern matcher runs over corpora that are: part-of-speech tagged using a widely used tagger (Brill, 1992); stemmed by using an experimental system that extends the Porter stemmer, a stemming algorithm widely used in information retrieval, with the Celex database on English morphology; (partly) semantically tagged using the CORELEX set of underspecified semantic tags as discussed in the previous section." ></td>
	<td class="line x" title="114:166	There are about 30 different patterns that are arranged around the headnoun of an NP." ></td>
	<td class="line x" title="115:166	They cover 30 the following syntactic constructions that roughly correspond to a VP, an S, an NP and an NP followed by a PP:  verb-headnoun  headnoun-verb  adjective-headnoun  modiflernoun-headnoun  headnoun-preposition-headnoun The patterns assume NP's of the following generic structure 2: PreDet* Det* Num* (Adj INamelNoun)* Noun The heuristics for finding the headnoun is then simply to take the rightmost noun in the NP, which for English is mostly correct." ></td>
	<td class="line x" title="116:166	The verb-headnoun patterns approach that of a true 'verb-obj' analysis by including a normalization of passive constructions as follows: \[Noun Have?" ></td>
	<td class="line x" title="117:166	Be Adv?" ></td>
	<td class="line x" title="118:166	Verb\] =~ \[Verb Noun\] Similarly, the headnoun-verb patterns approach a true 'sub j-verb' analysis." ></td>
	<td class="line x" title="119:166	However, because no deep syntactic analysis is performed, the patterns can only approximate subjects and Objects in this way and I therefore do not refer to these patterns as 'subject-verb' and 'verb-object' respectively." ></td>
	<td class="line x" title="120:166	The pattern matching is class-sensitive in employing the assigned CORELEX tag to determine if the application of this pattern is appropriate." ></td>
	<td class="line x" title="121:166	For instance, one of the headnoun-preposition-headnoun patterns is the following, that is used to detect partwhole (CONSTITUTIVE) relations: PreDet* Det* Num* (Adj \[ Name \[ Noun)* Noun of PreDet* Det* Num* (Adj \[NameJNoun)* Noun Clearly not every syntactic construction that fits this pattern is to be interpreted as the expression of a part-whole relation." ></td>
	<td class="line x" title="122:166	One of the heuristics we therefore use is that the pattern may only apply if both head nouns carry the same CORELEx tag or if the tag of the second head noun subsumes the tag of the first one through a dotted type." ></td>
	<td class="line x" title="123:166	That is, if the second head noun is of a dotted type and the first is of one of its composing types." ></td>
	<td class="line x" title="124:166	For instance, 'paragraph' ~The interpretation of '*' and ''?" ></td>
	<td class="line x" title="125:166	in this section follows that of common usage in regular expressions: 'w indicates 0 or more occurrences; ''?" ></td>
	<td class="line x" title="126:166	indicates 0 or 1 occurrence and 'journal' can be in a part-whole relation to each other because the first is of type information, while the second is of type information*physical." ></td>
	<td class="line x" title="127:166	Similar heuristics can be identified for the application of other patterns." ></td>
	<td class="line x" title="128:166	Recall of the patterns (percentage of nouns that are covered) is on average, among different corpora (wsJ, BROWN, PDGF a corpus we constructed for independent purposes from 1000 medical abstracts in the MEDLINE database on Platelet Derived Growth Factor and DARWIN the complete Origin of Species), about 70% to 80%." ></td>
	<td class="line x" title="129:166	Precision is much harder to measure, but depends both on the accuracy of the output of the part-of-speech tagger and on the accuracy of class-sensitive heuristics." ></td>
	<td class="line x" title="130:166	3.3 Probabilistic Classification The knowledge about the linguistic context of nouns in the corpus that is collected by the pattern matcher is now used to classify unknown nouns." ></td>
	<td class="line x" title="131:166	This involves a similarity measure between the linguistic contexts of classes of nouns that are in CORELEX and the linguistic context of unknown nouns." ></td>
	<td class="line x" title="132:166	For this purpose the pattern matcher keeps two separate arrays, one that collects knowledge only on COrtELEx nouns and the other collecting knowledge on all nouns." ></td>
	<td class="line oc" title="133:166	The classifier uses mutual information (MI) scores rather than the raw frequences of the occurring patterns (Church and Hanks, 1990)." ></td>
	<td class="line o" title="134:166	Computing MI scores is by now a standard procedure for measuring the co-occurrence between objects relative to their overall occurrence." ></td>
	<td class="line o" title="135:166	MI is defined in general as follows: y) I ix y) = log2 P(x) P(y) We can use this definition to derive an estimate of the connectedness between words, in terms of collocations (Smadja, 1993), but also in terms of phrases and grammatical relations (Hindle, 1990)." ></td>
	<td class="line o" title="136:166	For instance the co-occurrence of verbs and the heads of their NP objects iN: size of the corpus, i.e. the number of stems): N Cobj (v n) = log2 /(v) /(n) N N All nouns are now classified by running a similaxity measure over their MI scores and the MI scores of each CoRELEx class." ></td>
	<td class="line x" title="137:166	For this we use the Jaccard measure that compares objects relative to the attributes they share (Grefenstette, 1994)." ></td>
	<td class="line x" title="138:166	In our case the 'attributes' are the different linguistic constructions a noun occurs in: headnoun-verb, adjective-headnoun, modifiernoun-headnoun, etc. The Jaccard measure is defined as the number of attributes shared by two objects divided by the total number of unique attributes shared by both objects: A A+B+C A : attributes shared by both objects B : attributes unique to object 1 C : attributes unique to object 2 The Jaccard scores for each CORELEx class are sorted and the class with the highest score is assigned to the noun." ></td>
	<td class="line x" title="139:166	If the highest score is equal to 0, no class is assigned." ></td>
	<td class="line x" title="140:166	The classification process is evaluated in terms of precision and recall figures, but not directly on the classified unknown nouns, because their precision is hard to measure." ></td>
	<td class="line x" title="141:166	Rather we compute precision and recall on the classification of those nouns that are in CoreLex, because we can check their class automatically." ></td>
	<td class="line x" title="142:166	The assumption then is that the precision and recall figures for the classification of nouns that are known correspond to those that are unknown." ></td>
	<td class="line x" title="143:166	An additional measure of the effectiveness of the classifter is measuring the recall on classification of all nouns, known and unknown." ></td>
	<td class="line x" title="144:166	This number seems to correlate with the size of the corpus, in larger corpora more nouns are being classified, but not necessarily more correctly." ></td>
	<td class="line x" title="145:166	Correct classification rather seems to depend on the homogeneity of the corpus: if it is written in one style, with one theme and so on." ></td>
	<td class="line x" title="146:166	Recall of the classifier (percentage of all nouns that are classified > 0) is on average, among different larger corpora (> 100,000 tokens), about 80% to 90%." ></td>
	<td class="line x" title="147:166	Recall on the nouns in CoRELEx is between 35% and 55%, while precision is between 20% and 40%." ></td>
	<td class="line x" title="148:166	The last number is much better on smaller corpora (70% on average)." ></td>
	<td class="line x" title="149:166	More detailed information about the performance of the classifier, matcher and acquisition tool (see below) can be obtained from (Buitelaar, forthcoming)." ></td>
	<td class="line x" title="150:166	3.4 Lexical Knowledge Acquisition The final step in the process of adapting CORELEx to a specific domain involves the 'translation' of observed syntactic patterns into corresponding semantic ones and generating a semantic lexicon representing that information." ></td>
	<td class="line x" title="151:166	31 There are basically three kinds of semantic patterns that are utilized in a CORELEX lexicon: hyponymy (sub-supertype information) in the FORMAL role, meronymy (part-whole information) in the CONSTITUTIVE role and predicate-argument structure in the TELIC and AGENTIVE roles." ></td>
	<td class="line x" title="152:166	There are no compelling reasons to exclude other kinds of information, but for now we base our basic design on ~, which only includes these three in its definition of qualia structure." ></td>
	<td class="line x" title="153:166	Hyponymic information is acquired through the classification process discussed in Sections 2.2 and 3.3." ></td>
	<td class="line x" title="154:166	Meronymic information is obtained through a translation of various VP and PP patterns into 'has-part' and 'part-of' relations." ></td>
	<td class="line x" title="155:166	Predicate-argument structure finally, is derived from verb-headnoun and headnoun-verb constructions." ></td>
	<td class="line x" title="156:166	The semantic lexicon that is generated in such a way comes in two formats: T2), a Type Description Language based on typed feature-logic (Krieger and Schaefer, 1994a) (Krieger and Schaefer, 1994b) and HTML, the markup language for the World Wide Web." ></td>
	<td class="line x" title="157:166	The first provides a constraintbased formalism that allows CORELEX lexicons to be used stralghtforwardiy in constraint-based grammars." ></td>
	<td class="line x" title="158:166	The second format is used to present a generated semantic lexicon as a semantic index on a World Wide Web document." ></td>
	<td class="line x" title="159:166	We will not elaborate on this further because the subject of semantic indexing is out of the scope of this paper, but we refer to (Pustejovsky et al. , 1997)." ></td>
	<td class="line x" title="160:166	3.5 An Example: The PDGF Lexicon The semantic lexicon we generated for the PDGF corpus covers 1830 noun stems, spread over 81 CORELEX types." ></td>
	<td class="line x" title="161:166	For instance, the noun evidence is of type communication.psychological and the following representation is generated: 4 Conclusion In this paper I discuss the construction of a new type of semantic lexicon that supports underspecifled semantic tagging." ></td>
	<td class="line x" title="162:166	Traditional semantic tagging assumes a number of distinct senses for each lexical item between which the system should choose." ></td>
	<td class="line x" title="163:166	Underspecified semantic tagging however assumes no finite lists of senses, but instead tags each lexical item with a comprehensive knowledge representation from which a specific interpretation can be constructed." ></td>
	<td class="line x" title="164:166	CORZLEx provides such knowledge representations, and as such it is fundamentally different from existing semantic lexicons like WORDNET." ></td>
	<td class="line x" title="165:166	32 'evidence FORMAL '= \[ARG1 = commlmlcation\]\] CLOSED LARG2 psychological J J CONSTITUTIVE -~ I HAS-PAI~ ----TELIC = ip ov,.e \] FIRST L ARG-STRUCT ---~  REST ---.o. FIRST = structure\] 1 REST  Figure 10: Lexical entry for: evidence Additionally, it was shown that CoI~LEx provides for more consistent assignments of lexical semantic structure among classes of lexical items." ></td>
	<td class="line x" title="166:166	Finally, the approach described above allows one to generate domain specific semantic lexicons by enhancing CORELEX lexical entries with corpus based information." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-0709
Statistical Methods For Retrieving Most Significant Paragraphs In Newspaper Articles
Abracos, Jose;Pereira Lopes, Jose Gabriel;"></td>
	<td class="line x" title="1:16	7Statistical methods for retrieving most significant paragraphs in newspaper articles Jos~ Abrafos Departamento de Inform~ca." ></td>
	<td class="line x" title="2:16	Faculdade de Ci~nclas e Tecnologla / UNL 2825 Monte da Capanca, Portugal  jea@dt fct uni pt." ></td>
	<td class="line x" title="3:16	Gabriel Pereira Lopes  ~ Departamento de Inform~Uca, Faculdadede Cl~ncms e Tecnologta I UNL 2825 Monte da Capanca, Portugal gpl@& fct unl pt Abstract Retrieving a most stgulficant paragraph m a newspaper arUcle can act as a kind of surnmanzatmn It can gwe the human reader some hints on the contents of the arucle and help him to decide whether It deseei'ves a full readmg or not It may also act as a filter for a robust natural language understanding system, to extract relevant mformatton from that paragraph m order to enable conceptual mformauon retrieval Talang a newspaper arUcle and a base corpus, word co-occurrences w3th higher resolving power are ~dent~fied These co-occurrences are used to estabhsh hnks between the paragraphs of the arUcle The paragraph which presents the larger number of hnks tO other paragraphs ~s considered a most slgmficant one Though designed and tested for the Portuguese language, the staUshcal nature of our proposal should ensure ns portabtlny to other languages 1." ></td>
	<td class="line x" title="4:16	Introduction The advantages of using stattsucal methods when dealing w~th large volumes of text are known Namely, thelr capabdny of facing any kind of subjects, without feanng the most baroque syntacucal structures, and always produ~ng an answer whlch, though varying m habthty, ts always more useful than 'fad' The scope of the present work Is the use of stat|st|cal methods to remeve a most ssgn~ficant paragraph sn a newspaper amcle The method we propose nught help a reader m getung a qmck ghmpse of the contents of a newspaper and dccldmg whlch articles deserve a full reading It can besldes facthtate searches through journalmttc text bases But we are also interested on pruning the amount of text to be automatically processedfor robust understanding of natural language Thls wdl enable conceptual based document representation and conceptual mformat~on retrieval (Mauldm 1991) The process Is based on rcmeving the co-occurrences wlth hlgher resolving power m each document, using them to estabhsh hnks between paragraphs, and selecting the paragraph with more hnks to other paragraph s Tests performed vdth the support of a base corpus of about 500 thousand words were able to identify a most slgn!ficant paragraph m 7 out of I0 newspaper arucles We present, m annex, the results of some experiments concerning one of the arucles 2." ></td>
	<td class="line x" title="5:16	Antecedents An Idea borrowed from Information Retrieval, ts that a term will be so more relevant m a document the more frequently n occurs m that document, and the less frequently It occurs m a base corpus Maarek (1992), followmg other authors, considers that using paws of words as an indexing umt ~s more adequate to mformauon retrieval than usmg single words IntmUvely, n is planslble to adnut that, for mstanee, the pmr \[rile system\] ts far more mformauve than the words file and .~stem taken m lsolatton Maarek alms at remeving pmrs of lextcally related words In Enghsh, 98% of the lexlcal relations occur between words within a span of 5 words m a sentence." ></td>
	<td class="line oc" title="6:16	s e, the window to consider when extracting words related to word w, should span from postttuon w-5 to w+5 Maarek also defines the resolwng power of a parr m a document d as P = ~'Pd log Pc where Pd is the observed probabshty of appearance of the pan' m document d, Pc the observed probabdny of the pmr recorpus, and -log Pc the quantity of mformauon assocmted to the pmr It Is easdy seen that p wall be h|gher, the higher the frequency of the pmr m the document and the lower sts frequency m the corpus, which agrees wlth the sdea presented at the begmnmg of this sectton Church and Hanks (1990) propose the apphcatlon of the concept of mutual mformatton e(x,y) ~,(x.y) = hog2 ecx)e(y) 51 to the retrieval, ro a corpus, of pairs of lextcally related words They alsoconslder a word span of :e5 words and observe that 'roterestrog' pmr, s generally present a mutual mformatxon above 3 Salton and.Allan (1995) foc~as on paragraph level Each paragraph Is represented by a weighed vector, where each element is a term (typically." ></td>
	<td class="line x" title="7:16	word stems, a_f~r excluchng those in a stop hsO The weight of each term reflects (as usual) posmve~y its frequency in the document and negatively its frequency m the corpus Usrog a roeasure of smulanty between vectors and applying a sumlanty threshold, one can define which paragraphs are linked They then constder of central tmportance the paragraph with the largest number of conneottons to other paragraphs The idea underl3ang the present work was to integrate these 3 approaches and to apply the resulting roethod to newspaper articles, w~th the purpose of retnewng, ro each article, a roost mgmficant paragraph 3." ></td>
	<td class="line o" title="8:16	The proposed approach As stated before, the method of Church and Hanks identifies pmrs of lexlcally related words So, for instance, the pair \[conselho seguranfa\] (security conned), with an assocmted mutual mforn~uon of 5.3, can be considered as a potential mdexang term, while the pan' \[para a\] (to the), though 63 tunes roere frequent ro our corpus, having a mutual roformauon of 0.7, can be excluded We have then a erttenon for exclusion, that dispenses with the need for stop hsts, and that alms at assunng the exlstence of a leracal relation between the words of the rematrong pairs But not all pans of lexlcally related words are good rodexmg terms of a document The pair should also meet the reqmrement of being relevant m the considered document The method of Maarek proposes a measure of the resolvrog power of each pair ro the concerned document, thus enabling the selection, among all the poten.al indexing terms, of those that are relevant m each document For rostanco, \[estados umdos\] (united states) has a hxgh mutual roformatmn (8 1) but it can be of little relevance m an article about the hberatton of prisoners by the Serbs of Sarajevo (p=0007) The experiments earned out point to a threshold of the resolwng power around 0 01 We consider as relevant ro a document only the pairs vath a resolvrog power above this threshold When the same pmr occurs ro chfferent paragraphs of the same document, hnks can be estabhshed between those paragraphs At flus point, we only consider pairs that were not excluded ro prewous steps (mutual roformatton > 3 and resolwng power > 0 01) Though, each hnk Is not hnuted to pairs of words In fact, the 52 wider the hnk, the higher its relevance After processing a document, we often get overlapping pmrs For instance, m an amcle where the expression dos tr~s antJgos behgerantea (of the three former contenders) ts used repeatedly, the foll0vang pmrs were retrieved \[tr~s behgerantes\] \[an~gos behgerantes\] \[dos behgerantes\] 1 By ohserwng the overlap of these pmrs ro the very document, a single hnk can be retrieved, m the form of the tuple \[dos trOs antlgos behgerantes\] Adaptmg the roethod of Salton and Allan, we can formulate the hypothesis that the paragraph vath the larger number of hnks to other paragraphs would be of central impox~tance in the document In summary, the steps of the proposed method are *m a base corpus, compute the frequency of each word and the frequency of each co-occurrence, consadenng a window spanrong from posihon 14,-5 to w+5, *to each document c~mpute, smuIarly, the frequency of each word and each co-occurrence, *exclude, from the co-r.e~m'ences \]dent:fled m the document, those presenting a mutual mformatlon or a resolving power under the defined thresholds (I(x,y) < 3 or p < 0 01),  take the selected pans and group the overlapping ones, the resulting tuples (pairs and groups of pairs) occmTmg repeatedly ro different paragraphs estabhsh hnks between those paragraphs, *hypothetically, the paragraph presenting a larger number of hnk~ to other paragraphs wall be of central,mportanco in the document It should be noted that this proposal, compared to Salton and Allan's, has the advantages (at least ro theory) of avotchng the use, always arbitrary, of stop hsts 2, and of basing the calculations exclusively on the tuples that are relevant ro the document, instead of using the heavy vectors containing all the terms of each paragraph We don't have, so far, enough data to make any clmm about the comparative quahty of the links 1 pairs \[tr~s antlgos\] \[dos an#gos\], though considered relevant, didn't score enough mutuat reformat=on to be selected 2 the relevance of a word depends on the context, so, we prefer not to a pnon exclude any word, by sandtng it to a stop kst In fact, some of the tuplas we retheved as relevant include words that would otherw=se be pad of such a Ist An example m the pmr \[n~o ahnhados\] (nonaligned) where the word n,~o (not) though quite significant tn context, would be excluded wa stop #st . i I |j :!" ></td>
	<td class="line x" title="9:16	I i I i 1 I 4." ></td>
	<td class="line x" title="10:16	Applying the proposal The base corpus was uuually bruit vath news from Lusa news agency, m a total of 216 319 words Later, news from '0 P6bllco' newspaper (about 90 000 w~ds) and more news from Lusa were added, and the total reached 537 085 words The consequences of tins enlargement will be chscussed m the next secUon Experunents were made vath 10 articles from 'O Pdbhco', that chdn't belong to the corpus Both the corpus and the documents were subjected to a very elementary pre-processmg, wluch basically 6onslsted of  convemng all uppercase letters to lowercase * convemng all numbers to NUMERO (NUMBER) 3  ehmmaUng all non-letter characters Words or co-occurrences not present m the corpus, if occumng m a document, would lead, respectively m the computatlon of mutual mformatlon or resolwng power, to hwdmg by 0 or to log2 0 To prevent sltuatton, in such cases, and only for calculatlon purposes, the document is added to the corpus By doing so, though, the mutual mformatmn becomes overestamated For instance, the parr \[ha eslav6ma\] (m slavoma) occurs 3 tnnes in an article As eslav6ma doesn't occur m the corpus, the artacle m hdded to the corpus, for calculatmn purposes only concernmg tlus pair The result is the presuppositmn that, despite the qmte low frequencies of eslav6ma and \[na eslav6ma\], almost every tune the word eslav6ma occurs it IS preceded by ha, the mutual mfc~naUon of the parr being then artificially raised To overcome this overest~maUon, 2 adthUonal mutual mformauon thresholds were defined *tf one of the words (or both) doesn't occur m the cOrpus, it must be I(x,y) > 10, ." ></td>
	<td class="line x" title="11:16	* if both words occur in the corpus but they never co-occur, it must be I(x,y) > 8 These lurers are not defimtlve They were suggested by the experiments camed out, which were though too few to ensure their defimuon with certainty ' Theamclesanalyzed m those experiments are in average 500 words long Pre-processmg and frequency calculations are obtmned through gawk commands (Umx) The calculaUons of mutual lnformatmn, resolwng power and the filtenng of co.ocoxrrences through these criteria are implemented in C 3 the choee of reducing all numbers to NUMERO has to do wdh  the kind of documents under study,,n texts about law, for instance, the ~stmctmn between Law 12/86 and Law 47/95 may be important 53 Nevertheless, gwen the experimental nature of the system, optlm~zaUon was no mmn concern Searches m the file contammg the co-occurrences of the corpus (22 MB) are sequenual, this source of mefficien~." ></td>
	<td class="line x" title="12:16	being only palhated by prevmously sorting the' co-oocurrenc~s by dscreasmg order of probablhty In what concerns the arUcle presented m Annex A (441 words), pre-processmg, calculatmn of freque~cles and sorting takes about 5 seconds The calenlaUons revolved m selecting and somng co-ecru-fences take ~about 8 minutes 4 By the charactenst~c~ of the lmplementatmn, tins last tlme m (hre~y propo~onal, among other factors, to the amount of words m the corpus and to the amount of unknown words that occur m the document Out of the 10 arucles that were analyzed, the method we propose achieved the ~denttficaUon of the most slgmficant paragraph in 7 and was clearly n~staken m 1 In the remaining 2 articles, the~e doesn't seem to be, mtmUvely, a most representative paragraph Thls lntultmn m the evaluatton of the results is necessarily subjecUve N0twlthstanchng the very small number of arttcles involved in this test, it may be ~mous to compare our results vath those that would be obtmned by just picking up the 1 st paragraph of each amcle, or even both the la and the 2~ paragraphs # of articles removes a most slsmt cent  extstance of a most slgnd~cant  is not clear the  retrieved is not a most slgmficant one our proposal 7 2 1=+ I ~  2~d 5 6 2 2 3 2 5." ></td>
	<td class="line x" title="13:16	Discussion of the results The proposed method ignores a series of basic questmns, namely Lemmatization All the calculauons are made vathout any attempt of umfymg plural forms ruth singular forms, dtfferent conjugations of a same verb, etc Nevertheless, it doesn't look clear that new hnks, obtained by grouping words that, though shanng a common stem, were m fact used m chsUnct forms, wdl necessarily mcrease the performance of the system Would it make sense to unify tribunal de famlha (court that deals with famdy cases) vath tribunal fanuhar (farmhar court)?" ></td>
	<td class="line x" title="14:16	And 4 tzmes measured m a DECstabon 5000/200 dwfltos do homem (human rights) vnth dwezto dos homens 0aw of men)~ Anaphora resolution  ~ Though the umficauon of the anaphor with the antecedent, m most cases, makes obvtously sense, anaphora resolutzon would reqmre a complete analysis of the text, totally outside the scope of this proposal Curiously, m the only experiment that was made of full anaphora ~ resoluUon, the number of hnks between paragraphs substanUally increased, but the paragraph retrieved as most sigmficant the firstwas no longer the one obtmned by mtumon the second s -(refer to results m the annex) Unification of synonyms, hyponyms, hyperonyms The same arguments presented about lemmaUzaUon can apply here The experiment of umfymg lmUals vmh full names e g ONU ~ NafOes Umdas (UN ~ Umted NatJons) simple to do with the help of a thesaurus, gave s~xmlar results to those of appl3ang anaphora resoluUon Size of the co-o~currence window The wmdow spanning from posluon w-5 to w+5, defined for Enghsh language, may be not the most adequate to Portuguese No further expertments were performed vath other sizes of windows Indexing terms The resolving power criterion rams at assuqng that the selected co-occurrences are relevant m the document being analyzed A manual mdeyang could, nevertheless, choose other terms, pess~bly even foreign  to the document In fact, in an amcle describing a coup there may be references to derrube de governantes (overthrowmg of rulers), tomada do poder (tahng the corpora are though qmte small Nothing m&cates that the results would stand a more substanttal increase of the corpus We also tried to find out how far estabhslimg links could help in identifying a structure of the text The structures obtmned, by connecting lteratlvely each new paragraph to the one wRh more hnks m common, are not conclustve In some cases they are close to a posstble mtmUve structure of the text, while m other they dtverge considerably The structare obtained for the text m annex was among the most plausible 6." ></td>
	<td class="line x" title="15:16	Conclusion The methodology we propose integrates the concepts of mutual mformat~on associated to a pmr of words, resolwng power of that paw m a document and estabhshmg of links between paragraphs of a document, wRh the purpose of retrieving a most representattve paragraph The methods we use are pureIy staustacal Nevertheless, notw~thstan&ng their s~mphclty, the rough stmphficauons referred m the prevaous section and the extguousness of the corpus, the results seem quite Interesting The habihty of these results is though hn'nted by the amount of tests that weze performed and by an evaluation based on the mtmUon of the authors Probably, an increase of the corpus and the refinement of the process wtth some, even elementary, hngmsnc cntena, would benefit the performance Though designed and tested for the Portuguese language, the stattst~cal nature of tlus methodology should ensure its portabflRy to other languages power), vothout any explicit expression golpe de estado (coup) We present, lfi annex,, the results of processing a document using the miual corpus (216 319 words) and the augmented one (537 085 words) In what concerns the co-occurrences that were selected as estabhshmg hnks, one can notice the excluson of \[da ONU\] (of the UH) m the 2nd case (m the 1st case it already presented a mutual mformatmn very close to the threshold) All the other selected co-occurrences remain, and their ordenng m terms of resolving power m also preserved The paragraph retrieved as central ~s the same Both 5 in fact, in thin article, central reformation seems to concentrate." ></td>
	<td class="line x" title="16:16	the 2 mSal paragraphs, the 2rid rederahng most of the mformabon introduced by the 1st The 2rid refers the 2 actors (UN and NATO) whose achons vail be analysed latter This may suggest some preference agmnst the 1st Anyway, each one of these 2 paragraphs can be consclered as representatwe of the text occurrence of the,.~" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W97-0711
A Scalable Summarization System Using Robust NLP
Aone, Chinatsu;Okurowski, Mary Ellen;Gorlinsky, James;Larsen, Bjornar;"></td>
	<td class="line x" title="1:27	A Scalable Summarization System Using Robust NLP Chinatsu Aonet, Mary Ellen Okurowski~ t, James Gorlinskyt, Bjornar Larsent tSRA InternatlonM 4300 Fair Lakes Court Falrfax, VA 22033 {aonec, gorhnsk, larsenb}@sra corn ~Depaxtment of Defense 9800 Savage Road Fort George G Meade, MD 20755-6000 meokuro@afterhfe ncsc nnl Abstract We describe a scalable summarization system which takes advantage of robust NLP technology such as corpus-based statlshcal NLP techmques, information extractmn and readily available on-hne resources The system attempts to compensate for the bottlenecks of traditional frequency-based, knowledge-based or discourse-based summanzatlon approaches by uhhzlng features derived by these robust techniques Prehrmnary evaluation results are reported, and the multi-dimensional summary viewer is described 1 Introduction Summarization research and system development can be broadly characterized as frequency-based, knowledge-based or discourse-based These categories correspond to a continuum of increasing understanding of a text and increasing complextty in text processing Earliest attempts at summarization (Luhn, 1958, Edmundson, 1969, Rush, Salvador, and Zamora, 1971) essentially rehed on lexlcal and locahonal mformation within the text, 1 e, frequency of words or key terms, their proxnmty, and locatmn within the text More recent adaptations of tlns approach have employed an automated method to combine these types of feature sets through classification techniques (Kupmc, Pedersen, and Chen, 1995) O r have drawn upon tradlhonal information retrieval indexing methods to incorporate knowledge of a text corpus (Brandow, Mltze, and Rau, 1995) To a large extent, these types of shallow approaches are ignorant of dommn knowledge and the text macrostructure They create summaries by extracting sentences from the original document Knowledge-based approaches generally depend on rich domain knowledge sources to interpret the conceptual structure of the text Systems like TOPIC (Relmer and Hahn, 1988), SUSY (Fum, Gmda, and Tasso, 1985) or SCISORS (Ran, Jacobs, and Zermk, 1989) parse domaan specific texts and create conceptual representahons for the generation of text summarms These types of knowledgeba.~d systems apply knowledge of the domain to characterize specific conceptual knowledge of a text Palce (Pvace and Jones, 1993) provides a good example of the role of thts conceptual mformahon and thloff(Rlloff, 1995) gives a method for automahcally identifying relevant concepts lughly correlated with a category of interest Because these systems create a rich conceptual representation, there are multiple ways m whlcha text summary may be created For example, SUMMONS (McKeown and Radev; 1995) generates a text summary from such a template representahon, whle (Maybury, 1995) describes mulhpie methods for selecting events and presenting event summaries Knowledge-based approaches are usually very knowledge-intensive and domvan-specific Discourse-based approaches are grounded m theorms of text cohesion and coherence and vary conmderably m how much they push the lmnts of text understanding and the complemty as well as automahon of that processing Spearheaded by the lack of cohesion and coherence m extracts produced by frequency-based approaches, much of the work typifying discourse-based approaches focuses on lmgmstic processing of the text to identify the best cohesive sentence candidates (Palce, 1990, Johnson et al, 1993) or the best sentence candidates for represent' mg the rhetorical structure of the text (Mnke et al,  1994) Both approaches revolve parsing the text and analyzing dlscoarse relations to select sentences for extractmn Frequency-based approaches (Brandow, Mltze, and Rau, 1995) may incorporate heurmhcs to handle readabilityrelated issues and knowledge-based approaches  systematically perform discourse processmg m analyzing and condeusmg the text, but m a broad classificatmn schema It is the discoursebased approaches that tend to focus on the text macrostructure and surface clues to that structure At the far end of the continuum lies work by Sparck Jones (Jones, 1993, Jones, 1995) m describing a 66 ! !" ></td>
	<td class="line x" title="2:27	m, l i \[ ii li I i Ii a ! I Ii il l 1 manual method for source texi~ representatlon based on hngmstlc, dommn and communlcatlve reformation From the NLP technology point of wew, dincourse theory LS the least understood among subfields of hnguistlcs Our work addresses challenges encountered in these previous approaches by applying robust and proven NLP techmques such as corpus-based statmtlcal NLP,." ></td>
	<td class="line oc" title="3:27	robust mforrmatlon extractlon, and readlly-avmlable on-hne NLP resources These techtuques and resources allow us to create a richer indexed source of Imgmstlc and domain knowledge than other frequency approaches Our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon, ohtinned from automated methods m contrast to labor-lntenslve, discourse-based approaches Moreover, our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg hand-crafting domain-dependent rules of the knowledge-based approaches Our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 System Description Our summarization system DlmSum consmts of the Summarization Server and the Summarlzatzon Chent The Server extracts features (the Feature Extractor) from a document using various robust NLP techmques, described In Sectzon 2 1, and combines these features (the Feature Combiner) to basehne multiple combinations of features, as described m Section 2 2 Our work m progress to automattcally tram the Feature Combiner based upon user and apphcatlon needs m presented in Section 2 2 2 The Java-based Chent, which wdl be dmcnssed In Section 4, provides a graphical user interface (GUI) for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated sumInarles 2.1 Extracting Stlmmarization Features In this section, we describe how we apply robust NLP technology to extract summarization features Our goal IS to add more mtelhgence to frequencybased approaches, to acqmre domain knowledge In a more automated fashion, and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 2.1.1 Going Beyond a Word Frequency-based summarization systems typically use a single word stnng as a umt for counting frequencies Whde such a method IS very robust, it ignores the semantic content of words and their potential membership m multi-word phrases For example, zt does not dmtmgumh between 'bill' m 'Bdl Table 1 Collocations with 'chlps' {potato tortdla corn chocolate b~gle} chips {computer pentmm Intel macroprocessor memory} chips {wood oak plastlc} cchlps bsrgmmng clups blue clups mr chips Clmton' and 'bill' in 'reform bill' This may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multl-word phrases For DlrnSum, we use term frequency based on tf*Idf (Salton and McGdl, 1983, Brandow, Mitze, and Rau, 1995) to derive ssgnature words as one of the summarization features If single words were the sole basra of countmg for our summarization application, nome would be introduced both m term frequency and reverse document frequency However, recent advances in statmtlcal NLP and information extraction make it possible to utilize features which go beyond the single word level Our approach is to extract multi-word phrases automatlcally with high accuracy and use them as the basic unit in the summarization process, including frequency calculation Ftrst, just as word association methods have proven effective m lemcal analysis, e g (Church and Hanks, 1990), we are exploring whether frequently occurring Collocatlonal reformation can improve on simple word-based approaches We have preprocessed about 800 MB of LA tlmes/Wastnngton Post newspaper articles nsmg a POS tagger (Bnll, 1993) and derived two-word noun collocations using mutual information The." ></td>
	<td class="line x" title="4:27	result included, for example, varlons 'chips' phrases as shown m Table 1 The word 'ch~ps' occurred 1143 times m this corpus, and the table shows that thin word m semantically very amblguons In word associatmns, It can refer to food, computer components, abstract concepts, etc By incorporating these conocatlons, we can dlsamblguate dtfferent meamngs of 'chips' Secondly, as the recent Message Understanding Conference (MUC-6) showed (Adv, 1995), the accuracy and robustness of name extraction has reached a mature level, equahng the level of human performance m accuracy (lind-90%) and exceeding human speed by many thousands of times We employed SRA's NameTag TM (Krupka, 1995) to tag the aforementioned corpus with names of people, entztIes, and places, and derived a baseline database for tffIdf calculation In the database, we not only treated multi-word names (e g, 'Ball Clinton') as single tokens but also dmamblguated the semantic types of names so that, for instance, the company 'Ford' 67 ts treated separately from President 'Ford' Our approach is thus different from (Kupiec, Pedersen, and Chen, !995) where only capitalization reformation was used to identify and group various types of proper names 2.1.2 Acquiring Knowledge of the Domain In knowledge-based summarization approaches, the biggest challenge ts to acquire enough domain knowledge to create conceptual representations for a text Though summarization from conceptual representation has many advantages (as discussed m Section 1), extracting such representations constrains a system to domain dependency and is too knowledgeintensive for our approach Instead, we took an automatic and robust approach where we acqmre some domain knowledge from a large corpus and incorporate that knowledge as summarization features m the system We incorporated corpus knowledge m three ways, that is, by using a large corpus baseline to calculate'ldf values for selecting signature words, by denying collocations statistically from a large corpus, and by creating a word association index derived from a large corpus (Jmg and Croft, 1994) With thin method, the system can automatically adapt to each dmtmct domain, hke newspapers vs legal documents without manually developing domain knowledge Domain knowledge is embraced in szgnature words, which indicate key concepts of a given-document, in colIocat:on phrases, which provide richer key concepts than single-word key concepts (e g 'appropriations bill,' 'ommbus bill,' 'brady ball,' 'reconciliation bill,' 'crime bill,' 'stopgap bdl,', etc ), and in their assoczated words, which are clusters of dommn related terms (e g, 'Bayer' and 'aspirin,' 'Columbia Raver' and 'gorge,' 'Dead Sea' and 'scrolls') 2.i.3 Recognizing sources of Discourse Cohesion and Coherence Past research (Pmce, 1990) has described the negative impact on abstract quality of fathng to perform some type of discourse processing Since dincourse knowledge (e g, effective anaphora resolution and text segmentation) cannot currently be automatlcally acquired easily wlth high accuracy and robustness, heuristic techniques are often employed in summarization systems to suppress sentences with interdependent cohesive markers However, there are several shallower but robust methods we can employ now to acquire some discourse knowledge Namely, we exploit the dmcourse features of lexlcal items within a text by using name aliases, synonyms, and morphological variants Within a document, subsequent references to full names are often aliases Thus, linking name aliases provides some indication as to which sentences are interrelated, as shown below The Institutional Revolutionary Parry, or PRI, capped sis landmark assembly to reform,tself w,th a.Nourish of pomp and prom,ses Among the measures coming out of the assembly's fiercest pubhc debate, zn which party members rose up agaznst the,r leadership Saturday nlght, are new requsrements for future PRI pres-,denttal cand, dates, quahficatwns that netther ~eddlo nor any of Mezzco's prevzous four pres,dents would have met The NameTag name extractxon tool discussed m the previous section performs hnkmg of name aliases within a document such as 'Albnght' to 'Madeleme Albnght,' 'U S' to 'Umted States,' and 'IBM' for 'International Business Machine' We used tlus tool to link full names and." ></td>
	<td class="line x" title="5:27	their aliases so that term frequency can be more accurately reflected, x e, 'IBM' and 'International Business Machine' are counted as two occurrences of the same term Another overt clue for chscourse cohesion and coherence is synonymous words When a theme of an article m developed throughout the text, synonymous words often appear as variants m the text In the example below, forinstance, 'pictures' and ~mages' are used interchangeably A new medzcal imaging technzque may someday be able to detect lung cancer and diseases of the bra:n earher than conventwnal methods, according to doctors at the State Un:vers,ty of New York, Stony Brook, and Princeton Un:verszty If doctors want to take pictures of the lungs, he noted, they have to use X-ray machines, ezpos:ng thezr pat:ents to doses of radtatzon :n the process The new technlque uses an anesthetfc, tenon gas, instead of water to create images of the body Although synonym sets have not proven effective in reformation retrieval for query expansion (Vorhees, 1994), we are using WordNet (Mallet et al, 1990) to link synonymous words m an article In the IR task, a query term is expanded with Its synonyms without dlsambxguatmg the senses of the term Thus, semantically irrelevant query terms are added, and the system typically retrieves more irrelevant documents, decreasing the precision Our summarization approach, in contrast, attempts to exploit WordNet synonym sets of only signature terms m a szngle document Our hypothesis m that if a synonym of a signature term extsts m the article, the term has been dlsamblgnated by the context of the article and the 'correct' synonym, not a synonym of the term in a different sense, m likely to co-occur in the same document In addition, morphological analysts allows us to link morphological variants of the same word within a document Morphological variants are often used to refer to the same concept throughout a document, 68 ! !" ></td>
	<td class="line x" title="6:27	i I providing discourse clues In the above example, 'lma~ng' and 'Images' are morphologically linked Like synonyms, morphology or stemming has not proven to be useful for 'lmprowng information retrieval (Salton and Lesk, 1968, Harman,~1991) However, the recent work by (Church, 1995) showed that effectiveness of morphology, or correlations among morphological variants within a document, vanes from word to word A word hke 'hostage' has a large correlation with its variant Uhostages' while a word like 'await' does not According to his experiments, good keywords like 'hostage' and its variants are likely to be repeated more than chance within a document and highly correlate with variant forms Tins implies that important signature words we use for summarization are likely to appear In a single document multiple times using their variant forms 2.2 Combining Sl~rnrnarlzatlon Features The DlmSum summarizer exploits our flexible definition of a signature word and sources of domain and duscourse knowledge m the texts through  the creation of multiple basehne databases corresponding to multiple definitions of signature words  the application of the discourse features in multiple-term frequency calculation methods Different baseline databases can affect the inverse document frequency (ldf)." ></td>
	<td class="line x" title="7:27	values We have created multiple baseline databases based upon multiple deflmtions of the signature words Signature words are flexibly defined as collections of features Presently, we derive databases cousustmg of (a) terms alone, (b) terms plus multi-word names, (c) stemmed terms plus muti-words names, and (d) terms plus multi-word names and collocations The duscourse features, 1 e, synonyms, morphological variants or name ahases, for s~gnature words, on the other hand, can affect the term frequency (tf) values Using these discourse features boosts the term frequency score within a text .when they are . treated as var!ants of signature words Having multiple baseline databases available makes it easy to test the contribution of each feature or combination of features 2.2.1 The Feature Combiner: Current In order to select sentences for a summary, each sentence in the document us scored using different combinations of signature word features and discourse features Currently, every token m a document us assigned a score based on its tf*ldf value The token score us used, in turn, to calculate the score of each sentence in the document More specifically, the score of a sentence is calculated as the average of the scores of the tokens contained m that sentence with the exception that certain types of 69 tokens can be ehmmated from the sentence as discussed That m, the DlmSum system can Ignore any combination of name types (1 e, person, place, entity) from a ~ven document for sconng (cf Section 3 for more details) After every sentence is assigned a score, the top n tnghest scoring sentences are chosen as a summary of the content of the document Currently, the DunSum system chooses the number of sentences equal to a power k (between zero and one) of the total number of sentences Thus, the system can vary the length of a summary accordmg to ~ For instance, if 0 5 is chosen as the power, and the document consists of 100 sentences, the output summary would contain 10 sentences Thus scheme has an advantage over choosing a given percentage of document size as it yields more information for longer documents while keeping summary size manageable We use the results of thus method as the baseline summary performance (; e, without any training), and report them m Section 3 2.2.2 The Feature Combiner." ></td>
	<td class="line x" title="8:27	Future As our goal is to make our summarization system trainable to different user and application needs, we are currently workmg on learning the best feature combination method from a tralmng corpus automatically For training and evaluating our summa~ nzatlon system, we had a user create extract summaries by selecting relevant sentences m articles In order to compare with the results of a trainable summanzer reported by (Kuplec, Pedersen, and Chen, 1995), we first use Bayes' rules to learn the best scoring method Then, we will use an inductive learning algorithm such as the decusion tree algorithm (Qumlan, 1993) to learn summarization rules which can deal with feature dependencies across sentences 3 Evaluation Much research has been devoted to assessing correspondence between human and machine abstracts because of the complexity of analyzing 'ahoutness' as illustrated in (Hahn, 1990) As a result, most of the prehmmary evaluations of summarizatlon systerns have been developer-based A common aFproach IS to compare correspondence between automatlc performance and human performance (Rath, Restock, and Savage, 1961, Edmundson, 1969, Kuplec, Pedersen, and Chen, 1995) or summary accep~ ability (Brandow, Mltze, and Ran, 1995) Others have been task-based, comparing abstract and full text on~nals m terms of the browsing and search time (Mnke et al, 1994, Sumlta, Ono, and Mllke, 1993) or recall and precision m-document retrieval (Brandow, Mltze, and Ran, 1995) Our evaluation methodology us two-pronged First, we evaluate the system by scoring for correspondence with human generated extracts (Seco tlon 3 1) Second, m our future work we are collaborating with the Umverslty of Massachusetts to evaluate retrieval effectiveness for system-generated and human-generated summaries (Section 3 2) 3.1 Developer-based Evaluation The DlmSum development envtronment software incorporates automatic sconng software to calculate system recall and precision for any user's training or test data ThLs allows us to evaluate system performance for any user and for variatl0us m summary preferences We performed an informal experiment in which 6 users created summary extract versions of the same set of 15 texts These versions varied considerably among users, winch supports our view that a summarlzation system should he trained for user preference Then, we ran the DlmSum system over these 15 texts using multiple feature combinations (l e, combmatlous among names, synonyms, and morphologtcal variants), and scored against the six versions of summary extracts Though correspondence between the DlmSum summaries and user suminarles was low (ranging between 14% and 31% F-measures), clearly some feature sets were more effective for some users than for others For example, the best feature c0mbmatlon for the best-case correspondence between the user and DlmSum (l e, 31% case) was the combination of name, synonym and morphological mforinatlon On the other hand, the best combination for the worst-case correspondence between the user and DlmSum (l e, 14% case) was the combination of name and synonym reformation Some summary extracts, however, were not affected by different combinations of features The second step was to obtain a 'bottom-hne' score for a singl e user We ran the DlmSum system over a set of 86 texts using multiple feature combinatlous The features were combined by taking an average Of tf*ldf, tf or ldf scores of each token m a-sentence No training was performed We vaned the length of summaries (by changing/~ from 0 5 to 1 0), use of different types Of names (l e, person, place, and entity), use of abases, and use of synonyms for different parts of speech (l e, adjective, adverb, noun, and verb) Table 2 shows the top three F-measure scores (13), and the score for using the simplest baseline (4) For the best summary (1), place names were used winle person and entity ~ames were recognized but removed for sentence sconng Synonyms were also used The /c value was set to 0 65 (about 20-25% of a document as a summary) Use of aliases and synonyms chdn't make much difference m the scores (2-3) However, they all scored shghtly ingher than  the summary which &dn't use any of these features, i e, a summary which didn't use names, synonyms, or aliases (4) It Is interesting that using name tagging in a re70 verse way, 1 e, recogmzmg and then deleting person names from  sentence scoring, made a slgmficantly positive effect on summarization The best summary score with the person name used m sentence scoring was 38 6% (5) The.reason why person names made negative contnbutlous tothe summary seems ' to be because personal names were often mentioned as passing references (e g, names of spokespeople) m the corpus, but they had ingh ldf values Finally, m every feature combination, taking tf*ldf scores of each word outperformed the ldf-based calculation, and the latter m turn outperformed the if-based score calculation These results further motivate us to apply automated learning to combine summarization features The fact that humans vary m summarization suggests that recall/preclmon evaluation.is not meanmgful unless a summarization system Is trainable to a particular summary style Our current work is to identify through training what feature combinations produce an optimal summary for a given user We anticipate that the summary performance will improve with tralmng as DhmSum learns antomatlcally how or whether these, different mgna-." ></td>
	<td class="line x" title="9:27	 ture word definitions are contnbutmg to the summary The current design does not incorporate para~aph/sentence location reformation or genrespecific indicator phrases We are explonng if these features can be indirectly subsumed by the derived features we have already identified Also, the cursory look at the summaries of DimSum shows that the system-generated summary may be prowdmg the same reformation as the summary provided by the user, but the sentences were chosen differently ThE happens because the same reformation can be conveyed by dnTerent sentences within the same document This motivates us to conduct a more task-oriented summarization evaluation, winch Is discussed below 3.2 Task-based Evaluation As a more task-oriented evaluation, we." ></td>
	<td class="line x" title="10:27	are collaborating with the Umverslty of Massachusetts to evaluate retrieval etfectiveness for DlmSum systemgenerated .and human-generated extracts for topics from TREC-5 (Text REtrieval Conference-5) We have selected 30 topics, five assessed as difficult, five assessed as easy (Harman, 1996), and the remaining 15 randomly The top 50 documents judged relevant by the INQUERY system m TRECC-5 for each topic have been identified For each document, two extract versions are being manually created One extract m based on the topic description, wtule the second L9 generated independent of the topic description In addition, the DlmSum system will automatlcally.generate two versions (query dependent and generic) for each of the texts With the TREC-5 full text results as a baseline, multiple lteratlous of the INQUEI~Y system wall test retrieval performance on ! Ii 1!" ></td>
	<td class="line x" title="11:27	II 11 ! !" ></td>
	<td class="line x" title="12:27	\[nt.AV, .:." ></td>
	<td class="line x" title="13:27	4 stony I~." ></td>
	<td class="line x" title="14:27	2 I~n~ton unwers~t~ 1 IS~ o~nmunlty coilllge I star* umv~'s~ of ~w yak 1 food and dr~p aclmmsmmon -~ Persoo 8 sJbert, m~ll 7 IIl~'t 1 albor~, m~tChoII 1 balamcro dlhp -\[1) p~i 1 now york I~wa~t Vm* c~00~ l~2S M28 </D0011~ S'L~.YID cut,u \]m,,~." ></td>
	<td class="line x" title="15:27	sd.,~m--u~ x~ml .c~TC~.YID>  cPORMA'T> ~ ~1 ~T> cl~.AD~b a1744 ~-?.~ 0491 </}I~A.OS~, cPREAMBL~> ~-~ a1744 clffLDI1~> ~ ~m McMurme ~'LINIb." ></td>
	<td class="line x" title="16:27	cCPY~RIGHT> :c) ~ Nem~ #~.W~JOHT> Iff.ADI.LNE> .~1~." ></td>
	<td class="line x" title="17:27	DOCtOa ~XZnm M~l t0 VkwL~ Maw (~ls &~L :TEXt'> q~ :~." ></td>
	<td class="line x" title="18:27	md dbmr, ses c~ ire b.Mu eea~4~.,~l ccmm,~,m~ ~etb~ch~ qm,,w~mmlmsl~,m'mal~bmm~s m~ w~vm .,mwat~ ~ms b haz:s~8~m'now~r ~ klznm'~,'~*'-m'm:dthelIp &MD nm~ted~I u~al m oaeMs~c,z~l~ iLw4 ram4 d ws~ ~ ~tSe'b~cli~ 9ec~ me su s\]prr.~ds O~'o~lpm~,~ the m~ve c~b w~r.h ~ m:b m a type ~ ~ calh~Di~Is ~bas~e potmwd w ~h~t~= m ~ Um ~.m md I=,p m~ m=~ thin Figure 1 Name Mode Summary t,m,d ~ ~ b ~ o~ VIw  -~I\] m~word + 8 albert, m~tchell 4 s~ony brook G ~lnon  d' 7r~7~  5 irnlgms I p~JrlS 4 lungs 7 Mchn~w~ 14 extstlng 12 rm~gIs 2 a~iw~tim~ 2 semis 2 ~rtm:e~on umvlrs~ I hyplrDolaflzsS 5 Conventional 1 ~km~'O dshp 3 1 nassau commumW ~o)hlge 3 dlseesIS 3 mIls | h~l(~l I mcmUrlrll DOG~ Docm~ ~.s raze,c,~OCI0~  ST0\]tYZD cam-a\]p~.sd-qu-,~c, ~'Q0a <~?~YID3." ></td>
	<td class="line x" title="20:27	cFORMAT> ~ &DL </FO~.MAT> cS\[.UO> bc</SLUG> IW, ADD.> Lt744 07-25 04~J. </HEADI\[~:P r~AMBL)~." ></td>
	<td class="line x" title="21:27	m-* -a17~ &tin, (ndy){A1r'/q,t Ncws $~r.d~ms) A.QL ~/P'REAMDLB> caYLINE:, By~ ' ' -~</8YL~b ccPYRZOlt'r) ~YJUOFrr> :~ADLINIb." ></td>
	<td class="line x" title="22:27	&lYJt D~'s1,1~.~m i,j tDYmv,.~,d=~J~,&QL Aumv--,'x-'~ ~.Jq t . . i m~,m,~,Fbs~ ~.ct~.m I \[ 'fhe~ed=u,, eAsat~mtmo~e~,.e~l &t~l ~mapeh~ ~mem~mmn ~ ~du~uuda.~b~7 Butmmmswhm: ~t s m mthat o. ,~m-~hm had al0t ~'uuMo mlmx~ @ Tbs~.m~v:d,., cu~a,m. ,rb'.i  '~.~.,' p~ ~,,add~ ma.~as .~ ~ddm~ Decme~meSm spreztstbn~Mmutth~ Mood s~.am sod t~atb~ ~ corer.me'me ~ arias d ~be hod7 su, rJ~, ~  ~ = ~w'mcSz'enchmamm,W~c~.d~o~xthasthe ~  wrlmgr~U~S Figure 2 Keyword Mode Summary ? 71 Table 2 Summary scores for different feature combinations Feature Combination tf*ldf ! ldf I tf term+place+synonym 41 5 32 3 20 9 (1) term+place+entity, ' 41 2 33 9 21 2 (2) term.termWplace+ahas+synnym 409399 323242104 210 /~l . term-l-person+place+entlty+ahas+synonym 386 321 225 (5) the human and machine generated extracts to compare retrieval effectiveness 4 Multi-dimensional Summary Views . The DlmSum Summarization Clientprovldes a summary of a document in multiple dlmeuslons through a graphical user interface (GUI) to smt dflferent users' needs In contrast to a static view of a document, the system brings the contributing hngnmtie and other resources to the desktop and the user chooses the view he wants As shown m Flgnre 1, the GUI is divided mto the Lint Box on the left and the Text Viewer on the right When a user asks for a summary of a text, extracted summary sentences are hlghhghted m the Text Viewer The user can dynarmeally control a percentage of sentences to highlight for a summary In addition, the Client can automatically color-code top keywords m different colors for different types (1 e, person, entity, place and other) for quack and easy browsing In the Lint Box, the user can explore two different summary views of a text First, the user can choose the 'Name Mode,' and all the names of people, entities, and places which were recognized by the name extraction tool are sorted and displayed m the List Box (el Figure 1) The user can also select a subset of name types (e g, only person and entity, but not place) to d~play Aliases of a name are indented and hsted under their full names In the 'Keyword Mode,' the top keywords, or signature words, (including names) axe dmplayed in the Last Box Analogous to the name aliases, for each keyword its synonyms and morphological variants, if exast, are indented and hsted below it (cf Figure 2) The user can choose the score threshold or percentage to vary the number of keywords for display In both modes, the names and signature words in the List Box can be sorted alphabetically, by frequency, or by the tf*ldf score Choking on a term in the Lint Box also causes the first occurrence of the term to be hlghhghted in the Text Viewer From there, the user can use the FIRST, PREVIOUS, NEXT, or LAST button at the bottom of the GUI to track the other occurrences of the term, including its ahases, synonyms, and morphological variants This provides the user with a way to track themes of the text lnteractively 5 Summary The DlmSum summarization system leverages off." ></td>
	<td class="line oc" title="24:27	of the works of (Kuplec, Pedersen, and Chen, 1995) and (Brandow, Mltze, .and Ran, 1995), and advances summarmatlon technology by applynag corpus-based statistical NLP teehmques, robust information extraction, and readily avaalable on-hne resources Our prehxmnary experiments with combining different summarization features have been reported, and our current effort to learn to combine these features to produce the best summaries has been described The features derived by these robust NLP techmques were also utihzed m presentmg multiple summary.vtews to the user m a novel way References Advanced Research Projects Agency 1995 Proceed:rigs of S:zth Message Understanding Conference (MUC-6) Morgan Kanfmann Pubhshers Brandow, Ron, Karl Mltze, and Lisa Ran 1995 Automatic condensation of electromc pubhcatlous by sentence selection Information Processing and  Management, 31, forthcoming .Bull, Eric 1993 A Comps-based Approach to Language Learning Ph D thesm, Umverslty of Pennsylvania Church, Kenneth and Patrick Hanks 1990 Word  Aesoclatlon Norrns, Mutual Information, and Lexicography Computational Lmgmstscs, 16(1) Church, Kenneth W 1995 One term or two 9 In Proceedings of the 17th Annual International SIGIR Conference on Research and Development In Informatzon Retrzeral, pages 310-318 Edmundson, H P 1969 New methods m automatic abstracting Journal of the ACM, 16(2) 264-228 Fum, Dando, Glovanm Gmda, and Carlo Tasso 1985 Evalutatmg Importance A step towards text surnmarlzatlon In I3CAI85, pages 840-844IJCAi, AAAI Hahn, Udo 1990 Topic parsing Accounting for text macro structures m full-text analysm In format:on Processing and Management, 26(1)135170 Harman, Donna 1991 How effective is suttixang ~ Journal of the Amerlcan Sot:cry for Informatwn Sc:ence, 42(1) 7-15 Harman, Donna 1996 Overview of the fifth text retrieval conference (tree-5) In TREC-5 Conference Proceedings Jmg, Y and B Croft 1994 An Assoc:atwn Thesaurns for Informatzon Retrseval Umass Techmcal Report 94-I7 Center for Intelligent Information Retrieval, University of Massachusetts Johnson, F C, C D Prate, W J Black, and A P Neal 1993." ></td>
	<td class="line x" title="25:27	The apphcahon of hngumhc processnag to automatic abstract generation Journal of Docnmentatwn and Tezt Management, 1(3) 215241 Jones; Karen Sparck 1993 What mtght be in a summary?" ></td>
	<td class="line x" title="26:27	In Knorz, Krause, and WomserHacker, edttors, Informatwn Retrieval 'g3, pages 9-26 Jones, Karen Sparck 1995 Dmcourse modeling for automahc surnmarms In E Hajlcova, M Cervenka, O Leskn, and P Sgall, editors, Prague ~ : gmsttc Circle Papers, volume 1, pages 201-227 Krupka, George 1995 SRA Descnphon of the SRA System as Used for MUC-6 In Proceedrags of $:z'th Message Understanding Conference (MUC-~) Kuplec, Juhem, Jan Pedersen, and Francme Chert 1995 A trmnable document summarizer In Procee&ngs of the 18th Annual Internatwnai SIGIR Conference on Research and Development :n Informatwn Retrzeval, pages 68-73 Luhn, H P 1958 The automatic creation of hterature abstracts In IBM J Research Development, volume 2, pages 159-165 Maybury, Mark T 1995 Automated even summarxzatlon techmques In B Endres-Nlggemeyer, J Hobbs, and Karen Sparck Jones, editors, Summarizing Text for Intelhgent Commun:cat,on, pages 101-149 McKeown, Kathleen and Dragomar Radev 1995 Generating summaries of mulhple news articles In Proceedings of the 18th Annual Internat:onal SIGIR Conference on Research and Development :n In format:on, pages 74-78 Make, Seljl, Etsuo Itho, Kenj10no, and Kazuo Surmta 1994 A full text retrieval system with a dynannc abstract generation function In Proceed:ngs of 17th Annual Internatwnal ACM S1GIR Conference on Research and Development :n Informatwn Retrieval, pages 152-161 Miller, George, Richard Beekwith, Chnshane Fell' baum, Derek Gross, and Katherine Miller 1990 Five papers on WordNet Technical Report CSL Report 43, Cogmhve Science Laboratory, Princeton Umverslty . Pmce, C 1990 Constructing hterature abstracts by computer Techmques and prospects Informa::on Processing and Management, 26(1) 171-186 Prate, C and P A Jones 1993 The ldenhficahon of important concepts m lughly structured techmeal papers In Proceedings of the 16th Annual Internatwnal A CM SIGIR Conference of Research and Development m Inforraatzon Retrieval, pages 6978 Qumlan, J Ross 1993 C4 5 Programs for Machine Learmng Morgan Kaufmann Publmhers Rath, G J, A Restock, and T R Savage 1961 The formation of abstracts by the selechon of sentenees Amer:can Doeumentatwn, 12(2) 139-143 Rau, Lma F, Paul S Jacobs, and Un Zermk 1989 Information extraction and text surnmanzatlon umng hngmstlc knowledge acqmslhon Informa:son Processing and Management, 25(4) 419-428 Relmer, Ulrich and Udo Hahn 1988 Text condensahon as knowledge base ahstractmn In IEEE  Conference on AI Apphcatwns, pages 338-344 Rxlotf, Ellen 1995 A corpus-based approach to dommn-speclfic text surnmanzatlon In B Endres-Nlggemeyer, J Hobbs, and K Sparek Jones, e&tors, Summarizing TeE/or lntelhgent Commumcatwn, pages 69-84 Rush, J E, R Salvador, and A Zamora 1971 Automatic abstracting and mdemng Produchon of m&eahve abstracts by." ></td>
	<td class="line x" title="27:27	application of contextual inference and syntactic criteria Journal of the American Sot:cry for Informat:on Sc:ence, 22(4) 260-274 Salton, G and M McGdl, editors 1983 An Introductzon to Modern Informatwn Retrieval McGraw-Hall Salton, Gerald and Mark Leak 1968 Computer evaluation of indexing and text processing Journal of the ACM, 15(1) 8-36 Sumxta, Kazuo, Kenjl Ono, and Seljt Make 1993 Document structure extraction for mterachve document retrieval systems In Procee&ngs Of SIGDOC'93, pages 1301-310 Vorhees, E 1994 Query expansion using lexacalsemamc relations In Proceedings of the 17th Annual Internatwn.al ACM-SIGIR Conference on Research and Development of Informatwn Retrzeval, pages 61-69" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-1062
Thematic segmentation of texts: two methods for two kinds of texts
Ferret, Olivier;Grau, Brigitte;Masson, Nicolas;"></td>
	<td class="line x" title="1:140	Thematic segmentation of texts: two methods for two kinds of texts Olivier FERRET LIMSI-CNRS Brit." ></td>
	<td class="line x" title="2:140	508 BP 133 1:-91403, Orsay Cedex, France ferret@ limsi.fr Brigitte GRAU LIMSI-CNRS Brit." ></td>
	<td class="line x" title="3:140	508 BP 133 Fo91403, Orsay Cedex, France grau @ l imsi.fr Nicolas MASSON LIMSI-CNRS Brit." ></td>
	<td class="line x" title="4:140	508 BP 133 F-91403, Orsay Cedex, France masson@limsi.fr Abstract To segment texts in thematic units, we present here how a basic principle relying on word distribution can be applied on different kind of texts." ></td>
	<td class="line x" title="5:140	We start from an existing method well adapted for scientific texts, and we propose its adaptation to other kinds of texts by using semantic links between words." ></td>
	<td class="line x" title="6:140	These relations are found in a lexical network, automatically built from a large corpus." ></td>
	<td class="line x" title="7:140	We will compare their results and give criteria to choose the more suitable method according to text characteristics." ></td>
	<td class="line x" title="8:140	1." ></td>
	<td class="line x" title="9:140	Introduction Text segmentation according to a topical criterion is a useful process in many applications, such as text summarization or information extraction task." ></td>
	<td class="line x" title="10:140	Approaches that address this problem can be classified in knowledge-based approaches or word-based approaches." ></td>
	<td class="line x" title="11:140	Knowledge-based systems as Grosz and Sidner's (1986) require an extensive manual knowledge engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains." ></td>
	<td class="line x" title="12:140	To overcome this limitation, and to process a large amount of texts, word-based approaches have been developed." ></td>
	<td class="line x" title="13:140	Hearst (1997) and Masson (1995) make use of the word distribution in a text to find a thematic segmentation." ></td>
	<td class="line x" title="14:140	These works are well adapted to technical or scientific texts characterized by a specific vocabulary." ></td>
	<td class="line x" title="15:140	To process narrative or expository texts such as newspaper articles, Kozima's (1993) and Morris and Hirst's (1991) approaches are based on lexical cohesion computed from a lexical network." ></td>
	<td class="line x" title="16:140	These methods depend on the presence of the text vocabulary inside their network." ></td>
	<td class="line x" title="17:140	So, to avoid any restriction about domains in such kinds of texts, we present here a mixed method that augments Masson's system (1995), based on word distribution, by using knowledge represented by a lexical co-occurrence network automatically built from a corpus." ></td>
	<td class="line x" title="18:140	By making some experiments with these two latter systems, we show that adding lexical knowledge is not sufficient on its own to have an all-purpose method, able to process either technical texts or narratives." ></td>
	<td class="line x" title="19:140	We will then propose some solutions to choose the more suitable method." ></td>
	<td class="line x" title="20:140	2." ></td>
	<td class="line x" title="21:140	Overview In this paper, we propose to apply one and the same basic idea to find topic boundaries in texts, whatever kind they are, scientific/technical articles or newspaper articles." ></td>
	<td class="line x" title="22:140	This main idea is to consider smallest textual units, here the paragraphs, and try to link them to adjacent similar units to create larger thematic units." ></td>
	<td class="line x" title="23:140	Each unit is characterized by a set of descriptors, i.e. single and compound content words, defining a vector." ></td>
	<td class="line x" title="24:140	Descriptor values are the number of occurrences of the words in the unit, modified by the word distribution in the text." ></td>
	<td class="line x" title="25:140	Then, each successive units are compared through their descriptors to know if they refer to a same topic or not." ></td>
	<td class="line x" title="26:140	This kind of approach is well adapted to scientific articles, often characterized by domain technical term reiteration since there is often no synonym for such specific terms." ></td>
	<td class="line x" title="27:140	But, we will show that it is less efficient on narratives." ></td>
	<td class="line x" title="28:140	Although the same basic principle about word distribution applies, topics are not so easily detectable." ></td>
	<td class="line x" title="29:140	In fact, narrative or expository texts often refer to a same entity with a large set of different words." ></td>
	<td class="line x" title="30:140	Indeed, authors avoid repetitions and redundancies by using hyperonyms, synonyms and referentially equivalent expressions." ></td>
	<td class="line x" title="31:140	To deal with this specificity, we have developed another method that augments the first method by making use of information coming from a lexical co-occurrence network." ></td>
	<td class="line x" title="32:140	392 This network allows a mutual reinforcement of descriptors that are different but strongly related when occurring in the same unit." ></td>
	<td class="line x" title="33:140	Moreover, it is also possible to create new descriptors for units in order to link units sharing semantically close words." ></td>
	<td class="line x" title="34:140	In the two methods, topic boundaries are detected by a standard distance measure between each p. air of adjacent vectors." ></td>
	<td class="line x" title="35:140	Thus, the segmentatmn process produces a text representation with thematic blocks including paragraphs about the same topic." ></td>
	<td class="line x" title="36:140	The two methods have been tested on different kinds of texts." ></td>
	<td class="line x" title="37:140	We will discuss these results and give criteria to choose the more suitable method according to text characteristics." ></td>
	<td class="line x" title="38:140	3." ></td>
	<td class="line x" title="39:140	Pre-processing of the texts As we are interested in the thematic dimension of the texts, they have to be represented by their significant features from that point of view." ></td>
	<td class="line x" title="40:140	So, we only hold for each text the lemmatized form of its nouns, verbs and adjectives." ></td>
	<td class="line x" title="41:140	This has been done by combining existing tools." ></td>
	<td class="line x" title="42:140	MtSeg from the Multext project presented in V6ronis and Khouri (1995) is used for segmenting the raw texts." ></td>
	<td class="line x" title="43:140	As compound nouns are less polysemous than single ones, we have added to MtSeg the ability to identify 2300 compound nouns." ></td>
	<td class="line x" title="44:140	We have retained the most frequent compound nouns in 11 years of the French Le Monde newspaper." ></td>
	<td class="line x" title="45:140	They have been collected with the INTEX tool of Silberztein (1994)." ></td>
	<td class="line x" title="46:140	The part of speech tagger TreeTagger of Schmid (1994) is applied to disambiguate the lexical category of the words and to provide their lemmatized form." ></td>
	<td class="line x" title="47:140	The selection of the meaningful words, which do not include proper nouns and abbreviations, ends the pre-processing." ></td>
	<td class="line x" title="48:140	This one is applied to the texts both for building the collocation network and for their thematic segmentation." ></td>
	<td class="line x" title="49:140	4." ></td>
	<td class="line x" title="50:140	Building the collocation network Our segmentation mechanism relies on semantic relations between words." ></td>
	<td class="line x" title="51:140	In order to evaluate it, we have built a network of lexical collocations from a large corpus." ></td>
	<td class="line x" title="52:140	Our corpus, whose size is around 39 million words, is made up of 24 months of the Le Monde newspaper taken from 1990 to 1994." ></td>
	<td class="line oc" title="53:140	The collocations have been calculated according to the method described in Church and Hanks (1990) by moving a window on the texts." ></td>
	<td class="line x" title="54:140	The corpus was pre-processed as described above, which induces a 63% cut." ></td>
	<td class="line x" title="55:140	The window in which the collocations have been collected is 20 words wide and takes into account the boundaries of the texts." ></td>
	<td class="line x" title="56:140	Moreover, the collocations here are indifferent to order." ></td>
	<td class="line x" title="57:140	These three choices are motivated by our task point of view." ></td>
	<td class="line x" title="58:140	We are interested in finding if two words belong to the same thematic domain." ></td>
	<td class="line x" title="59:140	As a topic can be developed in a large textual unit, it requires a quite large window to detect these thematic relations." ></td>
	<td class="line x" title="60:140	But the process must avoid jumping across the texts boundaries as two adjacent texts from the corpus are rarely related to a same domain." ></td>
	<td class="line x" title="61:140	Lastly, the collocation wl-w2 is equivalent to the collocation w2-wl as we only try to characterize a thematic relation between wl and w2." ></td>
	<td class="line x" title="62:140	After filtering the non-significant collocations (collocations with less than 6 occurrences, which represent 2/3 of the whole), we obtain a network with approximately 31000 words and 14 million relations." ></td>
	<td class="line oc" title="63:140	The cohesion between two words is measured as in Church and Hanks (1990) by an estimation of the mutual information based on their collocation frequency." ></td>
	<td class="line o" title="64:140	This value is normalized by the maximal mutual information with regard to the corpus, which is given by: /max ---log2 N2(Sw1) with N: corpus size and Sw: window size 5." ></td>
	<td class="line x" title="65:140	Thematic segmentation without lexical network The first method, based on a numerical analysis of the vocabulary distribution in the text, is derived from the method described in Masson (1995)." ></td>
	<td class="line x" title="66:140	A basic discourse unit, here a paragraph, is represented as a term vector Gi = (gil,gi2,,git) where gi is the number of occurrences of a given descriptor in Gi." ></td>
	<td class="line x" title="67:140	The descriptors are the words extracted by the pre-processing of the current text." ></td>
	<td class="line x" title="68:140	Term vectors are weighted." ></td>
	<td class="line x" title="69:140	The weighting policy is tfidf which is an indicator of the importance of a term according to its distribution in a text." ></td>
	<td class="line x" title="70:140	It is defined by: wij = tf, j. log where tf, lj is the number of occurrences of a descriptm Tj in a paragraph i; dfi is the number of paragraphs in which 7)." ></td>
	<td class="line x" title="71:140	occurs and 393 N the total number of paragraphs in the text." ></td>
	<td class="line x" title="72:140	Terms that are scattered over the whole document are considered to be less important than those which are concentrated in particular paragraphs." ></td>
	<td class="line x" title="73:140	Terms that are not reiterated are considered as non significant to characterize the text topics." ></td>
	<td class="line x" title="74:140	Thus, descriptors whose occurrence counts are below a threshold are removed." ></td>
	<td class="line x" title="75:140	According to the length of the processed texts, the threshold is here three occurrences." ></td>
	<td class="line x" title="76:140	The topic boundaries are then detected by a standard distance measure between all pairs of adjacent paragraphs: first paragraph is compared to second paragraph, second one to third one and so on." ></td>
	<td class="line x" title="77:140	The distance measure is the Dice coefficient, defined for two vectors X= (Xl, x2  xt) and Y= (Yl, Y2  Yt) by: c(x,Y)-t 2 w(xi)w(yi) i=l t + t  w(y f i=l i=l where w(xi) is the number of occurrences of a descriptor xi weighted by tf idf factor Low coherence values show a thematic shift in the text, whereas high coherence values show local thematic consistency." ></td>
	<td class="line x" title="78:140	6." ></td>
	<td class="line x" title="79:140	Thematic segmentation with lexicai network Texts such as newspaper articles often refer to a same notion with a large set of different words linked by semantic or pragmatic relations." ></td>
	<td class="line x" title="80:140	Thus, there is often no reiteration of terms representative of the text topics and the first method described before becomes less efficient." ></td>
	<td class="line x" title="81:140	In this case, we modify the vector representation by adding information coming from the lexical network." ></td>
	<td class="line x" title="82:140	Modifications act on the vectorial representation of paragraphs by adding descriptors and modifying descriptor values." ></td>
	<td class="line x" title="83:140	They aim at bringing together paragraphs which refer to the same topic and whose words are not reiterated." ></td>
	<td class="line x" title="84:140	The main idea is that, if two words A and B are linked in the network, then ' when A is present in a text, B is also a little bit evoked, and vice versa ' That is to say that when two descriptors of a text A and B are linked with a weight w in the lexical network, their weights are reinforced into the paragraphs to which they simultaneously belong." ></td>
	<td class="line x" title="85:140	Moreover, the missing descriptor is added in the paragraph if absent." ></td>
	<td class="line x" title="86:140	In case of reinforcement, if the descriptor A is really present k times and B really present n times in a paragraph, then we add wn to the number of A occurrences and wk to the immber of B occurrences." ></td>
	<td class="line x" title="87:140	In case of descriptor addition, the descriptor weight is set to the number of occurrences of the linked descriptor multiplied by w. All the couples of text descriptors are processed using the original number of their occurrences to compute modified vector values." ></td>
	<td class="line x" title="88:140	These vector modifications favor emergence of significant descriptors." ></td>
	<td class="line x" title="89:140	If a set of words belonging to neighboring paragraphs are linked each other, then they are mutually reinforced and tend to bring these paragraphs nearer." ></td>
	<td class="line x" title="90:140	If there is no mutual reinforcement, the vector modifications are not significant." ></td>
	<td class="line x" title="91:140	These modifications are computed before applying a tf.idflike factor to the vector terms." ></td>
	<td class="line x" title="92:140	The descriptor addition may add many descriptors in all the text paragraphs because of the numerous links, even weak, between words in the network." ></td>
	<td class="line x" title="93:140	Thus, the effect of tf.idf is smoothed by the standard-deviation of the current descriptor distribution." ></td>
	<td class="line x" title="94:140	The resulting factor is: log( N--." ></td>
	<td class="line x" title="95:140	(1 -+ (k dj) ,)) with k, the paragraphs where Tj occurs." ></td>
	<td class="line x" title="96:140	7." ></td>
	<td class="line x" title="97:140	Experiments and discussion We have tested the two methods presented above on several kinds of texts." ></td>
	<td class="line x" title="98:140	0,8   0.~, :  i    0   :   :  ,< o,2 :  +  i  i /: 0  ''i~ 1 2 3 4 5 6 7 P~'eg'.e,p~ Figure 1 Improvement by the second method with low word reiteration 394 Figure 1 shows the results for a newspaper article from Le Monde made of 8 paragraphs." ></td>
	<td class="line x" title="99:140	The cohesion value associated to a paragraph i indicates the cohesion between paragraphs i and i+l. The graph for the first method is rather flat, with low values, which would a priori mean that a thematic shift would occur after each paragraph." ></td>
	<td class="line x" title="100:140	But significant words in this article are not repeated a lot although the paper is rather thematically homogeneous." ></td>
	<td class="line x" title="101:140	The second method, by the means of the links between the text words in the collocation network, is able to find the actual topic similarity between paragraphs 4 and 5 or 7 and 8." ></td>
	<td class="line x" title="102:140	The improvement resulting from the use of lexical cohesion also consists in separating paragraphs that would be set together by the only word reiteration criterion." ></td>
	<td class="line x" title="103:140	It is illustrated ill Figure 2 for a passage of a book by Jules Verne 1." ></td>
	<td class="line x" title="104:140	A strong link is found by the first method between paragraphs 3 and 4 although it is not thematically justified." ></td>
	<td class="line x" title="105:140	This situation occurs when too few words are left by the low frequency word and tf.idffilters." ></td>
	<td class="line x" title="106:140	o.I 0,8 0.4; 0,2 ~thod I -~thod 2  1 2 \] 4 5 Figure 2 Improvement by the second method when too many words are filtered More generally, the second method, even if it has not so impressive an effect as in Figures 1 and 2, allows to refine the results of the first method by proceeding with more significant words." ></td>
	<td class="line x" title="107:140	Several tests have been made on newspaper articles that show this tendency." ></td>
	<td class="line x" title="108:140	Experiments with scientific texts have also been made." ></td>
	<td class="line x" title="109:140	These texts use specific reiterated vocabulary (technical terms)." ></td>
	<td class="line x" title="110:140	By applying the first method, significant results are obtained l De la Terre ?~ la Lune." ></td>
	<td class="line x" title="111:140	2Lc vin jaune, Pour la science (French edition of Scientific American), Octobcr 1994, p. 18 because of this specificity (see Figure 3, the coherence graph in solid line)." ></td>
	<td class="line x" title="112:140	C~sL~ 1 0.8 'J,6 0,4 0,2 = : ~thc<l i -4  B 10 Figure 3 Test on a scientific paper 2 in a specialized domain On the contrary, by applying the second method to the same text, poor results are sometimes observed (see Figure 3, the coherence graph in dash line)." ></td>
	<td class="line x" title="113:140	This is due to the absence of highly specific descriptors, used for Dice coefficient computation, in the lexical network." ></td>
	<td class="line x" title="114:140	It means that descriptors reinforced or added are not really specific of the text domain and are nothing but noise in this case." ></td>
	<td class="line x" title="115:140	The two methods have been tested on 16 texts including 5 scientific articles and 11 expository or narrative texts." ></td>
	<td class="line x" title="116:140	They have been chosen according to their vocabulary specificity, their size (between 1 to 3 pages) and their paragraphs size." ></td>
	<td class="line x" title="117:140	Globally, the second method gives better results than the first one: it modulates some cohesion values." ></td>
	<td class="line x" title="118:140	But the second method cannot always be applied because problems arise on some scientific papers due to the lack of important specialized descriptors in the network." ></td>
	<td class="line x" title="119:140	As the network is built from the recurrence of collocations between words, such words, even belonging to the training corpus, would be too scarce to be retained." ></td>
	<td class="line x" title="120:140	So, specialized vocabulary will always be missing in the network." ></td>
	<td class="line x" title="121:140	This observation has lead us to define the following process to choose the more suitable method: Apply method 1; If x% of the descriptors whose value is not null after the application of tf.idf are not found in the network, then continue with method 1 otherwise apply method 2." ></td>
	<td class="line x" title="122:140	According to our actual studies, x has been settled to 25." ></td>
	<td class="line x" title="123:140	395 8." ></td>
	<td class="line x" title="124:140	Related works Without taking into account the collocation network, the methods described above rely on the same principles as Hearst (1997) and Nomoto and Nitta (1994)." ></td>
	<td class="line x" title="125:140	Although Hearst considers that paragraph breaks are sometimes invoked only for lightening the physical appearance of texts, we have chosen paragraphs as basic units because they are more natural thematic units than somewhat arbitrary sets of words." ></td>
	<td class="line x" title="126:140	We assume that paragraph breaks that indicate topic changes are always present in texts." ></td>
	<td class="line x" title="127:140	Those which are set for visual reasons are added between them and the segmentation algorithm is able to join them again." ></td>
	<td class="line x" title="128:140	Of course, the size of actual paragraphs are sometimes irregular." ></td>
	<td class="line x" title="129:140	So their comparison result is less reliable." ></td>
	<td class="line x" title="130:140	But the collocation network in the second method tends to solve this problem by homogenizing the paragraph representation." ></td>
	<td class="line x" title="131:140	As in Kozima (1993), the second method exploits lexical cohesion to segment texts, but in a different way." ></td>
	<td class="line x" title="132:140	Kozima's approach relies on computing the lexical cohesiveness of a window of words by spreading activation into a lexical network built from a dictionary." ></td>
	<td class="line x" title="133:140	We think that this complex method is specially suitable for segmenting small parts of text but not large texts." ></td>
	<td class="line x" title="134:140	First, it is too expensive and second, it is too precise to clearly show the major thematic shifts." ></td>
	<td class="line x" title="135:140	In fact, Kozima's method and ours do not take place at the same granularity level and so, are complementary." ></td>
	<td class="line x" title="136:140	9." ></td>
	<td class="line x" title="137:140	Conclusion From a first method that considers paragraphs as basic units and computes a similarity measure between adjacent paragraphs for building larger thematic units, we have developed a second method on the same principles, making use of a lexical collocation network to augment the vectorial representation of the paragraphs." ></td>
	<td class="line x" title="138:140	We have shown that this second method, if well adapted for processing such texts as newspapers articles, has less good results on scientific texts, because the characteristic terms do not emerge as well as in the first method, due to the addition of related words." ></td>
	<td class="line x" title="139:140	So, in order to build a text segmentation system independent of the kind of processed text, we have proposed to make a shallow analysis of the text characteristics to apply the suitable method." ></td>
	<td class="line x" title="140:140	10." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-1097
Text Segmentation Using Reiteration and Collocation
Jobbins, Amanda C.;Evett, Lindsay J.;"></td>
	<td class="line x" title="1:132	Text Segmentation Using Reiteration and Collocation Amanda C. Jobbins Department of Computing Nottingham Trent University Nottingham NG1 4BU, UK aj obbins @ resumix.com Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk Abstract A method is presented for segmenting text into subtopic areas." ></td>
	<td class="line x" title="2:132	The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity." ></td>
	<td class="line x" title="3:132	The lexical cohesion relations of reiteration and collocation are used to identify related words." ></td>
	<td class="line x" title="4:132	These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights." ></td>
	<td class="line x" title="5:132	This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects." ></td>
	<td class="line x" title="6:132	Introduction Many examples of heterogeneous data can be found in daily life." ></td>
	<td class="line x" title="7:132	The Wall Street Journal archives, for example, consist of a series of articles about different subject areas." ></td>
	<td class="line x" title="8:132	Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved." ></td>
	<td class="line x" title="9:132	Text segmentation could also be used as a pre-processing step in automatic summarisation." ></td>
	<td class="line x" title="10:132	Each segment could be summarised individually and then combined to provide an abstract for a document." ></td>
	<td class="line x" title="11:132	Previous work on text segmentation has used term matching to identify clusters of related text." ></td>
	<td class="line x" title="12:132	Salton and Buckley (1992) and later, Hearst (1994) extracted related text portions by matching high frequency terms." ></td>
	<td class="line x" title="13:132	Yaari (1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments." ></td>
	<td class="line x" title="14:132	Ponte and Croft (1997) used word co-occurrences to expand the number of terms for matching." ></td>
	<td class="line x" title="15:132	Reynar (1994) compared all words across a text rather than the more usual nearest neighbours." ></td>
	<td class="line x" title="16:132	A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994)." ></td>
	<td class="line x" title="17:132	Another approach to text segmentation is the detection of semantically related words." ></td>
	<td class="line x" title="18:132	Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994)." ></td>
	<td class="line x" title="19:132	Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented." ></td>
	<td class="line x" title="20:132	Another approach extracted semantic information from Roget's Thesaurus (RT)." ></td>
	<td class="line x" title="21:132	Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991)." ></td>
	<td class="line x" title="22:132	It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges." ></td>
	<td class="line x" title="23:132	However, RT does not capture all types of lexical cohesion relations." ></td>
	<td class="line x" title="24:132	In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus." ></td>
	<td class="line x" title="25:132	Furthermore, this process was not automated and relied on subjective decision making." ></td>
	<td class="line x" title="26:132	Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text." ></td>
	<td class="line x" title="27:132	The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated." ></td>
	<td class="line x" title="28:132	This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text." ></td>
	<td class="line x" title="29:132	614 1 Background Theory: Lexical Cohesion Cohesion concerns how words in a text are related." ></td>
	<td class="line x" title="30:132	The major work on cohesion in English was conducted by Halliday and Hasan (1976)." ></td>
	<td class="line x" title="31:132	An instance of cohesion between a pair of elements is referred to as a tie." ></td>
	<td class="line x" title="32:132	Ties can be anaphoric or cataphoric, and located at both the sentential and supra-sentential level." ></td>
	<td class="line x" title="33:132	Halliday and Hasan classified cohesion under two types: grammatical and lexical." ></td>
	<td class="line x" title="34:132	Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction." ></td>
	<td class="line x" title="35:132	Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words." ></td>
	<td class="line x" title="36:132	Identifying semantic relations in a text can be a useful indicator of its conceptual structure." ></td>
	<td class="line x" title="37:132	Lexical cohesion is divided into three classes: general noun, reiteration and collocation." ></td>
	<td class="line x" title="38:132	General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role." ></td>
	<td class="line x" title="39:132	Consequently, it was not further considered." ></td>
	<td class="line x" title="40:132	Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing)." ></td>
	<td class="line x" title="41:132	The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers." ></td>
	<td class="line x" title="42:132	A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel)." ></td>
	<td class="line x" title="43:132	All semantic relations not classified under the class of reiteration are attributed to the class of collocation." ></td>
	<td class="line x" title="44:132	2 Identifying Lexical Cohesion rib automatically detect lexical cohesion ties between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights." ></td>
	<td class="line x" title="45:132	The first two methods represent lexical cohesion relations." ></td>
	<td class="line x" title="46:132	Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety." ></td>
	<td class="line x" title="47:132	The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included)." ></td>
	<td class="line x" title="48:132	These types can be identified using relation weights (Jobbins and Evett, 1998)." ></td>
	<td class="line x" title="49:132	Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches oll inflections derived from the same stem." ></td>
	<td class="line x" title="50:132	An inflected word was reduced to its stem by lookup in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. 'orange oranges')." ></td>
	<td class="line oc" title="51:132	Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon." ></td>
	<td class="line x" title="52:132	Collocations were automatically located in a text by looking up pairwise words in this lexicon." ></td>
	<td class="line x" title="53:132	Figure 1 shows the record for the headword orange followed by its collocates." ></td>
	<td class="line x" title="54:132	For example, the pairwise words orange and peel form a collocation." ></td>
	<td class="line x" title="55:132	orange free green lemon peel red state yellow Figure 1." ></td>
	<td class="line x" title="56:132	Excerpt from the collocation lexicon." ></td>
	<td class="line x" title="57:132	Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995)." ></td>
	<td class="line x" title="58:132	A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured." ></td>
	<td class="line x" title="59:132	An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex)." ></td>
	<td class="line x" title="60:132	Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex." ></td>
	<td class="line x" title="61:132	3 Proposed Segmentation Algorithm The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity." ></td>
	<td class="line x" title="62:132	A window size of three sentences was found to produce the best results." ></td>
	<td class="line x" title="63:132	Multiple sentences were compared because 615 calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991)." ></td>
	<td class="line x" title="64:132	Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score." ></td>
	<td class="line x" title="65:132	Word repetitions are identified between identical words and words derived from the same stem." ></td>
	<td class="line x" title="66:132	Collocations are located by looking up word pairs in the collocation lexicon." ></td>
	<td class="line x" title="67:132	Relation weights are calculated between pairwise words according to their location in RT." ></td>
	<td class="line x" title="68:132	The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows." ></td>
	<td class="line x" title="69:132	Scores plotted on a graph show a series of peaks (high scores) and troughs (low scores)." ></td>
	<td class="line x" title="70:132	Low scores indicate a weak level of cohesion." ></td>
	<td class="line x" title="71:132	Hence, a trough signals a potential subject change and texts can be segmented at these points." ></td>
	<td class="line x" title="72:132	4 Experiment 1: Locating Subject Change An investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text." ></td>
	<td class="line x" title="73:132	Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web." ></td>
	<td class="line x" title="74:132	A total of 42 texts for test data were generated by concatenating pairs of these articles." ></td>
	<td class="line x" title="75:132	Hence, each generated text consisted of two articles." ></td>
	<td class="line x" title="76:132	The transition from the first article to the second represented a known subject change point." ></td>
	<td class="line x" title="77:132	Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms (Reynar, 1994; Stairmand, 1997)." ></td>
	<td class="line x" title="78:132	For each text, the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text." ></td>
	<td class="line x" title="79:132	An error margin of one sentence either side of this point, determined by empirical analysis, was allowed." ></td>
	<td class="line x" title="80:132	Results: Table 1 gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points." ></td>
	<td class="line x" title="81:132	troughs placed subject change linguistic feature ' points located average std." ></td>
	<td class="line x" title="82:132	dev." ></td>
	<td class="line x" title="83:132	(out of 42 poss.)" ></td>
	<td class="line x" title="84:132	word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) ' I I 41 word repetition 8.5 3.62 (97.6%) collocation 40 5.8 3.70 relation weights (95.2%) word repetition 40 collocation 6.4 4.72 relation weights (95.2%) 39 relation weights 7 4.23 (92.9%) 35 collocation 6.3 3.83 (83.3%) Table 1." ></td>
	<td class="line x" title="85:132	Comparison of segmentation algorithm using different linguistic features." ></td>
	<td class="line x" title="86:132	Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result." ></td>
	<td class="line x" title="87:132	A total of 41 out of a possible 42 known subject change points were identified from the least number of troughs placed per text (7.1)." ></td>
	<td class="line x" title="88:132	For the text where the known subject change point went undetected, a total of three troughs were placed at sentences 6, 11 and 18." ></td>
	<td class="line x" title="89:132	The subject change point occurred at sentence 13, just two sentences after a predicted subject change at sentence 11." ></td>
	<td class="line x" title="90:132	In this investigation, word repetition alone achieved better results than using either collocation or relation weights individually." ></td>
	<td class="line x" title="91:132	The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text." ></td>
	<td class="line x" title="92:132	5 Experiment 2: Test Subject Evaluation The objective of the current investigation was to determine whether all troughs coincide with a subject change." ></td>
	<td class="line x" title="93:132	The troughs placed by the 616 algorithm were compared to the segmentations identified by test subjects for the same texts." ></td>
	<td class="line x" title="94:132	Method: Twenty texts were randomly selected for test data each consisting of approximately 500 words." ></td>
	<td class="line x" title="95:132	These texts were presented to seven test subjects who were instructed to identify the sentences at which a new subject area commenced." ></td>
	<td class="line x" title="96:132	No restriction was placed on the number of subject changes that could be identified." ></td>
	<td class="line x" title="97:132	Segmentation points, indicating a change of subject, were determined by the agreement of three or more test subjects (Litman and Passonneau, 1996)." ></td>
	<td class="line x" title="98:132	Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change." ></td>
	<td class="line x" title="99:132	The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects." ></td>
	<td class="line x" title="100:132	In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points." ></td>
	<td class="line x" title="101:132	Due to that success, these five approaches were applied in this experiment." ></td>
	<td class="line x" title="102:132	To evaluate the results, the information retrieval metrics precision and recall were used." ></td>
	<td class="line x" title="103:132	These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997)." ></td>
	<td class="line x" title="104:132	The degree to which a segmentation point was 'missed' by a trough, for instance, is not considered." ></td>
	<td class="line x" title="105:132	Allowing an error margin provides some degree of flexibility." ></td>
	<td class="line x" title="106:132	An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar (1994) allowed three sentences." ></td>
	<td class="line x" title="107:132	In this investigation, an error margin of two sentences was considered." ></td>
	<td class="line x" title="108:132	Results: Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts." ></td>
	<td class="line x" title="109:132	Discussion: The segmentation algorithm using word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively." ></td>
	<td class="line x" title="110:132	For 9 out of the 20 texts segmented, all troughs were relevant." ></td>
	<td class="line x" title="111:132	Therefore, many of the troughs placed by the segmentation algorithm represented valid subject ,l,tgui~d,~ feature word repetition relation weights word repetition collocation word repetition collocation relation weights collocation relation weights word repetition mean values for all texts relevant nonrel." ></td>
	<td class="line x" title="112:132	relevant found found prec!" ></td>
	<td class="line x" title="113:132	rec." ></td>
	<td class="line x" title="114:132	4.50 3.10 1.00 0.80 0.69 4.50 2.80 0.85 0.80 0.62 4.50 2.80 0.85 0.80 0.62 4.50 2.75 0.90 0.80 0.60 4.50 2.50 0.95 0.78 0.56 Table 2." ></td>
	<td class="line x" title="115:132	Comparison of troughs to segmentation points placed by the test subjects." ></td>
	<td class="line x" title="116:132	changes." ></td>
	<td class="line x" title="117:132	Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62." ></td>
	<td class="line x" title="118:132	These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation." ></td>
	<td class="line x" title="119:132	As an example, a text segmentation algorithm developed by Hearst (1994) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61." ></td>
	<td class="line x" title="120:132	In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5)." ></td>
	<td class="line x" title="121:132	Each text was only 500 words in length and was related to a specific subject area." ></td>
	<td class="line x" title="122:132	These factors limited the degree of subject change that occurred." ></td>
	<td class="line x" title="123:132	Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect." ></td>
	<td class="line x" title="124:132	Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows." ></td>
	<td class="line x" title="125:132	The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69." ></td>
	<td class="line x" title="126:132	When used in 617 isolation, the performance of each feature was inferior to a combined approach." ></td>
	<td class="line x" title="127:132	This fact provides evidence that different lexical relations are detected by each linguistic feature considered." ></td>
	<td class="line x" title="128:132	Areas for improving the segmentation algorithm include incorporation of a threshold for troughs." ></td>
	<td class="line x" title="129:132	Currently, all troughs indicate a subject change, however, minor fluctuations in scores may be discounted." ></td>
	<td class="line x" title="130:132	Future work with this algorithm should include application to longer documents." ></td>
	<td class="line x" title="131:132	With trough thresholding the segments identified in longer documents could detect significant subject changes." ></td>
	<td class="line x" title="132:132	Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-2226
Structural Disambiguation Based on Reliable Estimation of Strength of Association
Wu, Haodong;Alves, Eduardo de Paiva;Furugori, Teiji;"></td>
	<td class="line x" title="1:139	Structural Disambiguation Based on Reliable Estimation of Strength of Association Haodong Wu Eduardo dc Paiva Alves Teiji Furugori Department of Computer Science University of Electro-Communications 1-5-1: Chofugaoka, Chofll, Tokyo 1828585, JAPAN {wu, ealve s, furugori }Ophaet on." ></td>
	<td class="line x" title="2:139	cs." ></td>
	<td class="line x" title="3:139	uec." ></td>
	<td class="line x" title="4:139	ac." ></td>
	<td class="line x" title="5:139	j p Abstract This paper proposes a new class-based method to estimate the strength of ,association in word co-occurrence for the purpose of structural disambiguation." ></td>
	<td class="line x" title="6:139	To deal with sparseness of data, we use a conceptual dictionary ,'us the source for acquiring upper closes of the words related in the co-occurrence, and then use t-scores to determine a pair of classes to be employed for calculating the strength of association." ></td>
	<td class="line x" title="7:139	We have applied our method to determining dependency relations in Japanese and prepositional phrase attachments in English." ></td>
	<td class="line x" title="8:139	The experimental resuits show that the method is sound, effective and usefifl in resolving structural ambiguities." ></td>
	<td class="line x" title="9:139	1 Introduction The strength of association between words provides lexical preferences for ambiguity resolution." ></td>
	<td class="line x" title="10:139	It is usuMly estimated fi'om statistics on word co-occurrences in large corpora (Hindle and Rooth, 1993)." ></td>
	<td class="line x" title="11:139	A problem with this approach is how to estimate the probability of word co-occurrences that are not observed in the training corpus." ></td>
	<td class="line x" title="12:139	There are two main approaches to estimate the probability: smoothing methods (e.g., Church and Gale, 1991; Jelinek and Mercer, 1985; Katz, 1987) and clmss-b~ed methods (e.g., Brown et al., 1992; Pereira and Tishby, 1992; Resnik, 1992; Yarowsky, 1992)." ></td>
	<td class="line x" title="13:139	Smoothing methods estimate the probability of the unobserved co-occurrences by using frequencies of the individual words." ></td>
	<td class="line x" title="14:139	For example, when eat and bread do not co-occur, tile probability of (eat, bread) would be estimated by using the frequency of (eat) and (bread)." ></td>
	<td class="line x" title="15:139	A problem with this approach is that it pays no attention to tile distributional characteristics of the individual words in question." ></td>
	<td class="line x" title="16:139	Using this method, the probability of (eat, bread} and (eat, cars) would become the same when bread and cars have the same frequency." ></td>
	<td class="line x" title="17:139	It is unacceptable fl'om the linguistic point of view." ></td>
	<td class="line x" title="18:139	Cl,~ss-bmsed methods, oll the other hand, estimate tile probabilities by associating a class with each word and collecting statistics on word class co-occurrences." ></td>
	<td class="line x" title="19:139	For instance, instead of calculating tile probability of (eat, bread) directly, these methods associate eat with the class \[ingest\] and bread with the class \[food\] and collect statistics on the classes \[ingest\] and \[food\]." ></td>
	<td class="line x" title="20:139	The accuracy of the estimation depends on the choice of classes, however." ></td>
	<td class="line x" title="21:139	Some classbased methods (e.g., Yarowsky, 1992) associate each word with a single class without considering the other words in the co-occurrence." ></td>
	<td class="line x" title="22:139	However, a word may need to bc replaced by different class depending on the co-occurrence." ></td>
	<td class="line x" title="23:139	Some classes may not have enough occurrences to allow a reliable estimation, while other classes may be too general and include too many words not relevant to the estimation." ></td>
	<td class="line x" title="24:139	An alternative is to obtain various classes ~sociated in a taxonomy with the words in question and select the closes according to a certain criteria." ></td>
	<td class="line x" title="25:139	There are a number of ways to select the classes used in the estimation." ></td>
	<td class="line x" title="26:139	Weischedel et al.(1993) chose the lowest classes in a taxonomy 1416 for which the association for the co-occurrence can be estimated." ></td>
	<td class="line x" title="28:139	This approach may result in unreliable estimates, since sonic of the class cooccurrences used may bc attributed to chance." ></td>
	<td class="line x" title="29:139	Resnik (1993) selected all pairs of closes corre,~ponding to the head of a prepositional phrase and weighted them to hi,us the computation of the association in favor of higher-frequency co-occurrences which hc considered 'more reliable.'" ></td>
	<td class="line x" title="30:139	Contrary to this ,assumption, high frequency co-occurrences arc unreliable when the probability thai; the co-occurrence may be att.ributcd to chance is high." ></td>
	<td class="line x" title="31:139	In this paper we propose a clmss-b;used method that selects the lowest cbLsses in a taxonomy for which the co-occurrence confidence is above a threshold." ></td>
	<td class="line x" title="32:139	We subsequently apply the method to solving structural ambiguities in Japanese dcpcndcncy structures and English prepositional phrase attachments." ></td>
	<td class="line x" title="33:139	2 Class-based Estimation of Strength of Association The strength of association (SA) may be measured using thc frequencies of word cooccurrences in large corpora." ></td>
	<td class="line oc" title="34:139	For instance, Church and Hanks (1990) calculated SA in terms of mutual infornlation between two words wl and w2: N * f(wl, w2) I(wl,w2) = lo.q2 f(wt)f(w2) (1) here N is the size of the corpus used in the estilnation, f(wl,W2) is the frequency of the cooccurrence, f(wl) and f(w2) that of each word." ></td>
	<td class="line x" title="35:139	When no co-occurrence is observed, SA may be estimated using the frequencies of word cl,'~sses that contain the words in question." ></td>
	<td class="line x" title="36:139	The nmtual information in this case is estimated by: N * f(C1, C2) I(CI,C2) ~-1o02 f(Cl)f(C2) (2) here C1 and C2 are the word classes that respectively contain wl and w2, f(C1) and f(C2) the nmnbers of occurrences of all the words included in the word classes C1 and C2, and f(C1, C2) is the number of co-occurrences of the word classes C1 and (72." ></td>
	<td class="line x" title="37:139	Normally, the estimation using word classes needs to select classes, from a taxonomy, for which co-occurrences arc significant." ></td>
	<td class="line x" title="38:139	We." ></td>
	<td class="line x" title="39:139	use tscores for this purpose'." ></td>
	<td class="line x" title="40:139	For a class co-occurrence (C1,C2), the t~ score may be approximated by: f(Cl, C2) l f(c1)f(C2) t ' (3) x/f(Ct,C2) We use the lowest cl~s co-occurrence for which the." ></td>
	<td class="line x" title="41:139	confidence memsured with t-scores is above a threshold 2." ></td>
	<td class="line x" title="42:139	Given a co-occurrence containing the word w, our method selects a class for w in the following way: Step 1: Obtain the classes C '1, C 2  C' associated with w in a taxonomy." ></td>
	<td class="line x" title="43:139	Step 2: Set i to 0." ></td>
	<td class="line x" title="44:139	Step 3: Set i to i + 1." ></td>
	<td class="line x" title="45:139	Step 4: Compute t using formula (3)." ></td>
	<td class="line x" title="46:139	Step 5: If t < threshold." ></td>
	<td class="line x" title="47:139	If i # n goto step 3." ></td>
	<td class="line x" title="48:139	Otherwise exit." ></td>
	<td class="line x" title="49:139	Step 6: Select the class C i to replace u,." ></td>
	<td class="line x" title="50:139	Let us see what this means with at: example." ></td>
	<td class="line x" title="51:139	Suppose we try to estimate SA for (produce,,tclcphonc) a. See Table 1." ></td>
	<td class="line x" title="52:139	Here f(v), f(n) and f(vn) are the frequencies for the verb Froduce, classes for the noun telephone, and cooccurrences between the verb and the claases for telephone, respectively; and t is the t-score d. I The t-score (Church and Mrrcer, 1993) cmnparcs the hyl)othesis that a co-occurrence is significant against the null hypothesis that tim co-occurrence can t)e attritmted to chalJ.Cc." ></td>
	<td class="line x" title="53:139	2The default tt, rcshohl for t-score is 1.28 which col responds to a confidence level of 90%." ></td>
	<td class="line x" title="54:139	t-scores are often inflated due to certain violations of assumptions." ></td>
	<td class="line x" title="55:139	3The data was obtained front 68,623 w:rb-noun lmirs in EDR Corpus (EDR, 1993)." ></td>
	<td class="line x" title="56:139	4In our theory, we are to use each pair of (C i, cJ), where i=1,2,m, j=l,2,,n, to calculate strengths of lexical associations." ></td>
	<td class="line x" title="57:139	But our experiments show that uppcr classes of a verl) are very unreliablc to bc used to measure the strengths." ></td>
	<td class="line x" title="58:139	The reason may be that, unlike nouns, the wrrbs would not have a 'neat' hierarchy or that the upper classes of a verl) become too general as they contain too many concel)ts un(lcrneath them." ></td>
	<td class="line x" title="59:139	Because of this obserwdion, we use, h)r the classes of a 1417 verb classes for telephone f(v) f(n) f(vn) t-score produce concrete thing 671 18926 100 -4.6 produce inanimate object 671 5593 69 0.83 produce implement/tool 671 2138 35 1.91 produce machine 671 664 19 2.86 produce connnunication machine 671 83 1 0.25 produce telephone 671 24 0 Table 1 Estimation of (produce telephone) The lowest cl~s co-occurrence (produce, communication machine} h~ a low t-score and produces a bad estimation." ></td>
	<td class="line x" title="60:139	The most frequent co-occurrence (produce, concrete thing) has a low t-score also reflecting the fact that it may be attributed to chance." ></td>
	<td class="line x" title="61:139	The t-scores for (produce, machine} and (produce, implement/tool) are high and show that these co-occurrences are significant." ></td>
	<td class="line x" title="62:139	Among them, our method selects the lowest class co-occurrence for which the t-score is above the threshold: (produce, machine)." ></td>
	<td class="line x" title="63:139	3 Disambiguation Using Class-Based Estimation We." ></td>
	<td class="line x" title="64:139	now apply our method to estimate SA for two different types of syntactic constructions and use the results in resolving structural ambiguities." ></td>
	<td class="line x" title="65:139	3.1 Disambiguation of Dependency Relations in Japanese Identifying the dependency structure of a Japanese sentence is a difficult problem since the language allows relatively free word orders." ></td>
	<td class="line x" title="66:139	A typical dependency relation in Japanese appears in the form of modifierparticle-nmdificand triplets." ></td>
	<td class="line x" title="67:139	When a modifier is followed by a mnnber of possible modificands, verb, the verb itself or, when it does not give us a good result, only the lowest class of the verb in calculating the strength of association (SA)." ></td>
	<td class="line x" title="68:139	Titus, for an example, the verb eat has a sequence of eat -+ ingest -+ put something into body -+ -+ event -~ concept in the (:lass hierarchy, but we use only eat and ingest for the verb eat when calculating SA for (eat, apple}." ></td>
	<td class="line x" title="69:139	there arise situations in which syntactic rules may be unable to determine the dependency relation or the modifier-modificand relation." ></td>
	<td class="line x" title="70:139	For instance, in '~-~ 0 '(vigorous) may modify either :~ ~#' (middle aged) or ' ~g{~:~ ' ( health care)." ></td>
	<td class="line x" title="71:139	But which one is the modificand of' ~ & ~ 0 ' ? We solve the ambiguity comparing the strength of association for the two or more possible dependency relations." ></td>
	<td class="line x" title="72:139	Calculation of Strength of Association We." ></td>
	<td class="line x" title="73:139	calculate the Strength of Association (SA) score for modifier particle modi f icand by: \[ N ._ I v, ,,, \] SA(my;p,,.,, m,,) = loq2 k, f(Cmyi~,.)f(p,,.tm,:) / (4) where Cm/i~.,stands for the classes that include the modifier word, P,,~t is the particle following the modifier, m,." ></td>
	<td class="line x" title="74:139	the content word in tile modificand phr,~se, and f the frequency." ></td>
	<td class="line x" title="75:139	Let us see the process of obtaining SA score in an example < ~ -/)~ ~ < ) (literally: professor subject-markerwork)." ></td>
	<td class="line x" title="76:139	To calculate the frequencies for the classes associated with ' ~ ', we obtain from the Co-occurrence Dictionary (COD) s the number of occurrences for (w5~'SCOD and CD are provided by Japan Electronic Dietionary Research Institute (EDR, 1993)." ></td>
	<td class="line x" title="77:139	COD contains the frequencies of individual words and of the modifier1418 < ), where w can be any modifier." ></td>
	<td class="line x" title="78:139	We then obtain fi'om the Concept Dictionary (CD) 6 the classes that include ' ~' and then stun up all the occurrences of words included in the classes." ></td>
	<td class="line x" title="79:139	The relevant portion of CD for ' ~' in ( ~\[~ oh ~-~< } is shown in Figure 1." ></td>
	<td class="line x" title="80:139	The numbers in parenthesis here indicate the summed-up fi'cquencies." ></td>
	<td class="line x" title="81:139	We then cMculate the t-score between ' ?)~ff~ < ' and all the classes that include' ~ '." ></td>
	<td class="line x" title="82:139	See 'Fable 2." ></td>
	<td class="line x" title="83:139	Classes for the tparticlemodifier 12~ score nmdificand ),N ~ tcN~ ~ ~ 7o ~i~ 4.57 ~~< 3,N 5.14 ~< {~R~\]-c}I~_?:XN 1.74 ~ < #~'e~." ></td>
	<td class="line x" title="84:139	f-)k ~ 0.74 /)~ < Table 2 t-scores for ( ~ re~J < ) The t-score for the co-occurrence of the modifier and particle-modificand pair, '~' and 'h~-t//< ', is higher than the threshold when : ~' is replaced with \[~.~U'~'~I~/~XN\]." ></td>
	<td class="line x" title="85:139	Using (4), the strength of association for the cooccurrence of ( ~ ~0~ ~J < ) is calculated from the SA between the class \[~Ij-C~_f.:)kN\] and ' rS~1I~ < .' When the word in question has more than one sense, we estimate SA corresponding to each sense and choose the one that results in the highest SA score." ></td>
	<td class="line x" title="86:139	For instance, we estimate SA between ' ~ ' and the various senses of :/~ < ', and choose the highest value: in this case the one corresponding to the sense 'to be employed.'" ></td>
	<td class="line x" title="87:139	Determination of Most Strongly Associated Structure After calculating SA for each possible construction, we choose the construction with lfighest SA score ~s the most probable strucparticlc-modificand triplets i,t a corpus that includes 220,000 parsed Japanese sentences." ></td>
	<td class="line x" title="88:139	CD provides a hierarchical structure of concepts corrcslmnding to all the words in COD." ></td>
	<td class="line x" title="89:139	The number of concepts in CD is about 400,000." ></td>
	<td class="line x" title="90:139	ture." ></td>
	<td class="line x" title="91:139	See the following example: vt~ 2.86   .~f*~N~ ~' ~I,,~ ~j< At) x b t~x tee\[ n qa progress work people stress i n nowd lon Here, the arrows show possible dependency relations, the numbers on the arrows the estimated SA, and the thick arrows the dependency with highest mutual information that means the most probable dependency relation." ></td>
	<td class="line x" title="92:139	In the example, ' ~$f:~?)~ ' modifies ' ~I~ ' and ' ~ < ' modifies ' A. '." ></td>
	<td class="line x" title="93:139	The estimated mutual information for ( ~'~/~g#h ~, i.~,~'e ) is 2.79 and that for ( ~ i, A ) is 6.13." ></td>
	<td class="line x" title="94:139	Thus, we choosc ' ~,~'C' ' as the modificaud for ' ~t$i~gN7)~ ' and ' Z. ' as that for '~< ' In the example shown in Figure 2, our method selects the nmst likely modifiermodificand relation." ></td>
	<td class="line x" title="95:139	Experiment Disa,nbiguation of dependency relations was done using 75 ambiguous constructions f,'mn Fukumoto (1992)." ></td>
	<td class="line x" title="96:139	Solving the ambiguity in the constructions involves choosing among two or nmre modifier-particlemodificand relations." ></td>
	<td class="line x" title="97:139	The training data consists of all 568,000 nmdifier-partiele-modificand triplets in COD." ></td>
	<td class="line x" title="98:139	Evaluation We evaluatcd the performance of our method comparing its results with those of other methods using the same test and training data." ></td>
	<td class="line x" title="99:139	Table 3 shows the various results (success rates)." ></td>
	<td class="line x" title="100:139	Here, (1) indicates the pcrfbrmance obtained using the principle of Closest Attachment (Kimball, 1973); (2) shows the performance obtained using the lowest observed class co-occurrence (Weischedel et al., 1993); (3) is the result from the maximum mutual information over all pairs of clm~ses corresponding to the words in the co-occurrence (Resnik, 1993; Alves, 1996); and (4) shows the performance of our method 7." ></td>
	<td class="line x" title="101:139	7The precision is for the 1.28 default threshold, The precision was 81.2% and 84.1% v, qu:n we set the thrcshohl to .84 and .95." ></td>
	<td class="line x" title="102:139	In all these cases the coverage was 92,0%." ></td>
	<td class="line x" title="103:139	1419 (42) I human or similar (3) person (39) human (3) person defined by race or origin (3) Japanese (2) worker (5) Figure 1 person defined by role (1) person defined by position (1) slave (0) professor An Extract of CD .'~.4 7 9.19 \[___~  ~ ~ ~ national investigation based cause prompt study expect Figure 2 An example of parsing a Japanese sentence method precision (1) closest attachnlent 70.6% (2) lowest classes 81.2% (3) maximmn MI 82.6% (4) our method 87.0% Table 3 Results for determining dependency relations Closest attachment (1) h,~s a low performance since it fails to take into consideration tile identity of the words involved in the decision." ></td>
	<td class="line x" title="104:139	Selecting the lowest classes (2) often produces unreliable estimates and wrong decisions due to data sparseness." ></td>
	<td class="line x" title="105:139	Selecting the classes with highest mutual information (3) results in overgeneralization that may lead to incorrect attachments." ></td>
	<td class="line x" title="106:139	Our method avoids both estimating from unreliable classes and overgeneralization and results in better estimates and a better performance." ></td>
	<td class="line x" title="107:139	A qualitative analysis of our results shows two causes of errors, however." ></td>
	<td class="line x" title="108:139	Some errors occurred when there were not enough occurrences of the particle-modificand pattern to estimate any of the strength of mssociation necessary for resolving ambiguity." ></td>
	<td class="line x" title="109:139	Other errors occun'ed when the decision could not be made without surrounding context." ></td>
	<td class="line x" title="110:139	3.2 Prepositional Phrase Attachment in English Prepositional phrase (PP) attachment is a paradigm c,~se of syntactic ambiguity." ></td>
	<td class="line x" title="111:139	The most probable attachnlent may be chosen comparing the SA between the PP and the various attachment elements." ></td>
	<td class="line x" title="112:139	Here SA is measured by: SA(v-attachlv'p'n'e) = lg'~ ( N * f(C''p'C''2) C,, 2 )  SA(n-attach.ln,,p, n2) = log.e \ -/-(C~p. C--,,~) ) (6) where Cw stands for the class that includes the word w and f is the frequency in a training data containing verb-nounl-preposition-noun2 constructions." ></td>
	<td class="line x" title="113:139	Our method selects fl'om a taxonomy the cla.sses to be used to calculate the SA score and 1420 then chooses the attachment with highest SA score ,as the most probable." ></td>
	<td class="line x" title="114:139	Experiment We performed a PP attachment experiment on the data that consists of all the 21,046 semantically annotated verb-nounpreposition-noun constructions found in EDR English Corpus." ></td>
	<td class="line x" title="115:139	We set aside 500 constructions for test and used the remaining 20,546 as training data." ></td>
	<td class="line x" title="116:139	We first performed the experiment using various values for the threshoM." ></td>
	<td class="line x" title="117:139	Table 4 shows the results." ></td>
	<td class="line x" title="118:139	The first line here shows the default which corresponds to the most likely attachment for each preposition." ></td>
	<td class="line x" title="119:139	For instance, the preposition of is attached to the noun, reflecting the fact that PP's led by of are nmstly attached to nouns in the training data." ></td>
	<td class="line x" title="120:139	The :confidence' values correspond to a binomial distribution and are given only as a reference s. confidence t coverage precision success 100% 68.0% 68.0% 50% .00 82% 82.2% 79.4% 70% .52 75% 87.3% 83.4% 80% .84 65% 88.6% 84.2% 85% .95 57% 89.6% 84.8% 90% 1.28 50% 91.3% 85.6% Table 4 Results for PP attachment with various thresholds for t-score The precision grows with t-scores, while coverage decreases." ></td>
	<td class="line x" title="121:139	In order to improve coverage, when the method cannot find a class co-occnrrence for which the t-score is above the threshold, we recursively tried to find a co-occurrence using the threshold immediately smaller (see Table 4)." ></td>
	<td class="line x" title="122:139	When the method could not find co-occurrences with t-score above the smallest threshold, the default was used." ></td>
	<td class="line x" title="123:139	The overall success rates are shown in 'success' column in Table 4." ></td>
	<td class="line x" title="124:139	SAs another way of reducing the sparse data problem, we clustered prepositions using the nmthod described in \u and Furugori (1996)." ></td>
	<td class="line x" title="125:139	Prepositions like synonylns and antonyms are chtstered into groups and replaced by a representative ineposition (e.g., till aim pending are replaced by until; among.~t, amid and amid,st are replaced by among.)." ></td>
	<td class="line x" title="126:139	Evaluation We evaluated the performance of our method comparing its results with those of other methods with the same test and training data." ></td>
	<td class="line x" title="127:139	The results are given in Table 5." ></td>
	<td class="line x" title="128:139	Here, (5) shows the performance of two native speakers who were just presented quadruples of four head words without surrounding contexts." ></td>
	<td class="line x" title="129:139	Method Success Rate (1)closest Attachment 59.6% (2)lowest classes 80.2% (3) nlaxinnlnl MI 79.0% (4)our nmthod 85.6% (5)human (head words only) 87.0% Table 5 Comparison with other methods The lower bound and the upper bound on the performance of our method seem to be 59.6% scored by the simple heuristic of closest attachment (1) and 87.0% by hmnan beings (4)." ></td>
	<td class="line x" title="130:139	Obviously, the success rate of closest attachment (1) is low as it Mways attaches a word to the noun without considering the words in question." ></td>
	<td class="line x" title="131:139	The unanticipated low success rate of human judges is partly due to the fact that sometimes constructions were inherently ambiguous so that their choices differed from the annotation in the corpus." ></td>
	<td class="line x" title="132:139	Our method (4) performed better than the lowest classes method (2) and nlaximum MI method (3)." ></td>
	<td class="line x" title="133:139	It owes mainly to the fact that our method makes the estimation fi'om class cooccurrences that are more reliable." ></td>
	<td class="line x" title="134:139	4 Concluding Remarks We, proposed a class-based method that selects classes to be used to estimate tile strength of association for word co-occurrences." ></td>
	<td class="line x" title="135:139	The classes selected by our method can be used to estimate various types of strength of association in (lifterent applications." ></td>
	<td class="line x" title="136:139	The nmthod differs fi'mn other clmss-ba,sed methods in that it allows identification of a reliable and specific class for each cooccurrence in consideration and call deal with date sparseness problem more efficiently." ></td>
	<td class="line x" title="137:139	It 1421 overcame the shortcomings fl'om other methods: overgeneralization and employment of unreliable class co-occurrences." ></td>
	<td class="line x" title="138:139	We applied our method to two structural disambiguation experiments." ></td>
	<td class="line x" title="139:139	In both experiments the performance is significantly better than those of others." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C98-2238
How to thematically segment texts by using lexical cohesion?
Ferret, Olivier;"></td>
	<td class="line x" title="1:79	How to thematically segment texts by using lexical cohesion?" ></td>
	<td class="line x" title="2:79	Olivier Ferret LIMS1-CNRS BP 133 F-91403 Orsay Cedex, FRANCE ferret@limsi.fr Abstract This article outlines a quantitative method for segmenting texts into thematically coherent units." ></td>
	<td class="line x" title="3:79	This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words." ></td>
	<td class="line x" title="4:79	We also present the results of an experiment about locating boundaries between a series of concatened texts." ></td>
	<td class="line x" title="5:79	1 Introduction Several quantitative methods exist for thematically segmenting texts." ></td>
	<td class="line x" title="6:79	Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at tile lexical level." ></td>
	<td class="line x" title="7:79	Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence." ></td>
	<td class="line x" title="8:79	Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion." ></td>
	<td class="line x" title="9:79	The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific." ></td>
	<td class="line x" title="10:79	As a concept is always expressed by the same word, word repetitions are thematically significant in these texts." ></td>
	<td class="line x" title="11:79	The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means." ></td>
	<td class="line x" title="12:79	However, tMs second approach requires knowledge about the cohesion between words." ></td>
	<td class="line x" title="13:79	Morris and Hirst (1991) extract this knowledge from a thesaurus." ></td>
	<td class="line x" title="14:79	Kozima (1993) exploits a lexical network built from a machine readable dictionary (MRD)." ></td>
	<td class="line x" title="15:79	This article presents a method for thematically segmenting texts by using knowledge about lexical cohesion that has been automatically built." ></td>
	<td class="line x" title="16:79	This knowledge takes the form of a network of lexical collocations." ></td>
	<td class="line x" title="17:79	We claim that this network is as suitable as a thesaurus or a MRD for segmenting texts." ></td>
	<td class="line x" title="18:79	Moreover, building it for a specific domain or for another language is quick." ></td>
	<td class="line x" title="19:79	2 Method The segmentation algorithm we propose includes two steps." ></td>
	<td class="line x" title="20:79	First, a computation of the cohesion of the different parts of a text is done by using a collocation network." ></td>
	<td class="line x" title="21:79	Second, we locate the major breaks in this cohesion to detect the thematic shifts and build segments." ></td>
	<td class="line x" title="22:79	2.1 The collocation network Our collocation network has been built from 24 months of tile French Lc Monde newspaper." ></td>
	<td class="line x" title="23:79	The size of this corpus is around 39 million words." ></td>
	<td class="line oc" title="24:79	The cohesion between words has been evaluated with the mutual information measure, as in (Church and Hanks, 1990)." ></td>
	<td class="line x" title="25:79	A large window, 20 words wide, was used to take into account the thematic links." ></td>
	<td class="line x" title="26:79	The texts were pre-processed with the probabilistic POS tagger TreeTagger (Schmid, 1994) in order to keep only the lemmatized form of their content words, i.e. nouns, adjectives and verbs." ></td>
	<td class="line x" title="27:79	The resulting network is composed of approximatively 31 thousand words and 14 million relations." ></td>
	<td class="line x" title="28:79	2.2 Computation of text cohesion As in Kozima's work, a cohesion value is computed at each position of a window in a text (after pre-processing) from the words in this window." ></td>
	<td class="line x" title="29:79	The collocation network is used for determining how close together these words are." ></td>
	<td class="line x" title="30:79	We suppose that if the words of the window are strongly connected in the network, they belong to the same domain and so, the cohesion in this part of text is high." ></td>
	<td class="line x" title="31:79	On the contrary, if they are not very much linked together, we assume that tile words of the window belong to two different domains." ></td>
	<td class="line x" title="32:79	It means that the window is located across the transition from one topic to another." ></td>
	<td class="line x" title="33:79	1481 Pw2XO.21+Pw3XO.lO = 0.31 0.31 0.48 = Pw3XO.18+Pw4XO.13 0 G +pw5xO'17 0 Ty,0 /l\r.,7 1.14 1.14 1.0 1.0 1.0 wl w2 w3 w4 w5 \] word from the collocation network (with its computed weight) word from the text (with its computed weight i.0 ex." ></td>
	<td class="line x" title="34:79	for the first word: Pwl+PwlXO.14 = 1.14) o.14 link in the collocation network (with its cohesion value) Pwi initial weight of the word of the window wi (equal to 1.0 here) Figure 1: Computation of word weight In practice, the cohesion inside the window is evaluated by the sum of the weights of the words in this window and the words selected from the collocation network common to at least two words of the window." ></td>
	<td class="line x" title="35:79	Selecting words from the network linked to those of the texts makes explicit words related to the same topic as the topic referred by the words in the window and produces a more stable description of this topic when the window moves." ></td>
	<td class="line x" title="36:79	As shown in Figure 1, each word w (from the window or from the network) is weighted by the sum of the contributions of all the words of the window it is linked to." ></td>
	<td class="line x" title="37:79	The contribution of such a word is equal to its number of occurrences in the window modulated by the cohesion measure associated to its link with w. Thus, the more the words belong to a same topic, the more they are linked together and the higher their weights are." ></td>
	<td class="line x" title="38:79	Finally, the value of the cohesion for one position of the window is the result of the following weighted sum: eoh(p) = ~-\]i sign(wi) . wght(wi), with wght(wi), the resulting weight of the word wi, sign(wi), the significance of wi, i.e. the normalized information of wi in the Le Monde corpus." ></td>
	<td class="line x" title="39:79	Figure 2 shows the smoothed cohesion graph for ten texts of the experiment." ></td>
	<td class="line x" title="40:79	Dotted lines are text boundaries (see 3.1)." ></td>
	<td class="line x" title="41:79	2.3 Segmenting the cohesion graph First, the graph is smoothed to more easily detect the main minima and maxima." ></td>
	<td class="line x" title="42:79	This operation is done again by moving a window on the text." ></td>
	<td class="line x" title="43:79	At each position, the cohesion associ50 45 40 35 25 20 15 10 0 : i i. i 50 100 150 1 2O0 : t, i: i : i I 250 300 350 Position of the words Figure 2: The cohesion graph of a series of texts ated to the window center is re-evaluated as the mean of all the cohesion values in the window." ></td>
	<td class="line x" title="44:79	After this smoothing, the derivative of the graph is calculated to locate the maxima and the minima." ></td>
	<td class="line x" title="45:79	We consider that a minimum marks a thematic shift." ></td>
	<td class="line x" title="46:79	So, a segment is characterized by the following sequence: minimum maximum minimum." ></td>
	<td class="line x" title="47:79	For making the delimitation of the segments more precise, they are stopped before the next (or the previous) minimum if there is a brutal break of the graph and after this, a very slow descent." ></td>
	<td class="line x" title="48:79	This is done by detecting that the cohesion values fall under a given percentage of the maximum value." ></td>
	<td class="line x" title="49:79	3 Results A first qualitative evaluation of the method has been done with about 20 texts but without a formal protocol as in (ttearst, 1997)." ></td>
	<td class="line x" title="50:79	The results of these tests are rather stable when parameters such as the size of the cohesion computing window or the size of the smoothing window are changed (from 9 to 21 words)." ></td>
	<td class="line x" title="51:79	Generally, the best results are obtained with a size of 19 words for the first window and 11 for the second one." ></td>
	<td class="line x" title="52:79	3.1 Discovering document breaks In order to have a more objective evaluation, the method has been applied to the 'classical' task of discovering boundaries between concatened texts." ></td>
	<td class="line x" title="53:79	Results are shown in Table 1." ></td>
	<td class="line x" title="54:79	As in (Hearst, 1997), boundaries found by the method are weighted and sorted in decreasing order." ></td>
	<td class="line x" title="55:79	Document breaks are supposed to be the boundaries that have the highest weights." ></td>
	<td class="line x" title="56:79	For the first Nb boundaries, Nt is the number of boundaries that match with document breaks." ></td>
	<td class="line x" title="57:79	Precision is 1482 Nb Nt Precision 10 5 0.5 20 10 0.5 30 17 0.58 38 19 0.5 40 20 0.5 50 24 0.48 60 26 0.43 67(Nb max ) 26 0.39 (P) Recall (R) 0.13 0.26 0,45 0.5 0.53 0.63 0.68 0.68 Table 1: Results of the experiment given by Nt/Nb and recall, by Nt/N, where N is the number of document breaks." ></td>
	<td class="line x" title="58:79	Our evaluation has been performed with 39 texts coming from the Le Monde newspaper, but not taken from the corpus used for building the collocation network." ></td>
	<td class="line x" title="59:79	Each text was 80 words long on average." ></td>
	<td class="line x" title="60:79	Each boundary, which is a minimum of the cohesion graph, was weighted by the sum of the differences between its value and the values of the two maxima around it, as in (Hearst, 1997)." ></td>
	<td class="line x" title="61:79	The match between a boundary and a document break was accepted if the boundary was no further than 9 words (after pre-processing)." ></td>
	<td class="line x" title="62:79	Globally, our results are not as good as Hearst's (with 44 texts; Nb: 10, P: 0.8, R: 0.19; Nb: 70, P: 0.59, R: 0.95)." ></td>
	<td class="line x" title="63:79	The first explanation for such a difference is the fact that the two methods do not apply to the same kind of texts." ></td>
	<td class="line x" title="64:79	Hearst does not consider texts smaller than 10 sentences long." ></td>
	<td class="line x" title="65:79	All the texts of this evaluation are under this limit." ></td>
	<td class="line x" title="66:79	In fact, our method, as Kozima's, is more convenient for closely tracking thematic evolutions than for detecting the major thematic shifts." ></td>
	<td class="line x" title="67:79	The second explanation for this difference is related to the way the document breaks are found, as shown by the precision values." ></td>
	<td class="line x" title="68:79	When Nb increases, precision decreases as it generally does, but very slowly." ></td>
	<td class="line x" title="69:79	The decrease actually becomes significant only when Nb becomes larger than N. It means that the weights associated to the boundaries are not very significant." ></td>
	<td class="line x" title="70:79	We have validated this hypothesis by changing the weighting policy of the boundaries without having significant changes in the results." ></td>
	<td class="line x" title="71:79	One way for increasing the performance would be to take as text boundary not the position of a minimum in the cohesion graph but the nearest sentence boundary from this position." ></td>
	<td class="line x" title="72:79	4 Conclusion and future work We have presented a method for segmenting texts into thematically coherent units that relies on a collocation network." ></td>
	<td class="line x" title="73:79	This collocation network is used to compute a cohesion value for the different parts of a text." ></td>
	<td class="line x" title="74:79	Segmentation is then done by analyzing the resulting cohesion graph." ></td>
	<td class="line x" title="75:79	But such a numerical value is a rough characterization of the current topic." ></td>
	<td class="line x" title="76:79	For future work we will build a more precise representation of the current topic based oil the words selected from the network." ></td>
	<td class="line x" title="77:79	By computing a similarity measure between the representation of the current topic at one position of the window and this representation at a flarther one, it will be possible to determine how thematically far two parts of a text are." ></td>
	<td class="line x" title="78:79	The minima of the measure will be used to detect the thematic shifts." ></td>
	<td class="line x" title="79:79	This new method is closer to Hearst's than the one presented above but it relies on a collocation network for finding relations between two parts of a text instead of using the word recurrence." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W98-1104
Using Suffix Arrays To Compute Term Frequency And Document Frequency For All Substrings In A Corpus
Yamamoto, Mikio;Church, Kenneth Ward;"></td>
	<td class="line x" title="1:254	Using Suffix Arrays to Compute Term Frequency and Document Frequency for All Substrings in a Corpus Mikio Yamamoto University of Tsukuba 1-1-1 Tennodai, Tsukuba 305-8573, JAPAN myama@is.tsukuba.ac.jp Kenneth W. Church AT&T Labs Research 180 Park Avenue Florham Park, NJ 07932, U.S.A kwc @research.att.com Abstract Mutual Information (MI) and similar measures are often used in corpus-based linguistics to find interesting ngrams." ></td>
	<td class="line x" title="2:254	MI looks for bigrams whose term frequency (~ is larger than chance." ></td>
	<td class="line x" title="3:254	Residual Inverse Document Frequency (RIDF) is similar, but it looks for ngrams whose document frequency (df) is larger than chance." ></td>
	<td class="line x" title="4:254	Previous studies have tended to focus on relatively short ngrams, typically bigrams and trigrams." ></td>
	<td class="line x" title="5:254	In this paper, we will show that this approach can be extended to arbitrarily long ngrams." ></td>
	<td class="line x" title="6:254	Using suffix arrays, we were able to compute tf, df and RIDF for all ngrams in two large corpora, an English corpus of 50 million words of Wall Street Journal news articles and a Japanese corpus of 216 million characters of Mainichi Shimbun news articles." ></td>
	<td class="line x" title="7:254	1 MI and RIDF Mutual Information (MI), l(x;y), compares the probability of observing word x and word y together (the joint probability) with the probabilities of observing x and y independently (chance)." ></td>
	<td class="line oc" title="8:254	l(x;y) = log (P(x,y) / e(x)e(y) ) MI has been used to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type to lexico-syntactic co-occurrence preferences of the save/from type (Church and Hanks, 1990)." ></td>
	<td class="line x" title="9:254	Church and Gale (1995) proposed Residual 28 Inverse Document Frequency (RIDF), the difference between the observed IDF and what would be expected under a Poisson model for a random word or phrase with comparable frequency." ></td>
	<td class="line x" title="10:254	RIDF is a variant of IDF, a standard method for weighting keywords in Information Retrieval (IR)." ></td>
	<td class="line x" title="11:254	Let D be the number of documents, tf be the term frequency (what we call '~frequency' in our field) and dfbe the document frequency (the number of documents which contain the word or phrase at least once)." ></td>
	<td class="line x" title="12:254	RIDF is defined as: Residual IDF ~ observed IDF predicted IDF = -log(df/D) +log(1-exp(8)) = -log(df/D) +log(1-exp(-tf/D))." ></td>
	<td class="line x" title="13:254	RIDF is, in certain sense, like MI; both are the log of the ratio between an empirical observation and a chance-based estimate." ></td>
	<td class="line x" title="14:254	Words or phrases with high RIDF or MI have distributions that cannot be attributed to chance." ></td>
	<td class="line x" title="15:254	However, the two measures look for different kinds of deviations from chance." ></td>
	<td class="line x" title="16:254	MI tends to pick out general vocabulary, the kind of words one would expect to find in a dictionary, whereas RIDF tends to pick out good keywords, the kind of words one would not expect to find in a dictionary." ></td>
	<td class="line x" title="17:254	This distinction is not surprising given the history of the two measures; MI, as it is currently used in our field, came from lexicography whereas RIDF came from Information Retrieval." ></td>
	<td class="line x" title="18:254	In addition, it is natural to compute RIDF for all substrings." ></td>
	<td class="line x" title="19:254	This is generally not done for MI, though there are many ways that MI could be generalized to apply to longer ngrams." ></td>
	<td class="line x" title="20:254	In the next section, we will show an algorithm based on suffix arrays for computing tf, df and RIDF for all substrings in a corpus in O(NlogN) time." ></td>
	<td class="line x" title="21:254	In section 3, we will compute RIDF's for all substrings in a corpus and compare and contrast MI and RIDF experimentally for phrases in a English corpus and words/phrases in a Japanese corpus." ></td>
	<td class="line x" title="22:254	We won't try to argue that one measure is better than the other; rather we prefer to view the two measures as mutually complementary." ></td>
	<td class="line x" title="23:254	2 Computing tf and df for all substrings 2.1 Suffix arrays A suffix array is a data structure designed to make it convenient to compute term frequencies for all substrings in a corpus." ></td>
	<td class="line x" title="24:254	Figure 1 shows an example of a suffix array for a corpus of N=6 words." ></td>
	<td class="line x" title="25:254	A suffix array, s, is an array of all N suffixes, pointers to substrings that start at position i and continue to the end of the corpus, sorted alphabetically." ></td>
	<td class="line x" title="26:254	The following very simple C function, suffixarray, takes a corpus as input and returns a suffix array." ></td>
	<td class="line x" title="27:254	int suffix_compare(char **a, char **b){ return strcmp(*a, *b); } /* The input is a string, terminated with a null */ char **suffix_array(char *corpus){ int i, N = strten(corpus); char **result=(char **)rnalloc(N*sizeof(char *)); /* initialize result\[i\] with the ith suffix */ for(i=0; i < N; i++) result\[il = corpus + i; ClSOr't(result, N, sizeof(char *), suffix_compare); return result; } Nagao and Mori (1994) describe this procedure, and report that it works well on their corpus, and that it requires O(NlogN)time, assuming that the sort step requires O(NlogN) comparisons, and that each comparison requires 0(1) time." ></td>
	<td class="line x" title="28:254	We tried this procedure on our two corpora, and it worked well for the Japanese one, but unfortunately, it can go quadratic for a corpus with long repeated substfings, where strcmp takes O(N) time rather than 0(1) time." ></td>
	<td class="line x" title="29:254	For our English corpus, after 50 hours of cpu time, we gave up and turned to Doug Mcllroy's implementation ( http : //cm." ></td>
	<td class="line x" title="30:254	bell-labs, corrJcm/cs/ who/doug/ssort, c) of Manber and Myers' (1993) algorithm, which took only 2 hours." ></td>
	<td class="line x" title="31:254	For a corpus that would 29 otherwise go quadratic, the Manber and Myers' algorithm is well worth the effort, but otherwise, the procedure described above is simpler, and often a bit faster." ></td>
	<td class="line x" title="32:254	As mentioned above, suffix arrays were designed to make it easy to compute term frequencies (~." ></td>
	<td class="line x" title="33:254	If you want the term frequency of 'to be,' you can do a binary search to find the first and last position in the suffix array that start with this phrase, i and j, and then tfl'to be') =j-i+l. In this case, i=5 and j=6, and consequently, tfl'to be')=6-5+1=2." ></td>
	<td class="line x" title="34:254	Similarly, tfl'be')= 2-1+1 = 2, and ~'to')=6-5+1=2." ></td>
	<td class="line x" title="35:254	This straightforward method of computing tf requires O(logN) string comparisons, though as before, each string comparison could take O(N) time." ></td>
	<td class="line x" title="36:254	There are more sophisticated algorithms that take O(logN) time, even for corpora with long repeated substrings." ></td>
	<td class="line x" title="37:254	A closely related concept is lcp (longest common prefix)." ></td>
	<td class="line x" title="38:254	Lcp is a vector of length N, where lcp\[i\] indicates the length of the common prefix between the ith suffix and the/+/st suffix in the suffix array." ></td>
	<td class="line x" title="39:254	Manber and Myers (1993) showed how to compute the lcp vector in O(NlogN) time, even for corpora with long repeated substrings, though for many corpora, the complications required to avoid quadratic behavior are unnecessary." ></td>
	<td class="line x" title="40:254	Corpus: 'to be or not to be' s\[i\] s\[z\] s\[3\] s\[4\] s\[s\] s\[s\] 1 2 3 4 Alphabet: \[to, be, or, not} lcp -Ior not to be I not to be I or not to to be to be or be\] not to be\] 0 0 0 2 0 Lcp's are denoted by bold vertical lines as well as the Icp table." ></td>
	<td class="line x" title="41:254	Figure 1: An example of a Suffix Array with lcp's 2.2 Classes of substrings Thus far we have seen how to compute tf for a single ngram, but how do we compute tfand dffor all ngrams?" ></td>
	<td class="line x" title="42:254	There are N(N+I)/2 substrings in a text of size N. If every substring has a different tf and df, the counting algorithm would require at least quadratic time and space." ></td>
	<td class="line x" title="43:254	Fortunately many substrings have the same tf and the same df." ></td>
	<td class="line x" title="44:254	We will cluster the N(N+I)/2 substrings into at most 2N-1 classes and compute tf and df over the classes." ></td>
	<td class="line x" title="45:254	There will be at most N distinct values of RIDF." ></td>
	<td class="line x" title="46:254	Let <i,j> be an interval on the suffix array: {s\[i\], s\[i+l\]  s\[j\]}." ></td>
	<td class="line x" title="47:254	We call the interval LCPdelimited if the lcp's are larger inside the interval than at its boundary: min(lcp\[i\], lcp\[i+ l\]  lcp\[j-1\]) > max(lcp\[i-1\], Icp\[l\]) (1) In Figure 1, for example, the interval <5,6> is LCP-delimited, and as a result, 0fCto') = tf('to be') = 2, and dfCto')=dfCto be')." ></td>
	<td class="line x" title="48:254	The interval <5,6> is associated with a class of substrings: 'to' and 'to be'." ></td>
	<td class="line x" title="49:254	Classes will turn out to be important because all of the substrings in a class have the same tf(property l) and the same df (property 2)." ></td>
	<td class="line x" title="50:254	In addition, we will show that classes partition the set of substrings (property 3) so that we can compute tf and df on the classes, rather than substrings." ></td>
	<td class="line x" title="51:254	Doing so is much more efficient because there many fewer classes than substfings (property 4)." ></td>
	<td class="line x" title="52:254	Classes of substrings are defined to be the (not necessarily least) common prefixes in an interval." ></td>
	<td class="line x" title="53:254	In Figure 1, for example, both 'to' and 'to be' are common prefixes throughout the interval <5,6>." ></td>
	<td class="line x" title="54:254	That is, every suffix in the interval <5,6> starts with 'to,' and every suffix also starts with 'to be'." ></td>
	<td class="line x" title="55:254	More formally, we define class(<ij>) as: {s\[i\]ml LBL<rn_<SIL}, where s\[i\]rn is a substring (the first m characters of s\[i\]), LBL(longest boundary lcp) is the fight hand of (1) and SIL (shortest interior Icp) is the left hand side' of (1)." ></td>
	<td class="line x" title="56:254	In Figure 1, for example, SIL(<5,6>) = min(lcp\[5\]) = 2, LBL(<5,6>) = max(lcp\[4\], lcp\[6\]) =0, and class(<5,6>) = {s\[5\]m I 0<m_<2} = {'to', 'to be'}." ></td>
	<td class="line x" title="57:254	Figure 2 shows six LCP-delimited intervals and the LBL and SIL of <2,4>." ></td>
	<td class="line x" title="58:254	For <2,4>, the bounding lcp's are lcp\[1 \] = 2 and lcp\[4\]=3 (LBL=3), and the interior lcp's are lcp\[2\]=4 and lcp\[3\]=6 (SIL=4)." ></td>
	<td class="line x" title="59:254	The interval <2,4> is LCPdelimited, because L B L<SIL." ></td>
	<td class="line x" title="60:254	Class(<2,4>)= {s\[2\]m13<m~<4} = {aacc}." ></td>
	<td class="line x" title="61:254	The interval <3,3> is *) SIL(<i,i>) is defined to be infinity, and consequently, all intervals <i,i> are LCP-delimited, forall i. 30 Doc-id ( ,." ></td>
	<td class="line x" title="62:254	382 s\[1\] ~84987 stZ\] \6892 s\[3\] --382 s\[4\] 2566 s\[5\] s\[6\] 1 2 3 4 5 6 7 a alb b c c d a aLc old d e 4, ::  Bounding Icps, LBL, SIL, Intedor Icps of <2, 4> Vertical lines denote lcps." ></td>
	<td class="line x" title="63:254	Gray area denotes endpoints of substrings in class(<2,4>)." ></td>
	<td class="line x" title="64:254	LCP-delimited Class interval <2,4> {aacc} <3,4> {aacce, aaccee} <1,1> {aab, aabb, aabbc, } <2,2> {aaccd, aaccdd, } <3,3> {aacceef, } <4,4> {aacceeg, } LBL SIL tf 2 4 3 3 6 2 2 infinity 1 4 infinity 1 6 infinity \] 6 infinity 1 Figure 2: Examples of intervals and classes LCP-delimited because SIL is infinite and LBL=6." ></td>
	<td class="line x" title="65:254	The interval <2,3> is not LCP-delimited because SIL is 4 and LBL is 6 (LBL>SIL)." ></td>
	<td class="line x" title="66:254	By construction, the suffixes within the interval <i,j> all start with the substrings in class( <i,j> ), and no suffixes outside this interval start with these substfings." ></td>
	<td class="line x" title="67:254	As a result, if sl and s2 are two substfings in class(<ij>) then Property 1: tflsJ) = tfls2) =j-i+l  and Property 2: dr(s1) = df(s2)." ></td>
	<td class="line x" title="68:254	The calculation of dfis more complicated than tf, and will be discussed in section 2.4." ></td>
	<td class="line x" title="69:254	It is not uncommon for an LCP-delimited interval to be nested within another." ></td>
	<td class="line x" title="70:254	In Figure 2, for example, the in~rval <3,4> is nested within <2,4>." ></td>
	<td class="line x" title="71:254	The computation of df in section 2.4 will take advantage of a very convenient nesting property." ></td>
	<td class="line x" title="72:254	Given two LCP-delimited intervals, either one is nested within the other (e.g. , <2,4> and <3,4>), or one precedes the other (e.g. , <2,2> and <3,4>), but they cannot overlap." ></td>
	<td class="line x" title="73:254	Thus, for example, the intervals <1,3> and <2,4> cannot both be LCP-delimited because they overlap." ></td>
	<td class="line x" title="74:254	Because of this nesting property, it is possible to express the dfof an interval recursively in terms of its constituents or subintervals." ></td>
	<td class="line x" title="75:254	As mentioned aboye, we will use the following partitioning property so that we can compute tfand dfon the classes rather than on the substrings." ></td>
	<td class="line x" title="76:254	Property 3: the classes partition the set of all substrings in a text." ></td>
	<td class="line x" title="77:254	There are two parts to this argument: every substfing belongs to at most one class (property 3a), and every substring belongs to at least one class (property 3b)." ></td>
	<td class="line x" title="78:254	Demonstration of property 3a (proof by contradiction): Suppose there is a substfing, s, that is a member of two classes: class(<ij>) and class(<u,v>)." ></td>
	<td class="line x" title="79:254	There are three possibilities: one interval precedes the other, they are property nested or they overlap." ></td>
	<td class="line x" title="80:254	The only interesting case is the nesting case." ></td>
	<td class="line x" title="81:254	Suppose without loss of generality that <u,v> is nested within <ij> as in Figure 3." ></td>
	<td class="line x" title="82:254	Because <u,v> is LCP-delimited, there must be a bounding lcp of <u,v> that is smaller than any lcp within <u,v>." ></td>
	<td class="line x" title="83:254	This bounding Icp must be within <i j>, and as a result, class(<ij>) and class(<u,v>) must be disjoint." ></td>
	<td class="line x" title="84:254	Therefore, s cannot be in both classes." ></td>
	<td class="line x" title="85:254	t Suffix Array / SIL of <i,j> $\[i\]1 I 1 I s\[.\]/ ^ I, II / I I s\[v\]/ I,fl I sb\],.<j'l ?h~s is an interior lcp of <i,j> and the LBL of <u, v>." ></td>
	<td class="line x" title="86:254	Figure 3: An example of nested intervals Demonstration of property 3b (constructive argument): Let s be an arbitrary substring in the corpus." ></td>
	<td class="line x" title="87:254	There will be at least one suffix in the suffix array that starts with s. Let i be the first such suffix and let j be the last such suffix." ></td>
	<td class="line x" title="88:254	By construction, the interval <i j> is LCP-delimited (LBL(<ij>) < Isl and S1L(<ij>) >_ Isl), and s is an element of class(<ij>)." ></td>
	<td class="line x" title="89:254	Finally, as mentioned above, computing over classes is much more efficient than computing over the substfings themselves because there are many fewer classes (at most 2N-l) than substrings (N(N+I)/2)." ></td>
	<td class="line x" title="90:254	31 Property 4: There are N classes with tf=l and at most N-1 classes with ~'> 1." ></td>
	<td class="line x" title="91:254	The first clause is relatively straightforward." ></td>
	<td class="line x" title="92:254	There are N intervals <i,i>." ></td>
	<td class="line x" title="93:254	These are all and only the intervals with tf=l. By construction, these intervals are LCP-delimited." ></td>
	<td class="line x" title="94:254	To argue the second clause, we will make use of a uniqueness property: an LCP-delimited interval <ij> can be uniquely determined by its S1L and a representative element k (i.~.k<j)." ></td>
	<td class="line x" title="95:254	Suppose there were two distinct intervals, <id> and <u,v>, with the same SIL, SIL(<ij>)= SIL(<u,v>), and the same representative, i.~.k<j and u_<k<v. Since they share a common representative, k, the two intervals must overlap." ></td>
	<td class="line x" title="96:254	But since they are distinct, there must be a distinguishing element, d, that is in one but not the other." ></td>
	<td class="line x" title="97:254	One of these distinguishing elements, d, would have to be a bounding lcp in one and an interior lcp in the other." ></td>
	<td class="line x" title="98:254	But then the two intervals couldn't both be LCP-delimited." ></td>
	<td class="line x" title="99:254	Given this uniqueness property, we can determine the N-1 upper bound on the number of LCP-delimited intervals by considering the N-1 elements in the Icp vector." ></td>
	<td class="line x" title="100:254	Each of these elements, lcp\[k\], has the opportunity to become the SIL of an LCP-delimited interval <ij> with a representative k. Thus there could be as many as N-1 LCPdelimited intervals (though there could be fewer if some of the opportunities don't work out)." ></td>
	<td class="line x" title="101:254	Moreover, there couldn't be any more intervals with 0f>l, because if there were one, its SIL should have been in the lcp vector." ></td>
	<td class="line x" title="102:254	(Note that this lcp counting argument excludes intervals with t~-I discussed above, because their SILs need not be in the lcp vector)." ></td>
	<td class="line x" title="103:254	From property 4, it follows that there are at most N distinct values of RIDF." ></td>
	<td class="line x" title="104:254	The N intervals <i,i> have just one RIDF value since 0~-'-df=l for these intervals." ></td>
	<td class="line x" title="105:254	The other N-1 intervals could have another N-1 RIDF values." ></td>
	<td class="line x" title="106:254	In summary, the four properties taken collectively make it practical to compute tf, df and RIDF over a relatively small number of classes; it would have been prohibitively expensive to compute these quantities directly over the N(N+ 1)/2 substrings." ></td>
	<td class="line x" title="107:254	2.3 Calculating classes using Suffix Array This section will describe a single pass procedure for Computing classes." ></td>
	<td class="line x" title="108:254	Since LCP-delimited intervals obey a convenient nesting property, the procedure is based on a push-down stack." ></td>
	<td class="line x" title="109:254	The procedure outputs 4-tuples, <s\[i\],LBL,SIL,~>, one for each LCP-delimited interval." ></td>
	<td class="line x" title="110:254	The stack elements are pairs (x,y), where x is an index, typically the left edge of a candidate LCPdelimited interval, and y is the SIL of this candidate interval." ></td>
	<td class="line x" title="111:254	Typically, y=lcp\[x\], though not always, as we will see in Figure 5." ></td>
	<td class="line x" title="112:254	The algorithm sweeps over the suffixes in suffix array s\[1N\] and their lcp\[1N\] (lcp\[N\]=O) successively." ></td>
	<td class="line x" title="113:254	While Icp's of suffixes are monotonically increasing, indexes and lcp's of the suffixes are pushed into a stack." ></td>
	<td class="line x" title="114:254	When it finds the i-th suffix whose lcp\[i\] is less than the lcp on the top of the stack, the index and Icp on the top are popped off the stack." ></td>
	<td class="line x" title="115:254	Popping is repeated until the lcp on the top becomes less than the lcp\[i\]." ></td>
	<td class="line x" title="116:254	A stack element popped out generates a class." ></td>
	<td class="line x" title="117:254	Suppose that a stack element composed of an index i and lcp\[i\] is popped out by lcp\[1\]." ></td>
	<td class="line x" title="118:254	Lcp\[i\] is used as the SIL." ></td>
	<td class="line x" title="119:254	The LBL is the Icp on the next top element in the stack or lcp\[j\]." ></td>
	<td class="line x" title="120:254	If the next top Icp will be popped out by lcp\[j\], then the algorithm uses the next top lop as the LBL, else it uses the lcp\[j\]." ></td>
	<td class="line x" title="121:254	Tf is the offset between the indexes i and j, that is, j-i+1." ></td>
	<td class="line x" title="122:254	Figure 4 shows the detailed algorithm for Create and clear stack." ></td>
	<td class="line x" title="123:254	Push (-1, -1) (dummy)." ></td>
	<td class="line x" title="124:254	Repeat i = 1  N do top (index1, Icpl)." ></td>
	<td class="line x" title="125:254	if Icp\[i\] > Icpl then push (i, Icp\[i\])." ></td>
	<td class="line x" title="126:254	else while Icp\[i\] _< Icpl do pop(index1, Icpl) top (index2, Icp2) if Icp\[i\] _< Icp2 then output <s\[index 1\], Icp2, Icpl, i-index1 +1 > else output <s\[indexl\], Icp\[i\], Icpl, i-index1+1> push (indext, Icp\[i\]) Icpl = Icp2." ></td>
	<td class="line x" title="127:254	Figure 4: An algorithm for computing all classes 32 computing all classes with tf > 1." ></td>
	<td class="line x" title="128:254	If classes with tf = 1 are needed, we can easily add the line to output those into the algorithm." ></td>
	<td class="line x" title="129:254	The expressions, push(x,y) and pop(x,y), operate on the stack in the obvious way, but note that x and y are inputs for push and outputs for pop." ></td>
	<td class="line x" title="130:254	The expression, top(x,y), is equivalent to pop(x,y) followed by push(x,y); it reads the top of the stack without changing the stack pointer." ></td>
	<td class="line x" title="131:254	As mentioned above, the stack elements are typically pairs (x,y) where y=lcp\[x\], but not always." ></td>
	<td class="line x" title="132:254	Pairs are typically pushed onto the stack by line 6, push(i, Icp\[i\]), and consequently, y=lcp\[x\], in many cases, but some pairs are pushed on by line 15." ></td>
	<td class="line x" title="133:254	Figure 5 (a) shows the typical case with the suffix array in Figure 2." ></td>
	<td class="line x" title="134:254	At this point, i=3 and the stack contains 4 pairs, a dummy element (-1, -1), followed by three pairs generated by line 6: (1, Icp\[l\]), (2, lcp\[2\]), (3, lcp\[3\])." ></td>
	<td class="line x" title="135:254	In contrast, Figure 5 (b) shows an atypical case." ></td>
	<td class="line x" title="136:254	In between snapshot (a) and snapshot (b), two LCPdelimited intervals were generated, <s\[3\], 4, 6, 2> and <s\[2\], 3, 4, 3>, and then the pair (2, 3) was pushed onto the stack by line 15, push(indexl, lcp\[i\]), to capture the fact that there is a candidate LCP-delimited interval starting at indexl=2, spanning past the representative element i=4, with an SIL of lcp\[i=4\]." ></td>
	<td class="line x" title="137:254	index lcp Note!" ></td>
	<td class="line x" title="138:254	(3, 6)\]\]Popped ilout ('2, 3) (2, 4)\[ss\[4\]., 2) (1, 2) (1 (-1,-1) I dummy (-1,-1) I I \] ushod (a) end of processing s\[3\] (b) end of processing s\[4\] Figure 5: Snapshots of the stack 2.4 Computing df for all classes This section will extend the algorithm in Figure 4 to include the calculation of dr. Straightforwardly computing dfindependently for each class would require at least quadratic time, because the program must check document id's for all substfings (N at most) in all classes (N-I at most)." ></td>
	<td class="line x" title="140:254	Instead of this, we will take advantage of the nesting property of intervals." ></td>
	<td class="line x" title="141:254	The df for one interval can be computed recursively in terms of its constituents (nested subintervals), avoiding unnecessary recomputation." ></td>
	<td class="line x" title="142:254	The stack elements in Figure 5 is augmented with two additional counters: (1) a df counter for summing the dfs over the nested subintervals and (2) a duplication counter for adjusting for overcounting documents that are referenced in multiple subintervals." ></td>
	<td class="line x" title="143:254	The df for an interval is simply the difference of these two counters, that is, the sum of the dfs of the subintervals, minus the duplication." ></td>
	<td class="line x" title="144:254	A C code implementation can be found at http://www.milab.is.tsukuba.ac.jp/-myama/oedf/tfdf c. The df counters are relatively straightforward to implement." ></td>
	<td class="line x" title="145:254	The crux of the problem is the adjustment for duplication." ></td>
	<td class="line x" title="146:254	The adjustment makes use of a document link table, as illustrated in Figure 6." ></td>
	<td class="line x" title="147:254	The left two columns indicate that suffixes s\[101\], s\[104\] and s\[107\] are Suffix Document Document id link (index) s\[101\] 382 ~ 66 j s\[102\] 84987 ~172 ~ silO31 -6892 21 s\[104l 382 01 s\[105\] 2566 / 112~) s\[106\] -6892 03 s\[107\] 382 ~,-I04 ',/ stl08\] l84987  \[102 '~-., Figure 6: An example of document link table s\[i\] sbq s\[k ~." ></td>
	<td class="line x" title="149:254	s\[t Suffix Array characters (suffix)  , Idfdf-counter \] h h h,,tdf-counter I Adf-counterl I6 -'---_ ;,11;,I I ;, 4 df-cou.te,T----IL_ II  I,o dup-counter I-~ In document links ~.~ ~Interval endpoints of substrings in the class of the interval Figure 7: Dfrelations among an interval and its constituents 33 all in document 382, and that several other suffixes are also in the same documents." ></td>
	<td class="line x" title="150:254	The third column links together suffixes that are in the same document." ></td>
	<td class="line x" title="151:254	Note, for example, that there is a pointer from suffix 104 to 101, indicating that s\[104\] and s\[101\] are in the same document." ></td>
	<td class="line x" title="152:254	The suffixes in one of these linked lists are kept sorted by their order in the suffix array." ></td>
	<td class="line x" title="153:254	When the algorithm is processing s\[t\], the algorithm searches the stack to find the suffix, s\[k\], with the largest k such k_<i and s\[i\] and s\[k\] are in the same document." ></td>
	<td class="line x" title="154:254	This search can be performed in O(logN) time." ></td>
	<td class="line x" title="155:254	Figure 7 shows the LCP-delimited intervals in a suffix array and four suffixes included in the same document." ></td>
	<td class="line x" title="156:254	I1 has four immediate constituents of intervals." ></td>
	<td class="line x" title="157:254	S\[j\] is included in the same document of s\[i\]." ></td>
	<td class="line x" title="158:254	Count for the document of s\[j\] will be duplicated at computing df of 11." ></td>
	<td class="line x" title="159:254	At the point of processing sO'\], the algorithm will increment duplication-counter of I!" ></td>
	<td class="line x" title="160:254	to cancel dfcount of sO'\]." ></td>
	<td class="line x" title="161:254	As the same way, df count of s\[k\] has to canceled at computing df of 11." ></td>
	<td class="line x" title="162:254	Figure 8 shows a snapshot of the stack after processing s\[4\] in Figure 2." ></td>
	<td class="line x" title="163:254	Each stack element is a 4-tuple of the index of suffix array, lcp, dfcounter and duplication-counter, (i, lcp, df dc)." ></td>
	<td class="line x" title="164:254	Figure 2 shows s\[1\] and s\[4\] are in the same document." ></td>
	<td class="line x" title="165:254	Looking up the document link table, the algorithm knows s\[1\] is the nearest suffix which is in the same document of s\[4\]." ></td>
	<td class="line x" title="166:254	The duplication-counter of the element of s\[1\] is incremented." ></td>
	<td class="line x" title="167:254	The duplication of counting s\[1\] and s\[4\] for the class generated by s\[1\] will be avoided using this duplication-counter." ></td>
	<td class="line x" title="168:254	At some processing point, the algorithm uses only a part of the document link table." ></td>
	<td class="line x" title="169:254	It duplication lcp counter (2, 3, 3,0) \[(1, 2, 1,1) I(-1,-1,-,-) Figure 8: A snapshot of the stack in dfcomputing Nearest Doc-id index  382 4 oH 6892 3 , 84987 2 Figure 9: Nearest indexes of documents needs only the nearest index on the link, but not the whole of the link." ></td>
	<td class="line x" title="170:254	So we can compress the link table to dynamic one in which an entry of each document holds the nearest index." ></td>
	<td class="line x" title="171:254	Figure 9 shows the nearest index+ table of document after processing s\[4\]." ></td>
	<td class="line x" title="172:254	The final algorithm to calculate all classes with tfand dftakes O(NlogN) time and O(N) space in the worst case." ></td>
	<td class="line x" title="173:254	3 Experimental results 3.1 RIDF and MI for English and Japanese We computed all RIDF's for all substrings of two corpora, Wall Street Journal of ACL/DCI in English (about 50M words and 113k articles) and Mainichi News Paper 1991-1995 (CD-Mainichi Shimbun 91-95) in Japanese (about 216M characters and 436k articles), using the algorithm in the previous section." ></td>
	<td class="line x" title="174:254	In English, we tokenized the text into words, delimited by white .space, whereas in Japanese we tokenized the text into characters (usually 2-bytes) because Japanese text has no word delimiter such as white space." ></td>
	<td class="line x" title="175:254	It took a few hours to compute all RIDF's using the suffix array." ></td>
	<td class="line x" title="176:254	It takes much longer to compute the suffix array than to compute tfand df." ></td>
	<td class="line x" title="177:254	We ignored substrings with tf< 10 to avoid noise, resulting in about 1.6M English phrases (#classes = 1.4M) and about 15M substrings of Japanese words/phrases (#classes = 10M)." ></td>
	<td class="line x" title="178:254	MI of the longest substring of each class was also computed by the following fformula., p(xyz) MI(xyz) = xog p(xy)p(z I y) Where xyz is a phrase or string, x and Z are a word or a character and y is a sub-phrase or substring." ></td>
	<td class="line x" title="180:254	3.2 Little correlation between RIDF and MI We are interested in comparing and contrasting RIDF and MI." ></td>
	<td class="line x" title="181:254	Figure 10 (a) plots RIDF vs MI for phrases in WSJ (length > 1), showing little, if any, correlation between RIDF and MI." ></td>
	<td class="line x" title="182:254	Figure 10 (b) also plots RIDF vs MI but this time the corpus is in Japanese and the words were manually selected by the newspaper to be keywords." ></td>
	<td class="line x" title="183:254	Both Figures I0 (a) and 10 (b) suggest that RIDF and MI are 34 largely independent." ></td>
	<td class="line x" title="184:254	There are many substrings with a large RIDF value and a small MI, and vice versa." ></td>
	<td class="line x" title="185:254	MI is very different from RIDF." ></td>
	<td class="line x" title="186:254	Both pick out interesting phrases, but phrases with large MI are interesting in different ways from phrases with large RIDF." ></td>
	<td class="line x" title="187:254	Consider the phrases in Table 1, which all contain the word 'having'." ></td>
	<td class="line x" title="188:254	These phrases have large MI values and small RIDF values." ></td>
	<td class="line x" title="189:254	A lexicographer such as Patrick Hanks, who works on dictionaries for learners, might be interested in these phrases because these kinds of collocations tend to be difficult for non-native speakers of the language." ></td>
	<td class="line x" title="190:254	On the other hand, these kinds of collocations are not very good keywords." ></td>
	<td class="line x" title="191:254	Table 2 is a random sample of phrases containing the substring/Mr/, sorted by RIDF." ></td>
	<td class="line x" title="192:254	The ones at the top of the list tend to be better keywords than the ones further down." ></td>
	<td class="line x" title="193:254	Table 3.A and 3.B show a few phrases starting with/the/, sorted by MI (Table 3.A) and sorted by RIDF (Table 3.B)." ></td>
	<td class="line x" title="194:254	Most of the phrases are interesting in one way or another, but those at the top of Table 3.A tend to be somewhat .+ + !$ o + .  : .'.~: ., .:'~Mi~-.t' ,.~llt~.~z:L= : o MI lo 20 (a) English phrases o'l~ -lo o MI lo (b) Japanese strings Figure 10: Scatter plot of RIDF and MI i,-Table5 20 Table 1: phrases with 'having' ff df RIDF MI Phrase 18 18 -0.0001 10.4564 admits to having 14 14 -0.0001 9.7154 admit to having 25 23 0.1201 8.8551 diagnosed as having 20 20 -0.0001 7.4444 suspected of having 301 293 0.0369 7.2870 without having 15 13 0.2064 6.9419 denies having 59 59 -0.0004 6.7612 avoid having 18 18 -0.0001 5.9760 without ever having 12 12 -0.0001 5.9157 Besides having 26 26 -0.0002 5.7678 denied having Table 2: phrases with 'Mr' tf df RIDF MT Phrase ii 3 1.8744 0.6486 . Mr. Hinz 18 5 1.8479 6.5583 Mr. Bradbury 51 16 1.6721 6.6880 Mr. Roemer 67 25 1.4218 6.7856 Mr. Melamed 54 27 0.9997 5.7704 Mr. Burnett 16 9 0.8300 5.8364 Mrs. Brown Ii 8 0.4594 1.0931 Mr. Eiszner said 53 40 0.4057 0.2855 Mr. Johnson . 21 16 0.3922 0.1997 Mr. Nichols said . 13 i0 0.3784 0.4197 . Mr. Shulman 176 138 0.3498 0.4580 Mr. Bush has 13 ii 0.2409 1.5295 to Mr. Trump's 13 Ii 0.2409 -0.9301 Mr. Bowman, 35 32 0.1291 1.1673 wrote Mr. 12 ii 0.1255 1.7330 M r. Lee to 22 21 0.0670 1.4293 facing Mr. ii ii -0.0001 0.7004 Mr. Poehl also 13 13 -0.0001 1.4061 inadequate . ' Mr. 16 16 -0.0001 1.5771 The 41-year-old Mr. 19 19 -0.0001 0.4738 14 . Mr. 26 26 -0.0002 0.0126 in November . Mr. 27 27 -0.0002 -0.0112 ' For his part, Mr. 38 38 -0.0002 1.3589 . AMR, 39 39 -0.0002 -0.3260 for instance, Mr. tf df Table 3.A: Worse Keywords RIDF MI Phrase ii ii -0.0001 11.0968 the up side 73 66 0.1450 9.3222 the will of 16 16 -0.0001 8.5967 the sell side 17 16 0.0874 8.5250 the Stock Exchange of 16 15 0.0930 8.4617 the buy side 20 20 -0.0001 8.4322 the down side 55 54 0.0261 8.3287 the will to 14 14 -0.0001 8.1208 the saying goes 15 15 -0.0001 7.5643 the going gets tf df Table 3.B: Better Keywords RIDF MI Phrase 37 3 3.6243 2.2561 the joint commission 66 8 3.0440 3.5640 the SSC 55 7 2.9737 2.0317 the Delaware & 37 5 2.8873 3.6492 the NHS 22 3 2.8743 3.3670 the kibbutz 22 3 2.8743 4.1142 the NSA's 29 4 2.8578 4.1502 the DeBartolos 36 5 2.8478 2.3061 the Basic Law 21 3 2.8072 2.2983 the national output Table 3.C: Concordance of the phrase 'the Basic Law' The first col. is the token id and the last col. is the doc id (position of the start word in the corpus) 2229521: line in the drafting of 2229902: s policy as expressed in 9746758: he U.S. Constitution and 11824764: any changes must follow 33007637: sts a tentative draft of 33007720: the relationship between 33007729: onstitution . Originally 33007945: wer of interpretation of 33007975: tation of a provision of 33008031: interpret provisions of 33008045: ration of a provision of 33008115: etation of an article of 33008205: nland representatives of 33008398: e : Mainland drafters of 33008488: pret all the articles of 33008506: y and power to interpret 33008521: pret those provisions of 33008545: r the tentative draft of 33008690: d of being guaranteed by 33008712: uncilor, is a member of 39020313: sts a tentative draft of 39020396: the relationship between 39020405: 39020621: 39020651: 39020707: 39020721: 39020791: 39020881: 39021074: 39021164: 39021182: 39021197: 39021221: 39021366: 39021388: onstitution . Originally wet of interpretation of tation of a provision of interpret provisions of tation of a provision of etation of an article of nland representatives of e : Mainland drafters of pret all the articles of y and power to interpret pret those provisions of r the tentative draft of d of being guaranteed by uncilor, is a member of the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic the Basic Law that will determine how Hon 2228648 Law -as Gov. Wilson's debut s 2228648 Law of the Federal Republic of 9746014 Law, Hong Kong's miniconstitut 11824269 Law, and although this may be 33007425 Law and the Chinese Constitutio 33007425 Law was to deal with this topic 33007425 Law shall be vested in the NPC 33007425 Law, the courts of the HKSAR { 33007425 Law . If a case involves the in 33007425 Law concerning defense, foreig 33007425 Law regarding ' defense, forei 33007425 Law Drafting Committee fear tha 33007425 Law simply do not appreciate th 33007425 Law . While recognizing that th 33007425 Law, it should irrevocably del 33007425 Law within the scope of Hong Ko 33007425 Law, I cannot help but conclud 33007425 Law, are being redefined out o 33007425 Law Drafting Committee . <EOA> 33007425 Law, and although this may be 39020101 Law and the Chinese Constitutio 39020101 Law was to deal with this topic 39020101 Law shall be vested in the NPC 39020101 Law, the courts of the HKSAR { 39020101 Law . If a case involves the in 39020101 Law concerning defense, foreig 39020101 Law regarding ' defense, forei 39020101 Law Drafting Committee fear tha 39020101 Law simply do not appreciate th 39020101 Law . While recognizing that th 39020101 Law, it should irrevocably del 39020101 Law within the scope of Hong Ko 39020101 Law, I cannot help but conclud 39020101 Law, are being redefined out o 39020101 Law Drafting Committee . <EOA> 39020101 35 tf df RIDF MI Table 4: Phrases with prepositions Phrase with 'for' tf df RIDF 14 14 15 15 12 12 i0 5 12 4 13 13 23 21 I0 2 i0 9 19 16 -0.0001 14.5587 -0.0001 14.4294 -0.0001 14.1123 0.9999 13.7514 1.5849 13.7514 -0.0001 13.6803 0.1311 13.6676 2,3219 13.4009 0.1519 13.3591 0.2478 12.9440 tf df RIDF MI feedlots for fattening ii 5 1.1374 error for subgroups II i0 0.1374 Voice for Food 13 12 0.1154 Quest for Value 16 16 -0.0001 Friends for Education 12 12 -0.0001 Commissioner for Refugee~ 12 12 -0.0001 meteorologist for Weathe\] 22 18 0.2894 Just for Men ii ii -0.0001 Witness for Peace 17 12 0.5024 priced for reoffering 22 20 0.1374 Phrase with'by' tf df RIDF Ii ii 13 13 13 13 15 15 16 16 61 59 17 17 12 12 ii ii 20 20 -0,0001 12.8665 -0.0001 12.5731 -0,0001 12.4577 -0,0001 12.4349 -0.0001 11.8276 0,0477 11.5281 -0,0001 11.4577 -0.0001 11.3059 -0.0001 10.8176 -0.0001 10.6641 piece by piece ii i0 0.1374 guilt by association 12 5 1.2630 step by step 16 16 -0.0001 bit by bit 14 13 0.1068 engineer by training 10 9 0.1519 side by side ii II -0.0001 each by Korea's i0 9 0.1519 hermaed in by I0 8 0.3219 dictated by formula 12 12 -0.0001 70%-owned by Exxon 16 4 1.9999 Table 5: Examples of keywords with interesting RIDF and MI RIDF MI Substrings Features ~E(native last name) SUN (company name) High Low z,J-~'(foreign name) 10% 10% ~Z~ b(brush) V 7 7 -(sofa) ~< \]'fl,~ (huge) Low High '~l~J (passive) I,~ 19 (determination) 10% 10% /~jJ(native full name) ~ii~l~'(native full name) Kanji character English character Katakana character Hiragana character Loan word, Katakana General vocabulary General vocab., Kanji General vocabulary Kanji character Kanji character idiomatic (in the WSJ domain) whereas those at the top of Table 3.B tend to pick out specific stories or events in the news." ></td>
	<td class="line x" title="196:254	For example, the phrase, 'the Basic Law,' selects for stories about the British handover of Hong Kong to China, as illustrated in Table 3.C. Table 4 shows a number of phrases with high M!" ></td>
	<td class="line x" title="197:254	containing common prepositions." ></td>
	<td class="line x" title="198:254	The high MI indicates an interesting association, but again most of them are not good keywords, though there are a few exceptions such as 'Just for Men,' a well-known brand name." ></td>
	<td class="line x" title="199:254	RIDF and MI for Japanese substrings tend to be similar." ></td>
	<td class="line x" title="200:254	Substrings with both high RIDF and MI tend to be good keywords such as ~ (merger), (stock certificate), ~,~ (dictionary), J~l~ (wireless) 36 MI Phrase with 'on' 14.3393 Terrorist on Trial 13.1068 War on Poverty 12.6849 Institute on Drug 12.5599 dead on arrival 11.5885 from on high 11.5694 knocking on doors 11.3317 warnings on cigarette 11.2137 Subcon~ittee on Oversight 11.1847 Group on Health 11.1421 free on bail MI Phrase with 'of 16.7880 Joan of Arc 16.2177 Ports of Call 16.0725 Articles of Confederation 16.0604 writ of mandamus 15.8551 Oil of Olay 15.8365 shortness of breath 15.6210 Archbishop of Canterbur 15.3454 Secret of My 15.2030 Lukman of Nigeria 15.1600 Days of Rage and so on." ></td>
	<td class="line x" title="201:254	Substrings with both low RIDF and MI tend to be poor keywords such as '~' ~q~ (current regular-season game) and meaningless fragments such as *&,_.~' (??)." ></td>
	<td class="line x" title="202:254	Table 5 shows examples where MI and RIDF point in opposite directions (rectangles in Figure 10 (b))." ></td>
	<td class="line x" title="203:254	Words with low RIDF and high MI tend to be general vocabulary (often written in Kanji characters)." ></td>
	<td class="line x" title="204:254	In contrast, words with high RIDF and low MI tend  to be domain specific words such as loan words (often written in Katakana characters)." ></td>
	<td class="line x" title="205:254	MI is high for words in general vocabulary (words found in dictionary) and RIDF is high for good keywords for IR." ></td>
	<td class="line x" title="206:254	3.3 Word extraction Sproat and Shih (1990) found MI to be useful for word extraction in Chinese." ></td>
	<td class="line x" title="207:254	We performed the following experiment to see if both MI and RIDF are useful for word extraction in Japanese." ></td>
	<td class="line x" title="208:254	We extracted four random samples of 100 substrings each." ></td>
	<td class="line x" title="209:254	The four samples cover all four combinations of high and low RIDF and high and low MI, where high is defined to be in the top decile and low is defined to be in the bottom decile." ></td>
	<td class="line x" title="210:254	Then we manually scored each sample substring using our own judgment as a good (the substring is a word) or bad the substring is not a word) or gray (the judge is not sure)." ></td>
	<td class="line x" title="211:254	The results are presented in Table 6, which shows that Table 6: RIDF and MI are complementary I MI MI All MI !" ></td>
	<td class="line x" title="212:254	(high 10%) (low 10%) All RIDF --20-44% 2-11% RIDF (high 10%) 29-51% 38-55% 11-35% RIDF (low 10%) 3-18% 4-13% 0-8% Each cell is computed over a sample of 100 examples." ></td>
	<td class="line x" title="213:254	The smaller values are counts of 'good' words and the larger values, 'not bad' words ('good' and 'gray' words)." ></td>
	<td class="line x" title="214:254	Good or 'not bad' word ratio of pairs of characters with high MI is 51-76%." ></td>
	<td class="line x" title="215:254	substrings with high scores in both dimensions are more likely to be words than substrings that score high in just one dimension." ></td>
	<td class="line x" title="216:254	Conversely, substrings with low scores in both dimensions are very unlikely to be words." ></td>
	<td class="line x" title="217:254	3.4 Case study: Names We also compared RIDF and MI for people's names." ></td>
	<td class="line x" title="218:254	We made a list of people's names from corpora using simple heuristics." ></td>
	<td class="line x" title="219:254	A phrase or substring is accepted as a person's name if English phrase starts with the title 'Mr'." ></td>
	<td class="line x" title="220:254	'Ms'." ></td>
	<td class="line x" title="221:254	or 'Dr'." ></td>
	<td class="line x" title="222:254	and is followed by a series of capitalized words." ></td>
	<td class="line x" title="223:254	For Japanese, we selected phrases in the keyword list ending with 'L~:' (-shi), which is roughly the equivalent of the English titles 'Mr'." ></td>
	<td class="line x" title="224:254	and 'Ms'." ></td>
	<td class="line x" title="225:254	Figure 11 plots RIDF and MI for names in English (a) and Japanese (b) with tf _> 10, respectively Figure 11 (a) shows that MI has a more limited range than RIDF, suggesting that RIDF may be more effective with names than MI." ></td>
	<td class="line x" title="226:254	The English name 'Mr. From' is a particularly L %  -, .:." ></td>
	<td class="line x" title="227:254	r' lr r.qp     ''' '?:',:i':''." ></td>
	<td class="line x" title="228:254	1 '' ' ' ' .1 5  : .'~~,: I 1 : ' '' '' ~.-~1~,.'~':." ></td>
	<td class="line x" title="229:254	I 0 4 MI 8 12 -8 -4 MI4 8 12 (a) English names (b) Japanese names Figure 11: MI and RIDF of people's names 37 interesting case, since both 'Mr'." ></td>
	<td class="line x" title="230:254	and 'From' is a stop word." ></td>
	<td class="line x" title="231:254	In this case, the RIDF was large and the MI was not." ></td>
	<td class="line x" title="232:254	The Japanese names in Figure 11 (b) split naturally at RIDF = 0.5." ></td>
	<td class="line x" title="233:254	Japanese names with RIDF below 0.5 are different from names after 0.5." ></td>
	<td class="line x" title="234:254	The group whose RIDF is under 0.5 included first name and full name (first and last name) at rate of 90% and another group whose RIDF is up to 0.5 included only lastname at rate of 90%." ></td>
	<td class="line x" title="235:254	The reason of this separation is that full name (and first name as a substring of full name) appears once in the beginning of the document, but last name is repeated as a reference in the article." ></td>
	<td class="line x" title="236:254	Recall that RIDF tends to give higher value to substrings which appear many times in a few documents." ></td>
	<td class="line x" title="237:254	In summary, RIDF can discriminate difference of some words which cannot be done by MI." ></td>
	<td class="line x" title="238:254	5 Conclusion We showed that RIDF is efficiently and naturally scalable to long phrases or substrings." ></td>
	<td class="line x" title="239:254	RIDF for all substrings in a corpus can be computed using the algorithm which computes tfs and dfs for all substrings based on Suffix Array." ></td>
	<td class="line x" title="240:254	It remains an open question how to do this for MI." ></td>
	<td class="line x" title="241:254	We found that RIDF is useful for finding good keywords, word extraction and so on." ></td>
	<td class="line o" title="242:254	The combination of MI and RIDF is better than either by itself." ></td>
	<td class="line oc" title="243:254	RIDF is like MI, but different References Church, K. and P. Hanks (1990)Word association norms, mutual information, and lexicography Computational Linguistics, 16:1, pp." ></td>
	<td class="line x" title="244:254	22 29." ></td>
	<td class="line x" title="245:254	Church, K. and W. Gale (1995) Poisson mixtures." ></td>
	<td class="line x" title="246:254	Natural Language Engineering, 1:2, pp." ></td>
	<td class="line x" title="247:254	163 190." ></td>
	<td class="line x" title="248:254	Manber, U. and G. Myers (1993) Suffix array: A new method for on-line string searches." ></td>
	<td class="line x" title="249:254	SIAM Journal on Computing, 22:5, pp." ></td>
	<td class="line x" title="250:254	935 948." ></td>
	<td class="line x" title="251:254	http://glimpse.cs.arizona, edu/udi.html Nagao, M. and S. Mori (1994) A new method of n-gram statistics for large number of n and automatic extraction of words and phrases from large text data of Japanese, Coling-94, pp.611-615." ></td>
	<td class="line x" title="252:254	Sproat, R and C. Shih (1990) A statistical method for finding word boundaries in Chinese text." ></td>
	<td class="line x" title="253:254	Computer Processing of Chinese and Oriental Languages, Vol.4, pp." ></td>
	<td class="line x" title="254:254	336 351 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E99-1005
Determinants Of Adjective-Noun Plausibility
Lapata, Mirella;McDonald, Scott;Keller, Frank;"></td>
	<td class="line x" title="1:160	Proceedings of EACL '99 Determinants of Adjective-Noun Plausibility Maria Lapata and Scott McDonald and Frank Keller School of Cognitive Science Division of Informatics, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW, UK {mlap, scottm, keller} @cogsci.ed.ac.uk Abstract This paper explores the determinants of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-based variables: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik's (1993) selectional association measure." ></td>
	<td class="line x" title="2:160	The highest correlation is obtained with the co-occurrence frequency, which points to the strongly lexicalist and collocational nature of adjective-noun combinations." ></td>
	<td class="line x" title="3:160	1 Introduction Research on linguistic plausibility has focused mainly on the effects of argument plausibility during the processing of locally ambiguous sentences." ></td>
	<td class="line x" title="4:160	Psycholinguists have investigated whether the plausibility of the direct object affects reading times for sentences like (1)." ></td>
	<td class="line x" title="5:160	Here, argument plausibility refers to 'pragmatic plausibility' or 'local semantic fit' (Holmes et al. , 1989), and judgements of plausibility are typically obtained by asking subjects to rate sentence fragments containing verb-argument combinations (as an example consider the bracketed parts of the sentences in (1))." ></td>
	<td class="line x" title="6:160	Such experiments typically use an ordinal scale for plausibility (e.g. , from 1 to 7)." ></td>
	<td class="line x" title="7:160	(1) a. \[The senior senator regretted the decision\] had ever been made public." ></td>
	<td class="line x" title="8:160	b. \[The senior senator regretted the reporter\] had ever seen the report." ></td>
	<td class="line x" title="9:160	The majority of research has focussed on investigating the effect of rated plausibility for verb-object combinations in human sentence processing (Garnsey et al. , 1997; Pickering and Traxler, 1998)." ></td>
	<td class="line x" title="10:160	However, plausibility effects have also been observed for adjectivenoun combinations in a head-modifier relationship." ></td>
	<td class="line x" title="11:160	Murphy (1990) has shown that typical adjectivenoun phrases (e.g. , salty olives) are easier to interpret in comparison to atypical ones (e.g. , sweet olives)." ></td>
	<td class="line x" title="12:160	Murphy provides a schema-based explanation for this finding by postulating that in typical adjective-noun phrases, the adjective modifies part of the noun's schema and consequently it is understood more quickly, whereas in atypical combinations, the adjective modifies non-schematic aspects of the noun, which leads to interpretation difficulties." ></td>
	<td class="line x" title="13:160	Smadja (1991) argues that the reason people prefer strong tea to powerful tea and powerful car to strong car is neither purely syntactic nor purely semantic, but rather lexical." ></td>
	<td class="line x" title="14:160	A similar argument is put forward by Cruse (1986), who observes that the adjective spotless collocates well with the noun kitchen, relatively worse with the noun complexion and not all with the noun taste." ></td>
	<td class="line x" title="15:160	According to Cruse, words like spotless have idiosyncratic collocational restrictions: differences in the degree of acceptability of the adjective and its collocates do not seem to depend on the meaning of the individual words." ></td>
	<td class="line x" title="16:160	1.1 Motivation Acquiring plausibility ratings for word combinations (e.g. , adjective-noun, verb-object, noun-noun) can be useful in particular for language generation." ></td>
	<td class="line x" title="17:160	Consider a generator which has to make a choice between spotless kitchen and flawless kitchen." ></td>
	<td class="line x" title="18:160	An empirical model of plausibility could predict that spotless kitchen is a plausible lexical choice, while flawless kitchen is not." ></td>
	<td class="line x" title="19:160	Adjective-noun combinations can be hard to generate given their collocational status." ></td>
	<td class="line x" title="20:160	For a generator which selects words solely on semantic grounds without taking into account lexical constraints, the choice between spotless kitchen and flawless kitchen may look equivalent." ></td>
	<td class="line x" title="21:160	Current work in natural language generation (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) has shown that corpus-based knowledge can be used to address lexical choice noncompositionally." ></td>
	<td class="line x" title="22:160	30 Proceedings of EACL '99 In the work reported here we acquire plausibility ratings for adjective-noun combinations by eliciting judgements from human subjects, and examine the extent to which different corpus-based models correlate with human intuitions about the 'goodness of fit' for a range of adjective-noun combinations." ></td>
	<td class="line x" title="23:160	The research presented in this paper is similar in motivation to Resnik's (1993) work on selectional restrictions." ></td>
	<td class="line x" title="24:160	Resnik evaluated his informationtheoretic model of selectional constraints against human plausibility ratings for verb-object combinations, and showed that, in most cases, his model assigned higher selectional association scores to verb-object combinations which were judged more plausible by human subjects." ></td>
	<td class="line x" title="25:160	We test five corpus-based models against human plausibility judgements: 1." ></td>
	<td class="line x" title="26:160	Familiarity of adjective-noun pair." ></td>
	<td class="line x" title="27:160	We operationalise familiarity as co-occurrence frequency in a large corpus." ></td>
	<td class="line x" title="28:160	We calculate the co-occurrence frequency of adjective-noun pairs in order to examine whether high corpus frequency is correlated with plausibility, and correspondingly low corpus frequency with implausibility." ></td>
	<td class="line x" title="29:160	2." ></td>
	<td class="line x" title="30:160	Familiarity of head noun." ></td>
	<td class="line x" title="31:160	We compare rated plausibility with the corpus frequency of the head noun, the motivation being that highly frequent nouns are more familiar than less frequent ones, and consequently may affect the judged plausibility of the whole noun phrase." ></td>
	<td class="line x" title="32:160	3." ></td>
	<td class="line x" title="33:160	Conditional probability." ></td>
	<td class="line x" title="34:160	Our inclusion of the conditional probability, P(noun I adjective), as a predictor variable also relies on the prediction that plausibility is correlated with corpus frequency." ></td>
	<td class="line x" title="35:160	It differs from simple co-occurrence frequency in that it additionally takes the overall adjective frequency into account." ></td>
	<td class="line x" title="36:160	4." ></td>
	<td class="line x" title="37:160	Coliocational status." ></td>
	<td class="line x" title="38:160	We employ the loglikelihood ratio as a measure of the collocational status of the adjective-noun pair (Dunning, 1993; Daille, 1996)." ></td>
	<td class="line x" title="39:160	If we assume that plausibility differences between strong tea and powerful tea or guilty verdict and guilty cat reflect differences in collocational status (i.e. , appearing together more often than expected by their individual occurrence frequencies), as opposed to being semantic in nature, then the log-likelihood ratio may also predict adjective-noun plausibility." ></td>
	<td class="line x" title="40:160	5." ></td>
	<td class="line x" title="41:160	Selectional association." ></td>
	<td class="line x" title="42:160	Finally, we evaluate plausibility ratings against Resnik's (1993) measure of selectional association." ></td>
	<td class="line x" title="43:160	This measure is attractive because it combines statistical and knowledge-based methods." ></td>
	<td class="line x" title="44:160	By exploiting a knowledge-based taxonomy, it can capture conceptual information about lexical items and hence can make predictions about word combinations which have not been seen in the corpus." ></td>
	<td class="line x" title="45:160	In the following section we describe our method for eliciting plausibility judgements for adjective-noun combinations." ></td>
	<td class="line x" title="46:160	Section 3 reports the results of using the five corpus-based models as predictors of adjectivenoun plausibility." ></td>
	<td class="line x" title="47:160	Finally, section 4 offers some discussion of future work, and section 5 concluding remarks." ></td>
	<td class="line x" title="48:160	2 Collecting Plausibility Ratings In order to evaluate the different corpus-based models of adjective-noun plausibility introduced above, we first needed to establish an independent measure of plausibility." ></td>
	<td class="line x" title="49:160	The standard approach used in experimental psycholinguistics is to elicit judgements from human subjects; in this section we describe our method for assembling the set of experimental materials and collecting plausibility ratings for these stimuli." ></td>
	<td class="line x" title="50:160	2.1 Method Materials and Design." ></td>
	<td class="line x" title="51:160	The ideal test of any of the proposed models of adjective-noun plausibility will be with randomly-chosen materials." ></td>
	<td class="line x" title="52:160	We chose 30 adjectives according to a set of minimal criteria (detailed below), and paired each adjective with a noun selected randomly from three different frequency ranges, which were defined by co-occurrence counts in the 100 million word British National Corpus (BNC; Burnard (1995))." ></td>
	<td class="line x" title="53:160	The experimental design thus consisted of one factor, Frequency Band, with three levels (High, Medium, and Low)." ></td>
	<td class="line x" title="54:160	We chose the adjectives to be minimally ambiguous: each adjective had exactly two senses according to WordNet (Miller et al. , 1990) and was unambiguously tagged as 'adjective' 98.6% of the time, measured as the number of different part-of-speech tags assigned to the word in the BNC." ></td>
	<td class="line x" title="55:160	The 30 adjectives ranged in BNC frequency from 1.9 to 49.1 per million." ></td>
	<td class="line x" title="56:160	We identified adjective-noun pairs by using Gsearch (Corley et al. , 1999), a chart parser which detects syntactic patterns in a tagged corpus by exploiting a userspecified context free grammar and a syntactic query." ></td>
	<td class="line x" title="57:160	Gsearch was run on a lemmatised version of the BNC so as to compile a comprehensive corpus count of all nouns occurring in a modifier-head relationship with each of the 30 adjectives." ></td>
	<td class="line x" title="58:160	Examples of the syntactic patterns the parser identified are given in Table 1." ></td>
	<td class="line x" title="59:160	From the syntactic analysis provided by the parser we extracted a table containing the adjective and the head of the noun phrase following it." ></td>
	<td class="line x" title="60:160	In the case of compound nouns, we only included sequences of two 31 Proceedings of EACL '99 nouns, and considered the rightmost occurring noun as the head." ></td>
	<td class="line x" title="61:160	From the retrieved adjective-noun pairs, we removed all pairs where the noun had a BNC frequency of less than 10 per million, as we wanted to reduce the risk of plausibility ratings being influenced by the presence of a noun unfamiliar to the subjects." ></td>
	<td class="line x" title="62:160	Finally, for each adjective we divided the set of pairs into three 'bands' (High, Medium, and Low), based on an equal division of the range of log-transformed co-occurrence frequency, and randomly chose one noun from each band." ></td>
	<td class="line x" title="63:160	Example stimuli are shown in Table 2." ></td>
	<td class="line x" title="64:160	The mean log co-occurrence frequencies were 3.839, 2.066 and.258, for the High, Medium, and Low groups, respectively." ></td>
	<td class="line x" title="65:160	30 filler items were also included, in order to ensure subjects produced a wide range of plausibility ratings." ></td>
	<td class="line x" title="66:160	These consisted of 30 adjective-noun combinations that were not found in a modifier-head relation in the BNC, and were also judged highly implausible by the authors." ></td>
	<td class="line x" title="67:160	Procedure." ></td>
	<td class="line x" title="68:160	The experimental paradigm was magnitude estimation (ME), a technique standardly used in psychophysics to measure judgements of sensory stimuli (Stevens, 1975), which Bard et al.(1996) and Cowart (1997) have applied to the elicitation of linguistic judgements." ></td>
	<td class="line x" title="70:160	The ME procedure requires subjects to estimate the magnitude of physical stimuli by assigning numerical values proportional to the stimulus magnitude they perceive." ></td>
	<td class="line x" title="71:160	In contrast to the 5or 7-point scale conventionally used to measure human intuitions, ME employs an interval scale, and therefore produces data for which parametric inferential statistics are valid." ></td>
	<td class="line x" title="72:160	ME requires subjects to assign numbers to a series of linguistic stimuli in a proportional fashion." ></td>
	<td class="line x" title="73:160	Subjects are first exposed to a modulus item, which they assign an arbitrary number." ></td>
	<td class="line x" title="74:160	All other stimuli are rated proportional to the modulus." ></td>
	<td class="line x" title="75:160	In this way, each subject can establish their own rating scale, thus yielding maximally fine-graded data and avoiding the known problems with the conventional ordinal scales forlinguistic data (Bard et al. , 1996; Cowart, 1997; Schfitze, 1996)." ></td>
	<td class="line x" title="76:160	In the present experiment, subjects were presented with adjective-noun pairs and were asked to rate the degree of adjective-noun fit proportional to a modulus item." ></td>
	<td class="line x" title="77:160	The experiment was carried out using WebExp, a set of Java-Classes for administering psycholinguistic studies over the Word-Wide Web (Keller et al. , 1998)." ></td>
	<td class="line x" title="78:160	Subjects first saw a set of instructions that explained the ME technique and included some exampies, and had to fill in a short questionnaire including basic demographic information." ></td>
	<td class="line x" title="79:160	Each subject saw all 120 items used in the experiment (3 x 30 experimental items and 30 fillers)." ></td>
	<td class="line x" title="80:160	Subjects." ></td>
	<td class="line x" title="81:160	The experiment was completed by 24 unpaid volunteers, all native speakers of English." ></td>
	<td class="line x" title="82:160	Subjects were recruited via postings to local Usenet newsgroups." ></td>
	<td class="line x" title="83:160	2.2 Results and Discussion As is standard in magnitude estimation studies, statistical tests were done using geometric means to normalise the data (the geometric mean is the mean of the logarithms of the ratings)." ></td>
	<td class="line x" title="84:160	An analysis of variance (ANOVA) indicated that the Frequency Band effect was significant, in both by-subjects and by-items analyses: FI(2, 46) = 79.09, p < .001; F2(2, 58) = 19.99, p < .001." ></td>
	<td class="line x" title="85:160	The geometric mean of the ratings for adjective-noun combinations in the High band was 2.966, compared to Medium items at 2.660 and Low pairs at 2.271.1 Post-hoc Tukey tests indicated that the differences between all pairs of conditions were significant at o~ = .01, except for the difference between the High and Medium bands in the by-items analysis, which was significant at o~ = .05." ></td>
	<td class="line x" title="86:160	These results are perhaps unsurprising: pairs that are more familiar are rated as more plausible than combinations that are less familiar." ></td>
	<td class="line x" title="87:160	In the next section we explore the linear relationship between plausibility and co-occurrence frequency further, using correlation analysis." ></td>
	<td class="line x" title="88:160	3 Corpus-based Modelling 3.1 Method We correlated rated plausibility (Plaus) with the following five corpus-based variables: (1) logtransformed co-occurrence frequency (CoocF), measured as the number of times the adjective-noun pair occurs in the BNC; (2) log-transformed noun frequency (NounF), measured as the number of times the head noun occurs in the BNC; (3) conditional probability (CondP) of the noun given the adjective estimated as shown in equation (2); (4) collocational status, 2 estimated using the log-likelihood statistic (LLRatio); and (5) Resnik's measure of selectional association (SelAssoc), which measures the semantic fit of a particular semantic class c as an argument to a predicate pi." ></td>
	<td class="line x" title="89:160	The selectional association between class c and predicate Pi is given in equations (3) and (4)." ></td>
	<td class="line x" title="90:160	More specifically, selectional association represents the contribution of a particular semantic class c to the total quantity of information provided by a predicate about the semantic class of its argument, when measured as the relative entropy between the prior distriI For comparison, the filler items had a mean rating of .998." ></td>
	<td class="line nc" title="91:160	2Mutual information, though potentially of interest as a measure of collocational status, was not tested due to its well-known property of overemphasising the significance of rare events (Church and Hanks, 1990)." ></td>
	<td class="line x" title="92:160	32 Proceedings of EACL '99 Pattern Example adjective noun educational material adjective specifier noun usual weekly classes adjective noun noun environmental health officers Table 1: Example of noun-adjective patterns Co-occurrence Frequency Band Adjective High l Medium I Low hungry animal 1.79 pleasure 1.38 application 0 guilty verdict 3.91 secret 2.56 cat 0 temporary job 4.71 post 2.07 cap .69 naughty girl 2.94 dog 1.6 lunch .69 Table 2: Example stimuli (with log co-occurrence frequencies in the BNC) bution of classes p(c) and the posterior distribution p(c I pi) of the argument classes for a particular predicate Pi." ></td>
	<td class="line x" title="93:160	f (adjective, noun) (2) P(noun l adjective) = f (adjective) (3) A(pi, c) = I. e(c I Pi)' log P(c I Pi_______~) rli P(c) (4) rli=~-~P(clpi).logP(Cplc;i) C In the case of adjective-noun combinations, the selectional association measures the semantic fit of an adjective and each of the semantic classes of the nouns it co-occurs with." ></td>
	<td class="line x" title="94:160	We estimated the probabilities P(c I Pi) and P(c) similarly to Resnik (1993) by using relative frequencies from the BNC, together with WordNet (Miller et al. , 1990) as a source of taxonomic semantic class information." ></td>
	<td class="line x" title="95:160	Although the selectional association is a function of the predicate and all semantic classes it potentially selects for, following Resnik's method for verb-object evaluation, we compared human plausibility judgements with the maximum value for the selectional association for each adjective-noun combination." ></td>
	<td class="line x" title="96:160	Table 3 shows the models' predictions for three sample stimuli." ></td>
	<td class="line x" title="97:160	The first row contains the geometric mean of the subjects' responses." ></td>
	<td class="line x" title="98:160	3.2 Results The five corpus-based variables were submitted to a correlation analysis (see Tables 5 and 4)." ></td>
	<td class="line x" title="99:160	The highest correlation with judged plausibility was obtained with the familiarity of the adjective-noun combination (as operationalised by corpus co-occurrence frequency)." ></td>
	<td class="line x" title="100:160	Three other variables were also significantly correlated with plausibility ratings: the conditional probability P(noun \[ adjective), the log-likelihood ratio, and Resnik's selectional association measure." ></td>
	<td class="line x" title="101:160	We discuss each predictor variable in more detail: I. Familiarity of adjective-noun pair." ></td>
	<td class="line x" title="102:160	Logtransformed corpus co-occurrence frequency was significantly correlated with plausibility (Pearson r = .570, n = 90, p < .01)." ></td>
	<td class="line x" title="103:160	This verifies the Frequency Band effect discovered by the ANOVA, in an analysis which compares the individual co-occurrence frequency for each item with rated plausibility, instead of collapsing 30 pairs together into an equivalence class." ></td>
	<td class="line x" title="104:160	Familiarity appears to be a strong determinant of adjective-noun plausibility." ></td>
	<td class="line x" title="105:160	2." ></td>
	<td class="line x" title="106:160	Familiarity of head noun." ></td>
	<td class="line x" title="107:160	Log frequency of the head noun was not significantly correlated with plausibility (r = .098), which suggests that adjective-noun plausibility judgements are not influenced by noun familiarity." ></td>
	<td class="line x" title="108:160	3." ></td>
	<td class="line x" title="109:160	Conditional probability." ></td>
	<td class="line x" title="110:160	The probability of the noun given the adjective was significantly correlated with plausibility (r = .220, p < .05)." ></td>
	<td class="line x" title="111:160	This is unsurprising, as conditional probability was also correlated with co-occurrence frequency (r = .497, p < .01)." ></td>
	<td class="line x" title="112:160	4." ></td>
	<td class="line x" title="113:160	Collocational status." ></td>
	<td class="line x" title="114:160	The log-likelihood statistic yielded a significant correlation with plausibility (r = .350, p < .01), a fact that supports the collocational nature of plausible adjectivenoun combinations." ></td>
	<td class="line x" title="115:160	The log-likelihood ratio was in turn correlated with co-occurrence frequency (r = .725, p < .01) and conditional probability (r = .405, p < .01)." ></td>
	<td class="line x" title="116:160	5." ></td>
	<td class="line x" title="117:160	Selectional association." ></td>
	<td class="line x" title="118:160	Resnik's measure of selectional association was also significantly correlated with plausibility (r = -.269, p < .05)." ></td>
	<td class="line x" title="119:160	33 Proceedings of EACL '99 Plaus CoocF NounF CondP LLRatio SelAssoc 1\[ hungry animal hungry application hungry pleasure 3.02 i .79 9.63 .003 26.81 .5 1.46 1.38 9.69 .002 14.33 .5 1.31 0 8.67 .0005 2.9 .22 Table 3: Models' prediction for hungry and its three paired noun heads However, it should be noted that selectional association was negatively correlated with plausibility, although Resnik found the measure was positively correlated with the judged plausibility of verb-object combinations, consistent with its information-theoretic motivation." ></td>
	<td class="line x" title="120:160	Resnik's metric was also negatively correlated with cooccurrence frequency (r = -.226, p < .05), but there was no correlation with noun frequency, conditional probability, or log-likelihood ratio." ></td>
	<td class="line x" title="121:160	Since several of the corpus-based variables were intercorrelated, we also calculated the squared semipartial correlations between plausibility and each corpusbased variable." ></td>
	<td class="line x" title="122:160	This allows the unique relationship between each predictor and plausibility (removing the effects of the other independent variables) to be determined." ></td>
	<td class="line x" title="123:160	Co-occurrence frequency accounted uniquely for 15.52% of the variance in plausibility ratings, while noun frequency, conditional probability, loglikelihood ratio, and selectional association accounted for .51%, .53%, .41% and 1.7% of the variance, respectively." ></td>
	<td class="line x" title="124:160	This confirms co-occurrence frequency as the best predictor of adjective-noun plausibility." ></td>
	<td class="line x" title="125:160	One explanation for the negative correlation between selectional association and plausibility, also pointed out by Resnik, is the difference between verb-object and adjective-noun combinations: combinations of the latter type are more lexical than conceptual in nature and hence cannot be accounted for on purely semantic or syntactic grounds." ></td>
	<td class="line x" title="126:160	The abstraction provided by a semantic taxonomy is at odds with the idiosyncratic (i.e. , lexical) nature of adjective-noun co-occurrences." ></td>
	<td class="line x" title="127:160	Consider for instance the adjective hungry." ></td>
	<td class="line x" title="128:160	The class (entity) yields the highest selectional association value for the highest rated pair hungry animal." ></td>
	<td class="line x" title="129:160	But (entity) also yields the highest association for the lowest rated pair hungry application (A(hungry, (entity}) = .50 in both cases)." ></td>
	<td class="line x" title="130:160	The highest association for hungry pleasure, on the other hand, is given by the class (act) (A(hungry, (act)) = .22)." ></td>
	<td class="line x" title="131:160	This demonstrates how the method tends to prefer the most frequent classes in the taxonomy (e.g. , (entity), (act)) over less frequent, but intuitively more plausible classes (e.g. , (feeling) for pleasure and (use} for application)." ></td>
	<td class="line x" title="132:160	This is a general problem with the estimation of the probability of a class of a given predicate in Resnik's method, as the probability is assumed to be uniform for all classes of a given noun with which the predicate co-occurs." ></td>
	<td class="line x" title="133:160	Although the improvements suggested by Ribas (1994) try to remedy this by taking the different senses of a given word into account and implementing selectional restrictions in the form of weighted disjunctions, the experiments reported here indicate that methods based on taxonomic knowledge have difficulties capturing the idiosyncratic (i.e. , lexicalist) nature of adjective-noun combinations." ></td>
	<td class="line x" title="134:160	Finally, idiosyncrasies in WordNet itself influence the performance of Resnik's model." ></td>
	<td class="line x" title="135:160	One problem is that sense distinctions in WordNet axe often too fine-grained (Palmer (1999) makes a similar observation)." ></td>
	<td class="line x" title="136:160	Furthermore, there is considerable redundancy in the definition of word senses." ></td>
	<td class="line x" title="137:160	Consider the noun application: it has 27 classes in WordNet which include (code), (coding system), (software), (communication}, (writing) and (written communication}." ></td>
	<td class="line x" title="138:160	It is difficult to see how (code} or (coding system} is not (software} or (writing) is not (written communication)." ></td>
	<td class="line x" title="139:160	The fine granularity and the degree of redundancy in the taxonomy bias the estimation of the frequency of a given class." ></td>
	<td class="line x" title="140:160	Resnik's model cannot distinguish classes which are genuinely frequent from classes which are infrequent but yet overly specified." ></td>
	<td class="line x" title="141:160	4 Future Work Although familiarity of the adjective-noun combination proved to be the most predictive measure of judged plausibility, it is obvious that this measure will fail for adjective-noun pairs that never co-occur at all in the training corpus." ></td>
	<td class="line x" title="142:160	Is a zero co-occurrence count merely the result of insufficient evidence, or is it a reflection of a linguistic constraint?" ></td>
	<td class="line x" title="143:160	We plan to conduct another rating experiment, this time with a selection of stimuli that have a co-occurrence frequency of zero in the BNC." ></td>
	<td class="line x" title="144:160	These data will allow a further test of Resnik's selectional association measure." ></td>
	<td class="line x" title="145:160	34 Proceedings of EACL '99 II Plaus t CoocF I NounF I CondP I Min .770 0 6.988 .0002 Max 3.240 5.037 11.929 .2139 Mean 2.632 2.054 9.411 .0165 Std Dev .529 1.583 1.100 .0312 LLRatio SelAssoc .02 .100 1734.88 !.000 176.24 .288 334.23 .170 Table 4: Descriptive statistics for the six experimental variables CoocF NounF CondP LLRatio SelAssoc Plaus .570** .098 .220* .350** -.269* CoocF .221' .497** .725** -.226* NounF I CondP .008 .001 -.191 .405** -.097 LLRatio .015 *p < .05 (2-tailed) **p < .01 (2-tailed) Table 5: Correlation matrix for plausibility and the five corpus-based variables We also plan to investigate the application of similarity-based smoothing (Dagan et ai., 1999) to zero co-occurrence counts, as this method is specifically aimed at distinguishing between unobserved events which are likely to occur in language from those that are not." ></td>
	<td class="line x" title="147:160	Plausibility ratings provide a suitable test of the psychological validity of co-occurrence frequencies 'recreated' with this method." ></td>
	<td class="line x" title="148:160	5 Conclusions This paper explored the determinants of linguistic plausibility, a concept that is potentially relevant for lexical choice in natural language generation systems." ></td>
	<td class="line x" title="149:160	Adjective-noun plausibility served as a test bed for a number of corpus-based models of linguistic plausibility." ></td>
	<td class="line x" title="150:160	Plausibility judgements were obtained from human subjects for 90 randomly selected adjective-noun pairs." ></td>
	<td class="line x" title="151:160	The ratings revealed a clear effect of familiarity of the adjective-noun pair (operationalised by corpus co-occurrence frequency)." ></td>
	<td class="line x" title="152:160	In a correlation analysis we compared judged plausibility with the predictions of five corpus-based variables." ></td>
	<td class="line x" title="153:160	The highest correlation was obtained with the co-occurrence frequency of the adjective-noun pair." ></td>
	<td class="line x" title="154:160	Conditional probability, the log-likelihood ratio, and Resnik's (1993) selectional association measure were also significantly correlated with plausibility ratings." ></td>
	<td class="line x" title="155:160	The correlation with Resnik's measure was negative, contrary to the predictions of his model." ></td>
	<td class="line x" title="156:160	This points to a problem with his technique for estimating word class frequencies, which is aggravated by the collocational nature of noun-adjective combinations." ></td>
	<td class="line x" title="157:160	Overall, the results confirm the strongly lexicalist and collocational nature of adjective-noun combinations." ></td>
	<td class="line x" title="158:160	This fact could be exploited in a generation system by taking into account corpus co-occurrence counts for adjective-noun pairs (which can be obtained straightforwardly) during lexical choice." ></td>
	<td class="line x" title="159:160	Future research has to identify how this approach can be generalised to unseen data." ></td>
	<td class="line x" title="160:160	Acknowledgements The authors acknowledge the support of the Alexander S. Onassis Foundation (Lapata), the UK Economic and Social Research Council (Keller, Lapata), the Natural Sciences and Engineering Research Council of Canada, and the ORS Awards Scheme (McDonald)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E99-1013
Complementing WordNet With Roget's And Corpus-Based Thesauri For Information Retrieval
Mandala, Rila;Tokunaga, Takenobu;Tanaka, Hozumi;"></td>
	<td class="line x" title="1:153	Proceedings of EACL '99 Complementing WordNet with Roget's and Corpus-based Thesauri for Information Retrieval Rila Mandala, Takenobu Tokunaga and Hozumi Tanaka Abstract This paper proposes a method to overcome the drawbacks of WordNet when applied to information retrieval by complementing it with Roget's thesaurus and corpus-derived thesauri." ></td>
	<td class="line x" title="2:153	Words and relations which are not included in WordNet can be found in the corpus-derived thesauri." ></td>
	<td class="line x" title="3:153	Effects of polysemy can be minimized with weighting method considering all query terms and all of the thesauri." ></td>
	<td class="line x" title="4:153	Experimental results show that our method enhances information retrieval performance significantly." ></td>
	<td class="line x" title="5:153	Department of Computer Science Tokyo Institute of Technology 2-12-1 Oookayama Meguro-Ku Tokyo 152-8522 Japan {rila,take,tanaka}@cs.titech.ac.jp expansion (Voorhees, 1994; Smeaton and Berrut, 1995), computing lexical cohesion (Stairmand, 1997), word sense disambiguation (Voorhees, 1993), and so on, but the results have not been very successful." ></td>
	<td class="line x" title="6:153	Previously, we conducted query expansion experiments using WordNet (Mandala et al. , to appear 1999) and found limitations, which can be summarized as follows : 1 Introduction Information retrieval (IR) systems can be viewed basically as a form of comparison between documents and queries." ></td>
	<td class="line x" title="7:153	In traditional IR methods, this comparison is done based on the use of common index terms in the document and the query (Salton and McGill, 1983)." ></td>
	<td class="line x" title="8:153	The drawback of such methods is that if semantically relevant documents do not contain the same terms as the query, then they will be judged irrelevant by the IR system." ></td>
	<td class="line x" title="9:153	This occurs because the vocabulary that the user uses is often not the same as the one used in documents (Blair and Maron, 1985)." ></td>
	<td class="line x" title="10:153	To avoid the above problem, several researchers have suggested the addition of terms which have similar or related meaning to the query, increasing the chances of matching words in relevant documents." ></td>
	<td class="line x" title="11:153	This method is called query expansion." ></td>
	<td class="line x" title="12:153	A thesaurus contains information pertaining to paradigmatic semantic relations such as term synonymy, hypernymy, and hyponymy (Aitchison and Gilchrist, 1987)." ></td>
	<td class="line x" title="13:153	It is thus natural to use a thesaurus as a source for query expansion." ></td>
	<td class="line x" title="14:153	Many researchers have used WordNet (Miller, 1990) in information retrieval as a tool for query  Interrelated words may have different parts of speech." ></td>
	<td class="line x" title="15:153	 Most domain-specific relationships between words are not found in WordNet." ></td>
	<td class="line x" title="16:153	 Some kinds of words are not included in WordNet, such as proper names." ></td>
	<td class="line x" title="17:153	To overcome all the above problems, we propose a method to enrich WordNet with Roget's Thesaurus and corpus-based thesauri." ></td>
	<td class="line x" title="18:153	The idea underlying this method is that the automatically constructed thesauri can counter all the above drawbacks of WordNet." ></td>
	<td class="line x" title="19:153	For example, as we stated earlier, proper names and their interrelations are not found in WordNet, but if proper names bear some strong relationship with other terms, they often cooccur in documents, as can be modelled by a corpus-based thesaurus." ></td>
	<td class="line x" title="20:153	Polysemous words degrade the precision of information retrieval since all senses of the original query term are considered for expansion." ></td>
	<td class="line x" title="21:153	To overcome the problem of polysemous words, we apply a restriction in that queries are expanded by adding those terms that are most similar to the entirety of the query, rather than selecting terms that are similar to a single term in the query." ></td>
	<td class="line x" title="22:153	In the next section we describe the details of our method." ></td>
	<td class="line x" title="23:153	94 Proceedings of EACL '99 2 Thesauri 2.1 WordNet In WordNet, words are organized into taxonomies where each node is a set of synonyms (a synset) representing a single sense." ></td>
	<td class="line x" title="24:153	There are 4 different taxonomies based on distinct parts of speech and many relationships defined within each." ></td>
	<td class="line x" title="25:153	In this paper we use only noun taxonomy with hyponymy/hypernymy (or is-a) relations, which relates more general and more specific senses (Miller, 1988)." ></td>
	<td class="line x" title="26:153	Figure 1 shows a fragment of the WordNet taxonomy." ></td>
	<td class="line x" title="27:153	The similarity between word wl and we is defined as the shortest path from each sense of wl to each sense of w2, as below (Leacock and Chodorow, 1988; Resnik, 1995) sim(wl, w2) = max\[log(2~) \] where N v is the number of nodes in path p from wl to w2 and D is the maximum depth of the taxonomy." ></td>
	<td class="line x" title="28:153	2.2 Roget's Thesaurus In Roget's Thesaurus (Chapman, 1977), words are classified according to the ideas they express, and these categories of ideas are numbered in sequence." ></td>
	<td class="line x" title="29:153	The terms within a category are further organized by part of speech (nouns, verbs, adjectives, adverbs, prepositions, conjunctions, and interjections)." ></td>
	<td class="line x" title="30:153	Figure 2 shows a fragment of Roget's category." ></td>
	<td class="line x" title="31:153	In this case, our similarity measure treat all the words in Roger as features." ></td>
	<td class="line x" title="32:153	A word w possesses the feature f if f and w belong to the same Roget category." ></td>
	<td class="line x" title="33:153	The similarity between two words is then defined as the Dice coefficient of the two feature vectors (Lin, 1998)." ></td>
	<td class="line x" title="34:153	sim(wl,w2) = 21R(wl) n R(w~)l tn(w,)l + In(w )l where R(w) is the set of words that belong to the same Roget category as w. 2.3 Corpus-based Thesaurus 2.3.1 Co-occurrence-based Thesaurus This method is based on the assumption that a pair of words that frequently occur together in the same document are related to the same subject." ></td>
	<td class="line x" title="35:153	Therefore word co-occurrence information can be used to identify semantic relationships between words (Schutze and Pederson, 1997; Schutze and Pederson, 1994)." ></td>
	<td class="line o" title="36:153	We use mutual information as a tool for computing similarity between words." ></td>
	<td class="line oc" title="37:153	Mutual information compares the probability of the co-occurence of words a and b with the independent probabilities of occurrence of a and b (Church and Hanks, 1990)." ></td>
	<td class="line x" title="38:153	P(a, b) I(a, b) = log P(a)P(b) where the probabilities of P(a) and P(b) are estimated by counting the number of occurrences of a and b in documents and normalizing over the size of vocabulary in the documents." ></td>
	<td class="line x" title="39:153	The joint probability is estimated by counting the number of times that word a co-occurs with b and is also normalized over the size of the vocabulary." ></td>
	<td class="line x" title="40:153	2.3.2 Syntactically-based Thesaurus In contrast to the previous section, this method attempts to gather term relations on the basis of linguistic relations and not document cooccurrence statistics." ></td>
	<td class="line x" title="41:153	Words appearing in similax grammatical contexts are assumed to be similar, and therefore classified into the same class (Lin, 1998; Grefenstette, 1994; Grefenstette, 1992; Ruge, 1992; Hindle, 1990)." ></td>
	<td class="line x" title="42:153	First, all the documents are parsed using the Apple Pie Parser." ></td>
	<td class="line x" title="43:153	The Apple Pie Parser is a natural language syntactic analyzer developed by Satoshi Sekine at New York University (Sekine and Grishman, 1995)." ></td>
	<td class="line x" title="44:153	The parser is a bottom-up probabilistic chart parser which finds the parse tree with the best score by way of the best-first search algorithm." ></td>
	<td class="line x" title="45:153	Its grammar is a semi-context sensitive grammar with two non-terminals and was automatically extracted from Penn Tree Bank syntactically tagged corpus developed at the University of Pennsylvania." ></td>
	<td class="line x" title="46:153	The parser generates a syntactic tree in the manner of a Penn Tree Bank bracketing." ></td>
	<td class="line x" title="47:153	Figure 3 shows a parse tree produced by this parser." ></td>
	<td class="line x" title="48:153	The main technique used by the parser is the best-first search." ></td>
	<td class="line x" title="49:153	Because the grammar is probabilistic, it is enough to find only one parse tree with highest possibility." ></td>
	<td class="line x" title="50:153	During the parsing process, the parser keeps the unexpanded active nodes in a heap, and always expands the active node with the best probability." ></td>
	<td class="line x" title="51:153	Unknown words are treated in a special manner." ></td>
	<td class="line x" title="52:153	If the tagging phase of the parser finds an unknown word, it uses a list of parts-of-speech defined in the parameter file." ></td>
	<td class="line x" title="53:153	This information has been collected from the Wall Street Journal corpus and uses part of the corpus for training and the rest for testing." ></td>
	<td class="line x" title="54:153	Also, it has separate lists for such information as special suffices like -ly, -y, -ed, -d, and -s." ></td>
	<td class="line x" title="55:153	The accuracy of this parser is reported 95 Proceedings of EACL '99 Synonyms/Hypernyms (Ordered by Frequency) of noun correlation 2 senses of correlation Sense 1 correlation, correlativity => reciprocality, reciprocity => relation => abstraction Figure 1: An Example WordNet entry 9." ></td>
	<td class="line x" title="56:153	Relation." ></td>
	<td class="line x" title="57:153	-N. relation, bearing, reference, connection, concern,, cogaation ; correlation c. 12; analogy; similarity c. 17; affinity, homology, alliance, homogeneity, association; approximation c." ></td>
	<td class="line x" title="58:153	(nearness) 197; filiation c." ></td>
	<td class="line x" title="59:153	(consanguinity) 11\[obs3\]; interest; relevancy c. 23; dependency, relationship, relative position." ></td>
	<td class="line x" title="60:153	comparison c. 464; ratio, proportion." ></td>
	<td class="line x" title="61:153	link, tie, bond of union." ></td>
	<td class="line x" title="62:153	Figure 2: A fragment of a Roget's Thesaurus entry as parseval recall 77.45 % and parseval precision 75.58 %." ></td>
	<td class="line x" title="63:153	Using the above parser, the following syntactic structures are extracted :  Subject-Verb a noun is the subject of a verb." ></td>
	<td class="line x" title="64:153	 Verb-Object a noun is the object of a verb." ></td>
	<td class="line x" title="65:153	 Adjective-Noun an adjective modifies a noun." ></td>
	<td class="line x" title="66:153	 Noun-Noun a noun modifies a noun." ></td>
	<td class="line x" title="67:153	Each noun has a set of verbs, adjectives, and nouns that it co-occurs with, and for each such relationship, a mutual information value is calculated." ></td>
	<td class="line x" title="68:153	 I~b(Vi, nj) = log f,~b(~,~,)/g,~b  (fsub(nj)/Ns,~b)(f(Vi)/Nzub) where fsub(vi, nj) is the frequency of noun nj occurring as the subject of verb vi, L~,b(n~) is the frequency of the noun nj occurring as subject of any verb, f(vi) is the frequency of the verb vi, and Nsub is the number of subject clauses." ></td>
	<td class="line x" title="69:153	fob~ (nj,11i )/Nobj  Iobj(Vi, nj) = log (Yob~(nj)/Nob~)(f(vl)/Nob~) where fobj(Vi, nj) is the frequency of noun nj occurring as the object of verb vi, fobj(nj) is the frequency of the noun nj occurring as object of any verb, f(vi) is the frequency of the verb vi, and Nsub is the number of object clauses." ></td>
	<td class="line x" title="70:153	 Iadj(ai,nj) = log Id;(n~'ai)/N*ai (fadj(nj)/Nadj)(f(ai)/ga#4) where f(ai, nj) is the frequency of noun nj occurring as the argument of adjective ai, fadj(nj) is the frequency of the noun nj occurring as the argument of any adjective, f(ai) is the frequency of the adjective ai, and Nadj is the number of adjective clauses." ></td>
	<td class="line x" title="71:153	 Inoun(ni,nj) = log f (~j,~)/N  where (f oun (nj )/ Nnou." ></td>
	<td class="line x" title="72:153	)(f (ni )/ Nnoun ) f(ai,nj) is the frequency of noun nj occurring as the argument of noun hi, fnoun(nj) is the frequency of the noun n~ occurring as the argument of any noun, f(ni) is the frequency of the noun hi, and N.o~,n is the number of noun clauses." ></td>
	<td class="line x" title="73:153	The similarity sim(w,wz) between two words w~ and w2 can be computed as follows : (r,w) 6T(w, )nT(w2) Ir(wl,w)+ (r,w) 6T(wt ) (r,w) eT(w2) Where r is the syntactic relation type, and w is  a verb, if r is the subject-verb or object-verb relation." ></td>
	<td class="line x" title="74:153	 an adjective, if r is the adjective-noun relation." ></td>
	<td class="line x" title="75:153	96 Proceedings of EACL '99 NP DT JJ NN That quill pen VP /N ADJ VBZ JJ CC looks good and VP VP NP VBZ DT JJ NN is a new product Figure 3: An example parse tree  a noun, if r is the noun-noun relation." ></td>
	<td class="line x" title="76:153	and T(w) is the set of pairs (r,w') such that It(w, w') is positive." ></td>
	<td class="line x" title="77:153	3 Combination and Term Expansion Method A query q is represented by the vector -~ = (ql, q2,---, qn), where each qi is the weight of each search term ti contained in query q. We used SMART version 11.0 (Saiton, 1971) to obtain the initial query weight using the formula ltc as belows : (log(tfik) + 1.0) * log(N/nk) ~-~\[(log(tfo + 1.0) * log(N/nj)\] 2 j=l where tfik is the occurrrence frequency of term tk in query qi, N is the total number of documents in the collection, and nk is the number of documents to which term tk is assigned." ></td>
	<td class="line x" title="78:153	Using the above weighting method, the weight of initial query terms lies between 0 and 1." ></td>
	<td class="line x" title="79:153	On the other hand, the similarity in each type of thesaurus does not have a fixed range." ></td>
	<td class="line x" title="80:153	Hence, we apply the following normalization strategy to each type of thesaurus to bring the similarity value into the range \[0, 1\]." ></td>
	<td class="line x" title="81:153	simold -Simmin Simnew = Simmaz -8immin The similarity value between two terms in the combined thesauri is defined as the average of their similarity value over all types of thesaurus." ></td>
	<td class="line x" title="82:153	The similarity between a query q and a term tj can be defined as belows : simqt(q, tj) = Z qi * sim(ti, tj) tiEq where the value of sim(ti, tj) is taken from the combined thesauri as described above." ></td>
	<td class="line x" title="83:153	With respect to the query q, all the terms in the collection can now be ranked according to their simqt." ></td>
	<td class="line x" title="84:153	Expansion terms are terms tj with high simqt (q, t j)." ></td>
	<td class="line x" title="85:153	The weight(q, tj) of an expansion term tj is defined as a function of simqt(q, tj): weight(q, tj) simqt(q, tj) ZtiEq qi where 0 < weight(q, tj) < 1." ></td>
	<td class="line x" title="86:153	The weight of an expansion term depends both on all terms appearing in a query and on the similarity between the terms, and ranges from 0 to 1." ></td>
	<td class="line x" title="87:153	The weight of an expansion term depends both on the entire query and on the similarity between the terms." ></td>
	<td class="line x" title="88:153	The weight of an expansion term can be interpreted mathematically as the weighted mean of the similarities between the term tj and all the query terms." ></td>
	<td class="line x" title="89:153	The weight of the original query terms are the weighting factors of those similarities (Qiu and Frei, 1993)." ></td>
	<td class="line x" title="90:153	Therefore the query q is expanded by adding the following query : ~ee = (al, a2,  , at) where aj is equal to weight(q, tj) if tj belongs to the top r ranked terms." ></td>
	<td class="line x" title="91:153	Otherwise aj is equal to 0." ></td>
	<td class="line x" title="92:153	97 Proceedings of EACL '99 The resulting expanded query is : ~ezpanded '~~ o ~ee where the o is defined as the concatenation operator." ></td>
	<td class="line x" title="93:153	The method above can accommodate polysemy, because an expansion term which is taken from a different sense to the original query term is given a very low weight." ></td>
	<td class="line x" title="94:153	4 Experiments Experiments were carried out on the TREC-7 Collection, which consists of 528,155 documents and 50 topics (Voorhees and Harman, to appear 1999)." ></td>
	<td class="line x" title="95:153	TREC is currently de facto standard test collection in information retrieval community." ></td>
	<td class="line x" title="96:153	Table 1 shows topic-length statistics, Table 2 shows document statistics, and Figure 4 shows an example topic." ></td>
	<td class="line x" title="97:153	We use the title, description, and combined title+description+narrative of these topics." ></td>
	<td class="line x" title="98:153	Note that in the TREC-7 collection the description contains all terms in the title section." ></td>
	<td class="line x" title="99:153	For our baseline, we used SMART version 11.0 (Salton, 1971) as information retrieval engine with the Inc.ltc weighting method." ></td>
	<td class="line x" title="100:153	SMART is an information retrieval engine based on the vector space model in which term weights are calculated based on term frequency, inverse document frequency and document length normalization." ></td>
	<td class="line x" title="101:153	Automatic indexing of a text in SMART system involves the following steps :  Tokenization : The text is first tokenized into individual words and other tokens." ></td>
	<td class="line x" title="102:153	 Stop word removal : Common function words (like the, of, an, etc)." ></td>
	<td class="line x" title="103:153	also called stop words, are removed from this list of tokens." ></td>
	<td class="line x" title="104:153	The SMART system uses a predefined list of 571 stop words." ></td>
	<td class="line x" title="105:153	 Stemming: Various morphological variants of a word are normalized to the same stem." ></td>
	<td class="line x" title="106:153	SMART system uses the variant of Lovin method to apply simple rules for suffix stripping." ></td>
	<td class="line x" title="107:153	 Weighting : The term (word and phrase) vector thus created for a text, is weighted using t f, idf, and length normalization considerations." ></td>
	<td class="line x" title="108:153	Table 3 gives the average of non-interpolated precision using SMART without expansion (baseline), expansion using only WordNet, expansion using only the corpus-based syntactic-relationbased thesaurus, expansion using only the corpusbased co-occurrence-based thesaurus, and expansion using combined thesauri." ></td>
	<td class="line x" title="109:153	For each method we also give the relative improvement over the baseline." ></td>
	<td class="line x" title="110:153	We can see that the combined method outperform the isolated use of each type of thesaurus significantly." ></td>
	<td class="line x" title="111:153	Table 1:TREC-7 Topic length statistics Topic Section Min Max Mean Title 1 3 2.5 Description 5 34 14.3 Narrative 14 92 40.8 All 31 114 57.6 5 Discussion In this section we discuss why our method using WordNet is able to improve information retrieval performance." ></td>
	<td class="line x" title="112:153	The three types of thesaurus we used have different characteristics." ></td>
	<td class="line x" title="113:153	Automatically constructed thesauri add not only new terms but also new relationships not found in WordNet." ></td>
	<td class="line x" title="114:153	If two terms often co-occur in a document then those two terms are likely to bear some relationship." ></td>
	<td class="line x" title="115:153	The reason why we should use not only automatically constructed thesauri is that some relationships may be missing in them For example, consider the words colour and color." ></td>
	<td class="line x" title="116:153	These words certainly share the same context, but would never appear in the same document, at least not with a frequency recognized by a co-occurrence-based method." ></td>
	<td class="line x" title="117:153	In general, different words used to describe similar concepts may never be used in the same document, and are thus missed by cooccurrence methods." ></td>
	<td class="line x" title="118:153	However their relationship may be found in WordNet, Roget's, and the syntacticallybased thesaurus." ></td>
	<td class="line x" title="119:153	One may ask why we included Roget's Thesaurus here which is almost identical in nature to WordNet." ></td>
	<td class="line x" title="120:153	The reason is to provide more evidence in the final weighting method." ></td>
	<td class="line x" title="121:153	Including Roget's as part of the combined thesaurus is better than not including it, although the improvement is not significant (4% for title, 2% for description and 0.9% for all terms in the query)." ></td>
	<td class="line x" title="122:153	One reason is that the coverage of Roget's is very limited." ></td>
	<td class="line x" title="123:153	A second point is our weighting method." ></td>
	<td class="line x" title="124:153	The advantages of our weighting method can be summarized as follows:  the weight of each expansion term considers the similarity of that term to all terms in the 98 Proceedings of EACL '99 Table 2:TREC-7 Document statistics Source Size(Mb) #Docs I Median# t Mean# Words/Doc Words/Doc Disk 4 FT 564 t210,1581 316 412.7 1155,630 588 644.7 FR94 395 Disk 5 FBIS 4701130,47113221543.6 131,896 351 526.5 LA Times 475 Title : ocean remote sensing Description: Identify documents discussing the development and application of spaceborne ocean remote sensing." ></td>
	<td class="line x" title="125:153	Narrative: Documents discussing the development and application of spaceborne ocean remote sensing in oceanography, seabed prospecting and mining, or any marinescience activity are relevant." ></td>
	<td class="line x" title="126:153	Documents that discuss the application of satellite remote sensing in geography, agriculture, forestry, mining and mineral prospecting or any land-bound science are not relevant, nor are references to international marketing or promotional advertizing of any remote-sensing technology." ></td>
	<td class="line x" title="127:153	Synthetic aperture radar (SAR) employed in ocean remote sensing is relevant." ></td>
	<td class="line x" title="128:153	Figure 4: Topics Example original query, rather than to just one query term." ></td>
	<td class="line x" title="129:153	 the weight of an expansion term also depends on its similarity within all types of thesaurus." ></td>
	<td class="line x" title="130:153	Our method can accommodate polysemy, because an expansion term taken from a different sense to the original query term sense is given very low weight." ></td>
	<td class="line x" title="131:153	The reason for this is that the weighting method depends on all query terms and all of the thesauri." ></td>
	<td class="line x" title="132:153	For example, the word bank has many senses in WordNet." ></td>
	<td class="line x" title="133:153	Two such senses are the financial institution and river edge senses." ></td>
	<td class="line x" title="134:153	In a document collection relating to financial banks, the river sense of bank will generally not be found in the cooccurrence-based thesaurus because of a lack of articles talking about rivers." ></td>
	<td class="line x" title="135:153	Even though (with small possibility) there may be some documents in the collection talking about rivers, if the query contained the finance sense of bank then the other terms in the query would also tend to be concerned with finance and not rivers." ></td>
	<td class="line x" title="136:153	Thus rivers would only have a relationship with the bank term and there would be no relations with other terms in the original query, resulting in a low weight." ></td>
	<td class="line x" title="137:153	Since our weighting method depends on both the query in its entirety and similarity over the three thesauri, wrong sense expansion terms are given very low weight." ></td>
	<td class="line x" title="138:153	6 Related Research Smeaton (1995) and Voorhees (1994; 1988) proposed an expansion method using WordNet." ></td>
	<td class="line x" title="139:153	Our method differs from theirs in that we enrich the coverage of WordNet using two methods of automatic thesaurus construction, and we weight the expansion term appropriately so that it can accommodate polysemy." ></td>
	<td class="line x" title="140:153	Although Stairmand (1997) and Richardson (1995) proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework." ></td>
	<td class="line x" title="141:153	Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval." ></td>
	<td class="line x" title="142:153	Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of Grefenstette (1994), who applied his 99 Proceedings of EACL '99 Table 3: Average non-interpolated precision for expansion using single or combined thesauri." ></td>
	<td class="line x" title="143:153	Topic Type Base Title 0.1175 Description 0.1428 All 0.1976 Expanded with WordNet Roget Syntac Cooccur Combined only only only only method 0.1276 0.1236 0.1386 0.1457 0.2314 (+8.6%) (+5.2 %) (+17.9%) (+24.0%) (+96.9%) 0.1509 0,1477 0.1648 0.1693 0.2645 (+5.7%) (+3.4%) (+15.4%) (+18.5%) (+85.2%) 0.2010 0.1999 0.2131 0.2191 0.2724 (+1.7%) (+1.2%) (+7.8%) (+10.8%) (+37.8%) syntactically-based thesaurus to information retrieval with mixed results." ></td>
	<td class="line x" title="144:153	Our system improves on Grefenstette's results since we factor in thesauri which contain hierarchical information absent from his automatically derived thesaurus." ></td>
	<td class="line x" title="145:153	Our weighting method follows the Qiu and Frei (1993) method, except that Qiu used it to expand terms from a single automatically constructed thesarus and did not consider the use of more than one thesaurus." ></td>
	<td class="line x" title="146:153	This paper is an extension of our previous work (Mandala et al. , to appear 1999) in which we ddid not consider the effects of using Roget's Thesaurus as one piece of evidence for expansion and used the Tanimoto coefficient as similarity coefficient instead of mutual information." ></td>
	<td class="line x" title="147:153	7 Conclusions We have proposed the use of different types of thesaurus for query expansion." ></td>
	<td class="line x" title="148:153	The basic idea underlying this method is that each type of thesaurus has different characteristics and combining them provides a valuable resource to expand the query." ></td>
	<td class="line x" title="149:153	Wrong expansion terms can be avoided by designing a weighting term method in which the weight of expansion terms not only depends on all query terms, but also depends on their similarity values in all type of thesaurus." ></td>
	<td class="line x" title="150:153	Future research will include the use of a parser with better performance and the use of more recent term weighting methods for indexing." ></td>
	<td class="line x" title="151:153	8 Acknowledgements The authors would like to thank Mr. Timothy Baldwin (TIT, Japan) and three anonymous referees for useful comments on the earlier version of this paper." ></td>
	<td class="line x" title="152:153	We also thank Dr. Chris Buckley (SabIR Research) for support with SMART, and Dr. Satoshi Sekine (New York University) for providing the Apple Pie Parser program." ></td>
	<td class="line x" title="153:153	This research is partially supported by JSPS project number JSPS-RFTF96P00502." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P99-1004
Measures Of Distributional Similarity
Lee, Lillian;"></td>
	<td class="line x" title="1:249	Measures of Distributional Similarity Lillian Lee Department of Computer Science Cornell University Ithaca, NY 14853-7501 llee@cs, cornell, edu Abstract We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences." ></td>
	<td class="line x" title="2:249	Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions." ></td>
	<td class="line x" title="3:249	1 Introduction An inherent problem for statistical methods in natural language processing is that of sparse data -the inaccurate representation in any training corpus of the probability of low frequency events." ></td>
	<td class="line x" title="4:249	In particular, reasonable events that happen to not occur in the training set may mistakenly be assigned a probability of zero." ></td>
	<td class="line x" title="5:249	These unseen events generally make up a substantial portion of novel data; for example, Essen and Steinbiss (1992) report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition." ></td>
	<td class="line x" title="6:249	We consider here the question of how to estimate the conditional cooccurrence probability P(v\[n) of an unseen word pair (n, v) drawn from some finite set N x V. Two state-of-the-art technologies are Katz's (1987) backoff method and Jelinek and Mercer's (1980) interpolation method." ></td>
	<td class="line x" title="7:249	Both use P(v) to estimate P(v\[n) when (n, v) is unseen, essentially ignoring the identity of n. An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for 25 cooccurrences involving similar words: 1 /P(v\[n) ---~-~mES(n) sim(n, m)P(v\[m) ~-\]mES(n) sim(n, m), (1) where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m. We focus on distributional rather than semantic similarity (e.g. , Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions -although the words 'chance' and 'probability' are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in." ></td>
	<td class="line x" title="8:249	There are many plausible measures of distributional similarity." ></td>
	<td class="line x" title="9:249	In previous work (Dagan et al. , 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability." ></td>
	<td class="line x" title="10:249	Our experiments on a frequency-controlled pseudoword disambiguation task showed that using any of the three in a distance-weighted averaging scheme yielded large improvements over Katz's backoff smoothing method in predicting unseen coocurrences." ></td>
	<td class="line x" title="11:249	Furthermore, by using a restricted version of model (1) that stripped incomparable parameters, we were able to empirically demonstrate that the confusion probability is fundamentally worse at selecting useful similar words." ></td>
	<td class="line x" title="12:249	D. Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b)." ></td>
	<td class="line x" title="13:249	1The term 'similarity-based', which we have used previously, has been applied to describe other models as well (L. Lee, 1997; Karov and Edelman, 1998)." ></td>
	<td class="line x" title="14:249	These empirical results indicate that investigating different similarity measures can lead to improved natural language processing." ></td>
	<td class="line x" title="15:249	On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised." ></td>
	<td class="line x" title="16:249	(van Rijsbergen, 1979, pg." ></td>
	<td class="line x" title="17:249	38) But no contradiction arises because, as van Rijsbergen continues, 'one would expect this since most measures incorporate the same information'." ></td>
	<td class="line x" title="18:249	In the language-modeling domain, there is currently no agreed-upon best similarity metric because there is no agreement on what the 'same information'the key data that a similarity function should incorporate -is. The overall goal of the work described here was to discover these key characteristics." ></td>
	<td class="line x" title="19:249	To this end, we first compared a number of common similarity measures, evaluating them in a parameter-free way on a decision task." ></td>
	<td class="line x" title="20:249	When grouped by average performance, they fell into several coherent classes, which corresponded to the extent to which the functions focused on the intersection of the supports (regions of positive probability) of the distributions." ></td>
	<td class="line x" title="21:249	Using this insight, we developed an information-theoretic metric, the skew divergence, which incorporates the support-intersection data in an asymmetric fashion." ></td>
	<td class="line x" title="22:249	This function yielded the best performance overall: an average error rate reduction of 4% (significant at the.01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al. , 1999)." ></td>
	<td class="line x" title="23:249	Our contributions are thus three-fold: an empirical comparison of a broad range of similarity metrics using an evaluation methodology that factors out inessential degrees of freedom; a proposal, building on this comparison, of a characteristic for classifying similarity functions; and the introduction of a new similarity metric incorporating this characteristic that is superior at evaluating potential proxy distributions." ></td>
	<td class="line x" title="24:249	2} 2 Distributional Similarity Functions In this section, we describe the seven distributional similarity functions we initally evaluated." ></td>
	<td class="line x" title="25:249	2 For concreteness, we choose N and V to be the set of nouns and the set of transitive verbs, respectively; a cooccurrence pair (n, v) results when n appears as the head noun of the direct object of v. We use P to denote probabilities assigned by a base language model (in our experiments, we simply used unsmoothed relative frequencies derived from training corpus counts)." ></td>
	<td class="line x" title="26:249	Let n and m be two nouns whose distributional similarity is to be determined; for notational simplicity, we write q(v) for P(vln ) and r(v) for P(vlm), their respective conditional verb cooccurrence probabilities." ></td>
	<td class="line x" title="27:249	Figure 1 lists several familiar functions." ></td>
	<td class="line x" title="28:249	The cosine metric and Jaccard's coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983)." ></td>
	<td class="line x" title="29:249	Note that Jaccard's coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q  r rather than the actual values of the distributions." ></td>
	<td class="line x" title="30:249	Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: JS(q,r)=-~l \[D(q aVgq,r)+D(r aVgq,r) \] The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): (v) D(pl(V) IIp2(V)) = EPl(V)log Pl p2(v) ' V The function avga, r denotes the average distribution avgq,r(V ) --= (q(v)+r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined." ></td>
	<td class="line x" title="31:249	In contrast, D(qllr ) is undefined if q is not absolutely continuous with respect to r (i.e. , the support of q is not a subset of the support of r)." ></td>
	<td class="line x" title="32:249	2Strictly speaking, some of these functions are dissimilarity measures, but each such function f can be recast as a similarity function via the simple transformation C f, where C is an appropriate constant." ></td>
	<td class="line x" title="33:249	Whether we mean f or C f should be clear from context." ></td>
	<td class="line x" title="34:249	Euclidean distance L1 norm cosine Jaccard's coefficient L2(q,r) = Ll(q,r) = cos(q, r) = Jac(q, r) = ~v (q(v) r(v)) 2 Iq(v) r(v)l V ~-~v q(v)r(v) X/~-~v q(v) 2 V/Y~-v r(v) 2 Iv : q(v) > 0 and r(v) > 0}l Iv I q(v) > 0 or r(v) > O}l Figure 1: Well-known functions The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al. , 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993); it measures the degree to which word m can be substituted into the contexts in which n appears." ></td>
	<td class="line x" title="35:249	If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al. , 1999), as is the case for relative frequencies, then we may write the confusion probability as follows: P(m) conf(q, r, P(m) ) = E q(v)r(v) -p-~(v) ' V Note that it incorporates unigram probabilities as well as the two distributions q and r. Finally, Kendall's % which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993)." ></td>
	<td class="line x" title="36:249	In our context, it looks for correlation between the behavior of q and r on pairs of verbs." ></td>
	<td class="line x" title="37:249	Three versions exist; we use the simplest, Ta, here: r(q,r) = E sign \[(q(vl) q(v2))(r(vl) r(v2))\] v,,v 2(l t) where sign(x) is 1 for positive arguments, -1 for negative arguments, and 0 at 0." ></td>
	<td class="line x" title="38:249	The intuition behind Kendall's T is as follows." ></td>
	<td class="line x" title="39:249	Assume all verbs have distinct conditional probabilities." ></td>
	<td class="line x" title="40:249	If sorting the verbs by the likelihoods assigned by q yields exactly the same ordering as that which results from ranking them according to r, then T(q, r) = 1; if it yields exactly the opposite ordering, then T(q, r) --1." ></td>
	<td class="line x" title="41:249	We treat a value of -1 as indicating extreme dissimilarity." ></td>
	<td class="line x" title="42:249	3 It is worth noting at this point that there are several well-known measures from the NLP literature that we have omitted from our experiments." ></td>
	<td class="line oc" title="43:249	Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al. , 1995; Luk, 1995; D. Lin, 1998a)." ></td>
	<td class="line x" title="44:249	It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(VIn ) and P(VIm)), but rather the similarity between a joint distribution P(X1,X2) and the corresponding product distribution P(X1)P(X2)." ></td>
	<td class="line x" title="45:249	Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature Values that are probabilities." ></td>
	<td class="line x" title="46:249	Variations of the value difference metric (Stanfill and Waltz, 1986) have been employed for supervised disambiguation (Ng and H.B. Lee, 1996; Ng, 1997); but it is not reasonable in language modeling to expect training data tagged with correct probabilities." ></td>
	<td class="line x" title="47:249	The Dice coej~cient (Smadja et al. , 1996; D. Lin, 1998a, 1998b) is monotonic in Jaccard's coefficient (van Rijsbergen, 1979), so its inclusion in our experiments would be redundant." ></td>
	<td class="line x" title="48:249	Finally, we did not use the KL divergence because it requires a smoothed base language model." ></td>
	<td class="line x" title="49:249	SZero would also be a reasonable choice, since it indicates zero correlation between q and r. However, it would then not be clear how to average in the estimates of negatively correlated words in equation (1)." ></td>
	<td class="line x" title="50:249	27 3 Empirical Comparison We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary comparison (Dagan et al. , 1999)." ></td>
	<td class="line x" title="51:249	That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools." ></td>
	<td class="line x" title="52:249	587,833 (80%) of the pairs served as a training set from which to calculate base probabilities." ></td>
	<td class="line x" title="53:249	From the other 20%, we prepared test sets as follows: after discarding pairs occurring in the training data (after all, the point of similarity-based estimation is to deal with unseen pairs), we split the remaining pairs into five partitions, and replaced each nounverb pair (n, vl) with a noun-verb-verb triple (n, vl, v2) such that P(v2) ~ P(vl)." ></td>
	<td class="line x" title="54:249	The task for the language model under evaluation was to reconstruct which of (n, vl) and (n, v2) was the original cooccurrence." ></td>
	<td class="line x" title="55:249	Note that by construction, (n, Vl) was always the correct answer, and furthermore, methods relying solely on unigram frequencies would perform no better than chance." ></td>
	<td class="line x" title="56:249	Test-set performance was measured by the error rate, defined as T(# of incorrect choices + (# of ties)/2), where T is the number of test triple tokens in the set, and a tie results when both alternatives are deemed equally likely by the language model in question." ></td>
	<td class="line x" title="57:249	To perform the evaluation, we incorporated each similarity function into a decision rule as follows." ></td>
	<td class="line x" title="58:249	For a given similarity measure f and neighborhood size k, let 3f, k(n) denote the k most similar words to n according to f. We define the evidence according to f for the cooccurrence ( n, v~) as Ef, k(n, vi) = \[(m E SLk(n) : P(vilm) > l }l  Then, the decision rule was to choose the alternative with the greatest evidence." ></td>
	<td class="line x" title="59:249	The reason we used a restricted version of the distance-weighted averaging model was that we sought to discover fundamental differences in behavior." ></td>
	<td class="line x" title="60:249	Because we have a binary decision task, Ef,k(n, vl) simply counts the number of k nearest neighbors to n that make the right decision." ></td>
	<td class="line x" title="61:249	If we have two functions f and g such that Ef,k(n, Vl) > Eg,k(n, vi), then the k most similar words according to f are on the whole better predictors than the k most similar words according to g; hence, f induces an inherently better similarity ranking for distance-weighted averaging." ></td>
	<td class="line x" title="62:249	The difficulty with using the full model (Equation (1)) for comparison purposes is that fundamental differences can be obscured by issues of weighting." ></td>
	<td class="line x" title="63:249	For example, suppose the probability estimate ~v(2 -Ll(q, r))." ></td>
	<td class="line x" title="64:249	r(v) (suitably normalized) performed poorly." ></td>
	<td class="line x" title="65:249	We would not be able to tell whether the cause was an inherent deficiency in the L1 norm or just a poor choice of weight function -perhaps (2Ll(q,r)) 2 would have yielded better estimates." ></td>
	<td class="line x" title="66:249	Figure 2 shows how the average error rate varies with k for the seven similarity metrics introduced above." ></td>
	<td class="line x" title="67:249	As previously mentioned, a steeper slope indicates a better similarity ranking." ></td>
	<td class="line x" title="68:249	All the curves have a generally upward trend but always lie far below backoff (51% error rate)." ></td>
	<td class="line x" title="69:249	They meet at k = 1000 because Sf, looo(n) is always the set of all nouns." ></td>
	<td class="line x" title="70:249	We see that the functions fall into four groups: (1) the L2 norm; (2) Kendall's T; (3) the confusion probability and the cosine metric; and (4) the L1 norm, Jensen-Shannon divergence, and Jaccard's coefficient." ></td>
	<td class="line x" title="71:249	We can account for the similar performance of various metrics by analyzing how they incorporate information from the intersection of the supports of q and r." ></td>
	<td class="line x" title="72:249	(Recall that we are using q and r for the conditional verb cooccurrrence probabilities of two nouns n and m)." ></td>
	<td class="line x" title="73:249	Consider the following supports (illustrated in Figure 3): Vq = {veV : q(v)>O} = {vV:r(v)>0} Yqr = {v  V : q(v)r(v) > 0} = Yq n We can rewrite the similarity functions from Section 2 in terms of these sets, making use of the identities ~-~veyq\yq~ q(v) + ~veyq~ q(v) = ~'~-v~U~\Vq~ r(v) + ~v~Vq~ r(v) = 1." ></td>
	<td class="line x" title="74:249	Table 1 lists these alternative forms in order of performance." ></td>
	<td class="line x" title="75:249	28 0.4 0.38 0.36 0.34 ~ 0.32 0.3-0.28 0.26 100 Error rates (averages and ranges) I i i I i I. ,2-*.-Jag~ 200 300 400 500 600 700 800 900 1000 k Figure 2: Similarity metric performance." ></td>
	<td class="line x" title="76:249	Errorbars denote the range of error rates over the five test sets." ></td>
	<td class="line x" title="77:249	Backoff's average error rate was 51%." ></td>
	<td class="line x" title="78:249	L2(q,r) . 2(l l) =,/Eq(v)2-2Eq(v)r(v)+ Er(v) 2 V vq~ v~ = 2 IVq~l IV \ (vq u V~)l 2 IVq \ Vail Iv~ \Vq~l + E E sign\[(q(vl) q(v2))(r(vl) r(v2))\] Vl E(VqA Vr) v2EYq~, + E E sign\[(q(vl)-q(v2))(r(vl)-r(v2))\] Vl eVqr v2EVqUVr conf(q, r, P(m)) cos(q, r) = P(ra) Y\] q(v)r(v)/P(v) v e Vq~ = E q(v)r(v)( E q(v) 2 E r(v)2) -1/2 v~ Vqr ve Vq v~ Vr Ll(q,r) JS(q, r) Jac(q, r) = 2-E (Iq(v)-r(v)l-q(v)-r(v)) vE Vqr = log2 + 1 E (h(q(v) + r(v)) h(q(v)) h(r(v))), v ~ Vq~ = IV~l/IV~ u v~l h( x ) = -x log x Table 1: Similarity functions, written in terms of sums over supports and grouped by average performance." ></td>
	<td class="line x" title="79:249	\ denotes set difference; A denotes symmetric set difference." ></td>
	<td class="line x" title="80:249	We see that for the non-combinatorial functions, the groups correspond to the degree to which the measures rely on the verbs in Vat." ></td>
	<td class="line x" title="81:249	The Jensen-Shannon divergence and the L1 norm can be computed simply by knowing the values of q and r on Vqr." ></td>
	<td class="line x" title="82:249	For the cosine and the confusion probability, the distribution values on Vqr are key, but other information is also incorporated." ></td>
	<td class="line x" title="83:249	The statistic Ta takes into account all verbs, including those that occur neither with 29 v Figure 3: Supports on V n nor m. Finally, the Euclidean distance is quadratic in verbs outside Vat; indeed, Kaufman and Rousseeuw (1990) note that it is 'extremely sensitive to the effect of one or more outliers' (pg." ></td>
	<td class="line x" title="84:249	117)." ></td>
	<td class="line x" title="85:249	The superior performance of Jac(q, r) seems to underscore the importance of the set Vqr." ></td>
	<td class="line x" title="86:249	Jaccard's coefficient ignores the values of q and r on Vqr; but we see that simply knowing the size of Vqr relative to the supports of q and r leads to good rankings." ></td>
	<td class="line x" title="87:249	4 The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared." ></td>
	<td class="line x" title="88:249	However, we can make a further observation: with the exception of the confusion probability, all the functions we compared are symmetric, that is, f(q, r) -= f(r, q)." ></td>
	<td class="line x" title="89:249	But the substitutability of one word for another need not symmetric." ></td>
	<td class="line x" title="90:249	For instance, 'fruit' may be the best possible approximation to 'apple', but the distribution of 'apple' may not be a suitable proxy for the distribution of 'fruit'.a In accordance with this insight, we developed a novel asymmetric generalization of the KL divergence, the a-skew divergence: sa(q,r) = D(r \[\[a'q + (1 a)-r) for 0 <_ a < 1." ></td>
	<td class="line x" title="91:249	It can easily be shown that sa depends only on the verbs in Vat." ></td>
	<td class="line x" title="92:249	Note that at a -1, the skew divergence is exactly the KL divergence, and su2 is twice one of the summands of JS (note that it is still asymmetric)." ></td>
	<td class="line x" title="93:249	40n a related note, an anonymous reviewer cited the following example from the psychology literature: we can say Smith's lecture is like a sleeping pill, but 'not the other way round'." ></td>
	<td class="line x" title="94:249	30 We can think of a as a degree of confidence in the empirical distribution q; or, equivalently, (1 a) can be thought of as controlling the amount by which one smooths q by r. Thus, we can view the skew divergence as an approximation to the KL divergence to be used when sparse data problems would cause the latter measure to be undefined." ></td>
	<td class="line x" title="95:249	Figure 4 shows the performance of sa for a = .99." ></td>
	<td class="line x" title="96:249	It performs better than all the other functions; the difference with respect to Jaccard's coefficient is statistically significant, according to the paired t-test, at all k (except k = 1000), with significance level .01 at all k except 100, 400, and 1000." ></td>
	<td class="line x" title="97:249	5 Discussion In this paper, we empirically evaluated a number of distributional similarity measures, including the skew divergence, and analyzed their information sources." ></td>
	<td class="line x" title="98:249	We observed that the ability of a similarity function f(q, r) to select useful nearest neighbors appears to be correlated with its focus on the intersection Vqr of the supports of q and r. This is of interest from a computational point of view because Vqr tends to be a relatively small subset of V, the set of all verbs." ></td>
	<td class="line x" title="99:249	Furthermore, it suggests downplaying the role of negative information, which is encoded by verbs appearing with exactly one noun, although the Jaccard coefficient does take this type of information into account." ></td>
	<td class="line x" title="100:249	Our explicit division of V-space into various support regions has been implicitly considered in other work." ></td>
	<td class="line x" title="101:249	Smadja et al.(1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X's occurring with a translation other than Y decreases one's belief in their association; but the absence of both X and Y yields no information." ></td>
	<td class="line x" title="103:249	In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important." ></td>
	<td class="line x" title="104:249	D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure." ></td>
	<td class="line x" title="105:249	Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema." ></td>
	<td class="line x" title="106:249	The 0.4 0.38 I 0.36 \[ 0.34 0.32 0.3 0.28 0.26 100 Error rates (averages and ranges) L1 JS ~0 300 ~0 ~0 600 700 800 ~0 1000 k Figure 4: Performance of the skew divergence with respect to the best functions from Figure 2." ></td>
	<td class="line x" title="107:249	definition of commonality is left to the user (several different definitions are proposed for different tasks)." ></td>
	<td class="line x" title="108:249	We view the empirical approach taken in this paper as complementary to Lin's." ></td>
	<td class="line x" title="109:249	That is, we are working in the context of a particular application, and, while we have no mathematical certainty of the importance of the 'common support' information, we did not assume it a priori; rather, we let the performance data guide our thinking." ></td>
	<td class="line x" title="110:249	Finally, we observe that the skew metric seems quite promising." ></td>
	<td class="line x" title="111:249	We conjecture that appropriate values for a may inversely correspond to the degree of sparseness in the data, and intend in the future to test this conjecture on larger-scale prediction tasks." ></td>
	<td class="line x" title="112:249	We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991)." ></td>
	<td class="line x" title="113:249	6 Acknowledgements Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No." ></td>
	<td class="line x" title="114:249	IRI9712068." ></td>
	<td class="line x" title="115:249	References Claire Cardie." ></td>
	<td class="line x" title="116:249	1993." ></td>
	<td class="line x" title="117:249	A case-based approach to knowledge acquisition for domain-specific sentence analysis." ></td>
	<td class="line x" title="118:249	In 11th National Conference on Artifical Intelligence, pages 798-803." ></td>
	<td class="line x" title="119:249	Kenneth Ward Church and Patrick Hanks." ></td>
	<td class="line x" title="120:249	1990." ></td>
	<td class="line x" title="121:249	Word association norms, mutual information, and lexicography." ></td>
	<td class="line x" title="122:249	Computational Linguistics, 16(1):22-29." ></td>
	<td class="line x" title="123:249	Kenneth W. Church." ></td>
	<td class="line x" title="124:249	1988." ></td>
	<td class="line x" title="125:249	A stochastic parts program and noun phrase parser for unrestricted text." ></td>
	<td class="line x" title="126:249	In Second Conference on Applied Natural Language Processing, pages 136-143." ></td>
	<td class="line x" title="127:249	Thomas M. Cover and Joy A. Thomas." ></td>
	<td class="line x" title="128:249	1991." ></td>
	<td class="line x" title="129:249	Elements of Information Theory." ></td>
	<td class="line x" title="130:249	John Wiley." ></td>
	<td class="line x" title="131:249	Ido Dagan, Shanl Marcus, and Shanl Markovitch." ></td>
	<td class="line x" title="132:249	1995." ></td>
	<td class="line x" title="133:249	Contextual word similarity and estimation from sparse data." ></td>
	<td class="line x" title="134:249	Computer Speech and Language, 9:123-152." ></td>
	<td class="line x" title="135:249	Ido Dagan, Lillian Lee, and Fernando Pereira." ></td>
	<td class="line x" title="136:249	1999." ></td>
	<td class="line x" title="137:249	Similarity-based models of cooccurrence probabilities." ></td>
	<td class="line x" title="138:249	Machine Learning, 34(13) :43-69." ></td>
	<td class="line x" title="139:249	Ute Essen and Volker Steinbiss." ></td>
	<td class="line x" title="140:249	1992." ></td>
	<td class="line x" title="141:249	Cooccurrence smoothing for stochastic language modeling." ></td>
	<td class="line x" title="142:249	In ICASSP 92, volume 1, pages 161-164." ></td>
	<td class="line x" title="143:249	Jean Dickinson Gibbons." ></td>
	<td class="line x" title="144:249	1993." ></td>
	<td class="line x" title="145:249	Nonparametric Measures of Association." ></td>
	<td class="line x" title="146:249	Sage University Paper series on Quantitative Applications in the Social Sciences, 07-091." ></td>
	<td class="line x" title="147:249	Sage Publications." ></td>
	<td class="line x" title="148:249	Ralph Grishman and John Sterling." ></td>
	<td class="line x" title="149:249	1993." ></td>
	<td class="line x" title="150:249	Smoothing of automatically generated selectional constraints." ></td>
	<td class="line x" title="151:249	In Human Language Technology: Proceedings of the ARPA Workshop, pages 254-259." ></td>
	<td class="line x" title="152:249	Vasileios Hatzivassiloglou and Kathleen McKeown." ></td>
	<td class="line x" title="153:249	1993." ></td>
	<td class="line x" title="154:249	Towards the automatic identification of adjectival scales: Clustering of adjectives according to meaning." ></td>
	<td class="line x" title="155:249	In 31st Annual Meeting of the ACL, pages 172-182." ></td>
	<td class="line x" title="156:249	Vasileios Hatzivassiloglou." ></td>
	<td class="line x" title="157:249	1996." ></td>
	<td class="line x" title="158:249	Do we need linguistics when we have statistics?" ></td>
	<td class="line x" title="159:249	A comparative analysis of the contributions of linguistic cues to a statistical word grouping system." ></td>
	<td class="line x" title="160:249	In Judith L. Klavans and Philip Resnik, editors, The Balancing Act, pages 6794." ></td>
	<td class="line x" title="161:249	MIT Press." ></td>
	<td class="line x" title="162:249	Don Hindle." ></td>
	<td class="line x" title="163:249	1990." ></td>
	<td class="line x" title="164:249	Noun classification from predicate-argument structures." ></td>
	<td class="line x" title="165:249	In 28th Annual Meeting of the A CL, pages 268-275." ></td>
	<td class="line x" title="166:249	Frederick Jelinek and Robert L. Mercer." ></td>
	<td class="line x" title="167:249	1980." ></td>
	<td class="line x" title="168:249	Interpolated estimation of Markov source parameters from sparse data." ></td>
	<td class="line x" title="169:249	In Proceedings of the Workshop on Pattern Recognition in Practice." ></td>
	<td class="line x" title="170:249	William P. Jones and George W. Furnas." ></td>
	<td class="line x" title="171:249	1987." ></td>
	<td class="line x" title="172:249	Pictures of relevance." ></td>
	<td class="line x" title="173:249	Journal of the American Society for Information Science, 38(6):420-442." ></td>
	<td class="line x" title="174:249	Yael Karov and Shimon Edelman." ></td>
	<td class="line x" title="175:249	1998." ></td>
	<td class="line x" title="176:249	Similarity-based word sense disambiguation." ></td>
	<td class="line x" title="177:249	Computational Linguistics, 24(1):41-59." ></td>
	<td class="line x" title="178:249	Slava M. Katz." ></td>
	<td class="line x" title="179:249	1987." ></td>
	<td class="line x" title="180:249	Estimation of probabilities from sparse data for the language model component of a speech recognizer." ></td>
	<td class="line x" title="181:249	IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400--401, March." ></td>
	<td class="line x" title="182:249	Leonard Kanfman and Peter J. Rousseeuw." ></td>
	<td class="line x" title="183:249	1990." ></td>
	<td class="line x" title="184:249	Finding Groups in Data: An Introduction to Cluster Analysis." ></td>
	<td class="line x" title="185:249	John Wiley and Sons." ></td>
	<td class="line x" title="186:249	Lillian Lee." ></td>
	<td class="line x" title="187:249	1997." ></td>
	<td class="line x" title="188:249	Similarity-Based Approaches to Natural Language Processing." ></td>
	<td class="line x" title="189:249	Ph.D. thesis, Harvard University." ></td>
	<td class="line x" title="190:249	Dekang Lin." ></td>
	<td class="line x" title="191:249	1997." ></td>
	<td class="line x" title="192:249	Using syntactic dependency as local context to resolve word sense ambiguity." ></td>
	<td class="line x" title="193:249	In 35th Annual Meeting of the ACL, pages 64-71." ></td>
	<td class="line x" title="194:249	Dekang Lin." ></td>
	<td class="line x" title="195:249	1998a." ></td>
	<td class="line x" title="196:249	Automatic retrieval and clustering of similar words." ></td>
	<td class="line x" title="197:249	In COLING-A CL '98, pages 768-773." ></td>
	<td class="line x" title="198:249	Dekang Lin." ></td>
	<td class="line x" title="199:249	1998b." ></td>
	<td class="line x" title="200:249	An information theoretic definition of similarity." ></td>
	<td class="line x" title="201:249	In Machine Learning: Proceedings of the Fiftheenth International Conference (ICML '98)." ></td>
	<td class="line x" title="202:249	Jianhua Lin." ></td>
	<td class="line x" title="203:249	1991." ></td>
	<td class="line x" title="204:249	Divergence measures based on the Shannon entropy." ></td>
	<td class="line x" title="205:249	IEEE Transactions on Information Theory, 37(1):145-151." ></td>
	<td class="line x" title="206:249	Alpha K. Luk." ></td>
	<td class="line x" title="207:249	1995." ></td>
	<td class="line x" title="208:249	Statistical sense disambiguation with relatively small corpora using dictionary definitions." ></td>
	<td class="line x" title="209:249	In 33rd Annual Meeting of the ACL, pages 181-188." ></td>
	<td class="line x" title="210:249	Hwee Tou Ng and Hian Beng Lee." ></td>
	<td class="line x" title="211:249	1996." ></td>
	<td class="line x" title="212:249	Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach." ></td>
	<td class="line x" title="213:249	In 3~th Annual Meeting of the ACL, pages 40--47." ></td>
	<td class="line x" title="214:249	Hwee Tou Ng." ></td>
	<td class="line x" title="215:249	1997." ></td>
	<td class="line x" title="216:249	Exemplar-based word sense disambiguation: Some recent improvements." ></td>
	<td class="line x" title="217:249	In Second Conference on Empirical Methods in Natural Language Processing (EMNLP-2), pages 208-213." ></td>
	<td class="line x" title="218:249	C. Radhakrishna Rao." ></td>
	<td class="line x" title="219:249	1982." ></td>
	<td class="line x" title="220:249	Diversity: Its measurement, decomposition, apportionment and analysis." ></td>
	<td class="line x" title="221:249	SankyhZt: The Indian Journal of Statistics, 44(A):1-22." ></td>
	<td class="line x" title="222:249	Philip Resnik." ></td>
	<td class="line x" title="223:249	1995." ></td>
	<td class="line x" title="224:249	Using information content to evaluate semantic similarity in a taxonomy." ></td>
	<td class="line x" title="225:249	In Proceedings of IJCAI-95, pages 448-453." ></td>
	<td class="line x" title="226:249	Gerard Salton and Michael J. McGill." ></td>
	<td class="line x" title="227:249	1983." ></td>
	<td class="line x" title="228:249	Introduction to Modern Information Retrieval." ></td>
	<td class="line x" title="229:249	McGraw-Hill." ></td>
	<td class="line x" title="230:249	Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou." ></td>
	<td class="line x" title="231:249	1996." ></td>
	<td class="line x" title="232:249	Translating collocations for bilingual lexicons: A statistical approach." ></td>
	<td class="line x" title="233:249	Computational Linguistics, 22(1):1-38." ></td>
	<td class="line x" title="234:249	Craig Stanfill and David Waltz." ></td>
	<td class="line x" title="235:249	1986." ></td>
	<td class="line x" title="236:249	Toward memory-based reasoning." ></td>
	<td class="line x" title="237:249	Communications of the ACM, 29(12):1213-1228." ></td>
	<td class="line x" title="238:249	K. Sugawara, M. Nishimura, K. Toshioka, M. Okochi, and T. Kaneko." ></td>
	<td class="line x" title="239:249	1985." ></td>
	<td class="line x" title="240:249	Isolated word recognition using hidden Markov models." ></td>
	<td class="line x" title="241:249	In ICASSP 85, pages 1-4." ></td>
	<td class="line x" title="242:249	C. J. van Rijsbergen." ></td>
	<td class="line x" title="243:249	1979." ></td>
	<td class="line x" title="244:249	Information Retrieval." ></td>
	<td class="line x" title="245:249	Butterworths, second edition." ></td>
	<td class="line x" title="246:249	Jakub Zavrel and Walter Daelemans." ></td>
	<td class="line x" title="247:249	1997." ></td>
	<td class="line x" title="248:249	Memory-based learning: Using similarity for smoothing." ></td>
	<td class="line x" title="249:249	In 35th Annual Meeting of the A CL, pages 436-443 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P99-1029
Using Mutual Information To Resolve Query Translation Ambiguities And Query Term Weighting
Jang, Myung-Gil;Myaeng, Sung Hyon;Park, Se Young;"></td>
	<td class="line x" title="1:153	Using Mutual Information to Resolve Query Translation Ambiguities and Query Term Weighting 1 Myung-Gil Jang, 2 Sung Hyon Myaeng and 1 Se Young Park 1 Dept. of Knowledge Information, Electronics and Telecommunications Research Institute 161 Kajong-Dong, Yusong-Gu, Taejon, Korea 305-350 { mgjang, sypark } @etri.re.kr 2 Dept. of Computer Science, Chungnam National University 220 Gung-Dong, Yusong-Gu, Taejon, Korea 305-764 shmyaeng@cs.chungnam.ac.kr Abstract An easy way of translating queries in one language to the other for cross-language information retrieval (IR) is to use a simple bilingual dictionary." ></td>
	<td class="line x" title="2:153	Because of the generalpurpose nature of such dictionaries, however, this simple method yields a severe translation ambiguity problem." ></td>
	<td class="line x" title="3:153	This paper describes the degree to which this problem arises in Korean-English cross-language IR and suggests a relatively simple yet effective method for disambiguation using mutual information statistics obtained only from the target document collection." ></td>
	<td class="line x" title="4:153	In this method, mutual information is used not only to select the best candidate but also to assign a weight to query terms in the target language." ></td>
	<td class="line x" title="5:153	Our experimental results based on the TREC-6 collection shows that this method can achieve up to 85% of the monolingual retrieval case and 96% of the manual disambiguation case." ></td>
	<td class="line x" title="6:153	Introduction Cross-language information retrieval (IR) enables a user to retrieve documents written in diverse languages using queries expressed in his or her own language." ></td>
	<td class="line x" title="7:153	For cross-language IR, either queries or documents are translated to overcome the language differences." ></td>
	<td class="line x" title="8:153	Although it is possible to apply a high-quality machine translation system for documents as in Oard & Hackett (1997), query translation has emerged as a more popular method because it is much simpler and more economical compared to document translation." ></td>
	<td class="line x" title="9:153	Query translation can be done in one or more of the three approaches: a dictionary-based approach, a thesaurus-based approach, or a corpus-based approach." ></td>
	<td class="line x" title="10:153	There are three problems that a cross-language IR system using a query translation method must solve (Grefenstette, 1998)." ></td>
	<td class="line x" title="11:153	The first problem is to figure out how a term expressed in one language might be written in another." ></td>
	<td class="line x" title="12:153	The second problem is to determine which of the possible translations should be retained." ></td>
	<td class="line x" title="13:153	The third problem is to determine how to properly weight the importance of translation alternatives when more than one is retained." ></td>
	<td class="line x" title="14:153	For cross-language IR between Korean and English, i.e. between Korean queries and English documents, an easy way to handle query, translation is to use a Korean-English machinereadable dictionary (MRD) because such bilingual MRDs are more widely available than other resources such as parallel corpora." ></td>
	<td class="line x" title="15:153	However, it has been known that with a simple use of bilingual dictionaries in other language pairs, retrieval effectiveness can be only 40%60% of that with monolingual retrieval (Ballesteros & Croft, 1997)." ></td>
	<td class="line x" title="16:153	It is obvious that other additional resources need to be used for better performance." ></td>
	<td class="line x" title="17:153	This paper focuses on the last two problems: pruning translations and calculating the weights for translation alternatives." ></td>
	<td class="line x" title="18:153	We first describe the overall query translation process and the extent to which the ambiguity problem arises in Korean-English cross-language IR." ></td>
	<td class="line oc" title="19:153	We then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information (MI) (Church and Hanks, 1990) statistics obtained only from the target document collection." ></td>
	<td class="line x" title="20:153	In this method, mutual 223 information is used not only to select the best candidate but also to assign a weight to query terms in the target language." ></td>
	<td class="line x" title="21:153	1 Overall Query Translation Process Our Korean-to-English query translation scheme works in four stages: keyword selection, dictionary-based query translation, bilingual word sense disambiguation, and query term weighting." ></td>
	<td class="line x" title="22:153	Although none of the common resources such as dictionaries, thesauri, and corpora alone is complete enough to produce high quality English queries, we decided to use a bilingual dictionary at the second stage and a target-language corpus for the third and the fourth stages." ></td>
	<td class="line x" title="23:153	Our strategy was to try not to depend on scarce resources to make the approach practical." ></td>
	<td class="line x" title="24:153	Figure 1 shows the four stages of Korean-to-English query translation." ></td>
	<td class="line x" title="25:153	Korean Query Korean-to-English \[ Query Translation Keyword Selection English Query T Query Term I Bilingual Word I Disambiguation \[ Dictionary-Based 1 Query Translation Fig." ></td>
	<td class="line x" title="26:153	1." ></td>
	<td class="line x" title="27:153	Four Stages for Korean-to-English Query Translation." ></td>
	<td class="line x" title="28:153	1.1 Keyword Selection At the first stage, Korean keywords to be fed into the query translation process are extracted from a quasi-natural language query." ></td>
	<td class="line x" title="29:153	This keyword selection is done with a morphological analyzer and a stochastic part-of-speech (POS) tagger for the Korean language (Shin et al. , 1996)." ></td>
	<td class="line x" title="30:153	The role of the tagger is to help select the exact morpheme sequence from the multiple candidate sequences generated by the morphological analysis." ></td>
	<td class="line x" title="31:153	This process of employing a morphological analysis and a tagger is crucial for selecting legitimate query words from the topic statements because Korean is an agglutinative language." ></td>
	<td class="line x" title="32:153	Without the tagger, all the extraneous candidate keywords generated from the morphological analyzer will have to be entered into the translation process, which in and of itself will generate extraneous words, due to one-to-many mapping in the bilingual dictionary." ></td>
	<td class="line x" title="33:153	1.2 Dictionary-Based Query Translation The second stage does the actual query translation based on a dictionary look-up, by applying both word-by-word translation and phrase-level translation." ></td>
	<td class="line x" title="34:153	For the correct identification of phrases in a Korean query, it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in Smadja (1993)." ></td>
	<td class="line x" title="35:153	Since the bilingual dictionary lacks some words that are essential for a correct interpretation of the Korean query, it is important to identify unknown words such as foreign words and transliterate them into English strings that need to be matched against an English dictionary (Jeong et al. , 1997)." ></td>
	<td class="line x" title="36:153	1.3 Selection of the Correct Translations At the word disambiguation stage, we filter out the extraneous words generated blindly from the dictionary lookup process." ></td>
	<td class="line x" title="37:153	In addition to the POS tagger, we employed a bilingual word disambiguation technique using the cooccurrence information extracted from the collection of target documents." ></td>
	<td class="line o" title="38:153	More specifically, The mutual information statistics between pairs of words were used to determine whether English words from different sets generated by the translation process are 'compatible'." ></td>
	<td class="line x" title="39:153	In a sense, we make use of mutual disambiguation effect among query terms." ></td>
	<td class="line x" title="40:153	More details are described in Section 3." ></td>
	<td class="line x" title="41:153	1.4 Query Term Weighting Finally, we apply our query term weighting technique to produce the final target query." ></td>
	<td class="line x" title="42:153	The term weighting scheme basically reflects the degree of associations between the translated terms, and we give a high or low term weighting value according to the degree of mutual association between query terms." ></td>
	<td class="line o" title="43:153	This is another area where we make use of mutual information obtained from a text corpus." ></td>
	<td class="line x" title="44:153	The result from the four stages is a set of query terms to be used in a 224 vector-space retrieval model." ></td>
	<td class="line x" title="45:153	2 Analysis of Translation Ambiguity Although an easy way to find translations of query terms is to use a bilingual dictionary, this method alone suffers from problems caused by translation ambiguity since there are often oneto-many correspondences in a bilingual dictionary." ></td>
	<td class="line x" title="46:153	For example, in a Korean query consisting of three words, ':Z\]-o--~-5~\]-~7\] _Q_~'(ja-dong-cha gong-gi oh-yum) that means air pollution caused by automobiles, each word can be translated into multiple English words when a Korean-English dictionary is used in a straightforward way." ></td>
	<td class="line x" title="47:153	The first word ':Z\]-o-~-5~\]-' (ja-dong-cha) of the query can be translated into English words with semantically similar but different words like 'motorcar', 'automobile', and 'car'." ></td>
	<td class="line x" title="48:153	The second word '--~-71' (gong-gi), a homonymous word, can be translated into English words with different meanings: 'air', 'atmosphere', 'empty vessel', and 'bowl'." ></td>
	<td class="line x" title="49:153	And the last word '_9--4' (oh-yum) can be translated into two English words, 'pollution' and 'contamination'." ></td>
	<td class="line x" title="50:153	Retaining multiple candidate words can be useful in promoting recall in monolingual IR system, but previous research indicates that failure to disambiguate the meanings of the words can hurt retrieval effectiveness tremendously." ></td>
	<td class="line x" title="51:153	For instance, it is obvious that a phrase like empty vessel would change the meaning of the query entirely." ></td>
	<td class="line x" title="52:153	Even a word like contamination, a synonym of pollution, may end up retrieving unrelated documents due to the slight differences in meaning." ></td>
	<td class="line x" title="53:153	Title Sho~ Long Table 1." ></td>
	<td class="line x" title="54:153	The De~ree of Ambiguities I \[Wrds I Wrd Pairs # in S. # in T. Average # in S. # in T. Average Lan." ></td>
	<td class="line x" title="55:153	Lang." ></td>
	<td class="line x" title="56:153	Ambiguity Lan." ></td>
	<td class="line x" title="57:153	Lang." ></td>
	<td class="line x" title="58:153	Ambiguity 48 158 I 3.29 \[ 29i 3212 8.83 112 447 3.99 1459 16.03 462 1835 3.97 6196 14.65 Table 1 shows the extent to which ambiguity occurs in our query translation when an EnglishKorean dictionary is used blindly after the morphological analysis and tagging." ></td>
	<td class="line x" title="59:153	The three rows, title, short, and long, indicate three different ways of composing queries from the topic statements in the TREC collection." ></td>
	<td class="line x" title="60:153	The left half shows the average number of English words per Korean word for each query, whereas the right half shows the average number of word pairs in English that can be formed from a single word pair in Korean." ></td>
	<td class="line x" title="61:153	The latter indicates that the disambiguation process will have to select one out of more than 9 possible pairs on the average, regardless of which part of the topic statements is used for formal query generation." ></td>
	<td class="line x" title="62:153	3 Query Translation and Mutual Information Our strategy for cross-language IR aims at practicality in that we try not to depend on scarce resources." ></td>
	<td class="line x" title="63:153	Along the same line of reasoning, we opted for a disambiguation approach that requires only a collection of documents in the target language, which is always available in any cross-language IR environment." ></td>
	<td class="line p" title="64:153	Since the goal of disambiguation is to select the best pair among many alternatives as described above, the mutual information statistic is a natural choice in judging the degree to which two words co-occur within a certain text boundary." ></td>
	<td class="line x" title="65:153	It would be reasonable to choose the pair of words that are most strongly associated with each other, thereby eliminating those translations that are not likely to be correct ones." ></td>
	<td class="line o" title="66:153	Mutual information values are calculated based on word co-occurrence statistics and used as a measure to calculate correlation between words." ></td>
	<td class="line oc" title="67:153	The mutual information Ml(x,y) is defined as the following formula (Church and Hanks, 1990)." ></td>
	<td class="line o" title="68:153	p(x, y) N fw(X, y ) MI(x, y) = log 2 = log z (1) p(x)p(y) f(x)f(y) Here x and y are words occurring within a window of w words." ></td>
	<td class="line x" title="69:153	The probabilities p(x) and p(y) are estimated by counting the number of observations of x and y in a corpus, f(x) and fly), and normalizing each by N, the size of the corpus." ></td>
	<td class="line x" title="70:153	Joint probabilities, p(x,y), are estimated by counting the number of times, f,(x,y), that x is followed by y in a window of w words and normalizing it by N. In our application of query translation, the joint cooccurrence frequency f,(x,y) has 6-word window size which seems to allow semantic relations of query as well as fixed expressions (idioms such 225 as bread and butter)." ></td>
	<td class="line x" title="71:153	We ensure that the word x be followed by the word y within the same sentence only." ></td>
	<td class="line o" title="72:153	In our query translation scheme, MI values are used to select most likely translations after each Korean query word is translated into one or more English words." ></td>
	<td class="line o" title="73:153	Our use of MI values is based on the assumption that when two words co-occur in the same query, they are likely to cooccur in the same affinity in documents." ></td>
	<td class="line x" title="74:153	Conversely, two words that do not co-occur in the same affinity are not likely to show up in the same query." ></td>
	<td class="line o" title="75:153	In a sense, we are conjecturing mutual information can reveal some degree of semantic association between words." ></td>
	<td class="line o" title="76:153	Table 2 gives some examples of MI values for the alternative word pairs for translated queries of TREC-6 Cross-Language IR Track." ></td>
	<td class="line o" title="77:153	These MI values were extracted from the English text corpus consisting of 1988 1990 AP news, which contains 116,759,540 words." ></td>
	<td class="line x" title="78:153	Table 2." ></td>
	<td class="line x" title="79:153	Exam Word x Word y respiratory ailment teddy bear fossil fuel air pollution research development AIDS spread ivory trade environment protection bear doll region country point interest law terrorism treatment result terrorism government opinion news food life copy price labor information )le of Ml(x, Values fix) fiy) fix,y) I Ml(x,y) 716 1134 74 9.272506 679 7932 262 8.644690 676 13176 333 8.381424 52216 4878 890 6.011214 24278 24213 1317 5.566768 18575 10199 212 4.872597 1885 86608 84 4.095613 7771 13139 36 3.717652 7932 1394 3 3.455646 21093 103833 358 2.948925 30419 51917 107 2.068232 70182 4762 20 1.944089 13432 38055 22 1.614487 4762 193977 29 1.299005 9124 82220 21 1.184332 32222 40625 30 0.984281 6803 90594 10 0.638950 26571 30245 11 0.468861 When Ml(x,y) is large, the word associations are strong and produce credible results for disambiguation of translations." ></td>
	<td class="line x" title="80:153	However, if Ml(x,y) < 0, we can predict that the word x and word y are in complementary distribution." ></td>
	<td class="line o" title="81:153	4 Disambiguation and Weight Calculation We can alleviate the translation ambiguity by discriminating against those word pairs with low MI values." ></td>
	<td class="line o" title="82:153	The word pair with the highest MI value is considered to be the correct one among all the candidates in the two sets." ></td>
	<td class="line x" title="83:153	Since a query is likely to be targeted at a single concept, regardless of how broad or narrow it is, we conjecture that words describing the concept are likely to have a high degree of association." ></td>
	<td class="line o" title="84:153	Although we use the mutual information statistic to measure the association, others such as those used by Ballesteros & Croft (1998) can be considered." ></td>
	<td class="line x" title="85:153	In the example of Section 2, each Korean word has multiple English words due to translation ambiguity." ></td>
	<td class="line o" title="86:153	Figure 2 shows the MI values calculated for the word pairs comprising the translations of the original query." ></td>
	<td class="line x" title="87:153	The words under wl, w2, and w3 are the translations from the three query words, respectively." ></td>
	<td class="line o" title="88:153	The lines indicate that mutual information values are available for the pairs, and the numbers show some of the significant MI values for the corresponding pairs among all the possible pairs." ></td>
	<td class="line x" title="89:153	wl w2 w3 bowl Fig." ></td>
	<td class="line x" title="90:153	2." ></td>
	<td class="line o" title="91:153	An Example of Word Pairs with MI Values Our bilingual word disambiguation and weighting schemes rely on both relative and absolute magnitudes of the MI vales." ></td>
	<td class="line o" title="92:153	The algorithm first looks for the pair with the highest MI value and selects the best candidates before and after the pair by comparing the MI values for the pairs that are connected with the initially chosen pairs." ></td>
	<td class="line x" title="93:153	This process is applied to the words immediately before or after the chosen pair in order to limit the effect of the choice that may be incorrect." ></td>
	<td class="line o" title="94:153	It should be noted that the words not chosen in this process are not used in the translated query unless the MI values are greater than a threshold." ></td>
	<td class="line x" title="95:153	As described below, we assume that the candidates not in the first tier may still be useful if they are strongly associated with the adjacent word selected." ></td>
	<td class="line x" title="96:153	226 For example, the word pair <air, pollution> that has the bold line representing the strongest association in the column is choisen first." ></td>
	<td class="line o" title="97:153	Then the three MI values for the pairs containing air are compared to select the <automobile, air> pair, resulting in <automobile, air, pollution>." ></td>
	<td class="line x" title="98:153	If there were additional columns in the example, the same process would be applied to the rest of the network." ></td>
	<td class="line x" title="99:153	There are three reasons why query term weighting is of some value in addition to the pruning of conceptually unrelated terms." ></td>
	<td class="line x" title="100:153	First, our word selection method is not guaranteed to give the correct translation." ></td>
	<td class="line x" title="101:153	The method would give a reasonable result only when two consecutive query terms are actually used together in many documents, which is a hypothesis yet to be confirmed for its validity." ></td>
	<td class="line x" title="102:153	Second, there may be more than one strong association whose degrees are different from each other by a large magnitude." ></td>
	<td class="line x" title="103:153	Third, seemingly extraneous terms may serve as a recall-enhancing device with a query expansion effect." ></td>
	<td class="line x" title="104:153	The basic idea in our term weighting scheme is to give a large weight to the best candidate and divide the remaining quantity to assign equal weights to the rest of the candidates." ></td>
	<td class="line x" title="105:153	In other words, the weight for the best candidate, W~, is either 1 if it is greater than a threshold value or expressed as follows." ></td>
	<td class="line o" title="106:153	Wb = f(x) 0.5 + 0.5 (2) 0+1 Here x and 0 are a MI value and a threshold, respectively." ></td>
	<td class="line o" title="107:153	The numerator, f(x), gives the smallest integer greater than the MI value so that the resulting weight is the same for all the candidates whose MI values are within a certain interval." ></td>
	<td class="line x" title="108:153	Once the value for W b is calculated, the weight for the rest of the candidates are calculated as follows: Wr _ 1 W h (3) n-1 where n is the number of candidates." ></td>
	<td class="line x" title="109:153	It should be noted that W~ + Z W = 1." ></td>
	<td class="line o" title="110:153	Based on our observation of the calculated MI values, we chose to use 3.0 as the cut-off value in choosing the best candidate and assign a fairly high weight." ></td>
	<td class="line o" title="111:153	The cut-off value was determined purely based on the data we obtained; it can vary based on the new range of MI values when different corpora are used." ></td>
	<td class="line x" title="112:153	In the example of Fig." ></td>
	<td class="line x" title="113:153	2, the word pair candidate between wl and w2 are (motorcar, air), (automobile, air), and (car, air)." ></td>
	<td class="line x" title="114:153	Here because the weight of the word pairs (automobile, air) is W, = 0.83, the word 'automobile' has a relatively higher term weight than the other two words 'motorcar' and 'car'." ></td>
	<td class="line x" title="115:153	Finally the optimal English query set with their term weight, <(motocar,0.085), (automobile, 0.83), (car, 0.085) >, is generated for the translations of wl." ></td>
	<td class="line x" title="116:153	5 Experiments We developed a system for our cross-language IR techniques and conducted some basic experiments using the collection from the CrossLanguage Track of TREC 6." ></td>
	<td class="line x" title="117:153	The 24 English queries are comprised of three fields: titles, descriptions, and narratives." ></td>
	<td class="line x" title="118:153	These English queries were manually translated into Korean queries so that we can pretend as if the Korean queries had been generated by human users for cross-language IR." ></td>
	<td class="line x" title="119:153	In order to compare crosslanguage IR and mono-language IR, we used the Smart 11.0 system developed by Cornell University." ></td>
	<td class="line x" title="120:153	Our goal was to examine the efficacy of the disambiguation and term weighting schemes in our query translation." ></td>
	<td class="line x" title="121:153	We ran our system with three sets of queries, differentiated by the query lengths: 'title' queries with title fields only, 'short' queries with description fields only, and 'long' queries with all the three fields." ></td>
	<td class="line x" title="122:153	The retrieval effectiveness measured with l 1-point average precision was used for comparison against the baseline of monolingual retrieval using the original English query." ></td>
	<td class="line x" title="123:153	Table 3 gives the experimental results from using the four types of query set." ></td>
	<td class="line x" title="124:153	The result from 'Translated Query I' was generated only with the keyword selection and dictionary-based query translation stages." ></td>
	<td class="line x" title="125:153	The result 'Translated Query II' was generated after all the stages of our word disambiguation and query term weighting were done." ></td>
	<td class="line x" title="126:153	And the result from the manually disambiguated query set was generated by manually selecting the best candidate terms from the Translated Query I. 227 Query Sets Original Quer)' Tran." ></td>
	<td class="line x" title="127:153	Query I Tran." ></td>
	<td class="line x" title="128:153	Query II M.Disam." ></td>
	<td class="line x" title="129:153	Query Table 3." ></td>
	<td class="line x" title="130:153	Ex 1 ~erimental Results i Title Short \] Lon~ l lpt." ></td>
	<td class="line x" title="131:153	P C/M(~,:) l lpt." ></td>
	<td class="line x" title="132:153	P C/M('~) \[ l lpt." ></td>
	<td class="line x" title="133:153	P C/M(,~) 0.3251 0.3189 0.2821 0.2290 70.44 0.21443 67.20 0.1587 56.26 0.2675 82.28 0.2698 84.60 0.2232 79.12 0.2779 85.48 0.3002 94.14 0.2433 86.25 The performance of the Translated query set I was about 70%, 67%, and 56% of monolingual retrieval for the three cases, respectively." ></td>
	<td class="line x" title="134:153	The performances of the translated query set II were about 82%, 85%, and 79% of monolingual retrieval for the three cases, respectively." ></td>
	<td class="line x" title="135:153	The performance of the disambiguated queries, 85%, 94%, and 86% of monolingual retrieval for the three cases, respectively, can be treated as the upper limit for the cross-language retrieval." ></td>
	<td class="line x" title="136:153	The reason why they are not 100% is attributed to the several factors." ></td>
	<td class="line x" title="137:153	They are: 1) the inaccuracy of the manual translation of the original English query into the Korean queries, 2) the inaccuracy of the Korean morphological analyzer and the tagger in generating query words, and 3) the inaccuracy in generating candidate terms using the bilingual dictionary." ></td>
	<td class="line x" title="138:153	The difference between Translated Query I and Translated Query II indicates that the Ml-based disambiguation and the term weighting schemes are effective in enhancing the retrieval effectiveness." ></td>
	<td class="line x" title="139:153	In addition, the results show that the use of these query translation schemes is more effective with long queries than with shorter queries." ></td>
	<td class="line x" title="140:153	This is expected because the longer the queries are, the more contextual information can be used for mutual disambiguation." ></td>
	<td class="line x" title="141:153	Conclusion It has been known that query translation using a simple bilingual dictionary leads to a more than 40% drop in retrieval effectiveness due to translation ambiguity." ></td>
	<td class="line o" title="142:153	Our query translation method uses mutual information extracted from the 1988 1990 AP corpus in order to solve the problems of the bilingual word disambiguation and query term weighting." ></td>
	<td class="line x" title="143:153	The experiments using test collection of TREC-6 Cross-Language Track show that the method improves retrieval effectiveness in Korean-to-English crosslanguage IR." ></td>
	<td class="line x" title="144:153	The performance can be up to 85% of the monolingual retrieval case." ></td>
	<td class="line x" title="145:153	We also found that we obtained the largest percent increase with long queries." ></td>
	<td class="line x" title="146:153	While the experimental results are very promising, there are several issues to be explored." ></td>
	<td class="line x" title="147:153	First, we need to test how effectively the method can be applied." ></td>
	<td class="line o" title="148:153	Second, we intend to experiment with other co-occurrence metrics, instead of the mutual information statistic, for possible improvement." ></td>
	<td class="line o" title="149:153	This investigation is motivated by our observation of some counterintuitive MI values." ></td>
	<td class="line x" title="150:153	Third, we also plan on using different algorithms for choosing the terms and calculating the weights." ></td>
	<td class="line x" title="151:153	In addition, we plan to use the pseudo relevance feedback method that has been proven to be effective in monolingual retrieval." ></td>
	<td class="line x" title="152:153	Terms in some top-ranked documents are thrown into the original query with an assumption that at least some, if not all, of the documents are relevant to the original query and that the terms appearing in the documents are useful in representing user's information need." ></td>
	<td class="line x" title="153:153	Here we need to determine a threshold value for the number of top ranked document for our cross-language retrieval situation, let alone other phenomenon." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P99-1051
Acquiring Lexical Generalizations From Corpora: A Case Study For Diathesis Alternations
Lapata, Mirella;"></td>
	<td class="line x" title="1:212	Acquiring Lexical Generalizations from Corpora: A Case Study for Diathesis Alternations Maria Lapata School of Cognitive Science Division of Informatics, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW, UK mlap@cogsci.ed.ac.uk Abstract This paper examines the extent to which verb diathesis alternations are empirically attested in corpus data." ></td>
	<td class="line x" title="2:212	We automatically acquire alternating verbs from large balanced corpora by using partialparsing methods and taxonomic information, and discuss how corpus data can be used to quantify linguistic generalizations." ></td>
	<td class="line x" title="3:212	We estimate the productivity of an alternation and the typicality of its members using type and token frequencies." ></td>
	<td class="line x" title="4:212	1 Introduction Diathesis alternations are changes in the realization of the argument structure of a verb that are sometimes accompanied by changes in meaning (Levin, 1993)." ></td>
	<td class="line x" title="5:212	The phenomenon in English is illustrated in (1)-(2) below." ></td>
	<td class="line x" title="6:212	(1) a. John offers shares to his employees." ></td>
	<td class="line x" title="7:212	b. John offers his employees shares." ></td>
	<td class="line x" title="8:212	(2) a. Leave a note for her." ></td>
	<td class="line x" title="9:212	b. Leave her a note." ></td>
	<td class="line x" title="10:212	Example (1) illustrates the dative alternation, which is characterized by an alternation between the prepositional frame 'V NP1 to NP2' and the double object frame 'V NP 1 NP2'." ></td>
	<td class="line x" title="11:212	The benefactive alternation (cf.(2)) is structurally similar to the dative, the difference being that it involves the preposition for rather than to." ></td>
	<td class="line x" title="13:212	Levin (1993) assumes that the syntactic realization of a verb's arguments is directly correlated with its meaning (cf.also Pinker (1989) for a similar proposal)." ></td>
	<td class="line x" title="15:212	Thus one would expect verbs that undergo the same alternations to form a semantically coherent class." ></td>
	<td class="line x" title="16:212	Levin's study on diathesis alternations has influenced recent work on word sense disambiguation (Dorr and Jones, 1996), machine translation (Dang et al. , 1998), and automatic lexical acquisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998)." ></td>
	<td class="line x" title="17:212	The objective of this paper is to investigate the extent to which diathesis alternations are empirically attested in corpus data." ></td>
	<td class="line x" title="18:212	Using the dative and benefactive alternations as a test case we attempt to determine: (a) if some alternations are more frequent than others, (b) if alternating verbs have frame preferences and (c) what the representative members of an alternation are." ></td>
	<td class="line x" title="19:212	In section 2 we describe and evaluate the set of automatic methods we used to acquire verbs undergoing the dative and benefactive alternations." ></td>
	<td class="line x" title="20:212	We assess the acquired frames using a filtering method presented in section 3." ></td>
	<td class="line x" title="21:212	The results are detailed in section 4." ></td>
	<td class="line x" title="22:212	Sections 5 and 6 discuss how the derived type and token frequencies can be used to estimate how productive an alternation is for a given verb semantic class and how typical its members are." ></td>
	<td class="line x" title="23:212	Finally, section 7 offers some discussion on future work and section 8 conclusive remarks." ></td>
	<td class="line x" title="24:212	2 Method 2.1 The parser The part-of-speech tagged version of the British National Corpus (BNC), a 100 million word collection of written and spoken British English (Burnard, 1995), was used to acquire the frames characteristic of the dative and benefactive alternations." ></td>
	<td class="line x" title="25:212	Surface syntactic structure was identified using Gsearch (Keller et al. , 1999), a tool which allows the search of arbitrary POS-tagged corpora for shallow syntactic patterns based on a user-specified context-free grammar and a syntactic query." ></td>
	<td class="line x" title="26:212	It achieves this by combining a left-corner parser with a regular expression matcher." ></td>
	<td class="line x" title="27:212	Depending on the grammar specification (i.e. , recursive or not) Gsearch can be used as a full contextfree parser or a chunk parser." ></td>
	<td class="line x" title="28:212	Depending on the syntactic query, Gsearch can parse full sentences, identify syntactic relations (e.g. , verb-object, adjectivenoun) or even single words (e.g. , all indefinite pro397 nouns in the corpus)." ></td>
	<td class="line x" title="29:212	Gsearch outputs all corpus sentences containing substrings that match a given syntactic query." ></td>
	<td class="line x" title="30:212	Given two possible parses that begin at the same point in the sentence, the parser chooses the longest match." ></td>
	<td class="line x" title="31:212	If there are two possible parses that can be produced for the same substring, only one parse is returned." ></td>
	<td class="line x" title="32:212	This means that if the number of ambiguous rules in the grammar is large, the correctness of the parsed output is not guaranteed." ></td>
	<td class="line x" title="33:212	2.2 Acquisition We used Gsearch to extract tokens matching the patterns 'V NP1 NP2', 'VP NP1 to NP2', and 'V NPI for NP2' by specifying a chunk grammar for recognizing the verbal complex and NPs." ></td>
	<td class="line x" title="34:212	POS-tags were retained in the parser's output which was postprocessed to remove adverbials and interjections." ></td>
	<td class="line x" title="35:212	Examples of the parser's output are given in (3)." ></td>
	<td class="line x" title="36:212	Although there are cases where Gsearch produces the right parse (cf.(3a)), the parser wrongly identifies as instances of the double object frame tokens containing compounds (cf.(3b)), bare relative clauses (cf.(3c)) and NPs in apposition (cf.(3d))." ></td>
	<td class="line x" title="41:212	Sometimes the parser attaches prepositional phrases to the wrong site (cf.(3e)) and cannot distinguish between arguments and adjuncts (cf.(3f)) or between different types of adjuncts (e.g. , temporal (cf.(3f)) versus benefactive (cf.(3g)))." ></td>
	<td class="line x" title="46:212	Erroneous output also arises from tagging mistakes." ></td>
	<td class="line x" title="47:212	(3) a. The police driver \[v shot\] \[NP Jamie\] \[ie a look of enquiry\] which he missed." ></td>
	<td class="line x" title="48:212	b. Some also \[v offer\] \[ipa free bus\] lip service\], to encourage customers who do not have their own transport." ></td>
	<td class="line x" title="49:212	c. A Jaffna schoolboy \[v shows\] \[NP a drawing\] lip he\] made of helicopters strafing his home town." ></td>
	<td class="line x" title="50:212	d. For the latter catalogue Barr \[v chose\] \[NP the Surrealist writer\] \[yp Georges Hugnet\] to write a historical essay." ></td>
	<td class="line x" title="51:212	e. It \[v controlled\] \[yp access\] \[pp to \[Nr' the vault\]\]." ></td>
	<td class="line x" title="52:212	f. Yesterday he \[v rang\] \[NP the bell\] \[Pl, for \[NP a long time\]\]." ></td>
	<td class="line x" title="53:212	g. Don't Iv save\] \[NP the bread\] \[pp for \[NP the birds\]\]." ></td>
	<td class="line x" title="54:212	We identified erroneous subcategorization frames (cf.(3b)-(3d)) by using linguistic heuristics and a process for compound noun detection (cf.section 2.3)." ></td>
	<td class="line x" title="57:212	We disambiguated the attachment site of PPs (cf.(3e)) using Hindle and Rooth's (1993) lexical association score (cf.section 2.4)." ></td>
	<td class="line x" title="60:212	Finally, we recognized benefactive PPs (cf.(3g)) by exploiting the WordNet taxonomy (cf.section 2.5)." ></td>
	<td class="line x" title="63:212	2.3 Guessing the double object frame We developed a process which assesses whether the syntactic patterns (called cues below) derived from the corpus are instances of the double object frame." ></td>
	<td class="line x" title="64:212	Linguistic Heuristics." ></td>
	<td class="line x" title="65:212	We applied several heuristics to the parser's output which determined whether corpus tokens were instances of the double object frame." ></td>
	<td class="line x" title="66:212	The 'Reject' heuristics below identified erroneous matches (cf.(3b-d)), whereas the 'Accept' heuristics identified true instances of the double object frame (cf.(3a))." ></td>
	<td class="line x" title="69:212	1." ></td>
	<td class="line x" title="70:212	Reject if cue contains at least two proper names adjacent to each other (e.g. , killed Henry Phipps )." ></td>
	<td class="line x" title="71:212	2." ></td>
	<td class="line x" title="72:212	Reject if cue contains possessive noun phrases (e.g. , give a showman's award)." ></td>
	<td class="line x" title="73:212	3." ></td>
	<td class="line x" title="74:212	Reject if cue's last word is a pronoun or an anaphor (e.g. , ask the subjects themselves)." ></td>
	<td class="line x" title="75:212	4." ></td>
	<td class="line x" title="76:212	Accept if verb is followed by a personal or indefinite pronoun (e.g. , found him a home)." ></td>
	<td class="line x" title="77:212	5." ></td>
	<td class="line x" title="78:212	Accept if verb is followed by an anaphor (e.g. , made herself a snack)." ></td>
	<td class="line x" title="79:212	6." ></td>
	<td class="line x" title="80:212	Accept if cue's surface structure is either 'V MOD l NP MOD NP' or 'V NP MOD NP' (e.g. , send Bailey a postcard)." ></td>
	<td class="line x" title="81:212	7." ></td>
	<td class="line x" title="82:212	Cannot decide if cue's surface structure is 'V MOD* N N+' (e.g. , offer a free bus service)." ></td>
	<td class="line x" title="83:212	Compound Noun Detection." ></td>
	<td class="line x" title="84:212	Tokens identified by heuristic (7) were dealt with separately by a procedure which guesses whether the nouns following the verb are two distinct arguments or parts of a compound." ></td>
	<td class="line x" title="85:212	This procedure was applied only to noun sequences of length 2 and 3 which were extracted from the parser's output 2 and compared against a compound noun dictionary (48,661 entries) compiled from WordNet." ></td>
	<td class="line x" title="86:212	13.9% of the noun sequences were identified as compounds in the dictionary." ></td>
	<td class="line x" title="87:212	I Here MOD represents any prenominal modifier (e.g. , articles, pronouns, adjectives, quantifiers, ordinals)." ></td>
	<td class="line x" title="88:212	2Tokens containing noun sequences with length larger than 3 (450 in total) were considered negative instances of the double object frame." ></td>
	<td class="line x" title="89:212	398 G-score ~' 2-word compound 1967.68 775.21 87.02 45.40 30.58 29.94 24.04 bank manager tax liability income tax book reviewer designer gear safety plan drama school Table 1 : Random sample of two word compounds Table G-score 3-word compound 574.48 382.92 77.78 48.84 36.44 32.35 23.98 \[\[energy efficiency\] office\] \[\[council tax\] bills\] \[alcohol \[education course\]\] \[hospital \[out-patient department\] \[\[turnout suppressor\] function\] \[\[nature conservation\] resources\] \[\[quality amplifier\] circuits\] 2: Random sample of three word compounds For sequences of length 2 not found in WordNet, we used the log-likelihood ratio (G-score) to estimate the lexical association between the nouns, in order to determine if they formed a compound noun." ></td>
	<td class="line oc" title="90:212	We preferred the log-likelihood ratio to other statistical scores, such as the association ratio (Church and Hanks, 1990) or ;(2, since it adequately takes into account the frequency of the co-occurring words and is less sensitive to rare events and corpussize (Dunning, 1993; Daille, 1996)." ></td>
	<td class="line x" title="91:212	We assumed that two nouns cannot be disjoint arguments of the verb if they are lexically associated." ></td>
	<td class="line x" title="92:212	On this basis, tokens were rejected as instances of the double object frame if they contained two nouns whose Gscore had a p-value less than 0.05." ></td>
	<td class="line x" title="93:212	A two-step process was applied to noun sequences of length 3: first their bracketing was determined and second the G-score was computed between the single noun and the 2-noun sequence." ></td>
	<td class="line x" title="94:212	We inferred the bracketing by modifying an algorithm initially proposed by Pustejovsky et al.(1993)." ></td>
	<td class="line x" title="96:212	Given three nouns n 1, n2, n3, if either \[n I n2\] or \[n2 n3\] are in the compound noun dictionary, we built structures \[\[nt n2\] n3\] or \[r/l \[n2 n3\]\] accordingly; if both \[n I n2\] and In2 n3\] appear in the dictionary, we chose the most frequent pair; if neither \[n l n2\] nor \[n2 n3\] appear in WordNet, we computed the G-score for \[nl n2\] and \[n2 n3\] and chose the pair with highest value (p < 0.05)." ></td>
	<td class="line x" title="97:212	Tables 1 and 2 display a random sample of the compounds the method found (p < 0.05)." ></td>
	<td class="line x" title="98:212	2.3.1 Evaluation The performance of the linguistic heuristics and the compound detection procedure were evaluated by randomly selecting approximate!y 3,000 corpus tokens which were previously accepted or rejected as instances of the double object frame." ></td>
	<td class="line x" title="99:212	Two judges decided whether the tokens were classified correctly." ></td>
	<td class="line x" title="100:212	The judges' agreement on the classification task was calculated using the Kappa coefficient (Siegel and Method l\[ Prec l\[ Kappa Reject heuristics 96.9% K = 0.76, N = 1000 Accept heuristics 73.6% K = 0.82, N = 1000 2-word compounds 98.9% K = 0.83, N = 553 3-word compounds 99.1% K = 0.70, N = 447 Verb attach-to 74.4% K = 0.78, N = 494 Noun attach-to 80.0% K = 0.80, N = 500 Verb attach-for 73.6% K = 0.85, N = 630 Noun attach-for 36.0% K = 0.88, N = 500 Table 3: Precision of heuristics, compound noun detection and lexical association Castellan, 1988) which measures inter-rater agreement among a set of coders making category judgments." ></td>
	<td class="line x" title="101:212	The Kappa coefficient of agreement (K) is the ratio of the proportion of times, P(A), that k raters agree to the proportion of times, P(E), that we would expect the raters to agree by chance (cf.(4))." ></td>
	<td class="line x" title="103:212	If there is a complete agreement among the raters, then K = 1." ></td>
	<td class="line x" title="104:212	P(A) -P(E) (4) K1 -P(E) Precision figures 3 (Prec) and inter-judge agreement (Kappa) are summarized in table 3." ></td>
	<td class="line x" title="105:212	In sum, the heuristics achieved a high accuracy in classifying cues for the double object frame." ></td>
	<td class="line x" title="106:212	Agreement on the classification was good given that the judges were given minimal instructions and no prior training." ></td>
	<td class="line x" title="107:212	2.4 Guessing the prepositional frames In order to consider verbs with prepositional frames as candidates for the dative and benefactive alternations the following requirements needed to be met: 1." ></td>
	<td class="line x" title="108:212	the PP must be attached to the verb; 3Throught the paper the reported percentages are the average of the judges' individual classifications." ></td>
	<td class="line x" title="109:212	399 2." ></td>
	<td class="line x" title="110:212	in the case of the 'V NPI to NP2' structure, the to-PP must be an argument of the verb; 3." ></td>
	<td class="line x" title="111:212	in the case of the 'V NPI for NP2' structure, the for-PP must be benefactive." ></td>
	<td class="line x" title="112:212	4 In older to meet requirements (1)-(3), we first determined the attachment site (e.g. , verb or noun) of the PP and secondly developed a procedure for distinguishing benefactive from non-benefactive PPs." ></td>
	<td class="line x" title="113:212	Several approaches have statistically addressed the problem of prepositional phrase ambiguity, with comparable results (Hindle and Rooth, 1993; Collins and Brooks, 1995; Ratnaparkhi, 1998)." ></td>
	<td class="line x" title="114:212	Hindle and Rooth (1993) used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n. We used a variant of the method described in Hindle and Rooth (1993), the main difference being that we applied their lexical association score (a log-likelihood ratio which compares the probability of noun versus verb attachment) in an unsupervised non-iterative manner." ></td>
	<td class="line x" title="115:212	Furthermore, the procedure was applied to the special case of tuples containing the prepositions to and for only." ></td>
	<td class="line x" title="116:212	2.4.1 Evaluation We evaluated the procedure by randomly selecting 2,124 tokens containing to-PPs and for-PPs for which the procedure guessed verb or noun attachment." ></td>
	<td class="line x" title="117:212	The tokens were disambiguated by two judges." ></td>
	<td class="line x" title="118:212	Precision figures are reported in table 3." ></td>
	<td class="line x" title="119:212	The lexicai association score was highly accurate on guessing both verb and noun attachment for to-PPs." ></td>
	<td class="line x" title="120:212	Further evaluation revealed that for 98.6% (K = 0.9, N = 494, k -2) of the tokens classified as instances of verb attachment, the to-PP was an argument of the verb, which meant that the log-likelihood ratio satisfied both requirements (1) and (2) for to-PPs." ></td>
	<td class="line x" title="121:212	A low precision of 36% was achieved in detecting instances of noun attachment for for-PPs." ></td>
	<td class="line x" title="122:212	One reason for this is the polysemy of the preposition for: for-PPs can be temporal, purposive, benefactive or causal adjuncts and consequently can attach to various sites." ></td>
	<td class="line x" title="123:212	Another difficulty is that benefactive forPPs semantically license both attachment sites." ></td>
	<td class="line x" title="124:212	To further analyze the poor performance of the log-likelihood ratio on this task, 500 tokens con4Syntactically speaking, benefactive for-PPs are not arguments but adjuncts (Jackendoff, 1990) and can appear on any verb with which they are semantically compatible." ></td>
	<td class="line x" title="125:212	taining for-PPs were randomly selected from the parser's output and disambiguated." ></td>
	<td class="line x" title="126:212	Of these 73.9% (K = 0.9, N = 500, k ---2) were instances of verb attachment, which indicates that verb attachments outnumber noun attachments for for-PPs, and therefore a higher precision for verb attachment (cf.requirement (1)) can be achieved without applying the log-likelihood ratio, but instead classifying all instances as verb attachment." ></td>
	<td class="line x" title="128:212	2.5 Benefactive PPs Although surface syntactic cues can be important for determining the attachment site of prepositional phrases, they provide no indication of the semantic role of the preposition in question." ></td>
	<td class="line x" title="129:212	This is particularly the case for the preposition for which can have several roles, besides the benefactive." ></td>
	<td class="line x" title="130:212	Two judges discriminated benefactive from nonbenefactive PPs for 500 tokens, randomly selected from the parser's output." ></td>
	<td class="line x" title="131:212	Only 18.5% (K ---0.73, N ---500, k = 2) of the sample contained benefactive PPs." ></td>
	<td class="line x" title="132:212	An analysis of the nouns headed by the preposition for revealed that 59.6% were animate, 17% were collective, 4.9% denoted locations, and the remaining 18.5% denoted events, artifacts, body parts,'or actions." ></td>
	<td class="line x" title="133:212	Animate, collective and location nouns account for 81.5% of the benefactive data." ></td>
	<td class="line x" title="134:212	We used the WordNet taxonomy (Miller et al. , 1990) to recognize benefactive PPs (cf.requirement (3))." ></td>
	<td class="line x" title="136:212	Nouns in WordNet are organized into an inheritance system defined by hypernymic relations." ></td>
	<td class="line x" title="137:212	Instead of being contained in a single hierarchy, nouns are partitioned into a set of semantic primitives (e.g. , act, animal, time) which are treated as the unique beginners of separate hierarchies." ></td>
	<td class="line x" title="138:212	We compiled a 'concept dictionary' from WordNet (87,642 entries), where each entry consisted of the noun and the semantic primitive distinguishing each noun sense (cf.table 4)." ></td>
	<td class="line x" title="140:212	We considered a for-PP to be benefactive if the noun headed by for was listed in the concept dictionary and the semantic primitive of its prime sense (Sense 1) was person, animal, group or location." ></td>
	<td class="line x" title="141:212	PPs with head nouns not listed in the dictionary were considered benefactive only if their head nouns were proper names." ></td>
	<td class="line x" title="142:212	Tokens containing personal, indefinite and anaphoric pronouns were also considered benefactive (e.g. , build a home for him)." ></td>
	<td class="line x" title="143:212	Two judges evaluated the procedure by judging 1,000 randomly selected tokens, which were accepted or rejected as benefactive." ></td>
	<td class="line x" title="144:212	The procedure achieved a precision of 48.8% (K ----0.89, N = 400 gift cooking teacher university city pencil Sense 1 Sense 2 Sense 3 possession food person group location artifact cognition act cognition artifact location act group group Table 4: Sample entries from WordNet concept dictionary 500, k = 2) in detecting benefactive tokens and 90.9% (K =.94, N = 499, k = 2) in detecting non-benefactive ones." ></td>
	<td class="line x" title="145:212	3 Filtering Filtering assesses how probable it is for a verb to be associated with a wrong frame." ></td>
	<td class="line x" title="146:212	Erroneous frames can be the result of tagging errors, parsing mistakes, or errors introduced by the heuristics and procedures we used to guess syntactic structure." ></td>
	<td class="line x" title="147:212	We discarded verbs for which we had very little evidence (frame frequency = 1) and applied a relative frequency cutoff: the verb's acquired frame frequency was compared against its overall frequency in the BNC." ></td>
	<td class="line x" title="148:212	Verbs whose relative frame frequency was lower than an empirically established threshold were discarded." ></td>
	<td class="line x" title="149:212	The threshold values varied from frame to flame but not from verb to verb and were determined by taking into account for each frame its overall frame frequency which was estimated from the COMLEX subcategorization dictionary (6,000 verbs) (Grishman et al. , 1994)." ></td>
	<td class="line x" title="150:212	This meant that the threshold was higher for less frequent frames (e.g. , the double object frame for which only 79 verbs are listed in COMLEX)." ></td>
	<td class="line x" title="151:212	We also experimented with a method suggested by Brent (1993) which applies the binomial test on frame frequency data." ></td>
	<td class="line x" title="152:212	Both methods yielded comparable results." ></td>
	<td class="line x" title="153:212	However, the relative frequency threshold worked slightly better and the results reported in the following section are based on this method." ></td>
	<td class="line x" title="154:212	4 Results We acquired 162 verbs for the double object frame, 426 verbs for the 'V NP1 to NP2' frame and 962 for the 'V NPl for NP2' frame." ></td>
	<td class="line x" title="155:212	Membership in alternations was judged as follows: (a) a verb participates in the dative alternation if it has the double object and 'V NP1 to NP2' frames and (b) a verb Dative Alternation Alternating V NPI NP2 allot, assign, bring, fax, feed, flick, give, grant, guarantee, leave, lend offer, owe, take pass, pay, render, repay, sell, show, teach, tell, throw, toss, write, serve, send, award allocate, bequeath, carry, catapult, cede, concede, drag, drive, extend, ferry, fly, haul, hoist, issue, lease, peddle, pose, preach, push, relay, ship, tug, yield V NPI to NP2 ask, chuck, promise, quote, read, shoot, slip Benefactive Alternation Alternating bake, build, buy, cast, cook, earn, fetch, find, fix, forge, gain, get, keep, knit, leave, make, pour, save procure, secure, set, toss, win, write V NPI NP2 arrange, assemble, carve, choose, compile, design, develop, dig, gather, grind, hire, play, prepare, reserve, run, sew V NP1 for NP2 boil, call, shoot Table 5: Verbs common in corpus and Levin participates in the benefactive alternation if it has the double object and 'V NP1 for NP2' frames." ></td>
	<td class="line x" title="156:212	Table 5 shows a comparison of the verbs found in the corpus against Levin's list of verbs; 5 rows 'V NP1 to NP2' and 'V NP1 for NP2' contain verbs listed as alternating in Levin but for which we acquired only one frame." ></td>
	<td class="line x" title="157:212	In Levin 115 verbs license the dative and 103 license the benefactive alternation." ></td>
	<td class="line x" title="158:212	Of these we acquired 68 for the dative and 43 for the benefactive alternation (in both cases including verbs for which only one frame was acquired)." ></td>
	<td class="line x" title="159:212	The dative and benefactive alternations were also acquired for 52 verbs not listed in Levin." ></td>
	<td class="line x" title="160:212	Of these, 10 correctly alternate (cause, deliver, hand, refuse, report and set for the dative alternation and cause, spoil, afford and prescribe for the benefactive), and 12 can appear in either frame but do not alternate (e.g. , appoint, fix, proclaim)." ></td>
	<td class="line x" title="161:212	For 18 verbs two frames were acquired but only one was correct (e.g. , swap and forgive which take only the double object frame), and finally 12 verbs neither alternated nor had the acquired frames." ></td>
	<td class="line x" title="162:212	A random sample of the acquired verb frames and their (log-transformed) frequencies is shown in figure 1." ></td>
	<td class="line x" title="163:212	5The comparisons reported henceforth exclude verbs listed in Levin with overall corpus frequency less than 1 per million." ></td>
	<td class="line x" title="164:212	401 I0 8 0= =.,4 ==,,d 2 NP-PP to frame NP-PP_for frame NP-NP frame i1\] Figure 1: Random sample of acquired frequencies for the dative and benefactive alternations class the number of verbs acquired from the corpus against the number of verbs listed in Levin." ></td>
	<td class="line x" title="166:212	As can be seen in figure 2, Levin and the corpus approximate each other for verbs of FUTURE HAVING (e.g. , guarantee), verbs of MESSAGE TRANSFER (e.g. , tell) and BRING-TAKE verbs (e.g. , bring)." ></td>
	<td class="line x" title="167:212	The semantic classes of GIVE (e.g. , sell), CARRY (e.g. , drag), SEND (e.g. , ship), GET (e.g. , buy) and PREPARE (e.g. , bake) verbs are also fairly well represented in the corpus, in contrast to SLIDE verbs (e.g. , bounce) for which no instances were found." ></td>
	<td class="line x" title="168:212	Note that the corpus and Levin did not agree with respect to the most popular classes licensing the dative and benefactive alternations: THROWING (e.g. , toss) and BUILD verbs (e.g. , carve) are the biggest classes in Levin allowing the dative and benefactive alternations respectively, in contrast to FUTURE HAVING and GET verbs in the corpus." ></td>
	<td class="line x" title="169:212	This can be explained by looking at the average corpus frequency of the verbs belonging to the semantic classes in question: FUTURE HAVING and GET Levi, I 1 1 verbs outnumber THROWING and BUILD verbs by 30 ~ Corpus dative . II 1 I a factor of two to one." ></td>
	<td class="line x" title="170:212	5 Productivity The relative productivity of an alternation for a se20 mantic class can be estimated by calculating the ratio of acquired to possible verbs undergoing the alternation (Aronoff, 1976; Briscoe and Copestake, Z l0 1996): (5) P(acquired\[class) = f (acquired, class) f (class) o We express the productivity of an alternation for o =." ></td>
	<td class="line x" title="171:212	'~ ~= ~,~ ~ =.~  .-= ~ Figure 2: Semantic classes for the dative and benefactive alternations Levin defines 10 semantic classes of verbs for which the dative alternation applies (e.g. , GIVE verbs, verbs of FUTURE HAVING, SEND verbs), and 5 classes for which the benefactive alternation applies (e.g. , BUILD, CREATE, PREPARE verbs), assuming that verbs participating in the same class share certain meaning components." ></td>
	<td class="line x" title="172:212	We partitioned our data according to Levin's predefined classes." ></td>
	<td class="line x" title="173:212	Figure 2 shows for each semantic a given class as f(acquired, class), the number of verbs which were found in the corpus and are members of the class, over f(class), the total number of verbs which are listed in Levin as members of the class (Total)." ></td>
	<td class="line x" title="174:212	The productivity values (Prod) for both the dative and the benefactive alternation (Alt) are summarized in table 6." ></td>
	<td class="line x" title="175:212	Note that productivity is sensitive to class size." ></td>
	<td class="line x" title="176:212	The productivity of BRING-TAKE verbs is estimated to be 1 since it contains only 2 members which were also found in the corpus." ></td>
	<td class="line x" title="177:212	This is intuitively correct, as we would expect the alternation to be more productive for specialized classes." ></td>
	<td class="line x" title="178:212	The productivity estimates discussed here can be potentially useful for treating lexical rules probabilistically, and for quantifying the degree to which language users are willing to apply' a rule in order 402 BRING-TAKE 2 2 1 0.327 FUTURE HAVING 19 17 0.89 0.313 GIVE 15 9 0.6 0.55 M.TRANSFER 17 10 0.58 0.66 CARRY 15 6 0.4 0.056 DRIVE 11 3 0.27 0.03 THROWING 30 7 0.23 0.658 SEND 23 3 0.13 0.181 INSTR." ></td>
	<td class="line x" title="179:212	COM." ></td>
	<td class="line x" title="180:212	18 1 0.05 0.648 SLIDE 5 0 0 0 Benefactive alternation Class Total Alt Prod Typ GET 33 17 0.51 0.54 PREPARE 26 9 0.346 0.55 BUILD 35 12 0.342 0.34 PERFORMANCE 19 1 0.05 0.56 CREATE 20 2 0.1 0.05 Table 6: Productivity estimates and typicality values for the dative and benefactive alternation to produce a novel form (Briscoe and Copestake, 1996)." ></td>
	<td class="line x" title="181:212	6 Typicality Estimating the productivity of an alternation for a given class does not incorporate information about the frequency of the verbs undergoing the alternation." ></td>
	<td class="line x" title="182:212	We propose to use frequency data to quantify the typicality of a verb or verb class for a given alternation." ></td>
	<td class="line x" title="183:212	The underlying assumption is that a verb is typical for an alternation if it is equally frequent for both frames which are characteristic for the alternation." ></td>
	<td class="line x" title="184:212	Thus the typicality of a verb can be defined as the conditional probability of the frame given the verb: f (framei, verb) (6) P(frameilverb) = y~ f fframe n, verb) n We calculate Pfframeilverb) by dividing f(frame i, verb), the number of times the verb was attested in the corpus with frame i, by ~-~.,, f(frame,,, verb), the overall number of times the verb was attested." ></td>
	<td class="line x" title="186:212	In our case a verb has two frames, hence P(frameilverb) is close to 0.5 for typical verbs (i.e. , verbs with balanced frequencies) and close to either 0 or 1 for peripheral verbs, depending on their preferred frame." ></td>
	<td class="line x" title="187:212	Consider the verb owe as an example (cf.figure 1)." ></td>
	<td class="line x" title="189:212	648 instances of owe were found, of which 309 were instances of the double object frame." ></td>
	<td class="line x" title="190:212	By dividing the latter by the former we can see that owe is highly typical of the dative alternation: its typicality score for the double object frame is 0.48." ></td>
	<td class="line x" title="191:212	By taking the average of P(framei, verb) for all verbs which undergo the alternation and belong to the same semantic class, we can estimate how typical this class is for the alternation." ></td>
	<td class="line x" title="192:212	Table 6 illustrates the typicality (Typ) of the semantic classes for the two alternations." ></td>
	<td class="line x" title="193:212	(The typicality values were computed for the double object frame)." ></td>
	<td class="line x" title="194:212	For the dative alternation, the most typical class is GIVE, and the most peripheral is DRIVE (e.g. , ferry)." ></td>
	<td class="line x" title="195:212	For the benefactive alternation, PERFORMANCE (e.g. , sing), PREPARE (e.g. , bake) and GET (e.g. , buy) verbs are the most typical, whereas CREATE verbs (e.g. , compose) are peripheral, which seems intuitively correct." ></td>
	<td class="line x" title="196:212	7 Future Work The work reported in this paper relies on frame frequencies acquired from corpora using partialparsing methods." ></td>
	<td class="line x" title="197:212	For instance, frame frequency data was used to estimate whether alternating verbs exhibit different preferences for a given frame (typicality)." ></td>
	<td class="line x" title="198:212	However, it has been shown that corpus idiosyncrasies can affect subcategorization frequencies (cf.Roland and Jurafsky (1998) for an extensive discussion)." ></td>
	<td class="line x" title="200:212	This suggests that different corpora may give different results with respect to verb alternations." ></td>
	<td class="line x" title="201:212	For instance, the to-PP frame is poorly' represented in the syntactically annotated version of the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="202:212	There are only 26 verbs taking the to-PP frame, of which 20 have frame frequency of 1." ></td>
	<td class="line x" title="203:212	This indicates that a very small number of verbs undergoing the dative alternation can be potentially acquired from this corpus." ></td>
	<td class="line x" title="204:212	In future work we plan to investigate the degree to which corpus differences affect the productivity and typicality estimates for verb alternations." ></td>
	<td class="line x" title="205:212	8 Conclusions This paper explored the degree to which diathesis alternations can be identified in corpus data via shallow syntactic processing." ></td>
	<td class="line x" title="206:212	Alternating verbs were acquired from the BNC by using Gsearch as a chunk parser." ></td>
	<td class="line x" title="207:212	Erroneous frames were discarded by applying linguistic heuristics, statistical scores (the loglikelihood ratio) and large-scale lexical resources 403 (e.g. , WordNet)." ></td>
	<td class="line x" title="208:212	We have shown that corpus frequencies can be used to quantify linguistic intuitions and lexical generalizations such as Levin's (1993) semantic classification." ></td>
	<td class="line x" title="209:212	Furthermore, corpus frequencies can make explicit predictions about word use." ></td>
	<td class="line x" title="210:212	This was demonstrated by using the frequencies to estimate the productivity of an alternation for a given semantic class and the typicality of its members." ></td>
	<td class="line x" title="211:212	Acknowledgments The author was supported by the Alexander S. Onassis Foundation and the UK Economic and Social Research Council." ></td>
	<td class="line x" title="212:212	Thanks to Chris Brew, Frank Keller, Alex Lascarides and Scott McDonald for valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C00-1059
Corpus-Dependent Association Thesauri For Information Retrieval
Kaji, Hiroyuki;Morimoto, Yasutsugu;Aizono, Toshiko;Yamasaki, Noriyuki;"></td>
	<td class="line x" title="1:184	Corpus-dependent Association Thesauri for Information Retrieval Hiroyuki Kaji '~, Yasutsugu Morimoto '~, Toshiko Aizono 'l, and Noriyuki Yamasaki '2 '~ Central Research Laboratory, Hitachi, LLtd.,2 Software Division, Hitachi, Ltd. 1-280 Higashi-Koigakubo, Kokubunji-shi 549-6 Shinano-cho, Totsuka-kt,, Yokohanla-shi Tokyo 185-8601, Japan Kanagawa 244-0801, Japan {kaji, morimoto, aizono}@crl.hitachi.co.jp, yamasa n@soft.hitachi.co.jp Abstract This paper presents a method for automatically generating an association thesaurus from a text corpus, and demonstrates its application to information retrieval." ></td>
	<td class="line x" title="3:184	The thesaurus generation method.consists of extracting tenns and co-occurrence data from a corpus and analyzing the correlation between terms statistically." ></td>
	<td class="line x" title="4:184	A new method for disambiguating the structure of compound nouns, which is a key component for term extraction, is also proposed." ></td>
	<td class="line x" title="5:184	The automatically generated thesaurus is effectively used as a tool for exploring infonnation." ></td>
	<td class="line x" title="6:184	A thesaurus navigator having novel functions such as term clustering, thesaurus overview, and zooming-in is proposed." ></td>
	<td class="line x" title="7:184	1 Introduction A thesaurus plays essential roles in information retrieval systems." ></td>
	<td class="line x" title="8:184	In particular, a domainspecific thesaurus greatly improves the effectiveness of information retrieval." ></td>
	<td class="line x" title="9:184	However, we are confronted with the difficult problem of how to construct and maintain a domain-specific thesaurus." ></td>
	<td class="line x" title="10:184	The goal of our present research is to establish a method for autolnatically generating a thesaurus from a text corpus of a domain and demonstrate its application to information retrieval." ></td>
	<td class="line x" title="11:184	Thesauri are classified into taxonomy-type thesauri and association thesauri." ></td>
	<td class="line x" title="12:184	There has been various research on the extraction of taxonomic information |'io111 a corpus, including extraction of hyponyms by using linguistic patterns (Hearst 1992) and extraction of synonyms based on the similarity of sets of co-occurring words (Ruge 1991; Grefenstette 1992)." ></td>
	<td class="line x" title="13:184	However, the performance of these methods is limited, and they should be considered as aids to augment handmade thesauri." ></td>
	<td class="line x" title="14:184	In contrast, an association thesaurus, that is a collection of pairs of semantically associated terms, can be possibly generated from a corpus entirely automatically." ></td>
	<td class="line oc" title="15:184	Word association norms based on co-occurrence information have been proposed by (Church and Hanks 1990)." ></td>
	<td class="line x" title="16:184	Here we focus on the automatic generation of an association thesanrus." ></td>
	<td class="line x" title="17:184	Association thesauri are as useful as taxon omy-type thesauri in information retrieval." ></td>
	<td class="line x" title="18:184	The improvement of retrieval effectiveness by using an association thesaurus has been reported by a number of papers (Jing and Croft 1994; Schutze and Pedersen 1994)." ></td>
	<td class="line x" title="19:184	We propose to use a coro pus-dependent association thesaurus interactively." ></td>
	<td class="line x" title="20:184	2 Automatic Generation of an Association Thesaurus from a Corpus 2.1 Outline of the thesaurus generation method The proposed thcsaurus generation method consists of term extraction, co-occurrence data extraction, and correlation analysis, as shown in Fig." ></td>
	<td class="line x" title="21:184	1." ></td>
	<td class="line x" title="22:184	~ Term F U Co-occ.rrence \[ data extraction rl pmrs and their 223___ Correlat!m~ Fig." ></td>
	<td class="line x" title="23:184	1." ></td>
	<td class="line x" title="24:184	Automatic generation of a thesaurus from a corpus." ></td>
	<td class="line x" title="25:184	404 2.1.1 Ternt extraction A thesaurus should consist of terms, each representing a domain-specific concept." ></td>
	<td class="line x" title="26:184	Most of tile terms representing important concepts are nol, lllS, simple or compound, that frequently occur in the corpus." ></td>
	<td class="line x" title="27:184	ThereR)re, we extract both simple nonns and compound nouns whose occurrence frequencies exceed a predetermined threshold." ></td>
	<td class="line x" title="28:184	We also use a list of stop words since frequently occurring nouns are not always terms." ></td>
	<td class="line x" title="29:184	Compound nouns are identified by a pattern matching method using a part-of-speech sequence pattern." ></td>
	<td class="line x" title="30:184	Naturally, the pattern is language specific." ></td>
	<td class="line x" title="31:184	The following is a pattern for Japanese compound nouns: COMP NOUN := { PREFIX } NOUN + SUFFIX } { PREFIX } NOUN + A problem in extracting compound nouns is that a word sequence matched to the above pattern, which actually defines just the type of noun phrase, is not always a term." ></td>
	<td class="line x" title="32:184	We filter out some kind of non-term noun phrases by using a list of stop words for the first and last elements of compound nouns." ></td>
	<td class="line x" title="33:184	Stop words for the first element of compound nouns include referential nouns (e.g. jouki (above-mentioned)) and determiner nouns (e.g. kaku (each))." ></td>
	<td class="line x" title="34:184	Stop words for the last element of compound nouns include time/place nouns (e.g. nai (inside)) and relational nouns (e.g. koyuu (peculiar))." ></td>
	<td class="line x" title="35:184	Another importmat problem we are confronted with in term extraction is the structural ambiguity of compound nouns." ></td>
	<td class="line x" title="36:184	For our purpose, we need to extract non-maximal compound nouns as well sts lnaxinlal comp()und nouns." ></td>
	<td class="line x" title="37:184	Here a nonmaximal compound noun means one that occurs as a part of a larger conlpound noun, and a nlaximal compound nonn means one thstt occurs not as a part of a larger compound noun." ></td>
	<td class="line x" title="38:184	We must disambiguate tile structure of compound nouns to correctly extract non-maximal compound nouns." ></td>
	<td class="line x" title="39:184	We have developed a statistical disambiguation method, the detail and evaluation of which are described in 2.2." ></td>
	<td class="line x" title="40:184	2.1.2 Co-occurrence data evtraction Our purpose is to collect pairs of semantically or contextually associated terms, no matter what kind of association." ></td>
	<td class="line x" title="41:184	So we extract cooccurrence in a window." ></td>
	<td class="line x" title="42:184	That is, every pair of terms occurring together within a window is extracted as the window is moved through a text." ></td>
	<td class="line x" title="43:184	The window size can be specified rather arbitrarily." ></td>
	<td class="line x" title="44:184	Considering our purpose, the window should accommodate a few sentences." ></td>
	<td class="line x" title="45:184	At tile same time, the window size should not be too large from the viewpoint of computational load." ></td>
	<td class="line x" title="46:184	Therefore, 20 to 50 words, excluding function words, seems to be an appropriate value." ></td>
	<td class="line x" title="47:184	Note that we filter out any pair of words cooccurring within a compound noun." ></td>
	<td class="line x" title="48:184	If such pairs were included in co-occurrence data, they would show high correlation." ></td>
	<td class="line x" title="49:184	However, they would be redundant because compound nouns are treated as entities in our thesaurus." ></td>
	<td class="line oc" title="50:184	2.1.3 Correlation analysis As a correlation measure between terms, we use mutual information (Church and Hanks 1990)." ></td>
	<td class="line o" title="51:184	The mutual inlbrmation between terms t~ and t i is defined by the following formula: g(ti, tj)/~' g(t,, t,) Ml(ti, tj) = log_, /i.i, {t'(t0/ i ~ f(t0}' { f(tj)//j~ f(ti, where f(t~) is the occurrence frequency of term t~, and g(ti,ti ) is the co-occurrence frequency of terms t~ and tj." ></td>
	<td class="line x" title="52:184	A rnaxinmm nunrber of associat-." ></td>
	<td class="line o" title="53:184	ed terms for each term is predetermined as well as a threshold for tile mutual information, and associated terms are selected based on the de-o scending order of mutual information." ></td>
	<td class="line n" title="54:184	Mutual infornaation involves a problem in that it is overestimated for low-frequency terms (I)unning 1993)." ></td>
	<td class="line x" title="55:184	Therefore, we determine whether two terms are related to each other by a log-likelihood ratio test, and we filter out pairs of terms that do not pass the test." ></td>
	<td class="line x" title="56:184	2.2 Disambiguation of compound noun structure 2.2.1 Disantbiguation based on coitus statistics Our disanabiguation method is described below for tile case of a compound noun consisting of three elelnents." ></td>
	<td class="line x" title="57:184	A compound noun W~W2W 3 has two possible structures: WI(W~W3) and (W~W,)W 3." ></td>
	<td class="line x" title="58:184	We deternfine its structure based on the occurrence t}equencies of maxilnal compound nouns as follows: If tile maximal compound noun W~W3 occurs more frequently than tile inaxinml compound noun W~W,, then tile 405 Table 1 (a) Examples Global-statistics-based disambiguation vs. local-statistics-based disambiguation." ></td>
	<td class="line x" title="59:184	Maximal compound noun Deta shori shisutemu (data processing system) Deta tsuskin seiL:vo souchi (Data communication controller) Kaisen seigyo purosessa (Line control processor) Freq." ></td>
	<td class="line x" title="60:184	Global-statistics-based Structure Freq." ></td>
	<td class="line x" title="61:184	478 (Deta shori) shisutemu 478 94 Deta (tsuskin seigvo 94 souchi) 54 Kaisen (seign'o 54 purosessa) Local-statistics-based Structure Freq." ></td>
	<td class="line x" title="62:184	(Deta shori) shisutemu 368 Deta (skori shisutemu) 110 (Deta tsushin) seigg'o souchi 40 Deta (tsuskin seig:vo souchi) 54 (Kaisen seign.,o) purosessa 54 (b) Summary of disambiguation results Global-statistics-based Correct structure 12,565 words (62.0%) Incorrect structure 7,688 words (38.0%) Total 20,253 words (100%) (Note: Numbers of words are occurrence-based)." ></td>
	<td class="line x" title="64:184	structure Wl(W2W3) is preferred." ></td>
	<td class="line x" title="65:184	On the con trary, if the maximal COlnpound noun W~W2 occurs more frequently than the maximal compound noun W2W3, then the structure (W~W2) W3 is preferred." ></td>
	<td class="line x" title="66:184	The generalized disambiguation rnle is as follows: If a compound noun CN includes two compound noun candidates CN~ and CN2, which are incompatible with each other, and the maximal compound noun CN~ occurs more frequently than the maximal compound noun CN> then a structure of CN including CN~ is preferred to a structure of CN including CN,." ></td>
	<td class="line x" title="67:184	We have two alternatives regarding the range where we count occurrence frequencies of maximal compound nouns." ></td>
	<td class="line x" title="68:184	One is global-statistics which means that frequencies are counted in the whole corpus and they are used to disambiguate all compound nouns in the corpus." ></td>
	<td class="line x" title="69:184	The other is local-statistics which means that frequencies are counted in each document in the corpus and they are used to disambiguate compound nouns in the corresponding document." ></td>
	<td class="line x" title="70:184	2.2.2 Evaluation' Global-statistics vs. localstatistics We evaluated both the global-statistics-based disambiguation and the local-statistics-based disambiguation by using a 23.7-M Byte corpus consisting of 800 patent documents." ></td>
	<td class="line x" title="71:184	Table l(a) shows comparative examples of these methods." ></td>
	<td class="line x" title="72:184	Evah, ation results for the 200 highest-frequency maximal COlnpound nouns consisting of three or Local-statistics-based more words are summa ~ rized in Table l(b)." ></td>
	<td class="line x" title="73:184	14,921 words (73.7%) They prove that the local-statistics-based 5,332 words (26.3%) disambiguation method 20,253 words (100%) is superior to the globalstatistics-based disambiguation method." ></td>
	<td class="line x" title="74:184	Note that in the local-statistics-based disambiguation method, we resorted to the globalstatistics when local-statistics were not available." ></td>
	<td class="line x" title="75:184	The percentage of cases the local-statistics were not available was 25.1 percent." ></td>
	<td class="line x" title="76:184	(Kobayasi et al. 1994) proposed a disambiguation method using collocation information and semantic categories, and reported that the structure of compound nouns was disambiguated at 83% accuracy." ></td>
	<td class="line x" title="77:184	Note that their accuracy was calculated for compound nouns including unambiguous compound nouns, i.e. those consisting of only two words." ></td>
	<td class="line x" title="78:184	If it were calculated for com-." ></td>
	<td class="line x" title="79:184	pound nouns consisting three or more words, it would be less than that of our method." ></td>
	<td class="line x" title="80:184	Thus, we can conclude that our local-statistics-based method compares quite well with rather sophisticated previous methods." ></td>
	<td class="line x" title="81:184	2.3 Prototype and an experiment We implemented a prototype thesaurus generator in which the local-statistics-based method was used to disambiguate the structure of compound nouns." ></td>
	<td class="line x" title="82:184	Using this thesaurus generator, we got a thesaurus consisting of 38,995 terms froln a 61M Byte corpus consisting of almost 48,000 articles in the financial pages of a Japanese newspaper." ></td>
	<td class="line x" title="83:184	In this experiment, the threshold for occurfence frequencies of terms in the term extraction step was set to 10, and the window size in the cooccurrence data extraction step was set to 25." ></td>
	<td class="line x" title="84:184	406 The abow+' rtm took 5.4 hours on a HP9000 C200 workstation." ></td>
	<td class="line x" title="85:184	The tlarouglaput is tolerable from a practical point of view." ></td>
	<td class="line x" title="86:184	We should also note that a thesaurus can be updated as efficiently as it can be initially generated." ></td>
	<td class="line x" title="87:184	Because \ve can run the first two steps (extraction of terms and extraction of co-occurrence data) in accumulative fashion, and we only need to run the third step over again when a considerable amount of terms and co-occurrence data are accunmlated." ></td>
	<td class="line x" title="88:184	3 Navigation in an Association ThesauFUS 3.1 Purpose and outline of the proposed thesaurus navigator A big problem with toclafs information retrieval systems based on search techniques is that they require users, who may not know exactly what thcy arc looking for, to explicitly describe their information needs." ></td>
	<td class="line x" title="89:184	Another problem is that mismatched vocabularies between users and the corpus would bring poor retrieval results." ></td>
	<td class="line x" title="90:184	To solve these problems, we propose a corptts-~ dependent association-thesaurus navigator enabling users to efficiently explore information through a corpus." ></td>
	<td class="line x" title="91:184	Users' requirements are summarized as fob lows: They want to grasp the overall information structure of a domain." ></td>
	<td class="line x" title="92:184	They want to know what topics or sub." ></td>
	<td class="line x" title="93:184	domains arc contained in the corpus." ></td>
	<td class="line x" title="94:184	They want to know terms that appropriately describe their w~gue information needs." ></td>
	<td class="line x" title="95:184	To meet the above requirements, our pro,posed thesaurus navigator has novel functions such as clustering of related terms, generation of a thesaurus overview, and zoom-in on a sub-domain of interest." ></td>
	<td class="line x" title="96:184	A conceptual image of thesaurus navigation using these ftmctions is shown in Fig." ></td>
	<td class="line x" title="97:184	2." ></td>
	<td class="line x" title="98:184	A typical informatio,> exploration session proceeds as follows." ></td>
	<td class="line x" title="99:184	At the beginning, the system displays an overview of a corpus-dependent thesaurus so that users can easily enter the information space of the corpus." ></td>
	<td class="line x" title="100:184	The overview is a kind of summary of the corpus, it consists of clusters of generic terms of the domain, and makes it easy to understand what topics or sub-domains are contained in the corpus." ></td>
	<td class="line x" title="101:184	Looking at the thesaurus overThesaurtts overview '  O Generic term Zoom-in Zoom-in  //  Specific term Fig." ></td>
	<td class="line x" title="102:184	2." ></td>
	<td class="line x" title="103:184	Conceptual image of thesaurus navigation." ></td>
	<td class="line x" title="104:184	view, the users can select one or a few term clusters they have interest in, and the screen will zoom in on the cluster(s)." ></td>
	<td class="line x" title="105:184	The zoomed view consists of a nulnber of clusters, each including more specific terms than those in the overview." ></td>
	<td class="line x" title="106:184	Users can repeat this zoom-in operation until they reach term clusters representing sufficiently specific topics." ></td>
	<td class="line x" title="107:184	3.2 Functions of the thesaurus navigator 3.2.1 Clustering of related terms We made a preliminary experiment to evaluate standard agglomerative clustering algorithms including the single-linkage method, the con> pletedinkage method, and the group-averagelinkage method (Eldqamdouchi and Willett 1989)~ Among them, the group--average-linkage method resulted in the best results, ttowever, several potential clusters tended to merge into a large one when we repeated the merge operation until a predetermined number of clusters were obtained." ></td>
	<td class="line x" title="108:184	Accordingly, we use the group-average-linkage method with an upper limit on the size of a cluster." ></td>
	<td class="line x" title="109:184	3.2.2 Generation era thesaurus overview Our method tot generating a thesaurus overview consists of major-term extraction and term clustering." ></td>
	<td class="line x" title="110:184	The m~0or-term extracting algorithm, which is carried out beforehand in batch mode, is described below." ></td>
	<td class="line x" title="111:184	See 3.2.1 for the term clustering algorithnl." ></td>
	<td class="line x" title="112:184	An overview of the thesaurus should consist of generic terms included in the corpus, flowever, we do not have a definite criterion for get> eric terms." ></td>
	<td class="line x" title="113:184	So we collect m~oor terms from the corpus as follows." ></td>
	<td class="line x" title="114:184	The number of m~!jor terms, 407 denoted by M below, was set to 300 in the prototype." ></td>
	<td class="line x" title="115:184	i) Determine a characteristic term set for each doculnent." ></td>
	<td class="line x" title="116:184	Calculate the weight w~j of term tj for document 4 according to the tf-idf (term frequency inverse document frequency) formula." ></td>
	<td class="line x" title="117:184	Then select the first re(i) terms in the descending order of u, u for each document d,, where re(i), the number of characteristic terms for document 4, is set to 20% of the total number of distinct terms in 4." ></td>
	<td class="line x" title="118:184	It is also limited to between 5 and 50." ></td>
	<td class="line x" title="119:184	ii) Select major terms in the corpus." ></td>
	<td class="line x" title="120:184	Select the first M terms in the descending order of the frequency of being contained in the characteristic term sets 3.2.3 Zoom-in on a term cluster of interest Our method for zooming in on a term cluster consists of term-set expansion and term cluster~ ing." ></td>
	<td class="line x" title="121:184	The term-set expanding algorithm is de~ scribed below." ></td>
	<td class="line x" title="122:184	See 3.2.1 for the term clustering algorithm." ></td>
	<td class="line x" title="123:184	A user-specified term set To = {t~, 6  t,,,} is expanded into a term set T,." ></td>
	<td class="line x" title="124:184	consisting of M terms as follows." ></td>
	<td class="line x" title="125:184	M was set to 300 in the prototype." ></td>
	<td class="line x" title="126:184	i) Set the initial value of 7',." ></td>
	<td class="line x" title="127:184	to 7',,." ></td>
	<td class="line x" title="128:184	ii) While IT,.I< M for i = 1, 2  do; While IE, I < Mforj = 1,2,  , m do; Add the tenn having the i-th highest correlation with tj to T,,; end; end; The reason why the above-described procedure implements the zoom-in is that generic terms tend to have higher correlation with semigeneric terms than with specific terms." ></td>
	<td class="line x" title="129:184	Assuming that high-frequency terms are generic and low-frequency terms are specific, we examined the distribution of terms by the distance from the major terms and the average occurrence frequency of terms for each distance." ></td>
	<td class="line x" title="130:184	Here the distance is the length of the shortest path in a graph that is obtained by connecting every pair of associated terms with an edge." ></td>
	<td class="line x" title="131:184	Table 2 shows the results for the example thesaurus mentioned in 2.3." ></td>
	<td class="line x" title="132:184	According to it, the average occurrence frequency decreases with the distance from the major terms." ></td>
	<td class="line x" title="133:184	Therefore, starting from an overview, our method is likely to produce more and more specific views." ></td>
	<td class="line x" title="134:184	3.3 Prototype and an experiment We developed a prototype as a client/server system." ></td>
	<td class="line x" title="135:184	The thesaurus navigator is available on WWW browsers." ></td>
	<td class="line x" title="136:184	It also has an interface to text-retrieval engines, through which a term cluster is transferred as a query." ></td>
	<td class="line x" title="137:184	Test use was made with the example thesaurus mentioned in 2.3." ></td>
	<td class="line x" title="138:184	The response time for the zoom-in operation during the navigation sessions was about 8 seconds." ></td>
	<td class="line x" title="139:184	This is acceptable given the rich information provided by the clustered view." ></td>
	<td class="line x" title="140:184	Note that the response time is almost independent of the size of the thesaurus or corpus, because the number of temls to be clustered is always constant, as described in 3.2.2 and 3.2.3~ An example from navigation sessions is shown in Fig." ></td>
	<td class="line x" title="141:184	3." ></td>
	<td class="line x" title="142:184	It demonstrates the useflflness of the corpus-dependent thesaurus navigation as a front-end for text retrieval." ></td>
	<td class="line x" title="143:184	The effectiveness of our thesaurus navigator is summarized as folo lows." ></td>
	<td class="line x" title="144:184	Improved accessibility to text retrieval systems: Users are not necessarily required to input terms to describe their information need." ></td>
	<td class="line x" title="145:184	They need only select from among terms pre~ sented on the screen." ></td>
	<td class="line x" title="146:184	This makes text re-." ></td>
	<td class="line x" title="147:184	trieval systems accessible even for those having vague information needs, or those unfamiliar with the domain." ></td>
	<td class="line x" title="148:184	Improved navigation efficiency: The unit of users' cognition is a topic rather than a term." ></td>
	<td class="line x" title="149:184	That is, they can recognize a topic from a cluster of terms at a glance." ></td>
	<td class="line x" title="150:184	Therefi)re, they can efficiently navigate through an information space." ></td>
	<td class="line x" title="151:184	Table 2 Distribution of terms by distance from major terms." ></td>
	<td class="line x" title="152:184	Distance flom major terms 0 1 2 Number of terms 245 4278 23832 Average occurrence frequency 2642.1 318.6 61.9 3 4 5 10408 149 l 82 10.6 7.7 9.0 408 I:~#-k, 99 Y,# -' '' ' '=' /I\['C~ :::Z ::::::::::::::::::::::::::: :/ ' :':' (a) Thesaurus overview 1'~ ~9~_/.x 9D 27,9 -' 2'~I~\]J!cx 72-'_ fAY \]'Z:-E.~ :iLtF'kMZ J,ff,'ib' 9,ft)." ></td>
	<td class="line x" title="153:184	v'~t/t!." ></td>
	<td class="line x" title="154:184	7,5_I r a~,:?: :,5,&5;: i!17.ii8 :~!~.I~fi ~< ~qT." ></td>
	<td class="line x" title="155:184	>g .'JJ/g i~;?" ></td>
	<td class="line x" title="156:184	E'~ 3:i~ /'l'.,'itR ;C,'i::i' '_Z tA: ki~':\[': ~LL.rJ.=\] 2>EU i::,.~:'\]:::~: ;~-:,':::,: i~Li~." ></td>
	<td class="line x" title="158:184	r~;' +." ></td>
	<td class="line x" title="159:184	't :f' Z ?0 ~t ? ~le'~ ' ' ~ I'*~a,-m/I':' ~ '~I o  : ' ' '~7:   (b) Zoom-in F ~-)Ea?':~A rivaL752 ~ oEo~ ~ aLW.eE tlffa ~ Y_22_rDjtm~~ f~ 7ai~.NL~ ;r_u'2-?.,in 97,_J (c) Further zoom-in An overview of the thesaurus was di.sTJko,ed." ></td>
	<td class="line x" title="161:184	Then the user selected the.fi/ih and seventh cluste#w which he was interested in: {China, col!\[L'rence, Asia, cooperation, meeting, Vietnam, region, development, technology, environment}, and/economy export, aid, toward, summit, Soviet Union, debt, Russia, reconstruction}." ></td>
	<td class="line x" title="162:184	This means that the user was interested in 'development assistance to developing countries or areas '." ></td>
	<td class="line x" title="163:184	The Jil?h aml seventh cluste~w J)'om (a) were shown close up, and clustelw indicating more .Vwc~/ic domains were presented." ></td>
	<td class="line x" title="164:184	The user couM undelwtand which topics the respective clustelw suggested: 'Economic assis'tance for the development of the AsiaPat(lie region ', 'Global environmental problems ', 'bTternational debt problems ', ''Mattetw on China ', 'Energy resource development', and so on." ></td>
	<td class="line x" title="165:184	Since he wax e.vwcially interested in 'International debt problems ', he selected the third cluster {debt, Egypt, Paris Club, creditor nation, q\[licial debt, de/'erred, Poland, .fin'eign debt, reimbursement, Paris, club, pro,merit, .fi, reign}." ></td>
	<td class="line x" title="166:184	The third cluster fi'om (I 0 was shown close up." ></td>
	<td class="line x" title="167:184	The resulting screen gave the user a choice el'many sT~ecific terms relevant to 'htternational debt problems ', although not all oJ'the clustel:v indicated spec!/ic topics." ></td>
	<td class="line x" title="168:184	The user was able to retrieve documents by simply selectin terms o/ interest./i'om those displayed on the screen." ></td>
	<td class="line x" title="169:184	Fig." ></td>
	<td class="line x" title="170:184	3." ></td>
	<td class="line x" title="171:184	Example of thesaurus navigation." ></td>
	<td class="line x" title="172:184	409 4 Comparison with related work Let us make a briefcolnparison with related work." ></td>
	<td class="line x" title="173:184	Both scatter/gather document clustering (Cutting et al. 1992; Hearst and Pedersen 1996) and Kohonen's self-organizing map (Lin et al. 1991; Lagus et al. 1996; Kohonen 1998) enable exploration through a corpus." ></td>
	<td class="line x" title="174:184	While they treat a corpus as a collection of documents, we treat it as a collection of terms." ></td>
	<td class="line x" title="175:184	Therefore our method can elicit finer information structure than these previous methods, and moreover, it can be applied to a corpus that includes multi-topic doculnents." ></td>
	<td class="line x" title="176:184	Our method compares quite well with the previotis methods for throughput and response time." ></td>
	<td class="line x" title="177:184	5 Conclusion We demonstrated the feasibility of automatic generation of an association thesaurus from a corpus." ></td>
	<td class="line x" title="178:184	The proposed thesaurus generation method consists of extracting terms and cooccurrence data from a corpus and analyzing the correlation between terms statistically." ></td>
	<td class="line x" title="179:184	As a component technology for thesaurus generation, a method for disambiguating the structure of compound nouns based on corpus statistics was developed and evaluated." ></td>
	<td class="line x" title="180:184	We also demonstrated the information retrieval application of an automatically generated association thesaurus." ></td>
	<td class="line x" title="181:184	A thesaurus navigator having novel functions such as term clustering, thesaurus overview, and zooming-in was developed." ></td>
	<td class="line x" title="182:184	An experiment with an association thesaurus generated from a newspaper article corpus proved that the thesaurus navigator allows us to efficiently explore information through a text corpus even when our information needs are vague." ></td>
	<td class="line x" title="183:184	Acknowledgements: This research was st, pported in part by the Next-generation Digital Library System R&D Project of MITI (Ministry of International Trade and Industry), IPA (Inlbrmation-technology Promotion Agency), and JIPDEC (Japan Information Processing Development Center)." ></td>
	<td class="line x" title="184:184	We thank Mainichi Newspapers, Ltd. for permitting us to use the CD-ROMs of the '91, '92, '93, '94 and '95 Mainichi Shimbun tbr the experiment." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C00-2128
A Statistical Approach To The Processing Of Metonymy
Utiyama, Masao;Murata, Masaki;Isahara, Hitoshi;"></td>
	<td class="line x" title="1:166	A Statistical Approach to the Processing of Metonymy Masao Utiyama, Masaki Murata, and Hitoshi Isahara Communications Research L~boratory, MPT, 588-2, Iwaoka, Nishi-ku, Kobe, Hyogo 651-2492 Japal {mutiyam~,murat~,isahara} ~crl.go.j p Abstract This paper describes a statistical approach to tile interpretation of metonymy." ></td>
	<td class="line x" title="2:166	A metonymy is received as an input, then its possible interp retations are ranked by al)t)lying ~ statistical measure." ></td>
	<td class="line x" title="3:166	The method has been tested experimentally." ></td>
	<td class="line x" title="4:166	It; correctly interpreted 53 out of 75 metonymies in Jat)anese." ></td>
	<td class="line x" title="5:166	1 Introduction Metonymy is a figure of st)eech in which tile name of one thing is substituted for that of something to which it is related." ></td>
	<td class="line x" title="6:166	The czplicit tc.~m is 'the name of one thing' and the implicit t;c~'m is 'the name of something to which it; is related'." ></td>
	<td class="line x" title="7:166	A typical examt)le of m(;tonymy is He read Shal(esl)eare." ></td>
	<td class="line x" title="8:166	(1) 'Slmkesl)(~are' is substitut(~d for 'the works of Shakespeare'." ></td>
	<td class="line x" title="9:166	'Shakest)eare' is the explicit term and 'works' is the implicit term." ></td>
	<td class="line x" title="10:166	Metonymy is pervasive in natural language." ></td>
	<td class="line x" title="11:166	The correc~ treatment of lnetonylny is vital tbr natural language l)rocessing api)lications, es1)ecially for machine translation (Kamei and Wakao, 19!)2; Fass, 1997)." ></td>
	<td class="line x" title="12:166	A metonymy may be aecel)table in a source language but unaccet)table in a target language." ></td>
	<td class="line x" title="13:166	For example, a direct translation of 'he read Mao', which is acceptable in English an(1 Japanese, is comt)letely unacceptal)le in Chinese (Kamei and Wakao, 1992)." ></td>
	<td class="line x" title="14:166	In such cases, the machine trmlslation system has to interl)ret metonynfies to generate acceptable translations." ></td>
	<td class="line x" title="15:166	Previous approaches to processing lnetonymy have used hand-constructed ontologies or semantic networks (.\]?ass, 1988; Iverson and Hehnreich, 1992; B(maud et al. , 1996; Fass, 1997)." ></td>
	<td class="line x" title="16:166	1 1As for metal)her l)rocessing, I,'errari (1996) used t;exSuch al)t)roaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very dif ficult." ></td>
	<td class="line x" title="17:166	The method outlined in this I)apcr, on the other hand, uses cortms statistics to interpret metonymy, so that ~ variety of metonynfies can be handled without using hand-constructed knowledge bases." ></td>
	<td class="line x" title="18:166	The method is quite t)romising as shown by the exl)erimental results given in section 5." ></td>
	<td class="line x" title="19:166	2 Recognition and Interpretation Two main steps, recognition and i'ntc.'q~vcration, are involved in the processing of metonyn~y (Fass, 1.!)97)." ></td>
	<td class="line x" title="20:166	in tile recognition st;el), metonylnic exl)ressions are labeled." ></td>
	<td class="line x" title="21:166	1111 the intel'l)r(:tation st;el), the meanings of those ext)ressions me int, eri)reted." ></td>
	<td class="line x" title="22:166	Sentence (1), for examl)le, is first recognized as a metonymy an(t ~Shakespeare' is identified as the explicit term." ></td>
	<td class="line x" title="23:166	't'he interpretation 'works' is selected as an implicit term and 'Shakespeare' is replaced 1)y 'the works of Shakespeare'." ></td>
	<td class="line x" title="24:166	A conq)rehensive survey by Fass (\]997) shows that the most COllllllOll metho(1 of recognizing metonymies is by selection-restriction violations." ></td>
	<td class="line x" title="25:166	Whether or not statistical approaches can recognize metonymy as well as the selectionrestriction violation method is an interesting question." ></td>
	<td class="line x" title="26:166	Our concern here, however, is the interpretation of metonymy, so we leave that question for a future work." ></td>
	<td class="line x" title="27:166	In interpretation, an implicit term (or terms) that is (are) related to the explicit term is (are) selected." ></td>
	<td class="line x" title="28:166	The method described in this paper uses corpus st~tistics for interpretation." ></td>
	<td class="line x" title="29:166	tual clues obtained through corl)us mmlysis tor detecting metal)lmrs." ></td>
	<td class="line x" title="30:166	885 This method, as applied to Japanese metonymies, receives a metonymy in a phrase of the tbnn 'Noun A Case-Marker R Predicate V' and returns a list of nouns ranked in order of the system's estimate of their suitability as interpretations of the metonylny, aSSulning that noun A is the explicit tenn. For example, given For'a a wo (accusative-case) kau (buy) (buy a Ford), Vay.sya (ear), V .st .sdl, r'uma (vehicle), etc. are returned, in that order." ></td>
	<td class="line x" title="31:166	Tile method fbllows tile procedure outlined below to interpret a inetonymy." ></td>
	<td class="line x" title="32:166	1." ></td>
	<td class="line x" title="33:166	Given a metonymy in the form 'Noun A Case-Marker R Predicate V', nouns that can 1)e syntactically related to the explicit term A are extracted from a corpus." ></td>
	<td class="line x" title="34:166	2." ></td>
	<td class="line x" title="35:166	The extracted nouns are rmlked according to their appropriateness as interpretations of the metonymy by applying a statistical measure." ></td>
	<td class="line x" title="36:166	The first step is discussed in section 3 and the second in section 4." ></td>
	<td class="line x" title="37:166	3 Information Source \e use a large corpus to extract nouns which can be syntactically related to the exl)licit term of a metonylny." ></td>
	<td class="line oc" title="38:166	A large corpus is vahmble as a source of such nouns (Church and Hanks, 1990; Brown et al. , 1992)." ></td>
	<td class="line x" title="39:166	We used Japanese noun phrases of the fornl A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al. , 1999), and occur relatively infrequently." ></td>
	<td class="line x" title="40:166	We thus also used an A near B relation, i.e. identifying tile other nouns within the target sentence, to extract nouns that may be more loosely related to A, trot occur more frequently." ></td>
	<td class="line x" title="41:166	These two types of syntactic relation are treated differently by the statistical nleasure which we will discuss in section 4." ></td>
	<td class="line x" title="42:166	The Japanese noun phrase A no B roughly corresponds to the English noun phrase B of A, lint it has a nmch broader range of usage (Kurohashi and Sakai, 1999)." ></td>
	<td class="line x" title="43:166	In fact, d no B can express most of the possible types of semmltic relation between two nouns including metonymic 2~Ford' is spelled qtSdo' ill Japanese." ></td>
	<td class="line x" title="44:166	We have used English when we spell Japanese loan-words from English for the sake of readability." ></td>
	<td class="line x" title="45:166	concepts such as that the name of a container can represent its contents and the name of an artist can imply an art~brnl (container for contents and artist for artform below).a Examples of these and similar types of metonymic concepts (Lakoff and Johnson, 1980; Fass, 1997) are given below." ></td>
	<td class="line x" title="46:166	Container for contents  glass no mizu (water)  naV (pot), y6 i (food) Artist for artform  Beethoven no kyoku (music)  Picas.so no e (painting) Object for user  ham .sandwich no kyaku (customer)  sax no .sO.sya (t)erformer) Whole tbr part  kuruma (car) no tirc  door' no knob These exalnt)les suggest that we can extract semantically related nouns by using tile A no B relation." ></td>
	<td class="line x" title="47:166	4 Statistical Measure A nletonymy 'Noun A Case-Marker R, Predicate V' can be regarded as a contraction of 'Noun A Syntactic-Relation (2 Noun B CaseMarker R Predicate V', where A has relation Q to B (Yamamoto et al. , 1998)." ></td>
	<td class="line x" title="48:166	For example, Shakc.spcare wo yomu (read) (read Shakespeare) is regarded as a contraction of Shakespeare no .sakuhin (works) 'wo yomu (read the works of Shakespeare), where A=Shake.spcare, Q=no, B=.sakuhin, R=wo, and V=yomu." ></td>
	<td class="line x" title="49:166	Given a metonymy in the fbrln A R 17, the appropriateness of noun B as an interpretation of the metonymy under the syntactic relation Q is defined by LQ(BIA,/~, V) Pr(BIA, (2, 1~, V), (2) ayamamoto et al.(\]998) also used A no /3 relation to interpret metonymy." ></td>
	<td class="line x" title="51:166	886 where Pr(.-)." ></td>
	<td class="line x" title="52:166	represents l)robal/ility and Q is either an A no B relation or an A near \]3 relation." ></td>
	<td class="line x" title="53:166	Next;, the appropriateness of noun \]3 is defined by M(BIA, Ie, V) -nlaxLc~(BIA, l~,V )." ></td>
	<td class="line x" title="54:166	(3) O We rank nouns 1)y at)plying the measure 214." ></td>
	<td class="line x" title="55:166	Equation (2) can be decomposed as follows: LQ(!31A, R,, V) = Pr(BIA, Q, R,, V) Pr(A, Q, B, R,, V) Pr( A, Q, R, v) Pr(A, Q, 13)lh'(R, VIA, Q, Ix) Pr(A, Q) Pr(R, VIA, Q) Pr(BIA, Q)Pr(R, VIB) -~ er(R, v) ' (4) where (A, O) and {\]~,, V} are assumed to l)e indel)endent of each other." ></td>
	<td class="line x" title="56:166	Let f(event)1)e the frequen(:y of an cve'nt and Classc.s(\])) be the set of semantic (:lasses to which B belongs." ></td>
	<td class="line x" title="57:166	'l'he expressions in Equation (4) are then detined t)y 4 I'r(~lA, Q) .t'(A, Q, ~x) _ f(A, Q, ~) f(A, Q) ~1~ f(A, Q, 13)' (5) Pr(~., riB) IU~,I~,v) i' ' *: .1 (U, ~, V) > 0, .~~,c~cl  (10 Pr(l)'l(/)f(C/'R'V) J'US) otherwise, ((0 Pr(BIC ) .f(13)/ICI-s.w-.XB)l j(c) (r) We onfitted Pr(H,, 17) fi'om Equation (4) whell we calculated Equation (3) in the experiment de, scribed in section 5 for the sake of simplicit> 4Strictly speaking, Equation (6) does not satist\]y X',,e,vpr(R, vl/x) -1." ></td>
	<td class="line x" title="59:166	We h~wc adopted this detinition for the sake of simplicity." ></td>
	<td class="line x" title="60:166	This simplification has little effect on the tilml results because ~--;c'cc~  (m Pr(l~lC)f(C,I~', V) << I will usually hohl." ></td>
	<td class="line x" title="61:166	More Sol)histieated methods (M;mning ml(t Schiitze, 1999) of smoothing f)robability distribution m~y I)e I)eneticial." ></td>
	<td class="line x" title="62:166	itowever, al)l)lying such methods and comparing their effects on the interpretation of metonymy is beyond the scope of this l)aper." ></td>
	<td class="line x" title="63:166	This treatment does not alter the order of the nouns ranked by the syst;em because l?r(H. , V) is a constant for a given metonymy of the form AR V. Equations (5) and (6) difl'er in their treatment of zero frequency nouns." ></td>
	<td class="line x" title="64:166	In Equation (5), a noun B such that f(A, Q, B) = 0 will l)e ignored (assigned a zero probal)ility) because it is unlikely that such a noml will have a close relationshii / with noun A. In Equation (6), on the other hand, a noun B such that f(B, R, V) = 0 is assigned a non-zero probability." ></td>
	<td class="line x" title="65:166	These treatments reflect the asymmetrical proper~y of inetonymy, i.e. ill a nletonylny of the form A 1 1~ an implicit term 13 will have a much tighter relationship with the explicit term A than with the predicate V. Consequently, a nouil \]3 such that f(A,Q, B) >> 0 A f(B, JR, V) = 0 may be appropri~te as an interpretation of the metonymy." ></td>
	<td class="line x" title="66:166	Therefore, a non-zero t)robat)ility should be assign(;d to Pr(l~., VI1X ) ev~,n it' I(B, 2e, V) ; ()." ></td>
	<td class="line x" title="68:166	~ Equation (7) is the probability that noun J3 occurs as a member of (::lass C. This is reduced to fU~) if13 is not ambiguous, i.e. IC/a,~,sc.,s,(/3)\[ = f(c) 1." ></td>
	<td class="line x" title="70:166	If it is ambiguous, then f(B) is distributed equally to all classes in Classes(B)." ></td>
	<td class="line x" title="71:166	The frequency of class C is ol)tained similarly: .f(B) (8) .f(c) = ~ ICl(-~c~(13)1' 11C-.(7 where 13 is a noun which belongs to the class C. Finally we derive f(13, ~, v) BqC (.0) In summary, we use the measure M as defined in Equation (3), and cah:ulated by applying Equation (4) to Equation (9), to rank nouns according to their apl)ropriateness as possible interpretations of a metonymy." ></td>
	<td class="line x" title="72:166	Example Given the statistics below, bottle we akeru (open) (open a bottle) will be interpreted 5The use of Equation (6) takes into account a noun/3 such that J'(l:~, l, V) = 0." ></td>
	<td class="line x" title="73:166	But, Stlch & llOtlll is usually ignored if there is another noun B' such that f(13', H. , V) > 0 be~,~,,se." ></td>
	<td class="line x" title="74:166	Eo'~ct  U~)P, USIO)J'(C,~e. ,V) << a < J'(lY, H,, V) will usually hokl." ></td>
	<td class="line x" title="75:166	This means thai the cooccurrence 1)rol)al)iliW between implicit terms and verbs are also important in eliminating inapl)rol)riate nomls." ></td>
	<td class="line x" title="76:166	887 as described in the fbllowing t)aragraphs, assuming that cap and rcizSko (refl'igerator) are the candidate implicit terms." ></td>
	<td class="line x" title="77:166	Statistics: f(bottlc, no, cap) = 1, f(bottlc, no, reizgko) = O, f(bottlc, no) = 2, f ( bottlc, ncar, cap) = 1, f (bottle, near, rciz6ko) = 2, f(bottlc, ncar) = 503, f(cap) = 478, f(rcizSko) = 1521, f(cap, wo, akcru) = 8, and f(rciz6ko, wo, akcru) = 23." ></td>
	<td class="line x" title="78:166	f(bottlc, no, rciz6ko) = 0 indicates that bottle and rcizSko are not close semantic relatives of each other." ></td>
	<td class="line x" title="79:166	This shows the effectiveness of using A no B relation to filter out loosely related words." ></td>
	<td class="line x" title="80:166	Measure: L,o(cap) Lncar(Cap) = Lno(reizSko) = Lncar ( reizS ko ) -~ f ( bott:le, no, cap) .f ( bottlc, no) \](ca,p, wo, a\]~c'ru) X 1 8 -8.3710 -3, 2 478 f (bottle, near, cap) f(bottlc, near) f ( caI), 'wo, a\]~cru) X .f ( ) 1 8 50--3 47-8 = 3.33  10 -5, .f ( bottlc, no, rcizSko ) .f ( bottlc, no) f ( rcizako, wo, ahcru )  .f ( rcizdko 0 23 2 1521 .f ( bottlc, near, rcizSko) f (bottlc, near) f(rcizSko, wo, akcru) X f ( rciz~ko ) 2 23 503 1521 6.01 x 1() -~, M(c p) = maxLno(cap),Lnea.,.(cap)} = 8.37 x lO-3, and ~r ( reizSko ) = 6.01 10 -5, where L,,o(Cap) = L,~o(Caplbo~tle, wo, akeru), M(c p) = M(c pl ot tz, and so o51." ></td>
	<td class="line x" title="82:166	Since M > M we conclude that cap is a more appropriate imt)licit term than rcizSho." ></td>
	<td class="line x" title="83:166	This conclusion agrees with our intuition." ></td>
	<td class="line x" title="84:166	5 Experiment 5.1 Material Metonymies Seventy-five lnetonymies were used in an ext)erilnent to test tile prol)osed lnethod." ></td>
	<td class="line x" title="85:166	Sixty-two of them were collected from literature oll cognitive linguistics (Yamanashi, 1988; Yamam~shi, 1995) and psycholinguistics (Kusumi, 1995) in Japanese, paying attention so that the types of metonymy were sufficiently diverse." ></td>
	<td class="line x" title="86:166	The remaining 13 metonymies were direct translations of the English metonymies listed in (Kalnei and Wakao, 1992)." ></td>
	<td class="line x" title="87:166	These 13 metonylnies are shown in Table 2, along with the results of the experiment." ></td>
	<td class="line x" title="88:166	Corpus A corpus which consists of seven years of issues of the Mainichi Newspaper (Dora 1991 to 1997) was used in the experiment." ></td>
	<td class="line x" title="89:166	The sentences in tlle cortms were mort)hologically analyzed by ChaSen version 2.0b6 (Matsumoto et al. , 1999)." ></td>
	<td class="line x" title="90:166	The corpus consists of about 153 million words." ></td>
	<td class="line x" title="91:166	Semantic Class A Japanese thesaurus, Bunrui Goi-tty6 (The N~tional Language Research Institute, 1996), was used in the experiment." ></td>
	<td class="line x" title="92:166	It has a six-layered hierarchy of abstractions and contains more than 55,000 nouns." ></td>
	<td class="line x" title="93:166	A class was defined as a set of nouns which are classified in the same abstractions in the top three layers." ></td>
	<td class="line x" title="94:166	The total nmnber of classes thus obtained was 43." ></td>
	<td class="line x" title="95:166	If a noun was not listed in the thesaurus, it was regarded as being in a class of its own." ></td>
	<td class="line x" title="96:166	888 5.2 Method '.1.11(; method we have dcseril)e,d was applied I;O the metonynfie, s (lescril)e,(t ill section 5.1." ></td>
	<td class="line x" title="97:166	Tile 1)r()eedure described 1)clew was followed in intert)rel;ing a metonynly." ></td>
	<td class="line x" title="98:166	1." ></td>
	<td class="line x" title="99:166	Given a mel,onymy of the, form :Noun A Case-Marker R Predicate, V', nouns re\]al;e(l to A 1)y A 'n,o .1:1 relation an(l/or A near H relation were extra(:ix'~(l from 1;he, corl)us described in Se(:tion 5.\]." ></td>
	<td class="line x" title="100:166	2." ></td>
	<td class="line x" title="101:166	The exl;racted llOllllS @an(lidatcs) were ranked acc()rding t() the nw, asure M d(;tined in \]{quation (3)." ></td>
	<td class="line x" title="102:166	5.3 Results The r(;sult of at)l)lying the proi)osexl me, thod to our sol; of metol~ymies is summarized in 'l'alfle 1." ></td>
	<td class="line x" title="103:166	A reasonably good result (:an 1)e s(;cn for q)oi;h r(,\]ai;ions', i.e. l;he result ot)i;aincd \])y using both A no 11 an(t d ncm' 1\] l'elal;ion~; wllen extracting nouus fl'onl th(' cOllmS, \[1'1~(', a(:(:ura(:y of q)ol;h re, l~tions', the ratio ()f lhe nllnil)er of (:orrc(:l;ly intcrl)r(;te,(1 (; t()l)-rank(;(l (:an(li(lates to l;he, total mmfl)er of m(',l;()nymies in ()it\]' set, w,,s 0.7:, (=5',Visa+22)) alld ('ol,ti(t(' l,ce inWwva.1 estimal;e was t)(;l;ween ().6\] an(t 0.8\] \e regard this result as quite t)ronfising." ></td>
	<td class="line x" title="104:166	Since the mc, i;onymies we used wcr(; g(m(u'a\]: (lomain-in(lel)(',ndca~t, on(s, l;h(~ (legr(', ~, ()f a(:curacy achi(;ve, l in this (~xp(;rim(;nt i~; likely t() t)(; r(',t)(',al;e(l when our mehod is ~q)l)lie(l t() oth(;r genural sets ()f mel;onymies." ></td>
	<td class="line x" title="105:166	'.\['~l)l(; l : tt3xl)erimental r('sults." ></td>
	<td class="line x" title="106:166	I,elal;ions used Corre(;t \'rong Both relations 53 22 Only A 'no B 50 25 Only A near 13 d3 32 Tal)le 1 also shows that 'both relations' is more ae(:ural;e than (',il;her the result obtained 1)y solely using the A no \]3 relation or the A near B relation." ></td>
	<td class="line x" title="107:166	The use of multit)le relations in mel, onyn~y int(;rl)retation is I;hus seen to l)e 1)enefieial." ></td>
	<td class="line x" title="108:166	aThe correct;hess was judged by the authors." ></td>
	<td class="line x" title="109:166	A candidat(; was judged correct when it; made sense in .Ial)anese." ></td>
	<td class="line x" title="110:166	For examl)le, we rcgard(;d bet:r, cola, all(l mizu (W;d;el') as all (:orr(!c\[; intcrl)r(~l;ations R)r glas.s we nom, u (drink) (drink a glass) because lhey llla(le,q(~llSC in some (:ontcxt." ></td>
	<td class="line x" title="111:166	Table 2 shows the, results of applying the method to the, thirteen directly translated metonymies dcscril)ed in sect;ion 5.1 Asterisks (*) in the tirst (;ohlillll indicate that direct translation of the sentences result in unaccel)table Japanes(;." ></td>
	<td class="line x" title="112:166	The, C's and W's in t;he second eohmm respectively indicate that the topranked ('andi(latcs were correct and wrong." ></td>
	<td class="line x" title="113:166	The s(;nten(:es in the l;hir(t column are the original English metonymi(;s adol)tc, d fl'om (Kamci and \akao, t992)." ></td>
	<td class="line x" title="114:166	The Japanese llletollylllies in th(: form hloun ease-lnarker predi(:ate 7', in the fourth column, are the illputs I;o the method." ></td>
	<td class="line x" title="115:166	In this ('ohunn, we and 9 a mainly r(;present I;he ac(:usal;ive-casc and nominative-ease, reSl)ectively." ></td>
	<td class="line x" title="116:166	The nouns listed in the last eolmnn m'e the tot) three candidates, in order, according to the." ></td>
	<td class="line x" title="117:166	measure M that was defined ill Equation (3)." ></td>
	<td class="line x" title="118:166	Th(,,se, l'csull;s (lemonstrate the et\[~(:tiveness of lhe m(',thod." ></td>
	<td class="line x" title="119:166	'.l>n out of t;11(: 13 m(;tonynfies w(u'c intc, rt)rete,(l (:orre, ctly." ></td>
	<td class="line x" title="120:166	Moreover, if we rcsl;ri(:t our al;l;(',nti()n to the ten nietonylHics i}mt m'e a(:(:Cl)tal)le, ill,/al)anese, all l)ut one w(;rc, inl;('rl)r(;te(t (:orrectly." ></td>
	<td class="line x" title="121:166	The a(:curacy was 0.9 ---(/)/\]0), higher than that for q)oth relations' in Tal)le i. The reason fi)r the higher degl'ee of ac(:tlra(;y is l;\]lal; the lll(;|;Ollyllli(;s in Tal)le 2 arc semi,what tyi)ical and relativ(;ly easy to int(~rl)rel;, while, the lnel;(nlynlics (:olle(:l;c(t fl'()m,lal)anese sour(:es included a (liversity of l;yl)es and wcr(~ more difficult to intext)let." ></td>
	<td class="line x" title="122:166	Finally, 1;11(', efl'ecl;iv(umss of using scnlanl;i(: classes is discussed." ></td>
	<td class="line x" title="123:166	The, l;op candidates ot!" ></td>
	<td class="line x" title="124:166	six out of the 75 metonynfies were assigned their al)prot)riatenc, ss by using their semantic classes, i.e. the wducs of 1;11o measure 114 was calculated with f(H,/~, V) = 0 in lgquat;ion (6)." ></td>
	<td class="line x" title="125:166	Of the, se, l;hrce were corrccl,." ></td>
	<td class="line x" title="126:166	011 l;hc, other hand, if scmanl;ic class is not use(l, then three of the six are still COITeC|;." ></td>
	<td class="line x" title="127:166	Here there was no lint)rovemerit." ></td>
	<td class="line x" title="128:166	However, when we surveyed the results of the whole experiment, wc found that nouns for wlfich .fiB, R,, V) -0 often lind (:lose relationship with exl)licit terms ill m(;tonynfics and were al)propriate as interpretations of the metonynfics." ></td>
	<td class="line x" title="129:166	We need more research betbre we (:an ju(lgc the etl'ectivc, ness of utilizing semantic classes." ></td>
	<td class="line x" title="130:166	rPl'edicatcs are lemmatized." ></td>
	<td class="line x" title="131:166	889 Table 2: Results of applying the proposed lnethod to direct translations of the metonymies in (Kanmi and Wakao, 1992)." ></td>
	<td class="line x" title="132:166	Sentences Noun Case-Mm'l~er Pred." ></td>
	<td class="line x" title="133:166	Candidates C Dave drank the glasses." ></td>
	<td class="line x" title="134:166	C The .kettle is boiling." ></td>
	<td class="line x" title="135:166	C Ile bought a Ford." ></td>
	<td class="line x" title="136:166	C lie has got a Pieasso in his room." ></td>
	<td class="line x" title="137:166	C Atom read Stcinbeck." ></td>
	<td class="line x" title="138:166	C C W C W C Ted played J3ach." ></td>
	<td class="line x" title="139:166	Ite read Mao." ></td>
	<td class="line x" title="140:166	We need a couple of strong bodies tbr our team." ></td>
	<td class="line x" title="141:166	There a r___q a lot of good heads in the university." ></td>
	<td class="line x" title="142:166	Exxon has raised its price again." ></td>
	<td class="line x" title="143:166	glass we nomu yakan ga waku Ford we kau Picasso we motu Stcinbcck we yomu Bach we hiku Mao we yomu karada ga hituy5 atama ga iru Exxon 9 a agcru Washington is insensitive to the needs of the people." ></td>
	<td class="line x" title="144:166	Washington ga musinkci C The T.V. said it was very crowded at; the festival." ></td>
	<td class="line x" title="145:166	W The sign said fishing was prohibited here." ></td>
	<td class="line x" title="146:166	T. V. 9a in hy&siki ga iu beer, cola, mizu (water) yu (hot water), oyu (hot water), nett5 (boiling water) zy@Ssya (car), best seller, kuruma (vehicle) c (painting), image, aizin (love,') gensaku (original work), mcisaku (fmnous story), daihySsaku (important work) mcnuetto (minuet), kyoku (music), piano si (poem), tyosyo (writings), tyosaku (writings) carc, ky~tsoku (rest;), kaigo (nursing) hire (person),tomodati (friend), bySnin (sick person) Nihon ( Japan),ziko (accident), kigy5 (company) zikanho (assistant vice-minister), scikai (political world), 9ikai (Congress) cotnlllentgtol'~ anllOllllcer I (:~stel' mawari (surrmmding), zugara (design) .seibi (lnaintclmnce) 6 Discussion Semantic Relation The method proposed in this pnper identifies implicit terms fbr tile explicit term in a metonymy." ></td>
	<td class="line x" title="147:166	However, it is not concerned with the semantic relation between an explicit; term and implicit term, because such semantic relations are not directly expressed ill corpora, i.e. noun phrases of the form A no B can be found in corpora bul; their senmntic relations are not." ></td>
	<td class="line x" title="148:166	If we need such semantic relations, we must semantically analyze the noun phrases (Kurohashi and Sakai, 1999)." ></td>
	<td class="line x" title="149:166	Applicability to other languages Japanese noun phrases of the form A no B are specitie to Japanese." ></td>
	<td class="line x" title="150:166	The proposed method, however, could easily be extended to other languages." ></td>
	<td class="line x" title="151:166	For exmnple, in English, noun phrases B of d could be used to extract semantically related nouns." ></td>
	<td class="line x" title="152:166	Nouns related by is-a relations or part-of relations could also be extracted from corpora (Hearst, 1992; Berland and Charniak, 1999)." ></td>
	<td class="line x" title="153:166	If such semantically related nouns are extracted, then they can be ranked according to the measure M defined in Equation (3)." ></td>
	<td class="line x" title="154:166	Lexically based approaches Generative Lexicon theory (Pustejovsky, 1995) proposed the qualia structure which encodes semantic relations among words explicitly." ></td>
	<td class="line x" title="155:166	It is useflfl to infer an implicit term of the explicit term in a metonymy." ></td>
	<td class="line x" title="156:166	The proposed approach, on the other hand, uses corpora to infer implicit terms and thus sidesteps the construction of qualia structure." ></td>
	<td class="line x" title="157:166	8 7 Conclusion This paper discussed a statistical approach to the interpretation of metonymy." ></td>
	<td class="line x" title="158:166	The method tbllows the procedure described below to interpret a metonymy in Japanese: 1." ></td>
	<td class="line x" title="159:166	Given a metonymy of the tbrm 'Noun A SBriscoe et al.(1990) discusses the use o1' machinereadable dictionaries and corpora for acquMng lexical semantic information." ></td>
	<td class="line x" title="161:166	890 Case-Marker 1 Predicate V', nouns that are syntactically related to the explicit terlll A are extracted front a corpus." ></td>
	<td class="line x" title="162:166	'.2." ></td>
	<td class="line x" title="163:166	The extracted nouns are ranked according to their degree of appropriateness as interpretations of the metonymy by applying a statistical measure." ></td>
	<td class="line x" title="164:166	The method has been tested experimentally." ></td>
	<td class="line x" title="165:166	Fifty-three out of seventy-five metonymies were correctly interpreted." ></td>
	<td class="line x" title="166:166	This is quite a prolnising first; step towm'd the statistical processing of metonymy." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J00-3001
Extracting The Lowest-Frequency Words: Pitfalls And Possibilities
Weeber, Marc;Baayen, R. Harald;Vos, Rein;"></td>
	<td class="line x" title="1:319	Extracting the Lowest-Frequency Words: Pitfalls and Possibilities Marc Weeber* University of Groningen R. Harald Baayen* Max Planck Institute for Psycholinguistics Rein Vos t University of Groningen, University of Maastricht In a medical information extraction system, we use common word association techniques to extract side-effect-related terms." ></td>
	<td class="line x" title="2:319	Many of these terms have a frequency of less than five." ></td>
	<td class="line x" title="3:319	Standard word-association-based applications disregard the lowest-frequency words, and hence disregard useful information." ></td>
	<td class="line x" title="4:319	We therefore devised an extraction system for the full word frequency range." ></td>
	<td class="line x" title="5:319	This system computes the significance of association by the log-likelihood ratio and Fisher's exact test." ></td>
	<td class="line x" title="6:319	The output of the system shows a recurrent, corpus-independent pattern in both recall and the number of significant words." ></td>
	<td class="line x" title="7:319	We will explain these patterns by the statistical behavior of the lowest-frequency words." ></td>
	<td class="line x" title="8:319	We used Dutch verb-particle combinations as a second and independent collocation extraction application to illustrate the generality of the observed phenomena." ></td>
	<td class="line x" title="9:319	We will conclude that a) word-association-based extraction systems can be enhanced by also considering the lowest-frequency words, b) significance levels should not be fixed but adjusted for the optimal window size, c) hapax legomena, words occurring only once, should be disregarded a priori in the statistical analysis, and d) the distribution of the targets to extract should be considered in combination with the extraction method." ></td>
	<td class="line x" title="10:319	1." ></td>
	<td class="line x" title="11:319	Introduction The research reported here arose from an attempt to determine the conditions under which optimal recall and precision are obtained for the extraction of terms related to side effects of drugs in medical abstracts." ></td>
	<td class="line x" title="12:319	We used the standard technique of defining a window around a seed term, side-effect in our case, and selected as potentially relevant terms those words that appeared more often in these windows than expected under chance conditions." ></td>
	<td class="line x" title="13:319	Our original question concerned the extent to which recall and precision are influenced by the size of the window." ></td>
	<td class="line x" title="14:319	It turns out, however, that a preliminary question needs to be answered first, namely, how to gauge the significance of the large effect of the lowest-frequency words on recall, precision, and the number of words extracted as potentially relevant terms." ></td>
	<td class="line x" title="15:319	* Groningen University Institute for Drug Exploration, Department of Social Pharmacy and Pharmacoepidemiology, Ant." ></td>
	<td class="line x" title="16:319	Deusinglaan 1, 9713 AV Groningen, The Netherlands." ></td>
	<td class="line x" title="17:319	E-mail: marc@farm.rug.nl t Faculty of Health Sciences, Department of Health Ethics and Philosophy, P.O. Box 616, 6200 MD Maastricht, The Netherlands." ></td>
	<td class="line x" title="18:319	E-mail: rein.vos@zw.unimaas.nl :~ Max Planck Institute for Psycholinguistics, P.O. Box 310, 6500 AH Nijmegen." ></td>
	<td class="line x" title="19:319	E-mail: baayen@mpi.nl (~) 2000 Association for Computational Linguistics Computational Linguistics Volume 26, Number 3 o.i Z 150 100 50 0 I \]lrll,,,.,, 5 10 15 20 0.15 Z 0.10 Z 0.05 5 10 15 20 Frequency Class Frequency Class (a) (b) Figure 1 Frequency distribution of medical expert word types." ></td>
	<td class="line x" title="21:319	Panel (a) shows the number of side-effect-related word types as judged by a medical expert (Nexpert) as a function of the first 23 frequency classes." ></td>
	<td class="line x" title="22:319	Panel (b) shows the proportion of expert types/total corpus types (Ntotal) for the first 23 frequency classes." ></td>
	<td class="line x" title="23:319	The horizontal dashed line indicates the mean proportion of 0.0619." ></td>
	<td class="line x" title="24:319	It is common practice in information retrieval to discard the lowest-frequency words a priori as nonsignificant (Rijsbergen 1979)." ></td>
	<td class="line x" title="25:319	In Smadja's collocation algorithm Xtract, the lowest-frequency words are effectively discarded as well (Smadja 1993)." ></td>
	<td class="line oc" title="26:319	Church and Hanks (1990) use mutual information to identify collocations, a method they claim is reasonably effective for words with a frequency of not less than five." ></td>
	<td class="line o" title="27:319	A frequency threshold of five seems quite low." ></td>
	<td class="line n" title="28:319	Unfortunately, even this lower frequency threshold of five is too high for the extraction of side-effect-related terms from our medical abstracts." ></td>
	<td class="line x" title="29:319	To see this, consider the left panel of Figure 1, which plots the number of side-effect-related words in our corpus of abstracts as judged by a medical expert, as a function of word-frequency class." ></td>
	<td class="line x" title="30:319	The side-effect-related words with a frequency of less than five account for 295 of a total of 432 expert words (68.3%)." ></td>
	<td class="line x" title="31:319	The right panel of Figure 1 shows that the first 23 word-frequency classes are characterized by, on average, the same proportion of side-effect-related words." ></td>
	<td class="line x" title="32:319	The a priori assumption of Rijsbergen (1979) that the lowest-frequency words are nonsignificant is not warranted for our data, and, we suspect not for many other data sets as well." ></td>
	<td class="line x" title="33:319	The recent literature has seen some discussion of the appropriate statistical methods for analyzing the contingency tables that contain the counts of how a word is distributed inside and outside the windows around a seed term." ></td>
	<td class="line x" title="34:319	Dunning (1993) has called attention to the log-likelihood ratio, G 2, as appropriate for the analysis of such contingency tables, especially when such contingency tables concern very low frequency words." ></td>
	<td class="line x" title="35:319	Pedersen (1996) and Pedersen, Kayaalp, and Bruce (1996) follow up Dunning's suggestion that Fisher's exact test might be even more appropriate for such contingency tables." ></td>
	<td class="line x" title="36:319	We have therefore investigated for the full range of word frequencies whether there is an optimal window size with respect to recall and the number of significant words extracted using both the log-likelihood ratio and Fisher's exact test." ></td>
	<td class="line x" title="37:319	In Section 2, we will show that indeed there seems to be an optimal window size for both statistical tests." ></td>
	<td class="line x" title="38:319	However, a recurrent pattern of local optima calls this conclusion into question." ></td>
	<td class="line x" title="39:319	Upon closer inspection, this recurrent pattern appears at fixed ratios of the number of words inside the window to the number of words outside the window (complement)." ></td>
	<td class="line x" title="40:319	302 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words In Section 3, we will relate the recurrent patterns of local optima at fixed windowcomplement ratios (henceforth W/C-ratios) to the distributions of the lowest-frequency words over window and complement." ></td>
	<td class="line x" title="41:319	We will call attention to the critical effect of the choice of W/C-ratios on the significance of the lowest-frequency words." ></td>
	<td class="line x" title="42:319	As the improvement in the extraction of side-effect terms from medical abstracts, as gauged by the F-measure, which combines recall and precision (Rijsbergen 1979), is small, we also applied the same approach to the extraction of Dutch verb-particle combinations from a newspaper corpus." ></td>
	<td class="line x" title="43:319	In Section 4, we report substantially better results for this more lexical extraction task, which is subject to the same statistical behavior of the lowest-frequency words." ></td>
	<td class="line x" title="44:319	In the last section, we will discuss the consequences of our findings for the optimization of word-based extraction systems and collocation research with respect to the lowest-frequency words." ></td>
	<td class="line x" title="45:319	2." ></td>
	<td class="line x" title="46:319	An Optimal Window Size for Medical Abstracts?" ></td>
	<td class="line x" title="47:319	The MEDLINE bibliographic database contains a large number of abstracts of scientific journal papers discussing medical and drug-related research." ></td>
	<td class="line x" title="48:319	Typically, abstracts discussing medical drugs mention the side effects of these drugs briefly." ></td>
	<td class="line x" title="49:319	Information on side effects is potentially relevant for finding new applications for existing drugs (Rikken and Vos 1995)." ></td>
	<td class="line x" title="50:319	We are therefore interested in any terms related to the side effects of drugs." ></td>
	<td class="line x" title="51:319	Before proceeding, it may be useful to clarify the way in which the present research differs from standard research on collocations." ></td>
	<td class="line x" title="52:319	In the latter kind of research, there is no a priori knowledge of which combinations of words are true collocations." ></td>
	<td class="line x" title="53:319	Moreover, the most salient collocations generally are found at the top of a list ranked according to measures for surprise or association, such as G 2 or mutual information (Manning and Sch~itze 1999)." ></td>
	<td class="line x" title="54:319	The large numbers of word combinations with significant but low values for these measures are often of less interest." ></td>
	<td class="line x" title="55:319	Low-frequency words are predominant among these kinds of collocations." ></td>
	<td class="line x" title="56:319	In our research, we likewise find many low-frequency terms for side effects with low ranks in medical abstracts." ></td>
	<td class="line x" title="57:319	The relatively well-known side effects that are mentioned frequently can be captured by examining the top ranks in the lists of extracted words." ></td>
	<td class="line x" title="58:319	At the same time, the rarely mentioned side-effect terms are no less important, and in post marketing surveillance the extraction of such side-effect terms may be crucial for the acceptance or rejection of new medicines." ></td>
	<td class="line x" title="59:319	Is reliable automatic extraction of both lowand high-frequency side-effect terms from MEDLINE abstracts feasible?" ></td>
	<td class="line x" title="60:319	To answer this question, we explored the efficacy of a standard collocation-based term extraction method that extracts those words that appear more frequently in the immediate neighborhood of a given seed term than might be expected under chance conditions." ></td>
	<td class="line x" title="61:319	We compiled two corpora on the side effects of the cardiovascular drugs captopril and enalapril from MEDLINE abstracts." ></td>
	<td class="line x" title="62:319	The first corpus contains all abstracts mentioning captopril and the word side." ></td>
	<td class="line x" title="63:319	The second corpus contains all abstracts mentioning captopril and at least one of the compounds side-effect, side effect, side-effects, and side effects." ></td>
	<td class="line x" title="64:319	Thus, the second corpus is a subset of the first." ></td>
	<td class="line x" title="65:319	The first corpus is comprised of 118,675 tokens and 7,678 types; the second corpus 103,603 tokens and 6,582 types." ></td>
	<td class="line x" title="66:319	A medical expert marked 432 of the latter word types as side-effect-related terms." ></td>
	<td class="line x" title="67:319	The left panel of Figure 1 summarizes the head of the frequency distribution of these terms in the larger corpus." ></td>
	<td class="line x" title="68:319	Note that most side-effect-related terms have a frequency lower 303 Computational Linguistics Volume 26, Number 3 Table 1 General 2x2 contingency table." ></td>
	<td class="line x" title="69:319	A = frequency of the target in the window corpus, B = frequency of the target in the complement corpus, W = total number of words in the window, C = total number of words in the complement." ></td>
	<td class="line x" title="70:319	Corpus size N = W + C. window complement frequency of target A B sum frequency of other words W A C B W C A+B W+C-A-B W+C than five." ></td>
	<td class="line x" title="71:319	What we need, then, is an extraction method that is sensitive enough to select such very low frequency terms." ></td>
	<td class="line x" title="72:319	In the collocation-based method studied here, the neighborhood of a given seed term is defined in terms of a window around the seed term." ></td>
	<td class="line x" title="73:319	We constructed windows around all seed terms in the corpus, leading to a window corpus and a complement corpus." ></td>
	<td class="line x" title="74:319	The window corpus contains all words that appear within a given window size of the seed term." ></td>
	<td class="line x" title="75:319	For instance, with a window size of 10, any word appearing from five words before the seed to five words after the seed as well as the seed itself is included in the window corpus." ></td>
	<td class="line x" title="76:319	The word tokens not in the window corpus comprise the complement corpus." ></td>
	<td class="line x" title="77:319	Any type in the window corpus is a potential side-effectrelated term." ></td>
	<td class="line x" title="78:319	For any such target type, we tabulate its distribution in window and complement corpora in a contingency table like Table 1." ></td>
	<td class="line x" title="79:319	Given W and C, we need to know whether the frequency of the target in the window corpus, A, is high enough to warrant extraction." ></td>
	<td class="line x" title="80:319	Typically, given the marginal B and distribution of the contingency table, a target is extracted for which wA--~A > ~-2-~, for which the tabulated distribution is nonhomogeneous according to tests such as G 2 and Fisher's exact test for a given cMevel." ></td>
	<td class="line x" title="81:319	In this approach, the window size is a crucial variable." ></td>
	<td class="line x" title="82:319	At small window sizes, many potentially relevant terms fail to appear in the window corpus." ></td>
	<td class="line x" title="83:319	However, at large window sizes, many irrelevant words are found in the window corpus and may be extracted spuriously." ></td>
	<td class="line x" title="84:319	To see to what extent window size may affect the results of the extraction procedure, consider the solid lines in panels (a) and (b) of Figure 2." ></td>
	<td class="line x" title="85:319	The left panel shows the results for recall when we use the log-likelihood ratio, G 2, the right panel the results for Fisher's exact test." ></td>
	<td class="line x" title="86:319	We define recall as the proportion of the number of side-effect words extracted and the total number of side-effect words available in the window." ></td>
	<td class="line x" title="87:319	For both statistical tests, recall seems to be optimal at window size 2." ></td>
	<td class="line x" title="88:319	However, at this window size, the number of words extracted is very small." ></td>
	<td class="line x" title="89:319	This can be seen in panels (c) and (d)." ></td>
	<td class="line x" title="90:319	Considered jointly, panels (a) and (c) suggest an optimal window size of 24 for our larger corpus (corpus 1), as recall is still high, and the number of significant words is maximal." ></td>
	<td class="line x" title="91:319	When Fisher's exact test is used instead of G 2, panels (b) and (d) suggest 42 as the optimal size." ></td>
	<td class="line x" title="92:319	The dashed lines in panels (a) to (d) show the corresponding results for our smaller corpus (corpus 2)." ></td>
	<td class="line x" title="93:319	Unsurprisingly, the general pattern for this subcorpus is quite similar, although the drops in recall and the number of significant words, Nsig, occur at somewhat smaller window sizes." ></td>
	<td class="line x" title="94:319	Interestingly, we can synchronize the curves for both corpora by plotting recall and the number of significant items, Nsig, against the window-complement ratio (W/C)." ></td>
	<td class="line x" title="95:319	This is shown in panels (e) and (f)." ></td>
	<td class="line x" title="96:319	These panels suggest not an optimal window size 304 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 04 f 0.3 i ~ 0.2 ~ i 0 20 40 6(J 80 100 120 24 86 Window Size (a) 6r A '~ 4t / ',il _.~~ ~ \[ .,/ i  0 20 40 60 80 100 120 24 86 Window Size (c) 4oo\[, 200I, 03 i } 0.2 o. h 0 20 40 60 80 100 120 6 42 82 Window Size (b) 3oo r, ~o 0 20 40 60 80 100 120 6 42 82 Window Size (d) 300 I iilI  '1 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.17 0.62 0.05 0.29 0.58 w/c w/c (e) (t) Figure 2 Results of the word extraction procedure (a = 0.05)." ></td>
	<td class="line x" title="97:319	Solid line = corpus 1, dashed line = corpus 2." ></td>
	<td class="line x" title="98:319	Panel (a) shows the log-likelihood, G 2, recall results as a function of the window size." ></td>
	<td class="line x" title="99:319	Panel (b) shows recall values for Fisher's exact test." ></td>
	<td class="line x" title="100:319	Panel (c) shows the total number of significant words (Nsig) as a function of the window size for G 2." ></td>
	<td class="line x" title="101:319	Panel (d) shows the same as (c) but for Fisher's exact test." ></td>
	<td class="line x" title="102:319	Panel (e), G 2, and (f), Fisher's exact test, also show the total number of significant words, but as a function of the W/C-ratio; the ratio of the number of words in the window corpus to the number of words in the complement corpus." ></td>
	<td class="line x" title="103:319	but an optimal W/C-ratio (0.17 for G 2 and 0.29 for Fisher's exact test)." ></td>
	<td class="line x" title="104:319	Although we now seem to have shown that recall and Nsig depend on the choice of window size, the sudden drops in recall and Nsig and the reoccurrence of such drops at various W/C-ratios is a source of worry, not only for G 2 results, but also for the results based on Fisher's exact test." ></td>
	<td class="line x" title="105:319	A further source of worry is the fact that the two tests diverge considerably with respect to the optimal W/C-ratio." ></td>
	<td class="line x" title="106:319	3." ></td>
	<td class="line x" title="107:319	Contingency Tables and the Lowest-Frequency Words Before we can have any confidence in the optimality of a given W/C-ratio, we should understand why the saw-tooth-shaped patterns of Nsig arise." ></td>
	<td class="line x" title="108:319	Both the log-likelihood ratio (G 2) and Fisher's exact test compute the significance of contingency tables similar to Table 1." ></td>
	<td class="line x" title="109:319	So why is it that the left panels in Figure 2 differ from the right panels?" ></td>
	<td class="line x" title="110:319	G 2 has a 2-distribution as N --* cx~." ></td>
	<td class="line x" title="111:319	This convergence is not guaranteed for low expected frequencies and sparse tables, which renders use of G 2 problematic for our lowest-frequency words in that it may suggest words to be more remarkable than they 305 Computational Linguistics Volume 26, Number 3 Table 2 Contingency tables for hapax legomena, dis legomena, and tris legomena." ></td>
	<td class="line x" title="112:319	W = number of words in window corpus; C = number of words in complement corpus." ></td>
	<td class="line x" title="113:319	Total corpus size: N = W + C." ></td>
	<td class="line x" title="114:319	(a): 1 0 (b): 2 0 (c): 1 1 W-1 C W-2 C W-1 C-1 (d): 3 0 (e): 2 1 (f): 1 2 W-3 C W-2 C-1 W-1 C-2 really are." ></td>
	<td class="line x" title="115:319	Fisher's exact test, on the other hand, does not use an approximation to a probability distribution but computes the exact hypergeometric distribution given the marginal totals of the contingency table." ></td>
	<td class="line x" title="116:319	While Fisher's exact test is suitable for the analysis of sparse tables, it is inherently conservative because it regards the marginal totals not as stochastic variables but as fixed boundary conditions." ></td>
	<td class="line x" title="117:319	Consequently, this test is likely to reject words that are in fact remarkably distributed in the contingency table." ></td>
	<td class="line x" title="118:319	The difference in behavior of the two tests is clearly visible in panels (c) and (d) of Figure 2: the number of significant words (Nsig) according to G 2 is roughly twice as large as that according to Fisher's exact test." ></td>
	<td class="line x" title="119:319	When a hapax legomenon 1, a word with frequency 1, occurs in the window corpus, we use contingency table (a) as shown in Table 2." ></td>
	<td class="line x" title="120:319	For dis legomena, words with a frequency of 2, that appear at least once in the window corpus, we obtain the two contingency tables (b) and (c)." ></td>
	<td class="line x" title="121:319	The interesting contingency tables for tris legomena are tables (d) to (f)." ></td>
	<td class="line x" title="122:319	These six tables are relevant for 63.8% of the side-effect-related terms as judged by our medical expert." ></td>
	<td class="line x" title="123:319	How do changes in the W/C-ratio affect G 2 and Fisher's exact test, when applied to contingency tables (a) to (f)?" ></td>
	<td class="line x" title="124:319	In other words, how does the choice of the window size affect whether a low-frequency word is judged to be a significant term, for fixed A and B (e.g. , A = 1 and B = 0 for a hapax legomenon)?" ></td>
	<td class="line x" title="125:319	First, consider contingency tables with B = 0, for instance tables (a), (b), and (d)." ></td>
	<td class="line x" title="126:319	For small A, (A ~ W, C), it is easily seen (see the appendix) that the critical W/C-ratio based on the log-likelihood ratio is: W 1 C ~/eX/2 1' (1) with X the X 2 value corresponding to a given s-level with 1 degree of freedom." ></td>
	<td class="line x" title="127:319	For A = 1 and c~ -0.05, X = 3.84, the critical W/C-ratio equals 0.1718." ></td>
	<td class="line x" title="128:319	This is exactly the W/C-ratio in panel (e) in Figure 2 at which the first and largest drop in the number of significant words occurs." ></td>
	<td class="line x" title="129:319	Up to this ratio, any hapax legomenon appearing in the window corpus is judged to be a significant term." ></td>
	<td class="line x" title="130:319	For W/C > 0.1718, no hapax legomenon will be extracted." ></td>
	<td class="line x" title="131:319	Fisher's exact test is far more conservative." ></td>
	<td class="line x" title="132:319	For this test, the critical W/C-ratio is 1 The term hapax legomenon (literally 'read once') goes back to classical studies and was originally used to refer to the words used once only in the works of a given author, e.g., Homer." ></td>
	<td class="line x" title="133:319	By analogy, dis legomenon and tris legomenon have come into use to refer to words occurring only twice or three times." ></td>
	<td class="line x" title="134:319	306 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words Table 3 Critical W/C-ratios where sparse and skewed contingency tables lose significance." ></td>
	<td class="line x" title="135:319	Equations 1 and 2 provide the ratios for the B = 0 cases." ></td>
	<td class="line x" title="136:319	The other ratios are obtained by simulations." ></td>
	<td class="line x" title="137:319	distribution G 2 Fisher A-B cz = 0.05 c~ = 0.01 c~ -0.05 c~ -0.01 1 0 0.1718 0.0375 0.0526 0.0101 1 1 0.0400 0.0092 0.0260 0.0050 2 0 0.6204 0.2348 0.2880 0.1111 1 2 0.0232 0.0053 0.0172 0.0033 2 1 0.1917 0.0824 0.1565 0.0626 3 0 1.1155 0.4938 0.5833 0.2746 hapax legomena dis legomena tris legomena (see the appendix for details): w C 1/-P' (2) where P is the s-level." ></td>
	<td class="line x" title="138:319	For A -1 and P = 0.05, the critical W/C-ratio for a hapax legomenon equals 0.0526." ></td>
	<td class="line x" title="139:319	In panel (f) of Figure 2, we observe the first drop in the number of significant words at precisely this W/C-ratio." ></td>
	<td class="line x" title="140:319	For very small W/C-ratios, any hapax legomenon in the window corpus is also judged to be significant according to Fisher's exact test." ></td>
	<td class="line x" title="141:319	Compared to G 2, Fisher's exact test rejects hapax legomena as significant at much smaller W/C-ratios." ></td>
	<td class="line x" title="142:319	Note that when W/C -0.05/0.95 = 0.0526, i.e., when the window corpus is exactly 1/20 of the total corpus, the probability that a hapax legomenon appears in the window corpus equals 0.05." ></td>
	<td class="line x" title="143:319	Our conclusion is that, with the W/C-ratio as the only determinant of significance, the windowing method is not powerful enough to distinguish between relevant and irrelevant hapax legomena." ></td>
	<td class="line x" title="144:319	In other words, hapax legomena should be removed from consideration a priori." ></td>
	<td class="line x" title="145:319	For dis legomena that appear exclusively in the window corpus, the critical ratios are 0.6204 for G 2, corresponding to the second major drop in panel (e) of Figure 2, and 0.2880 for Fisher's exact test, corresponding to the severe drop following the maximum of Nsig in panel (f)." ></td>
	<td class="line x" title="146:319	The third major drop in this panel corresponds to the critical W/C-ratio for tris legomena occurring three times in the window corpus." ></td>
	<td class="line x" title="147:319	For contingency tables with B > 0; A > B; A, B <~ W, C, critical W/C-ratios are not easy to capture analytically." ></td>
	<td class="line x" title="148:319	We therefore carried out a simulation study for W + C = 100,000." ></td>
	<td class="line x" title="149:319	For fixed A and B and a given s-level, we calculated the critical W/C-ratio by iterative approximation." ></td>
	<td class="line x" title="150:319	Results are summarized in Table 3." ></td>
	<td class="line x" title="151:319	When we highlight these critical ratios in Figure 2 by means of vertical dashed lines, we obtain Figure 3." ></td>
	<td class="line x" title="152:319	Panels (a) to (d) correspond to the curves for corpus 2 in the first four panels of Figure 2." ></td>
	<td class="line x" title="153:319	For the log-likelihood ratio, we observe that both the major and minor drops in recall and the number of significant words (Nsig) occur at the W/Cratios where different distributions of the lowest-frequency words lose significance." ></td>
	<td class="line x" title="154:319	For Fisher's exact test, we observe exactly the same pattern." ></td>
	<td class="line x" title="155:319	Panels (e) and (f) show the number of significant words for a pseudorandomized version of corpus 2 where we used the same tokens but randomized the order of their appearance." ></td>
	<td class="line x" title="156:319	Although the number of significant words is lower, the saw-tooth-shaped pattern with the sudden drops at fixed ratios reemerges." ></td>
	<td class="line x" title="157:319	We conclude that W and C are the prime determinants of both recall and the number of significant words." ></td>
	<td class="line x" title="158:319	At first sight, Fisher's test is clearly preferable to the 307 Computational Linguistics Volume 26, Number 3 1-1 1-0 2-1 3-1 2-0 0.41~ ~ = Fi i ~03 ~ ii i ~ 0.2, 0.0 0.2 0.4 0.6 0.8 W/C (a) 1-1 1-0 2-1 3-1 2-0 600 I 42oo1 0.0 0.2 0.4 0.6 0.8 W/C (c) 1-1 1-0 2-1 3-1 2-0 400 f 300\[ Z 200\[ 100\[ 1-0 2-1 2-0 ~ 0.3\[ 0.2 0.1 3-1 3-0 0.0 0.2 0.4 0.6 0.8 W/C (b) 1-0 2-1 2:0 3-1 3-0 0.0 0.2 0.4 0.6 0.8 W/C (d) 1-0 150\[ Z '~ 100\[ 50\[, 0.0 2-1 2-0 3-1 3-0 0.0 0.2 0.4 0.6 0.8 0.2 0.4 0.6 0.8 W/C W/C (e) (0 Figure 3 Results of word extraction procedure (a = 0.05) with A-B distributions." ></td>
	<td class="line x" title="159:319	Panels (a), log-likelihood ratio, G 2, and (b), Fisher's exact test, show the recall results of the extraction procedure for corpus 2." ></td>
	<td class="line x" title="160:319	Panels (c) and (d) show the total number of significant words (Nsig), again for G 2 and Fisher's exact test, respectively (see also Figure 2)." ></td>
	<td class="line x" title="161:319	Panels (e) and (f) show the results for a randomized corpus for G 2 and Fisher's exact test." ></td>
	<td class="line x" title="162:319	The numbers above the panels indicate the A-B distribution of the contingency tables in Table 2." ></td>
	<td class="line x" title="163:319	log-likelihood ratio because the extreme saw-tooth-shaped pattern is substantially reduced." ></td>
	<td class="line x" title="164:319	However, the use of Fisher's exact test does not eliminate the effect of the choice of window and complement size on the number of significant words and recall." ></td>
	<td class="line x" title="165:319	At specific W/C-ratios, nonnegligible numbers of words with the lowest frequency of occurrence suddenly lose significance." ></td>
	<td class="line x" title="166:319	Moreover, in our discussion thus far, we have not taken extraction precision into account nor the trade-off between precision and recall." ></td>
	<td class="line x" title="167:319	For the assessment of overall extraction results, we turn to the F-measure (Rijsbergen 1979), a measure that assigns equal weights to precision (P) and recall (R): 2PR F= P+R' (3) Figure 4 plots precision, recall, and F as a function of the W/C-ratio." ></td>
	<td class="line x" title="168:319	The common trade-off between recall and precision is clearly present for the smaller window sizes, with the F-measure providing a kind of average." ></td>
	<td class="line x" title="169:319	Thus far, we have applied a common collocation extraction technique to a semantic association task." ></td>
	<td class="line x" title="170:319	Actual extraction performance is low: F is maximally 0.17." ></td>
	<td class="line x" title="171:319	To gauge 308 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words 0.4 I  (}.3 k o.2 ~'i' {{7., i:, ' '/  ~-' q,_ 1 i (}.(7 0.2 {}.4 (}.6 0.8 I .{} {7.17 {7,62 u2 (}.3 0.2 {7.1 {}.(7 0.2 0.4 0.6 0.{}5 0.29 0.58 i t,i i k O.8 I.O w/c w/c Figure 4 F, recall, and precision as a function of the W/C-ratio." ></td>
	<td class="line x" title="173:319	Recall (R, dashed line), F (solid line), and precision (P, dotted line) using G 2 (left panel) and Fisher's exact test (right panel) for our second corpus plotted as a function of the W/C-ratio." ></td>
	<td class="line x" title="174:319	whether better results can be obtained with the present techniques, we examined the extraction of Dutch verb-particle combinations." ></td>
	<td class="line x" title="175:319	4." ></td>
	<td class="line x" title="176:319	Extracting Verb-Particle Combinations In English, the particle of verb-particle combinations always follows the verb, as in she rang him up." ></td>
	<td class="line x" title="177:319	In Dutch, the particle can occur either before or after the verb." ></td>
	<td class="line x" title="178:319	When it occurs before the verb, it is separated from the verb by te ('to') and/or one or more auxiliary verbs." ></td>
	<td class="line x" title="179:319	Extracting such particle-verb combinations is relatively straightforward." ></td>
	<td class="line x" title="180:319	However, when the particle follows the verb, it may be separated from the verb by many constituents of arbitrary complexity: Hij zegt de belangrijke afspraak met de programmeur voor vanmiddag af ('he says the important meeting with the programmer for this afternoon off'; i.e., he cancels the meeting)." ></td>
	<td class="line x" title="181:319	How well does our present approach lend itself to the extraction of verb-particle combinations with the particle af ('off') when the particle follows the verb?" ></td>
	<td class="line x" title="182:319	We investigated this question by studying verb-particle combinations with af from a Dutch newspaper corpus of about 4.5 million word tokens." ></td>
	<td class="line x" title="183:319	We extracted by hand all sentences from the corpus that contain af (3,802 sentences, 97,903 tokens) and singled out those sentences in which af belongs to a verb-particle combination in which the verb occurs to the left of the particle (2,202 sentences with 42,825 tokens)." ></td>
	<td class="line x" title="184:319	The targets to extract from the 2,202 sentences are 436 different verb inflections, of which 276 have a frequency of less than five." ></td>
	<td class="line x" title="185:319	Just as the judgments of a medical expert were used in the preceding extraction task to provide a frame of reference for the evaluation of precision and recall, the present lexical extraction task has as its frame of reference the 2,202 sentences that we judged to contain a verb followed at some point to the right by a particle." ></td>
	<td class="line x" title="186:319	How many of the 436 different verb inflections can we extract with our windowing technique, and what is the trade-off between recall and precision?" ></td>
	<td class="line x" title="187:319	To answer this question, we defined windows to the left of the seed term af in the range of positions \[-12, -1\]." ></td>
	<td class="line x" title="188:319	We calculated the W/C-ratio for each window size." ></td>
	<td class="line x" title="189:319	For each word in all windows, we calculated its significance according to G 2 and Fisher's exact test." ></td>
	<td class="line x" title="190:319	Using the 436 target verb inflections as a frame of reference, we computed precision, recall, and F. Panel (a) of Figure 5 plots F as a function of the W/C-ratio." ></td>
	<td class="line x" title="191:319	F reaches a maximum F of 0.31 at W/C = 0.59 for G 2 (the solid line in the figure) and a maximum of 0.27 at W/C = 0.50 for Fisher's exact test (the dashed line)." ></td>
	<td class="line x" title="192:319	These 309 Computational Linguistics Volume 26, Number 3 03\[ / {}2 /// O. _ ~}2 -{}14 {}.6 {}.8 I.{} Z 400 200 //J J J\ 0.2 0.4 0.6 {}T8 I.{} W/C (a} W/{'." ></td>
	<td class="line x" title="193:319	(b) {}.3 0.2 {}." ></td>
	<td class="line x" title="194:319	1 f.~,." ></td>
	<td class="line x" title="195:319	J / 7 / __ L t  J. 0,2 0.4 0.6 410 ~0 Z 200 1}.8 1,11 L   ___  L _  {}.2 0.4 0,6 0.8 I .{} W/C W/C (c) (d) Figure 5 Extraction results for the af corpus." ></td>
	<td class="line x" title="196:319	Panel (a) shows F for G 2 (solid line) and Fisher's exact test (dashed line) as a function of the W/C-ratio." ></td>
	<td class="line x" title="197:319	Panel (b) displays the number of significant words (Nsig) according to both tests." ></td>
	<td class="line x" title="198:319	Panel (c) shows F for G 2 at c~ = 0.05 (solid line) and Fisher's exact test at c~ = 0.1 (dotted line)." ></td>
	<td class="line x" title="199:319	Panel (d) shows Nsig for G 2 at c~ = 0.05 and for Fisher's exact test at c~ -0.1." ></td>
	<td class="line x" title="200:319	results compare favorably with the maximum F of 0.17 obtained for the extraction of side-effect terms from medical abstracts." ></td>
	<td class="line x" title="201:319	Panel (b) of Figure 5 shows the by-this-time familiar saw-tooth-shaped pattern of the number of significant word types as function of the W/C-ratio." ></td>
	<td class="line x" title="202:319	We observe again that Fisher's exact test is more conservative, and in the extraction task, less successful, than G 2." ></td>
	<td class="line x" title="203:319	However, by opting for a more liberal c~-level we can compensate for the conservatism of Fisher's exact test and obtain an F profile that is indistinguishable from that of G 2 as shown in panel (c) for ~ -0.1." ></td>
	<td class="line x" title="204:319	Panel (d) returns to the number of significant terms (Nsig) when Fisher's exact test is used with c~ = 0.1." ></td>
	<td class="line x" title="205:319	Note that the optimal W/C-ratio according to F for G 2 (0.59) still leads to a higher Nsig than the optimal W/C-ratio (0.83) for Fisher's exact test with c~ -0.1." ></td>
	<td class="line x" title="206:319	However, in the case of Fisher's exact test, the precision is much higher than when G 2 is used." ></td>
	<td class="line x" title="207:319	These results suggest that the choice of G 2 or Fisher's exact test should be guided by the desired trade-off between precision and recall." ></td>
	<td class="line x" title="208:319	5." ></td>
	<td class="line x" title="209:319	Discussion The question that originally motivated the present research concerned the determination of the optimal window size for the extraction of side-effect-related words." ></td>
	<td class="line x" title="210:319	Most 310 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words words that are judged by a medical expert to be related to side effects have frequencies of use that are so low that they fall below the frequency thresholds generally used in standard information extraction techniques." ></td>
	<td class="line x" title="211:319	Is it nevertheless possible to single out such low-frequency terms through optimal window size estimation, especially since the log-likelihood ratio and Fisher's exact test have recently been advanced as suitable techniques even for the analysis of the lowest-frequency ranges?" ></td>
	<td class="line x" title="212:319	Manipulation of the window size revealed a saw-tooth-shaped pattern in the number of significant words (Nsig) that depends not on the window size itself but on the W/C-ratio." ></td>
	<td class="line x" title="213:319	This saw-tooth-shaped pattern arises most prominently when the loglikelihood ratio is used to extract significant words, but it is also clearly visible when Fisher's exact test is used." ></td>
	<td class="line x" title="214:319	This pattern is due to the way in which these tests evaluate surprise as a function of the window size for the lowest-frequency words." ></td>
	<td class="line x" title="215:319	We argue that hapax legomena should be disregarded a priori, while for low-frequency words with frequency greater than 1, only the most extreme distributions over window and complement are reliable in that we are confident that these terms are really related to the seed." ></td>
	<td class="line x" title="216:319	For dis and tris legomena, for instance, all occurrences should in effect be concentrated in the window." ></td>
	<td class="line x" title="217:319	Only then are we confident that there is truly a relationship between the seed and the target." ></td>
	<td class="line x" title="218:319	With these restrictions, the optimum W/C-ratio for our side-effect data is just smaller than 0.2880, using Fisher's exact test, which amounts to an optimal window size of 36." ></td>
	<td class="line x" title="219:319	Of the 295 terms with a frequency of 4 or less that a medical expert judged to be side-effect-related terms, we capture 14, which amounts to 4.8%." ></td>
	<td class="line x" title="220:319	When we exclude the hapax legomena as impossible to extract reliably a priori, we capture 14/122 = 11.5%." ></td>
	<td class="line x" title="221:319	Although the gain in number of significant low-frequency items is small, the success for the low-frequency items is still reasonable when compared to the corresponding success rate of 26/137 = 19.0% for the items with a frequency of 5 or more." ></td>
	<td class="line x" title="222:319	These results suggest that the windowing technique is far from optimal for the extraction of side-effect terms from medical abstracts, irrespective of the frequencies of these terms." ></td>
	<td class="line x" title="223:319	The windowing technique applied to the extraction of Dutch verb-particle combinations led to more encouraging results." ></td>
	<td class="line x" title="224:319	Choosing 0.4625 as the optimal W/C-ratio for the af data, which amounts to accepting dis legomena with a 2-0 distribution, and using a = 0.1 with Fisher's exact test, we obtain an optimal window size of 5." ></td>
	<td class="line x" title="225:319	With this window, we extract 42 of the 139 lowest-frequency words in the 2 to 4 range, i.e., 30.2%." ></td>
	<td class="line x" title="226:319	This compares favorably to the success rate of 60/170 = 35.2% for verbs with a frequency greater than 4." ></td>
	<td class="line x" title="227:319	When we use G 2 instead of Fisher's exact test to obtain improved recall at the cost of lesser precision, we extract 58/139 = 41.7% of the lowestfrequency words in the 2 to 4 range and 64/170 = 37.6% of the higher-frequency words (optimum W/C-ratio 0.6204, corresponding window size of 7)." ></td>
	<td class="line x" title="228:319	For this more lexical extraction task, extraction success rates are comparable for the lower-frequency and the higher-frequency words." ></td>
	<td class="line x" title="229:319	Neglecting the extraction of the lower-frequency words a priori would have led to the loss of nearly half of the words currently extracted." ></td>
	<td class="line x" title="230:319	The difference in the results between the two extraction tasks, side effects in medical abstracts and verb-af combinations in a newspaper corpus, is due to the difference in the distributions of the targets around the seed terms." ></td>
	<td class="line x" title="231:319	Concentrating on the lowestfrequency word tokens, the left panel of Figure 6 shows their distribution for the side-effect corpus." ></td>
	<td class="line x" title="232:319	The right panel shows the corresponding distribution for the af corpus." ></td>
	<td class="line x" title="233:319	The side-effect terms reveal a wide scatter around the seed at position 0." ></td>
	<td class="line x" title="234:319	By contrast, verbs predominantly cluster close to the left of af." ></td>
	<td class="line x" title="235:319	Apparently, the distance between the verb and the particle is more constrained than the distance between sideeffect terms and the seed term." ></td>
	<td class="line x" title="236:319	The optimal window size of 7 (position -7) for G 2 311 Computational Linguistics Volume 26, Number 3 10 i k -300 -200 -100 0 100 200 40 20 -40 -20 Position Position (a) (b), ll,,,,,it,, ,, 20 Figure 6 Frequency distribution of words occurring two to four times." ></td>
	<td class="line x" title="237:319	Panel (a) shows for the side effect corpus how the expert words with a frequency of 2, 3, and 4 are distributed around the seed term." ></td>
	<td class="line x" title="238:319	Panel (b) shows this distribution for the af corpus." ></td>
	<td class="line x" title="239:319	obtained above ties in with the distribution of the lowest-frequency words: 68% of all lowest-frequency tokens are in this window." ></td>
	<td class="line x" title="240:319	For the side-effect corpus, only 31% of all low-frequency tokens are in the optimal window of 36 for Fisher's exact test." ></td>
	<td class="line x" title="241:319	This suggests that the optimal window size must be ascertained on the basis of the distribution of targets around the seed, on the one hand, and by optimizing the statistics, on the other hand." ></td>
	<td class="line x" title="242:319	As an illustration of how the statistics can be optimized, we return to the af data." ></td>
	<td class="line x" title="243:319	When we look at the distribution of the lowest-frequency words in Figure 6, an optimal window size of 8 to the left suggests itself." ></td>
	<td class="line x" title="244:319	This translates into a W/C-ratio of 0.6689." ></td>
	<td class="line x" title="245:319	Given that we want to retain dis legomena with a 2-0 distribution, we proceed to compute the corresponding significance levels for both G 2 and Fisher's exact test by Equations 1 and 2." ></td>
	<td class="line x" title="246:319	The critical X 2 value for G 2 equals 3.65, the critical P for Fisher's exact test is 0.161." ></td>
	<td class="line x" title="247:319	The extraction results for both tests as measured by F are 0.31 and 0.33, respectively." ></td>
	<td class="line x" title="248:319	This procedure allows us to extract 64/139 = 46.0% of the lowfrequency words and 66/170 -~ 38.8% of the high-frequency words using G 2, and 64/139 = 46.0% and 79/170 = 46.7%, respectively, using Fisher's exact test." ></td>
	<td class="line x" title="249:319	Note that this technique is optimal for the extraction of the lowest-frequency words, leading to identical performance for G 2 and Fisher's exact test for these words." ></td>
	<td class="line x" title="250:319	For the higherfrequency words, Fisher's exact test leads to a slightly better recall with the same precision scores (0.31 for both tests)." ></td>
	<td class="line oc" title="251:319	While we have observed reasonable results with both G 2 and Fisher's exact test, we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information (MI) measure (Church and Hanks 1990): I(x,y) --log 2 P(x,y) (4) P(x)P(y) In (4), y is the seed term and x a potential target word." ></td>
	<td class="line o" title="252:319	A high MI score for a given target word suggests an association between this target and the seed term." ></td>
	<td class="line o" title="253:319	Or perhaps more precisely, a low MI score suggests a dissociation between target and seed word (Manning and Sch/itze 1999)." ></td>
	<td class="line x" title="254:319	To compute recall, precision, and F, we require a cut-off value." ></td>
	<td class="line x" title="255:319	As there is no theoretically motivated cut-off value, we vary it systematically." ></td>
	<td class="line x" title="256:319	Panel (a) of Figure 7 plots the results for the af corpus." ></td>
	<td class="line o" title="257:319	The x-axis represents the MI 312 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words f-~k g o (a) / -< ~ 0.I 0.2~-'7~__7 _ .q,-' ',~',  ., 04." ></td>
	<td class="line x" title="258:319	Slgl~l/l~ ~nc (Z3 e/eLe ~9~?~\C (b) Figure 7 Extraction results (F) for the af corpus for mutual information and Fisher's exact test." ></td>
	<td class="line o" title="259:319	Panel (a) shows the F score as a function of both W/C-ratio and mutual information cut-off value." ></td>
	<td class="line x" title="260:319	Panel (b) shows F as a function of W/C-ratio and the significance level c~ used with Fisher's exact test." ></td>
	<td class="line x" title="261:319	cut-off value, the y-axis the W/C-ratio, and the z-axis the F value." ></td>
	<td class="line o" title="262:319	Note that F is rather indifferent to variation in window size and MI cut-off value." ></td>
	<td class="line x" title="263:319	It varies between 0 (at the right-hand edge) to 0.17, with most values around 0.15 (the plateau in the figure)." ></td>
	<td class="line o" title="264:319	Interestingly, the highest possible MI cut-off point equals 4.27: the right-hand edge of the plateau." ></td>
	<td class="line x" title="265:319	In fact, 4.27 is the maximum MI score for this corpus size (42,825) and the frequency of the seed term af (2,206), irrespective of the frequency of the target word, reached when all occurrences of the target word are concentrated in the window (see the appendix for details)." ></td>
	<td class="line x" title="266:319	Consequently, any hapax legomenon appearing in the window will automatically be assigned the maximum value of MI, along with target words with the most extreme W/C distributions (Window-Complement: 2-0, 3-0, 4-0, etc.)." ></td>
	<td class="line o" title="267:319	This has the unfortunate consequence that, with regard to their MI score, truly remarkably distributed target words become indistinguishable from the statistically unremarkable hapax legomena." ></td>
	<td class="line o" title="268:319	Panel (b) of Figure 7 displays the corresponding results when we use Fisher's exact test rather than MI." ></td>
	<td class="line o" title="269:319	Instead of varying the MI cut-off value, we vary the significance level a. Note that the resulting F scores tend to be roughly twice as high as those obtained with MI-based extraction." ></td>
	<td class="line x" title="270:319	As there are a number of very similar local maxima, the choice of window size and significance level should be based on the desired tradeoff between precision and recall given the general distribution of the target words around the seed term." ></td>
	<td class="line o" title="271:319	2 We conclude that, at least for the present word extraction task, Fisher's exact test compares favorably to mutual information (as does G2)." ></td>
	<td class="line x" title="272:319	All the analyses presented thus far are conditional analyses, in the sense that we compiled new corpora from the database of abstracts and from the newspaper corpus containing only relevant abstracts (containing the drug names captopril and enalapril as well as the term side-effect) and relevant sentences (containing the particle af and its verb to its left), respectively." ></td>
	<td class="line o" title="273:319	The size of the complement was always determined with respect to these new conditional corpora, and not with respect to all MEDLINE 2 Note that we manipulate the a-levels in the same way as the MI cut-off values." ></td>
	<td class="line x" title="274:319	In the present technique, the a-level is a parameter that we vary to optimize extraction results for a training data set." ></td>
	<td class="line x" title="275:319	Our use of a should be carefully distinguished from the function of preset a-levels when testing the significance of observed differences in experimentally obtained data." ></td>
	<td class="line x" title="276:319	313 Computational Linguistics Volume 26, Number 3 Table 4 General and specific 2 x 2 contingency tables for low-frequency words." ></td>
	<td class="line x" title="277:319	Table (a) provides the general notation of the counts in a 2 x 2 contingency table." ></td>
	<td class="line x" title="278:319	In table (b), A = frequency of rare words (1, 2, 3  ), W = number of words in window, C = number of words in complement." ></td>
	<td class="line x" title="279:319	Corpus size N = W + C." ></td>
	<td class="line x" title="280:319	(a): 1/11 /'/12 /'/1+ /'/21 /'/22 /'/2+ //+1 //+2 //+q(b): A W-A W 0 A C W+C-A C W+C abstracts or to the complete newspaper corpus." ></td>
	<td class="line x" title="281:319	This raises the question of whether better results might have been obtained if the complete data sets had been used." ></td>
	<td class="line x" title="282:319	In principle, more data might imply more power." ></td>
	<td class="line x" title="283:319	At the same time, more data also entails the risk of more noise." ></td>
	<td class="line x" title="284:319	At least for our af data, enlarging the complement leads to worse performance." ></td>
	<td class="line x" title="285:319	When we allow any sentence that contains af in our analyses, F decreases from 0.31 to 0.23 for G 2." ></td>
	<td class="line x" title="286:319	When we base the analyses on the complete newspaper corpus, F reduces further to 0.19." ></td>
	<td class="line x" title="287:319	The reason for this decrease in performance is probably due to the W/C-ratio being very low for all practical window sizes, i.e., at the very left part of the saw-tooth-shaped pattern characterizing Nsig as a function of W/C. Consequently, any low-frequency word is singled out as a significant item whenever it occurs at least once in the window." ></td>
	<td class="line x" title="288:319	Given the Zipfian structure of word-frequency distributions, a great many spurious low-frequency words are extracted." ></td>
	<td class="line x" title="289:319	As mentioned in the introduction, the received wisdom is that the windowing method is unreliable for events with a frequency of less than 5." ></td>
	<td class="line x" title="290:319	By means of an analysis of the behavior of statistical tests for 2 x 2 contingency tables with sparse data, a method for optimizing the use of these tests has been developed." ></td>
	<td class="line x" title="291:319	We hope that this technique will prove to be useful for domains in which the extraction of low-probability events is crucial." ></td>
	<td class="line x" title="292:319	Appendix Log-Likelihood Ratio For the general contingency table, table (a) in Table 4, the log-likelihood ratio is defined by (Agresti 1990): G 2 = 2 ~_, ~ nijin(nij/mq), i j where rhq = ni+n+j/n++." ></td>
	<td class="line x" title="293:319	When we use the specific contingency table for hapax legomena, table (b) in Table 4, we obtain for a specific G 2 of X the formula: W+C (W-A)(W+C) W+C X/2 = Aln~+(W-A)ln W(W+C-A) +CInw+C-A' = In(WA) w-A InW w + In(W qC) w+C In(W qC A) w+C-A, = In (WA)W-A(w qC) w+C wW(w qC A) w+C-A ' (W A)W-A(w qC) w+C eX/2 wW(w + C A) w+c-A ' 314 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words We rewrite the latter equation to: eX/2w w (W-A) w-A eX/2wW(w -A) A (W-A) w (W + C) w+c (W + C A) w+c-A' (w + c)W(w + c)c(w + c A?" ></td>
	<td class="line x" title="294:319	(W + C A)W(W + C A) c Because W >> A and therefore W + C >> A, we rewrite the formula above as follows: eX/2 W w W A (w + c)W(w + c)C(w + c) A w w (w + c)W(w + c) c eX/2w A = (W  C) A, ~W = W+ C. So that the ratio is: W 1 C ~-1' When N > 10,000, the error of this equation is smaller than 0.001." ></td>
	<td class="line x" title="295:319	Fisher's Exact Test With Fisher's exact test, the observed marginal totals are used to compute the hypergeometric distribution, which is defined for the general 2 x 2 table, table (a) of Table 4, as (Agresti 1990): ?/2+ ) (n'+)(n+, rill -F/ll n+l The probability of every possible table with given marginal totals adds to 1." ></td>
	<td class="line x" title="296:319	We use Fisher's exact test that sums the hypergeometric probabilities of all tables with probabilities less than or equal to the observed table." ></td>
	<td class="line x" title="297:319	With B -0, table (b) in Table 4 is the only table we are interested in so that the probability P for this contingency table is: P A C-A W-A ) (w + c A)!" ></td>
	<td class="line x" title="298:319	(w + c)~ ' W!C! W~(W + C A)~ (w A)~(W + C)~' W(W1) (W-A + 1)(W-A)!" ></td>
	<td class="line x" title="299:319	(W + C A)!" ></td>
	<td class="line x" title="300:319	(w-a)~ (w+ c)(w+ c1)(w+ c-A + 1)(w+ c-A)!" ></td>
	<td class="line x" title="301:319	315 Computational Linguistics Volume 26, Number 3 Because A = 1,2,3,, W >> A and therefore W + C >> A, we allow ourselves to formulate W!" ></td>
	<td class="line x" title="302:319	= wA(w A)!" ></td>
	<td class="line x" title="303:319	and (W + C)!" ></td>
	<td class="line x" title="304:319	= (W + c)A(w + C A)!." ></td>
	<td class="line x" title="305:319	We therefore rewrite Fisher's exact test as follows: The W/C-ratio is then: wAw (w + P = (W + c)Aw!(w + C)'!" ></td>
	<td class="line x" title="306:319	WA (W + C) W W+C' w C 1~Y-P' When N > 20,000, the error of this equation is smaller than 0.001." ></td>
	<td class="line x" title="307:319	Practical Issues Using Fisher's Exact Test." ></td>
	<td class="line x" title="308:319	We used a network algorithm to compute Fisher's exact test (Mehta and Patel 1986; Clarkson, Fan, and Joe 1993)." ></td>
	<td class="line x" title="309:319	This algorithm is computationally intensive, but since many words have the same table, only a few tables have to be computed and their results can be cached." ></td>
	<td class="line x" title="310:319	It takes an average of 50 seconds to compute one window size in a 100,000 word corpus on a Pentium 133MHz, 48MB Linux machine." ></td>
	<td class="line x" title="311:319	Source code for the algorithm can be found at: http://www, acm." ></td>
	<td class="line oc" title="312:319	org/pubs/citations/ j ournals/toms/1986-12-2/p154-meht a/ Mutual Information Given the definition of Mutual Information (Church and Hanks 1990), I(x,y) = log 2 P(x,y) P(x)P(y)' we consider the distribution of a window word according to the contingency table (a) in Table 4." ></td>
	<td class="line x" title="313:319	P(x) is the relative frequency of the target word, P(y) is the relative frequency of the seed term, and P(x,y) is the frequency of the target word in the window." ></td>
	<td class="line x" title="314:319	In terms of the contingency table, we have: /'/11 I(x,y) = log 2 n++ //1+ S ' f/++ 7'/++ where S is the frequency of the seed." ></td>
	<td class="line x" title="315:319	Substituting nn = nl+ n12, we find that /11+ -F/12 I(x,y) = log 2 n++ nl+ S ' //++ //++ 1 = log 2 n++ //1+ S ' n++(nl+ -nu) ' n++ 316 Weeber, Vos, and Baayen Extracting the Lowest-Frequency Words = log2(n++) log2(S) log2(nl+) + log2(nl+ n12)." ></td>
	<td class="line x" title="316:319	For a given corpus and extraction task, corpus size (n++) and the frequency of the seed term S are fixed, so that we can write I(x,y) = C log2(nl+) + log2(nl+ n12)." ></td>
	<td class="line x" title="317:319	As n12 K nl+, I(x,y) reaches its maximum value (C) when n12 = 0, i.e., when all instances of the target word are in the window, irrespective of the frequency of the target." ></td>
	<td class="line x" title="318:319	Acknowledgments We are indebted to three anonymous reviewers whose criticisms have led to substantial improvements." ></td>
	<td class="line x" title="319:319	This study was financially supported by the Dutch National Research Council NWO (PIONIER grant to the third author)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W00-1106
Corpus-Based Learning Of Compound Noun Indexing
Kwak, Byung-Kwan;Kim, Jee-Hyub;Lee, Gary Geunbae;Seo, Jungyun;"></td>
	<td class="line x" title="1:173	Corpus-Based Learning of Compound Noun Indexing * Byung-Kwan Kwak, Jee-Hyub Kim, and Geunbae Lee t NLP Lab., Dept. of CSE Pohang University of Science & Technology (POSTECH) {nerguri,gblee} @postech.ac.kr Jung Yun Seo NLP Lab., Dept. of Computer Science Sogang University seojy@ccs.sogang.ac.kr Abstract In this paper, we present a corpusbased learning method that can index diverse types of compound nouns using rules automatically extracted from a large tagged corpus." ></td>
	<td class="line x" title="4:173	We develop an efficient way of extracting the compound noun indexing rules automatically and perform extensive experiments to evaluate our indexing rules." ></td>
	<td class="line x" title="5:173	The automatic learning method shows about the same performance compared with the manual linguistic approach but is more portable and requires no human efforts." ></td>
	<td class="line x" title="6:173	We also evaluate the seven different filtering methods based on both the effectiveness and the efficiency, and present a new method to solve the problems of compound noun over-generation and data sparseness in statistical compound noun processing." ></td>
	<td class="line x" title="7:173	1 Introduction Compound nouns are more specific and expressive than simple nouns, so they are more valuable as index terms and can increase the precision in search experiments." ></td>
	<td class="line x" title="8:173	There are many definitions for the compound noun which cause ambiguities as to whether a given continuous noun sequence is a compound noun or not." ></td>
	<td class="line x" title="9:173	We, therefore, need a clean ' This research was supported by KOSEF special purpose basic research (1997.9 2000.8 #970-1020301-3) t Corresponding author definition of compound nouns in terms of information retrieval, so we define a compound noun as 'any continuous noun sequence that appears frequently in documents'." ></td>
	<td class="line x" title="10:173	1 In Korean documents, compound nouns are represented in various forms (shown in Table 1), so there is a difficulty in indexing all types of compound nouns." ></td>
	<td class="line x" title="11:173	Until now, there have been much works on compound noun indexing, but they still have limitations of covering all types of compound nouns and require much linguistic knowledge to accomplish this goal." ></td>
	<td class="line x" title="12:173	In this paper, we propose a corpusbased learning method for compound noun indexing which can extract the rules automatically with little linguistic knowledge." ></td>
	<td class="line x" title="13:173	Table 1: Various types of Korean compound noun with regard to 'jeong-bo geom-saeg (information retrieval)' jeong-bo-geom-saeg (information-retrieval) jeong-bo-eui geom-saeg (retrieval of information) jeong-bo geom-saeg (information retrieval) jeong-bo-leul geom-saeg-ha-neun (retrieving information) jeong-bo-geom-saeg si-seu-tem (information-retrieval system) As the number of the documents is growing retrieval, efficiency also becomes as important as effectiveness." ></td>
	<td class="line x" title="14:173	To increase the efficiency, we focus on reducing the number of indexed spurious compound nouns." ></td>
	<td class="line x" title="15:173	We perform experiments on several filtering methods to find the algorithm that can reduce spurious compound nouns most efficiently." ></td>
	<td class="line x" title="16:173	1 The frequency threshold can be adjusted according to application systems." ></td>
	<td class="line x" title="17:173	57 The remainder of this paper  is organized as follows." ></td>
	<td class="line x" title="18:173	Section 2 describes previous compound noun indexing methods for Korean and compound noun filtering methods." ></td>
	<td class="line x" title="19:173	We show overall compound noun indexing system architecture in Section 3, and expl~.~n each module of the system in Section 4 and 5 in detail." ></td>
	<td class="line x" title="20:173	We evaluate our method with standard Korean test collections in Section 6." ></td>
	<td class="line x" title="21:173	Finally, concluding remarks are given in Section 7." ></td>
	<td class="line x" title="22:173	2 Previous Research 2.1 Compound Noun Indexing There have been two different methods for compound noun indexing: statistical and linguistic." ></td>
	<td class="line x" title="23:173	In one Statistical method, (Fagan, 1989) indexed phrases using six different parameters, including information on co-occurrence of phrase elements, relative location of phrase elements, etc. , and achieved reasonable performance." ></td>
	<td class="line x" title="24:173	However, his method couldn't reveal consistent substantial improvements on five experimental document collections in effectiveness." ></td>
	<td class="line x" title="25:173	(Strzalkowski et al. , 1996; Evans and Zhai, 1996) indexed subcompounds from complex noun phrases using noun-phrase analysis." ></td>
	<td class="line x" title="26:173	These methods need to find the head-modifier relations from noun phrases and therefore require difficult syntactic parsing in Korean." ></td>
	<td class="line x" title="27:173	For Korean, in one statistical method, (Lee and Ahn, 1996) indexed general Korean nouns using n-grams without linguistic knowledge and the experiment results showed that the proposed method might be Mmost as effective as the linguistic noun indexing." ></td>
	<td class="line x" title="28:173	However, this method can generate many spurious n-grarn~ which decrease the precision in search performance." ></td>
	<td class="line x" title="29:173	In linguistic methods, (Kim, 1994) used five manually chosen compound noun indexing rule patterns based on linguistic knowledge." ></td>
	<td class="line x" title="30:173	However, this method cannot index the diverse types of compound nouns." ></td>
	<td class="line x" title="31:173	(Won et al. , 2000) used a full parser and increased the precision in search experiments." ></td>
	<td class="line x" title="32:173	However, this linguistic method cannot be applied to unrestricted texts robustly." ></td>
	<td class="line x" title="33:173	In summary, the previous methods, whether they are statistical or linguistic, have their own shortcomings." ></td>
	<td class="line x" title="34:173	Statistical methods require signiAcant amounts of co-occurrence information for reasonable performance and can not index the diverse types of compound nouns." ></td>
	<td class="line x" title="35:173	Linguistic methods need compound noun indexing rules described by human and sometimes result in meaningless compound nouns, which decreases the performance of information retrieval systems." ></td>
	<td class="line x" title="36:173	They cannot also cover the various types of compound nouns because of the limitation of human linguistic knowledge." ></td>
	<td class="line x" title="37:173	In this paper, we present a hybrid method that uses linguistic rules but these rules are automatically acquired from a large corpus through statistical learning." ></td>
	<td class="line x" title="38:173	Our method generates more diverse compound noun indexing rule patterns than the previous standard methods (Kim, 1994; Lee et ah, 1997), because previous methods use only most general rule patterns (shown in Table 2) and are based solely on human linguistic knowledge." ></td>
	<td class="line x" title="39:173	Table 2: Typical hand-written compound noun indexing rule patterns for Korean Noun without case makers / Noun Noun with a genitive case maker / Noun Noun with a nominal case maker or an accusative case maker \[ Verbal common noun or adjectival common noun Noun with an adnominal ending \] Noun Noun within predicate particle phrase / Noun (The two nouns before and after a slash in the pattern can form a single compound noun)." ></td>
	<td class="line x" title="40:173	2.2 Compound Noun Filtering Compound noun indexing methods, whether they are statistical or linguistic, tend to generate spurious compound nouns when they are actually applied." ></td>
	<td class="line x" title="41:173	Since an information retrieval system can be evaluated by its effectiveness and also by its efficiency (van Rijsbergen, 1979), the spurious compound nouns should be efficiently filtered." ></td>
	<td class="line x" title="42:173	(Kando et al. , 1998) insisted that, for Japanese, the smaller the number of index terms is, the better the performance of the information retrieval system should be." ></td>
	<td class="line x" title="43:173	58 For Korean, (Won et al. , 2000) showed that segmentation of compound nouns is more efficient than compound noun synthesis in search performance." ></td>
	<td class="line x" title="44:173	There have been many works on compound noun filtering methods; (Kim, 1994) used mutual information only, and (Yun et al. , 1997) used mutual information and relative frequency of POS (Part-OfSpeech) pairs together." ></td>
	<td class="line x" title="45:173	(Lee et ai., 1997) used stop word dictionaries which were constructed manually." ></td>
	<td class="line x" title="47:173	Most of the previous methods for compound noun filtering utilized only one consistent method for generated compound nouns irrespective of the different origin of compound noun indexing rules, and the methods cause many problems due to data sparsehess in dictionary and training data." ></td>
	<td class="line x" title="48:173	Our approach solves the data sparseness problem by using co-occurrence information on automatically extracted compound noun elements together with a statistical precision measure which fits best to each rule." ></td>
	<td class="line x" title="49:173	3 Overall System Architecture The compound noun indexing system proposed in this paper Consists of two major modules: one for automatically extracting compound noun indexing rules (in Figure 1) and the other for indexing documents, filtering the automatically generated compound nouns, and weighting the indexed compound nouns (in Figure 2)." ></td>
	<td class="line x" title="50:173	Compound ~ Tagged Corpus 1 Compound ~R Noun Statistical Information ~ Roles with Precision Extracted Rules I Filtered Rules Figure 1: Compound noun indexing-rule extraction module (control flow =~, data flow Compound Noun~-----~-~ Indexing I ~_Indexing~'~ / Rules wig f--?-.~ Compound // Compound Noun\[ s~i~ \[ Infonnadon \[ Find \[ Compound I Nouns, I Weighted." ></td>
	<td class="line x" title="51:173	Compound Nouns Figure 2: Compound noun indexing, filtering, and weighting module (control flow =~, data flow ~) 4 Automatic Extraction of Compound Noun Indexing Rules There are three major steps in automatically extracting compound noun indexing rules." ></td>
	<td class="line x" title="52:173	The first step is to collect compound noun statistical information, and the second step is to extract the rules from a large tagged corpus using the collected statistical information." ></td>
	<td class="line x" title="53:173	The final step is to learn each rule's precision 4.1 Collecting Compound Noun Statistics We collect initial compound noun seeds which were gathered from various types of wellbalanced documents such as ETRI Kemong encyclopaedia 2 and many dictionaries on the Internet, and we collected 10,368 seeds, as shown in Table 3." ></td>
	<td class="line x" title="54:173	The small number of seeds are bootstrapped to extract the Compound noun indexing rules for various corpora." ></td>
	<td class="line x" title="55:173	Table 3: Collected compound noun seeds No." ></td>
	<td class="line x" title="56:173	of 2 3 Total component elements ETRI Kemong encyclomedia 5,100 2,088 7,188 Internet dictionaries 2,071 1,109 3,180 To collect more practical statistics on the compound nouns, we made a 1,000,000 eojeol(Korean spacing unit which corresponds 2 Courteously provided by ETRI, Korea." ></td>
	<td class="line x" title="57:173	59 to an English word or phrase) tagged corpus for a compound noun indexing experiment from a large document set (Korean Information Base)." ></td>
	<td class="line x" title="58:173	We collected complete compound nouns (a continuous noun sequence composed of at least two nouns on the condition that both the preceding and the following POS of the sequence axe not nouns (Yoon et al. , 1998)) composed of 1 3 no, ms from the tagged training corpus (Table 4)." ></td>
	<td class="line x" title="59:173	Table 4: Statistics for complete compound nouns No." ></td>
	<td class="line x" title="60:173	of 1 2 3 component elements Vocabulary 264,359 200,455 63,790 4.2 Extracting Indexing Rules We define a template (in Table 5) to extract the compound noun indexing rules from a POS tagged corpus." ></td>
	<td class="line x" title="61:173	The template means that if a frontcondition-tag, a rear-condition-tag, and substring-tags are coincident with input sentence tags, the lexical item in the synthesis position of the sentence can be indexed as a compound noun as 'x/ y (for 3-noun compounds, x / y / z)'." ></td>
	<td class="line x" title="62:173	The tags used in the template are POS (Part-Of-Speech) tags and we use the POSTAG set (Table 17)." ></td>
	<td class="line x" title="63:173	The following is an algorithm to extract compound noun indexing rules from a large tagged corpus using the two-noun compound seeds and the template defined above." ></td>
	<td class="line x" title="64:173	The rule extraction scope is limited to the end of a sentence or, if there is a conjunctive ending (eCC) in the sentence, only to the conjunctive ending of the sentence." ></td>
	<td class="line x" title="65:173	A rule extraction example is shown in Figure 3." ></td>
	<td class="line x" title="66:173	Algorithm 1: Extracting compound noun indexing rules (for 2-noun compounds) Read Template Read Seed (Consist of Constituent 1 / Constituent 2) TokeD/ze Seed into Constituents Put Constituent 1 into Key1 and Constituent 2  into Key2 While (Not(End of Documents)) { Read Initial Tag of Sentence While (Not(End of Sentence or eCC)) { Read NeIt Tag of Sentence If (Read Tag =ffi Key1) { While (Not(End of Sentence or eCC)) ( Read Next Tag of Sentence If (Current Tag == Key2) Write Rule according to the Template } } } The next step is to refine the extracted rules to select the proper ones." ></td>
	<td class="line x" title="67:173	We used a rule filtering algorithm (Algorithm 2) using the frequency together with the heuristics that the rules with negative lexical items (shown in Table 6) will make spurious compound nouns." ></td>
	<td class="line x" title="68:173	Algorithm 2: Filtering extracted rules using frequency and heuristics I. For each compound noun seed, select the rules whose frequency is greater than 2." ></td>
	<td class="line x" title="69:173	2. Among rules selected by step 1, select only rules that are extracted at least by 2 seeds." ></td>
	<td class="line x" title="70:173	3." ></td>
	<td class="line x" title="71:173	Discard rules which contain negative lexical items." ></td>
	<td class="line x" title="72:173	Table 5: The template to extract the compound noun indexing rules,o front-condition-tag I sub-string-tags (tag 1 tag 2  tag n-1 tag n) \[ rear-condition-tag I synthesis locations (x y) lexicon x / lexicon y (for 3-noun compounds, synthesis locations (x, y, z) lexicon x / lexicon y / lexicon z) Table 6: Negative negative items (tags) je-oe(MC) (exclude) eobs(E) (not exist) mos-ha(D) (can not) lexical item examples example phrases no-jo-leul je-oe-han hoe-eui (meeting excluding union) sa-gwa-ga eobs~neun na-mu (tree without apple) dog-lib-eul mos-han gug-ga (country that cannot be liberated) We automatically extracted and filtered out 60 Tagged ~t~ B,~baI-Ib, MC< .kong-bo > .iC<tmb." ></td>
	<td class="line x" title="73:173	MC< geom-sacg > fron~com~illm1_~g I sub_s~ring_mgs (~ I lag2  tag n-I ~n) ~rcar_cond~iol~." ></td>
	<td class="line x" title="74:173	I~ syn~lcsls location (x y) ~> lexicon x I Icxlco~ y (i.formafioa~uicvall Indcxlag Rule: B I MC.jC<leul> MC I y I l 3 Figure 3: Rule Extraction Process Example 2,036 rules from the large tagged corpus (Korean Information Base, 1,000,000 eojeol) using the above Algorithm 2." ></td>
	<td class="line x" title="75:173	Among the illtered rules, there are 19 rules with negative lexical items and we finally selected 2,017 rules." ></td>
	<td class="line x" title="76:173	Table 7 shows a distribution of the final rules according to the number of elements in their sub-string-tags." ></td>
	<td class="line x" title="77:173	Table 7: Distribution of extracted rules by number of elements in sub-string-tags No." ></td>
	<td class="line x" title="78:173	Distribution Example 2 tags 79.6 % MC MC 3 tags 12.6 % MC jO(eui) MC 4 tags 4.7 % MC y eCNMG MC 5 tags 1.5 % MC MC jO(e) DI<sog-ha-neun) MC over 6 tags 1.6 % The automatically extracted rules have more rule patterns and lexical items than human-made rules so they can cover more diverse types of compound nouns (Table 8)." ></td>
	<td class="line x" title="79:173	When checking the overlap between the two rule collections, we found that the manual linguistic rules are a subset of our automatically generated statistical rules." ></td>
	<td class="line x" title="80:173	Table 9 shows some of the example rules newly generated from our extraction algorithm, which were originally missing in the manual rule patterns." ></td>
	<td class="line x" title="81:173	4.3 Learning the Precision of Extracted Rules In the proposed method, we use the precision of rules to solve the compound noun overgeneration and the data sparseness problems." ></td>
	<td class="line x" title="82:173	The precision of a rule can be defined by Table 8: Comparison between the automatically extracted rules and the manual rules Method Manual linguistic method Our method No." ></td>
	<td class="line x" title="83:173	of No." ></td>
	<td class="line x" title="84:173	of general lexical terms rule patterns used in rule patterns 16 23 78 Table 9: Examples of newly added rule patterns Rule Noun + bound noun / Noun Noun + suffix / Noun Noun + suffix + assignment verb + adnominal ending / Noun counting how many indexed compound noun candidates generated by the rule are actual compound nouns: Yactuat Prec(rule) = Ncandidate where Prec(rule) is the precision of a rule, Ndctual is the number of actual compound nouns, and Ncandidat e is the number of compound noun candidates generated by the automatic indexing rules." ></td>
	<td class="line x" title="85:173	To calculate the precision, we need a defining measurement for compound noun identification." ></td>
	<td class="line x" title="86:173	(Su et al. , 1994) showed that the average mutual information of a compound noun tends to be higher than that of a noncompound noun, so we try to use the mutual information as the measure for identifying the compound nouns." ></td>
	<td class="line x" title="87:173	If the mutual information of the compound noun candidate is higher than the average mutual information of the compound noun seeds, we decide that it is a compound noun." ></td>
	<td class="line oc" title="88:173	For mutual information (MI), we use two different equations: one for two-element compound nouns (Church and Hanks, 1990) and the other for three-element compound nouns (Suet al. , 1994)." ></td>
	<td class="line x" title="89:173	The equation for two-element compound nouns is as follow: P(x,y) I(x;y) = log 2 P(x) x P(y) 61 where x and y are two words in the corpus, and I(x; y) is the mutual information of these two words (in this order)." ></td>
	<td class="line x" title="90:173	Table 10 shows the average MI value of the two and three elements." ></td>
	<td class="line x" title="91:173	Table 10: Average value of the mutual information (MI) of compound noun seeds .Number of elements \[ 2 I 3 Average MI 3.56 3.62 The MI was calculated from the statistics of the complete compound nouns collected from the tagged training corpus (see Section 4.1)." ></td>
	<td class="line x" title="92:173	However, complete compound nouns are continuous noun sequences and cause the data sparseness problem." ></td>
	<td class="line x" title="93:173	Therefore, we need to expand the statistics." ></td>
	<td class="line x" title="94:173	Figure 4 shows the architecture of the precision learning module by expanding the statistics of the complete compound nouns along with an algorithmic explanation (Algorithm 3) of the process." ></td>
	<td class="line x" title="95:173	Table 11 shows the improvement in the average precision during the repetitive execution of this learning process." ></td>
	<td class="line x" title="96:173	Norm Statistical ) Compound Norm of Rules ~'~ Rule incision (step 5) (s~ 2.7) l~v v\[ (step s) Figure 4: Learning the precision of the compound noun indexing rules (The steps are shown in Algorithm 3) Algorithm 3: i. Calculate all rules' initial precision using initial complete compound noun statistical information." ></td>
	<td class="line x" title="97:173	2." ></td>
	<td class="line x" title="98:173	Calculate the average precision of the rules." ></td>
	<td class="line x" title="99:173	3." ></td>
	<td class="line x" title="100:173	Multiply a rule's precision by the frequency of the compound noun made by the rule." ></td>
	<td class="line x" title="101:173	We call this value the modified frequency (MF)." ></td>
	<td class="line x" title="102:173	4." ></td>
	<td class="line x" title="103:173	Collect the same compound nouns, and sum all the modified frequencies for each compound noun." ></td>
	<td class="line x" title="104:173	5." ></td>
	<td class="line x" title="105:173	If the sunm~ed modified frequency is greater than a threshold, add this compound noun to the complete compound noun statistical information." ></td>
	<td class="line x" title="106:173	6." ></td>
	<td class="line x" title="107:173	Calculate all rules' precision again using the changed complete compound noun statistical information." ></td>
	<td class="line x" title="108:173	7." ></td>
	<td class="line x" title="109:173	Calculate the average precision of the rules." ></td>
	<td class="line x" title="110:173	8." ></td>
	<td class="line x" title="111:173	If the average precision of the rules is equal to the previous average precision, stop." ></td>
	<td class="line x" title="112:173	Othervise, go to step 2." ></td>
	<td class="line x" title="113:173	Table 11: Improvement in the average precision of rules Learning 1 2 3 4 5 6 cycles Avg." ></td>
	<td class="line x" title="114:173	prec." ></td>
	<td class="line x" title="115:173	0.19 0.23 0.39 0.44 0.45 0.45 of rules 5 Compound Noun Indexing, Filtering, and Weighting In this section, we explai n how to use the automatically extracted rules to actually index the compound nouns, and describe how to filter and weight the indexed compound nouns." ></td>
	<td class="line x" title="116:173	5.1 Compound Noun Indexing To index compound nouns from documents, we use a natural language processing engine, SKOPE (Standard KOrean Processing Engine) (Cha et al. , 1998), which processes documents by analysing words into morphemes and tagging part-of-speeches." ></td>
	<td class="line x" title="117:173	The tagging results are compared with the automatically learned compound noun indexing rules and, if they are coincident with each other, we index them as compound nouns." ></td>
	<td class="line x" title="118:173	Figure 5 shows a process of the compound noun indexing with an example." ></td>
	<td class="line x" title="119:173	5.2 Compound Noun Filtering Among the indexed compound nouns above, still there can be meaningless compound nouns, which increases the number of index terms and the search time." ></td>
	<td class="line x" title="120:173	To solve compound noun over-generation problem, we experiment with seven different filtering methods (shown in Table 12) by analyzing their 62 ." ></td>
	<td class="line x" title="121:173	' Tagging Result: bbal-li jeong-bo-leul B<bbal-li> geom-saeg-ha-netm ~ Auaty~ ~." ></td>
	<td class="line x" title="122:173	~ MC<jeong-bo > (~evmg I \~'~?_'2:?" ></td>
	<td class="line x" title="123:173	'/ ~ jc<,e. ,> information I \ tagging / /1~  :-t.~~ \] ~ --/ / I.MU< geom-saeg > / I eCNMG<neun> '~d~'~g'l~ ~es-\]'X~mpoun'~ Indexed i 1,2 . Complete l/ ~ geom-saeg _ C?mp~No~ ~ -(mf,,~o./ Statistical Information \] retrieval) Figure 5: Compound noun indexing process relative effectiveness and efficiency, as shown in Table 16." ></td>
	<td class="line x" title="124:173	These methods can be divided into three categories: first one using MI, second one using the frequency of the compound nouns (FC), and the last one using the frequency of the compound noun elements (FE)." ></td>
	<td class="line x" title="125:173	MI (Mutual Information) is a measure of word association, and used under the assumption that a highly associated word n-gram is more likely to be a compound noun." ></td>
	<td class="line x" title="126:173	FC is used under the assumption that a frequently encountered word n-gram is more likely to be a compound than a rarely encountered n-gram." ></td>
	<td class="line x" title="127:173	FE is ;used under the assumption that a word n-gram with a frequently encountered specific element is more likely to be a compound." ></td>
	<td class="line x" title="128:173	In the method of C, D, E, and F, each threshold was decided by calculating the average number of compound nouns of each method." ></td>
	<td class="line x" title="129:173	Table 12: Seven different filtering methods (MI) A. Mutual information of compound noun elements (0) (MI) B. Mutual information of compound noun elements (average of MI of compound noun seeds) (FC) C. Frequency of compound nouns in the training corpus (4) (FC) D. Frequency of compound nouns in the test corpus (2) (FE) E. Frequency of compound noun heads in the training corpus (5) (FE) F. Frequency of compound noun modifiers in the training corpus (5) G. No filtering (The value in parantheses is a threshold)." ></td>
	<td class="line x" title="130:173	Among these methods, method B generated the smallest number of compound nouns best efficiency and showed the reasonable effectiveness (Table 16)." ></td>
	<td class="line x" title="131:173	On the basis of this filtering method, we develop a smoothing method by combining the precision of rules with the mutual information of the compound noun elements, and propose our final filtering method (H) as follows: P(x, y) + ~  Precision T(x, y) = log 2 P(x) x P(y) where a is a weighting coefficient and Precision is the applied rules learned in Section 4.3." ></td>
	<td class="line x" title="132:173	For the three-element compound nouns, the MI part is replaced with the three-element MI equation 3 (Su et al. , 1994)." ></td>
	<td class="line x" title="133:173	6 Experiment Results To calculate the similarity between a document and a query, we use the p-norm retrieval model (Fox, 1983) and use 2.0 as the p-value." ></td>
	<td class="line x" title="134:173	We also use fhe component nouns in a compound as the indexing terms." ></td>
	<td class="line x" title="135:173	We follow the standard TREC evaluation schemes (Salton and Buckley, 1991)." ></td>
	<td class="line x" title="136:173	For single index terms, we use the weighting method atn.ntc (Lee, 1995)." ></td>
	<td class="line x" title="137:173	6.1 Compound Noun Indexing Experiments This experiment shows how well the proposed method can index diverse types of compound nouns than the previous popular methods which use human-generated compound noun indexing rules (Kim, 1994; Lee et al. , 1997)." ></td>
	<td class="line x" title="138:173	For simplicity, we filtered the generated compound nouns using the mutual information of the compound noun elements with a threshold of zero (method A in Table 12)." ></td>
	<td class="line x" title="139:173	Table 13 shows that the terms indexed by previous linguistic approach are a subset of the ones made by our statistical approach." ></td>
	<td class="line x" title="140:173	This means that the proposed method can cover more diverse compound nouns than the 3 PD (x, ~, z) I(x;y;z) = log 2 Px(x,y,z) 63 Table 13: Compound noun indexing coverage experiment (With a 200,000 eojeol Korean Information Base) Manual linguistic rule patterns Our automatic rule patterns No." ></td>
	<td class="line x" title="141:173	of generated actual 22,276 30,168 compound nouns." ></td>
	<td class="line x" title="142:173	(+35.4 %) No." ></td>
	<td class="line x" title="143:173	of generated actual 7,892 compound nouns without overlap manual linguistic rule method." ></td>
	<td class="line x" title="144:173	We perform a retrieval experiment to evaluate the automatically extracted rules." ></td>
	<td class="line x" title="145:173	Table 144 and table 155 show that our method has slightly better recall and ll-point average precision than the manual linguistic rule method." ></td>
	<td class="line x" title="146:173	Table 14: Compound noun indexing effectiveness experiment I Avg." ></td>
	<td class="line x" title="147:173	recall Manual linguistic rule patterns 82.66 Our automatic rule patterns 83.62 (+1.16 %) ll-pt." ></td>
	<td class="line x" title="148:173	42.24 42.33 avg." ></td>
	<td class="line x" title="149:173	precision (+0.21%) No." ></td>
	<td class="line x" title="150:173	of 504,040 515,801 index terms (+2.33 %) Table 15: Compound noun indexing effectiveness experiment II Avg." ></td>
	<td class="line x" title="151:173	recall ll-pt, avg." ></td>
	<td class="line x" title="152:173	precision No." ></td>
	<td class="line x" title="153:173	of index terms Manual linguistic rule patterns 86.32 34.33 1,242,458 Our automatic rule patterns 87.50 (+1.35 %) 34.54 (+0.61%) 1,282,818 (+3.15 %) 4 With KTSET2.0 test collections (Courteously provided by KT, Korea." ></td>
	<td class="line x" title="154:173	(4,410 documents and 50 queries)) s With KRIST2.0 test collection (Courteously provided by KORDIC, Korea." ></td>
	<td class="line x" title="155:173	(13,514 documents and 30 queries)) 6.2 Retrieval Experiments Using Various Filtering Methods In this experiment, we compare the seven filtering methods to find out which one is the best in terms of effectiveness and efficiency." ></td>
	<td class="line x" title="156:173	For this experiment, we used our automatic rules for the compound noun indexing, and the test collection KTSET2.0." ></td>
	<td class="line x" title="157:173	To check the effectiveness, we used recall and ll-point average precision." ></td>
	<td class="line x" title="158:173	To check the efficiency, we used the number of index terms." ></td>
	<td class="line x" title="159:173	Table 16 shows the results of the various filtering experiments." ></td>
	<td class="line x" title="160:173	From Table 16, the methods using mutual information reduce the number of index terms, whereas they have lower precision." ></td>
	<td class="line x" title="161:173	The reason of this lower precision is that MI has a bias, i.e., scoring in favor of rare terms over common terms, so MI seems to have a problem in its sensitivity to probability estimation error (Yang and Pedersen, 1997)." ></td>
	<td class="line x" title="162:173	In this experiment 6, we see that method B generates the smallest number of compound nouns (best efficiency) and our final proposing method H has the best recall and precision  (effectiveness) with the reasonable number  of compound nouns (efficiency)." ></td>
	<td class="line x" title="163:173	We can conclude that the filtering method H is the best, considering the effectiveness and the efficiency at the same time." ></td>
	<td class="line x" title="164:173	7 Conclusion In this paper, we presented a method to extract the compound noun indexing rules automatically from a large tagged corpus, and showed that this method can index compound nouns appearing in diverse types of documents." ></td>
	<td class="line x" title="165:173	In the view of effectiveness, this method is slightly better than the previous linguistic approaches but requires no human effort." ></td>
	<td class="line x" title="166:173	The proposed method also uses no parser and no rules described by humans, therefore, it can be applied to unrestricted texts very robustly and has high domain porta6 Our Korean NLQ (Natural Language Querying) demo system (located in 'http:/ /nlp.postech.ac.kz /Resarch/POSNLQ/') can be tested." ></td>
	<td class="line x" title="167:173	64 Table 16: Retrieval experiment A B C Average 83.62 83.62 83.62 recall (+0.00) (+0.00) ll-pt, avg." ></td>
	<td class="line x" title="168:173	42.45 42.42 42.49 precision (-0.07) (+0.09) Precision 52.11 52.44 52.07 at 10 Docs." ></td>
	<td class="line x" title="169:173	No. of 515,80 508,20 514,54 5~ index terms (-1.47) (-0.24) (-+ results D 83.62 (+0.00) 42.55 (+0.24) 52.80 547,27 +6.10) of various filtering E F 83.62 (+0.00) 42.72 (+0.64) 52.26 572,36 (+10.97) 83.62 (+0.00) 42.48 (+0.07) 51.89 574,04 (+11.29) ; methods G 84.32 (+0.84) 42.48 (+0.07) 52.81 705,98 (+36.87) H 84.32 (.+0.84) 42.75 (+o.71) 52.98 509,90 (-1.14) bility." ></td>
	<td class="line x" title="170:173	We also presented a filtering method to solve the compound noun over-generation problem." ></td>
	<td class="line x" title="171:173	Our proposed filtering method (H) shows good retrieval performance both in the view of the effectiveness and the efficiency." ></td>
	<td class="line x" title="172:173	In the future, we need to perform some experiments on much larger commercial databases to test the practicality of our method." ></td>
	<td class="line x" title="173:173	Finally, our method doesn't require language dependent knowledge, so it needs to be verified whether it can be easily applied to other languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W00-1313
Query Translation In Chinese-English Cross-Language Information Retrieval
Zhang, Yibo;Sun, Le;Du, Lin;Sun, Yufang;"></td>
	<td class="line x" title="1:119	Query Translation in Chinese-English Cross-Language Information Retrieval Zhang Yibo, Sun Le, Du Lin, Sun Yufang Chinese Information Processing Center, Institute of Software, Chinese Academy of Sciences, P.O.Box 8718, Beijing, 100080, P.R. China e-mail: zyb, lesun, !du, yfsun@sonata.iscas.ac.cn Abstract This paper proposed a new query translation method based on the mutual information matrices of terms in the Chinese and English corpora." ></td>
	<td class="line x" title="2:119	Instead of looking up a  bilingual phrase dictionary, the compositional phrase (the translation of phrase can be derived from the translation of its components) in the query can be indirectly translated via a general-purpose Chinese-English dictionary look-up procedure." ></td>
	<td class="line x" title="3:119	A novel selection method for translations of query terms is also presented in detail." ></td>
	<td class="line x" title="4:119	Our query translation method ultimately constructs an English query in which each query term has a weight." ></td>
	<td class="line x" title="5:119	The evaluation results show that the retrieval performance achieved by our query translation method is about 73% of monolingual information retrieval and is about 28% higher than that of simple wordby-word translation way." ></td>
	<td class="line x" title="6:119	Introduction With the rapid growth of electronic documents and the great development of network in China, there are more and more people touching the Intemet, on which, however, English is the most popular language being used." ></td>
	<td class="line x" title="7:119	It is difficult for most people in China to use English fluently, so they would like to use Chinese to express their queries to retrieval the relevant English documents." ></td>
	<td class="line x" title="8:119	This situation motivates research in Cross Language Information Retrieval (CLIR)." ></td>
	<td class="line x" title="9:119	There are two approaches to CLIR, one is query translation; the other is translating original language documents to destination This research was supported by the National Science Fund of China for Distinguished Young Scholars under contact 69983009." ></td>
	<td class="line x" title="10:119	language equivalents." ></td>
	<td class="line x" title="11:119	Obviously, the latter is a very expensive task since there are so many documents in a collection and there is not yet a reliable machine translation system that can be used to process automatically." ></td>
	<td class="line x" title="12:119	Most researchers are inclined to choose the query translation approach \[Oard." ></td>
	<td class="line x" title="13:119	(1996)\]." ></td>
	<td class="line x" title="14:119	Methods for query translation have focused on three areas: the employment of machine translation techniques, dictionary based translation \[Hull & Grefenstette (1996); Ballesteros & Croft (1996)\], parallel or comparable corpora for generating a translation model \[Davis & Dunning (1995); Sheridan & Ballerini (1996); Nie, Jian-Yun et a1.(1999)\]." ></td>
	<td class="line x" title="15:119	Machine translation (MT) method has many obstacles to prevent its employment into CLIR such as deep syntactic and semantic analysis, user queries consisting of only one or two words, and an arduous task to build a MT system." ></td>
	<td class="line x" title="16:119	Dictionary based query translation is the most popular method because of its easiness to perform." ></td>
	<td class="line x" title="17:119	The main reasons leading to the great drops in CLIP,." ></td>
	<td class="line x" title="18:119	effectiveness by this method are ambiguities caused by more than one translation of a query term and failures to translate phrases during query translation." ></td>
	<td class="line x" title="19:119	Previous studies \[Hull & Grefenstette (1996); Ballesteros & Croft (1996)\] have shown that automatic word-byword (WBW) query translation via machine readable dictionary (MKD) results in a 40-60% loss in effectiveness below that of monolingual retrieval." ></td>
	<td class="line x" title="20:119	With regard to the use of parallel corpora translation method, the critiques one often raises concern the availability of reliable parallel text corpora." ></td>
	<td class="line x" title="21:119	An alternative way is that making use of the comparable corpora because they are easier to be obtained and there are more and more bilingual even multilingual documents on the Internet." ></td>
	<td class="line x" title="22:119	From analyzing a document collection, an associated word list can be yielded and it is often used to expansion the query in monolingual information retrieval \[Qiu & Frei(1993); Jing & Croft(1994)\]." ></td>
	<td class="line x" title="23:119	104 In this paper, a new query translation is presented by combination dictionary based method with the comparable corpora analyzing." ></td>
	<td class="line x" title="24:119	Ambiguity problem and phrase information lost are attacked in dictionary based ChineseEnglish Cross-Language information Retrieval (CECLIR)." ></td>
	<td class="line x" title="25:119	The remainder of this paper is organized as follows: section 1 gives a method to calculate the mutual information matrices of Chinese-English comparable corpora." ></td>
	<td class="line x" title="26:119	Section 2 develops a scheme to select the translations of the Chinese query terms and introduces how the compositional phrase can be kept in our method." ></td>
	<td class="line x" title="27:119	Section 3 presents a set of preliminary experiment on comparable corpora to evaluate our query translation method and gives some explanations." ></td>
	<td class="line x" title="28:119	1.Mutual information matrices calculation We hypothesize that the words in a sentence after being removed the stop words be associated with each other and work together to express a query requirement." ></td>
	<td class="line oc" title="29:119	The association relationship between two words can be indicated by their mutual information, which can be further used to discover phrases \[Church :& Hanks (1990)\]." ></td>
	<td class="line x" title="30:119	If two words are independent with each other, their mutual information would be close to zero." ></td>
	<td class="line x" title="31:119	On the other hand, if they are strongly related, the mutual information would be much greater than zero and they would be much like to be a phrase; if they occur complementarily, the mutual information would be negative." ></td>
	<td class="line x" title="32:119	In conclusion the bigger the mutual information of word pair, the more probable the word phrase would be a phrase." ></td>
	<td class="line x" title="33:119	According to \[Fano (1961)\], we can define the mutual information M1 (tl,t z) of term t I and t z as formula (1)." ></td>
	<td class="line x" title="34:119	MI(q,t2) =log z P(t~'t2) (1) P(t~)P(t2) Where P(tl, t z) is the co-occurrence probability of t~ and t~ in a Chinese sentence." ></td>
	<td class="line x" title="35:119	The reason we select a Chinese sentence to be a window other than a fixed length window is that a full Chinese sentence can keep more linguistic information and consequently, it is more reasonable that we can regard t~ and t 2 to be a phrase when they co-occur in a sentence." ></td>
	<td class="line x" title="36:119	P(t l) and P(t 2) are the occurrence probabilities of term t I and t 2 in a sentence." ></td>
	<td class="line x" title="37:119	These probabilities can be calculated by the occurrence of term t~ and t 2 in the collection as equation (2), (3) and (4)." ></td>
	<td class="line x" title="38:119	P(tl) = n,__~_ (2) N P(t2) = n,2 (3) N P(tl, t2 ) = n,,,,~ (4) N Where nt~, nt2 is the individual term frequency of term t I and t 2 respectively if either of them occur in a sentence of the collection, ntt,t ~ is the co-occurrence frequency of term t I and t 2 if they are all in a sentence of the collection." ></td>
	<td class="line x" title="39:119	N is the number of sentences of the collection." ></td>
	<td class="line x" title="40:119	Replacing (1) with equation (2), (3) and (4), the mutual information of term t I and t 2 can be expressed by following formula." ></td>
	<td class="line x" title="41:119	n,,." ></td>
	<td class="line x" title="42:119	N MI(q,t 2) = log 2 '(5) H h nt 2 Table 2 and table 3 show the occurrence frequency values and mutual information values calculated by formula (5) for three Chinese compositional phrases and their corresponding English phrases respectively found in our comparable corpora." ></td>
	<td class="line x" title="43:119	t 1 It 2 n,, n,2 n,,,,: MI ~,\[-~l~f\]~ 106 84 45 9.28 j~plt~ 45 97 21 9.21 ~\]~\[g~ 73 22 19 10.51 Table 2: Mutual information of three Chinese phrases (N = 123,000) tl I t2 nt~ ntz nt.t~ M1 File I system 158 126 52 8.91 User I management 59 112 18 8.97 Graphic \[ interface 92 41 34 10.70 Table 3: Mutual information of three English phrases (N = 184,000) Anal)zing the Chinese-English comparable corpora in this way, we can get two mutual information value matrices to indicate which two terms (as to the Chinese collection, they are 105 almost Chinese words after segmentation) would be most possible to be a phrase." ></td>
	<td class="line x" title="44:119	A word list associated to each Chinese query term can be obtained by looking up the mutual information value matrix of the Chinese corpus with a cutoff of M1 =1.50." ></td>
	<td class="line x" title="45:119	As discussed above, the bigger the mutual information value between two terms, the more possible the two words would be a phrase." ></td>
	<td class="line x" title="46:119	We can infer that the associated word list of the query term contains the terms that are the most possible components of a compositional phrase." ></td>
	<td class="line x" title="47:119	In other words, the phrase information can be kept by this way." ></td>
	<td class="line x" title="48:119	The Chinese query is translated into English via looking up the English senses of Chinese query term and words in its associated word list in a Chinese-English dictionary." ></td>
	<td class="line x" title="49:119	The procedures how to select appropriate tranlations and to construct the English query are discussed in section 2." ></td>
	<td class="line x" title="50:119	2 Translations selection and phrase keeping It is a naive method to translate a Chinese query only by looking up each Chinese term to get its English senses in a Chinese-English dictionary." ></td>
	<td class="line x" title="51:119	This method, however, results in too many ambiguities during the query translation and offers no path to select appropriate ones among the translations." ></td>
	<td class="line x" title="52:119	In addition, phrases in the query can not be translated effectively." ></td>
	<td class="line x" title="53:119	Previous study has showed that failure to translate phrases greatly reduces the performance by up to 25% over automatic word-by-word (WBW) query translation \[Ballesteros & Croft (1996)\]." ></td>
	<td class="line x" title="54:119	In our method, those English translations most likely co-occur with each other can be obtained via looking up the mutual information value matrix of the English corpus with a cutoff M1 = 1.50." ></td>
	<td class="line x" title="55:119	In this way, the English senses of terms in the associated word list can provide a good context for the translation of the Chinese query term and give a significant clue for its translations selection." ></td>
	<td class="line x" title="56:119	In addition, the information of two terms (either Chinese or English) to be a phrase can also be stored in the associated word list." ></td>
	<td class="line x" title="57:119	In the following, we firstly describe our method to select translations in detail, and then we give an example to demonstrate how to keep the phrase information in our method." ></td>
	<td class="line x" title="58:119	Supposing the Chinese query is expressed by (e 1, e~,.--, e, )." ></td>
	<td class="line x" title="59:119	el, e2,,, e, are the segmented Chinese words of the query after removing the stop words." ></td>
	<td class="line x" title="60:119	The translations of e m (m = 1,,r)by looking up the ChineseEnglish bilingual dictionary can be ordered in descending by following formula." ></td>
	<td class="line x" title="61:119	W(fm t ) = lOglO(Ot'i_Ml(f ~ )+ fl 'o_Ml(fm t )) (6) l~'llgmkl' t ' ' z z i_Mi(fm l ) = k=l j=l / I~llrmkl (7) k=l r \]~1 l Z ZMI(f,~,J~ k) o_ MI(fm l ) = i=l,im k=l (8) r lYd  i=l,i~m Where f~ is one sense of the English translation set F m of the word e,~ (l = 1,,IF.b g. is the association word set of e m . The size of E m is le.I and its element is e~ (k = 1 ,le.I) F~ is the English translation set of emk, its element is f,,~." ></td>
	<td class="line x" title="62:119	ct is the coefficient to emphasize the inner mutual information between the English sense f t of the single Chinese query term e m and the English sense f,~ of the e m's association word emk." ></td>
	<td class="line x" title="63:119	The first part of the formula (6) i_MI(f~) reflects the probability of English translation f,~ and f,~ to be a phrase." ></td>
	<td class="line x" title="64:119	/3 is the coefficient to emphasize the outside mutual information between f,~ and the English sense ~* of the other Chinese terms included in the query." ></td>
	<td class="line x" title="65:119	The second part of the formula (6) o_ Ml(f~) reflects the relevant value between the English sense f,~of e m and the whole query concept." ></td>
	<td class="line x" title="66:119	Our method of translations selection can be described as follows: if the weight of any translation of the Chinese query term is greater than 1.00, the sense is selected to construct the English query." ></td>
	<td class="line x" title="67:119	If there is no weight of any translation of the Chinese query term greater than 1.00, the sense with biggest one is selected to construct the English query." ></td>
	<td class="line x" title="68:119	In this way, we can make an English query by the following Boolean expression." ></td>
	<td class="line x" title="69:119	106 r /IF~I. t '~ Query = 21\[ XI ~glra'W(gra))) (9) Where o I is set element after the English Ora translation sense set F m which is detruncated by our translation selection method." ></td>
	<td class="line x" title="70:119	In order to demonstrate the procedure of our method, we give an example and explain how the English translations are selected and how the phrase information is kept." ></td>
	<td class="line x" title="71:119	Given a simple Chinese query ' ~ fi', '~ ~, ~ ~ (user, management, command)' after segmentation and removing stop words, the associated word list of term ' ~ ~ (user)' is ' '~' 2~ (management), 4-~,~ (information), --f~' (manual)' and the associated word list of term ''~ ~E(management)' is '~ ~ (user), *J~(harrd disk), ~,Aq-(file)'." ></td>
	<td class="line x" title="72:119	We process the associated word ''~2~2(management)' of the query term '~q ~ (user)' in a special way by adding an appropriate value to their mutual information value to let theirs be the biggest in the associated word list, because the associated word ''~(management)' also occurs in the original query." ></td>
	<td class="line x" title="73:119	Similar way is done with the associated word '~ ~ (user)' of the query term '~ ~ (management)'." ></td>
	<td class="line x" title="74:119	In this way,, the compositional phrase ' h~ ~ '~ JX (user management)' can be kept in both associated word list of term 'It/' (user)' and term '~X (management)'." ></td>
	<td class="line x" title="75:119	When term ')~ ~' is translated into English by looking up the general-purpose ChineseEnglish bilingual dictionary, we get its English sense set 'user, consumer' ordered by the formula (6)." ></td>
	<td class="line x" title="76:119	When term '~'JE' is translated into English, we get its English sense set 'management, administration, supervision, run' ordered by the formula (6)." ></td>
	<td class="line x" title="77:119	We can fred the first positions of the English translation set of the query term '~ P' and term ''~'JX' are 'user' and 'management' respectively." ></td>
	<td class="line x" title="78:119	From the point of view of translation, the phrase 'user management' can be regarded as the English phrase translation of '~ # '~ ~'." ></td>
	<td class="line x" title="79:119	According to our translation selection and formula (9), we can construct the English Boolean query as follows, in which each query term has a weight." ></td>
	<td class="line x" title="80:119	Query = (user, 1.86)and ((management, 1.83) or (administration, 1.63)) and (command, 1.92)." ></td>
	<td class="line x" title="81:119	3 Evaluation and discussion To evaluate our query translation method, we did a set of experiment to compare it to the word-by-word (WBW) translation method and manual translation method." ></td>
	<td class="line x" title="82:119	In the word-byword translation method, the Chinese queries are automatically segmented and the Chinese terms included in them are translated into English only by looking up the general-purpose Chinese-English bilingual dictionary." ></td>
	<td class="line x" title="83:119	In the manual translation method, the Chinese queries are translated into English by a Ph.D. student." ></td>
	<td class="line x" title="84:119	The segmentation we used is based on a small general-purpose Chinese-English bilingual dictionary that only contains 46,570 pairs in which each Chinese word has several English translations." ></td>
	<td class="line x" title="85:119	The forward and backward maximum matching algorithm is used to segment the texts and find the combinatorial ambiguities." ></td>
	<td class="line x" title="86:119	Of all the combinatorial ambiguities, 91.2% are removed with the word uni-gram prior probabilities." ></td>
	<td class="line x" title="87:119	A stop word list of 1210 elements is set up, which contains frequently used functional words as well as symbols \[Du & Sun (2000)\]." ></td>
	<td class="line x" title="88:119	Our Chinese query translation process contains following steps: (1) Segment the Chinese query according to the method introduced above." ></td>
	<td class="line x" title="89:119	(2) Get the associated word list of each Chinese term included in the query from the Chinese mutual information matrix." ></td>
	<td class="line x" title="90:119	(3) Look up the English sense set of each Chinese term and its associated word in the general-purpose Chinese-English bilingual dictionary." ></td>
	<td class="line x" title="91:119	(4) Select the English translation sense by the method introduced in section 2 (in formula (6) the coefficents tx and fl are selected by 1.0 and 0.5 respectively in our experiment) and construct the English query on the basis of the formula (9)." ></td>
	<td class="line x" title="92:119	The document collection used in our experiments consists of several Chinese and corresponding English computer manuals, which include Linux-HOWTO, PostgreSQL handbook, Mysql handbook, Linux kernel* and Linux Gazette 17 volumes (from July, 1998 to Dec. , 1999)'." ></td>
	<td class="line x" title="93:119	In order get a large number document Chinese and English collections, we decomposed these manuals and let every document no more than 15 sentences." ></td>
	<td class="line x" title="94:119	As a * http://www.linux forum.nct/books/index.html * *http://www.linuxgazette.com.cn 107 result, Chinese-English bilingual comparable corpora are obtained in which contain about 8,200 Chinese documents and 12,500 English documents." ></td>
	<td class="line x" title="95:119	We design 13 Chinese queries, the average length is about 7 single Chinese character (about three Chinese words)." ></td>
	<td class="line x" title="96:119	All work in this study was performed on the Search2000 information retrieval system \[Du & Zhang (2000)\], which can process both Chinese and English Boolean queries." ></td>
	<td class="line x" title="97:119	Table 4 shows the precision and recall table for the three methods." ></td>
	<td class="line x" title="98:119	The first column in table 4 contains precision values averaged 13 queries and interpolated to eleven recall points from 0.0 to 1.0 in steps of 0.1." ></td>
	<td class="line x" title="99:119	The third column contains precision values achieved by our translation method (QT)." ></td>
	<td class="line x" title="100:119	Precision Precision Precision Recall (WBW) (Manual) (QT) at 0.00 at 0.10 at 0.20 at 0.30 at 0.40 at 0.50 at 0.60 at 0.70 at 0.80 at 0.90 at 1.00 Avg." ></td>
	<td class="line x" title="101:119	0.5831 0.8975 0.6642 0.5132 0.7884 0.5825 0.4036 0.6573 0.5174 0.3771 0.6206 0.4728 0.3128 0.5840 0.4163 0.2816 0.5118 0.3838 0.2143 0.4876 0.3104 0.1641 0.3833 0.2645 0.1110 0.2114 0.1702 0.0741 0.1667 0.1020 0.0212 0.0428 0.0342 0.2778 0.4865 0.3562 Table 4: The results of the three methods The results in table 4 suggest that in this case, the WBW query translation leads to a great drop in effectiveness of 42.90% below that for monolingual retrieval (manual translation method)." ></td>
	<td class="line x" title="102:119	The result of our query translation method greatly improves effectiveness by 28.22% over the WBW method, and its effectiveness is about 73.21% of that for monolingual retrieval." ></td>
	<td class="line x" title="103:119	Although phrase translation is not executed directly in our method, the phrase information is kept effectively in the associated word list." ></td>
	<td class="line x" title="104:119	Therefore, the phrase can be well ~anslated." ></td>
	<td class="line x" title="105:119	The associated word list also provides a good context for translation of the Chinese query terms (corresponding to the first part of formula (6) i_Ml(f~t)) and a good English translation is given a relatively high weight." ></td>
	<td class="line x" title="106:119	The results in table 4 show that our query translation method can construct a good English query and indeed improve the effectiveness." ></td>
	<td class="line x" title="107:119	Conclusion Automatic word-by-word query translation is an attractive method because it is easy to perform, resources are readily available, and performance is similar to that of other CLIP,." ></td>
	<td class="line x" title="108:119	methods." ></td>
	<td class="line x" title="109:119	However, there are a lot of ambiguities in translation of the query terms and failures to translate phrases correctly, which are mainly responsible for the large drops in effectiveness below monolingual retrieval performance." ></td>
	<td class="line x" title="110:119	Aiming to tackle with these problems, we develop a new scheme for how to select translations in this paper." ></td>
	<td class="line x" title="111:119	In addition, rather than using a bilingual phrase dictionary, we also put forward a new method to translate phrases indirectly by using the mutual information between two words in a full sentence and keep the phrase information in the associated word list effectively." ></td>
	<td class="line x" title="112:119	As a result of our query translation method, an English query is constructed in which each query term has a weight." ></td>
	<td class="line x" title="113:119	In this study, our method leads to improve the effectiveness by 28.22% over the word by word query translation method, but is still about 27% below the monolingual retrieval performance." ></td>
	<td class="line x" title="114:119	If query expansion is employed in our method, we expect the performance should be further improved." ></td>
	<td class="line x" title="115:119	A shortcoming of our method is that the cost of calculation of the mutual information matrices is very large." ></td>
	<td class="line x" title="116:119	We are currently exploring an algorithm to generate the matrices more efficiently and the selection of coefficients in formula (6) also needs further research." ></td>
	<td class="line x" title="117:119	Acknowledgements The authors wish to express their appreciation to those interpreters of computer manuals." ></td>
	<td class="line x" title="118:119	Without theft selfless contribution, our experiment would be impossible." ></td>
	<td class="line x" title="119:119	Thanks to the anonymous reviewers for their helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P01-1059
Producing Biographical Summaries: Combining Linguistic Knowledge With Corpus Statistics
Schiffman, Barry;Mani, Inderjeet;Concepcion, Kristian;"></td>
	<td class="line x" title="1:181	Producing Biographical Summaries: Combining Linguistic Knowledge with Corpus Statistics1 Barry Schiffman Columbia University 1214 Amsterdam Avenue New York, NY 10027, USA Bschiff@cs.columbia.edu Inderjeet Mani2 The MITRE Corporation 11493 Sunset Hills Road Reston, VA 20190, USA imani@mitre.org Kristian J. Concepcion The MITRE Corporation 11493 Sunset Hills Road Reston, VA 20190, USA kjc9@mitre.org 1 This work has been funded by DARPAs Translingual Information Detection, Extraction, and Summarization (TIDES) research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049." ></td>
	<td class="line x" title="2:181	2 Also at the Department of Linguistics, Georgetown University, Washington, D. C. 20037." ></td>
	<td class="line x" title="3:181	Abstract We describe a biographical multidocument summarizer that summarizes information about people described in the news." ></td>
	<td class="line x" title="4:181	The summarizer uses corpus statistics along with linguistic knowledge to select and merge descriptions of people from a document collection, removing redundant descriptions." ></td>
	<td class="line x" title="5:181	The summarization components have been extensively evaluated for coherence, accuracy, and non-redundancy of the descriptions produced." ></td>
	<td class="line x" title="6:181	1 Introduction The explosion of the World Wide Web has brought with it a vast hoard of information, most of it relatively unstructured." ></td>
	<td class="line x" title="7:181	This has created a demand for new ways of managing this often unwieldy body of dynamically changing information." ></td>
	<td class="line x" title="8:181	The goal of automatic text summarization is to take a partially-structured source text, extract information content from it, and present the most important content in a condensed form in a manner sensitive to the needs of the user and task (Mani and Maybury 1999)." ></td>
	<td class="line x" title="9:181	Summaries can be generic, i.e., aimed at a broad audience, or topic-focused, i.e., tailored to the requirements of a particular user or group of users." ></td>
	<td class="line x" title="10:181	Multi-Document Summarization (MDS) is, by definition, the extension of single-document summarization to collections of related documents." ></td>
	<td class="line x" title="11:181	MDS can potentially help the user to see at a glance what a collection is about, or to examine similarities and differences in the information content in the collection." ></td>
	<td class="line x" title="12:181	Specialized multi-document summarization systems can be constructed for various applications; here we discuss a biographical summarizer." ></td>
	<td class="line x" title="13:181	Biographies can, of course, be long, as in book-length biographies, or short, as in an authors description on a book jacket." ></td>
	<td class="line x" title="14:181	The nature of descriptions in the biography can vary, from physical characteristics (e.g. , for criminal suspects) to scientific or other achievements (e.g. , a speakers biography)." ></td>
	<td class="line x" title="15:181	The crucial point here is that facts about a persons life are selected, organized, and presented so as to meet the compression and task requirements." ></td>
	<td class="line x" title="16:181	While book-quality biographies are out of reach of computers, many other kinds can be synthesized by sifting through large quantities of on-line information, a task that is tedious for humans to carry out." ></td>
	<td class="line x" title="17:181	We report here on the development of a biographical MDS summarizer that summarizes information about people described in the news." ></td>
	<td class="line x" title="18:181	Such a summarizer is of interest, for example, to analysts who want to automatically construct a dossier about a person over time." ></td>
	<td class="line x" title="19:181	Rather than determining in advance what sort of information should go into a biography, our approach is more data-driven, relying on discovering how people are actually described in news reports in a collection." ></td>
	<td class="line x" title="20:181	We use corpus statistics from a background corpus along with linguistic knowledge to select and merge descriptions from a document collection, removing redundant descriptions." ></td>
	<td class="line x" title="21:181	The focus here is on synthesizing succinct descriptions." ></td>
	<td class="line x" title="22:181	The problem of assembling these descriptions into a coherent narrative is not a focus of our paper; the system currently uses canned text methods to produce output text containing these descriptions." ></td>
	<td class="line x" title="23:181	Obviously, the merging of descriptions should take temporal information into account; this very challenging issue is also not addressed here." ></td>
	<td class="line x" title="24:181	To give a clearer idea of the systems output, here are some examples of biographies produced by our system (the descriptions themselves are underlined, the rest is canned text)." ></td>
	<td class="line x" title="25:181	The biographies contain descriptions of the salient attributes and activities of people in the corpus, along with lists of their associates." ></td>
	<td class="line x" title="26:181	These short summaries illustrate the extent of compression provided." ></td>
	<td class="line x" title="27:181	The first two summaries are of a collection of 1300 wire service news documents on the Clinton impeachment proceedings (707,000 words in all, called the Clinton corpus)." ></td>
	<td class="line x" title="28:181	In this corpus, there are 607 sentences mentioning Vernon Jordan by name, from which the system extracted 82 descriptions expressed as appositives (78) and relative clauses (4), along with 65 descriptions consisting of sentences whose deep subject is Jordan." ></td>
	<td class="line x" title="29:181	The 4 relative clauses are duplicates of one another: who helped Lewinsky find a job." ></td>
	<td class="line x" title="30:181	The 78 appositives fall into just 2 groups: friend (or equivalent descriptions, such as confidant), adviser (or equivalent such as lawyer)." ></td>
	<td class="line x" title="31:181	The sentential descriptions are filtered in part based on the presence of verbs like testify, plead, or greet that are strongly associated with the head noun of the appositive, namely friend." ></td>
	<td class="line x" title="32:181	The target length can be varied to produce longer summaries." ></td>
	<td class="line x" title="33:181	Vernon Jordan is a presidential friend and a Clinton adviser." ></td>
	<td class="line x" title="34:181	He is 63 years old." ></td>
	<td class="line x" title="35:181	He helped Ms. Lewinsky find a job." ></td>
	<td class="line x" title="36:181	He testified that Ms. Monica Lewinsky said that she had conversations with the president, that she talked to the president." ></td>
	<td class="line x" title="37:181	He has numerous acquaintances, including Susan Collins, Betty Currie, Pete Domenici, Bob Graham, James Jeffords and Linda Tripp." ></td>
	<td class="line x" title="38:181	1,300 docs, 707,000 words (Clinton corpus) 607 Jordan sentences, 78 extracted appositives, 2 groups: friend, adviser." ></td>
	<td class="line x" title="39:181	Henry Hyde is a Republican chairman of House Judiciary Committee and a prosecutor in Senate impeachment trial." ></td>
	<td class="line x" title="40:181	He will lead the Judiciary Committee's impeachment review." ></td>
	<td class="line x" title="41:181	Hyde urged his colleagues to heed their consciences, the voice that whispers in our ear, duty, duty, duty. Clinton corpus, 503 Hyde sentences, 108 extracted appositives, 2 groups: chairman, impeachment prosecutor." ></td>
	<td class="line x" title="42:181	Victor Polay is the Tupac Amaru rebels' top leader, founder and the organization's commander-and-chief." ></td>
	<td class="line x" title="43:181	He was arrested again in 1992 and is serving a life sentence." ></td>
	<td class="line x" title="44:181	His associates include Alberto Fujimori, Tupac Amaru Revolutionary, and Nestor Cerpa." ></td>
	<td class="line x" title="45:181	73 docs, 38,000 words, 24 Polay sentences, 10 extracted appositives, 3 groups: leader, founder and commander-in-chief." ></td>
	<td class="line x" title="46:181	2 Producing biographical descriptions 2.1 Preprocessing Each document in the collection to be summarized is processed by a sentence tokenizer, the Alembic part-of-speech tagger (Aberdeen et al. 1995), the Nametag named entity tagger (Krupka 1995) restricted to people names, and the CASS parser (Abney 1996)." ></td>
	<td class="line x" title="47:181	The tagged sentences are further analyzed by a cascade of finite state machines leveraging patterns with lexical and syntactic information, to identify constructions such as preand postmodifying appositive phrases, e.g., Presidential candidate George Bush, Bush, the presidential candidate, and relative clauses, e.g., Senator , who is running for re-election this Fall,." ></td>
	<td class="line x" title="48:181	These appositive phrases and relative clauses capture descriptive information which can correspond variously to a persons age, occupation, or some role a person played in an incident." ></td>
	<td class="line x" title="49:181	In addition, we also extract sentential descriptions in the form of sentences whose (deep) subjects are person names." ></td>
	<td class="line x" title="50:181	2.2 Cross-document coreference The classes of person names identified within each document are then merged across documents in the collection using a crossdocument coreference program from the Automatic Content Extraction (ACE) research program (ACE 2000), which compares names across documents based on similarity of a window of words surrounding each name, as well as specific rules having to do with different ways of abbreviating a persons name (Mani and MacMillan 1995)." ></td>
	<td class="line x" title="51:181	The end result of this process is that for each distinct person, the set of descriptions found for that person in the collection are grouped together." ></td>
	<td class="line x" title="52:181	2.3 Appositives 2.3.1 Introduction The appositive phrases usually provide descriptions of attributes of a person." ></td>
	<td class="line x" title="53:181	However, the preprocessing component described in Section 2.1 does produce errors in appositive extraction, which are filtered out by syntactic and semantic tests." ></td>
	<td class="line x" title="54:181	The system also filters out redundant descriptions, both duplicate descriptions as well as similar ones." ></td>
	<td class="line x" title="55:181	These filtering methods are discussed next." ></td>
	<td class="line x" title="56:181	2.3.2 Pruning Erroneous and Duplicate Appositives The appositive descriptions are first pruned to record only one instance of an appositive phrase which has multiple repetitions, and descriptions whose head does not appear to refer to a person." ></td>
	<td class="line x" title="57:181	The latter test relies on a person typing program which uses semantic information from WordNet 1.6 (Miller 1995) to test whether the head of the description is a person." ></td>
	<td class="line x" title="58:181	A given string is judged as a person if a threshold percentage 1 (set to 35% in our work) of senses of the string are descended from the synset for Person in WordNet." ></td>
	<td class="line x" title="59:181	For example, this picks out counsel as a person, but accessory as a non-person." ></td>
	<td class="line x" title="60:181	2.3.3 Merging Similar Appositives The pruning of erroneous and duplicate descriptions still leaves a large number of redundant appositive descriptions across documents." ></td>
	<td class="line x" title="61:181	The system compares each pair of appositive descriptions of a person, merging them based on corpus frequencies of the description head stem, syntactic information, and semantic information based on the relationship between the heads in WordNet." ></td>
	<td class="line x" title="62:181	The descriptions are merged if they have the same head stem, or if both heads have a common parent below Person in WordNet (in the latter case the head which is more frequent in the corpus is chosen as the merged head), or if one head subsumes the other under Person in WordNet (in which case the more general head is chosen)." ></td>
	<td class="line x" title="63:181	When the heads of descriptions are merged, the most frequent modifying phrase that appears in the corpus with the selected head is used." ></td>
	<td class="line x" title="64:181	When a person ends up with more than one description, the modifiers are checked for duplication, with distinct modifiers being conjoined together, so that Wisconsin lawmaker and Wisconsin democrat yields Wisconsin lawmaker and Democrat." ></td>
	<td class="line x" title="65:181	Prepositional phrase variants of descriptions are also merged here, so that chairman of the Budget Committee and Budget Committee Chairman are merged." ></td>
	<td class="line x" title="66:181	Modifiers are dropped but their original order is preserved for the sake of fluency." ></td>
	<td class="line x" title="67:181	2.3.4 Appositive Description Weighting The system then weights the appositives for inclusion in a summary." ></td>
	<td class="line x" title="68:181	A persons appositives are grouped into equivalence classes, with a single head noun being chosen for each equivalence class, with a weight for that class based on the corpus frequency of the head noun." ></td>
	<td class="line x" title="69:181	The system then picks descriptions in decreasing order of class weight until either the compression rate is achieved or the head noun is no longer in the top 2 % most frequent descriptions (2 is set to 90% in our work)." ></td>
	<td class="line x" title="70:181	Note that the summarizer refrains from choosing a subsuming term from WordNet that is not present in the descriptions, preferring to not risk inventing new descriptions, instead confining itself to cutting and pasting of actual words used in the document." ></td>
	<td class="line x" title="71:181	2.4 Relative Clause Weighting Once the relative clauses have been pruned for duplicates, the system weights the appositive clauses for inclusion in a summary." ></td>
	<td class="line x" title="72:181	The weighting is based on how often the relative clauses main verb is strongly associated with a (deep) subject in a large corpus, compared to its total number of appearances in the corpus." ></td>
	<td class="line x" title="73:181	The idea here is to weed out promiscuous verbs that are weakly associated with lots of subjects." ></td>
	<td class="line x" title="74:181	The corpus statistics are derived from the Reuters portion of the North American News Text Corpus (called Reuters in this paper) -nearly three years of wire service news reports containing 105.5 million words." ></td>
	<td class="line x" title="75:181	Examples of verbs in the Reuters corpus which show up as promiscuous include get, like, give, intend, add, want, be, do, hope, think, make, dream, have, say, see, tell, try." ></td>
	<td class="line x" title="76:181	In a test, detailed below in Section 4.2, this feature fired 40 times in 184 trials." ></td>
	<td class="line x" title="77:181	To compute strong associations, we proceed as follows." ></td>
	<td class="line x" title="78:181	First, all subject-verb pairs are extracted from the Reuters corpus with a specially developed finite state grammar and the CASS parser." ></td>
	<td class="line x" title="79:181	The head nouns and main verbs are reduced to their base forms by changing plural endings and tense markers for the verbs." ></td>
	<td class="line x" title="80:181	Also included are gapped subjects, such as the subject of run in the student promised to run the experiment; in this example, both pairs student-promise and student-run are recorded." ></td>
	<td class="line x" title="81:181	Passive constructions are also recognized and the object of the by-PP following the verb is taken as the deep subject." ></td>
	<td class="line oc" title="82:181	Strength of association between subject i and verb j is measured using mutual information (Church and Hanks 1990): )ln(),( ji ij tftf tfNjiMI  = . Here tfij is the maximum frequency of subject-verb pair ij in the Reuters corpus, tfi is the frequency of subject head noun i in the corpus, tfj is the frequency of verb j in the corpus, and N is the number of terms in the corpus." ></td>
	<td class="line x" title="83:181	The associations are only scored for tf counts greater than 4, and a threshold 3 (set to log score > -21 in our work) is used for a strong association." ></td>
	<td class="line x" title="84:181	The relative clauses are thus filtered initially (Filter 1) by excluding those whose main verbs are highly promiscuous." ></td>
	<td class="line x" title="85:181	Next, they are filtered (Filter 2) based on various syntactic features, as well as the number of proper names and pronouns." ></td>
	<td class="line x" title="86:181	Finally, the relative clauses are scored conventionally (Filter 3) by summing the within-document relative term frequency of content terms in the clause (i.e. , relative to the number of terms in the document), with an adjustment for sentence length (achieved by dividing by the total number of content terms in the clause)." ></td>
	<td class="line x" title="87:181	3 Sentential Descriptions These descriptions are the relatively large set of sentences which have a person name as a (deep) subject." ></td>
	<td class="line x" title="88:181	We filter them based on whether their main verb is strongly associated with either of the head nouns of the appositive descriptions found for that person name (Filter 4)." ></td>
	<td class="line x" title="89:181	The intuition here is that particular occupational roles will be strongly associated with particular verbs." ></td>
	<td class="line x" title="90:181	For example, politicians vote and elect, executives resign and appoint, police arrest and shoot; so, a summary of information about a policeman may include an arresting and shooting event he was involved with." ></td>
	<td class="line x" title="91:181	(The verboccupation association isnt manifest in relative clauses because the latter are too few in number)." ></td>
	<td class="line x" title="92:181	A portion of the results of doing this is shown in Table 1." ></td>
	<td class="line x" title="93:181	The results for executive are somewhat loose, whereas for politician and police, the associations seem tighter, with the associated verbs meeting our intuitions." ></td>
	<td class="line x" title="94:181	All sentences which survive Filter 4 are extracted and then scored, just as relative clauses are, using Filter 1 and Filter 3." ></td>
	<td class="line x" title="95:181	Filter 4 alone provides a high degree of compression; for example, it reduces a total of 16,000 words in the combined sentences that include Vernon Jordan' s name in the Clinton corpus to 578 words in 12 sentences; sentences up to the target length can be selected from these based on scores from Filter 1 and then Filter 3." ></td>
	<td class="line x" title="96:181	However, there are several difficulties with these sentences." ></td>
	<td class="line x" title="97:181	First, we are missing a lot of them due to the fact that we do not as yet handle pronominal subjects which are coreferential with the proper name." ></td>
	<td class="line x" title="98:181	Second, these sentences contain lots of dangling anaphors, which will need to be resolved." ></td>
	<td class="line x" title="99:181	Third, there may be redundancy between the sentential descriptions, on one hand, and the appositive and relative clause descriptions, on the other." ></td>
	<td class="line x" title="100:181	Finally, the entire sentence is extracted, including any subordinate clauses, although we are working on refinements involving sentence compaction." ></td>
	<td class="line x" title="101:181	As a result, we believe that more work is required before the sentential descriptions can be fully integrated into the biographies." ></td>
	<td class="line x" title="102:181	executive police politician reprimand 16.36 shoot 17.37 clamor 16.94 conceal 17.46 raid 17.65 jockey 17.53 bank 18.27 arrest 17.96 wrangle 17.59 foresee 18.85 detain 18.04 woo 18.92 conspire 18.91 disperse 18.14 exploit 19.57 convene 19.69 interrogate18.36 brand 19.65 plead 19.83 swoop 18.44 behave 19.72 sue 19.85 evict 18.46 dare 19.73 answer 20.02 bundle 18.50 sway 19.77 commit 20.04 manhandle18.59 criticize 19.78 worry 20.04 search 18.60 flank 19.87 accompany 20.11 confiscate 18.63 proclaim 19.91 own 20.22 apprehend18.71 annul 19.91 witness 20.28 round 18.78 favor 19.92 testify 20.40 corner 18.80 denounce20.09 shift 20.42 pounce 18.81 condemn20.10 target 20.56 hustle 18.83 prefer 20.14 lie 20.58 nab 18.83 wonder 20.18 expand 20.65 storm 18.90 dispute 20.18 learn 20.73 tear 19.00 interfere 20.37 shut 20.80 overpower19.09 voice 20.38 Table 1." ></td>
	<td class="line x" title="103:181	Verbs strongly associated with particular classes of people in the Reuters corpus (negative log scores)." ></td>
	<td class="line x" title="104:181	4 Evaluation 4.1 Overview Methods for evaluating text summarization can be broadly classified into two categories (Sparck-Jones and Galliers 1996)." ></td>
	<td class="line x" title="105:181	The first, an extrinsic evaluation, tests the summarization based on how it affects the completion of some other task, such as comprehension, e.g., (Morris et al. 1992), or relevance assessment (Brandow et al. 1995) (Jing et al. 1998) (Tombros and Sanderson 1998) (Mani et al. 1998)." ></td>
	<td class="line x" title="106:181	An intrinsic evaluation, on the other hand, can involve assessing the coherence of the summary (Brandow et al. 1995) (Saggion and Lapalme 2000)." ></td>
	<td class="line x" title="107:181	Another intrinsic approach involves assessing the informativeness of the summary, based on to what extent key information from the source is preserved in the system summary at different levels of compression (Paice and Jones 1993), (Brandow et al. 1995)." ></td>
	<td class="line x" title="108:181	Informativeness can also be assessed in terms of how much information in an ideal (or reference) summary is preserved in the system summary, where the summaries being compared are at similar levels of compression (Edmundson 1969)." ></td>
	<td class="line x" title="109:181	We have carried out a number of intrinsic evaluations of the accuracy of components involved in the summarization process, as well as the succinctness, coherence and informativeness of the descriptions." ></td>
	<td class="line x" title="110:181	As this is a MDS system, we also evaluate the nonredundancy of the descriptions, since similar information may be repeated across documents." ></td>
	<td class="line x" title="111:181	4.2 Person Typing Evaluation The component evaluation tests how accurately the tagger can identify whether a head noun in a description is appropriate as a person description The evaluation uses the WordNet 1.6 SEMCOR semantic concordance, which has files from the Brown corpus whose words have semantic tags (created by WordNet' s creators) indicating WordNet sense numbers." ></td>
	<td class="line x" title="112:181	Evaluation on 6,000 sentences with almost 42,000 nouns compares people tags generated by the program with SEMCOR tags, and provided the following results: right = 41,555, wrong = 1,298, missing = 0, yielding Precision, Recall, and F-Measure of 0.97." ></td>
	<td class="line x" title="113:181	4.3 Relative Clause Extraction Evaluation This component evaluation tests the wellformedness of the extracted relative clauses." ></td>
	<td class="line x" title="114:181	For this evaluation, we used the Clinton corpus." ></td>
	<td class="line x" title="115:181	The relative clause is judged correct if it has the right extent, and the correct coreference index indicating which person the relative clause description pertains to." ></td>
	<td class="line x" title="116:181	The judgments are based on 36 instances of relative clauses from 22 documents." ></td>
	<td class="line x" title="117:181	The results show 28 correct relative clauses found, plus 4 spurious finds, yielding Precision of 0.87, Recall of 0.78, and F-measure of .82." ></td>
	<td class="line x" title="118:181	Although the sample is small, the results are very promising." ></td>
	<td class="line x" title="119:181	4.4 Appositive Merging Evaluation This component evaluation tests the systems ability to accurately merge appositive descriptions." ></td>
	<td class="line x" title="120:181	The score is based on an automatic comparison of the systems merge of systemgenerated appositive descriptions against a human merge of them." ></td>
	<td class="line x" title="121:181	We took all the names that were identified in the Clinton corpus and ran the system on each document in the corpus." ></td>
	<td class="line x" title="122:181	We took the raw descriptions that the system produced before merging, and wrote a brief description by hand for each person who had two or more raw descriptions." ></td>
	<td class="line x" title="123:181	The hand-written descriptions were not done with any reference to the automatically merged descriptions nor with any reference to the underlying source material." ></td>
	<td class="line x" title="124:181	The hand-written descriptions were then compared with the final output of the system (i.e. , the result after merging)." ></td>
	<td class="line x" title="125:181	The comparison was automatic, measuring similarity among vectors of content words (i.e. , stop words such as articles and prepositions were removed)." ></td>
	<td class="line x" title="126:181	Here is an example to further clarify the strict standard of the automatic evaluation (words scored correct are underlined): System: E. Lawrence Barcella is a Washington lawyer, Washington white-collar defense lawyer, former federal prosecutor System Merge: Washington white-collar defense lawyer Human Merge: a Washington lawyer and former federal prosecutor Automatic Score: Correct=2; Extra-Words=2; Missed-Words=3 Thus, although lawyer and prosecutor are synonymous in WordNet, the automatic scorer doesnt know that, and so prosecutor is penalized as an extra word." ></td>
	<td class="line x" title="127:181	The evaluation was carried out over the entire Clinton corpus, with descriptions compared for 226 people who had more than one description." ></td>
	<td class="line x" title="128:181	65 out of the 226 descriptions were Correct (28%), with a further 32 cases being semantically correct obviously similar substitutions which the automatic scorer missed (giving an adjusted accuracy of 42%)." ></td>
	<td class="line x" title="129:181	As a baseline, a merging program which performed just a string match scored 21% accuracy." ></td>
	<td class="line x" title="130:181	The major problem areas were errors in coreference (e.g. , Clinton family members being put in the same coreference class), lack of good descriptions for famous people (news articles tend not to introduce such people), and parsing limitations (e.g. , Senator Clinton being parsed erroneously as an NP in The Senator Clinton disappointed)." ></td>
	<td class="line x" title="131:181	Ultimately, of course, domain-independent systems like ours are limited semantically in merging by the lack of world knowledge, e.g., knowing that Starr' s chief lieutenant can be a prosecutor." ></td>
	<td class="line x" title="132:181	4.5 Description Coherence and Informativeness Evaluation To assess the coherence and informativeness of the relative clause descriptions3, we asked 4 subjects who were unaware of our research to judge descriptions generated by our system from the Clinton corpus." ></td>
	<td class="line x" title="133:181	For each relative clause description, the subject was given the description, a person name to whom that description pertained, and a capsule description consisting of merged appositives created by the system." ></td>
	<td class="line x" title="134:181	The subject was asked to assess (a) the coherence of the relative clause description in terms of its succinctness (was it a good length?)" ></td>
	<td class="line x" title="135:181	and its comprehensibility (was it and understandable by itself or in conjunction with the capsule?), and (b) its informativeness in terms of whether it was an accurate description (does it conflict with the capsule or with what you know)?" ></td>
	<td class="line x" title="136:181	and whether it was non-redundant (is it distinct or does it repeat what is in the capsule?)." ></td>
	<td class="line x" title="137:181	The subjects marked 87% of the descriptions as accurate, 96% as non-redundant, and 65% as coherent." ></td>
	<td class="line x" title="138:181	A separate 3-subject inter3 Appositives are not assessed in this way as few errors of coherence or informativeness were noticed in the appositive extraction." ></td>
	<td class="line x" title="139:181	annotator agreement study, where all subjects judged the same 46 decisions, showed that all three subjects agreed on 82% of the accuracy decisions, 85% of the non-redundancy decisions and 82% of the coherence decisions." ></td>
	<td class="line x" title="140:181	5 Learning to Produce Coherent Descriptions 5.1 Overview To learn rules for coherence for extracting sentential descriptions, we used the examples and judgments we obtained for coherence in the evaluation of relative clause descriptions in Section 4.5." ></td>
	<td class="line x" title="141:181	Our focus was on features that might relate to content and specificity: low verb promiscuity scores, presence of proper names, pronouns, definite and indefinite clauses." ></td>
	<td class="line x" title="142:181	The entire list is as follows: badend: boolean." ></td>
	<td class="line x" title="143:181	is there an impossible end, indicating a bad extraction (  Mr.)?" ></td>
	<td class="line x" title="144:181	bestverb: continuous." ></td>
	<td class="line x" title="145:181	use the verb promiscuity threshhold 3 to find the score of the most nonpromiscuous verb in the clause classes (label): boolean." ></td>
	<td class="line x" title="146:181	accept the clause, reject the clause count pronouns: continuous." ></td>
	<td class="line x" title="147:181	number of personal pronouns count proper: continuous." ></td>
	<td class="line x" title="148:181	number of nouns tagged as NP hasobject: continuous." ></td>
	<td class="line x" title="149:181	how many np'sfollow the verb?" ></td>
	<td class="line x" title="150:181	haspeople: continuous." ></td>
	<td class="line x" title="151:181	how many 'name'constituents are found?" ></td>
	<td class="line x" title="152:181	has possessive: continuous." ></td>
	<td class="line x" title="153:181	how many possessive pronouns are there?" ></td>
	<td class="line x" title="154:181	hasquote: boolean." ></td>
	<td class="line x" title="155:181	is there a quotation?" ></td>
	<td class="line x" title="156:181	hassubc: boolean." ></td>
	<td class="line x" title="157:181	is there a subordinateclause?" ></td>
	<td class="line x" title="158:181	isdefinite: continuous." ></td>
	<td class="line x" title="159:181	how many definiteNP's are there?" ></td>
	<td class="line x" title="160:181	repeater: boolean." ></td>
	<td class="line x" title="161:181	is the subject's namerepeated, or is there no subject?" ></td>
	<td class="line x" title="162:181	timeref: boolean." ></td>
	<td class="line x" title="163:181	is there a timereference?" ></td>
	<td class="line x" title="164:181	withquit: is there a quit or resignverb?" ></td>
	<td class="line x" title="165:181	withsay: boolean." ></td>
	<td class="line x" title="166:181	is there a say verb inthe clause?" ></td>
	<td class="line x" title="167:181	5.2 Accuracy of Learnt Descriptions Table 2 provides information on different learning methods." ></td>
	<td class="line x" title="168:181	The results are for a ten-fold cross-validation on 165 training vectors and 19 test vectors, measured in terms of Predictive Accuracy (percentage test vectors correctly classified)." ></td>
	<td class="line x" title="169:181	Tool Accuracy Barrys Rules .69 MC4 Decision Tree .69 C4.5Rules .67 Ripper .62 Naive Bayes .62 Majority Class (coherent) .60 Table 2." ></td>
	<td class="line x" title="170:181	Accuracy of Different Description Learners on Clinton corpus The best learning methods are comparable with rules created by hand by one of the authors (Barrys rules)." ></td>
	<td class="line x" title="171:181	In the learners, the bestverb feature is used heavily in tests for the negative class, whereas in Barrys Rules it occurs in tests for the positive class." ></td>
	<td class="line x" title="172:181	6 Related Work Our work on measuring subject-verb associations has a different focus from the previous work." ></td>
	<td class="line x" title="173:181	(Lee and Pereira 1999), for example, examined verb-object pairs." ></td>
	<td class="line x" title="174:181	Their focus was on a method that would improve techniques for gathering statistics where there are a multitude of sparse examples." ></td>
	<td class="line x" title="175:181	We are focusing on the use of the verbs for the specific purpose of finding associations that we have previously observed to be strong, with a view towards selecting a clause or sentence, rather than just to measure similarity." ></td>
	<td class="line x" title="176:181	We also try to strengthen the numbers by dealing with gapped constructions." ></td>
	<td class="line x" title="177:181	While there has been plenty of work on extracting named entities and relations between them, e.g., (MUC-7 1998), the main previous body of work on biographical summarization is that of (Radev and McKeown 1998)." ></td>
	<td class="line x" title="178:181	The fundamental differences in our work are as follows: (1) We extract not only appositive phrases, but also clauses at large based on corpus statistics; (2) We make heavy use of coreference, whereas they dont use coreference at all; (3) We focus on generating succinct descriptions by removing redundancy and merging, whereas they categorize descriptions using WordNet, without a focus on succinctness." ></td>
	<td class="line x" title="179:181	7 Conclusion This research has described and evaluated techniques for producing a novel kind of summary called biographical summaries." ></td>
	<td class="line x" title="180:181	The techniques use syntactic analysis and semantic type-checking (from WordNet), in combination with a variety of corpus statistics." ></td>
	<td class="line x" title="181:181	Future directions could include improved sentential descriptions as well as further intrinsic and extrinsic evaluations of the summarizer as a whole (i.e. , including canned text)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W01-0513
Is Knowledge-Free Induction Of Multiword Unit Dictionary Headwords A Solved Problem?
Schone, Patrick;Jurafsky, Daniel;"></td>
	<td class="line x" title="1:211	Is Knowledge-Free Induction of Multiword Unit Dictionary Headwords a Solved Problem?" ></td>
	<td class="line x" title="2:211	Patrick Schone and Daniel Jurafsky University of Colorado, Boulder CO 80309 {schone, jurafsky}@cs.colorado.edu Abstract We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords." ></td>
	<td class="line x" title="3:211	We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement." ></td>
	<td class="line x" title="4:211	We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach." ></td>
	<td class="line x" title="5:211	1 Introduction A multiword unit (MWU) is a connected collocation: a sequence of neighboring words whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components (Choueka, 1988)." ></td>
	<td class="line x" title="6:211	In other words, MWUs are typically non-compositional at some linguistic level." ></td>
	<td class="line x" title="7:211	For example, phonological non-compositionality has been observed (Finke & Weibel, 1997; Gregory, et al, 1999) where words like got [gG3Ct] and to [tu] change phonetically to gotta [gG3CG72G46] when combined." ></td>
	<td class="line x" title="8:211	We have interest in inducing headwords for machine-readable dictionaries (MRDs), so our interest is in semantic rather than phonological non-compositionality." ></td>
	<td class="line x" title="9:211	As an example of semantic non-compositionality, consider compact disk: one could not deduce that it was a music medium by only considering the semantics of compact and disk. MWUs may also be non-substitutable and/or non-modifiable (Manning and Schtze, 1999)." ></td>
	<td class="line x" title="10:211	Nonsubstitutability implies that substituting a word of the MWU with its synonym should no longer convey the same original content: compact disk does not readily imply densely-packed disk. Nonmodifiability, on the other hand, suggests one cannot modify the MWUs structure and still convey the same content: compact disk does not signify disk that is compact. MWU dictionary headwords generally satisfy at least one of these constraints." ></td>
	<td class="line x" title="11:211	For example, a compositional phrase would typically be excluded from a hard-copy dictionary since its constituent words would already be listed." ></td>
	<td class="line x" title="12:211	These strategies allow hard-copy dictionaries to remain compact." ></td>
	<td class="line x" title="13:211	As mentioned, we wish to find MWU headwords for machine-readable dictionaries (MRDs)." ></td>
	<td class="line x" title="14:211	Although space is not an issue in MRDs, we desire to follow the lexicographic practice of reducing redundancy." ></td>
	<td class="line x" title="15:211	As Sproat indicated, 'simply expanding the dictionary to encompass every word one is ever likely to encounter is wrong: it fails to take advantage of regularities' (1992, p. xiii)." ></td>
	<td class="line x" title="16:211	Our goal is to identify an automatic, knowledge-free algorithm that finds all and only those collocations where it is necessary to supply a definition." ></td>
	<td class="line x" title="17:211	Knowledge-free means that the process should proceed without human input (other than, perhaps, indicating whitespace and punctuation)." ></td>
	<td class="line x" title="18:211	This seems like a solved problem." ></td>
	<td class="line x" title="19:211	Many collocation-finders exist, so one might suspect that most could suffice for finding MWU dictionary headwords." ></td>
	<td class="line x" title="20:211	To verify this, we evaluate nine existing collocation-finders to see which best identifies valid headwords." ></td>
	<td class="line x" title="21:211	We evaluate using two completely separate gold standards: (1) WordNet and (2) a compendium of Internet dictionaries." ></td>
	<td class="line x" title="22:211	Although web-based resources are dynamic and have better coverage than WordNet (especially for acronyms and names), we show that WordNet-based scores are comparable to those using Internet MRDs." ></td>
	<td class="line x" title="23:211	Yet the evaluations indicate that significant improvement is still needed in MWU-induction." ></td>
	<td class="line x" title="24:211	As an attempt to improve MWU headword induction, we introduce several algorithms using Latent Semantic Analysis (LSA)." ></td>
	<td class="line x" title="25:211	LSA is a technique which automatically induces semantic relationships between words." ></td>
	<td class="line x" title="26:211	We use LSA to try to eliminate proposed MWUs which are semantically compositional." ></td>
	<td class="line x" title="27:211	Unfortunately, this does not help." ></td>
	<td class="line x" title="28:211	Yet when we use LSA to identify substitutable delimiters." ></td>
	<td class="line x" title="29:211	This suggests that in a language with MWUs, we do show modest performance gains." ></td>
	<td class="line x" title="30:211	whitespace, one might prefer to begin at the word 2 Previous Approaches For decades, researchers have explored various techniques for identifying interesting collocations." ></td>
	<td class="line x" title="31:211	There have essentially been three separate kinds of approaches for accomplishing this task." ></td>
	<td class="line x" title="32:211	These approaches could be broadly classified into (1) segmentation-based, (2) word-based and knowledgedriven, or (3) word-based and probabilistic." ></td>
	<td class="line x" title="33:211	We will illustrate strategies that have been attempted in each of the approaches." ></td>
	<td class="line x" title="34:211	Since we assume knowledge of whitespace, and since many of the first and all of the second categories rely upon human input, we will be most interested in the third category." ></td>
	<td class="line x" title="35:211	2.1 Segmentation-driven Strategies Some researchers view MWU-finding as a natural by-product of segmentation." ></td>
	<td class="line x" title="36:211	One can regard text as a stream of symbols and segmentation as a means of placing delimiters in that stream so as to separate logical groupings of symbols from one another." ></td>
	<td class="line x" title="37:211	A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere." ></td>
	<td class="line x" title="38:211	In such cases, these larger units may be MWUs." ></td>
	<td class="line x" title="39:211	The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et." ></td>
	<td class="line x" title="40:211	al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al. , 2000; and many others)." ></td>
	<td class="line x" title="41:211	Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression." ></td>
	<td class="line x" title="42:211	Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not." ></td>
	<td class="line x" title="43:211	These approaches could be applied to languages where word delimiters exist (such as in European languages delimited by the space character)." ></td>
	<td class="line x" title="44:211	However, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the level and identify appropriate word combinations." ></td>
	<td class="line x" title="45:211	2.2 Word-based, knowledge-driven Strategies Some researchers start with words and propose MWU induction methods that make use of parts of speech, lexicons, syntax or other linguistic structure (Justeson and Katz, 1995; Jacquemin, et al. , 1997; Daille, 1996)." ></td>
	<td class="line x" title="46:211	For example, Justeson and Katz indicated that the patterns NOUN NOUN and ADJ NOUN are very typical of MWUs." ></td>
	<td class="line x" title="47:211	Daille also suggests that in French, technical MWUs follow patterns such as NOUN de NOUN' (1996, p. 50)." ></td>
	<td class="line x" title="48:211	To find word combinations that satisfy such patterns in both of these situations necessitates the use of a lexicon equipped with part of speech tags." ></td>
	<td class="line x" title="49:211	Since we are interested in knowledge-free induction of MWUs, these approaches are less directly related to our work." ></td>
	<td class="line x" title="50:211	Furthermore, we are not really interested in identifying constructs such as general noun phrases as the above rules might generate, but rather, in finding only those collocations that one would typically need to define." ></td>
	<td class="line x" title="51:211	2.3 Word-based, Probabilistic Approaches The third category assumes at most whitespace and punctuation knowledge and attempts to infer MWUs using word combination probabilities." ></td>
	<td class="line x" title="52:211	Table 1 (see next page) shows nine commonly-used probabilistic MWU-induction approaches." ></td>
	<td class="line x" title="53:211	In the table, f and P signify frequency and probability XX of a word X. A variable XY indicates a word bigram and indicates its expected frequency at random." ></td>
	<td class="line x" title="54:211	XY An overbar signifies a variables complement." ></td>
	<td class="line x" title="55:211	For more details, one can consult the original sources as well as Ferreira and Pereira (1999) and Manning and Schtze (1999)." ></td>
	<td class="line x" title="56:211	3 Lexical Access Prior to applying the algorithms, we lemmatize using a weakly-informed tokenizer that knows only that whitespace and punctuation separate words." ></td>
	<td class="line x" title="57:211	Punctuation can either be discarded or treated as words." ></td>
	<td class="line x" title="58:211	Since we are equally interested in finding units like Dr. and U. S., we opt to treat punctuation as words." ></td>
	<td class="line x" title="59:211	Once we tokenize, we use Churchs (1995) suffix array approach to identify word n-grams that occur at least T times (for T=10)." ></td>
	<td class="line oc" title="60:211	We then rank-order the P X|Y MI XY M Z Pr Z|Y MI ZY G092log [P X P Y P X P Y ] f Y [P XY P XY ] f XY [P XY P XY ] f XY M iG13X,X} jG13Y,Y} (f ij G09 ij ) 2 ij f XY G09 XY XY (1G09( XY /N)) f XY G09 XY f XY (1G09(f XY /N)) Table 1: Probabilistic Approaches METHOD FORMULA Frequency (Guiliano, 1964) f XY Pointwise Mutual Information (MI) (Fano, 1961; Church and Hanks, 1990) log (P / PP) 2XY XY Selectional Association (Resnik, 1996) Symmetric Conditional Probability (Ferreira and Pereira, 1999) P / PP XY X Y 2 Dice Formula (Dice, 1945) 2 f / (f +f ) XY X Y Log-likelihood (Dunning, 1993; (Daille, 1996)." ></td>
	<td class="line oc" title="61:211	Since we need knowledge-poor Daille, 1996) induction, we cannot use human-suggested filtering Chi-squared (G24 ) 2 (Church and Gale, 1991) Z-Score (Smadja, 1993; Fontenelle, et al. , 1994) Students t-Score (Church and Hanks, 1990) n-gram list in accordance to each probabilistic algorithm." ></td>
	<td class="line x" title="62:211	This task is non-trivial since most algorithms were originally suited for finding twoword collocations." ></td>
	<td class="line x" title="63:211	We must therefore decide how to expand the algorithms to identify general n-grams (say, C=w ww )." ></td>
	<td class="line x" title="64:211	We can either generalize or 12 n approximate." ></td>
	<td class="line x" title="65:211	Since generalizing requires exponential compute time and memory for several of the algorithms, approximation is an attractive alternative." ></td>
	<td class="line x" title="66:211	One approximation redefines X and Y to be, respectively, the word sequences www and 12 i www where i is chosen to maximize P P . i+1 i+2 n, X Y This has a natural interpretation of being the expected probability of concatenating the two most probable substrings in order to form the larger unit." ></td>
	<td class="line x" title="67:211	Since it can be computed rapidly with low memory costs, we use this approximation." ></td>
	<td class="line x" title="68:211	Two additional issues need addressing before evaluation." ></td>
	<td class="line x" title="69:211	The first regards document sourcing." ></td>
	<td class="line x" title="70:211	If an n-gram appears in multiple sources (eg., Congressional Record versus Associated Press), its likelihood of accuracy should increase." ></td>
	<td class="line x" title="72:211	This is particularly true if we are looking for MWU headwords for a general versus specialized dictionary." ></td>
	<td class="line x" title="73:211	Phrases that appear in one source may in fact be general MWUs, but frequently, they are text-specific units." ></td>
	<td class="line x" title="74:211	Hence, precision gained by excluding single-source n-grams may be worth losses in recall." ></td>
	<td class="line x" title="75:211	We will measure this trade-off." ></td>
	<td class="line x" title="76:211	Second, evaluating with punctuation as words and applying no filtering mechanism may unfairly bias against some algorithms." ></td>
	<td class="line x" title="77:211	Preor post-processing of n-grams with a linguistic filter has shown to improve some induction algorithms performance rules as in Section 2.2." ></td>
	<td class="line x" title="78:211	Yet we can filter by pruning n-grams whose beginning or ending word is among the top N most frequent words." ></td>
	<td class="line x" title="79:211	This unfortunately eliminates acronyms like U. S. and phrasal verbs like throw up. However, discarding some words may be worthwhile if the final list of n-grams is richer in terms of MRD headwords." ></td>
	<td class="line x" title="80:211	We therefore evaluate with such an automatic filter, arbitrarily (and without optimization) choosing N=75." ></td>
	<td class="line x" title="81:211	4 Evaluating Performance A natural scoring standard is to select a language and evaluate against headwords from existing dictionaries in that language." ></td>
	<td class="line x" title="82:211	Others have used similar standards (Daille, 1996), but to our knowledge, none to the extent described here." ></td>
	<td class="line x" title="83:211	We evaluate thousands of hypothesized units from an unconstrained corpus." ></td>
	<td class="line x" title="84:211	Furthermore, we use two separate evaluation gold standards: (1) WordNet (Miller, et al, 1990) and (2) a collection of Internet MRDs." ></td>
	<td class="line x" title="85:211	Using two gold standards helps valid MWUs." ></td>
	<td class="line x" title="86:211	It also provides evaluation using both static and dynamic resources." ></td>
	<td class="line x" title="87:211	We choose to evaluate in English due to the wealth of linguistic resources." ></td>
	<td class="line x" title="88:211	Rank ZScore G242SCPDice Mutual Info." ></td>
	<td class="line x" title="89:211	Select Assoc." ></td>
	<td class="line x" title="90:211	Log Like." ></td>
	<td class="line x" title="91:211	TScore Freq 1 Iwo Jima Buenos Aires Buenos Aires Buenos Aires Iwo Jima United States United States United States United States 2 bona fide Iwo Jima Iwo Jima Iwo Jima bona fide House of Representatives Los Angeles Los Angeles Los Angeles 4 Burkina Faso Suu Kyi Suu Kyi Suu Kyi Wounded Knee Los Angeles New York New York New York 8 Satanic Verses Sault Ste Sault Ste Sault Ste Hubble Space Telescope my colleagues Soviet Union my colleagues my colleagues 16 Ku Klux Ku Klux Ku Klux Ku Klux alma mater H . R Social Security High School High School 32 Pledge of Allegiance Pledge of Allegiance Pledge of Allegiance Pledge of Allegiance Coca Cola War II House of Representatives Wednesday * * * * 64 Telephone & amp ; Telegraph Telephone & amp ; Telegraph Telephone & amp ; Telegraph Internal Revenue Planned Parenthood Prime Minister * * * real estate New Jersey 128 Prime Minister Prime Minister Prime Minister Salman Rushdie Sault Ste . Marie both sides At the same time Wall Street term care 256 Lehman Hutton Lehman Hutton Lehman Hutton tongue in cheek o  clock At the same del Mar all over grand jury 512 La Habra La Habra La Habra compensatory and punitive 20th Century Monday night days later 80 percent Great Northern 1024 telephone interview telephone interview telephone interview Food and Agriculture Sheriff s deputies South Dakota County Jail where you 300 million Table 2: Outputs from each algorithm at different sorted ranks The * * and * * * are actual units." ></td>
	<td class="line x" title="92:211	In particular, we use a randomly-selected corpus the first five columns as information-like. consisting of a 6.7 million word subset of the TREC Similarly, since the last four columns share databases (DARPA, 1993-1997)." ></td>
	<td class="line x" title="93:211	properties of the frequency approach, we will refer Table 2 illustrates a sample of rank-ordered output to them as frequency-like. from each of the different algorithms (following the Ones application may dictate which set of cross-source, filtered paradigm described in section algorithms to use." ></td>
	<td class="line x" title="94:211	Our gold standard selection 3)." ></td>
	<td class="line x" title="95:211	Note that algorithms in the first four columns reflects our interest in general word dictionaries, so produce results that are similar to each other as do results we obtain may differ from results we might those in the last four columns." ></td>
	<td class="line x" title="96:211	Although the mutual have obtained using terminology lexicons." ></td>
	<td class="line x" title="97:211	information results seem to be almost in a class of If our gold standard contains K MWUs with their own, they actually are similar overall to the corpus frequencies satisfying threshold (T=10), our first four sets of results; therefore, we will refer to figure of merit (FOM) is given by 1 K M K iG0A1 P i, little or even negative impact." ></td>
	<td class="line x" title="98:211	On the other hand, where P (precision at i) equals i/H, and H is the i i i number of hypothesized MWUs required to find the i correct MWU." ></td>
	<td class="line x" title="99:211	This FOM corresponds to area th under a precision-recall curve." ></td>
	<td class="line x" title="100:211	4.1 WordNet-based Evaluation WordNet has definite advantages as an evaluation resource." ></td>
	<td class="line x" title="101:211	It has in excess of 50,000 MWUs, is freely accessible, widely used, and is in electronic form." ></td>
	<td class="line x" title="102:211	Yet, it obviously cannot contain every MWU." ></td>
	<td class="line x" title="103:211	For instance, our corpus contains 177,331 n-grams (for 2G06nG0610) satisfying TG0710, but WordNet contains only 2610 of these." ></td>
	<td class="line x" title="104:211	It is unclear, therefore, if algorithms are wrong when they propose MWUs that are not in WordNet." ></td>
	<td class="line x" title="105:211	We will assume they are wrong but with a special caveat for proper nouns." ></td>
	<td class="line x" title="106:211	WordNet includes few proper noun MWUs." ></td>
	<td class="line x" title="107:211	Yet several algorithms produce large numbers of proper nouns." ></td>
	<td class="line x" title="108:211	This biases against them." ></td>
	<td class="line x" title="109:211	One could contend that all proper nouns MWUs are valid, but we disagree." ></td>
	<td class="line x" title="110:211	Although such may be MWUs, they are not necessarily MRD headwords; one would not include every proper noun in a dictionary, but rather, those needing definitions." ></td>
	<td class="line x" title="111:211	To overcome this, we will have two scoring modes." ></td>
	<td class="line x" title="112:211	The first, S mode (standing for some) discards any proposed capitalized n-gram whose uncapitalized version is not in WordNet." ></td>
	<td class="line x" title="113:211	The second mode N (for none) disregards all capitalized n-grams." ></td>
	<td class="line x" title="114:211	Table 3 illustrates algorithmic performance as compared to the 2610 MWUs from WordNet." ></td>
	<td class="line x" title="115:211	The first double column illustrates out-of-the-box performance on all 177,331 possible n-grams." ></td>
	<td class="line x" title="116:211	The second double column shows cross-sourcing: only hypothesizing MWUs that appear in at least two separate datasets (124,952 in all), but being evaluated against all of the 2610 valid units." ></td>
	<td class="line x" title="117:211	Double columns 3 and 4 show effects from high-frequency filtering the n-grams of the first and second columns (reporting only 29,716 and 17,720 n-grams) respectively." ></td>
	<td class="line x" title="118:211	As Table 3 suggests, for every condition, the information-like algorithms seem to perform best at identifying valid, general MWU headwords." ></td>
	<td class="line x" title="119:211	Moreover, they are enhanced when cross-sourcing is considered; but since much of their strength comes from identifying proper nouns, filtering has the frequency-like approaches are independent of data source." ></td>
	<td class="line x" title="120:211	They also improve significantly with filtering." ></td>
	<td class="line x" title="121:211	Overall, though, after the algorithms are judged, even the best score of 0.265 is far short of the maximum possible, namely 1.0." ></td>
	<td class="line x" title="122:211	Table 3: WordNet-based scores Prob (1) (2) (3) (4) algoWordNet WordNet WordNet WordNet rithm cross+Filter crosssource source +Filter S N S N SNSN Zscore .222 .146 .263 .193 .220 .129 .265 .173 SCP .221 .145 .262 .192 .220 .129 .265 .173 Chi-sqr .222 .146 .263 .193 .220 .129 .265 .173 Dice .242 .167 .265 .199 .230 .142 .256 .172 MI .191 .122 .245 .169 .185 .111 .233 .151 SA .057 .051 .058 .053 .182 .125 .202 .143 Loglike .049 .050 .068 .064 .118 .095 .177 .129 T-score .050 .051 .050 .052 .150 .109 .160 .118 Freq .035 .037 .034 .037 .144 .105 .152 .112 4.2 Web-based Evaluation Since WordNet is static and cannot report on all of a corpus n-grams, one may expect different performance by using a more all-encompassing, dynamic resource." ></td>
	<td class="line x" title="123:211	The Internet houses dynamic resources which can judge practically every induced n-gram." ></td>
	<td class="line x" title="124:211	With permission and sufficient time, one can repeatedly query websites that host large collections of MRDs and evaluate each n-gram." ></td>
	<td class="line x" title="125:211	Having approval, we queried: (1) onelook.com, (2) acronymfinder.com, and (3) infoplease.com." ></td>
	<td class="line x" title="126:211	The first website interfaces with over 600 electronic dictionaries." ></td>
	<td class="line x" title="127:211	The second is devoted to identifying proper acronyms." ></td>
	<td class="line x" title="128:211	The third focuses on world facts such as historical figures and organization names." ></td>
	<td class="line x" title="129:211	To minimize disruption to websites by reducing the total number of queries needed for evaluation, we use an evaluation approach from the information retrieval community (Sparck-Jones and van Rijsbergen, 1975)." ></td>
	<td class="line x" title="130:211	Each algorithm reports its top 5000 MWU choices and the union of these choices (45192 possible n-grams) is looked up on the Internet." ></td>
	<td class="line x" title="131:211	Valid MWUs identified at any website are assumed to be the only valid units in the data." ></td>
	<td class="line x" title="132:211	{X i } n iG0A1 {X i G0B } n iG0A1 cos(X,Y) G0A XG23Y ||X|| ||Y|| . Algorithms are then evaluated based on this showed how one could compute latent semantic collection." ></td>
	<td class="line x" title="133:211	Although this strategy for evaluation is vectors for any word in a corpus (Schone and not flawless, it is reasonable and makes dynamic Jurafsky, 2000)." ></td>
	<td class="line x" title="134:211	Using the same approach, we evaluation tractable." ></td>
	<td class="line x" title="135:211	Table 4 shows the algorithms compute semantic vectors for every proposed word performance (including proper nouns)." ></td>
	<td class="line x" title="136:211	n-gram C=X X X Since LSA involves word Though Internet dictionaries and WordNet are counts, we can also compute semantic vectors completely separate gold standards, results are surprisingly consistent." ></td>
	<td class="line x" title="137:211	One can conclude that WordNet may safely be used as a gold standard in future MWU headword evaluations." ></td>
	<td class="line x" title="138:211	Also, Table 4 Performance on Internet data Prob (1) (2) (3) (4) algorithm Internet Internet Internet Internet cross+Filter crosssource source +Filter Z-Score .165 .260 .169 .269 SCP .166 .259 .170 .270 Chi-sqr .166 .260 .170 .270 Dice .183 .258 .187 .267 MI .139 .234 .140 .234 SA .027 .033 .107 .194 Log Like .023 .043 .087 .162 T-score .025 .027 .110 .142 Freq .016 .017 .104 .134 one can see that Z-scores, G24, and 2 SCP have virtually identical results and seem to best identify MWU headwords (particularly if proper nouns are desired)." ></td>
	<td class="line x" title="139:211	Yet there is still significant room for improvement." ></td>
	<td class="line x" title="140:211	5 Improvement strategies Can performance be improved?" ></td>
	<td class="line x" title="141:211	Numerous strategies could be explored." ></td>
	<td class="line x" title="142:211	An idea we discuss here tries using induced semantics to rescore the output of the best algorithm (filtered, cross-sourced Zscore) and eliminate semantically compositional or modifiable MWU hypotheses." ></td>
	<td class="line x" title="143:211	Deerwester, et al (1990) introduced Latent Semantic Analysis (LSA) as a computational technique for inducing semantic relationships between words and documents." ></td>
	<td class="line x" title="144:211	It forms highdimensional vectors using word counts and uses singular value decomposition to project those vectors into an optimal k-dimensional, semantic subspace (see Landauer, et al, 1998)." ></td>
	<td class="line x" title="145:211	Following an approach from Schtze (1993), we 12 n." ></td>
	<td class="line x" title="146:211	(denoted by G0D) for Cs subcomponents." ></td>
	<td class="line x" title="147:211	These can either include ( ) or exclude ( ) Cs counts." ></td>
	<td class="line x" title="148:211	We seek to see if induced semantics can help eliminate incorrectly-chosen MWUs." ></td>
	<td class="line x" title="149:211	As will be shown, the effort using semantics in this nature has a very small payoff for the expended cost." ></td>
	<td class="line x" title="150:211	5.1 Non-compositionality Non-compositionality is a key component of valid MWUs, so we may desire to emphasize n-grams that are semantically non-compositional." ></td>
	<td class="line x" title="151:211	Suppose we wanted to determine if C (defined above) were noncompositional." ></td>
	<td class="line x" title="152:211	Then given some meaning function, G0C, C should satisfy an equation like: g( G0C(C), h( G0C(X ),,G0C(X ) ) )G070, (1) 1n where h combines the semantics of Cs subcomponents and g measures semantic differences." ></td>
	<td class="line x" title="153:211	If C were a bigram, then if g(a,b) is defined to be |a-b|, if h(c,d) is the sum of c and d, and if G0C(e) is set to -log P, then equation (1) would e become the pointwise mutual information of the bigram." ></td>
	<td class="line x" title="154:211	If g(a,b) were defined to be (a-b)/b, and if  h(a,b)=ab/N and G0C(X)=f, we essentially get ZX scores." ></td>
	<td class="line x" title="155:211	These formulations suggest that several of the probabilistic algorithms we have seen include non-compositionality measures already." ></td>
	<td class="line x" title="156:211	However, since the probabilistic algorithms rely only on distributional information obtained by considering juxtaposed words, they tend to incorporate a significant amount of non-semantic information such as syntax." ></td>
	<td class="line x" title="157:211	Can semantic-only rescoring help?" ></td>
	<td class="line x" title="158:211	To find out, we must select g, h, and G0C." ></td>
	<td class="line x" title="159:211	Since we want to eliminate MWUs that are compositional, we want hs output to correlate well with C when there is compositionality and correlate poorly otherwise." ></td>
	<td class="line x" title="160:211	Frequently, LSA vectors are correlated using the cosine between them: A large cosine indicates strong correlation, so large values for g(a,b)=1-|cos(a,b)| should signal weak correlation or non-compositionality." ></td>
	<td class="line x" title="161:211	h could G4D n iG0A1 w i a i cos cos(X i,Y) G0A min kG13X i,Y} cos(G0D Xi,G0D Y )G09 k G31 k . represent a weighted vector sum of the components required for this task." ></td>
	<td class="line x" title="162:211	This seems to be a significant semantic vectors with weights (w ) set to either 1.0 component." ></td>
	<td class="line x" title="163:211	Yet there is still another: maybe i or the reciprocal of the words frequencies." ></td>
	<td class="line x" title="164:211	semantic compositionality is not always bad." ></td>
	<td class="line x" title="165:211	Table 5 indicates several results using these Interestingly, this is often the case." ></td>
	<td class="line x" title="166:211	Consider settings." ></td>
	<td class="line x" title="167:211	As the first four rows indicate and as vice_president, organized crime, and desired, non-compositionality is more apparent for Marine_Corps." ></td>
	<td class="line x" title="168:211	Although these are MWUs, one G0D * (i.e. , the vectors derived from excluding Cs X counts) than for G0D . Yet, performance overall is X horrible, particularly considering we are rescoring Z-score output whose score was 0.269." ></td>
	<td class="line x" title="169:211	Rescoring caused five-fold degradation!" ></td>
	<td class="line x" title="170:211	Table 5: Equation 1 settings g(a,b)h(a) (X) w Score on i Internet 1-|cos(a,b)| G0D X 1 0.0517 1/fi 0.0473 G0D * X 1 0.0598 1/fi* 0.0523 |cos(a,b)| G0D X 1 0.174 1/fi 0.169 G0D * X 1 0.131 1/fi* 0.128 What happens if we instead emphasize compositionality?" ></td>
	<td class="line x" title="171:211	Rows 5-8 illustrate the effect: there is a significant recovery in performance." ></td>
	<td class="line x" title="172:211	The most reasonable explanation for this is that if MWUs and their components are strongly correlated, the components may rarely occur except in context with the MWU." ></td>
	<td class="line x" title="173:211	It takes about 20 hours to compute the G0D * for each possible n-gram X combination." ></td>
	<td class="line x" title="174:211	Since the probabilistic algorithms already identify n-grams that share strong distributional properties with their components, it seems imprudent to exhaust resources on this LSAbased strategy for non-compositionality." ></td>
	<td class="line x" title="175:211	These findings warrant some discussion." ></td>
	<td class="line x" title="176:211	Why did non-compositionality fail?" ></td>
	<td class="line x" title="177:211	Certainly there is the possibility that better choices for g, h, and could yield improvements." ></td>
	<td class="line x" title="178:211	We actually spent months trying to find an optimal combination as well as a strategy for coupling LSA-based scores with the Zscores, but without avail." ></td>
	<td class="line x" title="179:211	Another possibility: although LSA can find semantic relationships, it may not make semantic decisions at the level would still expect that the first is related to president, the second relates to crime, and the last relates to Marine." ></td>
	<td class="line x" title="180:211	Similarly, tokens such as Johns_Hopkins and Elvis are anaphors for Johns_Hopkins_University and Elvis_Presley, so they should have similar meanings." ></td>
	<td class="line x" title="181:211	This begs the question: can induced semantics help at all?" ></td>
	<td class="line x" title="182:211	The answer is yes. The key is using LSA where it does best: finding things that are similar  or substitutable." ></td>
	<td class="line x" title="183:211	5.2 Non-substitutivity For every collocation C=X X X X X X, we 1 2 i-1 i+1 ni attempt to find other similar patterns in the data, X X X YX X . If X and Y are semantically 1 2 i-1 i+1 n i related, chances are that C is substitutable." ></td>
	<td class="line x" title="184:211	Since LSA excels at finding semantic correlations, we can compare G0D and G0D to see if C is Xi Y substitutable." ></td>
	<td class="line x" title="185:211	We use our earlier approach (Schone and Jurafsky, 2000) for performing the comparison; namely, for every word W, we compute cos(G0DG0D) w, R for 200 randomly chosen words, R. This allows for computation of a correlaton mean ( ) and standard W deviation (1 ) between W and other words." ></td>
	<td class="line x" title="186:211	As W before, we then compute a normalized cosine score ( ) between words of interest, defined by With this set-up, we now look for substitutivity." ></td>
	<td class="line x" title="187:211	Note that phrases may be substitutable and still be headword if their substitute phrases are themselves MWUs." ></td>
	<td class="line x" title="188:211	For example, dioxide in carbon_dioxide is semantically similar to monoxide in carbon_monoxide." ></td>
	<td class="line x" title="189:211	Moreover, there are other important instances of valid substitutivity: G26 Abbreviations AlG12Albert G3C Al_GoreG12Albert_Gore G26 Morphological similarities RicoG12Rican G3C Puerto_RicoG12Puerto_Rican G26 Taxonomic relationships bachelorG11masterG3C bachelor__s_degreeG11master__s_degree." ></td>
	<td class="line x" title="190:211	Figure 1: Precision-recall curve for rescoring However, guilty and innocent are semantically related, but pleaded_guilty and pleaded_innocent are not MWUs." ></td>
	<td class="line x" title="191:211	We would like to emphasize only ngrams whose substitutes are valid MWUs." ></td>
	<td class="line x" title="192:211	To show how we do this using LSA, suppose we want to rescore a list L whose entries are potential MWUs." ></td>
	<td class="line x" title="193:211	For every entry X in L, we seek out all other entries whose sorted order is less than some maximum value (such as 5000) that have all but one word in common." ></td>
	<td class="line x" title="194:211	For example, suppose X is bachelor__s_degree. The only other entry that matches in all but one word is master__s_degree. If the semantic vectors for bachelor and master have a normalized cosine score greater than a threshold of 2.0, we then say that the two MWUs are in each others substitution set." ></td>
	<td class="line x" title="195:211	To rescore, we assign a new score to each entry in substitution set." ></td>
	<td class="line x" title="196:211	Each element in the substitution set gets the same score." ></td>
	<td class="line x" title="197:211	The score is derived using a combination of the previous Z-scores for each element in the substitution set." ></td>
	<td class="line x" title="198:211	The combining function may be an averaging, or a computation of the median, the maximum, or something else." ></td>
	<td class="line x" title="199:211	The maximum outperforms the average and the median on our data." ></td>
	<td class="line x" title="200:211	By applying in to our data, we observe a small but visible improvement of 1.3% absolute to .282 (see Fig." ></td>
	<td class="line x" title="201:211	1)." ></td>
	<td class="line x" title="202:211	It is also possible that other improvements could be gained using other combining strategies." ></td>
	<td class="line x" title="203:211	6 Conclusions This paper identifies several new results in the area of MWU-finding." ></td>
	<td class="line x" title="204:211	We saw that MWU headword evaluations using WordNet provide similar results to those obtained from far more extensive webbased resources." ></td>
	<td class="line x" title="205:211	Thus, one could safely use WordNet as a gold standard for future evaluations." ></td>
	<td class="line x" title="206:211	We also noted that information-like algorithms, particularly Z-scores, SCP, and G242, seem to perform best at finding MRD headwords regardless of filtering mechanism, but that improvements are still needed." ></td>
	<td class="line x" title="207:211	We proposed two new LSA-based approaches which attempted to address issues of non-compositionality and non-substitutivity." ></td>
	<td class="line x" title="208:211	Apparently, either current algorithms already capture much non-compositionality or LSA-based models of non-compositionality are of little help." ></td>
	<td class="line x" title="209:211	LSA does help somewhat as a model of substitutivity." ></td>
	<td class="line x" title="210:211	However, LSA-based gains are small compared to the effort required to obtain them." ></td>
	<td class="line x" title="211:211	Acknowledgments The authors would like to thank the anonymous reviewers for their comments and insights." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C02-1033
Using Collocations For Topic Segmentation And Link Detection
Ferret, Olivier;"></td>
	<td class="line x" title="1:159	Using collocations for topic segmentation and link detection Olivier FERRET CEA  LIST/LIC2M Route du Panorama  BP 6 92265 Fontenay-aux-Roses Cedex olivier.ferret@cea.fr Abstract We present in this paper a method for achieving in an integrated way two tasks of topic analysis: segmentation and link detection." ></td>
	<td class="line x" title="2:159	This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches." ></td>
	<td class="line x" title="3:159	We report an evaluation of our method for segmentation on two corpora, one in French and one in English, and we propose an evaluation measure that specifically suits that kind of systems." ></td>
	<td class="line x" title="4:159	1 Introduction Topic analysis, which aims at identifying the topics of a text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest." ></td>
	<td class="line x" title="5:159	The largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the TDT (Topic Detection and Tracking) initiative (Fiscus et al. , 1999), which addresses all the tasks we have mentioned but from a domain-dependent viewpoint and not necessarily in an integrated way." ></td>
	<td class="line x" title="6:159	Systems that implement this work can be categorized according to what kind of knowledge they use." ></td>
	<td class="line x" title="7:159	Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997)." ></td>
	<td class="line x" title="8:159	They can be applied without restriction about domains but have low results when a text doesnt characterize its topical structure by surface clues." ></td>
	<td class="line x" title="9:159	Some systems exploit domain-independent knowledge about lexical cohesion: a network of words built from a dictionary in (Kozima, 1993); a large set of collocations collected from a corpus in (Ferret, 1998), (Kaufmann, 1999) and (Choi, 2001)." ></td>
	<td class="line x" title="10:159	To some extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains." ></td>
	<td class="line x" title="11:159	The last main type of systems relies on knowledge about the topics they may encounter in the texts they process." ></td>
	<td class="line x" title="12:159	This is typically the kind of approach developed in TDT where this knowledge is automatically built from a set of reference texts." ></td>
	<td class="line x" title="13:159	The work of Bigi (Bigi et al. , 1998) stands in the same perspective but focuses on much larger topics than TDT." ></td>
	<td class="line x" title="14:159	These systems have a limited scope due to their topic representations but they are also more precise for the same reason." ></td>
	<td class="line x" title="15:159	Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al. , 1999) relied on both collocations and linguistic cues." ></td>
	<td class="line x" title="16:159	The topic analysis we propose implements such a hybrid approach: it relies on a general language resource, a collocation network, but exploits it together with word recurrence in texts." ></td>
	<td class="line x" title="17:159	Moreover, it simultaneously achieves topic segmentation and link detection, i.e. determining whether two segments discuss the same topic." ></td>
	<td class="line x" title="18:159	We detail in this paper the implementation of this analysis by the TOPICOLL system, we report evaluations of its capabilities concerning segmentation for two languages, French and English, and finally, we propose an evaluation measure that integrates both segmentation and link detection." ></td>
	<td class="line x" title="19:159	2 Overview of TOPICOLL In accordance with much work about discourse analysis, TOPICOLL processes texts linearly: it detects topic shifts and finds links between segments without delaying its decision, i.e., by only taking into account the part of text that has been already analyzed." ></td>
	<td class="line x" title="20:159	A window that delimits the current focus of the analysis is moved over each text to be processed." ></td>
	<td class="line x" title="21:159	This window contains the lemmatized content words of the text, resulting from its pre-processing." ></td>
	<td class="line x" title="22:159	A topic context is associated to this focus window." ></td>
	<td class="line x" title="23:159	It is made up of both the words of the window and the words that are selected from a collocation network1 as strongly linked to the words of the window." ></td>
	<td class="line x" title="24:159	The current segment is also given a topic context." ></td>
	<td class="line x" title="25:159	This context results from the fusion of the contexts associated to the focus window when this window was in the segment space." ></td>
	<td class="line x" title="26:159	A topic shift is then detected when the context of the focus window and the context of the current segment are not similar any more for several successive positions of the focus window." ></td>
	<td class="line x" title="27:159	This process also performs link detection by comparing the topic context of each new segment to the context of the already delimited segments." ></td>
	<td class="line x" title="28:159	The use of a collocation network permits TOPICOLL to find relations beyond word recurrence and to associate a richer topical representation to segments, which facilitates tasks such as link detection or topic identification." ></td>
	<td class="line x" title="29:159	But work such as (Kozima, 1993), (Ferret, 1998) or (Kaufmann, 1999) showed that using a domainindependent source of knowledge for text segmentation doesnt necessarily lead to get better results than work that is only based on word distribution in texts." ></td>
	<td class="line x" title="30:159	One of the reasons of this fact is that these methods dont precisely control the relations they select or dont take into account the sparseness of their knowledge." ></td>
	<td class="line x" title="31:159	Hence, while they discard some incorrect topic shifts found by methods based on word recurrence, they also find incorrect shifts when the relevant relations are not present in their knowledge or dont find some correct shifts because of the selection of non relevant relations from a topical viewpoint." ></td>
	<td class="line x" title="32:159	By combining word recurrence and relations selected from a collocation network, TOPICOLL aims at exploiting a domain-independent source of knowledge for text segmentation in a more accurate way." ></td>
	<td class="line x" title="33:159	3 Collocation networks TOPICOLL depends on a resource, a collocation network, that is language-dependent." ></td>
	<td class="line x" title="34:159	Two collocation networks were built for it: one for French, 1 A collocation network is a set of collocations between words." ></td>
	<td class="line x" title="35:159	This set can be viewed as a network whose nodes are words and edges are collocations." ></td>
	<td class="line x" title="36:159	from the Le Monde newspaper (24 months between 1990 and 1994), and one for English, from the L.A. Times newspaper (2 years, part of the TREC corpus)." ></td>
	<td class="line x" title="37:159	The size of each corpus was around 40 million words." ></td>
	<td class="line x" title="38:159	The building process was the same for the two networks." ></td>
	<td class="line x" title="39:159	First, the initial corpus was preprocessed in order to characterize texts by their topically significant words." ></td>
	<td class="line x" title="40:159	Thus, we retained only the lemmatized form of plain words, that is, nouns, verbs and adjectives." ></td>
	<td class="line oc" title="41:159	Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts." ></td>
	<td class="line x" title="42:159	Parameters were chosen in order to catch topical relations: the window was rather large, 20-word wide, and took into account the boundaries of texts; moreover, collocations were indifferent to word order." ></td>
	<td class="line x" title="43:159	We also adopted an evaluation of mutual information as a cohesion measure of each collocation." ></td>
	<td class="line x" title="44:159	This measure was normalized according to the maximal mutual information relative to the considered corpus." ></td>
	<td class="line x" title="45:159	After filtering the less significant collocations (collocations with less than 10 occurrences and cohesion lower than 0.1), we got a network with approximately 23,000 words and 5.2 million collocations for French, 30,000 words and 4.8 million collocations for English." ></td>
	<td class="line x" title="46:159	4 Description of TOPICOLL TOPICOLL is based on the creation, the update and the use of a topical representation of both the segments it delimits and the content of its focus window at each position of a text." ></td>
	<td class="line x" title="47:159	These topical representations are called topic contexts." ></td>
	<td class="line x" title="48:159	Topic shifts are found by detecting that the topic context of the focus window is not similar anymore to the topic context of the current segment." ></td>
	<td class="line x" title="49:159	Link detection is performed by comparing the context of a new segment to the context of the previous segments." ></td>
	<td class="line x" title="50:159	4.1 Topic contexts A topic context characterizes the topical dimension of the entity it is associated to by two vectors of weighted words." ></td>
	<td class="line x" title="51:159	One of these vectors, called text vector, is made up of words coming from the text that is analyzed." ></td>
	<td class="line x" title="52:159	The other one, called collocation vector, contains words selected from a collocation network and strongly linked to the words of the processed text." ></td>
	<td class="line x" title="53:159	For both vectors, the weight of a word expresses its importance with regard to the other words of the vector." ></td>
	<td class="line x" title="54:159	4.1.1 Topic context of the focus window The text vector of the context associated to the focus window is made up of the content words of the window." ></td>
	<td class="line x" title="55:159	Their weight is given by: )()()( wsignifwoccNbwwghttxt = (1) where occNb(w) is the number of occurrences of the word w in the window and signif(w) is the significance of w. The weight given by (1) combines the importance of w in the part of text delimited by the window and its general significance." ></td>
	<td class="line x" title="56:159	This significance is defined as in (Kozima, 1993) as its normalized information in a reference corpus2: )1(log )(log)( 2 2 c cw S Sfwsignif  = (2) where fw is the number of occurrences of the word w in the corpus and Sc, the size of the corpus." ></td>
	<td class="line x" title="57:159	0.14 0.21 0.10 0.18 0.13 0.17 w5w4w3w2w1 0.48 = pw3x0.18+pw4x0.13 +pw5x0.17 selected word from the collocation network (with its weight) 1.0 word from text (with p its weight in the window, equal to 0.21 link in the collocation network (with its cohesion value) 1.0 1.0 1.0 1.0 1.0 wi, n1 n2 1.0 for all words of the window in this example) 0.48 Figure 1  Selection and weighting of words from the collocation network The building of the collocation vector for the windows context comes from the procedure presented in (Ferret, 1998) for evaluating the lexical cohesion of a text." ></td>
	<td class="line x" title="58:159	It consists in selecting words of the collocation network that are topically close to those in the window." ></td>
	<td class="line x" title="59:159	We assume that this closeness is related to the number of links that exist between a word of the network and the words of the window." ></td>
	<td class="line x" title="60:159	Thus, a word of the network is se2 In our case, this is the corpus used for building the collocation network." ></td>
	<td class="line x" title="61:159	lected if it is linked to at least wst (3 in our experiments) words of the window." ></td>
	<td class="line x" title="62:159	A collocation vector may also contain some words of the window as they are generally part of the collocation network and may be selected as its other words." ></td>
	<td class="line x" title="63:159	Each selected word from the network is then assigned a weight." ></td>
	<td class="line x" title="64:159	This weight is equal to the sum of the contributions of the window words to which it is linked to." ></td>
	<td class="line x" title="65:159	The contribution of a word of the window to the weight of a selected word is equal to its weight in the window, given by (1), modulated by the cohesion measure between these two words in the network (see Figure 1)." ></td>
	<td class="line x" title="66:159	More precisely, the combination of these two factors is achieved by a geometric mean:  = i iitxtcoll wwcohwwgthwwght ),()()( (3) where coh(w,wi) is the measure of the cohesion between w and wi in the collocation network." ></td>
	<td class="line x" title="67:159	4.1.2 Topic context of a segment The topic context of a segment results from the fusion of the contexts associated to the focus window when it was inside the segment." ></td>
	<td class="line x" title="68:159	The fusion is achieved as the segment is extended: the context associated to each new position of the segment is combined with the current context of the segment." ></td>
	<td class="line x" title="69:159	This combination, which is done separately for text vectors and collocation vectors, consists in merging two lists of weighted words." ></td>
	<td class="line x" title="70:159	First, the words of the window context that are not in the segment context are added to it." ></td>
	<td class="line x" title="71:159	Then, the weight of each word of the resulting list is computed according to its weight in the window context and its previous weight in the segment context: )),,()(( )1,,(),,( tCwwwghtwsignif tCswwghttCswwght x xx  += (4) with Cw, the context of the window, Cs, the context of the segment and wghtx(w,Cs,w},t), the weight of the word w in the vector x (txt or coll) of the context Cs,w} for the position t. For the words from the window context that are not part of the segment context, wghtx(w,Cs,t-1) is equal to 0." ></td>
	<td class="line x" title="72:159	The revaluation of the weight of a word in a segment context given by (4) is a solution halfway between a fast and a slow evolution of the content of segment contexts." ></td>
	<td class="line x" title="73:159	The context of a segment has to be stable because if it follows too narrowly the topical evolution of the window context, topic shifts could not be detected." ></td>
	<td class="line x" title="74:159	However, it must also adapt itself to small variations in the way a topic is expressed when progressing in the text in order not to detect false topic shifts." ></td>
	<td class="line x" title="75:159	4.1.3 Similarity between contexts In order to determine if the content of the focus window is topically coherent or not with the current segment, the topic context of the window is compared to the topic context of the segment." ></td>
	<td class="line x" title="76:159	This comparison is performed in two stages: first, a similarity measure is computed between the vectors of the two contexts; then, the resulting values are exploited by a decision procedure that states if the two contexts are similar." ></td>
	<td class="line x" title="77:159	As (Choi, 2000) or (Kaufmann, 1999), we use the cosine measure for evaluating the similarity between a vector of the context window (Vw) and the equivalent vector in the segment context (Vs):     = i ix i ix ix i ix xx CwwwgCswwg CwwgwCswwg VwVssim 22 ),(),( ),(),( ),( (5) where wgx(wi,Cs,w}) is the weight of the word wi in the vector x (txt or coll) of the context Cs,w}." ></td>
	<td class="line x" title="78:159	As we assume that the most significant words of a segment context are the most recurrent ones, the similarity measure takes into account only the words of a segment context whose the recurrence3 is above a fixed threshold." ></td>
	<td class="line x" title="79:159	This one is higher for text vectors than for collocation vectors." ></td>
	<td class="line x" title="80:159	This filtering is applied only when the context of a segment is considered as stable (see 4.2)." ></td>
	<td class="line x" title="81:159	The decision stage takes root in work about combining results of several systems that achieve the same task." ></td>
	<td class="line x" title="82:159	In our case, the evaluation of the similarity between Cs and Cw at each position is based on a vote that synthesizes the viewpoint of the text vector and the viewpoint of the collocation vector." ></td>
	<td class="line x" title="83:159	First, the value of the similarity measure for each vector is compared to a fixed threshold and a posi3 The recurrence of a word in a segment context is given by the ratio between the number of window contexts in which the word was present and the number of window contexts gathered by the segment context." ></td>
	<td class="line x" title="84:159	tive vote in favor of the similarity of the two contexts is decided if the value exceeds this threshold." ></td>
	<td class="line x" title="85:159	Then, the global similarity of the two contexts is rejected only if the votes for the two vectors are negative." ></td>
	<td class="line x" title="86:159	4.2 Topic segmentation The algorithm for detecting topic shifts is taken from (Ferret and Grau, 2000) and basically relies on the following principle: at each text position, if the similarity between the topic context of the focus window and the topic context of the current segment is rejected (see 4.1.3), a topic shift is assumed and a new segment is opened." ></td>
	<td class="line x" title="87:159	Otherwise, the active segment is extended up to the current position." ></td>
	<td class="line x" title="88:159	This algorithm assumes that the transition between two segments is punctual." ></td>
	<td class="line x" title="89:159	As TOPICOLL only operates at word level, its precision is limited." ></td>
	<td class="line x" title="90:159	This imprecision makes necessary to set a short delay before deciding that the active segment really ends and similarly, before deciding that a new segment with a stable topic begins." ></td>
	<td class="line x" title="91:159	Hence, the algorithm for detecting topic shifts distinguishes four states:  the NewTopicDetection state takes place when a new segment is going to be opened." ></td>
	<td class="line x" title="92:159	This opening is then confirmed provided that the content of the focus window context doesnt change for several positions." ></td>
	<td class="line x" title="93:159	Moreover, the core of the segment context is defined when TOPICOLL is in this state;  the InTopic state is active when the focus window is inside a segment with a stable topic;  the EndTopicDetection state occurs when the focus window is inside a segment but a difference between the context of the window and the context of the current segment suggests that this segment could end soon." ></td>
	<td class="line x" title="94:159	As for the NewTopicDetection state, this difference has to be confirmed for several positions before a change of state is decided;  the OutOfTopic state is active between two segments." ></td>
	<td class="line x" title="95:159	Generally, TOPICOLL stays in this state no longer than 1 or 2 positions but when neither the words from text nor the words selected from the collocation network are recurrent, i.e. no stable topic can be detected according to these features, this number of positions may be equal to the size of a segment." ></td>
	<td class="line x" title="96:159	The transition from one state to another follows the automaton of Figure 2 according to three parameters:  its current state;  the similarity between the context of the focus window and the context of the current segment: Sim or no Sim;  the number of successive positions of the focus window for which the current state doesnt change: confirmNb." ></td>
	<td class="line x" title="97:159	It must exceed the Tconfirm threshold (equal to 3 in our experiments) for leaving the NewTopicDetection or the EndTopicDetection state." ></td>
	<td class="line x" title="98:159	NewTopic Detection EndTopic Detection OutOfTopic InTopic Sim Sim Sim no Sim no Sim no Sim no Sim & confirmNb = Tconfirm no Sim & confirmNb < Tconfirm Sim & confirmNb = Tconfirm Sim & confirmNb < Tconfirm Figure 2  Automaton for topic shift detection The processing of a segment starts with the OutOfTopic state, after the end of the previous segment or at the beginning of the text." ></td>
	<td class="line x" title="99:159	As soon as the context of the focus window is stable enough between two successive positions, TOPICOLL enters into the NewTopicDetection state." ></td>
	<td class="line x" title="100:159	The InTopic state can then be reached only if the window context is found stable for the next confirmNb-1 positions." ></td>
	<td class="line x" title="101:159	Otherwise, TOPICOLL assumes that it is a false alarm and returns to the OutOfTopic state." ></td>
	<td class="line x" title="102:159	The detection of the end of a segment is symmetrical to the detection of its beginning." ></td>
	<td class="line x" title="103:159	TOPICOLL goes into the EndTopicDetection state as soon as the content of the window context begins to change significantly between two successive positions but the transition towards the OutOfTopic state is done only if this change is confirmed for the next confirmNb-1 next positions." ></td>
	<td class="line x" title="104:159	This algorithm is completed by a specific mechanism related to the OutOfTopic state." ></td>
	<td class="line x" title="105:159	When TOPICOLL stays in this state for a too long time (this time is defined as 10 positions of the focus window in our experiments), it assumes that the topic of the current part of text is difficult to characterize by using word recurrence or selection from a collocation network and it creates a new segment that covers all the concerned positions." ></td>
	<td class="line x" title="106:159	4.3 Link detection The algorithm of TOPICOLL for detecting identity links between segments is closely associated to its algorithm for delimiting segments." ></td>
	<td class="line x" title="107:159	When TOPICOLL goes from the NewTopicDetection state to the InTopic state, it first checks whether the current context of the new segment is similar to one of the contexts of the previous segments." ></td>
	<td class="line x" title="108:159	In this case, the similarity between contexts only relies on the similarity measure (see (5)) between their collocation vectors." ></td>
	<td class="line x" title="109:159	A specific threshold is used for the decision." ></td>
	<td class="line x" title="110:159	If the similarity value exceeds this threshold, the new segment is linked to the corresponding segment and takes the context of this one as its own context." ></td>
	<td class="line x" title="111:159	In this way, TOPICOLL assumes that the new segment continues to develop a previous topic." ></td>
	<td class="line x" title="112:159	When several segments fulfills the condition for link detection, TOPICOLL selects the one with the highest similarity value." ></td>
	<td class="line x" title="113:159	5 Experiments 5.1 Topic segmentation For evaluating TOPICOLL about segmentation, we applied it to the classical task of discovering boundaries between concatenated texts." ></td>
	<td class="line x" title="114:159	TOPICOLL was adapted for aligning boundaries with ends of sentences." ></td>
	<td class="line x" title="115:159	We used the probabilistic error metric Pk proposed in (Beeferman et al. , 1999) for measuring segmentation accuracy4." ></td>
	<td class="line x" title="116:159	Recall and precision was computed for the Le Monde corpus to compare TOPICOLL with older systems5." ></td>
	<td class="line x" title="117:159	In this case, the match between a boundary from TOPICOLL and a document break was accepted if the boundary was not farther than 9 plain words." ></td>
	<td class="line x" title="118:159	5.1.1 Le Monde corpus The evaluation corpus for French was made up of 49 texts, 133 words long on average, from the Le 4 Pk evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL while they are actually in different ones (miss of a document break) or they are found in different segments by TOPICOLL while they are actually in the same one (false alarm)." ></td>
	<td class="line x" title="119:159	5 Precision is given by Nt / Nb and recall by Nt / D, with D the number of document breaks, Nb the number of boundaries found by TOPICOLL and Nt the number of boundaries that are document breaks." ></td>
	<td class="line x" title="120:159	Monde newspaper." ></td>
	<td class="line x" title="121:159	Results in Tables 1 and 2 are average values computed from 10 different sequences of them." ></td>
	<td class="line x" title="122:159	The baseline procedure consisted in randomly choosing a fixed number of sentence ends as boundaries." ></td>
	<td class="line x" title="123:159	Its results in Tables 1 and 2 are average values from 1,000 draws." ></td>
	<td class="line x" title="124:159	Systems Recall Precision F1-measure baseline 0.51 0.28 0.36 SEGCOHLEX 0.68 0.37 0.48 SEGAPSITH 0.92 0.52 0.67 TextTiling 0.72 0.81 0.76 TOPICOLL1 0.86 0.74 0.80 TOPICOLL2 0.86 0.78 0.81 TOPICOLL3 0.66 0.60 0.63 Table 1  Precision/recall for Le Monde corpus TOPICOLL1 is the system described in section 4." ></td>
	<td class="line x" title="125:159	TOPICOLL2 is the same system but without its link detection part." ></td>
	<td class="line x" title="126:159	The results of these two variants show that the search for links between segments doesnt significantly debase TOPICOLLs capabilities for segmentation." ></td>
	<td class="line x" title="127:159	TOPICOLL3 is a version of TOPICOLL that only relies on word recurrence." ></td>
	<td class="line x" title="128:159	SEGCOHLEX and SEGAPSITH are the systems described in (Ferret, 1998) and (Ferret and Grau, 2000)." ></td>
	<td class="line x" title="129:159	TextTiling is our implementation of Hearsts algorithm with its standard parameters." ></td>
	<td class="line x" title="130:159	Systems Miss False alarm Error baseline 0.46 0.55 0.50 TOPICOLL1 0.17 0.24 0.21 TOPICOLL2 0.17 0.22 0.20 Table 2  Pk for Le Monde corpus First, Table 1 shows that TOPICOLL is more accurate when its uses both word recurrence and collocations." ></td>
	<td class="line x" title="131:159	Furthermore, it shows that TOPICOLL gets better results than a system that only relies on a collocation network such as SEGCOHLEX." ></td>
	<td class="line x" title="132:159	It also gets better results than a system such as TextTiling that is based on word recurrence and as TOPICOLL, works with a local context." ></td>
	<td class="line x" title="133:159	Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation." ></td>
	<td class="line x" title="134:159	Moreover, TOPICOLL is more accurate than a system such as SEGAPSITH that depends on topic representations." ></td>
	<td class="line x" title="135:159	Its accuracy is also slightly higher than the one reported in (Bigi et al. , 1998) for a system that uses topic representations in a probabilistic way: 0.75 as precision, 0.80 as recall and 0.77 as f1-measure got on a corpus made of Le Mondes articles too." ></td>
	<td class="line x" title="136:159	5.1.2 C99 corpus For English, we used the artificial corpus built by Choi (Choi, 2000) for comparing several segmentation systems." ></td>
	<td class="line x" title="137:159	This corpus is made up of 700 samples defined as follows: A sample is a concatenation of ten text segments." ></td>
	<td class="line x" title="138:159	A segment is the first n sentences of a randomly selected document for the Brown corpus." ></td>
	<td class="line x" title="139:159	Each column of Table 3 states for an interval of values for n. Systems 3-11 3-5 6-8 9-11 baseline 0.45 0.38 0.39 0.36 CWM 0.09 0.10 0.07 0.05 U00 0.10 0.09 0.07 0.05 C99 0.12 0.11 0.09 0.09 DotPlot 0.18 0.20 0.15 0.12 Segmenter 0.36 0.23 0.33 0.43 TextTiling 0.46 0.44 0.43 0.48 TOPICOLL1 0.30 0.28 0.27 0.34 TOPICOLL2 0.31 0.28 0.28 0.34 Table 3  Pk for C99 corpus The first seven lines of Table 3 results from Chois experiments (Choi, 2001)." ></td>
	<td class="line x" title="140:159	The baseline is a procedure that partitions a document into 10 segments of equal length." ></td>
	<td class="line x" title="141:159	CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al. , 1998)." ></td>
	<td class="line x" title="142:159	Table 3 confirms first that the link detection part of TOPICOLL doesnt debase its segmentation capabilities." ></td>
	<td class="line x" title="143:159	It also shows that TOPICOLLs results on this corpus are significantly lower than its results on the Le Monde corpus." ></td>
	<td class="line x" title="144:159	This is partially due to our collocation network for English: its density, i.e. the ratio between the size of its vocabulary and its number of collocations, is 30% lower than the density of the network for French, which has certainly a significant effect." ></td>
	<td class="line x" title="145:159	Table 3 also shows that TOPICOLL has worse results than systems such as CWM, U00, C99 or DotPlot." ></td>
	<td class="line x" title="146:159	This can be explained by the fact that TOPICOLL only works with a local context whereas these systems rely on the whole text they process." ></td>
	<td class="line x" title="147:159	As a consequence, they have a global view on texts but are more costly than TOPICOLL from an algorithmic viewpoint." ></td>
	<td class="line x" title="148:159	Moreover, link detection makes TOPICOLL functionally richer than they are." ></td>
	<td class="line x" title="149:159	5.2 Global evaluation The global evaluation of a system such as TOPICOLL faces a problem: a reference for link detection is relative to a reference for segmentation." ></td>
	<td class="line x" title="150:159	Hence, mapping it onto the segments delimited by a system to evaluate is not straightforward." ></td>
	<td class="line x" title="151:159	To bypass this problem, we chose an approach close the one adopted in TDT for the link detection task: we evaluated the probability of an error in classifying each couple of positions in a text as being part of the same topic (Cpsame) or belonging to different topics (Cpdiff)." ></td>
	<td class="line x" title="152:159	A miss is detected if a couple is found about different topics while they are about the same topic and a false alarm corresponds to the complementary case." ></td>
	<td class="line x" title="153:159	Systems Miss False alarm Error baseline 0.85 0.06 0.45 TOPICOLL 0.73 0.01 0.37 Table 4  Error rates for Le Monde corpus As the number of Cpdiff couples is generally much larger than the number of Cpsame couples, we randomly selected a number of Cpdiff couples equal to the number of Cpsame couples in order to have a large range of possible values." ></td>
	<td class="line x" title="154:159	Table 4 shows the results of TOPICOLL for the considered measure and compares them to a baseline procedure that randomly set a fixed number of boundaries and a fixed number of links between the delimited segments." ></td>
	<td class="line x" title="155:159	This measure is a first proposition that should certainly be improved, especially for balancing more soundly misses and false alarms." ></td>
	<td class="line x" title="156:159	6 Conclusion We have proposed a method for achieving both topic segmentation and link detection by using collocations together with word recurrence in texts." ></td>
	<td class="line x" title="157:159	Its evaluation showed the soundness of this approach for working with a local context." ></td>
	<td class="line x" title="158:159	We plan to extend it to methods that rely on the whole text they process." ></td>
	<td class="line x" title="159:159	We also aim at extending the evaluation part of this work by improving the global measure we have proposed and by comparing our results to human judgments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C02-1086
Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To-English Cross-Language Information Retrieval
Lee, Kyung-Soon;Kageura, Kyo;Choi, Key-Sun;"></td>
	<td class="line x" title="1:149	Implicit Ambiguity Resolution Using Incremental Clustering in Korean-to-English Cross-Language Information Retrieval Kyung-Soon Lee 1, Kyo Kageura 1, Key-Sun Choi 2 1 NII (National Institute of Informatics) 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430, Japan {kslee, kyo}@nii.ac.jp 2 Division of Computer Science, KAIST 373-1 Kusung Yusong Daejeon, 305-701, Korea kschoi@cs.kaist.ac.kr Abstract This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval." ></td>
	<td class="line x" title="2:149	In the framework we propose, a query in Korean is first translated into English by looking up Korean-English dictionary, then documents are retrieved based on the vector space retrieval for the translated query terms." ></td>
	<td class="line x" title="3:149	For the top-ranked retrieved documents, query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters." ></td>
	<td class="line x" title="4:149	In experiment on TREC-6 CLIR test collection, our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries." ></td>
	<td class="line x" title="5:149	This corresponds to 97.27% of the monolingual performance for original queries." ></td>
	<td class="line x" title="6:149	When we combine our method with query ambiguity resolution, our method even outperforms the monolingual retrieval." ></td>
	<td class="line x" title="7:149	1 Introduction This paper describes a method of applying dynamic incremental clustering to the implicit resolution of query ambiguities in Korean-to-English cross-language information retrieval." ></td>
	<td class="line x" title="8:149	The method uses the clusters of retrieved documents as a context for re-weighting each retrieved document and for re-ranking the retrieved documents." ></td>
	<td class="line x" title="9:149	Cross-language information retrieval (CLIR) enables users to retrieve documents written in a language different from a query language." ></td>
	<td class="line x" title="10:149	The methods used in CLIR fall into two categories: statistical approaches and translation approaches." ></td>
	<td class="line x" title="11:149	Statistical methods establish cross-lingual associations without language translation (Dumais et al, 1997; Rehder et al, 1997; Yang et al, 1998)." ></td>
	<td class="line x" title="12:149	They require large-scale bilingual corpora." ></td>
	<td class="line x" title="13:149	In translation approach, either queries or documents are translated." ></td>
	<td class="line x" title="14:149	Though document translation is possible when high quality machine translation systems are available (Kwon et al, 1997; Oard and Hackett, 1997), it is not very practical." ></td>
	<td class="line x" title="15:149	Query translation methods (Hull and Grefenstette, 1996; Davis, 1996; Eichmann et al, 1998; Yang et al, 1998; Jang et al, 1999; Chun, 2000) based on bilingual dictionaries, multilingual ontology or thesaurus are much more practical." ></td>
	<td class="line x" title="16:149	Many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries." ></td>
	<td class="line x" title="17:149	In order to achieve a high performance CLIR using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms." ></td>
	<td class="line oc" title="18:149	One way of resolving query ambiguities is to use the statistics, such as mutual information (Church and Hanks, 1990), to measure associations of query terms, on the basis of existing corpora (Jang et al, 1999)." ></td>
	<td class="line x" title="19:149	Document clusters, widely adopted in various applications such as browsing and viewing of document results (Hearst and Pedersen, 1996) or topic detection (Allan et al, 1998), also reflect the association of terms and documents." ></td>
	<td class="line x" title="20:149	Lee et al (2001) showed that incorporating a document re-ranking method based on document clusters into the vector space retrieval achieved the significant improvement in monolingual IR, as it contributed to resolving ambiguities caused by polysemous query terms." ></td>
	<td class="line x" title="21:149	The noise or ambiguity produced by dictionary-based query translation in CLIR is much larger than the polysemous ambiguities in monolingual IR." ></td>
	<td class="line x" title="22:149	For example, a Korean term [eun-haeng] is a polysemous term with two meanings: bank and ginkgo." ></td>
	<td class="line x" title="23:149	The English term bank itself is polysemous, so the translated query ends up having magnified ambiguities." ></td>
	<td class="line x" title="24:149	We will show that the method we propose, i.e. implicit ambiguity resolution using incremental clustering, is highly effective in dealing with the increased query ambiguities in CLIR." ></td>
	<td class="line x" title="25:149	2 Implicit ambiguity resolution using incremental clustering Figure 1 shows the overall architecture of our system which incorporates implicit ambiguity resolution method based on query-oriented document clusters." ></td>
	<td class="line x" title="26:149	In the system, a query in Korean is first translated into English by looking up dictionaries, and documents are retrieved based on the vector space retrieval for the translated query." ></td>
	<td class="line x" title="27:149	For the top-ranked retrieved documents, document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters with preference." ></td>
	<td class="line x" title="28:149	This phase is the core of our implicit ambiguity resolution method." ></td>
	<td class="line x" title="29:149	Below, we will describe each module in the system." ></td>
	<td class="line x" title="30:149	2.1 Dictionary-based query translation and ambiguities Queries are written in natural language in Korean." ></td>
	<td class="line x" title="31:149	We first apply morphological analysis and part-of-speech (POS) tagging to a query, and select keywords based on the POS information." ></td>
	<td class="line x" title="32:149	For each keyword, we look up Korean-English dictionaries, and all the English translations in the dictionaries are chosen as query terms." ></td>
	<td class="line x" title="33:149	We used a general-purpose bilingual dictionary and technical bilingual dictionaries (Chun, 2000)." ></td>
	<td class="line x" title="34:149	All in all, they have 282,511 Korean entries and 505,003 English translations." ></td>
	<td class="line x" title="35:149	Since a term can have multiple translations, the list of translated query terms can contain terms of different meanings as well as synonyms." ></td>
	<td class="line x" title="36:149	While synonyms can improve retrieval effectiveness, terms with different meanings produced from the same original term can degrade retrieval performance tremendously." ></td>
	<td class="line o" title="37:149	At this stage, we can apply statistical ambiguity resolution method based on mutual information." ></td>
	<td class="line x" title="38:149	In the experiment below, we will examine two cases, i.e. with and without ambiguity resolution at this stage." ></td>
	<td class="line x" title="39:149	2.2 Document retrieval based on vector space retrieval model For the query, documents are retrieved based on the vector space retrieval method." ></td>
	<td class="line x" title="40:149	This method simply checks the existence of query terms, and calculates similarities between the query and documents." ></td>
	<td class="line x" title="41:149	The query-document similarity of each document is calculated by vector inner product of the query and document vectors: di t i qi wwdqsimD =  =1 ),( (1) where query and document weight, qi w and di w, are calculated by ntc-ltn weighting scheme which yields the best retrieval result in Lee et al (2001) among several weighting schemes used in SMART system (Salton, 1989)." ></td>
	<td class="line x" title="42:149	As the translated query can contain noises, non-relevant documents may have higher ranks than relevant documents." ></td>
	<td class="line x" title="43:149	Figure 1." ></td>
	<td class="line x" title="44:149	System architecture of implicit ambiguity resolution by incremental clustering." ></td>
	<td class="line x" title="45:149	English Query with ambiguities TREC AP-news collection Korean-English Dictionaries Korean Query retrieved top N docs Dictionary-based Query Translation Vector Space Retrieval Each document view re-ranked results  Incremental Clusters Document context view Reflecting context to each document 2.3 Query-oriented incremental clustering for implicit ambiguity resolution In order to exclude non-relevant documents from higher ranks, we take top N documents to create clusters incrementally and dynamically, and use similarities between the clusters and the query to re-rank the documents." ></td>
	<td class="line x" title="46:149	Basic idea is: Each cluster created by clustering of retrieved documents can be seen as giving a context of the documents belonging to the cluster; by calculating the similarity between each cluster and the query, therefore, we can spot the relevant context of the query; documents that belong to more relevant context or cluster are likely to be relevant to the query." ></td>
	<td class="line x" title="47:149	It should be noted here that the static global clustering is not practical in the current setup, because it takes much computational time and the document space is too sparse (see Anick and Vaithyanathan (1997) for the comparison of static and dynamic clustering)." ></td>
	<td class="line x" title="48:149	2.3.1 Dynamic incremental centroid clustering We make clusters based on incremental centroid method." ></td>
	<td class="line x" title="49:149	There are a few variations in the agglomerative clustering method." ></td>
	<td class="line x" title="50:149	The agglomerative centroid method joins the pair of clusters with the most similar centroid at each stage (Frakes and Baeza-Yates, 1992)." ></td>
	<td class="line x" title="51:149	Incremental centroid clustering method is straightforward." ></td>
	<td class="line x" title="52:149	The input document of incremental clustering proceeds according to the ranks of the top-ranked N documents resulted from vector space retrieval for a query." ></td>
	<td class="line x" title="53:149	Document and cluster centroid are represented in vectors." ></td>
	<td class="line x" title="54:149	For the first input document (rank 1), create one cluster whose member is itself." ></td>
	<td class="line x" title="55:149	For each consecutive document (rank 2, , N), compute cosine similarity between the document and each cluster centroid in the already created clusters." ></td>
	<td class="line x" title="56:149	If the similarity between the document and a cluster is above a threshold, then add the document to the cluster as a member and update cluster centroid." ></td>
	<td class="line x" title="57:149	Otherwise, create a new cluster with this document." ></td>
	<td class="line x" title="58:149	Note that one document can be a member of several clusters as shown in Figure 2 (sold lines show that the document belongs to the cluster)." ></td>
	<td class="line x" title="59:149	2.3.2 Cluster preference Similarities between the clusters and the query, or query-cluster similarities, are calculated by the combination of the query inclusion ratio and vector inner product between the query vector and the centroid vectors of the clusters." ></td>
	<td class="line x" title="60:149	ci t i qi q ww q c cqsimC =  =1 ),( (2) where |q| is the number of terms in the query, |c q | is the number of query terms included in a cluster centroid, |c q |/|q| is the query inclusion ratio for the cluster." ></td>
	<td class="line x" title="61:149	The documents included in the same cluster have the same query-cluster similarity." ></td>
	<td class="line x" title="62:149	Cluster preferences are influenced by the query inclusion ratio, which prefers the cluster whose centroid includes more various query terms." ></td>
	<td class="line x" title="63:149	Thus incorporating this information into the weighting of each document means adding information which is related to the behavior of terms in documents as well as the association of terms and documents into the evaluation of the relevance of each document; it therefore has the effect of ambiguity resolution." ></td>
	<td class="line x" title="64:149	2.4 Reflecting cluster information to the documents Using the query-cluster similarity, we re-calculate the relevance of each document according to the following equation: ),(),(),( cqsimCMAXdqsimDdqsim cd = (3) where simD(q,d) is a query-document similarity by vector space retrieval as defined in equation (1) and simC(q,c) is a query-cluster similarity of a document d defined in equation (2)." ></td>
	<td class="line x" title="65:149	Since each document can be a member of several clusters, we assign the highest query-cluster similarity value to the document." ></td>
	<td class="line x" title="66:149	The new document similarity, sim(q,d), is calculated by multiplication of a query-cluster similarity and a query-document similarity." ></td>
	<td class="line x" title="67:149	Based on this new Figure 2." ></td>
	<td class="line x" title="68:149	Incremental centroid clustering in order of the top-ranked N documents similarity sim(q,d), we re-rank the retrieved documents." ></td>
	<td class="line x" title="69:149	In the equation, we tried to use weighted sum of a query-document similarity and a query-cluster similarity." ></td>
	<td class="line x" title="70:149	The combination by multiplication showed better performances than that of weighted sum." ></td>
	<td class="line x" title="71:149	Through this procedure, we can effectively take into account the contexts of all the terms in a document as well as of the query terms." ></td>
	<td class="line x" title="72:149	Thus, even if a document which has a low query-document similarity can have a high query-cluster similarity thanks to the effect of neighboring documents in the same cluster." ></td>
	<td class="line x" title="73:149	The reverse can be true as well." ></td>
	<td class="line x" title="74:149	3 Experiments 3.1 Experimental environment We evaluated our method on TREC-6 CLIR test collection which contains 242,918 English documents (AP news from 1988 to 1990) and 24 English queries." ></td>
	<td class="line x" title="75:149	English queries are translated to Korean queries manually." ></td>
	<td class="line x" title="76:149	We use title field of queries which consist of three fields such as title, description and narrative." ></td>
	<td class="line x" title="77:149	In dictionary-based query translation, one query term has multiple translations." ></td>
	<td class="line x" title="78:149	Table 3 shows the degree of ambiguities." ></td>
	<td class="line x" title="79:149	The number of Korean query terms 47 The number of translated terms 149 The average number of translations 3.2 Table 1." ></td>
	<td class="line x" title="80:149	The degree of ambiguities for 24 queries." ></td>
	<td class="line x" title="81:149	In our experiment, we only use 14 queries which consist of more than one term to observe real effects of our method." ></td>
	<td class="line x" title="82:149	This is because, if a query consists of more than one term, human can select the correct meaning of the term by its neighbours." ></td>
	<td class="line x" title="83:149	But if a query consists of one term such as bank and it is polysemous, no one can resolve ambiguities without considering additional external information." ></td>
	<td class="line x" title="84:149	The rest 10 queries which consist of one term are used to decide a threshold in incremental clustering." ></td>
	<td class="line x" title="85:149	We use SMART system (Salton, 1989) developed at Cornell as a vector space retrieval." ></td>
	<td class="line x" title="86:149	3.2 Results The retrieval effectiveness was evaluated using the 11-point average precision metric." ></td>
	<td class="line x" title="87:149	We compared our method with original English queries, with translated queries with ambiguities, and with translated queries with the best translation after disambiguation." ></td>
	<td class="line x" title="88:149	The followings are the brief descriptions for comparison methods: 1) monolingual: the performance of vector space retrieval system for original English queries as the monolingual baseline." ></td>
	<td class="line x" title="89:149	2) tall_base: the performance of vector space retrieval system for translated English queries which have all possible translations in bilingual dictionaries without ambiguity resolution." ></td>
	<td class="line x" title="90:149	3) tall_rerank: the performance of proposed method using dynamic incremental clusters for the retrieved documents of tall_base." ></td>
	<td class="line x" title="91:149	4) tone_base: the performance of vector space retrieval system for translated queries with the best translations for each query term after ambiguity resolution based on mutual information." ></td>
	<td class="line x" title="92:149	5) tone_rerank: the performance of proposed method using dynamic incremental clusters for the retrieved documents of tone_base." ></td>
	<td class="line x" title="93:149	tall_rerank and tone_rerank use our implicit disambiguation method." ></td>
	<td class="line x" title="94:149	The number of top N documents used in dynamic incremental clustering is 300 and thresholds for incremental centroid clustering are set as 0.41 which are learned from training 10 queries with one term in both tall_rerank and tone_rerank." ></td>
	<td class="line x" title="95:149	The main objective of this paper is to observe the performance change by incremental clusters for translated queries with ambiguities (tall_base and tall_rerank)." ></td>
	<td class="line x" title="96:149	Comparison 11-pt avg." ></td>
	<td class="line x" title="97:149	C/M Change precision (%) (%) 1) monolingual 0.2858 100 2) tall_base 0.2167 75.82 3) tall_rerank 0.2780 97.27 +28.29 4) tone_base 0.2559 89.54 5) tone_rerank 0.3026 105.87 +18.25 Table 2." ></td>
	<td class="line x" title="98:149	The retrieval effectiveness for comparison methods." ></td>
	<td class="line o" title="99:149	To observe the effect of clusters, we compared the results after disambiguation based on mutual information (tone_base and tone_rerank)." ></td>
	<td class="line o" title="100:149	We selected the best translation based on mutual information among all translation terms." ></td>
	<td class="line oc" title="101:149	Mutual information MI(x,y) is defined as following (Church and Hanks, 1990): )()( ),( log )()( ),( log),( 22 yfxf yxfN ypxp yxp yxMI  == (4) where f(x) and f(y) are frequency of term x and term y, respectively." ></td>
	<td class="line x" title="102:149	Co-occurrence frequency of term x and term y, f(x,y), is taken in window size 6 for AP 1988 news documents." ></td>
	<td class="line x" title="103:149	The 11-point average precision value, corresponding result to monolingual (C/M), and performance change are summarized in Table 2." ></td>
	<td class="line x" title="104:149	The retrieval effectiveness of tall_rerank is 0.2780, corresponding to 97.27% of monolingual performance." ></td>
	<td class="line x" title="105:149	The performance of tone_rerank yields 0.3026 (105.87%)." ></td>
	<td class="line x" title="106:149	This is even better than the monolingual performance." ></td>
	<td class="line p" title="107:149	The performance of our implicit ambiguity resolution method for all translations (tall_rerank) shows 8.63% improvement compared with that of ambiguity resolution based on mutual information (tone_base)." ></td>
	<td class="line x" title="108:149	The proposed method achieved 28% improvement for all translation queries and 18% for best translation queries compared with the vector space retrieval." ></td>
	<td class="line o" title="109:149	Our method after disambiguation (tone_rerank) using mutual information improved about 39.6% over vector space retrieval for all translations queries (tall_base)." ></td>
	<td class="line o" title="110:149	The cluster-based implicit disambiguation method, therefore, is more effective for performance improvement than the simple query disambiguation method based on mutual information; if used together, it shows yet further improvement." ></td>
	<td class="line x" title="111:149	3.3 Result analysis We examined the effects of our method for a query with ambiguities increased after bilingual dictionary-based term translation." ></td>
	<td class="line x" title="112:149	The Korean query is [ja-dong-cha] [gong-gi] [o-yeom] whose original English query is automobile air pollution." ></td>
	<td class="line x" title="113:149	The translated query with all the possible translations in Korean-English dictionaries for this query is as follows: In this query, the term  is polysemous which has several meanings such as <air>, <atmosphere>, <jackstone>, <co-occurrence>, and <bowl>." ></td>
	<td class="line x" title="114:149	This is the cause of degrading system performance." ></td>
	<td class="line x" title="115:149	146 clusters were created for the retrieved 300 documents of this query." ></td>
	<td class="line x" title="116:149	The token number of documents in the clusters was 435." ></td>
	<td class="line x" title="117:149	The distribution of cluster members is shown in Figure 3." ></td>
	<td class="line x" title="118:149	Most non-relevant documents had a tendency to make singleton cluster, and most relevant documents made large group clusters." ></td>
	<td class="line x" title="119:149	We examined inside the clusters how to see cluster give effects to resolve ambiguity and reflect context." ></td>
	<td class="line x" title="120:149	Cluster C4 in Figure 3 has 60 members, which contains 56 relevant documents and 4 non-relevant documents, among 209 relevant documents for this query." ></td>
	<td class="line x" title="121:149	This cluster centroid includes following terms related to the query: car 0.069 automobile 0.127 air 0.082 atmosphere 0.018 pollution 0.196 contamination 0.064  [ja-dong-cha] car, automobile, autocar, motorcar  [gong-gi] air, atmosphere, empty vessel, bowl, jackstone, pebble, marbles [o-yeom] contamination, pollution C2 C4 C17 0 10 20 30 40 50 60 70 80 90 100 0 20 40 60 80 100 120 140 cluster ID # o f me mb e r # o f me mb e r Figure 3." ></td>
	<td class="line x" title="122:149	The distribution of cluster members for the query with translation ambiguities." ></td>
	<td class="line x" title="123:149	Although this centroid includes a noise term atmosphere, its weight is low." ></td>
	<td class="line x" title="124:149	The other terms are appropriate to the query; they are synonyms." ></td>
	<td class="line x" title="125:149	Since all of the query terms are included in the centroid, query inclusion ratio is 1 and all synonyms affect positively to the vector inner product value." ></td>
	<td class="line x" title="126:149	Therefore, since this cluster preference is high, the ranks of all documents in this cluster changed higher." ></td>
	<td class="line x" title="127:149	The cluster performed as a context of the documents relevant to the query." ></td>
	<td class="line x" title="128:149	Cluster C85 is a singleton whose centroid includes one of three query terms: bowl 0.101 marble 0.19 Since query inclusion ratio is low, the cluster preference is low." ></td>
	<td class="line x" title="129:149	Therefore this clusters effect is weak to the document." ></td>
	<td class="line x" title="130:149	Figure 4 presents the rank changes, calculated by subtracting ranks by our method (tall_rerank) from those by vector space retrieval (tall_base) for each relevant document of the ambiguous query." ></td>
	<td class="line x" title="131:149	The ranks of most documents are changed higher through cluster analysis, although the ranks of some documents are changed lower." ></td>
	<td class="line x" title="132:149	Figure 5 shows recall/precision curves for the performances of original English query (monolingual; 0.6783 in 11-pt avg." ></td>
	<td class="line x" title="133:149	precision), translated query without disambiguation (tall_base; 0.5635), and our method (tall_rerank; 0.6622)." ></td>
	<td class="line x" title="134:149	For increased query ambiguity, we could achieve 97.62% performance compared to the monolingual retrieval." ></td>
	<td class="line x" title="135:149	These results indicate that cluster analysis help to resolve ambiguity." ></td>
	<td class="line x" title="136:149	Thus, we could effectively take into account the context of all the terms in a document as well as the query terms." ></td>
	<td class="line x" title="137:149	4 Conclusion We have proposed the method of applying dynamic incremental clustering to the implicit resolution of query ambiguities in Korean-to-English cross-language information retrieval." ></td>
	<td class="line x" title="138:149	The method used the clusters of 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.00.10.20.30.40.50.60.70.80.91.0 recall pr e c is io n monolingual tall_base tall_rerank Figure 5." ></td>
	<td class="line x" title="139:149	The performance comparison for the ambiguous query." ></td>
	<td class="line x" title="140:149	R a nk ch an g e s ( r ank o f t a l l _ ba s e  r a n k o f ta ll_ re ra n k ) -120 -80 -40 0 40 80 120 160 1177 19188 29999 54711 60407 72683 91425 100629 110822 118102 124995 128749 133254 139770 142584 152762 169323 174094 177572 178692 179716 182659 186115 196077 196985 207812 224256 227464 229475 233708 Relevant document ID for the query R a nk ch an g e s ( r ank o f t a l l _ ba s e  r a n k o f ta ll_ re ra n k ) Figure 4." ></td>
	<td class="line x" title="141:149	The rank changes of tall_rerank from rank of tall_base for each relevant document of the query." ></td>
	<td class="line x" title="142:149	retrieved documents as a context for re-weighting each retrieved document and for re-ranking the retrieved documents." ></td>
	<td class="line x" title="143:149	Our method was evaluated on TREC-6 CLIR test collection." ></td>
	<td class="line x" title="144:149	This method achieved 28.29% performance improvement for translated queries without ambiguity resolution." ></td>
	<td class="line x" title="145:149	This corresponds to 97.27% of the monolingual performance." ></td>
	<td class="line o" title="146:149	When our method was used with the query ambiguity resolution method based on mutual information, it showed 105.87% performance improvement of the monolingual retrieval." ></td>
	<td class="line x" title="147:149	These results indicate that cluster analysis help to resolve ambiguity greatly, and each cluster itself provide a context for a query." ></td>
	<td class="line x" title="148:149	Our method is a language independent model which can be applied to any language retrieval." ></td>
	<td class="line x" title="149:149	We expect that our method will further improve the results, although further research is needed on combining a method to improve recall such as query expansion and relevance feedback." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J02-2003
Class-Based Probability Estimation Using A Semantic Hierarchy
Clark, Stephen;Weir, David J.;"></td>
	<td class="line x" title="1:326	c 2002 Association for Computational Linguistics Class-Based Probability Estimation Using a Semantic Hierarchy Stephen Clark  David Weir  University of Edinburgh University of Sussex This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate." ></td>
	<td class="line x" title="2:326	In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses." ></td>
	<td class="line x" title="3:326	There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy." ></td>
	<td class="line x" title="4:326	A procedure is developed that uses a chi-square test to determine a suitable level of generalization." ></td>
	<td class="line x" title="5:326	In order to test the performance of the estimation method, a pseudo-disambiguation task is used, together with two alternative estimation methods." ></td>
	<td class="line x" title="6:326	Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resniks measure of selectional preference." ></td>
	<td class="line x" title="7:326	In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic." ></td>
	<td class="line x" title="8:326	1." ></td>
	<td class="line x" title="9:326	Introduction This article concerns the problem of how to estimate the probabilities of noun senses appearing as particular arguments of predicates." ></td>
	<td class="line x" title="10:326	Such probabilities can be useful for a variety of natural language processing (NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling." ></td>
	<td class="line x" title="11:326	To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity: Example 1 Fred ate strawberries with a spoon." ></td>
	<td class="line x" title="12:326	The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate." ></td>
	<td class="line x" title="13:326	The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of ate-with than strawberries-with (Li and Abe 1998; Clark and Weir 2000)." ></td>
	<td class="line x" title="14:326	The problem with estimating a probability model defined over a large vocabulary of predicates and noun senses is that this involves a huge number of parameters, which results in a sparse-data problem." ></td>
	<td class="line x" title="15:326	In order to reduce the number of parameters, we propose to define a probability model over senses in a semantic hierarchy and  Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK." ></td>
	<td class="line x" title="16:326	E-mail: stephenc@cogsci.ed.ac.uk." ></td>
	<td class="line x" title="17:326	 School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK." ></td>
	<td class="line x" title="18:326	E-mail: david.weir@cogs.susx.ac.uk." ></td>
	<td class="line x" title="19:326	188 Computational Linguistics Volume 28, Number 2 to exploit the fact that senses can be grouped into classes consisting of semantically similar senses." ></td>
	<td class="line x" title="20:326	The assumption underlying this approach is that the probability of a particular noun sense can be approximated by a probability based on a suitably chosen class." ></td>
	<td class="line x" title="21:326	For example, it seems reasonable to suppose that the probability of (the food sense of) chicken appearing as an object of the verb eat can be approximated in some way by a probability based on a class such as FOOD." ></td>
	<td class="line x" title="22:326	There are two elements involved in the problem of using a class to estimate the probability of a noun sense." ></td>
	<td class="line x" title="23:326	First, given a suitably chosen class, how can that class be used to estimate the probability of the sense?" ></td>
	<td class="line x" title="24:326	And second, given a particular noun sense, how can a suitable class be determined?" ></td>
	<td class="line x" title="25:326	This article offers novel solutions to both problems, and there is a particular focus on the second question, which can be thought of as how to find a suitable level of generalization in the hierarchy." ></td>
	<td class="line x" title="26:326	1 The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum 1998), version 1.6." ></td>
	<td class="line x" title="27:326	Previous work has considered how to estimate probabilities using classes from WordNet in the context of acquiring selectional preferences (Resnik 1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also addressed the question of how to determine a suitable level of generalization in the hierarchy." ></td>
	<td class="line x" title="28:326	Li and Abe use the minimum description length principle to obtain a level of generalization, and Resnik uses a simple technique based on a statistical measure of selectional preference." ></td>
	<td class="line x" title="29:326	(The work by Ribas builds on that by Resnik, and the work by McCarthy builds on that by Li and Abe)." ></td>
	<td class="line x" title="30:326	We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task." ></td>
	<td class="line x" title="31:326	Our method outperforms these alternatives on the pseudo-disambiguation task, and an analysis of the results shows that the generalization methods of Resnik and Li and Abe appear to be overgeneralizing, at least for this task." ></td>
	<td class="line x" title="32:326	Note that the problem being addressed here is the engineering problem of estimating predicate argument probabilities, with the aim of producing estimates that will be useful for NLP applications." ></td>
	<td class="line x" title="33:326	In particular, we are not addressing the problem of acquiring selectional restrictions in the way this is usually construed (Resnik 1993; Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000)." ></td>
	<td class="line x" title="34:326	The purpose of using a semantic hierarchy for generalization is to overcome the sparse data problem, rather than find a level of abstraction that best represents the selectional restrictions of some predicate." ></td>
	<td class="line x" title="35:326	This point is considered further in Section 5." ></td>
	<td class="line x" title="36:326	The next section describes the noun hierarchy from WordNet and gives a more precise description of the probabilities to be estimated." ></td>
	<td class="line x" title="37:326	Section 3 shows how a class from WordNet can be used to estimate the probability of a noun sense." ></td>
	<td class="line x" title="38:326	Section 4 shows how a chi-square test is used as part of the generalization procedure, and Section 5 describes the generalization procedure." ></td>
	<td class="line x" title="39:326	Section 6 describes the alternative class-based estimation methods used in the pseudo-disambiguation experiments, and Section 7 presents those experiments." ></td>
	<td class="line x" title="40:326	2." ></td>
	<td class="line x" title="41:326	The Semantic Hierarchy The noun hierarchy of WordNet consists of senses, or what Miller (1998) calls lexicalized concepts, organized according to the is-a-kind-of relation." ></td>
	<td class="line x" title="42:326	Note that we are using concept to refer to a lexicalized concept or sense and not to a set of senses; we use class to refer to a set of senses." ></td>
	<td class="line x" title="43:326	There are around 66,000 different concepts in the noun hierarchy 1 A third element of the problem, namely, how to obtain arguments of predicates as training data, is not considered here." ></td>
	<td class="line x" title="44:326	We assume the existence of such data, obtained from a treebank or shallow parser." ></td>
	<td class="line x" title="45:326	189 Clark and Weir Class-Based Probability Estimation of WordNet version 1.6." ></td>
	<td class="line x" title="46:326	A concept in WordNet is represented by a synset, which is the set of synonymous words that can be used to denote that concept." ></td>
	<td class="line x" title="47:326	For example, the synset for the concept cocaine 2 is {cocaine, cocain, coke, snow, C}." ></td>
	<td class="line x" title="48:326	Let syn(c) be the synset for concept c, and let cn(n)={c |n  syn(c) } be the set of concepts that can be denoted by noun n. The hierarchy has the structure of a directed acyclic graph (although only around 1% of the nodes have more than one parent), where the edges of the graph constitute what we call the directisa relation." ></td>
	<td class="line x" title="49:326	Let isa be the transitive, reflexive closure of directisa; then c prime isa c implies c prime is a kind of c.Ifc prime isa c, then c is a hypernym of c prime and c prime is a hyponym of c. In fact, the hierarchy is not a single hierarchy but instead consists of nine separate subhierarchies, each headed by the most general kind of concept, such as entity, abstraction, event, and psychological feature." ></td>
	<td class="line x" title="50:326	For the purposes of this work we add a common root dominating the nine subhierarchies, which we denote root." ></td>
	<td class="line x" title="51:326	There are some important points that need to be clarified regarding the hierarchy." ></td>
	<td class="line x" title="52:326	First, every concept in the hierarchy has a nonempty synset (except the notional concept root)." ></td>
	<td class="line x" title="53:326	Even the most general concepts, such as entity, can be denoted by some noun; the synset for entity is {entity, something}." ></td>
	<td class="line x" title="54:326	Second, there is an important distinction between an individual concept and a set of concepts." ></td>
	<td class="line x" title="55:326	For example, the individual concept entity should not be confused with the set or class consisting of concepts denoting kinds of entities." ></td>
	<td class="line x" title="56:326	To make this distinction clear, we use c = {c prime |c prime isa c } to denote the set of concepts dominated by concept c, including c itself." ></td>
	<td class="line x" title="57:326	For example, animal is the set consisting of those concepts corresponding to kinds of animals (including animal itself)." ></td>
	<td class="line x" title="58:326	The probability of a concept appearing as an argument of a predicate is written p(c | v, r), where c is a concept in WordNet, v is a predicate, and r is an argument position." ></td>
	<td class="line x" title="59:326	3 The focus in this article is on the arguments of verbs, but the techniques discussed can be applied to any predicate that takes nominal arguments, such as adjectives." ></td>
	<td class="line x" title="60:326	The probability p(c | v, r) is to be interpreted as follows: This is the probability that some noun n in syn(c), when denoting concept c, appears in position r of verb v (given v and r)." ></td>
	<td class="line x" title="61:326	The example used throughout the article is p(dog|run, subj), which is the conditional probability that some noun in the synset of dog, when denoting the concept dog, appears in the subject position of the verb run." ></td>
	<td class="line x" title="62:326	Note that, in practice, no distinction is made between the different senses of a verb (although the techniques do allow such a distinction) and that each use of a noun is assumed to correspond to exactly one concept." ></td>
	<td class="line x" title="63:326	4 3." ></td>
	<td class="line x" title="64:326	Class-Based Probability Estimation This section explains how a set of concepts, or class, from WordNet can be used to estimate the probability of an individual concept." ></td>
	<td class="line x" title="65:326	More specifically, we explain how a set of concepts c prime, where c prime is some hypernym of concept c, can be used to estimate p(c | v, r)." ></td>
	<td class="line x" title="66:326	(Recall that c prime denotes the set of concepts dominated by c prime, including c prime itself.)" ></td>
	<td class="line x" title="67:326	One possible approach would be simply to substitute c prime for the individual concept c. This is a poor solution, however, since p(c prime | v, r) is the conditional probability that 2 Angled brackets are used to denote concepts in the hierarchy." ></td>
	<td class="line x" title="68:326	3 The term predicate is used loosely here, in that the predicate does not have to be a semantic object but can simply be a word form." ></td>
	<td class="line x" title="69:326	4 A recent paper that extends the acquisition of selectional preferences to sense-sense relationships is Agirre and Martinez (2001)." ></td>
	<td class="line x" title="70:326	190 Computational Linguistics Volume 28, Number 2 some noun denoting a concept in c prime appears in position r of verb v. For example, p(animal|run, subj) is the probability that some noun denoting a kind of animal appears in the subject position of the verb run." ></td>
	<td class="line x" title="71:326	Probabilities of sets of concepts are obtained by summing over the concepts in the set: p(c prime | v, r)= summationdisplay c primeprime c prime p(c primeprime | v, r)(1) This means that p(animal|run, subj) is likely to be much greater than p(dog| run, subj) and thus is not a good approximation of p(dog|run, subj)." ></td>
	<td class="line x" title="72:326	What can be done, though, is to condition on sets of concepts." ></td>
	<td class="line x" title="73:326	If it can be shown that p(v | c prime, r), for some hypernym c prime of c, is a reasonable approximation of p(v | c, r), then we have a way of estimating p(c | v, r)." ></td>
	<td class="line x" title="74:326	The probability p(v | c, r) can be obtained from p(c | v, r) using Bayes theorem: p(c | v, r)=p(v | c, r) p(c | r) p(v | r) (2) Since p(c | r) and p(v | r) are conditioned on the argument slot only, we assume these can be estimated satisfactorily using relative frequency estimates." ></td>
	<td class="line x" title="75:326	Alternatively, a standard smoothing technique such as Good-Turing could be used." ></td>
	<td class="line x" title="76:326	5 This leaves p(v | c, r)." ></td>
	<td class="line x" title="77:326	Continuing with the dog example, the proposal is to estimate p(run |dog, subj) using a relative-frequency estimate of p(run | animal, subj) or an estimate based on a similar, suitably chosen class." ></td>
	<td class="line x" title="78:326	Thus, assuming this choice of class, p(dog|run, subj) would be approximated as follows: p(dog|run, subj)  p(run | animal, subj) p(dog|subj) p(run | subj) (3) The following derivation shows that if p(v | c prime i, r)=k for each child c prime i of c prime, and p(v | c prime, r)=k, then p(v | c prime, r) is also equal to k: p(v | c prime, r)=p(c prime | v, r) p(v | r) p(c prime | r) (4) = p(v | r) p(c prime | r) parenleftBigg summationdisplay i p(c prime i | v, r)+p(c prime | v, r) parenrightBigg (5) = p(v | r) p(c prime | r) parenleftBigg summationdisplay i p(v | c prime i, r) p(c prime i | r) p(v | r) + p(v | c prime, r) p(c prime | r) p(v | r) parenrightBigg (6) = 1 p(c prime | r) parenleftBigg summationdisplay i kp(c prime i | r)+kp(c prime | r) parenrightBigg (7) = k p(c prime | r) parenleftBigg summationdisplay i p(c prime i | r)+p(c prime | r) parenrightBigg (8) = k (9) 5 Unsmoothed estimates were used in this work." ></td>
	<td class="line x" title="79:326	191 Clark and Weir Class-Based Probability Estimation Note that the proof applies only to a tree, since the proof assumes that c prime is partitioned by c prime and the sets of concepts dominated by each of the daughters of c prime, which is not necessarily true for a directed acyclic graph (DAG)." ></td>
	<td class="line x" title="80:326	WordNet is a DAG but is a close approximation to a tree, and so we assume this will not be a problem in practice." ></td>
	<td class="line x" title="81:326	6 The derivation in (4)(9) shows how probabilities conditioned on sets of concepts can remain constant when moving up the hierarchy, and this suggests a way of finding a suitable set, c prime, as a generalization for concept c: Initially set c prime equal to c and move up the hierarchy, changing the value of c prime, until there is a significant change in p(v | c prime, r)." ></td>
	<td class="line x" title="82:326	Estimates of p(v | c prime i, r), for each child c prime i of c prime, can be compared to see whether p(v | c prime, r) has significantly changed." ></td>
	<td class="line x" title="83:326	(We ignore the probability p(v | c prime, r) and consider the probabilities p(v | c prime i, r) only)." ></td>
	<td class="line x" title="84:326	Note that this procedure rests on the assumption that p(v | c, r) is close to p(v | c, r)." ></td>
	<td class="line x" title="85:326	(In fact, p(v | c, r) is equal to p(v | c, r) when c is a leaf node)." ></td>
	<td class="line x" title="86:326	So when finding a suitable level for the estimation of p(sandwich|eat, obj), for example, we first assume that p(eat | sandwich, obj) is a good approximation of p(eat |sandwich, obj) and then apply the procedure to p(eat | sandwich, obj)." ></td>
	<td class="line x" title="87:326	A feature of the proposed generalization procedure is that comparing probabilities of the form p(v | C, r), where C is a class, is closely related to comparing ratios of probabilities of the form p(C | v, r)/p(C | r) (for a given verb and argument position): p(v | C, r)= p(C | v, r) p(C | r) p(v | r)(10) Note that, for a given verb and argument position, p(v | r) is constant across classes." ></td>
	<td class="line oc" title="88:326	Equation (10) is of interest because the ratio p(C | v, r)/p(C | r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resniks association score, which will be introduced in Section 6." ></td>
	<td class="line x" title="89:326	Thus the generalization procedure can be thought of as one that finds homogeneous areas of the hierarchy, that is, areas consisting of classes that are associated to a similar degree with the verb (Clark and Weir 1999)." ></td>
	<td class="line x" title="90:326	Finally, we note that the proposed estimation method does not guarantee that the estimates form a probability distribution over the concepts in the hierarchy, and so a normalization factor is required: p sc (c | v, r)= p(v | [c, v, r], r) p(c|r) p(v|r) summationtext c prime C p(v | [c prime, v, r], r) p(c prime |r) p(v|r) (11) We use p sc to denote an estimate obtained using our method (since the technique finds sets of semantically similar senses, or similarity classes) and [c, v, r] to denote the class chosen for concept c in position r of verb v; p denotes a relative frequency estimate, and C denotes the set of concepts in the hierarchy." ></td>
	<td class="line x" title="91:326	Before providing the details of the generalization procedure, we give the relativefrequency estimates of the relevant probabilities and deal with the problem of am6 Li and Abe (1998) also develop a theoretical framework that applies only to a tree and turn WordNet into a tree by copying each subgraph with multiple parents." ></td>
	<td class="line x" title="92:326	One way to extend the experiments in Section 7 would be to investigate whether this transformation has an impact on the results of those experiments." ></td>
	<td class="line x" title="93:326	192 Computational Linguistics Volume 28, Number 2 biguous data." ></td>
	<td class="line x" title="94:326	The relative-frequency estimates are as follows: p(c | r)= f(c,r) f(r) = summationtext v prime V f(c, v prime, r) summationtext v prime V summationtext c prime C f(c prime, v prime, r) (12) p(v | r)= f(v,r) f(r) = summationtext c prime C f(c prime, v, r) summationtext v prime V summationtext c prime C f(c prime, v prime, r) (13) p(v | c prime, r)= f(c prime,v,r) f(c prime,r) = summationtext c primeprime c prime f(c primeprime, v, r) summationtext v prime V summationtext c primeprime c prime f(c primeprime, v prime, r) (14) where f(c, v, r) is the number of (n, v, r) triples in the data in which n is being used to denote c, and V is the set of verbs in the data." ></td>
	<td class="line x" title="95:326	The problem is that the estimates are defined in terms of frequencies of senses, whereas the data are assumed to be in the form of (n, v, r) triples: a noun, verb, and argument position." ></td>
	<td class="line x" title="96:326	All the data used in this work have been obtained from the British National Corpus (BNC), using the system of Briscoe and Carroll (1997), which consists of a shallow-parsing component that is able to identify verbal arguments." ></td>
	<td class="line x" title="97:326	We take a simple approach to the problem of estimating the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f(c, v, r)= summationdisplay nsyn(c) f(n, v, r) |cn(n)| (15) where  f(c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n)| is the cardinality of cn(n)." ></td>
	<td class="line x" title="98:326	This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000)." ></td>
	<td class="line x" title="99:326	7 Resnik (1998) explains how this apparently crude technique works surprisingly well." ></td>
	<td class="line x" title="100:326	Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000)." ></td>
	<td class="line x" title="101:326	4." ></td>
	<td class="line x" title="102:326	Using a Chi-Square Test to Compare Probabilities In this section we show how to test whether p(v | c prime, r) changes significantly when considering a node higher in the hierarchy." ></td>
	<td class="line x" title="103:326	Consider the problem of deciding whether p(run | canine, subj) is a good approximation of p(run | dog, subj).(canine is the parent of dog in WordNet)." ></td>
	<td class="line x" title="104:326	To do this, the probabilities p(run | c prime i, subj) are compared using a chi-square test, where the c prime i are the children of canine." ></td>
	<td class="line x" title="105:326	In this case, the null hypothesis of the test is that the probabilities p(run | c i, subj) are the same for each child c i. By judging the strength of the evidence against the null hypothesis, how similar the true probabilities are likely to be can be determined." ></td>
	<td class="line x" title="106:326	If the test indicates that the probabilities are sufficiently unlikely to be the same, then the null hypothesis is rejected, and the conclusion is that p(run | canine, subj) is not a good approximation of p(run | dog, subj)." ></td>
	<td class="line x" title="107:326	An example contingency table, based on counts obtained from a subset of the BNC using the system of Briscoe and Carroll, is given in Table 1." ></td>
	<td class="line x" title="108:326	(Recall that the frequencies are estimated by distributing the count for a noun equally among the nouns senses; this explains the fractional counts)." ></td>
	<td class="line x" title="109:326	One column contains estimates of counts arising 7 Resnik takes a similar approach but divides the count evenly among the nouns senses and all the hypernyms of those senses." ></td>
	<td class="line x" title="110:326	193 Clark and Weir Class-Based Probability Estimation Table 1 Contingency table for the children of canine in the subject position of run." ></td>
	<td class="line x" title="111:326	c i  f(c i, run, subj)  f(c i, subj)   f(c i, run, subj)  f(c i, subj)= summationtext vV  f(c i, v, subj) bitch 0.3 (0.5) 26.7 (26.6) 27.0 dog 12.8 (10.5) 620.4 (622.7) 633.2 wolf 0.3 (0.6) 38.7 (38.4) 39.0 jackal 0.0 (0.3) 20.0 (19.7) 20.0 wild dog 0.0 (0.0) 3.0 (3.0) 3.0 hyena 0.0 (0.2) 10.0 (9.8) 10.0 fox 0.0 (1.2) 72.3 (71.1) 72.3 13.4 791.1 804.5 from concepts in c i appearing in the subject position of the verb run:  f(c i, run, subj).A second column presents estimates of counts arising from concepts in c i appearing in the subject position of a verb other than run." ></td>
	<td class="line x" title="112:326	The figures in brackets are the expected values if the null hypothesis is true." ></td>
	<td class="line x" title="113:326	There is a choice of which statistic to use in conjunction with the chi-square test." ></td>
	<td class="line x" title="114:326	The usual statistic encountered in textbooks is the Pearson chi-square statistic, denoted X 2 : X 2 = summationdisplay i,j (o ij  e ij ) 2 e ij (16) where o ij is the observed value for the cell in row i and column j, and e ij is the corresponding expected value." ></td>
	<td class="line x" title="115:326	An alternative statistic is the log-likelihood chi-square statistic, denoted G 2 : 8 G 2 = 2 summationdisplay i,j o ij log e o ij e ij (17) The two statistics have similar values when the counts in the contingency table are large (Agresti 1996)." ></td>
	<td class="line x" title="116:326	The statistics behave differently, however, when the table contains low counts, and, since corpus data are likely to lead to some low counts, the question of which statistic to use is an important one." ></td>
	<td class="line x" title="117:326	Dunning (1993) argues for the use of G 2 rather than X 2, based on an analysis of the sampling distributions of G 2 and X 2, and results obtained when using the statistics to acquire highly associated bigrams." ></td>
	<td class="line x" title="118:326	We consider Dunnings analysis at the end of this section, and the question of whether to use G 2 or X 2 will be discussed further there." ></td>
	<td class="line x" title="119:326	For now, we continue with the discussion of how the chi-square test is used in the generalization procedure." ></td>
	<td class="line x" title="120:326	For Table 1, the value of G 2 is 3.8, and the value of X 2 is 2.5." ></td>
	<td class="line x" title="121:326	Assuming a level of significance of  = 0.05, the critical value is 12.6 (for six degrees of freedom)." ></td>
	<td class="line x" title="122:326	Thus, for this  value, the null hypothesis would not be rejected for either statistic, and the conclusion would be that there is no reason to suppose that p(run | canine, subj) is not a reasonable approximation of p(run | dog, subj)." ></td>
	<td class="line x" title="123:326	8 An alternative formula for G 2 is given in Dunning (1993), but the two are equivalent." ></td>
	<td class="line x" title="124:326	194 Computational Linguistics Volume 28, Number 2 Table 2 Contingency table for the children of liquid in the object position of drink." ></td>
	<td class="line x" title="125:326	c i  f(c i, drink, obj)  f(c i, obj)   f(c i, drink, obj)  f(c i, obj)= summationtext vV  f(c i, v, obj) beverage 261.0 (238.7) 2,367.7 (2,390.0) 2,628.7 supernatant 0.0 (0.1) 1.0 (0.9) 1.0 alcohol 11.5 (9.4) 92.0 (94.1) 103.5 ammonia 0.0 (0.8) 8.5 (7.7) 8.5 antifreeze 0.0 (0.1) 1.0 (0.9) 1.0 distillate 0.0 (0.5) 6.0 (5.5) 6.0 water 12.0 (31.6) 335.7 (316.1) 347.7 ink 0.0 (2.9) 32.0 (29.1) 32.0 liquor 0.7 (1.1) 11.6 (11.2) 12.3 285.2 2,855.5 3,140.7 As a further example, Table 2 gives counts for the children of liquid in the object position of drink." ></td>
	<td class="line x" title="126:326	Again, the counts have been obtained from a subset of the BNC using the system of Briscoe and Carroll." ></td>
	<td class="line x" title="127:326	Not all the sets dominated by the children of liquid are shown, as some, such as sheep dip, never appear in the object position of a verb in the data." ></td>
	<td class="line x" title="128:326	This example is designed to show a case in which the null hypothesis is rejected." ></td>
	<td class="line x" title="129:326	The value of G 2 for this table is 29.0, and the value of X 2 is 21.2." ></td>
	<td class="line x" title="130:326	So for G 2, even if an  value as low as 0.0005 were being used (for which the critical value is 27.9 for eight degrees of freedom), the null hypothesis would still be rejected." ></td>
	<td class="line x" title="131:326	For X 2, the null hypothesis is rejected for  values greater than 0.005." ></td>
	<td class="line x" title="132:326	This seems reasonable, since the probabilities associated with the children of liquid and the object position of drink would be expected to show a lot of variation across the children." ></td>
	<td class="line x" title="133:326	A key question is how to select the appropriate value for ." ></td>
	<td class="line x" title="134:326	One solution is to treat  as a parameter and set it empirically by taking a held-out test set and choosing the value of  that maximizes performance on the relevant task." ></td>
	<td class="line x" title="135:326	For example, Clark and Weir (2000) describes a prepositional phrase attachment algorithm that employs probability estimates obtained using the WordNet method described here." ></td>
	<td class="line x" title="136:326	To set the value of , the performance of the algorithm on a development set could be compared across different values of , and the value that leads to the best performance could be chosen." ></td>
	<td class="line x" title="137:326	Note that this approach sets no constraints on the value of : The value could be as high as 0.995 or as low as 0.0005, depending on the particular application." ></td>
	<td class="line x" title="138:326	There may be cases in which the conditions for the appropriate application of a chisquare test are not met." ></td>
	<td class="line x" title="139:326	One condition that is likely to be violated is the requirement that expected values in the contingency table not be too small." ></td>
	<td class="line x" title="140:326	(A rule of thumb often found in textbooks is that the expected values should be greater than five)." ></td>
	<td class="line x" title="141:326	One response to this problem is to apply some kind of thresholding and either ignore counts below the threshold, or apply the test only to tables that do not contain low counts." ></td>
	<td class="line x" title="142:326	Ribas (1995), Li and Abe (1998), McCarthy (2000), and Wagner (2000) all use some kind of thresholding when dealing with counts in the hierarchy (although not in the context of a chi-square test)." ></td>
	<td class="line x" title="143:326	Another approach would be to use Fishers exact test (Agresti 1996; Pedersen 1996), which can be applied to tables regardless of the size of 195 Clark and Weir Class-Based Probability Estimation the counts they contain." ></td>
	<td class="line x" title="144:326	The main problem with this test is that it is computationally expensive, especially for large contingency tables." ></td>
	<td class="line x" title="145:326	What we have found in practice is that applying the chi-square test to tables dominated by low counts tends to produce an insignificant result, and the null hypothesis is not rejected." ></td>
	<td class="line x" title="146:326	The consequences of this for the generalization procedure are that low-count tables tend to result in the procedure moving up to the next node in the hierarchy." ></td>
	<td class="line x" title="147:326	But given that the purpose of the generalization is to overcome the sparsedata problem, moving up a node is desirable, and therefore we do not modify the test for tables with low counts." ></td>
	<td class="line x" title="148:326	The final issue to consider is which chi-square statistic to use." ></td>
	<td class="line x" title="149:326	Dunning (1993) argues for the use of G 2 rather than X 2, based on the claim that the sampling distribution of G 2 approaches the true chi-square distribution quicker than the sampling distribution of X 2 . However, Agresti (1996, page 34) makes the opposite claim: The sampling distributions of X 2 and G 2 get closer to chi-squared as the sample size n increasesThe convergence is quicker for X 2 than G 2 . In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic." ></td>
	<td class="line x" title="150:326	Finally, the results of the pseudo-disambiguation experiments presented in Section 7 are at least as good, if not better, when using X 2 rather than G 2, and so we conclude that the question of which statistic to use should be answered on a per application basis." ></td>
	<td class="line x" title="151:326	5." ></td>
	<td class="line x" title="152:326	The Generalization Procedure The procedure for finding a suitable class, c prime, to generalize concept c in position r of verb v works as follows." ></td>
	<td class="line x" title="153:326	(We refer to c prime as the similarity class of c with respect to v and r and the hypernym c prime as top(c, v, r), since the chosen hypernym sits at the top of the similarity class)." ></td>
	<td class="line x" title="154:326	Initially, concept c is assigned to a variable top." ></td>
	<td class="line x" title="155:326	Then, by working up the hierarchy, successive hypernyms of c are assigned to top, and this process continues until the probabilities associated with the sets of concepts dominated by top and the siblings of top are significantly different." ></td>
	<td class="line x" title="156:326	Once a node is reached that results in a significant result for the chi-square test, the procedure stops, and top is returned as top(c, v, r)." ></td>
	<td class="line x" title="157:326	In cases where a concept has more than one parent, the parent is chosen that results in the lowest value of the chi-square statistic, as this indicates the probabilities are the most similar." ></td>
	<td class="line x" title="158:326	The set top(c, v, r) is the similarity class of c for verb v and position r. Figure 1 gives an algorithm for determining top(c, v, r)." ></td>
	<td class="line x" title="159:326	Figure 2 gives an example of the procedure at work." ></td>
	<td class="line x" title="160:326	Here, top(soup, stir, obj) is being determined." ></td>
	<td class="line x" title="161:326	The example is based on data from a subset of the BNC, with 303 cases of an argument in the object position of stir." ></td>
	<td class="line x" title="162:326	The G 2 statistic is used, together with an  value of 0.05." ></td>
	<td class="line x" title="163:326	Initially, top is set to soup, and the probabilities corresponding to the children of dish are compared: p(stir | soup, obj), p(stir | lasagne, obj), p(stir | haggis, obj), and so on for the rest of the children." ></td>
	<td class="line x" title="164:326	The chi-square test results in a G 2 value of 14.5, compared to a critical value of 55.8." ></td>
	<td class="line x" title="165:326	Since G 2 is less than the critical value, the procedure moves up to the next node." ></td>
	<td class="line x" title="166:326	This process continues until a significant result is obtained, which first occurs at substance when comparing the children of object." ></td>
	<td class="line x" title="167:326	Thus substance is the chosen level of generalization." ></td>
	<td class="line x" title="168:326	Now we show how the chosen level of generalization varies with  and how it varies with the size of the data set." ></td>
	<td class="line x" title="169:326	A note of clarification is required before presenting the results." ></td>
	<td class="line x" title="170:326	In related work on acquiring selectional preferences (Ribas 1995; McCarthy 196 Computational Linguistics Volume 28, Number 2 Algorithm top(c, v, r): top  c sig result  false comment parent min gives lowest G 2 value, G 2 min while not sig result & top negationslash= root do G 2 min  for all parents of top do calculate G 2 for sets dominated by children of parent if G 2 < G 2 min then G 2 min  G 2 parent min  parent end if chi-square test for parent min is significant then sig result  true else move up to next node: top  parent min end return top Figure 1 An algorithm for determining top(c, v, r)." ></td>
	<td class="line x" title="171:326	CWCPCVCVCXD7D0CPD7CPCVD2CT CSCXD7CW D2D3D9D6CXD7CWD1CTD2D8 CUD3D3CS CUCPD6CT CQCTDACTD6CPCVCT CRD3D9D6D7CTD1CTCPD0 D7D9CQD7D8CPD2CRCT D3CQCYCTCRD8 ADD9CXCS D4D3CXD7D3D2 CPD6D8CXCUCPCRD8CVD6D3D9D2CS CTD2D8CXD8DD D7D3D9D4 BZ BE BM BDBGBMBHB8 CRD6CXD8CXCRCPD0 DACPD0D9CTBM BHBHBMBK BZ BE BM BHBMBGB8 CRD6CXD8 DACPD0BM BDBIBMBL BZ BE BM BHBMBHB8 CRD6CXD8 DACPD0BM BDBIBMBL BZ BE BM BEBLBMBLB8 CRD6CXD8 DACPD0BM BHBKBMBD BZ BE BM BDBGBDBMBDB8 CRD6CXD8 DACPD0BM BFBJBMBJ Figure 2 An example generalization: Determining top(soup, stir, obj)." ></td>
	<td class="line x" title="172:326	197 Clark and Weir Class-Based Probability Estimation 1997; Li and Abe 1998; Wagner 2000), the level of generalization is often determined for a small number of hand-picked verbs and the result compared with the researchers intuition about the most appropriate level for representing a selectional preference." ></td>
	<td class="line x" title="173:326	According to this approach, if sandwich were chosen to represent hotdog in the object position of eat, this might be considered an undergeneralization, since food might be considered more appropriate." ></td>
	<td class="line x" title="174:326	For this work we argue that such an evaluation is not appropriate; since the purpose of this work is probability estimation, the most appropriate level is the one that leads to the most accurate estimate, and this may or may not agree with intuition." ></td>
	<td class="line x" title="175:326	Furthermore, we show in Section 7 that to generalize unnecessarily can be harmful for some tasks: If we already have lots of data regarding sandwich, why generalize any higher?" ></td>
	<td class="line x" title="176:326	Thus the purpose of this section is not to show that the acquired levels are correct, but simply to show how the levels vary with  and the sample size." ></td>
	<td class="line x" title="177:326	To show how the level of generalization varies with changes in , top(c, v, obj) was determined for a number of hand-picked (c, v, obj) triples over a range of values for ." ></td>
	<td class="line x" title="178:326	The triples were chosen to give a range of strongly and weakly selecting verbs and a range of verb frequencies." ></td>
	<td class="line x" title="179:326	The data were again extracted from a subset of the BNC using the system of Briscoe and Carroll (1997), and the G 2 statistic was used in the chi-square test." ></td>
	<td class="line x" title="180:326	The results are shown in Table 3." ></td>
	<td class="line x" title="181:326	The number of times the verb occurred with some object is also given in the table." ></td>
	<td class="line x" title="182:326	The results suggest that the generalization level becomes more specific as  increases." ></td>
	<td class="line x" title="183:326	This is to be expected, since, given a contingency table chosen at random, a higher value of  is more likely to lead to a significant result than a lower value of ." ></td>
	<td class="line x" title="184:326	We also see that, for some cases, the value of  has little effect on the level." ></td>
	<td class="line x" title="185:326	We would expect there to be less change in the level of generalization for strongly selecting verbs, such as drink and eat, and a greater range of levels for weakly selecting verbs such as see." ></td>
	<td class="line x" title="186:326	This is because any significant difference in probabilities is likely to be more marked for a strongly selecting verb, and likely to be significant over a wider range of  values." ></td>
	<td class="line x" title="187:326	The table only provides anecdotal evidence, but provides some support to this argument." ></td>
	<td class="line x" title="188:326	To investigate more generally how the level of generalization varies with changes in , and also with changes in sample size, we took 6, 000 (c, v, obj) triples and calculated the difference in depth between c and top(c, v, r) for each triple." ></td>
	<td class="line x" title="189:326	The 6, 000 triples were taken from the first experimental test set described in Section 7, and the training data from this experiment were used to provide the counts." ></td>
	<td class="line x" title="190:326	(The test set contains nouns, rather than noun senses, and so the sense of the noun that is most probable given the verb and object slot was used)." ></td>
	<td class="line x" title="191:326	An average difference in depth was then calculated." ></td>
	<td class="line x" title="192:326	To give an example of how the difference in depth was calculated, suppose dog generalized to placental mammal via canine and carnivore; in this case the difference would be three." ></td>
	<td class="line x" title="193:326	The results for various levels of  and different sample sizes are shown in Table 4." ></td>
	<td class="line x" title="194:326	The figures in each column arise from using the contingency tables based on the complete training data, but with each count in the table multiplied by the percentage at the head of the column." ></td>
	<td class="line x" title="195:326	Thus the 50% column is based on contingency tables in which each original count is multiplied by 50%, which is equivalent to using a sample one-half the size of the original training set." ></td>
	<td class="line x" title="196:326	Reading across a row shows how the generalization varies with sample size, and reading down a column shows how it varies with ." ></td>
	<td class="line x" title="197:326	The results show clearly that the extent of generalization decreases with an increase in the value of , supporting the trend observed in Table 3." ></td>
	<td class="line x" title="198:326	The results also show that the extent of generalization increases with a decrease in sample 198 Computational Linguistics Volume 28, Number 2 Table 3 Example levels of generalization for different values of ." ></td>
	<td class="line x" title="199:326	(c, v, r), f(v, r)  (coffee, drink, obj)0.0005 coffeeBEVERAGEfoodobjectentity 0.05 coffeeBEVERAGEfoodobjectentity f(drink, obj)=849 0.5 coffeeBEVERAGEfoodobjectentity 0.995 coffeeBEVERAGEfoodobjectentity (hotdog, eat, obj)0.0005 hotdogsandwichsnack foodDISHfoodentity 0.05 hotdogsandwichsnack foodDISHfoodentity f(eat, obj)=1,703 0.5 hotdogsandwichsnack foodDISHfoodentity 0.995 hotdogSANDWICHsnack fooddishfoodentity (Socrates, kiss, obj)0.0005 Socratespersonlife formCAUSAL AGENTentity 0.05 Socratespersonlife formCAUSAL AGENTentity f(kiss, obj)=345 0.5 Socratespersonlife formCAUSAL AGENTentity 0.995 SocratesPERSONlife formcausal agententity (dream, remember, obj)0.0005 dreampreoccupationcognitive stateSTATE 0.05 dreampreoccupationcognitive stateSTATE f(remember, obj)=1,982 0.5 dreampreoccupationCOGNITIVE STATEstate 0.995 dreamPREOCCUPATIONcognitive statestate (man, see, obj)0.0005 manmammalANIMALlife formentity 0.05 manMAMMALanimallife formentity f(see, obj)=16,757 0.5 manMAMMALanimallife formentity 0.995 MANmammalanimallife formentity (belief, abandon, obj)0.0005 beliefmental objectcognitionPSYCHOLOGICAL FEATURE 0.05 beliefMENTAL OBJECTcognitionpsychological feature f(abandon, obj)=673 0.5 BELIEFmental objectcognitionpsychological feature 0.995 BELIEFmental objectcognitionpsychological feature (nightmare, have, obj)0.0005 nightmaredreamingIMAGINATIONpsychological feature 0.05 nightmaredreamingIMAGINATIONpsychological feature f(have, obj)=93,683 0.5 nightmareDREAMINGimaginationpsychological feature 0.995 nightmareDREAMINGimaginationpsychological feature Note: The selected level is shown in upper case." ></td>
	<td class="line x" title="200:326	Table 4 Extent of generalization for different values of  and sample sizes." ></td>
	<td class="line x" title="201:326	 100% 50% 10% 1% 0.0005 3.33.95.05.6 0.05 2.83.54.65.6 0.52.12.94.15.4 0.995 1.21.52.63.9 size." ></td>
	<td class="line x" title="202:326	Again, this is to be expected, since any difference in probability estimates is less likely to be significant for tables with low counts." ></td>
	<td class="line x" title="203:326	6." ></td>
	<td class="line x" title="204:326	Alternative Class-Based Estimation Methods The approaches used for comparison are that of Resnik (1993, 1998), subsequently developed by Ribas (1995), and that of Li and Abe (1998), which has been adopted by McCarthy (2000)." ></td>
	<td class="line x" title="205:326	These have been chosen because they directly address the question of how to find a suitable level of generalization in WordNet." ></td>
	<td class="line x" title="206:326	199 Clark and Weir Class-Based Probability Estimation The first alternative uses the association score, which is a measure of how well a set of concepts, C, satisfies the selectional preferences of a verb, v, for an argument position, r: 9 A (C, v, r)=p(C | v, r) log 2 p(C | v, r) p(C | r) (18) An estimate of the association score,  A (C, v, r), can be obtained using relative frequency estimates of the probabilities." ></td>
	<td class="line x" title="207:326	The key question is how to determine a suitable level of generalization for concept c, or, alternatively, how to find a suitable class to represent concept c (assuming the choice is from those classes that contain all concepts dominated by some hypernym of c)." ></td>
	<td class="line x" title="208:326	Resniks solution to this problem (which he neatly refers to as the vertical-ambiguity problem) is to choose the class that maximizes the association score." ></td>
	<td class="line x" title="209:326	It is not clear that the class with the highest association score is always the most appropriate level of generalization." ></td>
	<td class="line x" title="210:326	For example, this approach does not always generalize appropriately for arguments that are negatively associated with some verb." ></td>
	<td class="line x" title="211:326	To see why, consider the problem of deciding how well the concept location satisfies the preferences of the verb eat for its object." ></td>
	<td class="line x" title="212:326	Since locations are not the kinds of things that are typically eaten, a suitable level of generalization would correspond to a class that has a low association score with respect to eat." ></td>
	<td class="line x" title="213:326	However, location is a kind of entity in WordNet, 10 and choosing the class with the highest association score is likely to produce entity as the chosen class." ></td>
	<td class="line x" title="214:326	This is a problem, because the association score of entity with respect to eat may be too high to reflect the fact that location is a very unlikely object of the verb." ></td>
	<td class="line x" title="215:326	Note that the solution to the vertical-ambiguity problem presented in the previous sections is able to generalize appropriately in such cases." ></td>
	<td class="line x" title="216:326	Continuing with the eat location example, our generalization procedure is unlikely to get as high as entity (assuming a reasonable number of examples of eat in the training data), since the probabilities corresponding to the daughters of entity are likely to be very different with respect to the object position of eat." ></td>
	<td class="line x" title="217:326	The second alternative uses the minimum description length (MDL) principle." ></td>
	<td class="line x" title="218:326	Li and Abe use MDL to select a set of classes from a hierarchy, together with their associated probabilities, to represent the selectional preferences of a particular verb." ></td>
	<td class="line x" title="219:326	The preferences and class-based probabilities are then used to estimate probabilities of the form p(n | v, r), where n is a noun, v is a verb, and r is an argument slot." ></td>
	<td class="line x" title="220:326	Li and Abes application of MDL requires the hierarchy to be in the form of a thesaurus, in which each leaf node represents a noun and internal nodes represent the class of nouns that the node dominates." ></td>
	<td class="line x" title="221:326	The hierarchy is also assumed to be in the form of a tree." ></td>
	<td class="line x" title="222:326	The class-based models consist of a partition of the set of nouns (leaf nodes) and a probability associated with each class in the partition." ></td>
	<td class="line x" title="223:326	The probabilities are the conditional probabilities of each class, given the relevant verb and argument position." ></td>
	<td class="line x" title="224:326	Li and Abe refer to such a partition as a cut and the cut together with the probabilities as a tree cut model. The probabilities of the classes in a cut, , satisfy the following constraint: summationdisplay C p(C | v, r)=1 (19) 9 The definition used here is that given by Ribas (1995)." ></td>
	<td class="line x" title="225:326	10 For example, the hypernyms of the concept Dallas are as follows: city, municipality, urban area, geographical area, region, location, object, entity." ></td>
	<td class="line x" title="226:326	200 Computational Linguistics Volume 28, Number 2 <abstraction> <life_form> <plant> <object> <entity> <substance> <set> <root> <mushroom> <artifact> <rope> <food> <pizza><lobster> <fluid><solid> <animal> <lobster> <time><space> Figure 3 Possible cut returned by MDL." ></td>
	<td class="line x" title="227:326	In order to determine the probability of a noun, the probability of a class is assumed to be distributed uniformly among the members of that class: p(n | v, r)= 1 |C| p(C | v, r) for all n  C (20) Since WordNet is a hierarchy with noun senses, rather than nouns, at the nodes, Li and Abe deal with the issue of word sense ambiguity using the method described in Section 3, by dividing the count for a noun equally among the concepts whose synsets contain the noun." ></td>
	<td class="line x" title="228:326	Also, since WordNet is a DAG, Li and Abe turn WordNet into a tree by copying each subgraph with multiple parents." ></td>
	<td class="line x" title="229:326	And so that each noun in the data appears (in a synset) at a leaf node, Li and Abe remove those parts of the hierarchy dominated by a noun in the data (but only for that instance of WordNet corresponding to the relevant verb)." ></td>
	<td class="line x" title="230:326	An example cut showing part of the WordNet hierarchy is shown in Figure 3 (based on an example from Li and Abe [1998]; the dashed lines indicate parts of the hierarchy that are not shown in the diagram)." ></td>
	<td class="line x" title="231:326	This is a possible cut for the object position of the verb eat, and the cut consists of the following classes: life form, solid, fluid, food, artifact, space, time, set." ></td>
	<td class="line x" title="232:326	(The particular choice of classes for the cut in this example is not too important; the example is designed to show how probabilities of senses are estimated from class probabilities)." ></td>
	<td class="line x" title="233:326	Since the class in the cut containing pizza is food, the probability p(pizza|eat, obj) would be estimated as p(food|eat, obj)/|food|." ></td>
	<td class="line x" title="234:326	Similarly, since the class in the cut containing mushroom is life form, the probability p(mushroom|eat, obj) would be estimated as p(life form|eat, obj)/|life form|." ></td>
	<td class="line x" title="235:326	The uniform-distribution assumption (20) means that cuts close to the root of the hierarchy result in a greater smoothing of the probability estimates than cuts near the leaves." ></td>
	<td class="line x" title="236:326	Thus there is a trade-off between choosing a model that has a cut near the leaves, which is likely to overfit the data, and a more general (simple) model near the root, which is likely to underfit the data." ></td>
	<td class="line x" title="237:326	MDL looks ideally suited to the task of model selection, since it is designed to deal with precisely this trade-off." ></td>
	<td class="line x" title="238:326	The simplicity of a model is measured using the model description length, which is an information-theoretic 201 Clark and Weir Class-Based Probability Estimation term and denotes the number of bits required to encode the model." ></td>
	<td class="line x" title="239:326	The fit to the data is measured using the data description length, which is the number of bits required to encode the data (relative to the model)." ></td>
	<td class="line x" title="240:326	The overall description length is the sum of the model description length and the data description length, and the MDL principle is to select the model with the shortest description length." ></td>
	<td class="line x" title="241:326	We used McCarthys (2000) implementation of MDL." ></td>
	<td class="line x" title="242:326	So that every noun is represented at a leaf node, McCarthy does not remove parts of the hierarchy, as Li and Abe do, but instead creates new leaf nodes for each synset at an internal node." ></td>
	<td class="line x" title="243:326	McCarthy also does not transform WordNet into a tree, which is strictly required for Li and Abes application of MDL." ></td>
	<td class="line x" title="244:326	This did create a problem with overgeneralization: Many of the cuts returned by MDL were overgeneralizing at the entity node." ></td>
	<td class="line x" title="245:326	The reason is that person, which is close to entity and dominated by entity, has two parents: life form and causal agent." ></td>
	<td class="line x" title="246:326	This DAG-like property was responsible for the overgeneralization, and so we removed the link between person and causal agent." ></td>
	<td class="line x" title="247:326	This appeared to solve the problem, and the results presented later for the average degree of generalization do not show an overgeneralization compared with those given in Li and Abe (1998)." ></td>
	<td class="line x" title="248:326	7." ></td>
	<td class="line x" title="249:326	Pseudo-Disambiguation Experiments The task we used to compare the class-based estimation techniques is a decision task previously used by Pereira, Tishby, and Lee (1993) and Rooth et al.(1999)." ></td>
	<td class="line x" title="251:326	The task is to decide which of two verbs, v and v prime, is more likely to take a given noun, n,asan object." ></td>
	<td class="line x" title="252:326	The test and training data were obtained as follows." ></td>
	<td class="line x" title="253:326	A number of verbdirect object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll." ></td>
	<td class="line x" title="254:326	All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatized." ></td>
	<td class="line x" title="255:326	This resulted in a data set of around 1.3 million (v, n) pairs." ></td>
	<td class="line x" title="256:326	To form a test set, 3,000 of these pairs were randomly selected such that each selected pair contained a fairly frequent verb." ></td>
	<td class="line x" title="257:326	(Following Pereira, Tishby, and Lee, only those verbs that occurred between 500 and 5,000 times in the data were considered.)" ></td>
	<td class="line x" title="258:326	Each instance of a selected pair was then deleted from the data to ensure that the test data were unseen." ></td>
	<td class="line x" title="259:326	The remaining pairs formed the training data." ></td>
	<td class="line x" title="260:326	To complete the test set, a further fairly frequent verb, v prime, was randomly chosen for each (v, n) pair." ></td>
	<td class="line x" title="261:326	The random choice was made according to the verbs frequency in the original data set, subject to the condition that the pair (v prime, n) did not occur in the training data." ></td>
	<td class="line x" title="262:326	Given the set of (v, n, v prime ) triples, the task is to decide whether (v, n) or (v prime, n) is the correct pair." ></td>
	<td class="line x" title="263:326	11 We acknowledge that the task is somewhat artificial, but pseudo-disambiguation tasks of this kind are becoming popular in statistical NLP because of the ease with which training and test data can be created." ></td>
	<td class="line x" title="264:326	We also feel that the pseudo-disambiguation task is useful for evaluating the different estimation methods, since it directly addresses the question of how likely a particular predicate is to take a given noun as an argument." ></td>
	<td class="line x" title="265:326	An evaluation using a PP attachment task was attempted in Clark and Weir (2000), but the evaluation was limited by the relatively small size of the Penn Treebank." ></td>
	<td class="line x" title="266:326	11 We note that this procedure does not guarantee that the correct pair is more likely than the incorrect pair, because of noise in the data from the parser and also because a highly plausible incorrect pair could be generated by chance." ></td>
	<td class="line x" title="267:326	202 Computational Linguistics Volume 28, Number 2 Table 5 Results for the pseudo-disambiguation task." ></td>
	<td class="line x" title="268:326	Generalization technique % correct av.gen. sd.gen. Similarity class  = 0.0005 73.83.32.0  = 0.05 73.42.81.9  = 0.373.02.41.8  = 0.75 73.91.91.6  = 0.995 73.81.21.2 Low class 73.60.91.0 MDL 68.34.11.9 Assoc 63.94.22.1 Note: av.gen. is the average number of generalized levels; sd.gen. is the standard deviation." ></td>
	<td class="line x" title="269:326	Using our approach, the disambiguation decision for each (v, n, v prime ) triple was made according to the following procedure: if max ccn(n) p sc (c | v, obj) > max ccn(n) p sc (c | v prime, obj) then choose (v, n) else if max ccn(n) p sc (c | v prime, obj) > max ccn(n) p sc (c | v, obj) then choose (v prime, n) else choose at random If n has more than one sense, the sense is chosen that maximizes the relevant probability estimate; this explains the maximization over cn(n)." ></td>
	<td class="line x" title="270:326	The probability estimates were obtained using our class-based method, and the G 2 statistic was used for the chi-square test." ></td>
	<td class="line x" title="271:326	This procedure was also used for the MDL alternative, but using the MDL method to estimate the probabilities." ></td>
	<td class="line x" title="272:326	Using the association score for each test triple, the decision was made according to the following procedure: if max ccn(n) max c prime h(c)  A (c prime, v, obj) > max ccn(n) max c prime h(c)  A (c prime, v prime, obj) then choose (v, n) else if max ccn(n) max c prime h(c)  A (c prime, v prime, obj) > max ccn(n) max c prime h(c)  A (c prime, v, obj) then choose (v prime, n) else choose at random We use h(c) to denote the set consisting of the hypernyms of c. The inner maximization is over h(c), assuming c is the chosen sense of n, which corresponds to Resniks method of choosing a set to represent c. The outer maximization is over the senses of n, cn(n), which determines the sense of n by choosing the sense that maximizes the association score." ></td>
	<td class="line x" title="273:326	The first set of results is given in Table 5." ></td>
	<td class="line x" title="274:326	Our technique is referred to as the similarity class technique, and the approach using the association score is referred 203 Clark and Weir Class-Based Probability Estimation Table 6 Results for the pseudo-disambiguation task with one-fifth training data." ></td>
	<td class="line x" title="275:326	Generalization technique % correct av.gen. sd.gen. Similarity class  = 0.0005 66.74.51.9  = 0.05 68.44.11.9  = 0.370.23.71.9  = 0.75 72.33.01.9  = 0.995 72.41.91.6 Low class 71.91.11.1 MDL 62.94.71.9 Assoc 62.64.12.0 Note: av.gen. is the average number of generalized levels; sd.gen. is the standard deviation." ></td>
	<td class="line x" title="276:326	to as Assoc. The results are given for a range of  values and demonstrate clearly that the performance of similarity class varies little with changes in  and that similarity class outperforms both MDL and Assoc." ></td>
	<td class="line x" title="277:326	12 We also give a score for our approach using a simple generalization procedure, which we call low class. The procedure is to select the first class that has a count greater than zero (relative to the verb and argument position), which is likely to return a low level of generalization, on the whole." ></td>
	<td class="line x" title="278:326	The results show that our generalization technique only narrowly outperforms the simple alternative." ></td>
	<td class="line x" title="279:326	Note that, although low class is based on a very simple generalization method, the estimation method is still using our class-based technique, by applying Bayes theorem and conditioning on a class, as described in Section 3; the difference is in how the class is chosen." ></td>
	<td class="line x" title="280:326	To investigate the results, we calculated the average number of generalized levels for each approach." ></td>
	<td class="line x" title="281:326	The number of generalized levels for a concept c (relative to a verb v and argument position r) is the difference in depth between c and top(c, v, r), as explained in Section 5." ></td>
	<td class="line x" title="282:326	For each test case, the number of generalized levels for both verbs, v and v prime, was calculated, but only for the chosen sense of n. The results are given in the third column of Table 5 and demonstrate clearly that both MDL and Assoc are generalizing to a greater extent than similarity class." ></td>
	<td class="line x" title="283:326	(The fourth column gives a standard deviation figure)." ></td>
	<td class="line x" title="284:326	These results suggest that MDL and Assoc are overgeneralizing, at least for the purposes of this task." ></td>
	<td class="line x" title="285:326	To investigate why the value for  had no impact on the results, we repeated the experiment, but with one fifth of the data." ></td>
	<td class="line x" title="286:326	A new data set was created by taking every fifth pair of the original 1.3 million pairs." ></td>
	<td class="line x" title="287:326	A test set of 3,000 triples was created from this new data set, as before, but this time only verbs that occurred between 100 and 1,000 times were considered." ></td>
	<td class="line x" title="288:326	The results using these test and training data are given in Table 6." ></td>
	<td class="line x" title="289:326	These results show a variation in performance across values for , with an optimal performance when  is around 0.75." ></td>
	<td class="line x" title="290:326	(Of course, in practice, the value for  would need to be optimized on a held-out set)." ></td>
	<td class="line x" title="291:326	But even with this variation, similarity class is still outperforming MDL and Assoc across the whole range of  values." ></td>
	<td class="line x" title="292:326	Note that the 12 The results given for similarity class are different from those given in Clark and Weir (2001) because the probability estimates used in Clark and Weir (2001) were not normalized." ></td>
	<td class="line x" title="293:326	204 Computational Linguistics Volume 28, Number 2 Table 7 Disambiguation results for G 2 and X 2 .  value % correct (G 2 ) % correct (X 2 ) 0.0005 73.8 (3.3) 74.1 (3.0) 0.05 73.4 (2.8) 73.8 (2.5) 0.373.0 (2.4) 74.1 (2.2) 0.75 73.9 (1.9) 74.3 (1.8) 0.995 73.8 (1.2) 73.3 (1.2)  values corresponding to the lowest scores lead to a significant amount of generalization, which provides additional evidence that MDL and Assoc are overgeneralizing for this task." ></td>
	<td class="line x" title="294:326	The low-class method scores highly for this data set also, but given that the task is one that apparently favors a low level of generalization, the high score is not too surprising." ></td>
	<td class="line x" title="295:326	As a final experiment, we compared the task performance using the X 2, rather than G 2, statistic in the chi-square test." ></td>
	<td class="line x" title="296:326	The results are given in Table 7 for the complete data set." ></td>
	<td class="line x" title="297:326	13 The figures in brackets give the average number of generalized levels." ></td>
	<td class="line x" title="298:326	The X 2 statistic is performing at least as well as G 2, and the results show that the average level of generalization is slightly higher for G 2 than X 2 . This suggests a possible explanation for the results presented here and those in Dunning (1993): that the X 2 statistic provides a less conservative test when counts in the contingency table are low." ></td>
	<td class="line x" title="299:326	(By a conservative test we mean one in which the null hypothesis is not easily rejected)." ></td>
	<td class="line x" title="300:326	A less conservative test is better suited to the pseudo-disambiguation task, since it results in a lower level of generalization, on the whole, which is good for this task." ></td>
	<td class="line x" title="301:326	In contrast, the task that Dunning considers, the discovery of bigrams, is better served by a more conservative test." ></td>
	<td class="line x" title="302:326	8." ></td>
	<td class="line x" title="303:326	Conclusion We have presented a class-based estimation method that incorporates a procedure for finding a suitable level of generalization in WordNet." ></td>
	<td class="line x" title="304:326	This method has been shown to provide superior performance on a pseudo-disambiguation task, compared with two alternative approaches." ></td>
	<td class="line x" title="305:326	An analysis of the results has shown that the other approaches appear to be overgeneralizing, at least for this task." ></td>
	<td class="line x" title="306:326	One of the features of the generalization procedure is the way that , the level of significance in the chi-square test, is treated as a parameter." ></td>
	<td class="line x" title="307:326	This allows some control over the extent of generalization, which can be tailored to particular tasks." ></td>
	<td class="line x" title="308:326	We have also shown that the task performance is at least as good when using the Pearson chi-square statistic as when using the log-likelihood chi-square statistic." ></td>
	<td class="line x" title="309:326	There are a number of ways in which this work could be extended." ></td>
	<td class="line x" title="310:326	One possibility would be to use all the classes dominated by the hypernyms of a concept, rather than just one, to estimate the probability of the concept." ></td>
	<td class="line x" title="311:326	An estimate would be obtained for each hypernym, and the estimates combined in a linear interpolation." ></td>
	<td class="line x" title="312:326	An approach similar to this is taken by Bikel (2000), in the context of statistical parsing." ></td>
	<td class="line x" title="313:326	There is still room for investigation of the hidden-data problem when data are used that have not been sense disambiguated." ></td>
	<td class="line x" title="314:326	In this article, a very simple approach is taken, 13  2 performed slightly better than G 2 using the smaller data set also." ></td>
	<td class="line x" title="315:326	205 Clark and Weir Class-Based Probability Estimation which is to split the count for a noun evenly among the nouns senses." ></td>
	<td class="line x" title="316:326	Abney and Light (1999) have tried a more motivated approach, using the expectation maximization algorithm, but with little success." ></td>
	<td class="line x" title="317:326	The approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the  parameter, and ultimately does not improve on the best performance." ></td>
	<td class="line x" title="318:326	Finally, an issue that has not been much addressed in the literature (except by Li and Abe [1996]) is how the accuracy of class-based estimation techniques compare when automatically acquired classes, as opposed to the manually created classes from WordNet, are used." ></td>
	<td class="line x" title="319:326	The pseudo-disambiguation task described here has also been used to evaluate clustering algorithms (Pereira, Tishby, and Lee, 1993; Rooth et al. , 1999), but with different data, and so it is difficult to compare the results." ></td>
	<td class="line x" title="320:326	A related issue is how the structure of WordNet affects the accuracy of the probability estimates." ></td>
	<td class="line x" title="321:326	We have taken the structure of the hierarchy for granted, without any analysis, but it may be that an alternative design could be more conducive to probability estimation." ></td>
	<td class="line x" title="322:326	Acknowledgments This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001." ></td>
	<td class="line x" title="323:326	The work on which it is based was carried out while the first author was a D.Phil." ></td>
	<td class="line x" title="324:326	student at the University of Sussex and was supported by an EPSRC studentship." ></td>
	<td class="line x" title="325:326	We would like to thank Diana McCarthy for suggesting the pseudo-disambiguation task and providing the MDL software, John Carroll for supplying the data, and Ted Briscoe, Geoff Sampson, Gerald Gazdar, Bill Keller, Ted Pedersen, and the anonymous reviewers for their helpful comments." ></td>
	<td class="line x" title="326:326	We would also like to thank Ted Briscoe for presenting an earlier version of this article on our behalf at NAACL 2001." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W02-1115
Selecting The Most Highly Correlated Pairs Within A Large Vocabulary
Umemura, Kyoji;"></td>
	<td class="line x" title="1:111	Selecting the Most Highly Correlated Pairs within a Large Vocabulary Kyoji Umemura Department of Computer Science Toyoahshi University of Technology umemura@tutics.tut.ac.jp Abstract Occurence patterns of words in documents can be expressed as binary vectors." ></td>
	<td class="line x" title="2:111	When two vectors are similar, the two words corresponding to the vectors may have some implicit relationship with each other." ></td>
	<td class="line x" title="3:111	We call these two words a correlated pair." ></td>
	<td class="line x" title="4:111	This report describes a method for obtaining the most highly correlated pairs of a given size." ></td>
	<td class="line x" title="5:111	In practice, the method requires a0a2a1a4a3a6a5a8a7a10a9a12a11a13a1a4a3a15a14a16a14 computation time, and a0a2a1a4a3a17a14 memory space, where a3 is the number of documents or records." ></td>
	<td class="line x" title="6:111	Since this does not depend on the size of the vocabulary under analysis, it is possible to compute correlations between all the words in a corpus." ></td>
	<td class="line x" title="7:111	1 Introduction In order to find relationships between words in a large corpus or between labels in a large database, we may use a distance measure between the binary vectors of a3 dimensions, where a3 is the number of documents or records, and the a18 th element is 1 if the a18 th document/record contains the word or the label, or 0 otherwise." ></td>
	<td class="line oc" title="8:111	There are several distance measures suitable for this purpose, such as the mutual information(Church and Hanks, 1990), the dice coefficient(Manning and Schueutze 8.5, 1999), the phi coefficient(Manning and Schuetze 5.3.3, 1999), the cosine measure(Manning and Schueutze 8.5, 1999) and the confidence(Arrawal and Srikant, 1995)." ></td>
	<td class="line x" title="9:111	There are also special functions for certain applications, such as then complimentary similarity measure (CSM)(Hagita and Sawaki, 1995) which is known as to be suitable for cases with a noisy pattern." ></td>
	<td class="line x" title="10:111	All of these five measures can be obtained from a simple contingency table." ></td>
	<td class="line x" title="11:111	This table has four numbers for each word/label a19 and word/label a20." ></td>
	<td class="line x" title="12:111	The first number is the number of documents/records that have both a19 and a20 . We define this number as a21a23a22a25a24 a1 a19a27a26a16a20 a14 . The second number is the number of documents/records that have a19 but not a20 . We define this number as a21a23a22a12a28 a1 a19a27a26a16a20 a14 . The third number is the number of documents/records that do not have a19 but do have a20 . We define this number as a21a29a22a31a30 a1 a19a27a26a16a20 a14 . The fourth and the last number is the number of documents/records that have neither a19 nor a20 . We define this number as a21a29a22a12a32 a1 a19a27a26a16a20 a14 . An obvious method to obtain the most highly related pairs is to calculate a21a23a22a33a24, a21a29a22a25a28, a21a23a22a25a30, a21a23a22a12a32 for all pairs of words/labels, compute the similarity for all pairs and then select pairs of the highest values." ></td>
	<td class="line x" title="13:111	Let a34 be the number of possible words/labels, and a3 be the total number of documents/records in a corpus/database." ></td>
	<td class="line x" title="14:111	This method requires a0a2a1 a34a23a35 a14 memory space and a0a8a1 a34 a35a36a5a17a3a17a14 computation time." ></td>
	<td class="line x" title="15:111	However, its use is only feasible if a34 is smaller than a37a39a38a33a40 . When a34 is larger than ten thousand, execution of this procedure becomes difficult." ></td>
	<td class="line x" title="16:111	The method described here is based on the observation that there is an upper boundary to the number of different words in one document." ></td>
	<td class="line x" title="17:111	The assumption of such a boundary could even made of a large scale corpus." ></td>
	<td class="line x" title="18:111	For example, a collective corpus of a newspaper may become larger and larger, but the length of each article is stable." ></td>
	<td class="line x" title="19:111	It is not likely that one article would contain thousands of different words." ></td>
	<td class="line x" title="20:111	In view of this observation and the assumption, this method is effective for obtaining the most highly correlated pairs in a large corpus, and uses a0a2a1a4a3a17a14 memory space, and a0a2a1a4a3 a5 a7a10a9a12a11 a1a4a3a17a14a16a14 computation time." ></td>
	<td class="line x" title="21:111	2 Notations Several notations are introduced in this section to describe the method." ></td>
	<td class="line x" title="22:111	Assuming a corpus C, which is a set of sets of words, values are assigned as follows." ></td>
	<td class="line x" title="23:111	a1 a21 : documents (elements of the corpus)." ></td>
	<td class="line x" title="24:111	a21a3a2a5a4 a1 a19, a20, a6 : label (elements of a document)." ></td>
	<td class="line x" title="25:111	a19 a2 a21 a26a16a20 a2 a21 a26a7a6 a2 a21 a1 a19a9a8 a20 : a20 is placed after a19 in the alphabetical order." ></td>
	<td class="line x" title="26:111	a1 a3 : the total number of documents." ></td>
	<td class="line x" title="27:111	a3a11a10a13a12 a4 a12 a1 a21a29a22 a1 a19 a14 : the number of documents that contain a19 . a21a29a22 a1 a19 a14a14a10a13a12a16a15 a21 a12 a19 a2 a21a18a17 a12 a1 a21a29a22a25a24 a1 a19a27a26a16a20 a14 : the number of documents that contain a19 and contain a20 . a21a29a22a25a24 a1 a19 a26a16a20 a14a19a10a20a12a21a15 a21 a12 a19 a2 a21a23a22 a20 a2 a21a24a17 a12 a1 a21a29a22a25a28 a1 a19a27a26a16a20 a14 : the number of documents that contain a19 but not a20 . a21a29a22a25a28 a1 a19 a26a16a20 a14a25a10a13a12a21a15 a21 a12 a19 a2 a21a26a22 a20a28a27 a2 a21a18a17 a12 a1 a21a29a22a25a30 a1 a19a27a26a16a20 a14 : the number of documents that contains a20 but not a19 . a21a29a22a25a30 a1 a19 a26a16a20 a14a25a10a13a12a21a15 a21 a12 a19a29a27 a2 a21a23a22 a20 a2 a21a18a17 a12 a1 a21a29a22a25a32 a1 a19a27a26a16a20 a14 : the number of documents that conetain neither a19 nor a20 . a21a23a22a25a32 a1 a19a27a26a16a20 a14a19a10a13a12a21a15 a21 a12 a19a29a27 a2 a21a23a22 a20a30a27 a2 a21a24a17 a12 3 Problem Definition When the corpus of a set of sets of labels is provided, and the function a31a33a32 a34 a1 a19a27a26a16a20 a14 of a pair of labels to the number in the following form is also provided, we will obtain a34 : the set of pairs of a given size that satisfies the following condition." ></td>
	<td class="line o" title="28:111	a35 a1 a19 a35 a26a16a20 a35 a14 a27 a2 a34a37a36 a35 a1 a19a39a38 a26a16a20a40a38 a14 a2 a34a37a36a41a31 a1 a19a39a38a39a26a16a20a40a38 a14a43a42 a31 a1 a19 a35 a26a16a20 a35 a14 where a31 a1 a19a27a26a16a20 a14 a10 a22 a1 a21a23a22 a24 a1 a19a27a26a16a20 a14 a26 a21a23a22a25a28 a1 a19a27a26a16a20 a14 a26 a21a23a22 a30 a1 a19a27a26a16a20 a14 a26 a21a23a22a25a32 a1 a19 a26a16a20 a14a16a14 The following are examples of a22 a1a45a44 a26a47a46 a26a7a48 a26 a21 a14 . a1 cosine function a44 a49 a1a45a44a23a50 a46 a14 a5 a1a45a44a23a50 a48 a14 a1 dice coefficient a51 a5a52a44 a1a45a44a53a50 a46 a14a54a50 a1a45a44a53a50 a48 a14 a1 confidence a44 a44a26a50 a48 a1 pairwise mutual information a44 a3 a5 a7 a9a12a11 a44 a5 a3 a1a45a44a26a50 a46 a14 a5 a1a45a44a26a50 a48 a14 a1 phi coefficient a44 a5 a21a56a55 a46 a5 a48 a49 a1a45a44a26a50 a46 a14 a5 a1a45a44a26a50 a48 a14 a5 a1 a46 a50 a21 a14 a5 a1 a48 a50 a21 a14 a1 complementary similarity measure a44 a5 a21a56a55 a46 a5 a48 a49 a1a45a44a26a50 a48 a14 a5 a1 a46 a50 a21 a14a16a14 Implementation of a program that requires a0 a5 a34a23a35 memory space and a0 a5 a34 a35 a5 a3 computation time is easy." ></td>
	<td class="line x" title="29:111	A program of this type could be used to calculate a21a23a22a12a24, a21a23a22a25a28, a21a23a22a25a30, and a21a23a22a25a32 for all pairs of a19 and a20, and could then provide the most highly correlated pairs." ></td>
	<td class="line x" title="30:111	However, compuation with this method is not feasible when a34 is large." ></td>
	<td class="line x" title="31:111	For example, in order to calculate the most highly correlated words within a newspaper over several years of publication, a34 becomes roughly a37a39a38a2a1, and a3 becomes a37a39a38a4a3 . The amount of computation time is then increased to a37a39a38 a38 a3 . 4 Approach In terms of the actual data, the number of correlated pairs is usually much smaller than the number of uncorrelated pairs." ></td>
	<td class="line x" title="32:111	Moreover, most of the uncorrelated pairs usually satisfy the condition: a21a23a22a33a24 a1 a19a27a26a16a20 a14 a10 a38, and are not of interest." ></td>
	<td class="line x" title="33:111	This method takes this fact into account." ></td>
	<td class="line x" title="34:111	Moreover, it also uses the relationship between a15 a3 a26 a21a29a22a25a24a21a17 and a15 a21a23a22 a26 a21a23a22a12a28 a26 a21a23a22a25a30 a26 a21a23a22a25a32 a17 to make the computation feasible." ></td>
	<td class="line x" title="35:111	5 Relationship between a5a7a6a9a8a11a10a13a12 a24a15a14 and a5a7a10a13a12a16a8a17a10a13a12 a28 a8a11a10a13a12 a30 a8a11a10a13a12 a32 a14 Proofs of the following equations are provided below." ></td>
	<td class="line x" title="36:111	a21a23a22 a1 a19 a14 a10 a21a23a22a25a24 a1 a19a27a26a16a19 a14 a21a29a22a25a28 a1 a19 a26a16a20 a14 a10 a21a23a22a25a24 a1 a19a27a26a16a19 a14 a55 a21a23a22a25a24 a1 a19a27a26a16a20 a14 a21a29a22 a30 a1 a19 a26a16a20 a14 a10 a21a23a22 a24 a1 a20 a26a16a20 a14 a55 a21a29a22 a24a33a1 a19a27a26a16a20 a14 a21a23a22a25a32 a1 a19 a26a16a20 a14 a10 a3 a55 a21a23a22a25a24 a1 a19 a26a16a19 a14 a55 a21a23a22a25a24 a1 a20 a26a16a20 a14a39a50 a21a23a22a25a24 a1 a19a27a26a16a20 a14 Proof: 1." ></td>
	<td class="line x" title="37:111	a34 a22 a34 is equivalent to a34 . a21a29a22a25a24 a1 a19a27a26a16a19 a14 a10 a12a15 a21 a12 a19 a2 a21 a22 a19 a2 a21a18a17 a12 a10 a12a15 a21 a12 a19 a2 a21a18a17 a12 a10 a21a29a22 a1 a19 a14 2." ></td>
	<td class="line x" title="38:111	By definition, the sum of a21a29a22a31a24, a21a23a22a25a28, a21a23a22a25a30, and a21a29a22a25a32 always represents the total number of documents." ></td>
	<td class="line x" title="39:111	a21a23a22a25a24 a1 a19 a26a16a20 a14 a50 a21a23a22a25a28 a1 a19a27a26a16a20 a14 a50 a21a23a22a25a30 a1 a19a27a26a16a20 a14a39a50 a21a29a22a25a32 a1 a19a27a26a16a20 a14 a10 a3 3." ></td>
	<td class="line x" title="40:111	Similarly, the sum of a21a23a22a31a24 a1 a19a27a26a16a20 a14 and a21a23a22a25a28 a1 a19a27a26a16a20 a14 is the number of documents that contain a19 . This equals a21a23a22 a1 a19 a14 . a21a23a22a25a24 a1 a19 a26a16a20 a14 a50 a21a23a22a25a28 a1 a19a27a26a16a20 a14 a10 a12a15 a21 a12 a19 a2 a21a23a22 a20 a2 a21a24a17 a12 a50a13a12a15 a21 a12 a19 a2 a21a26a22 a20a30a27 a2 a21a24a17 a12 a10 a12a15 a21 a12 a19 a2 a21a18a17 a12 a10 a21a23a22 a1 a19 a14 4." ></td>
	<td class="line x" title="41:111	Similarly, the sum of a21a23a22a31a24 a1 a19a27a26a16a20 a14 and a21a23a22a25a30 a1 a19a27a26a16a20 a14 is the number of documents that contain a20 . This equals a21a23a22 a1 a20 a14 . a22a25a24 a1 a19a27a26a16a20 a14a39a50 a21a29a22a25a30 a1 a19a27a26a16a20 a14 a10 a12a15 a21 a12 a19 a2 a21a23a22 a20 a2 a21a24a17 a12 a50a13a12a15 a21 a12 a19 a27 a2 a21a26a22 a20 a2 a21a24a17 a12 a10 a12a15 a21 a12 a20 a2 a21a18a17 a12 a10 a21a23a22 a1 a20 a14 5." ></td>
	<td class="line x" title="42:111	These four equations make it possible to express a21a23a22, a21a23a22a12a28, a21a23a22a25a30 and a21a23a22a12a32 by a21a23a22 a24 and a3 . These formulas indicate that the number of required two-dimensional tables is not four, but just one." ></td>
	<td class="line x" title="43:111	In other words, if we create a table of a21a29a22 a24 a1 a19a27a26a16a20 a14 and one variable for a3, we can obtain a21a29a22 a1 a19 a14, a21a23a22a25a28 a1 a19 a26a16a20 a14, a21a29a22a25a30 a1 a19a27a26a16a20 a14, and a21a29a22a25a32 a1 a19 a26a16a20 a14 . 6 The memory requirement for a10a13a12 a24 Let a18 be the maximum number of different words/labels in one document." ></td>
	<td class="line x" title="44:111	The following property exists in a21a23a22a31a24 a1 a19a27a26a16a20 a14 . a19a21a20a22a19a24a23 a21a23a22a25a24 a1 a19 a26a16a20 a14a26a25 a18 a35 a5 a3 The left side of the formula equals the total number of all pairs of words/labels." ></td>
	<td class="line x" title="45:111	This cannot exceed a18 a35 a5 a3 . This relationship indicates that if the table is stored using tuples of a1 a19a27a26a16a20 a26 a21a23a22a31a24 a1 a19a27a26a16a20 a14a16a14 where a21a23a22 a24 a1 a19a27a26a16a20 a14a1a0 a38, the required memory space is a0a2a1a4a3a17a14 . Tuples where a21a23a22a31a24 a1 a19a27a26a16a20 a14 a10 a38 are not necessary because we know that a21a23a22a31a24 a1 a19 a26a16a20 a14 a10 a38 when the tuple for a1 a19 a26a16a20 a26 a21a29a22a25a24 a1 a19a27a26a16a20 a14a16a14 does not exist in memory." ></td>
	<td class="line x" title="46:111	This estimation is pessimistic." ></td>
	<td class="line x" title="47:111	The actual size of the tuples will be smaller than a18 a35 a5 a3, since not all documents will have a18 different words/labels." ></td>
	<td class="line x" title="48:111	7 Obtaining a10a13a12 a24, and a6 The algorithm to obtain a21a23a22a33a24 a1 a19a27a26a16a20 a14, and a3 is straightforward." ></td>
	<td class="line x" title="49:111	First, the corpus must be trasformed into a set of sets of words/labels." ></td>
	<td class="line x" title="50:111	Since this is a set form, there are no duplications of the words/labels of one document." ></td>
	<td class="line x" title="51:111	In the following program, the hashtable returns 0 for a non-existent item." ></td>
	<td class="line x" title="52:111	(01) Let DFA be empty hashtable." ></td>
	<td class="line x" title="53:111	(02) Let DF be empty hashtable (03) Let N be 0 (04) For each document, assign it to D (05) | N = N + 1 (06) | For each word in D (07) | assign the word to X (08) | | For each word in D (09) | | assign the word to Y (10) | | | DFA(X, Y)=DFA(X,Y)+1 (11) | | end of loop (12) | end of loop (13) end of loop The computation time for this program is less than a18 a35 a5 a3 . Since a18 is independent from a3, the computation time is a0a2a1a4a3a17a14 . Again, a18 a35 a5 a3 is a pessimistic estimation, since not all documents will have a18 different words/labels." ></td>
	<td class="line x" title="54:111	8 Selecting Pairs Even though a21a23a22 a24, a21a23a22a12a28, a21a23a22 a30, and a21a29a22 a30 can be obtained in constant time after a0a2a1a4a3a17a14 preprocessing, there are a34 a35 values to consider to obtain the best N correlated pairs." ></td>
	<td class="line x" title="55:111	Fortunately, many of the functions that are usable as indicators of correlation and, at least, all five functions, return a lower value than the known threshold if a21a23a22a12a24 a1 a19a27a26a16a20 a14a19a10 a38 . The cosine measure, the dice coefficient, and pairwise mutual information have property 1 and property 2 as defined below." ></td>
	<td class="line x" title="56:111	This implies that the value for a1 a19a27a26a16a20 a14 where a21a23a22a12a24 a1 a19 a26a16a20 a14 a10 a38 is actually the minimum value of all a1 a19a27a26a16a20 a14 . Therefore, the first part of the total ordered sequence of a1 a19a27a26a16a20 a14 is the sorted list of a1 a19a27a26a16a20 a14 where a21a23a22 a1 a19 a26a16a20 a14a2a0 a38 . The rest is an arbitary order of pairs where a21a23a22 a1 a19a27a26a16a20 a14a19a10 a38 . Property 1: the value is not negative." ></td>
	<td class="line x" title="57:111	Property 2: when a21a23a22a31a24 a1 a19a27a26a16a20 a14a19a10 a38, the value is a38 . The phi coefficient and the complementary similarity measure have the following properties 1, 2 and 3." ></td>
	<td class="line x" title="58:111	Therefore, the first part of the total ordered sequence where the value is positive, is equal to first part of the sorted list where a21a23a22 a1 a19a27a26a16a20 a14a3a0 a38 and the value is positive." ></td>
	<td class="line x" title="59:111	Moreover, this list contains all pairs that have a positive correlation." ></td>
	<td class="line x" title="60:111	This list is long enough for the actual application." ></td>
	<td class="line x" title="61:111	Property 1: when a21a23a22 a24a33a1 a19a27a26a16a20 a14a43a10 a38, the value is negative." ></td>
	<td class="line x" title="62:111	Property 2: when a19 and a20 are not correlated, the estimated value is a38 . Property 3: when a19 and a20 tend to appear at the same time, the estimated value is positive." ></td>
	<td class="line x" title="63:111	It should be recalled that the number of pairs where a21a29a22 a24a33a1 a19 a26a16a20 a14a4a0 a38 is less than a18 a35 a5a2a3 . The sorted list is obtained in a0a2a1 a18 a35 a5 a3 a5 a7 a9a12a11 a1 a18 a35 a5 a3a15a14a16a14 computation time, where a18 is the maximum number of different words/labels in one document." ></td>
	<td class="line x" title="64:111	Since a18 is constant, it becomes a0a8a1a4a3 a5 a7a10a9a12a11 a1a4a3a17a14a16a14, even if the size ofvocabulary is very large." ></td>
	<td class="line x" title="65:111	It is true that for the given some fixed vocabulary of size a34, a18 a35 a5 a3 might be larger than a34 a35 as we increase the size of corpus." ></td>
	<td class="line x" title="66:111	Fortunately, the actual memory consumption of this procedure also have the upper bound of a0a8a1 a34 a35 a14, and we will not loose any memory space." ></td>
	<td class="line x" title="67:111	When a34 is not fixed and a34 may become very large compare to a3 as is the case for proper nouns, a18 a35 a5 a3 is smaller than a34 a35 . 9 Case study of a Newspaper Corpus The computation time of the baseline system is a34 a35 a5 a3 where a34 is the distinct number of labels in the a3 time(sec)." ></td>
	<td class="line x" title="68:111	speed(sec./doc) 1000 2.4 a51 a0 a0 a5 a37a39a38 a1 a2 3000 7.8 a51 a0a4a3 a5 a37a39a38 a1 a2 10000 21.1 a51 a0 a37 a5 a37a39a38 a1 a2 30000 60.9 a51 a0 a38 a5 a37a39a38 a1 a2 Table 1: The actual execution time shows a linear relationship to the size of input data." ></td>
	<td class="line x" title="69:111	corpus." ></td>
	<td class="line x" title="70:111	When we analyzed labels of names of places in a newspaper over the course of one year, this corpus consisted of about 60,000 documents." ></td>
	<td class="line x" title="71:111	The place names totalled 1902 after morphological analysis." ></td>
	<td class="line x" title="72:111	The maximum number of names in one document was 142, and the average in one document was 4.02." ></td>
	<td class="line x" title="73:111	In this case, the method described here, was much more efficient than the baseline system." ></td>
	<td class="line x" title="74:111	Table 1 shows the actual execution time of the program in the appendix, changing the length of the corpus." ></td>
	<td class="line x" title="75:111	This program computes similarity values for all pairs of words where a21a23a22a31a24 a0 a38 . It indicates that the execution time is linear." ></td>
	<td class="line x" title="76:111	Our observation shows that even if the corpus were extended year by year, a18 which is the maximum number of different words in one document is stable, even though the total number of words would increase with the ongoing addition of proper nouns and new concepts." ></td>
	<td class="line x" title="77:111	10 For a large corpus Although the program in the appendix cannot be applied to a corpus larger than memory size, we can obtain a table of a21a29a22a12a24 using sequential access to file." ></td>
	<td class="line x" title="78:111	The program in the appendix stores every pair in memory." ></td>
	<td class="line x" title="79:111	The space requirement of a18 a35 a5 a3 may seem too great to hold in memory." ></td>
	<td class="line x" title="80:111	However, sequential file can be used to obtain the a21a23a22 a24 table, as follows." ></td>
	<td class="line x" title="81:111	Although the computation time for a21a23a22 a24 is a0a2a1a4a3 a5 a7a10a9a12a11 a1a4a3a17a14a16a14 rather than a0a2a1a4a3a17a14, the total computation time remains the same because computation of a0a8a1a4a3 a5 a7a10a9a12a11 a1a4a3a17a14a16a14 is required to select pairs in both cases." ></td>
	<td class="line x" title="82:111	Consider the following data." ></td>
	<td class="line x" title="83:111	Each line corresponds to one document." ></td>
	<td class="line x" title="84:111	a b a c x y x x y z a b c When the pairs of words in each document are recorded, the following file is obtained." ></td>
	<td class="line x" title="85:111	Note that since a21a23a22 a24a33a1 a19a27a26a16a20 a14a5a10 a21a23a22 a24 a1 a20 a26a16a19 a14, it is not necessary to record pairs where a19 a0 a20 . This reduces the memory requirement." ></td>
	<td class="line x" title="86:111	a a a b b b a a a c c c x x x y y y x x x x x y x z y y y z z z a a a b a c b b b c c c Using the merge sort algorithm which can sort a large file using sequential access only, the file can be sorted in a0a8a1a4a3 a5 a7a10a9a12a11a13a1a4a3a15a14a16a14 computation time." ></td>
	<td class="line x" title="87:111	After sorting in alphabetical order, same pairs come together." ></td>
	<td class="line x" title="88:111	Then, the pairs can be counted with sequential access, thereby providing the a21a29a22a33a24 table." ></td>
	<td class="line x" title="89:111	An example of this table fllows: a a 3 a b 2 a c 2 b b 2 b c 1 c c 2 x x 3 x y 2 x z 1 y y 2 y z 1 z z 1 It should be noted that the a21a23a22 table can be obtained easily by extracting lines in which letter of the first column and that of the second column are the same, since a21a29a22 a1 a19 a14a19a10 a21a23a22a12a24 a1 a19a27a26a16a19 a14 . The a21a29a22 table can usually be stored in memory since it is a one dimensional array." ></td>
	<td class="line x" title="90:111	After storing a21a29a22 in memory, similarity can be computed line by line." ></td>
	<td class="line x" title="91:111	The following example uses the phi coefficient." ></td>
	<td class="line x" title="92:111	The first column is the coefficient, followed by a21a23a22a12a24, a21a29a22a25a28, a21a23a22a25a30, a21a23a22a25a32, a19 and a20 . Since the phi coefficient is reflective, the a1 a19 a26a16a20 a14 value where a19 a0 a20 is not required." ></td>
	<td class="line x" title="93:111	When the function is not symmetric, a1 a19a27a26a16a20 a14 and a1 a21a23a22a25a28 a26 a21a29a22 a30 a14 can be exchanged at the same time." ></td>
	<td class="line x" title="94:111	0.544705 3 0 0 3 a a 0.384900 2 1 0 3 a b 0.384900 2 1 0 3 a c 0.624695 2 0 0 4 b b 0.156174 1 1 1 3 b c 0.624695 2 0 0 4 c c 0.544705 3 0 0 3 x x 0.384900 2 1 0 3 x y 0.242536 1 2 0 3 x z 0.624695 2 0 0 4 y y 0.392232 1 1 0 4 y z 0.674200 1 0 0 5 z z The ordered list can be obtained by sorting this table with the first column." ></td>
	<td class="line x" title="95:111	This example shows that pairs where a21a23a22a12a24 a1 a19a27a26a16a20 a14 a10 a38, such as a1a1a0 a26a3a2 a14 or a1a1a0 a26a5a4 a14, do not add any overhead to either memory or computation time." ></td>
	<td class="line x" title="96:111	11 Comparison with Apriori There is a well known algorithm for forming a list of related items termed Apriori(Arrawal and Srikant, 1995)." ></td>
	<td class="line x" title="97:111	Apriori lists all relationship using confidence, where a21a23a22a12a24 a1 a19a27a26a16a20 a14 is larger than a specified value." ></td>
	<td class="line x" title="98:111	Using Apriori, the a21a23a22a33a24 threshold can be specified in order to reduce computation, whereas with the proposed method, there is no way to adjust this threshold." ></td>
	<td class="line x" title="99:111	This implies that Apriori may be faster than our algorithm in terms of confidence." ></td>
	<td class="line x" title="100:111	However, since Apriori uses the property of confidence to reduce computation, it cannot be used for other functions, unlike the proposed method which can employ many standard functions, at least the five measures used here including confidence." ></td>
	<td class="line x" title="101:111	12 Correlation of All Substrings When computing correlations of all substrings in a corpus, a34 can be as large as a3 a5 a1a4a3 a55 a37 a14a5a6 a51 . Since the memory space requirement and computation time does not depend on a34, this method can be used to generate a list of the most hightly correlated substrings of any length." ></td>
	<td class="line x" title="102:111	In fact, in some cases, a18 may be too large to compute." ></td>
	<td class="line x" title="103:111	The Yamamoto-Church method(Yamamoto and Church, 2001) allows for the creation of a a21a29a22 a1 a19 a14 table using a0a8a1a4a3a15a14 memory space and a0a2a1a4a3 a5 a7 a9a12a11 a1a4a3a15a14a16a14 computation time, where a19 represents all substrings in a given corpus." ></td>
	<td class="line x" title="104:111	Yamamotos method shows that although there may be a3 a5 a1a4a3 a55 a37 a14a5a6 a51 kinds of substrings in a corpus, there is a51 a5 a3 occurence patterns (or sets of substrings which have same occurence pattern) at most." ></td>
	<td class="line x" title="105:111	The computational cost is greatly reduced if we deal with each pattern instead of each substring." ></td>
	<td class="line x" title="106:111	Although the order of computional complexity does not depend on a34, a18 differs whether the pattern is used or not." ></td>
	<td class="line x" title="107:111	We have also developed a system using the pattern which actually reduces the cost of computation." ></td>
	<td class="line x" title="108:111	Although the number of a18 is still problematic even using the YamamotoChurch method, and although the computation cost is much larger than using words, the program runs much faster than the simple method." ></td>
	<td class="line x" title="109:111	13 Conclusion This paper describes a method for selecting correlated pairs in a0a8a1a4a3a15a14 memory space and a0a2a1a4a3 a5 a7a10a9a12a11 a1a4a3a17a14a16a14 computation time, where a3 is the number of documents in a corpus, provided that there is an upper boundary in the number of different words/labels in one document/record." ></td>
	<td class="line x" title="110:111	We have observed that a corpus usually has this kind of upper boundary, and have shown that we can uses a sequential file for most of our memory requirements." ></td>
	<td class="line x" title="111:111	This method is useful not only for confidence but also for other functions whose values are decided by a21a23a22 a24, a21a29a22a25a28, a21a23a22 a30, a21a23a22a12a32 . Examples of these functions are mutual information, the dice coefficient, the confidence measure, the phi coefficient and the complimentary similarity measure." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N03-1032
Frequency Estimates For Statistical Word Similarity Measures
Terra, Egidio L.;Clarke, Charles L. A.;"></td>
	<td class="line x" title="1:206	Frequency Estimates for Statistical Word Similarity Measures Egidio Terra School of Computer Science University of Waterloo elterra@math.uwaterloo.ca C. L. A. Clarke School of Computer Science University of Waterloo claclark@plg2.uwaterloo.ca Abstract Statistical measures of word similarity have application in many areas of natural language processing, such as language modeling and information retrieval." ></td>
	<td class="line x" title="2:206	We report a comparative study of two methods for estimating word cooccurrence frequencies required by word similarity measures." ></td>
	<td class="line x" title="3:206	Our frequency estimates are generated from a terabyte-sized corpus of Web data, and we study the impact of corpus size on the effectiveness of the measures." ></td>
	<td class="line x" title="4:206	We base the evaluation on one TOEFL question set and two practice questions sets, each consisting of a number of multiple choice questions seeking the best synonym for a given target word." ></td>
	<td class="line x" title="5:206	For two question sets, a context for the target word is provided, and we examine a number of word similarity measures that exploit this context." ></td>
	<td class="line x" title="6:206	Our best combination of similarity measure and frequency estimation method answers 6-8% more questions than the best results previously reported for the same question sets." ></td>
	<td class="line oc" title="7:206	1 Introduction Many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts (Dunning, 1993; Church and Hanks, 1990; Dagan et al. , 1999)." ></td>
	<td class="line x" title="8:206	These tests attempt to measure dependence between words by using statistics taken from a large corpus." ></td>
	<td class="line x" title="9:206	In this context, a key assumption is that similarity between words is a consequence of word co-occurrence, or that the closeness of the words in text is indicative of some kind of relationship between them, such as synonymy or antonymy." ></td>
	<td class="line x" title="10:206	Although word sequences in natural language are unlikely to be independent, these statistical tests provide quantitative information that can be used to compare pairs of co-occurring words." ></td>
	<td class="line x" title="11:206	Also, despite the fact that word co-occurrence is a simple idea, there are a variety of ways to estimate word co-occurrence frequencies from text." ></td>
	<td class="line x" title="12:206	Two words can appear close to each other in the same document, passage, paragraph, sentence or fixed-size window." ></td>
	<td class="line x" title="13:206	The boundaries for determining cooccurrence will affect the estimates and as a consequence the word similarity measures." ></td>
	<td class="line x" title="14:206	Statistical word similarity measures play an important role in information retrieval and in many other natural language applications, such as the automatic creation of thesauri (Grefenstette, 1993; Li and Abe, 1998; Lin, 1998) and word sense disambiguation (Yarowsky, 1992; Li and Abe, 1998)." ></td>
	<td class="line x" title="15:206	Pantel and Lin (2002) use word similarity to create groups of related words, in order to discover word senses directly from text." ></td>
	<td class="line x" title="16:206	Recently, Tan et al.(2002) provide an analysis on different measures of independence in the context of association rules." ></td>
	<td class="line x" title="18:206	Word similarity is also used in language modeling applications." ></td>
	<td class="line x" title="19:206	Rosenfeld (1996) uses word similarity as a constraint in a maximum entropy model which reduces the perplexity on a test set by 23%." ></td>
	<td class="line x" title="20:206	Brown et al.(1992) use a word similarity measure for language modeling in an interpolated model, grouping similar words into classes." ></td>
	<td class="line x" title="22:206	Dagan et al.(1999) use word similarity to assign probabilities to unseen bigrams by using similar bigrams, which reduces perplexity up to 20% in held out data." ></td>
	<td class="line x" title="24:206	In information retrieval, word similarity can be used to identify terms for pseudo-relevance feedback (Harman, 1992; Buckley et al. , 1995; Xu and Croft, 2000; Vechtomova and Robertson, 2000)." ></td>
	<td class="line x" title="25:206	Xu and Croft (2000) expand queries under a pseudo-relevance feedback model by using similar words from documents retrieved and improve effectiveness by more than 20% on an 11-point average precision." ></td>
	<td class="line x" title="26:206	Landauer and Dumais (1997) applied word similarity measures to answer TOEFL (Test Of English as a Foreign Language) synonym questions using Latent Semantic Analysis." ></td>
	<td class="line x" title="27:206	Turney (2001) performed an evaluation of a specific word similarity measure using the same TOEFL questions and compared the results with those obtained Edmonton, May-June 2003 Main Papers, pp." ></td>
	<td class="line x" title="28:206	165-172 Proceedings of HLT-NAACL 2003 a0 = The results of the test were quite [unambiguous]. a1a3a2 = unambiguous a4 = a5 clear,doubtful,surprising, illegala6 Figure 1: Finding the best synonym option in presence of context a1a3a2 = boast a4 = a5 brag,yell,complain,explaina6 Figure 2: Finding the best synonym by Landauer and Dumais." ></td>
	<td class="line x" title="29:206	In our investigation of frequency estimates for word similarity measures, we compare the results of several different measures and frequency estimates to solve human-oriented language tests." ></td>
	<td class="line x" title="30:206	Our investigation is based in part on the questions used by Landauer and Dumais, and by Turney." ></td>
	<td class="line x" title="31:206	An example of such tests is the determination of the best synonym in a set of alternatives a4a8a7 a5 a4a10a9a12a11a13a4a3a14a15a11a13a4a3a16a17a11a18a4a20a19 a6 for a specific target word a1a3a2 in a context a0 a7 a5a22a21a3a23a9 a11 a21a3a23a14 a11a25a24a26a24a27a11 a21a28a23a29 a6a31a30 a1a3a2, as shown in figure 1." ></td>
	<td class="line x" title="32:206	Ideally, the context can provide support to choose best alternative for each question." ></td>
	<td class="line x" title="33:206	We also investigate questions where no context is available, as shown in figure 2." ></td>
	<td class="line x" title="34:206	These questions provides an easy way to assess the performance of measures and the co-occurrence frequency estimation methods used to compute them." ></td>
	<td class="line x" title="35:206	Although word similarity has been used in many different applications, to the best of our knowledge, ours is the first comparative investigation of the impact of cooccurrence frequency estimation on the performance of word similarity measures." ></td>
	<td class="line x" title="36:206	In this paper, we provide a comprehensive study of some of the most widely used similarity measures with frequency estimates taken from a terabyte-sized corpus of Web data, both in the presence of context and not." ></td>
	<td class="line x" title="37:206	In addition, we investigate frequency estimates for co-occurrence that are based both on documents and on a variety of different window sizes, and examine the impact of the corpus size on the frequency estimates." ></td>
	<td class="line x" title="38:206	In questions where context is available, we also investigate the effect of adding more words from context." ></td>
	<td class="line x" title="39:206	The remainder of this paper is organized as follows: In section 2 we briefly introduce some of the most commonly used methods for measuring word similarity." ></td>
	<td class="line x" title="40:206	In section 3 we present methods to assess word cooccurrence frequencies." ></td>
	<td class="line x" title="41:206	Section 4 presents our experimental evaluation, which is followed by a discussion of the results in section 5." ></td>
	<td class="line x" title="42:206	2 Measuring Word Similarity The notion for co-occurrence of two words can depicted by a contingency table, as shown in table 1." ></td>
	<td class="line x" title="43:206	Each dimension represents a random discrete variable a2a33a32 with range a34a35a7 a5a36a21 a32 a11a38a37 a21 a32 a6 (presence or absence of word a39 in a given text window or document)." ></td>
	<td class="line x" title="44:206	Each cell in the table represent the joint frequency a40a15a41a43a42a45a44a41a47a46 a7a8a48a50a49a52a51a54a53a28a55a57a56a59a58 a39 a11a61a60a63a62, where a48a50a49a52a51a54a53 is the maximum number of co-occurrences." ></td>
	<td class="line x" title="45:206	Under an independence assumption, the values of the cells in the contingency table are calculated using the probabilities in table 2." ></td>
	<td class="line x" title="46:206	The methods described below perform different measures of how distant observed values are from expected values under an independence assumption." ></td>
	<td class="line x" title="47:206	Tan et al.(2002) indicate that the difference between the methods arise from non-uniform marginals and how the methods react to this non-uniformity." ></td>
	<td class="line x" title="49:206	a21 a9 a37 a21 a9 a21 a14 a40 a41a65a64a18a44a41a43a66 a40a15a67 a41a65a64a18a44a41a43a66 a40 a41a43a66 a37 a21 a14 a40a12a41 a64 a44a67 a41 a66 a40 a67 a41 a64 a44a67 a41 a66 a40 a67 a41 a66 a40 a41a65a64 a40a15a67 a41a65a64 a48 Table 1: Contingency table a56a59a58 a21 a9a15a11 a21 a14a22a62a68a7a69a56a59a58 a21 a9a22a62a70a55a57a56a59a58 a21 a14a36a62 a56a59a58a71a37 a21 a9 a11 a21 a14 a62a72a7a73a56a59a58a61a37 a21 a9 a62a74a55a57a56a59a58 a21 a14 a62 a56a59a58 a21 a9 a11a18a37 a21 a14 a62a72a7a73a56a59a58 a21 a9 a62a70a55a57a56a59a58a71a37 a21 a14 a62 a56a59a58a71a37 a21 a9 a11a18a37 a21 a14 a62a72a7a73a56a59a58a61a37 a21 a9 a62a70a55a57a56a59a58a61a37 a21 a14 a62 Table 2: Probabilities under independence Occasionally, a context a0 is available and can provide support for the co-occurrence and alternative methods can be used to exploit this context." ></td>
	<td class="line x" title="50:206	The procedures to estimate a56a59a58 a21 a9a12a11 a21 a14a75a62, as well a56a59a58 a21 a32 a62, will be described in section 3." ></td>
	<td class="line x" title="51:206	2.1 Similarity between two words We first present methods to measure the similarity between two words a21 a9 and a21 a14 when no context is available." ></td>
	<td class="line oc" title="52:206	2.1.1 Pointwise Mutual Information This measure for word similarity was first used in this context by Church and Hanks (1990)." ></td>
	<td class="line o" title="53:206	The measure is given by equation 1 and is called Pointwise Mutual Information." ></td>
	<td class="line o" title="54:206	It is a straightforward transformation of the independence assumption (on a specific point), a56a59a58 a21 a9 a11 a21 a14 a62a76a7 a56a59a58 a21 a9 a62a47a55a72a56a59a58 a21 a14 a62, into a ratio." ></td>
	<td class="line x" title="55:206	Positive values indicate that words occur together more than would be expected under an independence assumption." ></td>
	<td class="line x" title="56:206	Negative values indicate that one word tends to appear only when the other does not." ></td>
	<td class="line x" title="57:206	Values close to zero indicate independence." ></td>
	<td class="line x" title="58:206	a77a20a78a80a79a82a81a84a83a86a85a88a87a33a89a90a85a54a91a92a83a94a93a95a87a33a89a76a93a54a96a70a87a80a97a26a98a100a99a75a93 a77a101a81a102a89 a85 a91a103a89 a93 a96 a77a101a81a102a89a90a85a92a96a45a77a101a81a102a89a76a93a38a96 (1) 2.1.2 a104 a14 -test This test is directly derived from observed and expected values in the contingency tables." ></td>
	<td class="line x" title="59:206	a105 a93 a87a107a106 a108a36a109a12a110 a64 a106 a111a22a109a75a110 a66 a81a84a112 a108a36a113a111a57a114a116a115a68a108a36a113a111 a96 a93 a115 a108a36a113a111 (2) The a104 a14 statistic determines a specific way to calculate the difference between values expected under independence and observed ones, as depicted in equation 2." ></td>
	<td class="line x" title="60:206	The values a40 a53 a44a117 correspond to the observed frequency estimates." ></td>
	<td class="line x" title="61:206	2.1.3 Likelihood ratio The likelihood ratio test provides an alternative to check two simple hypotheses based on parameters of a distribution." ></td>
	<td class="line x" title="62:206	Dunning (1993) used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution." ></td>
	<td class="line x" title="63:206	Two hypotheses used are: H1:a56a59a58 a21 a14a119a118a21 a9a22a62 a7 a56a59a58 a21 a14 a118a37 a21 a9 a62 (i.e. they occur independently); and H2: a56a59a58 a21 a14 a118a21 a9 a62a121a120a7a122a56a59a58 a21 a14 a118a37 a21 a9 a62 (i.e. not independent)." ></td>
	<td class="line x" title="64:206	These two conditionals are used as sample in the likelihood function a123 a58a84a56a59a58 a21 a14 a118a21 a9 a62a54a11a18a56a59a58 a21 a14 a118a37 a21 a9 a62a100a124a92a125a119a62, where a125 in this particular case represents the parameter of the binomial distribution a126 a58a128a127a95a11a18a129a65a124a92a125a119a62." ></td>
	<td class="line x" title="65:206	Under hypothesis H1, a56a59a58 a21 a14a130a118a21 a9a25a62a73a7a131a56a59a58 a21 a14a130a118a37 a21 a9a25a62a73a7a133a132, and for H2, a56a59a58 a21 a14a63a118 a21 a9a22a62a72a7a80a132a65a9a75a11a18a56a59a58 a21 a14a119a118a37 a21 a9a25a62a76a7a80a132a47a14 . a134 a87a136a135 a81a128a77a101a81a102a89a76a93a36a137a89a90a85a13a96a13a138a102a139a82a96a43a140 a135 a81a128a77a101a81a102a89a76a93a75a137a141a65a89a90a85a13a96a13a138a142a139a31a96 a135 a81a128a77a101a81a102a89 a93 a137a89 a85 a96a13a138a102a139 a85 a96a43a140 a135 a81a128a77a101a81a102a89 a93 a137a141a65a89 a85 a96a13a138a142a139 a93 a96 (3) Equation 3 represents the likelihood ratio." ></td>
	<td class="line x" title="66:206	Asymptotically, a143a28a144a17a145a84a146a36a147a31a148 is a104 a14 distributed." ></td>
	<td class="line x" title="67:206	2.1.4 Average Mutual Information This measure corresponds to the expected value of two random variables using the same equation as PMI." ></td>
	<td class="line x" title="68:206	Average mutual information was used as a word similarity measure by Rosenfeld (1996) and is given by equation 4." ></td>
	<td class="line x" title="69:206	a78a149a79a82a81a84a83 a85 a138a13a83 a93 a96a70a87 a106 a108a36a109a75a110 a64 a106 a111a25a109a12a110 a66 a77a101a81a102a150a43a91a61a151a130a96a45a97a27a98a25a99 a77a101a81a102a150a43a91a45a151a119a96 a77a101a81a102a150a82a96a45a77a101a81a102a151a130a96 (4) 2.2 Context supported similarity Similarity between two words can also be inferred from a context (if given)." ></td>
	<td class="line x" title="70:206	Given a context a0 a7 a5a22a21a3a23 a9 a11 a21a28a23 a14 a11a22a24a27a24a26a24a26a11 a21a28a23a29 a6, a21 a9 and a21 a14 are related if their co-occurrence with words in context are similar." ></td>
	<td class="line x" title="71:206	2.2.1 Cosine of Pointwise Mutual Information The PMI between each context word a21a10a23 and a21 a32 form a vector." ></td>
	<td class="line x" title="72:206	The elements in the vector represents the similarity weights of a21a28a23 and a21 a32 . The cosine value between the two vectors corresponding to a21 a9 and a21 a14 represents the similarity between the two words in the specified context, as depicted in equation 5." ></td>
	<td class="line x" title="73:206	a152 a77a101a81a102a89a90a85a18a138a153a89a76a93a54a96a74a87 a154 a155a82a156a27a157a27a158a36a159 a77a20a78a80a79a82a81a102a89a76a160a84a91a153a89 a85 a96a45a77a20a78a149a79a82a81a102a89a76a160a84a91a45a89 a93 a96 a161 a154 a155a82a156 a77a20a78a80a79a82a81a102a89 a160 a91a103a89 a85 a96 a93 a161 a154 a155a82a156 a77a20a78a80a79a82a81a102a89 a160 a91a153a89 a93 a96 a93 (5) Values closer to one indicate more similarity whereas values close to zero represent less similarity." ></td>
	<td class="line x" title="74:206	Lesk (1969) was one of the first to apply the cosine measure to word similarity, but did not use pointwise mutual information to compute the weights." ></td>
	<td class="line x" title="75:206	Pantel (2002) used the cosine of pointwise mutual information to uncover word sense from text." ></td>
	<td class="line x" title="76:206	2.2.2 a123 a9 norm In this method the conditional probability of each word a21a28a23 a32 in a0 given a21 a9 (and a21 a14 ) is computed." ></td>
	<td class="line x" title="77:206	The accumulated distance between the conditionals for all words in context represents the similarity between the two words, as shown in equation 6." ></td>
	<td class="line x" title="78:206	This method was proposed as an alternative word similarity measure in language modeling to overcome zero-frequency problems of bigrams (Dagan et al. , 1999)." ></td>
	<td class="line x" title="79:206	a135 a81a102a89 a85 a138a103a89 a93 a96a74a87 a106 a155 a156 a157a162a158a75a159 a137a77a101a81a102a89 a160 a137a89a28a163a54a96 a114 a77a101a81a102a89 a160 a137a89a57a164a36a96a38a137 (6) In this measure, a smaller value indicates a greater similarity." ></td>
	<td class="line x" title="80:206	2.2.3 Contextual Average Mutual Information The conditional probabilities between each word in the context and the two words a21 a9 and a21 a14 are used to calculate the mutual information of the conditionals (equation 7)." ></td>
	<td class="line x" title="81:206	This method was also used in Dagan et." ></td>
	<td class="line x" title="82:206	al." ></td>
	<td class="line x" title="83:206	(1999).a165 a78a80a79 a152 a81a102a89 a85 a138a153a89 a93 a96a88a87a149a106 a155 a156 a77a101a81a102a89 a160 a137a89a20a163a100a96a45a97a27a98a25a99 a77a101a81a102a89 a160 a137a89a20a163a100a96 a77a101a81a102a89 a160 a137a89a57a164a36a96 (7) 2.2.4 Contextual Jensen-Shannon Divergence This is an alternative to the Mutual Information formula (equation 8)." ></td>
	<td class="line x" title="84:206	It helps to avoid zero frequency problem by averaging the two distributions and also provides a symmetric measure (AMIC is not symmetric)." ></td>
	<td class="line x" title="85:206	This method was also used in Dagan et." ></td>
	<td class="line x" title="86:206	al." ></td>
	<td class="line x" title="87:206	(1999)." ></td>
	<td class="line x" title="88:206	a166 a135 a81a167a139a65a168a92a169a75a96a70a87a149a106a170a139a3a97a26a98a25a99 a139 a169 a165a90a171a28a172 a77a173a87 a77a101a81a102a89 a160 a137a89a90a85a13a96a175a174a176a77a101a81a102a89 a160 a137a89a76a93a54a96 a164 a79a15a177 a165a68a178 a81a102a89 a85 a138a103a89 a93 a96a74a87a80a166 a135 a81a128a77a101a81a102a89 a160 a137a89a20a163a100a96a38a168 a165a90a171a28a172 a77a20a96 a174a90a166 a135 a81a128a77a101a81a102a89 a160 a137a89a57a164a36a96a38a168 a165a90a171a28a172 a77a20a96 (8) 2.2.5 Pointwise Mutual Information of Multiple words Turney (2001) proposes a different formula for Pointwise Mutual Information when context is available, as depicted in equation 9." ></td>
	<td class="line x" title="89:206	The context is represented by a0 a23, which is any subset of the context a0 . In fact, Turney argued that bigger a0 a23 sets are worse because they narrow the estimate and as consequence can be affected by noise." ></td>
	<td class="line x" title="90:206	As a consequence, Turney used only one word a179 a32 from the context, discarding the remaining words." ></td>
	<td class="line x" title="91:206	The chosen word was the one that has biggest pointwise information with a21 a9 . Moreover, a21 a9 (a1a3a2 ) is fixed when the method is used to find the best a4 a32 for a1a3a2, so a56a59a58 a21 a9 a11 a0 a23 a62 is also fixed and can be ignored, which transforms the equation into the conditional a56a59a58 a21 a9 a118a21 a14 a11 a0 a62 . It is interesting to note that the equation a56a59a58 a21 a9a17a118a21 a14a17a11 a0 a62 is not the traditional n-gram model since no ordering is imposed on the words and also due to the fact that the words in this formula can be separated from one another by other words." ></td>
	<td class="line x" title="92:206	a77a20a78a80a79 a152 a81a102a89a90a85a38a91a153a89a76a93a100a138 a152 a160 a96a74a87 a77a101a81a102a89 a85 a91a153a89 a93 a91 a152 a160a142a96 a77a101a81a102a89a76a93a25a91 a152 a160 a96a45a77a101a81a102a89a90a85a38a91 a152 a160 a96 (9) 2.2.6 Other measures of word similarities Many other measures for word similarities exists." ></td>
	<td class="line x" title="93:206	Tan et al.(2002) present a comparative study with 21 different measures." ></td>
	<td class="line x" title="95:206	Lillian (2001) proposes a new word similarity measure in the context of language modeling, performing an comparative evaluation with other 7 similarity measures." ></td>
	<td class="line x" title="96:206	3 Co-occurrence Estimates We now discuss some alternatives to estimate word cooccurrence frequencies from an available corpus." ></td>
	<td class="line x" title="97:206	All probabilities mentioned in previous section can be estimated from these frequencies." ></td>
	<td class="line x" title="98:206	We describe two different approaches: a window-oriented approach and a document-oriented approach." ></td>
	<td class="line x" title="99:206	3.1 Window-oriented approach Let a40 a41 a42 be the frequency of a21 a32 and the co-occurrence frequency of a21 a9 and a21 a14 be denoted by a40 a41a180a64a38a44a41a47a66 . Let a48 be the size of the corpus in words." ></td>
	<td class="line x" title="100:206	In the window-oriented approach, individual word frequencies are the corpus frequencies." ></td>
	<td class="line x" title="101:206	The maximum likelihood estimate (MLE) for a21 a32 in the corpus is a56a59a58 a21 a32 a62a72a7 a40a12a41a43a42a92a181 a48 . The joint frequency a40 a41a65a64a18a44a41a43a66 is estimated by the number of windows where the two words co-occur." ></td>
	<td class="line oc" title="102:206	The window size may vary, Church and Hanks (1990) used windows of size 2 and 5." ></td>
	<td class="line x" title="103:206	Brown et al.(1992) used windows containing 1001 words." ></td>
	<td class="line x" title="105:206	Dunning (1993) also used windows of size 2, which corresponds to word bigrams." ></td>
	<td class="line x" title="106:206	Let the number of windows of size a182 in the corpus be a48 a41a180a183 . Recall that a48a184a49a90a51a100a53 is the maximum number of co-occurrences, i.e. a48a50a49a52a51a54a53a185a7a107a48 a41a180a183 in the windows-oriented approach." ></td>
	<td class="line x" title="107:206	The MLE of the co-occurrence probability is given by a56a59a58 a21 a9a15a11 a21 a14a22a62a68a7 a40 a41a65a64a18a44a41a43a66 a181 a48 a41a180a183 . In most common case, windows are overlapping, and in this case a48 a41a180a183 a7a69a48 a143a186a182a12a187a86a188 . The total frequency of windows for co-occurrence should be adjusted to reflect the multiple counts of the same co-occurrence." ></td>
	<td class="line x" title="108:206	One method to account for overlap is to divide the total count of windows by a21a20a39 a127a74a189 a146a75a21 a190a22a39a45a191a130a192a20a143a193a188 . This method also reinforces closer co-occurrences by assigning them a larger weight." ></td>
	<td class="line x" title="109:206	Smoothing techniques can be applied to address the zero-frequency problem, or alternatively, the window size can be increased, which also increases the chance of cooccurrence." ></td>
	<td class="line x" title="110:206	To avoid inconsistency, windows do not to cross document boundaries." ></td>
	<td class="line x" title="111:206	3.2 Document-oriented approach In information retrieval, one commonly uses document statistics rather than individual word statistics." ></td>
	<td class="line x" title="112:206	In an document-oriented approach, the frequency of a word a21 a32 is denoted by a189 a40 a41 a42 and corresponds to the number of documents in which the word appears, regardless of how frequently it occurs in each document." ></td>
	<td class="line x" title="113:206	The number of documents is denoted by a194 . The MLE for an individual word in document oriented approach is a56a59a58 a21 a32 a62a72a7a8a189 a40 a41 a42a92a181a12a194 . The co-occurrence frequency of two words a21 a9 and a21 a14, denoted by a189 a40a15a41 a64 a44a41 a66, is the number of documents where the words co-occur." ></td>
	<td class="line x" title="114:206	If we require only that the words co-occur in the same document, no distinction is made between distantly occurring words and adjacent words." ></td>
	<td class="line x" title="115:206	This distortion can be reduced by imposing a maximal distance for co-occurrence, (i.e. a fixed-sized window), but the frequency will still be the number of documents where the two words co-occur within this distance." ></td>
	<td class="line x" title="116:206	The MLE for the co-occurrence in this approach is a56a59a58 a21 a9a12a11 a21 a14a22a62a33a7a195a189 a40 a41a65a64a54a44a41a47a66 a181a15a194, since a48 a49a52a51a54a53 a7 a194 in the document-oriented approach." ></td>
	<td class="line x" title="117:206	3.3 Syntax based approach An alternative to the Window and Document-oriented approach is to use syntactical information (Grefenstette, 1993)." ></td>
	<td class="line x" title="118:206	For this purpose, a Parser or Part-Of-Speech tagger must be applied to the text and only the interesting pairs of words in correct syntactical categories used." ></td>
	<td class="line x" title="119:206	In this case, the fixed window can be superseded by the result of the syntax analysis or tagging process and the frequency of the pairs can be used directly." ></td>
	<td class="line x" title="120:206	Alternatively, the number of documents that contain the pair can also be used." ></td>
	<td class="line x" title="121:206	However, the nature of the language tests in this work make it impractical to be applied." ></td>
	<td class="line x" title="122:206	First, the alternatives are not in a context, and as such can have more than one part-of-speech tag." ></td>
	<td class="line x" title="123:206	Occasionally, it is possible to infer that the syntactic category of the alternatives from context of the target word a1a3a2, if there is such a context . When the alternatives, or the target word a1a3a2, are multiwords then the problem is harder, as depicted in the first example of figure 7." ></td>
	<td class="line x" title="124:206	Also, both parsers and POS tagger make mistakes, thus introducing error." ></td>
	<td class="line x" title="125:206	Finally, the size of the corpus used and its nature intensify the parser/POS taggers problems." ></td>
	<td class="line x" title="126:206	Figure 3: Results for TOEFL test set Figure 4: Impact of corpus size on TOEFL test set Figure 5: Results for TS1 and no context Figure 6: Results for TS1 and context 4 Experiments We evaluate the methods and frequency estimates using 3 test sets." ></td>
	<td class="line x" title="127:206	The first test set is a set of TOEFL questions first used by Landauer and Dumais (1997) and also by Turney (2001)." ></td>
	<td class="line x" title="128:206	This test set contains 80 synonym questions and for each question one a1a3a2 and four alternative options (a118a4a184a118a31a7a121a196 ) are given." ></td>
	<td class="line x" title="129:206	The other two test sets, which we will refer to as TS1 and TS2, are practice questions for the TOEFL." ></td>
	<td class="line x" title="130:206	These two test sets also contain four alternatives options, a118a4a186a118a43a7a197a196, and a1a10a2 is given in context a0 (within a sentence)." ></td>
	<td class="line x" title="131:206	TS1 has 50 questions and was also used by Turney (2001)." ></td>
	<td class="line x" title="132:206	TS2 has 60 questions extracted from a TOEFL practice guide (King and Stanley, 1989)." ></td>
	<td class="line x" title="133:206	For all test sets the answer to each question is known and unique." ></td>
	<td class="line x" title="134:206	For comparison purposes, we also use TS1 and TS2 with no context." ></td>
	<td class="line x" title="135:206	For the three test sets, TOEFL, TS1 and TS2 without context, we applied the word and document-oriented frequency estimates presented." ></td>
	<td class="line x" title="136:206	We investigated a variety of window sizes, varying the window size from 2 to 256 by powers of 2." ></td>
	<td class="line x" title="137:206	The labels used in figures 3, 5, 6, 8, 9, 10, 12 are composed from a keyword indicating the frequency estimate used (W-window oriented; and DR-document retrieval oriented) and a keyword indicating the word similarity measure." ></td>
	<td class="line x" title="138:206	For no-context measures the keywords are: PMI-Pointwise Mutual Information; CHI-Chi-Squared; MI-Average mutual information; and LL-Log-likelihood." ></td>
	<td class="line x" title="139:206	For the measures with context: CP-Cosine pointwise mutual information; L1-L1 norm; AMIC-Average Mutual Information in the presence of context; IRAD-JensenShannon Divergence; and PMIC-a127 Pointwise Mutual Information with a127 words of context." ></td>
	<td class="line x" title="140:206	For TS1 and TS2 with context, we also investigate Turneys hypothesis that the outcome of adding more words from a0 is negative, using DR-PMIC." ></td>
	<td class="line x" title="141:206	The result of this experiment is shown in figures 10 and 12 for TS1 and TS2 respectively." ></td>
	<td class="line x" title="142:206	It is important to note that in some of the questions, a1a3a2 or one or more of the a4 a32 s are multi-word strings." ></td>
	<td class="line x" title="143:206	For these questions, we assume that the strings may be treated as collocations and use them as is, adjusting the size of the windows by the collocation size when applicable." ></td>
	<td class="line x" title="144:206	The corpus used for the experiments is a terabyte of Web data crawled from the general web in 2001." ></td>
	<td class="line x" title="145:206	In order to balance the contents of the corpus, a breadth-first order search was used from a initial seed set of URLs representing the home page of 2392 universities and other educational organizations (Clarke et al. , 2002)." ></td>
	<td class="line x" title="146:206	No duplicate pages are included in the collection and the crawler also did not allow a large number of pages from the same site to be downloaded simultaneously." ></td>
	<td class="line x" title="147:206	Overall, the collection contains 53 billion words and 77 million documents." ></td>
	<td class="line x" title="148:206	A key characteristic of this corpus is that it consists of HTML files." ></td>
	<td class="line x" title="149:206	These files have a focus on the presentation, and not necessarily on the style of writing." ></td>
	<td class="line x" title="150:206	Parsing or tagging these files can be a hard process and prone to introduction of error in rates bigger than traditional corpora used in NLP or Information Retrieval." ></td>
	<td class="line x" title="151:206	We also investigate the impact of the collection size on a0 = The country is plagued by [turmoil]. a4 = a5 constant change,utter confusion,bad weather,fuel shortagesa6 a0 = [For] all their protestations, they heeded the judges ruling. a4 = a5 In spite of,Because of,On behalf of,withouta6 Figure 7: Examples of harder questions in TS2 Figure 8: Results for TS2 and no context Figure 9: Results for TS2 and context the results, as depicted in figures 4, 11 and 13 for TOEFL, TS1 and TS2 test sets, respectively." ></td>
	<td class="line x" title="152:206	5 Results and Discussion The results for the TOEFL questions are presented in figure 3." ></td>
	<td class="line x" title="153:206	The best performance found is 81.25% of the questions correctly answered." ></td>
	<td class="line x" title="154:206	That result used DR-PMI with a window size of 16-32 words." ></td>
	<td class="line x" title="155:206	This is an improvement over the results presented by Landauer and Dumais (1997) using Latent Semantic Analysis, where 64.5% of the questions were answered correctly, and Turney (2001), using pointwise mutual information and document retrieval, where the best result was 73.75%." ></td>
	<td class="line x" title="156:206	Although we use a similar method (DR-PMI), the difference between the results presented here and Turneys results may be due to differences in the corpora and differences in the queries." ></td>
	<td class="line x" title="157:206	Turney uses Altavista and we used our own crawl of web data." ></td>
	<td class="line x" title="158:206	We can not compare the collections since we do not know how Altavista collection is created." ></td>
	<td class="line x" title="159:206	As for the queries, we have more control over the queries since we can precisely specify the window size and we also do not know how queries are evaluated in Altavista." ></td>
	<td class="line x" title="160:206	PMI performs best overall, regardless of estimates used (DR or W)." ></td>
	<td class="line x" title="161:206	W-CHI performs up to 80% when using window estimates, outperforming DR-CHI." ></td>
	<td class="line x" title="162:206	MI and LL yield exactly the same results (and the same ranking of the alternatives), which suggests that the binomial distribution is a good approximation for word occurrence in text." ></td>
	<td class="line x" title="163:206	The results for MI and PMI indicate that, for the two discrete random variables a2 a9 and a2 a14 (and range a34a198a7 a5a22a21 a32 a11a18a37 a21 a32 a6 ), no further gain is achieved by calculating the expectation in the divergence." ></td>
	<td class="line x" title="164:206	Recall that the divergence formula has an embedded expectation to be calculated between the joint probability of these two random variables and their independence." ></td>
	<td class="line x" title="165:206	The peak of information is exactly where both words co-occur, i.e. when a2 a9a199a7 a21 a9 and a2 a14a200a7 a21 a14, and not any of the other three possible combinations." ></td>
	<td class="line x" title="166:206	Similar trends are seen when using TS1 and no context, as depicted in figure 5." ></td>
	<td class="line x" title="167:206	PMI is best overall, and DRPMI and W-PMI outperform each other with different windows sizes." ></td>
	<td class="line x" title="168:206	W-CHI has good performance in small windows sizes." ></td>
	<td class="line x" title="169:206	MI and LL yield identical (poor) results, being worst than chance for some window sizes." ></td>
	<td class="line x" title="170:206	Turney (2001) also uses this test set without context, achieving 66% peak performance compared with our best performance of 72% (DR-PMI)." ></td>
	<td class="line x" title="171:206	In the test set TS2 with no context, the trend seen between TOEFL and TS1 is repeated, as shown in figure 8." ></td>
	<td class="line x" title="172:206	PMI is best overall but W-CHI performs better than PMI in three cases." ></td>
	<td class="line x" title="173:206	DR-CHI performs poorly for small windows sizes." ></td>
	<td class="line x" title="174:206	MI and LL also perform poorly in comparison with PMI." ></td>
	<td class="line x" title="175:206	The peak performance is 75%, using DRPMI with a window size of 64." ></td>
	<td class="line x" title="176:206	The result are not what we expected when context is used in TS1 and TS2." ></td>
	<td class="line x" title="177:206	In TS1, figure 6, only one of the measures, DR-PMIC-1, outperforms the results from non-context measures, having a peak of 80% correct answers." ></td>
	<td class="line x" title="178:206	The condition for the best result (one word from context and a window size of 8) is similar to the one used for the best score reported by Turney." ></td>
	<td class="line x" title="179:206	L1, AMIC and IRAD perform poorly, worst than chance for some window sizes." ></td>
	<td class="line x" title="180:206	One difference in the results is that for DR-PMIC-1 only the best word from context was used, while the other methods used all words but stopwords." ></td>
	<td class="line x" title="181:206	We examine the context and discovered that using more words degrades the performance of DR-PMIC in all different windows sizes but, even using all words except stopwords, the result from DR-PMIC is better than any other contextual measure 76% correct answers in TS1 (with DR-PMIC and a window size of 8)." ></td>
	<td class="line x" title="182:206	For TS2, no measure using context was able to perform Figure 10: Influence from the context on TS1 Figure 11: Impact of corpus size on TS1 Figure 12: Influence from the context on TS2 Figure 13: Impact of corpus size on TS2 better than the non-contextual measures." ></td>
	<td class="line x" title="183:206	DR-PMIC-1 performs better overall but has worse performance than DR-CP with a window size of 8." ></td>
	<td class="line x" title="184:206	In this test set, the performance of DR-CP is better than W-CP." ></td>
	<td class="line x" title="185:206	L1 performs better than AMIC but both have poor results, IRAD is never better than chance." ></td>
	<td class="line x" title="186:206	The context in TS2 has more words than TS1 but the questions seem to be harder, as shown in figure 7." ></td>
	<td class="line x" title="187:206	In some of the TS2 questions, the target word or one of the alternatives uses functional words." ></td>
	<td class="line x" title="188:206	We also investigate the influence of more words from context in TS2, as depicted in figure 12, where the trends seen with TS1 are repeated." ></td>
	<td class="line x" title="189:206	The results in TS1 and TS2 suggest that the available context is not very useful or that it is not being used properly." ></td>
	<td class="line x" title="190:206	Finally, we selected the method that yields the best performance for each test set to analyze the impact of the corpus size on performance, as shown in figures 4, 11 and 13." ></td>
	<td class="line x" title="191:206	For TS1 we use W-PMI with a window size of 2 (W-PMI2) when no context is used and DR-PMIC-1 with a window size of 8 (DR-PMIC8-1) when context is used." ></td>
	<td class="line x" title="192:206	For those measures, very little improvement is noticed after 500 GBytes for DR-PMIC8-1, roughly half of the collection size." ></td>
	<td class="line x" title="193:206	No apparent improvement is achieved after 300-400 GBytes for W-PMI2." ></td>
	<td class="line x" title="194:206	For TS2 we use DR-PMI with a window size of 64 (DRPMI64) when no context is used, and DR-PMIC-1 with a windows size of 64 (DR-PMIC64-1) when context is used." ></td>
	<td class="line x" title="195:206	It is clear that for TS2 no substantial improvement in DR-PMI64 and DR-PMIC64-1 is achieved by increasing the corpus size to values bigger than 300-400 GBytes." ></td>
	<td class="line x" title="196:206	The most interesting impact of corpus size was on TOEFL test set using DR-PMI with a window size of 16 (DR-PMI16)." ></td>
	<td class="line x" title="197:206	Using the full corpus is no better than using 5% of the corpus, and the best result, 82.5% correct answers, is achieved when using 85-95% of corpus size." ></td>
	<td class="line x" title="198:206	6 Conclusion Using a large corpus and human-oriented tests we describe a comprehensive study of word similarity measures and co-occurrence estimates, including variants on corpus size." ></td>
	<td class="line x" title="199:206	Without any parameter training, we were able to correctly answer at least 75% questions in all test sets." ></td>
	<td class="line x" title="200:206	From all combinations of estimates and measures, document retrieval with a maximum window of 16 words and pointwise mutual information performs best on average in the three test sets used." ></td>
	<td class="line x" title="201:206	However, both document or windows-oriented approach for frequency estimates produce similar results in average." ></td>
	<td class="line x" title="202:206	The impact of the corpus size is not very conclusive, it suggests that the increase in the corpus size normally reaches an asymptote, but the points where this occurs is distinct among different measures and frequency estimates." ></td>
	<td class="line x" title="203:206	Our results outperform the previously reported results on test sets when no context is used, being able to correctly answer 81.25% of TOEFL synonym questions, compared with a previous best result of 73.5%." ></td>
	<td class="line x" title="204:206	A human average score on the same type of questions is 64.5% (Landauer and Dumais, 1997)." ></td>
	<td class="line x" title="205:206	We also perform better than previous work on another test set used as practice questions for TOEFL, obtaining 80% correct answers compared to a best result of 74% from previous work." ></td>
	<td class="line x" title="206:206	Acknowledgments This work was made possible also in part by PUC/RS and Ministry of Education of Brazil through CAPES agency." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1805
A Language Model Approach To Keyphrase Extraction
Tomokiyo, Takashi;Hurst, Matthew;"></td>
	<td class="line x" title="1:154	A Language Model Approach to Keyphrase Extraction Takashi Tomokiyo and Matthew Hurst Applied Research Center Intelliseek, Inc. Pittsburgh, PA 15213 a0 ttomokiyo,mhurst a1 @intelliseek.com Abstract We present a new approach to extracting keyphrases based on statistical language models." ></td>
	<td class="line x" title="2:154	Our approach is to use pointwise KL-divergence between multiple language models for scoring both phraseness and informativeness, which can be unified into a single score to rank extracted phrases." ></td>
	<td class="line x" title="3:154	1 Introduction In many real world deployments of text mining technologies, analysts are required to deal with large collections of documents from unfamiliar domains." ></td>
	<td class="line x" title="4:154	Familiarity with the domain is necessary in order to get full leverage from text analysis tools." ></td>
	<td class="line x" title="5:154	However, browsing data is not an efficient way to get an understanding of the topics and events which are particular to a domain." ></td>
	<td class="line x" title="6:154	For example, an analyst concerned with the area of hybrid cars may harvest messages from online forums." ></td>
	<td class="line x" title="7:154	They may then want to rapidly construct a hierarchy of topics based on the content of these messages." ></td>
	<td class="line x" title="8:154	In addition, in cases where these messages are harvested via a search of some sort, there is a requirement to obtain a rich and effective set of search terms." ></td>
	<td class="line x" title="9:154	The technology described in this paper is an example of a phrase finder capable of delivering a set of indicative phrases given a particular set of documents from a target domain." ></td>
	<td class="line x" title="10:154	In the hybrid car example, the result of this process is a set of phrases like that shown in Figure 1." ></td>
	<td class="line x" title="11:154	1 civic hybrid 2 honda civic hybrid 3 toyota prius 4 electric motor 5 honda civic 6 fuel cell 7 hybrid cars 8 honda insight 9 battery pack 10 sports car 11 civic si 12 hybrid car 13 civic lx 14 focus fcv 15 fuel cells 16 hybrid vehicles 17 tour de sol 18 years ago 19 daily driver 20 jetta tdi 21 mustang gt 22 ford escape 23 steering wheel 24 toyota prius today 25 electric motors 26 gasoline engine 27 internal combustion engine 28 gas engine 29 front wheels 30 key sense wire 31 civic type r 32 test drive 33 street race 34 united states 35 hybrid powertrain 36 rear bumper 37 ford focus 38 detroit auto show 39 parking lot 40 rear wheels Figure 1: Top 40 keyphrases automatically extracted from messages relevant to civic hybrid using our system In order to capture domain-specific terms efficiently in limited time, the extraction result should be ranked with more indicative and good phrase first, as shown in this example." ></td>
	<td class="line x" title="12:154	2 Phraseness and informativeness The word keyphrase implies two features: phraseness and informativeness." ></td>
	<td class="line x" title="13:154	Phraseness is a somewhat abstract notion which describes the degree to which a given word sequence is considered to be a phrase." ></td>
	<td class="line x" title="14:154	In general, phraseness is defined by the user, who has his own criteria for the target application." ></td>
	<td class="line x" title="15:154	For instance, one user might want only noun phrases while another user might be interested only in phrases describing a certain set of products." ></td>
	<td class="line x" title="16:154	Although there is no single definition of the term phrase, in this paper, we focus on collocation or cohesion of consecutive words." ></td>
	<td class="line x" title="17:154	Informativeness refers to how well a phrase captures or illustrates the key ideas in a set of documents." ></td>
	<td class="line x" title="18:154	Because informativeness is defined with respect to background information and new knowledge, users will have different perceptions of informativeness." ></td>
	<td class="line x" title="19:154	In our calculations, we make use of the relationship between foreground and background corpora to formalize the notion of informativeness." ></td>
	<td class="line x" title="20:154	The target document set from which representative keyphrases are extracted is called the foreground corpus." ></td>
	<td class="line x" title="21:154	The document set to which this target set is compared is called the background corpus." ></td>
	<td class="line x" title="22:154	For example, a foreground corpus of the current weeks news would be compared to a background corpus of an entire news article archive to determine that certain phrases, like press conference are typical of news stories in general and do not capture the particulars of current events in the way that national museum of antiquities does." ></td>
	<td class="line x" title="23:154	Other examples of foreground and background corpora include: a web site for a certain company and web data in general; a newsgroup and the whole Usenet archive; and research papers of a certain conference and research papers in general." ></td>
	<td class="line x" title="24:154	In order to get a ranked keyphrase list, we need to combine both phraseness and informativeness into a single score." ></td>
	<td class="line x" title="25:154	A sequence of words can be a good phrase but not an informative one, like the expression in spite of. A word sequence can be informative for a particular domain but not a phrase; toyota, honda, ford is an example of a non-phrase sequence of informative words in a hybrid car domain." ></td>
	<td class="line x" title="26:154	The algorithm we propose for keyphrase finding requires that the keyphrase score well for both phraseness and informativeness." ></td>
	<td class="line oc" title="27:154	3 Related work Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al. , 1991), the chi-square test, pointwise mutual information (MI) (Church and Hanks, 1990), and binomial loglikelihood ratio test (BLRT) (Dunning, 1993)." ></td>
	<td class="line x" title="28:154	According to (Manning and Schutze, 1999), BLRT is one of the most stable methods for collocation discovery." ></td>
	<td class="line o" title="29:154	(Pantel and Lin, 2001) reports, however, that BLRT score can be also high for two frequent terms that are rarely adjacent, such as the word pair the the, and uses a hybrid of MI and BLRT." ></td>
	<td class="line x" title="30:154	Keyphrase extraction Damerau (1993) uses the relative frequency ratio between two corpora to extract domain-specific keyphrases." ></td>
	<td class="line x" title="31:154	One problem of using relative frequency is that it tends to assign too high a score for words whose frequency in the background corpus is small (or even zero)." ></td>
	<td class="line x" title="32:154	Some work has been done in extracting keyphrases from technical documents treating keyphrase extraction as a supervised learning problem (Frank et al. , 1999; Turney, 2000)." ></td>
	<td class="line x" title="33:154	The portability of a learned classifier across various unstructured/structured text is not clear, however, and the agreement between classifier and human judges is not high.1 We would like to have the ability to extract keyphrases from a totally new domain of text without building a training corpus." ></td>
	<td class="line p" title="34:154	Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF is suitable for finding informative phrases." ></td>
	<td class="line o" title="35:154	They took the intersection of each top 10% of phrases identified by MI and RIDF, but did not extend the approach to combining the two metrics into a unified score." ></td>
	<td class="line x" title="36:154	4 Baseline method based on binomial log-likelihood ratio test We can use various statistics as a measure for phraseness and informativeness." ></td>
	<td class="line x" title="37:154	For our baseline, we have selected the method based on binomial loglikelihood ratio test (BLRT) described in (Dunning, 1993)." ></td>
	<td class="line x" title="38:154	The basic idea of using BLRT for text analysis is to consider a word sequence as a repeated sequence of binary trials comparing each word in a corpus to a target word, and use the likelihood ratio of two hypotheses that (i) two events, observed a0a2a1 times out of a3a4a1 total tokens and a0a6a5 times out of a3a7a5 total tokens respectively, are drawn from different distributions and (ii) from the same distribution." ></td>
	<td class="line x" title="39:154	1e.g. Turney reports 62% good, 18% bad, 20% no opinion from human judges." ></td>
	<td class="line x" title="40:154	The BLRT score is calculated with a0a2a1a4a3a6a5a8a7a10a9a12a11 a1a14a13 a0 a1a14a13 a3 a1a16a15 a7a10a9a12a11 a5a17a13 a0 a5a17a13 a3 a5a18a15 a7a10a9a12a11 a13 a0 a1 a13 a3a4a1 a15 a7a10a9a12a11 a13 a0 a5 a13 a3 a5 a15 (1) where a11a20a19a22a21 a0a6a19a24a23 a3a25a19, a11a26a21a27a9 a0 a1a29a28 a0 a5 a15 a23a30a9 a3a4a1a29a28 a3 a5 a15, and a7a10a9a12a11 a13 a0 a13 a3 a15 a21a31a11a33a32a34a9a36a35a38a37a26a11 a15a40a39a42a41 a32 (2) In the case of calculating the phraseness score of an adjacent word pair (a43 a13a45a44 ), the null hypothesis is that a43 and a44 are independent, which can be expressed as a11a2a9 a44a25a46a43 a15 a21a47a11a29a9 a44a22a46a49a48 a43 a15." ></td>
	<td class="line x" title="41:154	We can use Equation (1) to calculate phraseness by setting: a0 a1a50a21a52a51a53a9a54a43 a13a45a44a55a15a16a13 a3a4a1a56a21a57a51a53a9a54a43 a15a16a13 a0 a5a58a21a52a51a53a9 a48 a43 a13a45a44a59a15 a21a57a51a60a9 a44a59a15 a37a61a51a53a9a54a43 a13a45a44a55a15a16a13 a3 a5 a21a57a51a53a9 a48 a43 a15 a21a57a62a64a63a65a51a53a9a54a66 a15 a37a31a51a53a9a54a43 a15 (3) where a51a60a9a54a43 a15 is the frequency of the word a43 and a51a53a9a54a43 a13a45a44a59a15 is the frequency of a44 following a43 . For calculating informativeness of a word a66, a0 a1a50a21a57a51a68a67a70a69a42a9a54a66 a15a16a13 a3 a1 a21a57a62a64a63a65a51a68a67a70a69a42a9a54a66 a15a16a13 a0 a5 a21a57a51a50a71a72a69a42a9a54a66 a15a16a13 a3 a5a38a21a57a62a64a63a65a51a50a71a72a69a42a9a54a66 a15 (4) where a51a68a67a70a69a42a9a54a66 a15 and a51a68a71a72a69a42a9a54a66 a15 are the frequency of a66 in the foreground and background corpus, respectively." ></td>
	<td class="line x" title="42:154	Combining a phraseness score a73a25a74 and an informativeness score a73a29a19 into a single score value is not a trivial task since the the BLRT scores vary a lot between phraseness and informativeness and also depending on data (c.f. Figure 6 (a))." ></td>
	<td class="line x" title="43:154	One way to combine those scores is to use an exponential model." ></td>
	<td class="line x" title="44:154	We experimented with the following logistic function: a73a75a21 a35 a35a56a28a61a76a78a77a30a79a80a9a36a37a38a81a59a73 a74 a37a83a82a70a73 a19 a28a85a84 a15 (5) whose parameters a81,a82, and a84 are estimated on a heldout data set, given feedback from users (i.e. supervised)." ></td>
	<td class="line x" title="45:154	Figure 2 shows some example phrases extracted with this method from the data set described in Section 6.1, where the parameters, a81, a82, a84, are manually optimized on the test data." ></td>
	<td class="line x" title="46:154	Although it is possible to rank keyphrases using this approach, there are a couple of drawbacks." ></td>
	<td class="line x" title="47:154	1 message news 2 minority report 3 star wars 4 john harkness 5 derek janssen 6 robert frenchu 7 sean ohara 8 box office 9 dawn taylor 10 anthony gaza 11 star trek 12 ancient race 13 scooby doo 14 austin powers 15 home.attbi.com hey 16 sixth sense 17 hey kids 18 gaza man 19 lee harrison 20 years ago 21 julia roberts 22 national guard 23 bourne identity 24 metrotoday www.zap2it.com 25 starweek magazine 26 eric chomko 27 wilner starweek 28 tim gueguen 29 jodie foster 30 johnnie kendricks Figure 2: Keyphrases extracted with BLRT (a=0.0003, b=0.000005, c=8) Necessity of tuning parameters the existence of parameters in the combining function requires human labeling, which is sometimes an expensive task to do, and the robustness of learned weight across domains is unknown." ></td>
	<td class="line x" title="48:154	We would like to have a parameter-free and robust way of combining scores." ></td>
	<td class="line x" title="49:154	Inappropriate symmetry BLRT tests to see if two random variables are independent or not." ></td>
	<td class="line x" title="50:154	This sometimes leads to unwanted phrases getting a high score." ></td>
	<td class="line x" title="51:154	For example, when the background corpus happens to have many occurrences of phrase al jazeera which is an unusual phrase in the foreground corpus, then the phrase still gets high score of informativeness because the distribution is so different." ></td>
	<td class="line x" title="52:154	What we would like to have instead is asymmetric scoring function to test the loss of the action of not taking the target phrase as a keyphrase." ></td>
	<td class="line x" title="53:154	In the next section, we propose a new method trying to address these issues." ></td>
	<td class="line x" title="54:154	5 Proposed method 5.1 Language models and expected loss A language model assigns a probability value to every sequence of words a86a87a21a64a66 a1a88a66 a5a2a89a70a89a70a89a88a66 a39 . The probability a90a60a9a54a86 a15 can be decomposed as a90a60a9a54a86 a15 a21 a39 a91 a19a93a92 a1 a90a60a9a54a66a94a19 a46a66 a1a45a66 a5a2a89a70a89a70a89a88a66a94a19 a41 a1 a15 Assuming a66a38a19 only depends on the previous a95 words, N-gram language models are commonly used." ></td>
	<td class="line x" title="55:154	The following is the trigram language model case." ></td>
	<td class="line x" title="56:154	a90a60a9a54a86 a15 a21 a39 a91 a19a93a92 a1 a90 a9a54a66a94a19 a46a66a94a19 a41 a5 a13 a66a94a19 a41 a1 a15 Here each word only depends on the previous two words." ></td>
	<td class="line x" title="57:154	Please refer to (Jelinek, 1990) and (Chen and Goodman, 1996) for more about N-gram models and associated smoothing methods." ></td>
	<td class="line x" title="58:154	Now suppose we have a foreground corpus and a background corpus and have created a language model for each corpus." ></td>
	<td class="line x" title="59:154	The simplest language model is a unigram model, which assumes each word of a given word sequence is drawn independently." ></td>
	<td class="line x" title="60:154	We denote the unigram model for the foreground corpus as a7 a1 a1fg and for the background corpus as a7 a1 a1bg." ></td>
	<td class="line x" title="61:154	We can also train higher order models a7 a1a3a2 fg and a7 a1a3a2 bg for each corpus, each of which is a a95 -gram model, where a95a61a9a5a4 a35 a15 is the order." ></td>
	<td class="line x" title="62:154	a6a7 phrasenessa7 a8 a9 a37a30a37 informativeness a37a30a37a11a10 a7 a1a12a2 fg a7 a1a3a2 bg a7 a1 a1 fg a7 a1 a1 bg Figure 3: Phraseness and informativeness as loss between language models." ></td>
	<td class="line x" title="63:154	Among those four models, a7 a1 a2fg will be the best model to describe the foreground corpus in the sense that it has the smallest cross-entropy or perplexity value over the corpus." ></td>
	<td class="line x" title="64:154	If we use one of the other three models instead, then we have some inefficiency or loss to describe the corpus." ></td>
	<td class="line x" title="65:154	We expect the amount of loss between using a7 a1a3a2fg and a7 a1 a1fg is related to phraseness and the loss between a7 a1a13a2fg and a7 a1a3a2bg is related to informativeness." ></td>
	<td class="line x" title="66:154	Figure 3 illustrates these relationships." ></td>
	<td class="line x" title="67:154	5.2 Pointwise KL-divergence between models One natural metric to measure the loss between two language models is the Kullback-Leibler (KL) divergence." ></td>
	<td class="line x" title="68:154	The KL divergence (also called relative entropy) between two probability mass function a11a29a9a54a43 a15 and a14a59a9a54a43 a15 is defined as a15 a9a12a11a17a16a18a14 a15 a21a20a19a22a21 a11a29a9a54a43 a15 a1a93a3a6a5 a11a29a9a54a43 a15 a14a59a9a54a43 a15 (6) KL divergence is a measure of the inefficiency of assuming that the distribution is a14 when the true distribution is a11 . (Cover and Thomas, 1991) You can see this by the following relationship: a15 a9a12a11a17a16a18a14 a15 a21 a19a22a21 a11a29a9a54a43 a15 a1a4a3a6a5 a11a29a9a54a43 a15 a37 a19a23a21 a11a29a9a54a43 a15 a1a4a3a6a5 a14a59a9a54a43 a15 a21 a19a22a21 a11a29a9a54a43 a15 a35 a1a4a3a6a5 a14a59a9a54a43 a15 a37a17a24a85a9a26a25 a15 The first term a62 a21 a11a29a9a54a43 a15 a1a27a29a28a31a30a33a32a35a34 a21a37a36 is the cross entropy and the second term a24a85a9a26a25 a15 is the entropy of the random variable a25, which is how much we could compress symbols if we know the true distribution a11 . We define pointwise KL divergence a38a40a39 a9a12a11a41a16a42a14 a15 to be the term inside of the summation of Equation (6): a38 a39 a9a12a11a17a16a18a14 a15a44a43a46a45a48a47a21a27a11a2a9a54a86 a15 a1a4a3a6a5 a11a29a9a54a86 a15 a14a59a9a54a86 a15 (7) Intuitively, this is the contribution of the phrase a86 to the expected loss of the entire distribution." ></td>
	<td class="line x" title="69:154	We can now quantify phraseness and informativeness as follows: Phraseness of a86 is how much we lose information by assuming independence of each word by applying the unigram model, instead of the a95 gram model." ></td>
	<td class="line x" title="70:154	a38 a39 a9a72a7 a1 a2 fg a16a56a7 a1 a1 fga15 (8) Informativeness of a86 is how much we lose information by assuming the phrase is drawn from the background model instead of the foreground model." ></td>
	<td class="line x" title="71:154	a38a49a39 a9a72a7 a1 a2 fg a16a56a7 a1 a2 bga15a16a13 or (9) a38a49a39 a9a72a7 a1 a1 fg a16a56a7 a1 a1 bga15 (10) Combined The following is considered to be a mixture of phraseness and informativeness." ></td>
	<td class="line x" title="72:154	a38a49a39 a9a72a7 a1 a2 fg a16a56a7 a1 a1 bga15 (11) Note that the KL divergence is always nonnegative2, but the pointwise KL divergence can be a negative value." ></td>
	<td class="line x" title="73:154	An example is the phraseness of the bigram the the." ></td>
	<td class="line oc" title="74:154	a11a29a9 thea13 thea15 a1a4a3a6a5 a11a29a9 thea13 thea15 a11a29a9 thea15 a11a29a9 thea15a1a0 a2 since a11a2a9 thea13 thea15a4a3 a11a29a9 thea15 a11a29a9 thea15 . Also note that in the case of phraseness of a bigram, the equation looks similar to pointwise mutual information (Church and Hanks, 1990), but they are different." ></td>
	<td class="line o" title="75:154	Their relationship is as follows." ></td>
	<td class="line n" title="76:154	a38a49a39 a9a12a11a29a9a54a43 a13a45a44a55a15 a16a22a11a2a9a54a43 a15 a11a29a9 a44a55a15a45a15 a21a31a11a29a9a54a43 a13a45a44a59a15 a1a4a3a6a5 a11a29a9a54a43 a13a45a44a59a15 a11a2a9a54a43 a15 a11a29a9 a44a55a15 a5 a6a8a7 a9 pointwise MI The pointwise KL divergence does not assign a high score to a rare phrase, whose contribution of loss is small by definition, unlike pointwise mutual information, which is known to have problems (as described in (Manning and Schutze, 1999), e.g.)." ></td>
	<td class="line x" title="77:154	5.3 Combining phraseness and informativeness One way of getting a unified score of phraseness and informativeness is using equation (11)." ></td>
	<td class="line x" title="78:154	We can also calculate phraseness and informativeness separately and then combine them." ></td>
	<td class="line x" title="79:154	We combine the phraseness score a73a25a74 and informativeness score a73a80a19 by simply adding them into a single score a73 . a73a75a21a57a73a20a74 a28a47a73 a19 (12) Intuitively, this can be thought of as the total loss." ></td>
	<td class="line x" title="80:154	We will show some empirical results to justify this scoring in the next section." ></td>
	<td class="line x" title="81:154	6 Experimental results In this section, we show some preliminary experimental results of applying our method on real data." ></td>
	<td class="line x" title="82:154	6.1 Data set We used the 20 newsgroups data set3, which contains 20,000 messages (7.4 million words) between February and June 1993 taken from 20 2from Jensens inequality." ></td>
	<td class="line x" title="83:154	3http://www-2.cs.cmu.edu/afs/cs.cmu.edu/ project/theo-20/www/data/news20.html Usenet newsgroups, as the background data set, and another 20,000 messages (4 million words) between June and September 2002 taken from rec.arts.movies.current-films newsgroup as the foreground data set." ></td>
	<td class="line x" title="84:154	Each messages subject header and the body of the message (including quoted text) is tokenized into lowercase tokens on both data set." ></td>
	<td class="line x" title="85:154	No stemming is applied." ></td>
	<td class="line x" title="86:154	6.2 Finding key-bigrams The first experiment we show is to find key-bigrams, which is the simplest case requiring combination of phraseness and informativeness scores." ></td>
	<td class="line x" title="87:154	Figure 4 outlines the extraction procedure." ></td>
	<td class="line x" title="88:154	a10 Inputs: foreground and background corpus." ></td>
	<td class="line x" title="89:154	1." ></td>
	<td class="line x" title="90:154	create background language model from the background corpus." ></td>
	<td class="line x" title="91:154	2." ></td>
	<td class="line x" title="92:154	count all adjacent word pairs in the foreground corpus, skipping pre-annotated boundaries (such as HTML tag boundaries) and stopwords." ></td>
	<td class="line x" title="93:154	3." ></td>
	<td class="line x" title="94:154	for each pair of words (x,y) in the count, calculate phraseness froma11a13a12a15a14a17a16a19a18a21a20 fg and a11a13a12a15a14a22a20 fga11a17a12a15a18a23a20 fg and informativeness from a11a17a12a15a14a17a16a24a18a21a20 fg and a11a17a12a15a14a17a16a24a18a21a20 bg." ></td>
	<td class="line x" title="95:154	Add the two score values as the unified score." ></td>
	<td class="line x" title="96:154	4." ></td>
	<td class="line x" title="97:154	sort the results by the unified score." ></td>
	<td class="line x" title="98:154	a10 Output: a list of key-bigrams ranked by unified score." ></td>
	<td class="line x" title="99:154	Figure 4: Procedure to find key-bigrams For this experiment we used unsmoothed count for calculating phraseness a11a29a9a54a43 a13a45a44a59a15 a21 a51a53a9a54a43 a13a45a44a59a15 a23 a95, a11a29a9a54a66 a15 a21 a51a53a9a54a66 a15 a23 a95 where a95 a21 a62 a21 a51a53a9a54a43 a15 a21 a62 a21 a25a26 a51a60a9a54a43 a13a45a44a55a15, and used the unigram model for calculating informativeness with Katz smoothing (Chen and Goodman, 1996)4 to handle zero occurrences." ></td>
	<td class="line x" title="100:154	Figure 5 shows the extracted key-bigrams using this method." ></td>
	<td class="line x" title="101:154	Comparing to Figure 2, you can see that those two methods extract almost identical ranked phrases." ></td>
	<td class="line x" title="102:154	Note that we needed to tune three parameters to combine phraseness and informativeness in BLRT, but no parameter tuning was required in this method." ></td>
	<td class="line x" title="103:154	The reason why message news becomes the top phrase in both methods is that it appears frequently enough in message citation headers such 4with cutoff a27a29a28a31a30 1 message news 2 minority report 3 star wars 4 john harkness 5 robert frenchu 6 derek janssen 7 box office 8 sean ohara 9 dawn taylor 10 anthony gaza 11 star trek 12 ancient race 13 home.attbi.com hey 14 scooby doo 15 austin powers 16 hey kids 17 years ago 18 gaza man 19 sixth sense 20 lee harrison 21 julia roberts 22 national guard 23 bourne identity 24 metrotoday www.zap2it.com 25 starweek magazine 26 eric chomko 27 wilner starweek 28 tim gueguen 29 jodie foster 30 kevin filmnutboy Figure 5: Key-bigrams extracted with pointwise KL as John Smith a0 js@foo.coma1 wrote in message news:1pk0a@foo.com, which was not common in the 20 newsgroup dataset.5 A more sophisticated document analysis tool to remove citation headers is required to improve the quality further." ></td>
	<td class="line x" title="104:154	Figure 6 shows the distribution of phraseness and informativeness scores of bigrams extracted using the BLRT and pointwise KL methods." ></td>
	<td class="line x" title="105:154	One can see that there is little correlation between phraseness and informativeness in both ranking methods." ></td>
	<td class="line x" title="106:154	Also note that the range of x and y axis is very different in BLRT, but in the pointwise KL method they are comparable ranges." ></td>
	<td class="line x" title="107:154	That makes combining two scores easy in the pointwise KL approach." ></td>
	<td class="line x" title="108:154	6.3 Ranking n-length phrases The next example is ranking a3 -length phrases." ></td>
	<td class="line x" title="109:154	We applied a phrase extension algorithm based on the APriori algorithm (Agrawal and Srikant, 1994) to the output of the key-bigram finder in the previous example to generate a3 -length candidates whose frequency is greater than 5, then applied a linguistic filter which rejects phrases that do not occur in valid noun-phrase contexts (e.g. following articles or possessives) at least once in the corpus." ></td>
	<td class="line x" title="110:154	We ranked resulting phrases using pointwise KL score, using the same smoothing method as in the bigram case." ></td>
	<td class="line x" title="111:154	Figure 7 shows the result of re-ranking keyphrases extracted from the same movie corpus." ></td>
	<td class="line x" title="112:154	We can see that bigrams and trigrams are interleaved in natural order (although not many long phrases are extracted from the dataset, since longer NP did not occur more than five times)." ></td>
	<td class="line x" title="113:154	Figure 1 was another example of the result of the same pipeline of methods." ></td>
	<td class="line x" title="114:154	5a popular citation pattern in 1993 was In article a0 1pk0a@foo.coma1, js@foo.com (John Smith) writes: One question that might be asked is what if we just sort by frequency?." ></td>
	<td class="line x" title="115:154	If we sort by frequency, blair witch project is 92nd and empire strikes back is 110th on the ranked list." ></td>
	<td class="line x" title="116:154	Since the longer the phrase becomes, the lower the frequency of the phrase is, frequency is not an appropriate method for ranking phrases." ></td>
	<td class="line x" title="117:154	1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind Figure 7: Result of re-ranking output from the phrase extension module 6.4 Revisiting unigram informativeness An alternative approach to calculate informativeness from the foreground LM and the background LM is just to take the ratio of likelihood scores, a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 . This is a smoothed version of relative frequency ratio which is commonly used to find subject-specific terms (Damerau, 1993)." ></td>
	<td class="line x" title="118:154	Figure 8 compares extracted keywords ranked with pointwise KL and likelihood ratio scores, both of which use the same foreground and background unigram language model." ></td>
	<td class="line x" title="119:154	We used messages retrieved from the query Infiniti G35 as the foreground corpus and the same 20 newsgroup data as the background corpus." ></td>
	<td class="line x" title="120:154	Katz smoothing is applied to both language models." ></td>
	<td class="line x" title="121:154	As we can see, those two methods return very different ranked lists." ></td>
	<td class="line x" title="122:154	We think the pointwise KL returns a set of keywords closer to human judgment." ></td>
	<td class="line x" title="123:154	One example is the word infiniti, which we expected to be one of the informative words since it is the query word." ></td>
	<td class="line x" title="124:154	The pointwise KL score picked the word as the third informative word, but the likelihood score missed it." ></td>
	<td class="line x" title="125:154	Whereas 6mt, picked up by the likelihood ratio, which occurs 37 times in the 0 100000 200000 300000 400000 500000 600000 700000 800000 0 2000 4000 6000 8000 10000 12000 14000 16000 18000 20000 informativeness phraseness blrt -0.001 -0.0005 0 0.0005 0.001 0.0015 0.002 -0.0005 0 0.0005 0.001 0.0015 0.002 informativeness phraseness pointKR (a) BLRT (b) LM-pointKL Figure 6: Phraseness and informativeness score of bigrams extracted with BLRT (a) and pointwise KL divergence between LMs (b)." ></td>
	<td class="line x" title="126:154	point KL likelihood ratio rank freq term freq term 1 1599 g35 1599 g35 2 1145 car 156 330i 3 450 infiniti 117 350z 4 299 coupe 113 doo 5 299 nissan 90 wrx 6 383 bmw 76 is300 7 156 330i 47 willow 8 441 cars 39 rsx 9 248 sedan 37 6mt 10 331 originally 35 scooby 11 201 altima 35 s2000 12 117 350z 33 gt-r 13 113 doo 32 lol 14 235 sport 30 heatwave 15 172 maxima 28 g22 16 90 wrx 26 gtr 17 111 skyline 23 g21 18 76 is300 23 g17 19 186 honda 23 nsx 20 221 engine 22 tl-s Figure 8: Top 20 keywords extracted using pointwise-KL and likelihood ratio (after stopwords removed) from messages retrieved from the query Infiniti G35 foreground corpus and none in the background corpus does not seem to be a good keyword." ></td>
	<td class="line x" title="127:154	The following table shows statistics of those two words:6 token a11 fga9a54a66 a15 a11 bga9a54a66 a15 PKL LR 6mt 1.837E-4 8.705E-8 0.0020 2110 infiniti 2.269E-3 4.475E-6 0.0204 506 Since the likelihood of 6mt with respect to the background LM is so small, the likelihood ratio of the word becomes very large." ></td>
	<td class="line x" title="128:154	But the pointwise KL score discounts the score appropriately by consider6infiniti occurs 34 times in the rec.autos section of the 20 newsgroup data set." ></td>
	<td class="line x" title="129:154	ing that the frequency of the word is low." ></td>
	<td class="line x" title="130:154	Likelihood ratio (or relative frequency ratio) has a tendency to pick up rare words as informative." ></td>
	<td class="line x" title="131:154	Pointwise KL seems more robust in sparse data situations." ></td>
	<td class="line x" title="132:154	One disadvantage of the pointwise KL statistic might be that it also picks up stopwords or punctuation, when there is a significant difference in style of writing, etc. , since these words have significantly high frequency." ></td>
	<td class="line x" title="133:154	But stopwords are easy to define or can be generated automatically from corpora, and we dont consider this to be a significant drawback." ></td>
	<td class="line x" title="134:154	We also expect a better background model and better smoothing mechanism could reduce the necessity of the stopword list." ></td>
	<td class="line x" title="135:154	7 Discussion Necessity of both phraseness and informativeness Although phraseness itself is domain-dependent to some extent (Smadja, 1994), we have shown that there is little correlation between informativeness and phraseness scores." ></td>
	<td class="line x" title="136:154	Combining method One way to calculate a combined score is directly comparing a7 a1 a2fg and a7 a1 a1bg in Figure 3." ></td>
	<td class="line x" title="137:154	We have tried both approaches and got a better result from combining separate phraseness and informativeness scores." ></td>
	<td class="line x" title="138:154	We think this is due to data sparseness of the higher order ngram in the background corpus." ></td>
	<td class="line x" title="139:154	Further investigation is required to make a conclusion." ></td>
	<td class="line x" title="140:154	We have used the simplest method of combining two scores by adding them." ></td>
	<td class="line x" title="141:154	We have also tried harmonic mean and geometric mean but they did not improve the result." ></td>
	<td class="line x" title="142:154	We could also apply linear interpolation to put more weight on one score value, or use an exponential model to combine score, but this will require tuning parameters." ></td>
	<td class="line x" title="143:154	Benefits of using a language model One benefit of using a language model approach is that one can take advantage of various smoothing techniques." ></td>
	<td class="line x" title="144:154	For example, by interpolating with a character-based n-gram model, we can make the LM more robust with respect to spelling errors and variations." ></td>
	<td class="line x" title="145:154	Consider the following variations, which we need to treat as a single entity: al-Qaida, al Qaida, al Qaeda, al Queda, al-Qaeda, al-Qaida, al Qaida (found in online sources)." ></td>
	<td class="line x" title="146:154	Since these are such unique spellings in English, character n-gram is expected to be able to give enough likelihood score to different spellings as well." ></td>
	<td class="line x" title="147:154	It is also easy to incorporate other models such as topic or discourse model, use a cache LM to capture local context, and a class-based LM for the shared concept." ></td>
	<td class="line x" title="148:154	It is also possible to add a phrase length prior probability in the model for better likelihood estimation." ></td>
	<td class="line x" title="149:154	Another useful smoothing technique is linear interpolation of the foreground and background language models, when the foreground and background corpus are disjoint." ></td>
	<td class="line x" title="150:154	8 Conclusion We have explained that phraseness and informativeness should be unified into a single score to return useful ranked keyphrases for analysts." ></td>
	<td class="line x" title="151:154	Our proposed approach calculates both scores based on language models and unified into a single score." ></td>
	<td class="line x" title="152:154	The phrases generated by this method are intuitively very useful, but the results are difficult to evaluate quantitatively." ></td>
	<td class="line x" title="153:154	In future work we would like to further explore evaluation of keyphrases, as well as investigate different smoothing techniques." ></td>
	<td class="line x" title="154:154	Further extensions include developing a phrase boundary segmentation algorithm based on this framework and exploring applicability to other languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1105
Bilingual-Dictionary Adaptation To Domains
Kaji, Hiroyuki;"></td>
	<td class="line x" title="1:171	Bilingual-Dictionary Adaptation to Domains Hiroyuki Kaji Central Research Laboratory, Hitachi, Ltd. 1-280 Higashi-Koigakubo, Kokubunji-shi, Tokyo 185-8601, Japan kaji@crl.hitachi.co.jp Abstract Two methods using comparable corpora to select translation equivalents appropriate to a domain were devised and evaluated." ></td>
	<td class="line x" title="2:171	The first method ranks translation equivalents of a target word according to similarity of their contexts to that of the target word." ></td>
	<td class="line x" title="3:171	The second method ranks translation equivalents according to the ratio of associated words that suggest them." ></td>
	<td class="line x" title="4:171	An experiment using the EDR bilingual dictionary together with Wall Street Journal and Nihon Keizai Shimbun corpora proved that the method using the ratio of associated words outperforms the method based on contextual similarity." ></td>
	<td class="line x" title="5:171	Namely, in a quantitative evaluation using pseudo words, the maximum F-measure of the former method was 86%, while that of the latter method was 82%." ></td>
	<td class="line x" title="6:171	1 Introduction It is well known that appropriate translations for a word vary with domains, and bilingual-dictionary adaptation to domains is an effective way to improve the performance of, for example, machine translation and cross-language information retrieval." ></td>
	<td class="line x" title="7:171	However, bilingual dictionaries have commonly been adapted to domains on the basis of lexicographers intuition." ></td>
	<td class="line x" title="8:171	It is thus desirable to develop an automated method for bilingual-dictionary adaptation." ></td>
	<td class="line x" title="9:171	Technologies for extracting pairs of translation equivalents from parallel corpora have been established (Gale and Church 1991; Dagan, et al. 1993; Fung 1995; Kitamura and Matsumoto 1996; Melamed 1997)." ></td>
	<td class="line x" title="10:171	They can, naturally, be used to adapt a bilingual dictionary to domains, that is, to select corpus-relevant translation equivalents from among those provided by an existing bilingual dictionary." ></td>
	<td class="line x" title="11:171	However, their applicability is limited because of the limited availability of large parallel corpora." ></td>
	<td class="line x" title="12:171	Methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., a pair of two language corpora of the same domain, are therefore required." ></td>
	<td class="line x" title="13:171	There are a number of previous works related to bilingual-dictionary adaptation using comparable corpora." ></td>
	<td class="line x" title="14:171	Tanaka and Iwasakis (1996) optimization method for a translation-probability matrix mainly aims at adapting a bilingual dictionary to domains." ></td>
	<td class="line x" title="15:171	However, it is hampered by a huge amount of computation, and was only demonstrated in a small-scale experiment." ></td>
	<td class="line x" title="16:171	Several researchers have developed a contextual-similarity-based method for extracting pairs of translation equivalents (Kaji and Aizono 1996; Fung and McKeown 1997; Fung and Yee 1998; Rapp 1999)." ></td>
	<td class="line x" title="17:171	It is computationally efficient compared to Tanaka and Iwasakis method, but the precision of extracted translation equivalents is still not acceptable." ></td>
	<td class="line x" title="18:171	In the light of these works, the author proposes two methods for bilingual-dictionary adaptation." ></td>
	<td class="line x" title="19:171	The first one is a variant of the contextual-similarity-based method for extracting pairs of translation equivalents; it focuses on selecting corpus-relevant translation equivalents from among those provided by a bilingual dictionary." ></td>
	<td class="line x" title="20:171	This selecting may be easier than finding new pairs of translation equivalents." ></td>
	<td class="line x" title="21:171	The second one is a newly devised method using the ratio of associated words that suggest each translation equivalent; it was inspired by a research on word-sense disambiguation using bilingual comparable corpora (Kaji and Morimoto 2002)." ></td>
	<td class="line x" title="22:171	The two methods were evaluated and compared by using the EDR (Japan Electronic Dictionary Research Institute) bilingual dictionary together with Wall Street Journal and Nihon Keizai Shimbun corpora." ></td>
	<td class="line x" title="23:171	2 Method based on contextual similarity This method is based on the assumption that a word in a language and its translation equivalent in another language occur in similar contexts, albeit their contexts are represented by words in their respective languages." ></td>
	<td class="line x" title="24:171	In the case of the present task (i.e. , bilingual-dictionary adaptation), a bilingual dictionary provides a set of candidate translation equivalents for each target word 1." ></td>
	<td class="line x" title="25:171	The contextual similarity of each of the candidate translation equivalents to the target word is thus evaluated with the assistance of the bilingual dictionary, and a predetermined number of translation equivalents are selected in descending order of contextual similarity." ></td>
	<td class="line x" title="26:171	Note that it is difficult to preset a threshold for contextual similarity since the distribution of contextual similarity values varies with target words." ></td>
	<td class="line x" title="27:171	1 In this paper, target word is used to indicate the word for which translation equivalents are to be selected." ></td>
	<td class="line x" title="28:171	A flow diagram of the proposed method is shown in Figure 1." ></td>
	<td class="line x" title="29:171	The essential issues regarding this method are described in the following." ></td>
	<td class="line o" title="30:171	Word associations are extracted by setting a threshold for mutual information between words in the same language." ></td>
	<td class="line oc" title="31:171	The mutual information of a pair of words is defined in terms of their co-occurrence frequency and respective occurrence frequencies (Church and Hanks 1990)." ></td>
	<td class="line x" title="32:171	A medium-sized window, i.e., a window including a few-dozen words, is used to count co-occurrence frequencies." ></td>
	<td class="line x" title="33:171	Only word associations consisting of content words are extracted." ></td>
	<td class="line x" title="34:171	This is because function words neither have domain-dependent translation equivalents nor represent contexts." ></td>
	<td class="line x" title="35:171	Both a target word and each of its candidate translation equivalents are characterized by context vectors." ></td>
	<td class="line o" title="36:171	A context vector consists of associated words weighted with mutual information." ></td>
	<td class="line x" title="37:171	Similarity of a candidate translation equivalent to a target word is defined as the cosine coefficient between the context vector characterizing the target word and the translated context vector characterizing the candidate translation equivalent as follows." ></td>
	<td class="line x" title="38:171	Under the assumption that target word x and candidate translation equivalent y are characterized by first-language context vector a(x) = (a 1 (x), a 2 (x), , a m (x)) and second-language context vector b(y) = (b 1 (y), b 2 (y), , b n (y)), respectively, b(y) is translated into a first-language vector denoted as a'(y) = (a' 1 (y), a' 2 (y), , a' m (y))." ></td>
	<td class="line x" title="39:171	That is, m),1,2,()()(, n,1,2, L L == = iybmaxy'a jji j i , where  i,j =1 if the j-th element of b(y) is a translation of the i-th element of a(x); otherwise,  i,j =0." ></td>
	<td class="line x" title="40:171	Elements of b(y) that cannot be translated into elements of a'(y) constitute a residual second-language vector, denoted as b'(y) = (b' 1 (y), b' 2 (y), , b' n (y))." ></td>
	<td class="line x" title="41:171	That is, .j yb yb' m i jij j n),1,2,( otherwise0 0)( )( 1, L L L =      = =  =  The similarity of candidate translation equivalent y to target word x is then defined as ))()(),(()( yyxcosy,xSim b'a'a += . Note that a'(y)+b'(y) is a concatenation of a'(y) and b'(y) since they have no elements in common." ></td>
	<td class="line x" title="42:171	3 Method using the ratio of associated words 3.1 Outline This method is based on the assumption that each word associated with a target word suggests a specific sense of the target word, in other words, specific translation equivalents of the target word." ></td>
	<td class="line x" title="43:171	It is also assumed that dominance of a translation equivalent in a domain correlates with how many associated words suggesting it occur in a corpus of the domain." ></td>
	<td class="line x" title="44:171	It is thus necessary to identify which associated words suggest which translation equivalents." ></td>
	<td class="line x" title="45:171	This can be done by using the sense-vs.-clue correlation algorithm that the author developed for unsupervised word-sense disambiguation (Kaji and Morimoto 2002)." ></td>
	<td class="line x" title="46:171	The algorithm works with a set of senses of a target word, each of which is defined as a set of synonymous translation equivalents, and it results in a correlation matrix of senses vs. clues (i.e. , associated words)." ></td>
	<td class="line x" title="47:171	It is used here with a set of translation equivalents instead of a set of senses, resulting in a correlation matrix of translation equivalents vs. associated words." ></td>
	<td class="line x" title="48:171	The proposed method consists of the following steps (as shown in Figure 2)." ></td>
	<td class="line x" title="49:171	First, word associations are extracted from a corpus of each language." ></td>
	<td class="line x" title="50:171	The first step is the same as that of the contextual-similarity-based method described in Section 2." ></td>
	<td class="line x" title="51:171	Second, word associations are aligned translingually by consulting a bilingual dictionary, and pairwise correlation between translation equivalents of a target word and its associated words is calculated iteratively." ></td>
	<td class="line x" title="52:171	A detailed description of this step is given in the following subsection." ></td>
	<td class="line x" title="53:171	Third, each associated word is assigned to the translation equivalent having the highest correlation with it." ></td>
	<td class="line x" title="54:171	This procedure may be problematic, since an associated word often suggests two or more translation equivalents that represent the same sense." ></td>
	<td class="line x" title="55:171	However, it is difficult to separate translation equivalents suggested by an associated word from others." ></td>
	<td class="line x" title="56:171	Each associated word is therefore assigned to the translation equivalent it suggests most strongly." ></td>
	<td class="line x" title="57:171	Finally, a translation equivalent is selected when 1st-language corpus Extract word associationsExtract word associations 1st-language word associations 2nd-language word associations 2nd-language corpus Construct context vector Construct context vectors Original bilingual dictionary Translate context vectors Translated context vectors Calculate contextual similarity of candidate translation equivalents to target word Select N most-similar translation equivalents Adapted bilingual dictionary Context vectors characterizing candidate translation equivalents Context vector characterizing target word Figure 1: Bilingual-dictionary adaptation based on contextual similarity the ratio of associated words assigned to it exceeds a certain threshold." ></td>
	<td class="line x" title="58:171	In addition, representative associated words are selected for each selected translation equivalent." ></td>
	<td class="line x" title="59:171	A representativeness measure was devised under the assumption that representative associated words are near the centroid of a cluster consisting of associated words assigned to a translation equivalent." ></td>
	<td class="line x" title="60:171	The representative associated words help lexicographers validate the selected translation equivalents." ></td>
	<td class="line x" title="61:171	3.2 Calculation of correlation between translation equivalents and associated words The iterative algorithm described below has two main features." ></td>
	<td class="line x" title="62:171	First, it overcomes the problem of failure in word-association alignment due to incompleteness of the bilingual dictionary and disparity in topical coverage between the corpora of the two languages." ></td>
	<td class="line x" title="63:171	Second, it overcomes the problem of ambiguity in word-association alignment." ></td>
	<td class="line x" title="64:171	3.2.1 Alignment of word associations For a first-language word association (x, x(j))where a target word is given as x and its j-th associated word is given as x(j)a set consisting of second-language word associations alignable with it, denoted as Y(x, x(j)), is constructed." ></td>
	<td class="line x" title="65:171	That is, Y(x, x(j)) = {(y, y) | (y, y)R 2, (x, y)D, (x(j), y)D}, where R 2 is the collection of word associations extracted from a corpus of the second language, and D is a bilingual dictionary to be adapted." ></td>
	<td class="line x" title="66:171	Each first-language word association (x, x(j)) is characterized by a set consisting of accompanying associated words, denoted as Z(x, x(j))." ></td>
	<td class="line x" title="67:171	An accompanying associated word is a word that is associated with both words making up the word association in question." ></td>
	<td class="line x" title="68:171	That is, Z(x, x(j)) = {x | (x, x)R 1, (x(j), x)R 1 }, where R 1 is the collection of word associations extracted from a corpus of the first language." ></td>
	<td class="line x" title="69:171	In addition, alignment of a first-language word association (x, x(j)) with a second-language word association (y, y) (Y(x, x(j))) is characterized by a set consisting of translingually alignable accompanying associated words, denoted as W((x, x(j)), (y, y))." ></td>
	<td class="line x" title="70:171	A translingually alignable accompanying associated word is a word that is an accompanying associated word of the first-language word association making up the alignment in question and, at the same time, is alignable with an accompanying associated word of the second-language word association making up the alignment in question." ></td>
	<td class="line x" title="71:171	That is, W((x, x(j)), (y, y)) = Z(x, x(j))  {x |  y(V(y, y)) (x, y)D}, where V(y, y) = {y | (y, y)R 2, (y, y)R 2 }." ></td>
	<td class="line o" title="72:171	3.2.2 Iterative calculation of correlation The correlation between the i-th translation equivalent of target word x, denoted as y(i), and the j-th associated word x(j) is defined as ()() ( ) (), jx',kyPLmax jx',iyPL jx',xMIjx',iyC k )()( )()( )()()( = where MI(x, x(j)) is the mutual information between x and x(j), and PL(y(i), x(j)) is the plausibility factor for y(i) given by x(j)." ></td>
	<td class="line o" title="73:171	The mutual information between the target word and the associated word is the base of the correlation between each translation equivalent of the target word and the associated word; it is multiplied by the normalized plausibility factor." ></td>
	<td class="line x" title="74:171	The plausibility factor is defined as the weighted sum of two component plausibility factors." ></td>
	<td class="line x" title="75:171	That is, ()()(),jx',iyPLjx',iyPLjx',iyPL )()()()()()( 21 += where  is a parameter adjusting the relative weights of the component plausibility factors." ></td>
	<td class="line x" title="76:171	The first component plausibility factor, PL 1, is defined as the sum of correlations between the translation equivalent and the accompanying associated words." ></td>
	<td class="line x" title="77:171	That is, () ().x',iyCjx',iyPL jx',xx'   = ))(Z( 1 )()()( This is based on the assumption that an associated Target word Extract word associations 1st-language corpus Original bilingual dictionary 2nd-language corpus Extract word associations 2nd-language word associations 1st-language word associations Calculate correlation between translation equivalents and associated words Assign each associated word to the translation equivalent having the highest correlation with it Binary matrix of translation equivalents vs. associated words Representative associated words Aligned word associations Correlation matrix of translation equivalents vs. associated words Align word associations Candidate translation equivalents Adapted bilingual dictionary Select representative associated words Select translation equivalents to which more than a certain percentage of associated words are assigned Figure 2: Bilingual-dictionary adaptation using the ratio of associated words word usually correlates closely with the translation equivalent that correlates closely with a majority of its accompanying associated words." ></td>
	<td class="line o" title="78:171	The second component plausibility factor, PL 2, is defined as the maximum plausibility of alignment involving the translation equivalent, where the plausibility of alignment of a first-language word association with a second-language word association is defined as the mutual information of the second-language word association multiplied by the sum of correlations between the translation equivalent and the translingually alignable accompanying associated words." ></td>
	<td class="line x" title="79:171	That is, () ().x'iyCy'iyMImax jx'iyPL y'iyjx'xWx' jx'xYy'iy         =    ))),(()),(,(( ))(,()),(( 2 ),()),(( )(),( This is based on the assumption that correct alignment of word associations is usually accompanied by many associated words that are alignable with each other as well as the assumption that alignment with a strong word association is preferable to alignment with a weak word association." ></td>
	<td class="line x" title="80:171	The above definition of the correlations between translation equivalents and associated words is recursive, so they can be calculated iteratively." ></td>
	<td class="line x" title="81:171	Initial values are set as C 0 (y(i), x(j)) = MI(x, x(j))." ></td>
	<td class="line o" title="82:171	That is, the mutual information between the target word and an associated word is used as the initial value for the correlations between all translation equivalents of the target word and the associated word." ></td>
	<td class="line x" title="83:171	It was proved experimentally that the algorithm works well for a wide range of values of parameter  and that the correlation values converge rapidly." ></td>
	<td class="line x" title="84:171	Parameter  and the number of iterations were set to five and six, respectively, in the experiments described in Section 4." ></td>
	<td class="line x" title="85:171	4 Experiments 4.1 Material and preparation The experiment focused on nouns, whose appropriate translations often vary with domains." ></td>
	<td class="line x" title="86:171	A wide-coverage bilingual noun dictionary was constructed by collecting pairs of nouns from the EDR English-to-Japanese and Japanese-to-English dictionaries." ></td>
	<td class="line x" title="87:171	The resulting dictionary consists of 633,000 pairs of 269,000 English nouns and 276,000 Japanese nouns." ></td>
	<td class="line x" title="88:171	An English corpus consisting of Wall Street Journal articles (July 1994 to December 1995; 189MB) and a Japanese corpus consisting of Nihon Keizai Shimbun articles (December 1993 to November 1994; 275MB) were used as the comparable corpora." ></td>
	<td class="line x" title="89:171	English nouns occurring 10 or more times in the English corpus were selected as the target words." ></td>
	<td class="line x" title="90:171	The total number of selected target words was 12,848." ></td>
	<td class="line x" title="91:171	For each target word, initial candidate translation equivalents were selected from the bilingual dictionary in descending order of frequency in the Japanese corpus; the maximum number of candidates was set at 20, and the minimum frequency was set at 10." ></td>
	<td class="line x" title="92:171	The average number of candidate translation equivalents per target word was 3.3, and 1,251 target words had 10 or more candidate translation equivalents." ></td>
	<td class="line x" title="93:171	Extraction of word associations, which is the first step common to the method based on contextual similarity (abbreviated as the CS method hereinafter) and the method using the ratio of associated words (abbreviated as the RAW method hereinafter), was done as follows." ></td>
	<td class="line o" title="94:171	Co-occurrence frequencies of noun pairs were counted by using a window of 13 words, excluding function words, and then noun pairs having mutual information larger than zero were extracted." ></td>
	<td class="line x" title="95:171	Table 1: Example translation equivalents selected by the method based on contextual similarity Target word [Freq.]" ></td>
	<td class="line x" title="96:171	# Translation equivalent *) [Freq.]" ></td>
	<td class="line x" title="97:171	Similarity 1  (administration organ) [137] 0.127 2  (reign) [32] 0.119 3  (direction of domestic affairs) [2366] 0.116 4  (political power) [2370] 0.111 administration [2027] 5  (operation) [453] 0.111 1  (election campaign) [71] 0.067 2  (competition) [2608] 0.050 3  (aggressive activities) [561] 0.049 4  (movement) [947] 0.049 campaign [1656] 5  (military activities) [89] 0.040 1  (management) [4810] 0.116 2  (enterprise) [8735] 0.091 3  (conduct) [1431] 0.076 4  (tactics) [528] 0.074 operation [3469] 5  (function) [2721] 0.074 1  (energy) [913] 0.103 2  (force) [6276] 0.101 3  (majority) [1036] 0.101 4  (electric power) [1208] 0.079 power [2826] 5  (ability) [1254] 0.074 1  (husk) [135] 0.082 2  (ball) [137] 0.070 3  (cannonball) [32] 0.070 4  (ball) [1370] 0.062 shell [102] 6  (case) [4851] 0.060 1  (voice) [13536] 0.103 2  (target) [4676] 0.096 3  (business) [7163] 0.087 4  (indication) [215] 0.087 sign [4064] 5  (mark) [297] 0.084 * ) English translations other than target words are given in parentheses." ></td>
	<td class="line x" title="98:171	4.2 Experimental results Results of the CS and RAW methods for six target words are listed in Tables 1 and 2, respectively." ></td>
	<td class="line x" title="99:171	Table 1 lists the top-five translation equivalents in descending order of contextual similarity." ></td>
	<td class="line x" title="100:171	Table 2 lists translation equivalents with a ratio of associated words larger than 4% along with their top-four representative associated words." ></td>
	<td class="line x" title="101:171	In these tables, the occurrence frequencies in the test corpora are appended to both the target words and the translation equivalents." ></td>
	<td class="line x" title="102:171	These indicate the weak comparability between the Wall Street Journal and Nihon Keizai Shimbun corpora." ></td>
	<td class="line x" title="103:171	Moreover, it is clear that neither the CS method nor the RAW method relies on the occurrence frequencies of words." ></td>
	<td class="line x" title="104:171	Tables 1 and 2 clearly show that the two methods produce significantly different lists of translation equivalents." ></td>
	<td class="line x" title="105:171	It is difficult to judge the appropriateness of the results of the CS method without examining the comparable corpora." ></td>
	<td class="line x" title="106:171	However, it seems that inappropriate translation equivalents were often ranked high by the CS method." ></td>
	<td class="line x" title="107:171	In contrast, referring to the representative associated words enables the results of the RAW method to be judged as appropriate or inappropriate." ></td>
	<td class="line x" title="108:171	More than 90% of the selected translation equivalents were judged as definitely appropriate." ></td>
	<td class="line x" title="109:171	Table 2 also includes the orders of translation equivalents determined by a conventional bilingual dictionary (remarks column)." ></td>
	<td class="line x" title="110:171	They are quite different from the orders determined by the RAW method." ></td>
	<td class="line x" title="111:171	This shows the necessity and effectiveness of ranking translation equivalents according to relevancy to a domain." ></td>
	<td class="line x" title="112:171	Processing times were measured by separating both the CS and RAW methods into two parts." ></td>
	<td class="line x" title="113:171	The processing time of the first part shared by the two methods, i.e., extracting word associations from corpora, is roughly proportional to the corpus size." ></td>
	<td class="line x" title="114:171	For example, it took 2.80 hours on a Windows PC (CPU clock: 2.40 GHz; memory: 1 GB) to extract word associations from the 275 MB Japanese corpus." ></td>
	<td class="line x" title="115:171	The second part, i.e., selecting translation equivalents for target words, is specific to each method, and the processing time of it is proportional to the number of target words." ></td>
	<td class="line x" title="116:171	It took 11.5 minutes and 2.40 hours on another Windows PC (CPU clock: 2.40 GHz; memory: 512 MB) for the CS and RAW methods, respectively, to process the 12,848 target words." ></td>
	<td class="line x" title="117:171	It was thus proved that both the CS and RAW methods are computationally feasible." ></td>
	<td class="line x" title="118:171	4.3 Quantitative evaluation using pseudo target words 4.3.1 Evaluation method A method for bilingual-dictionary adaptation using comparable corpora should be evaluated by usTable 2: Example translation equivalents selected by the method using the ratio of associated words Target word [Freq.]" ></td>
	<td class="line x" title="119:171	# Translation equivalent *) [Freq.]" ></td>
	<td class="line x" title="120:171	Ratio Representative associated words Remarks **) 1  (cabinet) [1067] 0.419 House, Clinton, White House, Republican 3a 2  (political power) [2370] 0.236 U.S. official, Haiti, Haitian, Clinton administration 3a 3  (operation) [453] 0.147 GATT, fast-track, trade pact, Trade 4a administration [2027] 4  (control) [84] 0.058 China, U.S., import, Japan 1  (election campaign) [71] 0.612 Republican, candidate, GOP, Democrat 2a campaign [1656] 2  (aggressive activities) [561] 0.371 ad, advertise, brand, advertising 2a 1  (management) [4810] 0.788 Stock Exchange, last year, profit, loss 2b operation [3469] 2  (enterprise) [8735] 0.144 quarter, net, income, plant 1  (electric power) [1208] 0.434 electricity, power plant, utility, megawatt 8b 2  (influence) [826] 0.425 military, leader, President, Haiti 3 power [2826] 3  (authority) [909] 0.062 reform, law, Ukraine, amendment 5a 1  (cannonball) [32] 0.560 Serb, U.N., Sarajevo, NATO 4a 2  (shellfish) [100] 0.168 crab, fish, hermit crab, Mr. Soifer 1a 3  (ball) [137] 0.112 rupture, bacterium, implant, brain shell [102] 4  (external appearance) [267] 0.064 tape, camera, video, building 3a 1  (indication) [215] 0.568 inflation, interest rate, rate, economy 4a 2  (signboard) [566] 0.099 tourist, billboard, airport, exit 3b 3  (target) [4676] 0.086 accord, agreement, pact, treaty 4  (indication) [2396] 0.086 last year, month, demand, order sign [4064] 5  (signal) [231] 0.062 driver, accident, highway, motorist 2a * ) English translations other than target words are given in parentheses." ></td>
	<td class="line x" title="121:171	** ) This column shows the orders of translation equivalents determined by a conventional dictionary Kenkyushas New Collegiate English-Japanese Dictionary, 5th edition. For example, 3a indicates that a translation equivalent belongs to the subgroup a in the third group of translations." ></td>
	<td class="line x" title="122:171	A hyphen indicates that a translation equivalent is not contained in the dictionary." ></td>
	<td class="line x" title="123:171	ing recall and precision measures defined as, T TS ecisionPrand S TS callRe  =  = where S is a set consisting of pairs of translation equivalents contained in the test comparable corpora, and T is a set consisting of pairs of translation equivalents selected by the method." ></td>
	<td class="line x" title="124:171	To calculate these measures, it is necessary to know all pairs of translation equivalents contained in the test corpora." ></td>
	<td class="line x" title="125:171	This is almost impossible in the case that the test corpora are large." ></td>
	<td class="line x" title="126:171	To avoid this difficulty, an automated evaluation scheme using pseudo target words was devised." ></td>
	<td class="line x" title="127:171	A pseudo word is formed by three real words, and it has three distinctive pseudo senses corresponding to the three constituent words." ></td>
	<td class="line x" title="128:171	Translation equivalents of a constituent word are regarded as candidate translation equivalents of the pseudo word that represent the pseudo sense corresponding to the constituent word." ></td>
	<td class="line x" title="129:171	For example, a pseudo word action/address/application has three pseudo senses corresponding to action, address, and application. It has candidate translation equivalents such as <SOSHOU> and <KETSUGI> originating from action,  <ENZETSU> and   <SEIGAN> originating from address, and  <OUYOU> and <OUBO> originating from application. Furthermore, pseudo word associations are produced by combining a pseudo word with each of the associated words of the first two constituent words." ></td>
	<td class="line x" title="130:171	It is thus assumed that first two pseudo senses occur in the corpora but the third one does not." ></td>
	<td class="line x" title="131:171	For example, the pseudo word action/address/application has associated words including court and vote, which are associated with action, as well as President and legislation, which are associated with address. Using the pseudo word associations, a bilingual-dictionary-adaptation method selects translation equivalents for the pseudo target word." ></td>
	<td class="line x" title="132:171	On the one hand, when at least one of the translation equivalents originating from the first (second) constituent word is selected, it means that the first (second) pseudo sense is successfully selected." ></td>
	<td class="line x" title="133:171	For example, when  <SOSHOU> is selected as a translation equivalent for the pseudo target word action/address/application, it means that the pseudo sense corresponding to action is successfully selected." ></td>
	<td class="line x" title="134:171	On the other hand, when at least one of translation equivalents originating from the third constituent word is selected, it means that the third pseudo sense is erroneously selected." ></td>
	<td class="line x" title="135:171	For example, when <OUYOU> is selected as a translation equivalent for the pseudo target word action/address/application, it means that the pseudo sense corresponding to application is erroneously selected." ></td>
	<td class="line x" title="136:171	The method is thus evaluated by recall and precision of selecting pseudo senses." ></td>
	<td class="line x" title="137:171	That is,, 'T 'T'S ecisionPrand 'S 'T'S callRe  =  = where S is a set consisting of pseudo senses corresponding to the first two constituent words, and T is a set consisting of pseudo senses relevant to translation equivalents selected by the method." ></td>
	<td class="line x" title="138:171	4.3.2 Evaluation results A total of 1,000 pseudo target words were formed by using randomly selected words that occur more than 100 times in the Wall Street Journal corpus." ></td>
	<td class="line x" title="139:171	Using these pseudo target words, both the CS and RAW methods were evaluated." ></td>
	<td class="line x" title="140:171	As for the CS method, the recall and precision of selecting pseudo senses were calculated in the case that N most-similar translation equivalents are selected (N=2, 3,)." ></td>
	<td class="line x" title="141:171	As for the RAW method, the recall and precision of selecting pseudo senses were calculated in the case that the threshold for the ratio of associated words is set from 20% down to 1% in 1% intervals." ></td>
	<td class="line x" title="142:171	Recall vs. precision curves for the two methods are shown in Figure 3." ></td>
	<td class="line x" title="143:171	These curves clearly show that the RAW method outperforms the CS method." ></td>
	<td class="line x" title="144:171	The RAW method maximizes the F-measure, i.e., harmonic means of recall and precision, when the threshold for the ratio of associated words is set at 4%; the recall, precision, and F-measure are 92%, 80%, and 86%, respectively." ></td>
	<td class="line x" title="145:171	In contrast, the CS method maximizes the F-measure when N is set at nine; the recall, precision, and F-measure are 96%, 72%, and 82%, respectively." ></td>
	<td class="line x" title="146:171	It should be mentioned that the above evaluation was done under strict conditions." ></td>
	<td class="line x" title="147:171	That is, two out of three pseudo senses of each pseudo target word were assumed to occur in the corpus, while many real target words have only one sense in a specific domain." ></td>
	<td class="line x" title="148:171	Target words with only one sense occurring in a corpus are generally easier to cope with than those with multiple senses occurring in a corpus." ></td>
	<td class="line x" title="149:171	Accordingly, recall and precision for real target words would be higher than the above ones for the pseudo target words." ></td>
	<td class="line x" title="150:171	0.5 0.6 0.7 0.8 0.9 1.0 0.5 0.6 0.7 0.8 0.9 1.0 Recall Pre c ision CS RAW Figure 3: Recall and precision of selecting pseudo senses 5 Discussion The reasons for the superior performance of the RAW method to the CS method are discussed in the following." ></td>
	<td class="line x" title="151:171	 The RAW method overcomes both the sparseness of word-association data and the topical disparity between corpora of two languages." ></td>
	<td class="line x" title="152:171	This is due to the smoothing effects of the iterative algorithm for calculating correlation between translation equivalents and associated words; namely, associated words are correlated with translation equivalents even if they fail to be aligned with their counterpart." ></td>
	<td class="line x" title="153:171	In contrast, the CS method is much affected by the above-mentioned difficulties." ></td>
	<td class="line x" title="154:171	All low values of contextual similarity (see Table 1) support this fact." ></td>
	<td class="line x" title="155:171	 The RAW method assumes that a target word has more than one sense, and, therefore, it is effective for polysemous target words." ></td>
	<td class="line x" title="156:171	In contrast, contextual similarity is ineffective for a target word with two or more senses occurring in a corpus." ></td>
	<td class="line x" title="157:171	The context vector characterizing such a word is a composite of context vectors characterizing respective senses; therefore, the context vector characterizing any candidate translation equivalent does not show very high similarity." ></td>
	<td class="line x" title="158:171	 The RAW method can select an appropriate number of translation equivalents for each target word by setting a threshold for the ratio of associated words." ></td>
	<td class="line x" title="159:171	In contrast, the CS method is forced to select a fixed number of translation equivalents for all target words; it is difficult to predetermine a threshold for the contextual similarity, since the range of its values varies with target words (see Table 1)." ></td>
	<td class="line x" title="160:171	Finally, from a practical point of view, advantages of the RAW method are discussed in the following." ></td>
	<td class="line x" title="161:171	 The RAW method selects translation equivalents contained in the comparable corpora of a domain together with evidence, i.e., representative associated words that suggest the selected translation equivalents." ></td>
	<td class="line x" title="162:171	Accordingly, it allows lexicographers to check the appropriateness of selected translation equivalents efficiently." ></td>
	<td class="line x" title="163:171	 The ratio of associated words can be regarded as a rough approximation of a translation probability." ></td>
	<td class="line x" title="164:171	Accordingly, a translation equivalent can be fixed for a word, when the particular translation equivalent has an exceedingly large ratio of associated words." ></td>
	<td class="line x" title="165:171	A sophisticated procedure for word-sense disambiguation or translation-word selection needs to be applied only to words whose two or more translation equivalents have significant ratios of associated words." ></td>
	<td class="line x" title="166:171	6 Conclusion The method using the ratio of associated words was proved to be effective, while the method based on contextual similarity was not." ></td>
	<td class="line x" title="167:171	The former method has the following features that make it practical." ></td>
	<td class="line x" title="168:171	First, is uses weakly comparable corpora, which are available in many domains." ></td>
	<td class="line x" title="169:171	Second, it selects translation equivalents together with representative associated words that suggest them, enabling the translation equivalents to be validated." ></td>
	<td class="line x" title="170:171	The method will be applied to several domains, and its effect on the performance of application systems will be evaluated." ></td>
	<td class="line x" title="171:171	7 Acknowledgments This research was supported by the New Energy and Industrial Technology Development Organization of Japan (NEDO)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1141
Collocation Extraction Based On Modifiability Statistics
Wermter, Joachim;Hahn, Udo;"></td>
	<td class="line x" title="1:152	Collocation Extraction Based on Modifiability Statistics Joachim Wermter Udo Hahn Computerlinguistik, Friedrich-Schiller-Universitat Jena Furstengraben 30, D-07743 Jena, Germany wermter@coling.uni-freiburg.de Abstract We introduce a new, linguistically grounded measure of collocativity based on the property of limited modifiability and test it on German PP-verb combinations." ></td>
	<td class="line x" title="2:152	We show that our measure not only significantly outperforms the standard lexical association measures typically employed for collocation extraction, but also yields a valuable by-product for the creation of collocation databases, viz." ></td>
	<td class="line x" title="3:152	possible structural and lexical attributes." ></td>
	<td class="line x" title="4:152	Our approach is language-, structure-, and domain-independent because it only requires some shallow syntactic analysis (e.g. , a POS-tagger and a phrase chunker)." ></td>
	<td class="line x" title="5:152	1 Introduction Natural language is an open and very flexible communication system." ></td>
	<td class="line x" title="6:152	Syntax, of course, imposes constraints, e.g., on word order or the occurrence of particular phrasal types such as PPs or NPs, and lexical semantics imposes, e.g., selectional constraints on conceptually permitted sorts or types within the context of specific verbs or nouns." ></td>
	<td class="line x" title="7:152	Nevertheless, natural language speakers usually enjoy an enormous degree of freedom to express the content they want to convey in a great variety of linguistic forms." ></td>
	<td class="line x" title="8:152	There is, however, a significant subset of expressions which do not share this rather free combinability, so-called collocations." ></td>
	<td class="line x" title="9:152	From a linguistic perspective, they can be characterized by at least three recurrent and prominent properties (Manning and Schutze, 1999): a0 Non-(or limited) compositionality." ></td>
	<td class="line x" title="10:152	The meaning of a collocation is not a straightforward composition of the meanings of its parts." ></td>
	<td class="line x" title="11:152	For example, the meaning of red tape is completely different from the meaning of its components." ></td>
	<td class="line x" title="12:152	a0 Non-(or limited) substitutability." ></td>
	<td class="line x" title="13:152	The parts of a collocation cannot be substituted by semantically similar words." ></td>
	<td class="line x" title="14:152	Thus, gut in to spill gut cannot be substituted by intestine (see also Lin (1999))." ></td>
	<td class="line x" title="15:152	a0 Non-(or limited) modifiability." ></td>
	<td class="line x" title="16:152	Many collocations cannot be supplemented by additional lexical material." ></td>
	<td class="line x" title="17:152	For example, the noun in to kick the bucket cannot be modified as to kick the a1 holey/plastic/watera2 bucket." ></td>
	<td class="line x" title="18:152	Considering these observations, from a natural language processing perspective, collocations should not enter, e.g., the standard syntax-semantics pipeline so as to prevent compositional semantic readings of expressions for which this is absolutely not desired." ></td>
	<td class="line x" title="19:152	Hence, collocations need to be identified as such and subsequently be blocked, e.g., from compositional semantic interpretation." ></td>
	<td class="line x" title="20:152	In computational linguistics, a wide variety of lexical association measures have been employed for the task of (semi-)automatic collocation identification and extraction." ></td>
	<td class="line x" title="21:152	Almost all of these measures can be grouped into one of the following three categories: a0 frequency-based measures (e.g. , based on absolute and relative co-occurrence frequencies) a0 information-theoretic measures (e.g. , mutual information, entropy) a0 statistical measures (e.g. , chi-square, t-test, log-likelihood, Dices coefficient) The corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (Dunning, 1993; Manning and Schutze, 1999) and their suitability for the task of collocation extraction (see Evert and Krenn (2001) and Krenn and Evert (2001) for recent evaluations)." ></td>
	<td class="line x" title="22:152	Typically, they are applied to a set of candidate lexeme pairs which were obtained from preprocessors varying in linguistic sophistication.1 The selected measure then assigns an association score 1On the low end, this may just be a preset numeric window span." ></td>
	<td class="line x" title="23:152	In order to reduce the noise among the candidates, however, more elaborate linguistic processing, such as POS tagging, chunking, or even parsing, is increasingly being applied." ></td>
	<td class="line x" title="24:152	to each candidate pair, which is computed from its joint and marginal frequencies, thus expressing the strength of the hypothesis stating whether it constitutes a collocation or not." ></td>
	<td class="line x" title="25:152	While these association measures have their statistical merits in collocation identification, it is interesting to note that they have relatively little to do with the linguistic properties (such as those mentioned at the beginning) which are typically associated with the notion of collocativity." ></td>
	<td class="line x" title="26:152	Therefore, it may be interesting to investigate whether there is a way to implement a measure which directly incorporates linguistic criteria in the collocation identification task, and even more important, whether such a linguistically rooted approach would fare better in comparison to some of the standard lexical association measures." ></td>
	<td class="line x" title="27:152	In the following study, we will introduce such a linguistic measure for identifying PP-verb collocations in German, which is based on the property of nonor limited modifiability." ></td>
	<td class="line x" title="28:152	To the best of our knowledge, this is the first work to use this kind of linguistic measure to acquire collocations automatically." ></td>
	<td class="line x" title="29:152	By contrasting our method to previous studies which use the standard lexical association measures, we intend to emphasize a more linguistically inspired use of statistics in collocation mining." ></td>
	<td class="line x" title="30:152	Section 2 motivates our definition of the notion of collocation and Section 3 describes our methods, in particular the linguistically grounded collocation extraction algorithm, and the experimental setup derived from it." ></td>
	<td class="line x" title="31:152	In Section 4 we present and discuss the results of our experiments." ></td>
	<td class="line x" title="32:152	2 Kinds of Collocations There have been various approaches to define the notion of collocation." ></td>
	<td class="line x" title="33:152	This is by no means an easy task, especially when it comes to defining the demarcation line between collocations and free word combinations (modulo general syntactic and semantic semantic constraints)." ></td>
	<td class="line x" title="34:152	We favor an approach which draws this line on the semantic layer, viz." ></td>
	<td class="line x" title="35:152	the compositionality between the components of a linguistic expression." ></td>
	<td class="line x" title="36:152	For this purpose, we distinguish between three classes of collocations based on varying degrees of semantic compositionality of the basic lexical entities involved: 1." ></td>
	<td class="line x" title="37:152	Idiomatic Phrases." ></td>
	<td class="line x" title="38:152	In this case, none of the lexical components involved contribute to the overall meaning in a semantically transparent way." ></td>
	<td class="line x" title="39:152	The meaning of the expression is metaphorical or figurative." ></td>
	<td class="line x" title="40:152	For example, the literal meaning of the German PP-verb combination [jemanden] auf die Schippe nehmen is to take [someone] onto the shovel." ></td>
	<td class="line x" title="41:152	Its figurative meaning is to lampoon somebody." ></td>
	<td class="line x" title="42:152	2." ></td>
	<td class="line x" title="43:152	Support Verb Constructions/Narrow Collocations." ></td>
	<td class="line x" title="44:152	This second class contains expressions in which at least one component contributes to the overall meaning in a semantically transparent way and thus constitutes its semantic core." ></td>
	<td class="line x" title="45:152	For example, in the support verb construction zur Verfugung stellen (literal: to put to availabilty; actual: to make available), the noun Verfugung is the semantic core of the expression, whereas the verb only has a support function with some impact on argument structure, causativity or aktionsart." ></td>
	<td class="line x" title="46:152	There are, however, also narrow collocations in which the basic lexical meaning of the verb is the semantic core: For example, in aus eigener Tasche bezahlen (to pay out of ones own pocket) the verb bezahlen is the semantic core." ></td>
	<td class="line x" title="47:152	What unifies these two types is the fact that they function as predicates." ></td>
	<td class="line x" title="48:152	3." ></td>
	<td class="line x" title="49:152	Fixed Phrases." ></td>
	<td class="line x" title="50:152	Here, all basic lexical meanings of the components involved contribute to the overall meaning in a semantically much more transparent way." ></td>
	<td class="line x" title="51:152	Still, they are not as completely compositional as to classify them as free word combinations." ></td>
	<td class="line x" title="52:152	For example, all the basic lexical meanings of the different lexical components in im Koma liegen (literal: to lie in coma; actual: to be comatose) contribute to the overall meaning of the expression." ></td>
	<td class="line x" title="53:152	Still, this is different from a completely compositional free word combination, such as auf der Strasse gehen (to walk on the street)." ></td>
	<td class="line x" title="54:152	Our goal is to consider all three types of collocations as a whole, i.e., we will not distinguish between the three different kinds of collocations." ></td>
	<td class="line x" title="55:152	However, in order to focus our experiments, we will concentrate on a particular surface pattern in which they occur, viz." ></td>
	<td class="line x" title="56:152	PP-verb collocations." ></td>
	<td class="line x" title="57:152	3 Methods and Experiments 3.1 Construction and Statistics of the Testset We used a 114-million-word German-language newspaper corpus extracted from the Web to acquire candidate PP-verb collocations." ></td>
	<td class="line x" title="58:152	The corpus was first processed by means of the TNT partof-speech tagger (Brants, 2000)." ></td>
	<td class="line x" title="59:152	Then we ran a sentence/clause recognizer and an NP/PP chunker, both developed at the Text Knowledge Engineering Lab at Freiburg University, on the POS-tagged corpus." ></td>
	<td class="line x" title="60:152	From the XML-marked-up tree output, PPverb complexes were automatically selected in the following way: Taking a particular PP node as a fixed point, either the preceding or the following sibling V node was taken.2 From such a PPverb combination, we extracted and counted both its various heads, in terms of Preposition-Noun-Verb (PNV) triples, and all its associated supplements, i.e., here in this case any additional lexical material which also occurs in the nominal group of the PP, such as articles, adjectives, adverbs, cardinals, etc.3 The extraction of the associated supplements is essential to the linguistic measure described in subsection 3.3 below." ></td>
	<td class="line x" title="61:152	In order to reduce the amount of candidates for evaluation and to eliminate low-frequency data, we only considered PNV-triples with frequency a0a2a1 a3a5a4." ></td>
	<td class="line x" title="62:152	This was also motivated by the wellknown fact that collocations tend to have a higher co-occurrence frequency than free word combinations.4 Table 1 contains the data for the corresponding frequency distributions." ></td>
	<td class="line x" title="63:152	frequency PP-verb combinations candidate tokens candidate types all 1,663,296 1,159,133 a6a8a7a10a9a12a11 279,350 8,644 Table 1: Frequency distribution for PP-Verb tokens and types for our 114-million-word newspaper corpus 3.2 Classification of the Testset Three human judges manually classified the PPverb candidate types with a0a13a1 a3a5a4 in regard to whether they were a collocation or not." ></td>
	<td class="line x" title="64:152	For this purpose, they used a manual, in which the guidelines included the linguistic properties as described in Section 1 and the three collocation classes identified in Section 2." ></td>
	<td class="line x" title="65:152	Among the 8,644 PP-verb candidate types, 1,180 (13.7%) were identified as true collocations." ></td>
	<td class="line x" title="66:152	The inter-annotator agreement was 94.8% (with a standard deviation of 2.1)." ></td>
	<td class="line x" title="67:152	2The verbs in this study are restricted to main verbs and are reduced to their base form after extraction." ></td>
	<td class="line x" title="68:152	3It should be noted that both heads and associated supplements may of course vary depending on the particular linguistic structure targeted for collocation extraction." ></td>
	<td class="line x" title="69:152	4Cf." ></td>
	<td class="line x" title="70:152	also Evert and Krenn (2001) for empirical evidence justifying the exclusion of low-frequency data." ></td>
	<td class="line x" title="71:152	3.3 The Linguistic Measure The linguistic property around which we built our measure for collocativity is the nonor limited modifiabilty of collocations with additional lexical material (i.e. , supplements)." ></td>
	<td class="line x" title="72:152	The underlying assumption is that a PNV triple is less modifiable (and thus more likely to be a collocation) if it has a lexical supplement which, compared to all others, is particularly characteristic." ></td>
	<td class="line x" title="73:152	We express this assumption in the following way: Let a14 be the number of distinct supplements of a particular PNV triple (a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30 )." ></td>
	<td class="line x" title="74:152	The probability a32 of a particular supplement a33a35a34a37a36a38a36a40a39, a41a43a42a45a44 a3a47a46 a14a49a48, is described by its frequency scaled by the sum of all supplement frequencies: a32a51a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a59a58a59a60a61a42 (1) a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a59a58a5a60 a62a64a63 a24a23a65a67a66 a0 a50a52a15a17a16a19a18a68a20a23a22a69a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a29a70a71a60 with a62a19a63 a24a72a65a67a66 a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a73a30a57a53a55a74a56a12a26a54a26a29a70a71a60a64a42 a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a75a60 . 5 Then the modifiability a76a78a77a80a79 of a PNV triple can be described by its most probable supplement: a76a78a77a80a79a81a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a73a30a82a60a84a83a31a42 (2) a85a87a86a89a88a91a90a92a85a87a93 a32a51a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a54a53a55a38a56a57a26a54a26a59a58a59a60 a46 a41a51a42a94a44 a3a47a46 a14a49a48 To define a measure of collocativity a95a96a77a98a97a84a97 for a candidate set, some factor regarding frequency has to be taken into account." ></td>
	<td class="line x" title="75:152	Thus, besides a76a78a77a80a79, we take the relative co-occurrence frequency for a specific PNV triple a32a51a50a52a15a17a16a19a18a99a20a23a22a69a24a27a26a29a28a31a30a82a60 (a100 being the number of candidate types (here, 8,644)) a32a51a50a52a15a17a16a101a18a68a20a23a22a25a24a27a26a29a28a73a30a82a60a84a83a31a42 a0 a50a52a15a17a16a19a18a68a20a23a22a69a24a27a26a29a28a31a30a12a60 a62 a20 a102 a65a67a66 a0 a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a73a30a52a103a59a60 (3) and incorporate it as a second factor to a95a104a77a98a97a105a97 : a95a96a77a98a97a84a97a91a50a52a15a17a16a101a18a68a20a23a22a25a24a27a26a29a28a73a30a82a60a105a83a31a42 (4) a76a78a77a80a79a81a50a52a15a17a16a101a18a68a20a23a22a25a24a27a26a29a28a73a30a82a60a64a106a107a32a51a50a52a15a17a16a19a18a21a20a23a22a25a24a27a26a29a28a31a30a75a60 3.4 Methods of Evaluation Standard procedures for evaluating the goodness of collocativity measures usually involve identifying the true positives among the a14 -highest ranked candidates returned by a particular measure." ></td>
	<td class="line x" title="76:152	Because this is rather labor-intensive, a14 is usually small, ranging from 50 to several hundred." ></td>
	<td class="line x" title="77:152	Evert and Krenn 5Note that the zero supplement of the PNV triple, i.e., the one for which no lexical supplements co-occur is also included in this set." ></td>
	<td class="line x" title="78:152	(2001), however, point out the inadequacy of such methods claiming they usually lead to very superficial judgements about the measures to be examined." ></td>
	<td class="line x" title="79:152	In contrast, they suggest examining various a14 highest ranked samples, which allows plotting standard precision and recall graphs for the whole candidate set." ></td>
	<td class="line x" title="80:152	We evaluate the a95a104a77a98a97a105a97 measure against two widely used standard statistical tests (t-test and loglikelihood) and against co-occurrence frequency." ></td>
	<td class="line x" title="81:152	The comparison to the t-test is especially interesting because it was found to achieve the best overall precision scores in other studies (see Evert and Krenn (2001))." ></td>
	<td class="line x" title="82:152	Our baseline is defined by the proportion of true positives (13.7%; see subsection 3.2), which can be described as the likelihood of finding one by blindly picking from the candidate set." ></td>
	<td class="line x" title="83:152	4 Experimental Results and Discussion 4.1 Precision and Recall for Collocation Extraction In the first experiment, we incrementally examined parts of the a14 -highest ranked candidate lists returned by the each of the four measures we considered." ></td>
	<td class="line x" title="84:152	The precision values for various a14 were computed such that for each percent point of the list, the proportion of true positives was compared to the overall number of candidate items returned." ></td>
	<td class="line x" title="85:152	This yields the precision curves in Figure 1 and its associated values at selected list portions in the upper table from Table 2." ></td>
	<td class="line x" title="86:152	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10 20 30 40 50 60 70 80 90 100 Portion of ranked list (in %) Precision ModifiabilityT-test Frequency Log-likelihood Base Figure 1: Precision for Collocation Extraction First, we observe that all measures outperform the baseline by far and, thus, all are potentially useful measures of collocativity." ></td>
	<td class="line x" title="87:152	Of the statistical measures, log-likelihood (the most complex one) performs the worst, whereas t-test and frequency, almost indistinguishable, share the middle position, with frequency measurements having a very slight edge at six rank points." ></td>
	<td class="line x" title="88:152	This is in contrast to the findings reported by Krenn and Evert (2001), which gave the t-test an edge.6 As can be clearly seen, however, our linguistic modifiability measure substantially outperforms all other measures at all points in the ranked list." ></td>
	<td class="line x" title="89:152	Considering 1% (a14a1a0 a2a4a3 ), its precision value is ten percentage points higher than for t-test and frequency, and even 22 points higher compared to loglikelihood." ></td>
	<td class="line x" title="90:152	Until 50% (a14a5a0a7a6a9a8a4a10a4a10 ) of the ranked list is considered, modifiability maintains a three to five percentage point advantage in precision over ttest and frequency." ></td>
	<td class="line x" title="91:152	In the second half of the list, all curves and associated values start converging towards the baseline." ></td>
	<td class="line x" title="92:152	We also tested the significance of differences for our precision results, both between modifiability and frequency and between modifiability and t-test." ></td>
	<td class="line x" title="93:152	Because in both cases the ranked lists were taken from the same set of candidates, viz." ></td>
	<td class="line x" title="94:152	the 8,644 PPverb candidate types, and hence constitute dependent samples, we applied the McNemar test (Sachs, 1984) for statistical testing." ></td>
	<td class="line x" title="95:152	We selected 100 measure points in the ranked list, one after each increment of one percent, and then used the two-tailed test for a confidence interval of 99%." ></td>
	<td class="line x" title="96:152	Table 3, which lists the number of significant differences for 10, 50 and 100 measure points, shows that almost all of them are significantly different." ></td>
	<td class="line x" title="97:152	# of significance # of signicant differences measure points comparing modifiability with frequency t-test 10 9 9 50 49 49 100 96 97 Table 3: Significance testing of differences using the two-tailed McNemar test at 99% confidence interval The recall curves in Figure 2 and their corresponding values in the lower table from Table 2 measure which proportion of all true positives is identified by a particular measure at a certain part of the ranked list." ></td>
	<td class="line x" title="98:152	In this sense, recall is an even better indicator of a particular measures performance." ></td>
	<td class="line x" title="99:152	Again, the linguistically motivated collocation extraction algorithm outscores all others, even more pronounced than for precision." ></td>
	<td class="line x" title="100:152	When examining 20% (a14a11a0 a3a13a12 a10a4a14a38a60, 30% (a14a15a0 a10a4a16a4a14a4a8 ) and 40% 6The reason why frequency performs even slightly better than t-test may very well have to do with the size of our training corpus (114 million words)." ></td>
	<td class="line x" title="101:152	But this just underlines the fact that large corpora are essential for collocation discovery." ></td>
	<td class="line x" title="102:152	Portion of Precision scores of measure evaluated ranked list considered Modifiablity T-test Frequency Log-likelihood Baseline 1% 0.84 0.74 0.74 0.62 0.14 10% 0.51 0.46 0.45 0.39 0.14 20% 0.39 0.34 0.34 0.30 0.14 30% 0.31 0.27 0.28 0.25 0.14 40% 0.27 0.23 0.24 0.21 0.14 50% 0.24 0.20 0.21 0.18 0.14 60% 0.21 0.18 0.19 0.16 0.14 70% 0.19 0.17 0.17 0.15 0.14 80% 0.17 0.15 0.16 0.15 0.14 90% 0.15 0.14 0.15 0.14 0.14 (a0a2a1 a3a5a4a7a6a9a8a9a8 ) 100% 0.14 0.14 0.14 0.14 0.14 Portion of Recall scores of measure evaluated ranked list considered Modifiablity T-test Frequency Log-likelihood 1% 0.06 0.05 0.05 0.05 10% 0.37 0.33 0.33 0.28 20% 0.58 0.50 0.50 0.44 30% 0.69 0.60 0.61 0.55 40% 0.80 0.69 0.70 0.61 50% 0.87 0.75 0.76 0.66 60% 0.93 0.80 0.83 0.72 70% 0.96 0.85 0.88 0.78 80% 0.98 0.89 0.92 0.85 90% 0.99 0.93 0.96 0.92 100% 1.00 1.00 1.00 1.00 Table 2: Precision and Recall Scores for Collocation Extraction at Major Portions of the Ranked List (a14 a0 a8 a6a9a16 a2 ) of the ranked list, modifiability, respectively, identifies almost 60%, 70% and 80% of all true positives, holding a ten percentage point lead over t-test and frequency at each of these points." ></td>
	<td class="line x" title="103:152	When 50% (a14 a0 a6a9a8a4a10a4a10 ) are considered, this difference reaches eleven and twelve points (compared to frequency and t-test, respectively)." ></td>
	<td class="line x" title="104:152	0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 10 20 30 40 50 60 70 80 90 100 Portion of ranked list (in %) Recall Modifiability T-test Frequency Log-likelihood Figure 2: Recall for Collocation Extraction Even more strikingly, for the identification of 90% of all true positives, modifiability only needs to look at 55% (a14 a0 a6 a12 a16 a6 ) of the ranked list." ></td>
	<td class="line x" title="105:152	Frequency, on the other hand, needs to examine 75% (a14 a0 a3 a6 a2 a8 ) and t-test even 85% (a14 a0 a12 a8 a6 a12 ) of the ranked list to reach this high level of recall." ></td>
	<td class="line x" title="106:152	4.2 Modifiability Revisited The previous subsection showed that a measure for collocation discovery which takes into account the linguistic property of limited modifiability fares significantly better than linguistically not so founded, purely statistical measures." ></td>
	<td class="line x" title="107:152	Although the modifiability property constitutes common wisdom about collocations, it has not yet been empirically evaluated." ></td>
	<td class="line x" title="108:152	Thus, we ran an experiment which took both the PNV triples classified as collocations and the PNV triples classified as non-collocations and counted the numbers of distinct supplements (referred to as a14 in Subsection 3.3)." ></td>
	<td class="line x" title="109:152	From this data, we set up a distribution of collocational and noncollocational PNV triples in which the distributional ranking criterion was the number of distinct supplements (cf.Figure 3)." ></td>
	<td class="line x" title="111:152	PNV Triple NP Supplement Frequency in Griff bekommen den/ART Griff/NN 459 to get under control Griff/NN 2 den/ART gewerkschaftlichen/ADJA Griff/NN 1 den/ART dramatischen/ADJA Griff/NN 1 den/ART erzahlerischen/ADJA Griff/NN 1 unter Druck geraten Druck/NN 560 to get under pressure politischen/ADJA Druck/NN 6 erheblichen/ADJA politischen/ADJA Druck/NN 5 teilweise/ADV lebensgefahrlichen/ADJA Druck/NN 1 wachsenden/ADJA Druck/NN 1 noch/ADV starkeren/ADJA Druck/NN 1 schweren/ADJA Druck/NN 1 Table 4: Collocational PNV Triples with Associated Noun Phrase Supplements 0.001 0.01 0.1 1 10 100 1 10 100 1000 proportion of all PNV-triples (in %) number of distinct supplements of PNV-triples Collocations Non-Collocations Figure 3: Distribution of Supplements for (Non-) Collocations in PNV Triples." ></td>
	<td class="line x" title="112:152	The xand y-axes are log-scaled." ></td>
	<td class="line x" title="113:152	As Figure 3 reveals, not only is the proportion of collocational PNV triples with only one distinct supplement higher (36%) than the proportion for non-collocational ones (20%), but with each additional supplement, the collocational proportion curve declines more steeply than its noncollocational counterpart." ></td>
	<td class="line x" title="114:152	Moreover, the collocational proportion curve already ends with 54 distinct supplements, whereas the non-collocational proportion curve leads up 520 distinct supplements." ></td>
	<td class="line x" title="115:152	Thus, we are able to add some empirical grounding to the widespread textbook assumption about the limited modifiablity of collocations." ></td>
	<td class="line x" title="116:152	Another observation (which is also inherent to our linguistic measure) based on this experiment is that some collocations do possess at least limited modifiability." ></td>
	<td class="line x" title="117:152	Collocation acquisition is, of course, not a goal by itself, but rather aims at creating collocation lexicons for both language processing and generation (Smadja and McKeown, 1990)." ></td>
	<td class="line x" title="118:152	From this perspective, our linguistic modifiabilty measure actually yields quite a valuable by-product for the development of lexicons or collocational knowledge bases: A list of possible structural and lexical modifications associated with a particular collocational entry candidate." ></td>
	<td class="line x" title="119:152	In our case, these modifications refer to the nominal group of the PP." ></td>
	<td class="line x" title="120:152	We illustrate this point in Table 4 with two collocational PNV triples and some of their associated NP supplements plus their frequencies." ></td>
	<td class="line x" title="121:152	As can be seen, both structural and lexical attributes of collocations can thus be obtained." ></td>
	<td class="line x" title="122:152	The structural information comes in the form of part-ofspeech (POS) tags." ></td>
	<td class="line x" title="123:152	From this, possible prenominal POS types and their combinations can be used to describe a collocations structural make-up." ></td>
	<td class="line x" title="124:152	From a lexical viewpoint, the collocation can be described by the lexical semantic word classes used for modification.7 As can be seen in Table 4 under the PNV triple for to get under pressure, the noun Druck (pressure) is often modified by a certain semantic class of adjectives, such as stark (strong), schwer (heavy), erheblich (considerable, grave)." ></td>
	<td class="line oc" title="125:152	5 Related Work Although there have been many studies on collocation extraction and mining using only statistical approaches (Church and Hanks, 1990; Ikehara et al. , 1996), there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations." ></td>
	<td class="line x" title="126:152	Smadja (1993), which is the classic work on collocation extraction, uses a two-stage filtering model in which, in the first step, n-gram statistics determine possible collocations and, in the second step, these candidates are submitted to a syntactic valida7Of course, lexical material is always at least partially dependent on the domain in question." ></td>
	<td class="line x" title="127:152	In our case, this is the news domain with all its associated subdomains (politics, economics, finance, culture, etc.)." ></td>
	<td class="line x" title="128:152	tion procedure (e.g. , determining verb-object collocations) in order to filter out invalid collocations." ></td>
	<td class="line x" title="129:152	In a single-judge evaluation of 4,000 collocation candidates, the incorporation of linguistic criteria (via tagging and predicate-argument parsing) boosts precision up to a level of 80% and recall to 94%." ></td>
	<td class="line x" title="130:152	These results are, of course, not comparable to ours." ></td>
	<td class="line x" title="131:152	First of all, precision and recall are measured at a fixed point for a fixed unranked candidate list." ></td>
	<td class="line x" title="132:152	In order to obtain more reliable evaluation results, we plot these values continuously on a ranked candidate list." ></td>
	<td class="line x" title="133:152	Secondly, our kind of syntactic preprocessing (which is standard nowadays) allows collocation extraction algorithms to better control the structural types of collocations." ></td>
	<td class="line x" title="134:152	Lin (1998) acquires a lexical dependency database by assembling dependency relationships from a parsed corpus." ></td>
	<td class="line x" title="135:152	An entry in this database is classified as collocation if its log-likelihood value is greater than some threshold." ></td>
	<td class="line x" title="136:152	Using an automatically constructed similarity thesaurus, Lin (1999) then separates compositional from non-compositional collocations by taking into account the second linguistic property described in Section 1, viz." ></td>
	<td class="line x" title="137:152	their nonor limited substitutability." ></td>
	<td class="line x" title="138:152	In particular, he checks the existence and mutual information values of phrases obtained by substituting the words with similar ones, which results in the classification of the phrase as being compositional or noncompositional." ></td>
	<td class="line x" title="139:152	Although this study offers some promising results, its applicability rather falls into the category of fine-classifying an already acquired set of collocations, e.g., according to the criteria described in Section 2, and thus is not really comparable to our work." ></td>
	<td class="line x" title="140:152	Moreover, the linguistic property in his focus is of course a semantic one, whereas ours is purely syntactic in nature." ></td>
	<td class="line x" title="141:152	6 Conclusion We introduced a new, linguistically motivated measure of collocativity based on the property of limited modifiability and tested it on a large corpus with emphasis on German PP-verb combinations." ></td>
	<td class="line x" title="142:152	We showed that our measure not only significantly outperforms the standard lexical association measures typically used for collocation extraction, but also yields a valuable by-product for the creation of collocation databases, viz." ></td>
	<td class="line x" title="143:152	possible structural and lexical attributes of a collocation." ></td>
	<td class="line x" title="144:152	Our measure defines the modifiability property in a linguistically simple way, by e.g. ignoring the internal make-up of lexical supplements associated with a collocation candidate." ></td>
	<td class="line x" title="145:152	Hence, it may be worthwhile to investigate whether a more sophisticated approach, by e.g. taking into account internal POS types and their distribution etc. , would improve our results even more." ></td>
	<td class="line x" title="146:152	We may also consider other linguistic criteria (e.g. , limited substitutability) to further refine our measure and to categorize already identified collocations." ></td>
	<td class="line x" title="147:152	At the methodological level, our approach, although tested on German newspaper language data, is language-, structure-, and domain-independent." ></td>
	<td class="line x" title="148:152	All it requires is some sort of shallow syntactic analysis, e.g., POS tagging and phrase chunking." ></td>
	<td class="line x" title="149:152	Thus, in the future we plan to include other syntactic types of collocations, such as verb-object or verbobject-PP combinations, and also apply our methodology to other languages and domains, such as the biomedical field." ></td>
	<td class="line x" title="150:152	Acknowledgements." ></td>
	<td class="line x" title="151:152	We would like to thank our students, Sabine Demsar, Kristina Meller, and Konrad Feldmeier, for their excellent work as human collocation classifiers." ></td>
	<td class="line x" title="152:152	This work was partly supported by DFG grant KL 640/5-1." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1194
Discovering Word Senses From A Network Of Lexical Cooccurrences
Ferret, Olivier;"></td>
	<td class="line x" title="1:166	Discovering word senses from a network of lexical cooccurrences Olivier Ferret CEA  LIST/LIC2M 18, route du Panorama 92265 Fontenay-aux-Roses, France ferreto@zoe.cea.fr Abstract Lexico-semantic networks such as WordNet have been criticized about the nature of the senses they distinguish as well as on the way they define these senses." ></td>
	<td class="line x" title="2:166	In this article, we present a possible solution to overcome these limits by defining the sense of words from the way they are used." ></td>
	<td class="line x" title="3:166	More precisely, we propose to differentiate the senses of a word from a network of lexical cooccurrences built from a large corpus." ></td>
	<td class="line x" title="4:166	This method was tested both for French and English and was evaluated for English by comparing its results with WordNet." ></td>
	<td class="line x" title="5:166	1 Introduction Semantic resources have proved to be useful in information retrieval and information extraction for applications such as query expansion (Voorhees, 1998), text summarization (Harabagiu and Maiorano, 2002) or question/answering (Pasca and Harabagiu, 2001)." ></td>
	<td class="line x" title="6:166	But this work has also shown that these resources must be used with caution: they bring on an improvement of results only if word sense disambiguation is performed with a great accuracy." ></td>
	<td class="line x" title="7:166	These findings bring one of the first roles of a semantic resource to light: discriminating and characterizing the senses of a set of words." ></td>
	<td class="line x" title="8:166	The main semantic resources with a wide coverage that can be exploited by computers are lexico-semantic networks such as WordNet." ></td>
	<td class="line x" title="9:166	Because of the way they were built, mainly by hand, these networks are not fundamentally different from traditional dictionaries." ></td>
	<td class="line x" title="10:166	Hence, it is not very surprising that they were criticized, as in (Harabagiu et al. , 1999), for not being suitable for Natural Language Processing." ></td>
	<td class="line x" title="11:166	They were criticized both about the nature of the senses they discriminate and the way they characterize them." ></td>
	<td class="line x" title="12:166	Their senses are considered as too fine-grained but also incomplete." ></td>
	<td class="line x" title="13:166	Moreover, they are generally defined through their relations with synonyms, hyponyms and hyperonyms but not by elements that describe the contexts in which they occur." ></td>
	<td class="line x" title="14:166	One of the solutions for solving this problem consists in automatically discovering the senses of words from corpora." ></td>
	<td class="line x" title="15:166	Each sense is defined by a list of words that is not restricted to synonyms or hyperonyms." ></td>
	<td class="line x" title="16:166	The work done in this area can be divided into three main trends." ></td>
	<td class="line x" title="17:166	The first one, represented by (Pantel and Lin, 2002), is not focused on the problem of discovering word senses: its main objective is to build classes of equivalent words from a distributionalist viewpoint, hence to gather words that are mainly synonyms." ></td>
	<td class="line x" title="18:166	In the case of (Pantel and Lin, 2002), the discovering of word senses is a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses." ></td>
	<td class="line x" title="19:166	The second main trend, found in (Schtze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances." ></td>
	<td class="line x" title="20:166	Each cluster is then considered as a sense of the target word." ></td>
	<td class="line x" title="21:166	The last trend, explored by (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity." ></td>
	<td class="line x" title="22:166	Our work takes place in this last trend." ></td>
	<td class="line x" title="23:166	2 Overview The starting point of the method we present in this article is a network of lexical cooccurrences, that is a graph whose vertices are the significant words of a corpus and edges represent the cooccurrences between these words in the corpus." ></td>
	<td class="line x" title="24:166	The discovering of word senses is performed word by word and the processing of a word only relies on the subgraph that contains its cooccurrents." ></td>
	<td class="line x" title="25:166	The first step of the method consists in building a matrix of similarity between these cooccurrents by exploiting their relations in the subgraph." ></td>
	<td class="line x" title="26:166	An unsupervised clustering algorithm is then applied for grouping these cooccurrents and giving rise to the senses of the considered word." ></td>
	<td class="line x" title="27:166	This method, as the ones presented in (Vronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), relies on the following hypothesis: in the subgraph gathering the cooccurrents of a word, the number of relations between the cooccurrents defining a sense is higher than the number of relations that these cooccurrents have with those defining the other senses of the considered word." ></td>
	<td class="line x" title="28:166	The clustering algorithm that we use is an adaptation of the Shared Nearest Neighbors (SNN) algorithm presented in (Ertz et al. , 2001)." ></td>
	<td class="line x" title="29:166	This algorithm particularly fits our problem as it automatically determines the number of clusters, in our case the number of senses of a word, and does not take into account the elements that are not representative of the clusters it builds." ></td>
	<td class="line x" title="30:166	This last point is especially important for our application as there is a lot of noise among the cooccurrents of a word." ></td>
	<td class="line x" title="31:166	3 Networks of lexical cooccurrences The method we present in this article for discovering word senses was applied both for French and English." ></td>
	<td class="line x" title="32:166	Hence, two networks of lexical cooccurrences were built: one for French, from the Le Monde newspaper (24 months between 1990 and 1994), and one for English, from the L.A. Times newspaper (2 years, part of the TREC corpus)." ></td>
	<td class="line x" title="33:166	The size of each corpus was around 40 million words." ></td>
	<td class="line x" title="34:166	The building process was the same for the two networks." ></td>
	<td class="line x" title="35:166	First, the initial corpus was preprocessed in order to characterize texts by their topically significant words." ></td>
	<td class="line x" title="36:166	Thus, we retained only the lemmatized form of plain words, that is, nouns, verbs and adjectives." ></td>
	<td class="line x" title="37:166	Cooccurrences were classically extracted by moving a fixed-size window on texts." ></td>
	<td class="line x" title="38:166	Parameters were chosen in order to catch topical relations: the window was rather large, 20word wide, and took into account the boundaries of texts; moreover, cooccurrences were indifferent to word order." ></td>
	<td class="line oc" title="39:166	As (Church and Hanks, 1990), we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence." ></td>
	<td class="line o" title="40:166	This measure was normalized according to the maximal mutual information relative to the considered corpus." ></td>
	<td class="line x" title="41:166	After filtering the less significant cooccurrences (cooccurrences with less than 10 occurrences and cohesion lower than 0.1), we got a network with approximately 23,000 words and 5.2 million cooccurrences for French, 30,000 words and 4.8 million cooccurrences for English." ></td>
	<td class="line x" title="42:166	4 Word sense discovery algorithm 4.1 Building of the similarity matrix between cooccurrents The number and the extent of the clusters built by a clustering algorithm generally depend on a set of parameters that can be tuned in one way or another." ></td>
	<td class="line x" title="43:166	But this possibility is implicitly limited by the similarity measure used for comparing the elements to cluster." ></td>
	<td class="line x" title="44:166	In our case, the elements to cluster are the cooccurrents in the network of lexical cooccurrences of the word whose senses have to be discriminated." ></td>
	<td class="line x" title="45:166	Within the same framework, we tested two measures for evaluating the similarity between the cooccurrents of a word in order to get word senses with different levels of granularity." ></td>
	<td class="line x" title="46:166	The first measure corresponds to the cohesion measure between words in the cooccurrence network." ></td>
	<td class="line x" title="47:166	If there is no relation between two words in the network, the similarity is equal to zero." ></td>
	<td class="line x" title="48:166	This measure has the advantage of being simple and efficient from an algorithmic viewpoint but some semantic relations are difficult to catch only from cooccurrences in texts." ></td>
	<td class="line x" title="49:166	For instance, we experimentally noticed that there are few synonyms of a word among its cooccurrents 1." ></td>
	<td class="line x" title="50:166	Hence, we can expect that some senses that are discriminated by the algorithm actually refer to one sense." ></td>
	<td class="line x" title="51:166	To overcome this difficulty, we also tested a measure that relies not only on first order cooccurrences but also on second order cooccurrences, which are known to be less sparse and more robust than first order ones (Schtze, 1998)." ></td>
	<td class="line x" title="52:166	This measure is based on the following principle: a vector whose size is equal to the number of cooccurrents of the considered word is associated to each of its cooccurrents." ></td>
	<td class="line x" title="53:166	This vector contains the cohesion values between this cooccurrent and the other ones." ></td>
	<td class="line x" title="54:166	As for the first measure, a null value is taken when there is no relation between two words in the cooccurrence network." ></td>
	<td class="line x" title="55:166	The similarity matrix is then built by applying the cosine measure between 1 This observation comes from the intersection, for each word of the L.A. Times network, of its cooccurrents in the network and its synonyms in WordNet." ></td>
	<td class="line x" title="56:166	each couple of vectors, i.e. each couple of cooccurrents." ></td>
	<td class="line x" title="57:166	With this second measure, two cooccurrents can be found strongly linked even though they are not directly linked in the cooccurrence network: they just have to share a significant number of words with which they are linked in the cooccurrence network." ></td>
	<td class="line x" title="58:166	4.2 The Shared Nearest Neighbors (SNN) algorithm The SNN algorithm is representative of the algorithms that perform clustering by detecting the high-density areas of a similarity graph." ></td>
	<td class="line x" title="59:166	In such a graph, each vertex represents an element to cluster and an edge links two vertices whose similarity is not null." ></td>
	<td class="line x" title="60:166	In our case, the similarity graph directly corresponds to the cooccurrence network with the first order cooccurrences whereas with the second order cooccurrences, it is built from the similarity matrix described in Section 4.1." ></td>
	<td class="line x" title="61:166	The SNN algorithm can be split up into two main steps: the first one aims at finding the elements that are the most representative of their neighborhood by masking the less important relations in the similarity graph." ></td>
	<td class="line x" title="62:166	These elements are the seeds of the final clusters that are built in the second step by aggregating the remaining elements to those selected by the first step." ></td>
	<td class="line x" title="63:166	More precisely, the SNN algorithm is applied to the discovering of the senses of a target word as follows: 1." ></td>
	<td class="line x" title="64:166	sparsification of the similarity graph: for each cooccurrent of the target word, only the links towards the k (k=15 in our experiments) most similar other cooccurrents are kept." ></td>
	<td class="line x" title="65:166	2." ></td>
	<td class="line x" title="66:166	building of the shared nearest neighbor graph: this step only consists in replacing, in the sparsified graph, the value of each edge by the number of direct neighbors shared by the two cooccurrents linked by this edge." ></td>
	<td class="line x" title="67:166	3." ></td>
	<td class="line x" title="68:166	computation of the distribution of strong links among cooccurrents: as for the first step, this one is a kind of sparsification." ></td>
	<td class="line x" title="69:166	Its aim is to help finding the seeds of the senses, i.e. the cooccurrents that are the most representative of a set of cooccurrents." ></td>
	<td class="line x" title="70:166	This step is also a means for discarding the cooccurrents that have no relation with the other ones." ></td>
	<td class="line x" title="71:166	More precisely, two cooccurrents are considered as strongly linked if the number of the neighbors they share is higher than a fixed threshold." ></td>
	<td class="line x" title="72:166	The higher than a fixed threshold." ></td>
	<td class="line x" title="73:166	The number of strong links of each cooccurrent is then computed." ></td>
	<td class="line x" title="74:166	4." ></td>
	<td class="line x" title="75:166	identification of the sense seeds and filtering of noise: the sense seeds and the cooccurrents to discard are determined by comparing their number of strong links with a fixed threshold." ></td>
	<td class="line x" title="76:166	5." ></td>
	<td class="line x" title="77:166	building of senses: this step mainly consists in associating to the sense seeds identified by the previous step the remaining cooccurrents that are the most similar to them." ></td>
	<td class="line x" title="78:166	The result is a set of clusters that each represents a sense of the target word." ></td>
	<td class="line x" title="79:166	For associating a cooccurrent to a sense seed, the strength of the link between them must be higher than a given threshold." ></td>
	<td class="line x" title="80:166	If a cooccurrent can be tied to several seeds, the one that is the most strongly linked to it is chosen. Moreover, the seeds that are considered as too close from each other for giving rise to separate senses can also be grouped during this step in accordance with the same criteria than the other cooccurrents." ></td>
	<td class="line x" title="81:166	6." ></td>
	<td class="line x" title="82:166	extension of senses: after the previous steps, a set of cooccurrents that are not considered as noise are still not associated to a sense." ></td>
	<td class="line x" title="83:166	The size of this set depends on the strictness of the threshold controlling the aggregation of a cooccurrent to a sense seed but as we are interested in getting homogeneous senses, the value of this threshold cannot be too low." ></td>
	<td class="line x" title="84:166	Nevertheless, we are also interested in having a definition as complete as possible of each sense." ></td>
	<td class="line x" title="85:166	As senses are defined at this point more precisely than at step 4, the integration into these senses of cooccurrents that are not strongly linked to a sense seed can be performed on a larger basis, hence in a more reliable way." ></td>
	<td class="line x" title="86:166	4.3 Adaptation of the SNN algorithm For implementing the SNN algorithm presented in the previous section, one of the points that must be specified more precisely is the way its different thresholds are fixed." ></td>
	<td class="line x" title="87:166	In our case, we chose the same method for all of them: each threshold is set as a quantile of the values it is applied to." ></td>
	<td class="line x" title="88:166	In this way, it is adapted to the distribution of these values." ></td>
	<td class="line x" title="89:166	For the identification of the sense seeds (threshold equal to 0.9) and for the definition of the cooccurrents that are noise (threshold equal to LM-1 LM-2 LAT-1 LAT-1.no LAT-2.no number of words 17,261 17,261 13,414 6,177 6,177 percentage of words with at least one sense 44.4% 42.7% 39.8% 41.8% 39% average number of senses by word 2.8 2.2 1.6 1.9 1.5 average number of words describing a sense 16.1 16.3 18.7 20.2 18.9 Table 1: Statistics about the results of our word sense discovery algorithm 0.2), the thresholds are quantiles of the number of strong links of cooccurrents." ></td>
	<td class="line x" title="90:166	For defining strong links (threshold equal to 0.65), associating cooccurrents to sense seeds (threshold equal to 0.5) and aggregating cooccurrent to senses (threshold equal to 0.7), the thresholds are quantiles of the strength of the links between cooccurrents in the shared nearest neighbor graph." ></td>
	<td class="line x" title="91:166	We also introduced two main improvements to the SNN algorithm." ></td>
	<td class="line x" title="92:166	The first one is the addition of a new step between the last two ones." ></td>
	<td class="line x" title="93:166	This comes from the following observation: although a sense seed can be associated to another one during the step 5, which means that the two senses they represent are merged, some clusters that actually correspond to one sense are not merged." ></td>
	<td class="line x" title="94:166	This problem is observed with the first and the second order cooccurrences and cannot be solved, without merging unrelated senses, only by adjusting the threshold that controls the association of a cooccurrent to a sense seed." ></td>
	<td class="line x" title="95:166	In most of these cases, the split sense is scattered over one large cluster and one or several small clusters that only contain 3 or 4 cooccurrents." ></td>
	<td class="line x" title="96:166	More precisely, the sense seeds of the small clusters are not associated to the seed of the large cluster while most of the cooccurrents that are linked to them are associated to this seed." ></td>
	<td class="line x" title="97:166	Instead of defining a specific mechanism for dealing with these small clusters, we chose to let the SNN algorithm to solve the problem by only deleting these small clusters (size < 6) after the step 5 and marking their cooccurrents as unclassified." ></td>
	<td class="line x" title="98:166	The last step of the algorithm aggregates in most of the cases these cooccurrents to the large cluster." ></td>
	<td class="line x" title="99:166	Moreover, this new step makes the built senses more stable when the parameters of the algorithm are only slightly modified." ></td>
	<td class="line x" title="100:166	The second improvement, which has a smaller impact than the first one, aims at limiting the noise that is brought into clusters by the last step." ></td>
	<td class="line x" title="101:166	In the algorithm of (Ertz et al. , 2001), an element can be associated to a cluster when the strength of its link with one of the elements of this cluster is higher than a given threshold." ></td>
	<td class="line x" title="102:166	This condition is stricter in our case as it concerns the average strength of the links between the unclassified cooccurrent and those of the cluster." ></td>
	<td class="line x" title="103:166	5 Experiments We applied our algorithm for discovering word senses to the two networks of lexical cooccurrences we have described in Section 3 (LM: French; LAT: English) with the parameters given in Section 4." ></td>
	<td class="line x" title="104:166	For each network, we tested the use of first order cooccurrences (LM-1 and LAT-1) and second order ones (LM-2 and LAT-2)." ></td>
	<td class="line x" title="105:166	For English, the use of second order cooccurrences was tested only for the subpart of the words of the network that was selected for the evaluation of Section 6 (LAT-2.no)." ></td>
	<td class="line x" title="106:166	Table 1 gives some statistics about the results of the discovered senses for the different cases." ></td>
	<td class="line x" title="107:166	We can notice that a significant percentage of words do not have any sense, even with second order cooccurrences." ></td>
	<td class="line x" title="108:166	This comes from the fact that their cooccurrents are weakly linked to each other in the cooccurrence network they are part of, which probably means that their senses are not actually represented in this network." ></td>
	<td class="line x" title="109:166	We can also notice that the use of second order cooccurrence actually leads to have a smaller number of senses by word, hence to have senses with a larger definition." ></td>
	<td class="line x" title="110:166	As Vronis (2003), we give in Table 2 as an example of the results of our algorithm some of the words defining the senses of the polysemous French word barrage, which was part of the ROMANSEVAL evaluation." ></td>
	<td class="line x" title="111:166	Whatever the kind of cooccurrences it relies on, our algorithm finds three of the four senses distinguished in (Vronis, 2003): dam (senses 1.3 and 2.1); barricading, blocking (senses 1.1, 1.2 and 2.2); barrier, frontier (senses 1.4 and 2.3)." ></td>
	<td class="line x" title="112:166	The sense play-off game (match de barrage), which refers to the domain of sport, is not found as it is weakly represented in the cooccurrence network and is linked to words, such as division, that are also ambiguous (it refers LM-1 1.1 manifestant, forces_de_lordre, prfecture, agriculteur, protester, incendier, calme, pierre (demonstrator, the police, prefecture, farmer, to protest, to burn, quietness, stone) 1.2 conducteur, routier, vhicule, poids_lourd, camion, permis, trafic, bloquer, voiture, autoroute (driver, lorry driver, vehicule, lorry, truck, driving licence, traffic, to block, car, highway) 1.3 fleuve, rivire, lac, bassin, mtre_cube, crue, amont, pollution, affluent, saumon, poisson (river(2), lake, basin, cubic meter, swelling, upstream water, pollution, affluent, salmon, fish) 1.4 bless, casque_bleu, soldat, tir, milice, convoi, vacuer, croate, milicien, combattant (wounded, U.N. soldier, soldier, firing, militia, convoy, to evacuate, Croatian, militiaman, combatant) LM-2 2.1 eau, mtre, lac, pluie, rivire, bassin, fleuve, site, poisson, affluent, montagne, crue, valle (water, meter, lake, rain, river(2), basin, setting, fish, affluent, mountain, swelling, valley) 2.2 conducteur, trafic, routier, route, camion, chauffeur, voiture, chauffeur_routier, poids_lourd (driver, traffic, lorry driver(3), road, lorry, car, truck) 2.3 casque_bleu, soldat, tir, convoi, milicien, blind, milice, aroport, bless, incident, croate (U.N. soldier, soldier, firing, convoy, militiaman, tank, militia, airport, wounded, incident, Croatian) Table 2: Senses found by our algorithm for the word barrage both to the sport and the military domains)." ></td>
	<td class="line x" title="113:166	It should be note that barrage has only 1,104 occurrences in our corpus while it has 7,000 occurrences in the corpus of (Vronis, 2003), built by crawling from the Internet the pages found by a meta search engine queried with this word and its morphological variants." ></td>
	<td class="line x" title="114:166	This example is also a good illustration of the difference of granularity of the senses built from first order cooccurrences and those built from the second order ones." ></td>
	<td class="line x" title="115:166	The sense 1.1, which is close to the sense 1.2 as the two refers to demonstrations in relation to a category of workers, disappears when the second order cooccurrences are used." ></td>
	<td class="line x" title="116:166	Table 3 gives examples of discovered senses from first order cooccurrences only, one for French (LM-1) and two for English (LAT-1)." ></td>
	<td class="line x" title="117:166	6 Evaluation The discovering of word senses, as most of the work dedicated to the building of linguistic resources, comes up against the problem of evaluating its results." ></td>
	<td class="line x" title="118:166	The most direct way of doing it is to compare the resource to evaluate with a similar resource that is acknowledged as a golden standard." ></td>
	<td class="line x" title="119:166	For word senses, the WordNet-like lexicosemantic networks can be considered as such a standard." ></td>
	<td class="line x" title="120:166	Using this kind of networks for evaluating the word senses that we find is of course criticizable as our aim is to overcome their insufficiencies." ></td>
	<td class="line x" title="121:166	Nevertheless, as these networks are carefully controlled, such an evaluation provides at least a first judgment about the reliability of the discovered senses." ></td>
	<td class="line x" title="122:166	We chose to take up the evaluation method proposed in (Pantel and Lin, 2002)." ></td>
	<td class="line x" title="123:166	This method relies on WordNet and shows a rather good agreement between its results and human judgments (88% for Pantel and Lin)." ></td>
	<td class="line x" title="124:166	As a consequence, our evaluation was done only for English, and more precisely with WordNet 1.7." ></td>
	<td class="line x" title="125:166	For each considered word, the evaluation method tries to map one of its discovered senses with one of its synsets in WordNet by applying a specific similarity measure." ></td>
	<td class="line x" title="126:166	Hence, only the precision of the word sense discovering algorithm is evaluated but Pantel and Lin indicate that recall is not very significant in this context: a discovered sense may be correct and not present in WordNet and conversely, some senses in WordNet are very close and should be joined for most of the applications using WordNet." ></td>
	<td class="line x" title="127:166	They define a recall measure but only for ranking the results of a set of systems." ></td>
	<td class="line x" title="128:166	Hence, it cannot be applied in our case." ></td>
	<td class="line x" title="129:166	The similarity measure between a sense and a synset used for computing precision relies on the Lins similarity measure between two synsets: )2(log)1(log )(log2 )2,1( sPsP sP sssim +  = (1) where s is the most specific synset that subsumes s1 and s2 in the WordNet hierarchy and P(s) represents the probability of the synset s estimated from a reference corpus, in this case the SemCor corpus." ></td>
	<td class="line x" title="130:166	We used the implementation of this measure provided by the Perl module WordNet ::Similarity v0.06 (Patwardhan and Pedersen, 2003)." ></td>
	<td class="line x" title="131:166	The similarity between a sense and a synset is more precisely defined as the average value of the similarity values between the words that characterize the sense, or a subset of them, and the synset." ></td>
	<td class="line x" title="132:166	The similarity between a word and a synset organe (1300) 2 patient, transplantation, greffe, malade, thrapeutique, mdical, mdecine, greffer, rein (patient, transplantation, transplant, sick person, therapeutic, medical, medicine, to transplant, kidney) procration, embryon, thique, humain, relatif, biothique, corps_humain, gne, cellule (procreation, embryo, ethical, human, relative, bioethics, human body, gene, cell) constitutionnel, consultatif, constitution, instituer, excutif, lgislatif, siger, disposition (constitutional, consultative, constitution, to institute, executive, legislative, to sit, clause) article, hebdomadaire, publication, rdaction, quotidien, journal, ditorial, rdacteur (article, weekly, publication, editorial staff, daily, newspaper, editorial, sub-editor) mouse (563) compatible, software, computer, machine, user, desktop, pc, graphics, keyboard, device laboratory, researcher, cell, gene, generic, human, hormone, research, scientist, rat party (16999) candidate, democrat, republican, gubernatorial, presidential, partisan, reapportionment ballroom, cocktail, champagne, guest, bash, gala, wedding, birthday, invitation, festivity caterer, uninvited, party-goers, black-tie, hostess, buffet, glitches, napkins, catering Table 3: Senses found by our algorithm from first order cooccurrences (LM-1 and LAT-1) is equal to the highest similarity value among those between the synset and the synsets to which the word belongs to." ></td>
	<td class="line x" title="133:166	Each of these values is given by (1)." ></td>
	<td class="line x" title="134:166	A sense is mapped to the synset that is the most similar to it, providing that the similarity between them is higher than a fixed threshold (equal to 0.25 as in (Pantel and Lin, 2002))." ></td>
	<td class="line x" title="135:166	Finally, the precision for a word is given by the proportion of its senses that match one of its synsets." ></td>
	<td class="line x" title="136:166	Table 4 gives the results of the evaluation of our algorithm for the words of the English cooccurrence network that are nouns only and for which at least one sense was discovered." ></td>
	<td class="line x" title="137:166	As Pantel and Lin, we only take into account for evaluation 4 words of each sense, whatever the number of words that define it." ></td>
	<td class="line x" title="138:166	But, because of the way our senses are built, we have not a specific measure of the similarity between a word and the words that characterize its senses." ></td>
	<td class="line x" title="139:166	Hence, we computed two variants of the precision measure." ></td>
	<td class="line x" title="140:166	The first one selects the four words of each sense by relying on their number of strong links in the shared nearest neighbor graph." ></td>
	<td class="line x" title="141:166	The second one selects the four words that have the highest similarity score with one of the synsets of the target word, which is called optimal choice in Table 4 3 . A clear difference can be noted between the two variants." ></td>
	<td class="line x" title="142:166	With the optimal choice of the four words, we get results that are similar to those of Pantel and Lin: their precision is equal to 60.8 with an average number of words defining a sense equal to 14." ></td>
	<td class="line x" title="143:166	2 Each word is given with its frequency in the corpus used for building the cooccurrence network." ></td>
	<td class="line x" title="144:166	3 This selection procedure is only used for evaluation and we do no rely on WordNet for building our senses." ></td>
	<td class="line x" title="145:166	On the other hand, Table 4 shows that the words selected on the basis of their number of strong links are not strongly linked in WordNet (according to Lins measure) to their target word." ></td>
	<td class="line x" title="146:166	This does not mean that the selected words are not interesting for describing the senses of the target word but more probably that the semantic relations that they share with the target word are different from hyperonymy." ></td>
	<td class="line x" title="147:166	The results of Pantel and Lin can be explained by the fact that their algorithm is based on the clustering of similar words, i.e. words that are likely to be synonyms, and not on the clustering of the cooccurrents of a word, which are not often synonyms of that word." ></td>
	<td class="line x" title="148:166	Moreover, their initial corpus is much larger (around 144 millions words) than ours and they make use of more elaborated tools, such as a syntactic analyzer." ></td>
	<td class="line x" title="149:166	LAT-1.no LAT-2.no number of strong links 19.4 20.8 optimal choice 56.2 63.7 Table 4: Average precision of discovered senses for English in relation with WordNet As expected, the results obtained with first order cooccurrences (LAT-1.no), which produce a higher number of senses by word, are lower than the results obtained with second order cooccurrences (LAT-2.no)." ></td>
	<td class="line x" title="150:166	However, without a recall measure, it is difficult to draw a clear conclusion from this observation: some senses of LAT-1.no probably result from the artificial division of an actual word sense but the fact to have more homogeneous senses in LAT-2.no also facilitates in this case the mapping with WordNets synsets." ></td>
	<td class="line x" title="151:166	7 Related work As they rely on the detection of high-density areas in a network of cooccurrences, (Vronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours." ></td>
	<td class="line x" title="152:166	Nevertheless, two main differences can be noted with our work." ></td>
	<td class="line x" title="153:166	The first one concerns the direct use they make of the network of cooccurrences." ></td>
	<td class="line x" title="154:166	In our case, we chose a more general approach by working at the level of a similarity graph: when the similarity of two words is given by their relation of cooccurrence, our situation is comparable to the one of (Vronis, 2003) and (Dorow and Widdows, 2003); but in the same framework, we can also take into account other kinds of similarity relations, such as the second order cooccurrences." ></td>
	<td class="line x" title="155:166	The second main difference is the fact they discriminate senses in an iterative way." ></td>
	<td class="line x" title="156:166	This approach consists in selecting at each step the most obvious sense and then, to update the graph of cooccurrences by discarding the words that make up the new sense." ></td>
	<td class="line x" title="157:166	The other senses are then easier to discriminate." ></td>
	<td class="line x" title="158:166	We preferred to put emphasis on the ability to gather close or identical senses that are artificially distinguished (see Section 4.3)." ></td>
	<td class="line x" title="159:166	From a global viewpoint, these two differences lead (Vronis, 2003) and (Dorow and Widdows, 2003) to build finer senses than ours." ></td>
	<td class="line x" title="160:166	Nevertheless, as methods for discovering word senses from a corpus tend to find a too large number of close senses, it was more important from our viewpoint to favour the building of stable senses with a clear definition rather than to discriminate very fine senses." ></td>
	<td class="line x" title="161:166	8 Conclusion and future work In this article, we have presented a new method for discriminating and defining the senses of a word from a network of lexical cooccurrences." ></td>
	<td class="line x" title="162:166	This method consists in applying an unsupervised clustering algorithm, in this case the SNN algorithm, to the cooccurrents of the word by relying on the relations that these cooccurrents have in the cooccurrence network." ></td>
	<td class="line x" title="163:166	We have achieved a first evaluation based on the methodology defined in (Pantel and Lin, 2002)." ></td>
	<td class="line x" title="164:166	This evaluation has shown that in comparison with WordNet taken as a reference, the relevance of the discriminated senses is comparable to the relevance of Pantel and Lins word senses." ></td>
	<td class="line x" title="165:166	But it has also shown that the similarity between a discovered sense and a synset larity between a discovered sense and a synset of WordNet must be evaluated in our case by taking into account a larger set of semantic relations, especially those implicitly present in the glosses." ></td>
	<td class="line x" title="166:166	Moreover, an evaluation based on the use of the built senses in an application such as query expansion is necessary to determine the actual interest of this kind of resources in comparison with a lexicosemantic network such as WordNet." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J04-3002
Learning Subjective Language
Wiebe, Janyce M.;Wilson, Theresa;Bruce, Rebecca F.;Bell, Matthew;Martin, Melanie J.;"></td>
	<td class="line x" title="1:617	c 2004 Association for Computational Linguistics Learning Subjective Language Janyce Wiebe  Theresa Wilson  University of Pittsburgh University of Pittsburgh Rebecca Bruce  Matthew Bell  University of North Carolina University of Pittsburgh at Asheville Melanie Martin  New Mexico State University Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations." ></td>
	<td class="line x" title="2:617	There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization." ></td>
	<td class="line x" title="3:617	The goal of this work is learning subjective language from corpora." ></td>
	<td class="line x" title="4:617	Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity." ></td>
	<td class="line x" title="5:617	The features are also examined working together in concert." ></td>
	<td class="line x" title="6:617	The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets." ></td>
	<td class="line x" title="7:617	In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features." ></td>
	<td class="line x" title="8:617	Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article." ></td>
	<td class="line x" title="9:617	1." ></td>
	<td class="line x" title="10:617	Introduction Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations (Banfield 1982; Wiebe 1994)." ></td>
	<td class="line x" title="11:617	Many natural language processing (NLP) applications could benefit from being able to distinguish subjective language from language used to objectively present factual information." ></td>
	<td class="line x" title="12:617	Current extraction and retrieval technology focuses almost exclusively on the subject matter of documents." ></td>
	<td class="line x" title="13:617	However, additional aspects of a document influence its relevance, including evidential status and attitude (Kessler, Nunberg, Sch utze 1997)." ></td>
	<td class="line x" title="14:617	Information extraction systems should be able to distinguish between factual information (which should be extracted) and nonfactual information (which should be  Department of Computer Science, University of Pittsburgh, Pittsburgh, PA 15260." ></td>
	<td class="line x" title="15:617	E-mailwiebe,mbell}@cs.pitt.edu." ></td>
	<td class="line x" title="16:617	 Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260." ></td>
	<td class="line x" title="17:617	Email: twilson@cs.pitt.edu." ></td>
	<td class="line x" title="18:617	 Department of Computer Science, University of North Carolina at Asheville, Asheville, NC 28804." ></td>
	<td class="line x" title="19:617	E-mail: bruce@cs.unca.edu  Department of Computer Science, New Mexico State University, Las Cruces, NM 88003." ></td>
	<td class="line x" title="20:617	E-mail: mmartin@cs.nmsu.edu." ></td>
	<td class="line x" title="21:617	Submission received: 20 March 2002; Revised submission received: 30 September 2003; Accepted for publication: 23 January 2004 278 Computational Linguistics Volume 30, Number 3 discarded or labeled as uncertain)." ></td>
	<td class="line x" title="22:617	Question-answering systems should distinguish between factual and speculative answers." ></td>
	<td class="line x" title="23:617	Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources (Carbonell 1979; Wiebe et al. 2003)." ></td>
	<td class="line x" title="24:617	Multidocument summarization systems should summarize different opinions and perspectives." ></td>
	<td class="line x" title="25:617	Automatic subjectivity analysis would also be useful to perform flame recognition (Spertus 1997; Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intellectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio broadcasts (Barzialy et al. 2000), review mining (Terveen et al. 1997), review classification (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy 1987), and clustering documents by ideological point of view (Sack 1995)." ></td>
	<td class="line x" title="26:617	In general, nearly any information-seeking system could benefit from knowledge of how opinionated a text is and whether or not the writer purports to objectively present factual material." ></td>
	<td class="line x" title="27:617	To perform automatic subjectivity analysis, good clues must be found." ></td>
	<td class="line x" title="28:617	A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g. , the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no comprehensive dictionary of subjective language." ></td>
	<td class="line x" title="29:617	In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice." ></td>
	<td class="line x" title="30:617	An NLP system must disambiguate these expressions in context." ></td>
	<td class="line x" title="31:617	The goal of our work is learning subjective language from corpora." ></td>
	<td class="line x" title="32:617	In this article, we generate and test subjectivity clues and contextual features and use the knowledge we gain to recognize subjective sentences and opinionated documents." ></td>
	<td class="line x" title="33:617	Two kinds of data are available to us: a relatively small amount of data manually annotated at the expression level (i.e. , labels on individual words and phrases) of Wall Street Journal and newsgroup data and a large amount of data with existing documentlevel annotations from the Wall Street Journal (opinion pieces, such as editorials and reviews, versus nonopinion pieces)." ></td>
	<td class="line x" title="34:617	Both are used as training data to identify clues of subjectivity." ></td>
	<td class="line x" title="35:617	In addition, we cross-validate the results between the two types of annotation: The clues learned from the expression-level data are evaluated against the document-level annotations, and those learned using the document-level annotations are evaluated against the expression-level annotations." ></td>
	<td class="line x" title="36:617	There were a number of motivations behind our decision to use document-level annotations, in addition to our manual annotations, to identify and evaluate clues of subjectivity." ></td>
	<td class="line x" title="37:617	The document-level annotations were not produced according to our annotation scheme and were not produced for the purpose of training and evaluating an NLP system." ></td>
	<td class="line x" title="38:617	Thus, they are an external influence from outside the laboratory." ></td>
	<td class="line x" title="39:617	In addition, there are a great number of these data, enabling us to evaluate the results on a larger scale, using multiple large test sets." ></td>
	<td class="line x" title="40:617	This and cross-training between the two types of annotations allows us to assess consistency in performance of the various identification procedures." ></td>
	<td class="line x" title="41:617	Good performance in cross-validation experiments between different types of annotations is evidence that the results are not brittle." ></td>
	<td class="line x" title="42:617	We focus on three types of subjectivity clues." ></td>
	<td class="line x" title="43:617	The first are hapax legomena, the set of words that appear just once in the corpus." ></td>
	<td class="line x" title="44:617	We refer to them here as unique words." ></td>
	<td class="line x" title="45:617	The set of all unique words is a feature with high frequency and significantly higher precision than baseline (Section 3.2)." ></td>
	<td class="line x" title="46:617	The second are collocations (Section 3.3)." ></td>
	<td class="line x" title="47:617	We demonstrate a straightforward method for automatically identifying collocational clues of subjectivity in texts." ></td>
	<td class="line x" title="48:617	The method is first used to identify fixed n-grams, such as of the century and get out of here." ></td>
	<td class="line x" title="49:617	Interest279 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language ingly, many include noncontent words that are typically on stop lists of NLP systems (e.g. , of, the, get, out, here in the above examples)." ></td>
	<td class="line x" title="50:617	The method is then used to identify an unusual form of collocation: One or more positions in the collocation may be filled by any word (of an appropriate part of speech) that is unique in the test data." ></td>
	<td class="line x" title="51:617	The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity (Lin 1998) (Section 3.4)." ></td>
	<td class="line x" title="52:617	We hypothesized that two words may be distributionally similar because they are both potentially subjective (e.g. , tragic, sad, and poignant are identified from bizarre)." ></td>
	<td class="line x" title="53:617	In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors." ></td>
	<td class="line x" title="54:617	We show that the various subjectivity clues perform better and worse on the same data sets, exhibiting an important consistency in performance (Section 4.2)." ></td>
	<td class="line x" title="55:617	In addition to learning and evaluating clues associated with subjectivity, we address disambiguating them in context, that is, identifying instances of clues that are subjective in context (Sections 4.3 and 4.4)." ></td>
	<td class="line x" title="56:617	We find that the density of clues in the surrounding context is an important influence." ></td>
	<td class="line x" title="57:617	Using two types of annotations serves us well here, too." ></td>
	<td class="line x" title="58:617	It enables us to use manual judgments to identify parameters for disambiguating instances of automatically identified clues." ></td>
	<td class="line x" title="59:617	High-density clues are high precision in both the expression-level and document-level data." ></td>
	<td class="line x" title="60:617	In addition, we give the results of a new annotation study showing that most high-density clues are in subjective text spans (Section 4.5)." ></td>
	<td class="line x" title="61:617	Finally, we use the clues together to perform documentlevel classification, to further demonstrate the utility of the acquired knowledge (Section 4.6)." ></td>
	<td class="line x" title="62:617	At the end of the article, we discuss related work (Section 5) and conclusions (Section 6)." ></td>
	<td class="line x" title="63:617	2." ></td>
	<td class="line x" title="64:617	Subjectivity Subjective language is language used to express private states in the context of a text or conversation." ></td>
	<td class="line x" title="65:617	Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. 1985)." ></td>
	<td class="line x" title="66:617	The following are examples of subjective sentences from a variety of document types." ></td>
	<td class="line x" title="67:617	The first two examples are from Usenet newsgroup messages: (1) I had in mind your facts, buddy, not hers." ></td>
	<td class="line x" title="68:617	(2) Nice touch." ></td>
	<td class="line x" title="69:617	Alleges whenever facts posted are not in your persona of what is real. The next one is from an editorial: (3) We stand in awe of the Woodstock generations ability to be unceasingly fascinated by the subject of itself." ></td>
	<td class="line x" title="70:617	(Bad Acid, Wall Street Journal, August 17, 1989) The next example is from a book review: (4) At several different layers, its a fascinating tale." ></td>
	<td class="line x" title="71:617	(George Melloan, Whose Spying on Our Computers? Wall Street Journal, November 1, 1989) 280 Computational Linguistics Volume 30, Number 3 The last one is from a news story: (5) The cost of health care is eroding our standard of living and sapping industrial strength, complains Walter Maher, a Chrysler health-and-benefits specialist." ></td>
	<td class="line x" title="72:617	(Kenneth H. Bacon, Business and Labor Reach a Consensus on Need to Overhaul Health-Care System, Wall Street Journal, November 1, 1989) In contrast, the following are examples of objective sentences, sentences without significant expressions of subjectivity: (6) Bell Industries Inc. increased its quarterly to 10 cents from 7 cents a share." ></td>
	<td class="line x" title="73:617	(7) Northwest Airlines settled the remaining lawsuits filed on behalf of 156 people killed in a 1987 crash, but claims against the jetliners maker are being pursued, a federal judge said." ></td>
	<td class="line x" title="74:617	(Northwest Airlines Settles Rest of Suits, Wall Street Journal, November 1, 1989) A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues." ></td>
	<td class="line x" title="75:617	It is most fully presented in Wiebe and Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994)." ></td>
	<td class="line x" title="76:617	It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory." ></td>
	<td class="line x" title="77:617	The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) (theory of subjectivity versus communication)." ></td>
	<td class="line x" title="78:617	1 The remainder of this section sketches our conceptualization of subjectivity and describes the annotation projects it underlies." ></td>
	<td class="line x" title="79:617	Subjective elements are linguistic expressions of private states in context." ></td>
	<td class="line x" title="80:617	Subjective elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3) and eroding, sapping, and complains in (5))." ></td>
	<td class="line x" title="81:617	They may be single words (e.g. , complains) or more complex expressions (e.g. , stand in awe, what a NP)." ></td>
	<td class="line x" title="82:617	Purely syntactic or morphological devices may also be subjective elements (e.g. , fronting, parallelism, changes in aspect)." ></td>
	<td class="line x" title="83:617	A subjective element expresses the subjectivity of a source, who may be the writer or someone mentioned in the text." ></td>
	<td class="line x" title="84:617	For example, the source of fascinating in (4) is the writer, while the source of the subjective elements in (5) is Maher (according to the writer)." ></td>
	<td class="line x" title="85:617	In addition, a subjective element usually has a target, that is, what the subjectivity is about or directed toward." ></td>
	<td class="line x" title="86:617	In (4), the target is a tale; in (5), the target of Mahers subjectivity is the cost of health care." ></td>
	<td class="line x" title="87:617	Note our parenthetical aboveaccording to the writerconcerning Mahers subjectivity." ></td>
	<td class="line x" title="88:617	Maher is not directly speaking to us but is being quoted by the writer." ></td>
	<td class="line x" title="89:617	Thus, the source is a nested source, which we notate (writer, Maher); this represents the fact that the subjectivity is being attributed to Maher by the writer." ></td>
	<td class="line x" title="90:617	Since sources 1 For additional citations to relevant work from outside NLP, please see Banfield (1982), Fludernik (1993), Wiebe (1994), and Stein and Wright (1995)." ></td>
	<td class="line x" title="91:617	281 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language are not directly addressed by the experiments presented in this article, we merely illustrate the idea here with an example, to give the reader an idea: The Foreign Ministry said Thursday that it was surprised, to put it mildly by the U.S. State Departments criticism of Russias human rights record and objected in particular to the odious section on Chechnya." ></td>
	<td class="line x" title="92:617	(Moscow Times, March 8, 2002] Let us consider some of the subjective elements in this sentence, along with their sources: surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry) to put it mildly: (writer, Foreign Ministry) criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department) objected: (writer, Foreign Ministry) odious: (writer, Foreign Ministry) Consider surprised, to put it mildly." ></td>
	<td class="line x" title="93:617	This refers to a private state of the Foreign Ministry (i.e. , it is very surprised)." ></td>
	<td class="line x" title="94:617	This is in the context of The Foreign Ministry said, which is in a sentence written by the writer." ></td>
	<td class="line x" title="95:617	This gives us the three-level source (writer, Foreign Ministry, Foreign Ministry)." ></td>
	<td class="line x" title="96:617	The phrase to put it mildly, which expresses sarcasm, is attributed to the Foreign Ministry by the writer (i.e. , according to the writer, the Foreign Ministry said this)." ></td>
	<td class="line x" title="97:617	So its source is (writer, Foreign Ministry)." ></td>
	<td class="line x" title="98:617	The subjective element criticism has a deeply nested source: According to the writer, the Foreign Ministry said it is surprised by the U.S. State Departments criticism." ></td>
	<td class="line x" title="99:617	The nested-source representation allows us to pinpoint the subjectivity in a sentence." ></td>
	<td class="line x" title="100:617	For example, there is no subjectivity attributed directly to the writer in the above sentence: At the level of the writer, the sentence merely says that someone said something and objected to something (without evaluating or questioning this)." ></td>
	<td class="line x" title="101:617	If the sentence started The magnificent Foreign Ministry said, then we would have an additional subjective element, magnificent, with source (writer)." ></td>
	<td class="line x" title="102:617	Note that subjective does not mean not true." ></td>
	<td class="line x" title="103:617	Consider the sentence John criticized Mary for smoking." ></td>
	<td class="line x" title="104:617	The verb criticized is a subjective element, expressing negative evaluation, with nested source (writer, John)." ></td>
	<td class="line x" title="105:617	But this does not mean that John does not believe that Mary smokes." ></td>
	<td class="line x" title="106:617	(In addition, the fact that John criticized Mary is being presented as true by the writer.)" ></td>
	<td class="line x" title="107:617	Similarly, objective does not mean true." ></td>
	<td class="line x" title="108:617	A sentence is objective if the language used to convey the information suggests that facts are being presented; in the context of the discourse, material is objectively presented as if it were true." ></td>
	<td class="line x" title="109:617	Whether or not the source truly believes the information, and whether or not the information is in fact true, are considerations outside the purview of a theory of linguistic subjectivity." ></td>
	<td class="line x" title="110:617	An aspect of subjectivity highlighted when we are working with NLP applications is ambiguity." ></td>
	<td class="line x" title="111:617	Many words with subjective usages may be used objectively." ></td>
	<td class="line x" title="112:617	Examples are sapping and eroding." ></td>
	<td class="line x" title="113:617	In (5), they are used subjectively, but one can easily imagine objective usages, in a scientific domain, for example." ></td>
	<td class="line x" title="114:617	Thus, an NLP system may not merely consult a list of lexical items to accurately identify subjective language but must disambiguate words, phrases, and sentences in context." ></td>
	<td class="line x" title="115:617	In our terminology, a potential subjective element (PSE) is a linguistic element that may be used to express 282 Computational Linguistics Volume 30, Number 3 Table 1 Data Sets and Annotations used in Experiments." ></td>
	<td class="line x" title="116:617	Annotators M, MM, and T are co-authors of this paper." ></td>
	<td class="line x" title="117:617	D and R are not." ></td>
	<td class="line x" title="118:617	Name Source Number of Words Annotators Type of annotation WSJ-SE Wall Street Journal 18,341 D,M Subjective elements NG-SE Newsgroup 15,413 M Subjective elements NG-FE Newsgroup 88,210 MM,R Flame elements OP1 Wall Street Journal 640,975 M,T Documents Composed of 4 data sets: W9-4,W9-10,W9-22,W-33 OP2 Wall Street Journal 629,690 M,T Documents Composed of 4 data sets: W9-2,W9-20,W9-21,W-23 subjectivity." ></td>
	<td class="line x" title="119:617	A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe 1994)." ></td>
	<td class="line x" title="120:617	In this article, we focus on learning lexical items that are associated with subjectivity (i.e. , PSEs) and then using them in concert to disambiguate instances of them (i.e. , to determine whether the instances are subjective elements)." ></td>
	<td class="line x" title="121:617	2.1 Manual Annotations In our subjectivity annotation projects, we do not give the annotators lists of particular words and phrases to look for." ></td>
	<td class="line x" title="122:617	Rather, we ask them to label sentences according to their interpretations in context." ></td>
	<td class="line x" title="123:617	As a result, the annotators consider a large variety of expressions when performing annotations." ></td>
	<td class="line x" title="124:617	We use data that have been manually annotated at the expression level, the sentence level, and the document level." ></td>
	<td class="line x" title="125:617	For diversity, we use data from the Wall Street Journal Treebank as well as data from a corpus of Usenet newsgroup messages." ></td>
	<td class="line x" title="126:617	Table 1 summarizes the data sets and annotations used in this article." ></td>
	<td class="line x" title="127:617	None of the datasets overlap." ></td>
	<td class="line x" title="128:617	The annotation types listed in the table are those used in the experiments presented in this article." ></td>
	<td class="line x" title="129:617	In our first subjectivity annotation project (Wiebe, Bruce, and OHara 1999; Bruce and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus (Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated at the sentence level by multiple judges." ></td>
	<td class="line x" title="130:617	The judges were instructed to classify a sentence as subjective if it contained any significant expressions of subjectivity, attributed to either the writer or someone mentioned in the text, and to classify the sentence as objective, otherwise." ></td>
	<td class="line x" title="131:617	After multiple rounds of training, the annotators independently annotated a fresh test set of 500 sentences from WSJ-SE." ></td>
	<td class="line x" title="132:617	They achieved an average pairwise kappa score of 0.70 over the entire test set, an average pairwise kappa score of 0.80 for the 85% of the test set for which the annotators were somewhat sure of their judgments, and an average pairwise kappa score of 0.88 for the 70% of the test set for which the annotators were very sure of their judgments." ></td>
	<td class="line x" title="133:617	We later asked the same annotators to identify the subjective elements in WSJSE." ></td>
	<td class="line x" title="134:617	Specifically, each annotator was given the subjective sentences he identified in 283 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language the previous study and asked to put brackets around the words he believed caused the sentence to be classified as subjective." ></td>
	<td class="line x" title="135:617	2 For example (subjective elements are in parentheses): They paid (yet) more for (really good stuff)." ></td>
	<td class="line x" title="136:617	(Perhaps youll forgive me) for reposting his response." ></td>
	<td class="line x" title="137:617	No other instructions were given to the annotators and no training was performed for the expression-level task." ></td>
	<td class="line x" title="138:617	A single round of tagging was performed, with no communication between annotators." ></td>
	<td class="line x" title="139:617	There are techniques for analyzing agreement when annotations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera, and Amorortu 1999), but our focus in this article is on words." ></td>
	<td class="line x" title="140:617	Thus, our analyses are at the word level: Each word is classified as either appearing in a subjective element or not." ></td>
	<td class="line x" title="141:617	Punctuation and numbers are excluded from the analyses." ></td>
	<td class="line x" title="142:617	The kappa value for word agreement in this study is 0.42." ></td>
	<td class="line x" title="143:617	Another two-level annotation project was performed in Wiebe et al.(2001), this time involving document-level and expression-level annotations of newsgroup data (NG-FE in Table 1)." ></td>
	<td class="line x" title="145:617	In that project, we were interested in annotating flames, inflammatory messages in newsgroups or listservs." ></td>
	<td class="line x" title="146:617	Note that inflammatory language is a kind of subjective language." ></td>
	<td class="line x" title="147:617	The annotators were instructed to mark a message as a flame if the main intention of the message is a personal attack and the message contains insulting or abusive language." ></td>
	<td class="line x" title="148:617	After multiple rounds of training, three annotators independently annotated a fresh test set of 88 messages from NG-FE." ></td>
	<td class="line x" title="149:617	The average pairwise percentage agreement is 92% and the average pairwise kappa value is 0.78." ></td>
	<td class="line x" title="150:617	These results are comparable to those of Spertus (1997), who reports 98% agreement on noninflammatory messages and 64% agreement on inflammatory messages." ></td>
	<td class="line x" title="151:617	Two of the annotators were then asked to identify the flame elements in the entire corpus NG-FE." ></td>
	<td class="line x" title="152:617	Flame elements are the subset of subjective elements that are perceived to be inflammatory." ></td>
	<td class="line x" title="153:617	The two annotators were asked to do this in the entire corpus, even those messages not identified as flames, because messages that were not judged to be flames at the document level may contain some individual inflammatory phrases." ></td>
	<td class="line x" title="154:617	As above, no training was performed for the expression-level task, and a single round of tagging was performed, without communication between annotators." ></td>
	<td class="line x" title="155:617	Agreement was measured in the same way as in the subjective-element study above." ></td>
	<td class="line x" title="156:617	The kappa value for flame element annotations in corpus NG-FE is 0.46." ></td>
	<td class="line x" title="157:617	An additional annotation project involved a single annotator, who performed subjective-element annotations on the newsgroup corpus NG-SE." ></td>
	<td class="line x" title="158:617	The agreement results above suggest that good levels of agreement can be achieved at higher levels of classification (sentence and document), but agreement at the expression level is more challenging." ></td>
	<td class="line x" title="159:617	The agreement values are lower for the expression-level annotations but are still much higher than that expected by chance." ></td>
	<td class="line x" title="160:617	Note that our word-based analysis of agreement is a tough measure, because it requires that exactly the same words be identified by both annotators." ></td>
	<td class="line x" title="161:617	Consider the following example from WSJ-SE: D: (played the role well) (obligatory ragged jeans a thicket of long hair and rejection of all things conventional) 2 We are grateful to Aravind Joshi for suggesting this level of annotation." ></td>
	<td class="line x" title="162:617	284 Computational Linguistics Volume 30, Number 3 M: played the role (well) (obligatory) (ragged) jeans a (thicket) of long hair and (rejection) of (all things conventional) Judge D in the example consistently identifies entire phrases as subjective, while judge M prefers to select discrete lexical items." ></td>
	<td class="line x" title="163:617	Despite such differences between annotators, the expression-level annotations proved very useful for exploring hypotheses and generating features, as described below." ></td>
	<td class="line x" title="164:617	Since this article was written, a new annotation project has been completed." ></td>
	<td class="line x" title="165:617	A 10,000-sentence corpus of English-language versions of world news articles has been annotated with detailed subjectivity information as part of a project investigating multiple-perspective question answering (Wiebe et al. 2003)." ></td>
	<td class="line x" title="166:617	These annotations are much more detailed than the annotations used in this article (including, for example, the source of each private state)." ></td>
	<td class="line x" title="167:617	The interannotator agreement scores for the new corpus are high and are improvements over the results of the studies described above (Wilson and Wiebe 2003)." ></td>
	<td class="line x" title="168:617	The current article uses existing document-level subjective classes, namely editorials, letters to the editor, Arts & Leisure reviews, and Viewpoints in the Wall Street Journal." ></td>
	<td class="line x" title="169:617	These are subjective classes in the sense that they are text categories for which subjectivity is a key aspect." ></td>
	<td class="line x" title="170:617	We refer to them collectively as opinion pieces." ></td>
	<td class="line x" title="171:617	All other types of documents in the Wall Street Journal are collectively referred to as nonopinion pieces." ></td>
	<td class="line x" title="172:617	Note that opinion pieces are not 100% subjective." ></td>
	<td class="line x" title="173:617	For example, editorials contain objective sentences presenting facts supporting the writers argument, and reviews contain sentences objectively presenting facts about the product beign reviewed." ></td>
	<td class="line x" title="174:617	Similarly, nonopinion pieces are not 100% objective." ></td>
	<td class="line x" title="175:617	News reports present opinions and reactions to reported events (van Dijk 1988); they often contain segments starting with expressions such as critics claim and supporters argue." ></td>
	<td class="line x" title="176:617	In addition, quoted-speech sentences in which individuals express their subjectivity are often included (Barzilay et al. 2000)." ></td>
	<td class="line x" title="177:617	For concreteness, let us consider WSJ-SE, which, recall, has been manually annotated at the sentence level." ></td>
	<td class="line x" title="178:617	In WSJ-SE, 70% of the sentences in opinion pieces are subjective and 30% are objective." ></td>
	<td class="line x" title="179:617	In nonopinion pieces, 44% of the sentences are subjective and only 56% are objective." ></td>
	<td class="line x" title="180:617	Thus, while there is a higher concentration of subjective sentences in opinion versus nonopinion pieces, there are many subjective sentences in nonopinion pieces and objective sentences in opinion pieces." ></td>
	<td class="line x" title="181:617	An inspection of some data reveals that some editorial and review articles are not marked as such by the Wall Street Journal." ></td>
	<td class="line x" title="182:617	For example, there are articles whose purpose is to present an argument rather than cover a news story, but they are not explicitly labeled as editorials by the Wall Street Journal." ></td>
	<td class="line x" title="183:617	Thus, the opinion piece annotations of data sets OP1 and OP2 in Table 1 have been manually refined." ></td>
	<td class="line x" title="184:617	The annotation instructions were simply to identify any additional opinion pieces that were not marked as such." ></td>
	<td class="line x" title="185:617	To test the reliability of this annotation, two judges independently annotated two Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000 words." ></td>
	<td class="line x" title="186:617	This is an annotation lite task: With no training, the annotators achieved kappa values of 0.94 and 0.95, and each spent an average of three hours per Wall Street Journal file." ></td>
	<td class="line x" title="187:617	3." ></td>
	<td class="line x" title="188:617	Generating and Testing Subjective Features 3.1 Introduction The goal in this section is to learn lexical subjectivity clues of various types, single words as well as collocations." ></td>
	<td class="line x" title="189:617	Some require no training data, some are learned us285 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language ing the expression-level subjective-element annotations as training data, and some are learned using the document-level opinion piece annotations as training data (i.e. , opinion piece versus nonopinion piece)." ></td>
	<td class="line x" title="190:617	All of the clues are evaluated with respect to the document-level opinion piece annotations." ></td>
	<td class="line x" title="191:617	While these evaluations are our focus, because many more opinion piece than subjective-element data exist, we do evaluate the clues learned from the opinion piece data on the subjective-element data as well." ></td>
	<td class="line x" title="192:617	Thus, we cross-validate the results both ways between the two types of annotations." ></td>
	<td class="line x" title="193:617	Throughout this section, we evaluate sets of clues directly, by measuring the proportion of clues that appear in subjective documents or expressions, seeking those that appear more often than expected." ></td>
	<td class="line x" title="194:617	In later sections, the clues are used together to find subjective sentences and to perform text categorization." ></td>
	<td class="line x" title="195:617	The following paragraphs give details of the evaluation and experimental design used in this section." ></td>
	<td class="line x" title="196:617	The proportion of clues in subjective documents or expressions is their precision." ></td>
	<td class="line x" title="197:617	Specifically, the precision of a set S with respect to opinion pieces is prec(S)= number of instances of members of S in opinion pieces total number of instances of members of S in the data The precision of a set S with respect to subjective elements is prec(S)= number of instances of members of S in subjective elements total number of instances of members of S in the data In the above, S is a set of types (not tokens)." ></td>
	<td class="line x" title="198:617	The counts are of tokens (i.e. , instances or occurrences) of members of S. Why use a set rather than individual items?" ></td>
	<td class="line x" title="199:617	Many good clues of subjectivity occur with low frequency (Wiebe, McKeever, and Bruce 1998)." ></td>
	<td class="line x" title="200:617	In fact, as we shall see below, uniqueness in the corpus is an informative feature for subjectivity classification." ></td>
	<td class="line x" title="201:617	Thus, we do not want to discard low-frequency clues, because they are a valuable source of information, and we do not want to evaluate individual low-frequency lexical items, because the results would be unreliable." ></td>
	<td class="line x" title="202:617	Our strategy is thus to identify and evaluate sets of words and phrases, rather than individual items." ></td>
	<td class="line x" title="203:617	What kinds of results may we expect?" ></td>
	<td class="line x" title="204:617	We cannot expect absolutely high precision with respect to the opinion piece classifications, even for strong clues, for three reasons." ></td>
	<td class="line x" title="205:617	First, for our purposes, the data are noisy." ></td>
	<td class="line x" title="206:617	As mentioned above, while the proportion of subjective sentences is higher in opinion than in nonopinion pieces, the proportions are not 100 and 0: Opinion pieces contain objective sentences, and nonopinion pieces contain subjective sentences." ></td>
	<td class="line x" title="207:617	Second, we are trying to learn lexical items associated with subjectivity, that is, PSEs." ></td>
	<td class="line x" title="208:617	As discussed above, many words and phrases with subjective usages have objective usages as well." ></td>
	<td class="line x" title="209:617	Thus, even in perfect data with no noise, we would not expect 100% precision." ></td>
	<td class="line x" title="210:617	(This is the motivation for the work on density presented in section 4.4.) Third, the distribution of opinions and nonopinions is highly skewed in favor of nonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinion pieces." ></td>
	<td class="line x" title="211:617	In this work, increases in precision over a baseline precision are used as evidence that promising sets of PSEs have been found." ></td>
	<td class="line x" title="212:617	Our main baseline for comparison is the number of word instances in opinion pieces, divided by the total number of word instances: Baseline Precision = number of word instances in opinion pieces total number of word instances 286 Computational Linguistics Volume 30, Number 3 Table 2 Frequencies and increases in precision of unique words in subjective-element data." ></td>
	<td class="line x" title="213:617	Baseline frequency is the total number of words, and baseline precision is the proportion of words in subjective elements." ></td>
	<td class="line x" title="214:617	WSJ-SE D M freq +prec +prec Unique words 2,615 +.07 +.12 Baseline 18,341.07 .08 Words and phrases with higher proportions than this appear more than expected in opinion pieces." ></td>
	<td class="line x" title="215:617	To further evaluate the quality of a set of PSEs, we also perform the following significance test." ></td>
	<td class="line x" title="216:617	For a set of PSEs in a given data set, we test the significance of the difference between (1) the proportion of words in opinion pieces that are PSEs and (2) the proportion of words in nonopinion pieces that are PSEs, using the z-significance test for two proportions." ></td>
	<td class="line x" title="217:617	Before we continue, there are a few more technical items to mention concerning the data preparation and experimental design:  All of the data sets are stemmed using Karps morphological analyzer (Karp et al. 1994) and part-of-speech tagged using Brills (1992) tagger." ></td>
	<td class="line x" title="218:617	 When the opinion piece classifications are used for training, the existing classifications, assigned by the Wall Street Journal, are used." ></td>
	<td class="line x" title="219:617	Thus, the processes using them as training data may be applied to more data to learn more clues, without requiring additional manual annotation." ></td>
	<td class="line x" title="220:617	 When the opinion piece data are used for testing, the manually refined classifications (described at the end of Section 2.1) are used." ></td>
	<td class="line x" title="221:617	 OP1 and OP2 together comprise eight treebank files." ></td>
	<td class="line x" title="222:617	Below, we often give results separately for the component files, allowing us to assess the consistency of results for the various types of clues." ></td>
	<td class="line x" title="223:617	3.2 Unique Words In this section, we show that low-frequency words are associated with subjectivity in both the subjective-element and opinion piece data." ></td>
	<td class="line x" title="224:617	Apparently, people are creative when they are being opinionated." ></td>
	<td class="line x" title="225:617	Table 2 gives results for unique words in subjective-element data." ></td>
	<td class="line x" title="226:617	Recall that unique words are those that appear just once in the corpus, that is, hapax legomena." ></td>
	<td class="line x" title="227:617	The first row of Table 2 gives the frequency of unique words in WSJ-SE, followed by the percentage-point improvements in precision over baseline for unique words in subjective elements marked by two annotators (denoted as D and M in the table)." ></td>
	<td class="line x" title="228:617	The second row gives baseline frequency and precisions." ></td>
	<td class="line x" title="229:617	Baseline frequency is the total number of words in WSJ-SE." ></td>
	<td class="line x" title="230:617	Baseline precision for an annotator is the proportion of words included in subjective elements by that annotator." ></td>
	<td class="line x" title="231:617	Specifically, consider annotator M. The baseline precision of words in subjective elements marked by M is 0.08, 287 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 3 Frequencies and increases in precision for words that appear exactly once in the data sets composing OP1." ></td>
	<td class="line x" title="232:617	For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="233:617	W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec Unique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11 Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14 but the precision of unique words in these same annotations is 0.20, 0.12 points higher than the baseline." ></td>
	<td class="line x" title="234:617	This is a 150% improvement over the baseline." ></td>
	<td class="line x" title="235:617	The number of unique words in opinion pieces is also higher than expected." ></td>
	<td class="line x" title="236:617	Table 3 compares the precision of the set of unique words to the baseline precision (i.e. , the precision of the set of all words that appear in the corpus) in the four WSJ files composing OP1." ></td>
	<td class="line x" title="237:617	Before this analysis was performed, numbers were removed from the data (we are not interested in the fact that, say, the number 163,213.01 appears just once in the corpus)." ></td>
	<td class="line x" title="238:617	The number of words in each data set and baseline precisions are listed at the bottom of the table." ></td>
	<td class="line x" title="239:617	The freq columns give total frequencies." ></td>
	<td class="line x" title="240:617	The +prec columns show the percentage-point improvements in precision over baseline." ></td>
	<td class="line x" title="241:617	For example, in W9-10, unique words have precision 0.34: 0.18 baseline plus an improvement over baseline of 0.16." ></td>
	<td class="line x" title="242:617	The difference in the proportion of words that are unique in opinion pieces and the proportion of words that are unique in nonopinion pieces is highly significant, with p < 0.001 (z  22) for all of the data sets." ></td>
	<td class="line x" title="243:617	Note that not only does the set of unique words have higher than baseline precision, the set is a frequent feature." ></td>
	<td class="line x" title="244:617	The question arises, how does corpus size affect the precision of the set of unique words?" ></td>
	<td class="line x" title="245:617	Presumably, uniqueness in a larger corpus is more meaningful than uniqueness in a smaller one." ></td>
	<td class="line x" title="246:617	The results in Figure 1 provide evidence that it is. The y-axis in Figure 1 represents increase in precision over baseline and the x-axis represents corpus size." ></td>
	<td class="line x" title="247:617	Five graphs are plotted, one for the set of words that appear exactly once (uniques), one for the set of words that appear exactly twice ( freq2), one for the set of words that appear exactly three times ( freq3), etc. In Figure 1, increases in precision are given for corpora of size n, where n = 20, 40,, 2420, 2440 documents." ></td>
	<td class="line x" title="248:617	Each data point is an average over 25 sample corpora of size n. The sample corpora were chosen from the concatenation of OP1 and OP2, in which 9% of the documents are opinion pieces." ></td>
	<td class="line x" title="249:617	The sample corpora were created by randomly selecting documents from the large corpus, preserving the 9% distribution of opinion pieces." ></td>
	<td class="line x" title="250:617	At the smallest corpus size (containing 20 documents), the average number of words is 9,617." ></td>
	<td class="line x" title="251:617	At the largest corpus size (containing 2440 documents), the average is 1,225,186 words." ></td>
	<td class="line x" title="252:617	As can be seen in the figure, the precision of unique and other low-frequency words increases with corpus size, with increases tapering off at the largest corpus size tested." ></td>
	<td class="line x" title="253:617	Words with frequency 2 also realize a nice increase, although one that is not as dramatic, in precision over baseline." ></td>
	<td class="line x" title="254:617	Even words of frequency 3, 4, and 5 show modest increases." ></td>
	<td class="line x" title="255:617	To help us understand the importance of low-frequency words in large as opposed to small data sets, we can consider the following analogy." ></td>
	<td class="line x" title="256:617	With collectible trading cards, rare cards are the most valuable." ></td>
	<td class="line x" title="257:617	However, if we have some cards and are trying to determine thier value, looking in only a few packs of cards will not tell us if 288 Computational Linguistics Volume 30, Number 3 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18 0.20 20 620 1220 1820 2420 Corpus Size (documents) Increase in Precision uniques freq2 freq3 freq4 freq5 Figure 1 Precision of low-frequency words as corpus size increases." ></td>
	<td class="line x" title="258:617	any of our cards are valuable." ></td>
	<td class="line x" title="259:617	Only by looking at many packs of cards can we make a determination as to which are the rare ones." ></td>
	<td class="line x" title="260:617	Only in samples of sufficient size is uniqueness informative." ></td>
	<td class="line x" title="261:617	The results in this section suggest that an NLP system using uniqueness features to recognize subjectivity should determine uniqueness with respect to the test data augmented with an additional store of (unannotated) data." ></td>
	<td class="line x" title="262:617	3.3 Identifying Potentially Subjective Collocations from Subjective-Element and Flame-Element Annotations In this section, we describe experiments in identifying potentially subjective collocations." ></td>
	<td class="line x" title="263:617	Collocations are selected from the subjective-element data (i.e. , NG-SE, NG-FE, and WSJ-SE), using the union of the annotators tags for the data sets tagged by multiple taggers." ></td>
	<td class="line x" title="264:617	The results are then evaluated on opinion piece data." ></td>
	<td class="line x" title="265:617	The selection procedure is as follows." ></td>
	<td class="line x" title="266:617	First, all 1-grams, 2-grams, 3-grams, and 4-grams are extracted from the data." ></td>
	<td class="line x" title="267:617	In this work, each constituent of an n-gram is a word-stem, part-of-speech pair." ></td>
	<td class="line x" title="268:617	For example, (in-prep the-det can-noun) is a 3-gram that matches trigrams consisting of preposition in, followed by determiner the, and ending with noun can." ></td>
	<td class="line x" title="269:617	A subset of the n-grams are then selected based on precision." ></td>
	<td class="line x" title="270:617	The precision of an n-gram is the number of subjective instances of that n-gram in the data divided by the total number of instances of that n-gram in the data." ></td>
	<td class="line x" title="271:617	An instance of an n-gram is subjective if each word occurs in a subjective element in the data." ></td>
	<td class="line x" title="272:617	n-grams are selected based on two criteria." ></td>
	<td class="line x" title="273:617	First, the precision of the n-gram must be greater than the baseline precision (i.e. , the proportion of all word instances that 289 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language are in subjective elements)." ></td>
	<td class="line x" title="274:617	Second, the precision of the n-gram must be greater than the maximum precision of its constituents." ></td>
	<td class="line x" title="275:617	This criterion is used to avoid selecting unnecessarily long collocations." ></td>
	<td class="line x" title="276:617	For example, scumbag is a strongly subjective clue." ></td>
	<td class="line x" title="277:617	If be a scumbag does not have higher precision than scumbag alone, we do not want to select it." ></td>
	<td class="line x" title="278:617	Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2." ></td>
	<td class="line x" title="279:617	(W1,W2) is identified as a potential subjective element if prec(W1, W2)  0.1 and: prec(W1, W2) > max(prec(W1), prec(W2)) For trigrams, we extend the second condition as follows." ></td>
	<td class="line x" title="280:617	Let (W1, W2, W3) be a trigram consisting of consecutive words W1, W2, and W3." ></td>
	<td class="line x" title="281:617	The condition is then prec(W1, W2, W3) > max(prec(W1, W2), prec(W3)) or prec(W1, W2, W3) > max(prec(W1), prec(W2, W3)) The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gram first with the maximum of the precisions of word W1 and trigram (W2, W3, W4) and then with the maximum of the precisions of trigram (W1,W2,W3) and word W4." ></td>
	<td class="line x" title="282:617	We call the n-gram collocations identified as above fixed-n-grams." ></td>
	<td class="line x" title="283:617	We also define a type of collocation called a unique generalized n-gram (ugen-ngram)." ></td>
	<td class="line x" title="284:617	Such collocations have placeholders for unique words." ></td>
	<td class="line x" title="285:617	As will be seen below, these are our highest-precision features." ></td>
	<td class="line x" title="286:617	To find and select such generalized collocations, we first find every word that appears just once in the corpus and replace it with a new word, UNIQUE (but remembering the part of speech of the original word)." ></td>
	<td class="line x" title="287:617	In essence, we treat the set of single-instance words as a single, frequently occurring word (which occurs with various parts of speech)." ></td>
	<td class="line x" title="288:617	Precisely the same method used for extracting and selecting n-grams above is used to obtain the potentially subjective collocations with one or more positions filled by a UNIQUE, part-of-speech pair." ></td>
	<td class="line x" title="289:617	To test the ugen-n-grams extracted from the subjective-element training data using the method outlined above, we assess their precision with respect to opinion piece data." ></td>
	<td class="line x" title="290:617	As with the training data, all unique words in the test data are replaced by UNIQUE." ></td>
	<td class="line x" title="291:617	When a ugen-n-gram is matched against the test data, the UNIQUE fillers match words (of the appropriate parts of speech) that are unique in the test data." ></td>
	<td class="line x" title="292:617	Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patterns identified as described above on the four data sets composing OP1." ></td>
	<td class="line x" title="293:617	The freq columns give total frequencies, and the +prec columns show the improvements in precision from the baseline." ></td>
	<td class="line x" title="294:617	The number of words in each data set and baseline precisions are given at the bottom of the table." ></td>
	<td class="line x" title="295:617	For all n-gram features besides the fixed-4-grams and ugen-4-grams, the proportion of features in opinion pieces is significantly greater than the proportion of features in nonopinion pieces." ></td>
	<td class="line x" title="296:617	3 The question arises, how much overlap is there between instances of fixed-n-grams and instances of ugen-n-grams?" ></td>
	<td class="line x" title="297:617	In the test data of Table 4, there are a total of 8,577 fixed-n-grams instances." ></td>
	<td class="line x" title="298:617	Only 59 of these, fewer than 1% are contained (wholly or in part) in ugen-n-gram instances." ></td>
	<td class="line x" title="299:617	This small intersection set shows that two different types of potentially subjective collocations are being recognized." ></td>
	<td class="line x" title="300:617	3 Specifically, the difference between (1) the number of feature instances in opinion pieces divided by the number of words in opinion pieces and (2) the number of feature instances in nonopinion pieces divided by the number of words in nonopinion pieces is significant (p < 0.05) for all data sets." ></td>
	<td class="line x" title="301:617	290 Computational Linguistics Volume 30, Number 3 Table 4 Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocations learned from the subjective-element data." ></td>
	<td class="line x" title="302:617	For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="303:617	W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec fixed-2-grams 1,840 +.07 1,972 +.07 1,933 +.04 1,839 +.05 ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17 fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05 ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15 fixed-4-grams 18 +.15 17 +.06 12 +.29 14 .07 ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25 baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14 Randomly selected examples of our learned collocations that appear in the test data are given in Tables 5 and 6." ></td>
	<td class="line x" title="304:617	It is interesting to note that the unique generalized collocations were learned from the training data by their matching different unique words from the ones they match in the test data." ></td>
	<td class="line x" title="305:617	3.4 Generating Features from Document-Level Annotations Using Distributional Similarity In this section, we identify adjective and verb PSEs using distributional similarity." ></td>
	<td class="line x" title="306:617	Opinion-piece data are used for training, and (a different set of) opinion-piece data and the subjective-element data are used for testing." ></td>
	<td class="line x" title="307:617	With distributional similarity, words are judged to be more or less similar based on their distributional patterning in text (Lee 1999; Lee and Pereira 1999)." ></td>
	<td class="line x" title="308:617	Our Table 5 Random sample of fixed-3-gram collocations in OP1." ></td>
	<td class="line x" title="309:617	one-noun of-prep his-det worst-adj of-prep all-det quality-noun of-prep the-det to-prep do-verb so-adverb in-prep the-det company-noun you-pronoun and-conj your-pronoun have-verb taken-verb the-det rest-noun of-prep us-pronoun are-verb at-prep least-adj but-conj if-prep you-pronoun as-prep a-det weapon-noun continue-verb to-to do-verb purpose-noun of-prep the-det could-modal have-verb be-verb it-pronoun seem-verb to-prep to-pronoun continue-verb to-prep have-verb be-verb the-det do-verb something-noun about-prep cause-verb you-pronoun to-to evidence-noun to-to back-adverb that-prep you-pronoun are-verb i-pronoun be-verb not-adverb of-prep the-det century-noun of-prep money-noun be-prep 291 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 6 Random sample of unique generalized collocations in OP1." ></td>
	<td class="line x" title="310:617	U: UNIQUE." ></td>
	<td class="line x" title="311:617	Pattern Instances U-adj as-prep: drastic as; perverse as; predatory as U-adj in-prep: perk in; unsatisfying in; unwise in U-adverb U-verb: adroitly dodge; crossly butter; unceasingly fascinate U-noun back-adverb: cutting back; hearken back U-verb U-adverb: coexist harmoniously; flouncing tiresomely ad-noun U-noun: ad hoc; ad valorem any-det U-noun: any over-payment; any tapings; any write-off are-verb U-noun: are escapist; are lowbrow; are resonance but-conj U-noun: but belch; but cirrus; but ssa different-adj U-noun: different ambience; different subconferences like-prep U-noun: like hoffmann; like manute; like woodchuck national-adj U-noun: national commonplace; national yonhap particularly-adverb U-adj: particularly galling; particularly noteworthy so-adverb U-adj: so monochromatic; so overbroad; so permissive this-det U-adj: this biennial; this inexcusable; this scurrilous your-pronoun U-noun: your forehead; your manuscript; your popcorn U-adj and-conj U-adj: arduous and raucous; obstreperous and abstemious U-noun be-verb a-det: acyclovir be a; siberia be a U-noun of-prep its-pronoun: outgrowth of its; repulsion of its U-verb and-conj U-verb: wax and brushed; womanize and booze U-verb to-to a-det: cling to a; trek to a are-verb U-adj to-to: are opaque to; are subject to a-det U-noun and-conj: a blindfold and; a rhododendron and a-det U-verb U-noun: a jaundice ipo; a smoulder sofa it-pronoun be-verb U-adverb: it be humanly; it be sooo than-prep a-det U-noun: than a boob; than a menace the-det U-adj and-conj: the convoluted and; the secretive and the-det U-noun that-prep: the baloney that; the cachet that to-to a-det U-adj: to a gory; to a trappist to-to their-pronoun U-noun: to their arsenal; to their subsistence with-prep an-det U-noun: with an alias; with an avalanche 292 Computational Linguistics Volume 30, Number 3 trainingPrec(s) is the precision of s in the training data validationPrec(s) is the precision of s in the validation data testPrec(s) is the precision of s in the test data (similarly for trainingFreq, validationFreq, and testFreq) S = the set of all adjectives (verbs) in the training data for T in [0.01,0.04,,0.70]: for n in [2,3,,40]: retained = {} For s i in S: if trainingPrec({s i }C i,n ) > T: retained = retained {s i }C i,n R T,n = retained ADJ pses = {} (VERB pses = {}) for T in [0.01,0.04,,0.70]: for n in [2,3,,40]: if validationPrec(R T,n )  0.28 (0.23 for verbs) and validationFreq(R T,n )  100: ADJ pses = ADJ pses  R T,n (VERB pses = VERB pses  R T,n ) Results in Table 7 show testPrec(ADJ pses ) and testFreq(ADJ pses )." ></td>
	<td class="line x" title="312:617	Figure 2 Algorithm for selecting adjective and verb features using distributional similarity." ></td>
	<td class="line x" title="313:617	motivation for experimenting with it to identify PSEs was twofold." ></td>
	<td class="line x" title="314:617	First, we hypothesized that words might be distributionally similar because they share pragmatic usages, such as expressing subjectivity, even if they are not close synonyms." ></td>
	<td class="line x" title="315:617	Second, as shown above, low-frequency words appear more often in subjective texts than expected." ></td>
	<td class="line x" title="316:617	We did not want to discard all low-frequency words from consideration but cannot effectively judge the suitability of individual words." ></td>
	<td class="line x" title="317:617	Thus, to decide whether to retain a word as a PSE, we consider the precision not of the individual word, but of the word together with a cluster of words similar to it." ></td>
	<td class="line x" title="318:617	Many variants of distributional similarity have been used in NLP (Lee 1999; Lee and Pereira 1999)." ></td>
	<td class="line x" title="319:617	Dekang Lins (1998) method is used here." ></td>
	<td class="line x" title="320:617	In contrast to many implementations, which focus exclusively on verb-noun relationships, Lins method incorporates a variety of syntactic relations." ></td>
	<td class="line x" title="321:617	This is important for subjectivity recognition, because PSEs are not limited to verb-noun relationships." ></td>
	<td class="line x" title="322:617	In addition, Lins results are freely available." ></td>
	<td class="line x" title="323:617	A set of seed words begins the process." ></td>
	<td class="line x" title="324:617	For each seed s i, the precision of the set {s i }C i,n in the training data is calculated, where C i,n is the set of n words most similar to s i, according to Lins (1998) method." ></td>
	<td class="line x" title="325:617	If the precision of {s i }C i,n is greater than a threshold T, then the words in this set are retained as PSEs." ></td>
	<td class="line x" title="326:617	If it is not, neither s i nor the words in C i,n are retained." ></td>
	<td class="line x" title="327:617	The union of the retained sets will be denoted R T,n, that is, the union of all sets {s i }C i,n with precision on the training set > T. In Wiebe (2000), the seeds (the s i s) were extracted from the subjective-element annotations in corpus WSJ-SE." ></td>
	<td class="line x" title="328:617	Specifically, the seeds were the adjectives that appear at least once in a subjective element in WSJ-SE." ></td>
	<td class="line x" title="329:617	In this article, the opinion piece corpus is used to move beyond the manual annotations and small corpus of the earlier work, and a much looser criterion is used to choose the initial seeds: All of the adjectives (verbs) in the training data are used." ></td>
	<td class="line x" title="330:617	The algorithm for the process is given in Figure 2." ></td>
	<td class="line x" title="331:617	There is one small difference for adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for 293 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 7 Frequencies and increases in precision for adjective and verb features identified using distributional similarity with filtering." ></td>
	<td class="line x" title="332:617	For each test data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="333:617	Baseline ADJ pses VERB pses Training Validation Test freq prec freq +prec freq +prec W9-10 W9-22 W9-22 W9-10 W9-33 153,634 .14 1,576 +.12 1,490 +.11 W9-10 W9-33 W9-33 W9-10 W9-22 155,135 .13 859 +.15 535 +.11 W9-22 W9-33 W9-33 W9-22 W9-10 156,334 .18 249 +.22 224 +.10 All pairings of W9-10, W9-22,W9-33 W9-4 156,421 .19 1,872 +.17 1,777 +.15 adjectives versus 0.23 for verbs." ></td>
	<td class="line x" title="334:617	These thresholds were determined using validation data." ></td>
	<td class="line x" title="335:617	Seeds and their clusters are assessed on a training set for many parameter settings (cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70 by .03)." ></td>
	<td class="line x" title="336:617	As mentioned above, each (n, T) parameter pair yields a set of adjectives R T,n, that is, the union of all sets {s i }C i,n with precision on the training set > T. A subset, ADJ pses, of those sets is chosen based on precision and frequency in a validation set." ></td>
	<td class="line x" title="337:617	Finally, the ADJ pses are tested on the test set." ></td>
	<td class="line x" title="338:617	Table 7 shows the results for four opinion piece test sets." ></td>
	<td class="line x" title="339:617	Multiple trainingvalidation data set pairs are used for each test set, as given in Table 7." ></td>
	<td class="line x" title="340:617	The results are for the union of the adjectives (verbs) chosen for each pair." ></td>
	<td class="line x" title="341:617	The freq columns give total frequencies, and the +prec columns show the improvements in precision from the baseline." ></td>
	<td class="line x" title="342:617	For each data set, the difference between the proportion of instances of ADJ pses in opinion pieces and the proportion in nonopinion pieces is significant (p < 0.001, z  9.2)." ></td>
	<td class="line x" title="343:617	The same is true for VERB pses (p < 0.001, z  4.1)." ></td>
	<td class="line x" title="344:617	In the interests of testing consistency, Table 8 shows the results of assessing the adjective and verb features generated from opinion piece data (ADJ pses and VERB pses Table 8 Average frequencies and increases in precision in subjective-element data of the sets tested in Table 7." ></td>
	<td class="line x" title="345:617	The baselines are the precisions of adjectives/verbs that appear in subjective elements in the subjective-element data." ></td>
	<td class="line x" title="346:617	Adj baseline Verb baseline ADJ pses VERB pses freq prec freq prec freq +prec freq +prec WSJ-SE-D 1,632 .13 2,980 .15 136 +.16 151 +.10 WSJ-SE-M 1,632 .19 2,980 .12 136 +.24 151 +.13 NG-SE 1,104 .37 2,629 .15 185 +.25 275 +.08 294 Computational Linguistics Volume 30, Number 3 Table 9 Frequencies and increases in precision for all features." ></td>
	<td class="line x" title="347:617	For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces." ></td>
	<td class="line x" title="348:617	freq: total frequency; +prec: increase in precision over baseline." ></td>
	<td class="line x" title="349:617	W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec Unique words 4794 +.15 4763 +.16 4274 +.11 4567 +.11 Fixed-2-grams 1840 +.07 1972 +.07 1933 +.04 1839 +.05 ugen-2-grams 281 +.21 256 +.26 261 +.17 254 +.17 Fixed-3-grams 213 +.08 243 +.09 214 +.05 238 +.05 ugen-3-grams 148 +.29 133 +.27 147 +.16 133 +.15 Fixed-4-grams 18 +.15 17 +.06 12 +.29 14 .07 ugen-4-grams 13 +.12 3 +.82 15 +.27 13 +.25 Adjectives 1872 +.17 249 +.22 859 +.15 1576 +.12 Verbs 1777 +.15 224 +.10 535 +.11 1490 +.11 Baseline 156421 .19 156334 .18 155135 .13 153634 .14 in Table 7) on the subjective-element data." ></td>
	<td class="line x" title="350:617	The left side of the table gives baseline figures for each set of subjective-element annotations." ></td>
	<td class="line x" title="351:617	The right side of the table gives the average frequencies and increases in precision over baseline for the ADJ pses and VERB pses sets on the subjective-element data." ></td>
	<td class="line x" title="352:617	The baseline figures in the table are the frequencies and precisions of the sets of adjectives and verbs that appear at least once in a subjective element." ></td>
	<td class="line x" title="353:617	Since these sets include words that appear just once in the corpus (and thus have 100% precision), the baseline precision is a challenging one." ></td>
	<td class="line x" title="354:617	Testing the VERB pses and ADJ pses on the subjective-element data reveals some interesting consistencies for these subjectivity clues." ></td>
	<td class="line x" title="355:617	The precision increases of the VERB pses on the subjective-element data are comparable to their increases on the opinion piece data." ></td>
	<td class="line x" title="356:617	Similarly, the precision increases of the ADJ pses on the subjective-element data are as good as or better than the performance of this set of PSEs on the opinion piece data." ></td>
	<td class="line x" title="357:617	Finally, the precisions increases for the ADJ pses are higher than for the VERB pses on all data sets." ></td>
	<td class="line x" title="358:617	This is again consistent with the higher performance of the ADJ pses sets in the opinion piece data sets." ></td>
	<td class="line x" title="359:617	4." ></td>
	<td class="line x" title="360:617	Features Used in Concert 4.1 Introduction In this section, we examine the various types of clues used together." ></td>
	<td class="line x" title="361:617	In preparation for this work, all instances in OP1 and OP2 of all of the PSEs identified as described in Section 3 have been automatically identified." ></td>
	<td class="line x" title="362:617	All training to define the PSE instances in OP1 was performed on data separate from OP1, and all training to define the PSE instances in OP2 was performed on data separate from OP2." ></td>
	<td class="line x" title="363:617	4.2 Consistency in Precision among Data Sets Table 9 summarizes the results from previous sections in which the opinion piece data are used for testing." ></td>
	<td class="line x" title="364:617	The performance of the various features is consistently good or bad on the same data sets: the performance is better for all features on W9-10 and W9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with very low frequency, and the verbs, which have low frequency in W9-10)." ></td>
	<td class="line x" title="365:617	This is so despite the fact that the features were generated using different procedures and data: The 295 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language 0." ></td>
	<td class="line x" title="366:617	PSEs = all adjs, verbs, modals, nouns, and adverbs that appear at least once in an SE (except not, will, be, have)." ></td>
	<td class="line x" title="367:617	1." ></td>
	<td class="line x" title="368:617	PSEinsts = the set of all instances of PSEs 2." ></td>
	<td class="line x" title="369:617	HiDensity = {} 3." ></td>
	<td class="line x" title="370:617	For P in PSEinsts: 4." ></td>
	<td class="line x" title="371:617	leftWin(P) = the W words before P 5." ></td>
	<td class="line x" title="372:617	rightWin(P) = the W words after P 6." ></td>
	<td class="line x" title="373:617	density(P) = number of SEs whose first or last word is in leftWin(P) or rightWin(P) 7." ></td>
	<td class="line x" title="374:617	if density(P)  T: HiDensity = HiDensity {P} 8." ></td>
	<td class="line x" title="375:617	prec(PSEinsts)= number of PSEinsts in subject elements |PSEinsts| 9." ></td>
	<td class="line x" title="376:617	prec(HiDensity)= number of HiDensity in subject elements |HiDensity| Figure 3 Algorithm for calculating density in subjective-element data." ></td>
	<td class="line x" title="377:617	adjectives and verbs were generated from WSJ document-level opinion piece classifications; the n-gram features were generated from newsgroup and WSJ expression-level subjective-element classifications; and the unique unigram feature requires no training." ></td>
	<td class="line x" title="378:617	This consistency in performance suggests that the results are not brittle." ></td>
	<td class="line x" title="379:617	4.3 Choosing Density Parameters from Subjective-Element Data In Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, on how subjective the surrounding context is. We explore this idea in the current work, assessing whether PSEs are more likely to be subjective if they are surrounded by subjective elements." ></td>
	<td class="line x" title="380:617	In particular, we experiment with a density feature to decide whether or not a PSE instance is subjective: If a sufficient number of subjective elements are nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded." ></td>
	<td class="line x" title="381:617	The density parameters are a window size W and a frequency threshold T. In this section, we explore the density of manually annotated PSEs in subjectiveelement data and choose density parameters to use in Section 4.4, in which we apply them to automatically identified PSEs in opinion piece data." ></td>
	<td class="line x" title="382:617	The process for calculating density in the subjective-element data is given in Figure 3." ></td>
	<td class="line x" title="383:617	The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs that appear at least once in a subjective element, with the exception of some stop words (line 0 of Figure 3)." ></td>
	<td class="line x" title="384:617	Note that these PSEs depend only on the subjective-element manual annotations, not on the automatically identified features used elsewhere in the article or on the document-level opinion piece classes." ></td>
	<td class="line x" title="385:617	PSEinsts is the set of PSE instances to be disambiguated (line 1)." ></td>
	<td class="line x" title="386:617	HiDensity (initialized on line 2) will be the subset of PSEinsts that are retained." ></td>
	<td class="line x" title="387:617	In the loop, the density of each PSE instance P is calculated." ></td>
	<td class="line x" title="388:617	This is the number of subjective elements that begin or end in the W words preceding or following P (line 6)." ></td>
	<td class="line x" title="389:617	P is retained if its density is at least T (line 7)." ></td>
	<td class="line x" title="390:617	Lines 89 of the algorithm assess the precision of the original (PSEinsts) and new (HiDensity) sets of PSE instances." ></td>
	<td class="line x" title="391:617	If prec(HiDensity) is greater than prec(PSEinsts), then 296 Computational Linguistics Volume 30, Number 3 Table 10 Most frequent entry in the top three precision intervals for each subjective-element data set." ></td>
	<td class="line x" title="392:617	WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE Baseline freq 1,566 1,245 1,167 1,108 3,303 Baseline prec .49 .47 .41 .36 .51 Range .87.92 .951.0 .951.0 .951.0 .951.0 T, W 10, 20 12, 50 20, 50 14, 100 10, 10 freq 76 12 1 1 3 prec .89 1.0 1.0 1.0 1.0 Range .82.87 .90.95 .73.78 .51.56 .67.72 T, W 6, 10 12, 60 46, 190 22, 370 26, 90 freq 63 22 53 221 664 prec .84 .91 .78 .51 .67 Range .77.82 .84.89 .66.71 .46.51 .63.67 T, W 12, 40 12, 80 18, 60 16, 310 8, 30 freq 292 42 53 358 1504 prec .78 .88 .68 .47 .63 there is evidence that the number of subjective elements near a PSE instance is related to its subjectivity in context." ></td>
	<td class="line x" title="393:617	To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1 and WSJ-SE2) and annotations of the two judges are considered separately." ></td>
	<td class="line x" title="394:617	WSJ-SE2-D, for example, refers to Ds annotations of WSJ-SE2." ></td>
	<td class="line x" title="395:617	The process in Figure 3 was repeated for different parameter settings (T in [1, 2, 4,,48] and W in [1, 10, 20,, 490]) on each of the SE data sets." ></td>
	<td class="line x" title="396:617	To find good parameter settings, the results for each data set were sorted into five-point precision intervals and then sorted by frequency within each interval." ></td>
	<td class="line x" title="397:617	Information for the top three precision intervals for each data set are shown in Table 10, specifically, the parameter values (i.e. , T and W) and the frequency and precision of the most frequent result in each interval." ></td>
	<td class="line x" title="398:617	The intervals are in the rows labeled Range." ></td>
	<td class="line x" title="399:617	For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92, 0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92)." ></td>
	<td class="line x" title="400:617	The top of Table 10 gives baseline frequencies and precisions, which are |PSEinsts| and prec(PSEinsts), respectively, in line 8 of Figure 3." ></td>
	<td class="line x" title="401:617	The parameter values exhibit a range of frequencies and precisions, with the expected trade-off between precision and frequency." ></td>
	<td class="line x" title="402:617	We choose the following parameters to test in Section 4.4: For each data set, for each precision interval whose lower bound is at least 10 percentage points higher than the baseline for that data set, the top two (T, W) pairs yielding the highest frequencies in that interval are chosen." ></td>
	<td class="line x" title="403:617	Among the five data sets, a total of 45 parameter pairs were so selected." ></td>
	<td class="line x" title="404:617	This exercise was completed once, without experimenting with different parameter settings." ></td>
	<td class="line x" title="405:617	4.4 Density for Disambiguation In this section, density is exploited to find subjective instances of automatically identified PSEs." ></td>
	<td class="line x" title="406:617	The process is shown in Figure 4." ></td>
	<td class="line x" title="407:617	There are only two differences between the algorithms in Figures 3 and 4." ></td>
	<td class="line x" title="408:617	First, in Figure 3, density is defined in terms of the number of subjective elements nearby." ></td>
	<td class="line x" title="409:617	However, subjective-element annotations are not available in test data." ></td>
	<td class="line x" title="410:617	Thus in Figure 4, density is defined in terms of the 297 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language 0." ></td>
	<td class="line x" title="411:617	PSEinsts = the set of instances in the test data of all PSEs described in Section 3 1." ></td>
	<td class="line x" title="412:617	HiDensity = {} 2." ></td>
	<td class="line x" title="413:617	For P in PSEinsts: 3." ></td>
	<td class="line x" title="414:617	leftWin(P) = the W words before P 4." ></td>
	<td class="line x" title="415:617	rightWin(P) = the W words after P 5." ></td>
	<td class="line x" title="416:617	density(P) = number of PSEinsts whose first or last word is in leftWin(P) or rightWin(P) 6." ></td>
	<td class="line x" title="417:617	if density(P)  T: HiDensity = HiDensity {P} 7." ></td>
	<td class="line x" title="418:617	prec(PSEinsts)= #ofPSEinsts in OPs |PSEinsts| 8." ></td>
	<td class="line x" title="419:617	prec(HiDensity)= #ofHiDensity in OPs |HiDensity| Figure 4 Algorithm for calculating density in opinion piece (OP) data number of other PSE instances nearby, where PSEinsts consists of all instances of the automatically identified PSEs described in Section 3, for which results are given in Table 9." ></td>
	<td class="line x" title="420:617	Second, in Figure 4, we assess precision with respect to the document-level classes (lines 78)." ></td>
	<td class="line x" title="421:617	The test data are OP1." ></td>
	<td class="line x" title="422:617	An interesting question arose when we were defining the PSE instances: What should be done with words that are identified to be PSEs (or parts of PSEs) according to multiple criteria?" ></td>
	<td class="line x" title="423:617	For example, sunny, radiant, and exhilarating are all unique in corpus OP1, and are all members of the adjective PSE feature defined for testing on OP1." ></td>
	<td class="line x" title="424:617	Collocations add additional complexity." ></td>
	<td class="line x" title="425:617	For example, consider the sequence and splendidly, which appears in the test data." ></td>
	<td class="line x" title="426:617	The sequence and splendidly matches the ugen-2-gram (and-conj U-adj), and the word splendidly is unique." ></td>
	<td class="line x" title="427:617	In addition, a sequence may match more than one n-gram feature." ></td>
	<td class="line x" title="428:617	For example, is it that matches three fixed-n-gram features: is it, is it that, and it that." ></td>
	<td class="line x" title="429:617	In the current experiments, the more PSEs a word matches, the more weight it is given." ></td>
	<td class="line x" title="430:617	The hypothesis behind this treatment is that additional matches represent additional evidence that a PSE instance is subjective." ></td>
	<td class="line x" title="431:617	This hypothesis is realized as follows: Each match of each member of each type of PSE is considered to be a PSE instance." ></td>
	<td class="line x" title="432:617	Thus, among them, there are 11 members in PSEinsts for the five phrases sunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matches mentioned above." ></td>
	<td class="line x" title="433:617	The process in Figure 4 was conducted with the 45 parameter pair values (T and W) chosen from the subjective-element data as described in Section 4.3." ></td>
	<td class="line x" title="434:617	Table 11 shows results for a subset of the 45 parameters, namely, the most frequent parameter pair chosen from the top three precision intervals for each training set." ></td>
	<td class="line x" title="435:617	The bottom of the table gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts| and prec(PSEinsts), respectively, in line 7 of Figure 4." ></td>
	<td class="line x" title="436:617	The density features result in substantial increases in precision." ></td>
	<td class="line x" title="437:617	Of the 45 parameter pairs, the minimum percentage increase over baseline is 22%." ></td>
	<td class="line x" title="438:617	Fully 24% of the 45 parameter pairs yield increases of 200% or more; 38% yield increases between 100% 298 Computational Linguistics Volume 30, Number 3 Table 11 Results for high-density PSEs in test data OP1 using parameters chosen from subjective-element data." ></td>
	<td class="line x" title="439:617	WSJ-SE1-M WSJ-SE1-D WSJ-SE2-M WSJ-SE2-D NG-SE T, W 10, 20 12, 50 20, 50 14, 100 10, 10 freq 237 3,176 170 10,510 8 prec .87 .72 .97 .57 1.0 T, W 6, 10 12, 60 46, 190 22, 370 26, 90 freq 459 5,289 1,323 21,916 787 prec .68 .68 .95 .37 .92 T, W 12, 40 12, 80 18, 60 16, 310 8, 30 freq 1,398 9,662 906 24,454 3,239 prec .79 .58 .87 .34 .67 PSE baseline: freq = 30,938, prec = .28 and 199%, and 38% yield increases between 22% and 99%." ></td>
	<td class="line x" title="440:617	In addition, the increases are significant." ></td>
	<td class="line x" title="441:617	Using the set of high-density PSEs defined by the parameter pair with the least increase over baseline, we tested the difference in the proportion of PSEs in opinion pieces that are high-density and the proportion of PSEs in nonopinion pieces that are high-density." ></td>
	<td class="line x" title="442:617	The difference between these two proportions is highly significant (z = 46.2, p < 0.0001)." ></td>
	<td class="line x" title="443:617	Notice that, except for one blip (T, W = 6, 10 under WSJ-SE-M), the precisions decrease and the frequencies increase as we go down each column in Table 11." ></td>
	<td class="line x" title="444:617	The same pattern can be observed with all 45 parameter pairs (results not included here because of space considerations)." ></td>
	<td class="line x" title="445:617	But the parameter pairs are ordered in Table 11 based on performance in the manually annotated subjective-element data, not based on performance in the test data." ></td>
	<td class="line x" title="446:617	For example, the entry in the first row, first column (T, W = 10, 20) is the parameter pair giving the highest frequency in the top precision interval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process of Figure 3)." ></td>
	<td class="line x" title="447:617	Thus, the relative precisions and frequencies of the parameter pairs are carried over from the training to the test data." ></td>
	<td class="line x" title="448:617	This is quite a strong result, given that the PSEs in the training data are from manual annotations, while the PSEs in the test data are our automatically identified features." ></td>
	<td class="line x" title="449:617	4.5 High-Density Sentence Annotations To assess the subjectivity of sentences with high-density PSEs, we extracted the 133 sentences in corpus OP2 that contain at least one high-density PSE and manually annotated them." ></td>
	<td class="line x" title="450:617	We refer to these sentences as the system-identified sentences." ></td>
	<td class="line x" title="451:617	We chose the density-parameter pair (T, W = 12, 30), based on its precision and frequency in OP1." ></td>
	<td class="line x" title="452:617	This parameter setting yields results that have relatively high precision and low frequency." ></td>
	<td class="line x" title="453:617	We chose a low-frequency setting to make the annotation study feasible." ></td>
	<td class="line x" title="454:617	The extracted sentences were independently annotated by two judges." ></td>
	<td class="line x" title="455:617	One is a coauthor of this article (judge 1), and the other has performed subjectivity annotation before, but is not otherwise involved in this research (judge 2)." ></td>
	<td class="line x" title="456:617	Sentences were annotated according to the coding instructions of Wiebe, Bruce, and OHara (1999) which, recall, are to classify a sentence as subjective if there is a significant expression of subjectivity of either the writer or someone mentioned in the text, in the sentence." ></td>
	<td class="line x" title="457:617	299 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Table 12 Examples of system-identified sentences." ></td>
	<td class="line x" title="458:617	(1) The outburst of shooting came nearly two weeks after clashes between Moslem worshippers and oo Somali soldiers." ></td>
	<td class="line x" title="459:617	(2.a) But now the refugees are streaming across the border and alarming the world." ></td>
	<td class="line x" title="460:617	ss (2.b) In the middle of the crisis, Erich Honecker was hospitalized with a gall stone operation." ></td>
	<td class="line x" title="461:617	oo (2.c) It is becoming more and more obvious that his gallstone-age communism is dying with him:  ss (3.a) Not brilliantly, because, after all, this was a performer who was collecting paychecks from lounges ss at Hiltons and Holiday Inns, but creditably and with the air of someone for whom Ten Cents a Dance was more than a bit autobiographical." ></td>
	<td class="line x" title="462:617	(3.b) It was an exercise of blending Michelles singing with Susies singing, explained Ms. Stevens." ></td>
	<td class="line x" title="463:617	oo (4) Enlisted men and lower-grade officers were meat thrown into a grinder." ></td>
	<td class="line x" title="464:617	ss (5) If you believe in God and you believe in miracles, theres nothing particularly crazy about that. ss (6) He was much too eager to create something very weird and dynamic, ss catastrophic and jolly like this great and coily thing Lolita. (7) The Bush approach of mixing confrontation with conciliation strikes some people as sensible, perhaps ss even inevitable, because Mr. Bush faces a Congress firmly in the hands of the opposition." ></td>
	<td class="line x" title="465:617	(8) Still, despite their efforts to convince the world that we are indeed alone, the visitors do seem to keep ss coming and, like the recent sightings, theres often a detail or two that suggests they may actually be a little on the dumb side." ></td>
	<td class="line x" title="466:617	(9) As for the women, theyre pathetic." ></td>
	<td class="line x" title="467:617	ss (10) At this point, the truce between feminism and sensationalism gets might uneasy." ></td>
	<td class="line x" title="468:617	ss (11) MMPIs publishers say the test shouldnt be used alone to diagnose ss psychological problems or in hiring; it should be given in conjunction with other tests." ></td>
	<td class="line x" title="469:617	(12) While recognizing that professional environmentalists may feel threatened, ss I intend to urge that UV-B be monitored whenever I can." ></td>
	<td class="line x" title="470:617	Table 13 Sentence annotation contingency table; judge 1 counts are in rows and judge 2 counts are in columns." ></td>
	<td class="line x" title="471:617	Subjective Objective Unsure Subjective 98 2 3 Objective 2 14 0 Unsure 2 11 1 In addition to the subjective and objective classes, a judge can tag a sentence as unsure if he or she is unsure of his or her rating or considers the sentence to be borderline." ></td>
	<td class="line x" title="472:617	An equal number (133) of other sentences were randomly selected from the corpus to serve as controls." ></td>
	<td class="line x" title="473:617	The 133 system-identified sentences and the 133 control sentences were randomly mixed together." ></td>
	<td class="line x" title="474:617	The judges were asked to annotate all 266 sentences, not knowing which were system-identified and which were control." ></td>
	<td class="line x" title="475:617	Each sentence was presented with the sentence that precedes it and the sentence that follows it in the corpus, to provide some context for interpretation." ></td>
	<td class="line x" title="476:617	Table 12 shows examples of the system-identified sentences." ></td>
	<td class="line x" title="477:617	Sentences classified by both judges as objective are marked oo and those classified by both judges as subjective are marked ss." ></td>
	<td class="line x" title="478:617	300 Computational Linguistics Volume 30, Number 3 Table 14 Examples of subjective sentences adjacent to system-identified sentences." ></td>
	<td class="line x" title="479:617	Bathed in cold sweat, I watched these Dantesque scenes, holding tightly the damp hand of Edek or Waldeck who, like me, were convinced that there was no God." ></td>
	<td class="line x" title="480:617	The Japanese are amazed that a company like this exists in Japan, says Kimindo Kusaka, head of the Softnomics Center, a Japanese management-research organization." ></td>
	<td class="line x" title="481:617	And even if drugs were legal, what evidence do you have that the habitual drug user wouldnt continue to rob and steal to get money for clothes, food or shelter?" ></td>
	<td class="line x" title="482:617	The moral cost of legalizing drugs is great, but it is a cost that apparently lies outside the narrow scope of libertarian policy prescriptions." ></td>
	<td class="line x" title="483:617	I doubt that one exists." ></td>
	<td class="line x" title="484:617	They were upset at his committees attempt to pacify the program critics by cutting the surtax paid by the more affluent elderly and making up the loss by shifting more of the burden to the elderly poor and by delaying some benefits by a year." ></td>
	<td class="line x" title="485:617	Judge 1 classified 103 of the system-identified sentences as subjective, 16 as objective, and 14 as unsure." ></td>
	<td class="line x" title="486:617	Judge 2 classified 102 of the system-identified sentences as subjective, 27 as objective; and 4 as unsure." ></td>
	<td class="line x" title="487:617	The contingency table is given in Table 13." ></td>
	<td class="line x" title="488:617	4 The kappa value using all three classes is 0.60, reflecting the highly skewed distribution in favor of subjective sentences, and the disagreement on the lower-frequency classes (unsure and objective)." ></td>
	<td class="line x" title="489:617	Consistent with the findings in Wiebe, Bruce, and OHara (1999), the kappa value for agreement on the sentences for which neither judge is unsure is very high: 0.86." ></td>
	<td class="line x" title="490:617	A different breakdown of the sentences is illuminating." ></td>
	<td class="line x" title="491:617	For 98 of the sentences (call them SS), judges 1 and 2 tag the sentence as subjective." ></td>
	<td class="line x" title="492:617	Among the other sentences, 20 appear in a block of contiguous system-identified sentences that includes a member of SS." ></td>
	<td class="line x" title="493:617	For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block of subjective sentences as they are." ></td>
	<td class="line x" title="494:617	Similarly, (3.a) is in SS and (3.b) is in the same block." ></td>
	<td class="line x" title="495:617	Among the remaining 15 sentences, 6 are adjacent to subjective sentences that were not identified by our system (so were not annotated by the judges)." ></td>
	<td class="line x" title="496:617	All of those sentences contain significant expressions of subjectivity of the writer or someone mentioned in the text, the criterion used in this work for classifying a sentence as subjective." ></td>
	<td class="line x" title="497:617	Samples are shown in Table 14." ></td>
	<td class="line x" title="498:617	Thus, 93% of the sentences identified by the system are subjective or are near subjective sentences." ></td>
	<td class="line x" title="499:617	All the sentences, together with their tags and the sentences adjacent to them, are available on the Web at www.cs.pitt.edu/wiebe." ></td>
	<td class="line x" title="500:617	4.6 Using Features for Opinion Piece Recognition In this section, we assess the usefulness of the PSEs identified in Section 3 and listed in Table 9 by using them to perform document-level classification of opinion pieces." ></td>
	<td class="line x" title="501:617	Opinion-piece classification is a difficult task for two reasons." ></td>
	<td class="line x" title="502:617	First, as discussed in Section 2.1, both opinionated and factual documents tend to be composed of a mixture of subjective and objective language." ></td>
	<td class="line x" title="503:617	Second, the natural distribution of documents in our data is heavily skewed toward nonopinion pieces." ></td>
	<td class="line x" title="504:617	Despite these hurdles, using only 4 In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2 classified only 47 (36%) of them as subjective." ></td>
	<td class="line x" title="505:617	301 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language our PSEs, we achieve positive results in opinion-piece classification using the basic knearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997)." ></td>
	<td class="line x" title="506:617	Given a document, the basic KNN algorithm classifies the document according to the majority classification of the documents k closest neighbors." ></td>
	<td class="line x" title="507:617	For our purposes, each document is characterized by one feature, the count of all PSE instances (regardless of type) in the document, normalized by document length in words." ></td>
	<td class="line x" title="508:617	The distance between two documents is simply the absolute value of the difference between the normalized PSE counts for the two documents." ></td>
	<td class="line x" title="509:617	With leave-one-out cross-validation, the set of n documents to be classified is divided into a training set of size n1 and a validation set of size 1." ></td>
	<td class="line x" title="510:617	The one document in the validation set is then classified according to the majority classification of its k closest-neighbor documents in the training set." ></td>
	<td class="line x" title="511:617	This process is repeated until every document is classified." ></td>
	<td class="line x" title="512:617	Which value to use for k is chosen during a preprocessing phase." ></td>
	<td class="line x" title="513:617	During the preprocessing phase, we run the KNN algorithm with leave-one-out cross-validation on a separate training set, for odd values of k from 1 to 15." ></td>
	<td class="line x" title="514:617	The value of k that results in the best classification during the preprocessing phase is the one used for later KNN classification." ></td>
	<td class="line x" title="515:617	For the classification experiment, the data set OP1 was used in the preprocessing phase to select the value of k, and then classification was performed on the 1,222 documents in OP2." ></td>
	<td class="line x" title="516:617	During training on OP1, k equal to 15 resulted in the best classification." ></td>
	<td class="line x" title="517:617	On the test set, OP2, we achieved a classification accuracy of 0.939; the baseline accuracy for choosing the most frequent class (nonopinion pieces) was 0.915." ></td>
	<td class="line x" title="518:617	Our classification accuracy represents a 28% reduction in error and is significantly better than baseline according to McNemars test (Everitt 1997)." ></td>
	<td class="line x" title="519:617	The positive results from the opinion piece classification show the usefulness of the various PSE features when used together." ></td>
	<td class="line x" title="520:617	5." ></td>
	<td class="line x" title="521:617	Relation to Other Work There has been much work in other fields, including linguistics, literary theory, psychology, philosophy, and content analysis, involving subjective language." ></td>
	<td class="line x" title="522:617	As mentioned in Section 2, the conceptualization underlying our manual annotations is based on work in literary theory and linguistics, most directly Dolezel (1973), Uspensky (1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield (1982)." ></td>
	<td class="line x" title="523:617	We also mentioned existing knowledge resources such as affective lexicons (General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons (e.g. , the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998])." ></td>
	<td class="line x" title="524:617	Such knowledge may be used in future work to complement the work presented in this article, for example, to seed the distributional-similarity process described in Section 3.4." ></td>
	<td class="line x" title="525:617	There is also work in fields such as content analysis and psychology on statistically characterizing texts in terms of word lists manually developed for distinctions related to subjectivity." ></td>
	<td class="line x" title="526:617	For example, Hart (1984) performs counts on a manually developed list of words and rhetorical devices (e.g. , sacred terms such as freedom) in political speeches to explore potential reasons for public reactions." ></td>
	<td class="line x" title="527:617	Anderson and McMaster (1998) use fixed sets of high-frequency words to assign connotative scores to documents and sections of documents along dimensions such as how pleasant, acrimonious, pious, or confident, the text is. What distinguishes our work from work on subjectivity in other fields is that we focus on (1) automatically learning knowledge from corpora, (2) automatically 302 Computational Linguistics Volume 30, Number 3 performing contextual disambiguation, and (3) using knowledge of subjectivity in NLP applications." ></td>
	<td class="line x" title="528:617	This article expands and integrates the work reported in Wiebe and Wilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al.(2001) and Wiebe (2000)." ></td>
	<td class="line x" title="530:617	Previous work in NLP on the same or related tasks includes sentence-level and document-level subjectivity classifications." ></td>
	<td class="line x" title="531:617	At the sentence level, Wiebe, Bruce, and OHara (1999) developed a machine learning system to classify sentences as subjective or objective." ></td>
	<td class="line x" title="532:617	The accuracy of the system was more than 20 percentage points higher than a baseline accuracy." ></td>
	<td class="line x" title="533:617	Five part-of-speech features, two lexical features, and a paragraph feature were used." ></td>
	<td class="line x" title="534:617	These results suggested to us that there are clues to subjectivity that might be learned automatically from text and motivated the work reported in the current article." ></td>
	<td class="line x" title="535:617	The system was tested in 10-fold cross validation experiments using corpus WSJ-SE, a small corpus of only 1,001 sentences." ></td>
	<td class="line x" title="536:617	As discussed in Section 1, a main goal of our current work is to exploit existing document-level annotations, because they enable us to use much larger data sets, they were created outside our research group, and they allow us to assess consistency of performance by cross-validating between our manual annotations and the existing document-level annotations." ></td>
	<td class="line x" title="537:617	Because the document-level data are not annotated at the sentence level, sentence-level classification is not highlighted in this article." ></td>
	<td class="line x" title="538:617	The new sentence annotation study to evaluate sentences with high-density features (Section 4.5) uses different data from WSJ-SE, because some of the features (n-grams and density parameters) were identified using WSJ-SE as training data." ></td>
	<td class="line x" title="539:617	Other previous work in NLP has addressed related document-level classifications." ></td>
	<td class="line x" title="540:617	Spertus (1997) developed a system for recognizing inflammatory messages." ></td>
	<td class="line x" title="541:617	As mentioned earlier in the article, inflammatory language is a type of subjective language, so the task she addresses is closely related to ours." ></td>
	<td class="line x" title="542:617	She uses machine learning to select among manually developed features." ></td>
	<td class="line x" title="543:617	In contrast, the focus in our work is on automatically identifying features from the data." ></td>
	<td class="line x" title="544:617	A number of projects investigating genre detection include editorials as one of the targeted genres." ></td>
	<td class="line x" title="545:617	For example, in Karlgren and Cutting (1994), editorials are one of fifteen categories, and in Kessler, Nunberg, and Sch utze (1997), editorials are one of six." ></td>
	<td class="line x" title="546:617	Given the goal of these works to perform genre detection in general, they use low-level features that are not specific to editorials." ></td>
	<td class="line x" title="547:617	Neither shows significant improvements for editorial recognition." ></td>
	<td class="line x" title="548:617	Argamon, Koppel, and Avneri (1998) address a slightly different task, though it does involve editorials." ></td>
	<td class="line x" title="549:617	Their goal is to distinguish not only, for example, news from editorials, but also these categories in different publications." ></td>
	<td class="line x" title="550:617	Their best results are distinguishing among the news categories of different publications; their lowest results involve editorials." ></td>
	<td class="line x" title="551:617	Because we focus specifically on distinguishing opinion pieces from nonopinion pieces, our results are better than theirs for those categories." ></td>
	<td class="line x" title="552:617	In addition, in contrast to the above studies, the focus of our work is on learning features of subjectivity." ></td>
	<td class="line x" title="553:617	We perform opinion piece recognition in order to assess the usefulness of the various features when used together." ></td>
	<td class="line x" title="554:617	Other previous NLP research has used features similar to ours for other NLP tasks." ></td>
	<td class="line x" title="555:617	Low-frequency words have been used as features in information extraction (Weeber, Vos, and Baayen 2000) and text categorization (Copeck et al. 2000)." ></td>
	<td class="line x" title="556:617	A number of researchers have worked on mining collocations from text to extend lexicographic resources for machine translation and word sense disambiguation (e.g. , Smajda 1993; Lin 1999; Biber 1993)." ></td>
	<td class="line x" title="557:617	In Samuel, Carberry, and Vijay-Shankers (1998) work on identifying collocations for dialog-act recognition, a filter similar to ours was used to eliminate redundant n-gram features: n-grams were eliminated if they contained substrings with the same entropy score as or a better entropy score than the n-gram." ></td>
	<td class="line oc" title="558:617	303 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language While it is common in studies of collocations to omit low-frequency words and expressions from analysis, because they give rise to invalid or unrealistic statistical measures (Church and Hanks, 1990), we are able to identify higher-precision collocations by including placeholders for unique words (i.e. , the ugen-n-grams)." ></td>
	<td class="line x" title="559:617	We are not aware of other work that uses such collocations as we do." ></td>
	<td class="line x" title="560:617	Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999)." ></td>
	<td class="line x" title="561:617	We are not aware of other work identifying and using density parameters as described in this article." ></td>
	<td class="line x" title="562:617	Since our experiments, other related work in NLP has been performed." ></td>
	<td class="line x" title="563:617	Some of this work addresses related but different classification tasks." ></td>
	<td class="line x" title="564:617	Three studies classify reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, Pennock 2003)." ></td>
	<td class="line x" title="565:617	The input is assumed to be a review, so this task does not include finding subjective documents in the first place." ></td>
	<td class="line x" title="566:617	The first study listed above (Turney 2002) uses a variation of the semantic similarity procedure presented in Wiebe (2000) (Section 3.4)." ></td>
	<td class="line x" title="567:617	The third (Dave, Lawrence, and Pennock 2003) uses ngram features identified with a variation of the procedure presented in Wiebe, Wilson, and Bell (2001) (Section 3.3)." ></td>
	<td class="line x" title="568:617	Tong (2001) addresses finding sentiment timelines, that is, tracking sentiments over time in multiple documents." ></td>
	<td class="line x" title="569:617	For clues of subjectivity, he uses manually developed lexical rules, rather than automatically learning them from corpora." ></td>
	<td class="line x" title="570:617	Similarly, Gordon et al.(2003) use manually developed grammars to detect some types of subjective language." ></td>
	<td class="line x" title="572:617	Agrawal et al.(2003) partition newsgroup authors into camps based on quotation links." ></td>
	<td class="line x" title="574:617	They do not attempt to recognize subjective language." ></td>
	<td class="line x" title="575:617	The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloff and Wiebe (2003) and Yu and Hatzivassiloglou (2003)." ></td>
	<td class="line x" title="576:617	The first two focus on finding additional types of subjective clues (nouns and extraction patterns identified using extraction pattern bootstrapping)." ></td>
	<td class="line x" title="577:617	Yu and Hatzivassiloglou (2003) perform opinion text classification." ></td>
	<td class="line x" title="578:617	They also use existing WSJ document classes for training and testing, but they do not include the entire corpus in their experiments, as we do." ></td>
	<td class="line x" title="579:617	Their opinion piece class consists only of editorials and letters to the editor, and their nonopinion class consists only of business and news." ></td>
	<td class="line x" title="580:617	They report an average F-measure of 96.5%." ></td>
	<td class="line x" title="581:617	Our result of 94% accuracy on document level classification is almost comparable." ></td>
	<td class="line x" title="582:617	They also perform sentence-level classification." ></td>
	<td class="line x" title="583:617	We anticipate that knowledge of subjective language may be usefully exploited in a number of NLP application areas and hope that the work presented in this article will encourage others to experiment with subjective language in their applications." ></td>
	<td class="line x" title="584:617	More generally, there are many types of artificial intelligence systems for which state-ofaffairs types such as beliefs and desires are central, including systems that perform plan recognition for understanding narratives (Dyer 1982; Lehnert et al. 1983), for argument understanding (Alvarado, Dyer, and Flowers 1986), for understanding stories from different perspectives (Carbonell 1979), and for generating language under different pragmatic constraints (Hovy 1987)." ></td>
	<td class="line x" title="585:617	Knowledge of linguistic subjectivity could enhance the abilities of such systems to recognize and generate expressions referring to such states of affairs in natural text." ></td>
	<td class="line x" title="586:617	6." ></td>
	<td class="line x" title="587:617	Conclusions Knowledge of subjective language promises to be beneficial for many NLP applications including information extraction, question answering, text categorization, and 304 Computational Linguistics Volume 30, Number 3 summarization." ></td>
	<td class="line x" title="588:617	This article has presented the results of an empirical study in acquiring knowledge of subjective language from corpora in which a number of feature types were learned and evaluated on different types of data with positive results." ></td>
	<td class="line x" title="589:617	We showed that unique words are subjective more often than expected and that unique words are valuable clues to subjectivity." ></td>
	<td class="line x" title="590:617	We also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for unique words." ></td>
	<td class="line x" title="591:617	In addition, we used the results of a method for clustering words according to distributional similarity (Lin 1998) to identify adjectival and verbal clues of subjectivity." ></td>
	<td class="line x" title="592:617	Table 9 summarizes the results of testing all of the above types of PSEs." ></td>
	<td class="line x" title="593:617	All show increased precision in the evaluations." ></td>
	<td class="line x" title="594:617	Together, they show consistency in performance." ></td>
	<td class="line x" title="595:617	In almost all cases they perform better or worse on the same data sets, despite the fact that different kinds of data and procedures are used to learn them." ></td>
	<td class="line x" title="596:617	In addition, PSEs learned using expression-level subjective-element data have precisions higher than baseline on document-level opinion piece data, and vice versa." ></td>
	<td class="line x" title="597:617	Having a large stable of PSEs, it was important to disambiguate whether or not PSE instances are subjective in the contexts in which they appear." ></td>
	<td class="line x" title="598:617	We discovered that the density of other potentially subjective expressions in the surrounding context is important." ></td>
	<td class="line x" title="599:617	If a clue is surrounded by a sufficient number of other clues, then it is more likely to be subjective than if there were not." ></td>
	<td class="line x" title="600:617	Parameter values were selected using training data manually annotated at the expression level for subjective elements and then tested on data annotated at the document level for opinion pieces." ></td>
	<td class="line x" title="601:617	All of the selected parameters led to increases in precision on the test data, and most lead to increases over 100%." ></td>
	<td class="line x" title="602:617	Once again we found consistency between expression-level and document-level annotations." ></td>
	<td class="line x" title="603:617	PSE sets defined by density have high precision in both the subjective-element data and the opinion piece data." ></td>
	<td class="line x" title="604:617	The large differences between training and testing suggest that our results are not brittle." ></td>
	<td class="line x" title="605:617	Using a density feature selected from a training set, sentences containing highdensity PSEs were extracted from a separate test set, and manually annotated by two judges." ></td>
	<td class="line x" title="606:617	Fully 93% of the sentences extracted were found to be subjective or to be near subjective sentences." ></td>
	<td class="line x" title="607:617	Admittedly, the chosen density feature is a high-precision, lowfrequency one." ></td>
	<td class="line x" title="608:617	But since the process is fully automatic, the feature could be applied to more unannotated text to identify regions containing subjective sentences." ></td>
	<td class="line x" title="609:617	In addition, because the precision and frequency of the density features are stable across data sets, lower-precision but higher-frequency options are available." ></td>
	<td class="line x" title="610:617	Finally, the value of the various types of PSEs was demonstrated with the task of opinion piece classification." ></td>
	<td class="line x" title="611:617	Using the k-nearest-neighbor classification algorithm with leave-one-out cross-validation, a classification accuracy of 94% was achieved on a large test set, with a reduction in error of 28% from the baseline." ></td>
	<td class="line x" title="612:617	Future work is required to determine how to exploit density features to improve the performance of text categorization algorithms." ></td>
	<td class="line x" title="613:617	Another area of future work is searching for clues to objectivity, such as the politeness features used by Spertus (1997)." ></td>
	<td class="line x" title="614:617	Still another is identifying the type of a subjective expression (e.g. , positive or negative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) on classifying lexemes to the classification of instances in context (compare, e.g., great! and oh great.) In addition, it would be illuminating to apply our system to data annotated with discourse trees (Carlson, Marcu, and Okurowski 2001)." ></td>
	<td class="line x" title="615:617	We hypothesize that most objective sentences identified by our system are dominated in the discourse by subjective sentences and that we are moving toward identifying subjective discourse segments." ></td>
	<td class="line x" title="616:617	305 Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language Acknowledgments We thank the anonymous reviewers for their helpful and constructive comments." ></td>
	<td class="line x" title="617:617	This research was supported in part by the Office of Naval Research under grants N00014-95-1-0776 and N00014-01-1-0381." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1022
Collocation Translation Acquisition Using Monolingual Corpora
Lü, Yajuan;Zhou, Ming;"></td>
	<td class="line x" title="1:200	Collocation Translation Acquisition Using Monolingual Corpora Yajuan L Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian District, Beijing, China, 100080 t-yjlv@microsoft.com Ming ZHOU Microsoft Research Asia 5F Sigma Center, No. 49 Zhichun Road, Haidian District, Beijing, China, 100080 mingzhou@microsoft.com Abstract Collocation translation is important for machine translation and many other NLP tasks." ></td>
	<td class="line x" title="2:200	Unlike previous methods using bilingual parallel corpora, this paper presents a new method for acquiring collocation translations by making use of monolingual corpora and linguistic knowledge." ></td>
	<td class="line x" title="3:200	First, dependency triples are extracted from Chinese and English corpora with dependency parsers." ></td>
	<td class="line x" title="4:200	Then, a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption." ></td>
	<td class="line x" title="5:200	The generated triple translation model is used to extract collocation translations from two monolingual corpora." ></td>
	<td class="line x" title="6:200	Experiments show that our approach outperforms the existing monolingual corpus based methods in dependency triple translation and achieves promising results in collocation translation extraction." ></td>
	<td class="line x" title="7:200	1 Introduction A collocation is an arbitrary and recurrent word combination (Benson, 1990)." ></td>
	<td class="line x" title="8:200	Previous work in collocation acquisition varies in the kinds of collocations they detect." ></td>
	<td class="line x" title="9:200	These range from twoword to multi-word, with or without syntactic structure (Smadja 1993; Lin, 1998; Pearce, 2001; Seretan et al. 2003)." ></td>
	<td class="line x" title="10:200	In this paper, a collocation refers to a recurrent word pair linked with a certain syntactic relation." ></td>
	<td class="line x" title="11:200	For instance, <solve, verb-object, problem> is a collocation with a syntactic relation verb-object." ></td>
	<td class="line x" title="12:200	Translation of collocations is difficult for nonnative speakers." ></td>
	<td class="line x" title="13:200	Many collocation translations are idiosyncratic in the sense that they are unpredictable by syntactic or semantic features." ></td>
	<td class="line x" title="14:200	Consider Chinese to English translation." ></td>
	<td class="line x" title="15:200	The translations of   can be solve or resolve." ></td>
	<td class="line x" title="16:200	The translations of   can be problem or issue." ></td>
	<td class="line x" title="17:200	However, translations of the collocation  ~   as solve~problem or resolve~ issue is preferred over solve~issue or resolve ~problem." ></td>
	<td class="line x" title="18:200	Automatically acquiring these collocation translations will be very useful for machine translation, cross language information retrieval, second language learning and many other NLP applications." ></td>
	<td class="line x" title="19:200	(Smadja et al. , 1996; Gao et al. , 2002; Wu and Zhou, 2003)." ></td>
	<td class="line x" title="20:200	Some studies have been done for acquiring collocation translations using parallel corpora (Smadja et al, 1996; Kupiec, 1993; Echizen-ya et al. , 2003)." ></td>
	<td class="line x" title="21:200	These works implicitly assume that a bilingual corpus on a large scale can be obtained easily." ></td>
	<td class="line x" title="22:200	However, despite efforts in compiling parallel corpora, sufficient amounts of such corpora are still unavailable." ></td>
	<td class="line x" title="23:200	Instead of heavily relying on bilingual corpora, this paper aims to solve the bottleneck in a different way: to mine bilingual knowledge from structured monolingual corpora, which can be more easily obtained in a large volume." ></td>
	<td class="line x" title="24:200	Our method is based on the observation that despite the great differences between Chinese and English, the main dependency relations tend to have a strong direct correspondence (Zhou et al. , 2001)." ></td>
	<td class="line x" title="25:200	Based on this assumption, a new translation model based on dependency triples is proposed." ></td>
	<td class="line x" title="26:200	The translation probabilities are estimated from two monolingual corpora using the EM algorithm with the help of a bilingual translation dictionary." ></td>
	<td class="line x" title="27:200	Experimental results show that the proposed triple translation model outperforms the other three models in comparison." ></td>
	<td class="line x" title="28:200	The obtained triple translation model is also used for collocation translation extraction." ></td>
	<td class="line x" title="29:200	Evaluation results demonstrate the effectiveness of our method." ></td>
	<td class="line x" title="30:200	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="31:200	Section 2 provides a brief description on the related work." ></td>
	<td class="line x" title="32:200	Section 3 describes our triple translation model and training algorithm." ></td>
	<td class="line x" title="33:200	Section 4 extracts collocation translations from two independent monolingual corpora." ></td>
	<td class="line x" title="34:200	Section 5 evaluates the proposed method, and the last section draws conclusions and presents the future work." ></td>
	<td class="line x" title="35:200	2 Related work There has been much previous work done on monolingual collocation extraction." ></td>
	<td class="line o" title="36:200	They can in general be classified into two types: window-based and syntax-based methods." ></td>
	<td class="line oc" title="37:200	The former extracts collocations within a fixed window (Church and Hanks 1990; Smadja, 1993)." ></td>
	<td class="line x" title="38:200	The latter extracts collocations which have a syntactic relationship (Lin, 1998; Seretan et al. , 2003)." ></td>
	<td class="line x" title="39:200	The syntax-based method becomes more favorable with recent significant increases in parsing efficiency and accuracy." ></td>
	<td class="line x" title="40:200	Several metrics have been adopted to measure the association strength in collocation extraction." ></td>
	<td class="line x" title="41:200	Thanopoulos et al.(2002) give comparative evaluations on these metrics." ></td>
	<td class="line x" title="43:200	Most previous research in translation knowledge acquisition is based on parallel corpora (Brown et al. , 1993)." ></td>
	<td class="line x" title="44:200	As for collocation translation, Smadja et al.(1996) implement a system to extract collocation translations from a parallel EnglishFrench corpus." ></td>
	<td class="line x" title="46:200	English collocations are first extracted using the Xtract system, then corresponding French translations are sought based on the Dice coefficient." ></td>
	<td class="line x" title="47:200	Echizen-ya et al.(2003) propose a method to extract bilingual collocations using recursive chain-link-type learning." ></td>
	<td class="line x" title="49:200	In addition to collocation translation, there is also some related work in acquiring phrase or term translations from parallel corpus (Kupiec, 1993; Yamamoto and Matsumoto 2000)." ></td>
	<td class="line x" title="50:200	Since large aligned bilingual corpora are hard to obtain, some research has been conducted to exploit translation knowledge from non-parallel corpora." ></td>
	<td class="line x" title="51:200	Their work is mainly on word level." ></td>
	<td class="line x" title="52:200	Koehn and Knight (2000) presents an approach to estimating word translation probabilities using unrelated monolingual corpora with the EM algorithm." ></td>
	<td class="line x" title="53:200	The method exhibits promising results in selecting the right translation among several options provided by bilingual dictionary." ></td>
	<td class="line x" title="54:200	Zhou et al.(2001) proposes a method to simulate translation probability with a cross language similarity score, which is estimated from monolingual corpora based on mutual information." ></td>
	<td class="line x" title="55:200	The method achieves good results in word translation selection." ></td>
	<td class="line x" title="56:200	In addition, (Dagan and Itai, 1994) and (Li, 2002) propose using two monolingual corpora for word sense disambiguation." ></td>
	<td class="line x" title="57:200	(Fung, 1998) uses an IR approach to induce new word translations from comparable corpora." ></td>
	<td class="line x" title="58:200	(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus." ></td>
	<td class="line x" title="59:200	(Cao and Li, 2002) acquire noun phrase translations by making use of web data." ></td>
	<td class="line x" title="60:200	(Wu and Zhou, 2003) also make full use of large scale monolingual corpora and limited bilingual corpora for synonymous collocation extraction." ></td>
	<td class="line x" title="61:200	3 Training a triple translation model from monolingual corpora In this section, we first describe the dependency correspondence assumption underlying our approach." ></td>
	<td class="line x" title="62:200	Then a dependency triple translation model and the monolingual corpus based training algorithm are proposed." ></td>
	<td class="line x" title="63:200	The obtained triple translation model will be used for collocation translation extraction in next section." ></td>
	<td class="line x" title="64:200	3.1 Dependency correspondence between Chinese and English A dependency triple consists of a head, a dependant, and a dependency relation." ></td>
	<td class="line x" title="65:200	Using a dependency parser, a sentence can be analyzed into dependency triples." ></td>
	<td class="line x" title="66:200	We represent a triple as (w 1,r,w 2 ), where w 1 and w 2 are words and r is the dependency relation." ></td>
	<td class="line x" title="67:200	It means that w 2 has a dependency relation r with w 1." ></td>
	<td class="line x" title="68:200	For example, a triple (overcome, verb-object, difficulty) means that difficulty is the object of the verb overcome." ></td>
	<td class="line x" title="69:200	Among all the dependency relations, we only consider the following three key types that we think, are the most important in text analysis and machine translation: verb-object (VO), nounadj(AN), and verbadv(AV)." ></td>
	<td class="line x" title="70:200	It is our observation that there is a strong correspondence in major dependency relations in the translation between English and Chinese." ></td>
	<td class="line x" title="71:200	For example, an object-verb relation in Chinese (e.g.(, VO,  )) is usually translated into the same verb-object relation in English(e.g.(overcome, VO, difficulty))." ></td>
	<td class="line x" title="73:200	This assumption has been experimentally justified based on a large and balanced bilingual corpus in our previous work (Zhou et al. , 2001)." ></td>
	<td class="line x" title="74:200	We come to the conclusion that more than 80% of the above dependency relations have a one-one mapping between Chinese and English." ></td>
	<td class="line x" title="75:200	We can conclude that there is indeed a very strong correspondence between Chinese and English in the three considered dependency relations." ></td>
	<td class="line x" title="76:200	This fact will be used to estimate triple translation model using two monolingual corpora." ></td>
	<td class="line x" title="77:200	3.2 Triple translation model According to Bayess theorem, given a Chinese triple ),,( 21 crcc ctri =, and the set of its candidate English triple translations ),,( 21 eree etri =, the best English triple ),,( 21 eree etri = is the one that maximizes the Equation (1): )|()(maxarg )(/)|()(maxarg )|(maxarg tritritri e tritritritri e tritri e tri ecpep cpecpep cepe tri tri tri = = = (1) where )( tri ep is usually called the language model and )|( tritri ecp is usually called the translation model." ></td>
	<td class="line x" title="78:200	Language Model The language model )( tri ep is calculated with English triples database." ></td>
	<td class="line x" title="79:200	In order to tackle with the data sparseness problem, we smooth the language model with an interpolation method, as described below." ></td>
	<td class="line x" title="80:200	When the given English triple occurs in the corpus, we can calculate it as in Equation (2)." ></td>
	<td class="line x" title="81:200	N erefreq ep e tri ),,( )( 21 = (2) where ),,( 21 erefreq e represents the frequency of triple tri e . N represents the total counts of all the English triples in the training corpus." ></td>
	<td class="line x" title="82:200	For an English triple ),,( 21 eree etri =, if we assume that two words 1 e and 2 e are conditionally independent given the relation e r, Equation (2) can be rewritten as in (3)(Lin, 1998)." ></td>
	<td class="line x" title="83:200	)|()|()()( 21 eeetri repreprpep = (3) where N rfreq rp e e,*)(*, )( =,,*)(*,,*),( )|( 1 1 e e e rfreq refreq rep =,,*)(*, ),(*, )|( 2 22 e e rfreq erfreq rep = . The wildcard symbol * means it can be any word or relation." ></td>
	<td class="line x" title="84:200	With Equations (2) and (3), we get the interpolated language model as shown in (4)." ></td>
	<td class="line x" title="85:200	)|()|()()1( )( )( 21 eee tri tri repreprp N efreq ep  += (4) where 10 <<  .  is calculated as below: )(1 1 1 tri efreq+ = (5) Translation Model We simplify the translation model according the following two assumptions." ></td>
	<td class="line x" title="86:200	Assumption 1: Given an English triple tri e, and the corresponding Chinese dependency relation c r, 1 c and 2 c are conditionally independent." ></td>
	<td class="line x" title="87:200	We have: )|(),|(),|( )|,,()|( 21 21 trictrictric trictritri erpercpercp ecrcpecp = = (6) Assumption 2: For an English triple tri e, assume that i c only depends on {1,2}) (i  i e, and c r only depends on e r . Equation (6) is rewritten as: )|()|()|( )|(),|(),|()|( 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp = = (7) Notice that )|( 11 ecp and )|( 22 ecp are translation probabilities within triples, they are different from the unrestricted probabilities such as the ones in IBM models (Brown et al. , 1993)." ></td>
	<td class="line x" title="88:200	We distinguish translation probability between head ( )|( 11 ecp ) and dependant ( )|( 22 ecp )." ></td>
	<td class="line x" title="89:200	In the rest of the paper, we use )|( ecp head and )|( ecp dep to denote the head translation probability and dependant translation probability respectively." ></td>
	<td class="line x" title="90:200	As the correspondence between the same dependency relation across English and Chinese is strong, we simply assume 1)|( = ec rrp for the corresponding e r and c r, and 0)|( = ec rrp for the other cases." ></td>
	<td class="line x" title="91:200	)|( 11 ecp head and )|( 22 ecp dep cannot be estimated directly because there is no triple-aligned corpus available." ></td>
	<td class="line x" title="92:200	Here, we present an approach to estimating these probabilities from two monolingual corpora based on the EM algorithm." ></td>
	<td class="line x" title="93:200	3.3 Estimation of word translation probability using the EM algorithm Chinese and English corpora are first parsed using a dependency parser, and two dependency triple databases are generated." ></td>
	<td class="line x" title="94:200	The candidate English translation set of Chinese triples is generated through a bilingual dictionary and the assumption of strong correspondence of dependency relations." ></td>
	<td class="line x" title="95:200	There is a risk that unrelated triples in Chinese and English can be connected with this method." ></td>
	<td class="line x" title="96:200	However, as the conditions that are used to make the connection are quite strong (i.e. possible word translations in the same triple structure), we believe that this risk, is not very severe." ></td>
	<td class="line x" title="97:200	Then, the expectation maximization (EM) algorithm is introduced to iteratively strengthen the correct connections and weaken the incorrect connections." ></td>
	<td class="line x" title="98:200	EM Algorithm According to section 3.2, the translation probabilities from a Chinese triple tri c to an English triple tri e can be computed using the English triple language model )( tri ep and a translation model from English to Chinese )|( tritri ecp . The English language model can be estimated using Equation (4) and the translation model can be calculated using Equation (7)." ></td>
	<td class="line x" title="99:200	The translation probabilities )|( ecp head and )|( ecp dep are initially set to a uniform distribution as follows:       == otherwise cif ecpecp e edephead,0 )(, 1 )|()|( (8) Where e  represents the translation set of the English word e. Then, the word translation probabilities are estimated iteratively using the EM algorithm." ></td>
	<td class="line x" title="100:200	Figure 1 gives a formal description of the EM algorithm." ></td>
	<td class="line x" title="101:200	Figure 1: EM algorithm The basic idea is that under the restriction of the English triple language model )( tri ep and translation dictionary, we wish to estimate the translation probabilities )|( ecp head and )|( ecp dep that best explain the Chinese triple database as a translation from the English triple database." ></td>
	<td class="line x" title="102:200	In each iteration, the normalized triple translation probabilities are used to update the word translation probabilities." ></td>
	<td class="line x" title="103:200	Intuitively, after finding the most probable translation of the Chinese triple, we can collect counts for the word translation it contains." ></td>
	<td class="line x" title="104:200	Since the English triple language model provides context information for the disambiguation of the Chinese words, only the appropriate occurrences are counted." ></td>
	<td class="line x" title="105:200	Now, with the language model estimated using Equation (4) and the translation probabilities estimated using EM algorithm, we can compute the best triple translation for a given Chinese triple using Equations (1) and (7)." ></td>
	<td class="line x" title="106:200	4 Collocation translation extraction from two monolingual corpora This section describes how to extract collocation translation from independent monolingual corpora." ></td>
	<td class="line x" title="107:200	First, collocations are extracted from a monolingual triples database." ></td>
	<td class="line x" title="108:200	Then, collocation translations are acquired using the triple translation model obtained in section 3." ></td>
	<td class="line x" title="109:200	4.1 Monolingual collocation extraction As introduced in section 2, much work has been done to extract collocations." ></td>
	<td class="line x" title="110:200	Among all the measure metrics, log likelihood ratio (LLR) has proved to give better results (Duning, 1993; Thanopoulos et al. , 2002)." ></td>
	<td class="line x" title="111:200	In this paper, we take LLR as the metric to extract collocations from a dependency triple database." ></td>
	<td class="line x" title="112:200	For a given Chinese triple ),,( 21 crcc ctri =, the LLR score is calculated as follows: NN dcdcdbdb cacababa ddccbbaaLogl log )log()()log()( )log()()log()( loglogloglog + ++++ ++++ +++= (9) where, . ),,,(),(*, ),,,(,*),( ),,,( 212 211 21 cbaNd crcfreqcrfreqc crcfreqrcfreqb crcfreqa cc cc c = = = = N is the total counts of all Chinese triples." ></td>
	<td class="line x" title="113:200	Those triples whose LLR values are larger than a given threshold are taken as a collocation." ></td>
	<td class="line x" title="114:200	This syntax-based collocation has the advantage that it can represent both adjacent and long distance word association." ></td>
	<td class="line x" title="115:200	Here, we only extract the three main types of collocation that have been mentioned in section 3.1." ></td>
	<td class="line x" title="116:200	4.2 Collocation translation extraction For the acquired collocations, we try to extract their translations from the other monolingual Train language model for English triple )( tri ep ; Initialize word translation probabilities )|( ecp head and )|( ecp dep uniformly as in Equation (8); Iterate Set )|( ecscore head and )|( ecscore dep to 0 for all dictionary entries (c,e); for all Chinese triples ),,( 21 crcc ctri = for all candidate English triple translations ),,( 21 eree etri = compute triple translation probability )|( tritri cep by )|()|()|()( 2211 ecdepheadtri rrpecpecpep end for normalize )|( tritri cep, so that their sum is 1; for all triple translation ),,( 21 eree etri = add )|( tritri cep to )|( 11 ecscore head add )|( tritri cep to )|( 22 ecscore dep endfor endfor for all translation pairs (c,e) set )|( ecp head to normalized )|( ecscore head ; set )|( ecp dep to normalized )|( ecscore dep ; endfor enditerate corpus using the triple translation model trained with the method proposed in section 3." ></td>
	<td class="line x" title="117:200	Our objective is to acquire collocation translations as translation knowledge for a machine translation system, so only highly reliable collocation translations are extracted." ></td>
	<td class="line x" title="118:200	Figure 2 describes the algorithm for Chinese-English collocation translation extraction." ></td>
	<td class="line x" title="119:200	It can be seen that the best English triple candidate is extracted as the translation of the given Chinese collocation only if the Chinese collocation is also the best translation candidate of the English triple." ></td>
	<td class="line x" title="120:200	But the English triple is not necessarily a collocation." ></td>
	<td class="line x" title="121:200	English collocation translations can be extracted in a similar way." ></td>
	<td class="line x" title="122:200	Figure 2: Collocation translation extraction 4.3 Implementation of our approach Our English corpus is from Wall Street Journal (1987-1992) and Associated Press (1988-1990), and the Chinese corpus is from Peoples Daily (1980-1998)." ></td>
	<td class="line x" title="123:200	The two corpora are parsed using the NLPWin parser 1 (Heidorn, 2000)." ></td>
	<td class="line x" title="124:200	The statistics for three main types of dependency triples are shown in tables 1 and 2." ></td>
	<td class="line x" title="125:200	Token refers to the total number of triple occurrences and Type refers to the number of unique triples in the corpus." ></td>
	<td class="line x" title="126:200	Statistic for the extracted Chinese collocations and the collocation translations is shown in Table 3." ></td>
	<td class="line x" title="127:200	Class #Type #Token VO 1,579,783 19,168,229 AN 311,560 5,383,200 AV 546,054 9,467,103 Table 1: Chinese dependency triples 1 The NLPWin parser is a rule-based parser developed at Microsoft research, which parses several languages including Chinese and English." ></td>
	<td class="line x" title="128:200	Its output can be a phrase structure parse tree or a logical form which is represented with dependency triples." ></td>
	<td class="line x" title="129:200	Class #Type #Token VO 1,526,747 8,943,903 AN 1,163,440 6,386,097 AV 215,110 1,034,410 Table 2: English dependency triples Class #Type #Translated VO 99,609 28,841 AN 35,951 12,615 AV 46,515 6,176 Table 3: Extracted Chinese collocations and E-C translation pairs The translation dictionaries we used in training and translation are combined from two dictionaries: HITDic and NLPWinDic 2 . The final E-C dictionary contains 126,135 entries, and C-E dictionary contains 91,275 entries." ></td>
	<td class="line x" title="130:200	5 Experiments and evaluation To evaluate the effectiveness of our methods, two experiments have been conducted." ></td>
	<td class="line x" title="131:200	The first one compares our method with three other monolingual corpus based methods in triple translation." ></td>
	<td class="line x" title="132:200	The second one evaluates the accuracy of the acquired collocation translation." ></td>
	<td class="line x" title="133:200	5.1 Dependency triple translation Triple translation experiments are conducted from Chinese to English." ></td>
	<td class="line x" title="134:200	We randomly selected 2000 Chinese triples (whose frequency is larger than 2) from the dependency triple database." ></td>
	<td class="line x" title="135:200	The standard translation answer sets were built manually by three linguistic experts." ></td>
	<td class="line x" title="136:200	For each Chinese triple, its English translation set contain English triples provided by anyone of the three linguists." ></td>
	<td class="line x" title="137:200	Among 2000 candidate triples, there are 101 triples that cant be translated into English triples with same relation." ></td>
	<td class="line x" title="138:200	For example, the Chinese triple (, VO,  ) should be translated into bargain." ></td>
	<td class="line x" title="139:200	The two words in triple cannot be translated separately." ></td>
	<td class="line x" title="140:200	We call this kind of collocation translation no-compositional translations." ></td>
	<td class="line x" title="141:200	Our current model cannot deal with this kind of translation." ></td>
	<td class="line x" title="142:200	In addition, there are also 157 error dependency triples, which result from parsing mistakes." ></td>
	<td class="line x" title="143:200	We filtered out these two kinds of triples and got a standard test set with 1,742 Chinese triples and 4,645 translations in total." ></td>
	<td class="line x" title="144:200	We compare our triple translation model with three other models on the same standard test set with the same translation dictionary." ></td>
	<td class="line x" title="145:200	As the 2 These two dictionaries are built by Harbin Institute of Technology and Microsoft Research respectively." ></td>
	<td class="line x" title="146:200	For each Chinese collocation col c : a. Acquire the best English triple translation tri e using C-E triple translation model: )|()(maxarg tritritri e tri ecpepe tri = b. For the acquired tri e, calculate the best Chinese triple translation tri c using E-C triple translation model: )|()(maxarg tritritri c tri cepcpc tri = c. If col c = tri c, add col c  tri e to collocation translation database." ></td>
	<td class="line x" title="147:200	baseline experiment, Model A selects the highestfrequency translation for each word in triple; Model B selects translation with the maximal target triple probability, as proposed in (Dagan 1994); Model C selects translation using both language model and translation model, but the translation probability is simulated by a similarity score which is estimated from monolingual corpus using mutual information measure (Zhou et al. , 2001)." ></td>
	<td class="line x" title="148:200	And our triple translation model is model D. Suppose ),,( 21 crcc ctri = is the Chinese triple to be translated." ></td>
	<td class="line x" title="149:200	The four compared models can be formally expressed as follows: Model A: ))((maxarg,)),((maxarg( 2 )( 1 )( max 2211 efreqrefreqe cTranse e cTranse  = Model B: ),,(maxarg)(maxarg 21 )( )( max 22 11 erepepe e cTranse cTranse tri e tri   == Model C: )),Sim(),Sim()((maxarg ))|(likelyhood)((maxarg 2211 )( )( max 22 11 ceceep ecepe tri cTranse cTranse tritritri e tri = =   where, ),Sim( ce is similarity score between e and c (Zhou et al. , 2001)." ></td>
	<td class="line x" title="150:200	Model D (our model): ))|()|()|()((maxarg ))|()((maxarg 2211 )( )( max 22 11 ecdepheadtri cTranse cTranse tritritri e rrpecpecpep ecpepe tri   = = Accuracy(%) CoveRage(%) Top 1 Top 3 Oracle (%) Model A 17.21 ---Model B 33.56 53.79 Model C 35.88 57.74 Model D 83.98 36.91 58.58 66.30 Table 4: Translation results comparison The evaluation results on the standard test set are shown in Table 4, where coverage is the percentages of triples which can be translated." ></td>
	<td class="line x" title="151:200	Some triples cant be translated by Model B, C and D because of the lack of dictionary translations or data sparseness in triples." ></td>
	<td class="line x" title="152:200	In fact, the coverage of Model A is 100%." ></td>
	<td class="line x" title="153:200	It was set to the same as others in order to compare accuracy using the same test set." ></td>
	<td class="line x" title="154:200	The oracle score is the upper bound accuracy under the conditions of current translation dictionary and standard test set." ></td>
	<td class="line x" title="155:200	Top N accuracy is defined as the percentage of triples whose selected top N translations include correct translations." ></td>
	<td class="line x" title="156:200	We can see that both Model C and Model D achieve better results than Model B. This shows that the translation model trained from monolingual corpora really helps to improve the performance of translation." ></td>
	<td class="line x" title="157:200	Our model also outperforms Model C, which demonstrates the probabilities trained by our EM algorithm achieve better performance than heuristic similarity scores." ></td>
	<td class="line x" title="158:200	In fact, our evaluation method is very rigorous." ></td>
	<td class="line x" title="159:200	To avoid bias in evaluation, we take human translation results as standard." ></td>
	<td class="line x" title="160:200	The real translation accuracy is reasonably better than the evaluation results." ></td>
	<td class="line x" title="161:200	But as we can see, compared to the oracle score, the current models still have much room for improvement." ></td>
	<td class="line x" title="162:200	And coverage is also not high due to the limitations of the translation dictionary and the sparse data problem." ></td>
	<td class="line x" title="163:200	5.2 Collocation translation extraction 47,632 Chinese collocation translations are extracted with the method proposed in section 4." ></td>
	<td class="line x" title="164:200	We randomly selected 1000 translations for evaluation." ></td>
	<td class="line x" title="165:200	Three linguistic experts tag the acceptability of the translation." ></td>
	<td class="line x" title="166:200	Those translations that are tagged as acceptable by at least two experts are evaluated as correct." ></td>
	<td class="line x" title="167:200	The evaluation results are shown in Table 5." ></td>
	<td class="line x" title="168:200	Total Acceptance Accuracy (%) VO 590 373 63.22 AN 292 199 68.15 AV 118 60 50.85 All 1000 632 63.20 ColTrans 334 241 72.16 Table 5: Extracted collocation translation results We can see that the extracted collocation translations achieve a much better result than triple translation." ></td>
	<td class="line x" title="169:200	The average accuracy is 63.20% and the collocations with relation AN achieve the highest accuracy of 68.15%." ></td>
	<td class="line x" title="170:200	If we only consider those Chinese collocations whose translations are also English collocations, we obtain an even better accuracy of 72.16% as shown in the last row of Table 5." ></td>
	<td class="line x" title="171:200	The results justify our idea that we can acquire reliable translation for collocation by making use of triple translation model in two directions." ></td>
	<td class="line x" title="172:200	These acquired collocation translations are very valuable for translation knowledge building." ></td>
	<td class="line x" title="173:200	Manually crafting collocation translations can be time-consuming and cannot ensure high quality in a consistent way." ></td>
	<td class="line x" title="174:200	Our work will certainly improve the quality and efficiency of collocation translation acquisition." ></td>
	<td class="line x" title="175:200	5.3 Discussion Although our approach achieves promising results, it still has some limitations to be remedied in future work." ></td>
	<td class="line x" title="176:200	(1) Translation dictionary extension Due to the limited coverage of the dictionary, a correct translation may not be stored in the dictionary." ></td>
	<td class="line x" title="177:200	This naturally limits the coverage of triple translations." ></td>
	<td class="line x" title="178:200	Some research has been done to expand translation dictionary using a non-parallel corpus (Rapp, 1999; Keohn and Knight, 2002)." ></td>
	<td class="line x" title="179:200	It can be used to improve our work." ></td>
	<td class="line x" title="180:200	(2) Noise filtering of parsers Since we use parsers to generate dependency triple databases, this inevitably introduces some parsing mistakes." ></td>
	<td class="line x" title="181:200	From our triple translation test data, we can see that 7.85% (157/2000) types of triples are error triples." ></td>
	<td class="line x" title="182:200	These errors will certainly influence the translation probability estimation in the training process." ></td>
	<td class="line x" title="183:200	We need to find an effective way to filter out mistakes and perform necessary automatic correction." ></td>
	<td class="line x" title="184:200	(3) Non-compositional collocation translation." ></td>
	<td class="line x" title="185:200	Our model is based on the dependency correspondence assumption, which assumes that a triples translation is also a triple." ></td>
	<td class="line x" title="186:200	But there are still some collocations that cant be translated word by word." ></td>
	<td class="line x" title="187:200	For example, the Chinese triple (, VO,  ) usually be translated into be effective; the English triple (take, VO, place) usually be translated into  ." ></td>
	<td class="line x" title="188:200	The two words in triple cannot be translated separately." ></td>
	<td class="line x" title="189:200	Our current model cannot deal with this kind of non-compositional collocation translation." ></td>
	<td class="line x" title="190:200	Melamed (1997) and Lin (1999) have done some research on noncompositional phrases discovery." ></td>
	<td class="line x" title="191:200	We will consider taking their work as a complement to our model." ></td>
	<td class="line x" title="192:200	6 Conclusion and future work This paper proposes a novel method to train a triple translation model and extract collocation translations from two independent monolingual corpora." ></td>
	<td class="line x" title="193:200	Evaluation results show that it outperforms the existing monolingual corpus based methods in triple translation, mainly due to the employment of EM algorithm in cross language translation probability estimation." ></td>
	<td class="line x" title="194:200	By making use of the acquired triple translation model in two directions, promising results are achieved in collocation translation extraction." ></td>
	<td class="line x" title="195:200	Our work also demonstrates the possibility of making full use of monolingual resources, such as corpora and parsers for bilingual tasks." ></td>
	<td class="line x" title="196:200	This can help overcome the bottleneck of the lack of a large-scale bilingual corpus." ></td>
	<td class="line x" title="197:200	This approach is also applicable to comparable corpora, which are also easier to access than bilingual corpora." ></td>
	<td class="line x" title="198:200	In future work, we are interested in extending our method to solving the problem of noncompositional collocation translation." ></td>
	<td class="line x" title="199:200	We are also interested in incorporating our triple translation model for sentence level translation." ></td>
	<td class="line x" title="200:200	7 Acknowledgements The authors would like to thank John Chen, Jianfeng Gao and Yunbo Cao for their valuable suggestions and comments on a preliminary draft of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-3019
TANGO: Bilingual Collocational Concordancer
Jian, Jia-Yan;Chang, Yu-Chia;Chang, Jason S.;"></td>
	<td class="line x" title="1:28	TANGO: Bilingual Collocational Concordancer Jia-Yan Jian Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan g914339@oz.nthu.edu.tw Yu-Chia Chang Inst." ></td>
	<td class="line x" title="2:28	of Information System and Applictaion National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan u881222@alumni.nthu.e du.tw Jason S. Chang Department of Computer Science National Tsing Hua University 101, Kuangfu Road, Hsinchu, Taiwan jschang@cs.nthu.edu.tw Abstract In this paper, we describe TANGO as a collocational concordancer for looking up collocations." ></td>
	<td class="line x" title="3:28	The system was designed to answer users query of bilingual collocational usage for nouns, verbs and adjectives." ></td>
	<td class="line x" title="4:28	We first obtained collocations from the large monolingual British National Corpus (BNC)." ></td>
	<td class="line x" title="5:28	Subsequently, we identified collocation instances and translation counterparts in the bilingual corpus such as Sinorama Parallel Corpus (SPC) by exploiting the wordalignment technique." ></td>
	<td class="line x" title="6:28	The main goal of the concordancer is to provide the user with a reference tools for correct collocation use so as to assist second language learners to acquire the most eminent characteristic of native-like writing." ></td>
	<td class="line x" title="7:28	1 Introduction Collocations are a phenomenon of word combination occurring together relatively often." ></td>
	<td class="line x" title="8:28	Collocations also reflect the speakers fluency of a language, and serve as a hallmark of near nativelike language capability." ></td>
	<td class="line x" title="9:28	Collocation extraction is critical to a range of studies and applications, including natural language generation, computer assisted language learning, machine translation, lexicography, word sense disambiguation, cross language information retrieval, and so on." ></td>
	<td class="line oc" title="10:28	Hanks and Church (1990) proposed using pointwise mutual information to identify collocations in lexicography; however, the method may result in unacceptable collocations for low-count pairs." ></td>
	<td class="line x" title="11:28	The best methods for extracting collocations usually take into consideration both linguistic and statistical constraints." ></td>
	<td class="line x" title="12:28	Smadja (1993) also detailed techniques for collocation extraction and developed a program called XTRACT, which is capable of computing flexible collocations based on elaborated statistical calculation." ></td>
	<td class="line x" title="13:28	Moreover, log likelihood ratios are regarded as a more effective method to identify collocations especially when the occurrence count is very low (Dunning, 1993)." ></td>
	<td class="line x" title="14:28	Smadjas XTRACT is the pioneering work on extracting collocation types." ></td>
	<td class="line x" title="15:28	XTRACT employed three different statistical measures related to how associated a pair to be collocation type." ></td>
	<td class="line x" title="16:28	It is complicated to set different thresholds for each statistical measure." ></td>
	<td class="line x" title="17:28	We decided to research and develop a new and simple method to extract monolingual collocations." ></td>
	<td class="line x" title="18:28	We also provide a web-based user interface capable of searching those collocations and its usage." ></td>
	<td class="line x" title="19:28	The concordancer supports language learners to acquire the usage of collocation." ></td>
	<td class="line x" title="20:28	In the following section, we give a brief overview of the TANGO concordancer." ></td>
	<td class="line x" title="21:28	2 TANGO TANGO is a concordancer capable of answering users queries on collocation use." ></td>
	<td class="line x" title="22:28	Currently, TANGO supports two text collections: a monolingual corpus (BNC) and a bilingual corpus (SPC)." ></td>
	<td class="line x" title="23:28	The system consists of four main parts: 2.1 Chunk and Clause Information Integrated For CoNLL-2000 shared task, chunking is considered as a process that divides a sentence into syntactically correlated parts of words." ></td>
	<td class="line x" title="24:28	With the benefits of CoNLL training data, we built a chunker that turn sentences into smaller syntactic structure of non-recursive basic phrases to facilitate precise collocation extraction." ></td>
	<td class="line x" title="25:28	It becomes easier to identify the argument-predicate relationship by looking at adjacent chunks." ></td>
	<td class="line x" title="26:28	By doing so, we save time as opposed to n-gram statistics or full parsing." ></td>
	<td class="line x" title="27:28	Take a text in CoNLL2000 for example: The words correlated with the same chunk tag can be further grouped together (see Table 1)." ></td>
	<td class="line x" title="28:28	For instance, with chunk information, we can extract" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-1113
Using Synonym Relations In Chinese Collocation Extraction
Li, Wanyin;Lu, Qin;Xu, Ruifeng;"></td>
	<td class="line x" title="1:194	Using Synonym Relations In Chinese Collocation Extraction Wanyin Li Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong cswyli@comp.polyu.edu.hk Qin Lu Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong csluqin @comp.polyu.edu.hk Ruifeng Xu Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong csrfxu@comp.polyu.edu.hk Abstract A challenging task in Chinese collocation extraction is to improve both the precision and recall rate." ></td>
	<td class="line x" title="2:194	Most lexical statistical methods including Xtract face the problem of unable to extract collocations with lower frequencies than a given threshold." ></td>
	<td class="line x" title="3:194	This paper presents a method where HowNet is used to find synonyms using a similarity function." ></td>
	<td class="line x" title="4:194	Based on such synonym information, we have successfully extracted synonymous collocations which normally cannot be extracted using the lexical statistical approach." ></td>
	<td class="line x" title="5:194	We applied synonyms mapping to each headword to extract more synonymous word bi-grams." ></td>
	<td class="line x" title="6:194	Our evaluation over 60MB tagged corpus shows that we can extract synonymous collocations that occur with very low frequency, sometimes even for collocations that occur only once in the training set." ></td>
	<td class="line x" title="7:194	Comparing to a collocation extraction system based on Xtract, we have reached the precision rate of 43% on word bi-grams for a set of 9 headwords, almost 50% improvement from precision rate of 30% in the Xtract system." ></td>
	<td class="line x" title="8:194	Furthermore, it improves the recall rate of word bi-gram collocation extraction by 30%." ></td>
	<td class="line x" title="9:194	1 Introduction A Chinese collocation is a recurrent and conventional expression of words which holds syntactic and semantic relations." ></td>
	<td class="line x" title="10:194	A widely adopted definition given by Benson (Benson 1990) stated that a collocation is an arbitrary and recurrent word combination. For example, we say warm greetings rather than hot greetings, broad daylight rather than bright daylight." ></td>
	<td class="line x" title="11:194	Similarly, in Chinese       are three nouns with similar meanings, however, we say   rather than  ,  rather than  ." ></td>
	<td class="line oc" title="12:194	Study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction (Church and Hanks 1990, Smadja 1993, Choueka 1993, Lin 1998)." ></td>
	<td class="line x" title="13:194	As the lexical statistical approach is developed based on the recurrence property of collocations, only collocations with reasonably good recurrence can be extracted." ></td>
	<td class="line x" title="14:194	Collocations with low occurrence frequency cannot be extracted, thus affecting the recall rate." ></td>
	<td class="line x" title="15:194	The precision rate using the lexical statistics approach can reach around 60% if both word bi-gram extraction and n-gram extractions are taking into account (Smadja 1993, Lin 1997 and Lu et al. 2003)." ></td>
	<td class="line x" title="16:194	The low precision rate is mainly due to the low precision rate of word bigram extractions as only about 30% 40% precision rate can be achieved for word bi-grams." ></td>
	<td class="line x" title="17:194	In this paper, we propose a different approach to find collocations with low recurrences." ></td>
	<td class="line x" title="18:194	The main idea is to make use of synonym relations to extract synonymous collocations." ></td>
	<td class="line x" title="19:194	Lin (Lin 1997) described a distributional hypothesis that if two words have similar set of collocations, they are probably similar." ></td>
	<td class="line x" title="20:194	In HowNet, Liu Qun (Liu et al. 2002) defined the word similarity as two words that can substitute each other in the context and keep the sentence consistent in syntax and semantic structure." ></td>
	<td class="line x" title="21:194	That means, naturally, two similar words are very close to each other and they can be used in place of the other in certain context." ></td>
	<td class="line x" title="22:194	For example, we may either say  or   as and are semantically close to each other." ></td>
	<td class="line x" title="23:194	We apply this lexical phenomenal after the lexical statistics based extractor to find the low frequency synonymous collocations, thus increasing recall rate." ></td>
	<td class="line x" title="24:194	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="25:194	Section 2 describes related existing collocation extraction techniques based on both lexical statistics and synonymous collocation." ></td>
	<td class="line x" title="26:194	Section 3 describes our approach on collocation extraction." ></td>
	<td class="line x" title="27:194	Section 4 evaluates the proposed method." ></td>
	<td class="line x" title="28:194	Section 5 draws our conclusion and presents possible future work." ></td>
	<td class="line x" title="29:194	2 Related Work Methods have proposed to extract collocations based on lexical statistics." ></td>
	<td class="line x" title="30:194	Choueka (Choueka 1993) applied quantitative selection criteria based on frequency threshold to extract adjacent n-grams (including bi-grams)." ></td>
	<td class="line oc" title="31:194	Church and Hanks (Church and Hanks 1990) employed mutual information to extract both adjacent and distant bi-grams that tend to co-occur within a fixed-size window." ></td>
	<td class="line n" title="32:194	But the method did not extend to extract n-grams." ></td>
	<td class="line x" title="33:194	Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength." ></td>
	<td class="line x" title="34:194	This method successfully extracted both adjacent and distant bi-grams and n-grams." ></td>
	<td class="line x" title="35:194	However, the method failed to extract bi-grams with lower frequency." ></td>
	<td class="line x" title="36:194	The precision rate on bi-grams collocation is very low, only around high 20% and low 30%." ></td>
	<td class="line x" title="37:194	Even though, it is difficult to measure recall rate in collocation extraction (almost no report on recall estimation), It is understood that low occurrence collocations cannot be extracted." ></td>
	<td class="line x" title="38:194	Our research group has further applied the Xtract system to Chinese (Lu et al. 2003) by adjusting the parameters to optimize the algorithm for Chinese and a new weighted algorithm was developed based on mutual information to acquire word bigrams with one higher frequency word and one lower frequency word." ></td>
	<td class="line x" title="39:194	The result has achieved an estimated 5% improvement in recall rate and a 15% improvement in precision comparing to the Xtract system." ></td>
	<td class="line x" title="40:194	All of the above techniques do not take advantage of the wide range of lexical resources available including synonym information." ></td>
	<td class="line x" title="41:194	Pearce (Pearce 2001) presented a collocation extraction technique that relies on a mapping from a word to its synonyms for each of its senses." ></td>
	<td class="line x" title="42:194	The underlying intuitions is that if the difference between the occurrence counts of one synonyms pair with respect to a particular word was at least two, then this was deemed sufficient to consider them as a collocation." ></td>
	<td class="line x" title="43:194	To apply this approach, knowledge in word (concept) semantics and relations to other words must be available such as the use of WordNet." ></td>
	<td class="line x" title="44:194	Dagan (Dagan 1997) applied similaritybased smoothing method to solve the problem of data sparseness in statistical natural language processing." ></td>
	<td class="line x" title="45:194	The experiments conducted in his later works showed that this method achieved much better results than back-off smoothing methods in word sense disambiguation." ></td>
	<td class="line x" title="46:194	Similarly, Hua Wu (Wu and Zhou 2003) applied synonyms relationship between two different languages to automatically acquire English synonymous collocation." ></td>
	<td class="line x" title="47:194	This is the first time that the concept synonymous collocation is proposed." ></td>
	<td class="line x" title="48:194	A side intuition raised here is that nature language is full of synonymous collocations." ></td>
	<td class="line x" title="49:194	As many of them have low occurrences, they are failed to be retrieved by lexical statistical methods." ></td>
	<td class="line x" title="50:194	Even though there are Chinese synonym dictionaries, such as ( Tong Yi Ci Lin), the dictionaries lack structured knowledge and synonyms are too loosely defined to be used for collocation extraction." ></td>
	<td class="line x" title="51:194	HowNet developed by Dong et al (Dong and Dong 1999) is the best publicly available resource on Chinese semantics." ></td>
	<td class="line x" title="52:194	By making use of semantic similarities of words, synonyms can be defined by the closeness of their related concepts and the closeness can be calculated." ></td>
	<td class="line x" title="53:194	In Section 3, we present our method to extract synonyms from HowNet and using synonym relations to further extract collocations." ></td>
	<td class="line x" title="54:194	Sun (Sun 1997) did a preliminary Quantitative analysis on Chinese collocations based on their arbitrariness, recurrence and the syntax structure." ></td>
	<td class="line x" title="55:194	The purpose of this study is to help differentiate if a collocation is true or not according to the quantitative factors." ></td>
	<td class="line x" title="56:194	By observing the existence of synonyms information in natural language use, we consider it possible to identify different types of collocations using more semantic and syntactic information available." ></td>
	<td class="line x" title="57:194	We discuss the basic ideas in section 5 3 Our Approach Our method of extracting Chinese collocations consists of three steps." ></td>
	<td class="line x" title="58:194	Step 1: Take the output of any lexical statistical algorithm which extracts word bi-gram collocations." ></td>
	<td class="line x" title="59:194	The data is then sorted according to each headword, W h, with its coword, W c, listed." ></td>
	<td class="line x" title="60:194	Step 2: For each headword W h used to extract bigrams, we acquire its synonyms based on a similarity function using HowNet." ></td>
	<td class="line x" title="61:194	Any word in HowNet having similarity value over a threshold value is chosen as a synonym headword W s for additional extractions." ></td>
	<td class="line x" title="62:194	Step 3: For each synonym headword, W s, and the co-word W c of W h, as its synonym, if the bigram (W s, W c ) is not in the output of the lexical statistical algorithm in Step one, take this bi-gram (W s, W c ) as a collocation if the pair co-occurs in the corpus by additional search to the corpus." ></td>
	<td class="line x" title="63:194	3.1 Structure of HowNet Different from WordNet or other synonyms dictionary, HowNet describes words as a set of concepts and each concept is described by a set of primitives." ></td>
	<td class="line x" title="64:194	The following lists for the word, one of its corresponding concepts In the above record, DEF is where the primitives are specified." ></td>
	<td class="line x" title="65:194	DEF contains up to four types of primitives: the basic independent primitive, the other independent primitive, the relation primitive, and the symbol primitive, where the basic independent primitive and the other independent primitive are used to indicate the semantics of a concept and the others are used to indicate syntactical relationships." ></td>
	<td class="line x" title="66:194	The similarity model described in the next subsection will consider both of these relationships." ></td>
	<td class="line x" title="67:194	The primitives are linked by a hierarchical tree to indicate the parent-child relationships of the primitives as shown in the following example: This hierarchical structure provides a way to link one concept with any other concept in HowNet, and the closeness of concepts can be simulated by the distance between two concepts." ></td>
	<td class="line x" title="68:194	3.2 Similarity Model Based on HowNet Liu Qun (Liu 2002) defined word similarity as two words which can substitute each other in the same context and still maintain the sentence consistent syntactically and semantically." ></td>
	<td class="line x" title="69:194	This is very close to our definition of synonyms." ></td>
	<td class="line x" title="70:194	Thus we directly used their similarity function, which is stated as follows." ></td>
	<td class="line x" title="71:194	A word in HowNet is defined as a set of concepts and each concept is represented by primitives." ></td>
	<td class="line x" title="72:194	Thus, HowNet can be described by W, a collection of n words, as: W = { w 1, w 2,  w n }Each word w i is, in turn, described by a set of concepts S as: W i = { S i1, S i2, S ix }, And, each concept S i is, in turn, described by a set of primitives: S i = { p i1, p i2 p iy } For each word pair, w 1 and w 2, the similarity function is defined by )1(),(max),( 21 1,1 21 ji mjni SSSimwwSim == = where S 1i is the list of concepts associated with W 1 and S 2j is the list of concepts associated with W 2 . As any concept S i is presented by its primitives, the similarity of primitives for any p 1, and p 2 of the same type, can be expressed by the following formula:   + = ),( ),( 21 21 ppDis ppSim (2) where  is an adjustable parameter set to 1.6, and ),( 21 ppDis is the path length between p 1 and p 2 based on the semantic tree structure." ></td>
	<td class="line x" title="73:194	The above formula where  is a constant does not indicate explicitly the fact that the depth of a pair of nodes in the tree affects their similarity." ></td>
	<td class="line x" title="74:194	For two pairs of nodes (p 1, p 2 ) and (p 3, p 4 ) with the same distance, the deeper the depth is, the more commonly shared ancestros they would have which should be semantically closer to each other." ></td>
	<td class="line x" title="75:194	In following two tree structures, the pair of nodes (p 1, p 2 ) in the left tree should be more similar than (p 3, p 4 ) in the right tree." ></td>
	<td class="line x" title="76:194	root p 2 p 1 root P 3 P 4 To indicate this observation,  is modified as a function of tree depths of the nodes using the formula  =min(d(p 1 ), d(p 2 )) . Consequently, the formula (2) is rewritten as formular (2) during the experiment." ></td>
	<td class="line x" title="77:194	))(),(min(),( ))(),(min( ),( 2121 21 21 pdpdppDis pdpd ppSim + = (2) where d(p i ) is the depth of node p i in the tree . The comparison of calculating the word similarity by applying the formula (2) and (2) is shown in Section 4.4." ></td>
	<td class="line x" title="78:194	Based on the DEF description in HowNet, different primitive types play different roles only some are directly related to semantics." ></td>
	<td class="line x" title="79:194	To make use of both the semantic and syntactic information included in HowNet to describe a word, the similarity of two concepts should take into consideration of all primitive types with weighted considerations and thus the formula is defined as )3(),(),( 21 1 4 1 21 jj i j j i i ppSimSSSim  == =  where  i is a weighting factor given in (Liu 2002) with the sum of  1 +  2 +  3 +  4 being 1 and  1   2   3   4 . The distribution of the weighting factors is given for each concept a priori in HowNet to indicate the importance of primitive p i in defining the corresponding concept S. 3.3 Collocation Extraction In order to extract collocations from a corpus, and to obtain result for Step 1 of our algorithm, we used the collocation extraction algorithm developed by the research group at the Hong Kong Polytechnic University(Lu et al. 2003)." ></td>
	<td class="line x" title="80:194	The extraction of bi-gram collocation is based on the English Xtract(Smaja 1993) with improvements." ></td>
	<td class="line x" title="81:194	Based on the three Steps mentioned earlier, we will present the extractions in each step in the subsections." ></td>
	<td class="line x" title="82:194	3.3.1 Bi-gram Extraction Based on the lexical statistical model proposed by Smadja in Xtract on extracting English collocations, an improved algorithm was developed for Chinese collocation by our research group and the system is called CXtract." ></td>
	<td class="line x" title="83:194	For easy of understanding, we will explain the algorithm briefly here." ></td>
	<td class="line x" title="84:194	According to Xtract, word cooccurence is denoted by a tripplet (w h, w i, d) where w h is a given headword, w i is a co-word appeared in the corpus in a distance d within the window of [-5, 5]." ></td>
	<td class="line x" title="85:194	The frequency f i of the co-word w i in the window of [-5, 5] is defined as:  = = 5 5, j jii ff (4) where f i, j is the frequency of the co-word at distance j in the corpus within the window." ></td>
	<td class="line x" title="86:194	The average frequency of f i, denoted by i f, is given by 10/ 5 5, = = j jii ff (5) Then, the average frequency f, and the standard deviation  are defined by  = = n i i f n f 1 1 ; 2 1 )( 1  = = n i i ff n  (6) The Strength of the co-occurrence for the pair (w h, w i,), denoted by k i, is defined by  ff k i i  =  (7) Furthermore, the Spread of (w h, w i,),, denoted as U i, which characterizes the distribution of w i around w h is define as: 10 )( 2,  = iji i ff U ; (8) To eliminate the bi-grams with unlikely cooccurrence, the following sets of threshold values is defined: 0 :1 K ff kC i i   =  (9) 0 :2 UUC i  (10) )(:3 1, iiji UKffC + (11) However, the above statistical model given by Smadja fails to extract the bi-grams with a much higher frequency of w h but a relatively low frequency word of w i,, For example, in the bigram, freq ( ) is much lower than the freq ( )." ></td>
	<td class="line x" title="87:194	Therefore, we further defined a weighted mutual information to extract this kind of bi-grams:, )( ),w( 0 h R wf wf R i i i = (12) As a result, the system should return a list of triplets (w h, w i, d), where (w h, w i,) is considered collocations." ></td>
	<td class="line x" title="88:194	3.3.2 Synonyms Set For each given headword w h, before taking it as an input to extract its bi-grams directly, we fist apply the similarity formula described in Equation (1) to generate a set of synonyms headwords W syn : }),(:{ >= shssyn wwSimwW (13) Where 0 < <1 is an algorithm parameter which is adjusted based on experience." ></td>
	<td class="line x" title="89:194	We set it as 0.85 from the experiment because we would like to balance the strength of the synonyms relationship and the coverage of the synonyms set." ></td>
	<td class="line x" title="90:194	The setting of the parameter  < 0.85 weaks the similarity strength of the extracted synonyms." ></td>
	<td class="line x" title="91:194	For example, for a given collocation  , that is unlikely to include the candidates  ,  ,  ." ></td>
	<td class="line x" title="92:194	On the other hand, by setting the parameter  > 0.85 will limit the coverage of the synonyms set and hence lose valuable synonyms." ></td>
	<td class="line x" title="93:194	For example, for a given bi-gram  , we hope to include the candidate synonymous collocations such as  ,  ,  ." ></td>
	<td class="line x" title="94:194	We will show the test of  in the section 4.2." ></td>
	<td class="line x" title="95:194	This synonyms headwords set provides the possibility to extract the synonymous collocation with the lower frequency that failed to be extracted by lexical statistic." ></td>
	<td class="line x" title="96:194	3.3.3 Synonymous Collocations A phenomenal among the collocations in natural language is that there are many synonymous collocations exist." ></td>
	<td class="line x" title="97:194	For example, switch on light and turn on light,   and  ." ></td>
	<td class="line x" title="98:194	Due to the domain specification of the corpus, some of the synonymous collocations may fail to be extracted by the lexical statistic model because of their lower frequency." ></td>
	<td class="line x" title="99:194	Based on this observation, this paper takes a further step." ></td>
	<td class="line x" title="100:194	The basic idea is for a bi-gram collocation (w h, w c, d ) we select the synonyms w s of w h with the maximum similarity respect to all the concepts contained by w h, we deem (w s, w c, d ) as a collocation if its occurrence is greater than 1 in the corpus." ></td>
	<td class="line x" title="101:194	There are similar works discussed by Pearce (Pearce 2001)." ></td>
	<td class="line x" title="102:194	For a given collocation (w s, w c,, d), if w s  W syn, then we deem the triple (w s, w c,, d) as a synonymous collocation with respect to the collocation (w h, w c,, d) if the co-occurrence of (w s, w c,, d) in the corpus is greater than one." ></td>
	<td class="line x" title="103:194	Therefore, we define the collection of synonymous collocations C syn as: }1),,(:),,{( >= dwwFreqdwwC cscssyn (14) where w s  W syn . 4 Evaluation The performance of collocation is normally evaluated by precision and recall as defined below." ></td>
	<td class="line x" title="104:194	nsCollocatioextractedofnumbertotal nsCollocatioExtractedcorrectofnumber precision = (15) nsCollocatioactualofnumbertotal nsCollocatioExtractedcorrectofnumber recall = (16) To evaluate the performance of our approach, we conducted a set of experiments based on 9 selected headwords." ></td>
	<td class="line x" title="105:194	A baseline system using only lexical statistics given in 3.3.1 is used to get a set of baseline data called Set A. The output using our algorithm is called Set B. Results are checked by hand for validation on what is true collocation and what is not a true collocation." ></td>
	<td class="line x" title="106:194	Table 1." ></td>
	<td class="line x" title="107:194	Sample table for the true collocation with headword   Table 2." ></td>
	<td class="line x" title="108:194	Sample table for the bi-grams that are not true collocations Table 1 shows samples of extracted word bi-grams using our algorithm that are considered synonymous collocations for the headword  ." ></td>
	<td class="line x" title="109:194	Table 2 shows extracted bi-grams by our algorithm that are not considered true collocations." ></td>
	<td class="line x" title="110:194	4.1 Test Set Our experiment is based on a corpus of six months tagged People Daily with 11 millions number of words." ></td>
	<td class="line x" title="111:194	For word bi-gram extractions, we consider only content words, thus headwords are selected from noun, verb and adjective only." ></td>
	<td class="line x" title="112:194	For evaluation purpose, we selected randomly 3 nouns, 3 verbs and 3 adjectives with frequency of low, medium and high." ></td>
	<td class="line x" title="113:194	Thus, in Step 1 of the algorithm, 9 headwords were used to extract bigram collocations from the corpus, and 253 pairs of collocations were extracted." ></td>
	<td class="line x" title="114:194	Evaluation by hand has identified 77 true collocations in Set A test set." ></td>
	<td class="line x" title="115:194	The overall precision rate is 30% (see Table 3)." ></td>
	<td class="line x" title="116:194	Noun+Verb +Adjective Headword 9 Extracted Bi-grams 253 True collocations using lexical statistics only 77 Precision rate 30% Table 3: Statistics in test set for set A Using Step 2 of our algorithm, where  =0.85 is used, we have obtained 55 synonym headwords (include the 9 headwords)." ></td>
	<td class="line x" title="117:194	Out of these 55 synonyms, 614 bi-gram pairs were then extracted from the lexical statistics based algorithm, in which 179 are consider true collocations." ></td>
	<td class="line x" title="118:194	Then, by applying Step 3 of our algorithm, we extracted an additional 201 bi-gram pairs, among them, 178 are considered true collocations." ></td>
	<td class="line x" title="119:194	Therefore, using our algorithm, the overall precision rate has achieved 43%, an improvement of almost 50%." ></td>
	<td class="line x" title="120:194	The data is summarized in Table 4." ></td>
	<td class="line x" title="121:194	n., v, and adj." ></td>
	<td class="line x" title="122:194	Synonyms headword 55 Bi-grams (lexical statistics) 614 Non-synonym collocations (lexical statistics only) 179 Extracted synonym collocations Step 2 201 True synonym collocations using Step 2 178 Overall precision rate 43% Table 4: Statistics in test set for mode B 4.2 The choice of  We also conducted a set of experiments to choose the best value for the similarity functions threshold  . We tested the best value of  with both the precision rate and the estimated recall rate using the so called remainder bi-grams." ></td>
	<td class="line x" title="123:194	The remainder bi-grams is the total number of bi-grams extracted by the algorithm." ></td>
	<td class="line x" title="124:194	When precision goes up, the size of the result is smaller, which in a way is an indicator of less recalled collocations." ></td>
	<td class="line x" title="125:194	Figure 1 shows the precision rate and the estimated recall rate in testing the value of  . Figure 1." ></td>
	<td class="line x" title="126:194	Precision Rate vs. value of  From Figure 1, it is obvious that at  =0.85 the recall rate starts to drop more drastically without much incentive for precision." ></td>
	<td class="line x" title="127:194	Extracted Bigrams using lexical statistics Extracted Synonyms Collocations using Step 2 (1.2,1.4,12) 465 328 (1.4,1.4,12) 457 304 (1.4,1.6,12) 394 288 (1.2,1.2,12) 513 382 (1.2,1.2,14) 503 407 (1.2,1.2,16) 481 413 Table 5: Value of (K 0, K 1, U 0 )." ></td>
	<td class="line x" title="128:194	4.3 The test of (K 0, K 1, U 0 ) The original threshold for CXtract is (1.2, 1.2, 12) for the parameters (K 0, K 1, U 0 )." ></td>
	<td class="line x" title="129:194	However, with synonyms collocations, we have also conducted some experiments to see whether the parameters should be adjusted." ></td>
	<td class="line x" title="130:194	Table 5 shows the statistics to test the value of (K 0, K 1, U 0 )." ></td>
	<td class="line x" title="131:194	The similarity threshold  was fixed at 0.85 throughout the experiments." ></td>
	<td class="line x" title="132:194	The experimental shows that varying the value of (k 0, k 1 ) does not bring any benefit to our algorithm." ></td>
	<td class="line x" title="133:194	However, increasing the value of u 0 did improve the extraction of synonymous collocations." ></td>
	<td class="line x" title="134:194	Figure 2 shows that U 0 =14 is a good trade-off for the precision rate and the remainder Bi-grams." ></td>
	<td class="line x" title="135:194	The basic meaning behind the result is reasonable." ></td>
	<td class="line x" title="136:194	According to Smadja, U 0 defined in the formula (8) represents the co-occurrence distribution of the candidate collocation (w h, w c ) in the position of d (-5  d  5)." ></td>
	<td class="line x" title="137:194	For a true collocation (w h, w c,, d), its cooccurrence in the position d is much higher than in other positions which leads to a peak in the cooccurrence distribution." ></td>
	<td class="line x" title="138:194	Therefore, it is selected by the statistical algorithm based on the formula (10)." ></td>
	<td class="line x" title="139:194	Based on the physical meaning behind, one way to improve the precision rate is to increase the value of the threshold U 0." ></td>
	<td class="line x" title="140:194	A side effect to an increased value of U 0 is that the recall is decreased because some true collocations do not meet the condition of cooccurrence greater than U 0." ></td>
	<td class="line x" title="141:194	Step 2 of the new algorithm regains some true collocations lost because of a higher U 0." ></td>
	<td class="line x" title="142:194	in Step 1." ></td>
	<td class="line x" title="143:194	Figure 2." ></td>
	<td class="line x" title="144:194	Precision Rate vs. Value of U 0 4.4 The comparison of similarity calculation based on formula (2) and (2) Table 6 shows the similarity value given by formula (2) where  is a constant given the value 1.6 and by formula (2) where  is replaced by a function of the depths of the nodes." ></td>
	<td class="line x" title="145:194	Results show that (2) is more fine tuned and reflects the nature of the data better." ></td>
	<td class="line x" title="146:194	For example, and are more similar than and . and are much similar but not the same." ></td>
	<td class="line x" title="147:194	Table 6: comparison of similarity calculation 5 Conclusion and Further Work In this paper, we have presented a method to extract bi-gram collocations using lexical statistics model with synonyms information." ></td>
	<td class="line x" title="148:194	Our method reaches the precision rate of 43% for the tested data." ></td>
	<td class="line x" title="149:194	Comparing to the precision of 30% using lexical statistics only, our improvement is close to 50%." ></td>
	<td class="line x" title="150:194	In additional, the recall improved 30%." ></td>
	<td class="line x" title="151:194	The contribution is that we have made use of synonym information which is plentiful in the natural language use and it works well to supplement the shortcomings of lexical statistical method." ></td>
	<td class="line x" title="152:194	Manning claimed that the lack of valid substitution for a synonym is a characteristics of collocations in general (Manning and Schutze 1999)." ></td>
	<td class="line x" title="153:194	To extend our work, we consider the use of synonym information can be further applied to help identify collocations of different types." ></td>
	<td class="line x" title="154:194	Our preliminary study has suggested that collocation can be classified into 4 types: Type 0 Collocation: Fully fixed collocation which include some idioms, proverbs and sayings such as     and so on." ></td>
	<td class="line x" title="155:194	Type 1 Collocation: Fixed collocation in which the appearance of one word implies the cooccurrence of another one such as  ." ></td>
	<td class="line x" title="156:194	Type 2 Collocation: Strong collocation which allows very limited substitution of the components, for example,  ,  ,   and so on." ></td>
	<td class="line x" title="157:194	Type 3 Collocation: Normal collocation which allows more substitution of the components, however a limitation is still required." ></td>
	<td class="line x" title="158:194	For example,         . By using synonym information and define substitutability, we can validate whether collocations are fixed collocations, strong collocations with very limited substitutions, or general collocations that can be substituted more freely." ></td>
	<td class="line x" title="159:194	6 Acknowledgements Our great thanks to Dr. Liu Qun of the Chinese Language Research Center of Peking University for letting us share their data structure in the Synonyms Similarity Calculation." ></td>
	<td class="line x" title="160:194	This work is partially supported by the Hong Kong Polytechnic University (Project Code A-P203) and CERG Grant (Project code 5087/01E) References M. Benson, 1990." ></td>
	<td class="line x" title="161:194	Collocations and General Purpose Dictionaries." ></td>
	<td class="line x" title="162:194	International Journal of Lexicography, 3(1): 23-35 Y. Choueka, 1993." ></td>
	<td class="line x" title="163:194	Looking for Needles in a Haystack or Locating Interesting Collocation Expressions in Large Textual Database." ></td>
	<td class="line x" title="164:194	Proceedings of RIAO Conference on Useroriented Content-based Text and Image Handling: 21-24, Cambridge." ></td>
	<td class="line xc" title="165:194	K. Church, and P. Hanks, 1990." ></td>
	<td class="line x" title="166:194	Word Association Norms, Mutual Information,and Lexicography." ></td>
	<td class="line x" title="167:194	Computational Linguistics, 6(1): 22-29." ></td>
	<td class="line x" title="168:194	I. Dagan, L. Lee, and F. Pereira." ></td>
	<td class="line x" title="169:194	1997." ></td>
	<td class="line x" title="170:194	Similaritybased method for word sense disambiguation." ></td>
	<td class="line x" title="171:194	Proceedings of the 35th Annual Meeting of ACL: 56-63, Madrid, Spain." ></td>
	<td class="line x" title="172:194	Z. D. Dong and Q. Dong." ></td>
	<td class="line x" title="173:194	1999." ></td>
	<td class="line x" title="174:194	Hownet, http://www.keenage.com D. K. Lin, 1997." ></td>
	<td class="line x" title="175:194	Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity." ></td>
	<td class="line x" title="176:194	Proceedings of ACL/EACL-97: 64-71, Madrid, Spain Q. Liu, 2002." ></td>
	<td class="line x" title="177:194	The Word Similarity Calculation on <<HowNet>>." ></td>
	<td class="line x" title="178:194	Proceedings of 3 rd Conference on Chinese lexicography, TaiBei Q. Lu, Y. Li, and R. F. Xu, 2003." ></td>
	<td class="line x" title="179:194	Improving Xtract for Chinese Collocation Extraction." ></td>
	<td class="line x" title="180:194	Proceedings of IEEE International Conference on Natural Language Processing and Knowledge Engineering, Beijing C. D. Manning and H. Schutze, 1999." ></td>
	<td class="line x" title="181:194	Foundations of Statistical Natural Language Processing." ></td>
	<td class="line x" title="182:194	The MIT Press, Cambridge, Massachusetts D. Pearce, 2001." ></td>
	<td class="line x" title="183:194	Synonymy in Collocation Extraction." ></td>
	<td class="line x" title="184:194	Proceedings of NAACL'01 Workshop on Wordnet and Other Lexical Resources: Applications, Extensions and Customizations F. Smadja, 1993." ></td>
	<td class="line x" title="185:194	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="186:194	Computational Linguistics, 19(1): 143177 H. Wu, and M. Zhou, 2003." ></td>
	<td class="line x" title="187:194	Synonymous Collocation Extraction Using Translation Information." ></td>
	<td class="line x" title="188:194	Proceeding of the 41st Annual Meeting of ACL D. K. Lin, 1998." ></td>
	<td class="line x" title="189:194	Extracting collocations from text corpora." ></td>
	<td class="line x" title="190:194	In Proc." ></td>
	<td class="line x" title="191:194	First Workshop on Computational Terminology, Montreal, Canada." ></td>
	<td class="line x" title="192:194	M. S. Sun, C. N. Huang and J. Fang, 1997." ></td>
	<td class="line x" title="193:194	Preliminary Study on Quantitative Study on Chinese Collocations." ></td>
	<td class="line x" title="194:194	ZhongGuoYuWen, No.1, 29-38, (in Chinese) ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-1114
The Construction Of A Chinese Shallow Treebank
Xu, Ruifeng;Lu, Qin;Li, Yin;Li, Wanyin;"></td>
	<td class="line x" title="1:271	The Construction of A Chinese Shallow Treebank Ruifeng Xu Dept. Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong csrfxu@comp.polyu.edu.hk Qin Lu Dept. Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong csluqin@comp.polyu.edu.hk Yin Li Dept. Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong csyinli@comp.polyu.edu.hk Wanyin Li Dept. Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong cswyli@comp.polyu.edu.hk Abstract This paper presents the construction of a manually annotated Chinese shallow Treebank, named PolyU Treebank." ></td>
	<td class="line x" title="2:271	Different from traditional Chinese Treebank based on full parsing, the PolyU Treebank is based on shallow parsing in which only partial syntactical structures are annotated." ></td>
	<td class="line x" title="3:271	This Treebank can be used to support shallow parser training, testing and other natural language applications." ></td>
	<td class="line x" title="4:271	Phrase-based Grammar, proposed by Peking University, is used to guide the design and implementation of the PolyU Treebank." ></td>
	<td class="line x" title="5:271	The design principles include good resource sharing, low structural complexity, sufficient syntactic information and large data scale." ></td>
	<td class="line x" title="6:271	The design issues, including corpus material preparation, standard for word segmentation and POS tagging, and the guideline for phrase bracketing and annotation, are presented in this paper." ></td>
	<td class="line x" title="7:271	Well-designed workflow and effective semiautomatic and automatic annotation checking are used to ensure annotation accuracy and consistency." ></td>
	<td class="line x" title="8:271	Currently, the PolyU Treebank has completed the annotation of a 1-million-word corpus." ></td>
	<td class="line x" title="9:271	The evaluation shows that the accuracy of annotation is higher than 98%." ></td>
	<td class="line x" title="10:271	1 Introduction A Treebank can be defined as a syntactically processed corpus." ></td>
	<td class="line x" title="11:271	It is a language resource containing annotations of information at various linguistic levels such as words, phrases, clauses and sentences to form a bank of linguistic trees." ></td>
	<td class="line x" title="12:271	There are many Treebanks built for different languages such as the Penn Treebank (Marcus 1993), ICE-GB (Wallis 2003), and so on." ></td>
	<td class="line x" title="13:271	The Penn Chinese Treebank is an important resource (Xia et al. 2000; Xue et al. 2002)." ></td>
	<td class="line x" title="14:271	Its annotation is based on Head-driven Phrase Structure Grammar (HPSG)." ></td>
	<td class="line x" title="15:271	The corpus of 100,000 Chinese words has been manually annotated with a strict quality assurance process." ></td>
	<td class="line x" title="16:271	Another important work is the Sinica Treebank at the Academic Sinica, Taiwan ( Chen et al. 1999; Chen et al. 2003)." ></td>
	<td class="line x" title="17:271	Information-based Case Grammar (ICG) was selected as the language framework." ></td>
	<td class="line x" title="18:271	A head-driven chart parser was performed to do phrase bracketing and annotating." ></td>
	<td class="line x" title="19:271	Then, manual post-editing was conducted." ></td>
	<td class="line x" title="20:271	According to the report, The Sinica Treebank contains 38,725 parsed trees with 329,532 words." ></td>
	<td class="line x" title="21:271	Most reported Chinese Treebanks, including the two above, are based on full parsing which requires complete syntactical analysis including determining syntactic categories of words, locating chunks that can be nested, finding relations between phrases and resolving the attachment ambiguities." ></td>
	<td class="line x" title="22:271	The output of full parsing is a set of complete syntactic trees." ></td>
	<td class="line x" title="23:271	Automatic full parsing, however, is difficult to achieve good performance." ></td>
	<td class="line x" title="24:271	Shallow parsing (or partial parsing) is usually defined as a parsing process aiming to provide a limited amount of local syntactic information such as non-recursive noun phrases, V-O structures and S-V structures etc. Since shallow parsing can recognize the backbone of a sentence more effectively and accurately with lower cost, people has in recent years started to work using results from shallow parsing." ></td>
	<td class="line x" title="25:271	A shallow parsed Treebank can be used to extract information for different applications especially for training shallow parsers." ></td>
	<td class="line x" title="26:271	Different from full parsing, annotation to a shallow Treebank is only targeted at certain local structures in a sentence." ></td>
	<td class="line x" title="27:271	The depth of shallowness and the scope of annotation vary from different reported work." ></td>
	<td class="line x" title="28:271	Thus, two issues in shallow Treebank annotation is (1) what information and (2) to what depths the syntactic information should be annotated." ></td>
	<td class="line x" title="29:271	Generally speaking, the degree of shallowness and the syntactical labeling are determined by the requirement of the serving applications." ></td>
	<td class="line x" title="30:271	The choice of full parsing or shallow parsing is dependent on the need of the application including resources and the capability of system to be developed (Xia et al. 2000; Chen et al. 2000; Li et al. 2003)." ></td>
	<td class="line x" title="31:271	Currently, there is no large-scale shallow annotated Treebank available as a publicly resource for training and testing." ></td>
	<td class="line x" title="32:271	In this paper, we present a manually annotated shallow Treebank, called the PolyU Treebank." ></td>
	<td class="line x" title="33:271	It is targeted to contain 1-million-word contemporary Chinese text." ></td>
	<td class="line x" title="34:271	The whole work on the PolyU Treebank follows the Phrase-based Grammar proposed by Peking University (Yu et al. 1998)." ></td>
	<td class="line x" title="35:271	In this language framework, a phrase, lead by a lexical word(or sometimes called a content word) as a head, is considered the basic syntactical unit in a Chinese sentence." ></td>
	<td class="line x" title="36:271	The building of the PolyU Treebank was originally designed as training data for a shallow parser used for Chinese collocation extraction." ></td>
	<td class="line x" title="37:271	From linguistics viewpoint, a collocation occurs only in words within a phrase, or between the headwords of related phrases (Zhang and Lin 1992)." ></td>
	<td class="line x" title="38:271	Therefore, the use of syntactic information is naturally considered an effective way to improve the performance of collocation extraction systems." ></td>
	<td class="line oc" title="39:271	The typical problems like doctor-nurse (Church and Hanks 1990) could be avoided by using such information." ></td>
	<td class="line x" title="40:271	When employing syntactical information in collocation extraction, we restrict ourselves to identify the stable phrases in the sentences with certain levels of nesting." ></td>
	<td class="line x" title="41:271	Thus it has motivated us to produce a shallow Treebank." ></td>
	<td class="line x" title="42:271	A natural way to obtain a shallow Treebank is through extracting shallow structures from a fully parsed Treebank." ></td>
	<td class="line x" title="43:271	Unfortunately, all the available fully parsed Treebank, such as the Penn Treebank and the Sinica Treebank, are annotated using different grammars than our chosen Phrase-based Grammar." ></td>
	<td class="line x" title="44:271	Also, the sizes of these Treebank are much smaller in scale to be useful for training our shallow parser." ></td>
	<td class="line x" title="45:271	This paper presents the most important design issues of the PolyU Treebank and the quality control mechanisms." ></td>
	<td class="line x" title="46:271	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="47:271	Section 2 introduces the overview and design principles." ></td>
	<td class="line x" title="48:271	Section 3 to Section5, present the design issues on corpus material preparation, the standard for word segmentation and POS tagging, and the guideline for phrase bracketing and labeling, respectively." ></td>
	<td class="line x" title="49:271	Section 6 discusses the quality assurance mechanisms including a carefully designed workflow, parallel annotation, and automatic and semi-automatic post-annotation checking." ></td>
	<td class="line x" title="50:271	Section 7 gives the current progress and future work." ></td>
	<td class="line x" title="51:271	2 Overview and Design Principles The objective of this project is to manually construct a large shallow Treebank with high accuracy and consistency." ></td>
	<td class="line x" title="52:271	The design principles of The PolyU Treebank are: high resource sharing ability, low structural complexity, sufficient syntactic information and large data scale." ></td>
	<td class="line x" title="53:271	First of all, the design and construction of The PolyU Treebank aims to provide as much a general purpose Treebank as possible so that different applications can make use of it as a NLP resource." ></td>
	<td class="line x" title="54:271	With this objective, we chose to follow the well-known Phrase-based Grammar as the framework for annotation as this grammar is widely accepted by Chinese language researchers, and thus our work can be easily understood and accepted." ></td>
	<td class="line x" title="55:271	Due to the lack of word delimitation in Chinese, word segmentation must be performed before any further syntactical annotation." ></td>
	<td class="line x" title="56:271	High accuracy of word segmentation is very important for this project." ></td>
	<td class="line x" title="57:271	In this project, we chose to use the segmented and tagged corpus of People Daily annotated by the Peking University." ></td>
	<td class="line x" title="58:271	The annotated corpus contains articles appeared in the People Daily Newspaper in 1998." ></td>
	<td class="line x" title="59:271	The segmentation is based on the guidelines, given in the Chinese national standard GB13715, (Liu et al. 1993) and the POS tagging specification was developed according to the Grammatical Knowledge-base of contemporary Chinese." ></td>
	<td class="line x" title="60:271	According to the report from Peking University, the accuracy of this annotated corpus in terms of segmentation and POS tagging are 99.9% and 99.5%, respectively (Yu et al. 2001)." ></td>
	<td class="line x" title="61:271	The use of such mature and widely adopted resource can effectively reduce our cost, ensure syntactical annotation quality." ></td>
	<td class="line x" title="62:271	With consistency in segmentation, POS, and syntactic annotation, the resulting Treebank can be readily shared by other researchers as a public resource." ></td>
	<td class="line x" title="63:271	The second design principle is low structural complexity." ></td>
	<td class="line x" title="64:271	That means, the annotation framework should be clear and simple, and the labeled syntactic and functional information should be commonly used and accepted." ></td>
	<td class="line x" title="65:271	Considering the characteristics of shallow annotation, our project has focused on the annotation of phrases and headwords while the sentence level syntax are ignored." ></td>
	<td class="line x" title="66:271	Following the framework of Phrase-based Grammar, a base-phrase is regarded as the smallest unit where a base-phrase is defined as a stable and simple phrase without nesting components." ></td>
	<td class="line x" title="67:271	Study on Chinese syntactical analysis suggests that phrases should be the fundamental unit instead of words in a sentence." ></td>
	<td class="line x" title="68:271	This is because, firstly, the usage of Chinese words is very flexible." ></td>
	<td class="line x" title="69:271	A word may have different POS tags serving for different functions in sentences." ></td>
	<td class="line x" title="70:271	On the contrary, the use of Chinese phrases is much more stable." ></td>
	<td class="line x" title="71:271	That is, a phrase has very limited functional use in a sentence." ></td>
	<td class="line x" title="72:271	Secondly, the construction rules of Chinese phrases are nearly the same as that of Chinese sentences." ></td>
	<td class="line x" title="73:271	Therefore, the analysis of phrases can help identifying POS and grammatical functions of words." ></td>
	<td class="line x" title="74:271	Naturally, it should be regarded as the basic syntactical unit." ></td>
	<td class="line x" title="75:271	Usually, a base-phrase is driven by a lexical word as its headword." ></td>
	<td class="line x" title="76:271	Examples of base-phrases include base NP, base VP and so on, such as the sample shown below." ></td>
	<td class="line x" title="77:271	Using base-phrases as the start point, nested levels of phrases are then identified, until the maximum phrases (will be defined later) are identified." ></td>
	<td class="line x" title="78:271	Since we do not intend to provide full parsing information, there has to be a limit on the level of nesting." ></td>
	<td class="line x" title="79:271	For practical reasons, we choose to limit the nesting of brackets to 3 levels." ></td>
	<td class="line x" title="80:271	That means, the depth of our shallow parsed Treebank will be limited to 3." ></td>
	<td class="line x" title="81:271	This restriction can limit the structural complexity to a manageable level." ></td>
	<td class="line x" title="82:271	Our nested bracketing is not strictly bottom up." ></td>
	<td class="line x" title="83:271	That is we do not simply extend from base-phrase and move up until the 3 rd level." ></td>
	<td class="line x" title="84:271	Instead, we first identify the maximal-phrase which is used to identify the backbone of the sentence." ></td>
	<td class="line x" title="85:271	The maximal-phrase provides the framework under which the base-phrases of up to 2 levels can be identified." ></td>
	<td class="line x" title="86:271	The principles for the identification of scope and depth of phrase bracketing are briefly explained below and the operating procedure is indicated by the given order in which these principles are presented." ></td>
	<td class="line x" title="87:271	More details is given in Section 5." ></td>
	<td class="line x" title="88:271	Step 1: Annotation of maximal-phrase which is the shortest word sequence of maximally spanning non-overlapping edges which plays a distinct semantic role of a predicate." ></td>
	<td class="line x" title="89:271	A maximal-phrase contains two or more lexical words." ></td>
	<td class="line x" title="90:271	Step 2: Annotation of base-phrases within a maximal-phrase." ></td>
	<td class="line x" title="91:271	In case a base-phrase and a maximal-phrase are identical and the maximal-phrase is already bracketed in Step 1, no bracketing is done in this step." ></td>
	<td class="line x" title="92:271	For each identified base-phrase, its headword will be marked." ></td>
	<td class="line x" title="93:271	Step 3: Annotation of next level of bracketing, called mid-phrase which is expended from a base-phrase." ></td>
	<td class="line x" title="94:271	A mid-phrase is annotated only if it is deemed necessary." ></td>
	<td class="line x" title="95:271	The process starts from the identified base-phrase." ></td>
	<td class="line x" title="96:271	One more level of syntactical structure is then bracketed if it exists within the maximal-phrase." ></td>
	<td class="line x" title="97:271	The third design principle is to provide sufficient syntactical information for natural language application even though shallow annotation does not necessarily contain complete syntactic information at sentence level." ></td>
	<td class="line x" title="98:271	Some past research in Chinese shallow parsing were on single level base-phrases only (Sun 2001)." ></td>
	<td class="line x" title="99:271	However, for certain applications, such as for collocation extraction, identification of base-phrases only are not very useful." ></td>
	<td class="line x" title="100:271	In this project, we have decided to annotate phrases within three levels of nesting within a sentence." ></td>
	<td class="line x" title="101:271	For each phrase, a label is be given to indicate its syntactical information, and an optional semantic or structural label is given if applicable." ></td>
	<td class="line x" title="102:271	Furthermore, the headword of a base-phrase is annotated." ></td>
	<td class="line x" title="103:271	We believe these information are sufficient for many natural language processing research work and it is also manageable for this project within its working schedule." ></td>
	<td class="line x" title="104:271	Fourthly, aiming to support practical language processing, a reasonably large annotated Treebank is expected." ></td>
	<td class="line x" title="105:271	Studies on English have shown that Treebank of word size 500K to 1M is reasonable for syntactical structure analysis (Leech and Garside 1996)." ></td>
	<td class="line x" title="106:271	In consideration of the resources available and the reference of studies on English, we have set out our Treebank size to be one million words." ></td>
	<td class="line x" title="107:271	We hope such a reasonably large-scale data can effectively support some language research, such as collocation extraction." ></td>
	<td class="line x" title="108:271	We chose to use the XML format to record the annotated data." ></td>
	<td class="line x" title="109:271	Other information such as original article related information (author, date, etc.), annotator name, and other useful information are also given through the meta-tags provided by XML." ></td>
	<td class="line x" title="110:271	All the meta-tags can be removed by a program to recover the original data." ></td>
	<td class="line x" title="111:271	We have performed a small-scale experiment to compare the annotation cost of shallow annotation and full annotation (followed Penn Chinese Treebank specification) on 500 Chinese sentences by the same annotators." ></td>
	<td class="line x" title="112:271	The time cost in shallow annotation is only 25% of that for full annotation." ></td>
	<td class="line x" title="113:271	Meanwhile, due to the reduced structural complexity in shallow annotation, the accuracy of first pass shallow annotation is much higher than full annotation." ></td>
	<td class="line x" title="114:271	3 Corpus Materials Preparation The People Daily corpus, developed by PKU, consists of more than 13k articles totaling 5M words." ></td>
	<td class="line x" title="115:271	As we need one million words for our Treebank, we have selected articles covering different areas in different time span to avoid duplications due to short-lived events and news topics." ></td>
	<td class="line x" title="116:271	Our selection takes each days news as one single unit, and then several distant dates are randomly selected among the whole 182 days in the entire collection." ></td>
	<td class="line x" title="117:271	We have also decided to keep the original articles structures and topics indicators as they may be useful for some applications." ></td>
	<td class="line x" title="118:271	4 Word Segmentation and Part-of-Speech Tagging The articles selected from PKU corpus are already segmented into words following the guidelines given in GB13715." ></td>
	<td class="line x" title="119:271	The annotated corpus has a basic lexicon of over 60,000 words." ></td>
	<td class="line x" title="120:271	We simply use this segmentation without any change and the accuracy is claimed to be 99.9%." ></td>
	<td class="line x" title="121:271	Each word in the PKU corpus is given a POS tag." ></td>
	<td class="line x" title="122:271	In this tagging scheme, a total of 43 POS tags are listed (Yu et al. 2001)." ></td>
	<td class="line x" title="123:271	Our project takes the PKU POS tags with only notational changes explained as follows: The morphemes tags including Ag (Adjectives morphemes), Bg, Dg, Ng, Mg, Rg, Tg, Qg, and Ug are re-labeled as lowercase letters, ag, bg, dg, ng, mg, rg, tg, qg and ug, respectively." ></td>
	<td class="line x" title="124:271	This modification is to ensure consistent labeling in our system where the lower cases are used to indicate word-level tags and upper cases are used to indicate phrase-level labels." ></td>
	<td class="line x" title="125:271	5 Phrase Bracketing and Annotation Phrase bracketing and annotation is the core part of this project." ></td>
	<td class="line x" title="126:271	Not only all the original annotated files are converted to XML files, results of our annotations are also given in XML form." ></td>
	<td class="line x" title="127:271	The meta tags provided by XML are very helpful for further processing and searching to the annotated text." ></td>
	<td class="line x" title="128:271	Note that in our project, the basic phrasal analysis looks at the context of a clause, not a sentence." ></td>
	<td class="line x" title="129:271	Here, the term clause refers the text string ended by some punctuations including comma (,), semicolon (;), colon (:), or period (.)." ></td>
	<td class="line x" title="130:271	Certain punctuation marks such as  , <, and > are not considered clause separators." ></td>
	<td class="line x" title="131:271	For example, is considered having two clauses and thus will be bracketed separately." ></td>
	<td class="line x" title="132:271	It should be pointed out that he set of Chinese punctuation marks are different from that of English and their usage can also be different." ></td>
	<td class="line x" title="133:271	Therefore, an English sentence and their Chinese translation may use different punctuation marks." ></td>
	<td class="line x" title="134:271	For example, the sentence is the translation of the English Tom, John, and Jack go back to school together, which uses   rather than comma(,) to indicate parallel structures, and is thus considered one clause." ></td>
	<td class="line x" title="135:271	Each clause will then be processed according to the principles discussed in Section 2." ></td>
	<td class="line x" title="136:271	The symbols [ and ] are used to indicate the left and right boundaries of a phrase." ></td>
	<td class="line x" title="137:271	The right bracket is appended with syntactic labels as described in the general form of [Phrase]SS-FF, where SS is a mandatory syntactic label such as NP(noun phrase) and AP(adjective phrase), and FF is an optional label indicating internal structures and semantic functions such as BL(parallel), SB(a noun is the object of verb within a verb phrase)." ></td>
	<td class="line x" title="138:271	A total of 21 SS labels and 20 FF labels are given in our phrase annotation specification." ></td>
	<td class="line x" title="139:271	For example, the functional label BL identifies parallel components in a phrase as indicated in the example." ></td>
	<td class="line x" title="140:271	As in another example shown below, the phrase is a verb phrase, thus it is labeled as VP." ></td>
	<td class="line x" title="141:271	Furthermore, the verb phrase can be further classified as a verb-complement type." ></td>
	<td class="line x" title="142:271	Thus an additional SBU function label is marked." ></td>
	<td class="line x" title="143:271	We should point out that since the FF labels are not syntactical information and are thus not expected to be used by any shallow parsers." ></td>
	<td class="line x" title="144:271	The FF labels carry structural and/or semantic information which are of help in annotation." ></td>
	<td class="line x" title="145:271	We consider it useful for other applications and thus decide to keep them in the Treebank." ></td>
	<td class="line x" title="146:271	Appendix 1 lists all the FF labels used in the annotation." ></td>
	<td class="line x" title="147:271	5.1 Identification of Maximal-phrase: The maximal-phrases are the main syntactical structures including subject, predicate, and objects in a clause." ></td>
	<td class="line x" title="148:271	Again, maximal-phrase is defined as the phrase with the maximum spanning non-overlapping length, and it is a predicate playing a distinct semantic role and containing more than one lexical word." ></td>
	<td class="line x" title="149:271	That means a maximal-phrase contains at least one base-phrase." ></td>
	<td class="line x" title="150:271	As this is the first stage in the bracketing process, no nesting should occur." ></td>
	<td class="line x" title="151:271	In the following annotated sentence, (Eg.1) there are two separate maximal-phrases,, and . Note that is considered a base-phrase, but not a maximal-phrase because it contains only one lexical word." ></td>
	<td class="line x" title="152:271	Unlike many annotations where the object of a sentence is included as a part of the verb phrase, we treat them as separate maximal-phrases both due to our requirement and also for reducing nesting." ></td>
	<td class="line x" title="153:271	If a clause is completely embedded in a larger clause, it is considered a special clause and given a special name called an internal clause . We will bracket such an internal clause as a maximal phrase with the tag IC as shown in the following example, 5.2 Annotation of Base-phrases: A base-phrase is the phrase with stable, close and simple structure without nesting components." ></td>
	<td class="line x" title="154:271	Normally a base-phrase contains a lexical word as headword." ></td>
	<td class="line x" title="155:271	Taking the maximal-phrase in Eg.1 as an example, and, are base-phrases in this maximal-phrase." ></td>
	<td class="line x" title="156:271	Thus, the sentence is annotated as In fact, and are also base-phrases." ></td>
	<td class="line x" title="157:271	is not bracketed because it is a single lexical word as a base-phrase without any ambiguity and it is thus by default not being bracketed." ></td>
	<td class="line x" title="158:271	is not further bracketed because it overlaps with a maximal-phrase." ></td>
	<td class="line x" title="159:271	Our annotation principle here is that if a base-phrase overlaps with a maximal-phrase, it will not be bracketed twice." ></td>
	<td class="line x" title="160:271	The identification of base-phrase is done only within an already identified maximal-phrase." ></td>
	<td class="line x" title="161:271	In other words, if a base-phrase is identified, it must be nested inside a maximal-phrase or at most overlaps with it." ></td>
	<td class="line x" title="162:271	It should be pointed out that the identification of a base-phrase is the most fundamental and most important goal of Treebank annotation." ></td>
	<td class="line x" title="163:271	The identification of maximal-phrases can be considered as parsing a clause using a top-down approach." ></td>
	<td class="line x" title="164:271	On the other hand, the identification of a base-phrase is a bottom up approach to find the most basic units within a maximal-phrase." ></td>
	<td class="line x" title="165:271	5.3 Mid-Phrase Identification: Due to the fact that sometimes there may be more syntactic structures between the base-phrases and maximal-phrases, this step uses base-phrase as the starting point to further identify one more level of the syntactical structure in a maximal-phrase." ></td>
	<td class="line x" title="166:271	Takes Eg.1 as an example, it is further annotated as where the underlined text shows the additional annotation." ></td>
	<td class="line x" title="167:271	As we only limit our nesting to three levels, any further nested phrases will be ignored." ></td>
	<td class="line x" title="168:271	The following sentence shows the result of our annotation with three levels of nesting: However, a full annotation should have 4 levels of nesting as shown below." ></td>
	<td class="line x" title="169:271	The underlined text is the 4 th level annotation skipped by our system." ></td>
	<td class="line x" title="170:271	5.4 Annotation of Headword In our system, a # tag will be appended after a word to indicate that it is a headword of the base-phrase." ></td>
	<td class="line x" title="171:271	Here, a headword must be a lexical word rather than a function word." ></td>
	<td class="line x" title="172:271	In most cases, a headword stays in a fixed position of a base-phrase." ></td>
	<td class="line x" title="173:271	For example, the headword of a noun phrase is normally the last noun in this phrase." ></td>
	<td class="line x" title="174:271	Thus, we call this position the default position." ></td>
	<td class="line x" title="175:271	If a headword is in the default position, annotation is not needed." ></td>
	<td class="line x" title="176:271	Otherwise, a # tag is used to indicate the headword." ></td>
	<td class="line x" title="177:271	For example, in a clause,, is a verb phrase, and the headword of the phrase is, which is not in the default position of a verb phrase." ></td>
	<td class="line x" title="178:271	Thus, this phrase is further annotated as: Note that is also a headword, but since it is in the default position, no explicit annotation is needed." ></td>
	<td class="line x" title="179:271	6 Annotation and Quality Assurance Our research team is formed by four people at the Hong Kong Polytechnic University, two linguists from Beijing Language and Culture University and some research collaborators from Peking University." ></td>
	<td class="line x" title="180:271	Furthermore, the annotation work has been conducted by four post-graduate students in language studies and computational linguistics from the Beijing Language and Culture University." ></td>
	<td class="line x" title="181:271	The annotation work is conducted in 5 separate stages to ensure quality output of the annotation work." ></td>
	<td class="line x" title="182:271	The preparation of annotation specification and corpus selection was done in the first stage." ></td>
	<td class="line x" title="183:271	Researchers in Hong Kong invited two linguists from China to come to Hong Kong to prepare for the corpus collection and selection work." ></td>
	<td class="line x" title="184:271	A thorough study on the reported work in this area was conducted." ></td>
	<td class="line x" title="185:271	After the project scope was defined, the SS labels and the FF labels were then defined." ></td>
	<td class="line x" title="186:271	A Treebank specification was then documented." ></td>
	<td class="line x" title="187:271	The Treebank was given the name PolyU Treebank to indicate that it is produced at the Hong Kong Polytechnic University." ></td>
	<td class="line x" title="188:271	In order to validate the specifications drafted, all the six members first manually annotated 10k-word material, separately." ></td>
	<td class="line x" title="189:271	The outputs were then compared, and the problems and ambiguities occurred were discussed and consolidated and named Version 1.0." ></td>
	<td class="line x" title="190:271	Stage 1 took about 5 months to complete." ></td>
	<td class="line x" title="191:271	Details of the specification can be downloaded from the project website www.comp.polyu.edu.hk/~cclab." ></td>
	<td class="line x" title="192:271	In Stage 2, the annotators in Beijing were then involved." ></td>
	<td class="line x" title="193:271	They had to first study the specification and understand the requirement of the annotation." ></td>
	<td class="line x" title="194:271	Then, the annotators under the supervision of a team member in Stage 1 annotated 20k-word materials together and discussed the problems occurred." ></td>
	<td class="line x" title="195:271	During this two-month work, the annotators were trained to understand the specification." ></td>
	<td class="line x" title="196:271	The emphasis at this stage was to train the annotators good understanding of the specification as well as consistency by each annotator and consistency by different annotators." ></td>
	<td class="line x" title="197:271	Further problems occurred in the actual annotation practice were then solved and the specification was also further refined or modified." ></td>
	<td class="line x" title="198:271	In Stage 3, which took about 2 months, each annotator was assigned 40k-word material each in which 5k-words material were duplicate annotated to all the annotators." ></td>
	<td class="line x" title="199:271	Meanwhile, the team members in Hong Kong also developed a post-annotation checking tool to verify the annotation format, phrase bracketing, annotation tags, and phrase marks to remove ambiguities and mistakes." ></td>
	<td class="line x" title="200:271	Furthermore, an evaluation tool was built to check the consistency of annotation output." ></td>
	<td class="line x" title="201:271	The detected annotation errors were then sent back to the annotators for discussion and correction." ></td>
	<td class="line x" title="202:271	Any further problems occurred were submitted for group discussion and minor modification on the specification was also done." ></td>
	<td class="line x" title="203:271	In stage 4, each annotator was dispatched with one set of 50k-word material each time." ></td>
	<td class="line x" title="204:271	For each distribution, 15k-word data in each set were distributed to more than two annotators in duplicates so that for any three annotators, there would be 5K duplicated materials." ></td>
	<td class="line x" title="205:271	When the annotators finished the first pass annotation, we used the post-annotation checking tool to do format checking in order to remove the obvious annotation errors such as wrong tag annotation and cross bracketing." ></td>
	<td class="line x" title="206:271	However, it was quite difficult to check the difference in annotation due to different interpretation of a sentence." ></td>
	<td class="line x" title="207:271	What we did was to make use of the annotations done on the duplicate materials to compare for consistency." ></td>
	<td class="line x" title="208:271	When ambiguity or differences were identified, discussions were conducted and a result used by the majority would be chosen as the accepted result." ></td>
	<td class="line x" title="209:271	The re-annotated results were regarded as the Golden Standard to evaluate the accuracy of annotation and consistency between different annotators." ></td>
	<td class="line x" title="210:271	The annotators were required to study this Golden Standard and go back to remove similar mistakes." ></td>
	<td class="line x" title="211:271	The annotated 50k data was accepted only after this." ></td>
	<td class="line x" title="212:271	Then, a new 50k-word materials was distributed and repeated in the same way." ></td>
	<td class="line x" title="213:271	During this stage, the ambiguous and out-of-tag-set phrase structures were marked as OT for further process." ></td>
	<td class="line x" title="214:271	The annotation specification was not modified in order to avoid frequent revisit to already annotated data." ></td>
	<td class="line x" title="215:271	About 4 months were spent on this stage." ></td>
	<td class="line x" title="216:271	In Stage 5, all the members and annotators were grouped and discuss the OT cases." ></td>
	<td class="line x" title="217:271	Some typical new phrase structure and function types were appended in the specification and thus the final formal annotation specification was established." ></td>
	<td class="line x" title="218:271	Using this final specification, the annotators had to go back to check their output, modify the mistakes and substitute the OT tags by the agreed tags." ></td>
	<td class="line x" title="219:271	Currently, the project was already in Stage 5 with 2 months of work finished." ></td>
	<td class="line x" title="220:271	A further 2 months was expected to complete this work." ></td>
	<td class="line x" title="221:271	Since it is impossible to do all the checking and analysis manually, a series of checking and evaluating tools are established." ></td>
	<td class="line x" title="222:271	One of the tools is to check the consistency between text corpus files and annotated XML files including checking the XML format, the filled XML header, and whether the original txt material is being altered by accident." ></td>
	<td class="line x" title="223:271	This program ensures that the XML header information is correctly filled and during annotation process, no additional mistakes are introduced due to typing errors." ></td>
	<td class="line x" title="224:271	Furthermore, we have developed and trained a shallow parser using the Golden Standard data." ></td>
	<td class="line x" title="225:271	This shallow parser is performed on the original text data, and its output and manually annotated result are compared for verification to further remove errors Now, we are in the process of developing an effective analyzer to evaluate the accuracy and consistency for the whole annotated corpus." ></td>
	<td class="line x" title="226:271	For the exactly matched bracketed phrases, we check whether the same phrase labels are given." ></td>
	<td class="line x" title="227:271	Abnormal cases will be manually checked and confirmed." ></td>
	<td class="line x" title="228:271	Our final goal is to ensure the bracketing can reach 99% accuracy and consistency." ></td>
	<td class="line x" title="229:271	7 Current Progress and Future Work As mentioned earlier, we are now in Stage 5 of the annotation." ></td>
	<td class="line x" title="230:271	The resulting annotation contains 2,639 articles selected from PKU People Daily corpus." ></td>
	<td class="line x" title="231:271	These articles contains 1, 035, 058 segmented Chinese words, with on average, around 394 words in each article." ></td>
	<td class="line x" title="232:271	There are a total of 284, 665 bracketed phrases including nested phrases." ></td>
	<td class="line x" title="233:271	A summary of the different SS labels used are given in Table 1." ></td>
	<td class="line x" title="234:271	Table 1." ></td>
	<td class="line x" title="235:271	Statistics of annotated syntactical phrases For each bracketed phrase, if its FF label does not fit into the corresponding default pattern, (like for the noun phrase(NP), the default grammatical structure is that the last noun in the phrase is the headword and other components are the modifiers, using PZ tags), its FF labels should then be explicitly labeled." ></td>
	<td class="line x" title="236:271	The statistics of annotated FF tags are listed in Table 2." ></td>
	<td class="line x" title="237:271	Table 2." ></td>
	<td class="line x" title="238:271	Statistics of function and structure tags For the material annotated by multiple annotators as duplicates, the evaluation program has reported that the accuracy of phrase annotation is higher than 99.5% and the consistency between different annotators is higher than 99.8%." ></td>
	<td class="line x" title="239:271	As for other annotated materials, the quality evaluation program preliminarily reports the accuracy of phrase annotation is higher than 98%." ></td>
	<td class="line x" title="240:271	Further checking and evaluation work are ongoing to ensure the final overall accuracy achieves 99%." ></td>
	<td class="line x" title="241:271	Up to now, the FF labels of 5,255 phrases are annotated as OT." ></td>
	<td class="line x" title="242:271	That means about 1.8% (5,255 out of a total of 284,665) of them do not fit into any patterns listed in Table 2." ></td>
	<td class="line x" title="243:271	Most of them are proper noun phrase, syntactically labeled as PP." ></td>
	<td class="line x" title="244:271	We are investigating these cases and trying to identify whether some of them can be in new function and structure patterns and give a new label." ></td>
	<td class="line x" title="245:271	It is also our intention to further develop our tools to improve the automatic annotation analysis and evaluation program to find out the potential annotation error and inconsistency." ></td>
	<td class="line x" title="246:271	Other visualization tools are also being developed to support keyword searching, context indexing, and annotation case searching." ></td>
	<td class="line x" title="247:271	Once we complete Stage 5, we intend to make the PolyU Treebank data available for public access." ></td>
	<td class="line x" title="248:271	Furthermore, we are developing a shallow parser and using The PolyU Treebank as training and testing data." ></td>
	<td class="line x" title="249:271	Acknowledgement This project is partially supported by the Hong Kong Polytechnic University (Project Code A-P203) and CERG Grant (Project code 5087/01E) References Baoli Li, Qin Lu and Yin Li." ></td>
	<td class="line x" title="250:271	2003." ></td>
	<td class="line x" title="251:271	Building a Chinese Shallow Parsed Treebank for Collocation Extraction, Proceedings of CICLing 2003: 402-405 Fei Xia, et al. 2000." ></td>
	<td class="line x" title="252:271	Developing Guidelines and Ensuring Consistency for Chinese Text Annotation Proceedings of LREC-2000, Greece Feng-yi Chen, et al. 1999." ></td>
	<td class="line x" title="253:271	Sinica Treebank, Computational Linguistics and Chinese Language Processing, 4(2):183-204 G. N. Leech, R.Garside." ></td>
	<td class="line x" title="254:271	1996." ></td>
	<td class="line x" title="255:271	Running a grammar factory: the production of syntactically analyzed corpora or treebanks, Johansson and Stenstron." ></td>
	<td class="line x" title="256:271	Honglin Sun, 2001." ></td>
	<td class="line x" title="257:271	A Content Chunk Parser for Unrestricted Chinese Text, Ph.D Thesis, Peking University, 2001 Keh-jiann Chen et al. 2003." ></td>
	<td class="line x" title="258:271	Building and Using Parsed Corpora (Anne Abeill ed." ></td>
	<td class="line x" title="259:271	s) KLUWER, Dordrecht Kenneth Church, and Patrick Hanks." ></td>
	<td class="line x" title="260:271	1990." ></td>
	<td class="line x" title="261:271	Word association norms, mutual information, and lexicography, Computational Linguistics, 16(1): 22-29 Marcus, M. et al. 1993." ></td>
	<td class="line x" title="262:271	Building a Large Annotated Corpus of English: The Penn Treebank, Computational Linguistics, 19(1): 313-330." ></td>
	<td class="line x" title="263:271	Nianwen Xue, et al. 2002." ></td>
	<td class="line x" title="264:271	Building a Large-Scale Annotated Chinese Corpus, Proceedings of COLING 2002, Taipei, Taiwan Sean Wallis, 2003." ></td>
	<td class="line x" title="265:271	Building and Using Parsed Corpora (Anne Abeill eds) KLUWER, Dordrecht Shiwen Yu, et al. 1998." ></td>
	<td class="line x" title="266:271	The Grammatical Knowledgebase of contemporary Chinese: a complete specification." ></td>
	<td class="line x" title="267:271	Tsinghua University Press, Beijing, China Shiwen Yu, et al. 2001." ></td>
	<td class="line x" title="268:271	Guideline of People Daily Corpus Annotation, Technical report, Beijing University Shoukang Zhang and Xingguang Lin, 1992." ></td>
	<td class="line x" title="269:271	Collocation Dictionary of Modern Chinese Lexical Words, Business Publisher, China Yuan Liu, et al. 1993." ></td>
	<td class="line x" title="270:271	Segmentation standard for Modern Chinese Information Processing and automatic segmentation methodology." ></td>
	<td class="line x" title="271:271	Tsinghua University Press, Beijing, China" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W04-2105
Word Lookup On The Basis Of Associations : From An Idea To A Roadmap
Zock, Michael;Bilac, Slaven;"></td>
	<td class="line x" title="1:158	Word lookup on the basis of associations: from an idea to a roadmap Michael ZOCK LIMSI-CNRS B.P. 133, 91403 Orsay, France zock@limsi.fr Slaven BILAC Tokyo Institute of Technology Ookayama 2-12-1, Meguro 152-8552, Japan sbilac@cl.cs.titech.ac.jp Abstract Word access is an obligatory step in language production." ></td>
	<td class="line x" title="2:158	In order to achieve his communicative goal, a speaker/writer needs not only to have something to say, he must also find the corresponding word(s)." ></td>
	<td class="line x" title="3:158	Yet, knowing a word, i.e. having it stored in a data-base or memory (human mind or electronic device) does not imply that one is able to access it in time." ></td>
	<td class="line x" title="4:158	This is a clearly a case where computers (electronic dictionaries) can be of great help." ></td>
	<td class="line x" title="5:158	In this paper we present our ideas of how an enhanced electronic dictionary can help people to find the word they are looking for." ></td>
	<td class="line x" title="6:158	The yet-to-be-built resource is based on the age-old notion of association: every idea, concept or word is connected." ></td>
	<td class="line x" title="7:158	In other words, we assume that people have a highly connected conceptuallexical network in their mind." ></td>
	<td class="line x" title="8:158	Finding a word amounts thus to entering the network at any point by giving the word or concept coming to their mind (source word) and then following the links (associations) leading to the word they are looking for(target word)." ></td>
	<td class="line x" title="9:158	Obviously, in order to allow for this kind of access, the resource has to be built accordingly." ></td>
	<td class="line x" title="10:158	This requires at least two things: (a) indexing words by the associations they evoke, (b) identification and labeling of the most frequent/useful associations." ></td>
	<td class="line x" title="11:158	This is precisely our goal." ></td>
	<td class="line x" title="12:158	Actually, we propose to build an associative network by enriching an existing electronic dictionary (essentially) with (syntagmatic) associations coming from a corpus, representing the average citizens shared, basic knowledge of the world (encyclopedia)." ></td>
	<td class="line x" title="13:158	Such an enhanced electronic database resembles in many respects our mental dictionary." ></td>
	<td class="line x" title="14:158	Combining the power of computers and the flexibility of the human mind (omnidirectional navigation and quick jumps), it emulates to some extent the latter in its capacity to navigate quickly and efficiently in a large data base." ></td>
	<td class="line x" title="15:158	While the notions of association and spreading activation are fairly old, their use to support word access via computer is new." ></td>
	<td class="line x" title="16:158	The resource still needs to be built, and this is not a trivial task." ></td>
	<td class="line x" title="17:158	We discuss here some of the strategies and problems involved in accomplishing it with the help of people and computers (automation)." ></td>
	<td class="line x" title="18:158	1 Introduction We all experience now and then the problem of being unable to find the word expressing the idea we have in our mind." ></td>
	<td class="line x" title="19:158	It we care and have time we may reach for a dictionary." ></td>
	<td class="line x" title="20:158	Yet, this kind of resource may be of little help, if it expects from us precisely what we are looking for : a perfectly spelled word, expressing the idea we try to convey." ></td>
	<td class="line x" title="21:158	While perfect input may be reasonable in the case of analysis (comprehension), it certainly is not in the case of synthesis (generation) where the starting point is conceptual in nature: a message, the (partial) definition of a word, a concept or a word related to the target word." ></td>
	<td class="line x" title="22:158	The language producer needs a dictionary allowing for reverse access." ></td>
	<td class="line x" title="23:158	A thesaurus does that, but only in a very limited way: the entry points are basically topical." ></td>
	<td class="line x" title="24:158	People use various methods to initiate search in their mind : words, concepts, partial descriptions, etc. If we want to mimic these functionalities by a computer, we must build the resource accordingly." ></td>
	<td class="line x" title="25:158	Let us assume that the text producer is looking for a word that he cannot access." ></td>
	<td class="line x" title="26:158	Instead he comes up with another word (or concept)1 somehow related to the former." ></td>
	<td class="line x" title="27:158	He may not know precisely how the two relate, but he knows that they are related." ></td>
	<td class="line x" title="28:158	He may also know to some extent how close their relationship is, whether a given link is relevant or not, that is, whether it can lead directly (synonym, 1We will comment below on the difference between concepts and words." ></td>
	<td class="line x" title="29:158	antonym, hyperonym) or indirectly to the target word." ></td>
	<td class="line x" title="30:158	Since the relationship between the sourceand the target word is often indirect, several lookups may be necessary: each one of them having the potential to contain either the target word (direct lookup), or a word leading towards it (indirect lookup)." ></td>
	<td class="line x" title="31:158	2 How reasonable is it to expect perfect input?" ></td>
	<td class="line x" title="32:158	The expectation of perfect input is unrealistic even in analysis,2 but clearly more so in generation." ></td>
	<td class="line x" title="33:158	The user may well be unable to provide the required information: be it because he cannot access in time the word he is looking for, even though he knows it,3 or because he does not know the word yet expressing the idea he wants to convey." ></td>
	<td class="line x" title="34:158	This latter case typically occurs when using a foreign language or when trying to use a very technical term." ></td>
	<td class="line x" title="35:158	Yet, not being able to find a word, does not imply that one does not know anything concerning the word." ></td>
	<td class="line x" title="36:158	Actually, quite often the contrary is the case." ></td>
	<td class="line x" title="37:158	Suppose, you were looking for a word expressing the following ideas: domesticated animal, producing milk suitable for making cheese." ></td>
	<td class="line x" title="38:158	Suppose further that you knew that the target word was neither cow nor sheep." ></td>
	<td class="line x" title="39:158	While none of this information is sufficient to guarantee the access of the intended word goat, the information at hand (part of the definition) could certainly be used." ></td>
	<td class="line x" title="40:158	For some concrete proposals going in this direction, see (Bilac et al. , 2004), or the OneLook reverse dictionary.4 Besides the definition information, people often have other kind of knowledge concerning the target word." ></td>
	<td class="line x" title="41:158	In particular, they know how the latter relates to other words." ></td>
	<td class="line x" title="42:158	For example, they know that goats and sheep are somehow connected, that both of them are animals, that sheep are appreciated for their wool and meet, that sheep tend to follow each other blindly, while goats manage to survive, while hardly eating anything, etc. In sum, people have in their mind lexical networks: all words, concepts or ideas they express are highly interconnected." ></td>
	<td class="line x" title="43:158	As a result, any one of the words or concepts has the potential to evoke each other." ></td>
	<td class="line x" title="44:158	The likelihood for 2Obviously, looking for pseudonym under the letter S in a dictionary wont be of great help." ></td>
	<td class="line x" title="45:158	3Temporary amnesia, known as the TOT, or tip-ofthe-tongue problem (Brown and McNeill, 1996; Zock and Fournier, 2001; Zock, 2002) 4http://www.onelook.com/reverse-dictionary." ></td>
	<td class="line x" title="46:158	shtml this to happen depends, among other things, on such factors as frequency (associative strength), saliency and distance (direct vs. indirect access)." ></td>
	<td class="line x" title="47:158	As one can see, associations are a very general and powerful mechanism." ></td>
	<td class="line x" title="48:158	No matter what we hear, read or say, any idea is likely to remind us of something else.5 This being so, we should make use of it.6 3 Search based on the relations between concepts and words If one agrees with what we have just said, one could view the mental dictionary as a huge semantic network composed of nodes (words and concepts) and links (associations), with either being able to activate the other.7 Finding a 5The idea according to which the mental dictionary (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new, neither is the idea of spreading activation." ></td>
	<td class="line x" title="49:158	Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James & Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989)." ></td>
	<td class="line x" title="50:158	For surveys in psycholinguistics see (Hormann, 1972), or more recent work (Spitzer, 1999)." ></td>
	<td class="line x" title="51:158	The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al. , 1999) and, of course, in WordNet (Miller et al. , 1993; Fellbaum, 1998)." ></td>
	<td class="line x" title="52:158	6In the preceding sections we used several times the terms words and concepts interchangeably, as if they were the same." ></td>
	<td class="line x" title="53:158	Of course, they are very different." ></td>
	<td class="line x" title="54:158	Yet, not knowing what a concept looks like (a single node, or every node, i.e. headword of the words definition?), we think it is safer to assume that the user can communicate with the computer (dictionary) only via words." ></td>
	<td class="line x" title="55:158	Hence, concepts are represented by words, yet, since the two are connected, one can be accessed via the other, which addresses the interface problem with the computer." ></td>
	<td class="line x" title="56:158	Another point worth mentionning is the fact that associations may depend on the nature of the arguments (words vs. concepts)." ></td>
	<td class="line x" title="57:158	While in theory anything can be associated with anything (words with words, words with concepts, concepts with concepts, etc.), in practice words tend to trigger a different set of associations than concepts." ></td>
	<td class="line x" title="58:158	Also, the connectivity between words and concepts explains to some extent the power and the flexibility of the human mind." ></td>
	<td class="line x" title="59:158	Words are shorthand labels for concepts, and given the fact that the two are linked, one can make big leaps in no time and easily move from one plane (lets say the conceptual level) to the other (the linguistic counterpart)." ></td>
	<td class="line x" title="60:158	Words can be reached via concepts, but the latter can also serve as starting point to find a word." ></td>
	<td class="line x" title="61:158	Compared to the links between concepts which are a superhighway, associations between words are more like countryroads." ></td>
	<td class="line x" title="62:158	7Actually, one could question the very notion of mental dictionary which is convenient, but misleading in as it supposes a dedicated part for this task in our brain." ></td>
	<td class="line x" title="63:158	A Figure 1: Search based on propagation in a network (internal representation) word amounts thus to entering the network and following the links leading from the source node (the first word that comes to your mind) to the target word (the one you are looking for)." ></td>
	<td class="line x" title="64:158	Suppose you wanted to find the word nurse (target word), yet the only token coming to your mind were hospital." ></td>
	<td class="line x" title="65:158	In this case the system would generate internally a graph with the source word at the center and all the associated words at the periphery." ></td>
	<td class="line x" title="66:158	Put differently, the system would build internally a semantic network with hospital in the center and all its associated words as satellites (figure 1).8 Obviously, the greater the number of associations, the more complex the graph." ></td>
	<td class="line x" title="67:158	Given the diversity of situations in which a given object may occur we are likely to build many associations." ></td>
	<td class="line x" title="68:158	In other words, lexical graphs tend to bemultiply indexed mental encyclopedia, composed of polymorph information (concepts, words, meta-linguistic information) seems much more plausible to us." ></td>
	<td class="line x" title="69:158	8AKO: a kind of; ISA: subtype; TIORA: typically involved object, relation or actor." ></td>
	<td class="line x" title="70:158	come complex, too complex to be a good representation to support navigation." ></td>
	<td class="line x" title="71:158	Readability is hampered by at least two factors: high connectivity (the great number of links or associations emanating from each word), and distribution: conceptually related nodes, that is, nodes activated by the same kind of assocation are scattered around, that is, they do not necessarily occur next to each other, which is quite confusing for the user." ></td>
	<td class="line x" title="72:158	In order to solve this problem we suggest to display by category (chunks) all the words linked by the same kind of association to the source word (see figure 2)." ></td>
	<td class="line x" title="73:158	Hence, rather than displaying all the connected words as a flat list, we suggest to present them in chunks to allow for categorial search." ></td>
	<td class="line x" title="74:158	Having chosen a category, the user will be presented a list of words or categories from which he must choose." ></td>
	<td class="line x" title="75:158	If the target word is in the category chosen by the user (suppose he looked for a hyperonyme, hence he checked the ISA-bag), search stops, otherwise it goes on." ></td>
	<td class="line x" title="76:158	The user could choose either another category (eg." ></td>
	<td class="line x" title="77:158	AKO or TIORA), or a word in Figure 2: Proposed candidates, grouped according to the nature of the link the current list, which would then become the new starting point." ></td>
	<td class="line x" title="78:158	4 A resource still to be built The fact that the links are labeled has some very important consequences." ></td>
	<td class="line x" title="79:158	(a) While maintaining the power of a highly connected graph (possible cyclic navigation), it has at the interface level the simplicity of a tree: each node points only to data of the same type, i.e. same kind of association." ></td>
	<td class="line x" title="80:158	(b) Words being presented in clusters, navigation can be accomplished by clicking on the appropriate category." ></td>
	<td class="line x" title="81:158	The assumption being that the user generally knows to which category the target word belongs (or at least, he can recognize within which of the listed categories it falls), and that categorical search is in principle faster than search in a huge list of unordered (or, alphabetically ordered) words." ></td>
	<td class="line x" title="82:158	Word access, as described here, amounts to navigating in a huge associative network." ></td>
	<td class="line x" title="83:158	Of course, such a network has to be built." ></td>
	<td class="line x" title="84:158	The question is how." ></td>
	<td class="line x" title="85:158	Our proposal is to build it automatically by parsing an existing corpus containing sufficient amount of information on world knowledge (for example, an encyclopedia)." ></td>
	<td class="line x" title="86:158	This would yield a set of associations (see below),9 which still need to be labeled." ></td>
	<td class="line x" title="87:158	A rich ontology should be helpful in determining the adequate label for many, if not most of the links." ></td>
	<td class="line x" title="88:158	Unlike private information,10 which by 9The assumption being that every word co-occurring with another word in the same sentence is a candidate of an association." ></td>
	<td class="line x" title="89:158	The more frequently two words co-occur in a given corpus, the greater their associative strength." ></td>
	<td class="line x" title="90:158	10For example, the word elephant may remind you of a definition cannot and should not be put into a public dictionary,11 encyclopedic knowledge can be added in terms of associations, as this information expresses commonly shared knowledge, that is, the kind of associations most people have when encountering a given word." ></td>
	<td class="line x" title="91:158	Take for example the word elephant." ></td>
	<td class="line x" title="92:158	An electronic dictionary like Word Net associates the following gloss with the headword: large, gray, four-legged mammal, while Webster gives the following information: A mammal of the order Proboscidia, of which two living species, Elephas Indicus and E. Africanus, and several fossil species, are known." ></td>
	<td class="line x" title="93:158	They have a proboscis or trunk, and two large ivory tusks proceeding from the extremity of the upper jaw, and curving upwards." ></td>
	<td class="line x" title="94:158	The molar teeth are large and have transverse folds." ></td>
	<td class="line x" title="95:158	Elephants are the largest land animals now existing." ></td>
	<td class="line x" title="96:158	While this latter entry is already quite rich (trunk, ivory tusk, size), an encyclopedia contains even more information.12 If all this information were added to an electronic resource, it would enable us to access the same word (e.g. elephant) via many more associations than ever before." ></td>
	<td class="line x" title="97:158	By looking at the definition here above, one will notice that many associations are quite straightforward (color, size, origin, etc.), and since most of them appear frequently in a pattern-like manner it should be possible to extract them automatically (see footnote 18 below)." ></td>
	<td class="line x" title="98:158	If one agrees with these views, the remaining question is how to extract this encyclopedic information and to add it to an existing electronic resource." ></td>
	<td class="line x" title="99:158	Below we will outline some methods for extracting associated words and discuss the feasibility of using current methodology to achieve this goal." ></td>
	<td class="line x" title="100:158	5 Automatic extraction of word associations Above we outlined the need for obtaining associations between words and using them to improve dictionary accessibility." ></td>
	<td class="line x" title="101:158	While the associations can be obtained through association experiments with human subjects, this strategy is specific animal, trip or location (zoo, country in Africa)." ></td>
	<td class="line x" title="102:158	11This does not (and should not) preclude the possibility to add it to ones personal dictionary." ></td>
	<td class="line x" title="103:158	12You may consider taking a look at Wikipedia (http: //en.wikipedia.org/wiki/) which is free." ></td>
	<td class="line x" title="104:158	not very satisfying due to the high cost of running the experiments (time and money), and due to its static nature." ></td>
	<td class="line x" title="105:158	Indeed, given the costs, it is impossible to repeat these experiments to take into account the evolution of a society." ></td>
	<td class="line x" title="106:158	Hence, the goal is to automatically extract associations from large corpora." ></td>
	<td class="line x" title="107:158	This problem was addressed by a large number of researchers, but in most cases it was reduced to extraction of collocations which are a proper subset of the set of associated words." ></td>
	<td class="line x" title="108:158	While hard to define, collocations appear often enough in corpora to be extractable by statistical and information-theory based methods." ></td>
	<td class="line oc" title="109:158	There are several basic methods for evaluating associations between words: based on frequency counts (Choueka, 1988; Wettler and Rapp, 1993), information theoretic (Church and Hanks, 1990) and statistical significance (Smadja, 1993)." ></td>
	<td class="line x" title="110:158	The statistical significance often evaluate whether two words are independant using hypothesis tests such as t-score (Church et al. , 1991), the X2, the log-likelihood (Dunning, 1993) and Fishers exact test (Pedersen, 1996)." ></td>
	<td class="line x" title="111:158	Extracted sets for associated words are further pruned using numerical methods, or linguistic knowledge to obtain a subset of collocations." ></td>
	<td class="line x" title="112:158	The various extraction measures have been discussed in great detail in the literature (Manning and Schutze, 1999; McKeown and Radev, 2000), their performance has been compared (Dunning, 1993; Pedersen, 1996; Evert and Krenn, 2001), and the methods have been combined to improve overall performance (Inkpen and Hirst, 2002)." ></td>
	<td class="line x" title="113:158	Most of these methods were originally applied in large text corpora, but more recently the web has been used as a corpus (Pearce, 2001; Inkpen and Hirst, 2002)." ></td>
	<td class="line x" title="114:158	Collocation extraction methods have been used not only for English, but for many other languages: French (Ferret, 2002), German (Evert and Krenn, 2001) and Japanese (Nagao and Mori, 1994), to cite but those." ></td>
	<td class="line x" title="115:158	The most obvious question in this context is to clarify to what extent available collocation extraction techniques fulfill our needs of extracting and labeling word associations." ></td>
	<td class="line x" title="116:158	Since collocations are a subset of association, it is possible to apply collocation extraction techniques to obtain related words, ordered in terms of the relative strength of association." ></td>
	<td class="line x" title="117:158	The result of this kind of numerical extraction would be a large set of numerically weighted word pairs." ></td>
	<td class="line x" title="118:158	The problem with this approach is that the links are only labeled in terms of their relative associative strength, but not categorically, which makes it impossible to group and present them in a meaningful way for the dictionary user." ></td>
	<td class="line x" title="119:158	Clusters based only on the notion of association strength are inadequate for the kind of navigation described here above." ></td>
	<td class="line x" title="120:158	Hence another step is necessary: qualification of the links according to their types." ></td>
	<td class="line x" title="121:158	Only once this is done, a human being could use it to navigate through a large conceptual-lexical network (the dictionary) as described above." ></td>
	<td class="line x" title="122:158	Unfortunately, research on automatic link identification has been rather sparse." ></td>
	<td class="line x" title="123:158	Most attempts have been devoted to the extraction of certain types of links (usually syntactic type (Lin, 1998) or on extensions of WordNet with topical information contained in a thesaurus (Stevenson, 2002) or on the WWW (Agirre et al. , 2000)." ></td>
	<td class="line x" title="124:158	Additional methods need to be considered in order to reveal (automatically) the kind of associations holding between words and/or concepts." ></td>
	<td class="line x" title="125:158	Earlier in this paper we have suggested the use of an encyclopedia as a source of general world knowledge." ></td>
	<td class="line x" title="126:158	It should be noted, though, that there are important differences between large corpora and encyclopedias." ></td>
	<td class="line x" title="127:158	Large corpora usually contain a lot of repetitive texts on a limited number of topics (e.g. newspaper articles) which makes them very suitable for statistical methods." ></td>
	<td class="line x" title="128:158	On the other hand, while being maximally informative and comprehensive, encyclopedias are written in a highly controlled language, and their content is continually updated and re-edited, with the goal to avoid unnecessary repetition." ></td>
	<td class="line x" title="129:158	While most of the information contained in an entry is important, there is a lack of redundancy." ></td>
	<td class="line x" title="130:158	Hence, measures capable of handling word pairs with low appearance counts (e.g. log-likelihood or Fishers exact test) should be favored." ></td>
	<td class="line x" title="131:158	Also, rather than looking at individual words, one might want to look at word patterns instead." ></td>
	<td class="line x" title="132:158	6 Discussion and Conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access." ></td>
	<td class="line x" title="133:158	We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader)." ></td>
	<td class="line x" title="134:158	It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it." ></td>
	<td class="line x" title="135:158	To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation." ></td>
	<td class="line x" title="136:158	Actually our basic proposal is to extend a resource like WordNet by adding certain links, in particular on the horizontal axis (syntagmatic relations)." ></td>
	<td class="line x" title="137:158	These links are associations, and their role consists in helping the encoder to find ideas (concepts/words) related to a given stimulus (brainstorming), or to find the word he is thinking of (word access)." ></td>
	<td class="line x" title="138:158	One problem that we are confronted with is to identify possible associations." ></td>
	<td class="line x" title="139:158	Ideally we would need a complete list, but unfortunately, this does not exist." ></td>
	<td class="line x" title="140:158	Yet, there is a lot of highly relevant information out there." ></td>
	<td class="line x" title="141:158	For example, Melcuks lexical functions (Melcuk, 1992), Fillmores FRAMENET13, work on ontologies (CYC), thesaurus (Roget), WordNets (the original version from Princeton, divers EuroWordNets, BalkaNet), HowNet14, the work done by MICRA, the FACTOTUM project15 or the Wordsmyth dictionary/thesaurus combination16." ></td>
	<td class="line x" title="142:158	Of course, one would need to make choices here and probably add links." ></td>
	<td class="line x" title="143:158	Another problem is to identify useful associations." ></td>
	<td class="line x" title="144:158	Not every possible association is necessarily plausible." ></td>
	<td class="line x" title="145:158	Hence, the idea to take as corpus something that expresses shared knowledge, for example, an encyclopedia." ></td>
	<td class="line x" title="146:158	The associations it contains can be considered as being plausible." ></td>
	<td class="line x" title="147:158	We could also collect data by watching people using a dictionary and identify search patterns.17 Next, we could run psycholinguistic experiments.18 While the typical paradigm has been to ask people to produce a response (red) to some stimulus (rose), we could ask them to identify or label the links between words (e.g. apple-fruit, lemon-yellow, etc.)." ></td>
	<td class="line x" title="148:158	The ease of la13http://www.icsi.berkeley.edu/~framenet/ 14http://www.keenage.com/html/e_index.html 15http://humanities.uchicago.edu/homes/MICRA/ 16http://www.wordsmyth.com/ 17One such pattern could be: give me the word for a bird with yellow feet and a long beak, that can swim." ></td>
	<td class="line x" title="149:158	Actually, word access problems frequently come under the form of questions like: What is the word for X that Y?, where X is usually a hypernym and Y a stereotypical, possibly partial functional/relational/case description of the target word." ></td>
	<td class="line x" title="150:158	18Actually, this has been done for decades, but with a different goal in mind (Nelson, 1967), http://cyber." ></td>
	<td class="line x" title="151:158	acomp.usf.edu/FreeAssociation/." ></td>
	<td class="line x" title="152:158	beling will probably depend upon the origin of the words (the person asked to label the link or somebody else)." ></td>
	<td class="line x" title="153:158	Another approach would be to extract collocations from a corpus and label them automatically." ></td>
	<td class="line x" title="154:158	There are tools for extracting cooccurrences (see section 5.5), and ontologies could be used to qualify some of the links between collocational elements." ></td>
	<td class="line x" title="155:158	While this approach might work fine for couples like coffeestrong, or wine-red (since an ontology would reveal that red is a kind of color, which is precisely the link type: i.e. association), one may doubt that it could reveal the nature of the link between smoke and fire." ></td>
	<td class="line x" title="156:158	Yet, most humans would immediately recognize this as a causal link." ></td>
	<td class="line x" title="157:158	As one can see, there are still quite a few serious problems to be solved." ></td>
	<td class="line x" title="158:158	Nevertheless, we do believe that these obstacles can be removed, and that the approach presented here has the potential to improve word access, making the whole process more powerful, natural and intuitive, hence efficient." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1014
The Distributional Inclusion Hypotheses And Lexical Entailment
Geffet, Maayan;Dagan, Ido;"></td>
	<td class="line x" title="1:199	Proceedings of the 43rd Annual Meeting of the ACL, pages 107114, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:199	c2005 Association for Computational Linguistics The Distributional Inclusion Hypotheses and Lexical Entailment Maayan Geffet School of Computer Science and Engineering Hebrew University, Jerusalem, Israel, 91904 mary@cs.huji.ac.il Ido Dagan Department of Computer Science Bar-Ilan University, Ramat-Gan, Israel, 52900 dagan@cs.biu.ac.il Abstract This paper suggests refinements for the Distributional Similarity Hypothesis." ></td>
	<td class="line x" title="3:199	Our proposed hypotheses relate the distributional behavior of pairs of words to lexical entailment  a tighter notion of semantic similarity that is required by many NLP applications." ></td>
	<td class="line x" title="4:199	To automatically explore the validity of the defined hypotheses we developed an inclusion testing algorithm for characteristic features of two words, which incorporates corpus and web-based feature sampling to overcome data sparseness." ></td>
	<td class="line x" title="5:199	The degree of hypotheses validity was then empirically tested and manually analyzed with respect to the word sense level." ></td>
	<td class="line x" title="6:199	In addition, the above testing algorithm was exploited to improve lexical entailment acquisition." ></td>
	<td class="line x" title="7:199	1 Introduction Distributional Similarity between words has been an active research area for more than a decade." ></td>
	<td class="line x" title="8:199	It is based on the general idea of Harris' Distributional Hypothesis, suggesting that words that occur within similar contexts are semantically similar (Harris, 1968)." ></td>
	<td class="line oc" title="9:199	Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words (Church and Hanks, 1990; Ruge, 1992; Pereira et al. , 1993; Grefenstette, 1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002; Weeds and Weir, 2003)." ></td>
	<td class="line x" title="10:199	As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1)." ></td>
	<td class="line x" title="11:199	It does not ensure that the meaning of one word is preserved when replacing it with the other one in some context." ></td>
	<td class="line x" title="12:199	However, many semantic information-oriented applications like Question Answering, Information Extraction and Paraphrase Acquisition require a tighter similarity criterion, as was also demonstrated by papers at the recent PASCAL Challenge on Recognizing Textual Entailment (Dagan et al. , 2005)." ></td>
	<td class="line x" title="13:199	In particular, all these applications need to know when the meaning of one word can be inferred (entailed) from another word, so that one word could substitute the other in some contexts." ></td>
	<td class="line x" title="14:199	This relation corresponds to several lexical semantic relations, such as synonymy, hyponymy and some cases of meronymy." ></td>
	<td class="line x" title="15:199	For example, in Question Answering, the word company in a question can be substituted in the text by firm (synonym), automaker (hyponym) or division (meronym)." ></td>
	<td class="line x" title="16:199	Unfortunately, existing manually constructed resources of lexical semantic relations, such as WordNet, are not exhaustive and comprehensive enough for a variety of domains and thus are not sufficient as a sole resource for application needs1." ></td>
	<td class="line x" title="17:199	Most works that attempt to learn such concrete lexical semantic relations employ a co-occurrence pattern-based approach (Hearst, 1992; Ravichandran and Hovy, 2002; Moldovan et al. , 2004)." ></td>
	<td class="line x" title="18:199	Typically, they use a set of predefined lexicosyntactic patterns that characterize specific semantic relations." ></td>
	<td class="line x" title="19:199	If a candidate word pair (like company-automaker) co-occurs within the same sentence satisfying a concrete pattern (like ' companies, such as automakers'), then it is expected that the corresponding semantic relation holds between these words (hypernym-hyponym in this example)." ></td>
	<td class="line x" title="20:199	In recent work (Geffet and Dagan, 2004) we explored the correspondence between the distributional characterization of two words (which may hardly co-occur, as is usually the case for syno1We found that less than 20% of the lexical entailment relations extracted by our method appeared as direct or indirect WordNet relations (synonyms, hyponyms or meronyms)." ></td>
	<td class="line x" title="21:199	107 nyms) and the kind of tight semantic relationship that might hold between them." ></td>
	<td class="line x" title="22:199	We formulated a lexical entailment relation that corresponds to the above mentioned substitutability criterion, and is termed meaning entailing substitutability (which we term here for brevity as lexical entailment)." ></td>
	<td class="line x" title="23:199	Given a pair of words, this relation holds if there are some contexts in which one of the words can be substituted by the other, such that the meaning of the original word can be inferred from the new one." ></td>
	<td class="line x" title="24:199	We then proposed a new feature weighting function (RFF) that yields more accurate distributional similarity lists, which better approximate the lexical entailment relation." ></td>
	<td class="line x" title="25:199	Yet, this method still applies a standard measure for distributional vector similarity (over vectors with the improved feature weights), and thus produces many loose similarities that do not correspond to entailment." ></td>
	<td class="line x" title="26:199	This paper explores more deeply the relationship between distributional characterization of words and lexical entailment, proposing two new hypotheses as a refinement of the distributional similarity hypothesis." ></td>
	<td class="line x" title="27:199	The main idea is that if one word entails the other then we would expect that virtually all the characteristic context features of the entailing word will actually occur also with the entailed word." ></td>
	<td class="line x" title="28:199	To test this idea we developed an automatic method for testing feature inclusion between a pair of words." ></td>
	<td class="line x" title="29:199	This algorithm combines corpus statistics with a web-based feature sampling technique." ></td>
	<td class="line x" title="30:199	The web is utilized to overcome the data sparseness problem, so that features which are not found with one of the two words can be considered as truly distinguishing evidence." ></td>
	<td class="line x" title="31:199	Using the above algorithm we first tested the empirical validity of the hypotheses." ></td>
	<td class="line x" title="32:199	Then, we demonstrated how the hypotheses can be leveraged in practice to improve the precision of automatic acquisition of the entailment relation." ></td>
	<td class="line x" title="33:199	2 Background 2.1 Implementations of Distributional Similarity This subsection reviews the relevant details of earlier methods that were utilized within this paper." ></td>
	<td class="line x" title="34:199	In the computational setting contexts of words are represented by feature vectors." ></td>
	<td class="line x" title="35:199	Each word w is represented by a feature vector, where an entry in the vector corresponds to a feature f. Each feature represents another word (or term) with which w cooccurs, and possibly specifies also the syntactic relation between the two words as in (Grefenstette, 1994; Lin, 1998; Weeds and Weir, 2003)." ></td>
	<td class="line x" title="36:199	Pado and Lapata (2003) demonstrated that using syntactic dependency-based vector space models can help distinguish among classes of different lexical relations, which seems to be more difficult for traditional bag of words co-occurrence-based models." ></td>
	<td class="line x" title="37:199	A syntactic feature is defined as a triple <term, syntactic_relation, relation_direction> (the direction is set to 1, if the feature is the words modifier and to 0 otherwise)." ></td>
	<td class="line x" title="38:199	For example, given the word company the feature <earnings_report, gen, 0> (genitive) corresponds to the phrase companys earnings report, and <profit, pcomp, 0> (prepositional complement) corresponds to the profit of the company." ></td>
	<td class="line x" title="39:199	Throughout this paper we used syntactic features generated by the Minipar dependency parser (Lin, 1993)." ></td>
	<td class="line x" title="40:199	The value of each entry in the feature vector is determined by some weight function weight(w,f), which quantifies the degree of statistical association between the feature and the corresponding word." ></td>
	<td class="line oc" title="41:199	The most widely used association weight function is (point-wise) Mutual Information (MI) (Church and Hanks, 1990; Lin, 1998; Dagan, 2000; Weeds et al. , 2004)." ></td>
	<td class="line x" title="42:199	<=> element, component <=> gap, spread * town, airport <= loan, mortgage => government, body * warplane, bomb <=> program, plan * tank, warplane * match, winner => bill, program <= conflict, war => town, location Table 1: Sample of the data set of top-40 distributionally similar word pairs produced by the RFFbased method of (Geffet and Dagan, 2004)." ></td>
	<td class="line x" title="43:199	Entailment judgments are marked by the arrow direction, with '*' denoting no entailment." ></td>
	<td class="line x" title="44:199	108 Once feature vectors have been constructed, the similarity between two words is defined by some vector similarity metric." ></td>
	<td class="line x" title="45:199	Different metrics have been used, such as weighted Jaccard (Grefenstette, 1994; Dagan, 2000), cosine (Ruge, 1992), various information theoretic measures (Lee, 1997), and the widely cited and competitive (see (Weeds and Weir, 2003)) measure of Lin (1998) for similarity between two words, w and v, defined as follows:, ),(),( ),(),( ),( )()( )()(" ></td>
	<td class="line x" title="46:199	  + + = fvweightfwweight fvweightfwweight vwsim vFfwFf vFwFf Lin where F(w) and F(v) are the active features of the two words (positive feature weight) and the weight function is defined as MI." ></td>
	<td class="line x" title="47:199	As typical for vector similarity measures, it assigns high similarity scores if many of the two words features overlap, even though some prominent features might be disjoint." ></td>
	<td class="line x" title="48:199	This is a major reason for getting such semantically loose similarities, like company government and country economy." ></td>
	<td class="line x" title="49:199	Investigating the output of Lins (1998) similarity measure with respect to the above criterion in (Geffet and Dagan, 2004), we discovered that the quality of similarity scores is often hurt by inaccurate feature weights, which yield rather noisy feature vectors." ></td>
	<td class="line x" title="50:199	Hence, we tried to improve the feature weighting function to promote those features that are most indicative of the word meaning." ></td>
	<td class="line x" title="51:199	A new weighting scheme was defined for bootstrapping feature weights, termed RFF (Relative Feature Focus)." ></td>
	<td class="line x" title="52:199	First, basic similarities are generated by Lins measure." ></td>
	<td class="line x" title="53:199	Then, feature weights are recalculated, boosting the weights of features that characterize many of the words that are most similar to the given one2." ></td>
	<td class="line x" title="54:199	As a result the most prominent features of a word are concentrated within the top-100 entries of the vector." ></td>
	<td class="line x" title="55:199	Finally, word similarities are recalculated by Lin's metric over the vectors with the new RFF weights." ></td>
	<td class="line x" title="56:199	The lexical entailment prediction task of (Geffet and Dagan, 2004) measures how many of the top ranking similarity pairs produced by the 2 In concrete terms RFF is defined by:" ></td>
	<td class="line x" title="57:199	= ),()()(),( vwsimwNfWSvfwRFF, where sim(w,v) is an initial approximation of the similarity space by Lins measure, WS(f) is a set of words co-occurring with feature f, and N(w) is the set of the most similar words of w by Lins measure." ></td>
	<td class="line x" title="58:199	RFF-based metric hold the entailment relation, in at least one direction." ></td>
	<td class="line x" title="59:199	To this end a data set of 1,200 pairs was created, consisting of top-N (N=40) similar words of 30 randomly selected nouns, which were manually judged by the lexical entailment criterion." ></td>
	<td class="line x" title="60:199	Quite high Kappa agreement values of 0.75 and 0.83 were reported, indicating that the entailment judgment task was reasonably well defined." ></td>
	<td class="line x" title="61:199	A subset of the data set is demonstrated in Table 1." ></td>
	<td class="line x" title="62:199	The RFF weighting produced 10% precision improvement over Lins original use of MI, suggesting the RFF capability to promote semantically meaningful features." ></td>
	<td class="line x" title="63:199	However, over 47% of the word pairs in the top-40 similarities are not related by entailment, which calls for further improvement." ></td>
	<td class="line x" title="64:199	In this paper we use the same data set 3 and the RFF metric as a basis for our experiments." ></td>
	<td class="line x" title="65:199	2.2 Predicting Semantic Inclusion Weeds et al.(2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other." ></td>
	<td class="line x" title="67:199	They present a distributional generality concept and expect it to correlate with semantic generality." ></td>
	<td class="line x" title="68:199	Their conjecture is that the majority of the features of the more specific word are included in the features of the more general one." ></td>
	<td class="line x" title="69:199	They define the feature recall of w with respect to v as the weighted proportion of features of v that also appear in the vector of w. Then, they suggest that a hypernym would have a higher feature recall for its hyponyms (specifications), than vice versa." ></td>
	<td class="line x" title="70:199	However, their results in predicting the hyponymy-hyperonymy direction (71% precision) are comparable to the nave baseline (70% precision) that simply assumes that general words are more frequent than specific ones." ></td>
	<td class="line x" title="71:199	Possible sources of noise in their experiment could be ignoring word polysemy and data sparseness of word-feature cooccurrence in the corpus." ></td>
	<td class="line x" title="72:199	3 The Distributional Inclusion Hypotheses In this paper we suggest refined versions of the distributional similarity hypothesis which relate distributional behavior with lexical entailment." ></td>
	<td class="line x" title="73:199	3 Since the original data set did not include the direction of entailment, we have enriched it by adding the judgments of entailment direction." ></td>
	<td class="line x" title="74:199	109 Extending the rationale of Weeds et al. , we suggest that if the meaning of a word v entails another word w then it is expected that all the typical contexts (features) of v will occur also with w. That is, the characteristic contexts of v are expected to be included within all w's contexts (but not necessarily amongst the most characteristic ones for w)." ></td>
	<td class="line x" title="75:199	Conversely, we might expect that if v's characteristic contexts are included within all w's contexts then it is likely that the meaning of v does entail w. Taking both directions together, lexical entailment is expected to highly correlate with characteristic feature inclusion." ></td>
	<td class="line x" title="76:199	Two additional observations are needed before concretely formulating these hypotheses." ></td>
	<td class="line x" title="77:199	As explained in Section 2, word contexts should be represented by syntactic features, which are more restrictive and thus better reflect the restrained semantic meaning of the word (it is difficult to tie entailment to looser context representations, such as co-occurrence in a text window)." ></td>
	<td class="line x" title="78:199	We also notice that distributional similarity principles are intended to hold at the sense level rather than the word level, since different senses have different characteristic contexts (even though computational common practice is to work at the word level, due to the lack of robust sense annotation)." ></td>
	<td class="line x" title="79:199	We can now define the two distributional inclusion hypotheses, which correspond to the two directions of inference relating distributional feature inclusion and lexical entailment." ></td>
	<td class="line x" title="80:199	Let vi and wj be two word senses of the words w and v, correspondingly, and let vi => wj denote the (directional) entailment relation between these senses." ></td>
	<td class="line x" title="81:199	Assume further that we have a measure that determines the set of characteristic features for the meaning of each word sense." ></td>
	<td class="line x" title="82:199	Then we would hypothesize: Hypothesis I: If vi => wj then all the characteristic (syntacticbased) features of vi are expected to appear with wj." ></td>
	<td class="line x" title="83:199	Hypothesis II: If all the characteristic (syntactic-based) features of vi appear with wj then we expect that vi => wj." ></td>
	<td class="line x" title="84:199	4 Word Level Testing of Feature Inclusion To check the validity of the hypotheses we need to test feature inclusion." ></td>
	<td class="line x" title="85:199	In this section we present an automated word-level feature inclusion testing method, termed ITA (Inclusion Testing Algorithm)." ></td>
	<td class="line x" title="86:199	To overcome the data sparseness problem we incorporated web-based feature sampling." ></td>
	<td class="line x" title="87:199	Given a test pair of words, three main steps are performed, as detailed in the following subsections: Step 1: Computing the set of characteristic features for each word." ></td>
	<td class="line x" title="88:199	Step 2: Testing feature inclusion for each pair, in both directions, within the given corpus data." ></td>
	<td class="line x" title="89:199	Step 3: Complementary testing of feature inclusion for each pair in the web." ></td>
	<td class="line x" title="90:199	4.1 Step 1: Corpus-based generation of characteristic features To implement the first step of the algorithm, the RFF weighting function is exploited and its top100 weighted features are taken as most characteristic for each word." ></td>
	<td class="line x" title="91:199	As mentioned in Section 2, (Geffet and Dagan, 2004) shows that RFF yields high concentration of good features at the top of the vector." ></td>
	<td class="line x" title="92:199	4.2 Step 2: Corpus-based feature inclusion test We first check feature inclusion in the corpus that was used to generate the characteristic feature sets." ></td>
	<td class="line x" title="93:199	For each word pair (w, v) we first determine which features of w do co-occur with v in the corpus." ></td>
	<td class="line x" title="94:199	The same is done to identify features of v that co-occur with w in the corpus." ></td>
	<td class="line x" title="95:199	4.3 Step 3: Complementary Webbased Inclusion Test This step is most important to avoid inclusion misses due to the data sparseness of the corpus." ></td>
	<td class="line x" title="96:199	A few recent works (Ravichandran and Hovy, 2002; Keller et al. , 2002; Chklovski and Pantel, 2004) used the web to collect statistics on word cooccurrences." ></td>
	<td class="line x" title="97:199	In a similar spirit, our inclusion test is completed by searching the web for the missing (non-included) features on both sides." ></td>
	<td class="line x" title="98:199	We call this web-based technique mutual web-sampling." ></td>
	<td class="line x" title="99:199	The web results are further parsed to verify matching of the feature's syntactic relationship." ></td>
	<td class="line x" title="100:199	110 We denote the subset of w's features that are missing for v as M(w, v) (and equivalently M(v, w))." ></td>
	<td class="line x" title="101:199	Since web sampling is time consuming we randomly sample a subset of k features (k=20 in our experiments), denoted as M(v,w,k)." ></td>
	<td class="line x" title="102:199	Mutual Web-sampling Procedure: For each pair (w, v) and their k-subsets M(w, v, k) and M(v, w, k) execute: 1." ></td>
	<td class="line x" title="103:199	Syntactic Filtering of Bag-of-Words Search: Search the web for sentences including v and a feature f from M(w, v, k) as bag of words, i. e. sentences where w and f appear in any distance and in either order." ></td>
	<td class="line x" title="104:199	Then filter out the sentences that do not match the defined syntactic relation between f and v (based on parsing)." ></td>
	<td class="line x" title="105:199	Features that co-occur with w in the correct syntactic relation are removed from M(w, v, k)." ></td>
	<td class="line x" title="106:199	Do the same search and filtering for w and features from M(v, w, k)." ></td>
	<td class="line x" title="107:199	2." ></td>
	<td class="line x" title="108:199	Syntactic Filtering of Exact String Matching: On the missing features on both sides (which are left in M(w, v, k) and M(v, w, k) after stage 1), apply exact string search of the web." ></td>
	<td class="line x" title="109:199	For this, convert the tuple (v, f) to a string by adding prepositions and articles where needed." ></td>
	<td class="line x" title="110:199	For example, for (element, <project, pcomp_of, 1>) generate the corresponding string element of the project and search the web for exact matches of the string." ></td>
	<td class="line x" title="111:199	Then validate the syntactic relationship of f and v in the extracted sentences." ></td>
	<td class="line x" title="112:199	Remove the found features from M(w, v, k) and M(v, w, k), respectively." ></td>
	<td class="line x" title="113:199	3." ></td>
	<td class="line x" title="114:199	Missing Features Validation: Since some of the features may be too infrequent or corpus-biased, check whether the remaining missing features do co-occur on the web with their original target words (with which they did occur in the corpus data)." ></td>
	<td class="line x" title="115:199	Otherwise, they should not be considered as valid misses and are also removed from M(w, v, k) and M(v, w, k)." ></td>
	<td class="line x" title="116:199	Output: Inclusion in either direction holds if the corresponding set of missing features is now empty." ></td>
	<td class="line x" title="117:199	We also experimented with features consisting of words without syntactic relations." ></td>
	<td class="line x" title="118:199	For example, exact string, or bag-of-words match." ></td>
	<td class="line x" title="119:199	However, almost all the words (also non-entailing) were found with all the features of each other, even for semantically implausible combinations (e.g. a word and a feature appear next to each other but belong to different clauses of the sentence)." ></td>
	<td class="line x" title="120:199	Therefore we conclude that syntactic relation validation is very important, especially on the web, in order to avoid coincidental co-occurrences." ></td>
	<td class="line x" title="121:199	5 Empirical Results To test the validity of the distributional inclusion hypotheses we performed an empirical analysis on a selected test sample using our automated testing procedure." ></td>
	<td class="line x" title="122:199	5.1 Data and setting We experimented with a randomly picked test sample of about 200 noun pairs of 1,200 pairs produced by RFF (for details see Geffet and Dagan, 2004) under Lins similarity scheme (Lin, 1998)." ></td>
	<td class="line x" title="123:199	The words were judged by the lexical entailment criterion (as described in Section 2)." ></td>
	<td class="line x" title="124:199	The original percentage of correct (52%) and incorrect (48%) entailments was preserved." ></td>
	<td class="line x" title="125:199	To estimate the degree of validity of the distributional inclusion hypotheses we decomposed each word pair of the sample (w, v) to two directional pairs ordered by potential entailment direction: (w, v) and (v, w)." ></td>
	<td class="line x" title="126:199	The 400 resulting ordered pairs are used as a test set in Sections 5.2 and 5.3." ></td>
	<td class="line x" title="127:199	Features were computed from co-occurrences in a subset of the Reuters corpus of about 18 million words." ></td>
	<td class="line x" title="128:199	For the web feature sampling the maximal number of web samples for each query (word feature) was set to 3,000 sentences." ></td>
	<td class="line x" title="129:199	5.2 Automatic Testing the Validity of the Hypotheses at the Word Level The test set of 400 ordered pairs was examined in terms of entailment (according to the manual judgment) and feature inclusion (according to the ITA algorithm), as shown in Table 2." ></td>
	<td class="line x" title="130:199	According to Hypothesis I we expect that a pair (w, v) that satisfies entailment will also preserve feature inclusion." ></td>
	<td class="line x" title="131:199	On the other hand, by Hypothesis II if all the features of w are included by v then we expect that w entails v. 111 We observed that Hypothesis I is better attested by our data than the second hypothesis." ></td>
	<td class="line x" title="132:199	Thus 86% (97 out of 113) of the entailing pairs fulfilled the inclusion condition." ></td>
	<td class="line x" title="133:199	Hypothesis II holds for approximately 70% (97 of 139) of the pairs for which feature inclusion holds." ></td>
	<td class="line x" title="134:199	In the next section we analyze the cases of violation of both hypotheses and find that the first hypothesis held to an almost perfect extent with respect to word senses." ></td>
	<td class="line x" title="135:199	It is also interesting to note that thanks to the web-sampling procedure over 90% of the nonincluded features in the corpus were found on the web, while most of the missing features (in the web) are indeed semantically implausible." ></td>
	<td class="line x" title="136:199	5.3 Manual Sense Level Testing of Hypotheses Validity Since our data was not sense tagged, the automatic validation procedure could only test the hypotheses at the word level." ></td>
	<td class="line x" title="137:199	In this section our goal is to analyze the findings of our empirical test at the word sense level as our hypotheses were defined for senses." ></td>
	<td class="line x" title="138:199	Basically, two cases of hypotheses invalidity were detected: Case 1: Entailments with non-included features (violation of Hypothesis I); Case 2: Feature Inclusion for non-entailments (violation of Hypothesis II)." ></td>
	<td class="line x" title="139:199	At the word level we observed 14% invalid pairs of the first case and 30% of the second case." ></td>
	<td class="line x" title="140:199	However, our manual analysis shows, that over 90% of the first case pairs were due to a different sense of one of the entailing word, e.g. capital town (capital as money) and spread gap (spread as distribution) (Table 3)." ></td>
	<td class="line x" title="141:199	Note that ambiguity of the entailed word does not cause errors (like town  area, area as domain) (Table 3)." ></td>
	<td class="line x" title="142:199	Thus the first hypothesis holds at the sense level for over 98% of the cases (Table 4)." ></td>
	<td class="line x" title="143:199	Two remaining invalid instances of the first case were due to the web sampling method limitations and syntactic parsing filtering mistakes, especially for some less characteristic and infrequent features captured by RFF." ></td>
	<td class="line x" title="144:199	Thus, in virtually all the examples tested in our experiment Hypothesis I was valid." ></td>
	<td class="line x" title="145:199	We also explored the second case of invalid pairs: non-entailing words that pass the feature inclusion test." ></td>
	<td class="line x" title="146:199	After sense based analysis their percentage was reduced slightly to 27.4%." ></td>
	<td class="line x" title="147:199	Three possible reasons were discovered." ></td>
	<td class="line x" title="148:199	First, there are words with features typical to the general meaning of the domain, which tend to be included by many other words of this domain, like valley  town." ></td>
	<td class="line x" title="149:199	The features of valley (eastern valley, central valley, attack in valley, industry of the valley) are not discriminative enough to be distinguished from town, as they are all characteristic to any geographic location." ></td>
	<td class="line x" title="150:199	Inclusion Entailment + + 97 16 42 245 Table 2: Distribution of 400 entailing/nonentailing ordered pairs that hold/do not hold feature inclusion at the word level." ></td>
	<td class="line x" title="151:199	Inclusion Entailment + + 111 2 42 245 Table 4: Distribution of the entailing/nonentailing ordered pairs that hold/do not hold feature inclusion at the sense level." ></td>
	<td class="line x" title="152:199	spread  gap (mutually entail each other) <weapon, pcomp_of> The Committee was discussing the Programme of the Big Eight, aimed against spread of weapon of mass destruction." ></td>
	<td class="line x" title="153:199	town  area (town entails area) <cooperation, pcomp_for> This is a promising area for cooperation and exchange of experiences." ></td>
	<td class="line x" title="154:199	capital  town (capital entails town) <flow, nn> Offshore financial centers affect cross-border capital flow in China." ></td>
	<td class="line x" title="155:199	Table 3: Examples of ambiguity of entailmentrelated words, where the disjoint features belong to a different sense of the word." ></td>
	<td class="line x" title="156:199	112 The second group consists of words that can be entailing, but only in a context-dependent (anaphoric) manner rather than ontologically." ></td>
	<td class="line x" title="157:199	For example, government and neighbour, while neighbour is used in the meaning of neighbouring (country) government." ></td>
	<td class="line x" title="158:199	Finally, sometimes one or both of the words are abstract and general enough and also highly ambiguous to appear with a wide range of features on the web, like element (violence  element, with all the tested features of violence included by element)." ></td>
	<td class="line x" title="159:199	To prevent occurrences of the second case more characteristic and discriminative features should be provided." ></td>
	<td class="line x" title="160:199	For this purpose features extracted from the web, which are not domain-biased (like features from the corpus) and multi-word features may be helpful." ></td>
	<td class="line x" title="161:199	Overall, though, there might be inherent cases that invalidate Hypothesis II." ></td>
	<td class="line x" title="162:199	6 Improving Lexical Entailment Prediction by ITA (Inclusion Testing Algorithm) In this section we show that ITA can be practically used to improve the (non-directional) lexical entailment prediction task described in Section 2." ></td>
	<td class="line x" title="163:199	Given the output of the distributional similarity method, we employ ITA at the word level to filter out non-entailing pairs." ></td>
	<td class="line x" title="164:199	Word pairs that satisfy feature inclusion of all k features (at least in one direction) are claimed as entailing." ></td>
	<td class="line x" title="165:199	The same test sample of 200 word pairs mentioned in Section 5.1 was used in this experiment." ></td>
	<td class="line x" title="166:199	The results were compared to RFF under Lins similarity scheme (RFF-top-40 in Table 5)." ></td>
	<td class="line x" title="167:199	Precision was significantly improved, filtering out 60% of the incorrect pairs." ></td>
	<td class="line x" title="168:199	On the other hand, the relative recall (considering RFF recall as 100%) was only reduced by 13%, consequently leading to a better relative F1, when considering the RFF-top-40 output as 100% recall (Table 5)." ></td>
	<td class="line x" title="169:199	Since our method removes about 35% of the original top-40 RFF output, it was interesting to compare our results to simply cutting off the 35% of the lowest ranked RFF words (top-26)." ></td>
	<td class="line x" title="170:199	The comparison to the baseline (RFF-top-26 in Table 5) showed that ITA filters the output much better than just cutting off the lowest ranking similarities." ></td>
	<td class="line x" title="171:199	We also tried a couple of variations on feature sampling for the web-based procedure." ></td>
	<td class="line x" title="172:199	In one of our preliminary experiments we used the top-k RFF features instead of random selection." ></td>
	<td class="line x" title="173:199	But we observed that top ranked RFF features are less discriminative than the random ones due to the nature of the RFF weighting strategy, which promotes features shared by many similar words." ></td>
	<td class="line x" title="174:199	Then, we attempted doubling the sampling to 40 random features." ></td>
	<td class="line x" title="175:199	As expected the recall was slightly decreased, while precision was increased by over 5%." ></td>
	<td class="line x" title="176:199	In summary, the behavior of ITA sampling of k=20 and k=40 features is closely comparable (ITA-20 and ITA-40 in Table 5, respectively)4." ></td>
	<td class="line x" title="177:199	7 Conclusions and Future Work The main contributions of this paper were: 1." ></td>
	<td class="line x" title="178:199	We defined two Distributional Inclusion Hypotheses that associate feature inclusion with lexical entailment at the word sense level." ></td>
	<td class="line x" title="179:199	The Hypotheses were proposed as a refinement for Harris Distributional hypothesis and as an extension to the classic distributional similarity scheme." ></td>
	<td class="line x" title="180:199	2." ></td>
	<td class="line x" title="181:199	To estimate the empirical validity of the defined hypotheses we developed an automatic inclusion testing algorithm (ITA)." ></td>
	<td class="line x" title="182:199	The core of the algorithm is a web-based feature inclusion testing procedure, which helped significantly to compensate for data sparseness." ></td>
	<td class="line x" title="183:199	3." ></td>
	<td class="line x" title="184:199	Then a thorough analysis of the data behavior with respect to the proposed hypotheses was conducted." ></td>
	<td class="line x" title="185:199	The first hypothesis was almost fully attested by the data, particularly at the sense level, while the second hypothesis did not fully hold." ></td>
	<td class="line x" title="186:199	4." ></td>
	<td class="line x" title="187:199	Motivated by the empirical analysis we proposed to employ ITA for the practical task of improving lexical entailment acquisition." ></td>
	<td class="line x" title="188:199	The algorithm was applied as a filtering technique on the distributional similarity (RFF) output." ></td>
	<td class="line x" title="189:199	We ob4 The ITA-40 sampling fits the analysis from section 5.2 and 5.3 as well." ></td>
	<td class="line x" title="190:199	Method Precision Recall F1 ITA-20 0.700 0.875 0.777 ITA-40 0.740 0.846 0.789 RFF-top-40 0.520 1.000 0.684 RFF-top-26 0.561 0.701 0.624 Table 5: Comparative results of using the filter, with 20 and 40 feature sampling, compared to RFF top-40 and RFF top-26 similarities." ></td>
	<td class="line x" title="191:199	ITA-20 and ITA-40 denote the websampling method with 20 and random 40 features, respectively." ></td>
	<td class="line x" title="192:199	113 tained 17% increase of precision and succeeded to improve relative F1 by 15% over the baseline." ></td>
	<td class="line x" title="193:199	Although the results were encouraging our manual data analysis shows that we still have to handle word ambiguity." ></td>
	<td class="line x" title="194:199	In particular, this is important in order to be able to learn the direction of entailment." ></td>
	<td class="line x" title="195:199	To achieve better precision we need to increase feature discriminativeness." ></td>
	<td class="line x" title="196:199	To this end syntactic features may be extended to contain more than one word, and ways for automatic extraction of features from the web (rather than from a corpus) may be developed." ></td>
	<td class="line x" title="197:199	Finally, further investigation of combining the distributional and the co-occurrence pattern-based approaches over the web is desired." ></td>
	<td class="line x" title="198:199	Acknowledgement We are grateful to Shachar Mirkin for his help in implementing the web-based sampling procedure heavily employed in our experiments." ></td>
	<td class="line x" title="199:199	We thank Idan Szpektor for providing the infrastructure system for web-based data extraction." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1075
A Nonparametric Method For Extraction Of Candidate Phrasal Terms
Deane, Paul;"></td>
	<td class="line x" title="1:157	Proceedings of the 43rd Annual Meeting of the ACL, pages 605613, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:157	c2005 Association for Computational Linguistics A Nonparametric Method for Extraction of Candidate Phrasal Terms Paul Deane Center for Assessment, Design and Scoring Educational Testing Service pdeane@ets.org Abstract This paper introduces a new method for identifying candidate phrasal terms (also known as multiword units) which applies a nonparametric, rank-based heuristic measure." ></td>
	<td class="line x" title="3:157	Evaluation of this measure, the mutual rank ratio metric, shows that it produces better results than standard statistical measures when applied to this task." ></td>
	<td class="line x" title="4:157	1 Introduction The ordinary vocabulary of a language like English contains thousands of phrasal terms -multiword lexical units including compound nouns, technical terms, idioms, and fixed collocations." ></td>
	<td class="line x" title="5:157	The exact number of phrasal terms is difficult to determine, as new ones are coined regularly, and it is sometimes difficult to determine whether a phrase is a fixed term or a regular, compositional expression." ></td>
	<td class="line x" title="6:157	Accurate identification of phrasal terms is important in a variety of contexts, including natural language parsing, question answering systems, information retrieval systems, among others." ></td>
	<td class="line x" title="7:157	Insofar as phrasal terms function as lexical units, their component words tend to cooccur more often, to resist substitution or paraphrase, to follow fixed syntactic patterns, and to display some degree of semantic noncompositionality (Manning, 1999:183-186)." ></td>
	<td class="line x" title="8:157	However, none of these characteristics are amenable to a simple algorithmic interpretation." ></td>
	<td class="line x" title="9:157	It is true that various term extraction systems have been developed, such as Xtract (Smadja 1993), Termight (Dagan & Church 1994), and TERMS (Justeson & Katz 1995) among others (cf.Daille 1996, Jacquemin & Tzoukermann 1994, Jacquemin, Klavans, & Toukermann 1997, Boguraev & Kennedy 1999, Lin 2001)." ></td>
	<td class="line x" title="11:157	Such systems typically rely on a combination of linguistic knowledge and statistical association measures." ></td>
	<td class="line x" title="12:157	Grammatical patterns, such as adjective-noun or noun-noun sequences are selected then ranked statistically, and the resulting ranked list is either used directly or submitted for manual filtering." ></td>
	<td class="line x" title="13:157	The linguistic filters used in typical term extraction systems have no obvious connection with the criteria that linguists would argue define a phrasal term (noncompositionality, fixed order, nonsubstitutability, etc.)." ></td>
	<td class="line x" title="14:157	They function, instead, to reduce the number of a priori improbable terms and thus improve precision." ></td>
	<td class="line x" title="15:157	The association measure does the actual work of distinguishing between terms and plausible nonterms." ></td>
	<td class="line oc" title="16:157	A variety of methods have been applied, ranging from simple frequency (Justeson & Katz 1995), modified frequency measures such as c-values (Frantzi, Anadiou & Mima 2000, Maynard & Anadiou 2000) and standard statistical significance tests such as the t-test, the chi-squared test, and loglikelihood (Church and Hanks 1990, Dunning 1993), and information-based methods, e.g. pointwise mutual information (Church & Hanks 1990)." ></td>
	<td class="line x" title="17:157	Several studies of the performance of lexical association metrics suggest significant room for improvement, but also variability among tasks." ></td>
	<td class="line x" title="18:157	One series of studies (Krenn 1998, 2000; Evert & Krenn 2001, Krenn & Evert 2001; also see Evert 2004) focused on the use of association metrics to identify the best candidates in particular grammatical constructions, such as adjective-noun pairs or verb plus prepositional phrase constructions, and compared the performance of simple frequency to several common measures (the log-likelihood, the t-test, the chi-squared test, the dice coefficient, relative entropy and mutual information)." ></td>
	<td class="line x" title="19:157	In Krenn & Evert 2001, frequency outperformed mutual information though not the ttest, while in Evert and Krenn 2001, log-likelihood and the t-test gave the best results, and mutual information again performed worse than frequency." ></td>
	<td class="line x" title="20:157	However, in all these studies performance was generally low, with precision falling rapidly after the very highest ranked phrases in the list." ></td>
	<td class="line x" title="21:157	By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and online dictionaries as gold standards." ></td>
	<td class="line x" title="22:157	Once again, the general level of performance was low, with precision falling off rapidly as larger portions 605 of the n-best list were included, but they report better performance with statistical and information theoretic measures (including mutual information) than with frequency." ></td>
	<td class="line x" title="23:157	The overall pattern appears to be one where lexical association measures in general have very low precision and recall on unfiltered data, but perform far better when combined with other features which select linguistic patterns likely to function as phrasal terms." ></td>
	<td class="line x" title="24:157	The relatively low precision of lexical association measures on unfiltered data no doubt has multiple explanations, but a logical candidate is the failure or inappropriacy of underlying statistical assumptions." ></td>
	<td class="line x" title="25:157	For instance, many of the tests assume a normal distribution, despite the highly skewed nature of natural language frequency distributions, though this is not the most important consideration except at very low n (cf.Moore 2004, Evert 2004, ch." ></td>
	<td class="line x" title="27:157	4)." ></td>
	<td class="line x" title="28:157	More importantly, statistical and information-based metrics such as the log-likelihood and mutual information measure significance or informativeness relative to the assumption that the selection of component terms is statistically independent." ></td>
	<td class="line x" title="29:157	But of course the possibilities for combinations of words are anything but random and independent." ></td>
	<td class="line x" title="30:157	Use of linguistic filters such as 'attributive adjective followed by noun' or 'verb plus modifying prepositional phrase' arguably has the effect of selecting a subset of the language for which the standard null hypothesis -that any word may freely be combined with any other word -may be much more accurate." ></td>
	<td class="line x" title="31:157	Additionally, many of the association measures are defined only for bigrams, and do not generalize well to phrasal terms of varying length." ></td>
	<td class="line x" title="32:157	The purpose of this paper is to explore whether the identification of candidate phrasal terms can be improved by adopting a heuristic which seeks to take certain of these statistical issues into account." ></td>
	<td class="line x" title="33:157	The method to be presented here, the mutual rank ratio, is a nonparametric rank-based approach which appears to perform significantly better than the standard association metrics." ></td>
	<td class="line x" title="34:157	The body of the paper is organized as follows: Section 2 will introduce the statistical considerations which provide a rationale for the mutual rank ratio heuristic and outline how it is calculated." ></td>
	<td class="line x" title="35:157	Section 3 will present the data sources and evaluation methodologies applied in the rest of the paper." ></td>
	<td class="line x" title="36:157	Section 4 will evaluate the mutual rank ratio statistic and several other lexical association measures on a larger corpus than has been used in previous evaluations." ></td>
	<td class="line x" title="37:157	As will be shown below, the mutual rank ratio statistic recognizes phrasal terms more effectively than standard statistical measures." ></td>
	<td class="line x" title="38:157	2 Statistical considerations 2.1 Highly skewed distributions As first observed e.g. by Zipf (1935, 1949) the frequency of words and other linguistic units tend to follow highly skewed distributions in which there are a large number of rare events." ></td>
	<td class="line x" title="39:157	Zipf's formulation of this relationship for single word frequency distributions (Zipf's first law) postulates that the frequency of a word is inversely proportional to its rank in the frequency distribution, or more generally if we rank words by frequency and assign rank z, where the function fz(z,N) gives the frequency of rank z for a sample of size N, Zipf's first law states that: fz(z,N) = Cz where C is a normalizing constant and is a free parameter that determines the exact degree of skew; typically with single word frequency data, approximates 1 (Baayen 2001: 14)." ></td>
	<td class="line x" title="40:157	Ideally, an association metric would be designed to maximize its statistical validity with respect to the distribution which underlies natural language text -which is if not a pure Zipfian distribution at least an LNRE (large number of rare events, cf.Baayen 2001) distribution with a very long tail, containing events which differ in probability by many orders of magnitude." ></td>
	<td class="line x" title="42:157	Unfortunately, research on LNRE distributions focuses primarily on unigram distributions, and generalizations to bigram and ngram distributions on large corpora are not as yet clearly feasible (Baayen 2001:221)." ></td>
	<td class="line x" title="43:157	Yet many of the best-performing lexical association measures, such as the t-test, assume normal distributions, (cf.Dunning 1993) or else (as with mutual information) eschew significance testing in favor of a generic information-theoretic approach." ></td>
	<td class="line x" title="45:157	Various strategies could be adopted in this situation: finding a better model of the distribution,or adopting a nonparametric method." ></td>
	<td class="line x" title="46:157	2.2 The independence assumption Even more importantly, many of the standard lexical association measures measure significance (or information content) against the default assumption that word-choices are statistically independent events." ></td>
	<td class="line x" title="47:157	This assumption is built into the highest-performing measures as observed in Evert & Krenn 2001, Krenn & Evert 2001 and Schone & Jurafsky 2001." ></td>
	<td class="line x" title="48:157	This is of course untrue, and justifiable only as a simplifying idealization in the absence of a better model." ></td>
	<td class="line x" title="49:157	The actual probability of any sequence of words is strongly influenced by the base grammatical and semantic structure of language, particularly since phrasal terms usually conform to 606 the normal rules of linguistic structure." ></td>
	<td class="line x" title="50:157	What makes a compound noun, or a verb-particle construction, into a phrasal term is not deviation from the base grammatical pattern for noun-noun or verb-particle structures, but rather a further pattern (of meaning and usage and thus heightened frequency) superimposed on the normal linguistic base." ></td>
	<td class="line x" title="51:157	There are, of course, entirely aberrant phrasal terms, but they constitute the exception rather than the rule." ></td>
	<td class="line x" title="52:157	This state of affairs poses something of a chicken-and-the-egg problem, in that statistical parsing models have to estimate probabilities from the same base data as the lexical association measures, so the usual heuristic solution as noted above is to impose a linguistic filter on the data, with the association measures being applied only to the subset thus selected." ></td>
	<td class="line x" title="53:157	The result is in effect a constrained statistical model in which the independence assumption is much more accurate." ></td>
	<td class="line x" title="54:157	For instance, if the universe of statistical possibilities is restricted to the set of sequences in which an adjective is followed by a noun, the null hypothesis that word choice is independent -i.e., that any adjective may precede any noun -is a reasonable idealization." ></td>
	<td class="line x" title="55:157	Without filtering, the independence assumption yields the much less plausible null hypothesis that any word may appear in any order." ></td>
	<td class="line x" title="56:157	It is thus worth considering whether there are any ways to bring additional information to bear on the problem of recognizing phrasal terms without presupposing statistical independence." ></td>
	<td class="line x" title="57:157	2.3 Variable length; alternative/overlapping phrases Phrasal terms vary in length." ></td>
	<td class="line x" title="58:157	Typically they range from about two to six words in length, but critically we cannot judge whether a phrase is lexical without considering both shorter and longer sequences." ></td>
	<td class="line x" title="59:157	That is, the statistical comparison that needs to be made must apply in principle to the entire set of word sequences that must be distinguished from phrasal terms, including longer sequences, subsequences, and overlapping sequences, despite the fact that these are not statistically independent events." ></td>
	<td class="line x" title="60:157	Of the association metrics mentioned thus far, only the C-Value method attempts to take direct notice of such word sequence information, and then only as a modification to the basic information provided by frequency." ></td>
	<td class="line x" title="61:157	Any solution to the problem of variable length must enable normalization allowing direct comparison of phrases of different length." ></td>
	<td class="line x" title="62:157	Ideally, the solution would also address the other issues -the independence assumption and the skewed distributions typical of natural language data." ></td>
	<td class="line x" title="63:157	2.4 Mutual expectation An interesting proposal which seeks to overcome the variable-length issue is the mutual expectation metric presented in Dias, Guillor, and Lopes (1999) and implemented in the SENTA system (Gil and Dias 2003a)." ></td>
	<td class="line x" title="64:157	In their approach, the frequency of a phrase is normalized by taking into account the relative probability of each word compared to the phrase." ></td>
	<td class="line x" title="65:157	Dias, Guillor, and Lopes take as the foundation of their approach the idea that the cohesiveness of a text unit can be measured by measuring how strongly it resists the loss of any component term." ></td>
	<td class="line x" title="66:157	This is implemented by considering, for any ngram, the set of [continuous or discontinuous] (n-1)-grams which can be formed by deleting one word from the n-gram." ></td>
	<td class="line x" title="67:157	A normalized expectation for the n-gram is then calculated as follows: 1 2 1 2 ([, ]) ([,  ]) n n p w w w FPE w w w where [w1, w2  wn] is the phrase being evaluated and FPE([w1, w2  wn]) is: 1 2 1 1 ^1 ([,  ]) [   ] n n i n i p w w w p w w wn = + where wi is the term omitted from the n-gram." ></td>
	<td class="line x" title="68:157	They then calculate mutual expectation as the product of the probability of the n-gram and its normalized expectation." ></td>
	<td class="line x" title="69:157	This statistic is of interest for two reasons: first, it provides a single statistic that can be applied to n-grams of any length; second, it is not based upon the independence assumption." ></td>
	<td class="line x" title="70:157	The core statistic, normalized expectation, is essentially frequency with a penalty if a phrase contains component parts significantly more frequent than the phrase itself." ></td>
	<td class="line x" title="71:157	It is of course an empirical question how well mutual expectation performs (and we shall examine this below) but mutual expectation is not in any sense a significance test." ></td>
	<td class="line x" title="72:157	That is, if we are examining a phrase like the east end, the conditional probability of east given [__ end] or of end given [__ east] may be relatively low (since other words can appear in that context) and yet the phrase might still be very lexicalized if the association of both words with this context were significantly stronger than their association for 607 other phrases." ></td>
	<td class="line x" title="73:157	That is, to the extent that phrasal terms follow the regular patterns of the language, a phrase might have a relatively low conditional probability (given the wide range of alternative phrases following the same basic linguistic patterns) and thus have a low mutual expectation yet still occur far more often than one would expect from chance." ></td>
	<td class="line x" title="74:157	In short, the fundamental insight -assessing how tightly each word is bound to a phrase -is worth adopting." ></td>
	<td class="line x" title="75:157	There is, however, good reason to suspect that one could improve on this method by assessing relative statistical significance for each component word without making the independence assumption." ></td>
	<td class="line x" title="76:157	In the heuristic to be outlined below, a nonparametric method is proposed." ></td>
	<td class="line x" title="77:157	This method is novel: not a modification of mutual expectation, but a new technique based on ranks in a Zipfian frequency distribution." ></td>
	<td class="line x" title="78:157	2.5 Rank ratios and mutual rank ratios This technique can be justified as follows." ></td>
	<td class="line x" title="79:157	For each component word in the n-gram, we want to know whether the n-gram is more probable for that word than we would expect given its behavior with other words." ></td>
	<td class="line x" title="80:157	Since we do not know what the expected shape of this distribution is going to be, a nonparametric method using ranks is in order, and there is some reason to think that frequency rank regardless of n-gram size will be useful." ></td>
	<td class="line x" title="81:157	In particular, Ha, Sicilia-Garcia, Ming and Smith (2002) show that Zipf's law can be extended to the combined frequency distribution of n-grams of varying length up to rank 6, which entails that the relative rank of words in such a combined distribution provide a useful estimate of relative probability." ></td>
	<td class="line x" title="82:157	The availability of new techniques for handling large sets of n-gram data (e.g. Gil & Dias 2003b) make this a relatively feasible task." ></td>
	<td class="line x" title="83:157	Thus, given a phrase like east end, we can rank how often __ end appears with east in comparison to how often other phrases appear with east.That is, if {__ end, __side, the __, toward the __, etc.} is the set of (variable length) n-gram contexts associated with east (up to a length cutoff), then the actual rank of __ end is the rank we calculate by ordering all contexts by the frequency with which the actual word appears in the context." ></td>
	<td class="line x" title="84:157	We also rank the set of contexts associated with east by their overall corpus frequency." ></td>
	<td class="line x" title="85:157	The resulting ranking is the expected rank of __ end based upon how often the competing contexts appear regardless of which word fills the context." ></td>
	<td class="line x" title="86:157	The rank ratio (RR) for the word given the context can then be defined as: RR(word,context) = ( )( ),,ER word contextAR word context where ER is the expected rank and AR is the actual rank." ></td>
	<td class="line x" title="87:157	A normalized, or mutual rank ratio for the ngram can then be defined as 2 11, [__  ] 2, [ __  ], [ 1, 2 _]( )* ( )* ( )n nw w w w n w wn RR w RR w RR w The motivation for this method is that it attempts to address each of the major issues outlined above by providing a nonparametric metric which does not make the independence assumption and allows scores to be compared across n-grams of different lengths." ></td>
	<td class="line x" title="88:157	A few notes about the details of the method are in order." ></td>
	<td class="line x" title="89:157	Actual ranks are assigned by listing all the contexts associated with each word in the corpus, and then ranking contexts by word, assigning the most frequent context for word n the rank 1, next next most frequent rank 2, etc. Tied ranks are given the median value for the ranks occupied by the tie, e.g., if two contexts with the same frequency would occupy ranks 2 and 3, they are both assigned rank 2.5." ></td>
	<td class="line x" title="90:157	Expected ranks are calculated for the same set of contexts using the same algorithm, but substituting the unconditional frequency of the (n-1)-gram for the gram's frequency with the target word.1 3 Data sources and methodology The Lexile Corpus is a collection of documents covering a wide range of reading materials such as a child might encounter at school, more or less evenly divided by Lexile (reading level) rating to cover all levels of textual complexity from kindergarten to college." ></td>
	<td class="line x" title="91:157	It contains in excess of 400 million words of running text, and has been made available to the Educational Testing Service under a research license by Metametrics Corporation." ></td>
	<td class="line x" title="92:157	This corpus was tokenized using an in-house tokenization program, toksent, which treats most punctuation marks as separate tokens but makes single tokens out of common abbreviations, numbers like 1,500, and words like o'clock." ></td>
	<td class="line x" title="93:157	It should be noted that some of the association measures are known to perform poorly if punctuation marks and common stopwords are 1 In this study the rank-ratio method was tested for bigrams and trigrams only, due to the small number of WordNet gold standard items greater than two words in length." ></td>
	<td class="line x" title="94:157	Work in progress will assess the metrics' performance on n-grams of orders four through six." ></td>
	<td class="line x" title="95:157	608 included; therefore, n-gram sequences containing punctuation marks and the 160 most frequent word forms were excluded from the analysis so as not to bias the results against them." ></td>
	<td class="line x" title="96:157	Separate lists of bigrams and trigrams were extracted and ranked according to several standard word association metrics." ></td>
	<td class="line x" title="97:157	Rank ratios were calculated from a comparison set consisting of all contexts derived by this method from bigrams and trigrams, e.g., contexts of the form word1__, ___word2, ___word1 word2, word1 ___ word3, and word1 word2 ___.2 Table 1 lists the standard lexical association measures tested in section four3." ></td>
	<td class="line x" title="98:157	The logical evaluation method for phrasal term identification is to rank n-grams using each metric and then compare the results against a gold standard containing known phrasal terms." ></td>
	<td class="line x" title="99:157	Since Schone and Jurafsky (2001) demonstrated similar results whether WordNet or online dictionaries were used as a gold standard, WordNet was selected." ></td>
	<td class="line x" title="100:157	Two separate lists were derived containing twoand three-word phrases." ></td>
	<td class="line x" title="101:157	The choice of WordNet as a gold standard tests ability to predict general dictionary headwords rather than technical terms, appropriate since the source corpus consists of nontechnical text." ></td>
	<td class="line x" title="102:157	Following Schone & Jurafsky (2001), the bigram and trigram lists were ranked by each statistic then scored against the gold standard, with results evaluated using a figure of merit (FOM) roughly characterizable as the area under the precisionrecall curve." ></td>
	<td class="line x" title="103:157	The formula is: 1 1 k i i PK = where Pi (precision at i) equals i/Hi, and Hi is the number of n-grams into the ranked n-gram list required to find the ith correct phrasal term." ></td>
	<td class="line x" title="104:157	It should be noted, however, that one of the most pressing issues with respect to phrasal terms is that they display the same skewed, long-tail distribution as ordinary words, with a large 2 Excluding the 160 most frequent words prevented evaluation of a subset of phrasal terms such as verbal idioms like act up or go on." ></td>
	<td class="line x" title="105:157	Experiments with smaller corpora during preliminary work indicated that this exclusion did not appear to bias the results." ></td>
	<td class="line x" title="106:157	3 Schone & Jurafsky's results indicate similar results for log-likelihood & T-score, and strong parallelism among information-theoretic measures such as ChiSquared, Selectional Association (Resnik 1996), Symmetric Conditional Probability (Ferreira and Pereira Lopes, 1999) and the Z-Score (Smadja 1993)." ></td>
	<td class="line x" title="107:157	Thus it was not judged necessary to replicate results for all methods covered in Schone & Jurafsky (2001)." ></td>
	<td class="line x" title="108:157	proportion of the total displaying very low frequencies." ></td>
	<td class="line x" title="109:157	This can be measured by considering Table 1." ></td>
	<td class="line x" title="110:157	Some Lexical Association Measures the overlap between WordNet and the Lexile corpus." ></td>
	<td class="line x" title="111:157	A list of 53,764 two-word phrases were extracted from WordNet, and 7,613 three-word phrases." ></td>
	<td class="line x" title="112:157	Even though the Lexile corpus is quite large -in excess of 400 million words of running text -only 19,939 of the two-word phrases and 4 Due to the computational cost of calculating CValues over a very large corpus, C-Values were calculated over bigrams and trigrams only." ></td>
	<td class="line x" title="113:157	More sophisticated versions of the C-Value method such as NC-values were not included as these incorporate linguistic knowledge and thus fall outside the scope of the study." ></td>
	<td class="line oc" title="114:157	METRIC FORMULA Frequency (Guiliano, 1964) x yf Pointwise Mutual Information [PMI] (Church & Hanks, 1990) ( )xy x y2log /P P P True Mutual Information [TMI] (Manning, 1999) ( )xy 2 xy x ylog /P P P P Chi-Squared ( 2 ) (Church and Gale, 1991) { }{ },, 2( ) i X X Y Y i j i j i j j f     T-Score (Church & Hanks, 1990) 1 2 2 2 1 2 1 2 x x s s n n  + C-Values4 (Frantzi, Anadiou & Mima 2000) 2 is not nested 2 log ( ) log ( ) 1 ( ) ( ) a a b T a f f f b P T         where is the candidate string f( ) is its frequency in the corpus T is the set of candidate terms that contain P(T ) is the number of these candidate terms 609 1,700 of the three-word phrases are attested in the Lexile corpus." ></td>
	<td class="line x" title="115:157	14,045 of the 19,939 attested twoword phrases occur at least 5 times, 11,384 occur at least 10 times, and only 5,366 occur at least 50 times; in short, the strategy of cutting off the data at a threshold sacrifices a large percent of total recall." ></td>
	<td class="line x" title="116:157	Thus one of the issues that needs to be addressed is the accuracy with which lexical association measures can be extended to deal with relatively sparse data, e.g., phrases that appear less than ten times in the source corpus." ></td>
	<td class="line x" title="117:157	A second question of interest is the effect of filtering for particular linguistic patterns." ></td>
	<td class="line x" title="118:157	This is another method of prescreening the source data which can improve precision but damage recall." ></td>
	<td class="line x" title="119:157	In the evaluation bigrams were classified as N-N and A-N sequences using a dictionary template, with the expected effect." ></td>
	<td class="line x" title="120:157	For instance, if the WordNet two word phrase list is limited only to those which could be interpreted as noun-noun or adjective noun sequences, N>=5, the total set of WordNet terms that can be retrieved is reduced to 9,757 4 Evaluation Schone and Jurafsky's (2001) study examined the performance of various association metrics on a corpus of 6.7 million words with a cutoff of N=10." ></td>
	<td class="line x" title="121:157	The resulting n-gram set had a maximum recall of 2,610 phrasal terms from the WordNet gold standard, and found the best figure of merit for any of the association metrics even with linguistic filterering to be 0.265." ></td>
	<td class="line x" title="122:157	On the significantly larger Lexile corpus N must be set higher (around N=50) to make the results comparable." ></td>
	<td class="line x" title="123:157	The statistics were also calculated for N=50, N=10 and N=5 in order to see what the effect of including more (relatively rare) n-grams would be on the overall performance for each statistic." ></td>
	<td class="line x" title="124:157	Since many of the statistics are defined without interpolation only for bigrams, and the number of WordNet trigrams at N=50 is very small, the full set of scores were only calculated on the bigram data." ></td>
	<td class="line x" title="125:157	For trigrams, in addition to rank ratio and frequency scores, extended pointwise mutual information and true mutual information scores were calculated using the formulas log (Pxyz/PxPy Pz)) and Pxyz log (Pxyz/PxPy Pz))." ></td>
	<td class="line x" title="126:157	Also, since the standard lexical association metrics cannot be calculated across different n-gram types, results for bigrams and trigrams are presented separately for purposes of comparison." ></td>
	<td class="line x" title="127:157	The results are are shown in Tables 2-5." ></td>
	<td class="line x" title="128:157	Two points should should be noted in particular." ></td>
	<td class="line x" title="129:157	First, the rank ratio statistic outperformed the other association measures tested across the board." ></td>
	<td class="line o" title="130:157	Its best performance, a score of 0.323 in the part of speech filtered condition with N=50, outdistanced METRIC POS Filtered Unfiltered RankRatio 0.323 0.196 Mutual Expectancy 0.144 0.069 TMI 0.209 0.096 PMI 0.287 0.166 Chi-sqr 0.285 0.152 T-Score 0.154 0.046 C-Values 0.065 0.048 Frequency 0.130 0.044 Table 2." ></td>
	<td class="line o" title="131:157	Bigram Scores for Lexical Association Measures with N=50 METRIC POS Filtered Unfiltered RankRatio 0.218 0.125 MutualExpectation 0.140 0.071 TMI 0.150 0.070 PMI 0.147 0.065 Chi-sqr 0.145 0.065 T-Score 0.112 0.048 C-Values 0.096 0.036 Frequency 0.093 0.034 Table 3." ></td>
	<td class="line x" title="132:157	Bigram Scores for Lexical Association Measures with N=10 METRIC POS Filtered Unfiltered RankRatio 0.188 0.110 Mutual Expectancy 0.141 0.073 TMI 0.131 0.063 PMI 0.108 0.047 Chi-sqr 0.107 0.047 T-Score 0.098 0.043 C-Values 0.084 0.031 Frequency 0.081 0.021 Table 4." ></td>
	<td class="line o" title="133:157	Bigram Scores for Lexical Association Measures with N=5 METRIC N=50 N=10 N=5 RankRatio 0.273 0.137 0.103 PMI 0.219 0.121 0.059 TMI 0.137 0.074 0.056 Frequency 0.089 0.047 0.035 Table 5." ></td>
	<td class="line x" title="134:157	Trigram scores for Lexical Association Measures at N=50, 10 and 5 without linguistic filtering." ></td>
	<td class="line x" title="135:157	610 the best score in Schone & Jurafsky's study (0.265), and when large numbers of rare bigrams were included, at N=10 and N=5, it continued to outperform the other measures." ></td>
	<td class="line x" title="136:157	Second, the results were generally consistent with those reported in the literature, and confirmed Schone & Jurafsky's observation that the information-theoretic measures (such as mutual information and chisquared) outperform frequency-based measures (such as the T-score and raw frequency.)5 4.1 Discussion One of the potential strengths of this method is that is allows for a comparison between n-grams of varying lengths." ></td>
	<td class="line x" title="137:157	The distribution of scores for the gold standard bigrams and trigrams appears to bear out the hypothesis that the numbers are comparable across n-gram length." ></td>
	<td class="line x" title="138:157	Trigrams constitute approximately four percent of the gold standard test set, and appear in roughly the same percentage across the rankings; for instance, they consistute 3.8% of the top 10,000 ngrams ranked by mutual rank ratio." ></td>
	<td class="line x" title="139:157	Comparison of trigrams with their component bigrams also seems consistent with this hypothesis; e.g., the bigram Booker T. has a higher mutual rank ratio than the trigram Booker T. Washington, which has a higher rank that the bigram T. Washington." ></td>
	<td class="line x" title="140:157	These results suggest that it would be worthwhile to examine how well the method succeeds at ranking n-grams of varying lengths, though the limitations of the current evaluation set to bigrams and trigrams prevented a full evaluation of its effectiveness across n-grams of varying length." ></td>
	<td class="line x" title="141:157	The results of this study appear to support the conclusion that the Mutual Rank Ratio performs notably better than other association measures on this task." ></td>
	<td class="line x" title="142:157	The performance is superior to the nextbest measure when N is set as low as 5 (0.110 compared to 0.073 for Mutual Expectation and 0.063 for true mutual information and less than .05 for all other metrics)." ></td>
	<td class="line x" title="143:157	While this score is still fairly low, it indicates that the measure performs relatively well even when large numbers of lowprobability n-grams are included." ></td>
	<td class="line x" title="144:157	An examination of the n-best list for the Mutual Rank ratio at N=5 supports this contention." ></td>
	<td class="line x" title="145:157	The top 10 bigrams are: 5 Schone and Jurafsky's results differ from Krenn & Evert (2001)'s results, which indicated that frequency performed better than the statistical measures in almost every case." ></td>
	<td class="line x" title="146:157	However, Krenn and Evert's data consisted of n-grams preselected to fit particular collocational patterns." ></td>
	<td class="line x" title="147:157	Frequency-based metrics seem to be particularly benefited by linguistic prefiltering." ></td>
	<td class="line x" title="148:157	Julius Caesar, Winston Churchill, potato chips, peanut butter, Frederick Douglass, Ronald Reagan, Tia Dolores, Don Quixote, cash register, Santa Claus At ranks 3,000 to 3,010, the bigrams are: Ted Williams, surgical technicians, Buffalo Bill, drug dealer, Lise Meitner, Butch Cassidy, Sandra Cisneros, Trey Granger, senior prom, Ruta Skadi At ranks 10,000 to 10,010, the bigrams are: egg beater, sperm cells, lowercase letters, methane gas, white settlers, training program, instantly recognizable, dried beef, television screens, vienna sausages In short, the n-best list returned by the mutual rank ratio statistic appears to consist primarily of phrasal terms far down the list, even when N is as low as 5." ></td>
	<td class="line x" title="149:157	False positives are typically: (i) morphological variants of established phrases; (ii) bigrams that are part of longer phrases, such as cream sundae (from ice cream sundae); (iii) examples of highly productive constructions such as an artist, three categories or January 2." ></td>
	<td class="line x" title="150:157	The results for trigrams are relatively sparse and thus less conclusive, but are consistent with the bigram results: the mutual rank ratio measure performs best, with top ranking elements consistently being phrasal terms." ></td>
	<td class="line x" title="151:157	Comparison with the n-best list for other metrics bears out the qualitative impression that the rank ratio is performing better at selecting phrasal terms even without filtering." ></td>
	<td class="line x" title="152:157	The top ten bigrams for the true mutual information metric at N=5 are: a little, did not, this is, united states, new york, know what, a good, a long, a moment, a small Ranks 3000 to 3010 are: waste time, heavily on, earlier than, daddy said, ethnic groups, tropical rain, felt sure, raw materials, gold medals, gold rush Ranks 10,000 to 10,010 are: quite close, upstairs window, object is, lord god, private schools, nat turner, fire going, bering sea,little higher, got lots The behavior is consistent with known weaknesses of true mutual information -its tendency to overvalue frequent forms." ></td>
	<td class="line x" title="153:157	Next, consider the n-best lists for loglikelihood at N=5." ></td>
	<td class="line x" title="154:157	The top ten n-grams are: sheriff poulson, simon huggett, robin redbreast, eric torrosian, colonel hillandale, colonel sapp, nurse leatheran, st. catherines, karen torrio, jenny yonge N-grams 3000 to 3010 are: comes then, stuff who, dinner get, captain see, tom see, couple get, fish see, picture go, building go, makes will, pointed way 611 N-grams 10000 to 10010 are: sayings is, writ this, llama on, undoing this, dwahro did, reno on, squirted on, hardens like, mora did, millicent is, vets did Comparison thus seems to suggest that if anything the quality of the mutual rank ratio results are being understated by the evaluation metric, as the metric is returning a large number of phrasal terms in the higher portion of the n-best list that are absent from the gold standard." ></td>
	<td class="line x" title="155:157	Conclusion This study has proposed a new method for measuring strength of lexical association for candidate phrasal terms based upon the use of Zipfian ranks over a frequency distribution combining n-grams of varying length." ></td>
	<td class="line x" title="156:157	The method is related in general philosophy of Mutual Expectation, in that it assesses the strenght of connection for each word to the combined phrase; it differs by adopting a nonparametric measure of strength of association." ></td>
	<td class="line x" title="157:157	Evaluation indicates that this method may outperform standard lexical association measures, including mutual information, chi-squared, log-likelihood, and the T-score." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0101
Teaching Applied Natural Language Processing: Triumphs And Tribulations
Hearst, Marti A.;"></td>
	<td class="line x" title="1:202	Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 18, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:202	c2005 Association for Computational Linguistics Teaching Applied Natural Language Processing: Triumphs and Tribulations Marti Hearst School of Information Management & Systems University of California, Berkeley Berkeley, CA 94720 hearst@sims.berkeley.edu Abstract In Fall 2004 I introduced a new course called Applied Natural Language Processing, in which students acquire an understanding of which text analysis techniques are currently feasible for practical applications." ></td>
	<td class="line x" title="3:202	The class was intended for interdisciplinary students with a somewhat technical background." ></td>
	<td class="line x" title="4:202	This paper describes the topics covered and the programming exercises, emphasizing which aspects were successful and which problematic, and makes recommendations for future versions of the course." ></td>
	<td class="line x" title="5:202	1 Introduction In Fall 2005 I introduced a new graduate level course called Applied Natural Language Processing.1 The goal of this course was to acquaint students with the state-of-the-art of the field of NLP with an emphasis on applications." ></td>
	<td class="line x" title="6:202	The intention was for students to leave the class with an understanding of what is currently feasible (and just on the horizon) to expect from content analysis, and how to use and extend existing NLP tools and technology." ></td>
	<td class="line x" title="7:202	The course did not emphasize the theoretical underpinnings of NLP, although we did cover the most important algorithms." ></td>
	<td class="line x" title="8:202	A companion graduate course on Statistical NLP was taught by Dan Klein in the Computer Science department." ></td>
	<td class="line x" title="9:202	Dans course focused on 1Lecture notes, assignments, and other resources can be found at http://www.sims.berkeley.edu/courses/is290-2/f04/." ></td>
	<td class="line x" title="10:202	foundations and core NLP algorithms." ></td>
	<td class="line x" title="11:202	Several computer science students took both courses, and thus learned both the theoretical and the applied sides of NLP." ></td>
	<td class="line x" title="12:202	Dan and I discussed the goals and content of our respective courses in advance, but developed the courses independently." ></td>
	<td class="line x" title="13:202	2 Course Role within the SIMS Program The primary target audience of the Applied NLP course were masters students, and to a lesser extent, PhD students, in the School of Information Management and Systems." ></td>
	<td class="line x" title="14:202	(Nevertheless, PhD students in computer science and other fields also took the course)." ></td>
	<td class="line x" title="15:202	MIMS students (as the SIMS masters students are known) pursue a professional degree studying information at the intersection of technology and social sciences." ></td>
	<td class="line x" title="16:202	The students technical backgrounds vary widely; each year a significant fraction have Computer Science undergraduate degrees, and another significant fraction have social science or humanities backgrounds." ></td>
	<td class="line x" title="17:202	All students have an interest in technology and are required to take some challenging technical courses, but most non-CS background students are uncomfortable with advanced mathematics and are not as comfortable with coding as CS students are." ></td>
	<td class="line x" title="18:202	A key aspect of the program is the capstone final project, completed in the last semester, that (ideally) combines knowledge and skills obtained from throughout the program." ></td>
	<td class="line x" title="19:202	Most students form a team of 3-4 students and build a system, usually to meet the requirements of an outside client or customer (although some students write policy papers and others get involved in research with faculty mem1 bers)." ></td>
	<td class="line x" title="20:202	Often the execution of these projects makes use of user-centered design, including a needs assessment, and iterative design and testing of the artifact." ></td>
	<td class="line x" title="21:202	These projects often also have a backend design component using database design principles, document engineering modeling, or information architecture and organization principles, with sensitivity to legal considerations for privacy and intellectual property." ></td>
	<td class="line x" title="22:202	Students are required to present their work to an audience of students, faculty, and professionals, produce a written report, and produce a website that describes and demonstrates their work." ></td>
	<td class="line x" title="23:202	In many cases these projects would benefit greatly from content analysis." ></td>
	<td class="line x" title="24:202	Past projects have included a system to query on and monitor news topics as they occur across time and sources, a system to analyze when and where company names are mentioned in text and graph interconnections among them, a system to allow customization of news channels by topic, and systems to search and analyze blogs." ></td>
	<td class="line x" title="25:202	Our past course offerings in this space focused on information retrieval with very little emphasis on content analysis, so students were using only IR-type techniques for these projects." ></td>
	<td class="line x" title="26:202	The state of the art in NLP had advanced sufficiently that the available tools can be employed for a number of projects like these." ></td>
	<td class="line x" title="27:202	Furthermore, it is important for students attempting such projects to have an understanding of what is currently feasible and what is too ambitious." ></td>
	<td class="line x" title="28:202	In fact, I find that this is a key aspect of teaching an applied class: learning what is possible with existing tools, what is feasible but requires more expertise than can be engineered in a semester with existing tools, and what is beyond the scope of current techniques." ></td>
	<td class="line x" title="29:202	3 Choosing Tools and Readings The main challenges for a hands-on course as Id envisioned surrounded finding usable interoperable tools, and defining feasible assignments that make use of programming without letting it interfere with learning." ></td>
	<td class="line x" title="30:202	There is of course the inevitable decision of which programming language(s) to work with." ></td>
	<td class="line x" title="31:202	Scripting tools such as python are fast and easy to prototype with, but require the students to learn a new programming language." ></td>
	<td class="line x" title="32:202	Java is attractive because many tools are written in it and the MIMS students were familiar with java  they are required to use it for two of their required courses but still tend to struggle with it." ></td>
	<td class="line x" title="33:202	I did not consider perl since python is a more principled language and is growing in acceptance and in tool availability." ></td>
	<td class="line x" title="34:202	In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002)." ></td>
	<td class="line x" title="35:202	One goal of NLTK is to remove the emphasis on programming to enable students to achieve results quickly; and this aligned with my primary goal." ></td>
	<td class="line x" title="36:202	NLTK seemed promising because it contained some well-written tutorials on n-grams, POS tagging and chunking, and contained text categorization modules." ></td>
	<td class="line x" title="37:202	(I also wanted support for entity extraction, which NLTK does not supply)." ></td>
	<td class="line x" title="38:202	NLTK is written in python, and so I decided to try it and have the students learn a new programming language." ></td>
	<td class="line x" title="39:202	As will be described in detail below, our use of NLTK was somewhat successful, but we experienced numerous problems as well." ></td>
	<td class="line x" title="40:202	I made a rather large mistake early on by not spending time introducing python, since I wanted the assignments to correspond to the lectures and did not want to spend lecture time on the programming language itself." ></td>
	<td class="line x" title="41:202	I instructed students who had registered for the course to learn python during the summer, but (not surprisingly) many of did not and had to struggle in the first few weeks." ></td>
	<td class="line x" title="42:202	In retrospect, I realize I should have allowed time for people to learn python, perhaps via a lab session that met only during the first few weeks of class." ></td>
	<td class="line x" title="43:202	Another sticking point was student exposure to regular expressions." ></td>
	<td class="line x" title="44:202	Regexs were very important and useful practical tools both for tokenization assignments and for shallow parsing." ></td>
	<td class="line x" title="45:202	I assumed that the MIMS students had gotten practice with regular expressions because they are required to take a computer concepts foundations course which I designed several years ago." ></td>
	<td class="line x" title="46:202	Unfortunately, the lecturer who took over the class from me had decided to omit regexs and related topics." ></td>
	<td class="line x" title="47:202	I realized that I had to do some remedial coverage of the topic, which of course bored the CS students and which was not complete enough for the MIMS students." ></td>
	<td class="line x" title="48:202	Again this suggests that perhaps some kind of lab is needed for getting people caught up in topics, or that perhaps 2 the first few weeks of the class should be optional for more advanced students." ></td>
	<td class="line x" title="49:202	I was also unable to find an appropriate textbook." ></td>
	<td class="line x" title="50:202	Neither Schutze & Manning nor Jurafsky & Martin focus on the right topics." ></td>
	<td class="line x" title="51:202	The closest in terms of topic is Natural Language Processing for Online Applications by Peter Jackson & Isabelle Moulinier, but much of this book focuses on Information Retrieval (which we teach in two other courses) and did not go into depth on the topics I most cared about." ></td>
	<td class="line x" title="52:202	Instead of a text, students read a small selection of research papers and the NLTK tutorials." ></td>
	<td class="line x" title="53:202	4 Topics The course met twice weekly for 80 minute periods." ></td>
	<td class="line x" title="54:202	The topic coverage is shown below; topics followed by (2) indicate two lecture periods were needed." ></td>
	<td class="line x" title="55:202	Course Introduction Using Large Collections (intro to NLTK) Tokenization, Morphological Analysis Part-of-Speech Tagging Conditional Probabilities Shallow Parsing (2) Text Classification: Introduction Text Classification: Feature Selection Text Classification: Algorithms Text Classification: Using Weka Information Extraction (2) Email and Anti-Spam Analysis Text Data Mining Lexicons and Ontologies FrameNet (guest lecture by Chuck Fillmore) Enron email dataset (in-class work) (2) Spelling Correction / Clustering Summarization (guest lecture by Drago Radev) Question Answering (2) Machine Translation (slides by Kevin Knight) Topic Segmentation / Discourse Processing Class Presentations Note the lack of coverage of full syntactic parsing, which is covered extensively in Dan Kleins course." ></td>
	<td class="line x" title="56:202	I touched on it briefly in the second shallow parsing lecture and felt this level of coverage was acceptable because shallow parsing is often as useful if not more so than full parsing for most applications." ></td>
	<td class="line x" title="57:202	Note also the lack of coverage of word sense disambiguation." ></td>
	<td class="line x" title="58:202	This topic is rich in algorithms, but was omitted primarily due to time constraints, but in part because of the lack of well-known applications." ></td>
	<td class="line x" title="59:202	Based on the kinds of capstone projects the MIMS students have done in the past, I knew that the most important techniques for their needs surrounded text categorization and information extraction/entity recognition." ></td>
	<td class="line x" title="60:202	There are terrific software resources for text categorization and the field is fairly mature, so I had my PhD students Preslav Nakov and Barbara Rosario gave the lectures on this topic, in order to provide them with teaching experience." ></td>
	<td class="line x" title="61:202	The functionality provided by named entity recognition is very important for a wide range of real-world applications." ></td>
	<td class="line x" title="62:202	Unfortunately, none of the free tools that we tried were particularly successful." ></td>
	<td class="line x" title="63:202	Those that are available are difficult to configure and get running in a short amount of time, and have virtually no documentation." ></td>
	<td class="line x" title="64:202	Furthermore, the state-ofthe-art in algorithms is not present in the available tools in the way that more mature technologies such as POS tagging, parsing, and categorization are." ></td>
	<td class="line x" title="65:202	5 Using NLTK 5.1 Benefits We used the latest version of NLTK, which at the time was version 1.4.2 NLTK supplies some preprocessed text collections, which are quite useful." ></td>
	<td class="line x" title="66:202	(Unfortunately, the different corpora have different types of preprocessing applied to them, which often lead to confusion and extra work for the class.)" ></td>
	<td class="line x" title="67:202	The NLTK tokenizer, POS taggers and the shallow parser (chunker) have terrific functionality once they are understood; some students were able to get quite accurate results using these and the supplied training sets." ></td>
	<td class="line x" title="68:202	The ability to combine different n-gram taggers within the structure of a backoff tagger also supported an excellent exercise." ></td>
	<td class="line x" title="69:202	However, a somewhat minor problem with the taggers is that there is no compact way to store the model resulting from tagging for later use." ></td>
	<td class="line x" title="70:202	A serialized object could be created and stored, but the size of such object was so large that it takes about as long to load it into memory as it does to retrain the tagger." ></td>
	<td class="line x" title="71:202	2http://nltk.sourceforge.org 3 5.2 Drawbacks There were four major problems with NLTK from the perspective of this course." ></td>
	<td class="line x" title="72:202	The first major problem was the inconsistency in the different releases of code, both in terms of incompatibilities between the data structures in the different versions, and incompatibility of the documentation and tutorials within the different versions." ></td>
	<td class="line x" title="73:202	It was tricky to determine which documentation was associated with which code version." ></td>
	<td class="line x" title="74:202	And much of the contributed code did not work with the current version." ></td>
	<td class="line x" title="75:202	The second major problem was related to the first, but threw a major wrench into our plans: some of the advertised functionality simply was not available in the current version of the software." ></td>
	<td class="line x" title="76:202	Notably, NLTK advertised a text categorization module; without this I would not have adopted NLTK as the coding platform for the class." ></td>
	<td class="line x" title="77:202	Unfortunately, the most current version did not in fact support categorization, and we discovered this just days before we were to begin covering this topic." ></td>
	<td class="line x" title="78:202	The third major problem was the incompleteness of the documentation for much of the code." ></td>
	<td class="line x" title="79:202	This to some degree undermined the goal of reducing the amount of work for students, since they (and I) had to struggle to figure out what was going on in the code and data structures." ></td>
	<td class="line x" title="80:202	One of these documentation problems centered around the data structure for conditional probabilities." ></td>
	<td class="line x" title="81:202	NLTK creates a FreqDist class which is explained well in the documentation (it records a count for each occurrence of some phenomenon, much like a hash table) and provides methods for retrieving the max, the count and frequency of each occurrence, and so on." ></td>
	<td class="line x" title="82:202	It also provides a class called a CondFreqDist, but does not document its methods nor explain its implementation." ></td>
	<td class="line x" title="83:202	Users have to scrutinize the examples given and try to reverse engineer the data structure." ></td>
	<td class="line x" title="84:202	Eventually I realized that it is simply a list of objects of type FreqDist, but this was difficult to determine at first, and caused much wasting of time and confusion among the students." ></td>
	<td class="line x" title="85:202	There is also confusion surrounding the use of the method names count and frequency for FreqDist." ></td>
	<td class="line x" title="86:202	Count refers to number of occurrences and frequency to a probability distribution across items, but this distinction is never stated explicitly although it can be inferred from a table of methods in the tutorial." ></td>
	<td class="line x" title="87:202	A less dramatic but still hampering problem was with the design of the core data structures, which make use of attribute tags rather than classes." ></td>
	<td class="line x" title="88:202	This leads to rather awkward code structures." ></td>
	<td class="line x" title="89:202	For example, after a sentence is tokenized, the results of tokenization are appended to the sentence data structure and are accessed via use of a subtoken keyword such as TOKENS." ></td>
	<td class="line x" title="90:202	To then run a POS tagger over the tokenized results, the TOKENS keyword has to be specified as the value for a SUBTOKENS attribute, and another keyword must be supplied to act as the name of the tagged results." ></td>
	<td class="line x" title="91:202	In my opinion it would be better to use the class system and define objects of different types and operations on those objects." ></td>
	<td class="line x" title="92:202	6 Assignments One of the major goals of the class was for the students to obtain hands-on experience using and extending existing NLP tools." ></td>
	<td class="line x" title="93:202	This was accomplished through a series of homework assignments and a final project." ></td>
	<td class="line x" title="94:202	My pedagogical philosophy surrounding assignments is to supply as much as the functionality as necessary so that the coding that students do leads directly to learning." ></td>
	<td class="line x" title="95:202	Thus, I try to avoid making students deal with details of formatting files and so on." ></td>
	<td class="line x" title="96:202	I also try to give students a starting point to build up on." ></td>
	<td class="line x" title="97:202	The first assignment made use of some exercises from the NLTK tutorials." ></td>
	<td class="line x" title="98:202	Students completed tokenizing exercises which required the use of the NLTK corpus tool accessors and the FreqDist and CondFreqDist classes." ></td>
	<td class="line x" title="99:202	They also did POS tagging exercises which exposed them to the idea of ngrams, backoff algorithms, and to the process of training and testing." ></td>
	<td class="line x" title="100:202	This assignment was challenging (especially because of some misleading text in the tagging tutorial, which has since been fixed) but the students learned a great deal." ></td>
	<td class="line x" title="101:202	As mentioned above, I should have begun with a preliminary assignment which got students familiar with python basics before attempting this assignment." ></td>
	<td class="line x" title="102:202	For assignment 2, I provided a simple set of regular expression grammar rules for the shallow parser class, and asked the students to improve on these." ></td>
	<td class="line oc" title="103:202	After building the chunker, students were asked to 4 choose a verb and then analyze verb-argument structure (they were provided with two relevant papers (Church and Hanks, 1990; Chklovski and Pantel, 2004))." ></td>
	<td class="line x" title="104:202	As mentioned above, most of the MIMS students were not familiar with regular expressions, so I should have done a longer unit on this topic, at the expense of boring the CS students." ></td>
	<td class="line x" title="105:202	The students learned a great deal from working to improve the grammar rules, but the verb-argument analysis portion was not particularly successful, in part because the corpus analyzed was too small to yield many sentences for a given verb and because we did not have code to automatically find regularities about the semantics of the arguments of the verbs." ></td>
	<td class="line x" title="106:202	Other causes of difficulty were the students lack of linguistic background, and the fact that the chunking part took longer than I expected, leaving students little time for the analysis portion of the assignment." ></td>
	<td class="line x" title="107:202	Assignments 3 and 4 are described in the following subsections." ></td>
	<td class="line x" title="108:202	6.1 Text Categorization Assignment As mentioned above, text categorization is useful for a wide range SIMS applications, and we made it a centerpiece of the course." ></td>
	<td class="line x" title="109:202	Unfortunately, we had to make a mid-course correction when I suddenly realized that text categorization was no longer available in NLTK." ></td>
	<td class="line x" title="110:202	After looking at a number of tools, we decided to use the Weka toolkit for categorization (Witten and Frank, 2000)." ></td>
	<td class="line x" title="111:202	We did not want the students to feel they had wasted their time learning python and NLTK, so we decided to make it easy for the students to reuse their python code by providing an interface between it and Weka." ></td>
	<td class="line x" title="112:202	My PhD student Preslav Nakov provided great help by writing code to translate the output of our python code into the input format expected by Weka." ></td>
	<td class="line x" title="113:202	(Weka is written in java but has command line and GUI interfaces, and can read in input files and store models as output files)." ></td>
	<td class="line x" title="114:202	As time went on we added increasingly more functionality to this code, tying it in with the NLTK modules so that the students could use the NLTK corpora for training and testing.3 3Available at http://www.sims.berkeley.edu/courses/is2902/f04/assignments/assignment3.html Both Preslav and I had used Weka in the past but mainly with the command-line interface, and not taking advantage of its rich functionality." ></td>
	<td class="line x" title="115:202	As with NLTK, the documentation for Weka was incomplete and out of date, and it was difficult to determine how to use the more advanced features." ></td>
	<td class="line x" title="116:202	We performed extended experimentation with the system and developed a detailed tutorial on how to use the system; this tutorial should be of general use.4 For the categorization task, we used the twenty newsgroups collection that was supplied with NLTK." ></td>
	<td class="line x" title="117:202	Unfortunately, it was not preprocessed into sentences, so I also had to write some sentence splitting code (based on Palmer and Hearst (1997)) so students could make use of their tokenizer and tagger code." ></td>
	<td class="line x" title="118:202	We selected one pair of newsgroups which contained very different content (rec.motorcycles vs. sci.space)." ></td>
	<td class="line x" title="119:202	We called this the diverse set." ></td>
	<td class="line x" title="120:202	We then created two groups of newsgroups with more homogeneous content (a) rec.autos, rec.motorcycles, rec.sport.baseball, rec.sport.hockey, and (b) sci.crypt, sci.electronics, sci.med.original, sci.space." ></td>
	<td class="line x" title="121:202	The intention was to show the students that it is easier to automatically distinguish the heterogeneous groups than the homogeneous ones." ></td>
	<td class="line x" title="122:202	We set up the code to allow students to adjust the size of their training and development sets, and to separate out a reserved test set that would be used for comparing students solutions." ></td>
	<td class="line x" title="123:202	We challenged the students to get the best scores possible on the held out test set, telling them not to use this test set until they were completely finished training and testing on the development set." ></td>
	<td class="line x" title="124:202	(We relied on the honor system for this)." ></td>
	<td class="line x" title="125:202	We made it known that we would announce which were the top-scoring assignments." ></td>
	<td class="line x" title="126:202	As a general rule I avoid competition in my classes, but this was kept very low-key; only the top-scoring results would be named." ></td>
	<td class="line x" title="127:202	Furthermore, innovative approaches that perhaps did not do as well as some others were also highlighted." ></td>
	<td class="line x" title="128:202	Students were required to try at least 2 different types of features and 3 different classifiers." ></td>
	<td class="line x" title="129:202	This assignment was quite successful, as the stu4Available at http://www.sims.berkeley.edu/courses/is2902/f04/lectures/lecture11.ppt 5 dents were creative about building their features, and it was possible to achieve very strong results (much stronger than I expected) on both sets of newsgroups." ></td>
	<td class="line x" title="130:202	The best scoring approaches got 99% accuracy on the 2-way diverse distinction and 97% accuracy on the 4-way homogeneous distinction." ></td>
	<td class="line x" title="131:202	6.2 Enron Email Assignment Many of the SIMS students are interested in social networking and related topics." ></td>
	<td class="line x" title="132:202	I decided as part of the class that we would analyze a relatively new text collection that had become available and that contained the potential for interesting text mining and analysis." ></td>
	<td class="line x" title="133:202	I was also interested in having the class help produce a resource that would be of use to other classes and researchers." ></td>
	<td class="line x" title="134:202	Thus we decided to take on the Enron email corpus,5 on which limited analysis had been done." ></td>
	<td class="line x" title="135:202	My PhD student Andrew Fiore wrote code to preprocess this text, removing redundancies, normalizing email addresses, labeling quoted text, and so on." ></td>
	<td class="line x" title="136:202	He and I designed a database schema for representing much of the structure of the collection and loaded in the parsed text." ></td>
	<td class="line x" title="137:202	I created a Lucene6 index for doing free text queries while Andrew built a highly functional web interface for searching fielded components." ></td>
	<td class="line x" title="138:202	Andrews system eventually allowed for individual students to login and register annotations on the email messages." ></td>
	<td class="line x" title="139:202	This collection consists of approximately 200,000 messages after the duplicates have been removed." ></td>
	<td class="line x" title="140:202	We wanted to identify a subset of emails that might be interesting for analysis while at the same time avoiding highly personal messages, messages consisting mainly of jokes, and so on." ></td>
	<td class="line x" title="141:202	After doing numerous searches, we decided to try to focus primarily on documents relating to the California energy crisis, trading discrepancies, and messages occurring near the end of the time range (just before the companys stock crashed)." ></td>
	<td class="line x" title="142:202	After selecting about 1500 messages, I devised an initial set of categories." ></td>
	<td class="line x" title="143:202	In class we refined these." ></td>
	<td class="line x" title="144:202	One student had the interesting idea of trying to identify change in emotional tone as the scandals surrounding the company came to light, so we added emotional tone as a category type." ></td>
	<td class="line x" title="145:202	Each message 5http://www-2.cs.cmu.edu/ enron/ 6http://lucene.apache.org was then read and annotated by two students using the pre-defined categories." ></td>
	<td class="line x" title="146:202	Students were asked to reconcile their differences when they had them." ></td>
	<td class="line x" title="147:202	Despite these safeguards, my impression is that the resulting assignments are far from consistent and the categories themselves are still rather ad hoc and oftentimes overlapping." ></td>
	<td class="line x" title="148:202	There were many difficult curation issues, such as how to categorize a message with forwarded content when that content differed in kind from the new material." ></td>
	<td class="line x" title="149:202	If wed spent more time on this we could have done a better job, but as this was not an information organization course, I felt we could not spend more time on perfecting the labels." ></td>
	<td class="line x" title="150:202	Thus, I do not recommend the category labels be used for serious analysis." ></td>
	<td class="line x" title="151:202	Nevertheless, a number of researchers have asked for the cleaned up database and categories, and we have made them publicly available, along with the search interface.7 The students were then given two weeks to process the collection in some manner." ></td>
	<td class="line x" title="152:202	I made several suggestions, including trying to automatically assign the hand-assigned categories, extending some automatic acronym recognition work that wed done in our research (Schwartz and Hearst, 2003), using named entity recognition code to identify various actors, clustering the collection, or doing some kind of social network analysis." ></td>
	<td class="line x" title="153:202	Students were told that they could extend this assignment into their final projects if they chose." ></td>
	<td class="line x" title="154:202	For most students it was difficult to obtain a strong result using this collection." ></td>
	<td class="line x" title="155:202	The significant exception was for those students who worked on extending our acronym recognition algorithm; these projects were quite successful." ></td>
	<td class="line x" title="156:202	(In fact, one student managed to improve on our results with a rather simple modification to our code)." ></td>
	<td class="line x" title="157:202	Students often had creative ideas that were stymied by the poor quality of the available tools." ></td>
	<td class="line x" title="158:202	Two groups used the MALLET named entity recognizer toolkit8 in order to do various kinds of social network analysis, but the results were poor." ></td>
	<td class="line x" title="159:202	(Students managed to make up for this deficiency in creative ways.)" ></td>
	<td class="line x" title="160:202	I was a bit worried about students trying to use clustering to analyze the results, given the general difficulty of making sense of the results of cluster7http://bailando.sims.berkeley.edu/enron email.html 8http://mallet.cs.umass.edu 6 ing, and this concern was justified." ></td>
	<td class="line x" title="161:202	Clustering based on Weka and other tools is of course memoryand compute-intensive, but more problematically, the results are difficult to interpret." ></td>
	<td class="line x" title="162:202	I would recommend against allowing students to do a text clustering exercise unless within a more constrained environment." ></td>
	<td class="line x" title="163:202	In summary, students were excited about building a resource based on relatively untapped and very interesting data." ></td>
	<td class="line x" title="164:202	The resulting analysis on this untamed text was somewhat disappointing, but given that only two weeks were spent on this part of the assignment, I believe it was a good learning experience." ></td>
	<td class="line x" title="165:202	Furthermore, the resulting resource seems to be of interest to a number of researchers, as was our intention." ></td>
	<td class="line x" title="166:202	6.3 Final Projects I deliberately kept the time for the final projects short (about 3 weeks) so students would not go overboard or feel pressure to do something hugely timeconsuming." ></td>
	<td class="line x" title="167:202	The goal was to allow students to tie together some of the different ideas and skills theyd acquired in the class (and elsewhere), and to learn them in more depth by applying them to a topic of personal interest." ></td>
	<td class="line x" title="168:202	Students were encouraged to work in pairs, and I suggested a list of project ideas." ></td>
	<td class="line x" title="169:202	Students who adopted suggested projects tended to be more successful than those who developed their own." ></td>
	<td class="line x" title="170:202	Those who tried other topics were often too ambitious and had trouble getting meaningful results." ></td>
	<td class="line x" title="171:202	However, several of those students were trying ideas that they planned to apply to their capstone projects, and so it was highly valuable for them to get a preview of what worked and what did not." ></td>
	<td class="line x" title="172:202	One suggestion I made was to create a back-ofthe-book indexer, specifically for a recipe book, and one team did a good job with this project." ></td>
	<td class="line x" title="173:202	Another was to improve on or apply an automatic hierarchy generation tool that we have developed in our research (Stoica and Hearst, 2004)." ></td>
	<td class="line x" title="174:202	Students working on a project to collect metadata for camera phone images successfully applied this tool to this problem." ></td>
	<td class="line x" title="175:202	Again, social networking analysis topics were popular but not particularly successful; NLP tools are not advanced enough yet to meet the needs of this intriguing topic area." ></td>
	<td class="line x" title="176:202	Not surprisingly, when students started with a new (interesting) text collection, they were bogged down in the preprocessing stage before they could get much interesting work done." ></td>
	<td class="line x" title="177:202	6.4 Reflecting on Assignments Although students were excited about the Enron collection and we created a resource that is actively being used by other researchers, I think in future versions of the class I will omit this kind of assignment and have the students start their final projects sooner." ></td>
	<td class="line x" title="178:202	This will allow them time to do any preprocessing necessary to get the text into shape for doing the interesting work." ></td>
	<td class="line x" title="179:202	I will also exercise more control over what they are allowed to attempt (which is not my usual style) in order to ensure more successful outcomes." ></td>
	<td class="line x" title="180:202	I am not sure if I will use NLTK again or not." ></td>
	<td class="line x" title="181:202	If the designers make significant improvements on the code and documentation, then I probably will." ></td>
	<td class="line x" title="182:202	The style and intent of the tutorials are quite appropriate for the goals of the class." ></td>
	<td class="line x" title="183:202	Students with stronger coding background tended to use java for their final projects, whereas the others tended to build on the python code we developed in the class assignments, which suggests that this kind of toolkit approach is useful for them." ></td>
	<td class="line x" title="184:202	7 Conclusions Overall, I feel the main goals of the course were met." ></td>
	<td class="line x" title="185:202	Although I am emphasizing how the course could be improved, most students were quite positive about the class, giving it an overall score of 5.8 out of 7 with a mode of 6 in their anonymous course reviews." ></td>
	<td class="line x" title="186:202	(This is on the low side for my courses; most who gave it low scores found the programming too difficult.)" ></td>
	<td class="line x" title="187:202	Most students found the material highly stimulating and the work challenging but not overwhelming." ></td>
	<td class="line x" title="188:202	Several students mentioned that a lab session with a dedicated TA would have been desirable." ></td>
	<td class="line x" title="189:202	Several suggested covering less material in more depth and several commented that the Enron exercise was a neat idea although not entirely successful in execution." ></td>
	<td class="line x" title="190:202	Students remarked on liking reading research papers rather than a textbook (they also liked the relatively light reading load, which I feel was appropriate given the heavy assignment load)." ></td>
	<td class="line x" title="191:202	Some students 7 wanted more emphasis on real-world applications; I think it would be useful to have guest speakers from industry talk about this if possible." ></td>
	<td class="line x" title="192:202	I would like to see more research tools developed to a point to which they can be applied more successfully, especially in the area of information extraction." ></td>
	<td class="line x" title="193:202	I would also recommend to colleagues that careful control be retained over assignments and projects to ensure feasibility in the outcome." ></td>
	<td class="line x" title="194:202	It is more difficult to get good results on class projects in NLP than in other areas Ive taught." ></td>
	<td class="line x" title="195:202	As we so often see in text analysis work, it can often be difficult to do better than simple word counts for many projects." ></td>
	<td class="line x" title="196:202	I am interested in hearing ideas about how to accommodate both the somewhat technical and the highly technical students, especially in the early parts of the course." ></td>
	<td class="line x" title="197:202	Perhaps the best solution is to offer an optional laboratory section, at least for the first few weeks, but perhaps for the entire term, but this solution obviously requires more resources." ></td>
	<td class="line x" title="198:202	When designing this course I did a fairly extensive web search looking for courses that offered what I was interested in, but didnt find much." ></td>
	<td class="line x" title="199:202	I used the proceedings of the ACL-02 workshop on teaching NLP (where I learned about NLTK) as well as the NLP Universe." ></td>
	<td class="line x" title="200:202	I think it would be a good idea to start an archive of teaching resources; ACM SIGCHI is in the midst of creating such an educational digital library and this example is worth studying.9 Acknowledgements Thanks to Preslav Nakov, Andrew Fiore, and Barbara Rosario for their help with the class, and for all the students who took the class." ></td>
	<td class="line x" title="201:202	Thanks also to Steven Bird and Edward Loper for developing and sharing NLTK, and for their generous time and help with the system during the course of the class." ></td>
	<td class="line x" title="202:202	This work was supported in part by NSF DBI-0317510." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0829
Competitive Grouping In Integrated Phrase Segmentation And Alignment Model
Zhang, Ying;Vogel, Stephan;"></td>
	<td class="line x" title="1:80	Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 159162, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:80	cAssociation for Computational Linguistics, 2005 Competitive Grouping in Integrated Phrase Segmentation and Alignment Model Ying Zhang Stephan Vogel Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 {joy+,vogel+}@cs.cmu.edu Abstract This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model." ></td>
	<td class="line x" title="3:80	ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do." ></td>
	<td class="line x" title="4:80	Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach." ></td>
	<td class="line x" title="5:80	1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al. , 1999; Koehn et al. , 2003) have been shown to outperform word-to-word translation models (Brown et al. , 1993)." ></td>
	<td class="line x" title="6:80	Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path." ></td>
	<td class="line x" title="7:80	The Integrated Segmentation and Alignment (ISA) model (Zhang et al. , 2003) does not require such word alignment." ></td>
	<td class="line x" title="8:80	ISA segments the sentence into phrases and finds their alignment simultaneously." ></td>
	<td class="line x" title="9:80	ISA is simple and fast." ></td>
	<td class="line x" title="10:80	Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training." ></td>
	<td class="line x" title="11:80	In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment." ></td>
	<td class="line x" title="12:80	2 Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f,e) and the marginal frequency C(f) = summationtexte C(f,e) and C(e) = summationtextf C(f,e)." ></td>
	<td class="line x" title="13:80	We can apply various statistical tests (Manning and Schutze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations." ></td>
	<td class="line oc" title="14:80	In the following sections, we will use 2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990)." ></td>
	<td class="line x" title="15:80	3 The Core of the Integrated Phrase Segmentation and Alignment The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm." ></td>
	<td class="line x" title="16:80	It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens fi in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be es translation is the one for which the likelihood L(f,e) of translational equivalence is highest." ></td>
	<td class="line x" title="17:80	The simplicity of this algorithm depends on a one-to-one alignment assumption." ></td>
	<td class="line x" title="18:80	Each word translates to at most one other word." ></td>
	<td class="line x" title="19:80	Thus when one pair {f,e} is linked, neither f nor e can be aligned with any other words." ></td>
	<td class="line x" title="20:80	This assumption renders CLA unusable in phrase level alignment." ></td>
	<td class="line x" title="21:80	159 We propose an extension, the competitive grouping, as the core component in the ISA model." ></td>
	<td class="line x" title="22:80	3.1 Competitive Grouping Algorithm (CGA) The key modification to the competitive linking algorithm is to make it less greedy." ></td>
	<td class="line x" title="23:80	When a word pair is found to be the winner of the competition, we allow it to invite its neighbors to join the winners club and group them together as an aligned phrase pair." ></td>
	<td class="line x" title="24:80	The one-to-one assumption is thus discarded in CGA." ></td>
	<td class="line x" title="25:80	In addition, we introduce the locality assumption for phrase alignment." ></td>
	<td class="line x" title="26:80	Locality states that a source phrase of adjacent words can only be aligned to a target phrase composed of adjacent words." ></td>
	<td class="line x" title="27:80	This is not true of most language pairs in cases such as the relative clause, passive tense, and prepositional clause, etc. however this assumption renders the problem tractable." ></td>
	<td class="line x" title="28:80	Here is a description of CGA: For a sentence pair {f,e}, represent the word pair statistics for each word pair {f,e} in a two dimensional matrix LIJ, where L(i,j) = 2(fi,ej) in our implementation." ></td>
	<td class="line x" title="29:80	1 Figure 1: Expanding the current phrase pair Denote an aligned phrase pair {f,e} as a tuple [istart,iend,jstart,jend] where f is fistart,fistart+1,,fiend and similarly for e. 1." ></td>
	<td class="line x" title="30:80	Find i and j such that L(i,j) is the highest." ></td>
	<td class="line x" title="31:80	Create a seed phrase pair [i,i,j,j] which is simply the word pair {fi,ej} itself." ></td>
	<td class="line x" title="32:80	2." ></td>
	<td class="line x" title="33:80	Expand the current phrase pair [istart,iend,jstart,jend] to the neighboring territory to include adjacent source and target words in the phrase alignment group." ></td>
	<td class="line x" title="34:80	There 12 statistics were found to be more discriminative in our experiments than other symmetric word association measures, such as the averaged mutual information, 2 statistics and Dicecoefficient." ></td>
	<td class="line x" title="35:80	are 8 ways to group new words into the phrase pair." ></td>
	<td class="line x" title="36:80	For example, one can expand to the north by including an additional source word fistart1 to be aligned with all the target words in the current group; or one can expand to the northeast by including fistart1 and ejend+1 (Figure 1)." ></td>
	<td class="line x" title="37:80	Two criteria have to be satisfied for each expansion: (a) If a new source word fiprime is to be grouped, maxjstartjjend L(iprime,j) should be no smaller than max1jJ L(iprime,j)." ></td>
	<td class="line x" title="38:80	Since CGA is a greedy algorithm as described below, this is to guarantee that fiprime will not regret the decision of joining the phrase pair because it does not have other better target words to be aligned with." ></td>
	<td class="line x" title="39:80	Similar constraint is applied if a new target word ejprime is to be grouped." ></td>
	<td class="line x" title="40:80	(b) The highest value in the newly-expanded area needs to be similar to the seed value L(i,j)." ></td>
	<td class="line x" title="41:80	Expand the current phrase pair to the largest extend possible as long as both criteria are satisfied." ></td>
	<td class="line x" title="42:80	3." ></td>
	<td class="line x" title="43:80	The locality assumption means that the aligned phrase cannot be aligned again." ></td>
	<td class="line x" title="44:80	Therefore, all the source and target words in the phrase pair are marked as invalid and will be skipped in the following steps." ></td>
	<td class="line x" title="45:80	4." ></td>
	<td class="line x" title="46:80	If there is another valid pair {fi,ej}, then repeat from Step 1." ></td>
	<td class="line x" title="47:80	Figure 2 and Figure 3 show a simple example of applying CGA on the sentence pair {je declare reprise la session/i declare resumed the session}." ></td>
	<td class="line x" title="48:80	Figure 2: Seed pair {je / i}, no expansion allowed 160 Figure 3: Seed pair {session/session}, expanded to {la session/the session} 3.2 Exploring all possible groupings The similarity criterion 2-(b) described previously is used to control the granularity of phrase pairs." ></td>
	<td class="line x" title="49:80	In cases where the pairs {f1f2,e1e2}, {f1,e1} and {f2,e2} are all valid translations pairs, similarity is used to control whether we want to align {f1f2,e1e2} as one phrase pair or two shorter ones." ></td>
	<td class="line x" title="50:80	The granularity of the phrase pairs is hard to optimize especially when the test data is unknown." ></td>
	<td class="line x" title="51:80	On the one hand, we prefer long phrases since interaction among the words in the phrase, for example word sense, morphology and local reordering could be encapsulated." ></td>
	<td class="line x" title="52:80	On the other hand, long phrase pairs are less likely to occur in the test data than the shorter ones and may lead to low coverage." ></td>
	<td class="line x" title="53:80	To have both long and short phrases in the alignment, we apply a range of similarity thresholds for each of the expansion operations." ></td>
	<td class="line x" title="54:80	By applying a low similarity threshold, the expanded phrase pairs tend to be large, while a higher similarity threshold results in shorter phrase pairs." ></td>
	<td class="line x" title="55:80	As described above, CGA is a greedy algorithm and the expansion of the seed pair restricts the possible alignments for the rest of the sentence." ></td>
	<td class="line x" title="56:80	Figure 4 shows an example as we explore all the possible grouping choices in a depth-first search." ></td>
	<td class="line x" title="57:80	In the end, all unique phrase pairs along the path traveled are output as phrase translation candidates for the current sentence pair." ></td>
	<td class="line x" title="58:80	3.3 Phrase translation probabilities Each aligned phrase pair {f,e} is assigned a likelihood score L(f,e), defined as: summationtext i maxj logL(fi,ej)+ summationtext j maxi logL(fi,ej) |f|+|e| where i ranges over all words in f and similarly j in e. Given the collected phrase pairs and their likelihood, we estimate the phrase translation probability Figure 4: Depth-first itinerary of all possible grouping choices." ></td>
	<td class="line x" title="59:80	by their weighted frequency: P(f|e) = count( f,e)L(f,e) summationtext f count(f,e)L(f,e) No smoothing is applied to the probabilities." ></td>
	<td class="line x" title="60:80	4 Learning co-occurrence information In most cases, word alignment information is not given and is treated as a hidden parameter in the training process." ></td>
	<td class="line x" title="61:80	We initialize a word pair cooccurrence frequency by assuming uniform alignment for each sentence pair, i.e. for sentence pair (f,e) where f has I words and e has J words, each word pair {f,e} is considered to be aligned with frequency 1IJ. These co-occurrence frequencies will be accumulated over the whole corpus to calculate the initial L(f,e)." ></td>
	<td class="line x" title="62:80	Then we iterate the ISA model: 1." ></td>
	<td class="line x" title="63:80	Apply the competitive grouping algorithm to each sentence pair to find all possible phrase pairs." ></td>
	<td class="line x" title="64:80	2." ></td>
	<td class="line x" title="65:80	For each identified phrase pair {f,e}, increase the co-occurrence counts for all word pairs inside {f,e} with weight 1|f||e|." ></td>
	<td class="line x" title="66:80	3." ></td>
	<td class="line x" title="67:80	Calculate L(f,e) again and goto Step 1 for several iterations." ></td>
	<td class="line x" title="68:80	5 Experiments We participated the shared task in the WPT05 workshop2 and applied ISA to all four language pairs 2http://www.statmt.org/wpt05/mt-shared-task/ 161 (French-English, Finnish-English, German-English and Spanish-English)." ></td>
	<td class="line x" title="69:80	Table 1 shows the n-gram coverage of the dev-test set." ></td>
	<td class="line x" title="70:80	French and Spanish data are better covered by the training data compared to the German and Finnish sets." ></td>
	<td class="line x" title="71:80	Since our phrase alignment is constrained by the locality assumption and we can only extract phrase pairs of adjacent words, lower n-gram coverage will result in lower translation scores." ></td>
	<td class="line x" title="72:80	We used the training data Dev-test DE ES FI FR N=1 99.2 99.6 98.2 99.8 N=2 88.2 93.3 73.0 94.7 N=3 59.4 71.7 38.2 76.0 N=4 30.0 42.9 17.0 50.6 N=5 13.0 21.7 6.8 29.8 N=16 (8) (65) (1) (101) N=19 (1) (23) (34) N=23 (1) (1) Table 1: Percentage of dev-test n-grams covered by the training data." ></td>
	<td class="line x" title="73:80	Numbers in parenthesis are the actual counts of n-gram tokens in the dev-test data." ></td>
	<td class="line x" title="74:80	and the language model as provided and manually tuned the parameters of the Pharaoh decoder3 to optimize BLEU scores." ></td>
	<td class="line x" title="75:80	Table 2 shows the translation results on the dev-test and the test set of WPT05." ></td>
	<td class="line x" title="76:80	The BLEU scores appear comparable to those of other state-of-the-art phrase alignment systems, in spite of the simplicity of the ISA model and ease of training." ></td>
	<td class="line x" title="77:80	DE ES FI FR Dev-test 18.63 26.20 12.88 26.20 Test 18.93 26.14 12.66 26.71 Table 2: BLEU scores of ISA in WPT05 6 Conclusion In this paper, we introduced the competitive grouping algorithm which is at the core of the ISA phrase alignment model." ></td>
	<td class="line x" title="78:80	As an extension to the competitive linking algorithm which is used for word-to-word alignment, CGA overcomes the assumption of oneto-one mapping and makes it possible to align phrase 3http://www.isi.edu/licensed-sw/pharaoh/ pairs." ></td>
	<td class="line x" title="79:80	Despite its simplicity, the ISA model has achieved competitive translation results." ></td>
	<td class="line x" title="80:80	We plan to release ISA toolkit4 to the community in the near future." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1036
Enhancing Electronic Dictionaries With An Index Based On Associations
Ferret, Olivier;Zock, Michael;"></td>
	<td class="line x" title="1:169	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 281288, Sydney, July 2006." ></td>
	<td class="line x" title="2:169	c2006 Association for Computational Linguistics Enhancing electronic dictionaries with an index based on associations Olivier Ferret CEA LIST/LIC2M 18 Route du Panorama F-92265 Fontenay-aux-Roses ferreto@zoe.cea.fr Michael Zock 1 LIF-CNRS 163 Avenue de Luminy F-13288 Marseille Cedex 9 michael.zock@lif.univ-mrs.fr Abstract A good dictionary contains not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information." ></td>
	<td class="line x" title="3:169	Information access depends crucially on the quality of the index." ></td>
	<td class="line x" title="4:169	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for." ></td>
	<td class="line x" title="5:169	To this end we suggest to add to an existing electronic resource an index based on the notion of association." ></td>
	<td class="line x" title="6:169	We will also present preliminary work of how a subset of such associations, for example, topical associations, can be acquired by filtering a network of lexical co-occurrences extracted from a corpus." ></td>
	<td class="line x" title="7:169	1 Introduction A dictionary user typically pursues one of two goals (Humble, 2001): as a decoder (reading, listening), he may look for the definition or the translation of a specific target word, while as an encoder (speaker, writer) he may want to find a word that expresses well not only a given concept, but is also appropriate in a given context." ></td>
	<td class="line x" title="8:169	Obviously, readers and writers come to the dictionary with different mindsets, information and expectations concerning input and output." ></td>
	<td class="line x" title="9:169	While the decoder can provide the word he wants additional information for, the encoder (language producer) provides the meaning of a word for which he lacks the corresponding form." ></td>
	<td class="line x" title="10:169	In sum, users with different goals need access to different indexes, one that is based on form (decoding), 1 In alphabetical order the other being based on meaning or meaning relations (encoding)." ></td>
	<td class="line x" title="11:169	Our concern here is more with the encoder, i.e. lexical access in language production, a feature largely neglected in lexicographical work." ></td>
	<td class="line x" title="12:169	Yet, a good dictionary contains not only many entries and a lot of information concerning each one of them, but also efficient means to reveal the stored information." ></td>
	<td class="line x" title="13:169	Because, what is a huge dictionary good for, if one cannot access the information it contains?" ></td>
	<td class="line x" title="14:169	2 Lexical access on the basis of what: concepts (i.e. meanings) or words?" ></td>
	<td class="line x" title="15:169	Broadly speaking, there are two views concerning lexicalization: the process is conceptuallydriven (meaning, or parts of it are the starting point) or lexically-driven 2 : the target word is accessed via a source word." ></td>
	<td class="line x" title="16:169	This is typically the case when we are looking for a synonym, antonym, hypernym (paradigmatic associations), or any of its syntagmatic associates (red-rose, coffee-black), the kind of association we will be concerned with here." ></td>
	<td class="line x" title="17:169	Yet, besides conceptual knowledge, people seem also to know a lot of things concerning the lexical form (Brown and Mc Neill, 1966): number of syllables, beginning/ending of the target word, part of speech (noun, verb, adjective, etc.), origin (Greek or Latin), gender (Vigliocco et al. , 2 Of course, the input can also be hybrid, that is, it can be composed of a conceptual and a linguistic component." ></td>
	<td class="line x" title="18:169	For example, in order to express the notion of intensity, MAGN in MelAuks theory (MelAuk et al. , 1995), a speaker or writer has to use different words (very, seriously, high) depending on the form of the argument (ill, wounded, price), as he says very ill, seriously wounded, high price." ></td>
	<td class="line x" title="19:169	In each case he expresses the very same notion, but by using a different word." ></td>
	<td class="line x" title="20:169	While he could use the adverb very for qualifying the state of somebodys health (he is ill), he cannot do so when qualifying the words injury or price." ></td>
	<td class="line x" title="21:169	Likewise, he cannot use this specific adverb to qualify the noun illness." ></td>
	<td class="line x" title="22:169	281 1997)." ></td>
	<td class="line x" title="23:169	While in principle, all this information could be used to constrain the search space, we will deal here only with one aspect, the words relations to other concepts or words (associative knowledge)." ></td>
	<td class="line x" title="24:169	Suppose, you were looking for a word expressing the following ideas: domesticated animal, producing milk suitable for making cheese." ></td>
	<td class="line x" title="25:169	Suppose further that you knew that the target word was neither cow, buffalo nor sheep." ></td>
	<td class="line x" title="26:169	While none of this information is sufficient to guarantee the access of the intended word goat, the information at hand (part of the definition) could certainly be used 3." ></td>
	<td class="line x" title="27:169	Besides this type of information, people often have other kinds of knowledge concerning the target word." ></td>
	<td class="line x" title="28:169	In particular, they know how the latter relates to other words." ></td>
	<td class="line x" title="29:169	For example, they know that goats and sheep are somehow connected, sharing a great number of features, that both are animals (hypernym), that sheep are appreciated for their wool and meat, that they tend to follow each other blindly, etc. , while goats manage to survive, while hardly eating anything, etc. In sum, people have in their mind a huge lexico-conceptual network, with words 4, concepts or ideas being highly interconnected." ></td>
	<td class="line x" title="30:169	Hence, any one of them can evoke the other." ></td>
	<td class="line x" title="31:169	The likelihood for this to happen depends on such factors as frequency (associative strength), saliency and distance (direct vs. indirect access)." ></td>
	<td class="line x" title="32:169	As one can see, associations are a very general and powerful mechanism." ></td>
	<td class="line x" title="33:169	No matter what we hear, read or say, anything is likely to remind us of something else." ></td>
	<td class="line x" title="34:169	This being so, we should make use of it." ></td>
	<td class="line x" title="35:169	3 For some concrete proposals going in this direction, see dictionaries offering reverse lookup: http://www.ultralingua." ></td>
	<td class="line x" title="36:169	net/,http://www.onelook.com/reverse-dictionary.shtml." ></td>
	<td class="line x" title="37:169	4 Of course, one can question the very fact that people store words in their mind." ></td>
	<td class="line x" title="38:169	Rather than considering the human mind as a wordstore one might consider it as a wordfactory." ></td>
	<td class="line x" title="39:169	Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (Levelt et al. , 1999) one gets the impression that words are synthesized rather than located and call up." ></td>
	<td class="line x" title="40:169	In this case one might conclude that rather than having words in our mind we have a set of highly distributed, more or less abstract information." ></td>
	<td class="line x" title="41:169	By propagating energy rather than data (as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever), that we propagate signals, activating ultimately certain peripherical organs (larynx, tongue, mouth, lips, hands) in such a way as to produce movements or sounds, that, not knowing better, we call words." ></td>
	<td class="line x" title="42:169	3 Accessing the target word by navigating in a huge associative network If one agrees with what we have just said, one could view the mental lexicon as a huge semantic network composed of nodes (words and concepts) and links (associations), with either being able to activate the other 5 . Finding a word involves entering the network and following the links leading from the source node (the first word that comes to your mind) to the target word (the one you are looking for)." ></td>
	<td class="line x" title="43:169	Suppose you wanted to find the word nurse (target word), yet the only token coming to your mind is hospital." ></td>
	<td class="line x" title="44:169	In this case the system would generate internally a graph with the source word at the center and all the associated words at the periphery." ></td>
	<td class="line x" title="45:169	Put differently, the system would build internally a semantic network with hospital in the center and all its associated words as satellites (see Figure 1, next page)." ></td>
	<td class="line x" title="46:169	Obviously, the greater the number of associations, the more complex the graph." ></td>
	<td class="line x" title="47:169	Given the diversity of situations in which a given object may occur we are likely to build many associations." ></td>
	<td class="line x" title="48:169	In other words, lexical graphs tend to become complex, too complex to be a good representation to support navigation." ></td>
	<td class="line x" title="49:169	Readability is hampered by at least two factors: high connectivity (the great number of links or associations emanating from each word), and distribution: conceptually related nodes, that is, nodes activated by the same kind of association are scattered around, that is, they do not necessarily occur next to each other, which is quite confusing for the user." ></td>
	<td class="line x" title="50:169	In order to solve this problem, we suggest to display by category (chunks) all the words linked by the same kind of association to the source word (see Figure 2)." ></td>
	<td class="line x" title="51:169	Hence, rather than displaying all the connected words as a flat list, we suggest to present them in chunks to allow for categorial search." ></td>
	<td class="line x" title="52:169	Having chosen a category, the user will be presented a list of words or categories from which he must choose." ></td>
	<td class="line x" title="53:169	If the target word is in the category chosen by the user (suppose he looked for a hypernym, hence he checked the ISA-bag), search stops, otherwise it continues." ></td>
	<td class="line x" title="54:169	The user could choose either another category (e.g. AKO or TIORA), or a word in the current list, which would then become the new starting point." ></td>
	<td class="line x" title="55:169	5 While the links in our brain may only be weighted, they need to be labelled to become interpretable for human beings using them for navigational purposes in a lexicon." ></td>
	<td class="line x" title="56:169	282 DENTIST assistant near-synonym GYNECOLOGIST PHYSICIAN HEALTH INSTITUTION CLINIC DOCTOR SANATORIUM PSYCHIATRIC HOSPITAL MILITARY HOSPITAL ASYLUM treat AOK take care of treat HOSPITAL PATIENT INMATE TIORA synonym ISA A OK A OK AOK AOK ISA ISA ISA ISA ISA TIORA TIORA nurse Internal Representation Figure 1: Search based on navigating in a network (internal representation) AKO: a kind of; ISA: subtype; TIORA: Typically Involved Object, Relation or Actor." ></td>
	<td class="line x" title="57:169	list of potential target words (LOPTW) s ou r c e w or d li n k li n k li n k li n k li n k LO P TW LO P TW l i st of po t e nt i a l t a r ge t w o r ds ( LO P TW )  . A b s t r a c t r e pr es en t a t i on of t he s e a r c h g r a p h ho s pi t a l TI O R A IS A AK O c lin ic, s an a to r i um, .  mi lit ar y h os pit al, p sy chi at ric h osp it al in ma t eS YN O N YM nur s e doc to r,  . pat i e n t  . A c on cr e t e e xam pl e F ig u r e 2 : P r opos e d c a ndi da t e s, g r oupe d by f a m il y, i. e . a c c or di ng t o t he na t ur e of t he l i nk A s one c a n s e e, t he f a c t t ha t t he l i nk s a r e l a be l e d h as so m e v er y i m p o r t a n t co n seq u e n ces: ( a ) W h i l e m a i nt a i ni ng t he pow e r of a hi g hl y c o n n e c t e d gr a p h ( p os s i b l e c yc l i c n a vi ga t i on ), it h a s a t th e in te r f a c e le v e l t h e s im p l ic i ty o f a t r e e : e a c h no de po i nt s on l y t o da t a of t he sam e t y p e, i. e . t o t he s a m e k i nd of a s s oc i a t i on." ></td>
	<td class="line x" title="58:169	( b) W i t h w or d s be i ng pr e s e nt e d i n c l us t e r s, na v i g a t i on c a n be a c c om pl i s he d by c l i c k i ng on t he a ppr opr i a t e c a t e g or y . T he a s s um pt i on be i ng t h a t t he us e r g e ne r a l l y k now s t o w hi c h c a t e g or y t h e t a r g e t w or d be l ong s ( o r a t l ea s t, h e can r eco g n i z e w i t h i n w h i ch o f t h e l is te d c a te g o r i e s i t fa l l s ), a n d th a t c a t e g o r i c a l sear c h i s i n p r i n ci p l e f a s t er t h an se a r c h i n a h u g e l is t o f u n o rd e re d (o r, a l p h a b e t ic a l l y o rd e re d ) w or ds 6 . O bv i ous l y, i n or d e r t o a l l ow f or t hi s k i nd o f ac ce ss, t h e r e so u r ce h as t o b e b u i l t ac co r d i n g l y . T h i s r e qui r e s a t l e a s t t w o t hi ng s : ( a ) i nde x i ng w or ds by t he a s s oc i a t i on s t he y e v ok e, ( b) i de n t i 6 Ev e n th o u g h v e r y i m p o r ta n t, a t th is s ta g e w e s h a ll n o t w o r r y t oo m uc h f or t he na m e s g i v e n t o t he l i nk s . I nde e d, o n e m ig h t q u e s tio n n e a r ly a ll o f t h e m . W h a t is im p o r ta n t is th e u n d e r ly in g r a tio n a l: h e lp u s e r s to n a v ig a te o n th e b a s is o f s y m b o lic a ll y q u a lif ie d lin k s . I n r e a lity a w h o le s e t o f w o r d s ( s y no ny m s, o f c our s e, b ut not o n l y ) c o ul d a m oun t t o a lin k, i. e . b e i t s co n cep t u al eq u i v al en t . 283 fying and labelling the most frequent/useful associations." ></td>
	<td class="line x" title="59:169	This is precisely our goal." ></td>
	<td class="line x" title="60:169	Actually, we propose to build an associative network by enriching an existing electronic dictionary (essentially) with (syntagmatic) associations coming from a corpus, representing the average citizens shared, basic knowledge of the world (encyclopaedia)." ></td>
	<td class="line x" title="61:169	While some associations are too complex to be extracted automatically by machine, others are clearly within reach." ></td>
	<td class="line x" title="62:169	We will illustrate in the next section how this can be achieved." ></td>
	<td class="line x" title="63:169	4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words." ></td>
	<td class="line x" title="64:169	Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al. , 1998) contain them." ></td>
	<td class="line x" title="65:169	However, as various researchers have pointed out (Harabagiu et al. , 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic." ></td>
	<td class="line x" title="66:169	These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario." ></td>
	<td class="line x" title="67:169	Word-pairs like doctorhospital, burglarpoliceman or planeairport, are examples in case." ></td>
	<td class="line x" title="68:169	The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998)." ></td>
	<td class="line x" title="69:169	Some of these links have been introduced more recently in WordNet via the domain relation." ></td>
	<td class="line x" title="70:169	Yet their number remains still very small." ></td>
	<td class="line x" title="71:169	For instance, WordNet 2.1 does not contain any of the three associations mentioned here above, despite their high frequency." ></td>
	<td class="line x" title="72:169	The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale." ></td>
	<td class="line x" title="73:169	This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet." ></td>
	<td class="line x" title="74:169	In (Harabagiu & Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets." ></td>
	<td class="line x" title="75:169	Other researchers used external sources: Mandala et al.(1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al.(2001) built topic signatures from texts in relation to synsets; Magnini and Cavagli (2000) annotated the synsets with Subject Field Codes." ></td>
	<td class="line x" title="78:169	This last idea has been taken up and extended by (Avancini et al. , 2003) who expanded the domains built from this annotation." ></td>
	<td class="line x" title="79:169	Despite the improvements, all these approaches are limited by the fact that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets)." ></td>
	<td class="line x" title="80:169	While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks." ></td>
	<td class="line x" title="81:169	Moreover, these methods attempt to learn topical knowledge from a lexical network rather than topical relations." ></td>
	<td class="line x" title="82:169	Since our goal is different, we have chosen not to rely on any significant resource, all the more as we would like our method to be applicable to a wide array of languages." ></td>
	<td class="line x" title="83:169	In consequence, we took an incremental approach (Ferret, 2006): starting from a network of lexical co-occurrences 7 collected from a large corpus, we used these latter to select potential topical relations by using a topical analyzer." ></td>
	<td class="line x" title="84:169	4.2 From a network of co-occurrences to a set of Topical Units We start by extracting lexical co-occurrences from a corpus to build a network." ></td>
	<td class="line oc" title="85:169	To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts." ></td>
	<td class="line x" title="86:169	The parameters of this extraction were set in such a way as to catch the most obvious topical relations: the window was fairly large (20-words wide), and while it took text boundaries into account, it ignored the order of the co-occurrences." ></td>
	<td class="line oc" title="87:169	Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words." ></td>
	<td class="line x" title="88:169	The finite size of the corpus allows us to normalize this measure in line with the maximal mutual information relative to the corpus." ></td>
	<td class="line x" title="89:169	This network is used by TOPICOLL (Ferret, 2002), a topic analyzer, which performs simultaneously three tasks, relevant for this goal: it segments texts into topically homogeneous segments; it selects in each segment the most representative words of its topic; 7 Such a network is only another view of a set of cooccurrences: its nodes are the co-occurrent words and its edges are the co-occurrence relations." ></td>
	<td class="line x" title="90:169	284 it proposes a restricted set of words from the co-occurrence network to expand the selected words of the segment." ></td>
	<td class="line x" title="91:169	These three tasks rely on a common mechanism: a window is moved over the text to be analyzed in order to limit the focus space of the analysis." ></td>
	<td class="line x" title="92:169	This latter contains a lemmatized version of the texts plain words." ></td>
	<td class="line x" title="93:169	For each position of this window, we select only words of the cooccurrence network that are linked to at least three other words of the window (see Figure 3)." ></td>
	<td class="line x" title="94:169	This leads to select both words that are in the window (first order co-occurrents) and words coming from the network (second order cooccurrents)." ></td>
	<td class="line x" title="95:169	The number of links between the selected words of the network, called expansion words, and those of the window is a good indicator of the topical coherence of the windows content." ></td>
	<td class="line x" title="96:169	Hence, when their number is small, a segment boundary can be assumed." ></td>
	<td class="line x" title="97:169	This is the basic principle underlying our topic analyzer." ></td>
	<td class="line x" title="98:169	0.14 0.21 0.10 0.18 0.13 0.17 w5w4w3w2w1 0.48 = p w3 x0.18+p w4 x0.13 +p w5 x0.17 selected word from the co-occurrence network (with its weight) 1.0 word from text (with p its weight in the window, equal to 0.21 link in the co-occurrence network (with its cohesion value) 1.0 1.0 1.0 1.0 1.0 wi, n1 n2 1.0 for all words of the window in this example) 0.48 Figure 3: Selection and weighting of words from the co-occurrence network The words selected for each position of the window are summed, to keep only those occurring in 75% of the positions of the segment." ></td>
	<td class="line x" title="99:169	This allows reducing the number of words selected from non-topical co-occurrences." ></td>
	<td class="line x" title="100:169	Once a corpus has been processed by TOPICOLL, we obtain a set of segments and a set of expansion words for each one of them." ></td>
	<td class="line x" title="101:169	The association of the selected words of a segment and its expansion words is called a Topical Unit." ></td>
	<td class="line x" title="102:169	Since both sets of words are selected for reasons of topical homogeneity, their co-occurrence is more likely to be a topical relation than in our initial network." ></td>
	<td class="line x" title="103:169	4.3 Filtering of Topical Units Before recording the co-occurrences in the Topical Units built in this way, the units are filtered twice." ></td>
	<td class="line x" title="104:169	The first filter aims at discarding heterogeneous Topical Units, which can arise as a side effect of a document whose topics are so intermingled that it is impossible to get a reliable linear segmentation of the text." ></td>
	<td class="line x" title="105:169	We consider that this occurs when for a given text segment, no word can be selected as a representative of the topic of the segment." ></td>
	<td class="line x" title="106:169	Moreover, we only keep the Topical Units that contain at least two words from their original segment." ></td>
	<td class="line x" title="107:169	A topic is defined here as a configuration of words." ></td>
	<td class="line x" title="108:169	Note that the identification of such a configuration cannot be based solely on a single word." ></td>
	<td class="line x" title="109:169	Text words Expansion words surveillance (watch) police_judiciaire (judiciary police) tlphonique (telephone) crouer (to imprison) juge (judge) garde__vue (police custody) policier (policeman) coute_tlphonique (phone tapping) brigade (squad) juge_dinstruction (examining judge) enqute (investigation) contrle_judiciaire (judicial review) placer (to put) Table 1: Content of a filtered Topical Unit The second filter is applied to the expansion words of each Topical Unit to increase their topical homogeneity." ></td>
	<td class="line x" title="110:169	The principle of the filtering of these words is the same as the principle of their selection described in Section 4.2: an expansion word is kept if it is linked in the co-occurrence network to at least three text words of the Topical Unit." ></td>
	<td class="line x" title="111:169	Moreover, a selective threshold is applied to the frequency and the cohesion of the cooccurrences supporting these links: only cooccurrences whose frequency and cohesion are respectively higher or equal to 15 and 0.15 are used." ></td>
	<td class="line x" title="112:169	For instance in Table 1, which shows an example of a Topical Unit after its filtering, crouer (to imprison) is selected, because it is linked in the co-occurrence network to the following words of the text: juge (judge): 52 (frequency)  0.17 (cohesion) policier (policeman): 56  0.17 enqute (investigation): 42  0.16 285 word freq." ></td>
	<td class="line x" title="113:169	word freq." ></td>
	<td class="line x" title="114:169	word freq." ></td>
	<td class="line x" title="115:169	word freq." ></td>
	<td class="line x" title="116:169	scne (stage) 884 thtral (dramatic) 62 cynique (cynical) 26 scnique (theatrical) 14 thtre (theater) 679 scnariste (scriptwriter) 51 miss (miss) 20 Chabol (Chabol) 13 ralisateur (director) 220 comique (comic) 51 parti_pris (bias) 16 Tchekov (Tchekov) 13 cinaste (film-marker) 135 oscar (oscar) 40 monologue (monolog) 15 allocataire (beneficiary) 13 comdie (comedy) 104 film_amricain (american film) 38 revisiter (to revisit) 14 satirique (satirical) 13 costumer (to dress up) 63 hollywoodien (Hollywood) 30 gros_plan (close-up) 14 Table 2: Co-occurrents of the word acteur (actor) with a cohesion of 0.16 (the co-occurrents removed by our filtering method are underlined) 4.4 From Topical Units to a network of topical relations After the filtering, a Topical Unit gathers a set of words supposed to be strongly coherent from the topical point of view." ></td>
	<td class="line x" title="117:169	Next, we record the cooccurrences between these words for all the Topical Units remaining after filtering." ></td>
	<td class="line x" title="118:169	Hence, we get a large set of topical co-occurrences, despite the fact that a significant number of nontopical co-occurrences remains, the filtering of Topical Units being an unsupervised process." ></td>
	<td class="line x" title="119:169	The frequency of a co-occurrence in this case is given by the number of Topical Units containing both words simultaneously." ></td>
	<td class="line x" title="120:169	No distinction concerning the origin of the words of the Topical Units is made." ></td>
	<td class="line x" title="121:169	The network of topical co-occurrences built from Topical Units is a subset of the initial network." ></td>
	<td class="line x" title="122:169	However, it also contains co-occurrences that are not part of it, i.e. co-occurrences that were not extracted from the corpus used for setting the initial network or co-occurrences whose frequency in this corpus was too low." ></td>
	<td class="line x" title="123:169	Only some of these new co-occurrences are topical." ></td>
	<td class="line x" title="124:169	Since it is difficult to estimate globally which ones are interesting, we have decided to focus our attention only on the co-occurrences of the topical network already present in the initial network." ></td>
	<td class="line x" title="125:169	Thus, we only use the network of topical cooccurrences as a filter for the initial cooccurrence network." ></td>
	<td class="line x" title="126:169	Before doing so, we filter the topical network in order to discard cooccurrences whose frequency is too low, that is, co-occurrences that are unstable and not representative." ></td>
	<td class="line x" title="127:169	From the use of the final network by TOPICOLL (see Section 4.5), we set the threshold experimentally to 5." ></td>
	<td class="line x" title="128:169	Finally, the initial network is filtered by keeping only co-occurrences present in the topical network." ></td>
	<td class="line x" title="129:169	Their frequency and cohesion are taken from the initial network." ></td>
	<td class="line x" title="130:169	While the frequencies given by the topical network are potentially interesting for their topical significance, we do not use them because the results of the filtering of Topical Units are too hard to evaluate." ></td>
	<td class="line x" title="131:169	4.5 Results and evaluation We applied the method described here to an initial co-occurrence network extracted from a corpus of 24 months of Le Monde, a major French newspaper." ></td>
	<td class="line x" title="132:169	The size of the corpus was around 39 million words." ></td>
	<td class="line x" title="133:169	The initial network contained 18,958 words and 341,549 relations." ></td>
	<td class="line x" title="134:169	The first run produced 382,208 Topical Units." ></td>
	<td class="line x" title="135:169	After filtering, we kept 59% of them." ></td>
	<td class="line x" title="136:169	The network built from these Topical Units was made of 11,674 words and 2,864,473 co-occurrences." ></td>
	<td class="line x" title="137:169	70% of these cooccurrences were new with regard to the initial network and were discarded." ></td>
	<td class="line x" title="138:169	Finally, we got a filtered network of 7,160 words and 183,074 relations, which represents a cut of 46% of the initial network." ></td>
	<td class="line x" title="139:169	A qualitative study showed that most of the discarded relations are non-topical." ></td>
	<td class="line x" title="140:169	This is illustrated by Table 2, which gives the cooccurrents of the word acteur (actor) that are filtered by our method among its co-occurrents with a high cohesion (equal to 0.16)." ></td>
	<td class="line x" title="141:169	For instance, the words cynique (cynical) or allocataire (beneficiary) are cohesive co-occurrents of the 286 word actor, even though they are not topically linked to it." ></td>
	<td class="line x" title="142:169	These words are filtered out, while we keep words like gros_plan (close-up) or scnique (theatrical), which topically cohere with acteur (actor) despite their lower frequency than the discarded words." ></td>
	<td class="line x" title="143:169	Recall 8 Precision F1measure Error (P k ) 9 initial (I) 0.85 0.79 0.82 0.20 topical filtering (T) 0.85 0.79 0.82 0.21 frequency filtering (F) 0.83 0.71 0.77 0.25 Table 3: TOPICOLLs results with different networks In order to evaluate more objectively our work, we compared the quantitative results of TOPICOLL with the initial network and its filtered version." ></td>
	<td class="line x" title="144:169	The evaluation showed that the performance of the segmenter remains stable, even if we use a topically filtered network (see Table 3)." ></td>
	<td class="line x" title="145:169	Moreover, it became obvious that a network filtered only by frequency and cohesion performs significantly less well, even with a comparable size." ></td>
	<td class="line x" title="146:169	For testing the statistical significance of these results, we applied to the P k values a one-side t-test with a null hypothesis of equal means." ></td>
	<td class="line x" title="147:169	Levels lower or equal to 0.05 are considered as statistically significant: p val (I-T): 0.08 p val (I-F): 0.02 p val (T-F): 0.05 These values confirm that the difference between the initial network (I) and the topically filtered one (T) is actually not significant, whereas the filtering based on co-occurrence frequencies leads to significantly lower results, both compared to the initial network and the topically filtered one." ></td>
	<td class="line x" title="148:169	Hence, one may conclude that our 8 Precision is given by N t / N b and recall by N t / D, with D being the number of document breaks, N b the number of boundaries found by TOPICOLL and N t the number of boundaries that are document breaks (the boundary should not be farther than 9 plain words from the document break)." ></td>
	<td class="line x" title="149:169	9 P k (Beeferman et al. , 1999) evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm)." ></td>
	<td class="line x" title="150:169	method is an effective way of selecting topical relations by preference." ></td>
	<td class="line x" title="151:169	5 Discussion and conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access, a question initially addressed in (Zock, 2002) and (Zock and Bilac, 2004)." ></td>
	<td class="line x" title="152:169	We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader)." ></td>
	<td class="line x" title="153:169	It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it." ></td>
	<td class="line x" title="154:169	To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation." ></td>
	<td class="line x" title="155:169	Actually our basic proposal is to extend a resource like WordNet by adding certain links, in particular on the syntagmatic axis." ></td>
	<td class="line x" title="156:169	These links are associations, and their role consists in helping the encoder to find ideas (concepts/words) related to a given stimulus (brainstorming), or to find the word he is thinking of (word access)." ></td>
	<td class="line x" title="157:169	One problem that we are confronted with is to identify possible associations." ></td>
	<td class="line x" title="158:169	Ideally we would need a complete list, but unfortunately, this does not exist." ></td>
	<td class="line x" title="159:169	Yet, there is a lot of highly relevant information out there." ></td>
	<td class="line x" title="160:169	For example, Melcuks lexical functions (Melcuk, 1995), Fillmores FRAMENET 10, work on ontologies (CYC), thesaurus (Roget), WordNets (the original version from Princeton, various Euro-WordNets, BalkaNet), HowNet 11, the work done by MICRA, the FACTOTUM project 12, or the Wordsmyth dictionary/thesaurus 13 . Since words are linked via associations, it is important to reveal these links." ></td>
	<td class="line x" title="161:169	Once this is done, words can be accessed by following these links." ></td>
	<td class="line x" title="162:169	We have presented here some preliminary work for extracting an important subset of such links from texts, topical associations, which are generally absent from dictionaries or resources like WordNet." ></td>
	<td class="line x" title="163:169	An evaluation of the topic segmentation has shown that the relations extracted are sound from the topical point of view, and that they can be extracted automatically." ></td>
	<td class="line x" title="164:169	However, 10 http://www.icsi.berkeley.edu/~framenet/ 11 http://www.keenage.com/html/e_index.html 12 http://humanities.uchicago.edu/homes/MICRA/ 13 http://www.wordsmyth.com/ 287 they still contain too much noise to be directly exploitable by an end user for accessing a word in a dictionary." ></td>
	<td class="line x" title="165:169	One way of reducing the noise of the extracted relations would be to build from each text a representation of its topics and to record the co-occurrences in these representations rather than in the segments delimited by a topic segmenter." ></td>
	<td class="line x" title="166:169	This is a hypothesis we are currently exploring." ></td>
	<td class="line x" title="167:169	While we have focused here only on word access on the basis of (other) words, one should not forget that most of the time speakers or writers start from meanings." ></td>
	<td class="line x" title="168:169	Hence, we shall consider this point more carefully in our future work, by taking a serious look at the proposals made by Bilac et al.(2004); Durgar and Oflazer (2004), or Dutoit and Nugues (2002)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2033
Conceptual Coherence In The Generation Of Referring Expressions
Gatt, Albert;Van Deemter, Kees;"></td>
	<td class="line x" title="1:326	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 255262, Sydney, July 2006." ></td>
	<td class="line x" title="2:326	c2006 Association for Computational Linguistics Conceptual Coherence in the Generation of Referring Expressions Albert Gatt Department of Computing Science University of Aberdeen agatt@csd.abdn.ac.uk Kees van Deemter Department of Computing Science University of Aberdeen kvdeemte@csd.abdn.ac.uk Abstract One of the challenges in the automatic generation of referring expressions is to identify a set of domain entities coherently, that is, from the same conceptual perspective." ></td>
	<td class="line x" title="3:326	We describe and evaluate an algorithm that generates a conceptually coherent description of a target set." ></td>
	<td class="line x" title="4:326	The design of the algorithm is motivated by the results of psycholinguistic experiments." ></td>
	<td class="line x" title="5:326	1 Introduction Algorithms for the Generation of Referring Expressions (GRE) seek a set of properties that distinguish an intended referent from its distractors in a knowledge base." ></td>
	<td class="line x" title="6:326	Much of the GRE literature has focused on developing efficient content determination strategies that output the best available description according to some interpretation of the Gricean maxims (Dale and Reiter, 1995), especially Brevity." ></td>
	<td class="line x" title="7:326	Work on reference to sets has also proceeded within this general framework (van Deemter, 2002; Gardent, 2002; Horacek, 2004)." ></td>
	<td class="line x" title="8:326	One problem that has not received much attention is that of conceptual coherence in the generation of plural references, i.e. the ascription of related properties to elements of a set, so that the resulting description constitutes a coherent cover for the plurality." ></td>
	<td class="line x" title="9:326	As an example, consider a reference to {e1,e3} in Table 1 using the Incremental Algorithm (IA) (Dale and Reiter, 1995)." ></td>
	<td class="line x" title="10:326	IA searches along an ordered list of attributes, selecting properties of the intended referents that remove some distractors." ></td>
	<td class="line x" title="11:326	Assuming the ordering in the top row, IA would yield the postgraduate and the chef, which is fine in case occupation is the relevant attribute in the discourse, but otherwise is arguably worse than an alternative like the italian and the maltese, because it is more difficult to see what a postgraduate and a chef have in common." ></td>
	<td class="line x" title="12:326	type occupation nationality e1 man postgraduate maltese e2 man undergraduate greek e3 man chef italian Table 1: Example domain Such examples lead us to hypothesise the following constraint: Conceptual Coherence Constraint (CC): As far as possible, describe objects using related properties." ></td>
	<td class="line x" title="13:326	Related issues have been raised in the formal semantics literature." ></td>
	<td class="line x" title="14:326	Aloni (2002) argues that an appropriate answer to a question of the form Wh x? must conceptualise the different instantiations of x using a perspective which is relevant given the hearers information state and the context." ></td>
	<td class="line x" title="15:326	Kronfeld (1989) distinguishes a descriptions functional relevance  i.e. its success in distinguishing a referent  from its conversational relevance, which arises in part from implicatures." ></td>
	<td class="line x" title="16:326	In our example, describing e1 as the postgraduate carries the implicature that the entitys academic role is relevant." ></td>
	<td class="line x" title="17:326	When two entities are described using contrasting properties, say the student and the italian, the contrast may be misleading for the listener." ></td>
	<td class="line x" title="18:326	Any attempt to port these observations to the GRE scenario must do so without sacrificing logical completeness." ></td>
	<td class="line x" title="19:326	While a GRE algorithm should attempt to find the most coherent description available, it should not fail in the absence of a coherent set of properties." ></td>
	<td class="line x" title="20:326	This paper aims to achieve a dual goal." ></td>
	<td class="line x" title="21:326	First (2), we will show that the CC can be explained and modelled in terms of lexical semantic forces within a description, a claim supported by the results of two experiments." ></td>
	<td class="line x" title="22:326	Our focus on low-level, lexical, determinants of adequacy constitutes a departure from the standard Gricean view." ></td>
	<td class="line x" title="23:326	Second, we describe an algorithm 255 motivated by the experimental findings (3) which seeks to find the most coherent description available in a domain according to CC." ></td>
	<td class="line x" title="24:326	2 Empirical evidence We take as paradigmatic the case where a plural reference involves disjunction/union, that is, has the logical form x(p(x)q(x)), realised as a description of the form the N1 and the N2." ></td>
	<td class="line x" title="25:326	By hypothesis, the case where all referents can be described using identical properties (logically, a conjunction), is a limiting case of CC." ></td>
	<td class="line x" title="26:326	Previous work on plural anaphor processing has shown that pronoun resolution is easier when antecedents are ontologically similar (e.g. all humans) (Kaup et al. , 2002; Koh and Clifton, 2002)." ></td>
	<td class="line x" title="27:326	Reference to a heterogeneous set increases processing difficulty." ></td>
	<td class="line x" title="28:326	Our experiments extended these findings to full definite NP reference." ></td>
	<td class="line x" title="29:326	Throughout, we used a distributional definition of similarity, as defined by Lin (1998), which was found to be highly correlated to peoples preferences for disjunctive descriptions (Gatt and van Deemter, 2005)." ></td>
	<td class="line x" title="30:326	The similarity of two arbitrary objects a and b is a function of the information gained by giving a joint description of a and b in terms of what they have in common, compared to describing a and b separately." ></td>
	<td class="line x" title="31:326	The relevant data in the lexical domain is the grammatical environment in which words occur." ></td>
	<td class="line x" title="32:326	This information is represented as a set of triples rel,w,w, where rel is a grammatical relation, w the word of interest and w its co-argument in rel (e.g.  premodifies, dog, domestic )." ></td>
	<td class="line x" title="33:326	Let F(w) be a list of such triples." ></td>
	<td class="line oc" title="34:326	The information content of this set is defined as mutual information I(F(w)) (Church and Hanks, 1990)." ></td>
	<td class="line x" title="35:326	The similarity of two words w1 and w2, of the same grammatical category, is: (w1,w2) = 2I(F(w1)F(w2))I(F(w 1)) + I(F(w2)) (1) For example, if premodifies is one of the relevant grammatical relations, then dog and cat might occur several times in a corpus with the same premodifiers (tame, domestic, etc)." ></td>
	<td class="line x" title="36:326	Thus, (dog,cat) is large because in a corpus, they often occur in the same contexts and there is considerable information gain in a description of their common data." ></td>
	<td class="line x" title="37:326	Rather than using a hand-crafted ontology to infer similarity, this definition looks at real language Condition a b c distractor HDS spanner chisel plug thimble LDS toothbrush knife ashtray clock Figure 1: Conditions in Experiment 1 use." ></td>
	<td class="line x" title="38:326	It covers ontological similarity to the extent that ontologically similar objects are talked about in the same contexts, but also cuts across ontological distinctions (for example newspaper and journalist might turn out to be very similar)." ></td>
	<td class="line x" title="39:326	We use the information contained in the SketchEngine database1 (Kilgarriff, 2003), a largescale implementation of Lins theory based on the BNC, which contains grammatical triples in the form of Word Sketches for each word, with each triple accompanied by a salience value indicating the likelihood of occurrence of the word with its argument in a grammatical relation." ></td>
	<td class="line x" title="40:326	Each word also has a thesaurus entry, containing a ranked list of words of the same category, ordered by their similarity to the head word." ></td>
	<td class="line x" title="41:326	2.1 Experiment 1 In Experiment 1, participants were placed in a situation where they were buying objects from an online store." ></td>
	<td class="line x" title="42:326	They saw scenarios containing four pictures of objects, three of which (the targets) were identically priced." ></td>
	<td class="line x" title="43:326	Participants referred to them by completing a 2-sentence discourse: S1 The object1 and the object 2 cost amount." ></td>
	<td class="line x" title="44:326	S2 The object3 also costs amount." ></td>
	<td class="line x" title="45:326	If similarity is a constraint on referential coherence in plural references, then if two targets are similar (and dissimilar to the third), a plural reference to them in S1 should be more likely, with the third entity referred to in S2." ></td>
	<td class="line x" title="46:326	Materials, design and procedure All the pictures were artefacts selected from a set of drawings normed in a picture-naming task with British English speakers (Barry et al. , 1997)." ></td>
	<td class="line x" title="47:326	Each trial consisted of the four pictures arranged in an array on a screen." ></td>
	<td class="line x" title="48:326	Of the three targets (a, b, c), c was always an object whose name in the norms was dissimilar to that of a and b. The semantic similarity of (nouns denoting) a and b was manipulated as a factor with two levels: High Distributional Similarity (HDS) meant that b occurred among the top 50 most similar items to a in its Sketchengine thesaurus entry." ></td>
	<td class="line x" title="49:326	Low DS (LDS)) 1http://www.sketchengine.co.uk 256 meant that b did not occur in the top 500 entries for a. Examples are shown in Figure 2.1." ></td>
	<td class="line x" title="50:326	Visual Similarity (VS) of a and b was also controlled." ></td>
	<td class="line x" title="51:326	Pairs of pictures were first normed with a group who rated them on a 10-point scale based on their visual properties." ></td>
	<td class="line x" title="52:326	High-VS (HVS) pairs had a mean rating  6; Low-VS LVS) pairs had mean ratings  2." ></td>
	<td class="line x" title="53:326	Two sets of materials were constructed, for a total of 2 (DS)  2 (V S)  2 = 8 trials." ></td>
	<td class="line x" title="54:326	29 self-reported native or fluent speakers of English completed the experiment over the web." ></td>
	<td class="line x" title="55:326	To complete the sentences, participants clicked on the objects in the order they wished to refer to them." ></td>
	<td class="line x" title="56:326	Nouns appeared in the next available space2." ></td>
	<td class="line x" title="57:326	Results and discussion Responses were coded according to whether objects a and b were referred to in the plural subject of S1 (a + b responses) or not (ab responses)." ></td>
	<td class="line x" title="58:326	If our hypothesis is correct, there should be a higher proportion of a + b responses in the HDS condition." ></td>
	<td class="line x" title="59:326	We did not expect an effect of VS. In what follows, we report bysubjects Friedman analyses (21); by-items analyses (22); and by-subjects sign tests (Z) on proportions of responses for pairwise comparisons." ></td>
	<td class="line x" title="60:326	Response frequencies across conditions differed reliably by subjects (21 = 46.124,p <.001)." ></td>
	<td class="line x" title="61:326	The frequency of a + b responses in S1 was reliably higher than that of ab in the HDS condition (22 = 41.371,p < .001), but not the HVS condition (22 = 1.755,ns)." ></td>
	<td class="line x" title="62:326	Pairwise comparisons between HDS and LDS showed a significantly higher proportion of a + b responses in the former (Z = 4.48,p < .001); the difference was barely significant across VS conditions (Z = 1.9,p = .06)." ></td>
	<td class="line x" title="63:326	The results show that, given a clear choice of entities to refer to in a plurality, people are more likely to describe similar entities in a plural description." ></td>
	<td class="line x" title="64:326	However, these results raise two further questions." ></td>
	<td class="line x" title="65:326	First, given a choice of distinguishing properties for individuals making up a target set, will participants follow the predictions of the CC?" ></td>
	<td class="line x" title="66:326	(In other words, is distributional similarity relevant for content determination)?" ></td>
	<td class="line x" title="67:326	Second, does the similarity effect carry over to modifiers, such as adjectives, or is the CC exclusively a constraint on types?" ></td>
	<td class="line x" title="68:326	2Earler replications involving typing yielded parallel results and high conformity between the words used and those predicted by the picture norms." ></td>
	<td class="line x" title="69:326	Three millionaires with a passion for antiques were spotted dining at a London restaurant." ></td>
	<td class="line x" title="70:326	e1 One of the men, a Rumanian, is a dealeri." ></td>
	<td class="line x" title="71:326	e2 The second, a princej, is a collectori." ></td>
	<td class="line x" title="72:326	e3 The third, a dukej, is a bachelor." ></td>
	<td class="line x" title="73:326	The XXXX were both accompanied by servants, but the bachelor wasnt . Figure 2: Example discourses 2.2 Experiment 2 Experiment 2 was a sentence continuation task, designed to closely approximate content determination in GRE." ></td>
	<td class="line x" title="74:326	Participants saw a series of discourses, in which three entities (e1, e2, e3) were introduced, each with two distinguishing properties." ></td>
	<td class="line x" title="75:326	The final sentence in each discourse had a missing plural subject NP referring to two of these." ></td>
	<td class="line x" title="76:326	The context made it clear which of the three entities had to be referred to." ></td>
	<td class="line x" title="77:326	Our hypothesis was that participants would prefer to use semantically similar properties for the plural reference, even if dissimilar properties were also available." ></td>
	<td class="line x" title="78:326	Materials, design and procedure Materials consisted of 24 discourses, such as those in Figure 2.2." ></td>
	<td class="line x" title="79:326	After an initial introductory sentence, the 3 entities were introduced in separate sentences." ></td>
	<td class="line x" title="80:326	In all discourses, the pairs {e1,e2} and {e2,e3} could be described using either pairwise similar or dissimilar properties (similar pairs are coindexed in the figure)." ></td>
	<td class="line x" title="81:326	In half the discourses, the distinguishing properties of each entity were nouns; thus, although all three entities belonged to the same ontological category (e.g. all human), they had distinct types (e.g. duke, prince, bachelor)." ></td>
	<td class="line x" title="82:326	In the other half, entities were of the same type, that is the NPs introducing them had the same nominal head, but had distinguishing adjectival modifiers." ></td>
	<td class="line x" title="83:326	For counterbalancing, two versions of each discourse were constructed, such that, if {e1,e2} was the target set in Version 1, then {e2,e3} was the target in Version 2." ></td>
	<td class="line x" title="84:326	Twelve filler items requiring singular reference in the continuation were also included." ></td>
	<td class="line x" title="85:326	The order in which the entities were introduced was randomised across participants, as was the order of trials." ></td>
	<td class="line x" title="86:326	The experiment was completed by 18 native speakers of English, selected from the Aberdeen NLG Group database." ></td>
	<td class="line x" title="87:326	They were randomly assigned to either Version 1 or 2." ></td>
	<td class="line x" title="88:326	Results and discussion Responses were coded 1 if the semantically similar properties were used (e.g. the prince and the duke in Fig." ></td>
	<td class="line x" title="89:326	2.2); 2 if the 257 similar properties were used together with other properties (e.g. the prince and the bachelor duke); 3 if a superordinate term was used to replace the similar properties (e.g. the noblemen); 4 otherwise (e.g. The duke and the collector)." ></td>
	<td class="line x" title="90:326	Response types differed significantly in the nominal condition both by subjects (21 = 45.89,p < .001) and by items (22 = 287.9,p < .001)." ></td>
	<td class="line x" title="91:326	Differences were also reliable in the modifier condition (21 = 36.3,p < .001, 22 = 199.2,p < .001)." ></td>
	<td class="line x" title="92:326	However, the trends across conditions were opposed, with more items in the 1 response category in the nominal condition (53.7%) and more in the 4 category in the modifier condition (47.2%)." ></td>
	<td class="line x" title="93:326	Recoding responses as binary (similar = 1,2,3; dissimilar = 4) showed a significant difference in proportions for the nominal category (2 = 4.78,p = .03), but not the modifier category." ></td>
	<td class="line x" title="94:326	Pairwise comparisons showed a significantly larger proportion of 1 (Z = 2.7,p = .007) and 2 responses (Z = 2.54,p = .01) in the nominal compared to the modifier condition." ></td>
	<td class="line x" title="95:326	The results suggest that in a referential task, participants are likely to conform to the CC, but that the CC operates mainly on nouns, and less so on (adjectival) modifiers." ></td>
	<td class="line x" title="96:326	Nouns (or types, as we shall sometimes call them) have the function of categorising objects; thus similar types facilitate the mental representation of a plurality in a conceptually coherent way." ></td>
	<td class="line x" title="97:326	According to the definition in (1), this is because similarity of two types implies a greater likelihood of their being used in the same predicate-argument structures." ></td>
	<td class="line x" title="98:326	As a result, it is easier to map the elements of a plurality to a common role in a sentence." ></td>
	<td class="line x" title="99:326	A related proposal has been made by Moxey and Sanford (1995), whose Scenario Mapping Principle holds that a plural reference is licensed to the extent that the elements of the plurality can be mapped to a common role in the discourse." ></td>
	<td class="line x" title="100:326	This is influenced by how easy it is to conceive of such a role for the referents." ></td>
	<td class="line x" title="101:326	Our results can be viewed as providing a handle on the notion of ease of conception of a common role; in particular we propose that likelihood of occurrence in the same linguistic contexts directly reflects the extent to which two types can be mapped to a single plural role." ></td>
	<td class="line x" title="102:326	As regards modifiers, while it is probably premature to suggest that CC plays no role in modifier selection, it is likely that modifiers play a different role from nouns." ></td>
	<td class="line x" title="103:326	Previous work has shown that id base type occupation specialisation girth e1 woman professor physicist plump e2 woman lecturer geologist thin e3 man lecturer biologist plump e4 man chemist thin Table 2: An example knowledge base restrictions on the plausibility of adjective-noun combinations exist (Lapata et al. , 1999), and that using unlikely combinations (e.g. the immaculate kitchen rather than the spotless kitchen) impacts processing in online tasks (Murphy, 1984)." ></td>
	<td class="line x" title="104:326	Unlike types, which have a categorisation function, modifiers have the role of adding information about an element of a category." ></td>
	<td class="line x" title="105:326	This would partially explain the experimental results: When elements of a plurality have identical types (as in the modifier version of our experiment), the CC is already satisfied, and selection of modifiers would presumably depend on respecting adjective-noun combination restrictions." ></td>
	<td class="line x" title="106:326	Further research is required to verify this, although the algorithm presented below makes use of the Sketch Engine database to take modifier-noun combinations into account." ></td>
	<td class="line x" title="107:326	3 An algorithm for referring to sets Our next task is to port the results to GRE." ></td>
	<td class="line x" title="108:326	The main ingredient to achieve conceptual coherence will be the definition of semantic similarity." ></td>
	<td class="line x" title="109:326	In what follows, all examples will be drawn from the domain in Table 3." ></td>
	<td class="line x" title="110:326	We make the following assumptions." ></td>
	<td class="line x" title="111:326	There is a set U of domain entities, properties of which are specified in a KB as attribute-value pairs." ></td>
	<td class="line x" title="112:326	We assume a distinction between types, that is, any property that can be realised as a noun; and modifiers, or non-types." ></td>
	<td class="line x" title="113:326	Given a set of target referents R  U, the algorithm described below generates a description D in Disjunctive Normal Form (DNF), having the following properties: 1." ></td>
	<td class="line x" title="114:326	Any disjunct in D contains a type property, i.e. a property realisable as a head noun." ></td>
	<td class="line x" title="115:326	2." ></td>
	<td class="line x" title="116:326	If D has two or more disjuncts, each a conjunction containing at least one type, then the disjoined types should be as similar as possible, given the information in the KB and the completeness requirement: that the algorithm find a distinguishing description whenever one exists." ></td>
	<td class="line x" title="117:326	258 We first make our interpretation of the CC more precise." ></td>
	<td class="line x" title="118:326	Let T be the set of types in the KB, and let (t,t) be the (symmetrical) similarity between any two types t and t." ></td>
	<td class="line x" title="119:326	These determine a semantic space S = T,." ></td>
	<td class="line x" title="120:326	We define the notion of a perspective as follows." ></td>
	<td class="line x" title="121:326	Definition 1." ></td>
	<td class="line x" title="122:326	Perspective A perspective P is a convex subset of S, i.e.: t,t,t  T : {t,t}  P (t,t)  (t,t)  t  P The aims of the algorithm are to describe elements of R using types from the same perspective, failing which, it attempts to minimise the distance between the perspectives from which types are selected in the disjunctions of D. Distance between perspectives is defined below." ></td>
	<td class="line x" title="123:326	3.1 Finding perspectives The system makes use of the SketchEngine database as its primary knowledge source." ></td>
	<td class="line x" title="124:326	Since the definition of similarity applies to words, rather than properties, the first step is to generate all possible lexicalisations of the available attribute-value pairs in the domain." ></td>
	<td class="line x" title="125:326	In this paper, we simplify by assuming a one-to-one mapping between properties and words." ></td>
	<td class="line x" title="126:326	Another requirement is to distinguish between type properties (the set T), and non-types (M)3." ></td>
	<td class="line x" title="127:326	The Thesaurus is used to find pairwise similarity of types in order to group them into related clusters." ></td>
	<td class="line x" title="128:326	Word Sketches are used to find, for each type, the modifiers in the KB that are appropriate to the type, on the basis of the associated salience values." ></td>
	<td class="line x" title="129:326	For example, in Table 3, e3 has plump as the value for girth, which combines more felicitously with man, than with biologist." ></td>
	<td class="line x" title="130:326	Types are clustered using the algorithm described in Gatt (2006)." ></td>
	<td class="line x" title="131:326	For each type t, the algorithm finds its nearest neighbour nt in semantic space." ></td>
	<td class="line x" title="132:326	Clusters are then found by recursively grouping elements with their nearest neighbours." ></td>
	<td class="line x" title="133:326	If t, t have a common nearest neighbour n, then {t,t,n} is a cluster." ></td>
	<td class="line x" title="134:326	Clearly, the resulting sets are convex in the sense of Definition 1." ></td>
	<td class="line x" title="135:326	Each modifier is assigned to a cluster by finding in its Word Sketch the type with which it co-occurs with the greatest salience value." ></td>
	<td class="line x" title="136:326	Thus, a cluster is a pair 3This is determined using corpus-derived information." ></td>
	<td class="line x" title="137:326	Note that T and M need not be disjoint, and entities can have more than one type property T: {lecturer, professor} T: {woman, man} M: {plump, thin} T: {geologist, physicist, biologist, chemist} 3 2 1 1 0.6 1 Figure 3: Perspective Graph P,M where P is a perspective, and M  M. The distance (A,B) between two clusters A and B is defined straightforwardly in terms of the distance between their perspectives PA and PB: (A,B) = 1 1 + P xPA,yPB (x,y) |PAPB| (2) Finally, a weighted, connected graph G = V,E, is created, where V is the set of clusters, and E is the set of edges with edge weights defined as the semantic distance between perspectives." ></td>
	<td class="line x" title="138:326	Figure 3.1 shows the graph constructed for the domain in Table 3." ></td>
	<td class="line x" title="139:326	We now define the coherence of a description more precisely." ></td>
	<td class="line x" title="140:326	Given a DNF description D, we shall say that a perspective P is realised in D if there is at least one type t  P which is in D. Let PD be the set of perspectives realised in D. Since G is connected, PD determines a connected subgraph of G. The total weight of D, w(D) is the sum of weights of the edges in PD. Definition 2." ></td>
	<td class="line x" title="141:326	Maximal coherence A description D is maximally coherent iff there is no description D coextensive with D such that w(D) > w(D)." ></td>
	<td class="line x" title="142:326	(Note that several descriptions of the same referent may all be maximally coherent.)" ></td>
	<td class="line x" title="143:326	3.2 Content determination The core of the content determination procedure maintains the DNF description D as an associative array, such that for any r  R, D[r] is a conjunction of properties true of r. Given a cluster P,M, the procedure searches incrementally first through P, and then M, selecting properties that are true of at least one referent and exclude some distractors, as in the IA (Dale and Reiter, 1995)." ></td>
	<td class="line x" title="144:326	By Definition 2, the task of the algorithm is to minimise the total weight w(D)." ></td>
	<td class="line x" title="145:326	If PD is the 259 set of perspectives represented in D on termination, then maximal coherence would require PD to be the subgraph of G with the lowest total cost from which a distinguishing description could be constructed." ></td>
	<td class="line x" title="146:326	Under this interpretation, PD corresponds to a Shortest Connection, or Steiner, Network." ></td>
	<td class="line x" title="147:326	Finding such networks is known to be NPHard." ></td>
	<td class="line x" title="148:326	Therefore, we adopt a weaker (greedy) interpretation." ></td>
	<td class="line x" title="149:326	Under the new definition, if D is the only description for R, then it trivially satisfies maximal coherence." ></td>
	<td class="line x" title="150:326	Otherwise, the algorithm aims to maximise local coherence." ></td>
	<td class="line x" title="151:326	Definition 3." ></td>
	<td class="line x" title="152:326	Local coherence A description D is locally coherent iff: a. either D is maximally coherent or b. there is no D coextensive with D, obtained by replacing types from some perspective in PD with types from another perspective such that w(D) > w(D)." ></td>
	<td class="line x" title="153:326	Our implementation of this idea begins the search for distinguishing properties by identifying the vertex of G which contains the greatest number of referents in its extension." ></td>
	<td class="line x" title="154:326	This constitutes the root node of the search path." ></td>
	<td class="line x" title="155:326	For each node of the graph it visits, the algorithm searches for properties that are true of some subset of R, and removes some distractors, maintaining a set N of the perspectives which are represented in D up to the current point." ></td>
	<td class="line x" title="156:326	The crucial choice points arise when a new node (perspective) needs to be visited in the graph." ></td>
	<td class="line x" title="157:326	At each such point, the next node n to be visited is the one which minimises the total weight of N, that is: min nV summationdisplay uN w(u,n) (3) The results of this procedure closely approximate maximal coherence, because the algorithm starts with the vertex most likely to distinguish the referents, and then greedily proceeds to those nodes which minimise w(D) given the current state, that is, taking all previously used nodes into account." ></td>
	<td class="line x" title="158:326	As an example of the output, we will take R = {e1,e3,e4} as the intended referents in Table 3." ></td>
	<td class="line x" title="159:326	First, the algorithm determines the cluster with the greatest number of referents in its extension." ></td>
	<td class="line x" title="160:326	In this case, there is a tie between clusters 2 and 3 in Figure 3.1, since all three entities have type properties in these clusters." ></td>
	<td class="line x" title="161:326	In either case, the entities are distinguishable from a single cluster." ></td>
	<td class="line x" title="162:326	If cluster 3 is selected as the root, the output is x[physicist(x)biologist(x)chemist(x)]." ></td>
	<td class="line x" title="163:326	In case the algorithm selects cluster 2 as the root node the final output is the logical form x[man(x)(woman(x)plump(x))]." ></td>
	<td class="line x" title="164:326	There is an alternative description that the algorithm does not consider." ></td>
	<td class="line x" title="165:326	An algorithm that aimed for conciseness would generate x[professor(x)man(x)] (the professor and the men), which does not satisfy local coherence." ></td>
	<td class="line x" title="166:326	These examples therefore highlight the possible tension between the avoidance of redundancy and achieving coherence." ></td>
	<td class="line x" title="167:326	It is to an investigation of this tension that we now turn." ></td>
	<td class="line x" title="168:326	4 Evaluation It has been known at least since Dale and Reiter (1995) that the best distinguishing description is not always the shortest one." ></td>
	<td class="line x" title="169:326	Yet, brevity plays a part in all GRE algorithms, sometimes in a strict form (Dale, 1989), or by letting the algorithm approximate the shortest description (for example, in the Dale and Reiters IA)." ></td>
	<td class="line x" title="170:326	This is also true of references to sets, the clearest example being Gardents constraint based approach, which always finds the description with the smallest number of logical operators." ></td>
	<td class="line x" title="171:326	Such proposals do not take coherence (in our sense of the word) into account." ></td>
	<td class="line x" title="172:326	This raises obvious questions about the relative importance of brevity and coherence in reference to sets." ></td>
	<td class="line x" title="173:326	The evaluation took the form of an experiment to compare the output of our Coherence Model with the family of algorithms that have placed Brevity at the centre of content determination." ></td>
	<td class="line x" title="174:326	Participants were asked to compare pairs of descriptions of one and the same target set, selecting the one they found most natural." ></td>
	<td class="line x" title="175:326	Each description could either be optimally brief or not (b) and also either optimally coherent or not (c)." ></td>
	<td class="line x" title="176:326	Non-brief descriptions, took the form the A, the B and the C. Brief descriptions aggregated two disjuncts into one (e.g. the A and the Ds where D comprises the union of B and C)." ></td>
	<td class="line x" title="177:326	We expected to find that: H1 +c descriptions are preferred over c. H2 (+c,b) descriptions are preferred over ones that are (c,+b)." ></td>
	<td class="line x" title="178:326	H3 +b descriptions are preferred over b. Confirmation of H1 would be interpreted as evidence that, by taking coherence into account, our 260 Three old manuscripts were auctioned at Sothebys. e1 One of them is a book, a biography of a composer." ></td>
	<td class="line x" title="179:326	e2 The second, a sailors journal, was published in the form of a pamphlet." ></td>
	<td class="line x" title="180:326	It is a record of a voyage." ></td>
	<td class="line x" title="181:326	e3 The third, another pamphlet, is an essay by Hume." ></td>
	<td class="line x" title="182:326	(+c,b) The biography, the journal and the essay were sold to a collector." ></td>
	<td class="line x" title="183:326	(+c,+b) The book and the pamphlets were sold to a collector." ></td>
	<td class="line x" title="184:326	(c,+b) The biography and the pamphlets were sold to a collector." ></td>
	<td class="line x" title="185:326	(c,b) The book, the record and the essay were sold to a collector." ></td>
	<td class="line x" title="186:326	Figure 4: Example domain in the evaluation algorithm is on the right track." ></td>
	<td class="line x" title="187:326	If H3 were confirmed, then earlier algorithms were (also) on the right track by taking brevity into account." ></td>
	<td class="line x" title="188:326	Confirmation of H2 would be interpreted as meaning that, in references to sets, conceptual coherence is more important than brevity (defined as the number of disjuncts in a disjunctive reference to a set)." ></td>
	<td class="line x" title="189:326	Materials, design and procedure Six discourses were constructed, each introducing three entities." ></td>
	<td class="line x" title="190:326	Each set of three could be described using all 4 possible combinations of b  c (see Figure 4)." ></td>
	<td class="line x" title="191:326	Entities were human in two of the discourses, and artefacts of various kinds in the remainder." ></td>
	<td class="line x" title="192:326	Properties of entities were introduced textually; the order of presentation was randomised." ></td>
	<td class="line x" title="193:326	A forced-choice task was used." ></td>
	<td class="line x" title="194:326	Each discourse was presented with 2 possible continuations consisting of a sentence with a plural subject NP, and participants were asked to indicate the one they found most natural." ></td>
	<td class="line x" title="195:326	The 6 comparisons corresponded to 6 sub-conditions: C1." ></td>
	<td class="line x" title="196:326	Coherence constant a." ></td>
	<td class="line x" title="197:326	(+c,b) vs." ></td>
	<td class="line x" title="198:326	(+c,+b) b." ></td>
	<td class="line x" title="199:326	(c,b) vs." ></td>
	<td class="line x" title="200:326	(c,+b) C2." ></td>
	<td class="line x" title="201:326	Brevity constant a." ></td>
	<td class="line x" title="202:326	(+c,b) vs." ></td>
	<td class="line x" title="203:326	(c,b) b." ></td>
	<td class="line x" title="204:326	(+c,+b) vs." ></td>
	<td class="line x" title="205:326	(c,+b) C3." ></td>
	<td class="line x" title="206:326	Tradeoff/control a." ></td>
	<td class="line x" title="207:326	(+c,b) vs." ></td>
	<td class="line x" title="208:326	(c,+b) b." ></td>
	<td class="line x" title="209:326	(c,b) vs." ></td>
	<td class="line x" title="210:326	(+c,+b) Participants saw each discourse in a single condition." ></td>
	<td class="line x" title="211:326	They were randomly divided into six groups, so that each discourse was used for a different condition in each group." ></td>
	<td class="line x" title="212:326	39 native English speakers, all undergraduates at the University of Aberdeen, took part in the study." ></td>
	<td class="line x" title="213:326	Results and discussion Results were coded according to whether a participants choice was b C1a C1b C2a C2b C3a C3b +b 51.3 43.6   30.8 76.9 +c   82.1 79.5 69.2 76.9 Table 3: Response proportions (%) and/or c. Table 4 displays response proportions." ></td>
	<td class="line x" title="214:326	Overall, the conditions had a significant impact on responses, both by subjects (Friedman 2 = 107.3,p < .001) and by items (2 = 30.2,p < .001)." ></td>
	<td class="line x" title="215:326	When coherence was kept constant (C1a and C1b), the likelihood of a response being +b was no different from b (C1a: 2 = .023,p = .8; C1b: 2 = .64,p = .4); the conditions C1a and C1b did not differ significantly (2 = .46,p = .5)." ></td>
	<td class="line x" title="216:326	By contrast, conditions where brevity was kept constant (C2a and C2b) resulted in very significantly higher proportions of +c choices (C2a: 2 = 16.03,p < .001; C2b: 2 = 13.56,p < .001)." ></td>
	<td class="line x" title="217:326	No difference was observed between C2a and C2b (2 = .08,p = .8)." ></td>
	<td class="line x" title="218:326	In the tradeoff case (C3a), participants were much more likely to select a +c description than a +b one (2 = 39.0,p < .001); a majority opted for the (+b,+c) description in the control case (2 = 39.0,p < .001)." ></td>
	<td class="line x" title="219:326	The results strongly support H1 and H2, since participants choices are impacted by Coherence." ></td>
	<td class="line x" title="220:326	They do not indicate a preference for brief descriptions, a finding that echoes Jordans (2000), to the effect that speakers often relinquish brevity in favour of observing task or discourse constraints." ></td>
	<td class="line x" title="221:326	Since this experiment compared our algorithm against the current state of the art in references to sets, these results do not necessarily warrant the affirmation of the null hypothesis in the case of H3." ></td>
	<td class="line x" title="222:326	We limited Brevity to number of disjuncts, omitting negation, and varying only between length 2 or 3." ></td>
	<td class="line x" title="223:326	Longer or more complex descriptions might evince different tendencies." ></td>
	<td class="line x" title="224:326	Nevertheless, the results show a strong impact of Coherence, compared to (a kind of) brevity, in strong support of the algorithm presented above, as a realisation of the Coherence Model." ></td>
	<td class="line x" title="225:326	5 Conclusions and future work This paper started with an empirical investigation of conceptual coherence in reference, which led to a definition of local coherence as the basis for a new greedy algorithm that tries to minimise the semantic distance between the perspectives repre261 sented in a description." ></td>
	<td class="line x" title="226:326	The evaluation strongly supports our Coherence Model." ></td>
	<td class="line x" title="227:326	We are extending this work in two directions." ></td>
	<td class="line x" title="228:326	First, we are investigating similarity effects across noun phrases, and their impact on text readability." ></td>
	<td class="line x" title="229:326	Finding an impact of such factors would make this model a useful complement to current theories of discourse, which usually interpret coherence in terms of discourse/sentential structure." ></td>
	<td class="line x" title="230:326	Second, we intend to relinquish the assumption of a one-to-one correspondence between properties and words (cf.Siddharthan and Copestake (2004)), making use of the fact that words can be disambiguated by nearby words that are similar." ></td>
	<td class="line x" title="232:326	To use a well-worn example: the financial institution sense of bank might not make the river and its bank lexically incoherent as a description of a piece of scenery, since the word river might cause the hearer to focus on the aquatic reading of the word anyway." ></td>
	<td class="line x" title="233:326	6 Acknowledgements Thanks to Ielka van der Sluis, Imtiaz Khan, Ehud Reiter, Chris Mellish, Graeme Ritchie and Judith Masthoff for useful comments." ></td>
	<td class="line x" title="234:326	This work is part of the TUNA project (http://www.csd.abdn.ac.uk/ research/tuna), supported by EPSRC grant no." ></td>
	<td class="line x" title="235:326	GR/S13330/01 References M. Aloni." ></td>
	<td class="line x" title="236:326	2002." ></td>
	<td class="line x" title="237:326	Questions under cover." ></td>
	<td class="line x" title="238:326	In D. BarkerPlummer, D. Beaver, J. van Benthem, and P. Scotto de Luzio, editors, Words, Proofs, and Diagrams." ></td>
	<td class="line x" title="239:326	CSLI, Stanford, Ca." ></td>
	<td class="line x" title="240:326	C. Barry, C. M. Morrison, and A. W. Ellis." ></td>
	<td class="line x" title="241:326	1997." ></td>
	<td class="line x" title="242:326	Naming the snodgrass and vanderwart pictures." ></td>
	<td class="line x" title="243:326	Quarterly Journal of Experimental Psychology, 50A(3):560585." ></td>
	<td class="line x" title="244:326	K. W. Church and P. Hanks." ></td>
	<td class="line x" title="245:326	1990." ></td>
	<td class="line x" title="246:326	Word association norms, mutual information and lexicography." ></td>
	<td class="line x" title="247:326	Computational Linguistics, 16(1):2229." ></td>
	<td class="line x" title="248:326	R. Dale and E. Reiter." ></td>
	<td class="line x" title="249:326	1995." ></td>
	<td class="line x" title="250:326	Computational interpretation of the Gricean maxims in the generation of referring expressions." ></td>
	<td class="line x" title="251:326	Cognitive Science, 19(8):233 263." ></td>
	<td class="line x" title="252:326	Robert Dale." ></td>
	<td class="line x" title="253:326	1989." ></td>
	<td class="line x" title="254:326	Cooking up referring expressions." ></td>
	<td class="line x" title="255:326	In Proc." ></td>
	<td class="line x" title="256:326	27th Annual Meeting of the Association for Computational Linguistics." ></td>
	<td class="line x" title="257:326	C. Gardent." ></td>
	<td class="line x" title="258:326	2002." ></td>
	<td class="line x" title="259:326	Generating minimal definite descriptions." ></td>
	<td class="line x" title="260:326	In Proc." ></td>
	<td class="line x" title="261:326	40th Annual Meeting of the Association for Computational Linguistics." ></td>
	<td class="line x" title="262:326	A. Gatt and K. van Deemter." ></td>
	<td class="line x" title="263:326	2005." ></td>
	<td class="line x" title="264:326	Semantic similarity and the generation of referring expressions: A first report." ></td>
	<td class="line x" title="265:326	In Proceedings of the 6th International Workshop on Computational Semantics, IWCS-6." ></td>
	<td class="line x" title="266:326	A. Gatt." ></td>
	<td class="line x" title="267:326	2006." ></td>
	<td class="line x" title="268:326	Structuring knowledge for reference generation: A clustering algorithm." ></td>
	<td class="line x" title="269:326	In Proc." ></td>
	<td class="line x" title="270:326	11th Conference of the European Chapter of the Association for Computational Linguistics." ></td>
	<td class="line x" title="271:326	H. Horacek." ></td>
	<td class="line x" title="272:326	2004." ></td>
	<td class="line x" title="273:326	On referring to sets of objects naturally." ></td>
	<td class="line x" title="274:326	In Proc." ></td>
	<td class="line x" title="275:326	3rd International Conference on Natural Language Generation." ></td>
	<td class="line x" title="276:326	P. W. Jordan." ></td>
	<td class="line x" title="277:326	2000." ></td>
	<td class="line x" title="278:326	Can nominal expressions achieve multiple goals?" ></td>
	<td class="line x" title="279:326	In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics." ></td>
	<td class="line x" title="280:326	B. Kaup, S. Kelter, and C. Habel." ></td>
	<td class="line x" title="281:326	2002." ></td>
	<td class="line x" title="282:326	Representing referents of plural expressions and resolving plural anaphors." ></td>
	<td class="line x" title="283:326	Language and Cognitive Processes, 17(4):405450." ></td>
	<td class="line x" title="284:326	A. Kilgarriff." ></td>
	<td class="line x" title="285:326	2003." ></td>
	<td class="line x" title="286:326	Thesauruses for natural language processing." ></td>
	<td class="line x" title="287:326	In Proc." ></td>
	<td class="line x" title="288:326	NLP-KE, Beijing." ></td>
	<td class="line x" title="289:326	S. Koh and C. Clifton." ></td>
	<td class="line x" title="290:326	2002." ></td>
	<td class="line x" title="291:326	Resolution of the antecedent of a plural pronoun: Ontological categories and predicate symmetry." ></td>
	<td class="line x" title="292:326	Journal of Memory and Language, 46:830844." ></td>
	<td class="line x" title="293:326	A. Kronfeld." ></td>
	<td class="line x" title="294:326	1989." ></td>
	<td class="line x" title="295:326	Conversationally relevant descriptions." ></td>
	<td class="line x" title="296:326	In Proc." ></td>
	<td class="line x" title="297:326	27th Annual Meeting of the Association for Computational Linguistics." ></td>
	<td class="line x" title="298:326	M. Lapata, S. McDonald, and F. Keller." ></td>
	<td class="line x" title="299:326	1999." ></td>
	<td class="line x" title="300:326	Determinants of adjective-noun plausibility." ></td>
	<td class="line x" title="301:326	In Proc." ></td>
	<td class="line x" title="302:326	9th Conference of the European Chapter of the Association for Computational Linguistics." ></td>
	<td class="line x" title="303:326	D. Lin." ></td>
	<td class="line x" title="304:326	1998." ></td>
	<td class="line x" title="305:326	An information-theoretic definition of similarity." ></td>
	<td class="line x" title="306:326	In Proc." ></td>
	<td class="line x" title="307:326	International Conference on Machine Learning." ></td>
	<td class="line x" title="308:326	L. Moxey and A. Sanford." ></td>
	<td class="line x" title="309:326	1995." ></td>
	<td class="line x" title="310:326	Notes on plural reference and the scenario-mapping principle in comprehension." ></td>
	<td class="line x" title="311:326	In C.Habel and G.Rickheit, editors, Focus and cohesion in discourse." ></td>
	<td class="line x" title="312:326	de Gruyter, Berlin." ></td>
	<td class="line x" title="313:326	G.L. Murphy." ></td>
	<td class="line x" title="314:326	1984." ></td>
	<td class="line x" title="315:326	Establishing and accessing referents in discourse." ></td>
	<td class="line x" title="316:326	Memory and Cognition, 12:489 497." ></td>
	<td class="line x" title="317:326	A. Siddharthan and A. Copestake." ></td>
	<td class="line x" title="318:326	2004." ></td>
	<td class="line x" title="319:326	Generating referring expressions in open domains." ></td>
	<td class="line x" title="320:326	In Proc." ></td>
	<td class="line x" title="321:326	42nd Annual Meeting of the Association for Computational Linguistics." ></td>
	<td class="line x" title="322:326	K. van Deemter." ></td>
	<td class="line x" title="323:326	2002." ></td>
	<td class="line x" title="324:326	Generating referring expressions: Boolean extensions of the incremental algorithm." ></td>
	<td class="line x" title="325:326	Computational Linguistics, 28(1):3752." ></td>
	<td class="line x" title="326:326	262" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2069
Examining The Content Load Of Part Of Speech Blocks For Information Retrieval
Lioma, Christina;Ounis, Iadh;"></td>
	<td class="line x" title="1:185	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 531538, Sydney, July 2006." ></td>
	<td class="line x" title="2:185	c2006 Association for Computational Linguistics Examining the Content Load of Part of Speech Blocks for Information Retrieval Christina Lioma Department of Computing Science University of Glasgow 17 Lilybank Gardens Scotland, U.K. xristina@dcs.gla.ac.uk Iadh Ounis Department of Computing Science University of Glasgow 17 Lilybank Gardens Scotland, U.K. ounis@dcs.gla.ac.uk Abstract We investigate the connection between part of speech (POS) distribution and content in language." ></td>
	<td class="line x" title="3:185	We define POS blocks to be groups of parts of speech." ></td>
	<td class="line x" title="4:185	We hypothesise that there exists a directly proportional relation between the frequency of POS blocks and their content salience." ></td>
	<td class="line x" title="5:185	We also hypothesise that the class membership of the parts of speech within such blocks reflects the content load of the blocks, on the basis that open class parts of speech are more content-bearing than closed class parts of speech." ></td>
	<td class="line x" title="6:185	We test these hypotheses in the context of Information Retrieval, by syntactically representing queries, and removing from them content-poor blocks, in line with the aforementioned hypotheses." ></td>
	<td class="line x" title="7:185	For our first hypothesis, we induce POS distribution information from a corpus, and approximate the probability of occurrence of POS blocks as per two statistical estimators separately." ></td>
	<td class="line x" title="8:185	For our second hypothesis, we use simple heuristics to estimate the content load within POS blocks." ></td>
	<td class="line x" title="9:185	We use the Text REtrieval Conference (TREC) queries of 1999 and 2000 to retrieve documents from the WT2G and WT10G test collections, with five different retrieval strategies." ></td>
	<td class="line x" title="10:185	Experimental outcomes confirm that our hypotheses hold in the context of Information Retrieval." ></td>
	<td class="line x" title="11:185	1 Introduction The task of an Information Retrieval (IR) system is to retrieve documents from a collection, in response to a user need, which is expressed in the form of a query." ></td>
	<td class="line x" title="12:185	Very often, this task is realised by indexing the documents in the collection with keyword descriptors." ></td>
	<td class="line x" title="13:185	Retrieval consists in matching the query against the descriptors of the documents, and returning the ones that appear closest, in ranked lists of relevance (van Rijsbergen, 1979)." ></td>
	<td class="line x" title="14:185	Usually, the keywords that constitute the document descriptors are associated with individual weights, which capture the importance of the keywords to the content of the document." ></td>
	<td class="line x" title="15:185	Such weights, commonly referred to as term weights, can be computed using various term weighting schemes." ></td>
	<td class="line x" title="16:185	Not all words can be used as keyword descriptors." ></td>
	<td class="line x" title="17:185	In fact, a relatively small number of words accounts for most of a documents content (van Rijsbergen, 1979)." ></td>
	<td class="line x" title="18:185	Function words make noisy index terms, and are usually ignored during the retrieval process." ></td>
	<td class="line x" title="19:185	This is practically realised with the use of stopword lists, which are lists of words to be exempted when indexing the collection and the queries." ></td>
	<td class="line x" title="20:185	The use of stopword lists in IR is a manifestation of a well-known bifurcation in linguistics between open and closed classes of words (Lyons, 1977)." ></td>
	<td class="line x" title="21:185	In brief, open class words are more content-bearing than closed class words." ></td>
	<td class="line x" title="22:185	Generally, the open class contains parts of speech that are morphologically and semantically flexible, while the closed class contains words that primarily perform linguistic well-formedness functions." ></td>
	<td class="line x" title="23:185	The membership of the closed class is mostly fixed and largely restricted to function words, which are not prone to semantic or morphological alterations." ></td>
	<td class="line x" title="24:185	We define a block of parts of speech (POS block) as a block of fixed length a0, where a0 is set empirically." ></td>
	<td class="line x" title="25:185	We define POS block tokens as individual instances of POS blocks, and POS block 531 types as distinct POS blocks in a corpus." ></td>
	<td class="line x" title="26:185	The purpose of this paper is to test two hypotheses." ></td>
	<td class="line x" title="27:185	The intuition behind both of these hypotheses is that, just as individual words can be content-rich or content-poor, the same can hold for blocks of parts of speech." ></td>
	<td class="line x" title="28:185	According to our first hypothesis, POS blocks can be categorized as content-rich or content-poor, on the basis of their distribution within a corpus." ></td>
	<td class="line x" title="29:185	Specifically, we hypothesise that the more frequently a POS block occurs in language, the more content it is likely to bear." ></td>
	<td class="line x" title="30:185	According to our second hypothesis, POS blocks can be categorized as content-rich or content-poor, on the basis of the part of speech class membership of their individual components." ></td>
	<td class="line x" title="31:185	Specifically, we hypothesise that the more closed class components found in a POS block, the less content the block is likely to bear." ></td>
	<td class="line x" title="32:185	Both aforementioned hypotheses are evaluated in the context of IR as follows." ></td>
	<td class="line x" title="33:185	We observe the distribution of POS blocks in a corpus." ></td>
	<td class="line x" title="34:185	We create a list of POS block types with their respective probabilities of occurrence." ></td>
	<td class="line x" title="35:185	As a first step, to test our first hypothesis, we remove the POS blocks with a low probability of occurrence from each query, on the assumption that these blocks are content-poor." ></td>
	<td class="line x" title="36:185	The decision regarding the threshold a0 of low probability of occurrence is realised empirically." ></td>
	<td class="line x" title="37:185	As a second step, we further remove from each query POS blocks that contain less open class than closed class components, in order to test the validity of our second hypothesis, as an extension of the first hypothesis." ></td>
	<td class="line x" title="38:185	We retrieve documents from two standard IR English test collections, namely WT2G and WT10G." ></td>
	<td class="line x" title="39:185	Both of these collections are commonly used for retrieval effectiveness evaluations in the Text REtrieval Conference (TREC), and come with sets of queries and query relevance assessments1." ></td>
	<td class="line x" title="40:185	Query relevance assessments are lists of relevant documents, given a query." ></td>
	<td class="line x" title="41:185	We retrieve relevant documents using firstly the original queries, secondly the queries produced after step 1, and thirdly the queries produced after step 2." ></td>
	<td class="line x" title="42:185	We use five statistically different term weighting schemes to match the query terms to the document keywords, in order to assess our hypotheses across a range of retrieval techniques." ></td>
	<td class="line x" title="43:185	We associate improvement of retrieval performance with successful noise reduction in the queries." ></td>
	<td class="line x" title="44:185	We assume noise reduction to reflect the correct iden1http://trec.nist.gov/ tification of content-poor blocks, in line with our hypotheses." ></td>
	<td class="line x" title="45:185	Section 2 presents related studies in this field." ></td>
	<td class="line x" title="46:185	Section 3 introduces our methodology." ></td>
	<td class="line x" title="47:185	Section 4 presents the experimental settings used to test our hypotheses, and their evaluation outcomes." ></td>
	<td class="line x" title="48:185	Section 5 provides our conclusions and remarks." ></td>
	<td class="line x" title="49:185	2 Related Studies We examine the distribution of POS blocks in language." ></td>
	<td class="line x" title="50:185	This is but one type of language distribution analysis that can be realised." ></td>
	<td class="line oc" title="51:185	One can also examine the distribution of character or word ngrams, e.g. Language Modeling (Croft and Lafferty, 2003), phrases (Church and Hanks, 1990; Lewis, 1992), and so on." ></td>
	<td class="line x" title="52:185	In class-based n-gram modeling (Brown et al. , 1992) for example, classbased n-grams are used to determine the probability of occurrence of a POS class, given its preceding classes, and the probability of a particular word, given its own POS class." ></td>
	<td class="line x" title="53:185	Unlike the classbased n-gram model, we do not use POS blocks to make predictions." ></td>
	<td class="line x" title="54:185	We estimate their probability of occurrence as blocks, not the individual probabilities of their components, motivated by the intuition that the more frequently a POS block occurs, the more content it bears." ></td>
	<td class="line x" title="55:185	In the context of IR, efforts have been made to use syntactic information to enhance retrieval (Smeaton, 1999; Strzalkowski, 1996; Zukerman and Raskutti, 2002), but not by using POS block-based distribution representations." ></td>
	<td class="line x" title="56:185	3 Methodology We present the steps realised in order to assess our hypotheses in the context of IR." ></td>
	<td class="line x" title="57:185	Firstly, POS blocks with their respective frequencies are extracted from a corpus." ></td>
	<td class="line x" title="58:185	The probability of occurrence of each POS block is statistically estimated." ></td>
	<td class="line x" title="59:185	In order to test our first hypothesis, we remove from the query all but POS blocks of high probability of occurrence, on the assumption that the latter are content-rich." ></td>
	<td class="line x" title="60:185	In order to test our second hypothesis, POS blocks that contain more closed class than open class tags are removed from the queries, on the assumption that these blocks are contentpoor." ></td>
	<td class="line x" title="61:185	3.1 Inducing POS blocks from a corpus We extract POS blocks from a corpus and estimate their probability of occurrence, as follows." ></td>
	<td class="line x" title="62:185	532 The corpus is POS tagged." ></td>
	<td class="line x" title="63:185	All lexical word forms are eliminated." ></td>
	<td class="line x" title="64:185	Thus, sentences are constituted solely by sequences of POS tags." ></td>
	<td class="line x" title="65:185	The following example illustrates this point." ></td>
	<td class="line x" title="66:185	[Original sentence] Many of the proposals for directives and action programmes planned by the Commission have for some obscure reason never seen the light of day." ></td>
	<td class="line x" title="67:185	[Tagged sentence] Many/JJ of/IN the/DT proposals/NNS for/IN directives/NNS and/CC action/NN programmes/NNS planned/VVN by/IN the/DT Commission/NP have/VHP for/IN some/DT obscure/JJ reason/NN never/RB seen/VVN the/DT light/NN of/IN day/NN [Tags-only sentence] JJ IN DT NNS IN NNS CC NN NNS VVN IN DT NP VHP IN DT JJ NN RB VVN DT NN IN NN For each sentence in the corpus, all possible POS blocks are extracted." ></td>
	<td class="line x" title="68:185	Thus, for a given sentence ABCDEFGH, where POS tags are denoted by single letters, and where POS block length a0 = 4, the POS blocks extracted are ABCD, BCDE, CDEF, and so on." ></td>
	<td class="line x" title="69:185	The extracted POS blocks overlap." ></td>
	<td class="line x" title="70:185	The order in which the POS blocks occur in the sentence is disregarded." ></td>
	<td class="line x" title="71:185	We statistically infer the probability of occurrence of each POS block, on the basis of the individual POS block frequencies counted in the corpus." ></td>
	<td class="line x" title="72:185	Maximum Likelihood inference is eschewed, as it assigns the maximum possible likelihood to the POS blocks observed in the corpus, and no probability to unseen POS blocks." ></td>
	<td class="line x" title="73:185	Instead, we employ statistical estimation that accounts for unseen POS blocks, namely Laplace and Good-Turing (Manning and Schutze, 1999)." ></td>
	<td class="line x" title="74:185	3.2 Removing POS blocks from the queries In order to test our first hypothesis, POS blocks of low probability of occurrence are removed from the queries." ></td>
	<td class="line x" title="75:185	Specifically, we POS tag the queries, and remove the POS blocks that have a probability of occurrence below an empirical threshold a0." ></td>
	<td class="line x" title="76:185	The following example illustrates this point." ></td>
	<td class="line x" title="77:185	[Original query] A relevant document will focus on the causes of the lack of integration in a significant way; that is, the mere mention of immigration difficulties is not relevant." ></td>
	<td class="line x" title="78:185	Documents that discuss immigration problems unrelated to Germany are also not relevant." ></td>
	<td class="line x" title="79:185	[Tags-only query] DT JJ NN MD VV IN DT NNS IN DT NN IN NN IN DT JJ NN; WDT VBZ DT JJ NN IN NN NNS VBZ RB JJ." ></td>
	<td class="line x" title="80:185	NNS WDT VVP NN NNS JJ TO NP VBP RB RB JJ [Query with high-probability POS blocks] DT NNS IN DT NN IN NN IN NN IN NN NNS [Resulting query] the causes of the lack of integration in mention of immigration difficulties Some of the low-probability POS blocks, which are removed from the query in the above example, are DT JJ NN MD, JJ NN MD VV, NN MD VV IN, and so on." ></td>
	<td class="line x" title="81:185	The resulting query contains fragments of the original query, assumed to be content-rich." ></td>
	<td class="line x" title="82:185	In the context of the bag-of-words approach to IR investigated here, the grammatical well-formedness of the query is thus not an issue to be considered." ></td>
	<td class="line x" title="83:185	In order to test the second hypothesis, we remove from the queries POS blocks that contain less open class than closed class components." ></td>
	<td class="line x" title="84:185	We propose a simple heuristic Content Load algorithm, to count the presence of content within a POS block, on the premise that open class tags bear more content than closed class tags." ></td>
	<td class="line x" title="85:185	The order of tags within a POS block is ignored." ></td>
	<td class="line x" title="86:185	Figure 1 displays our Content Load algorithm." ></td>
	<td class="line x" title="87:185	After the a0a1a0a3a2 POS block component has been counted, if the Content Load is zero or more, we consider the POS block content-rich." ></td>
	<td class="line x" title="88:185	If the Figure 1: The Content Load algorithm function CONTENT-LOAD(POSblock) returns ContentLoad INITIALISE-FOR-EACH-POSBLOCK(query) for pos a4 from 1 to POSblock-size do if(current-tag = = OpenClass) (ContentLoad)+ + elseif(current-tag = = ClosedClass) (ContentLoad)end return(ContentLoad) 533 Content Load is strictly less than zero, we consider the POS block content-poor." ></td>
	<td class="line x" title="89:185	We assume an underlying equivalence of content in all open class parts of speech, which albeit being linguistically counter-intuitive, is shown to be effective when applied to IR (Section 4)." ></td>
	<td class="line x" title="90:185	The following example illustrates this point." ></td>
	<td class="line x" title="91:185	In this example, POS block length a0 = 4." ></td>
	<td class="line x" title="92:185	[Original query] A relevant document will focus on the causes of the lack of integration in a significant way; that is, the mere mention of immigration difficulties is not relevant." ></td>
	<td class="line x" title="93:185	Documents that discuss immigration problems unrelated to Germany are also not relevant." ></td>
	<td class="line x" title="94:185	[Tags-only query] DT JJ NN MD VV IN DT NNS IN DT NN IN NN IN DT JJ NN; WDT VBZ DT JJ NN IN NN NNS VBZ RB JJ." ></td>
	<td class="line x" title="95:185	NNS WDT VVP NN NNS JJ TO NP VBP RB RB JJ [Query with high-probability POS blocks] DT NNS IN DT NN IN NN IN NN IN NN NNS [Content Load of POS blocks] DT NNS IN DT (-2), NN IN NN IN (0), NN IN NN NNS (+2) [Query with high-probability POS blocks of zero or positive Content Load] NN IN NN IN NN IN NN NNS [Resulting query] lack of integration in mention of immigration difficulties 4 Evaluation We present the experiments realised to test the two hypotheses formulated in Section 1." ></td>
	<td class="line x" title="96:185	Section 4.1 presents our experimental settings, and Section 4.2 our evaluation results." ></td>
	<td class="line x" title="97:185	4.1 Experimental Settings We induce POS blocks from the English language component of the second release of the parallel Europarl corpus(75MB)2." ></td>
	<td class="line x" title="98:185	We POS tag the corpus using the TreeTagger3, which is a probabilistic POS tagger that uses the Penn TreeBank tagset 2http://people.csail.mit.edu/koehn/publications/europarl/ 3http://www.ims.uni-stuttgart.de/projekte/corplex/ TreeTagger/ Table 1: Correspondence between the TreeBank (TB) and Reduced TreeBank (RTB) tags." ></td>
	<td class="line x" title="99:185	TB TBR JJ, JJR, JJS JJ RB,RBR,RBS RB CD, LS CD CC CC DT, WDT, PDT DT FW FW MD, VB, VBD, VBG, VBN, VBP, VBZ, VH, VHD, VHG, VHN, VHP, VHZ MD NN, NNS, NP, NPS NN PP, WP, PP$, WP$, EX, WRB PP IN, TO IN POS PO RP RP SYM SY UH UH VV, VVD, VVG, VVN, VVP, VVZ VB (Marcus et al. , 1993)." ></td>
	<td class="line x" title="100:185	Since we are solely interested in a POS analysis, we introduce a stage of tagset simplification, during which, any information on top of surface POS classification is lost (Table 1)." ></td>
	<td class="line x" title="101:185	Practically, this leads to 48 original TreeBank (TB) tag classes being narrowed down to 15 Reduced TreeBank (RTB) tag classes." ></td>
	<td class="line x" title="102:185	Additionally, tag names are shortened into two-letter names, for reasons of computational efficiency." ></td>
	<td class="line x" title="103:185	We consider the TBR tags JJ, FW, NN, and VB as open-class, and the remaining tags as closed class (Lyons, 1977)." ></td>
	<td class="line x" title="104:185	We extract 214,398,227 POS block tokens and 19,343 POS block types from the corpus." ></td>
	<td class="line x" title="105:185	We retrieve relevant documents from two standard TREC test collections, namely WT2G (2GB) and WT10G (10GB), from the 1999 and 2000 TREC Web tracks, respectively." ></td>
	<td class="line x" title="106:185	We use the queries 401-450 from the ad-hoc task of the 1999 Web track, for the WT2G test collection, and the queries 451-500 from the ad-hoc task of the 2000 Web track, for the WT10G test collection, with their respective relevance assessments." ></td>
	<td class="line x" title="107:185	Each query contains three fields, namely title, description, and narrative." ></td>
	<td class="line x" title="108:185	The title contains keywords describing the information need." ></td>
	<td class="line x" title="109:185	The description expands briefly on the information need." ></td>
	<td class="line x" title="110:185	The narrative part consists of sentences denoting key concepts to be considered or ignored." ></td>
	<td class="line x" title="111:185	We use all three 534 query fields to match query terms to document keyword descriptors, but extract POS blocks only from the narrative field of the queries." ></td>
	<td class="line x" title="112:185	This choice is motivated by the two following reasons." ></td>
	<td class="line x" title="113:185	Firstly, the narrative includes the longest sentences in the whole query." ></td>
	<td class="line x" title="114:185	For our experiments, longer sentences provide better grounds upon which we can test our hypotheses, since the longer a sentence, the more POS blocks we can match within it." ></td>
	<td class="line x" title="115:185	Secondly, the narrative field contains the most noise in the whole query." ></td>
	<td class="line x" title="116:185	Especially when using bag-ofwords term weighting, such as in our evaluation, information on what is not relevant to the query only introduces noise." ></td>
	<td class="line x" title="117:185	Thus, we select the most noisy field of the query to test whether the application of our hypotheses indeed results in the reduction of noise." ></td>
	<td class="line x" title="118:185	During indexing, we remove stopwords, and stem the collections and the queries, using Porters4 stemming algorithm." ></td>
	<td class="line x" title="119:185	We use the Terrier5 IR platform, and apply five different weighting schemes to match query terms to document descriptors." ></td>
	<td class="line x" title="120:185	In IR, term weighting schemes estimate the relevance a0a2a1a4a3a6a5a8a7a10a9 of a document a3 for a query a7, as: a0a2a1a4a3a11a5a8a7a12a9a14a13a16a15 a0a4a17a19a18a21a20a23a22a25a24a27a26a28a24 a1 a22 a5a29a3a30a9, where a22 is a term in a7, a20a23a22a25a24 is the query term weight, and a24 a1 a22 a5a29a3a31a9 is the weight of document a3 for term a22 . For example, we use the classical TF IDF weighting scheme (Sparck-Jones, 1972; Robertson et al. , 1995): a24 a1 a22 a5a29a3a31a9a12a13 a22a33a32 a0 a26a35a34a37a36a35a38a40a39a42a41 a43a37a44a46a45a48a47, where a22a33a32 a0 is the normalised term frequency in a document: a22a49a32 a0 a13 a50a46a51a33a52 a0 a44 a0 a44a46a45 a50a53a51a49a54 a47a49a55a11a56a57a45a58a56a60a59 a61a49a62a64a63 a59a57a65 ; a22a49a32 is the frequency of a term in a document; a66a58a67, and a68 are parameters; a69 and a70a72a71a72a73 a69 are the document length and the average document length in the collection, respectively; a74 is the number of documents in the collection; and a3 a32 is the number of documents containing the term a22 . For all weighting schemes we use, a20a23a22a75a24 a13 a76 a0 a44 a76 a0 a44a25a77 a61a49a78, where a20a28a22a49a32 is the query term frequency, and a20a23a22a33a32a19a79a81a80a29a82 is the maximum a20a23a22a49a32 among all query terms." ></td>
	<td class="line x" title="121:185	We also use the well-established probabilistic BM25 weighting scheme (Robertson et al. , 1995), and three distinct weighting schemes from the more recent Divergence From Randomness (DFR) framework (Amati, 2003), namely BB2, PL2, and DLH." ></td>
	<td class="line x" title="122:185	Note that, even though we use three weighting schemes from the DFR framework, the said schemes are statistically different to one another." ></td>
	<td class="line x" title="123:185	Also, DLH is the only parameter-free 4http://snowball.tartarus.org/ 5http://ir.dcs.gla.ac.uk/terrier/ weighting scheme we use, as it computes all of the a24 a1 a22 a5a29a3a30a9 variables automatically from the collection statistics." ></td>
	<td class="line x" title="124:185	We use the default values of all parameters, namely, for the TF IDF and BM25 weighting schemes (Robertson et al. , 1995), a66a58a67 a13 a67a19a83a85a84, a66a87a86 a13 a67a53a88a35a88a35a88, and a68 a13 a88a30a83a90a89a19a91 for both test collections; while for the PL2 and BB2 term weighting schemes (Amati, 2003), a92 a13a94a93 a83a85a95a19a88 for the WT2G test collection, and a92 a13 a91a31a83a85a91a35a95 for the WT10G test collection." ></td>
	<td class="line x" title="125:185	We use default values, instead of tuning the term weighting parameters, because our focus lies in testing our hypotheses, and not in optimising retrieval performance." ></td>
	<td class="line x" title="126:185	If the said parameters are optimised, retrieval performance may be further improved." ></td>
	<td class="line x" title="127:185	We measure the retrieval performance using the Mean Average Precision (MAP) measure (van Rijsbergen, 1979)." ></td>
	<td class="line x" title="128:185	Throughout all experiments, we set POS block length at a0 = 4." ></td>
	<td class="line x" title="129:185	We employ Good-Turing and Laplace smoothing, and set the threshold of high probability of occurrence empirically at a0 = 0.01." ></td>
	<td class="line x" title="130:185	We present all evaluation results in tables, the format of which is as follows: GT and LA indicate Good-Turing and Laplace respectively, and a96a98a97 denotes the % difference in MAP from the baseline." ></td>
	<td class="line x" title="131:185	Statistically significant scores, as per the Wilcoxon test (a99a101a100a102a88a30a83a103a88a104a91 ), appear in boldface, while highest a96 percentages appear in italics." ></td>
	<td class="line x" title="132:185	4.2 Evaluation Results Our retrieval baseline consists in testing the performance of each term weighting scheme, with each of the two test collections, using the original queries." ></td>
	<td class="line x" title="133:185	We introduce two retrieval combinations on top of the baseline, which we call POS and POSC." ></td>
	<td class="line x" title="134:185	The POS retrieval experiments, which relate to our first hypothesis, and the POSC retrieval experiments, which relate to our second hypothesis, are described in Section 4.2.1." ></td>
	<td class="line x" title="135:185	Section 4.2.2 presents the assessment of our hypotheses using a performance-boosting retrieval technique, namely query expansion." ></td>
	<td class="line x" title="136:185	4.2.1 POS and POSC Retrieval Experiments The aim of the POS and POSC experiments is to test our first and second hypotheses, respectively." ></td>
	<td class="line x" title="137:185	Firstly, to test the first hypothesis, namely that there is a direct connection between the removal of low-frequency POS blocks from the queries and noise reduction in the queries, we remove all lowfrequency POS blocks from the narrative field of 535 the queries." ></td>
	<td class="line x" title="138:185	Secondly, to test our second hypothesis as an extension of our first hypothesis, we refilter the queries used in the POS experiments by removing from them POS blocks that contain more closed class than open class tags." ></td>
	<td class="line x" title="139:185	The processes involved in both hypotheses take place prior to the removal of stop words and stemming of the queries." ></td>
	<td class="line x" title="140:185	Table 2 displays the relevant evaluation results." ></td>
	<td class="line x" title="141:185	Overall, the removal of low-probability POS blocks from the queries (Hypothesis 1 section in Table 2) is associated with an improvement in retrieval performance over the baseline in most cases, which sometimes is statistically significant." ></td>
	<td class="line x" title="142:185	This improvement is quite similar across the two statistical estimators." ></td>
	<td class="line x" title="143:185	Moreover, two interesting patterns emerge." ></td>
	<td class="line x" title="144:185	Firstly, the DFR weighting schemes seem to be divided, performance-wise, between the parametric BB2 and PL2, which are associated with the highest improvement in retrieval performance, and the non-parametric DLH, which is associated with the lowest improvement, or even deterioration in retrieval performance." ></td>
	<td class="line x" title="145:185	This may indicate that the parameter used in BB2 and PL2 is not optimal, which would explain a low baseline, and thus a very high improvement over it." ></td>
	<td class="line x" title="146:185	Secondly, when comparing the improvement in performance related to the WT2G and the WT10G test collections, we observe a more marked improvement in retrieval performance with WT2G than with WT10G." ></td>
	<td class="line x" title="147:185	The combination of our two hypotheses (Hypotheses 1+2 section in Table 2) is associated with an improvement in retrieval performance over the baseline in most cases, which sometimes is statistically significant." ></td>
	<td class="line x" title="148:185	This improvement is very similar across the two statistical estimators, namely Good-Turing and Laplace." ></td>
	<td class="line x" title="149:185	When combining hypotheses 1+2, retrieval performance improves more than it did for hypothesis 1 only, for the WT2G test collection, which indicates that our second hypothesis might further reduce the amount of noise in the queries successfully." ></td>
	<td class="line x" title="150:185	For the WT10G collection, we object similar results, with the exception of DLH." ></td>
	<td class="line x" title="151:185	Generally, the improvement in performance associated to the WT2G test collection is more marked than the improvement associated to WT10G." ></td>
	<td class="line x" title="152:185	To recapitulate on the evaluation outcomes of our two hypotheses, we report an improvement in retrieval performance over the baseline for most, but not all cases, which is sometimes statistically significant." ></td>
	<td class="line x" title="153:185	This may be indicative of successful noise reduction in the queries, as per our hypotheses." ></td>
	<td class="line x" title="154:185	Also, the difference in the improvement in retrieval performance across the two test collections may suggest that data sparseness affects retrieval performance." ></td>
	<td class="line x" title="155:185	4.2.2 POS and POSC Retrieval Experiments with Query Expansion Query expansion (QE) is a performanceboosting technique often used in IR, which consists in extracting the most relevant terms from the top retrieved documents, and in using these terms to expand the initial query." ></td>
	<td class="line x" title="156:185	The expanded query is then used to retrieve documents anew." ></td>
	<td class="line x" title="157:185	Query expansion has the distinct property of improving retrieval performance when queries do not contain noise, but harming retrieval performance when queries contain noise, furnishing us with a strong baseline, against which we can measure our hypotheses." ></td>
	<td class="line x" title="158:185	We repeat the experiments described in Section 4.2.1 with query expansion." ></td>
	<td class="line x" title="159:185	We use the Bo1 query expansion scheme from the DFR framework (Amati, 2003)." ></td>
	<td class="line x" title="160:185	We optimise the query expansion settings, so as to maximise its performance." ></td>
	<td class="line x" title="161:185	This provides us with an even stronger baseline, against which we can compare our proposed technique, which we tune empirically too through the tuning of the threshold a0 . We optimise query expansion on the basis of the corresponding relevance assessments available for the queries and collections employed, by selecting the most relevant terms from the top retrieved documents." ></td>
	<td class="line x" title="162:185	For the WT2G test collection, the relevant terms / top retrieved documents ratio we use is (i) 20/5 with TF IDF, BM25, and DLH; (ii) 30/5 with PL2; and (iii) 10/5 with BB2." ></td>
	<td class="line x" title="163:185	For the WT10G collection, the said ratio is (i) 10/5 for TF IDF; (ii) 20/5 for BM25 and DLH; and (iii) 5/5 for PL2 and BB2." ></td>
	<td class="line x" title="164:185	We repeat our POS and POSC retrieval experiments with query expansion." ></td>
	<td class="line x" title="165:185	Table 3 displays the relevant evaluation results." ></td>
	<td class="line x" title="166:185	Query expansion has overall improved retrieval performance (compare Tables 2 and 3), for both test collections, with two exceptions, where query expansion has made no difference at all, namely for BB2 and PL2, with the WT10G collection." ></td>
	<td class="line x" title="167:185	The removal of low-probability POS blocks from the queries, as per our first hypothesis, combined with query expansion, is associated with an im536 Table 2: Mean Average Precision (MAP) scores of the POS and POSC experiments." ></td>
	<td class="line x" title="168:185	WT2G collection Hypothesis 1 Hypotheses 1+2 w(t,d) base POSGT a96 % POSLA a96 % POSCGT a96 % POSCLA a96 % TFIDF 0.276 0.295 +6.8 0.293 +6.1 0.298 +8.0 0.294 +6.4 BM25 0.280 0.294 +4.8 0.292 +4.1 0.297 +5.9 0.293 +4.5 BB2 0.237 0.291 +22.8 0.287 +21.0 0.295 +24.2 0.288 +21.5 PL2 0.268 0.298 +11.2 0.297 +10.9 0.306 +14.1 0.302 +12.8 DLH 0.237 0.239 +0.7 0.238 +0.4 0.243 +2.3 0.241 +1.6 WT10G collection Hypothesis 1 Hypotheses 1+2 w(t,d) base POSGT a96 % POSLA a96 % POSCGT a96 % POSCLA a96 % TFIDF 0.231 0.234 +1.2 0.238 +2.8 0.233 +0.7 0.237 +2.6 BM25 0.234 0.234 none 0.238 +1.5 0.233 -0.4 0.237 +1.2 BB2 0.206 0.213 +3.5 0.214 +4.0 0.216 +5.0 0.220 +6.7 PL2 0.237 0.253 +6.8 0.253 +7.0 0.251 +6.1 0.256 +8.2 DLH 0.232 0.231 -0.7 0.233 +0.5 0.230 -1.0 0.234 +0.9 Table 3: Mean Average Precision (MAP) scores of the POS and POSC experiments with Query Expansion." ></td>
	<td class="line x" title="169:185	WT2G collection Hypothesis 1 Hypotheses 1+2 w(t,d) base POSGT a96 % POSLA a96 % POSCGT a96 % POSCLA a96 % TFIDF 0.299 0.323 +8.0 0.329 +10.0 0.322 +7.7 0.325 +8.7 BM25 0.302 0.320 +5.7 0.326 +7.9 0.319 +5.6 0.322 +6.6 BB2 0.239 0.291 +21.7 0.288 +20.5 0.291 +21.7 0.287 +20.1 PL2 0.285 0.312 +9.5 0.315 +10.5 0.315 +10.5 0.316 +10.9 DLH 0.267 0.283 +6.0 0.283 +6.0 0.284 +6.4 0.283 +6.0 WT10G collection Hypothesis 1 Hypotheses 1+2 w(t,d) base POSGTQE a96 % POSLAQE a96 % POSCGT a96 % POSCLA a96 % TFIDF 0.233 0.241 +3.4 0.249 +6.9 0.240 +3.0 0.250 +7.3 BM25 0.240 0.248 +3.3 0.250 +4.2 0.244 +1.7 0.249 +3.7 BB2 0.206 0.213 +3.4 0.214 +3.9 0.216 +4.8 0.220 +6.8 PL2 0.237 0.253 +6.7 0.253 +6.7 0.251 +5.9 0.256 +8.0 DLH 0.236 0.250 +5.9 0.246 +4.2 0.250 +5.9 0.253 +7.2 537 provement in retrieval performance over the new baseline at all times, which is sometimes statistically significant." ></td>
	<td class="line x" title="170:185	This may indicate that noise has been further reduced in the queries." ></td>
	<td class="line x" title="171:185	Also, the two statistical estimators lead to similar improvements in retrieval performance." ></td>
	<td class="line x" title="172:185	When we compare these results to the ones reported with identical settings but without query expansion (Table 2), we observe the following." ></td>
	<td class="line x" title="173:185	Firstly, the previously reported division in the DFR weighting schemes, where BB2 and PL2 improved the most from our hypothesised noise reduction in the queries, while DLH improved the least, is no longer valid." ></td>
	<td class="line x" title="174:185	The improvement in retrieval performance now associated to DLH is similar to the improvement associated with the other weighting schemes." ></td>
	<td class="line x" title="175:185	Secondly, the difference in the retrieval improvement previously observed between the two test collections is now smaller." ></td>
	<td class="line x" title="176:185	To recapitulate on the evaluation outcomes of our two hypotheses combined with query expansion, we report an improvement in retrieval performance over the baseline at all times, which is sometimes statistically significant." ></td>
	<td class="line x" title="177:185	It appears that the combination of our hypotheses with query expansion tones down previously reported sharp differences in retrieval improvements over the baseline (Table 2), which may be indicative of further noise reduction." ></td>
	<td class="line x" title="178:185	5 Conclusion We described a block-based part of speech (POS) modeling of language distribution, induced from a corpus, and statistically smoothened using two different estimators." ></td>
	<td class="line x" title="179:185	We hypothesised that highfrequency POS blocks bear more content than lowfrequency POS blocks." ></td>
	<td class="line x" title="180:185	Also, we hypothesised that the more closed class components a POS block contains, the less content it bears." ></td>
	<td class="line x" title="181:185	We evaluated both hypotheses in the context of Information Retrieval, across two standard test collections, and five statistically different term weighting schemes." ></td>
	<td class="line x" title="182:185	Our hypotheses led to a general improvement in retrieval performance." ></td>
	<td class="line x" title="183:185	This improvement was overall higher for the smaller of the two collections, indicating that data sparseness may have an effect on retrieval." ></td>
	<td class="line x" title="184:185	The use of query expansion worked well with our hypotheses, by helping weaker weighting schemes to benefit more from the reduction of noise in the queries." ></td>
	<td class="line x" title="185:185	In the future, we wish to investigate varying the size a0 of POS blocks, as well as testing our hypotheses on shorter queries." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0308
Towards A Validated Model For Affective Classification Of Texts
Genereux, Michel;Evans, Roger;"></td>
	<td class="line x" title="1:160	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 5562, Sydney, July 2006." ></td>
	<td class="line x" title="2:160	c2006 Association for Computational Linguistics Towards a validated model for affective classification of texts Michel Genereux and Roger Evans Natural Language Technology Group (NLTG) University of Brighton, United Kingdom {M.Genereux,R.P.Evans}@brighton.ac.uk Abstract In this paper, we present the results of experiments aiming to validate a twodimensional typology of affective states as a suitable basis for affective classi cation of texts." ></td>
	<td class="line x" title="3:160	Using a corpus of English weblog posts, annotated for mood by their authors, we trained support vector machine binary classi ers to distinguish texts on the basis of their af liation with one region of the space." ></td>
	<td class="line x" title="4:160	We then report on experiments which go a step further, using four-class classi ers based on automated scoring of texts for each dimension of the typology." ></td>
	<td class="line x" title="5:160	Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more ne-grained classi cation along these two axes." ></td>
	<td class="line x" title="6:160	1 Introduction We are investigating the subjective use of language in text and the automatic classi cation of texts according to their subjective characteristics, or affect." ></td>
	<td class="line x" title="7:160	Our approach is to view affective states (such as happy, angry) as locations in Osgoods Evaluation-Activation (EA) space (Osgood et aal., 1957), and draws on work in psychology which has a long history of work seeking to construct a typology of such affective states (Scherer, 1984)." ></td>
	<td class="line x" title="9:160	A similar approach has been used more recently to describe emotional states that are expressed in speech (Cowie and Cornelius, 2002; Schrcurrency1oder and Cowie, 2005)." ></td>
	<td class="line x" title="10:160	Our overall aim is to determine the extent to which such a typology can be validated and applied to the task of text classi cation using automatic methods." ></td>
	<td class="line x" title="11:160	In this paper we describe some initial experiments aimed at validating a basic two dimensional classi cation of weblog data, rst with Support Vector Machine (SVM) binary classi ers, then with Pointwise Mutual Information Information Retrieval (PMI-IR)." ></td>
	<td class="line x" title="12:160	The domain of weblog posts is particularly well-suited for this task given its highly subjective nature and the availability of data, including data which has been author-annotated for mood, which is a reasonable approximation of affect." ></td>
	<td class="line x" title="13:160	Recent attempts to classify weblog posts have shown modest, but consistent improvements over a 50% baseline, only slightly worse than human performance (Mishne, 2005)." ></td>
	<td class="line x" title="14:160	One important milestone is the elaboration of a typology of affective states." ></td>
	<td class="line x" title="15:160	To devise such a typology, our starting point is Figure 1, which is based on a model of emotion as a multicomponent process (Scherer, 1984)." ></td>
	<td class="line x" title="16:160	In this model, the distribution of the affective states is the result of analysing similarity judgments by humans for 235 emotion terms1 using cluster-analysis and multidimensional scaling techniques to map out the structure as a twodimensional space." ></td>
	<td class="line x" title="17:160	The positioning of words is not so much controversial as fuzzy; an affective state such as angry to describe facial expression in speech may have a slightly different location than an angry weblog post." ></td>
	<td class="line x" title="18:160	In this model, the well-studied sentiment classi cation is simply a speci c case (left vs. right halves of the space)." ></td>
	<td class="line x" title="19:160	The experiments we describe here seek to go beyond this basic distinction." ></td>
	<td class="line x" title="20:160	They involve an additional dimension of affect, the activity dimension, allowing textual data to be classi ed into four categories corresponding to each of the four quad1Reduced to less than 100 in Figure 1." ></td>
	<td class="line x" title="21:160	55 Figure 1: Typology of affective states based on (Scherer, 1984) rants in the space." ></td>
	<td class="line x" title="22:160	Ultimately, once scores have been promoted to real measures, classi cation can be more precise; for example, a text is not only negative and passive, it is more precisely depressive." ></td>
	<td class="line x" title="23:160	With such a more precise classi cation one might, for example, be able to detect individuals at risk of suicide." ></td>
	<td class="line x" title="24:160	In Experiment 1, we use binary classi ers to investigate how the four quadrants de ned by the typology hold together, the assumption being that if the typology is correct, the classi ers should perform substantially better than a random baseline." ></td>
	<td class="line x" title="25:160	In Experiment 2, we go a step closer towards a more ne-grained classi cation by evaluating the performance of an unsupervised automated technique for scoring texts on both axes." ></td>
	<td class="line x" title="26:160	Both these experiments are preliminary our long term goal is to be able to validate the whole typology in terms of computationally effective classi cation." ></td>
	<td class="line x" title="27:160	2 Corpus We have collected from Livejournal2 a total of 346723 weblogs (mood-annotated by authors) in 2http://www.livejournal.com." ></td>
	<td class="line x" title="28:160	English, from which almost half are annotated with a mood belonging to one of the four quadrants, described as follows: Quadrant1 bellicose, tense, alarmed, envious, hateful, angry, enraged, de ant, annoyed, jealous, indignant, frustrated, distressed, disgusted, suspicious, discontented, bitter, insulted, distrustful, startled, contemptuous and impatient." ></td>
	<td class="line x" title="29:160	Quadrant2 apathetic, disappointed, miserable, dissatis ed, taken aback, worried, languid, feel guilt, ashamed, gloomy, sad, uncomfortable, embarrassed, melancholic, depress, desperate, hesitant, bored, wavering, droopy, tired, insecured, anxious, lonely and doubtful." ></td>
	<td class="line x" title="30:160	Quadrant3 feel well, impressed, pleased, amourous, astonished, glad, content, hopeful, solemn, attentive, longing, relaxed, serious, serene, content, at ease, friendly, satis ed, calm, contemplative, polite, pensive, peaceful, conscientious, empathic, reverent and sleepy." ></td>
	<td class="line x" title="31:160	Quadrant4 happy, ambitious, amused, adventurous, aroused, astonished, triumphant, excited, 56 conceited, self con dent, courageous, feeling superior, enthusiastic, light hearthed, determined, passionate, expectant, interested, joyous and delighted." ></td>
	<td class="line x" title="32:160	In our experiments, we used 15662 from quadrant Q1 (see Figure 1), 54940 from Q2, 49779 from Q3 and 35634 from Q4." ></td>
	<td class="line x" title="33:160	3 Experiment 1: Distinguishing the four Quadrants Our hypothesis is that the classi cation of two disjoint sets of moods should yield a classi cation accuracy signi cantly above a baseline of 50%." ></td>
	<td class="line x" title="34:160	To verify our hypothesis, we conducted a series of experiments using machine learning to classify weblog posts according to their mood, each class corresponding to one particular quadrant." ></td>
	<td class="line x" title="35:160	We used Support Vector Machines (Joachims, 2001) with three basic classic features (unigrams, POS and stems) to classify the posts as belonging to one quadrant or one of the three others." ></td>
	<td class="line x" title="36:160	For each classi cation task, we extracted randomly 1000 testing examples, and trained separately with 2000, 4000, 8000 and 16000 examples." ></td>
	<td class="line x" title="37:160	In each case, examples were divided equally among positive and negative examples3." ></td>
	<td class="line x" title="38:160	The set of features used varied for each of these tasks, they were selected by thresholding each (distinct) training data set, after removing words (unigrams) from the categories poor in affective content (prepositions, determiners, etc.)." ></td>
	<td class="line x" title="39:160	To qualify as a feature, each unigram, POS or stem had to occur at least three times in the training data." ></td>
	<td class="line x" title="40:160	The value of each feature corresponds to its number of occurence in the training examples." ></td>
	<td class="line x" title="41:160	3.1 Results Our hypothesis is that, if the four quadrants depicted in Figure 1 are a suitable arrangement for affective states in the EA space, a classi er should perform signi cantly better than chance (50%)." ></td>
	<td class="line x" title="42:160	Table 1 shows the results for the binary classi cation of the quadrants." ></td>
	<td class="line x" title="43:160	In this table, the rst column identi es the classi cation task in the form P vs N, where P stands for positive examples and N for negative examples." ></td>
	<td class="line x" title="44:160	The Random row shows results for selecting positive and negative examples randomly from all four quadrants." ></td>
	<td class="line x" title="45:160	By 3For instance, 1000 = 500 positives from one QUADRANT + 500 negatives among the other three QUADRANTS." ></td>
	<td class="line x" title="46:160	micro-averaging accuracy for the classi cation of each quadrant vs all others (rows 10 to 13), we obtain at least 60% accuracy for the four binary classi cations of the quadrants4." ></td>
	<td class="line x" title="47:160	The rst six rows show evidence that each quadrant forms a distinctive whole, as the classifer can easily decide between any two of them." ></td>
	<td class="line x" title="48:160	Testing Size of training set 1000 examples 2k 4k 8k 16k Q1 vs Q3 67% 70% 72% 73% Q2 vs Q4 61% 64% 65% 67% Q1 vs Q2 64% 66% 68% 69% Q2 vs Q3 58% 59% 59% 59% Q3 vs Q4 59% 60% 60% 61% Q4 vs Q1 69% 72% 73% 75% Q1+4 vs Q2+3 56% 58% 58% 61% Q3+4 vs Q1+2 62% 65% 67% 66% Random 49% 52% 50% 50% Q1 vs Q2+3+4 67% 72% 72% 73% Q2 vs Q1+3+4 59% 60% 63% 63% Q3 vs Q1+2+4 57% 58% 58% 59% Q4 vs Q1+2+3 60% 63% 65% 65% Micro-accuracy 61% 64% 65% 65% Table 1: Accuracy of binary classi cation 3.2 Analysis of Results We introduce now table 2 that shows two thresholds of signi cance (1% and 5%) for the interpretation of current and coming results." ></td>
	<td class="line x" title="49:160	For example, if we have 1000 trials with each trial having a probability of success of 0.5, the likelihood of getting at least 53.7% of the trials right is only 1%." ></td>
	<td class="line x" title="50:160	This gives us a baseline to see how signi cantly well above chance a classi er performs." ></td>
	<td class="line x" title="51:160	The SVM algorithm has linearly separated the data for each quadrant according to lexical and POS content (the features)." ></td>
	<td class="line x" title="52:160	The most sensible explanation is that the features for each class (quadrant) are semantically related, a piece of information which is relevant for the model (see section 4)." ></td>
	<td class="line x" title="53:160	It is safe to conclude that the results cannot be allocated to chance, that there is something else at work that explains the 4Micro-averaged accuracy is de ned as: summationtext i (tpi + tni)summationtext i (tpi + tni + fpi + fni) where tp stands for true positive, fn for false negative, etc. 57 Trials Prob(Success) 1% 5% 1000 0.50 53.7% 52.6% 750 0.50 54.3% 53.1% 500 0.50 55.2% 53.6% 250 0.50 57.2% 55.2% 1000 0.25 28.2% 27.3% 750 0.25 28.7% 27.6% 500 0.25 29.6% 28.2% 250 0.25 31.6% 29.6% Table 2: Statistical Signi cance accuracies consistently well above a baseline, and this something else is the typology." ></td>
	<td class="line x" title="54:160	These results show that the abstraction offered by the four quadrants in the model seems correct." ></td>
	<td class="line x" title="55:160	This is also supported by the observation that the classi er shows no improvements over the baseline if trained over a random selection of examples in the entire space." ></td>
	<td class="line x" title="56:160	4 Experiment 2: Classification using Semantic Orientation from Association Our next goal is to be able to classify a text according to more than four classes (positive/negative, active/passive), by undertaking multi-category classi cation of texts according to particular regions of the space, (such as angry, sad, etc.)." ></td>
	<td class="line x" title="57:160	In order to do that we need a scoring system for each axis." ></td>
	<td class="line x" title="58:160	In the following experiments we explore the use of such scores and give some insights into how to transform these scores of affect as measures of affect." ></td>
	<td class="line x" title="59:160	Using binary classi ers, we have already established that if we look at the lexical contents of weblog posts tagged according to their mood by their author, these mood classes tend to cluster according to a two-dimensional typology de ned by their semantic orientation: positive or negative (evaluation), active or passive (activity)." ></td>
	<td class="line x" title="60:160	Beyond academic importance, the typology really becomes of practical interest if we can classify the posts using pre-de ned automated scores for both axis." ></td>
	<td class="line x" title="61:160	One strategy of scoring is to extract phrases, including single words, which are good indicators of subjectivity in texts, and score them according to how they relate or associate to one or the other extremity of each axis." ></td>
	<td class="line x" title="62:160	This strategy, called Semantic Orientation (SO) from Association (A) has been used successfully (Turney and Littman, 2003) to classify texts or adjectives of all sorts according to their sentiments (in our typology this corresponds to the evaluation dimension)." ></td>
	<td class="line x" title="63:160	According to these scores, a text or adjective can be said to have, for example, a more or less positive or negative evaluation." ></td>
	<td class="line x" title="64:160	We will use this strategy to go further in the validation of our model of affective states by scoring also the activity dimension; to our knowledge, this is the rst time this strategy is employed to get (text) scores for dimensions other than evaluation." ></td>
	<td class="line x" title="65:160	In SO-A, we score the strength of the association between an indicator from the text and a set of positive or negative words (the paradigms Pwords and Nwords) capturing the very positive/active or negative/passive semantic orientation of the axis poles." ></td>
	<td class="line x" title="66:160	To get the SO-A of a text, we sum over positive scores for indicators positively related to Pwords and negatively related to Nwords and negative scores for indicators positively related to Nwords and negatively related to Pwords." ></td>
	<td class="line x" title="67:160	In mathematical terms, the SO-A of a text is: Textsummationdisplay ind ( Pwordssummationdisplay p A(ind, p)  Nwordssummationdisplay n A(ind, n)) where ind stands for indicator." ></td>
	<td class="line x" title="68:160	Note that the quantity of Pwords must be equal to Nwords." ></td>
	<td class="line x" title="69:160	To compute A, (Kamps et aal., 2004) focus on the use of lexical relations de ned in WordNet5 and de ne a distance measure between two terms which amounts to the length of the shortest path that connects the two terms." ></td>
	<td class="line x" title="71:160	This strategy is interesting because it constrains all values to belong to the [-1,+1] range, but can be applied only to a nite set of indicators and has yet to be tested for the classi cation of texts." ></td>
	<td class="line x" title="72:160	(Turney and Littman, 2003) use Pointwise Mutual Information Information Retrieval (PMI-IR); PMI-IR operates on a wider variety of multi-words indicators, allowing for contextual information to be taken into account, has been tested extensively on different types of texts, and the scoring system can be potentially normalized between [-1,+1], as we will soon see." ></td>
	<td class="line oc" title="73:160	PMI (Church and Hanks, 1990) between two phrases is de ned as: log2 prob(ph1 is near ph2)prob(ph 1)  prob(ph2) PMI is positive when two phrases tend to co-occur and negative when they tend to be in a complementary distribution." ></td>
	<td class="line x" title="74:160	PMI-IR refers to the fact 5http://wordnet.princeton.edu/." ></td>
	<td class="line x" title="75:160	58 that, as in Informtion Retrieval (IR), multiple occurrences in the same document count as just one occurrence: according to (Turney and Littman, 2003), this seems to yield a better measure of semantic similarity, providing some resistance to noise." ></td>
	<td class="line x" title="76:160	Computing probabilities using hit counts from IR, this yields to a value for PMI-IR of: logn N  (hits(ph1 NEAR ph2) + 1/N)(hits(ph 1) + 1)  (hits(ph2) + 1) where N is the total number of documents in the corpus." ></td>
	<td class="line x" title="77:160	We are going to use this method for computing A in SO-A, which we call SO-PMI-IR." ></td>
	<td class="line x" title="78:160	The con guration depicted in the remaining of this section follows mostly (Turney and Littman, 2003)." ></td>
	<td class="line x" title="79:160	Smoothing values (1/N and 1) are chosen so that PMI-IR will be zero for words that are not in the corpus, two phrases are considered NEAR if they co-occur within a window of 20 words, and log2 has been replaced by logn, since the natural log is more common in the literature for log-odds ratio and this makes no difference for the algorithm." ></td>
	<td class="line x" title="80:160	Two crucial aspects of the method are the choice of indicators to be extracted from the text to be classi ed, as well as the sets of positive and negative words to be used as paradigms for the evaluation and activity dimensions." ></td>
	<td class="line x" title="81:160	The ve part-ofspeech (POS) patterns from (Turney, 2002) were used for the extraction of indicators, all involving at least one adjective or adverb." ></td>
	<td class="line x" title="82:160	POS tags were acquired with TreeTagger (Schmid, 1994)6." ></td>
	<td class="line x" title="83:160	Ideally, words used as paradigms should be context insensitive, i.e their semantic orientation is either always positive or negative." ></td>
	<td class="line x" title="84:160	The adjectives good, nice, excellent, positive, fortunate, correct, superior and bad, nasty, poor, negative, unfortunate, wrong, inferior were used as near pure representations of positive and negative evaluation respectively, while fast, alive, noisy, young and slow, dead, quiet, old as near pure representations of active and passive activity (Summers, 1970)." ></td>
	<td class="line x" title="85:160	Departing from (Turney and Littman, 2003), who uses the Alta Vista advanced search with approximately 350 millions web pages, we used the Waterloo corpus7, with approximately 46 millions pages." ></td>
	<td class="line x" title="86:160	To avoid introducing confusing heuristics, we stick to the con guration described above, but (Turney and Littman, 2003) have experimented with different con guation in computing SO-PMIIR." ></td>
	<td class="line x" title="87:160	6(Turney and Littman, 2003) uses (Brill, 1994)." ></td>
	<td class="line x" title="88:160	7http://canola1.uwaterloo.ca/." ></td>
	<td class="line x" title="89:160	4.1 The Typology and SO-PMI-IR We now use the typology with an automated scoring method for semantic orientation." ></td>
	<td class="line x" title="90:160	The results are presented in the form of a Confusion Matrix (CM)." ></td>
	<td class="line x" title="91:160	In this and the following matrices, the topleft cell indicates the overall accuracy8, the POSitive (ACTive) and NEGative (PASsive) columns represent the instances in a predicted class, the P/T column (where present) indicates the average number of patterns per text (blog post), E/P indicates the average evaluation score per pattern and A/P indicates the average activity score per pattern." ></td>
	<td class="line x" title="92:160	Each row represents the instances in an actual class9." ></td>
	<td class="line x" title="93:160	First, it is useful to get a clear idea of how the SO-PMI-IR experimental setup we presented compares with (Turney and Littman, 2003) on a human-annotated set of words according to their evaluation dimension: the General Inquirer (GI, (Stone, 1966)) lexicon is made of 3596 words (1614 positives and 1982 negatives)10." ></td>
	<td class="line x" title="94:160	Table 3 summarizes the results." ></td>
	<td class="line x" title="95:160	(Turney and Littman, (U) 76.4% POS NEG E/P POS(1614) 59.3% 40.7% 1.5 NEG(1982) 9.6% 90.4% -4.3 (T) 82.8% POS NEG E/P POS(1614) 81.2% 18.8% 3.2 NEG(1982) 15.8% 84.2% -3.6 Table 3: CM for the GI: (U)Us and (T)(Turney and Littman, 2003) 2003) reports an accuracy of 82.8% while classifying those words, while our experiment yields an accuracy of 76.4% for the same words." ></td>
	<td class="line x" title="96:160	Their results show that their classi er errs very slightly towards the negative pole (as shown by the accuracies of both predicted classes) and has a very balanced distribution of the word scores (as shown by the almost equal but opposite in signs values of E/Ps)." ></td>
	<td class="line x" title="97:160	This is some evidence that the paradigm words are appropriate as near pure representations of positive and negative evaluation." ></td>
	<td class="line x" title="98:160	By contrast, 8Recall that table 2 gives an interpretation of the statistical signi ance of accuracy, with trials  750 and Prob(success) = 0.5." ></td>
	<td class="line x" title="99:160	9For example, in the comparative evaluation shown in table 3, our classi er classi ed 59.3% of the 1614 positive instances as positive and 40.7% as negative, with an average score of 1.5 per pattern." ></td>
	<td class="line x" title="100:160	10Note that all moods in the typology present in the GI have the same polarity for evaluation in both, which is some evidence in favour of the typology." ></td>
	<td class="line x" title="101:160	59 our classi er appears to be more strongly biased towards the negative pole, probably due to the use of different corpora." ></td>
	<td class="line x" title="102:160	This bias11should be kept in mind in the interpretation of the results to come." ></td>
	<td class="line x" title="103:160	The second experiment focuses on the words from the typology." ></td>
	<td class="line x" title="104:160	Table 4 shows the results." ></td>
	<td class="line x" title="105:160	The 81.1% POS NEG P/T E/P POS(43) 60.5% 39.5% 1 0.4 NEG(47) 0.0% 100.0% 1 -6.4 66.7% ACT PAS P/T A/P ACT(39) 33.3% 66.7% 1 -0.9 PAS(51) 7.8% 92.2% 1 -2.9 Table 4: CM for the Typology affective states value of 1 under P/T re ects the fact that the experiment amounts, in practical terms, to classifying the annotation of the post (a single word)." ></td>
	<td class="line x" title="106:160	For the evaluation dimension, there is another shift towards the negative pole of the axis, which suggests that words in the typology are distributed not exactly as shown on gure 1, but instead appear to have a true location shifted towards the negative pole." ></td>
	<td class="line x" title="107:160	The activity dimension also appear to have a negative (i.e passive) bias." ></td>
	<td class="line x" title="108:160	There are two main possible reasons for that: words in the typology should be shifted towards the passive pole (as in the evaluation case), or the paradigm words for the passive pole are not pure representations of the extremity of the pole 12." ></td>
	<td class="line x" title="109:160	Having established that our classi er has a negative bias for both axes, we now turn to the classi cation of the quadrants per se." ></td>
	<td class="line x" title="110:160	In the next section, we used SO-PMI-IR to classify 1000 randomnly selected blog posts from our corpus, i.e 250 in each of the four quadrants." ></td>
	<td class="line x" title="111:160	Some of these posts were found to have no pattern and were therefore not classi ed, which means that less than 1000 posts were actually classi ed in each experiment." ></td>
	<td class="line x" title="112:160	We also report on the classi cation of an important subcategory of these moods called the Big Six emotions." ></td>
	<td class="line x" title="113:160	11Bias can be introduced by the use of a small corpus, inadequate paradigm words or typology." ></td>
	<td class="line x" title="114:160	In practice, a quick x for neutralizing bias would be to normalize the SO-PMI-IR values by subtracting the average." ></td>
	<td class="line x" title="115:160	This work aims at tuning the model to remove bias introduced by unsound paradigm words or typology." ></td>
	<td class="line x" title="116:160	12At the time of experimenting, we were not aware of an equivalent of the GI to independently verify our paradigm words for activity, but one reviewer pointed out such a resource, see http://www.wjh.harvard.edu/ inquirer/spreadsheet_guide.htm." ></td>
	<td class="line x" title="117:160	4.2 Results Of the 1000 blog posts, there were 938 with at least one pattern." ></td>
	<td class="line x" title="118:160	Table 5 shows the accuracy for the classi cation of these posts." ></td>
	<td class="line x" title="119:160	56.8% POS NEG P/T E/P POS(475) 76.2% 23.8% 10 5.2 NEG(463) 63.1% 36.9% 9 3.5 51.8% ACT PAS P/T A/P ACT(461) 20.6% 79.4% 8 -4.3 PAS(477) 18.0% 82.0% 11 -4.2 Table 5: CM for all Moods An important set of emotions found in the literature (Ekman, 1972) has been termed the Big Six." ></td>
	<td class="line x" title="120:160	These emotions are fear, anger, happiness, sadness, surprise and disgust." ></td>
	<td class="line x" title="121:160	We have used a minimally extended set, adding love and desire (Cowie and Cornelius, 2002), to cover all four quadrants (we called this set the Big Eight)." ></td>
	<td class="line x" title="122:160	Fear, anger and disgust belong to quadrant 1, sadness and surprise (we have taken it to be a synonym of taken aback in the typology) belong to quadrant 2, love and desire (taken to be synonyms of amorous and longing in the typology) belong to quadrant 3 and happy to quadrant 4." ></td>
	<td class="line x" title="123:160	Table 6 shows the results for the classi cation of the blog posts that were tagged with one of these emotions." ></td>
	<td class="line x" title="124:160	This amounts to classifying the posts containing only the Big Eight affective states." ></td>
	<td class="line x" title="125:160	59.0% POS NEG P/T E/P POS(467) 72.4% 27.6% 9 5.1 NEG(351) 58.7% 41.3% 6 2.3 54.9% ACT PAS P/T A/P ACT(357) 23.8% 76.2% 8 -4.4 PAS(461) 21.0% 79.0% 8 -4.6 Table 6: CM for the Big Eight In the remaining two experiments, blog posts have been classifed using a discrete scoring system." ></td>
	<td class="line x" title="126:160	Disregarding the real value of SO, each pattern was scored with a value of +1 for a positive score and -1 for a negative score." ></td>
	<td class="line x" title="127:160	This amounts to counting the number of patterns on each side and has the advantage of providing a normalized value for E/T and A/T between -1 and +1." ></td>
	<td class="line x" title="128:160	Normalized values are the rst step towards a measure of affect, not merely a score, in the sense that it gives an estimate of the strength of affect." ></td>
	<td class="line x" title="129:160	We have not 60 classi ed the posts for which the resulting score was zero, which means that even fewer posts (741) than the previous experiment were actually evaluated." ></td>
	<td class="line x" title="130:160	Table 7 shows the results for all moods and table 8 for the Big Eight." ></td>
	<td class="line x" title="131:160	55.7% POS NEG P/T E/P POS(374) 53.2% 46.8% 11 0.03 NEG(367) 41.7% 58.3% 9 -0.11 53.3% ACT PAS P/T A/P ACT(357) 21.8% 78.2% 8 -0.3 PAS(384) 17.4% 82.6% 12 -0.34 Table 7: CM for all Moods: Discrete scoring 59.8% POS NEG P/T E/P POS(373) 52.3% 47.7% 10 0.01 NEG(354) 32.2% 67.8% 9 -0.2 52.8% ACT PAS P/T A/P ACT(361) 25.8% 74.2% 10 -0.3 PAS(366) 20.5% 79.5% 9 -0.4 Table 8: CM for the Big Eight: Discrete scoring 4.3 Analysis of Results Our concerns about the paradigm words for evaluating the activity dimension are clearly revealed in the classi cation results." ></td>
	<td class="line x" title="132:160	The classi er shows a heavy negative (passive) bias in all experiments." ></td>
	<td class="line x" title="133:160	The overall accuracy for activity is consistently below that for evaluation: three of them are not statistically signi cant at 1% (51.8%, 53.3% and 52.8%) and two at even 5% (51.8% and 52.8%)." ></td>
	<td class="line x" title="134:160	The classi er appears particularly confused in table 5, averaging a score for active posts (-4.3) smaller than for passive posts (-4.2)." ></td>
	<td class="line x" title="135:160	It is not impossible that the moods present in the typology may have to be shifted towards the passive dimension, but further research should look rst at nding better paradigm words for activity." ></td>
	<td class="line x" title="136:160	A good starting point for the calibration of the classi er for activity is the creation of a list of humanannotated words for activity, comparable in size to the GI list, combined with an experiment similar to the one for which results are reported in table 3." ></td>
	<td class="line x" title="137:160	With regards to the evaluation dimension, tables 5 and 6 reveal a positive bias (despite having a classi er which has a built-in negative bias, see section 4.1)." ></td>
	<td class="line x" title="138:160	Possible explanations for this phenomenon include the use of irony by people in negative posts, blogs which are expressed in more positive terms than their annotation would suggest, and failure to detect negative contexts for patterns one example of the latter is provided in table 9." ></td>
	<td class="line x" title="139:160	This phenomena appears to be alleviated Mood: bored (evaluation-) Post: gah!!" ></td>
	<td class="line x" title="140:160	i need new music, any suggestions?" ></td>
	<td class="line x" title="141:160	by the way, GOOD MUSIC." ></td>
	<td class="line x" title="142:160	Patterns: new music [JJ NN] +4.38 GOOD MUSIC [JJ NN] +53.40 Average SO: +57.78 (evaluation+) Table 9: Missclassi ed post by the use of discrete scores (see tables 7 and 8)." ></td>
	<td class="line x" title="143:160	One way of re ning the scoring system is to reduce the effect of scoring antonyms as high as synonyms by not counting co-occurences in the corpus where the word not is in the neighbourhood (Turney, 2001)." ></td>
	<td class="line x" title="144:160	Also, The long-term goal of this research is to be able to classify texts by locating their normalized scores for evaluation and activity between -1 and +1, and we have suggested a simple method of achieving that by averaging over discrete scores." ></td>
	<td class="line x" title="145:160	However, by combining individual results for evaluation and activity for each post13, we can already classify text into one of the four quadrants, and we can expect the average accuracy of this classi cation to be approximately the product of the accuracy for each dimension." ></td>
	<td class="line x" title="146:160	Table 10 shows the results for the classi cation directly into quadrants of the 727 posts already classi ed into halves (E, A) in table 8." ></td>
	<td class="line x" title="147:160	The overall accuracy is 31.1% (expected accuracy is 59.8% * 52.8% = 31.6%)." ></td>
	<td class="line x" title="148:160	There are biases towards Q2 and Q3, but no clear cases of confusion between two or more classes." ></td>
	<td class="line x" title="149:160	31.1% Q1 Q2 Q3 Q4 Q1(180) 21.1% 47.8% 22.2% 8.9% Q2(174) 15.5% 51.1% 25.3% 8.0% Q3(192) 9.9% 42.2% 40.1% 7.8% Q4(181) 9.4% 33.7% 44.8% 12.2% Table 10: CM for Big Eight: Discrete scoring Finally, our experiments show no correlation between the length of a post (in number of patterns) and the accuracy of the classi cation." ></td>
	<td class="line x" title="150:160	13For example, a post with Eand A+ would be classi ed in Q1." ></td>
	<td class="line x" title="151:160	61 5 Conclusion and Future Work In this paper, we have used a machine learning approach to show that there is a relation between the semantic content of texts and the affective state they (wish to) convey, so that a typology of affective states based on semantic association is a good description of the distribution of affect in a twodimensional space." ></td>
	<td class="line x" title="152:160	Using automated methods to score semantic association, we have demonstrated a method to compute semantic orientation on both dimensions, giving some insights into how to go beyond the customary sentiment analysis." ></td>
	<td class="line x" title="153:160	In the classi cation experiments, accuracies were always above a random baseline, although not always statistically signi cant." ></td>
	<td class="line x" title="154:160	To improve the typology and the accuracies of classi ers based on it, a better calibration of the activity axis is the most pressing task." ></td>
	<td class="line x" title="155:160	Our next steps are experiments aiming at re ning the translation of scores to normalized measures, so that individual affects can be distinguished within a single quadrant." ></td>
	<td class="line x" title="156:160	Other interesting avenues are studies investigating how well the typology can be ported to other textual data domains, the inclusion of a neutral tag, and the treatment of texts with multiple affects." ></td>
	<td class="line x" title="157:160	Finally, the domain of weblog posts is attractive because of the easy access to annotated data, but we have found through our experiments that the content is very noisy, annotation is not always consistent among bloggers, and therefore classi cation is dif cult." ></td>
	<td class="line x" title="158:160	We should not underestimate the positive effects that cleaner data, consistent tagging and access to bigger corpora would have on the accuracy of the classi er." ></td>
	<td class="line x" title="159:160	Acknowledgement This work was partially funded by the European Commission through the Network of Excellence EPOCH ( Excellence in Processing Open Cultural Heritage )." ></td>
	<td class="line x" title="160:160	Thanks to Peter Turney for the provision of access to the Waterloo MultiText System." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1101
Linguistic Distances
Nerbonne, John;Hinrichs, Erhard W.;"></td>
	<td class="line x" title="1:234	Proceedings of the Workshop on Linguistic Distances, pages 16, Sydney, July 2006." ></td>
	<td class="line x" title="2:234	c2006 Association for Computational Linguistics Linguistic Distances John Nerbonne Alfa-informatica University of Groningen j.nerbonne@rug.nl Erhard Hinrichs Seminar fur Sprachwissenschaft Universitat Tubingen eh@sfs.uni-tuebingen.de Abstract In many theoretical and applied areas of computational linguistics researchers operate with a notion of linguistic distance or, conversely, linguistic similarity, which is the focus of the present workshop." ></td>
	<td class="line x" title="3:234	While many CL areas make frequent use of such notions, it has received little focused attention, an honorable exception being Lebart & Rajman (2000)." ></td>
	<td class="line x" title="4:234	This workshop brings a number of these strands together, highlighting a number of common issues." ></td>
	<td class="line x" title="5:234	1 Introduction In many theoretical and applied areas of computational linguistics researchers operate with a notion of linguistic distance or, conversely, linguistic similarity, which is the focus of the present workshop." ></td>
	<td class="line x" title="6:234	While many CL areas make frequent use of such notions, it has received little focused attention, an honorable exception being Lebart & Rajman (2000)." ></td>
	<td class="line x" title="7:234	In information retrieval (IR), also the focus of Lebart & Rajmans work, similarity is at heart of most techniques seeking an optimal match between query and document." ></td>
	<td class="line x" title="8:234	Techniques in vector space models operationalize this via (weighted) cosine measures, but older tf/idf models were also arguably aiming at a notion of similarity." ></td>
	<td class="line x" title="9:234	Word sense disambiguation models often work with a notion of similarity among the contexts within which word (senses) appear, and MT identifies candidate lexical translation equivalents via a comparable measure of similarity." ></td>
	<td class="line x" title="10:234	Many learning algorithms currently popular in CL, including not only supervised techniques such as memorybased learning (k-nn) and support-vector machines, but also unsupervised techniques such as Kohonen maps and clustering, rely essentially on measures of similarity for their processing." ></td>
	<td class="line x" title="11:234	Notions of similarity are often invoked in linguistic areas such as dialectology, historical linguistics, stylometry, second-language learning (as a measure of learners proficiency), psycholinguistics (accounting for lexical neighborhood effects, where neighborhoods are defined by similarity) and even in theoretical linguistics (novel accounts of the phonological constraints on semitic roots)." ></td>
	<td class="line x" title="12:234	This volume reports on a workshop aimed at bringing together researchers employing various measures of linguistic distance or similarity, including novel proposals, especially to demonstrate the importance of the abstract properties of such measures (consistency, validity, stability over corpus size, computability, fidelity to the mathematical distance axioms), but also to exchange information on how to analyze distance information further." ></td>
	<td class="line x" title="13:234	We assume that there is always a hidden variable in the similarity relation, so that we should always speak of similarity with respect to some property, and we suspect that there is such a plethora of measures in part because researchers are often inexplicit on this point." ></td>
	<td class="line x" title="14:234	It is useful to tease the different notions apart." ></td>
	<td class="line x" title="15:234	Finally, it is most intriguing to try to make a start on understanding how some of the different notions might construed as alternative realizations of a single abstract notion." ></td>
	<td class="line x" title="16:234	2 Pronunciation John Laver, the author of the most widely used textbook in phonetics, claimed that one of the 1 most basic concepts in phonetics, and one of the least discussed, is that of phonetic similarity [boldface in original, JN & EH] (Laver, 1994, p. 391), justifying the attention the workshop pays to it." ></td>
	<td class="line x" title="17:234	Laver goes on to sketch the work that has been done on phonetic similarity, or, more exactly, phonetic distance, in particular, the empirical derivation of confusion matrices, which indicate the likelihood with which people or speech recognition systems confusion one sound for another." ></td>
	<td class="line x" title="18:234	Miller & Nicely (1955) founded this approach with studies of how humans confused some sounds more readily than others." ></td>
	<td class="line x" title="19:234	Although confusability is a reasonable reflection of phonetic similarity, it is perhaps worth noting that confusion matrices are often asymmetric, suggesting that something more complex is at play." ></td>
	<td class="line x" title="20:234	Clark & Yallop (1995, p. 319ff) discuss this line of work further, suggesting more sophisticated analyses which aggregate confusion matrices based on segments." ></td>
	<td class="line x" title="21:234	In addition to the phonetic interest (above), phonologists have likewise shown interest in the question of similarity, especially in recent work." ></td>
	<td class="line x" title="22:234	Albright and Hayes (2003) have proposed a model of phonological learning which relies on minimal generalization." ></td>
	<td class="line x" title="23:234	The idea is that children learn e.g. rules of allomorphy on the basis not merely of rules and individual lexical exceptions (the earlier standard wisdom), but rather on the basis of slight but reliable generalizations." ></td>
	<td class="line x" title="24:234	An example is the formation of the past tense of verbs ending in [IN], ing (fling, sing, sting, spring, string) that build past tenses as ung [2N]." ></td>
	<td class="line x" title="25:234	We omit details but note that the minimal generalization is minimally DISTANT in pronunciation." ></td>
	<td class="line x" title="26:234	Frisch, Pierrehumbert & Broe (2004) have also kindled an interest in segmental similarity among phonologists with their claim that syllables in Semitic languages are constrained to have unlike consonants in syllable onset and coda." ></td>
	<td class="line x" title="27:234	Their work has not gone unchallenged (Bailey and Hahn, 2005; Hahn and Bailey, 2005), but it has certainly created further theoretical interest in phonological similarity." ></td>
	<td class="line x" title="28:234	There has been a great deal of attention in psycholinguistics to the the problem of word recognition, and several models appeal explicitly to the degree of phonetic similarity among the words (Luce and Pisoni, 1998, p. 1), but most of these models employ relatively simple notions of sequence similarity and/or, e.g., the idea that distance may be operationalized by the number or replacements needed to derive one word from anotherignoring the problem of similarity among words of different lengths (Vitevitch and Luce, 1999)." ></td>
	<td class="line x" title="29:234	Perhaps more sophisticated computational models of pronunciation distance could play a role in these models in the future." ></td>
	<td class="line x" title="30:234	Kessler (1995) showed how to employ edit distance to operationalize pronunciation difference in order to investigate dialectology more precisely, an idea which, particular, Heeringa (2004) pursued at great length." ></td>
	<td class="line x" title="31:234	Kondrak (2002) created a variant of the dynamic programming algorithm used to compute edit distance which he used to identify cognates in historical linguistics." ></td>
	<td class="line x" title="32:234	McMahon & McMahon (2005) include investigations of pronunciation similarity in their recent book on phylogenetic techniques in historical linguistics." ></td>
	<td class="line x" title="33:234	Several of the contributions to this volume build on these earlier efforts or are relevant to them." ></td>
	<td class="line x" title="34:234	Kondrak and Sherif (this volume) continue the investigation into techniques for identifying cognates, now comparing several techniques which rely solely on parameters set by the researcher to machine learning techniques which automatically optimize those parameters." ></td>
	<td class="line x" title="35:234	They show the the machine learning techniques to be superior, in particular, techniques basic on hidden Markov models and dynamic Bayesian nets." ></td>
	<td class="line x" title="36:234	Heeringa et al.(this volume) investigate several extensions of the fundamental edit distance algorithm for use in dialectology, including sensitivity to order and context as well syllabicity constraints, which they argue to be preferable, and length normalization and graded weighting schemes, which they argue against." ></td>
	<td class="line x" title="38:234	Dinu & Dinu (this volume) investigate metrics on string distances which attach more importance to the initial parts of the string." ></td>
	<td class="line x" title="39:234	They embed this insight into a scheme in whichn-grams are ranked (sorted) by frequency, and the difference in the rankings is used to assay language differences." ></td>
	<td class="line x" title="40:234	Their paper proves that difference in rankings is a proper mathematical metric." ></td>
	<td class="line x" title="41:234	Singh (this volume) investigates the technical question of identifying languages and character encoding systems from limited amounts of text." ></td>
	<td class="line x" title="42:234	He collects about 1,000 or so of the most frequent n-grams of various sizes and then classifies next texts based on the similarity between the fre2 quency distributions of the known texts with those of texts to be classified." ></td>
	<td class="line x" title="43:234	His empirical results show mutual cross entropy to identify similarity most reliably, but there are several close competitors." ></td>
	<td class="line x" title="44:234	3 Syntax Although there is less interest in similarity at the syntactic level among linguistic theorists, there is still one important areas of theoretical research in which it could play an important role and several interdisciplinary studies in which similarity and/or distant is absolutely crucial." ></td>
	<td class="line x" title="45:234	Syntactic TYPOLOGY is an area of linguistic theory which seeks to identify syntactic features which tend to be associated with one another in all languages (Comrie, 1989; Croft, 2001)." ></td>
	<td class="line x" title="46:234	The fundamental vision is that some sorts of languages may be more similar to one anothertypologicallythan would first appear." ></td>
	<td class="line x" title="47:234	Further, there are two interdisciplinary linguistic studies in which similarity and/or distance plays a great role, including similarity at the syntactic level (without, however, exclusively focusing on syntax)." ></td>
	<td class="line x" title="48:234	LANGUAGE CONTACT studies seek to identify the elements of one language which have been adopted in a second in a situation in which two or more languages are used in the same community (Thomason and Kaufmann, 1988; van Coetsem, 1988)." ></td>
	<td class="line x" title="49:234	Naturally, these may be non-syntactic, but syntactic CONTAMINATION is a central concept which is recognized in contaminated varieties which have become more similar to the languages which are the source of contamination." ></td>
	<td class="line x" title="50:234	Essentially the same phenomena is studied in SECOND-LANGUAGE LEARNING, in which syntactic patterns from a dominant, usually first, language are imposed on a second." ></td>
	<td class="line x" title="51:234	Here the focus is on the psychology of the individual language user as opposed to the collective habits of the language community." ></td>
	<td class="line x" title="52:234	Nerbonne and Wiersma (this volume) collect frequency distributions of part-of-speech (POS) trigrams and explore simple measures of distance between these." ></td>
	<td class="line x" title="53:234	They approach issues of statistical significance using permutation tests, which requires attention to tricky issues of normalization between the frequency distributions." ></td>
	<td class="line x" title="54:234	Homola & Kubon (this volume) join Nerbonne and Wiersma in advocating a surface-oriented measure of syntactic difference, but base their measure on dependency trees rather than POS tags, a more abstract level of analysis." ></td>
	<td class="line x" title="55:234	From there they propose an analogue to edit distance to gauge the degree of difference." ></td>
	<td class="line x" title="56:234	The difference between two tree is the sum of the costs of the tree-editing operations needed to obtain one tree from another (Noetzel and Selkow, 1999)." ></td>
	<td class="line x" title="57:234	Emms (this volume) concentrates on applications of the notion tree similarity in particular in order to identify text which is syntactically similar to questions and which may therefore be expected to constitute an answer to the question." ></td>
	<td class="line x" title="58:234	He is able to show that the tree-distance measure outperforms sequence distance measures, at least if lexical information is also emphasized." ></td>
	<td class="line x" title="59:234	Kubler (this volume) uses the similarity measure in memory-based learning to parse." ></td>
	<td class="line x" title="60:234	This is a surprising approach, since memory-based techniques are normally used in classification tasks where the target is one of a small number of potential classifications." ></td>
	<td class="line x" title="61:234	In parsing, the targets may be arbitrarily complex, so a key step is select an initial structure in a memory-based way, and then to adapt it further." ></td>
	<td class="line x" title="62:234	In this paper Kubler first applies chunking to the sentence to be parsed and selects an initial parse based on chunk similarity." ></td>
	<td class="line x" title="63:234	4 Semantics While similarity as such has not been a prominent term in theoretical and computational research on natural language semantics, the study of LEXICAL SEMANTICS, which attempts to identify regularities of and systematic relations among word meanings, is more often than not predicated on an implicit notion of semantic similarity." ></td>
	<td class="line x" title="64:234	Research on the lexical semantics of verbs tries to identify verb classes whose members exhibit similar syntactic and semantic behavior." ></td>
	<td class="line x" title="65:234	In logic-based theories of word meaning (e.g. , Vendler (1967) and Dowty (1979)), verb classes are identified by similarity patterns of inference, while Levins (1993) study of English verb classes demonstrates that similarities of word meanings for verbs can be gleaned from their syntactic behavior, in particular from their ability or inability to participate in diatheses, i.e. patterns of argument alternations." ></td>
	<td class="line x" title="66:234	With the increasing availability of large electronic corpora, recent computational research on word meaning has focused on capturing the notion of context similarity of words." ></td>
	<td class="line oc" title="67:234	Such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the British 3 linguist J.R. Firth: You shall know a word by the company it keeps. (Firth, 1957, p. 11) Context similarity has been used as a means of extracting collocations from corpora, e.g. by Church & Hanks (1990) and by Dunning (1993), of identifying word senses, e.g. by Yarowski (1995) and by Schutze (1998), of clustering verb classes, e.g. by Schulte im Walde (2003), and of inducing selectional restrictions of verbs, e.g. by Resnik (1993), by Abe & Li (1996), by Rooth et al.(1999) and by Wagner (2004)." ></td>
	<td class="line x" title="69:234	A third approach to lexical semantics, developed by linguists and by cognitive psychologists, primarily relies on the intuition of lexicographers for capturing word meanings, but is also informed by corpus evidence for determining word usage and word senses." ></td>
	<td class="line x" title="70:234	This type of approach has led to two highly valued semantic resources: the Princeton WordNet (Fellbaum, 1998) and the Berkeley Framenet (Baker et al. , 1998)." ></td>
	<td class="line x" title="71:234	While originally developed for English, both approaches have been successfully generalized to other languages." ></td>
	<td class="line x" title="72:234	The three approaches to word meaning discussed above try to capture different aspects of the notion of semantic similarity, all of which are highly relevant for current and future research in computational linguistics." ></td>
	<td class="line x" title="73:234	In fact, the five papers that discuss issues of semantic similarity in the present volume build on insights from these three frameworks or address open research questions posed by these frameworks." ></td>
	<td class="line x" title="74:234	Zesch and Gurevych (this volume) discuss how measures of semantic similarityand more generally: semantic relatednesscan be obtained by similarity judgments of informants who are presented with word pairs and who, for each pair, are asked to rate the degree of semantic relatedness on a predefined scale." ></td>
	<td class="line x" title="75:234	Such similarity judgments can provide important empirical evidence for taxonomic models of word meanings such as wordnets, which thus far rely mostly on expert knowledge of lexicographers." ></td>
	<td class="line x" title="76:234	To this end, Zesch and Gurevych propose a corpus-based system that supports fast development of relevant data sets for large subject domains." ></td>
	<td class="line x" title="77:234	St-Jacques and Barri`ere (this volume) review and contrast different philosophical and psychological models for capturing the notion of semantic similarity and different mathematical models for measuring semantic distance." ></td>
	<td class="line x" title="78:234	They draw attention to the fact that, depending on which underlying models are in use, different notions of semantic similarity emerge and conjecture that different similarity metrics may be needed for different NLP tasks." ></td>
	<td class="line x" title="79:234	Dagan (this volume) also explores the idea that different notions of semantic similarity are needed when dealing with semantic disambiguation and language modeling tasks on the one hand and with applications such as information extraction, summarization, and information retrieval on the other hand." ></td>
	<td class="line x" title="80:234	Dridan and Bond (this volume) and Hachey (this volume) both consider semantic similarity from an application-oriented perspective." ></td>
	<td class="line x" title="81:234	Dridan and Bond employ the framework of robust minimal recursion semantics in order to obtain a more adequate measure of sentence similarity than can be obtained by word-overlap metrics for bag-of-words representations of sentences." ></td>
	<td class="line x" title="82:234	They show that such a more fine-grained measure, which is based on compact representations of predicate-logic, yields better performance for paraphrase detection as well as for sentence selection in question-answering tasks than simple word-overlap metrics." ></td>
	<td class="line x" title="83:234	Hachey considers an automatic content extraction (ACE) task, a particular subtask of information extraction." ></td>
	<td class="line x" title="84:234	He demonstrates that representations based on term cooccurrence outperform representations based on term-by-document matrices for the task of identifying relationships between named objects in texts." ></td>
	<td class="line x" title="85:234	Acknowledgments We are indebted to our program committee and to the incidental reviewers named in the organizational section of the book, and to others who remain anonymous." ></td>
	<td class="line x" title="86:234	We thank Peter Kleiweg for managing the production of the book and Therese Leinonen for discussions about phonetic similarity." ></td>
	<td class="line x" title="87:234	We are indebted to the Netherlands Organization for Scientific Research (NWO), grant 20002100, for cooperation between the Center for Language and Cognition, Groningen, and the Seminar fur Sprachwissenschaft, Tubingen, for support of the work which is reported on here." ></td>
	<td class="line x" title="88:234	We are also indebted to the Volkswagen Stiftung for their support of a joint project Measuring Linguistic Unity and Diversity in Europe that is carried out in cooperation with the Bulgarian Academy of Science, Sofia." ></td>
	<td class="line x" title="89:234	The work reported here is directly related to the research objectives of this project." ></td>
	<td class="line x" title="90:234	4 References Naoki Abe and Hang Li." ></td>
	<td class="line x" title="91:234	1996." ></td>
	<td class="line x" title="92:234	Learning word association norms using tree cut pair models." ></td>
	<td class="line x" title="93:234	In Proceedings of 13th International Conference on Machine Learning." ></td>
	<td class="line x" title="94:234	Adam Albright and Bruce Hayes." ></td>
	<td class="line x" title="95:234	2003." ></td>
	<td class="line x" title="96:234	Rules vs. analogy in English past tenses: A computational/experimental study." ></td>
	<td class="line x" title="97:234	Cognition, 90:119161." ></td>
	<td class="line x" title="98:234	Todd M. Bailey and Ulrike Hahn." ></td>
	<td class="line x" title="99:234	2005." ></td>
	<td class="line x" title="100:234	Phoneme Similarity and Confusability." ></td>
	<td class="line x" title="101:234	Journal of Memory and Language, 52(3):339362." ></td>
	<td class="line x" title="102:234	Collin F. Baker, Charles J. Fillmore, and John B. Lowe." ></td>
	<td class="line x" title="103:234	1998." ></td>
	<td class="line x" title="104:234	The Berkeley FrameNet project." ></td>
	<td class="line x" title="105:234	In Christian Boitet and Pete Whitelock, editors, Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics, pages 8690, San Francisco, California." ></td>
	<td class="line x" title="106:234	Morgan Kaufmann Publishers." ></td>
	<td class="line x" title="107:234	Kenneth Ward Church and Patrick Hanks." ></td>
	<td class="line x" title="108:234	1990." ></td>
	<td class="line x" title="109:234	Word association norms, mutual information, and lexicography." ></td>
	<td class="line x" title="110:234	Computational Linguistics, 16(1):2229." ></td>
	<td class="line x" title="111:234	John Clark and Colin Yallop." ></td>
	<td class="line x" title="112:234	1995." ></td>
	<td class="line x" title="113:234	An Introduction to Phonetics and Phonology." ></td>
	<td class="line x" title="114:234	Blackwell, Oxford." ></td>
	<td class="line x" title="115:234	Bernard Comrie." ></td>
	<td class="line x" title="116:234	1989." ></td>
	<td class="line x" title="117:234	Language Universals and Linguistic Typology:Syntax and Morphology." ></td>
	<td class="line x" title="118:234	Oxford, Basil Blackwell." ></td>
	<td class="line x" title="119:234	William Croft." ></td>
	<td class="line x" title="120:234	2001." ></td>
	<td class="line x" title="121:234	Radical Construction Grammar: Syntactic Theory in Typological Perspective." ></td>
	<td class="line x" title="122:234	Oxford University Press, Oxford." ></td>
	<td class="line x" title="123:234	David Dowty." ></td>
	<td class="line x" title="124:234	1979." ></td>
	<td class="line x" title="125:234	Word Meaning and Montague Grammar." ></td>
	<td class="line x" title="126:234	Reidel, Dordrecht." ></td>
	<td class="line x" title="127:234	Ted Dunning." ></td>
	<td class="line x" title="128:234	1993." ></td>
	<td class="line x" title="129:234	Accurate methods for the statistics of surprise and coincidence." ></td>
	<td class="line x" title="130:234	Computational Linguistics, 19(1):6174." ></td>
	<td class="line x" title="131:234	Christiane Fellbaum." ></td>
	<td class="line x" title="132:234	1998." ></td>
	<td class="line x" title="133:234	WordNet: An Electronic Lexical Database." ></td>
	<td class="line x" title="134:234	MIT Press." ></td>
	<td class="line x" title="135:234	J. R. Firth." ></td>
	<td class="line x" title="136:234	1957." ></td>
	<td class="line x" title="137:234	A synopsis of linguistic theory." ></td>
	<td class="line x" title="138:234	Oxford: Philological Society." ></td>
	<td class="line x" title="139:234	Reprinted in F. Palmer (ed.)(1968)." ></td>
	<td class="line x" title="140:234	Studies in Linguistic Analysis 19301955." ></td>
	<td class="line x" title="141:234	Selected Papers of J.R. Firth., Harlow: Longman. Stefan A. Frisch, Janet B. Pierrehumbert, and Michael B. Broe." ></td>
	<td class="line x" title="142:234	2004." ></td>
	<td class="line x" title="143:234	Similarity Avoidance and the OCP." ></td>
	<td class="line x" title="144:234	Natural Language & Linguistic Theory, 22(1):179228." ></td>
	<td class="line x" title="145:234	Ulrike Hahn and Todd M. Bailey." ></td>
	<td class="line x" title="146:234	2005." ></td>
	<td class="line x" title="147:234	What Makes Words Sound Similar?" ></td>
	<td class="line x" title="148:234	Cognition, 97(3):227267." ></td>
	<td class="line x" title="149:234	Wilbert Heeringa." ></td>
	<td class="line x" title="150:234	2004." ></td>
	<td class="line x" title="151:234	Measuring Dialect Pronunciation Differences using Levenshtein Distance." ></td>
	<td class="line x" title="152:234	Ph.D. thesis, Rijksuniversiteit Groningen." ></td>
	<td class="line x" title="153:234	Brett Kessler." ></td>
	<td class="line x" title="154:234	1995." ></td>
	<td class="line x" title="155:234	Computational dialectology in Irish Gaelic." ></td>
	<td class="line x" title="156:234	In Proc." ></td>
	<td class="line x" title="157:234	of the European ACL, pages 6067, Dublin." ></td>
	<td class="line x" title="158:234	Grzegorz Kondrak." ></td>
	<td class="line x" title="159:234	2002." ></td>
	<td class="line x" title="160:234	Algorithms for Language Reconstruction." ></td>
	<td class="line x" title="161:234	Ph.D. thesis, University of Toronto." ></td>
	<td class="line x" title="162:234	John Laver." ></td>
	<td class="line x" title="163:234	1994." ></td>
	<td class="line x" title="164:234	Principles of Phonetics." ></td>
	<td class="line x" title="165:234	Cambridge Univeristy Press, Cambridge." ></td>
	<td class="line x" title="166:234	Ludovic Lebart and Martin Rajman." ></td>
	<td class="line x" title="167:234	2000." ></td>
	<td class="line x" title="168:234	Computing similarity." ></td>
	<td class="line x" title="169:234	In Robert Dale, Hermann Moisl, and Harold Somers, editors, Handbook of Natural Language Processing, pages 477505." ></td>
	<td class="line x" title="170:234	Dekker, Basel." ></td>
	<td class="line x" title="171:234	Beth Levin." ></td>
	<td class="line x" title="172:234	1993." ></td>
	<td class="line x" title="173:234	English Verb Classes and Alternations: a Preliminary Investigation." ></td>
	<td class="line x" title="174:234	University of Chicago Press, Chicago and London." ></td>
	<td class="line x" title="175:234	Paul A. Luce and David B. Pisoni." ></td>
	<td class="line x" title="176:234	1998." ></td>
	<td class="line x" title="177:234	Recognizing spoken words: The neighborhood activation model." ></td>
	<td class="line x" title="178:234	Ear and Hearing, 19(1):136." ></td>
	<td class="line x" title="179:234	April McMahon and Robert McMahon." ></td>
	<td class="line x" title="180:234	2005." ></td>
	<td class="line x" title="181:234	Language Classification by the Numbers." ></td>
	<td class="line x" title="182:234	Oxford University Press, Oxford." ></td>
	<td class="line x" title="183:234	George A. Miller and Patricia E. Nicely." ></td>
	<td class="line x" title="184:234	1955." ></td>
	<td class="line x" title="185:234	An Analysis of Perceptual Confusions Among Some English Consonants." ></td>
	<td class="line x" title="186:234	The Journal of the Acoustical Society of America, 27:338352." ></td>
	<td class="line x" title="187:234	Andrew S. Noetzel and Stanley M. Selkow." ></td>
	<td class="line x" title="188:234	1999." ></td>
	<td class="line x" title="189:234	An analysis of the general tree-editing problem." ></td>
	<td class="line x" title="190:234	In David Sankoff and Joseph Kruskal, editors, Time Warps, String Edits and Macromolecules: The Theory and Practice of Sequence Comparison, pages 237252." ></td>
	<td class="line x" title="191:234	CSLI, Stanford." ></td>
	<td class="line x" title="192:234	11983." ></td>
	<td class="line x" title="193:234	Philip Stuart Resnik." ></td>
	<td class="line x" title="194:234	1993." ></td>
	<td class="line x" title="195:234	Selection and Information: A Class-Based Approach to Lexical Relationships." ></td>
	<td class="line x" title="196:234	Ph.D. thesis, University of Pennsylvania." ></td>
	<td class="line x" title="197:234	Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil." ></td>
	<td class="line x" title="198:234	1999." ></td>
	<td class="line x" title="199:234	Inducing an semantically annotated lexicon via em-based clustering." ></td>
	<td class="line x" title="200:234	In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, Maryland." ></td>
	<td class="line x" title="201:234	Sabine Schulte im Walde." ></td>
	<td class="line x" title="202:234	2003." ></td>
	<td class="line x" title="203:234	Experiments on the Automatic Induction of German Semantic Verb Classes." ></td>
	<td class="line x" title="204:234	Ph.D. thesis, Institut fur Maschinelle Sprachverarbeitung, Universitat Stuttgart." ></td>
	<td class="line x" title="205:234	Published as AIMS Report 9(2)." ></td>
	<td class="line x" title="206:234	Hinrich Schutze." ></td>
	<td class="line x" title="207:234	1998." ></td>
	<td class="line x" title="208:234	Automatic word sense discrimination." ></td>
	<td class="line x" title="209:234	Computational Linguistics, 24(1):97 123." ></td>
	<td class="line x" title="210:234	Sarah Thomason and Terrence Kaufmann." ></td>
	<td class="line x" title="211:234	1988." ></td>
	<td class="line x" title="212:234	Language Contact, Creolization, and Genetic Linguistics." ></td>
	<td class="line x" title="213:234	University of California Press, Berkeley." ></td>
	<td class="line x" title="214:234	Frans van Coetsem." ></td>
	<td class="line x" title="215:234	1988." ></td>
	<td class="line x" title="216:234	Loan Phonology and the Two Transfer Types in Language Contact." ></td>
	<td class="line x" title="217:234	Publications in Language Sciences." ></td>
	<td class="line x" title="218:234	Foris Publications, Dordrecht." ></td>
	<td class="line x" title="219:234	Zeno Vendler." ></td>
	<td class="line x" title="220:234	1967." ></td>
	<td class="line x" title="221:234	Linguistics in Philosophy." ></td>
	<td class="line x" title="222:234	Cornell University Press, Ithaca, NY." ></td>
	<td class="line x" title="223:234	Michael S. Vitevitch and Paul A. Luce." ></td>
	<td class="line x" title="224:234	1999." ></td>
	<td class="line x" title="225:234	Probabilistic Phonotactics and Neighborhood Activation in Spoken Word Recognition." ></td>
	<td class="line x" title="226:234	Journal of Memory and Language, 40(3):374408." ></td>
	<td class="line x" title="227:234	Andreas Wagner." ></td>
	<td class="line x" title="228:234	2004." ></td>
	<td class="line x" title="229:234	Learning Thematic Role Relations for Lexical Semantic Nets." ></td>
	<td class="line x" title="230:234	Ph.D. thesis, Universitat Tubingen." ></td>
	<td class="line x" title="231:234	David Yarowsky." ></td>
	<td class="line x" title="232:234	1995." ></td>
	<td class="line x" title="233:234	Unsupervised word sense disambiguation rivaling supervised methods." ></td>
	<td class="line x" title="234:234	In Proceedings of 33rd Annual Meeting of the Association for Computational Linguistics, pages 189196, Cambridge, MA." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1605
Distributional Measures Of Concept-Distance: A Task-Oriented Evaluation
Mohammad, Saif;Hirst, Graeme;"></td>
	<td class="line x" title="1:219	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 3543, Sydney, July 2006." ></td>
	<td class="line x" title="2:219	c2006 Association for Computational Linguistics Distributional Measures of Concept-Distance: A Task-oriented Evaluation Saif Mohammad and Graeme Hirst Department of Computer Science University of Toronto Toronto, ON M5S 3G4, Canada CUsmm,ghCV@cs.toronto.edu Abstract We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences." ></td>
	<td class="line x" title="3:219	We use the categories in a published thesaurus as coarse-grained concepts, allowing all possible distance values to be stored in a conceptconcept matrix roughly.01% the size of that created by existing measures." ></td>
	<td class="line x" title="4:219	We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of (1) ranking word pairs in order of semantic distance, and (2) correcting realword spelling errors." ></td>
	<td class="line x" title="5:219	In the latter task, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional conceptdistance measures." ></td>
	<td class="line x" title="6:219	1 Semantic and distributional measures Measures of distance of meaning are of two kinds." ></td>
	<td class="line x" title="7:219	The first kind, which we will refer to as semantic measures, rely on the structure of a resource such as WordNet or, in some cases, a semantic network, and hence they measure the distance between the concepts or word-senses that the nodes of the resource represent." ></td>
	<td class="line x" title="8:219	Examples include the measure for MeSH proposed by Rada et al.(1989) and those for WordNet proposed by Leacock and Chodorow (1998) and Jiang and Conrath (1997)." ></td>
	<td class="line x" title="10:219	(Some of the more successful measures, such as JiangConrath, also use information content derived from word frequency)." ></td>
	<td class="line x" title="11:219	Typically, these measures rely on an extensive hierarchy of hyponymy relationships for nouns." ></td>
	<td class="line x" title="12:219	Therefore, these measures are expected to perform poorly when used to estimate distance between senses of part-of-speech pairs other than nounnoun, not just because the WordNet hierarchies for other parts of speech are less well developed, but also because the hierarchies for the different parts of speech are not well connected." ></td>
	<td class="line x" title="13:219	The second kind of measures, which we will refer to as distributional measures, are inspired by the maxim You shall know a word by the company it keeps (Firth, 1957)." ></td>
	<td class="line x" title="14:219	These measures rely simply on raw text, and hence are much less resource-hungry than the semantic measures; but they measure the distance between words rather than word-senses or concepts." ></td>
	<td class="line x" title="15:219	In these measures, two words are considered close if they occur in similar contexts." ></td>
	<td class="line x" title="16:219	The context (or company) of a target word is represented by its distributional profile (DP), which lists the strength of association between the target and each of the lexical, syntactic, and/or semantic units that co-occur with it." ></td>
	<td class="line x" title="17:219	Commonly used measures of strength of association are conditional probability (0 to 1) and pointwise mutual information (A0 to ) 1 .Commonly used units of co-occurrence with the target are other words, and so we speak of the lexical distributional profile of a word (lexical DPW).The co-occurring words may be all those in a predetermined window around the target, or may be restricted to those that have a certain syntactic (e.g. , verbobject) or semantic (e.g. , agenttheme) relation with the target word." ></td>
	<td class="line x" title="18:219	We will refer to the former kind of DPs as relation-free." ></td>
	<td class="line nc" title="19:219	Usually in 1 In our experiments, we set negative PMI values to 0, because Church and Hanks (1990), in their seminal paper on word association ratio, show that negative PMI values are not expected to be accurate unless co-occurrence counts are made from an extremely large corpus." ></td>
	<td class="line x" title="20:219	35 Table 1: Measures of DP distance and measures of strength of association." ></td>
	<td class="line x" title="21:219	DP distance Strength of association -skew divergence conditional probability cosine pointwise mutual information JensenShannon divergence Lin the latter case, separate association values are calculated for each of the different relations between the target and the co-occurring units." ></td>
	<td class="line x" title="22:219	We will refer to such DPs as relation-constrained." ></td>
	<td class="line x" title="23:219	Typical relation-free DPs are those of Schutze and Pedersen (1997) and Yoshida et al.(2003)." ></td>
	<td class="line x" title="25:219	Typical relation-constrained DPs are those of Lin (1998) and Lee (2001)." ></td>
	<td class="line x" title="26:219	Below are contrived, but plausible, examples of each for the word pulse; the numbers are conditional probabilities." ></td>
	<td class="line x" title="27:219	relation-free DP pulse: beat (.28), racing (.2), grow (.13), beans (.09), heart (.04), . . ." ></td>
	<td class="line x" title="28:219	relation-constrained DP pulse: BObeat, subjectverbBQ (.34), BOracing, nounqualifying adjectiveBQ (.22), BOgrow, subjectverbBQ (.14), . . ." ></td>
	<td class="line x" title="29:219	The distance between two words, given their DPs, is calculated using a measure of DP distance, such as cosine." ></td>
	<td class="line x" title="30:219	While any of the measures of DP distance may be used with any of the measures of strength of association (see Table 1), in practice -skew divergence (ASD), cosine, and JensenShannon divergence (JSD) are used with conditional probability (CP), whereas Lin is used with PMI, resulting in the distributional measures ASD cp (Lee, 2001), Cos cp (Schutze and Pedersen, 1997), JSD cp,andLin pmi (Lin, 1998), respectively." ></td>
	<td class="line x" title="31:219	ASD cp is a modification of Kullback-Leibler divergence that overcomes the latters problem of division by zero, which can be caused by data sparseness." ></td>
	<td class="line x" title="32:219	JSD cp is another relative entropybased measure (like ASD cp ) but it is symmetric." ></td>
	<td class="line x" title="33:219	JSD cp and ASD cp are distance measures that give scores between 0 (identical) and infinity (maximally distant)." ></td>
	<td class="line x" title="34:219	Lin pmi and Cos cp are similarity measures that give scores between 0 (maximally distant) and 1 (identical)." ></td>
	<td class="line x" title="35:219	See Mohammad and Hirst (2005) for a detailed study of these and other measures." ></td>
	<td class="line x" title="36:219	2 The distributional hypothesis and its limitations The distributional hypothesis (Firth, 1957) states that words that occur in similar contexts tend to be semantically similar." ></td>
	<td class="line x" title="37:219	It is often suggested, therefore, that a distributional measure can act as a proxy for a semantic measure: the distance between the DPs of words will approximate the distance between their senses." ></td>
	<td class="line x" title="38:219	But when words have more than one sense, it is not at all clear what semantic distance between them actually means." ></td>
	<td class="line x" title="39:219	A word in each of its senses is likely to co-occur with different sets of words." ></td>
	<td class="line x" title="40:219	For example, bank in the financial institution sense is likely to cooccur with interest, money, accounts, andsoon, whereas the river bank sense might have words such as river, erosion, and silt around it." ></td>
	<td class="line x" title="41:219	If we define the distance between two words, at least one of which is ambiguous, to be the closest distance between some sense of one and some sense of the other, then distributional distance between words may indeed be used in place of semantic distance between concepts." ></td>
	<td class="line x" title="42:219	However, because measures of distributional distance depend on occurrences of the target word in all its senses, this substitution is inaccurate." ></td>
	<td class="line x" title="43:219	For example, observe that both DPWs of pulse above have words that co-occur with its throbbing arteries sense and words that co-occur with its edible seed sense." ></td>
	<td class="line x" title="44:219	Relation-free DPs of pulse in its two separate senses might be as follows: pulse throbbing arteries: beat (.36), racing (.27), heart (.11), . . ." ></td>
	<td class="line x" title="45:219	pulse edible seeds: grow (.24), beans (.14), . . ." ></td>
	<td class="line x" title="46:219	Thus, it is clear that different senses of a word have different distributional profiles (different company)." ></td>
	<td class="line x" title="47:219	Using a single DP for the word will mean the union of those profiles." ></td>
	<td class="line x" title="48:219	While this might be useful for certain applications, we believe that in a number of tasks (including estimating linguistic distance), acquiring different DPs for the different senses is not only more intuitive, but also, as we will show through experiments in Section 5, more useful." ></td>
	<td class="line x" title="49:219	We argue that distributional profiles of senses or concepts (DPCs) can be used to infer semantic properties of the senses: You shall know a sense by the company it keeps. 36 3 Conceptual grain size and storage requirements As applications for linguistic distance become more sophisticated and demanding, it becomes attractive to pre-compute and store the distance values between all possible pairs of words or senses." ></td>
	<td class="line x" title="50:219	But both kinds of measures have large space requirements to do this, requiring matrices of size N A2N,whereN is the size of the vocabulary (perhaps 100,000 for most languages) in the case of distributional measures and the number of senses (75,000 just for nouns in WordNet) in the case of semantic measures." ></td>
	<td class="line x" title="51:219	It is generally accepted, however, that WordNet senses are far too fine-grained (Agirre and Lopez de Lacalle Lekuona (2003) and citations therein)." ></td>
	<td class="line x" title="52:219	On the other hand, published thesauri, such as Rogets and Macquarie, group near-synonymous and semantically related words into a relatively small number of categoriestypically between 800 and 1100that roughly correspond to very coarse concepts or senses (Yarowsky, 1992)." ></td>
	<td class="line x" title="53:219	Words with more than one sense are listed in more than one category." ></td>
	<td class="line x" title="54:219	A published thesaurus thus provides us with a very coarse human-developed set or inventory of word senses or concepts 2 that are more intuitive and discernible than the concepts generated by dimensionality-reduction methods such as latent semantic analysis." ></td>
	<td class="line x" title="55:219	Using coarse senses from a known inventory means that the senses can be represented unambiguously by a large number of possibly ambiguous words (conveniently available in the thesaurus)a feature that we exploited in our earlier work (Mohammad and Hirst, 2006) to determine useful estimates of the strength of association between a concept and co-occurring words." ></td>
	<td class="line x" title="56:219	In this paper, we go one step further and use the idea of a very coarse sense inventory to develop a framework for distributional measures of concepts that can more naturally and more accurately be used in place of semantic measures of word senses." ></td>
	<td class="line x" title="57:219	We use the Macquarie Thesaurus (Bernard, 1986) as a sense inventory and repository of words pertaining to each sense." ></td>
	<td class="line x" title="58:219	It has 812 categories with around 176,000 word tokens and 98,000 word types." ></td>
	<td class="line x" title="59:219	This allows us to have much smaller conceptconcept distance matrices of size just 812A2812 (roughly .01% the size 2 We use the terms senses and concepts interchangeably." ></td>
	<td class="line x" title="60:219	This is in contrast to studies, such as that of Cooper (2005), that attempt to make a principled distinction between them." ></td>
	<td class="line x" title="61:219	of matrices required by existing measures)." ></td>
	<td class="line x" title="62:219	We evaluate our distributional concept-distance measures on two tasks: ranking word pairs in order of their semantic distance, and correcting realword spelling errors." ></td>
	<td class="line x" title="63:219	We compare performance with distributional word-distance measures and the WordNet-based concept-distance measures." ></td>
	<td class="line x" title="64:219	4 Distributional measures of concept-distance 4.1 Capturing distributional profiles of concepts We use relation-free lexical DPsboth DPWs and DPCsin our experiments, as they allow determination of semantic properties of the target from just its co-occurring words." ></td>
	<td class="line x" title="65:219	Determining lexical DPWs simply involves making wordword co-occurrence counts in a corpus." ></td>
	<td class="line x" title="66:219	A direct method to determine lexical DPCs, on the other hand, requires information about which words occur with which concepts." ></td>
	<td class="line x" title="67:219	This means that the text from which counts are made has to be sense annotated." ></td>
	<td class="line x" title="68:219	Since existing labeled data is minimal and manual annotation is far too expensive, indirect means must be used." ></td>
	<td class="line x" title="69:219	In an earlier paper (Mohammad and Hirst, 2006), we showed how this can be done with simple word sense disambiguation and bootstrapping techniques." ></td>
	<td class="line x" title="70:219	Here, we summarize the method." ></td>
	<td class="line x" title="71:219	First, we create a wordcategory cooccurrence matrix (WCCM) using the British National Corpus (BNC) and the Macquarie Thesaurus." ></td>
	<td class="line x" title="72:219	The WCCM has the following form: c 1 c 2 BMBMBM c j BMBMBM w 1 m 11 m 12 BMBMBM m 1 j BMBMBM w 2 m 21 m 22 BMBMBM m 2 j BMBMBM . . ." ></td>
	<td class="line x" title="73:219	BMBMBM BMBMBM w i m i1 m i2 BMBMBM m ij BMBMBM . . ." ></td>
	<td class="line x" title="74:219	A cell m ij, corresponding to word w i and category c j, contains the number of times w i co-occurs (in a window of A65 words in the corpus) with any of the words listed under category c j in the thesaurus." ></td>
	<td class="line x" title="75:219	Intuitively, the cell m ij captures the number of times c j and w i co-occur." ></td>
	<td class="line x" title="76:219	A contingency table for a single word and single category can be created by simply collapsing all other rows and columns into one and summing their frequencies." ></td>
	<td class="line x" title="77:219	Applying a suitable statistic, such as odds 37 distributional measures BNC distributional relatedness of words wordword co-occurrence matrix co-occurrence counting wordword Figure 1: Distributional word-distance." ></td>
	<td class="line x" title="78:219	ratio, on the contingency table gives the strength of association between a concept (category) and co-occurring word." ></td>
	<td class="line x" title="79:219	Therefore, the WCCM can be used to create the lexical DP for any concept." ></td>
	<td class="line x" title="80:219	The matrix that is created after one pass of the corpus, which we call the base WCCM, although noisy (as it is created from raw text and not senseannotated data), captures strong associations between categories and co-occurring words." ></td>
	<td class="line x" title="81:219	Therefore the intended sense (thesaurus category) of a word in the corpus can now be determined using frequencies of co-occurring words and its various senses as evidence." ></td>
	<td class="line x" title="82:219	A new bootstrapped WCCM is created, after a second pass of the corpus, in which the cell m ij contains the number of times any word used in sense c j co-occurs with w i .We have shown (Mohammad and Hirst, 2006) that the bootstrapped WCCM captures wordcategory cooccurrences much more accurately than the base WCCM, using the task of determining word sense dominance 3 as a test bed." ></td>
	<td class="line x" title="83:219	4.2 Applying distributional measures to DPCs Recall that in computing distributional worddistance, we consider two target words to be distributionally similar (less distant) if they occur in similar contexts." ></td>
	<td class="line x" title="84:219	The contexts are represented by the DPs of the target words, where a DP gives the strength of association between the target and the co-occurring units." ></td>
	<td class="line x" title="85:219	A distributional measure uses a measure of DP distance to determine the distance between two DPs and thereby between the two target words (see Figure 1)." ></td>
	<td class="line x" title="86:219	The various measures differ in what statistic they use to calculate the strength of association and the measure of DP dis3 Near-upper-bound results were achieved in the task of determining predominant senses of 27 words in 11 target texts with a wide range of sense distributions over their two most dominant senses." ></td>
	<td class="line x" title="87:219	distributional measures BNC Thesaurus distributional relatedness of concepts wordcategory co-occurrence matrix sense disambiguation bootstrapping and co-occurrence counting wordcategory Figure 2: Distributional concept-distance." ></td>
	<td class="line x" title="88:219	tance they use (see Mohammad and Hirst (2005) for details)." ></td>
	<td class="line x" title="89:219	For example, following is the cosine formula for distance between words w 1 and w 2 using relation-free lexical DPWs, with conditional probability of the co-occurring word given the target as the strength of association: Cos cp B4w 1 BNw 2 B5BP  wBECB4w 1 B5CJCB4w 2 B5 B4PB4wCYw 1 B5A2PB4wCYw 2 B5B5 D5  wBECB4w 1 B5 PB4wCYw 1 B5 2 A2 D5  wBECB4w 2 B5 PB4wCYw 2 B5 2 Here, CB4xB5 is the set of words that co-occur with word x within a pre-determined window." ></td>
	<td class="line x" title="90:219	In order to calculate distributional conceptdistance, consider the same scenario, except that the targets are now senses or concepts." ></td>
	<td class="line x" title="91:219	Two concepts are closer if their DPs are similar, and these DPCs require the strength of association between the target concepts and their co-occurring words." ></td>
	<td class="line x" title="92:219	The associations can be estimated from the bootstrapped WCCM, described in Section 4.1 above." ></td>
	<td class="line x" title="93:219	Any of the distributional measures used for DPWs can now be used to estimate concept-distance with DPCs." ></td>
	<td class="line x" title="94:219	Figure 2 illustrates our methodology." ></td>
	<td class="line x" title="95:219	Below is the formula for cosine with conditional probabilities when applied to concepts: Cos cp B4c 1 BNc 2 B5BP  wBECB4c 1 B5CJCB4c 2 B5 B4PB4wCYc 1 B5A2PB4wCYc 2 B5B5 D5  wBECB4c 1 B5 PB4wCYc 1 B5 2 A2 D5  wBECB4c 2 B5 PB4wCYc 2 B5 2 Now, CB4xB5 is the set of words that co-occur with concept x within a pre-determined window." ></td>
	<td class="line x" title="96:219	We will refer to such measures as distributional measures of concept-distance (Distrib concept ), in contrast to the earlier-described distributional measures of word-distance (Distrib word ) and WordNet-based (or semantic) measures of concept-distance (WNet concept )." ></td>
	<td class="line x" title="97:219	We shall refer 38 to these three kinds of distance measures as measure-types." ></td>
	<td class="line x" title="98:219	Individual measures in each kind will be referred to simply as measures." ></td>
	<td class="line x" title="99:219	A distributional measure of concept-distance can be used to populate a small 812 A2 812 conceptconcept distance matrix where a cell m ij, pertaining to concepts c i and c j, contains the distance between the two concepts." ></td>
	<td class="line x" title="100:219	In contrast, a wordword distance matrix for a conservative vocabulary of 100,000 word types will have a size 100,000 A2 100,000, and a WordNet-based conceptconcept distance matrix will have a size 75,000 A2 75,000 just for nouns." ></td>
	<td class="line x" title="101:219	Our concept concept distance matrix is roughly .01% the size of these matrices." ></td>
	<td class="line x" title="102:219	Note that the DPs we are using are relation-free because (1) we use all co-occurring words (not just those that are related to the target by certain syntactic or semantic relations) and (2) the WCCM, as described in Section 4.1, does not maintain separate counts for the different relations between the target and co-occurring words." ></td>
	<td class="line x" title="103:219	Creating a larger matrix with separate counts for the different relations would lead to relation-constrained DPs." ></td>
	<td class="line x" title="104:219	5 Evaluation To evaluate the distributional concept-distance measures, we used them in the tasks of ranking word pairs in order of their semantic distance and of correcting real-word spelling errors, and compared our results to those that we obtained on the same tasks with distributional word-distance measures and those that Budanitsky and Hirst (2006) obtained with WordNet-based semantic measures." ></td>
	<td class="line x" title="105:219	The distributional concept-distance measures used a bootstrapped WCCM created from the BNC and the Macquarie Thesaurus." ></td>
	<td class="line x" title="106:219	The word-distance measures used a wordword co-occurrence matrix created from the BNC alone." ></td>
	<td class="line x" title="107:219	The BNC was not lemmatized, part of speech tagged, or chunked." ></td>
	<td class="line x" title="108:219	The vocabulary was restricted to the words present in the thesaurus (about 98,000 word types) both to provide a level evaluation platform and to keep the matrix to a manageable size." ></td>
	<td class="line x" title="109:219	Co-occurrence counts less than 5 were reset to 0, and words that co-occurred with more than 2000 other words were stoplisted (543 in all)." ></td>
	<td class="line x" title="110:219	We used ASD cp ( BP 0BM99), Cos cp, JSD cp,andLin pmi 4 to populate corresponding conceptconcept distance matrices and 4 Whereas Lin (1998) used relation-constrained DPs, in our experiments all DPs are relation-free." ></td>
	<td class="line x" title="111:219	Table 2: Correlation of distributional measures with human ranking." ></td>
	<td class="line x" title="112:219	Best results for each measure-type are shown in boldface." ></td>
	<td class="line x" title="113:219	Measure-type Distrib word Distrib concept Measure closest average ASD cp .45 .60  Cos cp .54 .69 .42 JSD cp .48 .61  Lin pmi .52 .71 .59 wordword distance matrices." ></td>
	<td class="line x" title="114:219	Applications that require distance values will enjoy a run-time benefit if the distances are precomputed." ></td>
	<td class="line x" title="115:219	While it is easy to completely populate the conceptconcept co-occurrence matrix, completely populating the wordword distance matrix is a non-trivial task because of memory and time constraints." ></td>
	<td class="line x" title="116:219	5 5.1 Ranking word pairs A direct approach to evaluating linguistic distance measures is to determine how close they are to human judgment and intuition." ></td>
	<td class="line x" title="117:219	Given a set of word-pairs, humans can rank them in order of their distanceplacing near-synonyms on one end of the ranking and unrelated pairs on the other." ></td>
	<td class="line x" title="118:219	Rubenstein and Goodenough (1965) provide a gold-standard list of 65 human-ranked word-pairs (based on the responses of 51 subjects)." ></td>
	<td class="line x" title="119:219	One automatic word-distance estimator, then, is deemed to be more accurate than another if its ranking of word-pairs correlates more closely with this human ranking." ></td>
	<td class="line x" title="120:219	Measures of conceptdistance can perform this task by determining word-distance for each word-pair by finding the concept-distance between all pairs of senses of the two words, and choosing the distance of the closest sense pair." ></td>
	<td class="line x" title="121:219	This is based on the assumption that when humans are asked to judge the semantic distance between a pair of words, they implicitly consider its closest senses." ></td>
	<td class="line x" title="122:219	For example, most people will agree that bank and interest are semantically related, even though both have multiple senses most of which are unrelated." ></td>
	<td class="line x" title="123:219	Alternatively, the method could take the average of the distance of all pairs of senses." ></td>
	<td class="line x" title="124:219	5 As we wanted to perform experiments with both conceptconcept and wordword distance matrices, we populated them as and when new distance values were calculated." ></td>
	<td class="line x" title="125:219	39 Table 3: Hirst and St-Onge metrics for evaluation of real-word spelling correction." ></td>
	<td class="line x" title="126:219	suspect ratio = no." ></td>
	<td class="line x" title="127:219	of true-suspects no." ></td>
	<td class="line x" title="128:219	of malaps no." ></td>
	<td class="line x" title="129:219	of false-suspects no." ></td>
	<td class="line x" title="130:219	of non-malaps alarm ratio = no." ></td>
	<td class="line x" title="131:219	of true-alarms no." ></td>
	<td class="line x" title="132:219	of true-suspects no." ></td>
	<td class="line x" title="133:219	of false-alarms no." ></td>
	<td class="line x" title="134:219	of false-suspects detection ratio = no." ></td>
	<td class="line x" title="135:219	of true-alarms no." ></td>
	<td class="line x" title="136:219	of malaps no." ></td>
	<td class="line x" title="137:219	of false-alarms no." ></td>
	<td class="line x" title="138:219	of non-malaps correction ratio = no." ></td>
	<td class="line x" title="139:219	corrected malaps no." ></td>
	<td class="line x" title="140:219	of malaps no." ></td>
	<td class="line x" title="141:219	of false-alarms no." ></td>
	<td class="line x" title="142:219	of non-malaps correction accuracy = no." ></td>
	<td class="line x" title="143:219	of corrected malaps no." ></td>
	<td class="line x" title="144:219	of true-alarms Table 2 lists correlations of human rankings with those created using distributional measures." ></td>
	<td class="line x" title="145:219	Observe that Distrib concept measures give markedly higher correlation values than Distrib word measures." ></td>
	<td class="line x" title="146:219	Also, using the distance of the closest sense pair (for Cos cp and Lin pmi )gives much better results than using the average distance of all relevant sense pairs." ></td>
	<td class="line x" title="147:219	(We do not report average distance for ASD cp and JSD cp because they give very large distance values when sensepairs are unrelatedvalues that dominate the averages, overwhelming the others, and making the results meaningless)." ></td>
	<td class="line x" title="148:219	These correlations are, however, notably lower than those obtained by the best WordNet-based measures (not shown in the table), which fall in the range .78 to .84 (Budanitsky and Hirst, 2006)." ></td>
	<td class="line x" title="149:219	5.2 Real-word spelling error correction The set of Rubenstein and Goodenough word pairs is much too small to safely assume that measures that work well on them do so for the entire English vocabulary." ></td>
	<td class="line x" title="150:219	Consequently, semantic measures have traditionally been evaluated through applications that use them, such as the work by Hirst and Budanitsky (2005) on correcting real-word spelling errors (or malapropisms)." ></td>
	<td class="line x" title="151:219	If a word in a text is not semantically close to any other word in its context, then it is considered a suspect." ></td>
	<td class="line x" title="152:219	If the suspect has a spelling-variant that is semantically close to a word in its context, then the suspect is declared a probable real-word spelling error and an alarm is raised; the related spelling-variant is considered its correction." ></td>
	<td class="line x" title="153:219	Hirst and Budanitsky tested the method on 500 articles from the 198789 Wall Street Journal corpus for their experiments, replacing every 200th word by a spelling-variant." ></td>
	<td class="line x" title="154:219	We adopt this method and this test data, but whereas Hirst and Budanitsky used WordNet-based semantic measures, we use distributional measures Distrib word and Distrib concept . In order to determine whether two words are semantically close or not as per any measure of distance, a threshold must be set." ></td>
	<td class="line x" title="155:219	If the distance between two words is less than the threshold, then they will be considered semantically close." ></td>
	<td class="line x" title="156:219	Hirst and Budanitsky (2005) pointed out that there is a notably wide band between 1.83 and 2.36 (on a scale of 04), such that all Rubenstein and Goodenough word pairs were assigned values either higher than 2.36 or lower than 1.83 by human subjects." ></td>
	<td class="line x" title="157:219	They argue that somewhere within this band is a suitable threshold between semantically close and semantically distant, and therefore set thresholds for the WordNet-based measures such that there was maximum overlap in what the measures and human judgments considered semantically close and distant." ></td>
	<td class="line x" title="158:219	Following this idea, we use an automatic method to determine thresholds for the various Distrib word and Distrib concept measures." ></td>
	<td class="line x" title="159:219	Given a list of Rubenstein and Goodenough word pairs ordered according to a distance measure, we repeatedly consider the mean of all consecutive distance values as candidate thresholds." ></td>
	<td class="line x" title="160:219	Then we determine the number of word-pairs correctly classified as semantically close or semantically distant for each candidate threshold, considering which side of the band they lie as per human judgments." ></td>
	<td class="line x" title="161:219	The candidate threshold with highest accuracy is chosen as the threshold." ></td>
	<td class="line x" title="162:219	We follow Hirst and St-Onge (1998) in the metrics that we use to evaluate real-word spelling correction; they are listed in Table 3." ></td>
	<td class="line x" title="163:219	Suspect ratio and alarm ratio evaluate the processes of identifying suspects and raising alarms, respectively." ></td>
	<td class="line x" title="164:219	Detection ratio is the product of the two, and measures overall performance in detecting the errors." ></td>
	<td class="line x" title="165:219	Correction ratio indicates overall correction performance, and is the bottom-line statistic that we focus on." ></td>
	<td class="line x" title="166:219	Values greater than 1 for each of these ratios indicate results better than random guessing." ></td>
	<td class="line x" title="167:219	The ability of the system to determine the intended word, given that it has correctly detected an error, is indicated by the correction accuracy (0 to 1)." ></td>
	<td class="line x" title="168:219	40 Table 4: Real-word error correction using distributional word-distance (Distrib word ), distributional concept-distance (Distrib concept ), and Hirst and Budanitskys (2005) results using WordNet-based concept-distance measures (WNet concept )." ></td>
	<td class="line x" title="169:219	Best results for each measure-type are shown in boldface." ></td>
	<td class="line x" title="170:219	suspect alarm detection correction correction detection correction Measure ratio ratio ratio accuracy ratio PRFperformance Distrib word ASD cp 3.36 1.78 5.98 0.84 5.03 7.37 45.53 12.69 10.66 Cos cp 2.91 1.64 4.77 0.85 4.06 5.97 37.15 10.28 8.74 JSD cp 3.29 1.77 5.82 0.83 4.88 7.19 44.32 12.37 10.27 Lin pmi 3.63 2.15 7.78 0.84 6.52 9.38 58.38 16.16 13.57 Distrib concept ASD cp 4.11 2.54 10.43 0.91 9.49 12.19 25.28 16.44 14.96 Cos cp 4.00 2.51 10.03 0.90 9.05 11.77 26.99 16.38 14.74 JSD cp 3.58 2.46 8.79 0.90 7.87 10.47 34.66 16.08 14.47 Lin pmi 3.02 2.60 7.84 0.88 6.87 9.45 36.86 15.04 13.24 WNet concept HirstSt-Onge 4.24 1.95 8.27 0.93 7.70 9.67 26.33 14.15 13.16 JiangConrath 4.73 2.97 14.02 0.92 12.91 14.33 46.22 21.88 20.13 LeacockChodrow 3.23 2.72 8.80 0.83 7.30 11.56 60.33 19.40 16.10 Lin 3.57 2.71 9.70 0.87 8.48 9.56 51.56 16.13 14.03 Resnik 2.58 2.75 7.10 0.78 5.55 9.00 55.00 15.47 12.07 Notice that the correction ratio is the product of the detection ratio and correction accuracy." ></td>
	<td class="line x" title="171:219	The overall (single-point) precision P (no. of true-alarms / no." ></td>
	<td class="line x" title="172:219	of alarms), recall R (no. of true-alarms / no." ></td>
	<td class="line x" title="173:219	of malapropisms), and F-score ( 2A2PA2R PB7R ) of detection are also computed." ></td>
	<td class="line x" title="174:219	The product of detection F-score and correction accuracy, which we will call correction performance, can also be used as a bottom-line performance metric." ></td>
	<td class="line x" title="175:219	Table 4 details the performance of Distrib word and Distrib concept measures." ></td>
	<td class="line x" title="176:219	For comparison, results obtained by Hirst and Budanitsky (2005) with the use of WNet concept measures are also shown." ></td>
	<td class="line x" title="177:219	Observe that the correction ratio results for the Distrib word measures are poor compared to Distrib concept measures; the concept-distance measures are clearly superior, in particular ASD cp and Cos cp . Moreover, if we consider correction ratio to be the bottom-line statistic, then the Distrib concept measures outperform all WNet concept measures except the JiangConrath measure." ></td>
	<td class="line x" title="178:219	If we consider correction performance to be the bottom-line statistic, then again we see that the distributional concept-distance measures outperform the worddistance measures, except in the case of Lin pmi, which gives slightly poorer results with conceptdistance." ></td>
	<td class="line x" title="179:219	Also, in contrast to correction ratio values, using the LeacockChodorow measure results in relatively higher correction performance values than the best Distrib concept measures." ></td>
	<td class="line x" title="180:219	While it is clear that the LeacockChodorow measure is relatively less accurate in choosing the right spellingvariant for an alarm (correction accuracy), detection ratio and detection F-score present contrary pictures of relative performance in detection." ></td>
	<td class="line x" title="181:219	As correction ratio is determined by the product of a number of ratios, each evaluating the various stages of malapropism correction (identifying suspects, raising alarms, and applying the correction), we believe it is a better indicator of overall performance than correction performance, which is a not-so-elegant product of an F-score and accuracy." ></td>
	<td class="line x" title="182:219	However, no matter which of the two is chosen as the bottom-line performance statistic, the results show that the newly proposed distributional concept-distance measures are clearly superior to word-distance measures." ></td>
	<td class="line x" title="183:219	Further, of all the WordNet-based measures, only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures consistently with respect to both bottom-line statistics." ></td>
	<td class="line x" title="184:219	6 Related Work Patwardhan and Pedersen (2006) create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss." ></td>
	<td class="line x" title="185:219	The distance between two senses is then determined by the cosine of the an41 gle between their aggregate vectors." ></td>
	<td class="line x" title="186:219	However, as we pointed out in Mohammad and Hirst (2005), such aggregate co-occurrence vectors are expected to be noisy because they are created from data that is not sense-annotated." ></td>
	<td class="line x" title="187:219	Therefore, we employed simple word sense disambiguation and bootstrapping techniques on our base WCCM to create more-accurate co-occurrence vectors, which gave markedly higher accuracies in the task of determining word sense dominance." ></td>
	<td class="line x" title="188:219	In the experiments described in this paper, we used these bootstrapped co-occurrence vectors to determine concept-distance." ></td>
	<td class="line x" title="189:219	Pantel (2005) also provides a way to create co-occurrence vectors for WordNet senses." ></td>
	<td class="line x" title="190:219	The lexical co-occurrence vectors of words in a leaf node are propagated up the WordNet hierarchy." ></td>
	<td class="line x" title="191:219	A parent node inherits those co-occurrences that are shared by its children." ></td>
	<td class="line x" title="192:219	Lastly, co-occurrences not pertaining to the leaf nodes are removed from its vector." ></td>
	<td class="line x" title="193:219	Even though the methodology attempts at associating a WordNet node or sense with only those co-occurrences that pertain to it, no attempt is made at correcting the frequency counts." ></td>
	<td class="line x" title="194:219	After all, word1word2 co-occurrence frequency (or association) is likely not the same as SENSE1word2 co-occurrence frequency (or association), simply because word1 may have senses other than SENSE1, as well." ></td>
	<td class="line x" title="195:219	The co-occurrence frequency of a parent is the weighted sum of cooccurrence frequencies of its children." ></td>
	<td class="line x" title="196:219	The frequencies of the child nodes are used as weights." ></td>
	<td class="line x" title="197:219	Sense ambiguity issues apart, this is still problematic because a parent concept (say, BIRD)may co-occur much more frequently (or infrequently) with a word than its children (such as, hen, archaeopteryx, aquatic bird, trogon, and others)." ></td>
	<td class="line x" title="198:219	In contrast, the bootstrapped WCCM we use not only identifies which words co-occur with which concepts, but also has more sophisticated estimates of the co-occurrence frequencies." ></td>
	<td class="line x" title="199:219	7Conclusion We have proposed a framework that allows distributional measures to estimate concept-distance using a published thesaurus and raw text." ></td>
	<td class="line x" title="200:219	We evaluated them in comparison with traditional distributional word-distance measures and WordNetbased measures through their ability in ranking word-pairs in order of their human-judged linguistic distance, and in correcting real-word spelling errors." ></td>
	<td class="line x" title="201:219	We showed that distributional conceptdistance measures outperformed word-distance measures in both tasks." ></td>
	<td class="line x" title="202:219	They do not perform as well as the best WordNet-based measures in ranking a small set of word pairs, but in the task of correcting real-word spelling errors, they beat all WordNet-based measures except for Jiang Conrath (which is markedly better) and LeacockChodorow (which is slightly better if we consider correction performance as the bottom-line statistic, but slightly worse if we rely on correction ratio)." ></td>
	<td class="line x" title="203:219	It should be noted that the Rubenstein and Goodenough word-pairs used in the ranking task, as well as all the real-word spelling errors in the correction task are nouns." ></td>
	<td class="line x" title="204:219	We expect that the WordNet-based measures will perform poorly when other parts of speech are involved, as those hierarchies of WordNet are not as extensively developed." ></td>
	<td class="line x" title="205:219	On the other hand, our DPC-based measures do not rely on any hierarchies (even if they exist in a thesaurus) but on sets of words that unambiguously represent each sense." ></td>
	<td class="line x" title="206:219	Further, because our measures are tied closely to the corpus from which co-occurrence counts are made, we expect the use of domain-specific corpora to result in even better results." ></td>
	<td class="line x" title="207:219	All the distributional measures that we have considered in this paper are lexicalthat is, the distributional profiles of the target word or concept are based on their co-occurrence with words in a text." ></td>
	<td class="line x" title="208:219	By contrast, semantic DPs would be based on information such as what concepts usually co-occur with the target word or concept." ></td>
	<td class="line x" title="209:219	Semantic profiles of words can be obtained from the WCCM itself (using the row entry for the word)." ></td>
	<td class="line x" title="210:219	It would be interesting to see how distributional measures of word-distance that use these semantic DPs of words perform." ></td>
	<td class="line x" title="211:219	We also intend to explore the use of semantic DPs of concepts acquired from a conceptconcept co-occurrence matrix (CCCM)." ></td>
	<td class="line x" title="212:219	A CCCM can be created from the WCCM by setting the row entry for a concept or category to be the average of WCCM row values for all the words pertaining to it." ></td>
	<td class="line x" title="213:219	Both DPWand WordNet-based measures have large space and time requirements for precomputing and storing all possible distance values for a language." ></td>
	<td class="line x" title="214:219	However, by using the categories of a thesaurus as very coarse concepts, precomputing and storing all possible distance values for our DPC-based measures requires a matrix of 42 size only about 800A2800." ></td>
	<td class="line x" title="215:219	This level of conceptcoarseness might seem drastic at first glance, but we have shown that distributional measures of distance between these coarse concepts are quite useful." ></td>
	<td class="line x" title="216:219	Part of our future work will be to try an intermediate degree of coarseness (still much coarser than WordNet) by using the paragraph subdivisions of the thesaurus instead of its categories to see if this gives even better results." ></td>
	<td class="line x" title="217:219	Acknowledgments We thank Afsaneh Fazly, Siddharth Patwardhan, and the CL group at the University of Toronto for their valuable feedback." ></td>
	<td class="line x" title="218:219	We thank Alex Budanitsky for helping us adapt his malapropismcorrection software to work with distributional measures." ></td>
	<td class="line x" title="219:219	This research is financially supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3307
Integrating Co-Occurrence Statistics With Information Extraction For Robust Retrieval Of Protein Interactions From Medline
Bunescu, Razvan C.;Mooney, Raymond J.;Ramani, Arun;Marcotte, Edward;"></td>
	<td class="line x" title="1:147	Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 4956, New York City, June 2006." ></td>
	<td class="line x" title="2:147	c2006 Association for Computational Linguistics Integrating Co-occurrence Statistics with Information Extraction for Robust Retrieval of Protein Interactions from Medline Razvan Bunescu, Raymond Mooney Department of Computer Sciences University of Texas at Austin 1 University Station C0500 Austin, TX 78712 razvan@cs.utexas.edu mooney@cs.utexas.edu Arun Ramani, Edward Marcotte Institute for Cellular and Molecular Biology University of Texas at Austin 1 University Station A4800 Austin, TX 78712 arun@icmb.utexas.edu marcotte@icmb.utexas.edu Abstract The task of mining relations from collections of documents is usually approached in two different ways." ></td>
	<td class="line x" title="3:147	One type of systems do relation extraction from individual sentences, followed by an aggregation of the results over the entire collection." ></td>
	<td class="line x" title="4:147	Other systems follow an entirely different approach, in which co-occurrence counts are used to determine whether the mentioning together of two entities is due to more than simple chance." ></td>
	<td class="line x" title="5:147	We show that increased extraction performance can be obtained by combining the two approaches into an integrated relation extraction model." ></td>
	<td class="line x" title="6:147	1 Introduction Information Extraction (IE) is a natural language processing task in which text documents are analyzed with the aim of finding mentions of relevant entities and important relationships between them." ></td>
	<td class="line x" title="7:147	In many cases, the subtask of relation extraction reduces to deciding whether a sentence asserts a particular relationship between two entities, which is still a difficult, unsolved problem." ></td>
	<td class="line x" title="8:147	There are however cases where the decision whether the two entities are in a relationship is made relative to an entire document, or a collection of documents." ></td>
	<td class="line x" title="9:147	In the biomedical domain, for example, one may be interested in finding the pairs of human proteins that are said to be interacting in any of the Medline abstracts, where the answer is not required to specify which abstracts are actually describing the interaction." ></td>
	<td class="line x" title="10:147	Assembling a ranked list of interacting proteins can be very useful to biologists based on this list, they can make more informed decisions with respect to which genes to focus on in their research." ></td>
	<td class="line x" title="11:147	In this paper, we investigate methods that use multiple occurrences of the same pair of entities across a collection of documents in order to boost the performance of a relation extraction system." ></td>
	<td class="line x" title="12:147	The proposed methods are evaluated on the task of finding pairs of human proteins whose interactions are reported in Medline abstracts." ></td>
	<td class="line x" title="13:147	The majority of known human protein interactions are derived from individual, small-scale experiments reported in Medline." ></td>
	<td class="line x" title="14:147	Some of these interactions have already been collected in the Reactome (Joshi-Tope et al. , 2005), BIND (Bader et al. , 2003), DIP (Xenarios et al. , 2002), and HPRD (Peri et al. , 2004) databases." ></td>
	<td class="line x" title="15:147	The amount of human effort involved in creating and updating these databases is currently no match for the continuous growth of Medline." ></td>
	<td class="line x" title="16:147	It is therefore very useful to have a method that automatically and reliably extracts interaction pairs from Medline." ></td>
	<td class="line x" title="17:147	Systems that do relation extraction from a collection of documents can be divided into two major categories." ></td>
	<td class="line x" title="18:147	In one category are IE systems that first extract information from individual sentences, and then combine the results into corpuslevel results (Craven, 1999; Skounakis and Craven, 2003)." ></td>
	<td class="line x" title="19:147	The second category corresponds to approaches that do not exploit much information from the context of individual occurrences." ></td>
	<td class="line x" title="20:147	Instead, based on co-occurrence counts, various statistical 49 or information-theoretic tests are used to decide whether the two entities in a pair appear together more often than simple chance would predict (Lee et al. , 2004; Ramani et al. , 2005)." ></td>
	<td class="line x" title="21:147	We believe that a combination of the two approaches can inherit the advantages of each method and lead to improved relation extraction accuracy." ></td>
	<td class="line x" title="22:147	The following two sections describe the two orthogonal approaches to corpus-level relation extraction." ></td>
	<td class="line x" title="23:147	A model that integrates the two approaches is then introduced in Section 4." ></td>
	<td class="line x" title="24:147	This is followed by a description of the dataset used for evaluation in Section 5, and experimental results in Section 6." ></td>
	<td class="line x" title="25:147	2 Sentence-level relation extraction Most systems that identify relations between entities mentioned in text documents consider only pair of entities that are mentioned in the same sentence (Ray and Craven, 2001; Zhao and Grishman, 2005; Bunescu and Mooney, 2005)." ></td>
	<td class="line x" title="26:147	To decide the existence and the type of a relationship, these systems generally use lexico-semantic clues inferred from the sentence context of the two entities." ></td>
	<td class="line x" title="27:147	Much research has been focused recently on automatically identifying biologically relevant entities and their relationships such as protein-protein interactions or subcellular localizations." ></td>
	<td class="line x" title="28:147	For example, the sentence TR6 specifically binds Fas ligand, states an interaction between the two proteins TR6 and Fas ligand." ></td>
	<td class="line x" title="29:147	One of the first systems for extracting interactions between proteins is described in (Blaschke and Valencia, 2001)." ></td>
	<td class="line x" title="30:147	There, sentences are matched deterministically against a set of manually developed patterns, where a pattern is a sequence of words or Partof-Speech (POS) tags and two protein-name tokens." ></td>
	<td class="line x" title="31:147	Between every two adjacent words is a number indicating the maximum number of words that can be skipped at that position." ></td>
	<td class="line x" title="32:147	An example is: interaction of (3) BOPBQ (3) with (3) BOPBQ." ></td>
	<td class="line x" title="33:147	This approach is generalized in (Bunescu and Mooney, 2005), where subsequences of words (or POS tags) from the sentence are used as implicit features." ></td>
	<td class="line x" title="34:147	Their weights are learned by training a customized subsequence kernel on a dataset of Medline abstracts annotated with proteins and their interactions." ></td>
	<td class="line x" title="35:147	A relation extraction system that works at the sentence-level and which outputs normalized confidence values for each extracted pair of entities can also be used for corpus-level relation extraction." ></td>
	<td class="line x" title="36:147	A straightforward way to do this is to apply an aggregation operator over the confidence values inferred for all occurrences of a given pair of entities." ></td>
	<td class="line x" title="37:147	More exactly, if D4 BD and D4 BE are two entities that occur in a total of D2 sentences D7 BD, D7 BE, , D7 D2 in the entire corpus BV, then the confidence C8B4CAB4D4 BD BND4 BE B5CYBVB5 that they are in a particular relationship CA is defined as: C8B4CAB4D4 BD BND4 BE B5CYBVB5 BP A0B4CUC8B4CAB4D4 BD BND4 BE B5CYD7 CX B5CYCXBPBDBMD2CVB5 Table 1 shows only four of the many possible choices for the aggregation operator A0." ></td>
	<td class="line x" title="38:147	max A0 D1CPDC BP D1CPDC CX C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5 noisy-or A0 D2D3D6 BP BD A0 CH CX B4BD A0C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5B5 avg A0 CPDACV BP CG CX C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5 D2 and A0 CPD2CS BP CH CX C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5 BDBPD2 Table 1: Aggregation Operators." ></td>
	<td class="line x" title="39:147	Out of the four operators in Table 1, we believe that the max operator is the most appropriate for aggregating confidence values at the corpus-level." ></td>
	<td class="line x" title="40:147	The question that needs to be answered is whether there is a sentence somewhere in the corpus that asserts the relationship CA between entities D4 BD and D4 BE . Using avg instead would answer a different question whether CAB4D4 BD BND4 BE B5 is true in most of the sentences containing D4 BD and D4 BE . Also, the and operator would be most appropriate for finding whether CAB4D4 BD BND4 BE B5 is true in all corresponding sentences in the corpus." ></td>
	<td class="line x" title="41:147	The value of the noisy-or operator (Pearl, 1986) is too dependent on the number of occurrences, therefore it is less appropriate for a corpus where the occurrence counts vary from one entity pair to another (as confirmed in our experiments from Section 6)." ></td>
	<td class="line x" title="42:147	For examples, if the confidence threshold is set at BCBMBH, and the entity pair B4D4 BD BND4 BE B5 occurs in 6 sentences or less, each with confidence BCBMBD, then CAB4D4 BD BND4 BE B5 is false, according to the noisy-or operator." ></td>
	<td class="line x" title="43:147	However, if B4D4 BD BND4 BE B5 occur in more than 6 sentences, with the same confidence value of BCBMBD, then the corresponding noisy-or value exceeds BCBMBH, making CAB4D4 BD BND4 BE B5 true." ></td>
	<td class="line x" title="44:147	50 3 Co-occurrence statistics Given two entities with multiple mentions in a large corpus, another approach to detect whether a relationship holds between them is to use statistics over their occurrences in textual patterns that are indicative for that relation." ></td>
	<td class="line x" title="45:147	Various measures such as pointwise mutual information (PMI), chi-square (AVBE) or log-likelihood ratio (LLR) (Manning and Schutze, 1999) use the two entities occurrence statistics to detect whether their co-occurrence is due to chance, or to an underlying relationship." ></td>
	<td class="line x" title="46:147	A recent example is the co-citation approach from (Ramani et al. , 2005), which does not try to find specific assertions of interactions in text, but rather exploits the idea that if many different abstracts reference both protein D4 BD and protein D4 BE, then D4 BD and D4 BE are likely to interact." ></td>
	<td class="line x" title="47:147	Particularly, if the two proteins are co-cited significantly more often than one would expect if they were cited independently at random, then it is likely that they interact." ></td>
	<td class="line x" title="48:147	The model used to compute the probability of random co-citation is based on the hypergeometric distribution (Lee et al. , 2004; Jenssen et al. , 2001)." ></td>
	<td class="line x" title="49:147	Thus, if C6 is the total number of abstracts, D2 of which cite the first protein, D1 cite the second protein, and CZ cite both, then the probability of co-citation under a random model is: C8B4CZCYC6BND1BND2B5 BP AI D2 CZ AJAI C6 A0D2 D1A0CZ AJ AI C6 D1 AJ (1) The approach that we take in this paper is to constrain the two proteins to be mentioned in the same sentence, based on the assumption that if there is a reason for two protein names to co-occur in the same sentence, then in most cases that is caused by their interaction." ></td>
	<td class="line oc" title="50:147	To compute the degree of interaction between two proteins D4 BD and D4 BE, we use the information-theoretic measure of pointwise mutual information (Church and Hanks, 1990; Manning and Schutze, 1999), which is computed based on the following quantities: 1." ></td>
	<td class="line x" title="51:147	C6 : the total number of protein pairs cooccurring in the same sentence in the corpus." ></td>
	<td class="line x" title="52:147	2." ></td>
	<td class="line x" title="53:147	C8B4D4 BD BND4 BE B5 B3 D2 BDBE BPC6 : the probability that D4 BD and D4 BE co-occur in the same sentence; D2 BDBE = the number of sentences mentioning both D4 BD and D4 BE . 3." ></td>
	<td class="line x" title="54:147	C8B4D4 BD BND4B5 B3 D2 BD BPC6 : the probability that D4 BD cooccurs with any other protein in the same sentence; D2 BD = the number of sentences mentioning D4 BD and D4." ></td>
	<td class="line x" title="55:147	4." ></td>
	<td class="line x" title="56:147	C8B4D4 BE BND4B5 B3 D2 BE BPC6 : the probability that D4 BE cooccurs with any other protein in the same sentence; D2 BE = the number of sentences mentioning D4 BE and D4." ></td>
	<td class="line x" title="57:147	The PMI is then defined as in Equation 2 below: C8C5C1B4D4 BD BND4 BE B5 BP D0D3CV C8B4D4 BD BND4 BE B5 C8B4D4 BD BND4B5 A1C8B4D4 BE BND4B5 B3 D0D3CVC6 D2 BDBE D2 BD A1D2 BE (2) Given that the PMI will be used only for ranking pairs of potentially interacting proteins, the constant factor C6 and the D0D3CV operator can be ignored." ></td>
	<td class="line x" title="58:147	For sake of simplicity, we use the simpler formula from Equation 3." ></td>
	<td class="line x" title="59:147	D7C8C5C1B4D4 BD BND4 BE B5 BP D2 BDBE D2 BD A1 D2 BE (3) 4 Integrated model The D7C8C5C1B4D4 BD BND4 BE B5 formula can be rewritten as: D7C8C5C1B4D4 BD BND4 BE B5 BP BD D2 BD A1 D2 BE A1 D2 BDBE CG CXBPBD BD (4) Let D7 BD, D7 BE,  , D7 D2 BDBE be the sentence contexts corresponding to the D2 BDBE co-occurrences of D4 BD and D4 BE, and assume that a sentence-level relation extractor is available, with the capability of computing normalized confidence values for all extractions." ></td>
	<td class="line x" title="60:147	Then one way of using the extraction confidence is to have each co-occurrence weighted by its confidence, i.e. replace the constant BD with the normalized scores C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5, as illustrated in Equation 5." ></td>
	<td class="line x" title="61:147	This results in a new formula DBC8C5C1 (weighted PMI), which is equal with the product between D7C8C5C1 and the average aggregation operator A0 CPDACV . DBC8C5C1B4D4 BD BND4 BE B5 BP BD D2 BD A1D2 BE A1 D2 BDBE CG CXBPBD C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5 BP D2 BDBE D2 BD A1D2 BE A1 A0 CPDACV (5) 51 The operator A0 CPDACV can be replaced with any other aggregation operator from Table 1." ></td>
	<td class="line x" title="62:147	As argued in Section 2, we consider D1CPDC to be the most appropriate operator for our task, therefore the integrated model is based on the weighted PMI product illustrated in Equation 6." ></td>
	<td class="line x" title="63:147	DBC8C5C1B4D4 BD BND4 BE B5 BP D2 BDBE D2 BD A1 D2 BE A1 A0 D1CPDC (6) BP D2 BDBE D2 BD A1 D2 BE A1 D1CPDC CX C8B4CAB4D4 BD BND4 BE B5CYD7 CX B5 If a pair of entities D4 BD and D4 BE is ranked by DBC8C5C1 among the top pairs, this means that it is unlikely that D4 BD and D4 BE have co-occurred together in the entire corpus by chance, and at the same time there is at least one mention where the relation extractor decides with high confidence that CAB4D4 BD BND4 BE B5 BP BD." ></td>
	<td class="line x" title="64:147	5 Evaluation Corpus Contrasting the performance of the integrated model against the sentence-level extractor or the PMIbased ranking requires an evaluation dataset that provides two types of annotations: 1." ></td>
	<td class="line x" title="65:147	The complete list of interactions reported in the corpus (Section 5.1)." ></td>
	<td class="line x" title="66:147	2." ></td>
	<td class="line x" title="67:147	Annotation of mentions of genes and proteins, together with their corresponding gene identifiers (Section 5.2)." ></td>
	<td class="line x" title="68:147	We do not differentiate between genes and their protein products, mapping them to the same gene identifiers." ></td>
	<td class="line x" title="69:147	Also, even though proteins may participate in different types of interactions, we are concerned only with detecting whether they interact in the general sense of the word." ></td>
	<td class="line x" title="70:147	5.1 Medline Abstracts and Interactions In order to compile an evaluation corpus and an associated comprehensive list of interactions, we exploited information contained in the HPRD (Peri et al. , 2004) database." ></td>
	<td class="line x" title="71:147	Every interaction listed in HPRD is linked to a set of Medline articles where the corresponding experiment is reported." ></td>
	<td class="line x" title="72:147	More exactly, each interaction is specified in the database as a tuple that contains the LocusLink (now EntrezGene) identifiers of all genes involved and the PubMed identifiers of the corresponding articles (as illustrated in Table 2)." ></td>
	<td class="line x" title="73:147	Interaction (XML) (HPRD) BOinteractionBQ BOgeneBQ2318BO/geneBQ BOgeneBQ58529BO/geneBQ BOpubmedBQ10984498 11171996BO/pubmedBQ BO/interactionBQ Participant Genes (XML) (NCBI) BOgene id=2318BQ BOnameBQFLNCBO/nameBQ BOdescriptionBQfilamin C, gammaBO/descriptionBQ BOsynonymsBQ BOsynonymBQABPABO/synonymBQ BOsynonymBQABPLBO/synonymBQ BOsynonymBQFLN2BO/synonymBQ BOsynonymBQABP-280BO/synonymBQ BOsynonymBQABP280ABO/synonymBQ BO/synonymsBQ BOproteinsBQ BOproteinBQgamma filaminBO/proteinBQ BOproteinBQfilamin 2BO/proteinBQ BOproteinBQgamma-filaminBO/proteinBQ BOproteinBQABP-L, gamma filaminBO/proteinBQ BOproteinBQactin-binding protein 280BO/proteinBQ BOproteinBQgamma actin-binding proteinBO/proteinBQ BOproteinBQfilamin C, gammaBO/proteinBQ BO/proteinsBQ BO/geneBQ BOgene id=58529BQ BOnameBQMYOZ1BO/nameBQ BOdescriptionBQmyozenin 1BO/descriptionBQ BOsynonymsBQ  BO/synonymsBQ BOproteinsBQ  BO/proteinsBQ BO/geneBQ Medline Abstract (XML) (NCBI) BOPMIDBQ10984498BO/PMIDBQ BOAbstractTextBQ We found that this protein binds to three other Z-disc proteins; therefore, we have named it FATZ, gamma-filamin, alpha-actinin and telethonin binding protein of the Z-disc." ></td>
	<td class="line x" title="74:147	BO/AbstractTextBQ Table 2: Interactions, Genes and Abstracts." ></td>
	<td class="line x" title="75:147	The evaluation corpus (henceforth referred to as the HPRD corpus) is created by collecting the Medline abstracts corresponding to interactions between human proteins, as specified in HPRD." ></td>
	<td class="line x" title="76:147	In total, 5,617 abstracts are included in this corpus, with an associated list of 7,785 interactions." ></td>
	<td class="line x" title="77:147	This list is comprehensive the HPRD database is based on an annotation process in which the human annotators report all interactions described in a Medline article." ></td>
	<td class="line x" title="78:147	On the other hand, the fact that only abstracts are included in the corpus (as opposed to including the full article) means that the list may contain interactions that are not actually reported in the HPRD corpus." ></td>
	<td class="line x" title="79:147	Nevertheless, if the abstracts were annotated 52 with gene mentions and corresponding GIDs, then a quasi-exact interaction list could be computed based on the following heuristic: [H] If two genes with identifiers CVCXCS BD and CVCXCS BE are mentioned in the same sentence in an abstract with PubMed identifier D4D1CXCS, and if CVCXCS BD and CVCXCS BE are participants in an interaction that is linked to D4D1CXCS in HPRD, then consider that the abstract (and consequently the entire HPRD corpus) reports the interaction between CVCXCS BD and CVCXCS BE . A4 An application of the above heuristic is shown at the bottom of Table 2." ></td>
	<td class="line x" title="80:147	The HPRD record at the top of the table specifies that the Medline article with ID 10984498 reports an interaction between the proteins FATZ (with ID 58529) and gamma-filamin (with ID 2318)." ></td>
	<td class="line x" title="81:147	The two protein names are mentioned in a sentence in the abstract for 10984498, therefore, by [H], we consider that the HPRD corpus reports this interaction." ></td>
	<td class="line x" title="82:147	This is very similar to the procedure used in (Craven, 1999) for creating a weakly-labeled dataset of subcellular-localization relations." ></td>
	<td class="line x" title="83:147	[H] is a strong heuristic  it is already known that the full article reports an interaction between the two genes." ></td>
	<td class="line x" title="84:147	Finding the two genes collocated in the same sentence in the abstract is very likely to be due to the fact that the abstract discusses their interaction." ></td>
	<td class="line x" title="85:147	The heuristic can be made even more accurate if a pair of genes is considered as interacting only if they cooccur in a (predefined) minimum number of sentences in the entire corpus  with the evaluation modified accordingly, as described later in Section 6." ></td>
	<td class="line x" title="86:147	5.2 Gene Name Annotation and Normalization For the annotation of gene names and their normalization, we use a dictionary-based approach similar to (Cohen, 2005)." ></td>
	<td class="line x" title="87:147	NCBI1 provides a comprehensive dictionary of human genes, where each gene is specified by its unique identifier, and qualified with an official name, a description, synonym names and one or more protein names, as illustrated in Table 2." ></td>
	<td class="line x" title="88:147	All of these names, including the description, are considered as potential referential expressions for the gene entity." ></td>
	<td class="line x" title="89:147	Each name string is reduced to a normal form by: replacing dashes with spaces, introducing spaces between sequences of letters and se1URL: http://www.ncbi.nih.gov quences of digits, replacing Greek letters with their Latin counterparts (capitalized), substituting Roman numerals with Arabic numerals, decapitalizing the first word if capitalized." ></td>
	<td class="line x" title="90:147	All names are further tokenized, and checked against a dictionary of close to 100K English nouns." ></td>
	<td class="line x" title="91:147	Names that are found in this dictionary are simply filtered out." ></td>
	<td class="line x" title="92:147	We also ignore all ambiguous names (i.e. names corresponding to more than one gene identifier)." ></td>
	<td class="line x" title="93:147	The remaining nonambiguous names are added to the final gene dictionary, which is implemented as a trie-like structure in order to allow a fast lookup of gene IDs based on the associated normalized sequences of tokens." ></td>
	<td class="line x" title="94:147	Each abstract from the HPRD corpus is tokenized and segmented in sentences using the OpenNLP2 package." ></td>
	<td class="line x" title="95:147	The resulting sentences are then annotated by traversing them from left to right and finding the longest token sequences whose normal forms match entries from the gene dictionary." ></td>
	<td class="line x" title="96:147	6 Experimental Evaluation The main purpose of the experiments in this section is to compare the performance of the following four methods on the task of corpus-level relation extraction: 1." ></td>
	<td class="line x" title="97:147	Sentence-level relation extraction followed by the application of an aggregation operator that assembles corpus-level results (SSK.Max)." ></td>
	<td class="line x" title="98:147	2." ></td>
	<td class="line x" title="99:147	Pointwise Mutual Information (PMI)." ></td>
	<td class="line x" title="100:147	3." ></td>
	<td class="line x" title="101:147	The integrated model, a product of the two base models (PMI.SSK.Max)." ></td>
	<td class="line x" title="102:147	4." ></td>
	<td class="line x" title="103:147	The hypergeometric co-citation method (HG)." ></td>
	<td class="line x" title="104:147	7 Experimental Methodology All abstracts, either from the HPRD corpus, or from the entire Medline, are annotated using the dictionary-based approach described in Section 5.2." ></td>
	<td class="line x" title="105:147	The sentence-level extraction is done with the subsequence kernel (SSK) approach from (Bunescu and Mooney, 2005), which was shown to give good results on extracting interactions from biomedical abstracts." ></td>
	<td class="line x" title="106:147	The subsequence kernel was trained on a set of 225 Medline abstracts which were manually 2URL: http://opennlp.sourceforge.net 53 annotated with protein names and their interactions." ></td>
	<td class="line x" title="107:147	It is known that PMI gives undue importance to low frequency events (Dunning, 1993), therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus." ></td>
	<td class="line x" title="108:147	When evaluating corpus-level extraction on HPRD, because the quasi-exact list of interactions is known, we report the precision-recall (PR) graphs, where the precision (P) and recall (R) are computed as follows: C8 BP AZtrue interactions extracted AZtotal interaction extracted CA BP AZtrue interactions extracted AZtrue interactions All pairs of proteins are ranked based on each scoring method, and precision recall points are computed by considering the top C6 pairs, where C6 varies from 1 to the total number of pairs." ></td>
	<td class="line x" title="109:147	When evaluating on the entire Medline, we used the shared protein function benchmark described in (Ramani et al. , 2005)." ></td>
	<td class="line x" title="110:147	Given the set of interacting pairs recovered at each recall level, this benchmark calculates the extent to which interaction partners in a data set share functional annotation, a measure previously shown to correlate with the accuracy of functional genomics data sets (Lee et al. , 2004)." ></td>
	<td class="line x" title="111:147	The KEGG (Kanehisa et al. , 2004) and Gene Ontology (Ashburner et al. , 2000) databases provide specific pathway and biological process annotations for approximately 7,500 human genes, assigning human genes into 155 KEGG pathways (at the lowest level of KEGG) and 1,356 GO pathways (at level 8 of the GO biological process annotation)." ></td>
	<td class="line x" title="112:147	The scoring scheme for measuring interaction set accuracy is in the form of a log odds ratio of gene pairs sharing functional annotations." ></td>
	<td class="line x" title="113:147	To evaluate a data set, a log likelihood ratio (LLR) is calculated as follows: C4C4CA BP D0D2 C8B4BWCYC1B5 C8B4BWCYBMC1B5 BP D0D2 C8B4C1CYBWB5C8B4BMC1B5 C8B4BMC1CYBWB5C8B4C1B5 (7) where C8B4BWCYC1B5 and C8B4BWCYBMC1B5 are the probability of observing the data BW conditioned on the genes sharing benchmark associations (C1) and not sharing benchmark associations (BMC1)." ></td>
	<td class="line x" title="114:147	In its expanded form (obtained by Bayes theorem), C8B4C1CYBWB5 and C8B4BMC1CYBWB5 are estimated using the frequencies of interactions observed in the given data set BW between annotated genes sharing benchmark associations and not sharing associations, respectively, while the priors C8B4C1B5 and C8B4BMC1B5 are estimated based on the total frequencies of all benchmark genes sharing the same associations and not sharing associations, respectively." ></td>
	<td class="line x" title="115:147	A score of zero indicates interaction partners in the data set being tested are no more likely than random to belong to the same pathway or to interact; higher scores indicate a more accurate data set." ></td>
	<td class="line x" title="116:147	8 Experimental Results The results for the HPRD corpus-level extraction are shown in Figure 1." ></td>
	<td class="line x" title="117:147	Overall, the integrated model has a more consistent performance, with a gain in precision mostly at recall levels past BGBCB1." ></td>
	<td class="line x" title="118:147	The SSK.Max and HG models both exhibit a sudden decrease in precision at around BHB1 recall level." ></td>
	<td class="line x" title="119:147	While SSK.Max goes back to a higher precision level, the HG model begins to recover only late at BJBCB1 recall." ></td>
	<td class="line x" title="120:147	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall PMI.SSK.Max PMI SSK.Max HG Figure 1: PR curves for corpus-level extraction." ></td>
	<td class="line x" title="121:147	A surprising result in this experiment is the behavior of the HG model, which is significantly outperformed by PMI, and which does only marginally better than a simple baseline that considers all pairs to be interacting." ></td>
	<td class="line x" title="122:147	We also compared the two methods on corpuslevel extraction from the entire Medline, using the shared protein function benchmark." ></td>
	<td class="line x" title="123:147	As before, we considered only protein pairs occurring in the same 54 sentence, with a minimum frequency count of 5." ></td>
	<td class="line x" title="124:147	The resulting 47,436 protein pairs were ranked according to their PMI and HG scores, with pairs that are most likely to be interacting being placed at the top." ></td>
	<td class="line x" title="125:147	For each ranking, the LLR score was computed for the top N proteins, where N varied in increments of 1,000." ></td>
	<td class="line x" title="126:147	The comparative results for PMI and HG are shown in Figure 2, together with the scores for three human curated databases: HPRD, BIND and Reactome." ></td>
	<td class="line x" title="127:147	On the top 18,000 protein pairs, PMI outperforms HG substantially, after which both converge to the same value for all the remaining pairs." ></td>
	<td class="line x" title="128:147	2 2.25 2.5 2.75 3 3.25 3.5 3.75 4 4.25 4.5 4.75 5 2500 5000 7500 10000 12500 15000 17500 20000 22500 25000 LLR Top N pairs PMI HG HPRD BIND Reactome Figure 2: Functional annotation benchmark." ></td>
	<td class="line x" title="129:147	Figure 3 shows a comparison of the four aggregation operators on the same HPRD corpus, which confirms that, overall, max is most appropriate for integrating corpus-level results." ></td>
	<td class="line x" title="130:147	9 Future Work The piece of related work that is closest to the aim of this paper is the Bayesian approach from (Skounakis and Craven, 2003)." ></td>
	<td class="line x" title="131:147	In their probabilistic model, cooccurrence statistics are taken into account by using a prior probability that a pair of proteins are interacting, given the number of co-occurrences in the corpus." ></td>
	<td class="line x" title="132:147	However, they do not use the confidences of the sentence-level extractions." ></td>
	<td class="line x" title="133:147	The GeneWays system from (Rzhetsky et al. , 2004) takes a different approach, in which co-occurrence frequencies are simply used to re-rank the ouput from the relation extractor." ></td>
	<td class="line x" title="134:147	An interesting direction for future research is to 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Max Noisy-Or Avg And Figure 3: PR curves for aggregation operators." ></td>
	<td class="line x" title="135:147	design a model that takes into account both the extraction confidences and the co-occurrence statistics, without losing the probabilistic (or informationtheoretic) interpretation." ></td>
	<td class="line x" title="136:147	One could investigate ways of integrating the two orthogonal approaches to corpus-level extraction based on other statistical tests, such as chi-square and log-likelihood ratio." ></td>
	<td class="line x" title="137:147	The sentence-level extractor used in this paper was trained to recognize relation mentions in isolation." ></td>
	<td class="line x" title="138:147	However, the trained model is later used, through the max aggregation operator, to recognize whether multiple mentions of the same pair of proteins indicate a relationship between them." ></td>
	<td class="line x" title="139:147	This points to a fundamental mismatch between the training and testing phases of the model." ></td>
	<td class="line x" title="140:147	We expect that better accuracy can be obtained by designing an approach that is using information from multiple occurrences of the same pair in both training and testing." ></td>
	<td class="line x" title="141:147	10 Conclusion Extracting relations from a collection of documents can be approached in two fundamentally different ways." ></td>
	<td class="line x" title="142:147	In one approach, an IE system extracts relation instances from corpus sentences, and then aggregates the local extractions into corpus-level results." ></td>
	<td class="line x" title="143:147	In the second approach, statistical tests based on co-occurrence counts are used for deciding if a given pair of entities are mentioned together more often than chance would predict." ></td>
	<td class="line x" title="144:147	We have described 55 a method to integrate the two approaches, and given experimental results that confirmed our intuition that an integrated model would have a better performance." ></td>
	<td class="line x" title="145:147	11 Acknowledgements This work was supported by grants from the N.S.F." ></td>
	<td class="line x" title="146:147	(IIS-0325116, EIA-0219061), N.I.H." ></td>
	<td class="line x" title="147:147	(GM0677901), Welch (F1515), and a Packard Fellowship (E.M.M.)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1039
Detecting Compositionality of Verb-Object Combinations using Selectional Preferences
McCarthy, Diana;Venkatapathy, Sriram;Joshi, Aravind K.;"></td>
	<td class="line x" title="1:203	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:203	369379, Prague, June 2007." ></td>
	<td class="line x" title="3:203	c2007 Association for Computational Linguistics Detecting Compositionality of Verb-Object Combinations using Selectional Preferences Diana McCarthy University of Sussex Falmer, East Sussex BN1 9QH, UK dianam@sussex.ac.uk Sriram Venkatapathy International Institute of Information Technology Hyderabad, India sriram@research.iiit.ac.in Aravind K. Joshi University of Pennsylvania, Philadelphia PA, USA." ></td>
	<td class="line x" title="4:203	joshi@linc.cis.upenn.edu Abstract In this paper we explore the use of selectional preferences for detecting noncompositional verb-object combinations." ></td>
	<td class="line x" title="5:203	To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference." ></td>
	<td class="line x" title="6:203	Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation." ></td>
	<td class="line x" title="7:203	In previous work on selectional preference acquisition, the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types." ></td>
	<td class="line x" title="8:203	In our distributional thesaurus models and one of the methods using WordNet we select classes for representing the preferences by virtue of the number of argument types that they cover, and then only tokens under these classes which are representative of the argument head data are used to estimate the probability distribution for the selectional preference model." ></td>
	<td class="line x" title="9:203	We demonstrate a highly signi cant correlation between measures which use these typebased selectional preferences and compositionality judgements from a data set used in previous research." ></td>
	<td class="line x" title="10:203	The type-based models perform better than the models which use tokens for selecting the classes." ></td>
	<td class="line x" title="11:203	Furthermore, the models which use the automatically acquired thesaurus entries produced the best results." ></td>
	<td class="line x" title="12:203	The correlation for the thesaurus models is stronger than any of the individual features used in previous research on the same dataset." ></td>
	<td class="line x" title="13:203	1 Introduction Characterising the semantic behaviour of phrases in terms of compositionality has particularly attracted attention in recent years (Lin, 1999; Schone and Jurafsky, 2001; Bannard, 2002; Bannard et al. , 2003; Baldwin et al. , 2003; McCarthy et al. , 2003; Bannard, 2005; Venkatapathy and Joshi, 2005)." ></td>
	<td class="line x" title="14:203	Typically the phrases are putative multiwords and noncompositionality is viewed as an important feature of many such words with spaces (Sag et al. , 2002)." ></td>
	<td class="line x" title="15:203	For applications such as paraphrasing, information extraction and translation, it is essential to take the words of non-compositional phrases together as a unit because the meaning of a phrase cannot be obtained straightforwardly from the constituent words." ></td>
	<td class="line x" title="16:203	In this work we are investigate methods of determining semantic compositionality of verb-object 1 combinations on a continuum following previous research in this direction (McCarthy et al. , 2003; Venkatapathy and Joshi, 2005)." ></td>
	<td class="line x" title="17:203	Much previous research has used a combination of statistics and distributional approaches whereby distributional similarity is used to compare the constituents of the multiword with the multiword itself." ></td>
	<td class="line x" title="18:203	In this paper, we will investigate the use of selectional preferences of verbs." ></td>
	<td class="line x" title="19:203	We will use the preferences to nd atypical verb-object combinations as we anticipate that such combinations are more likely to be non-compositional." ></td>
	<td class="line x" title="20:203	1We use object to refer to direct objects." ></td>
	<td class="line x" title="21:203	369 Selectional preferences of predicates have been modelled using the man-made thesaurus WordNet (Fellbaum, 1998), see for example (Resnik, 1993; Li and Abe, 1998; Abney and Light, 1999; Clark and Weir, 2002)." ></td>
	<td class="line x" title="22:203	There are also distributional approaches which use co-occurrence data to cluster distributionally similar words together." ></td>
	<td class="line x" title="23:203	The cluster output can then be used as classes for selectional preferences (Pereira et al. , 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994)." ></td>
	<td class="line x" title="24:203	We used three different types of probabilistic models, which vary in the classes selected for representation over which the probability distribution of the argument heads 2 is estimated." ></td>
	<td class="line x" title="25:203	Two use WordNet and the other uses the entries in a thesaurus of distributionally similar words acquired automatically following (Lin, 1998)." ></td>
	<td class="line x" title="26:203	The rst method is due to Li and Abe (1998)." ></td>
	<td class="line x" title="27:203	The classes over which the probability distribution is calculated are selected according to the minimum description length principle (MDL) which uses the argument head tokens for nding the best classes for representation." ></td>
	<td class="line x" title="28:203	This method has previously been tried for modelling compositionality of verb-particle constructions (Bannard, 2002)." ></td>
	<td class="line x" title="29:203	The other two methods (we refer to them as typebased) also calculate a probability distribution using argument head tokens but they select the classes over which the distribution is calculated using the number of argument head types (of a verb in a corpus) in a given class, rather than the number of argument head tokens in contrast to previous WordNet models (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002)." ></td>
	<td class="line x" title="30:203	For example, if the object slot of the verb park contains the argument heads { car, car, car, car, van, jeep } then the type-based models use the word type car only once when determining the classes over which the probability distribution is to be estimated." ></td>
	<td class="line x" title="31:203	Classes are selected which maximise the number of types that they cover, rather than the number of tokens." ></td>
	<td class="line x" title="32:203	This is done to avoid the selectional preferences being heavily in uenced by noise from highly frequent arguments which may be polysemous and some or all of their meanings may not be 2Argument heads are the nouns occurring in the object slot of the target verb." ></td>
	<td class="line x" title="33:203	semantically related to the prototypical arguments of the verb." ></td>
	<td class="line x" title="34:203	For example car has a gondola sense in WordNet." ></td>
	<td class="line x" title="35:203	The third method uses entries in a distributional thesaurus rather than classes from WordNet." ></td>
	<td class="line x" title="36:203	The entries used as classes for representation are selected by virtue of the number of argument types they encompass." ></td>
	<td class="line x" title="37:203	As with the WordNet models, the tokens are used to estimate a probability distribution over these entries." ></td>
	<td class="line x" title="38:203	In the next section, we discuss related work on identifying compositionality." ></td>
	<td class="line x" title="39:203	In section 3, we describe the methods we are using for acquiring our models of selectional preference." ></td>
	<td class="line x" title="40:203	In section 4, we test our models on a dataset used in previous research." ></td>
	<td class="line x" title="41:203	We compare the three types of models individually and also investigate the best performing model when used in combination with other features used in previous research." ></td>
	<td class="line x" title="42:203	We conclude in section 5." ></td>
	<td class="line x" title="43:203	2 Related Work Most previous work using distributional approaches to compositionality either contrasts distributional information of candidate phrases with constituent words (Schone and Jurafsky, 2001; Bannard et al. , 2003; Baldwin et al. , 2003; McCarthy et al. , 2003) or uses distributionally similar words to detect nonproductive phrases (Lin, 1999)." ></td>
	<td class="line x" title="44:203	Lin (1999) used his method (Lin, 1998) for automatic thesaurus construction." ></td>
	<td class="line x" title="45:203	He identi ed candidate phrases involving several open-class words output from his parser and ltered these by the loglikelihood statistic." ></td>
	<td class="line x" title="46:203	Lin proposed that if there is a phrase obtained by substitution of either the head or modi er in the phrase with a nearest neighbour from the thesaurus then the mutual information of this and the original phrase must be signi cantly different for the original phrase to be considered noncompositional." ></td>
	<td class="line x" title="47:203	He evaluated the output manually." ></td>
	<td class="line x" title="48:203	As well as distributional similarity, researchers have used a variety of statistics as indicators of non-compositionality (Blaheta and Johnson, 2001; Krenn and Evert, 2001)." ></td>
	<td class="line x" title="49:203	Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom." ></td>
	<td class="line x" title="50:203	Although they are not speci cally detecting compositionality, there is a strong corre370 lation between syntactic rigidity and semantic idiosyncrasy." ></td>
	<td class="line x" title="51:203	Venkatapathy and Joshi (2005) combine different statistical and distributional methods using support vector machines (SVMs) for identifying noncompositional verb-object combinations." ></td>
	<td class="line x" title="52:203	They explored seven features as measures of compositionality: 1." ></td>
	<td class="line x" title="53:203	frequency 2." ></td>
	<td class="line oc" title="54:203	pointwise mutual information (Church and Hanks, 1990), 3." ></td>
	<td class="line x" title="55:203	least mutual information difference with similar collocations, based on (Lin, 1999) and using Lins thesaurus (Lin, 1998) for obtaining the similar collocations." ></td>
	<td class="line x" title="56:203	4." ></td>
	<td class="line x" title="57:203	The distributed frequency of an object, which takes an average of the frequency of occurrence with an object over all verbs occurring with the object above a threshold." ></td>
	<td class="line x" title="58:203	5." ></td>
	<td class="line x" title="59:203	distributed frequency of an object, using the verb, which considers the similarity between the target verb and the verbs occurring with the target object above the speci ed threshold." ></td>
	<td class="line x" title="60:203	6." ></td>
	<td class="line x" title="61:203	a latent semantic approach (LSA) based on (Schcurrency1utze, 1998; Baldwin et al. , 2003) and considering the dissimilarity of the verb-object pair with its constituent verb 7." ></td>
	<td class="line x" title="62:203	the same LSA approach, but considering the similarity of the verb-object pair with the verbal form of the object (to capture support verb constructions e.g. give a smile Venkatapathy and Joshi (2005) produced a dataset of verb-object pairs with human judgements of compositionality." ></td>
	<td class="line x" title="63:203	We say more about this dataset and Venkatapathy and Joshis results in section 4 since we use the dataset for our experiments." ></td>
	<td class="line x" title="64:203	In this paper, we investigate the use of selectional preferences to detect compositionality." ></td>
	<td class="line x" title="65:203	Bannard (2002) did some pioneering work to try and establish a link between the compositionality of verb particle constructions and the selectional preferences of the multiword and its constituent verb." ></td>
	<td class="line x" title="66:203	His results were hampered by models based on (Li and Abe, 1998) which involved rather uninformative models at the roots of WordNet." ></td>
	<td class="line x" title="67:203	There are several reasons for this." ></td>
	<td class="line x" title="68:203	The classes for the model are selected using MDL by compromising between a simple model with few classes and one which explains the data well." ></td>
	<td class="line x" title="69:203	The models are particularly affected by the quantity of data available (Wagner, 2002)." ></td>
	<td class="line x" title="70:203	Also noise from frequent but idiosyncratic or polysemous arguments weakens the signal." ></td>
	<td class="line x" title="71:203	There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat." ></td>
	<td class="line x" title="72:203	In contrast to Bannard, our experiments are with verb-object combinations rather than verb particle constructions." ></td>
	<td class="line x" title="73:203	We compare Li and Abe models with WordNet models which use the number of argument types to obtain the classes for representation of the selectional preferences." ></td>
	<td class="line x" title="74:203	In addition to experiments with these WordNet models, we propose models using entries in distributional thesauruses for representing preferences." ></td>
	<td class="line x" title="75:203	3 Three Methods for Acquiring Selectional Preferences All models were acquired from verb-object data extracted using the RASP parser (Briscoe and Carroll, 2002) from the 90 million words of written English from the BNC (Leech, 1992)." ></td>
	<td class="line x" title="76:203	We extracted verb and common noun tuples where the noun is the argument head of the object relation." ></td>
	<td class="line x" title="77:203	The parser was also used to extract the grammatical relation data used for acquisition of the thesaurus described below in section 3.3." ></td>
	<td class="line x" title="78:203	3.1 TCMs This approach is a reimplementation of Li and Abe (1998)." ></td>
	<td class="line x" title="79:203	Each selectional preference model (referred to as a tree cut model, or TCM) comprises a set of disjunctive noun classes selected from all the possibilities in the WordNet hyponym hierarchy 3 using MDL (Rissanen, 1978)." ></td>
	<td class="line x" title="80:203	The TCM covers all the 3We use WordNet version 2.1 for the work in this paper." ></td>
	<td class="line x" title="81:203	371 noun senses in the WordNet hierarchy and is associated with a probability distribution over these noun senses in the hierarchy re ecting the argument head data occurring in the given grammatical relationship with the speci ed verb." ></td>
	<td class="line x" title="82:203	MDL nds the classes in the TCM by considering the cost measured in bits of describing both the model and the argument head data encoded in the model." ></td>
	<td class="line x" title="83:203	A compromise is made by having as simple a model as possible using classes further up the hierarchy whilst also providing a good model for the set of argument head tokens (TK)." ></td>
	<td class="line x" title="84:203	The classes are selected by recursing from the top of the WordNet hierarchy comparing the cost (or description length) of using the mother class to the cost of using the hyponym daughter classes." ></td>
	<td class="line x" title="85:203	In any path, the mother is preferred unless using the daughters would reduce the cost." ></td>
	<td class="line x" title="86:203	If using the daughters for the model is less costly than the mother then the recursion continues to compare the cost of the hyponyms beneath." ></td>
	<td class="line x" title="87:203	The cost (or description length) for a set of classes is calculated as the model description length (mdl) and the data description length (ddl) 4 :mdl + ddl k 2  log|TK| +  summationtext tkTK log p(tk) (1) k, is the number of WordNet classes being currently considered for the TCM minus one." ></td>
	<td class="line x" title="88:203	The MDL method uses the size of TK on the assumption that a larger dataset warrants a more detailed model." ></td>
	<td class="line x" title="89:203	The cost of describing the argument head data is calculated using the log of the probability estimate from the classes currently being considered for the model." ></td>
	<td class="line x" title="90:203	The probability estimate for a class being considered for the model is calculated using the cumulative frequency of all the hyponym nouns under that class that occur in TK, divided by the number of noun senses that these nouns have, to account for their polysemy." ></td>
	<td class="line x" title="91:203	This cumulative frequency is also divided by the total number of noun hyponyms under that class in WordNet to obtain a smoothed estimate for all nouns under the class." ></td>
	<td class="line x" title="92:203	The probability of the class is obtained by dividing this frequency estimate by the total frequency of the argument heads." ></td>
	<td class="line x" title="93:203	The algorithm is described fully by Li and Abe (1998)." ></td>
	<td class="line x" title="94:203	4See (Li and Abe, 1998) for a full explanation." ></td>
	<td class="line x" title="95:203	0.17 distance street van car mile street car distancelane corner 0.18 0.10 0.17 0.03 entity physical_ entityentity abstract_ way gondola Example nouns hyponym classes locationvehicle tcm selfpropelled Figure 1: portion of the TCM for the objects of park." ></td>
	<td class="line x" title="96:203	A small portion of the TCM for the object slot of park is shown in gure 1." ></td>
	<td class="line x" title="97:203	WordNet classes are displayed in boxes with a label which best re ects the meaning of the class." ></td>
	<td class="line x" title="98:203	The probability estimates are shown for the classes on the TCM." ></td>
	<td class="line x" title="99:203	Examples of the argument head data are displayed below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes." ></td>
	<td class="line x" title="100:203	We cannot show the full TCM due to lack of space, but we show some of the higher probability classes which cover some typical nouns that occur as objects of park." ></td>
	<td class="line x" title="101:203	Note that probability under the classes abstract entity, way and location arise because of a systematic parsing error where adverbials such as distance in park illegally some distance from the railway station are identi ed by the parser as objects." ></td>
	<td class="line x" title="102:203	Systematic noise from the parser has an impact on all the selectional preference models described in this paper." ></td>
	<td class="line x" title="103:203	3.2 WNPROTOs We propose a method of acquiring selectional preferences which instead of covering all the noun senses in WordNet, just gives a probability distribution over a portion of prototypical classes, we refer to these models as WNPROTOs." ></td>
	<td class="line x" title="104:203	A WNPROTO consists of classes within the noun hierarchy which have the highest proportion of word types occurring in the argument head data, rather than using the number of tokens, or frequency, as is used for the TCMs." ></td>
	<td class="line x" title="105:203	This allows less frequent, but potentially informative arguments to have some bearing on the models acquired to reduce the impact of highly frequent but polysemous arguments." ></td>
	<td class="line x" title="106:203	We then used the frequency data to populate these selected classes." ></td>
	<td class="line x" title="107:203	372 The classes (C) in the WNPROTO are selected from those which include at least a threshold of 2 argument head types 5 occurring in the training data." ></td>
	<td class="line x" title="108:203	Each argument head in the training data is disambiguated according to whichever of the WordNet classes it occurs at or under which has the highest type ratio." ></td>
	<td class="line x" title="109:203	Let TY be the set of argument head types in the object slot of the verb for which we are acquiring the preference model." ></td>
	<td class="line x" title="110:203	The type ratio for a class (c) is the ratio of noun types (ty  TY ) occurring in the training data also listed at or beneath that class in WordNet to the total number of noun types listed at or beneath that particular class in WordNet (wnty  c)." ></td>
	<td class="line x" title="111:203	The argument types attested in the training data are divided by the number of WordNet classes that the noun (classes(ty)) belongs to, to account for polysemy in the training data." ></td>
	<td class="line x" title="112:203	type ratio(c) = summationtext tyTY c 1 |classes(ty)| |wnty  c| (2) If more than one class has the same type ratio then the argument is not used for calculating the probability of the preference model." ></td>
	<td class="line x" title="113:203	In this way, only arguments that can be disambiguated are used for calculating the probability distribution." ></td>
	<td class="line x" title="114:203	The advantage of using the type ratio to determine the classes used to represent the model and to disambiguate the arguments is that it prevents high frequency verb noun combinations from masking the information from prototypical but low frequency arguments." ></td>
	<td class="line x" title="115:203	We wish to use classes which are as representative of the argument head types as possible to help detect when an argument head is not related to these classes and is therefore more likely to be non-compositional." ></td>
	<td class="line x" title="116:203	For example, the class motor vehicle is selected for the WNPROTO model of the object slot of park even though there are 5 meanings of car in WordNet including elevator car and gondola." ></td>
	<td class="line x" title="117:203	There are 174 occurrences of car which overwhelms the frequency of the other objects (e.g. van 11, vehicle 8) but by looking for classes with a high proportion of types (rather than word tokens) car is disambiguated appropriately and the class motor vehicle is selected for representation." ></td>
	<td class="line x" title="118:203	5We have experimented with a threshold of 3 and obtained similar results." ></td>
	<td class="line x" title="119:203	0.03 0.04 tanker boat pram.61 car 0.05 van caravan entity physical_ entity Example nouns hyponym classes vehicle selfpropelled transport vehicle wheeled model motor vehicle caravan classes in Figure 2: Part of WNPROTO for the object slot of park The relative frequency of each class is obtained from the set of disambiguated argument head tokens and used to provide the probability distribution over this set of classes." ></td>
	<td class="line x" title="120:203	Note that in WNPROTO, classes can be subsumed by others in the hyponym hierarchy." ></td>
	<td class="line x" title="121:203	The probability assigned to a class is applicable to any descendants in the hyponym hierarchy, except those within any hyponym classes within the WNPROTO." ></td>
	<td class="line x" title="122:203	The algorithm for selecting C and calculating the probability distribution is shown as Algorithm 1." ></td>
	<td class="line x" title="123:203	Note that we use brackets for comments." ></td>
	<td class="line x" title="124:203	In gure 2 we show a small portion of the WNPROTO for park." ></td>
	<td class="line x" title="125:203	Again, WordNet classes are displayed in boxes with a label which best re ects the meaning of the class." ></td>
	<td class="line x" title="126:203	The probability estimates are shown in the boxes for all the classes included in the WNPROTO." ></td>
	<td class="line x" title="127:203	The classes in the WNPROTO model are shown with dashed lines." ></td>
	<td class="line x" title="128:203	Examples of the argument head data are displayed below the WordNet classes with dotted lines indicating membership at a hyponym class beneath these classes." ></td>
	<td class="line x" title="129:203	We cannot show the full WNPROTO due to lack of space, but we show some of the classes with higher probability which cover some typical nouns that occur as objects of park." ></td>
	<td class="line x" title="130:203	373 Algorithm 1 WNPROTO algorithm C = (){classes in WNPROTO} D = () {disambiguated ty  TY } fD = 0 {frequency of disambiguated items} TY = argument head types {nouns occurring as objects of verb, with associated frequencies} C1  WordNet where |ty  TY occurring in c  C1| > 1 for all ty  TY do nd c  classes(ty)  C1 where c = argmaxc typeratio(c) if c & c / C then add c to C add ty  c to D {Disambiguated ty with c} end if end for for all c  C do if |ty  c  D| > 1 then fD = fD + frequency(ty){sum frequencies of types under classes to be used in model} else remove c from C {classes with less than two disambiguated nouns are removed} end if end for for all c  C do p(c) = frequency-of-all-tys-disambiguated-to-class(c,D)fD {calculating class probabilities} end for Algorithm 2 DSPROTO algorithm C = (){classes in DSPROTO} D = () {disambiguated ty  TY } fD = 0 {frequency of disambiguated items} TY = argument head types {nouns occurring as objects of verb, with associated frequencies} C1 = cty  TY where num-types-in-thesaurus(cty,TY ) > 1 order C1 by num-types-in-thesaurus(cty,TY ) {classes ordered by coverage of argument head types} for all cty  ordered C1 do Dcty = () {disambiguated for this class} for all ty  TY where in-thesaurus-entry(cty,ty) do if ty / D then add ty to Dcty {types disambiguated to this class only if not disambiguated by a class used already} end if end for if |Dcty| > 1 then add cty to C for all ty  Dcty do add ty  cty to D {Disambiguated ty with cty} fD = fD + frequency(ty) end for end if end for for all cty  C do p(cty) = frequency-of-all-tys-disambiguated-to-class(cty,D)fD {calculating class probabilities} end for 374 3.3 DSPROTOs We use a thesaurus acquired using the method proposed by Lin (1998)." ></td>
	<td class="line x" title="131:203	For input we used the grammatical relation data from automatic parses of the BNC." ></td>
	<td class="line x" title="132:203	For each noun we considered the cooccurring verbs in the object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations." ></td>
	<td class="line x" title="133:203	Each thesaurus entry consists of the target noun and the 50 most similar nouns, according to Lins measure of distributional similarity, to the target." ></td>
	<td class="line x" title="134:203	The argument head noun types (TY ) are used to nd the entries in the thesaurus as the classes (C) of the selectional preference for a given verb." ></td>
	<td class="line x" title="135:203	As with WNPROTOs, we only cover argument types which form coherent groups with other argument types since we wish i) to remove noise and ii) to be able to identify argument types which are not related with the other types and therefore may be noncompositional." ></td>
	<td class="line x" title="136:203	As our starting point we only consider an argument type as a class for C if its entry in the thesaurus covers at least a threshold of 2 types." ></td>
	<td class="line x" title="137:203	6 To select C we use a best rst search." ></td>
	<td class="line x" title="138:203	This method processes each argument type in TY in order of the number of the other argument types from TY that it has in its thesaurus entry of 50 similar nouns." ></td>
	<td class="line x" title="139:203	An argument head is selected as a class for C (cty  C) 7 if it covers at least 2 of the argument heads that are not in the thesaurus entries of any of the other classes already selected for C. Each argument head is disambiguated by whichever class in C under which it is listed in the thesaurus and which has the largest number of the TY in its thesaurus entry." ></td>
	<td class="line x" title="140:203	When the algorithm nishes processing the ordered argument heads to select C, all argument head types are disambiguated by C apart from those which after disambiguation occur in isolation in a class without other argument types." ></td>
	<td class="line x" title="141:203	Finally a probability distribution over C is estimated using the frequency (tokens) of argument types that occur in the thesaurus entries for any cty  C. If an argument type occurs in the entry of more than one cty then it is assigned to whichever of these has the largest number 6As with the WNPROTOs, we experimented with a value of 3 for this threshold and obtained similar results." ></td>
	<td class="line x" title="142:203	7We use cty for the classes of the DSPROTO." ></td>
	<td class="line x" title="143:203	These classes are simply groups of nouns which occur under the entry of a noun (ty) in the thesaurus." ></td>
	<td class="line x" title="144:203	class (p(c)) disambiguated objects (freq) van (0.86) car (174) van (11) vehicle (8)." ></td>
	<td class="line x" title="145:203	mile (0.05) street (5) distance (4) mile (1) . . ." ></td>
	<td class="line x" title="146:203	yard (0.03) corner (4) lane (3) door (1) backside (0.02) backside (2) bum (1) butt (1) . . ." ></td>
	<td class="line x" title="147:203	Figure 3: First four classes of DSPROTO model for park of disambiguated argument head types and its token frequency is attributed to that class." ></td>
	<td class="line x" title="148:203	We show the algorithm as Algorithm 2." ></td>
	<td class="line x" title="149:203	The algorithms for WNPROTO algorithm 1 and DSPROTO (algorithm 2) differ because of the nature of the inventories of candidate classes (WordNet and the distributional thesaurus)." ></td>
	<td class="line x" title="150:203	There are a great many candidate classes in WordNet." ></td>
	<td class="line x" title="151:203	The WNPROTO algorithm selects the classes from all those that the argument heads belong to directly and indirectly by looping over all argument types to nd the class that disambiguates each by having the largest type ratio calculated using the undisambiguated argument heads." ></td>
	<td class="line x" title="152:203	The DSPROTO only selects classes from the xed set of argument types." ></td>
	<td class="line x" title="153:203	The algorithm loops over the argument types with at least two argument heads in the thesaurus entry and ordered by the number of undisambiguated argument heads in the thesaurus entry." ></td>
	<td class="line x" title="154:203	This is a best rst search to minimise the number of argument heads used in C but maximise the coverage of argument types." ></td>
	<td class="line x" title="155:203	In gure 3, we show part of a DSPROTO model for the object of park." ></td>
	<td class="line x" title="156:203	8 Note again that the class mile arises because of a systematic parsing error where adverbials such as distance in park illegally some distance from the railway station are identi ed by the parser as objects." ></td>
	<td class="line x" title="157:203	4 Experiments Venkatapathy and Joshi (2005) produced a dataset of verb-object pairs with human judgements of compositionality." ></td>
	<td class="line x" title="158:203	They obtained values of rs between 0.111 and 0.300 by individually applying the 7 features described above in section 2." ></td>
	<td class="line x" title="159:203	The best correlation was given by feature 7 and the second best was feature 3." ></td>
	<td class="line x" title="160:203	They combined all 7 features using SVMs and splitting their data into test and training data and achieve a rs of 0.448, which demonstrates 8We cannot show the full model due to lack of space." ></td>
	<td class="line x" title="161:203	375 signi cantly better correlation with the human goldstandard than any of the features in isolation We evaluated our selectional preference models using the verb-object pairs produced by Venkatapathy and Joshi (2005)." ></td>
	<td class="line x" title="162:203	9 This dataset has 765 verbobject collocations which have been given a rating between 1 and 6, by two annotators (both uent speakers of English)." ></td>
	<td class="line x" title="163:203	Kendalls Tau (Siegel and Castellan, 1988) was used to measure agreement, and a score of 0.61 was obtained which was highly signi cant." ></td>
	<td class="line x" title="164:203	The ranks of the two annotators gave a Spearmans rank-correlation coef cient (rs) of 0.71." ></td>
	<td class="line x" title="165:203	The Verb-Object pairs included some adjectives (e.g. happy, difficult, popular), pronouns and complements e.g. become director." ></td>
	<td class="line x" title="166:203	We used the subset of 638 verb-object pairs that involved common nouns in the object relationship since our preference models focused on the object relation for common nouns." ></td>
	<td class="line x" title="167:203	For each verb-object pair we used the preference models acquired from the RASP parses of the BNC to obtain the probability of the class that this object occurs under." ></td>
	<td class="line x" title="168:203	Where the object noun is a member of several classes (classes(noun)  C) in the model, the class with the largest probability is used." ></td>
	<td class="line x" title="169:203	Note though that for WNPROTOs we have the added constraint that a hyponym class from C is selected in preference to a hypernym in C. Compositionality of an object noun and verb is computed as:comp(noun, verb) = maxcclasses(noun)C p(c|verb) (3) We use the probability of the class, rather than an estimate of the probability of the object, because we want to determine how likely any word belonging to this class might occur with the given verb, rather than the probability of the speci c noun which may be infrequent, yet typical, of the objects that occur with this verb." ></td>
	<td class="line x" title="170:203	For example, convertible may be an infrequent object of park, but it is quite likely given its membership of the class motor vehicle." ></td>
	<td class="line x" title="171:203	We do not want to assume anything about the frequency of non-compositional verb-object combinations, just that they are unlikely to be members of classes which represent prototypical objects." ></td>
	<td class="line x" title="172:203	We 9This verb-object dataset is available from http://www.cis.upenn.edu/ sriramv/mywork.html." ></td>
	<td class="line x" title="173:203	method rs p < (one tailed) selectional preferences TCM 0.090 0.0119 WNPROTO 0.223 0.00003 DSPROTO 0.398 0.00003 features from V&J frequency (f1) 0.141 0.00023 MI (f2) 0.274 0.00003 Lin99 (f3) 0.139 0.00023 LSA2 (f7) 0.209 0.00003 combination with SVM f2,3,7 0.413 0.00003 f1,2,3,7 0.419 0.00003 DSPROTO f1,2,3,7 0.454 0.00003 Table 1: Correlation scores for 638 verb object pairs will contrast these models with a baseline frequency feature used by Venkatapathy and Joshi." ></td>
	<td class="line x" title="174:203	We use our selectional preference models to provide the probability that a candidate is representative of the typical objects of the verb." ></td>
	<td class="line x" title="175:203	That is, if the object might typically occur in such a relationship then this should lessen the chance that this verb-object combination is non-compositional." ></td>
	<td class="line x" title="176:203	We used the probability of the classes from our 3 selectional preference models to rank the pairs and then used Spearmans rank-correlation coef cient (rs) to compare these ranks with the ranks from the goldstandard." ></td>
	<td class="line x" title="177:203	Our results for the three types of preference models are shown in the rst section of table 1." ></td>
	<td class="line x" title="178:203	10 All the correlation values are signi cant, but we note that using the type based selectional preference models achieves a far greater correlation than using the TCMs." ></td>
	<td class="line x" title="179:203	The DSPROTO models achieve the best results which is very encouraging given that they only require raw data and an automatic parser to obtain the grammatical relations." ></td>
	<td class="line x" title="180:203	We applied 4 of the features used by Venkatapathy and Joshi (2005) 11 and described in section 2 to our subset of 638 items." ></td>
	<td class="line x" title="181:203	These features were ob10We show absolute values of correlation following (Venkatapathy and Joshi, 2005)." ></td>
	<td class="line x" title="182:203	11The other 3 features performed less well on this dataset so we do not report the details here." ></td>
	<td class="line x" title="183:203	This seems to be because they worked particularly well with the adjective and pronoun data in the full dataset." ></td>
	<td class="line x" title="184:203	376 tained using the same BNC dataset used by Venkatapathy and Joshi which was obtained using Bikels parser (Bikel, 2004)." ></td>
	<td class="line o" title="185:203	We obtained correlation values for these features as shown in table 1 under V&J. These features are feature 1 frequency, feature 2 pointwise mutual information, feature 3 based on (Lin, 1999) and feature 7 LSA feature which considers the similarity of the verb-object pair with the verbal form of the object." ></td>
	<td class="line n" title="186:203	Pointwise mutual information did surprisingly well on this 84% subset of the data, however the DSPROTO preferences still outperformed this feature." ></td>
	<td class="line x" title="187:203	We combined the DSPROTO and V&J features with an SVM ranking function and used 10 fold cross validation as Venkatapathy and Joshi did." ></td>
	<td class="line x" title="188:203	We contrast the result with the V&J features without the preference models." ></td>
	<td class="line x" title="189:203	The results in the bottom section of table 1 demonstrate that the preference models can be combined with other features to produce optimal results." ></td>
	<td class="line x" title="190:203	5 Conclusions and Directions for Future Work We have demonstrated that the selectional preferences of a verbal predicate can be used to indicate if a speci c combination with an object is noncompositional." ></td>
	<td class="line x" title="191:203	We have shown that selectional preference models which represent prototypical arguments and focus on argument types (rather than tokens) do well at the task." ></td>
	<td class="line x" title="192:203	Models produced from distributional thesauruses are the most promising which is encouraging as the technique could be applied to a language without a man-made thesaurus." ></td>
	<td class="line x" title="193:203	We nd that the probability estimates from our models show a highly signi cant correlation, and are very promising for detecting non-compositional verb-object pairs, in comparison to individual features used previously." ></td>
	<td class="line x" title="194:203	Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002)." ></td>
	<td class="line x" title="195:203	A bene t of the DSPROTOs is that they do not require a hand-crafted inventory." ></td>
	<td class="line x" title="196:203	It would also be worthwhile comparing the use of raw data directly, both from the BNC and from googles Web 1T corpus (Brants and Franz, 2006) since web counts have been shown to outperform the Clark and Weir models on a pseudo-disambiguation task (Keller and Lapata, 2003)." ></td>
	<td class="line x" title="197:203	We believe that preferences should NOT be used in isolation." ></td>
	<td class="line x" title="198:203	Whilst a low preference for a noun may be indicative of peculiar semantics, this may not always be the case, for example chew the fat." ></td>
	<td class="line x" title="199:203	Certainly it would be worth combining the preferences with other measures, such as syntactic xedness (Fazly and Stevenson, 2006)." ></td>
	<td class="line x" title="200:203	We also believe it is worth targeting features to speci c types of constructions, for example light verb constructions undoubtedly warrant special treatment (Stevenson et al. , 2003) The selectional preference models we have proposed here might also be applied to other tasks." ></td>
	<td class="line x" title="201:203	We hope to use these models in tasks such as diathesis alternation detection (McCarthy, 2000; Tsang and Stevenson, 2004) and contrast with WordNet models previously used for this purpose." ></td>
	<td class="line x" title="202:203	6 Acknowledgements We acknowledge support from the Royal Society UK for a Dorothy Hodgkin Fellowship to the rst author." ></td>
	<td class="line x" title="203:203	We thank the anonymous reviewers for their constructive comments on this work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2201
Using Self-Trained Bilexical Preferences to Improve Disambiguation Accuracy
Van Noord, Gertjan;"></td>
	<td class="line x" title="1:189	Proceedings of the 10th Conference on Parsing Technologies, pages 110, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:189	c2007 Association for Computational Linguistics Using Self-Trained Bilexical Preferences to Improve Disambiguation Accuracy Gertjan van Noord University of Groningen vannoord@let.rug.nl Abstract A method is described to incorporate bilexical preferences between phrase heads, such as selection restrictions, in a MaximumEntropy parser for Dutch." ></td>
	<td class="line x" title="3:189	The bilexical preferencesaremodelledasassociationrates which are determined on the basis of a very large parsed corpus (about 500M words)." ></td>
	<td class="line x" title="4:189	We show that the incorporation of such selftrained preferences improves parsing accuracy significantly." ></td>
	<td class="line x" title="5:189	1 Motivation In parse selection, the task is to select the correct syntactic analysis of a given sentence from a set of parses generated by some other mechanism." ></td>
	<td class="line x" title="6:189	On the basis of correctly labelled examples, supervised parse selection techniques can be employed to obtain reasonable accuracy." ></td>
	<td class="line x" title="7:189	Although parsing has improved enormously over the last few years, even the most successful parsers make very silly, sometimes embarassing, mistakes." ></td>
	<td class="line x" title="8:189	In our experiments with a large wide-coverage stochastic attribute-value grammar of Dutch, we noted that the system sometimes is insensitive to the naturalness of the various lexical combinations it has to consider." ></td>
	<td class="line x" title="9:189	Although parsers often employ lexical features which are in principle able to represent preferences with respect to word combinations, the size of the training data will be too small to be able to learn the relevance of such features successfully." ></td>
	<td class="line x" title="10:189	In maximum-entropy parsing, the supervised parsingtechniquethatweuseinourexperiments, arbitrary features can be defined which are employed to characterize different parses." ></td>
	<td class="line x" title="11:189	So it is possible to construct features for any property that is thought to be important for disambiguation." ></td>
	<td class="line x" title="12:189	However, such features can be useful for disambiguation only in case the training set contains a sufficient number of occurrences of these features." ></td>
	<td class="line x" title="13:189	This is problematic, in practice, for features that encode bilexical preferences such as selection restrictions, because typical training sets are much too small to estimate the relevance of features representing cooccurrences of two words." ></td>
	<td class="line x" title="14:189	Asasimpleexampleconsidertheambiguous Dutch sentence (1) Melk drinkt de baby niet Milk drinks the baby not The standard model of the parser we experimented with employs a wide variety of features including syntactic features and lexical features." ></td>
	<td class="line x" title="15:189	In particular, the model also includes features which encode whether or not the subject or the object is fronted in a parse." ></td>
	<td class="line x" title="16:189	Since subjects, in general, are fronted much more frequently than objects, the model has learnt to prefer readings in which the fronted constituent is analysed as the subject." ></td>
	<td class="line x" title="17:189	Although the model also contains features to distinguish whether e.g. milk occurs as the subject or the object of drink, the model has not learnt a preference for either of these features, since there were no sentences in the training data that involved both these two words." ></td>
	<td class="line x" title="18:189	To make this point more explicit, we found that in about 200 sentences of our parsed corpus of 27 million sentences milk is the head of the direct object of the verb drink." ></td>
	<td class="line x" title="19:189	Suppose that we would need at leastperhaps5to10sentencesinourtrainingcorpus 1 in order to be able to learn the specific preference between milk and drink." ></td>
	<td class="line x" title="20:189	The implication is that we would need a (manually labeled)!" ></td>
	<td class="line x" title="21:189	training corpus of approximately 1 million sentences (20 million words)." ></td>
	<td class="line x" title="22:189	In contrast, the disambiguation model of the Dutch parser we are reporting on in this paper is trained on a manually labeled corpus of slightly over 7,000 sentences (145,000 words)." ></td>
	<td class="line x" title="23:189	It appears that semi-supervised or un-supervised methods are required here." ></td>
	<td class="line x" title="24:189	Note that the problem not only occurs for artificial examples such as (1); here are a few mis-parsed examples actually encountered in a large parsed corpus: (2) a. Campari moet u gedronken hebben Campari must have drunk you You must have drunk Campari b. De wijn die Elvis zou hebben gedronken als hij wijn zou hebben gedronken The wine Elvis would have drunk if he had drunk wine The wine that would have drunk Elvis if he had drunk wine c. De paus heeft tweehonderd daklozen te eten gehad The pope had twohunderd homeless people for dinner In this paper, we describe an alternative approach in which we employ pointwise mutual information association score in the maximum entropy disambiguation model." ></td>
	<td class="line oc" title="25:189	Pointwise mutual information (Fano, 1961) was used to measure strength of selection restrictions for instance by Church and Hanks (1990)." ></td>
	<td class="line x" title="26:189	The association scores used here are estimated using a very large parsed corpus of 500 million words (27 million sentences)." ></td>
	<td class="line x" title="27:189	We show that the incorporation of this additional knowledge source improves parsing accuracy." ></td>
	<td class="line x" title="28:189	Because the association scores are estimated on the basis of a large corpus that is parsed by the parser that we aim to improve upon, this technique can be described as a somewhat particular instance of self-training." ></td>
	<td class="line x" title="29:189	Self-training has been investigated for statistical parsing before." ></td>
	<td class="line x" title="30:189	Although naively adding self-labeled material to extend training data is normally not succesfull, there have been successful variants of self-learning for parsing as well." ></td>
	<td class="line x" title="31:189	For instance, in McClosky et al.(2006) self-learning is used to improve a two-phase parser reranker, with very good results for the classical Wall Street Journal parsing task." ></td>
	<td class="line x" title="33:189	Clearly, the idea that selection restrictions ought to be useful for parsing accuracy is not new." ></td>
	<td class="line x" title="34:189	However, as far as we know this is the first time that automatically acquired selection restrictions have been shown to improve parsing accuracy results." ></td>
	<td class="line x" title="35:189	Related research includes Abekawa and Okumura (2006) and Kawahara and Kurohashi (2006) where statistical information between verbs and case elements is collected on the basis of large automatically analysed corpora." ></td>
	<td class="line x" title="36:189	2 Background: Alpino parser The experiments are performed using the Alpino parser for Dutch." ></td>
	<td class="line x" title="37:189	In this section we briefly describe the parser, as well as the corpora that we have used in the experiments described later." ></td>
	<td class="line x" title="38:189	2.1 Grammar and Lexicon The Alpino system is a linguistically motivated, wide-coverage grammar and parser for Dutch in the tradition of HPSG." ></td>
	<td class="line x" title="39:189	It consists of over 600 grammar rules and a large lexicon of over 100,000 lexemes and various rules to recognize special constructs such as named entities, temporal expressions, etc. The grammar takes a constructional approach, with rich lexical representations and a large number of detailed, construction specific rules." ></td>
	<td class="line x" title="40:189	Both the lexicon and the rule component are organized in a multiple inheritance hierarchy." ></td>
	<td class="line x" title="41:189	Heuristics have been implementedtodealwithunknownwordsandwordsequences, andungrammaticalorout-of-coveragesentences (which may nevertheless contain fragments that are analysable)." ></td>
	<td class="line x" title="42:189	The Alpino system includes a POS-taggerwhichgreatlyreduceslexicalambiguity, without an observable decrease in parsing accuracy (Prins, 2005)." ></td>
	<td class="line x" title="43:189	2.2 Parser Based on the categories assigned to words, and the set of grammar rules compiled from the HPSG grammar, a left-corner parser finds the set of all parses, and stores this set compactly in a packed parse forest." ></td>
	<td class="line x" title="44:189	All parses are rooted by an instance 2 of the top category, which is a category that generalizes over all maximal projections (S, NP, VP, ADVP, AP, PP and some others)." ></td>
	<td class="line x" title="45:189	If there is no parse covering the complete input, the parser finds all parses for each substring." ></td>
	<td class="line x" title="46:189	In such cases, the robustness component will then select the best sequence of nonoverlapping parses (i.e. , maximal projections) from this set." ></td>
	<td class="line x" title="47:189	In order to select the best parse from the compact parse forest, a best-first search algorithm is applied." ></td>
	<td class="line x" title="48:189	The algorithm consults a Maximum Entropy disambiguation model to judge the quality of (partial) parses." ></td>
	<td class="line x" title="49:189	Since the disambiguation model includes inherently non-local features, efficient dynamic programming solutions are not directly applicable." ></td>
	<td class="line x" title="50:189	Instead, a best-first beam-search algorithm is employed(vanNoordandMalouf, 2005; vanNoord, 2006)." ></td>
	<td class="line x" title="51:189	2.3 Maximum Entropy disambiguation model The maximum entropy model is a conditional model which assigns a probability to a parse t for a given sentence s. Furthermore, fi(t) are the feature functions which count the occurrence of each feature i in a parse t. Each feature ihas an associated weight i. The score  of a parse t is defined as the sum of the weighted feature counts: (t) =summationdisplay i ifi(t) If t is a parse of s, the actual conditional probability is given by the following, where T(s) are all parses of s: P(t|s) = exp((t))summationtext uT(s) exp((u)) However, note that if we only want to select the bestparsewecanignoretheactualprobability, andit suffices to use the score  to rank competing parses." ></td>
	<td class="line x" title="52:189	TheMaximumEntropymodelemploysalargeset of features." ></td>
	<td class="line x" title="53:189	The standard model uses about 42,000 different features." ></td>
	<td class="line x" title="54:189	Features describe various properties of parses." ></td>
	<td class="line x" title="55:189	For instance, the model includes features which signal the application of particular grammar rules, as well as local configurations of grammar rules." ></td>
	<td class="line x" title="56:189	There are features signalling specific POS-tags and subcategorization frames." ></td>
	<td class="line x" title="57:189	Other features signal local or non-local occurrences of extraction (WH-movement, relative clauses etc.), the grammatical role of the extracted element (subject vs. non-subject etc.), features to represent the distance of a relative clause and the noun it modifies, features describing the amount of parallelism between conjuncts in a coordination, etc. In addition, there are lexical features which represent the cooccurrence of two specific words in a specific dependency, and the occurrence of a specific word as a specific dependent for a given POS-tag." ></td>
	<td class="line x" title="58:189	Each parse is characterized by its feature vector (the counts for each of the 42,000 features)." ></td>
	<td class="line x" title="59:189	Once the model is trained, each feature is associated with its weight  (a positive or negative number, typically close to 0)." ></td>
	<td class="line x" title="60:189	To find out which parse is the best parse according to the model, it suffices to multiply the frequency of each feature with its corresponding weight, and sum these weighted frequencies." ></td>
	<td class="line x" title="61:189	The parse with the highest sum is the best parse." ></td>
	<td class="line x" title="62:189	Formal details of the disambiguation model are presented in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="63:189	2.4 Dependency structures Although Alpino is not a dependency grammar in the traditional sense, dependency structures are generated by the lexicon and grammar rules as the value of a dedicated feature dt." ></td>
	<td class="line x" title="64:189	The dependency structures are based on CGN (Corpus Gesproken Nederlands, Corpus of Spoken Dutch) (Hoekstra et al. , 2003), D-Coi and LASSY (van Noord et al. , 2006)." ></td>
	<td class="line x" title="65:189	Such dependency structures are somewhat idiosyncratic, as can be observed in the example in figure 1 for the sentence: (3) waar en wanneer dronk Elvis wijn?" ></td>
	<td class="line x" title="66:189	where and when did Elvis drink wine?" ></td>
	<td class="line x" title="67:189	2.5 Evaluation The output of the parser is evaluated by comparing the generated dependency structure for a corpus sentence to the gold standard dependency structure in a treebank." ></td>
	<td class="line x" title="68:189	For this comparison, we represent the dependency structure (a directed acyclic graph) as a set of named dependency relations." ></td>
	<td class="line x" title="69:189	The dependency graph in figure 1 is represented with the following set of dependencies: 3  whq whd 1 conj cnj adv waar0 crd vg en1 cnj adv wanneer2 body sv1 mod 1 hd verb drink3 su name Elvis4 obj1 noun wijn5 Figure 1: Dependency graph example." ></td>
	<td class="line x" title="70:189	Reentrant nodes are visualized using a bold-face index." ></td>
	<td class="line x" title="71:189	Root forms of head words are explicitly included in separate nodes, and different types of head receive a different relation label such as hd, crd (for coordination), whd (for WH-phrases) etc. In this case, the WH-phrase is both the whd element of the top-node, as well as a mod dependent of drink." ></td>
	<td class="line x" title="72:189	crd/cnj(en,waar) crd/cnj(en,wanneer) whd/body(en,drink) hd/mod(drink,en) hd/obj1(drink,wijn) hd/su(drink,Elvis) Comparing these sets, we count the number of dependencies that are identical in the generated parse and the stored structure, which is expressed traditionally using f-score (Briscoe et al. , 2002)." ></td>
	<td class="line x" title="73:189	We prefer to express similarity between dependency structures by concept accuracy: CA = 1 summationtext i Dif max(summationtexti Dig,summationtexti Dip) where Dip is the number of dependencies produced by the parser for sentence i, Dg is the number of dependencies in the treebank parse, and Df is the number of incorrect and missing dependencies produced by the parser." ></td>
	<td class="line x" title="74:189	The standard version of Alpino that we use here as baseline system is trained on the 145,000 word Alpino treebank, which contains dependency structures for the cdbl (newspaper) part of the Eindhovencorpus." ></td>
	<td class="line x" title="75:189	Theparametersfortrainingthemodel are the same for the baseline model, as well as the model that includes the self-trained bilexical preferences (introduced below)." ></td>
	<td class="line x" title="76:189	These parameters include #sentences 100% 30,000,000 #words 500,000,000 #sentences without parse 0.2% 100,000 #sentences with fragments 8% 2,500,000 #single full parse 92% 27,500,000 Table 1: Approximate counts of the number of sentences and words in the parsed corpus." ></td>
	<td class="line x" title="77:189	About 0,2% of the sentences did not get a parse, for computational reasons (out of memory, or maximum parse time exceeded)." ></td>
	<td class="line x" title="78:189	the Gaussian penalty, thresholds for feature selection, etc. Details of the training procedure are described in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="79:189	2.6 Parsed Corpora Over the course of about a year, Alpino has been used to parse most of the TwNC-02 (Twente Newspaper Corpus), Dutch Wikipedia, and the Duch part of Europarl." ></td>
	<td class="line x" title="80:189	TwNC consists of Dutch newspaper texts from 1994 2004." ></td>
	<td class="line x" title="81:189	We did not use the material from Trouw 2001, since part of that material is used in the test set used below." ></td>
	<td class="line x" title="82:189	We used the 200 node Beowulf Linux cluster of the HighPerformance Computing center of the University of Groningen." ></td>
	<td class="line x" title="83:189	The dependency structures are stored in XML." ></td>
	<td class="line x" title="84:189	The XML files can be processed and searched in various ways, for instance, using XPATH, XSLT and Xquery (Bouma and Kloosterman, 2002)." ></td>
	<td class="line x" title="85:189	Some quantitative information of this parsed corpus is listed in table 1." ></td>
	<td class="line x" title="86:189	In the experiments described below, we do not distinguish between full and fragmentparses; sentenceswithoutaparseareobviously ignored." ></td>
	<td class="line x" title="87:189	3 Bilexical preferences 3.1 Association Score The parsed corpora described in the previous section have been used in order to compute association scores between lexical dependencies." ></td>
	<td class="line x" title="88:189	The parses constructed by Alpino are dependency structures." ></td>
	<td class="line x" title="89:189	In such dependency structures, the basic dependencies are of the form r(w1,w2) where r is a relation such as subject, object, modifier, prepositional complement, , and wi are root forms of words." ></td>
	<td class="line x" title="90:189	Bilexical preference between two root forms w1 4 tokens 480,000,000 types 100,000,000 types with frequency  20 2,000,000 Table 2: Number of lexical dependencies in parsed corpora (approximate counts) bijltje gooi neer 13 duimschroef draai aan 13 peentje zweet 13 traantje pink weg 13 boontje dop 12 centje verdien bij 12 champagne fles ontkurk 12 dorst les 12 Table 3: Pairs involving a direct object relationship withthehighestpointwisemutualinformationscore." ></td>
	<td class="line oc" title="91:189	andw2 iscomputedusinganassociationscorebased on pointwise mutual information, asdefinedbyFano (1961) and used for a similar purpose in Church and Hanks (1990), as well as in many other studies in corpus linguistics." ></td>
	<td class="line x" title="92:189	The association score is defined here as follows: I(r(w1,w2) = log f(r(w1,w2))f(r(w 1, ))f( (,w2)) where f(X) is the relative frequency of X. In the above formula, the underscore is a place holder for an arbitrary relation or an arbitrary word." ></td>
	<td class="line x" title="93:189	The association score I compares the actual relative frequency of w1 and w2 with dependency r, with the relative frequency we would expect if the words were independent." ></td>
	<td class="line x" title="94:189	For instance, to compute I(hd/obj1(drink,melk)) we lookup the number of timesdrinkoccurs with a direct object out of all 462,250,644 dependencies (15,713) and the number oftimesmelkoccursasadependent(10,172)." ></td>
	<td class="line x" title="95:189	Ifwe multiply the two corresponding relative frequencies, we get the expected relative frequency (0.35) for hd/obj1(drink,melk), which is about 560 times as big as the actual frequence, 195." ></td>
	<td class="line x" title="96:189	Taking the log of this gives us the association score (6.33) for this bi-lexical dependency." ></td>
	<td class="line x" title="97:189	Note that pairs that we have seen fewer than 20 times are ignored." ></td>
	<td class="line x" title="98:189	Mutual information scores are unreliable for low frequencies." ></td>
	<td class="line x" title="99:189	An additional benefit of a frequency threshold is a manageable size of the resulting data-structures." ></td>
	<td class="line x" title="100:189	The pairs involving a direct object relationship with the highest scores are listed in table 3." ></td>
	<td class="line x" title="101:189	The biertje small glass of beer 8 borreltje strong alcoholic drink 8 glaasje small glass 8 pilsje small glass of beer 8 pintje small glass of beer 8 pint glass of beer 8 wijntje small glass of wine 8 alcohol alcohol 7 bier beer 7 Table 4: Pairs involving a direct object relationship with the highest pointwise mutual information score for the verb drink." ></td>
	<td class="line x" title="102:189	overlangs snijd door 12 welig tier 12 dunnetjes doe over 11 stief moederlijk bedeel 11 on zedelijk betast 11 stierlijk verveel 11 cum laude studeer af 10 hermetisch grendel af 10 ingespannen tuur 10 instemmend knik 10 kostelijk amuseer 10 Table 5: Pairs involving a modifier relationship between a verb and an adverbial with the highest association score." ></td>
	<td class="line x" title="103:189	highest scoring nouns that occur as the direct object of drink are listed in table 4." ></td>
	<td class="line x" title="104:189	Selection restrictions are often associated only with direct objects." ></td>
	<td class="line x" title="105:189	We include bilexical association scores for all types of dependencies." ></td>
	<td class="line x" title="106:189	We found that association scores for other types of dependencies also captures both collocational preferences as well as weaker cooccurrence preferences." ></td>
	<td class="line x" title="107:189	Some examples including modifiers are listed in table 5." ></td>
	<td class="line x" title="108:189	Such preferences are useful for disambiguation as well." ></td>
	<td class="line x" title="109:189	Consider the ambiguous Dutch sentence (4) omdat we lauw bier dronken because we drank warm beer because we drank beer warmly The adjective lauw (cold, lukewarm, warm) can be used to modify both nouns and verbs; this latter possibility is exemplified in: (5) We hebben lauw gereageerd We reacted indifferently 5  smain obj1 conj cnj noun bier0 crd vg of1 cnj noun wijn2 hd verb drink3 su name Elvis4 mod adv niet5 Figure 2: Dependency structure produced for coordination 3.2 Extending pairs The CGN dependencies that we work with fail to relate pairs of words in certain syntactic constructions for which it can be reasonably assumed that bilexical preferences should be useful." ></td>
	<td class="line x" title="110:189	We have identified two such constructions, namely relative clauses and coordination, and for these constructions we generalize our method, to take such dependencies into account too." ></td>
	<td class="line x" title="111:189	Consider coordinations such as: (6) Bier of wijn drinkt Elvis niet Beer or wine, Elvis does not drink The dependency structure of the intended analysis is given in figure 2." ></td>
	<td class="line x" title="112:189	The resulting set of dependencies for this example treats the coordinator as the head of the conjunction: hd/obj1(drink,of) crd/cnj(of,bier) crd/cnj(of,wijn) hd/su(drink,elvis) hd/mod(drink,niet) Sotherearenodirectdependenciesbetweentheverb and the individual conjuncts." ></td>
	<td class="line x" title="113:189	For this reason, we add additional dependencies r(A,C) for every pair of dependency r(A,B),crd/cnj(B,C)." ></td>
	<td class="line x" title="114:189	Relative clauses are another syntactic phenomenon where we extend the set of dependencies." ></td>
	<td class="line x" title="115:189	For a noun phrase such as: (7) Wijn die Elvis niet dronk Wine which Elvis did not drink there is no direct dependency between wijn and drink, as can be seen in the dependency structure  np hd noun wijn0 mod rel rhd 1 pron die1 body ssub obj1 1 su name Elvis2 mod adv niet3 hd verb drink4 Figure 3: Dependency structure produced for relative clause given in figure 3." ></td>
	<td class="line x" title="116:189	Sets of dependencies are extended in such cases, to make the relation between the noun and the role it plays in the relative clause explicit." ></td>
	<td class="line x" title="117:189	3.3 Using association scores as features The association scores for all dependencies are used in our maximum entropy disambiguation model as follows." ></td>
	<td class="line x" title="118:189	The technique is reminiscent of the inclusion of auxiliary distributions in stochastic attributevalue grammar (Johnson and Riezler, 2000)." ></td>
	<td class="line x" title="119:189	Recall that a maximum entropy disambiguation model exploits features." ></td>
	<td class="line x" title="120:189	Features are properties of parses, and we can use such features to describe any property of parses that we believe is of importance for disambiguation." ></td>
	<td class="line x" title="121:189	For the disambiguation model, a parse is fully characterized by a vector of feature counts." ></td>
	<td class="line x" title="122:189	We introduce features z(t,r) for each of the major POS labels t (verb, noun, adjective, adverb, ) and each of the dependency relations r. The count of such a feature is determined by the association scores for actually occuring dependency pairs." ></td>
	<td class="line x" title="123:189	For example, if in a given parse a given verb v has a direct object dependent n, then we compute the association of this particular pair, and use the resulting number as the count of that feature." ></td>
	<td class="line x" title="124:189	Of course, if there are multiple dependencies of this type in a single parse, the corresponding association scores are all summed." ></td>
	<td class="line x" title="125:189	To illustrate this technique, consider the dependency structure given earlier in figure 2." ></td>
	<td class="line x" title="126:189	For this 6 example, there are four of these new features with a non-zero count." ></td>
	<td class="line x" title="127:189	The counts are given by the corresponding association scores as follows: z(verb,hd/su) = I(hd/su(drink,elvis)) z(verb,hd/mod) = I(hd/mod(drink,niet)) z(verb,hd/obj1) = I(hd/obj1(drink,of)) + I(hd/obj1(drink,bier)) + I(hd/obj1(drink,wijn)) z(conj,crd/cnj) = I(crd/cnj(of,bier)) + I(crd/cnj(of,wijn)) It is crucial to observe that the new features do not include any direct reference to actual words." ></td>
	<td class="line x" title="128:189	This means that there will be only a fairly limited number of new features (depending on the number of tags t and relations r), and we can expect that these features are frequent enough to be able to estimate their weights in training material of limited size." ></td>
	<td class="line x" title="129:189	Associationscorescanbenegativeiftwowordsin a lexical dependency occur less frequently than one would expect if the words were independent." ></td>
	<td class="line x" title="130:189	However, since association scores are unreliable for low frequencies (including, often, frequencies of zero), and since such negative associations involve low frequencies by their nature, we only take into account positive association scores." ></td>
	<td class="line x" title="131:189	4 Experiments We report on two experiments." ></td>
	<td class="line x" title="132:189	In the first experiment, we report on the results of tenfold crossvalidation on the Alpino treebank." ></td>
	<td class="line x" title="133:189	This is the material that is standardly used for training and testing." ></td>
	<td class="line x" title="134:189	For each of the sentences of this corpus, the system produces atmost the first 1000 parses." ></td>
	<td class="line x" title="135:189	For every parse we compute the quality by comparing its dependency structure with the gold standard dependency structure in the treebank." ></td>
	<td class="line x" title="136:189	For training, atmost 100 parses are selected randomly for each sentence." ></td>
	<td class="line x" title="137:189	For (tenfold cross-validated) testing, we use all available parses for a given sentence." ></td>
	<td class="line x" title="138:189	In order to testthequalityofthemodel,wecheckforeachgiven sentence which of its atmost 1000 parses is selected by the disambiguation model." ></td>
	<td class="line x" title="139:189	The quality of that parse is used in the computation of the accuracy, as listed in table 6." ></td>
	<td class="line x" title="140:189	The column labeled exact measures the proportion of sentences for which the model selected the best possible parse (there can be multiple fscore err.red." ></td>
	<td class="line x" title="141:189	exact CA % % % % baseline 74.02 0.00 16.0 73.48 oracle 91.97 100.00 100.0 91.67 standard 87.41 74.60 52.0 87.02 +self-training 87.91 77.38 54.8 87.51 Table 6: Results with ten-fold cross-validation on the Eindhoven-cdbl part of the Alpino treebank." ></td>
	<td class="line x" title="142:189	In these experiments, the models are used to select a parsefromagivensetofatmost1000parsespersentence." ></td>
	<td class="line x" title="143:189	best possible parses)." ></td>
	<td class="line x" title="144:189	The baseline row reports on the quality of a disambiguation model which simply selects the first parse for each sentence." ></td>
	<td class="line x" title="145:189	The oracle row reports on the quality of the best-possible disambiguationmodel, whichwould(bymagic)always select the best possible parse (some parses are outside the coverage of the system, and some parses are generatedonlyaftermorethan1000inferiorparses)." ></td>
	<td class="line x" title="146:189	The error reduction column measures which part of the disambiguation problem (difference between the baseline and oracle scores) is solved by the model.1 The results show a small but clear increase in error reduction, if the standard model (without the association score features) is compared with a (retrained) model that includes the association score features." ></td>
	<td class="line x" title="147:189	The relatively large improvement of the exact score suggests that the bilexical preference features are particularly good at choosing between very good parses." ></td>
	<td class="line x" title="148:189	For the second experiment, we evaluate how well theresultingmodelperformsinthefullsystem." ></td>
	<td class="line x" title="149:189	First of all, this is the only really convincing evaluation which measures progress for the system as a whole by virtue of including bilexical preferences." ></td>
	<td class="line x" title="150:189	The second motivation for this experiment is for methodological reasons: we now test on a truly unseen test-set." ></td>
	<td class="line x" title="151:189	The first experiment can be criti1Note that the error reduction numbers presented in the table are lower than those presented in van Noord and Malouf (2005)." ></td>
	<td class="line x" title="152:189	The reason is, that we report here on experiments in which parses are generated with a version of Alpino with the POS-tagger switched on." ></td>
	<td class="line x" title="153:189	The POS-tagger already reduces the number of ambiguities, and in particular solves many of the easy cases." ></td>
	<td class="line x" title="154:189	The resulting models, however, are more effective in practice (where the model also is applied after the POStagger)." ></td>
	<td class="line x" title="155:189	7 prec rec fscore CA % % % % standard 90.77 90.49 90.63 90.32 +self-training 91.19 90.89 91.01 90.73 Table 7: Results on the WR-P-P-H part of the D-Coi corpus (2267 sentences from the newspaper Trouw, from 2001)." ></td>
	<td class="line x" title="156:189	In these experiments, we report on the full system." ></td>
	<td class="line x" title="157:189	In the full system, the disambiguation model is used to guide a best-first beam-search procedure which extracts a parse from the parse forest." ></td>
	<td class="line x" title="158:189	Difference in CA was found to be significant (using paired T-test on the per sentence CA scores)." ></td>
	<td class="line x" title="159:189	cized on methodological grounds as follows." ></td>
	<td class="line x" title="160:189	The Alpino Treebank was used to train the disambiguation model which was used to construct the large parsed treebank from which we extracted the counts fortheassociationscores." ></td>
	<td class="line x" title="161:189	Thosescoresmightsomehow therefore indirectly reflect certain aspects of the Alpino Treebank training data." ></td>
	<td class="line x" title="162:189	Testing on that data later (with the inclusion of the association scores) is therefore not sound." ></td>
	<td class="line x" title="163:189	For this second experiment we used the WR-P-PH (newspaper) part of the D-Coi corpus." ></td>
	<td class="line x" title="164:189	This part contains 2256 sentences from the newspaper Trouw (2001)." ></td>
	<td class="line x" title="165:189	In table 7 we show the resulting f-score and CA for a system with and without the inclusion of the z(t,r) features." ></td>
	<td class="line x" title="166:189	The improvement found in the previous experiment is confirmed." ></td>
	<td class="line x" title="167:189	5 Conclusion and Outlook One might wonder why self-training works in the case of selection restrictions, at least in the set-up described above." ></td>
	<td class="line x" title="168:189	One may argue that, in order to learn that milk is a good object for drink, the parser has to analyse examples of drink milk in the raw data correctly." ></td>
	<td class="line x" title="169:189	But if the parser is capable of analysing these examples, why does it need selection restrictions?" ></td>
	<td class="line x" title="170:189	The answer appears to be that the parser (without selection restrictions) is able to analyse the large majority of cases correctly." ></td>
	<td class="line x" title="171:189	These cases include the many easy occurrences where no (difficult)ambiguitiesarise(casemarking, numberagreement and other syntactic characteristics often force a single reading)." ></td>
	<td class="line x" title="172:189	The easy cases outnumber the misparsed difficult cases, and therefore the selection restrictions can be learned." ></td>
	<td class="line x" title="173:189	Using these selection restrictions as additional features, the parser is then able to also get the difficult, ambiguous, cases right." ></td>
	<td class="line x" title="174:189	There are various aspects of our method that need further investigation." ></td>
	<td class="line x" title="175:189	First of all, existing techniques that involve selection restrictions (e.g. , Resnik (1993)) typically assume classes of nouns, rather than individual nouns." ></td>
	<td class="line x" title="176:189	In future work we hope to generalize our method to take classes into account, where the aim is to learn class membership also on the basis of large parsed corpora." ></td>
	<td class="line x" title="177:189	Another aspect of the technique that needs further research involves the use of a threshold in establishing the association score, and perhaps related to this issue, the incorporation of negative association scores (for instance for cases where a large number of cooccurrences of a pair would be expected but where in fact none or very few were found)." ></td>
	<td class="line x" title="178:189	There are also some more practical issues that perhaps had a negative impact on our results." ></td>
	<td class="line x" title="179:189	First, the large parsed corpus was collected over a period of about a year, but during that period, the actual system was not stable." ></td>
	<td class="line x" title="180:189	In particular, due to various improvements of the dictionary, the root form of words that was used by the system changed over time." ></td>
	<td class="line x" title="181:189	Since we used root forms in the computation of the association scores, this could be harmful in some specific cases." ></td>
	<td class="line x" title="182:189	A further practical issue concerns repeated sentences or even full paragraphs." ></td>
	<td class="line x" title="183:189	This happens in typical newspaper material for instance in the case of short descriptions of movies that may be repeated weekly for as long as that movie is playing." ></td>
	<td class="line x" title="184:189	Pairs of words that occur in such repeated sentences receive association scores that are much too high." ></td>
	<td class="line x" title="185:189	The method should be adapted to take this into account, perhaps simply by removing duplicated sentences." ></td>
	<td class="line x" title="186:189	Clearly, the idea that selection restrictions ought to be useful for parsing is not new." ></td>
	<td class="line x" title="187:189	However, as far as we know this is the first time that automatically acquired selection restrictions have been shown to improve parsing accuracy results." ></td>
	<td class="line x" title="188:189	Acknowledgements This research was carried out in part in the context of the D-Coi and Lassy projects." ></td>
	<td class="line x" title="189:189	The D-Coi and Lassy projects are carried 8 out within the STEVIN programme which is funded by the Dutch and Flemish governments (http://taalunieversum.org/taal/technologie/stevin/)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1117
Using Three Way Data for Word Sense Discrimination
Van de Cruys, Tim;"></td>
	<td class="line x" title="1:212	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 929936 Manchester, August 2008 Using Three Way Data for Word Sense Discrimination Tim Van de Cruys Humanities Computing University of Groningen t.van.de.cruys@rug.nl Abstract In this paper, an extension of a dimensionality reduction algorithm called NONNEGATIVE MATRIX FACTORIZATION is presented that combines both bag of words data and syntactic data, in order to find semantic dimensions according to which both words and syntactic relations can be classified." ></td>
	<td class="line x" title="2:212	The use of three way data allows one to determine which dimension(s) are responsible for a certain sense of a word, and adapt the corresponding feature vector accordingly, subtracting one sense to discover another one." ></td>
	<td class="line x" title="3:212	The intuition in this is that the syntactic features of the syntax-based approach can be disambiguated by the semantic dimensions found by the bag of words approach." ></td>
	<td class="line x" title="4:212	The novel approach is embedded into clustering algorithms, to make it fully automatic." ></td>
	<td class="line x" title="5:212	The approach is carried out for Dutch, and evaluated against EuroWordNet." ></td>
	<td class="line x" title="6:212	1 Introduction Automatically acquiring semantics from text is a subject that has gathered a lot of attention for quite some time now." ></td>
	<td class="line x" title="7:212	As Manning and Schutze (Manning and Schutze, 2000) point out, most work on acquiring semantic properties of words has focused on semantic similarity." ></td>
	<td class="line x" title="8:212	Automatically acquiring a relative measure of how similar a word is to known words [] is much easier than determining what the actual meaning is. (Manning and Schutze, 2000,8.5) c2008." ></td>
	<td class="line x" title="9:212	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:212	Some rights reserved." ></td>
	<td class="line x" title="11:212	Most work on semantic similarity relies on the distributional hypothesis (Harris, 1985)." ></td>
	<td class="line x" title="12:212	This hypothesis states that words that occur in similar contexts tend to be similar." ></td>
	<td class="line x" title="13:212	With regard to the context used, two basic approaches exist." ></td>
	<td class="line x" title="14:212	One approach makes use of bag of words co-occurrence data; in this approach, a certain window around a word is used for gathering co-occurrence information." ></td>
	<td class="line x" title="15:212	The window may either be a fixed number of words, or the paragraph or document that a word appears in." ></td>
	<td class="line x" title="16:212	Thus, words are considered similar if they appear in similar windows (documents)." ></td>
	<td class="line x" title="17:212	One of the dominant methods using this method is LATENT SEMANTIC ANALYSIS (LSA)." ></td>
	<td class="line x" title="18:212	The second approach uses a more fine grained distributional model, focusing on the syntactic relations that words appear with." ></td>
	<td class="line x" title="19:212	Typically, a large text corpus is parsed, and dependency triples are extracted.1 Words are considered similar if they appear with similar syntactic relations." ></td>
	<td class="line x" title="20:212	Note that the former approach does not need any kind of linguistic annotation, whereas for the latter, some form of syntactic annotation is needed." ></td>
	<td class="line x" title="21:212	The results yielded by both approaches are typically quite different in nature: the former approach typically puts its finger on a broad, thematic kind of similarity, while the latter approach typically grasps a tighter, synonym-like similarity." ></td>
	<td class="line x" title="22:212	Example (1) shows the difference between both approaches; for each approach, the top ten most similar nouns to the Dutch noun muziek music are given." ></td>
	<td class="line x" title="23:212	In (a), the window-based approach is used, while (b) uses the syntax-based approach." ></td>
	<td class="line x" title="24:212	(a) shows indeed more thematic similarity, whereas (b) shows tighter similarity." ></td>
	<td class="line x" title="25:212	1e.g. dependency relations that qualify apple might be object of eat and adjective red." ></td>
	<td class="line x" title="26:212	This gives us dependency triples like < apple,obj,eat >." ></td>
	<td class="line x" title="27:212	929 (1) a. muziek music: gitaar guitar, jazz jazz, cd cd, rock rock, bas bass, song song, muzikant musician, musicus musician, drum drum, slagwerker drummer b. muziek music: dans dance, kunst art, klank sound, liedje song, geluid sound, poezie poetry, literatuur literature, popmuziek pop music, lied song, melodie melody Especially the syntax-based method has been adopted by many researchers, in order to find semantically similar words." ></td>
	<td class="line x" title="28:212	There is, however, one important problem with this kind of approach: the method is not able to cope with ambiguous words." ></td>
	<td class="line x" title="29:212	Take the examples: (2) een a oneven odd nummer number an odd number (3) een a steengoed great nummer number a great song The word nummer does not have the same meaning in these examples." ></td>
	<td class="line x" title="30:212	In example (2), nummer is used in the sense of designator of quantity." ></td>
	<td class="line x" title="31:212	In example (3), it is used in the sense of musical performance." ></td>
	<td class="line x" title="32:212	Accordingly, we would like the word nummer to be disambiguated into two senses, the first sense being similar to words like getal number, cijfer digit and the second to words like liedje song, song song." ></td>
	<td class="line x" title="33:212	While it is relatively easy for a human language user to distinguish between the two senses, this is a difficult task for a computer." ></td>
	<td class="line x" title="34:212	Even worse: the results get blurred because the attributes of both senses (in this example oneven and steengoed) are grouped together into one sense." ></td>
	<td class="line x" title="35:212	This is the main drawback of the syntax-based method." ></td>
	<td class="line x" title="36:212	On the other hand, methods that capture semantic dimensions are known to be useful in disambiguating different senses of a word." ></td>
	<td class="line x" title="37:212	Particularly, PROBABILISTIC LATENT SEMANTIC ANALYSIS (PLSA) is known to simultaneously encode various senses of words according to latent semantic dimensions (Hofmann, 1999)." ></td>
	<td class="line x" title="38:212	In this paper, we want to explore an approach that tries to remedy the shortcomings of the former, syntax-based approach with the benefits of the latter." ></td>
	<td class="line x" title="39:212	The intuition in this is that the syntactic features of the syntaxbased approach can be disambiguated by the latent semantic dimensions found by the windowbased approach." ></td>
	<td class="line x" title="40:212	2 Previous Work 2.1 Distributional Similarity There have been numerous approaches for computing the similarity between words from distributional data." ></td>
	<td class="line x" title="41:212	We mention some of the most important ones." ></td>
	<td class="line x" title="42:212	With regard to the first approach  using a context window  we already mentioned LSA (Landauer and Dumais, 1997)." ></td>
	<td class="line x" title="43:212	In LSA, a termdocument matrix is created, containing the frequency of each word in a specific document." ></td>
	<td class="line x" title="44:212	This matrix is then decomposed into three other matrices with a mathematical technique called SINGULAR VALUE DECOMPOSITION." ></td>
	<td class="line x" title="45:212	The most important dimensions that come out of the SVD allegedly represent latent semantic dimensions, according to which nouns and documents can be presented more efficiently." ></td>
	<td class="line x" title="46:212	LSA has been criticized for not being the most appropriate data reduction method for textual applications." ></td>
	<td class="line x" title="47:212	The SVD underlying the method assumes normally-distributed data, whereas textual count data (such as the term-document matrix) can be more appropriately modeled by other distributional models such as Poisson (Manning and Schutze, 2000,15.4.3)." ></td>
	<td class="line x" title="48:212	Successive methods such as PROBABILISTIC LATENT SEMANTIC ANALYSIS (PLSA) (Hofmann, 1999), try to remedy this shortcoming by imposing a proper latent variable model, according to which the values can be estimated." ></td>
	<td class="line x" title="49:212	The method we adopt in our research  NON-NEGATIVE MATRIX FACTORIZATION  is similar to PLSA, and adequately remedies this problem as well." ></td>
	<td class="line x" title="50:212	The second approach  using syntactic relations  has been adopted by many researchers, in order to acquire semantically similar words." ></td>
	<td class="line x" title="51:212	One of the most important is Lins (1998)." ></td>
	<td class="line x" title="52:212	For Dutch, the approach has been applied by Van der Plas & Bouma (2005)." ></td>
	<td class="line x" title="53:212	2.2 Discriminating senses Schutze (1998) uses a disambiguation algorithm  called context-group discrimination  based on the clustering of the context of ambiguous words." ></td>
	<td class="line x" title="54:212	The clustering is based on second-order co-occurrence: the contexts of the ambiguous word are similar if the words they in turn co-occur with are similar." ></td>
	<td class="line x" title="55:212	Pantel and Lin (2002) present a clustering algorithm  coined CLUSTERING BY COMMITTEE (CBC)  that automatically discovers word senses 930 from text." ></td>
	<td class="line x" title="56:212	The key idea is to first discover a set of tight, unambiguous clusters, to which possibly ambiguous words can be assigned." ></td>
	<td class="line x" title="57:212	Once a word has been assigned to a cluster, the features associated with that particular cluster are stripped off the words vector." ></td>
	<td class="line x" title="58:212	This way, less frequent senses of the word can be discovered." ></td>
	<td class="line x" title="59:212	The former approach uses a window-based method; the latter uses syntactic data." ></td>
	<td class="line x" title="60:212	But none of the algorithms developed so far have combined both sources in order to discriminate among different senses of a word." ></td>
	<td class="line x" title="61:212	3 Methodology 3.1 Non-negative Matrix Factorization 3.1.1 Theory Non-negative matrix factorization (NMF) (Lee and Seung, 2000) is a group of algorithms in which a matrix V is factorized into two other matrices, W and H. VnmWnrHrm (1) Typically r is much smaller than n,m so that both instances and features are expressed in terms of a few components." ></td>
	<td class="line x" title="62:212	Non-negative matrix factorization enforces the constraint that all three matrices must be nonnegative, so all elements must be greater than or equal to zero." ></td>
	<td class="line x" title="63:212	The factorization turns out to be particularly useful when one wants to find additive properties." ></td>
	<td class="line x" title="64:212	Formally, the non-negative matrix factorization is carried out by minimizing an objective function." ></td>
	<td class="line x" title="65:212	Two kinds of objective function exist: one that minimizes the Euclidean distance, and one that minimizes the Kullback-Leibler divergence." ></td>
	<td class="line x" title="66:212	In this framework, we will adopt the latter, as  from our experience  entropy-based measures tend to work well for natural language." ></td>
	<td class="line x" title="67:212	Thus, we want to find the matrices W and H for which the KullbackLeibler divergence between V and WH (the multiplication of W and H) is the smallest." ></td>
	<td class="line x" title="68:212	Practically, the factorization is carried out through the iterative application of update rules." ></td>
	<td class="line x" title="69:212	Matrices W and H are randomly initialized, and the rules in 2 and 3 are iteratively applied  alternating between them." ></td>
	<td class="line x" title="70:212	In each iteration, each vector is adequately normalized, so that all dimension values sum to 1." ></td>
	<td class="line x" title="71:212	HaHa summationtext i Wia Vi (WH)isummationtext k Wka (2) WiaWia summationtext  Ha Vi (WH)isummationtext v Hav (3) 3.1.2 Example We can now straightforwardly apply NMF to create semantic word models." ></td>
	<td class="line x" title="72:212	NMF is applied to a frequency matrix, containing bag of words cooccurrence data." ></td>
	<td class="line x" title="73:212	The additive property of NMF ensures that semantic dimensions emerge, according to which the various words can be classified." ></td>
	<td class="line x" title="74:212	Two sample dimensions are shown in example (4)." ></td>
	<td class="line x" title="75:212	For each dimension, the words with the largest value on that dimension are given." ></td>
	<td class="line x" title="76:212	Dimension (a) can be qualified as a transport dimension, and dimension (b) as a cooking dimension." ></td>
	<td class="line x" title="77:212	(4) a. bus bus, taxi taxi, trein train, halte stop, reiziger traveler, perron platform, tram tram, station station, chauffeur driver, passagier passenger b. bouillon broth, slagroom cream, ui onion, eierdooier egg yolk, laurierblad bay leaf, zout salt, deciliter decilitre, boter butter, bleekselderij celery, saus sauce 3.2 Extending Non-negative Matrix Factorization We now propose an extension of NMF that combines both the bag of words approach and the syntactic approach." ></td>
	<td class="line x" title="78:212	The algorithm finds again latent semantic dimensions, according to which nouns, contexts and syntactic relations are classified." ></td>
	<td class="line x" title="79:212	Since we are interested in the classification of nouns according to both bag-of-words context and syntactic context, we first construct three matrices that capture the co-occurrence frequency information for each mode." ></td>
	<td class="line x" title="80:212	The first matrix contains co-occurrence frequencies of nouns crossclassified by dependency relations, the second matrix contains co-occurrence frequencies of nouns cross-classified by words that appear in the nouns context window, and the third matrix contains cooccurrence frequencies of dependency relations cross-classified by co-occurring context words." ></td>
	<td class="line x" title="81:212	We then apply NMF to the three matrices, but we interleave the separate factorizations: the results of the former factorization are used to initialize the factorization of the next matrix." ></td>
	<td class="line x" title="82:212	This implies that we need to initialize only three matrices at random; the other three are initialized by calculations of the 931 previous step." ></td>
	<td class="line x" title="83:212	The process is represented graphically in figure 1." ></td>
	<td class="line x" title="84:212	Figure 1: A graphical representation of the extended NMF In the example in figure 1, matrix H is initialized at random, and the update of matrix W is calculated." ></td>
	<td class="line x" title="85:212	The result of update W is then used to initialize matrix V , and the update of matrix G is calculated." ></td>
	<td class="line x" title="86:212	This matrix is used again to initialize matrix U, and the update of matrix F is calculated." ></td>
	<td class="line x" title="87:212	This matrix can be used to initialize matrix H, and the process is repeated until convergence." ></td>
	<td class="line x" title="88:212	In (5), an example is given of the kind of semantic dimensions found." ></td>
	<td class="line x" title="89:212	This dimension may be coined the transport dimension, as is shown by the top 10 nouns (a), context words (b) and syntactic relations (c)." ></td>
	<td class="line x" title="90:212	(5) a. auto car, wagen car, tram tram, motor motorbike, bus bus, metro subway, automobilist driver, trein trein, stuur steering wheel, chauffeur driver b. auto car, trein train, motor motorbike, bus bus, rij drive, chauffeur driver, fiets bike, reiziger reiziger, passagier passenger, vervoer transport c. viertrapsadj four pedal, verplaats metobj move with, toeteradj honk, tank in houdobj [parsing error], tanksubj refuel, tankobj refuel, rij voorbijsubj pass by, rij voorbijadj pass by, rij afsubj drive off, peperduuradj very expensive 3.3 Sense Subtraction Next, we want to use the factorization that has been created in the former step for word sense discrimination." ></td>
	<td class="line x" title="91:212	The intuition is that we switch off one dimension of an ambiguous word, to reveal possible other senses of the word." ></td>
	<td class="line x" title="92:212	From matrix H, we know the importance of each syntactic relation given a dimension." ></td>
	<td class="line x" title="93:212	With this knowledge, we can subtract the syntactic relations that are responsible for a certain dimension from the original noun vector: v new =v orig(1 h dim) (4) Equation 4 multiplies each feature (syntactic relation) of the original noun vector (v orig) with a scaling factor, according to the load of the feature on the subtracted dimension (h dim  the vector of matrix H containing the dimension we want to subtract)." ></td>
	<td class="line x" title="94:212	1 is a vector of ones, the size ofh dim." ></td>
	<td class="line x" title="95:212	3.4 A Clustering Framework The last step is to determine which dimension(s) are responsible for a certain sense of the word." ></td>
	<td class="line x" title="96:212	In order to do so, we embed our method in a clustering approach." ></td>
	<td class="line x" title="97:212	First, a specific word is assigned to its predominant sense (i.e. the most similar cluster)." ></td>
	<td class="line x" title="98:212	Next, the dominant semantic dimension(s) for this cluster are subtracted from the word vector (equation 4), and the resulting vector is fed to the clustering algorithm again, to see if other word senses emerge." ></td>
	<td class="line x" title="99:212	The dominant semantic dimension(s) can be identified by folding in the cluster centroid into our factorization (so we get a vectorw of dimension size r), and applying a threshold to the result (in our experiments a threshold of  = .05  so dimensions responsible for > 5% of the centroid are subtracted)." ></td>
	<td class="line x" title="100:212	We used two kinds of clustering algorithms to determine our initial centroids." ></td>
	<td class="line x" title="101:212	The first algorithm is a standard K-means algorithm." ></td>
	<td class="line x" title="102:212	The second one is the CBC algorithm by Pantel and Lin (2002)." ></td>
	<td class="line oc" title="103:212	The initial vectors to be clustered are adapted with pointwise mutual information (Church and Hanks, 1990)." ></td>
	<td class="line x" title="104:212	3.4.1 K-means First, a standard K-means algorithm is applied to the nouns we want to cluster." ></td>
	<td class="line x" title="105:212	This yields a hard clustering, in which each noun is assigned to exactly one (dominant) cluster." ></td>
	<td class="line x" title="106:212	In the second step, we try to determine for each noun whether it can be assigned to other, less dominant clusters." ></td>
	<td class="line x" title="107:212	First, the salient dimension(s) of the centroid to which the noun is assigned are determined." ></td>
	<td class="line o" title="108:212	We compute the centroid of the cluster by averaging the frequencies of all cluster elements except for the target element we want to reassign, and adapt the centroid with pointwise mutual information." ></td>
	<td class="line x" title="109:212	After 932 subtracting the salient dimensions from the noun vector, we check whether the vector is reassigned to another cluster centroid (i.e. whether it is more similar to a different centroid)." ></td>
	<td class="line x" title="110:212	If this is the case, (another instance of) the noun is assigned to the cluster, and we repeat the second step." ></td>
	<td class="line x" title="111:212	If there is no reassignment, we continue with the next word." ></td>
	<td class="line x" title="112:212	The target element is removed from the centroid to make sure that we only subtract the dimensions associated with the sense of the cluster." ></td>
	<td class="line x" title="113:212	Note that K-means requires to set the number of clusters beforehand, so k is a parameter to be set." ></td>
	<td class="line x" title="114:212	3.4.2 CBC The second clustering algorithm operates in a similar vein, but instead of using simple K-means, we use Pantel and Lins CBC algorithm to find the initial centroids (coined COMMITTEES)." ></td>
	<td class="line x" title="115:212	In order to find committees, the top k nouns for each noun in the database are clustered with average-link clustering." ></td>
	<td class="line x" title="116:212	The clusters are scored and sorted in such a way that preference is given to tight, representative clusters." ></td>
	<td class="line x" title="117:212	If the committees do not cover all elements sufficiently, the algorithm recursively tries to find more committees." ></td>
	<td class="line x" title="118:212	An elaborate description of the algorithm can be found in (Pantel and Lin, 2002)." ></td>
	<td class="line x" title="119:212	In the second step, we start assigning elements to committees." ></td>
	<td class="line x" title="120:212	Once an element is assigned, the salient dimensions are subtracted from the noun vector in the same way as in 3.4.1 (only do we not have to remove any target word from the centroid; committees are supposed to represent tight, unambiguous clusters)." ></td>
	<td class="line x" title="121:212	CBC attempts to find the number of committees automatically from the data, so k does not have to be set." ></td>
	<td class="line x" title="122:212	4 Examples 4.1 Sense Subtraction In what follows, we will talk about semantic dimensions as, e.g., the music dimension or the city dimension." ></td>
	<td class="line x" title="123:212	In the vast majority of the cases, the dimensions are indeed as clear-cut as the transport dimension shown above, so that the dimensions can be rightfully labeled this way." ></td>
	<td class="line x" title="124:212	Two examples are given of how the semantic dimensions that have been found can be used for word sense discrimination." ></td>
	<td class="line x" title="125:212	We will consider two ambiguous nouns: pop, which can mean pop music as well as doll, and Barcelona, which can designate either the Spanish city or the Spanish football club." ></td>
	<td class="line x" title="126:212	First, we look up the top dimensions for each noun." ></td>
	<td class="line x" title="127:212	Next, we successively subtract the dimensions dealing with a particular sense of the noun, as described in 3.3." ></td>
	<td class="line x" title="128:212	This gives us three vectors for each noun: the original vector, and two vectors with one of the dimensions eliminated." ></td>
	<td class="line x" title="129:212	For each of these vectors, the top ten similar nouns are given, in order to compare the changes brought about." ></td>
	<td class="line x" title="130:212	(6) a. pop, rock, jazz, meubilair furniture, popmuziek pop music, heks witch, speelgoed toy, kast cupboard, servies [tea] service, vraagteken question mark b. pop, meubilair furniture, speelgoed toy, kast cupboard, servies [tea] service, heks witch, vraagteken question mark sieraad jewel, sculptuur sculpture, schoen shoe c. pop, rock, jazz, popmuziek pop music, heks witch, danseres dancer, servies [tea] service, kopje cup, house house music, aap monkey Example (6) shows the top similar words for the three vectors of pop." ></td>
	<td class="line x" title="131:212	In (a), the most similar words to the original vector are shown." ></td>
	<td class="line x" title="132:212	In (b), the top dimension (the music dimension) has been subtracted from (a), and in (c), the second highest dimension (a domestic items dimension) has been subtracted from (a)." ></td>
	<td class="line x" title="133:212	The differences between the three vectors are clear: in vector (a), both senses are mixed together, with pop music and doll items interleaved." ></td>
	<td class="line x" title="134:212	In (b), no more music items are present." ></td>
	<td class="line x" title="135:212	Only items related to the doll sense are among the top similar words." ></td>
	<td class="line x" title="136:212	In (c), the music sense emerges much more clearly, with rock, jazz and popmuziek being the most similar, and a new music term (house) showing up among the top ten." ></td>
	<td class="line x" title="137:212	Admittedly, in vector (c), not all items related to the doll sense are filtered out." ></td>
	<td class="line x" title="138:212	We believe this is due to the fact that this sense cannot be adequately filtered out by one dimension (in this case, a dimension of domestic items alone), whereas it is much easier to filter out the music sense with only one music dimension." ></td>
	<td class="line x" title="139:212	We will try to remedy this in our clustering framework, in which it is possible to subtract multiple dimensions related to one sense." ></td>
	<td class="line x" title="140:212	A second example, the ambiguous proper name Barcelona, is given in (7)." ></td>
	<td class="line x" title="141:212	(7) a. Barcelona, Arsenal, Inter, Juventus, Vitesse, Milaan Milan, Madrid, Parijs Paris, Wenen Vienna, Munchen Munich b. Barcelona, Milaan Milan, Munchen Mu933 nich, Wenen Vienna, Madrid, Parijs Paris, Bonn, Praag Prague, Berlijn Berlin, Londen London c. Barcelona, Arsenal, Inter, Juventus, Vitesse, Parma, Anderlecht, PSV, Feyenoord, Ajax In (a), the two senses of Barcelona are clearly mixed up, showing cities as well as football clubs among the most similar nouns." ></td>
	<td class="line x" title="142:212	In (b), where the football dimension has been subtracted, only cities show up." ></td>
	<td class="line x" title="143:212	In (c), where the city dimension has been subtracted, only football clubs remain." ></td>
	<td class="line x" title="144:212	4.2 Clustering Output In (8), an example of our clustering algorithm with initial K-means clusters is given." ></td>
	<td class="line x" title="145:212	(8) a. werk work beeld image foto photo schilderij painting tekening drawing doek canvas installatie installation afbeelding picture sculptuur sculpture prent picture illustratie illustration handschrift manuscript grafiek print aquarel aquarelle maquette scale-model collage collage ets etching b. werk work boek book titel title roman novel boekje booklet debuut debut biografie biography bundel collection toneelstuk play bestseller bestseller kinderboek child book autobiografie autobiography novelle short story c. werk work voorziening service arbeid labour opvoeding education kinderopvang child care scholing education huisvesting housing faciliteit facility accommodatie acommodation arbeidsomstandigheid working condition The example shows three different clusters to which the noun werk work is assigned." ></td>
	<td class="line x" title="146:212	In (a), werk refers to a work of art." ></td>
	<td class="line x" title="147:212	In (b), it refers to a written work." ></td>
	<td class="line x" title="148:212	In (c), the labour sense of werk emerges." ></td>
	<td class="line x" title="149:212	5 Evaluation 5.1 Methodology The clustering results are evaluated according to Dutch EuroWordNet (Vossen and others, 1999)." ></td>
	<td class="line x" title="150:212	Precision and recall are calculated by comparing the results to EuroWordNet synsets." ></td>
	<td class="line x" title="151:212	The precision is the number of clusters found that correspond to an actual sense of the word." ></td>
	<td class="line x" title="152:212	Recall is the number of word senses in EuroWordNet that are found by the algorithm." ></td>
	<td class="line x" title="153:212	Our evaluation method is largely the same as the one used by Pantel and Lin (2002)." ></td>
	<td class="line x" title="154:212	Both precision and recall are based on wordnet similarity." ></td>
	<td class="line x" title="155:212	A number of similarity measures have been developed to calculate semantic similarity in a hierarchical wordnet." ></td>
	<td class="line x" title="156:212	Among these measures, the most important are Wu & Palmers (Wu and Palmer, 1994), Resniks (Resnik, 1995) and Lins (Lin, 1998)." ></td>
	<td class="line x" title="157:212	In this evaluation, Wu & Palmers (1994) measure will be adopted." ></td>
	<td class="line x" title="158:212	The similarity is calculated according to the formula in (5), in which N1 and N2 are the number of is-a links from A and B to their most specific common superclass C; N3 is the number of is-a links from C to the root of the taxonomy." ></td>
	<td class="line x" title="159:212	simWu&Palmer(A,B) = 2N3N 1 +N2 + 2N3 (5) iets object wezen organisme dier zoogdier vis hond zalm Figure 2: Extract from the Dutch EuroWordNet Hierarchy For example, the most common superclass of hond dog en zalm salmon is dier animal (as can be seen on the extract from Dutch EuroWordNet in figure 2)." ></td>
	<td class="line x" title="160:212	Consequently, N1 = 2, N2 = 2, N3 = 4 and simWP(hond,zalm) = 0.67." ></td>
	<td class="line x" title="161:212	To calculate precision, we apply the same methodology as Pantel and Lin (2002).2 Let S(w) be the set of EuroWordNet senses." ></td>
	<td class="line x" title="162:212	simW(s,u), the similarity between a synset s and a word u is then defined as the maximum similarity between s and a sense of u: simW(s,u) = max tS(u) sim(s,t) (6) Let ck be the top k-members of a cluster c, where these are the k most similar members to the centroid of c. simC(c,s), the similarity between s and c, is then defined as the average similarity between s and the top-k members of c: simC(s,c) = summationdisplay uck simW(s,u) k (7) 2Note, however, that our similarity measure is different." ></td>
	<td class="line x" title="163:212	Where Pantel and Lin use Lins (1998) measure, we use Wu and Palmers (1994) measure." ></td>
	<td class="line x" title="164:212	934 An assigment of a word w to a cluster c can now be classified as correct if max sS(w) simC(s,c) >  (8) and the EuroWordNet sense of w that corresponds to c is argmax sS(w) simC(s,c) (9) When multiple clusters correspond to the same EuroWordNet sense, only one of them is counted as correct." ></td>
	<td class="line x" title="165:212	Precision of a word w is the percentage of correct clusters to which it is assigned." ></td>
	<td class="line x" title="166:212	Recall of a word w is the percentage of senses from EuroWordnet that have a corresponding cluster.3 Precision and recall of a clustering algorithm is the average precision and recall of all test words." ></td>
	<td class="line x" title="167:212	5.2 Experimental Design We have applied the interleaved NMF presented in section 3.2 to Dutch, using the TWENTE NIEUWS CORPUS (Ordelman, 2002), containing > 500M words of Dutch newspaper text." ></td>
	<td class="line x" title="168:212	The corpus is consistently divided into paragraphs, which have been used as the context window for the bag-of-words mode." ></td>
	<td class="line x" title="169:212	The corpus has been parsed by the Dutch dependency parser Alpino (van Noord, 2006), and dependency triples have been extracted." ></td>
	<td class="line x" title="170:212	Next, the three matrices needed for our method have been constructed: one containing nouns by dependency relations (5K  80K), one containing nouns by context words (5K 2K) and one containing dependency relations by context words (80K2K)." ></td>
	<td class="line x" title="171:212	We did 200 iterations of the algorithm, factorizing the matrices into 50 dimensions." ></td>
	<td class="line x" title="172:212	The NMF algorithm has been implemented in Matlab." ></td>
	<td class="line x" title="173:212	For the evaluation, we use all the words that appear in our original clustering input as well as in EuroWordNet." ></td>
	<td class="line x" title="174:212	This yields a test set of 3683 words." ></td>
	<td class="line x" title="175:212	5.3 Results Table 1 shows precision and recall figures for four different algorithms, according to two similarity thresholds  (equation 8)." ></td>
	<td class="line x" title="176:212	kmeansnmf describes the results of our algorithm with K-means clusters, as described in section 3.4.1." ></td>
	<td class="line x" title="177:212	CBC describes 3Our notion of recall is slightly different from the one used by Pantel and Lin, as they use the number of senses in which w was used in the corpus as gold standard." ></td>
	<td class="line x" title="178:212	This information, as they acknowledge, is difficult to get at, so we prefer to use the sense information in EuroWordNet." ></td>
	<td class="line x" title="179:212	the results of our algorithm with the CBC committees, as described in section 3.4.2." ></td>
	<td class="line x" title="180:212	For comparison, we have also included the results of a standard Kmeans clustering (kmeansorig, k = 600), and the original CBC algorithm (CBCorig) as described by Pantel and Lin (2002)." ></td>
	<td class="line x" title="181:212	threshold  .40 (%) .60 (%) kmeansnmf prec." ></td>
	<td class="line x" title="182:212	78.97 55.16 rec." ></td>
	<td class="line x" title="183:212	63.90 44.77 CBCnmf prec." ></td>
	<td class="line x" title="184:212	82.70 54.87 rec." ></td>
	<td class="line x" title="185:212	60.27 40.51 kmeansorig prec." ></td>
	<td class="line x" title="186:212	86.13 58.97 rec." ></td>
	<td class="line x" title="187:212	60.23 41.80 CBCorig prec." ></td>
	<td class="line x" title="188:212	44.94 29.74 rec." ></td>
	<td class="line x" title="189:212	69.61 48.00 Table 1: Precision and recall for four different algorithms according to two similarity thresholds The results show the same tendency across all similarity thresholds: kmeansnmf has a high precision, but lower recall compared to CBCorig." ></td>
	<td class="line x" title="190:212	Still the recall is higher compared to standard K-means, which indicates that the algorithm is able to find multiple senses of nouns, with high precision." ></td>
	<td class="line x" title="191:212	The results of CBCnmf are similar to the results of kmeansorig, indicating that few words are reassigned to multiple clusters when using CBC committees with our method." ></td>
	<td class="line x" title="192:212	Obviously, kmeansorig scores best with regard to precision, but worse with regard to recall." ></td>
	<td class="line x" title="193:212	CBCorig finds most senses (highest recall), but precision is considerably worse." ></td>
	<td class="line x" title="194:212	The fact that recall is already quite high with standard K-means clustering indicates that the evaluation is skewed towards nouns with only one sense, possibly due to a lack of coverage in EuroWordNet." ></td>
	<td class="line x" title="195:212	In future work, we specifically want to evaluate the discrimination of ambiguous words." ></td>
	<td class="line x" title="196:212	Also, we want to make use of the new Cornetto Database4, a successor of EuroWordNet for Dutch which is currently under development." ></td>
	<td class="line x" title="197:212	Still, the evaluation shows that our method provides a genuine way of finding multiple senses of words, while retaining high precision." ></td>
	<td class="line x" title="198:212	Especially the method using a simple K-means clustering per4http://www.let.vu.nl/onderzoek/ projectsites/cornetto/index.html 935 forms particularly well." ></td>
	<td class="line x" title="199:212	The three way data allows the algorithm to put its finger on the particular sense of a centroid, and adapt the feature vector of a possibly ambiguous noun accordingly." ></td>
	<td class="line x" title="200:212	6 Conclusion & Future Work In this paper, an extension of NMF has been presented that combines both bag of words data and syntactic data in order to find latent semantic dimensions according to which both words and syntactic relations can be classified." ></td>
	<td class="line x" title="201:212	The use of three way data allows one to determine which dimension(s) are responsible for a certain sense of a word, and adapt the corresponding feature vector accordingly, subtracting one sense to discover another one." ></td>
	<td class="line x" title="202:212	When embedded in a clustering framework, the method provides a fully automatic way to discriminate the various senses of words." ></td>
	<td class="line x" title="203:212	The evaluation against EuroWordNet shows that the algorithm is genuinely able to disambiguate the features of a given word, and accordingly its word senses." ></td>
	<td class="line x" title="204:212	We conclude with some issues for future work." ></td>
	<td class="line x" title="205:212	First of all, we would like to test the method that has been explored in this paper with other evaluation frameworks." ></td>
	<td class="line x" title="206:212	We already mentioned the focus on ambiguous nouns, and the use of the new Cornetto database for Dutch." ></td>
	<td class="line x" title="207:212	Next, we would like to work out a proper probabilistic framework for the subtraction of dimensions." ></td>
	<td class="line x" title="208:212	At this moment, the subtraction (using a cut-off) is somewhat ad hoc." ></td>
	<td class="line x" title="209:212	A probabilistic modeling of this intuition might lead to an improvement." ></td>
	<td class="line x" title="210:212	And finally, we would like to use the results of our method to learn selectional preferences." ></td>
	<td class="line x" title="211:212	Our method is able to discriminate the syntactic features that are linked to a particular word sense." ></td>
	<td class="line x" title="212:212	If we can use the results to improve a parsers performance, this will also provide an external evaluation of the algorithm." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1007
Discriminative Learning of Selectional Preference from Unlabeled Text
Bergsma, Shane;Lin, Dekang;Goebel, Randy;"></td>
	<td class="line x" title="1:260	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 5968, Honolulu, October 2008." ></td>
	<td class="line x" title="2:260	c2008 Association for Computational Linguistics Discriminative Learning of Selectional Preference from Unlabeled Text Shane Bergsma Department of Computing Science University of Alberta Edmonton, Alberta Canada, T6G 2E8 bergsma@cs.ualberta.ca Dekang Lin Google, Inc. 1600 Amphitheatre Parkway Mountain View California, 94301 lindek@google.com Randy Goebel Department of Computing Science University of Alberta Edmonton, Alberta Canada, T6G 2E8 goebel@cs.ualberta.ca Abstract We present a discriminative method for learning selectional preferences from unlabeled text." ></td>
	<td class="line x" title="3:260	Positive examples are taken from observed predicate-argument pairs, while negatives are constructed from unobserved combinations." ></td>
	<td class="line x" title="4:260	We train a Support Vector Machine classifier to distinguish the positive from the negative instances." ></td>
	<td class="line x" title="5:260	We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances." ></td>
	<td class="line x" title="6:260	The model outperforms other recent approaches, achieving excellent correlation with human plausibility judgments." ></td>
	<td class="line x" title="7:260	Compared to Mutual Information, it identifies 66% more verb-object pairs in unseen text, and resolves 37% more pronouns correctly in a pronoun resolution experiment." ></td>
	<td class="line x" title="8:260	1 Introduction Selectional preferences (SPs) tell us which arguments are plausible for a particular predicate." ></td>
	<td class="line x" title="9:260	For example, Table 2 (Section 4.4) lists plausible and implausible direct objects (arguments) for particular verbs (predicates)." ></td>
	<td class="line x" title="10:260	SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community." ></td>
	<td class="line x" title="11:260	One way to determine SPs is from co-occurrences of predicates and arguments in text." ></td>
	<td class="line x" title="12:260	Unfortunately, no matter how much text we use, many acceptable pairs will be missing." ></td>
	<td class="line x" title="13:260	Bikel (2004) found that only 1.49% of the bilexical dependencies considered by Collins parser during decoding were observed during training." ></td>
	<td class="line x" title="14:260	In our parsed corpus (Section 4.1), for example, we find eat with nachos, burritos, and tacos, but not with the equally tasty quesadillas, chimichangas, or tostadas." ></td>
	<td class="line x" title="15:260	Rather than solely relying on co-occurrence counts, we would like to use them to generalize to unseen pairs." ></td>
	<td class="line x" title="16:260	In particular, we would like to exploit a number of arbitrary and potentially overlapping properties of predicates and arguments when we assign SPs." ></td>
	<td class="line x" title="17:260	We do this by representing these properties as features in a linear classifier, and training the weights using discriminative learning." ></td>
	<td class="line x" title="18:260	Positive examples are taken from observed predicate-argument pairs, while pseudo-negatives are constructed from unobserved combinations." ></td>
	<td class="line x" title="19:260	We train a Support Vector Machine (SVM) classifier to distinguish the positives from the negatives." ></td>
	<td class="line x" title="20:260	We refer to our models scores as Discriminative Selectional Preference (DSP)." ></td>
	<td class="line x" title="21:260	By creating training vectors automatically, DSP enjoys all the advantages of supervised learning, but without the need for manual annotation of examples." ></td>
	<td class="line x" title="22:260	We evaluate DSP on the task of assigning verbobject selectional preference." ></td>
	<td class="line x" title="23:260	We encode a nouns textual distribution as feature information." ></td>
	<td class="line x" title="24:260	The learned feature weights are linguistically interesting, yielding high-quality similar-word lists as latent information." ></td>
	<td class="line x" title="25:260	Despite its representational power, DSP scales to real-world data sizes: examples are partitioned by predicate, and a separate SVM is trained for each partition." ></td>
	<td class="line x" title="26:260	This allows us to efficiently learn with over 57 thousand features and 6.5 million examples." ></td>
	<td class="line x" title="27:260	DSP outperforms recently proposed alternatives in a range of experiments, and better correlates with human plausibility judgments." ></td>
	<td class="line x" title="28:260	It also shows strong gains over a Mutual Information-based co59 occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data." ></td>
	<td class="line x" title="29:260	2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996)." ></td>
	<td class="line x" title="30:260	For example, we might have a class Mexican Food and learn that the entire class is suitable for eating." ></td>
	<td class="line x" title="31:260	Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999)." ></td>
	<td class="line x" title="32:260	Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models." ></td>
	<td class="line x" title="33:260	Another line of research generalizes using similar words." ></td>
	<td class="line x" title="34:260	Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text." ></td>
	<td class="line x" title="35:260	Dagan et al.(1999) define the similarity-weighted probability, PrSIM, to be: PrSIM(n|v) = summationdisplay vSIMS(v) Sim(v,v)Pr(n|v) (1) where Sim(v,v) returns a real-valued similarity between two verbs v and v (normalized over all pair similarities in the sum)." ></td>
	<td class="line x" title="37:260	In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al.(2005) use the cross-product of similar pairs." ></td>
	<td class="line x" title="39:260	One key issue is how to define the set of similar words, SIMS(w)." ></td>
	<td class="line x" title="40:260	Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)s information-theoretic metric work best." ></td>
	<td class="line x" title="41:260	Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet." ></td>
	<td class="line x" title="42:260	Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007)." ></td>
	<td class="line x" title="43:260	Inferences such as [X wins Y]  [X plays Y] are only valid for certain arguments X and Y. We follow Pantel et al.(2007) in using automatically-extracted semantic classes to help characterize plausible arguments." ></td>
	<td class="line x" title="45:260	Discriminative techniques are widely used in NLP and have been applied to the related tasks of word prediction and language modeling." ></td>
	<td class="line x" title="46:260	Even-Zohar and Roth (2000) use a classifier to predict the most likely word to fill a position in a sentence (in their experiments: a verb) from a set of candidates (sets of verbs), by inspecting the context of the target token (e.g., the presence or absence of a particular nearby word in the sentence)." ></td>
	<td class="line x" title="47:260	This approach can therefore learn which specific arguments occur with a particular predicate." ></td>
	<td class="line x" title="48:260	In comparison, our features are second-order: we learn what kinds of arguments occur with a predicate by encoding features of the arguments." ></td>
	<td class="line x" title="49:260	Recent distributed and latentvariable models also represent words with feature vectors (Bengio et al., 2003; Blitzer et al., 2005)." ></td>
	<td class="line x" title="50:260	Many of these approaches learn both the feature weights and the feature representation." ></td>
	<td class="line x" title="51:260	Vectors must be kept low-dimensional for tractability, while learning and inference on larger scales is impractical." ></td>
	<td class="line x" title="52:260	By partitioning our examples by predicate, we can efficiently use high-dimensional, sparse vectors." ></td>
	<td class="line x" title="53:260	Our technique of generating negative examples is similar to the approach of Okanohara and Tsujii (2007)." ></td>
	<td class="line x" title="54:260	They learn a classifier to disambiguate actual sentences from pseudo-negative examples sampled from an N-gram language model." ></td>
	<td class="line x" title="55:260	Smith and Eisner (2005) also automatically generate negative examples." ></td>
	<td class="line x" title="56:260	They perturb their input sequence (e.g. the sentence word order) to create a neighborhood of implicit negative evidence." ></td>
	<td class="line x" title="57:260	We create negatives by substitution rather than perturbation, and use corpuswide statistics to choose our negative instances." ></td>
	<td class="line x" title="58:260	3 Methodology 3.1 Creating Examples To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text." ></td>
	<td class="line x" title="59:260	To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data." ></td>
	<td class="line oc" title="60:260	We measure this association using pointwise Mutual Information (MI) (Church and Hanks, 1990)." ></td>
	<td class="line o" title="61:260	The MI between a 60 verb predicate, v, and its object argument, n, is: MI(v,n) = log Pr(v,n)Pr(v)Pr(n) = log Pr(n|v)Pr(n) (2) If MI>0, the probability v and n occur together is greater than if they were independently distributed." ></td>
	<td class="line o" title="62:260	We create sets of positive and negative examples separately for each predicate, v. First, we extract all pairs where MI(v,n)> as positives." ></td>
	<td class="line o" title="63:260	For each positive, we create pseudo-negative examples, (v,n), by pairing v with a new argument, n, that either has MI below the threshold or did not occur with v in the corpus." ></td>
	<td class="line x" title="64:260	We require each negative n to have a similar frequency to its corresponding n. This prevents our learning algorithm from focusing on any accidental frequency-based bias." ></td>
	<td class="line x" title="65:260	We mix in K negatives for each positive, sampling without replacement to create all the negatives for a particular predicate." ></td>
	<td class="line x" title="66:260	For each v, 1K+1 of its examples will be positive." ></td>
	<td class="line x" title="67:260	The threshold  represents a trade-off between capturing a large number of positive pairs and ensuring these pairs have good association." ></td>
	<td class="line x" title="68:260	Similarly, K is a tradeoff between the number of examples and the computational efficiency." ></td>
	<td class="line x" title="69:260	Ultimately, these parameters should be optimized for task performance." ></td>
	<td class="line x" title="70:260	Of course, some negatives will actually be plausible arguments that were unobserved due to sparseness." ></td>
	<td class="line x" title="71:260	Fortunately, modern discriminative methods like soft-margin SVMs can learn in the face of label error by allowing slack, subject to a tunable regularization penalty (Cortes and Vapnik, 1995)." ></td>
	<td class="line o" title="72:260	If MI is a sparse and imperfect model of SP, what can DSP gain by training on MIs scores?" ></td>
	<td class="line o" title="73:260	We can regard DSP as learning a view of SP that is orthogonal to MI, in a co-training sense (Blum and Mitchell, 1998)." ></td>
	<td class="line o" title="74:260	MI labels the data based solely on co-occurrence; DSP uses these labels to identify other regularities  ones that extend beyond cooccurring words." ></td>
	<td class="line o" title="75:260	For example, many instances of n where MI(eat,n)> also have MI(buy,n)> and MI(cook,n)>." ></td>
	<td class="line x" title="76:260	Also, compared to other nouns, a disproportionate number of eat-nouns are lowercase, single-token words, and they rarely contain digits, hyphens, or begin with a human first name like Bob." ></td>
	<td class="line x" title="77:260	DSP encodes these interdependent properties as features in a linear classifier." ></td>
	<td class="line o" title="78:260	This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only assign high plausibility to observed (eat,n) pairs." ></td>
	<td class="line x" title="79:260	Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained stringand token-based features." ></td>
	<td class="line x" title="80:260	Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999)." ></td>
	<td class="line x" title="81:260	This data consists of triples (v,n,n) where v,n is a predicateargument pair observed in the corpus and v,n has not been observed." ></td>
	<td class="line x" title="82:260	The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones." ></td>
	<td class="line x" title="83:260	We refer to this as Pairwise Disambiguation." ></td>
	<td class="line x" title="84:260	Unlike this task, we classify each predicate-argument pair independently as plausible/implausible." ></td>
	<td class="line o" title="85:260	We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise.1 3.2 Partitioning for Efficient Training After creating our positive and negative training pairs, we must select a feature representation for our examples." ></td>
	<td class="line x" title="86:260	Let  be a mapping from a predicateargument pair (v,n) to a feature vector,  : (v,n)  1k." ></td>
	<td class="line x" title="87:260	Predictions are made based on a weighted combination of the features, y = (v,n), where  is our learned weight vector." ></td>
	<td class="line x" title="88:260	We can make training significantly more efficient by using a special form of attribute-value features." ></td>
	<td class="line x" title="89:260	Let every feature i be of the form i(v,n) = v = vf(n)." ></td>
	<td class="line x" title="90:260	That is, every feature is an intersection of the occurrence of a particular predicate, v, and some feature of the argument f(n)." ></td>
	<td class="line x" title="91:260	For example, a feature for a verb-object pair might be, the verb is eat and the object is lower-case. In this representation, features for one predicate will be completely independent from those for every other predicate." ></td>
	<td class="line o" title="92:260	Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a 1For a fixed verb, MI is proportional to Keller and Lapata (2003)s conditional probability scores for pseudodisambiguation of (v,n,n) triples: Pr(v|n) = Pr(v,n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f(v,n)." ></td>
	<td class="line o" title="93:260	Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs." ></td>
	<td class="line o" title="94:260	MI was also recently used for inference-rule SPs by Pantel et al.(2007)." ></td>
	<td class="line x" title="96:260	61 classifier for each predicate independently." ></td>
	<td class="line x" title="97:260	The prediction becomes yv = v v(n), where v are the learned weights corresponding to predicate v and all features v(n)=f(n) depend on the argument only." ></td>
	<td class="line x" title="98:260	Some predicate partitions may have insufficient examples for training." ></td>
	<td class="line x" title="99:260	Also, a predicate may occur in test data that was unseen during training." ></td>
	<td class="line x" title="100:260	To handle these instances, we decided to cluster lowfrequency predicates." ></td>
	<td class="line x" title="101:260	In our experiments assigning SP to verb-object pairs, we cluster all verbs that have less than 250 positive examples, using clusters generated by the CBC algorithm (Pantel and Lin, 2002)." ></td>
	<td class="line x" title="102:260	For example, the low-frequency verbs incarcerate, parole, and court-martial are all mapped to the same partition, while more-frequent verbs like arrest and execute each have their own partition." ></td>
	<td class="line x" title="103:260	About 5.5% of examples are clustered, corresponding to 30% of the 7367 total verbs." ></td>
	<td class="line x" title="104:260	40% of verbs (but only 0.6% of examples) were not in any CBC cluster; these were mapped to a single backoff partition." ></td>
	<td class="line x" title="105:260	The parameters for each partition, v, can be trained with any supervised learning technique." ></td>
	<td class="line x" title="106:260	We use SVM (Section 4.1) because it is effective in similar high-dimensional, sparse-vector settings, and has an efficient implementation (Joachims, 1999)." ></td>
	<td class="line x" title="107:260	In SVM, the sign of yv gives the classification." ></td>
	<td class="line x" title="108:260	We can also use the scalar yv as our DSP score (i.e. the positive distance from the separating SVM hyperplane)." ></td>
	<td class="line x" title="109:260	3.3 Features This section details our argument features, f(n), for assigning verb-object selectional preference." ></td>
	<td class="line x" title="110:260	For a verb predicate (or partition) v and object argument n, the form of our classifier is yv =summationtexti vifi(n)." ></td>
	<td class="line x" title="111:260	3.3.1 Verb co-occurrence We provide features for the empirical probability of the noun occurring as the object argument of other verbs, Pr(n|v)." ></td>
	<td class="line x" title="112:260	If we were to only use these features (indexing the feature weights by each verb v), the form of our classifier would be: yv = summationdisplay v vvPr(n|v) (3) Note the similarity between Equation (3) and Equation (1)." ></td>
	<td class="line x" title="113:260	Now the feature weights, vv, take the role of the similarity function, Sim(v,v)." ></td>
	<td class="line x" title="114:260	Unlike Equation (1), however, these weights are not set by an external similarity algorithm, but are optimized to discriminate the positive and negative training examples." ></td>
	<td class="line x" title="115:260	We need not restrict ourselves to a short list of similar verbs; we include Probj(n|v) features for every verb that occurs more than 10 times in our corpus." ></td>
	<td class="line x" title="116:260	vv may be positive or negative, depending on the relation between v and v. We also include features for the probability of the noun occurring as the subject of other verbs, Prsubj(n|v)." ></td>
	<td class="line x" title="117:260	For example, nouns that can be the object of eat will also occur as the subject of taste and contain." ></td>
	<td class="line x" title="118:260	Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated." ></td>
	<td class="line x" title="119:260	The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir (2005)." ></td>
	<td class="line x" title="120:260	They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation." ></td>
	<td class="line x" title="121:260	Our approach, on the other hand, discriminatively sets millions of individual similarity values." ></td>
	<td class="line x" title="122:260	Like Weeds and Weir (2005), our similarity values are asymmetric." ></td>
	<td class="line x" title="123:260	3.3.2 String-based We include several simple character-based features of the noun string: the number of tokens, the case, and whether it contains digits, hyphens, an apostrophe, or other punctuation." ></td>
	<td class="line x" title="124:260	We also include a feature for the first and last token, and fire indicator features if any token in the noun occurs on in-house lists of given names, family names, cities, provinces, countries, corporations, languages, etc. We also fire a feature if a token is a corporate designation (like inc. or ltd.) or a human one (like Mr. or Sheik)." ></td>
	<td class="line x" title="125:260	3.3.3 Semantic classes Motivated by previous SP models that make use of semantic classes, we generated word clusters using CBC (Pantel and Lin, 2002) on a 10 GB corpus, giving 3620 clusters." ></td>
	<td class="line x" title="126:260	If a noun belongs in a cluster, a corresponding feature fires." ></td>
	<td class="line x" title="127:260	If a noun is in none of the clusters, a no-class feature fires." ></td>
	<td class="line x" title="128:260	As an example, CBC cluster 1891 contains: sidewalk, driveway, roadway, footpath, bridge, highway, road, runway, street, alley, path, Interstate, . . ." ></td>
	<td class="line x" title="129:260	In our training data, we have examples like widen highway, widen road and widen motorway." ></td>
	<td class="line x" title="130:260	If we 62 see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al.(2003).2 This data provides counts for pairs such as Edwin Moses, hurdler and William Farley, industrialist. We have features for all concepts and therefore learn their association with each verb." ></td>
	<td class="line o" title="132:260	4 Experiments and Results 4.1 Set up We parsed the 3 GB AQUAINT corpus (Voorhees, 2002) using Minipar (Lin, 1998b), and collected verb-object and verb-subject frequencies, building an empirical MI model from this data." ></td>
	<td class="line x" title="133:260	Verbs and nouns were converted to their (possibly multi-token) root, and string case was preserved." ></td>
	<td class="line x" title="134:260	Passive subjects (the car was bought) were converted to objects (bought car)." ></td>
	<td class="line o" title="135:260	We set the MI-threshold, , to be 0, and the negative-to-positive ratio, K, to be 2." ></td>
	<td class="line x" title="136:260	Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999)." ></td>
	<td class="line x" title="137:260	Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness." ></td>
	<td class="line x" title="138:260	We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1))." ></td>
	<td class="line x" title="139:260	We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise." ></td>
	<td class="line x" title="140:260	Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature." ></td>
	<td class="line x" title="141:260	When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns." ></td>
	<td class="line x" title="142:260	This processing results in a set of 6.8 million pairs, divided into 2318 partitions (192 of which are verb clusters (Section 3.2))." ></td>
	<td class="line x" title="143:260	For each partition, we take 95% of the examples for training, 2.5% for development and 2.5% for a final unseen test set." ></td>
	<td class="line x" title="144:260	We provide full results for two models: DSPcooc which only uses the verb co-occurrence features, and DSPall which uses all the features men2Available at http://www.mit.edu/mbf/instances.txt.gz tioned in Section 3.3." ></td>
	<td class="line x" title="145:260	Feature values are normalized within each feature type." ></td>
	<td class="line x" title="146:260	We train our (linear kernel) discriminative models using SVMlight (Joachims, 1999) on each partition, but set meta-parameters C (regularization) and j (cost of positive vs. negative misclassifications: max at j=2) on the macroaveraged score across all development partitions." ></td>
	<td class="line x" title="147:260	Note that we can not use the development set to optimize  and K because the development examples are obtained after setting these values." ></td>
	<td class="line x" title="148:260	4.2 Feature weights It is interesting to inspect the feature weights returned by our system." ></td>
	<td class="line x" title="149:260	In particular, the weights on the verb co-occurrence features (Section 3.3.1) provide a high-quality, argument-specific similarityranking of other verb contexts." ></td>
	<td class="line x" title="150:260	The DSP parameters for eat, for example, place high weight on features like Pr(n|braise), Pr(n|ration), and Pr(n|garnish)." ></td>
	<td class="line x" title="151:260	Lin (1998a)s similar word list for eat misses these but includes sleep (ranked 6) and sit (ranked 14), because these have similar subjects to eat." ></td>
	<td class="line x" title="152:260	Discriminative, context-specific training seems to yield a better set of similar predicates, e.g. the highest-ranked contexts for DSPcooc on the verb join,3 lead 1.42, rejoin 1.39, form 1.34, belong to 1.31, found 1.31, quit 1.29, guide 1.19, induct 1.19, launch (subj) 1.18, work at 1.14 give a better SIMS(join) for Equation (1) than the top similarities returned by (Lin, 1998a): participate 0.164, lead 0.150, return to 0.148, say 0.143, rejoin 0.142, sign 0.142, meet 0.142, include 0.141, leave 0.140, work 0.137 Other features are also weighted intuitively." ></td>
	<td class="line x" title="153:260	Note that case is a strong indicator for some arguments, for example the weight on being lower-case is high for become (0.972) and eat (0.505), but highly negative for accuse (-0.675) and embroil (-0.573) which often take names of people and organizations." ></td>
	<td class="line x" title="154:260	4.3 Pseudodisambiguation We first evaluate DSP on disambiguating positives from pseudo-negatives, comparing to recently3Which all correspond to nouns occurring in the object position of the verb (e.g. Probj(n|lead)), except launch (subj) which corresponds to Prsubj(n|launch)." ></td>
	<td class="line x" title="155:260	63 System MacroAvg MicroAvg PairwiseP R F P R F Acc Cov Dagan et al.(1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98 Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83 Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57 DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00 DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00 Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (MicroAvg), plus coverage and accuracy of pairwise competition (Pairwise)." ></td>
	<td class="line x" title="157:260	proposed systems that also require no manuallycompiled resources like WordNet." ></td>
	<td class="line o" title="158:260	We convert Dagan et al.(1999)s similarity-smoothed probability to MI by replacing the empirical Pr(n|v) in Equation (2) with the smoothed PrSIM from Equation (1)." ></td>
	<td class="line o" title="160:260	We also test an MI model inspired by Erk (2007): MISIM(n,v) = log summationdisplay nSIMS(n) Sim(n,n) Pr(v,n ) Pr(v)Pr(n) We gather similar words using Lin (1998a), mining similar verbs from a comparable-sized parsed corpus, and collecting similar nouns from a broader 10 GB corpus of English text.4 We also use Keller and Lapata (2003)s approach to obtaining web-counts." ></td>
	<td class="line x" title="161:260	Rather than mining parse trees, this technique retrieves counts for the pattern V Det N in raw online text, where V is any inflection of the verb, Det is the, a, or the empty string, and N is the singular or plural form of the noun." ></td>
	<td class="line x" title="162:260	We compute a web-based MI by collecting Pr(n,v), Pr(n), and Pr(v) using all inflections, except we only use the root form of the noun." ></td>
	<td class="line x" title="163:260	Rather than using a search engine, we obtain counts from the Google Web 5-gram Corpus.5 All systems are thresholded at zero to make a classification." ></td>
	<td class="line x" title="164:260	Unlike DSP, the comparison systems may 4For both the similar-noun and similar-verb smoothing, we only smooth over similar pairs that occurred in the corpus." ></td>
	<td class="line x" title="165:260	While averaging over all similar pairs tends to underestimate the probability, averaging over only the observed pairs tends to overestimate it." ></td>
	<td class="line x" title="166:260	We tested both and adopt the latter because it resulted in better performance on our development set." ></td>
	<td class="line x" title="167:260	5Available from the LDC as LDC2006T13." ></td>
	<td class="line x" title="168:260	This collection was generated from approximately 1 trillion tokens of online text." ></td>
	<td class="line x" title="169:260	Unfortunately, tokens appearing less than 200 times have been mapped to the UNK symbol, and only N-grams appearing more than 40 times are included." ></td>
	<td class="line x" title="170:260	Unlike results from search engines, however, experiments with this corpus are replicable." ></td>
	<td class="line x" title="171:260	not be able to provide a score for each example." ></td>
	<td class="line x" title="172:260	The similarity-smoothed examples will be undefined if SIMS(w) is empty." ></td>
	<td class="line x" title="173:260	Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web." ></td>
	<td class="line x" title="174:260	As a reasonable default for these cases, we assign them a negative decision." ></td>
	<td class="line x" title="175:260	We evaluate disambiguation using precision (P), recall (R), and their harmonic mean, F-Score (F)." ></td>
	<td class="line x" title="176:260	Table 1 gives the results of our comparison." ></td>
	<td class="line x" title="177:260	In the MacroAvg results, we weight each example equally." ></td>
	<td class="line x" title="178:260	For MicroAvg, we weight each example by the frequency of the noun." ></td>
	<td class="line x" title="179:260	To more directly compare with previous work, we also reproduced Pairwise Disambiguation by randomly pairing each positive with one of the negatives and then evaluating each system by the percentage it ranks correctly (Acc)." ></td>
	<td class="line x" title="180:260	For the comparison approaches, if one score is undefined, we choose the other one." ></td>
	<td class="line x" title="181:260	If both are undefined, we abstain from a decision." ></td>
	<td class="line x" title="182:260	Coverage (Cov) is the percent of pairs where a decision was made.6 Our simple system with only verb co-occurrence features, DSPcooc, outperforms all comparison approaches." ></td>
	<td class="line x" title="183:260	Using the richer feature set in DSPall results in a statistically significant gain in performance, up to an F-Score of 0.65 and a pairwise disambiguation accuracy of 0.81.7 DSPall has both broader coverage and better accuracy than all competing approaches." ></td>
	<td class="line x" title="184:260	In the remainder of the experiments, we use DSPall and refer to it simply as DSP." ></td>
	<td class="line x" title="185:260	Some errors are because of plausible but unseen arguments being used as test-set pseudo-negatives." ></td>
	<td class="line x" title="186:260	For example, for the verb damage, DSPs three most high-scoring false positives are the nouns jetliner, carpet, and gear." ></td>
	<td class="line x" title="187:260	While none occur with damage in 6I.e. we use the half coverage condition from Erk (2007)." ></td>
	<td class="line x" title="188:260	7The differences between DSPall and all comparison systems are statistically significant (McNemars test, p<0.01)." ></td>
	<td class="line x" title="189:260	64  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  10  100  1000  10000  100000  1e+06 F-Score Noun Frequency DSPall Erk (2007) Keller and Lapata (2003) Figure 1: Disambiguation results by noun frequency." ></td>
	<td class="line x" title="190:260	our corpus, all intuitively satisfy the verbs SPs." ></td>
	<td class="line x" title="191:260	MacroAvg performance is worse than MicroAvg because all systems perform better on frequent nouns." ></td>
	<td class="line x" title="192:260	When we plot F-Score by noun frequency (Figure 1), we see that DSP outperforms comparison approaches across all frequencies, but achieves its biggest gains on the low-frequency nouns." ></td>
	<td class="line x" title="193:260	A richer feature set allows DSP to make correct inferences on examples that provide minimal co-occurrence data." ></td>
	<td class="line o" title="194:260	These are also the examples for which we would expect co-occurrence models like MI to fail." ></td>
	<td class="line x" title="195:260	As a further experiment, we re-trained DSP but with only the string-based features removed." ></td>
	<td class="line x" title="196:260	Overall macro-averaged F-score dropped from 0.65 to 0.64 (a statistically significant reduction in performance)." ></td>
	<td class="line x" title="197:260	The system scored nearly identically to DSP on the high-frequency nouns, but performed roughly 15% worse on the nouns that occurred less than ten times." ></td>
	<td class="line x" title="198:260	This shows that the string-based features are important for selectional preference, and particularly helpful for low-frequency nouns." ></td>
	<td class="line x" title="199:260	4.4 Human Plausibility Table 2 compares some of our systems on data used by Resnik (1996) (also Appendix 2 in Holmes et al.(1989))." ></td>
	<td class="line x" title="201:260	The plausibility of these pairs was initially judged based on the experimenters intuitions, and later confirmed in a human experiment." ></td>
	<td class="line x" title="202:260	We include the scores of Resniks system, and note that its errors were attributed to sense ambiguity and other limitations of class-based approaches (Resnik, 1996).8 8For example, warn-engine scores highly because engines are in the class entity, and physical entities (e.g. people) are often objects of warn." ></td>
	<td class="line o" title="203:260	Unlike DSP, Resniks approach cannot learn that for warn, the property of being a person is more Seen Criteria Unseen Verb-Object Freq.All = 1 = 2 = 3 > 3 MI > 0 0.44 0.33 0.57 0.70 0.82 Freq." ></td>
	<td class="line x" title="204:260	> 0 0.57 0.45 0.76 0.89 0.96 DSP > 0 0.73 0.69 0.80 0.85 0.88 Table 3: Recall on identification of Verb-Object pairs from an unseen corpus (divided by pair frequency)." ></td>
	<td class="line x" title="205:260	The other comparison approaches also make a number of mistakes, which can often be traced to a misguided choice of similar word to smooth with." ></td>
	<td class="line o" title="206:260	We also compare to our empirical MI model, trained on our parsed corpus." ></td>
	<td class="line p" title="207:260	Although Resnik (1996) reported that 10 of the 16 plausible pairs did not occur in his training corpus, all of them occurred in ours and hence MI gives very reasonable scores on the plausible objects." ></td>
	<td class="line n" title="208:260	It has no statistics, however, for many of the implausible ones." ></td>
	<td class="line n" title="209:260	DSP can make finer decisions than MI, recognizing that warning an engine is more absurd than judging a climate. 4.5 Unseen Verb-Object Identification We next compare MI and DSP on a much larger set of plausible examples, and also test how well the models generalize across data sets." ></td>
	<td class="line o" title="210:260	We took the MI and DSP systems trained on AQUAINT and asked them to rate observed (and thus likely plausible) verb-object pairs taken from an unseen corpus." ></td>
	<td class="line x" title="211:260	We extracted the pairs by parsing the San Jose Mercury News (SJM) section of the TIPSTER corpus (Harman, 1992)." ></td>
	<td class="line x" title="212:260	Each unique verb-object pair is a single instance in this evaluation." ></td>
	<td class="line x" title="213:260	Table 3 gives recall across all pairs (All) and grouped by pair-frequency in the unseen corpus (1, 2, 3, >3)." ></td>
	<td class="line n" title="214:260	DSP accepts far more pairs than MI (73% vs. 44%), even far more than a system that accepts any previously observed verb-object combination as plausible (57%)." ></td>
	<td class="line x" title="215:260	Recall is higher on more frequent verb-object pairs, but 70% of the pairs occurred only once in the corpus." ></td>
	<td class="line o" title="216:260	Even if we smooth MI by smoothing Pr(n|v) in Equation 2 using modified KN-smoothing (Chen and Goodman, 1998), the recall of MI>0 on SJM only increases from 44.1% to 44.9%, still far below DSP." ></td>
	<td class="line x" title="217:260	Frequency-based models have fundamentally low coverage." ></td>
	<td class="line x" title="218:260	As furimportant than the property of being an entity (Resnik, 1996)." ></td>
	<td class="line x" title="219:260	65 Verb Plaus./Implaus." ></td>
	<td class="line o" title="220:260	Resnik Dagan et al. Erk MI DSP see friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02 read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/ 2.12/-0.65 find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81 hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67 write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31 urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/ -0.34/-0.62 warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/ 2.00/-0.99 judge contest/climate 1.30/0.28 1.50/1.90* 1.70/1.70* 3.90/ 1.00/0.51 teach language/distance 1.87/1.86 2.50/1.30 3.60/2.70 3.53/ 1.86/0.19 show sample/travel 1.44/0.41 1.60/0.14 0.40/-0.82 0.53/-0.49 1.00/-0.83 expect visit/mouth 0.59/5.93* 1.40/1.50* 1.40/0.37 1.05/-0.65 1.44/-0.15 answer request/tragedy 4.49/3.88 2.70/1.50 3.10/-0.64 2.93/ 1.00/0.01 recognize author/pocket 0.50/0.50* 0.03/0.37* 0.77/1.30* 0.48/ 1.00/0.00 repeat comment/journal 1.23/1.23* 2.30/1.40 2.90/ 2.59/ 1.00/-0.48 understand concept/session 1.52/1.51 2.70/0.25 2.00/-0.28 3.96/ 2.23/-0.46 remember reply/smoke 1.31/0.20 2.10/1.20 0.54/2.60* 1.13/-0.06 1.00/-0.42 Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al., 1989)." ></td>
	<td class="line x" title="221:260	Mistakes are marked with an asterisk (*), undefined scores are marked with a dash ()." ></td>
	<td class="line x" title="222:260	Only DSP is completely defined and completely correct." ></td>
	<td class="line o" title="223:260	0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0  0.2  0.4  0.6  0.8  1 Interpolated Precision Recall DSP>T MI>T DSP>0 MI>0 Figure 2: Pronoun resolution precision-recall on MUC." ></td>
	<td class="line o" title="224:260	ther evidence, if we build a model of MI on the SJM corpus and use it in our pseudodisambiguation experiment (Section 4.3), MI>0 gets a MacroAvg precision of 86% but a MacroAvg recall of only 12%.9 4.6 Pronoun Resolution Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004)." ></td>
	<td class="line x" title="225:260	We study the cases where a 9Recall that even the Keller and Lapata (2003) system, built on the worlds largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5)." ></td>
	<td class="line x" title="226:260	pronoun is the direct object of a verb predicate, v. A pronouns antecedent must obey vs selectional preferences." ></td>
	<td class="line x" title="227:260	If we have a better model of SP, we should be able to better select pronoun antecedents." ></td>
	<td class="line x" title="228:260	We parsed the MUC-7 (1997) coreference corpus and extracted all pronouns in a direct object relation." ></td>
	<td class="line x" title="229:260	For each pronoun, p, modified by a verb, v, we extracted all preceding nouns within the current or previous sentence." ></td>
	<td class="line x" title="230:260	Thirty-nine anaphoric pronouns had an antecedent in this window and are used in the evaluation." ></td>
	<td class="line x" title="231:260	For each p, let N(p)+ by the set of preceding nouns coreferent with p, and let N(p) be the remaining non-coreferent nouns." ></td>
	<td class="line x" title="232:260	We take all (v,n+) where n+  N(p)+ as positive, and all other pairs (v,n), n  N(p) as negative." ></td>
	<td class="line o" title="233:260	We compare MI and DSP on this set, classifying every (v,n) with MI>T (or DSP>T) as positive." ></td>
	<td class="line x" title="234:260	By varying T, we get a precision-recall curve (Figure 2)." ></td>
	<td class="line x" title="235:260	Precision is low because, of course, there are many nouns that satisfy the predicates SPs that are not coreferent." ></td>
	<td class="line p" title="236:260	DSP>0 has both a higher recall and higher precision than accepting every pair previously seen in text (the right-most point on MI>T)." ></td>
	<td class="line n" title="237:260	The DSP>T system achieves higher precision than MI>T for points where recall is greater than 60% (where MI<0)." ></td>
	<td class="line n" title="238:260	Interestingly, the recall of MI>0 is 66 System Acc Most-Recent Noun 17.9% Maximum MI 28.2% Maximum DSP 38.5% Table 4: Pronoun resolution accuracy on nouns in current or previous sentence in MUC." ></td>
	<td class="line x" title="239:260	higher here than it is for general verb-objects (Section 4.5)." ></td>
	<td class="line p" title="240:260	On the subset of pairs with strong empirical association (MI>0), MI generally outperforms DSP at equivalent recall values." ></td>
	<td class="line o" title="241:260	We next compare MI and DSP as stand-alone pronoun resolution systems (Table 4)." ></td>
	<td class="line x" title="242:260	As a standard baseline, for each pronoun, we choose the most recent noun in text as the pronouns antecedent, achieving 17.9% resolution accuracy." ></td>
	<td class="line x" title="243:260	This baseline is quite low because many of the most-recent nouns are subjects of the pronouns verb phrase, and therefore resolution violates syntactic coreference constraints." ></td>
	<td class="line o" title="244:260	If instead we choose the previous noun with the highest MI as antecedent, we get an accuracy of 28.2%, while choosing the previous noun with the highest DSP achieves 38.5%." ></td>
	<td class="line n" title="245:260	DSP resolves 37% more pronouns correctly than MI." ></td>
	<td class="line o" title="246:260	We leave as future work a full-scale pronoun resolution system that incorporates both MI and DSP as backed-off, interpolated, or separate semantic features." ></td>
	<td class="line x" title="247:260	5 Conclusions and Future Work We have presented a simple, effective model of selectional preference based on discriminative training." ></td>
	<td class="line x" title="248:260	Supervised techniques typically achieve higher performance than unsupervised models, and we duplicate these gains with DSP." ></td>
	<td class="line x" title="249:260	Here, however, these gains come at no additional labeling cost, as training examples are generated automatically from unlabeled text." ></td>
	<td class="line x" title="250:260	DSP allows an arbitrary combination of features, including verb co-occurrence features that yield high-quality similar-word lists as latent output." ></td>
	<td class="line x" title="251:260	This work only scratches the surface of possible feature mining; information from WordNet relations, Wikipedia categories, or parallel corpora could also provide valuable clues to SP." ></td>
	<td class="line x" title="252:260	Also, if any other system were to exceed DSPs performance, it could also be included as one of DSPs features." ></td>
	<td class="line x" title="253:260	It would be interesting to expand our cooccurrence features, including co-occurrence counts across more grammatical relations and using counts from external, unparsed corpora like the world wide web." ></td>
	<td class="line x" title="254:260	We could also reverse the role of noun and verb in our training, having verb-specific features and discriminating separately for each argument noun." ></td>
	<td class="line x" title="255:260	The latent information would then be lists of similar nouns." ></td>
	<td class="line x" title="256:260	Finally, note that while we focused on word-word co-occurrences, sense-sense SPs can also be learned with our algorithm." ></td>
	<td class="line x" title="257:260	If our training corpus was senselabeled, we could run our algorithm over the senses rather than the words." ></td>
	<td class="line x" title="258:260	The resulting model would then require sense-tagged input if it were to be used within an application like parsing or coreference resolution." ></td>
	<td class="line x" title="259:260	Also, like other models of SP, our technique can also be used for sense disambiguations: the weightings on our semantic class features indicate, for a particular noun, which of its senses (classes) is most compatible with each verb." ></td>
	<td class="line x" title="260:260	Acknowledgments We gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada, the Alberta Ingenuity Fund, and the Alberta Informatics Circle of Research Excellence." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1044
Scalable Language Processing Algorithms for the Masses: A Case Study in Computing Word Co-occurrence Matrices with MapReduce
Lin, Jimmy;"></td>
	<td class="line x" title="1:218	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 419428, Honolulu, October 2008." ></td>
	<td class="line x" title="2:218	c2008 Association for Computational Linguistics Scalable Language Processing Algorithms for the Masses: A Case Study in Computing Word Co-occurrence Matrices with MapReduce Jimmy Lin The iSchool, University of Maryland National Center for Biotechnology Information, National Library of Medicine jimmylin@umd.edu Abstract This paper explores the challenge of scaling up language processing algorithms to increasingly large datasets." ></td>
	<td class="line x" title="3:218	While cluster computing has been available in commercial environments for several years, academic researchers have fallen behind in their ability to work on large datasets." ></td>
	<td class="line x" title="4:218	I discuss two barriers contributing to this problem: lack of a suitable programming model for managing concurrency and difficulty in obtaining access to hardware." ></td>
	<td class="line x" title="5:218	Hadoop, an open-source implementation of Googles MapReduce framework, provides a compelling solution to both issues." ></td>
	<td class="line x" title="6:218	Its simple programming model hides system-level details from the developer, and its ability to run on commodity hardware puts cluster computing within the reach of many academic research groups." ></td>
	<td class="line x" title="7:218	This paper illustrates these points with a case study in building word cooccurrence matrices from large corpora." ></td>
	<td class="line x" title="8:218	I conclude with an analysis of an alternative computing model based on renting instead of buying computer clusters." ></td>
	<td class="line x" title="9:218	1 Introduction Over the past couple of decades, the field of computational linguistics (and more broadly, human language technologies) has seen the emergence and later dominance of empirical techniques and datadriven research." ></td>
	<td class="line x" title="10:218	Concomitant with this trend is a coherent research thread that focuses on exploiting increasingly-large datasets." ></td>
	<td class="line x" title="11:218	Banko and Brill (2001) were among the first to demonstrate the importance of dataset size as a significant factor governing prediction accuracy in a supervised machine learning task." ></td>
	<td class="line x" title="12:218	In fact, they argued that size of training set was perhaps more important than the choice of machine learning algorithm itself." ></td>
	<td class="line x" title="13:218	Similarly, experiments in question answering have shown the effectiveness of simple pattern-matching techniques when applied to large quantities of data (Brill et al., 2001; Dumais et al., 2002)." ></td>
	<td class="line x" title="14:218	More recently, this line of argumentation has been echoed in experiments with Web-scale language models." ></td>
	<td class="line x" title="15:218	Brants et al.(2007) showed that for statistical machine translation, a simple smoothing technique (dubbed Stupid Backoff) approaches the quality of the Kneser-Ney algorithm as the amount of training data increases, and with the simple method one can process significantly more data." ></td>
	<td class="line x" title="17:218	Challenges in scaling algorithms to increasinglylarge datasets have become a serious issue for researchers." ></td>
	<td class="line x" title="18:218	It is clear that datasets readily available today and the types of analyses that researchers wish to conduct have outgrown the capabilities of individual computers." ></td>
	<td class="line x" title="19:218	The only practical recourse is to distribute the computation across multiple cores, processors, or machines." ></td>
	<td class="line x" title="20:218	The consequences of failing to scale include misleading generalizations on artificially small datasets and limited practical applicability in real-world contexts, both undesirable." ></td>
	<td class="line x" title="21:218	This paper focuses on two barriers to developing scalable language processing algorithms: challenges associated with parallel programming and access to hardware." ></td>
	<td class="line x" title="22:218	Googles MapReduce framework (Dean and Ghemawat, 2004) provides an attractive programming model for developing scalable algorithms, and with the release of Hadoop, an open-source implementation of MapReduce lead 419 by Yahoo, cost-effective cluster computing is within the reach of most academic research groups." ></td>
	<td class="line x" title="23:218	It is emphasized that this work focuses on largedata algorithms from the perspective of academia colleagues in commercial environments have long enjoyed the advantages of cluster computing." ></td>
	<td class="line x" title="24:218	However, it is only recently that such capabilities have become practical for academic research groups." ></td>
	<td class="line x" title="25:218	These points are illustrated by a case study in building large word co-occurrence matrices, a simple task that underlies many NLP algorithms." ></td>
	<td class="line x" title="26:218	The remainder of the paper is organized as follows: the next section overviews the MapReduce framework and why it provides a compelling solution to the issues sketched above." ></td>
	<td class="line x" title="27:218	Section 3 introduces the task of building word co-occurrence matrices, which provides an illustrative case study." ></td>
	<td class="line x" title="28:218	Two separate algorithms are presented in Section 4." ></td>
	<td class="line x" title="29:218	The experimental setup is described in Section 5, followed by presentation of results in Section 6." ></td>
	<td class="line x" title="30:218	Implications and generalizations are discussed following that." ></td>
	<td class="line x" title="31:218	Before concluding, I explore an alternative model of computing based on renting instead of buying hardware, which makes cluster computing practical for everyone." ></td>
	<td class="line x" title="32:218	2 MapReduce The only practical solution to large-data challenges today is to distribute the computation across multiple cores, processors, or machines." ></td>
	<td class="line x" title="33:218	The development of parallel algorithms involves a number of tradeoffs." ></td>
	<td class="line x" title="34:218	First is that of cost: a decision must be made between exotic hardware (e.g., large shared memory machines, InfiniBand interconnect) and commodity hardware." ></td>
	<td class="line x" title="35:218	There is significant evidence (Barroso et al., 2003) that solutions based on the latter are more cost effectiveand for resource-constrained academic NLP groups, commodity hardware is often the only practical route." ></td>
	<td class="line x" title="36:218	Given appropriate hardware, researchers must still contend with the challenge of developing software." ></td>
	<td class="line x" title="37:218	Quite simply, parallel programming is difficult." ></td>
	<td class="line x" title="38:218	Due to communication and synchronization issues, concurrent operations are notoriously challenging to reason about." ></td>
	<td class="line x" title="39:218	Reliability and fault tolerance become important design considerations on clusters containing large numbers of unreliable commodity parts." ></td>
	<td class="line x" title="40:218	With traditional parallel programming models (e.g., MPI), the developer shoulders the burden of explicitly managing concurrency." ></td>
	<td class="line x" title="41:218	As a result, a significant amount of the programmers attention is devoted to system-level details, leaving less time for focusing on the actual problem." ></td>
	<td class="line x" title="42:218	Recently, MapReduce (Dean and Ghemawat, 2004) has emerged as an attractive alternative to existing parallel programming models." ></td>
	<td class="line x" title="43:218	The MapReduce abstraction shields the programmer from having to explicitly worry about system-level issues such as synchronization, inter-process communication, and fault tolerance." ></td>
	<td class="line x" title="44:218	The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics." ></td>
	<td class="line x" title="45:218	This frees the programmer to focus on solving the problem at hand." ></td>
	<td class="line x" title="46:218	MapReduce builds on the observation that many information processing tasks have the same basic structure: a computation is applied over a large number of records (e.g., Web pages, bitext pairs, or nodes in a graph) to generate partial results, which are then aggregated in some fashion." ></td>
	<td class="line x" title="47:218	Naturally, the perrecord computation and aggregation function vary according to task, but the basic structure remains fixed." ></td>
	<td class="line x" title="48:218	Taking inspiration from higher-order functions in functional programming, MapReduce provides an abstraction at the point of these two operations." ></td>
	<td class="line x" title="49:218	Specifically, the programmer defines a mapper and a reducer with the following signatures: map: (k1,v1)[(k2,v2)] reduce: (k2,[v2])[(k3,v3)] Key-value pairs form the basic data structure in MapReduce." ></td>
	<td class="line x" title="50:218	The mapper is applied to every input key-value pair to generate an arbitrary number of intermediate key-value pairs ([] is used to denote a list)." ></td>
	<td class="line x" title="51:218	The reducer is applied to all values associated with the same intermediate key to generate output key-value pairs." ></td>
	<td class="line x" title="52:218	This two-stage processing structure is illustrated in Figure 1." ></td>
	<td class="line x" title="53:218	Under the framework, a programmer needs only to provide implementations of the mapper and reducer." ></td>
	<td class="line x" title="54:218	On top of a distributed file system (Ghemawat et al., 2003), the runtime transparently handles all other aspects of execution, on clusters ranging from a few to a few thousand nodes." ></td>
	<td class="line x" title="55:218	The runtime is responsible for scheduling map and reduce 420 Shuf fling : gro up v alue s by keys map map map map redu ce redu ce redu ce inpu t inpu t inpu t inpu t outp ut outp ut outp ut Figure 1: Illustration of the MapReduce framework: the mapper is applied to all input records, which generates results that are aggregated by the reducer." ></td>
	<td class="line x" title="56:218	The runtime groups together values by keys." ></td>
	<td class="line x" title="57:218	workers on commodity hardware assumed to be unreliable, and thus is tolerant to various faults through a number of error recovery mechanisms." ></td>
	<td class="line x" title="58:218	In the distributed file system, data blocks are stored on the local disks of machines in the clusterthe MapReduce runtime handles the scheduling of mappers on machines where the necessary data resides." ></td>
	<td class="line x" title="59:218	It also manages the potentially very large sorting problem between the map and reduce phases whereby intermediate k y-value pairs must be grouped by key." ></td>
	<td class="line x" title="60:218	As an optimization, MapReduce supports the use of combiners, which are similar to reducers except that they operate directly on the output of mappers (in memory, before intermediate output is written to disk)." ></td>
	<td class="line x" title="61:218	Combiners operate in isolation on each node in the cluster and cannot use partial results from other nodes." ></td>
	<td class="line x" title="62:218	Since the output of mappers (i.e., the key-value pairs) must ultimately be shuffled to the appropriate reducer over a network, combiners allow a programmer to aggregate partial results, thus reducing network traffic." ></td>
	<td class="line x" title="63:218	In cases where an operation is both associative and commutative, reducers can directly serve as combiners." ></td>
	<td class="line x" title="64:218	Googles proprietary implementation of MapReduce is in C++ and not available to the public." ></td>
	<td class="line x" title="65:218	However, the existence of Hadoop, an open-source implementation in Java spearheaded by Yahoo, allows anyone to take advantage of MapReduce." ></td>
	<td class="line x" title="66:218	The growing popularity of this technology has stimulated a flurry of recent work, on applications in machine learning (Chu et al., 2006), machine translation (Dyer et al., 2008), and document retrieval (Elsayed et al., 2008)." ></td>
	<td class="line x" title="67:218	3 Word Co-occurrence Matrices To illustrate the arguments outlined above, I present a case study using MapReduce to build word cooccurrence matrices from large corpora, a common task in natural language processing." ></td>
	<td class="line x" title="68:218	Formally, the co-occurrence matrix of a corpus is a square N  N matrix where N corresponds to the number of unique words in the corpus." ></td>
	<td class="line x" title="69:218	A cell mij contains the number of times word wi co-occurs with word wj within a specific contexta natural unit such as a sentence or a certain window of m words (where m is an application-dependent parameter)." ></td>
	<td class="line x" title="70:218	Note that the upper and lower triangles of the matrix are identical since co-occurrence is a symmetric relation." ></td>
	<td class="line oc" title="71:218	This task is quite common in corpus linguistics and provides the starting point to many other algorithms, e.g., for computing statistics such as pointwise mutual information (Church and Hanks, 1990), for unsupervised sense clustering (Schutze, 1998), and more generally, a large body of work in lexical semantics based on distributional profiles, dating back to Firth (1957) and Harris (1968)." ></td>
	<td class="line x" title="72:218	The task also has applications in information retrieval, e.g., (Schutze and Pedersen, 1998; Xu and Croft, 1998), and other related fields as well." ></td>
	<td class="line x" title="73:218	More generally, this problem relates to the task of estimating distributions of discrete events from a large number of observations (more on this in Section 7)." ></td>
	<td class="line x" title="74:218	It is obvious that the space requirement for this problem is O(N2), where N is the size of the vocabulary, which for real-world English corpora can be hundreds of thousands of words." ></td>
	<td class="line x" title="75:218	The computation of the word co-occurrence matrix is quite simple if the entire matrix fits into memoryhowever, in the case where the matrix is too big to fit in memory, a naive implementation can be very slow as memory is paged to disk." ></td>
	<td class="line x" title="76:218	For large corpora, one needs to optimize disk access and avoid costly seeks." ></td>
	<td class="line x" title="77:218	As illustrated in the next section, MapReduce handles exactly these issues transparently, allowing the programmer to express the algorithm in a straightforward manner." ></td>
	<td class="line x" title="78:218	A bit more discussion of the task before moving on: in many applications, researchers have discovered that building the complete word cooccurrence matrix may not be necessary." ></td>
	<td class="line x" title="79:218	For example, Schutze (1998) discusses feature selection 421 techniques in defining context vectors; Mohammad and Hirst (2006) present evidence that conceptual distance is better captured via distributional profiles mediated by thesaurus categories." ></td>
	<td class="line x" title="80:218	These objections, however, miss the pointthe focus of this paper is on practical cluster computing for academic researchers; this particular task serves merely as an illustrative example." ></td>
	<td class="line x" title="81:218	In addition, for rapid prototyping, it may be useful to start with the complete co-occurrence matrix (especially if it can be built efficiently), and then explore how algorithms can be optimized for specific applications and tasks." ></td>
	<td class="line x" title="82:218	4 MapReduce Implementation This section presents two MapReduce algorithms for building word co-occurrence matrices for large corpora." ></td>
	<td class="line x" title="83:218	The goal is to illustrate how the problem can be concisely captured in the MapReduce programming model, and how the runtime hides many of the system-level details associated with distributed computing." ></td>
	<td class="line x" title="84:218	Pseudo-code for the first, more straightforward, algorithm is shown in Figure 2." ></td>
	<td class="line x" title="85:218	Unique document ids and the corresponding texts make up the input key-value pairs." ></td>
	<td class="line x" title="86:218	The mapper takes each input document and emits intermediate key-value pairs with each co-occurring word pair as the key and the integer one as the value." ></td>
	<td class="line x" title="87:218	In the pseudo-code, EMIT denotes the creation of an intermediate key-value pair that is collected (and appropriately sorted) by the MapReduce runtime." ></td>
	<td class="line x" title="88:218	The reducer simply sums up all the values associated with the same co-occurring word pair, arriving at the absolute counts of the joint event in the corpus (corresponding to each cell in the co-occurrence matrix)." ></td>
	<td class="line x" title="89:218	For convenience, I refer to this algorithm as the pairs approach." ></td>
	<td class="line x" title="90:218	Since co-occurrence is a symmetric relation, it suffices to compute half of the matrix." ></td>
	<td class="line x" title="91:218	However, for conceptual clarity and to generalize to instances where the relation may not be symmetric, the algorithm computes the entire matrix." ></td>
	<td class="line x" title="92:218	The Java implementation of this algorithm is quite conciseless than fifty lines long." ></td>
	<td class="line x" title="93:218	Notice the MapReduce runtime guarantees that all values associated with the same key will be gathered together at the reduce stage." ></td>
	<td class="line x" title="94:218	Thus, the programmer does not need to explicitly manage the collection and distribution of 1: procedure MAP1(n,d) 2: for all wd do 3: for all u NEIGHBORS(w) do 4: EMIT((w,u),1) 1: procedure REDUCE1(p,[v1,v2,]) 2: for all v[v1,v2,] do 3: sumsum+v 4: EMIT(p,sum) Figure 2: Pseudo-code for the pairs approach for computing word co-occurrence matrices." ></td>
	<td class="line x" title="95:218	1: procedure MAP2(n,d) 2: INITIALIZE(H) 3: for all wd do 4: for all u NEIGHBORS(w) do 5: H{u}H{u}+1 6: EMIT(w,H) 1: procedure REDUCE2(w,[H1,H2,H3,]) 2: INITIALIZE(Hf) 3: for all H[H1,H2,H3,] do 4: MERGE(Hf,H) 5: EMIT(w,Hf) Figure 3: Pseudo-code for the stripes approach for computing word co-occurrence matrices." ></td>
	<td class="line x" title="96:218	partial results across a cluster." ></td>
	<td class="line x" title="97:218	In addition, the programmer does not need to explicitly partition the input data and schedule workers." ></td>
	<td class="line x" title="98:218	This example shows the extent to which distributed processing can be dominated by system issues, and how an appropriate abstraction can significantly simplify development." ></td>
	<td class="line x" title="99:218	It is immediately obvious that Algorithm 1 generates an immense number of key-value pairs." ></td>
	<td class="line x" title="100:218	Although this can be mitigated with the use of a combiner (since addition is commutative and associative), the approach still results in a large amount of network traffic." ></td>
	<td class="line x" title="101:218	An alternative approach is presented in Figure 3, first reported in Dyer et al.(2008)." ></td>
	<td class="line x" title="103:218	The major difference is that counts of co-occurring words are first stored in an associative array (H)." ></td>
	<td class="line x" title="104:218	The output of the mapper is a number of key-value pairs with words as keys and the corresponding associative arrays as the values." ></td>
	<td class="line x" title="105:218	The reducer performs an element-wise sum of all associative arrays with the same key (denoted by the function MERGE), thus ac422 cumulating counts that correspond to the same cell in the co-occurrence matrix." ></td>
	<td class="line x" title="106:218	Once again, a combiner can be used to cut down on the network traffic by merging partial results." ></td>
	<td class="line x" title="107:218	In the final output, each key-value pair corresponds to a row in the word cooccurrence matrix." ></td>
	<td class="line x" title="108:218	For convenience, I refer to this as the stripes approach." ></td>
	<td class="line x" title="109:218	Compared to the pairs approach, the stripes approach results in far fewer intermediate key-value pairs, although each is significantly larger (and there is overhead in serializing and deserializing associative arrays)." ></td>
	<td class="line x" title="110:218	A critical assumption of the stripes approach is that at any point in time, each associative array is small enough to fit into memory (otherwise, memory paging may result in a serious loss of efficiency)." ></td>
	<td class="line x" title="111:218	This is true for most corpora, since the size of the associative array is bounded by the vocabulary size." ></td>
	<td class="line x" title="112:218	Section 6 compares the efficiency of both algorithms.1 5 Experimental Setup Work reported in this paper used the English Gigaword corpus (version 3),2 which consists of newswire documents from six separate sources, totaling 7.15 million documents (6.8 GB compressed, 19.4 GB uncompressed)." ></td>
	<td class="line x" title="113:218	Some experiments used only documents from the Associated Press Worldstream (APW), which contains 2.27 million documents (1.8 GB compressed, 5.7 GB uncompressed)." ></td>
	<td class="line x" title="114:218	By LDCs count, the entire collection contains approximately 2.97 billion words." ></td>
	<td class="line x" title="115:218	Prior to working with Hadoop, the corpus was first preprocessed." ></td>
	<td class="line x" title="116:218	All XML markup was removed, followed by tokenization and stopword removal using standard tools from the Lucene search engine." ></td>
	<td class="line x" title="117:218	All tokens were replaced with unique integers for a more efficient encoding." ></td>
	<td class="line x" title="118:218	The data was then packed into a Hadoop-specific binary file format." ></td>
	<td class="line x" title="119:218	The entire Gigaword corpus took up 4.69 GB in this format; the APW sub-corpus, 1.32 GB." ></td>
	<td class="line x" title="120:218	Initial experiments used Hadoop version 0.16.0 running on a 20-machine cluster (1 master, 19 slaves)." ></td>
	<td class="line x" title="121:218	This cluster was made available to the Uni1Implementations of both algorithms are included in Cloud9, an open source Hadoop library that I have been developing to support research and education, available from my homepage." ></td>
	<td class="line x" title="122:218	2LDC catalog number LDC2007T07 versity of Maryland as part of the Google/IBM Academic Cloud Computing Initiative." ></td>
	<td class="line x" title="123:218	Each machine has two single-core processors (running at either 2.4 GHz or 2.8 GHz), 4 GB memory." ></td>
	<td class="line x" title="124:218	The cluster has an aggregate storage capacity of 1.7 TB." ></td>
	<td class="line x" title="125:218	Hadoop ran on top of a virtualization layer, which has a small but measurable impact on performance; see (Barham et al., 2003)." ></td>
	<td class="line x" title="126:218	Section 6 reports experimental results using this cluster; Section 8 explores an alternative model of computing based on renting cycles." ></td>
	<td class="line x" title="127:218	6 Results First, I compared the running time of the pairs and stripes approaches discussed in Section 4." ></td>
	<td class="line x" title="128:218	Running times on the 20-machine cluster are shown in Figure 4 for the APW section of the Gigaword corpus: the x-axis shows different percentages of the sub-corpus (arbitrarily selected) and the y-axis shows running time in seconds." ></td>
	<td class="line x" title="129:218	For these experiments, the co-occurrence window was set to two, i.e., wi is said to co-occur with wj if they are no more than two words apart (after tokenization and stopword removal)." ></td>
	<td class="line x" title="130:218	Results demonstrate that the stripes approach is far more efficient than the pairs approach: 666 seconds (11m 6s) compared to 3758 seconds (62m 38s) for the entire APW sub-corpus (improvement by a factor of 5.7)." ></td>
	<td class="line x" title="131:218	On the entire sub-corpus, the mappers in the pairs approach generated 2.6 billion intermediate key-value pairs totally 31.2 GB." ></td>
	<td class="line x" title="132:218	After the combiners, this was reduced to 1.1 billion key-value pairs, which roughly quantifies the amount of data involved in the shuffling and sorting of the keys." ></td>
	<td class="line x" title="133:218	On the other hand, the mappers in the stripes approach generated 653 million intermediate key-value pairs totally 48.1 GB; after the combiners, only 28.8 million key-value pairs were left." ></td>
	<td class="line x" title="134:218	The stripes approach provides more opportunities for combiners to aggregate intermediate results, thus greatly reducing network traffic in the sort and shuffle phase." ></td>
	<td class="line x" title="135:218	Figure 4 also shows that both algorithms exhibit highly desirable scaling characteristicslinear in the corpus size." ></td>
	<td class="line x" title="136:218	This is confirmed by a linear regression applied to the running time data, which yields R2 values close to one." ></td>
	<td class="line x" title="137:218	Given that the stripes algorithm is more efficient, it is used in the remainder of the experiments." ></td>
	<td class="line x" title="138:218	423  0 500 1000 1500 2000  2500 3000 3500 4000  20 40 60 80 100running time (seconds) percentage of the APW sub-corpora of the English Gigaword Efficiency comparison of approaches to computing word co-occurrence matrices R2 = 0.992 R2 = 0.999'stripes' approach'pairs' approach Figure 4: Running time of the two algorithms (stripes vs. pairs) for computing word co-occurrence matrices on the APW section of the Gigaword corpus." ></td>
	<td class="line x" title="139:218	The cluster used for this experiment contains 20 machines, each with two single-core processors." ></td>
	<td class="line x" title="140:218	0 1000 2000 3000  4000 5000 6000  1 2 3 4 5 6 7window size (number of words) Running time for different widow sizesR2 = 0.992 Figure 5: Running times for computing word co-occurrence matrices from the entire Gigaword corpus with varying window sizes." ></td>
	<td class="line x" title="141:218	The cluster used for this experiment contains 20 machines, each with two single-core processors." ></td>
	<td class="line x" title="142:218	424 With a window size of two, computing the word co-occurrence matrix for the entire Gigaword corpus (7.15 million documents) takes 37m 11s on the 20machine cluster." ></td>
	<td class="line x" title="143:218	Figure 5 shows the running time as a function of window size." ></td>
	<td class="line x" title="144:218	With a window of six words, running time on the complete Gigaword corpus rises to 1h 23m 45s." ></td>
	<td class="line x" title="145:218	Once again, the stripes algorithm exhibits the highly desirable characteristic of linear scaling in terms of window size, as confirmed by the linear regression with an R2 value very close to one." ></td>
	<td class="line x" title="146:218	7 Discussion The elegance of the programming model and good scaling characteristics of resulting implementations make MapReduce a compelling tool for a variety of natural language processing tasks." ></td>
	<td class="line x" title="147:218	In fact, MapReduce excels at a large class of problems in NLP that involves estimating probability distributions of discrete events from a large number of observations according to the maximum likelihood criterion: PMLE(B|A) = c(A,B)c(A) = c(A,B)summationtext Bprime c(A,Bprime) (1) In practice, it matters little whether these events are words, syntactic categories, word alignment links, or any construct of interest to researchers." ></td>
	<td class="line x" title="148:218	Absolute counts in the stripes algorithm presented in Section 4 can be easily converted into conditional probabilities by a final normalization step." ></td>
	<td class="line x" title="149:218	Recently, Dyer et al.(2008) used this approach for word alignment and phrase extraction in statistical machine translation." ></td>
	<td class="line x" title="151:218	Of course, many applications require smoothing of the estimated distributionsthis problem also has known solutions in MapReduce (Brants et al., 2007)." ></td>
	<td class="line x" title="152:218	Synchronization is perhaps the single largest bottleneck in distributed computing." ></td>
	<td class="line x" title="153:218	In MapReduce, this is handled in the shuffling and sorting of keyvalue pairs between the map and reduce phases." ></td>
	<td class="line x" title="154:218	Development of efficient MapReduce algorithms critically depends on careful control of intermediate output." ></td>
	<td class="line x" title="155:218	Since the network link between different nodes in a cluster is by far the component with the largest latency, any reduction in the size of intermediate output or a reduction in the number of key-value pairs will have significant impact on efficiency." ></td>
	<td class="line x" title="156:218	8 Computing on Demand The central theme of this paper is practical cluster computing for NLP researchers in the academic environment." ></td>
	<td class="line x" title="157:218	I have identified two key aspects of what it means to be practical: the first is an appropriate programming model for simplifying concurrency management; the second is access to hardware resources." ></td>
	<td class="line x" title="158:218	The Hadoop implementation of MapReduce addresses the first point and to a large extent the second point as well." ></td>
	<td class="line x" title="159:218	The cluster used for experiments in Section 6 is modest by todays standards and within the capabilities of many academic research groups." ></td>
	<td class="line x" title="160:218	It is not even a requirement for the computers to be rack-mounted units in a machine room (although that is clearly preferable); there are plenty of descriptions on the Web about Hadoop clusters built from a handful of desktop machines connected by gigabit Ethernet." ></td>
	<td class="line x" title="161:218	Even without access to hardware, cluster computing remains within the reach of resource-constrained academics." ></td>
	<td class="line x" title="162:218	Utility computing is an emerging concept whereby anyone can provision clusters on demand from a third-party provider." ></td>
	<td class="line x" title="163:218	Instead of upfront capital investment to acquire a cluster and reoccurring maintenance and administration costs, one could rent computing cycles as they are needed this is not a new idea (Rappa, 2004)." ></td>
	<td class="line x" title="164:218	One such service is provided by Amazon, called Elastic Compute Cloud (EC2).3 With EC2, researchers could dynamically create a Hadoop cluster on-the-fly and tear down the cluster once experiments are complete." ></td>
	<td class="line x" title="165:218	To demonstrate the use of this technology, I replicated some of the previous experiments on EC2 to provide a case study of this emerging model of computing." ></td>
	<td class="line x" title="166:218	Virtualized computation units in EC2 are called instances." ></td>
	<td class="line x" title="167:218	At the time of these experiments, the basic instance offers, according to Amazon, 1.7 GB of memory, 1 EC2 Compute Unit (1 virtual core with 1 EC2 Compute Unit), and 160 GB of instance storage." ></td>
	<td class="line x" title="168:218	Each instance-hour costs $0.10 (all prices given in USD)." ></td>
	<td class="line x" title="169:218	Computational resources are simply charged by the instance-hour, so that a ten-instance cluster for ten hours costs the same as a hundredinstance cluster for one hour (both $10)the Amazon infrastructure allows one to dynamically provision and release resources as necessary." ></td>
	<td class="line x" title="170:218	This is at3http://www.amazon.com/ec2 425  0 1000 2000 3000  4000 5000  10 20 30 40 50 60 70 80 901x2x3x 4x1x2x3x4xrunning time (seconds) relative speedup size of EC2 cluster (number of slave instances) Computing word co-occurrence matrices on Amazon EC2relative size of EC2 cluster$2.76$2.92$2.89 $2.64$2.69$2.63$2.5920-machine clusterR2 = 0.997 Figure 6: Running time analysis on Amazon EC2 with various cluster sizes; solid squares are annotated with the cost of each experiment." ></td>
	<td class="line x" title="171:218	Alternate axes (circles) plot scaling characteristics in terms increasing cluster size." ></td>
	<td class="line x" title="172:218	tractive for researchers, who could on a limited basis allocate clusters much larger than they could otherwise afford if forced to purchase the hardware outright." ></td>
	<td class="line x" title="173:218	Through virtualization technology, Amazon is able to parcel out allotments of processor cycles while maintaining high overall utilization across a data center and exploiting economies of scale." ></td>
	<td class="line x" title="174:218	Using EC2, I built word co-occurrence matrices from the entire English Gigaword corpus (window of two) on clusters of various sizes, ranging from 20 slave instances all the way up to 80 slave instances." ></td>
	<td class="line x" title="175:218	The entire cluster consists of the slave instances plus a master controller instance that serves as the job submission queue; the clusters ran Hadoop version 0.17.0 (the latest release at the time these experiments were conducted)." ></td>
	<td class="line x" title="176:218	Running times are shown in Figure 6 (solid squares), with varying cluster sizes on the x-axis." ></td>
	<td class="line x" title="177:218	Each data point is annotated with the cost of running the complete experiment.4 Results show that computing the complete word co-occurrence matrix costs, quite literally, a couple of dollarscertainly affordable by any academic researcher without access to hardware." ></td>
	<td class="line x" title="178:218	For reference, Figure 6 also plots the running time of the same experiment on the 20-machine cluster used 4Note that Amazon bills in whole instance-hour increments; these figures assume fractional accounting." ></td>
	<td class="line x" title="179:218	in Section 6 (which contains 38 worker cores, each roughly comparable to an instance)." ></td>
	<td class="line x" title="180:218	The alternate set of axes in Figure 6 shows the scaling characteristics of various cluster sizes." ></td>
	<td class="line x" title="181:218	The circles plot the relative size and speedup of the EC2 experiments, with respect to the 20-slave cluster." ></td>
	<td class="line x" title="182:218	The results show highly desirable linear scaling characteristics." ></td>
	<td class="line x" title="183:218	The above figures include only the cost of running the instances." ></td>
	<td class="line x" title="184:218	One must additionally pay for bandwidth when transferring data in and out of EC2." ></td>
	<td class="line x" title="185:218	At the time these experiments were conducted, Amazon charged $0.10 per GB for data transferred in and $0.17 per GB for data transferred out." ></td>
	<td class="line x" title="186:218	To complement EC2, Amazon offers persistent storage via the Simple Storage Service (S3),5 at a cost of $0.15 per GB per month." ></td>
	<td class="line x" title="187:218	There is no charge for data transfers between EC2 and S3." ></td>
	<td class="line x" title="188:218	The availability of this service means that one can choose between paying for data transfer or paying for persistent storage on a cyclic basisthe tradeoff naturally depends on the amount of data and its permanence." ></td>
	<td class="line x" title="189:218	The cost analysis presented above assumes optimally-efficient use of Amazons services; endto-end cost might better quantify real-world usage conditions." ></td>
	<td class="line x" title="190:218	In total, the experiments reported in this 5http://www.amazon.com/s3 426 section resulted in a bill of approximately thirty dollars." ></td>
	<td class="line x" title="191:218	The figure includes all costs associated with instance usage and data transfer costs." ></td>
	<td class="line x" title="192:218	It also includes time taken to learn the Amazon tools (I previously had no experience with either EC2 or S3) and to run preliminary experiments on smaller datasets (before scaling up to the complete corpus)." ></td>
	<td class="line x" title="193:218	The lack of fractional accounting on instance-hours contributed to the larger-than-expected costs, but such wastage would naturally be reduced with more experiments and higher sustained use." ></td>
	<td class="line x" title="194:218	Overall, these cost appear to be very reasonable, considering that the largest cluster in these experiments (1 master + 80 slave instances) might be too expensive for most academic research groups to own and maintain." ></td>
	<td class="line x" title="195:218	Consider another example that illustrates the possibilities of utility computing." ></td>
	<td class="line x" title="196:218	Brants et al.(2007) described experiments on building language models with increasingly-large corpora using MapReduce." ></td>
	<td class="line x" title="198:218	Their paper reported experiments on a corpus containing 31 billion tokens (about an order of magnitude larger than the English Gigaword): on 400 machines, the model estimation took 8 hours.6 With EC2, such an experiment would cost a few hundred dollarssufficiently affordable that availability of data becomes the limiting factor, not computational resources themselves." ></td>
	<td class="line x" title="199:218	The availability of computing-on-demand services and Hadoop make cluster computing practical for academic researchers." ></td>
	<td class="line x" title="200:218	Although Amazon is currently the most prominent provider of such services, they are not the sole player in an emerging marketin the future there will be a vibrant market with many competing providers." ></td>
	<td class="line x" title="201:218	Considering the tradeoffs between buying and renting, I would recommend the following model for an academic research group: purchase a modest cluster for development and for running smaller experiments; use a computing-on-demand service for scaling up and for running larger experiments (since it would be more difficult to economically justify a large cluster if it does not receive high sustained utilization)." ></td>
	<td class="line x" title="202:218	If the concept of utility computing takes hold, it would have a significant impact on computer science research in general: the natural implication is 6Brants et al. were affiliated with Google, so access to hardware was not an issue." ></td>
	<td class="line x" title="203:218	that algorithms should not only be analyzed in traditional terms such as asymptotic complexity, but also in terms of monetary costs, in relationship to dataset and cluster size." ></td>
	<td class="line x" title="204:218	One can argue that cost is a more direct and practical measure of algorithmic efficiency." ></td>
	<td class="line x" title="205:218	9 Conclusion This paper address two challenges faced by academic research groups in scaling up natural language processing algorithms to large corpora: the lack of an appropriate programming model for expressing the problem and the difficulty in getting access to hardware." ></td>
	<td class="line x" title="206:218	With this case study in building word co-occurrence matrices from large corpora, I demonstrate that MapReduce, via the open source Hadoop implementation, provides a compelling solution." ></td>
	<td class="line x" title="207:218	A large class of algorithms in computational linguistics can be readily expressed in MapReduce, and the resulting code can be transparently distributed across commodity clusters." ></td>
	<td class="line x" title="208:218	Finally, the cycle-renting model of computing makes access to large clusters affordable to researchers with limited resources." ></td>
	<td class="line x" title="209:218	Together, these developments dramatically lower the entry barrier for academic researchers who wish to explore large-data issues." ></td>
	<td class="line x" title="210:218	Acknowledgments This work was supported by the Intramural Research Program of the NIH, National Library of Medicine; NSF under awards IIS-0705832 and IIS-0836560; DARPA/IPTO Contract No." ></td>
	<td class="line x" title="211:218	HR0011-06-2-0001 under the GALE program." ></td>
	<td class="line x" title="212:218	Any opinions, findings, conclusions, or recommendations expressed in this paper are the authors and do not necessarily reflect those of the sponsors." ></td>
	<td class="line x" title="213:218	I would like to thank Yahoo!" ></td>
	<td class="line x" title="214:218	for leading the development of Hadoop, IBM and Google for hardware support via the Academic Cloud Computing Initiative (ACCI), and Amazon for EC2/S3 support." ></td>
	<td class="line x" title="215:218	This paper provides a neutral evaluation of EC2 and S3, and should not be interpreted as endorsement for the commercial services offered by Amazon." ></td>
	<td class="line x" title="216:218	I wish to thank Philip Resnik and Doug Oard for comments on earlier drafts of this paper, and Ben Shneiderman for helpful editing suggestions." ></td>
	<td class="line x" title="217:218	I am, as always, grateful to Esther and Kiri for their kind support." ></td>
	<td class="line x" title="218:218	427" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1014
Determining the Unithood of Word Sequences Using a Probabilistic Approach
Wong, Wilson;Liu, Wei;Bennamoun, Mohammed;"></td>
	<td class="line x" title="1:157	Determining the Unithood of Word Sequences using a Probabilistic Approach Wilson Wong, Wei Liu and Mohammed Bennamoun School of Computer Science and Software Engineering University of Western Australia Crawley WA 6009 {wilson,wei,bennamou}@csse.uwa.edu.au Abstract Most research related to unithood were conducted as part of a larger effort for the determination of termhood." ></td>
	<td class="line x" title="2:157	Consequently, novelties are rare in this small sub-field of term extraction." ></td>
	<td class="line x" title="3:157	In addition, existing work were mostly empirically motivated and derived." ></td>
	<td class="line x" title="4:157	We propose a new probabilistically-derived measure, independent of any influences of termhood, that provides dedicated measures to gather linguistic evidence from parsed text and statistical evidence from Google search engine for the measurement of unithood." ></td>
	<td class="line x" title="5:157	Our comparative study using 1,825 test cases against an existing empiricallyderived function revealed an improvement in terms of precision, recall and accuracy." ></td>
	<td class="line x" title="6:157	1 Introduction Automatic term recognition, also referred to as term extraction or terminology mining, is the process of extracting lexical units from text and filtering them for the purpose of identifying terms which characterise certain domains of interest." ></td>
	<td class="line x" title="7:157	This process involves the determination of two factors: unithood and termhood." ></td>
	<td class="line x" title="8:157	Unithood concerns with whether or not a sequence of words should be combined to form a more stable lexical unit." ></td>
	<td class="line x" title="9:157	On the other hand, termhood measures the degree to which these stable lexical units are related to domain-specific concepts." ></td>
	<td class="line x" title="10:157	Unithood is only relevant to complex terms (i.e. multi-word terms) while termhood (Wong et al., 2007a) deals with both simple terms (i.e. singleword terms) and complex terms." ></td>
	<td class="line x" title="11:157	Recent reviews by (Wong et al., 2007b) show that existing research on unithood are mostly carried out as a prerequisite to the determination of termhood." ></td>
	<td class="line x" title="12:157	As a result, there is only a small number of existing measures dedicated to determining unithood." ></td>
	<td class="line x" title="13:157	Besides the lack of dedicated attention in this sub-field of term extraction, the existing measures are usually derived from term or document frequency, and are modified as per need." ></td>
	<td class="line x" title="14:157	As such, the significance of the different weights that compose the measures usually assume an empirical viewpoint." ></td>
	<td class="line x" title="15:157	Obviously, such methods are at most inspired by, but not derived from formal models (Kageura and Umino, 1996)." ></td>
	<td class="line x" title="16:157	The three objectives of this paper are (1) to separate the measurement of unithood from the determination of termhood, (2) to devise a probabilisticallyderived measure which requires only one threshold for determining the unithood of word sequences using non-static textual resources, and (3) to demonstrate the superior performance of the new probabilistically-derived measure against existing empirical measures." ></td>
	<td class="line x" title="17:157	In regards to the first objective, we will derive our probabilistic measure free from any influence of termhood determination." ></td>
	<td class="line x" title="18:157	Following this, our unithood measure will be an independent tool that is applicable not only to term extraction, but many other tasks in information extraction and text mining." ></td>
	<td class="line x" title="19:157	Concerning the second objective, we will devise our new measure, known as the Odds of Unithood (OU), which are derived using Bayes Theorem and founded on a few elementary probabilities." ></td>
	<td class="line x" title="20:157	The probabilities are estimated using Google page counts in an attempt to eliminate problems related to the use of static corpora." ></td>
	<td class="line x" title="21:157	Moreover, only 103 one threshold, namely, OUT is required to control the functioning of OU." ></td>
	<td class="line x" title="22:157	Regarding the third objective, we will compare our new OU against an existing empirically-derived measure called Unithood (UH) (Wong et al., 2007b) in terms of their precision, recall and accuracy." ></td>
	<td class="line x" title="23:157	In Section 2, we provide a brief review on some of existing techniques for measuring unithood." ></td>
	<td class="line x" title="24:157	In Section 3, we present our new probabilistic approach, the measures involved, and the theoretical and intuitive justification behind every aspect of our measures." ></td>
	<td class="line x" title="25:157	In Section 4, we summarize some findings from our evaluations." ></td>
	<td class="line x" title="26:157	Finally, we conclude this paper with an outlook to future work in Section 5." ></td>
	<td class="line oc" title="27:157	2 Related Works Some of the most common measures of unithood include pointwise mutual information (MI) (Church and Hanks, 1990) and log-likelihood ratio (Dunning, 1994)." ></td>
	<td class="line o" title="28:157	In mutual information, the co-occurrence frequencies of the constituents of complex terms are utilised to measure their dependency." ></td>
	<td class="line o" title="29:157	The mutual information for two words a and b is defined as: MI(a,b) = log2 p(a,b)p(a)p(b) (1) where p(a) and p(b) are the probabilities of occurrence of a and b. Many measures that apply statistical techniques assuming strict normal distribution, and independence between the word occurrences (Franz, 1997) do not fare well." ></td>
	<td class="line x" title="30:157	For handling extremely uncommon words or small sized corpus, log-likelihood ratio delivers the best precision (Kurz and Xu, 2002)." ></td>
	<td class="line x" title="31:157	Log-likelihood ratio attempts to quantify how much more likely one pair of words is to occur compared to the others." ></td>
	<td class="line x" title="32:157	Despite its potential, How to apply this statistic measure to quantify structural dependency of a word sequence remains an interesting issue to explore. (Kit, 2002)." ></td>
	<td class="line x" title="33:157	(Seretan et al., 2004) tested mutual information, loglikelihood ratio and t-tests to examine the use of results from web search engines for determining the collocational strength of word pairs." ></td>
	<td class="line x" title="34:157	However, no performance results were presented." ></td>
	<td class="line x" title="35:157	(Wong et al., 2007b) presented a hybrid approach inspired by mutual information in Equation 1, and C-value in Equation 3." ></td>
	<td class="line x" title="36:157	The authors employ Google page counts for the computation of statistical evidences to replace the use of frequencies obtained from static corpora." ></td>
	<td class="line x" title="37:157	Using the page counts, the authors proposed a function known as Unithood (UH) for determining the mergeability of two lexical units ax and ay to produce a stable sequence of words s. The word sequences are organised as a set W = {s,ax,ay} where s = axbay is a term candidate, b can be any preposition, the coordinating conjunction and or an empty string, and ax and ay can either be noun phrases in the form AdjN+ or another s (i.e. defining a new s in terms of other s)." ></td>
	<td class="line o" title="38:157	The authors define UH as: UH(ax,ay) =            1 if (MI(ax,ay) > MI+)  (MI+  MI(ax,ay)  MI ID(ax,s)  IDT  ID(ay,s)  IDT  IDR+  IDR(ax,ay)  IDR) 0 otherwise (2) where MI+, MI, IDT , IDR+ and IDR are thresholds for determining mergeability decisions, and MI(ax,ay) is the mutual information between ax and ay, while ID(ax,s), ID(ay,s) and IDR(ax,ay) are measures of lexical independence of ax and ay from s. For brevity, let z be either ax or ay, and the independence measure ID(z,s) is then defined as: ID(z,s) = braceleftBigg log10(nz ns) if(nz > ns) 0 otherwise where nz and ns is the Google page count for z and s respectively." ></td>
	<td class="line x" title="39:157	On the other hand, IDR(ax,ay) = ID(ax,s) ID(ay,s)." ></td>
	<td class="line o" title="40:157	Intuitively, UH(ax,ay) states that the twolexical units a x and ay can only be merged in two cases, namely, 1) if ax and ay has extremely high mutual information (i.e. higher than a certain threshold MI+), or 2) if ax and ay achieve average mutual information (i.e. within the acceptable range of two thresholds MI+ and MI) due to both of their extremely high independence (i.e. higher than the threshold IDT ) from s." ></td>
	<td class="line x" title="41:157	(Frantzi, 1997) proposed a measure known as Cvalue for extracting complex terms." ></td>
	<td class="line x" title="42:157	The measure 104 is based upon the claim that a substring of a term candidate is a candidate itself given that it demonstrates adequate independence from the longer version it appears in." ></td>
	<td class="line x" title="43:157	For example, E. coli food poisoning, E. coli and food poisoning are acceptable as valid complex term candidates." ></td>
	<td class="line x" title="44:157	However, E. coli food is not." ></td>
	<td class="line x" title="45:157	Therefore, some measures are required to gauge the strength of word combinations to decide whether two word sequences should be merged or not." ></td>
	<td class="line x" title="46:157	Given a word sequence a to be examined for unithood, the Cvalue is defined as: Cvalue(a) = braceleftBigglog 2 |a|fa if |a| = g log2 |a|(fa  summationtext lLa fl |La| ) otherwise (3) where |a| is the number of words in a, La is the set of longer term candidates that contain a, g is the longest n-gram considered, fa is the frequency of occurrence of a, and a / La. While certain researchers (Kit, 2002) consider Cvalue as a termhood measure, others (Nakagawa and Mori, 2002) accept it as a measure for unithood." ></td>
	<td class="line x" title="47:157	One can observe that longer candidates tend to gain higher weights due to the inclusion of log2 |a| in Equation 3." ></td>
	<td class="line x" title="48:157	In addition, the weights computed using Equation 3 are purely dependent on the frequency of a. 3 A Probabilistically-derived Measure for Unithood Determination We propose a probabilistically-derived measure for determining the unithood of word pairs (i.e. potential term candidates) extracted using the headdriven left-right filter (Wong, 2005; Wong et al., 2007b) and Stanford Parser (Klein and Manning, 2003)." ></td>
	<td class="line x" title="49:157	These word pairs will appear in the form of (ax,ay)  A with ax and ay located immediately next to each other (i.e. x + 1 = y), or separated by a preposition or coordinating conjunction and (i.e. x+2 = y)." ></td>
	<td class="line x" title="50:157	Obviously, ax has to appear before ay in the sentence or in other words, x < y for all pairs where x and y are the word offsets produced by the Stanford Parser." ></td>
	<td class="line x" title="51:157	The pairs in A will remain as potential term candidates until their unithood have been examined." ></td>
	<td class="line x" title="52:157	Once the unithood of the pairs in A have been determined, they will be referred to as term candidates." ></td>
	<td class="line x" title="53:157	Formally, the unithood of any two lexical units ax and ay can be defined as Definition 1 The unithood of two lexical units is the degree of strength or stability of syntagmatic combinations and collocations (Kageura and Umino, 1996) between them." ></td>
	<td class="line x" title="54:157	It is obvious that the problem of measuring the unithood of any pair of words is the determination of their degree of collocational strength as mentioned in Definition 1." ></td>
	<td class="line x" title="55:157	In practical terms, the degree mentioned above will provide us with a way to determine if the units ax and ay should be combined to form s, or left alone as separate units." ></td>
	<td class="line x" title="56:157	The collocational strength of ax and ay that exceeds a certain threshold will demonstrate to us that s is able to form a stable unit and hence, a better term candidate than ax and ay separated." ></td>
	<td class="line x" title="57:157	It is worth pointing that the size (i.e. number of words) of ax and ay is not limited to 1." ></td>
	<td class="line x" title="58:157	For example, we can have ax=National Institute, b=of and ay=Allergy and Infectious Diseases." ></td>
	<td class="line x" title="59:157	In addition, the size of ax and ay has no effect on the determination of their unithood using our approach." ></td>
	<td class="line x" title="60:157	As we have discussed in Section 2, most of the conventional practices employ frequency of occurrence from local corpora, and some statistical tests or information-theoretic measures to determine the coupling strength between elements in W = {s,ax,ay}." ></td>
	<td class="line x" title="61:157	Two of the main problems associated with such approaches are:  Data sparseness is a problem that is welldocumented by many researchers (Keller et al., 2002)." ></td>
	<td class="line x" title="62:157	It is inherent to the use of local corpora that can lead to poor estimation of parameters or weights; and  Assumption of independence and normality of word distribution are two of the many problems in language modelling (Franz, 1997)." ></td>
	<td class="line x" title="63:157	While the independence assumption reduces text to simply a bag of words, the assumption of normal distribution of words will often lead to incorrect conclusions during statistical tests." ></td>
	<td class="line x" title="64:157	As a general solution, we innovatively employ results from web search engines for use in a probabilistic framework for measuring unithood." ></td>
	<td class="line x" title="65:157	As an attempt to address the first problem, we utilise page counts by Google for estimating the probability of occurrences of the lexical units in W. 105 We consider the World Wide Web as a large general corpus and the Google search engine as a gateway for accessing the documents in the general corpus." ></td>
	<td class="line x" title="66:157	Our choice of using Google to obtain the page count was merely motivated by its extensive coverage." ></td>
	<td class="line x" title="67:157	In fact, it is possible to employ any search engines on the World Wide Web for this research." ></td>
	<td class="line x" title="68:157	As for the second issue, we attempt to address the problem of determining the degree of collocational strength in terms of probabilities estimated using Google page count." ></td>
	<td class="line x" title="69:157	We begin by defining the sample space, N as the set of all documents indexed by Google search engine." ></td>
	<td class="line x" title="70:157	We can estimate the index size of Google, |N| using function words as predictors." ></td>
	<td class="line x" title="71:157	Function words such as a, is and with, as opposed to content words, appear with frequencies that are relatively stable over many different genres." ></td>
	<td class="line x" title="72:157	Next, we perform random draws (i.e. trial) of documents from N. For each lexical unit w  W, there will be a corresponding set of outcomes (i.e. events) from the draw." ></td>
	<td class="line x" title="73:157	There will be three basic sets which are of interest to us: Definition 2 Basic events corresponding to each w  W:  X is the event that ax occurs in the document  Y is the event that ay occurs in the document  S is the event that s occurs in the document It should be obvious to the readers that since the documents in S have to contain all two units ax and ay, S is a subset of X  Y or S  X  Y . It is worth noting that even though S  X  Y , it is highly unlikely that S = X  Y since the two portions ax and ay may exist in the same document without being conjoined by b. Next, subscribing to the frequency interpretation of probability, we can obtain the probability of the events in Definition 2 in terms of Google page count: P(X) = nx|N| (4) P(Y) = ny|N| P(S) = ns|N| where nx, ny and ns is the page count returned as the result of Google search using the term [+ax], [+ay] and [+s], respectively." ></td>
	<td class="line x" title="74:157	The pair of quotes that encapsulates the search terms is the phrase operator, while the character + is the required operator supported by the Google search engine." ></td>
	<td class="line x" title="75:157	As discussed earlier, the independence assumption required by certain information-theoretic measures and other Bayesian approaches may not always be valid, especially when we are dealing with linguistics." ></td>
	<td class="line x" title="76:157	As such, P(X  Y) negationslash= P(X)P(Y) since the occurrences of ax and ay in documents are inevitably governed by some hidden variables and hence, not independent." ></td>
	<td class="line x" title="77:157	Following this, we define the probabilities for two new sets which result from applying some set operations on the basic events in Definition 2: P(X Y) = nxy|N| (5) P(X Y \S) = P(X Y)P(S) where nxy is the page count returned by Google for the search using [+ax +ay]." ></td>
	<td class="line x" title="78:157	Defining P(XY) in terms of observable page counts, rather than a combination of two independent events will allow us to avoid any unnecessary assumption of independence." ></td>
	<td class="line x" title="79:157	Next, referring back to our main problem discussed in Definition 1, we are required to estimate the strength of collocation of the two units ax and ay." ></td>
	<td class="line x" title="80:157	Since there is no standard metric for such measurement, we propose to address the problem from a probabilistic perspective." ></td>
	<td class="line x" title="81:157	We introduce the probability that s is a stable lexical unit given the evidence s possesses: Definition 3 Probability of unithood: P(U|E) = P(E|U)P(U)P(E) where U is the event that s is a stable lexical unit and E is the evidences belonging to s. P(U|E) is the posterior probability that s is a stable unit given the evidence E. P(U) is the prior probability that s is a unit without any evidence, and P(E) is the prior probability of evidences held by s. As we shall see later, these two prior probabilities will be immaterial in the final computation of unithood." ></td>
	<td class="line x" title="82:157	Since s can either be a stable unit or not, we can state that, P(U|E) = 1P(U|E) (6) 106 where U is the event that s is not a stable lexical unit." ></td>
	<td class="line x" title="83:157	Since Odds = P/(1  P), we multiply both sides of Definition 3 by (1P(U|E))1 to obtain, P(U|E) 1P(U|E) = P(E|U)P(U) P(E)(1P(U|E)) (7) By substituting Equation 6 in Equation 7 and later, applying the multiplication rule P(U|E)P(E) = P(E|U)P(U) to it, we will obtain: P(U|E) P(U|E) = P(E|U)P(U) P(E|U)P(U) (8) We proceed to take the log of the odds in Equation 8 (i.e. logit) to get: log P(E|U)P(E|U) = log P(U|E)P(U|E) log P(U)P(U) (9) While it is obvious that certain words tend to cooccur more frequently than others (i.e. idioms and collocations), such phenomena are largely arbitrary (Smadja, 1993)." ></td>
	<td class="line x" title="84:157	This makes the task of deciding on what constitutes an acceptable collocation difficult." ></td>
	<td class="line x" title="85:157	The only way to objectively identify stable lexical units is through observations in samples of the language (e.g. text corpus) (McKeown and Radev, 2000)." ></td>
	<td class="line x" title="86:157	In other words, assigning the apriori probability of collocational strength without empirical evidence is both subjective and difficult." ></td>
	<td class="line x" title="87:157	As such, we are left with the option to assume that the probability of s being a stable unit and not being a stable unit without evidence is the same (i.e. P(U) = P(U) = 0.5)." ></td>
	<td class="line x" title="88:157	As a result, the second term in Equation 9 evaluates to 0: log P(U|E)P(U|E) = log P(E|U)P(E|U) (10) We introduce a new measure for determining the odds of s being a stable unit known as Odds of Unithood (OU): Definition 4 Odds of unithood OU(s) = log P(E|U)P(E|U) Assuming that the evidences in E are independent of one another, we can evaluate OU(s) in terms of: OU(s) = log producttext i P(ei|U)producttext i P(ei|U) (11) = summationdisplay i log P(ei|U)P(e i|U) (a) The area with darker shade is the set X  Y \ S. Computing the ratio of P(S) and the probability of this area will give us the first evidence." ></td>
	<td class="line x" title="89:157	(b) The area with darker shade is the set S." ></td>
	<td class="line x" title="90:157	Computing the ratio of P(S) and the probability of this area (i.e. P(S) = 1P(S)) will give us the second evidence." ></td>
	<td class="line x" title="91:157	Figure 1: The probability of the areas with darker shade are the denominators required by the evidences e1 and e2 for the estimation of OU(s)." ></td>
	<td class="line x" title="92:157	where ei are individual evidences possessed by s. With the introduction of Definition 4, we can examine the degree of collocational strength of ax and ay in forming s, mentioned in Definition 1 in terms of OU(s)." ></td>
	<td class="line x" title="93:157	With the base of the log in Definition 4 more than 1, the upper and lower bound of OU(s) would be + and , respectively." ></td>
	<td class="line x" title="94:157	OU(s) = + and OU(s) =  corresponds to the highest and the lowest degree of stability of the two units ax and ay appearing as s, respectively." ></td>
	<td class="line x" title="95:157	A high1 OU(s) would indicate the suitability for the two units ax and ay to be merged to form s. Ultimately, we have reduced the vague problem of the determination of unithood introduced in Definition 1 into a practical and computable solution in Definition 4." ></td>
	<td class="line x" title="96:157	The evidences that we propose to employ for determining unithood are based on the occurrences of s, or the event S if the readers recall from Definition 2." ></td>
	<td class="line x" title="97:157	We are interested in two types of occurrences of s, namely, the occurrence of s given that ax and ay have already occurred or X  Y , and the occurrence of s as it is in our sample space, N. We refer to the first evidence e1 as local occurrence, while the second one e2 as global occurrence." ></td>
	<td class="line x" title="98:157	We will discuss the intuitive justification behind each type of occurrences." ></td>
	<td class="line x" title="99:157	Each evidence ei captures the occurrences of s within a different confinement." ></td>
	<td class="line x" title="100:157	We will estimate these evidences in terms of the elementary probabilities already defined in Equations 4 and 5." ></td>
	<td class="line x" title="101:157	The first evidence e1 captures the probability of occurrences of s within the confinement of ax and ay 1A subjective issue that may be determined using a threshold 107 or XY . As such, P(e1|U) can be interpreted as the probability of s occurring within X Y as a stable unit or P(S|X  Y)." ></td>
	<td class="line x" title="102:157	On the other hand, P(e1|U) captures the probability of s occurring in X Y not as a unit." ></td>
	<td class="line x" title="103:157	In other words, P(e1|U) is the probability of s not occurring in X  Y , or equivalently, equal to P((X Y \S)|(X Y))." ></td>
	<td class="line x" title="104:157	The set X Y \S is shown as the area with darker shade in Figure 1(a)." ></td>
	<td class="line x" title="105:157	Let us define the odds based on the first evidence as: OL = P(e1|U)P(e 1|U) (12) Substituting P(e1|U) = P(S|X  Y) and P(e1|U) = P((X  Y \ S)|(X  Y)) into Equation 12 will give us: OL = P(S|X Y)P((X Y \S)|(X Y)) = P(S (X Y))P(X Y) P(X Y)P((X Y \S)(X Y)) = P(S (X Y))P((X Y \S)(X Y)) and since S  (XY) and (XY \S)  (XY), OL = P(S)P(X Y \S) if(P(X Y \S) negationslash= 0) and OL = 1 if P(X Y \S) = 0." ></td>
	<td class="line x" title="106:157	The second evidence e2 captures the probability of occurrences of s without confinement." ></td>
	<td class="line x" title="107:157	If s is a stable unit, then its probability of occurrence in the sample space would simply be P(S)." ></td>
	<td class="line x" title="108:157	On the other hand, if s occurs not as a unit, then its probability of non-occurrence is 1P(S)." ></td>
	<td class="line x" title="109:157	The complement of S, which is the set S is shown as the area with darker shade in Figure 1(b)." ></td>
	<td class="line x" title="110:157	Let us define the odds based on the second evidence as: OG = P(e2|U)P(e 2|U) (13) Substituting P(e2|U) = P(S) and P(e2|U) = 1  P(S) into Equation 13 will give us: OG = P(S)1P(S) Intuitively, the first evidence attempts to capture the extent to which the existence of the two lexical units ax and ay is attributable to s. Referring back to OL, whenever the denominator P(XY \S) becomes less than P(S), we can deduce that ax and ay actually exist together as s more than in other forms." ></td>
	<td class="line x" title="111:157	At one extreme when P(X  Y \ S) = 0, we can conclude that the co-occurrence of ax and ay is exclusively for s. As such, we can also refer to OL as a measure of exclusivity for the use of ax and ay with respect to s. This first evidence is a good indication for the unithood of s since the more the existence of ax and ay is attributed to s, the stronger the collocational strength of s becomes." ></td>
	<td class="line x" title="112:157	Concerning the second evidence, OG attempts to capture the extent to which s occurs in general usage (i.e. World Wide Web)." ></td>
	<td class="line x" title="113:157	We can consider OG as a measure of pervasiveness for the use of s. As s becomes more widely used in text, the numerator in OG will increase." ></td>
	<td class="line x" title="114:157	This provides a good indication on the unithood of s since the more s appears in usage, the likelier it becomes that s is a stable unit instead of an occurrence by chance when ax and ay are located next to each other." ></td>
	<td class="line x" title="115:157	As a result, the derivation of OU(s) using OL and OG will ensure a comprehensive way of determining unithood." ></td>
	<td class="line x" title="116:157	Finally, expanding OU(s) in Equation 11 using Equations 12 and 13 will give us: OU(s) = logOL + logOG (14) = log P(S)P(X Y \S) + log P(S)1P(S) As such, the decision on whether ax and ay should be merged to form s can be made based solely on the Odds of Unithood (OU) defined in Equation 14." ></td>
	<td class="line x" title="117:157	We will merge ax and ay if their odds of unithood exceeds a certain threshold, OUT . 4 Evaluations and Discussions For this evaluation, we employed 500 news articles from Reuters in the health domain gathered between December 2006 to May 2007." ></td>
	<td class="line x" title="118:157	These 500 articles are fed into the Stanford Parser whose output is then used by our head-driven left-right filter (Wong, 2005; Wong et al., 2007b) to extract word sequences in the form of nouns and noun phrases." ></td>
	<td class="line x" title="119:157	Pairs of word sequences (i.e. ax and ay) located immediately next to each other, or separated by a preposition or the conjunction and in the same sentence are mea108 sured for their unithood." ></td>
	<td class="line x" title="120:157	Using the 500 news articles, we managed to obtain 1,825 pairs of words to be tested for unithood." ></td>
	<td class="line x" title="121:157	We performed a comparative study of our new probabilistic approach against the empiricallyderived unithood function described in Equation 2." ></td>
	<td class="line x" title="122:157	Two experiments were conducted." ></td>
	<td class="line x" title="123:157	In the first one, we assessed our probabilistically-derived measure OU(s) as described in Equation 14 where the decisions on whether or not to merge the 1,825 pairs are done automatically." ></td>
	<td class="line x" title="124:157	These decisions are known as the actual results." ></td>
	<td class="line x" title="125:157	At the same time, we inspected the same list manually to decide on the merging of all the pairs." ></td>
	<td class="line x" title="126:157	These decisions are known as the ideal results." ></td>
	<td class="line x" title="127:157	The threshold OUT employed for our evaluation is determined empirically through experiments and is set to 8.39." ></td>
	<td class="line x" title="128:157	However, since only one threshold is involved in deciding mergeability, training algorithms and data sets may be employed to automatically decide on an optimal number." ></td>
	<td class="line x" title="129:157	This option is beyond the scope of this paper." ></td>
	<td class="line x" title="130:157	The actual and ideal results for this first experiment are organised into a contingency table (not shown here) for identifying the true and the false positives, and the true and the false negatives." ></td>
	<td class="line x" title="131:157	In the second experiment, we conducted the same assessment as carried out in the first one but the decisions to merge the 1,825 pairs are based on the UH(ax,ay) function described in Equation 2." ></td>
	<td class="line o" title="132:157	The thresholds required for this function are based on the values suggested by (Wong et al., 2007b), namely, MI+ = 0.9, MI = 0.02, IDT = 6, IDR+ = 1.35, and IDR = 0.93." ></td>
	<td class="line x" title="133:157	Table 1: The performance of OU(s) (from Experiment 1) and UH(ax,ay) (from Experiment 2) in terms of precision, recall and accuracy." ></td>
	<td class="line x" title="134:157	The last column shows the difference in the performance of Experiment 1 and 2." ></td>
	<td class="line x" title="135:157	Using the results from the contingency tables, we computed the precision, recall and accuracy for the two measures under evaluation." ></td>
	<td class="line x" title="136:157	Table 1 summarises the performance of OU(s) and UH(ax,ay) in determining the unithood of 1,825 pairs of lexical units." ></td>
	<td class="line x" title="137:157	One will notice that our new measure OU(s) outperformed the empirically-derived function UH(ax,ay) in all aspects, with an improvement of 2.63%, 3.33% and 2.74% for precision, recall and accuracy, respectively." ></td>
	<td class="line x" title="138:157	Our new measure achieved a 100% precision with a lower recall at 95.83%." ></td>
	<td class="line x" title="139:157	As with any measures that employ thresholds as a cutoff point in accepting or rejecting certain decisions, we can improve the recall of OU(s) by decreasing the threshold OUT . In this way, there will be less false negatives (i.e. pairs which are supposed to be merged but are not) and hence, increases the recall rate." ></td>
	<td class="line x" title="140:157	Unfortunately, recall will improve at the expense of precision since the number of false positives will definitely increase from the existing 0." ></td>
	<td class="line x" title="141:157	Since our application (i.e. ontology learning) requires perfect precision in determining the unithood of word sequences, OU(s) is the ideal candidate." ></td>
	<td class="line x" title="142:157	Moreover, with only one threshold (i.e. OUT ) required in controlling the function of OU(s), we are able to reduce the amount of time and effort spent on optimising our results." ></td>
	<td class="line x" title="143:157	5 Conclusion and Future Work In this paper, we highlighted the significance of unithood and that its measurement should be given equal attention by researchers in term extraction." ></td>
	<td class="line x" title="144:157	We focused on the development of a new approach that is independent of influences of termhood measurement." ></td>
	<td class="line x" title="145:157	We proposed a new probabilistically-derived measure which provide a dedicated way to determine the unithood of word sequences." ></td>
	<td class="line x" title="146:157	We refer to this measure as the Odds of Unithood (OU)." ></td>
	<td class="line x" title="147:157	OU is derived using Bayes Theorem and is founded upon two evidences, namely, local occurrence and global occurrence." ></td>
	<td class="line x" title="148:157	Elementary probabilities estimated using page counts from the Google search engine are utilised to quantify the two evidences." ></td>
	<td class="line x" title="149:157	The new probabilistically-derived measure OU is then evaluated against an existing empirical function known as Unithood (UH)." ></td>
	<td class="line x" title="150:157	Our new measure OU achieved a precision and a recall of 100% and 95.83% respectively, with an accuracy at 97.26% in measuring the unithood of 1,825 test cases." ></td>
	<td class="line x" title="151:157	OU outperformed UH by 2.63%, 3.33% and 2.74% in terms of precision, 109 recall and accuracy, respectively." ></td>
	<td class="line x" title="152:157	Moreover, our new measure requires only one threshold, as compared to five in UH to control the mergeability decision." ></td>
	<td class="line x" title="153:157	More work is required to establish the coverage and the depth of the World Wide Web with regards to the determination of unithood." ></td>
	<td class="line x" title="154:157	While the Web has demonstrated reasonable strength in handling general news articles, we have yet to study its appropriateness in dealing with unithood determination for technical text (i.e. the depth of the Web)." ></td>
	<td class="line x" title="155:157	Similarly, it remains a question the extent to which the Web is able to satisfy the requirement of unithood determination for a wider range of genres (i.e. the coverage of the Web)." ></td>
	<td class="line x" title="156:157	Studies on the effect of noises (e.g. keyword spamming) and multiple word senses on unithood determination using the Web is another future research direction." ></td>
	<td class="line x" title="157:157	Acknowledgement This research was supported by the Australian Endeavour International Postgraduate Research Scholarship, and the Research Grant 2006 by the University of Western Australia." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1013
Applying a Grammar-Based Language Model to a Simplified Broadcast-News Transcription Task
Kaufmann, Tobias;Pfister, Beat;"></td>
	<td class="line x" title="1:210	Proceedings of ACL-08: HLT, pages 106113, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:210	c2008 Association for Computational Linguistics Applying a Grammar-based Language Model to a Simplified Broadcast-News Transcription Task Tobias Kaufmann Speech Processing Group ETH Zurich Zurich, Switzerland kaufmann@tik.ee.ethz.ch Beat Pfister Speech Processing Group ETH Zurich Zurich, Switzerland pfister@tik.ee.ethz.ch Abstract We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree." ></td>
	<td class="line x" title="3:210	The language model is applied by means of an N-best rescoring step, which allows to directly measure the performance gains relative to the baseline system without rescoring." ></td>
	<td class="line x" title="4:210	To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task." ></td>
	<td class="line x" title="5:210	We report a significant reduction in word error rate compared to a state-of-the-art baseline system." ></td>
	<td class="line x" title="6:210	1 Introduction It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance." ></td>
	<td class="line x" title="7:210	More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003), Charniak (2000) and Ratnaparkhi (1999)." ></td>
	<td class="line x" title="8:210	Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition." ></td>
	<td class="line x" title="9:210	These models have in common that they explicitly or implicitly use a context-free grammar induced from a treebank, with the exception of Chelba and Jelinek (2000)." ></td>
	<td class="line x" title="10:210	The probability of a rule expansion or parser operation is conditioned on various contextual information and the derivation history." ></td>
	<td class="line x" title="11:210	An important reason for the success of these models is the fact that they are lexicalized: the probability distributions are also conditioned on the actual words occuring in the utterance, and not only on their parts of speech." ></td>
	<td class="line x" title="12:210	Most statistical parsers achieve a high robustness with respect to out-of-grammar sentences by allowing for arbitrary derivations and rule expansions." ></td>
	<td class="line x" title="13:210	On the other hand, they are not suited to reliably decide on the grammaticality of a given phrase, as they do not accurately model the linguistic constraints inherent in natural language." ></td>
	<td class="line x" title="14:210	We take a completely different position." ></td>
	<td class="line x" title="15:210	In the first place, we want our language model to reliably distinguish between grammatical and ungrammatical phrases." ></td>
	<td class="line x" title="16:210	To this end, we have developed a precise, linguistically motivated grammar." ></td>
	<td class="line x" title="17:210	To distinguish between common and uncommon phrases, we use a statistical model that estimates the probability of a phrase based on the syntactic dependencies established by the parser." ></td>
	<td class="line x" title="18:210	We achieve some degree of robustness by letting the grammar accept arbitrary sequences of words and phrases." ></td>
	<td class="line x" title="19:210	To keep the grammar restrictive, such sequences are penalized by the statistical model." ></td>
	<td class="line x" title="20:210	Accurate hand-crafted grammars have been applied to speech recognition before, e.g. Kiefer et al.(2000) and van Noord et al.(1999)." ></td>
	<td class="line x" title="23:210	However, they primarily served as a basis for a speech understanding component and were applied to narrowdomain tasks such as appointment scheduling or public transport information." ></td>
	<td class="line x" title="24:210	We are mainly concerned with speech recognition performance on broad-domain recognition tasks." ></td>
	<td class="line x" title="25:210	Beutler et al.(2005) pursued a similar approach." ></td>
	<td class="line x" title="27:210	106 However, their grammar-based language model did not make use of a probabilistic component, and it was applied to a rather simple recognition task (dictation texts for pupils read and recorded under good acoustic conditions, no out-of-vocabulary words)." ></td>
	<td class="line x" title="28:210	Besides proposing an improved language model, this paper presents experimental results for a much more difficult and realistic task and compares them to the performance of a state-of-the-art baseline system." ></td>
	<td class="line x" title="29:210	In the following Section, we will first describe our grammar-based language model." ></td>
	<td class="line x" title="30:210	Next, we will turn to the linguistic components of the model, namely the grammar, the lexicon and the parser." ></td>
	<td class="line x" title="31:210	We will point out some of the challenges arising from the broad-domain speech recognition application and propose ways to deal with them." ></td>
	<td class="line x" title="32:210	Finally, we will describe our experiments on broadcast news data and discuss the results." ></td>
	<td class="line x" title="33:210	2 Language Model 2.1 The General Approach Speech recognizers choose the word sequence W which maximizes the posterior probability P(W|O), where O is the acoustic observation." ></td>
	<td class="line x" title="34:210	This is achieved by optimizing W = argmax W P(O|W)P(W) ip|W| (1) The language model weight  and the word insertion penalty ip lead to a better performance in practice, but they have no theoretical justification." ></td>
	<td class="line x" title="35:210	Our grammar-based language model is incorporated into the above expression as an additional probability Pgram(W), weighted by a parameter : W = argmax W P(O|W)P(W)Pgram(W)ip|W| (2) Pgram(W) is defined as the probability of the most likely parse tree of a word sequence W: Pgram(W) = max Tparses(W) P(T) (3) To determine Pgram(W) is an expensive operation as it involves parsing." ></td>
	<td class="line x" title="36:210	For this reason, we pursue an N-best rescoring approach." ></td>
	<td class="line x" title="37:210	We first produce the N best hypotheses according to the criterion in equation (1)." ></td>
	<td class="line x" title="38:210	From these hypotheses we then choose the final recognition result according to equation (2)." ></td>
	<td class="line x" title="39:210	2.2 The Probability of a Parse Tree The parse trees produced by our parser are binarybranching and rather deep." ></td>
	<td class="line x" title="40:210	In order to compute the probability of a parse tree, it is transformed to a flat dependency tree similar to the syntax graph representation used in the TIGER treebank Brants et al (2002)." ></td>
	<td class="line x" title="41:210	An inner node of such a dependency tree represents a constituent or phrase." ></td>
	<td class="line x" title="42:210	Typically, it directly connects to a leaf node representing the most important word of the phrase, the head child." ></td>
	<td class="line x" title="43:210	The other children represent phrases or words directly depending on the head child." ></td>
	<td class="line x" title="44:210	To give an example, the immediate children of a sentence node are the finite verb (the head child), the adverbials, the subject and the all other (verbal and non-verbal) complements." ></td>
	<td class="line x" title="45:210	This flat structure has the advantage that the information which is most relevant for the head child is represented within the locality of an inner node." ></td>
	<td class="line x" title="46:210	Assuming statistical independence between the internal structures of the inner nodes ni, we can factor P(T) much like it is done for probabilistic contextfree grammars: P(T)  productdisplay ni P(childtags(ni)|tag(ni)) (4) In the above equation, tag(ni) is simply the label assigned to the tree node ni, and childtags(ni) denotes the tags assigned to the child nodes of ni." ></td>
	<td class="line x" title="47:210	Our statistical model for German sentences distinguishes between eight different tags." ></td>
	<td class="line x" title="48:210	Three tags are used for different types of noun phrases: pronominal NPs, non-pronominal NPs and prenominal genitives." ></td>
	<td class="line x" title="49:210	Prenominal genitives were given a dedicated tag because they are much more restricted than ordinary NPs." ></td>
	<td class="line x" title="50:210	Another two tags were used to distinguish between clauses with sentence-initial finite verbs (main clauses) and clauses with sentence-final finite verbs (subordinate clauses)." ></td>
	<td class="line x" title="51:210	Finally, there are specific tags for infinitive verb phrases, adjective phrases and prepositional phrases." ></td>
	<td class="line x" title="52:210	P was modeled by means of a dedicated probability distribution for each conditioning tag." ></td>
	<td class="line x" title="53:210	The probability of the internal structure of a sentence was modeled as the trigram probability of the corresponding tag sequence (the sequence of the sentence nodes child tags)." ></td>
	<td class="line x" title="54:210	The probability of an adjective phrase was decomposed into the probability 107 of the adjective type (participle or non-participle and attributive, adverbial or predicative) and the probability of its length in words given the adjective type." ></td>
	<td class="line x" title="55:210	This allows the model to directly penalize long adjective phrases, which are very rare." ></td>
	<td class="line x" title="56:210	The model for noun phrases is based on the joint probability of the head type (either noun, adjective or proper name), the presence of a determiner and the presence of preand postnominal modifiers." ></td>
	<td class="line x" title="57:210	The probabilities of various other events are conditioned on those four variables, namely the number of prepositional phrases, relative clauses and adjectives, as well as the presence of appositions and prenominal or postnominal genitives." ></td>
	<td class="line x" title="58:210	The resulting probability distributions were trained on the German TIGER treebank which consists of about 50000 sentences of newspaper text." ></td>
	<td class="line x" title="59:210	2.3 Robustness Issues A major problem of grammar-based approaches to language modeling is how to deal with out-ofgrammar utterances." ></td>
	<td class="line x" title="60:210	Obviously, the utterance to be recognized may be ungrammatical, or it could be grammatical but not covered by the given grammar." ></td>
	<td class="line x" title="61:210	But even if the utterance is both grammatical and covered by the grammar, the correct word sequence may not be among the N best hypotheses due to out-of-vocabulary words or bad acoustic conditions." ></td>
	<td class="line x" title="62:210	In all these cases, the best hypothesis available is likely to be out-of-grammar, but the language model should nevertheless prefer it to competing hypotheses." ></td>
	<td class="line x" title="63:210	To make things worse, it is not unlikely that some of the competing hypotheses are grammatical." ></td>
	<td class="line x" title="64:210	It is therefore important that our language model is robust with respect to out-of-grammar sentences." ></td>
	<td class="line x" title="65:210	In particular this means that it should provide a reasonable parse tree for any possible word sequence W. However, our approach is to use an accurate, linguistically motivated grammar, and it is undesirable to weaken the constraints encoded in the grammar. Instead, we allow the parser to attach any sequence of words or correct phrases to the root node, where each attachment is penalized by the probabilistic model P(T)." ></td>
	<td class="line x" title="66:210	This can be thought of as adding two probabilistic context-free rules: S  Sprime S with probability q S  Sprime with probability 1q In order to guarantee that all possible word sequences are parseable, Sprime can produce both saturated phrases and arbitrary words." ></td>
	<td class="line x" title="67:210	To include such a productive set of rules into the grammar would lead to serious efficiency problems." ></td>
	<td class="line x" title="68:210	For this reason, these rules were actually implemented as a dynamic programming pass: after the parser has identified all correct phrases, the most probable sequence of phrases or words is computed." ></td>
	<td class="line x" title="69:210	2.4 Model Parameters Besides the distributions required to specify P(T), our language model has three parameters: the language model weight , the attachment probability q and the number of hypotheses N. The parameters  and q are considered to be task-dependent." ></td>
	<td class="line x" title="70:210	For instance, if the utterances are well-covered by the grammar and the acoustic conditions are good, it can be expected that  is relatively large and that q is relatively small." ></td>
	<td class="line x" title="71:210	The choice of N is restricted by the available computing power." ></td>
	<td class="line x" title="72:210	For our experiments, we chose N = 100." ></td>
	<td class="line x" title="73:210	The influence of N on the word error rate is discussed in the results section." ></td>
	<td class="line x" title="74:210	3 Linguistic Resources 3.1 Particularities of the Recognizer Output The linguistic resources presented in this Section are partly influenced by the form of the recognizer output." ></td>
	<td class="line x" title="75:210	In particular, the speech recognizer does not always transcribe numbers, compounds and acronyms as single words." ></td>
	<td class="line x" title="76:210	For instance, the word einundzwanzig (twenty-one) is transcribed as ein und zwanzig, Kriegsplane (war plans) as Kriegs Plane and BMW as B. M. W. These transcription variants are considered to be correct by our evaluation scheme." ></td>
	<td class="line x" title="77:210	Therefore, the grammar should accept them as well." ></td>
	<td class="line x" title="78:210	3.2 Grammar and Parser We used the Head-driven Phrase Structure Grammar (HPSG, see Pollard and Sag (1994)) formalism to develop a precise large-coverage grammar for German. HPSG is an unrestricted grammar (Chomsky type 0) which is based on a context-free skeleton and the unification of complex feature structures." ></td>
	<td class="line x" title="79:210	There are several variants of HPSG which mainly differ in the formal tools they provide for stating lin108 guistic constraints." ></td>
	<td class="line x" title="80:210	Our particular variant requires that constituents (phrases) be continuous, but it provides a mechanism for dealing with discontinuities as present e.g. in the German main clause, see Kaufmann and Pfister (2007)." ></td>
	<td class="line x" title="81:210	HPSG typically distinguishes between immediate dominance schemata (rough equivalents of phrase structure rules, but making no assumptions about constituent order) and linear precedence rules (constraints on constituent order)." ></td>
	<td class="line x" title="82:210	We do not make this distinction but rather let immediate dominance schemata specify constituent order." ></td>
	<td class="line x" title="83:210	Further, the formalism allows to express complex linguistic constraints by means of predicates or relational constraints." ></td>
	<td class="line x" title="84:210	At parse time, predicates are backed by program code that can perform arbitrary computations to check or specify feature structures." ></td>
	<td class="line x" title="85:210	We have implemented an efficient Java parser for our variant of the HPSG formalism." ></td>
	<td class="line x" title="86:210	The parser supports ambiguity packing, which is a technique for merging constituents with different derivational histories but identical syntactic properties." ></td>
	<td class="line x" title="87:210	This is essential for parsing long and ambiguous sentences." ></td>
	<td class="line x" title="88:210	Our grammar incorporates many ideas from existing linguistic work, e.g. Muller (2007), Muller (1999), Crysmann (2005), Crysmann (2003)." ></td>
	<td class="line x" title="89:210	In addition, we have modeled a few constructions which occur frequently but are often neglected in formal syntactic theories." ></td>
	<td class="line x" title="90:210	Among them are prenominal and postnominal genitives, expressions of quantity and expressions of date and time." ></td>
	<td class="line x" title="91:210	Further, we have implemented dedicated subgrammars for analyzing written numbers, compounds and acronyms that are written as separate words." ></td>
	<td class="line x" title="92:210	To reduce ambiguity, only noun-noun compounds are covered by the grammar." ></td>
	<td class="line x" title="93:210	Noun-noun compounds are by far the most productive compound type." ></td>
	<td class="line x" title="94:210	The grammar consists of 17 rules for general linguistic phenomena (e.g. subcategorization, modification and extraction), 12 rules for modeling the German verbal complex and another 13 construction-specific rules (relative clauses, genitive attributes, optional determiners, nominalized adjectives, etc.)." ></td>
	<td class="line x" title="95:210	The various subgrammars (expressions of date and time, written numbers, noun-noun compounds and acronyms) amount to a total of 43 rules." ></td>
	<td class="line x" title="96:210	The grammar allows the derivation of intermediate products which cannot be regarded as complete phrases." ></td>
	<td class="line x" title="97:210	We consider complete phrases to be sentences, subordinate clauses, relative and interrogative clauses, noun phrases, prepositional phrases, adjective phrases and expressions of date and time." ></td>
	<td class="line x" title="98:210	3.3 Lexicon The lexicon was created manually based on a list of more than 5000 words appearing in the N-best lists of our experiment." ></td>
	<td class="line x" title="99:210	As the domain of our recognition task is very broad, we attempted to include any possible reading of a given word." ></td>
	<td class="line x" title="100:210	Our main source of dictionary information was Duden (1999)." ></td>
	<td class="line x" title="101:210	Each word was annotated with precise morphological and syntactic information." ></td>
	<td class="line x" title="102:210	For example, the roughly 2700 verbs were annotated with over 7000 valency frames." ></td>
	<td class="line x" title="103:210	We distinguish 86 basic valency frames, for most of which the complement types can be further specified." ></td>
	<td class="line x" title="104:210	A major difficulty was the acquisition of multiword lexemes." ></td>
	<td class="line x" title="105:210	Slightly deviating from the common notion, we use the following definition: A syntactic unit consisting of two or more words is a multiword lexeme, if the grammar cannot derive it from its parts." ></td>
	<td class="line x" title="106:210	English examples are idioms like by and large and phrasal verbs such as to call sth off." ></td>
	<td class="line x" title="107:210	Such multi-word lexemes have to be entered into the lexicon, but they cannot directly be identified in the word list." ></td>
	<td class="line x" title="108:210	Therefore, they have to be extracted from supplementary resources." ></td>
	<td class="line x" title="109:210	For our work, we used a newspaper text corpus of 230M words (Frankfurter Rundschau and Neue Zurcher Zeitung)." ></td>
	<td class="line x" title="110:210	This corpus included only articles which are dated before the first broadcast news show used in the experiment." ></td>
	<td class="line x" title="111:210	In the next few paragraphs we will discuss some types of multiword lexemes and our methods of extracting them." ></td>
	<td class="line x" title="112:210	There is a large and very productive class of German prefix verbs whose prefixes can appear separated from the verb, similar to English phrasal verbs." ></td>
	<td class="line x" title="113:210	For example, the prefix of the verb untergehen (to sink) is separated in das Schiff geht unter (the ship sinks) and attached in weil das Schiff untergeht (because the ship sinks)." ></td>
	<td class="line x" title="114:210	The set of possible valency frames of a prefix verb has to be looked up in a dictionary as it cannot be derived systematically from its parts." ></td>
	<td class="line x" title="115:210	Exploiting the fact that prefixes are attached to their verb under certain circumstances, we extracted a list of prefix verbs from the above newspaper text corpus." ></td>
	<td class="line x" title="116:210	As the number of prefix verbs is 109 very large, a candidate prefix verb was included into the lexicon only if there is a recognizer hypothesis in which both parts are present." ></td>
	<td class="line x" title="117:210	Note that this procedure does not amount to optimizing on test data: when parsing a hypothesis, the parser chart contains only those multiword lexemes for which all parts are present in the hypothesis." ></td>
	<td class="line x" title="118:210	Other multi-word lexemes are fixed word clusters of various types." ></td>
	<td class="line x" title="119:210	For instance, some prepositional phrases appearing in support verb constructions lack an otherwise mandatory determiner, e.g. unter Beschuss (under fire)." ></td>
	<td class="line x" title="120:210	Many multi-word lexemes are adverbials, e.g. nach wie vor (still), auf die Dauer (in the long run)." ></td>
	<td class="line oc" title="121:210	To extract such word clusters we used suffix arrays proposed in Yamamoto and Church (2001) and the pointwise mutual information measure, see Church and Hanks (1990)." ></td>
	<td class="line x" title="122:210	Again, it is feasible to consider only those clusters appearing in some recognizer hypothesis." ></td>
	<td class="line x" title="123:210	The list of candidate clusters was reduced using different filter heuristics and finally checked manually." ></td>
	<td class="line x" title="124:210	For our task, split compounds are to be considered as multi-word lexemes as well." ></td>
	<td class="line x" title="125:210	As our grammar only models noun-noun compounds, other compounds such as unionsgefuhrt (led by the union) have to be entered into the lexicon." ></td>
	<td class="line x" title="126:210	We applied the decompounding algorithm proposed in AddaDecker (2003) to our corpus to extract such compounds." ></td>
	<td class="line x" title="127:210	The resulting candidate list was again filtered manually." ></td>
	<td class="line x" title="128:210	We observed that many proper nouns (e.g. personal names and geographic names) are identical to some noun, adjective or verb form." ></td>
	<td class="line x" title="129:210	For example, about 40% of the nouns in our lexicon share inflected forms with personal names." ></td>
	<td class="line x" title="130:210	Proper nouns considerably contribute to ambiguity, as most of them do not require a determiner." ></td>
	<td class="line x" title="131:210	Therefore, a proper noun which is a homograph of an open-class word was entered only if it is relevant for our task." ></td>
	<td class="line x" title="132:210	The relevant proper nouns were extracted automatically from our text corpus." ></td>
	<td class="line x" title="133:210	We used small databases of unambiguous given names and forms of address to spot personal names in significant bigrams." ></td>
	<td class="line x" title="134:210	Relevant geographic names were extracted by considering capitalized words which significantly often follow certain local prepositions." ></td>
	<td class="line x" title="135:210	The final lexicon contains about 2700 verbs (including 1900 verbs with separable prefixes), 3500 nouns, 450 adjectives, 570 closed-class words and 220 multiword lexemes." ></td>
	<td class="line x" title="136:210	All lexicon entries amount to a total of 137500 full forms." ></td>
	<td class="line x" title="137:210	Noun-noun compounds are not included in these numbers, as they are handled in a morphological analysis component." ></td>
	<td class="line x" title="138:210	4 Experiments 4.1 Experimental Setup The experiment was designed to measure how much a given speech recognition system can benefit from our grammar-based language model." ></td>
	<td class="line x" title="139:210	To this end, we used a baseline speech recognition system which provided the N best hypotheses of an utterance along with their respective scores." ></td>
	<td class="line x" title="140:210	The grammarbased language model was then applied to the N best hypotheses as described in Section 2.1, yielding a new best hypothesis." ></td>
	<td class="line x" title="141:210	For a given test set we could then compare the word error rate of the baseline system with that of the extended system employing the grammar-based language model." ></td>
	<td class="line x" title="142:210	4.2 Data and Preprocessing Our experiments are based on word lattice output from the LIMSI German broadcast news transcription system (McTait and Adda-Decker, 2003), which employs 4-gram backoff language models." ></td>
	<td class="line x" title="143:210	From the experiment reported in McTait and AddaDecker (2003), we used the first three broadcast news shows1 which corresponds to a signal length of roughly 50 minutes." ></td>
	<td class="line x" title="144:210	Rather than applying our model to the original broadcast-news transcription task, we used the above data to create an artificial recognition task with manageable complexity." ></td>
	<td class="line x" title="145:210	Our primary aim was to design a task which allows us to investigate the properties of our grammar-based approach and to compare its performance with that of a competitive baseline system." ></td>
	<td class="line x" title="146:210	As a first simplification, we assumed perfect sentence segmentation." ></td>
	<td class="line x" title="147:210	We manually split the original word lattices at the sentence boundaries and merged them where a sentence crossed a lattice boundary." ></td>
	<td class="line x" title="148:210	This resulted in a set of 636 lattices (sentences)." ></td>
	<td class="line x" title="149:210	Second, we classified the sentences with respect to content type and removed those classes with an excep1The 8 oclock broadcasts of the Tagesschau from the 14th of April, 21st of April and 7th of Mai 2002." ></td>
	<td class="line x" title="150:210	110 tionally high baseline word error rate." ></td>
	<td class="line x" title="151:210	These classes are interviews (a word error rate of 36.1%), sports reports (28.4%) and press conferences (25.7%)." ></td>
	<td class="line x" title="152:210	The baseline word error rate of the remaining 447 lattices (sentences) is 11.8%." ></td>
	<td class="line x" title="153:210	From each of these 447 lattices, the 100 best hypotheses were extracted." ></td>
	<td class="line x" title="154:210	We next compiled a list containing all words present in the recognizer hypotheses." ></td>
	<td class="line x" title="155:210	These words were entered into the lexicon as described in Section 3.3." ></td>
	<td class="line x" title="156:210	Finally, all extracted recognizer hypotheses were parsed." ></td>
	<td class="line x" title="157:210	Only 25 of the 44000 hypotheses2 caused an early termination of the parser due to the imposed memory limits." ></td>
	<td class="line x" title="158:210	However, the inversion of ambiguity packing (see Section 3.2) turned out to be a bottleneck." ></td>
	<td class="line x" title="159:210	As P(T) does not directly apply to parse trees, all possible readings have to be unpacked." ></td>
	<td class="line x" title="160:210	For 24 of the 447 lattices, some of the N best hypotheses contained phrases with more than 1000 readings." ></td>
	<td class="line x" title="161:210	For these lattices the grammar-based language model was simply switched off in the experiment, as no parse trees were produced for efficiency reasons." ></td>
	<td class="line x" title="162:210	To assess the difficulty of our task, we inspected the reference transcriptions, the word lattices and the N-best lists for the 447 selected utterances." ></td>
	<td class="line x" title="163:210	We found that for only 59% of the utterances the correct transcription is among the 100-best hypotheses." ></td>
	<td class="line x" title="164:210	The first-best hypothesis is completely correct for 34% of the utterances." ></td>
	<td class="line x" title="165:210	The out-of-vocabulary rate (estimated from the number of reference transcription words which do not appear in any of the lattices) is 1.7%." ></td>
	<td class="line x" title="166:210	The first-best word error rate is 11.79%, and the 100-best oracle word error rate is 4.8%." ></td>
	<td class="line x" title="167:210	We further attempted to judge the grammaticality of the reference transcriptions." ></td>
	<td class="line x" title="168:210	We considered only 1% of the sentences to be clearly ungrammatical." ></td>
	<td class="line x" title="169:210	19% of the remaining sentences were found to contain general grammatical constructions which are not handled by our grammar." ></td>
	<td class="line x" title="170:210	Some of these constructions (most notably ellipses, which are omnipresent in broadcast-news reports) are notoriously difficult as they would dramatically increase ambiguity when implemented in a grammar." ></td>
	<td class="line x" title="171:210	About 45% of the reference sentences were correctly analyzed by the grammar." ></td>
	<td class="line x" title="172:210	2Some of the word lattices contain less than 100 different hypotheses." ></td>
	<td class="line x" title="173:210	4.3 Training and Testing The parameter N, the maximum number of hypotheses to be considered, was set to 100 (the effect of choosing different values of N will be discussed in section 4.4)." ></td>
	<td class="line x" title="174:210	The remaining parameters  and q were trained using the leave-one-out crossvalidation method: each of the 447 utterances served as the single test item once, whereas the remaining 446 utterances were used for training." ></td>
	<td class="line x" title="175:210	As the error landscape is complex and discrete, we could not use gradient-based optimization methods." ></td>
	<td class="line x" title="176:210	Instead, we chose  and q from 500 equidistant points within the intervals [0,20] and [0,0.25], respectively." ></td>
	<td class="line x" title="177:210	The word error rate was evaluated for each possible pair of parameter values." ></td>
	<td class="line x" title="178:210	The evaluation scheme was taken from McTait and Adda-Decker (2003)." ></td>
	<td class="line x" title="179:210	It ignores capitalization, and written numbers, compounds and acronyms need not be written as single words." ></td>
	<td class="line x" title="180:210	4.4 Results As shown in Table 1, the grammar-based language model reduced the word error rate by 9.2% relative over the baseline system." ></td>
	<td class="line x" title="181:210	This improvement is statistically significant on a level of < 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemars test (Gillick and Cox, 1989)." ></td>
	<td class="line x" title="182:210	If the parameters are optimized on all 447 sentences (i.e. on the test data), the word error rate is reduced by 10.7% relative." ></td>
	<td class="line x" title="183:210	For comparison, we redefined the probabilistic model as P(T) = (1q)qk1, where k is the number of phrases attached to the root node." ></td>
	<td class="line x" title="184:210	This reduced model only considers the grammaticality of a phrase, completely ignoring the probability of its internal structure." ></td>
	<td class="line x" title="185:210	It achieved a relative word error reduction of 5.9%, which is statistically significant on a level of < 0.1% for both tests." ></td>
	<td class="line x" title="186:210	The improvement of the full model compared to the reduced model is weakly significant on a level of 2.6% for the MAPSSWE test." ></td>
	<td class="line x" title="187:210	For both models, the optimal value of q was 0.001 for almost all training runs." ></td>
	<td class="line x" title="188:210	The language model weight  of the reduced model was about 60% smaller than the respective value for the full model, which confirms that the full model provides more reliable information." ></td>
	<td class="line x" title="189:210	111 experiment word error rate baseline 11.79% grammar, no statistics 11.09% (-5.9% rel.)" ></td>
	<td class="line x" title="190:210	grammar 10.70% (-9.2% rel.)" ></td>
	<td class="line x" title="191:210	grammar, cheating 10.60% (-10.7% rel.)" ></td>
	<td class="line x" title="192:210	100-best oracle 4.80% Table 1: The impact of the grammar-based language model on the word error rate." ></td>
	<td class="line x" title="193:210	For comparison, the results for alternative experiments are shown." ></td>
	<td class="line x" title="194:210	In the experiment grammar, cheating, the parameters were optimized on test data." ></td>
	<td class="line x" title="195:210	Figure 1 shows the effect of varying N (the maximum number of hypotheses) on the word error rate both for leave-one-out training and for optimizing the parameters on test data." ></td>
	<td class="line x" title="196:210	The similar shapes of the two curves suggest that the observed variations are partly due to the problem structure." ></td>
	<td class="line x" title="197:210	In fact, if N is increased and new hypotheses with a high value of Pgram(W) appear, the benefit of the grammarbased language model can increase (if the hypotheses are predominantly good with respect to word error rate) or decrease (if they are bad)." ></td>
	<td class="line x" title="198:210	This horizon effect tends to be reduced with increasing N (with the exception of 89  N  93) because hypotheses with high ranks need a much higher Pgram(W) in order to compensate for their lower value of P(O|W)P(W)." ></td>
	<td class="line x" title="199:210	For small N, the parameter estimation is more severely affected by the rather accidental horizon effects and therefore is prone to overfitting." ></td>
	<td class="line x" title="200:210	5 Conclusions and Outlook We have presented a language model based on a precise, linguistically motivated grammar, and we have successfully applied it to a difficult broad-domain task." ></td>
	<td class="line x" title="201:210	It is a well-known fact that natural language is highly ambiguous: a correct and seemingly unambiguous sentence may have an enormous number of readings." ></td>
	<td class="line x" title="202:210	A related  and for our approach even more relevant  phenomenon is that many weirdlooking and seemingly incorrect word sequences are in fact grammatical." ></td>
	<td class="line x" title="203:210	This obviously reduces the benefit of pure grammaticality information." ></td>
	<td class="line x" title="204:210	A solution is to use additional information to asses how natural a reading of a word sequence is. We have done a 0 20 40 60 80 10012 10 8 6 4 2 0 N WER (relative)   leaveoneout optimized on test data Figure 1: The word error rate as a function of the maximum number of best hypotheses N. first step in this direction by estimating the probability of a parse tree." ></td>
	<td class="line x" title="205:210	However, our model only looks at the structure of a parse tree and does not take the actual words into account." ></td>
	<td class="line x" title="206:210	As N-grams and statistical parsers demonstrate, word information can be very valuable." ></td>
	<td class="line x" title="207:210	It would therefore be interesting to investigate ways of introducing word information into our grammar-based model." ></td>
	<td class="line x" title="208:210	Acknowledgements This work was supported by the Swiss National Science Foundation." ></td>
	<td class="line x" title="209:210	We cordially thank Jean-Luc Gauvain of LIMSI for providing us with word lattices from their German broadcast news transcription system." ></td>
	<td class="line x" title="210:210	112" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-2046
Evolving New Lexical Association Measures Using Genetic Programming
Å najder, Jan;Dalbelo BaÅ¡iÄ‡, Bojana;PetroviÄ‡, SaÅ¡a;SikiriÄ‡, Ivan;"></td>
	<td class="line x" title="1:93	Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 181184, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:93	c2008 Association for Computational Linguistics Evolving new lexical association measures using genetic programming Jan Snajder Bojana Dalbelo Basic Sasa Petrovic Ivan Sikiric Faculty of Electrical Engineering and Computing, University of Zagreb Unska 3, Zagreb, Croatia {jan.snajder, bojana.dalbelo, sasa.petrovic, ivan.sikiric}@fer.hr Abstract Automatic extraction of collocations from large corpora has been the focus of many research efforts." ></td>
	<td class="line x" title="3:93	Most approaches concentrate on improving and combining known lexical association measures." ></td>
	<td class="line x" title="4:93	In this paper, we describe a genetic programming approach for evolving new association measures, which is not limited to any specific language, corpus, or type of collocation." ></td>
	<td class="line x" title="5:93	Our preliminary experimental results show that the evolved measures outperform three known association measures." ></td>
	<td class="line x" title="6:93	1 Introduction A collocation is an expression consisting of two or more words that correspond to some conventional way of saying things (Manning and Schutze, 1999)." ></td>
	<td class="line x" title="7:93	Related to the term collocation is the term n-gram, which is used to denote any sequence of n words." ></td>
	<td class="line x" title="8:93	There are many possible applications of collocations: automatic language generation, word sense disambiguation, improving text categorization, information retrieval, etc. As different applications require different types of collocations that are often not found in dictionaries, automatic extraction of collocations from large textual corpora has been the focus of much research in the last decade; see, for example, (Pecina and Schlesinger, 2006; Evert and Krenn, 2005)." ></td>
	<td class="line x" title="9:93	Automatic extraction of collocations is usually performed by employing lexical association measures (AMs) to indicate how strongly the words comprising an n-gram are associated." ></td>
	<td class="line x" title="10:93	However, the use of lexical AMs for the purpose of collocation extraction has reached a plateau; recent research in this field has focused on combining the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006)." ></td>
	<td class="line x" title="11:93	In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming." ></td>
	<td class="line x" title="12:93	A similar approach has been usefully applied in text mining (Atkinson-Abutridy et al., 2004) as well as in information retrieval (Gordon et al., 2006)." ></td>
	<td class="line x" title="13:93	Genetic programming is an evolutionary computational technique designed to mimic the process of natural evolution for the purpose of solving complex optimization problems by stochastically searching through the whole space of possible solutions (Koza, 1992)." ></td>
	<td class="line x" title="14:93	The search begins from an arbitrary seed of possible solutions (the initial population), which are then improved (evolved) through many iterations by employing the operations of selection, crossover, and mutation." ></td>
	<td class="line x" title="15:93	The process is repeated until a termination criterion is met, which is generally defined by the goodness of the best solution or the expiration of a time limit." ></td>
	<td class="line x" title="16:93	2 Genetic programming of AMs 2.1 AM representation In genetic programming, possible solutions (in our case lexical AMs) are mathematical expressions represented by a tree structure (Koza, 1992)." ></td>
	<td class="line x" title="17:93	The leaves of the tree can be constants, or statistical or linguistic information about an n-gram." ></td>
	<td class="line x" title="18:93	A constant can be any real number in an arbitrarily chosen interval; our experiments have shown that variation of this interval does not affect the performance." ></td>
	<td class="line x" title="19:93	One special constant that we use is the number of words in the corpus." ></td>
	<td class="line x" title="20:93	The statistical information about an n-gram can be the frequency of any part of the n-gram." ></td>
	<td class="line x" title="21:93	For ex181 ample, for a trigram abc the statistical information can be the frequency f(abc) of the whole trigram, frequencies f(ab) and f(bc) of the digrams, and the frequencies of individual words f(a), f(b), and f(c)." ></td>
	<td class="line x" title="22:93	The linguistic information about an n-gram is the part-of-speech (POS) of any one of its words." ></td>
	<td class="line x" title="23:93	Inner nodes in the tree are operators." ></td>
	<td class="line x" title="24:93	The binary operators are addition, subtraction, multiplication, and division." ></td>
	<td class="line x" title="25:93	We also use one unary operator, the natural logarithm, and one ternary operator, the IFTHEN-ELSE operator." ></td>
	<td class="line x" title="26:93	The IF-THEN-ELSE node has three descendant nodes: the left descendant is the condition in the form i-th word of the n-gram has a POS tag T, and the other two descendants are operators or constants." ></td>
	<td class="line x" title="27:93	If the condition is true, then the subexpression corresponding to the middle descendant is evaluated, otherwise the subexpression corresponding to the right descendant is evaluated." ></td>
	<td class="line x" title="28:93	The postfix expression of an AM can be obtained by traversing its tree representation in postorder." ></td>
	<td class="line x" title="29:93	Figure 1 shows the representation of the Dice coefficient using our representation." ></td>
	<td class="line x" title="30:93	2.2 Genetic operators The crossover operator combines two parent solutions into a new solution." ></td>
	<td class="line x" title="31:93	We defined the crossover operator as follows: from each of the two parents, one node is chosen randomly, excluding any nodes that represent the condition of the IF-THEN-ELSE operator." ></td>
	<td class="line x" title="32:93	A new solution is obtained by replacing the subtree of the chosen node of the first parent with the subtree of the chosen node of the second parent." ></td>
	<td class="line x" title="33:93	This method of defining the crossover operator is the same as the one described in (Gordon et al., 2006)." ></td>
	<td class="line x" title="34:93	The mutation operator introduces new genetic material into a population by randomly changing a solution." ></td>
	<td class="line x" title="35:93	In our case, the mutation operator can do one of two things: either remove a randomly selected inner node (with probability of 25%), or insert an inner node at a random position in the tree (with probability of 75%)." ></td>
	<td class="line x" title="36:93	If a node is being removed from the tree, one of its descendants (randomly chosen) takes its place." ></td>
	<td class="line x" title="37:93	An exception to this rule is the IF-THEN-ELSE operator, which cannot be replaced by its condition node." ></td>
	<td class="line x" title="38:93	If a node is being inserted, a randomly created operator node replaces an existing node that then becomes a descendant of the new node." ></td>
	<td class="line x" title="39:93	If the inserted node is not a unary operator, the required number of random leaves is created." ></td>
	<td class="line x" title="40:93	The selection operator is used to copy the best solutions into the next iteration." ></td>
	<td class="line x" title="41:93	The goodness of the solution is determined by the fitness function, which assigns to each solution a number indicating how good that particular solution actually is. We measure the goodness of an AM in terms of its F1 score, obtained from the precision and recall computed on a random sample consisting of 100 positive n-grams (those considered collocations) and 100 negative ngrams (non-collocations)." ></td>
	<td class="line x" title="42:93	These n-grams are ranked according to the AM value assigned to them, after which we compute the precision and recall by considering first n best-ranked n-grams as positives and the rest as negatives, repeating this for each n between 1 and 200." ></td>
	<td class="line x" title="43:93	The best F1 score is then taken as the AMs goodness." ></td>
	<td class="line x" title="44:93	Using the previous definition of the fitness function, preliminary experiments showed that solutions soon become very complex in terms of number of nodes in the tree (namely, on the order of tens of thousands)." ></td>
	<td class="line x" title="45:93	This is a problem both in terms of space and time efficiency; allowing unlimited growth of the tree quickly consumes all computational resources." ></td>
	<td class="line x" title="46:93	Also, it is questionable whether the performance benefits from the increased size of the solution." ></td>
	<td class="line x" title="47:93	Thus, we modified the fitness function to also take into account the size of the tree (that is, the less nodes a tree has, the better)." ></td>
	<td class="line x" title="48:93	Favoring shorter solutions at the expense of some loss in performance is known as parsimony, and it has already been successfully used in genetic programming (Koza, 1992)." ></td>
	<td class="line x" title="49:93	Therefore, the final formula for the fitness function we used incorporates the parsimony factor and is given by fitness(j) = F1(j) + Lmax L(j)L max , (1) where F1(j) is the F1 score (ranging from 0 to 1) of the solution j,  is the parsimony factor, Lmax is the maximal size (measured in number of nodes), and L(j) is the size of solution j. By varying  we can control how much loss of performance we will tolerate in order to get smaller, more elegant solutions." ></td>
	<td class="line x" title="50:93	Genetic programming algorithms usually iterate until a termination criterion is met." ></td>
	<td class="line x" title="51:93	In our case, the algorithm terminates when a certain number, k, of iterations has passed without an improvement in the 182 Dice(a,b,c) = f(abc)f(a)+f(b)+f(c) Figure 1: Dice coefficient for digrams represented by tree results." ></td>
	<td class="line x" title="52:93	To prevent the overfitting problem, we measure this improvement on another sample (validation sample) that also consists of 100 collocations and 100 non-collocations." ></td>
	<td class="line x" title="53:93	3 Preliminary results 3.1 Experimental setting We use the previously described genetic programming approach to evolve AMs for extracting collocations consisting of three words from a corpus of 7008 Croatian legislative documents." ></td>
	<td class="line x" title="54:93	Prior to this, words from the corpus were lemmatized and POS tagged." ></td>
	<td class="line x" title="55:93	Conjunctions, propositions, pronouns, interjections, and particles were treated as stop-words and tagged with a POS tag X. N-grams starting or ending with a stopword, or containing a verb, were filtered out." ></td>
	<td class="line x" title="56:93	For evaluation purposes we had a human expert annotate 200 collocations and 200 noncollocations, divided into the evaluation and validation sample." ></td>
	<td class="line x" title="57:93	We considered an n-gram to be a collocation if it is a compound noun, terminological expression, or a proper name." ></td>
	<td class="line x" title="58:93	Note that we could have adopted any other definition of a collocation, since this definition is implicit in the samples provided." ></td>
	<td class="line x" title="59:93	In our experiments, we varied a number of genetic programming parameters." ></td>
	<td class="line x" title="60:93	The size of the initial population varied between 50 and 50 thousand randomly generated solutions." ></td>
	<td class="line oc" title="61:93	To examine the effects of including some known AMs on the performance, the following AMs had a 50% chance of being included in the initial population: pointwise mutual information (Church and Hanks, 1990), the Dice coefficient, and the heuristic measure defined in (Petrovic et al., 2006): H(a,b,c) =    2log f(abc)f(a)f(c) if POS(b) = X, log f(abc)f(a)f(b)f(c) otherwise." ></td>
	<td class="line x" title="62:93	For the selection operator we used the well-known three-tournament selection." ></td>
	<td class="line x" title="63:93	The probability of mutation was chosen from the interval [0.0001,0.3], and the parsimony factor  from the interval [0,0.05], thereby allowing a maximum of 5% loss of F1 in favor of smaller solutions." ></td>
	<td class="line x" title="64:93	The maximal size of the tree in nodes was chosen from the interval [20,1000]." ></td>
	<td class="line x" title="65:93	After the F1 score for the validation sample began dropping, the algorithm would continue for another k iterations before stopping." ></td>
	<td class="line x" title="66:93	The parameter k was chosen from the interval [104,107]." ></td>
	<td class="line x" title="67:93	The experiments were run with 800 different random combinations of the aforementioned parameters." ></td>
	<td class="line x" title="68:93	3.2 Results Around 20% of the evolved measures (that is, the solutions that remained after the algorithm terminated) achieved F1 scores of over 80% on both the evaluation and validation samples." ></td>
	<td class="line x" title="69:93	This proportion was 13% in the case when the initial population did not include any known AMs, and 23% in the case when it did, thus indicating that including known AMs in the initial population is beneficial." ></td>
	<td class="line x" title="70:93	The overall best solution had 205 nodes and achieved an F1 score of 88.4%." ></td>
	<td class="line x" title="71:93	In search of more elegant AMs, we singled out solutions that had less than 30 nodes." ></td>
	<td class="line x" title="72:93	Among these, a solution that consisted of 13 nodes achieved the highest F1." ></td>
	<td class="line x" title="73:93	This measure is given by M13(a,b,c) =    0.423f(a)f(c)f2(abc) if POS(b) = X, 1 f(b)f(abc) otherwise." ></td>
	<td class="line x" title="74:93	The association measure M13 is particularly interesting because it takes into account whether the middle word in a trigram is a stopword (denoted by the POS tag X)." ></td>
	<td class="line x" title="75:93	This supports the claim laid out in (Petrovic et al., 2006) that the trigrams containing stopwords (e.g., cure for cancer) should be treated differently, in that the frequency of the stopword should be ignored." ></td>
	<td class="line x" title="76:93	It is important to note that the aforementioned measure H was not included in the initial population from which M13 evolved." ></td>
	<td class="line x" title="77:93	It is also worthwhile noting that in such populations, out of 100 best evolved measures, all but four of them featured a condition identical to that of M13 (POS(b) = X)." ></td>
	<td class="line x" title="78:93	In other words, the majority of the measures evolved this condition completely independently, without H being included in the initial population." ></td>
	<td class="line o" title="79:93	183 1 2 3 4 5 6 7 8 9 100 10 20 30 40 50 60 70 80 90 100 Number of ngrams ( 105) F 1  score   Dice PMI H M13 M205 Figure 2: Comparison of association measures on a corpus of 7008 Croatian documents Figure 2 shows the comparison of AMs in terms of their F1 score obtained on the corpus of 7008 documents." ></td>
	<td class="line x" title="80:93	The x axis shows the number of n best ranked n-grams that are considered positives (we show only the range of n in which all the AMs achieve their maximum F1; all measures tend to perform similarly with increasing n)." ></td>
	<td class="line x" title="81:93	The maximum F1 score is achieved if we take 5  105 n-grams ranked best by the M205 measure." ></td>
	<td class="line x" title="82:93	From Fig." ></td>
	<td class="line x" title="83:93	2 we can see that the evolved AMs M13 and M205 outperformed the other three considered AMs." ></td>
	<td class="line x" title="84:93	For example, collocations kosilica za travu (lawn mower) and digitalna obrada podataka (digital data processing) were ranked at the 22th and 34th percentile according to Dice, whereas they were ranked at the 97th and 87th percentile according to M13." ></td>
	<td class="line x" title="85:93	4 Conclusion In this paper we described a genetic programming approach for evolving new lexical association measures in order to extract collocations." ></td>
	<td class="line x" title="86:93	The evolved association measure will perform at least as good as any other AM included in the initial population." ></td>
	<td class="line x" title="87:93	However, the evolved association measure may be a complex expression that defies interpretation, in which case it may be treated as a blackbox suitable for the specific task of collocation extraction." ></td>
	<td class="line x" title="88:93	Our approach only requires an evaluation sample, thus it is not limited to any specific type of collocation, language or corpus." ></td>
	<td class="line x" title="89:93	The preliminary experiments, conducted on a corpus of Croatian documents, showed that the best evolved measures outperformed other considered association measures." ></td>
	<td class="line x" title="90:93	Also, most of the best evolved association measures took into account the linguistic information about an n-gram (the POS of the individual words)." ></td>
	<td class="line x" title="91:93	As part of future work, we intend to apply our approach to corpora in other languages and compare the results with existing collocation extraction systems." ></td>
	<td class="line x" title="92:93	We also intend to apply our approach to collocations consisting of more than three words, and to experiment with additional linguistic features." ></td>
	<td class="line x" title="93:93	Acknowledgments This work has been supported by the Government of Republic of Croatia, and Government of Flanders under the grant No. 036-1300646-1986 and KRO/009/06." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-1901"></td>
	<td class="line x" title="1:191	Coling 2008 22nd International Conference on Computational Linguistics Proceedings of the workshop on Cognitive Aspects of the Lexicon Workshop chairs: Michael ZOCK and Chu-Ren HUANG 24 August 2008 Manchester, UK c2008 The Coling 2008 Organizing Committee Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Nonported license http://creativecommons.org/licenses/by-nc-sa/3.0/ Some rights reserved Order copies of this and other Coling proceedings from: Association for Computational Linguistics (ACL) 209 N. Eighth Street Stroudsburg, PA 18360 USA Tel: +1-570-476-8006 Fax: +1-570-476-0860 acl@aclweb.org ISBN 978-1-905593-56-9 Design by Chimney Design, Brighton, UK Production and manufacture by One Digital, Brighton, UK ii Preface Information access and exchange play a major role in our globalized world." ></td>
	<td class="line x" title="2:191	Hence, building resources (lexica, thesauri, ontologies or annotated corpora) and providing access to words become an important goal." ></td>
	<td class="line x" title="3:191	The lexicon is a vital resource for building applications." ></td>
	<td class="line x" title="4:191	It is also a crucial element in the study of human language processing." ></td>
	<td class="line x" title="5:191	The spirit of this workshop multidisciplinary, the goal being to gather experts with various backgrounds and to allow them to exchange ideas, to compare their methodologies and theoretical perspectives, to create synergy, and to encourage future collaborations." ></td>
	<td class="line x" title="6:191	In sum, the participants will be discussing questions concerning the cognitive aspects of the lexicon, and their answers should guide the design of on-line dictionaries." ></td>
	<td class="line x" title="7:191	While completeness is a virtue, the quality of a dictionary depends not only on coverage (number of entries) and granularity, but also on accessibility of information." ></td>
	<td class="line x" title="8:191	Access strategies vary with the task (text understanding vs. text production) and the knowledge available at the moment of consultation (word, concept, sound)." ></td>
	<td class="line x" title="9:191	Unlike readers, who look for meanings, writers start from them, searching for the right words." ></td>
	<td class="line x" title="10:191	While paper dictionaries are static, permitting only limited strategies for accessing information, their electronic counterparts promise dynamic, proactive search via multiple criteria (meaning, sound, related word) and via diverse access routes." ></td>
	<td class="line x" title="11:191	Navigation takes place in a huge conceptual-lexical space, and the results are displayable in a multitude of forms (as trees, as lists, as graphs, or sorted alphabetically, by topic, by frequency)." ></td>
	<td class="line x" title="12:191	Many lexicographers work nowadays with huge digital corpora, using language technology to build and to maintain the resource." ></td>
	<td class="line x" title="13:191	But access to the potential wealth in dictionaries remains limited for the common user." ></td>
	<td class="line x" title="14:191	Yet, the new possibilities of electronic media in terms of comfort, speed and flexibility (multiple inputs, polymorph outputs) are enormous and probably beyond our imagination." ></td>
	<td class="line x" title="15:191	More than just allowing electronic versions of paper-bound dictionaries, computers provide a freedom for rethinking dictionaries, thesauri, encyclopedia, etc., a distinction necessary in the past for economical reasons, but not justified anymore." ></td>
	<td class="line x" title="16:191	The goal of this workshop is to perform the groundwork for the next generation of electronic dictionaries, that is, to study the possibility of integrating the different resources, as well as to explore the feasibility of taking the users needs, knowledge and access strategies into account." ></td>
	<td class="line x" title="17:191	To reach this goal we have asked authors to address one or more of the following: 1." ></td>
	<td class="line x" title="18:191	Conceptual input of a dictionary user: what is present in speakers/writers minds when they are generating a message and looking for a (target) word?" ></td>
	<td class="line x" title="19:191	Does the user have in mind conceptual primitives, semantically related words, some type of partial definition, something like synsets, or something completely different?" ></td>
	<td class="line x" title="20:191	2." ></td>
	<td class="line x" title="21:191	Access, navigation and search strategies: how can search be supported by taking into account prior, i.e. available knowledge?" ></td>
	<td class="line x" title="22:191	Entries should be accessible in many ways: by word forms, by meaning, by sounds (syllables), or in a combined form, and this even if input is given in an incomplete, imprecise or degraded form." ></td>
	<td class="line x" title="23:191	The more precise the conceptual input, the less iii navigation should be needed and vice versa." ></td>
	<td class="line x" title="24:191	How can we create manageable search spaces, and provide a user with the tools for navigating within them?" ></td>
	<td class="line x" title="25:191	3." ></td>
	<td class="line x" title="26:191	Indexing words and organizing the lexicon: Words and concepts can be organized in many ways, varying according to typology and conceptual systems." ></td>
	<td class="line x" title="27:191	For example, words are traditionally organized alphabetically in Western languages, but by semantic radicals and stroke counts in Chinese." ></td>
	<td class="line x" title="28:191	The way words and concepts are organized affects indexing and access." ></td>
	<td class="line x" title="29:191	Indexing must robustly allow for multiple ways of navigation and access." ></td>
	<td class="line x" title="30:191	What efficient organizational principles allow the greatest flexibility for access?" ></td>
	<td class="line x" title="31:191	What about lexical entry standardization?" ></td>
	<td class="line x" title="32:191	Are universal definitions possible?" ></td>
	<td class="line x" title="33:191	What about efforts such as the Lexical Markup Framework (LMF) and other global structures for the lexicon?" ></td>
	<td class="line x" title="34:191	Can ontologies be combined with standards for the lexicon?" ></td>
	<td class="line x" title="35:191	4." ></td>
	<td class="line x" title="36:191	NLP Applications: Contributors can also address the issue of how such enhanced dictionaries, once embedded in existing NLP applications, can boost performance and help solve lexical and textual-entailment problems such as those evaluated in SEMEVAL 2007, or, more generally, generation problems encountered in the context of summarization, question-answering, interactive paraphrasing or translation." ></td>
	<td class="line x" title="37:191	Weve received 18 papers, of which 6 were accepted as full papers, while 8 were chosen as poster presentations." ></td>
	<td class="line x" title="38:191	While we did not get papers on all the issues mentioned in our call, we did get a quite rich panel on ideas as divers as use of ontologies; sense extraction; computation of associative responses to multi-word stimuli; saliency relations; lexical relationships within collocations and word association norms; cognitive organization of dictionaries; user-adapted views on a lexicographic database; access based on conceptual input; search in onomasiological dictionaries, access based on underspecified input; dictionary use for authoring aids or MT, use of feature vectors, corpora and machine learning, etc It was also interesting to see the variety of languages in which these issues are addressed." ></td>
	<td class="line x" title="39:191	The proposals range from Japanese, English, German, Russian, Dutch, Bulgarian, Romanian, Spanish, to French and Chinese." ></td>
	<td class="line x" title="40:191	In sum, the community working on dictionaries is dynamic, and there seems to be a growing awareness of the importance of some of the problems presented in our call for papers." ></td>
	<td class="line x" title="41:191	We would like to express here our sincerest thanks to all the specialists who have assisted us to assure a good selection of papers, despite the very tight schedule." ></td>
	<td class="line x" title="42:191	Their reviews were helpful not only for us as decision makers, but also for the authors, helping them to improve their work." ></td>
	<td class="line x" title="43:191	In the hope that the results will inspire you, provoke fruitful discussions and result in future collaborations." ></td>
	<td class="line x" title="44:191	Michael Zock and Chu-Ren Huang iv Organizers: Michael Zock,LIF, CNRS, Marseille,(France) Chu-Ren Huang, Sinica, (Taiwan) Programme Committee: Slaven Bilac, Google-Tokyo, (Japan) Pierrette Bouillon, ISSCO, Geneva, (Switzerland) Dan Cristea, University of Iasi, (Romania) Christiane Fellbaum, Princeton, (USA) Olivier Ferret, CEA LIST, (France) Thierry Fontenelle, Microsoft, Redmont, (USA) Gregory Grefenstette, CEA LIST, (France) Graeme Hirst, University of Toronto, (Canada) Ed Hovy, ISI, Los Angeles, (USA) Chu-Ren Huang, Sinica, (Taiwan) Terry Joyce, Tama University, Kanagawa-ken, (Japan) Adam Kilgarriff, Brighton, Lexical Computing Ltd, (UK) Philippe Langlais, University of Montreal, (Canada) Dekang Lin, Google, Mountain View, California, (USA) Rada Mihalcea, University of North Texas, (USA) Alain Polgu`ere, University of Montreal, (Canada) Reinhard Rapp, University of Tarragona, (Spain) Sabine Schulte im Walde, University of Stuttgart, (Germany) Gilles Serasset, Imag, Grenoble, (France) Anna Sinopalnikova, FIT, BUT, Brno, (Czech Republic) Takenobu Tokunaga, Titech, Tokyo, (Japan) Dan Tufis, RACAI, Bucharest, (Romania) Jean Veronis, Universite dAix-Marseille, (France) Yorick Wilks, Oxford Internet Institute, (UK) Michael Zock, LIF, CNRS, Marseille, (France) Pierre Zweigenbaum, Limsi, Orsay, (France) v  Table of Contents Comparing Lexical Relationships Observed within Japanese Collocation Data and Japanese Word Association Norms Terry Joyce and Irena Srdanovic . . ." ></td>
	<td class="line x" title="45:191	1 Lexical access based on underpecified input Michael Zock and Schwab Didier . . ." ></td>
	<td class="line x" title="46:191	9 Accessing the ANW Dictionary Fons Moerdijk, Carole Tiberius and Jan Niestadt . . ." ></td>
	<td class="line x" title="47:191	18 ProPOSEL: a human-oriented prosody and PoS English lexicon for machine-learning and NLP Claire Brierley and Eric Atwell." ></td>
	<td class="line x" title="48:191	.25 Natural Language Searching in Onomasiological Dictionaries Gerardo Sierra." ></td>
	<td class="line x" title="49:191	.32 First ideas of user-adapted views of lexicographic data exemplified on OWID and elexiko Carolin Moller-Spitzer and Christine Mohrs . . ." ></td>
	<td class="line x" title="50:191	39 Multilingual Conceptual Access to Lexicon based on Shared Orthography: An ontology-driven study of Chinese and Japanese Chu-Ren Huang, Ya-Min Chou, Chiyo Hotani, Sheng-Yi Chen and Wan-Ying Lin." ></td>
	<td class="line x" title="51:191	.47 Extracting Sense Trees from the Romanian Thesaurus by Sense Segmentation & Dependency Parsing Neculai Curteanu, Alex Moruz and Diana Trandabat . . ." ></td>
	<td class="line x" title="52:191	55 Lexical-Functional Correspondences and Their Use in the System of Machine Translation ETAP-3 Andreyeva Sasha . . ." ></td>
	<td class="line x" title="53:191	64 The Close-Distant Relation of Adjectival Concepts Based on Self-Organizing Map Kyoko Kanzaki, Noriko Tomuro and Hitoshi Isahara . . ." ></td>
	<td class="line x" title="54:191	73 Looking up phrase rephrasings via a pivot language Aurelien Max and Michael Zock . . ." ></td>
	<td class="line x" title="55:191	77 Toward a cognitive organization for electronic dictionaries, the case for semantic proxemy Bruno Gaume, Karine Duvignau, Laurent Prevot and Yann Desalle . . ." ></td>
	<td class="line x" title="56:191	86 Cognitively Salient Relations for Multilingual Lexicography Gerhard Kremer, Andrea Abel and Marco Baroni . . ." ></td>
	<td class="line x" title="57:191	94 The Computation of Associative Responses to Multiword Stimuli Reinhard Rapp . . ." ></td>
	<td class="line x" title="58:191	102 vii  Conference Programme Sunday, August 24, 2008 9:009:10 Opening Remarks Session 1: Regular Talks 9:109:50 Comparing Lexical Relationships Observed within Japanese Collocation Data and Japanese Word Association Norms Terry Joyce and Irena Srdanovic 10:5010:30 Lexical access based on underpecified input Michael Zock and Schwab Didier 10:3011:00 Cofee Break + Poster Installation 11:0011:40 Accessing the ANW Dictionary Fons Moerdijk, Carole Tiberius and Jan Niestadt Session 2: Poster Presentations (8 minutes each) 11:4011:48 ProPOSEL: a human-oriented prosody and PoS English lexicon for machinelearning and NLP Claire Brierley and Eric Atwell 11:4811:56 Natural Language Searching in Onomasiological Dictionaries Gerardo Sierra 11:5612:04 First ideas of user-adapted views of lexicographic data exemplified on OWID and elexiko Carolin Moller-Spitzer and Christine Mohrs 12:0412:12 Multilingual Conceptual Access to Lexicon based on Shared Orthography: An ontology-driven study of Chinese and Japanese Chu-Ren Huang, Ya-Min Chou, Chiyo Hotani, Sheng-Yi Chen and Wan-Ying Lin 12:1212:20 Extracting Sense Trees from the Romanian Thesaurus by Sense Segmentation & Dependency Parsing Neculai Curteanu, Alex Moruz and Diana Trandabat 12:2012:28 Lexical-Functional Correspondences and Their Use in the System of Machine Translation ETAP-3 Andreyeva Sasha ix Sunday, August 24, 2008 (continued) 12:2812:36 The Close-Distant Relation of Adjectival Concepts Based on Self-Organizing Map Kyoko Kanzaki, Noriko Tomuro and Hitoshi Isahara 12:3612:45 Looking up phrase rephrasings via a pivot language Aurelien Max and Michael Zock 12:4514:00 Lunch Session 3: Regular Talks 14:0014:40 Toward a cognitive organization for electronic dictionaries, the case for semantic proxemy Bruno Gaume, Karine Duvignau, Laurent Prevot and Yann Desalle 14:4015:20 Cognitively Salient Relations for Multilingual Lexicography Gerhard Kremer, Andrea Abel and Marco Baroni 15:2015:50 Coffee Break + Poster Session 15:5016:30 The Computation of Associative Responses to Multiword Stimuli Reinhard Rapp Session 4: Poster Session + Wrap Up Discussion 16:3017:00 Poster Session 17:0017:30 Wrap Up Dission 17:3017:30 End of the Workshop x Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 18 Manchester, August 2008 Comparing Lexical Relationships Observed within Japanese Collocation Data and Japanese Word Association Norms Terry Joyce School of Global Studies, Tama University, 802 Engyo, Fujisawa, Kanagawa, 252-0805, JAPAN terry@tama.ac.jp Irena Srdanovi Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo 152-8552, JAPAN srdanovic.i.ab@m.titech.ac.jp  Abstract 1  While large-scale corpora and various corpus query tools have long been recognized as essential language resources, the value of word association norms as language resources has been largely overlooked." ></td>
	<td class="line x" title="59:191	This paper conducts some initial comparisons of the lexical relationships observed within Japanese collocation data extracted from a large corpus using the Japanese language version of the Sketch Engine (SkE) tool (Srdanovi et al., 2008) and the relationships found within Japanese word association sets taken from the large-scale Japanese Word Association Database (JWAD) under ongoing construction by Joyce (2005, 2007)." ></td>
	<td class="line x" title="60:191	The comparison results indicate that while some relationships are common to both linguistic resources, many lexical relationships are only observed in one resource." ></td>
	<td class="line x" title="61:191	These findings suggest that both resources are necessary in order to more adequately cover the diverse range of lexical relationships." ></td>
	<td class="line x" title="62:191	Finally, the paper reflects briefly on the implementation of association-based word-search strategies into electronic dictionaries proposed by Zock and Bilac (2004) and Zock (2006)." ></td>
	<td class="line x" title="63:191	1 Introduction Large-scale corpora and various corpus query tools have long been recognized as extremely important language resources." ></td>
	<td class="line x" title="64:191	The impact of   2008." ></td>
	<td class="line x" title="65:191	Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="66:191	Some rights reserved." ></td>
	<td class="line x" title="67:191	corpora and corpus query tools has been particularly significant in the area of compiling and developing lexicographic materials (Kilgarriff and Rundell, 2002) and in the area of creating various kinds of lexical resources, such as WordNet (Fellbaum, 1998) and FrameNet (Atkins et al., 2003; Fillmore et al., 2003)." ></td>
	<td class="line x" title="68:191	In contrast, although the significance of databases of free word association norms have long been recognized within psychology in providing insights into higher cognitive processes (Cramer, 1968; Deese, 1965; Nelson et al., 1998; Steyvers and Tenenbaum, 2005), their value as a language resource has been largely overlooked." ></td>
	<td class="line x" title="69:191	However, as Sinopalnikova and Pavel (2004) point out, databases of word association norms represent an extremely useful supplement to the range of traditional language resources, such as large-scale corpora, thesauri, and dictionaries, and can potentially contribute greatly to the development of more sophisticated linguistic resources." ></td>
	<td class="line x" title="70:191	This paper seeks to demonstrate the potential value of word association databases as language resources." ></td>
	<td class="line x" title="71:191	Specifically, we conduct some initial comparisons of the lexical relationships observed within Japanese collocation data, as extracted from a large corpus with the Japanese language version of the Sketch Engine (SkE) tool (Srdanovi et al., 2008), with those found within Japanese word association sets, which were created through the ongoing construction of the large-scale Japanese Word Association Database (JWAD) (Joyce, 2005, 2007)." ></td>
	<td class="line x" title="72:191	Interesting similarities and differences between the two language resources in terms of captured lexical relationships affirm the value of word association databases as rich linguistic resources." ></td>
	<td class="line x" title="73:191	In concluding, we speculate briefly on how the wider range of lexical relationships identifiable through the combination of collocation data and word associ1 ation databases could be utilized in organizing lexical entries within electronic dictionaries in ways that are cognitively salient." ></td>
	<td class="line x" title="74:191	While we fully acknowledge that the challenges involved are formidable ones (Zock, 2006), the principled incorporation of word association knowledge within electronic dictionaries could greatly facilitate the development of more flexible and userfriendly navigation and search strategies (Zock and Bilac, 2004)." ></td>
	<td class="line x" title="75:191	2 Basic Concepts: Word Sketches and Word Association Norms This section briefly provides some background information about SkE, which is the corpus query tool used in this study to extract and display word collocation data, and about word association norms as gathered through psychological experimentation." ></td>
	<td class="line x" title="76:191	2.1 Sketch Engine (SkE): Word Sketches and Thesaurus Tools Sketch Engine (SkE) (Kilgarriff et al. 2004) is a web-based corpus query tool that supports a number of functions." ></td>
	<td class="line x" title="77:191	These include fast concordancing, grammatical processing, word sketching (one-page summaries of a words grammatical and collocation behavior), a distributional thesaurus, and robot use." ></td>
	<td class="line x" title="78:191	SkE has been applied to a number of languages." ></td>
	<td class="line x" title="79:191	In this study, we utilize the Word Sketches and Thesaurus functions for the Japanese language." ></td>
	<td class="line x" title="80:191	As both tools process raw collocation data by organizing words according to grammatical and lexical relationships, they are particularly suited to the conducted comparisons with the word association data." ></td>
	<td class="line x" title="81:191	Word Sketches (Kilgarriff and Tugwell, 2001) present the most frequent and statistically-salient collocations and grammatical relations for a given word." ></td>
	<td class="line x" title="82:191	These relations are derived as the results of grammatical analysis (a gramrel file) that employs regular expressions over PoS-tags." ></td>
	<td class="line x" title="83:191	The distributional thesaurus groups together words that occur in similar contexts and have common collocation words." ></td>
	<td class="line x" title="84:191	Estimations of semantic similarity are based on shared triples." ></td>
	<td class="line x" title="85:191	For example, <read a book> and <read a magazine> share the same triple pattern of <read a ?>, and because book and magazine exhibit high salience for the triple, they are both assumed to belong to the same thesaurus category." ></td>
	<td class="line x" title="86:191	This approach is similar to conventional techniques for automatic thesaurus construction (Lin, 1998)." ></td>
	<td class="line x" title="87:191	2.2 Word Association Norms In contrast to the Word Sketch collocation and thesaurus tools that take the corpus as the basic input language resource, databases of word association norms are the results of psychological experiments." ></td>
	<td class="line x" title="88:191	The free word association task typically asks the respondent to respond with the first semantically-related word that comes to mind on presentation of a stimulus word." ></td>
	<td class="line x" title="89:191	The collection of word association normative data can be traced back to the seminal study by Kent and Rosanoff (1910) which gathered word association responses for a list of 100 stimulus words." ></td>
	<td class="line x" title="90:191	However, despite the insightful remarks of Deese (1965) and Cramer (1968) that word associations closely mirror the structured patterns of relations that exist among conceptsclaims that undoubtedly warrant further investigation there are, unfortunately, still relatively few largescale databases of word association norms." ></td>
	<td class="line x" title="91:191	The notable exceptions for the English language include the Edinburgh Association Thesaurus (EAT) (Kiss et al., 1973), which consists of approximately 56,000 responses to a stimulus list of 8,400 words, and the University of South Florida Word Association, Rhyme, and Word Fragment Norms compiled by Nelson et al.(1998), consisting of nearly three-quarters of a million responses to 5,019 stimulus words." ></td>
	<td class="line x" title="93:191	Another database deserving mention is the Russian Association Thesaurus compiled by Karaulov et al.(1994, 1996, 1998) which has approximately 23,000 responses for 8,000 stimulus words (cited in Sinopalnikova and Pavel, 2004)." ></td>
	<td class="line x" title="95:191	3 Japanese Language Resources This section introduces the Japanese language resources utilized in this study: namely, the Japanese Word Sketches and Thesaurus (Srdanovi et al., 2008) and the Japanese Word Association Database (Joyce, 2005, 2007)." ></td>
	<td class="line x" title="96:191	3.1 Japanese Word Sketches and Thesaurus The Japanese version of SkE is based on JpWaC (Erjavec et al., 2007; Srdanovi et al., 2008), which is a 400-million word Japanese web corpus that has been morphologically analyzed and POS-tagged with the ChaSen tool (http://chasen.naist.jp/)." ></td>
	<td class="line x" title="97:191	The Word Sketches are based on Japanese grammatical analysis results (gramrel file), where 22 grammatical relations are defined based on ChaSen PoS tags and tokens (Srdanovi et al 2008)." ></td>
	<td class="line x" title="98:191	Figure 1 presents 2 parts of word sketches for the noun fuyu ( winter), showing adjective modifications and two verb relations involving the particles of wa ( topic marker) and ni ( time marker), respectively." ></td>
	<td class="line x" title="99:191	Figure 1." ></td>
	<td class="line x" title="100:191	Parts of the Word Sketch results for the noun fuyu ( winter)." ></td>
	<td class="line x" title="101:191	3.2 Japanese Word Association Database To an even greater extent than for the English language, there has been a serious lack of word association norms for the Japanese language." ></td>
	<td class="line x" title="102:191	While Umemotos (1969) survey collected associations from 1,000 university students, the limited set of just 210 words merely underscores the deficient." ></td>
	<td class="line x" title="103:191	More recently, Okamoto and Ishizaki (2001) compiled an Associative Concept Dictionary (ACD) consisting of 33,018 word association responses provided by 10 respondents for 1,656 nouns." ></td>
	<td class="line x" title="104:191	However, it should be noted that the ACD is not strictly free association data because response category was specified as part of the task." ></td>
	<td class="line x" title="105:191	Under ongoing construction by Joyce (2005, 2007), the Japanese Word Association Database (JWAD) aims to eventually develop into a very large-scale database of free word association norms for the Japanese language in terms of both the number of stimulus items and the numbers of association responses collected." ></td>
	<td class="line x" title="106:191	The present JWAD stimulus list consists of 5,000 basic Japanese kanji and words." ></td>
	<td class="line x" title="107:191	The currently available JWAD Version 1 (JWAD-V1) consists of 104,800 free word association responses collected through a paper questionnaire survey with a sample of 2,099 items presented to up to 50 respondents." ></td>
	<td class="line x" title="108:191	The association sets compared with work sketch profiles in the subsequent sections are from JWAD-V1." ></td>
	<td class="line x" title="109:191	4 Conducted Comparisons This section presents the results of our initial comparison for the lexical relationships observed within the Japanese collocation data with those in the Japanese word association sets." ></td>
	<td class="line x" title="110:191	The comparisons focused on approximately 350 word association responses constituting the association sets for the two verbs of kizuku ( to notice) and sagasu ( to search for), the adjective of omoshiroi ( interesting), and the three nouns of jitensha ( bicycle), natsu ( summer), and yama ( mountain), as examples of basic Japanese vocabulary." ></td>
	<td class="line x" title="111:191	Taking into account the considerable degree of orthographic variation present with the Japanese writing system, all possible orthographic variations were searched for in the SkE, such as kizuku (/ ) and omoshiroi (/)." ></td>
	<td class="line x" title="112:191	4.1 Word Sketches and Thesaurus Versus Word Association Norms The Japanese SkE employs a large-scale Japanese corpus and detailed grammatical analysis based on ChaSen POS tags." ></td>
	<td class="line x" title="113:191	Accordingly, numerous lexical relationships are identified in the word sketches and thesaurus results." ></td>
	<td class="line x" title="114:191	For example, kizuku appears 12,134 times in the corpus in approximately 200 collocation examples in total, which are grouped under 12 different collocation and grammatical relations and sorted according to the statistical salience of the relations frequency within the corpus (note that searches were conducted with the default setting of only including collocations with frequencies of five or more)." ></td>
	<td class="line x" title="115:191	The thesaurus function also yields numerous results, typically displaying around 60 salient relations that are clustered into five semantic groups." ></td>
	<td class="line x" title="116:191	In contrast, while JWAD-V1 is quite large-scale for a word association databases, it is naturally far smaller than the Japanese SkE corpus." ></td>
	<td class="line x" title="117:191	As already noted, it consists of word association collected from about 50 respondents (although there are 100 respondents in the case of kizuku), and where some responses would obviously be provided by multiple respondents." ></td>
	<td class="line x" title="118:191	Comparisons of the SkE results with the sets of word association responses revealed that there is considerable overlap in the range of lexical relationships observed in the two linguistics resources." ></td>
	<td class="line x" title="119:191	However, the comparisons also identified many lexical relationships that are only present in one of the language resources." ></td>
	<td class="line x" title="120:191	3 Because of the large differences in the overall sizes of the association responses in JWAD-V1 and the collocations in SkE, it is not surprising that the word association data does not cover the numerous collocation words present in the SkE results." ></td>
	<td class="line x" title="121:191	(In future studies, we plan to examine the kinds of relationships that are extracted from the corpora but which are not observed in the word association database)." ></td>
	<td class="line x" title="122:191	However, it is very interesting to note that a considerable number of the JWAD word associations were not present in the SkE results, even though the tool is drawing on a much larger resource." ></td>
	<td class="line x" title="123:191	In this study, we concentrate on describing these lexical relationships." ></td>
	<td class="line x" title="124:191	Table 1." ></td>
	<td class="line x" title="125:191	The numbers of word association norms present (+) and absent (-) in the Word Sketches (WS) and the Thesaurus (T) results Norms Ass." ></td>
	<td class="line x" title="126:191	Freq  2 Ass." ></td>
	<td class="line x" title="127:191	Freq = 1 WS+ WST+ WS+ WST+ omoshiroi 6 5 2 1 16 2 kizuku 6 8 3 9 44 2 sagasu 4 8 1 2 13 1 jitensha 7 13 0 2 10 0 natsu 3 4 1 5 13 1 yama 6 3 2 8 7 2  Table 1 shows that considerable numbers of word association responses with frequencies of two or more, as well as many with frequencies of one, are not observed in the word sketches and thesaurus results." ></td>
	<td class="line x" title="128:191	While these results could be indicating a need to consider new methods or approaches to corpus-extraction in addition to those currently employed, these findings also strongly suggest that some of the lexical relationships might be unique to the normative word association data." ></td>
	<td class="line x" title="129:191	Both resources unquestionably tap into fundamental aspects of lexical relationships, but the resources would seem to be quite different in nature." ></td>
	<td class="line x" title="130:191	Accordingly, the present results suggest that investigations into lexical relationships would do well to employ both corpus-based results and databases of word association norms in complementary ways, in order to provide more comprehensive coverage of the diverse range of lexical relationships." ></td>
	<td class="line x" title="131:191	The thesaurus function only outputs lexical relationships between words of the same word class." ></td>
	<td class="line x" title="132:191	This function also yields synonym relationships that are also found in the word association norms, and are rated as being highly salient for the thesaurus results." ></td>
	<td class="line x" title="133:191	For example, tanoshii and kyomibukai ( interesting) are word association responses for omoshiroi." ></td>
	<td class="line x" title="134:191	4.2 Lexical Relationships that are Common to Both the Corpus-Based Results and the Word Association Norms This section discusses some of the lexical relationships common to the two resources." ></td>
	<td class="line x" title="135:191	The most frequent of these are presented in Table 2." ></td>
	<td class="line x" title="136:191	The first coord group includes kawa ( river) with the noun of yama, tanoshii ( pleasant) with the adjective of omoshiroi, and odoroku ( to be surprised) with the verb of kizuku." ></td>
	<td class="line x" title="137:191	Other frequent relationships are verbal phrases involving appropriate particles (such as nounNI (e.g., jitensha ni noru ( to ride a bicycle), noPronom, nounWO (e.g., michi wo sagasu ( to look for a road), deVerb, niVerb)." ></td>
	<td class="line x" title="138:191	Table 2 also includes a number of modification relationships (modifier_Adv, modifier_Ai (e.g., atsui natsu ( hot summer))." ></td>
	<td class="line x" title="139:191	Note that these terms are those employed in the Word Sketch results." ></td>
	<td class="line x" title="140:191	Table 2." ></td>
	<td class="line x" title="141:191	Lexical relationships common to both the Word Sketch (WS) results and the word association norms Relationship WS Example Coord 15 (yama/kawa),  (omoshiroi/tanoshii),  (kizuku/ odoroku) nounNI 8  (machigai ni kizuku) noPronom 7  (jitensha no kagi)  (yama no midori) gaAdj 5  (yama ga kirei) nounWO 4  (michi wo sagasu) waAdj 4  (natsu wa suki) waVerb 4  (jitensha wa hashiru) deVerb 3  (jitensha de korobu) modifier_Adv 3  (futo kizuku) modifier_Ai 3  (atsui natsu) niVerb 3 (jitensha ni noru) nounWA 3  (hanashi wa omoshiroi) woVerb 3  (jitensha wo kogu)  4 4.3 Relations Specific to Association Norms While acknowledging that it could be beneficial to examine the types of lexical relationships observed in the corpus-based results but not in the word association data, given the relative differences in the sizes of the two resources, the present study focuses on the relationships that were only present in the database of word association norms." ></td>
	<td class="line x" title="142:191	Briefly, these relationships can be classified under six categories." ></td>
	<td class="line x" title="143:191	(1) Relationships involving a specific concept related to the stimulus word and its contextual meaning." ></td>
	<td class="line x" title="144:191	In Table 3 below, many of these are classified as typically associated words." ></td>
	<td class="line x" title="145:191	Examples include omoshiroi and warai (  laughter), kizuku and ch i ( attention), and natsu and taiy  ( sun)." ></td>
	<td class="line x" title="146:191	These relationships are neither collocational nor grammatical in nature, and so the grammatical analysis currently employed in the word sketches cannot identify them." ></td>
	<td class="line x" title="147:191	On the other hand, while they are semantically related, because they often belong to different word classes, the thesaurus function also fails to identify them." ></td>
	<td class="line x" title="148:191	(2) Relationships that are semantically similar (could be regarded as close synonyms) but do not belong to the same word class." ></td>
	<td class="line x" title="149:191	Examples include sagasu and tanky  ( search) and kizuku andkikubari ( care, attention)." ></td>
	<td class="line x" title="150:191	While these are not grammatical or collocational relations, again, the thesaurus function is also unable to find them because they belong to different word classes." ></td>
	<td class="line x" title="151:191	(3) Association responses consisting of more than one word." ></td>
	<td class="line x" title="152:191	Examples include explanatory phrases such as kibun ga ii ( lit." ></td>
	<td class="line x" title="153:191	feeling is good, comfortable) as response to omoshiroi, as well as concepts denoted by phrases, such as hito no kao ( human faces), also a response to omoshiroi." ></td>
	<td class="line x" title="154:191	(4) Relationships that could be recognized by the SkE, but which the present version fails to detect." ></td>
	<td class="line x" title="155:191	These would seem to reflect limitations with the present ChaSen dictionary (e.g., it does not list chari / charinko ( casual words for bicycle) or morphological/POStagging errors with ChaSen, or relationships that are not regarded as being sufficiently salient within the complete corpus, because they may appear frequently as both independent words and as constituents of many poly-morpheme words (e.g., omoshiroi hito ( interesting person))." ></td>
	<td class="line x" title="156:191	(5) Relationships that can be identified when search is executed for orthographic variants of the word, such as tsumaranai ( boring) being found when omoshiori is written in hiragana (as )." ></td>
	<td class="line x" title="157:191	(6) Word association responses that are rather idiosyncratic in nature, often reflecting private experiences of a single respondent." ></td>
	<td class="line x" title="158:191	The importance of such responses in word association databases should be judged on the size of the database, although one also should be cautious about sampling issues with lower respondent numbers." ></td>
	<td class="line x" title="159:191	While it would certainly be interesting to conduct further comparisons between the association norms and other kinds of corpora, such as literary works, newspapers, or more balanced corpora, processed by the SkE, the main purpose of the present paper is to draw attention to the value of word association databases as linguistic resources." ></td>
	<td class="line x" title="160:191	Although the lexical relationships in categories 1 and 2 were not observed in the present corpus-based results, they are unquestionably of great relevance to efforts to develop more principled organizations of the lexicon for navigational purposes, and would enhance existing lexical resources, such as WordNet." ></td>
	<td class="line x" title="161:191	With trends to increasingly include multiple word idioms and phrases within various dictionaries and linguistic resources, the multiple-word association responses of category 3 may provide further insights into how such items are stored and processed." ></td>
	<td class="line x" title="162:191	Moreover, categories 4 and 5 clearly suggest that free word association norms can be a very useful resource for evaluating and further improving morphological analyzers, as well as corpus query tools." ></td>
	<td class="line x" title="163:191	5 Lexicographical Implications: Organizing Lexicons According to Association Relationships As the merits of SkE and its significant contributions to the compilation of a number of major dictionaries are discussed in detail elsewhere (e.g., Kilgarriff and Rundell, 2002), and because Srdanovi and Nishina (2008) outline some possible lexicographical applications of the Japanese language version of the SkE, in this section, we focus on the lexical relationships observed within the JWAD and their lexicographical implications for realizing a principled association-based organization of the lexicon." ></td>
	<td class="line x" title="164:191	5 Table 3." ></td>
	<td class="line x" title="165:191	Tentative classification of the word association responses elicited for fuyu ( winter) Relationship Description Examples Modification Attribute: Temperate  (samui cold) Modification Attribute: Color  (shiroi white) Modification Attribute: Emotion  (setsunai bitter, severe) Lexical siblings Hyponyms of seasons  (natsu summer),  (haru spring) Typically associated Meteorological phenomena   (yuki snow),  (koori ice) Typically associated Activity  (t min hibernation),  (ett  passing of winter),  (ky kei rest),  (yasumi rest, holiday) Typically associated Cultural artifacts  (kotatsu quilt for lower body when sitting around low table),  (kamakura snow hut) Typically associated Time  (t ji winter solstice) Typically associated Location  (kita north) Typically associated Animal  (kuma bear) Typically associated Cultural symbolization  (fuyu-sh gun General Winter; hard winter; Jack Frost)  5.1 Linguistic Approaches to Association Data and Its Potential As previously commented, Deese (1965) and Cramer (1968) have both argued that word associations closely mirror the structured patterns of relations that exist among concepts." ></td>
	<td class="line oc" title="166:191	Indeed, as Sinopalnikova and Pavel (2004) note, Deese (1965) was the first to conduct linguistic analyses of word association norms, such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of Church and Hanks (1990) notion of mutual information." ></td>
	<td class="line x" title="167:191	However, as we have also remarked already, the linguistic value of word association data has, regrettably, been largely overlooked." ></td>
	<td class="line x" title="168:191	In a similar spirit to Hirsts (2004) claim that, notwithstanding certain caveats on the complex relationships between them, a lexicon can often serve as a useful basis for developing a practical ontology, we believe that a very promising approach to organizing the lexicon would be to more fully appreciate and utilize the rich variety of associative relationships that exist within word association norms." ></td>
	<td class="line x" title="169:191	While the required, more thoroughgoing investigation into how to appropriately classify the complex nature of associative relationships is beyond the scope of this present study, in the next sub-section, we attempt to highlight the potential contributions that word association norms could provide to efforts seeking to explore lexical knowledge." ></td>
	<td class="line x" title="170:191	5.2 Tentative Classification of Association Relationships To illustrate some of the issues for developing a comprehensive, yet a parsimonious, classification of associative relationships, it is useful to briefly consider the notion proposed by Zock and Bilac (2004) and Zock (2006) of word search strategies in electronic dictionaries based on associations." ></td>
	<td class="line x" title="171:191	Their outline of how such a look-up system might function employs three kinds of basic association relationships; namely, a kind of (AKO), subtype (ISA), and typically involved object, relation or actor (TIORA)." ></td>
	<td class="line x" title="172:191	While we accept that the limited set of just three types was probably motivated primarily in the interests of simplicity, given Zocks (2006) suggestion to enhance the navigability of the system by categorizing relationships, clearly the classification of association relationships is a fundamental issue." ></td>
	<td class="line x" title="173:191	Table 3 presents a tentative classification of the word association responses for the noun winter." ></td>
	<td class="line x" title="174:191	As the comparisons introduced in Section 4 clearly demonstrate, it is usually possible to extract the modification and lexical sibling relationships included in Table 3 from corpora with corpus query tools such as SkE." ></td>
	<td class="line x" title="175:191	However, the comparisons also highlighted the fact that it is far more difficult to identify the kinds of relationships classified in Table 3 as typically associated with such linguistic resources alone." ></td>
	<td class="line x" title="176:191	While highly provisional in nature, we believe that the attempt to classify the association relationships within the association responses for fuyu can 6 serve to highlight some important issues for Zock and Bilacs (2004) approach." ></td>
	<td class="line x" title="177:191	While the lexical siblings relationships between fuyu and the two response words of natsu ( summer) and haru ( spring) could feasibly be represented by AKO or ISA relationship links to shiki ( the four seasons) outside of the association set itself, having to rely on external references would not be a very satisfactory approach to classifying the direct association relationships." ></td>
	<td class="line x" title="178:191	Incidentally, although the hyponyms of seasons description would seem fairly natural from the perspective of a thesaurus, the absence of aki ( autumn) from the set would indicate that the strengths of associations can vary even among lexical siblings (although the absence of aki from the present data could simply be due to sampling issues)." ></td>
	<td class="line x" title="179:191	Given that fuyu is a noun, the presence of several modification relationships is not very surprising, at least not for the prime associate of samui ( cold), but the idea of fuyu having a color attribute is perhaps initially more startling (while one many not expect winter to have a default color slot within its range of attributes, the association of shiroi ( white) with fuyu is initiatively appealing)." ></td>
	<td class="line x" title="180:191	For the fuyu association set, the most relevant of the association relationships specified by Zock and Bilac (2004) is the TIORA relationship." ></td>
	<td class="line x" title="181:191	However, even for this relatively small association set containing just 11 main relationship types, because seven of them can be initially classified as typically associated, clearly this designation alone is too encompassing to be a useful classification category." ></td>
	<td class="line x" title="182:191	The inclusion of the description field in Table 3 is an attempt to further define meaningful sub-categories." ></td>
	<td class="line x" title="183:191	In the case of the sub-category meteorological phenomena, it would seem to be well motivated to explain the associations between fuyu as the stimulus word and yuki ( snow) and k ri ( ice) as two response words." ></td>
	<td class="line x" title="184:191	However, while the subcategory of cultural artifacts clearly goes some way to pinpointing the underlying association between fuyu and kotatsu (), it does rely on a certain cultural familiarity with the kind of quilted kind of blanket that are used for keeping ones legs warm when sitting around a low family table during winter." ></td>
	<td class="line x" title="185:191	A natural association for anyone who has ever lived in Japan during the winter months, but typically associated + cultural artifact seems to miss something of the naturalness." ></td>
	<td class="line x" title="186:191	6 Conclusions This paper has compared the lexical relationships observed within Japanese collocation data extracted from a large corpus using the Japanese language version of the Sketch Engine (SkE) tool and the relationships found within Japanese word association sets taken from the large-scale Japanese Word Association Database (JWAD)." ></td>
	<td class="line x" title="187:191	The comparison results indicate that while many lexical relationships are common to both linguistic resources, a number of lexical relationships were only observed in one of the resources." ></td>
	<td class="line x" title="188:191	The fact that some lexical relationships might be unique to word association norms demonstrates the value of word association databases as linguistic resources." ></td>
	<td class="line x" title="189:191	The present findings suggest that both resources can be effectively used in combination in order to provide more comprehensive coverage of the wide range of lexical relationships." ></td>
	<td class="line x" title="190:191	Finally, we presented a tentative classification of the association relationships in the association set for fuyu." ></td>
	<td class="line x" title="191:191	Our brief discussion of the classification sought to reflect on some of the challenges to realizing a principled association-based organization of the lexicon as a fundamental step toward implementing cognitively-salient wordsearch strategies based on associations in electronic dictionaries." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:149	Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 102109 Manchester, August 2008 The Computation of Associative Responses to Multiword Stimuli Reinhard Rapp Universitat Rovira i Virgili Plaza Imperial Tarraco, 1 43005 Tarragona, Spain reinhardrapp@gmx.de   Abstract It is shown that the behaviour of test persons as observed in association experiments can be simulated statistically on the basis of the common occurrences of words in large text corpora, thereby applying the law of association by contiguity which is well known from psychological learning theory." ></td>
	<td class="line x" title="2:149	In particular, the focus of this work is on the prediction of the word associations as obtained from subjects on presentation of multiword stimuli." ></td>
	<td class="line x" title="3:149	Results are presented for applications as diverse as crossword puzzle solving and the identification of word translations based on non-parallel texts." ></td>
	<td class="line x" title="4:149	1 Introduction The idea that human memory functions associatively goes back to Aristotle who formulated that the sequence of our memories is determined by the concepts of similarity and proximity (Strube, 1984:34)." ></td>
	<td class="line x" title="5:149	As early as 1879, Francis Galton tried to systematically observe human associative behaviour by introducing an association experiment." ></td>
	<td class="line x" title="6:149	In this experiment, given a particular stimulus word, subjects had to respond with the first other word that occurred to them spontaneously." ></td>
	<td class="line x" title="7:149	The resulting tables of associative responses are called association norms." ></td>
	<td class="line x" title="8:149	To explain the behavior documented in the association norms, in the literature a multiplicity of different mechanisms underlying human memory are proposed, thereby, for example, assuming phonological, morphological, syntactical, semantical, and contextual relations between words   2008." ></td>
	<td class="line x" title="9:149	Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http:// creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:149	Some rights reserved." ></td>
	<td class="line x" title="11:149	(Wettler, 1980)." ></td>
	<td class="line x" title="12:149	However, as yet there is no agreement whether these mechanisms should be considered of equal status, or if some may be derived from others." ></td>
	<td class="line x" title="13:149	Already in 1750 the physiologist David Hartley suggested that it may be possible to reduce the multiplicity of proposed association laws to only a single one based on temporal contiguity." ></td>
	<td class="line x" title="14:149	This was formulated as one of the earliest psychological laws by William James (1890: 561): Objects once experienced together tend to become associated in the imagination, so that when any one of them is thought of, the others are likely to be thought of also, in the same order of sequence or coexistence as before." ></td>
	<td class="line x" title="15:149	This statement we may name the law of mental association by contiguity. Assuming that the objects referred to in this law are words, the law of association by contiguity implies the following two phases: 1) Learning phase: When perceiving language, strong associative connections are developed between words that frequently occur in close temporal succession." ></td>
	<td class="line x" title="16:149	2) Retrieval phase: These associations determine the words that come to mind during generation." ></td>
	<td class="line x" title="17:149	Only words that are strongly interconnected or have strong associations to external stimuli can be uttered or written down." ></td>
	<td class="line x" title="18:149	Pre-supposing the validity of the law of association, it should be possible to derive free word associations from the distribution of words in texts." ></td>
	<td class="line oc" title="19:149	Following Church & Hanks (1990), Rapp (2004), and Wettler et al.(2005) this actually seems to be successful." ></td>
	<td class="line x" title="21:149	The recent simulation algorithms generate results which largely agree with the free word associations as found in the association norms." ></td>
	<td class="line x" title="22:149	An example is shown in Table 1, where the observed and the simulated responses to the stimulus word cold are compared." ></td>
	<td class="line x" title="23:149	102 OBSERVED RESPONSE NUMBER OF SUBJECTS PREDICTED RESPONSE NUMBER OF SUBJECTS hot ice warm water freeze wet feet freezing nose room sneeze sore winter  34  10  7  5  3  3  2  2  2  2  2  2  2 hot winter weather warm water heat ice wet wind temperature shiver freeze rain  34  2  0  7  5  1  10  3  0  0  0  3  0  Table 1: Observed and predicted associative responses to the stimulus word cold." ></td>
	<td class="line x" title="24:149	When judging these results it should be kept in mind that among subjects there is some variation of responses." ></td>
	<td class="line x" title="25:149	Therefore, the simulation results can be considered satisfactory if the difference between the predicted and the observed answers is on average not larger than the difference between an answer of an average test subject and the answers of the remaining test subjects." ></td>
	<td class="line x" title="26:149	In the current paper we try to build on these results." ></td>
	<td class="line x" title="27:149	However, while most previous work considered only associations to individual stimulus words, the question to be dealt with here is whether the associative responses to several stimuli can likewise be predicted from the cooccurrences of words in texts." ></td>
	<td class="line x" title="28:149	This is of considerable interest as all utterances and texts can be considered as accumulations of stimulus words, which together lead to a systematic activation of other words and concepts in the mind of the listener or reader." ></td>
	<td class="line x" title="29:149	How uniform the reactions of test subjects can be upon presentation of several stimulus words can be seen from examples like the word pairs circus  laugh or King  girl where subjects tend to think of clown and princess, respectively." ></td>
	<td class="line x" title="30:149	Starting from the association norms for individual stimuli, the observed results are not always obvious." ></td>
	<td class="line x" title="31:149	For example, in a large database of association norms, namely the Edinburgh Associative Thesaurus (Kiss et al., 1973), among the responses to King the word princess is completely missing, and the same is true for girl." ></td>
	<td class="line x" title="32:149	This means that the combination of stimulus words can lead to associations which are only weakly linked to the individual words and therefore cannot easily be deduced from conventional association norms." ></td>
	<td class="line x" title="33:149	Accordingly it is not obvious whether the method used for the simulation of the associative behavior to single words can be extended in a straightforward way in the case of several stimulus words." ></td>
	<td class="line x" title="34:149	The organization of this paper is as follows: We first look at association norms collected for pairs of stimulus words." ></td>
	<td class="line x" title="35:149	We then introduce a corpus-based algorithm that simulates the observed behavior which is applicable in the case of single or multiple stimuli." ></td>
	<td class="line x" title="36:149	We then present some results of the algorithm and apply it to some related problems." ></td>
	<td class="line x" title="37:149	2 Association norms for word pairs For individual English words, several association norms have been published, with the largest being the Edinburgh Associative Thesaurus." ></td>
	<td class="line x" title="38:149	However, in the case of several stimulus words hardly any data seems to exist, with Rapp (1996, 1998) being an exception." ></td>
	<td class="line x" title="39:149	This is a study that collected the responses of 31 subjects to pairs of German2 nouns." ></td>
	<td class="line x" title="40:149	In compiling these association norms, a list of 10 common German nouns had been selected, namely Mdchen (girl), Krankheit (illness), Junge (boy), Musik (music), Brger (citizen), Erde (earth), Strae (street), Knig (King), Freude (joy), Sorge (worry)." ></td>
	<td class="line x" title="41:149	Then all 90 possible pairs of these words were constructed, and the answers of the subjects upon presentation of these pairs were collected." ></td>
	<td class="line x" title="42:149	The subjects were asked to come up with the first word spontaneously coming to mind." ></td>
	<td class="line x" title="43:149	In addition, associations to the individual words were also collected." ></td>
	<td class="line x" title="44:149	As for the pairs it turned out that word order did not have a noticeable effect on the responses, the responses to pairs differing only in word order were merged." ></td>
	<td class="line x" title="45:149	In Table 2 the associative responses as given by the test subjects for two sample pairs of stimulus words are listed." ></td>
	<td class="line x" title="46:149	In comparison to responses to individual stimulus words, the responses to pairs of words are generally less uniform, i.e. there is considerably more variation in the case of word pairs." ></td>
	<td class="line x" title="47:149	For example 25 of 31 test subjects come up with the association Mdchen (girl) given the stimulus word Junge (boy)." ></td>
	<td class="line x" title="48:149	In contrast, the most frequently mentioned associative response upon presentation of the stimulus pair Junge Mdchen (boy girl), which is Kinder (children), is given by only seven test persons." ></td>
	<td class="line x" title="49:149	2 As we are not aware of such data for English, the current study was conducted for German, with translations given throughout the paper." ></td>
	<td class="line x" title="50:149	103 STIMULUS PAIR ASSOCIATIVE RESPONSES Erde (earth)  Sorge (worry) Umwelt (environment) 8, Umweltverschmutzung (environmental pollution) 5, Weltuntergang (end of the world) 2, ai (AI), Ausbeutung (exploitation), Katastrophen (catastrophe), Klimakatastrophe (climatical catastrophe), Krieg (war), Luft (air), Macht (might), Mll (garbage), Mutter (mother), Ozonloch (ozone hole), Resignation (resignation), berbevlkerung (overpopulation), Umweltzerstrung (destruction of the environment), unfruchtbar (infertile), Verschmutzung (pollution), Zerstrung (destruction) Knig (King)  Mdchen (girl) Prinzessin (princess) 15, Knigin (queen) 3, Tochter (daughter) 2, Abhngigkeit (dependency), Dienerin (maid), Hochzeit (wedding), Kinderspiele (childrens games), Kitsch (kitsch), Knigspaar (royal couple), Mrchen (fairy tale), Mibrauch (abuse), Pferd (horse), Vater (father), Vorbild (model) Table 2: Associations to the stimulus pairs Erde Sorge (earth worry) and Knig Mdchen (King girl)." ></td>
	<td class="line x" title="51:149	Figures indicate the number of subjects with the respective response, with the default being one." ></td>
	<td class="line x" title="52:149	For a more exact quantitative analysis of this observation a measure is needed for the homogeneity of the answers." ></td>
	<td class="line x" title="53:149	For this purpose, it was computed how many subjects gave the same answer to a particular stimulus pair." ></td>
	<td class="line x" title="54:149	On average, this was the case for 4% of the subjects." ></td>
	<td class="line x" title="55:149	In comparison, the corresponding value for individual stimulus words is 15%." ></td>
	<td class="line x" title="56:149	Thus the impression of a substantially larger homogeneity of the associative answers for individual stimuli is confirmed." ></td>
	<td class="line x" title="57:149	3 Simulation program The simulation is based on the detection of statistical regularities of the common occurrences between the words in a large text corpus." ></td>
	<td class="line x" title="58:149	As we did not have a large and at the same time balanced corpus of German at our disposal, we decided to use a corpus of the newspaper Frankfurter Allgemeine Zeitung (FAZ) comprising the years 1993 to 1996 (135 million words)." ></td>
	<td class="line x" title="59:149	As in the association experiment the subjects rarely answer with inflected forms or function words, for computational reasons we lemmatized this corpus (Lezius, Rapp & Wettler, 1998) and  based on a list of stop words  removed closed class words such as articles, pronouns, and particles." ></td>
	<td class="line x" title="60:149	To determine word co-occurrences, for each word in the corpus it was counted how often its close neighbors occurred within a text window of plus and minus six words." ></td>
	<td class="line x" title="61:149	Assuming that approximately every second word is a function word, a window size of plus and minus six words after removal of the function words roughly corresponds to a window size of plus and minus 12 words without such pre-processing." ></td>
	<td class="line x" title="62:149	This is a window size that corresponds with what had been found appropriate for the computation of associations in other studies (e.g. Rapp, 2004)." ></td>
	<td class="line x" title="63:149	As the co-occurrence counts largely depend on overall word frequency, some association measure needs to be applied to eliminate this undesired influence." ></td>
	<td class="line x" title="64:149	Many previous studies have shown that the log-likelihood ratio is well suited for this purpose (Dunning, 1993)." ></td>
	<td class="line x" title="65:149	It successfully eliminates word-frequency effects and emphasizes significant word pairs by comparing their observed co-occurrence counts with their expected co-occurrence counts." ></td>
	<td class="line x" title="66:149	It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998)." ></td>
	<td class="line x" title="67:149	To compute the associations to pairs of stimulus words, it would in principle be possible to consider text positions where both stimulus words occur together, and to count the cooccurrence frequencies with their neighboring words." ></td>
	<td class="line x" title="68:149	This would result in a three-dimensional association matrix whose first two dimensions correspond to the two stimulus words and whose third dimension corresponds to their associations." ></td>
	<td class="line x" title="69:149	However, the problem of data sparseness would be very severe with such an approach, and it would not scale well if more than two stimulus words were considered." ></td>
	<td class="line x" title="70:149	We therefore propose another approach, which to our knowledge is novel in this context: The idea is that a potential associative response to a pair of stimulus words should have a strong and preferably symmetric associative connection to each of the stimulus words, and that a strong association to only one of them does not suffice." ></td>
	<td class="line x" title="71:149	Such a behavior can usually be ensured by a multiplication." ></td>
	<td class="line x" title="72:149	However, we do not multiply the association strengths, as the log-likelihood ratio has an inappropriate (exponential) value characteristic." ></td>
	<td class="line x" title="73:149	This value characteristic has the effect that a weak association to one of the stimuli can easily be overcompensated by a very strong association to 104 the other stimulus, which is not desirable." ></td>
	<td class="line x" title="74:149	Instead of multiplying the association strengths, we therefore suggest to multiply their ranks." ></td>
	<td class="line x" title="75:149	This improves the results considerably." ></td>
	<td class="line x" title="76:149	These considerations lead us to the following procedure: Given an association matrix of vocabulary V containing the log-likelihood ratios between all possible pairs of words, to compute the associative response given words a and b, the following steps are conducted: 1) For each word in V (by applying a searchand-compare operation on the association matrix) look up the ranks of words a and b in its list of associations, and compute the product of these ranks." ></td>
	<td class="line x" title="77:149	2) Sort the words in V according to these products, with the sort order such that the lowest value obtains the top rank (i.e. conduct a reverse sort)." ></td>
	<td class="line x" title="78:149	Note that this procedure is somewhat time consuming as computations are required for each word in a large vocabulary.3 On the plus side, the procedure is applicable to any number of stimulus words, and with increasing number of stimuli there is only a moderate increase in computational requirements." ></td>
	<td class="line x" title="79:149	(The application presented in section 5.2 successfully processes 30 stimulus words.)" ></td>
	<td class="line x" title="80:149	A minor issue is the assignment of ranks to words that have identical log-likelihood scores, especially in the frequent case of zero cooccurrence counts." ></td>
	<td class="line x" title="81:149	In such cases, the assignment of possibly almost arbitrary ranks could adversely affect the results." ></td>
	<td class="line x" title="82:149	We therefore suggest assigning corrected ranks, which are to be chosen as the average ranks of all words with identical scores." ></td>
	<td class="line x" title="83:149	With large numbers of stimuli, depending on the application it can be helpful to introduce a limit to the maximum rank, thereby reducing the effects of the sparse-data problem." ></td>
	<td class="line x" title="84:149	The benefit of this measure is similar to smoothing, but more sophisticated smoothing methods can of course also be considered (as described, e.g. in Church & Gale, 1991)." ></td>
	<td class="line x" title="85:149	Note that for the current work we only used a rank limit of 10,000, but did not apply any sophisticated smoothing as this usually has little impact if the focus is mainly on the top ranks, as is the case here." ></td>
	<td class="line x" title="86:149	3 Considerable time savings are possible by using an index of the non-zero co-occurrences." ></td>
	<td class="line x" title="87:149	4 Results The algorithm as described above was applied to the FAZ corpus." ></td>
	<td class="line x" title="88:149	That is, based on a window size of plus and minus six words, an association matrix with log-likelihood scores and (in both rows and columns) comprising all words with a corpus frequency of 200 or higher was computed." ></td>
	<td class="line x" title="89:149	For each of the 45 word pairs, the top associations as resulting from the product of ranks were computed." ></td>
	<td class="line x" title="90:149	To give some examples, the following tables show the outcome for a few stimulus pairs." ></td>
	<td class="line x" title="91:149	Hereby, the columns in the tables have the following meanings: 1) rank 2) corpus frequency of association 3) score (product of stimulus ranks) 4) association  Junge Mdchen (boy girl) 1 247 11.33 fnfzehnjhrig (15 year old) 2 2960 9.81 dreizehn (13) 3 398 9.72 gleichaltrig (same age) 4 86559 9.72 alt (old) 5 850 9.66 blond (blond)  Brger Mdchen (citizen girl) 1 1276 11.51 brav (well behaved) 2 1268 7.26 unschuldig (innocent) 3 223 6.73 verngstigt (scared) 4 979 6.41 anvertrauen (to intrust) 5 362 5.97 belstigen (to molest)  Strae Mdchen (street girl) 1 2509 7.50 tanzen (to dance) 2 242 7.12 pflastern (to pave) 3 272 6.96 Brgersteig (sidewalk) 4 529 6.87 Prostitution (prostitution) 5 4367 6.76 begegnen (to encounter)  Sorge Mdchen (worry girl) 1 317 7.03 elterlich (parental) 2 210 6.62 Burschen (fellows) 3 222 6.23 Beschneidung (concision) 4 7508 5.81 Eltern (parents) 5 271 5.77 zwlfjhrig (12 year old)  Junge Krankheit (boy illness) 1 8891 7.33 leiden (to suffer) 2 3553 7.14 tdlich (lethal) 3 16468 7.04 sterben (to die) 4 423 6.83 Heilung (cure) 5 261 6.62 Schizophrenie (schizophrenia) 105 Strae Krankheit (street illness) 1 308 6.94 Tuberkulose (tuberculosis) 2 4704 6.74 Unfall (accident) 3 276 6.71 tckisch (malicious) 4 232 6.34 heimtckisch (malignant) 5 620 6.07 anstecken (to infect)  Strae Brger (street citizen) 1 272 7.21 Brgersteig (sidewalk) 2 235 7.18 Gibraltar (Gibraltar) 3 207 7.09 flanieren (to stroll) 4 242 7.02 pflastern (to pave) 5 366 6.58 Fugngerzone (pedestrian zone)  Sorge Freude 1 6331 1.11 bereiten (to cause) 2 8747 9.21 Anla (occasion) 3 950 8.54 berwiegen (to outweigh) 4 27136 7.54 Grund (reason) 5 248 7.21 ungetrbt (untroubled)  If we look at all 45 word pairs, we obtain the following evaluation: Whereas an associative answer given by a subject is on average also given by 4% of the other subjects, only about 0.8% of the subjects give the answer produced in the simulation, i.e. the word ending up on the top rank." ></td>
	<td class="line x" title="92:149	However, due to the low number of cases, this value may be subject to some sampling error." ></td>
	<td class="line x" title="93:149	A method less sensitive to sampling errors is to look at the overall simulation ranks of the subjects responses." ></td>
	<td class="line x" title="94:149	Hereby it is better to consider the median of the ranks rather than the mean, as the medians treatment of outliers is more appropriate." ></td>
	<td class="line x" title="95:149	Note that when computing the median, associative responses given by n subjects obtain an n-fold higher weight." ></td>
	<td class="line x" title="96:149	To further reduce the effects of outliers, only responses that are given by at least two subjects are taken into account." ></td>
	<td class="line x" title="97:149	Under these assumptions, the overall median (computed over all stimulus pairs) has a value of 245." ></td>
	<td class="line x" title="98:149	With the total vocabulary of corpus frequency 200 and higher comprising about 25000 words, this value is at the 1% level." ></td>
	<td class="line x" title="99:149	This compares to 12500 at the 50% level, which could be expected in the case of random behaviour." ></td>
	<td class="line x" title="100:149	5 Applications 5.1 Crossword puzzle solver As crossword puzzles have definitions which usually consist of several words, the proposed algorithm can be applied as a crossword puzzle solver." ></td>
	<td class="line x" title="101:149	In order not to reduce this task to a (for computers) relatively simple combinatorial problem, we hereby only restrict the ranked list of words as produced by the simulation program to those words that have the correct number of characters, but do not utilize as clues the common characters of horizontal and vertical words." ></td>
	<td class="line x" title="102:149	As an example, Figure 1 shows a crossword puzzle which is attributed to be the worlds first one." ></td>
	<td class="line x" title="103:149	It was designed by Arthur Wynne and published on December 21, 1913 in The New York World." ></td>
	<td class="line x" title="104:149	Table 3 shows the definitions of this crossword puzzle together with the supposed solutions and the ranks of the respective words as computed by our algorithm based on three different corpora, namely the British National Corpus (BNC), the years 1990 to 1994 of the newspaper The Guardian, and the English part of the Wikipedia XML Corpus (Denoyer & Gallinari, 2006)." ></td>
	<td class="line x" title="105:149	These three corpora have a size of roughly 100, 150, and 300 million words, respectively." ></td>
	<td class="line x" title="106:149	To allow a better judgment of the simulation results, the number of words of the respective length in the underlying vocabulary is specified in column 5." ></td>
	<td class="line x" title="107:149	This vocabulary was chosen to consist of all words that have a corpus frequency of 100 or higher in the BNC but did not occur in our list of about 200 function words." ></td>
	<td class="line x" title="108:149	To this vocabulary, all words occurring in the crossword puzzle were added." ></td>
	<td class="line x" title="109:149	The purpose of limiting the vocabulary was solely for computational reasons, as our algorithm is rather demanding with regard to both execution time and memory requirements." ></td>
	<td class="line x" title="110:149	Note that the BNC-based vocabulary was also used for the other somewhat larger corpora as not many words were missing there: In the Guardian corpus of the altogether 34,448 words all but 390 occurred at least one time, and in the larger Wikipedia corpus all but 131." ></td>
	<td class="line x" title="111:149	We did not lemmatize the English corpora as in several cases inflected forms of words occurred in the definitions or in the solutions of the crossword puzzle." ></td>
	<td class="line x" title="112:149	Figure 1: Crossword puzzle by Arthur Wynne." ></td>
	<td class="line x" title="113:149	106 As described in section 2, for counting the cooccurrences of words a window of plus and minus six words around a given word was considered, and for the computation of the associative strengths the log-likelihood ratio was used." ></td>
	<td class="line x" title="114:149	Stop words were also removed from the corpora beforehand, but no lemmatization was conducted." ></td>
	<td class="line x" title="115:149	As many of the words used in the crossword puzzle are rare and several are outdated, solving this problem by a simulation is a non-trivial task." ></td>
	<td class="line x" title="116:149	Nevertheless, for the Wikipedia corpus the algorithm got 8 of 31 answers ranked among the top five." ></td>
	<td class="line x" title="117:149	When inspecting the examples that the algorithm got wrong, it appears that these are often the ones where humans would also have difficulties." ></td>
	<td class="line x" title="118:149	For example, the solution side for to agree with got consistently poor ranks with all  three corpora." ></td>
	<td class="line x" title="119:149	On the other hand, rather surprisingly, the solution for such and nothing more, namely mere, received top rankings despite the fact that there are no salient content words in the description." ></td>
	<td class="line x" title="120:149	This may be an indication that the algorithm grasps something that is related to cognitive processes." ></td>
	<td class="line x" title="121:149	However, a similar example, namely what we all should be ( moral) only obtains a reasonable ranking with the Wikipedia corpus." ></td>
	<td class="line x" title="122:149	According to the average rankings (bottom line of Table 2), this corpus seems to be better suited for this task than the other two corpora." ></td>
	<td class="line x" title="123:149	5.2 Identifying word translations The proposed core algorithm also has applications that may come somewhat unexpectedly." ></td>
	<td class="line x" title="124:149	What we suggest here is to identify word transla POS." ></td>
	<td class="line x" title="125:149	DEFINITION SOLU-TION LENGTH WORDS OF THIS LENGTH RANK BNC RANK GUARDIAN RANK WIKIPEDIA 2-3 what bargain hunters enjoy sales 5 4254 1014 70 338 4-5 a written acknowledgement receipt 7 5371 2 44 355 6-7 such and nothing more mere 4 2916 16 17 4 10-11 a bird dove 4 2916 17 87 4 14-15 opposed to less more 4 2916 42 34 5 18-19 what this puzzle is hard 4 2916 1486 115 384 22-23 an animal of prey lion 4 2916 84 16 324 26-27 the close of a day evening 7 5371 603 494 185 28-29 to elude evade 5 4254 80 64 38 30-31 the plural of is are 3 1424 238 119 412 8-9 to cultivate farm 4 2916 2316 2783 1070 12-13 a bar of wood or iron rail 4 2916 1658 1419 925 16-17 what artists learn to do draw 4 2916 227 1437 86 20-21 fastened tied 4 2916 15 2335 2078 24-25 found on the seashore sand 4 2916 124 19 757 10-18 the fibre of the gomuti palm doh 3 1424 585 279 711 6-22 what we all should be moral 5 4254 4107 1163 51 4-26 a day dream reverie 6 5371 489 572 2 2-11 a talon sere 4 2916 676 803 492 19-28 a pigeon dove 4 2916 36 8 1 F-7 part of your head face 4 2916 63 20 143 23-30 a river in Russia Neva 4 2916 174 413 3 1-32 to govern rule 4 2916 48 9 13 33-34 an aromatic plant nard 4 2916 616 2753 393 N-8 a fist neif 4 2916 ------24-31 to agree with side 4 2916 2836 2393 1387 3-12 part of a ship spar 4 2916 2693 1932 90 20-29 one tane 4 2916 2814 2773 2680 5-27 exchanging trading 7 5371 3444 5216 2347 9-25 sunk in mud mired 5 4254 3 2 1 13-21 a boy lad 3 1424 3 2 2 AVERAGE RANK 891.6 922.3 520.2  Table 2: Crossword puzzle definitions and the computed ranks of their solutions based on three corpora." ></td>
	<td class="line x" title="126:149	(--- means that a solution does not occur in a corpus (not taken into account when computing average ranks)." ></td>
	<td class="line x" title="127:149	107 tions from monolingual English and German corpora, i.e. from corpora that are not translations of each other (Rapp, 1999)." ></td>
	<td class="line x" title="128:149	This is a rather difficult task." ></td>
	<td class="line x" title="129:149	As our textual basis, for German we use the FAZ corpus as described above, with exactly the same pre-processing." ></td>
	<td class="line x" title="130:149	For English we use a similarly sized corpus of the newspaper The Guardian, with analogous pre-processing." ></td>
	<td class="line x" title="131:149	We apply a two-stage procedure to compute the translation of a source language word: First, by considering the log-likelihood ratios, its strongest source language associations are determined and translated to the target language using a small pocket dictionary." ></td>
	<td class="line x" title="132:149	Hereby, associations that are missing in the dictionary are discarded, and of the remaining associations only the top 30 are selected." ></td>
	<td class="line x" title="133:149	The second step exactly corresponds to the computation of associations when given multiple stimulus words as described above." ></td>
	<td class="line x" title="134:149	That is, for each word in the target language vocabulary (comprising all words that in the Guardian corpus occur with a frequency of 100 or higher) the ranks of the 30 translations are determined, and the product of these ranks is computed." ></td>
	<td class="line x" title="135:149	The word obtaining the smallest value for the product is considered to be the translation of the source language word." ></td>
	<td class="line x" title="136:149	This algorithm turned out to be a significant improvement over the previous algorithm described in Rapp (1999) as it provides a better accuracy and at the same time a considerably higher robustness." ></td>
	<td class="line x" title="137:149	Based on this novel algorithm, a large dictionary for German to English was computed." ></td>
	<td class="line x" title="138:149	As for the translation of the source language vectors a base dictionary is required, we adapted for this purpose a small Collins pocket dictionary which comprised in the order of 20 000 entries." ></td>
	<td class="line x" title="139:149	In essence, the adaptation procedure involves deriving word equations from the dictionary, each consisting of the source word and its first translation as mentioned in the dictionary." ></td>
	<td class="line x" title="140:149	To give an impression of the results, the following tables show the top ten computed translations for the six words Historie (history), Leibwchter (bodyguard), Raumfhre (space shuttle), spirituell (spiritual), ukrainisch (Ukranian), and umdenken (rethink)." ></td>
	<td class="line x" title="141:149	Hereby, the columns have the following meanings: 1) Rank of a potential translation 2) Corpus frequency of translation 3) Score assigned to translation 4) Computed translation Historie (history)  1 29453 13.73 history 2 4997 12.87 literature 3 4758 8.74 historical 4 2670 0.67 essay 5 6969 0.11 contemporary 6 18909 -1.72 art 7 18382 -2.81 modern 8 15728 -4.31 writing 9 1447 -5.52 photography 10 2442 -5.53 narrative  Leibwchter (body guard)  1 949 40.02 bodyguard 2 5619 23.34 policeman 3 2535 8.18 gunman 4 26347 3.69 kill 5 9180 2.92 guard 6 401 -0.56 bystander 7 815 -1.24 POLICE 8 8503 -2.33 injured 9 2973 -3.23 stab 10 1876 -3.58 murderer  Raumfhre (space shuttle)  1 1259 46.20 shuttle 2 666 26.25 Nasa 3 473 25.95 astronaut 4 287 25.76 spacecraft 5 1062 16.92 orbit 6 16086 11.72 space 7 525 9.50 manned 8 125 7.69 cosmonaut 9 254 5.24 mir 10 7080 3.70 plane  spirituell (spritual)  1 2964 56.10 spiritual 2 1380 8.34 Christianity 3 7721 8.08 religious 4 9525 4.10 moral 5 1414 0.63 secular 6 5685 0.06 emotional 7 4678 -1.04 religion 8 6447 -1.49 intellectual 9 8749 -2.25 belief 10 8863 -4.07 cultural  ukrainisch (Ukrainian)  1 1753 50.69 Ukrainian 2 22626 39.88 Russian 3 3205 29.25 Ukraine 4 34572 23.63 Soviet 108 5 978 21.13 Lithuanian 6 1005 18.88 Kiev 7 10968 15.07 Gorbachev 8 10209 14.51 Yeltsin 9 16616 13.38 republic 10 502 11.71 Latvian  umdenken (rethink)  1 1119 20.76 rethink 2 248 15.46 reassessment 3 84109 13.39 change 4 12497 12.13 reform 5 236 10.00 reappraisal 6 9220 9.97 improvement 7 5212 9.48 implement 8 1139 8.25 overhaul 9 13550 7.89 unless 10 9807 7.88 immediate  6 Summary It could be shown that word associations to multiple stimuli as collected from test persons can be predicted with reasonable accuracy using a simulation program that analyzes the co-occurrences of words in texts." ></td>
	<td class="line x" title="142:149	This result makes the automatic construction of an associative thesaurus of responses to multiple stimuli feasible." ></td>
	<td class="line x" title="143:149	Note that such a thesaurus could not realistically be compiled by collecting the responses of human subjects as there are too many possible combinations of stimuli." ></td>
	<td class="line x" title="144:149	Finally, by looking at two sample applications we showed the pracical utility of the method." ></td>
	<td class="line x" title="145:149	In principle, there should be many more applications, as all utterances and texts can be considered as collections of stimulus words." ></td>
	<td class="line x" title="146:149	A notable one is search word generation in the context of internet search engines." ></td>
	<td class="line x" title="147:149	Of course, all existing algorithms for speech and text processing, although often not claiming any cognitive plausibility, necessarily also have some implicit mechanisms that deal with multiword stimuli." ></td>
	<td class="line x" title="148:149	We nevertheless hope that the specific perspective that we presented here may add to a better understanding of the underlying cognitive mechanisms, and that it offers a systematic way of approaching these challenges." ></td>
	<td class="line x" title="149:149	7 Acknowledgments This research was in part supported by a Marie Curie Intra European Fellowship within the 6th European Community Framework Programme." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="Data not found"></td>
	<td class="line x" title="1:191	Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 3340 Manchester, August 2008 Graph-based Clustering for Semantic Classication of Onomatopoetic Words Kenichi Ichioka Interdiscipli nary Graduate School of Medicine and Engineering University of Yamanashi, Japan g07mk001@yamanashi.ac.jp Fumiyo Fukumoto Interdiscipl inary Graduate School of Medicine and Engineering University of Yamanashi, Japan fukumoto@yamanashi.ac.jp Abstract This paper presents a method for semantic classication of onomatopoe tic words like uni3072.891uni3085.910uni30FC.660uni3072.891uni3085.910uni30FC.660 (hum) and uni304B.852uni3089.914uni3093.924 uni3053.860uni308D.918uni3093.924 (clip clop) which exist in every language, especially Japanese being rich in onomat opoetic words." ></td>
	<td class="line x" title="2:191	We used a graph-based clustering algorithm called Newman clustering." ></td>
	<td class="line x" title="3:191	The algorithm calculates a simple quality function to test whether a particular division is meaning ful." ></td>
	<td class="line x" title="4:191	The quality function is calculated based on the weights of edges between nodes." ></td>
	<td class="line x" title="5:191	We combin ed two different similarity measure s, distributional similarit y, and orthographic similarity to calculate weights . The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure . 1 Introduction Onomatop oeia which we call onomato poetic word (ono word) is the formati on of words whose sound is imitative of the sound of the noise or action designated, such as hiss (McLeod, 1991)." ></td>
	<td class="line x" title="6:191	It is one of the linguistic features of Japanese." ></td>
	<td class="line x" title="7:191	Consider two sentences from Japanese." ></td>
	<td class="line x" title="8:191	(1) uni79C1.2226uni306F.888uni5ECA.4051uni4E0B.1340uni306E.887uni30B9.949uni30EA.998uni30C3.959uni30D1.973uni306E.887uni97F3.1339uni3067.880uni8D77.1609uni3053.860uni3055.862uni308C.917uni305F.872uni306E.887uni3067.880uni3001.634 uni3068.881uni3066.879uni3082.907uni7720.3774uni3044.845uni3002.635 Im too sleepy because I awoke to the slippers in the hall. c2008." ></td>
	<td class="line x" title="9:191	Licensed under the Creative Commons Attribution-Noncommer cial-Sh are Alike 3.0 Unported license (http://creati vecommon s.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:191	Some rights reserved." ></td>
	<td class="line x" title="11:191	(2) uni79C1.2226uni306F.888uni5ECA.4051uni4E0B.1340uni3092.923uni3071.890uni305F.872uni3071.890uni305F.872uni8D70.2808uni308B.916uni30B9.949uni30EA.998uni30C3.959uni30D1.973uni306E.887uni97F3.1339uni3067.880uni8D77.1609 uni3053.860uni3055.862uni308C.917uni305F.872uni306E.887uni3067.880uni3001.634uni3068.881uni3066.879uni3082.907uni7720.3774uni3044.845uni3002.635 Im too sleepy because I awoke to the pit-apat of slippers in the hall. Sentenc es (1) and (2) are almost the same sense." ></td>
	<td class="line x" title="12:191	However, sentence (2) which includes ono word, uni3071.890uni305F.872uni3071.890uni305F.872 (pit-a-pat) is much better to make the scene alive, or represents an image clearly." ></td>
	<td class="line x" title="13:191	Therefo re large-scale semantic resource of ono words is indispensable for not only NLP, but also many semantic-oriented applications such as Questio n Answerin g, Paraphras ing, and MT systems." ></td>
	<td class="line x" title="14:191	Although several machine-readable dictionaries which are ne-grained and large-scale semantic knowledge like WordNet, COMLEX, and EDR dictionary exist, there are none or few onomatopo etic thesaurus." ></td>
	<td class="line x" title="15:191	Because (i) it is easy to understand its sense of ono word for Japanese, and (ii) it is a fast-changing linguistic expressions, as it is a vogue word." ></td>
	<td class="line x" title="16:191	Therefor e, considering this resource scarcity problem, semantic classication of ono words which do not appear in the resource but appear in corpora is very importa nt." ></td>
	<td class="line x" title="17:191	In this paper, we focus on Japanese onomatopoetic words, and propose a method for classifying them into a set with similar meanin g. We used the Web as a corpus to collect ono words, as they appear in different genres of dialogues including broadcast news, novels and comics, rather than a well-edi ted, balanced corpus like newspaper articles." ></td>
	<td class="line x" title="18:191	The problem using a large, heterogeneous collection of Web data is that the Web counts are far more noisy than counts obtained from textual corpus." ></td>
	<td class="line x" title="19:191	We thus used a graph-based clustering algorithm, called Newman clustering for classifying ono words." ></td>
	<td class="line x" title="20:191	The algorithm does not simply calculate the number of shortest paths between pairs of nodes, but instead calculates a quality function 33 of how good a cluster structure found by an algorithm is, and thus makes the computation far more efcient." ></td>
	<td class="line x" title="21:191	The efcacy of the algorithm depends on a quality function which is calculated by using the weights of edges between nodes." ></td>
	<td class="line x" title="22:191	We combined two different similarity measures, and used them to calculate weights." ></td>
	<td class="line x" title="23:191	One is co-occurrence based distributional similarity measure . We tested mutual informa tion (MI) and a  2 statistic as a similarity measure . Another is orthographic similarity which is based on a feature of ono words called sound symbolism." ></td>
	<td class="line x" title="24:191	Sound symbol ism indicates that phonemes or phonetic sequences express their senses." ></td>
	<td class="line x" title="25:191	As ono words imitate the sounds associated with the objects or actions they refer to, their phonetic sequences provide semantic clues for classication." ></td>
	<td class="line x" title="26:191	The empirical results are encouraging, and showed a 9.0% improvement over the baseline single distributio nal similarity measure . 2 Previous Work There are quite a lot of work on semant ic classication of words with corpus-based approach." ></td>
	<td class="line x" title="27:191	The earliest work in this direction are those of (Hindle, 1990), (Lin, 1998), (Dagan et al., 1999), (Chen and Chen, 2000), (Geffet and Dagan, 2004) and (Weeds and Weir, 2005)." ></td>
	<td class="line x" title="28:191	They used distributional similarity." ></td>
	<td class="line x" title="29:191	Similarity measure s based on distributional hypothesis compar e a pair of weighte d feature vectors that characterize two words." ></td>
	<td class="line x" title="30:191	Features typically correspond to other words that co-occur with the characterized word in the same context." ></td>
	<td class="line x" title="31:191	Lin (1998) proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus." ></td>
	<td class="line x" title="32:191	He compared the result of automat ically created thesaurus with WordNet and Roget, and reported that the result was signicantly closer to WordNet than Roget Thesaurus was." ></td>
	<td class="line x" title="33:191	Graph representations for word similarity have also been proposed by several researchers (Jannink and Wiederhold, 1999; Galley and McKeown, 2003; Muller et al., 2006)." ></td>
	<td class="line x" title="34:191	Sinha and Mihalcea (2007) proposed a graph-based algorithm for unsupervised word sense disambiguation which combines several semanti c similarity measures including Resniks metric (Resnik , 1995), and algorithms for graph centrality." ></td>
	<td class="line x" title="35:191	They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5  8% as compar ed to the previous work (Mihalcea, 2005)." ></td>
	<td class="line x" title="36:191	In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition." ></td>
	<td class="line x" title="37:191	The graph structure is built by linking pairs of words which participate in particular syntactic relationships." ></td>
	<td class="line x" title="38:191	An incremen tal cluster-building algorithm using the graph structure achieved 82% accuracy at a lexical acquisition task, evaluated against WordNet 10 classes, and each class consists of 20 words." ></td>
	<td class="line x" title="39:191	Matsuo et al.(2006) proposed a method of word clustering based on a word similarity measure by Web counts." ></td>
	<td class="line x" title="41:191	They used Newman clustering for clustering algorithm." ></td>
	<td class="line x" title="42:191	They evaluated their method using two sets of word classes." ></td>
	<td class="line x" title="43:191	One is derived from the Web data, and another is from WordNet." ></td>
	<td class="line x" title="44:191	1 Each set consists of 90 noun words." ></td>
	<td class="line x" title="45:191	They reported that the results obtained by Newman clustering were better than those obtained by average-link agglomerative clustering." ></td>
	<td class="line x" title="46:191	Our work is similar to their method in the use of Newman clustering." ></td>
	<td class="line x" title="47:191	However, they classied Japanese noun words, while our work is the rst to aim at detecting semantic classication of onomato poetic words." ></td>
	<td class="line x" title="48:191	Moreover, they used only a single similarity metric, cooccurrence based similarity, while Japanese, especially kanji characters of noun words provide semantic clues for classifying words." ></td>
	<td class="line x" title="49:191	3 System Description The method consists of three steps: retrieving cooccurrences using the Web, calculating similarity between ono words, and classifying ono words by using Newman clustering." ></td>
	<td class="line x" title="50:191	3.1 Retrieving Co-occurrence using the Web One criterion for calculating semantic similarity between ono words is co-occurrence based similarity." ></td>
	<td class="line x" title="51:191	We retrieved frequency of two ono words occurring together by using the Web search engine, Google." ></td>
	<td class="line x" title="52:191	The similarity between them is calculated based on their co-occurrence frequency." ></td>
	<td class="line x" title="53:191	Like much previous work on semant ic classication of the lexicons, our assumption is that semantically similar words appear in similar contexts." ></td>
	<td class="line x" title="54:191	A lot of strategies for searching words are provided in Google." ></td>
	<td class="line x" title="55:191	Of these we focused on two method s: Boolean search AND and phrase-based search." ></td>
	<td class="line x" title="56:191	1 They used WordNet hypernym information." ></td>
	<td class="line x" title="57:191	It consists of 10 classes." ></td>
	<td class="line x" title="58:191	They assigned 90 Japanese noun words to each class." ></td>
	<td class="line x" title="59:191	34 When we use AND boolean search, i.e., (O i O j ) where O i and O j are ono words, we can retrieve the number of documents which include both O i and O j . In contrast, phrase-based search, i.e., (O i O j ) retrieves documents which include two adjacent words O i and O j . 3.2 Similarity Measures The second step is to calculate semantic similar ity between ono words." ></td>
	<td class="line x" title="60:191	We combined two different similar ity measur es: the co-occurrence frequency based similarity and orthographic similarit y measures." ></td>
	<td class="line o" title="61:191	3.2.1 Co-occurrence based Similarit y Measure We focused on two popular measures : the mutual information (MI) and  2 statistics." ></td>
	<td class="line x" title="62:191	1." ></td>
	<td class="line oc" title="63:191	Mutual Informatio n Church and Hanks (1990) discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena, ranging from semanti c relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence preferences between verbs and prepositions (content word/function word)." ></td>
	<td class="line x" title="64:191	Let O i and O j be ono words retrieved from the Web." ></td>
	<td class="line o" title="65:191	The mutual information MI(O i , O j ) is dened as: MI(O i , O j ) = log S all  f(O i , O j ) S O i  S O j , (1) where S O i = summationdisplay kO all f(O i , O k ), (2) S all = summationdisplay O i O all S O i ." ></td>
	<td class="line x" title="66:191	(3) In Eq." ></td>
	<td class="line x" title="67:191	(1), f(O i , O j ) refers to the frequency of O i and O j occurring together, and O all is a set of all ono words retrieved from the Web." ></td>
	<td class="line x" title="68:191	2." ></td>
	<td class="line x" title="69:191	 2 statistic The  2 (O i , O j ) is dened as:  2 (O i , O j ) = f(O i , O j )  E(O i , O j ) E(O i , O j ) , (4) where E(O i , O j ) = S O i  S O j S all ." ></td>
	<td class="line x" title="70:191	(5) S O i and S all in Eq." ></td>
	<td class="line x" title="71:191	(5) refer to Eq." ></td>
	<td class="line x" title="72:191	(2) and (3), respectively." ></td>
	<td class="line x" title="73:191	A major difference between  2 and MI is that the former is a normali zed value." ></td>
	<td class="line x" title="74:191	3.2.2 Orthographic Similarity Measure Orthogr aphic similarity has been widely used in spell checking and speech recognition systems (Damerau, 1964)." ></td>
	<td class="line x" title="75:191	Our orthographic similar ity measure is based on a unit of phonetic sequence." ></td>
	<td class="line x" title="76:191	The key steps of the similarit y between two ono words is dened as: 1." ></td>
	<td class="line x" title="77:191	Convert each ono word into phonetic sequences." ></td>
	<td class="line x" title="78:191	The hiragana characters of ono word are converted into phonetic sequences by a unique rule." ></td>
	<td class="line x" title="79:191	Basically, there are 19 consonants and 5 vowels, as listed in Table 1." ></td>
	<td class="line x" title="80:191	Table 1: Japanese consonants and vowels Conson ant , N, Q, h, hy, k, ky, m, my, n, ny, r, ry, s, sy, t, ty, w, y Vowel a, i, u, e, o Conside r phonetic sequences hyu-hyu- of ono word uni3072.891uni3085.910uni30FC.660uni3072.891uni3085.910uni30FC.660 (hum)." ></td>
	<td class="line x" title="81:191	It is segmented into 4 consonants hy, -, hy and -, and two vowels, u and u." ></td>
	<td class="line x" title="82:191	2." ></td>
	<td class="line x" title="83:191	Form a vector in n-dimens ional space." ></td>
	<td class="line x" title="84:191	Each ono word is represented as a vector of consonants(vowels), where each dimension of the vector corresponds to each consonant and vowel, and each value of the dimension is frequencies of its corresponding consonant(vowel)." ></td>
	<td class="line x" title="85:191	3." ></td>
	<td class="line x" title="86:191	Calculat e orthographic similar ity." ></td>
	<td class="line x" title="87:191	The orthographic similar ity between ono words, O i and O j is calculated based on the consonant and vowel distributio ns." ></td>
	<td class="line x" title="88:191	We used two popular measure s, i.e., the cosine similarity, and -skew divergence." ></td>
	<td class="line x" title="89:191	The cosine measures the similarity of the two vectors by calculating the cosine of the angle between vectors." ></td>
	<td class="line x" title="90:191	-skew divergence is dened as: div(x, y) = D(y ||   x + (1  )  y), where D(x||y) refers to Kullback-Leibler and dened as: D(x||y) = n summationdisplay i=1 x i  log x i y i ." ></td>
	<td class="line x" title="91:191	(6) 35 Lee (1999) reported the best results with  = 0.9." ></td>
	<td class="line x" title="92:191	We used the same value." ></td>
	<td class="line x" title="93:191	We dened a similari ty metric by combini ng co-occurrence based and orthographic similarit y measures 2 : Sim(O i , O j ) = MI(O i , O j )  (Cos(O i , O j ) + 1) (7) 3.3 The Newman Clustering Algorithm We classied ono words collected from the WWW." ></td>
	<td class="line x" title="94:191	Therefo re, the clustering algorithm should be efcient and effective even in the very high dimensional spaces." ></td>
	<td class="line x" title="95:191	For this purpose, we chose a graphbased clustering algorithm, called Newman clustering." ></td>
	<td class="line x" title="96:191	The Newman clustering is a hierarchical clustering algorithm which is based on Network structure (Newman, 2004)." ></td>
	<td class="line x" title="97:191	The network structure consists of nodes within which the node-node connections are edges." ></td>
	<td class="line x" title="98:191	It produces some division of the nodes into communiti es, regardless of whether the network structure has any natural such division." ></td>
	<td class="line x" title="99:191	Here, communit y or cluster have in common that they are groups of densely interconnected nodes that are only sparsely connected with the rest of the network." ></td>
	<td class="line x" title="100:191	To test whether a particular division is meaning ful a quality function Q is dened: Q = summationdisplay i (e ii  a 2 i ) where e ij is the sum of the weight of edges between two commun ities i and j divided by the sum of the weight of all edges, and a i = summationtext j e ij , i.e., the expected fraction of edges within the cluster." ></td>
	<td class="line x" title="101:191	Here are the key steps of that algorithm: 1." ></td>
	<td class="line x" title="102:191	Given a set of n ono words S = {O 1 ,   , O n }." ></td>
	<td class="line x" title="103:191	Create a network structure which consists of nodes O 1 ,   , O n , and edges." ></td>
	<td class="line x" title="104:191	Here, the weight of an edge betwee n O i and O j is a similarity value obtained by Eq." ></td>
	<td class="line x" title="105:191	(7)." ></td>
	<td class="line x" title="106:191	If the network density of ono words is smaller than the parameter , we cut the edge." ></td>
	<td class="line x" title="107:191	Here, network density refers to a ratio selected from the topmost edges." ></td>
	<td class="line o" title="108:191	For example , if it 2 When we used  2 statistic as a co-occurren ce based similarity, MI in Eq." ></td>
	<td class="line x" title="109:191	(7) is replaced by  2 . In a similar way, Cos(O i , O j ) is replaced by max  div(x, y), where max is the maximum value among all div(x, y) values." ></td>
	<td class="line x" title="110:191	was 0.9, we used the topmost 90% of all edges and cut the remain s, where edges are sorted in the descending order of their similarity values." ></td>
	<td class="line x" title="111:191	2." ></td>
	<td class="line x" title="112:191	Starting with a state in which each ono word is the sole member of one of n communi ties, we repeatedly joined communiti es together in pairs, choosing at each step the join that results in the greatest increase." ></td>
	<td class="line x" title="113:191	3." ></td>
	<td class="line x" title="114:191	Suppose that two communit ies are merged into one by a join operation." ></td>
	<td class="line x" title="115:191	The change in Q upon joining two communiti es i and j is given by: triangleQ ij = e ij + e ji  2a i a j = 2(e ij  a i a j ) 4." ></td>
	<td class="line x" title="116:191	Apply step 3." ></td>
	<td class="line x" title="117:191	to every pair of communi ties." ></td>
	<td class="line x" title="118:191	5." ></td>
	<td class="line x" title="119:191	Join two commun ities such that triangleQ is maximum and create one communit y. If triangleQ < 0, go to step 7." ></td>
	<td class="line x" title="120:191	6. Re-calcu late e ij and a i of the joined community, and go to step 3." ></td>
	<td class="line x" title="121:191	7. Words within the same community are regarded as semantically similar." ></td>
	<td class="line x" title="122:191	The computational cost of the algorithm is known as O((m + n)n) or O(n 2 ), where m and n are the number of edges and nodes, respectively." ></td>
	<td class="line x" title="123:191	4 Experimen ts 4.1 Experimental Setup The data for the classication of ono words have been taken from the Japanese ono dictionary (Ono, 2007) that consisted of 4,500 words." ></td>
	<td class="line x" title="124:191	Of these, we selected 273 words, which occurred at least 5,000 in the docume nt URLs from the WWW." ></td>
	<td class="line x" title="125:191	The minimum frequency of a word was found to be 5,220, while the maximum was about 26 million." ></td>
	<td class="line x" title="126:191	These words are classied into 10 classes." ></td>
	<td class="line x" title="127:191	Word classes and examples of ono words from the dictionary are listed in Table 2." ></td>
	<td class="line x" title="128:191	Id denotes id number of each class." ></td>
	<td class="line x" title="129:191	Sense refers to each sense of ono word within the same class, and Num is the number of words which should be assigned to each class." ></td>
	<td class="line x" title="130:191	Each word 36 Table 2: Onomatop oetic words and # of words in each class Id Sense Num Onomatopo etic words 1 laugh 63 uni3042.843uni3063.876uni306F.888uni3063.876uni306F.888 (a,Q,h,a,Q,h,a), uni3042.843uni306F.888uni306F.888 (a,h,a,h,a), uni308F.920uni306F.888uni306F.888 (w,a,h,a,h,a) uni3042.843uni306F.888uni3042.843uni306F.888 (a,h,a,a,h,a), uni3044.845uni3072.891uni3072.891 (i,h,i,h,i), uni3046.847uni3063.876uni3057.864uni3063.876uni3057.864 (u,Q,s,i,Q,s,i),    2 cry 34 uni3042.843uni30FC.660uni3093.924 (a,,N), uni3046.847uni308F.920uni30FC.660uni3093.924 (u,w,a,,N), uni3042.843uni3093.924uni3042.843uni3093.924 (a,N,a,N), uni3048.849uni3093.924uni3048.849uni3093.924 (e,N,e,N) uni3046.847uni308B.916uni3046.847uni308B.916 (u,r,u,u,r,u), uni3046.847uni308B.916uni308B.916uni3093.924 (u,r,u,r,u,N), uni3046.847uni308B.916uni3063.876(u,r,u,Q), uni3048.849uni30FC.660uni3093.924 (e,,N),    3 pain 34 uni3044.845uni304C.853uni3044.845uni304C.853 (i,k,a,i,k,a), uni3072.891uni308A.915uni3072.891uni308A.915 (h,i,r,i,h,i,r,i), uni304C.853uni3058.865uni304C.853uni3058.865 (k,a,s,i,k,a,s,i) uni304C.853uni3093.924uni304C.853uni3093.924 (k,a,N,k,a,N),    4 anger 33 uni304B.852uni30FC.660uni3063.876(k,a,,Q), uni304B.852uni3061.874uni3093.924 (k,a,t,i,N), uni304B.852uni3064.877uni3093.924 (k,a,t,u,N), uni304B.852uni3063.876(k,a,Q), uni304B.852uni3063.876uni304B.852 (k,a,Q,k,a), uni304C.853uni307F.904uni304C.853uni307F.904 (k,a,m,i,k,a,m, i), uni304B.852uni308A.915uni304B.852uni308A.915 (k,a,r,i,k,a,r,i), uni304B.852uni3093.924uni304B.852uni3093.924 (k,a,N,k,a,N),    5 spook 31 uni3042.843uni308F.920uni308F.920 (a,w,a,w,a), uni3046.847uni304E.855uni3083.908uni30FC.660 (u,ky,a,), uni304C.853uni30FC.660uni3093.924 (k,a,,N), uni304E.855uni304F.856 (k,i,k,u) uni304E.855uni304F.856uni3063.876(k,i,k,u,Q), uni304E.855uni304F.856uni308A.915 (k,i,k,u,r,i), uni304E.855uni304F.856uni3093.924 (k,i,k,u,N),    6 panic 25 uni3042.843uni304F.856uni305B.868uni304F.856 (a,k,u,s,e,k,u), uni3042.843uni305F.872uni3075.894uni305F.872 (a,t,a,h,u,t,a), uni3042.843uni3063.876uni3077.896uni3042.843uni3063.876uni3077.896 (a,Q,h,u,a,Q,h,u), uni3042.843uni308F.920uni3042.843uni308F.920 (a,w,a,a,w,a)   7 bloodless 27 uni304B.852uni304F.856uni3063.876(k,a,k,u,Q), uni304C.853uni304F.856uni3063.876(k,a,k,u,Q), uni304C.853uni3063.876uni304B.852uni308A.915 (k,a,Q,k,a,r ,i), uni304C.853uni3063.876uni304F.856uni308A.915 (k,a,Q,k,u,r ,i) uni304B.852uni304F.856uni3093.924 (k,a,k,u,N), uni304E.855uni3083.908uni3075.894uni3093.924 (ky,a,h,u,N), uni304E.855uni3085.910uni30FC.660 (ky,u,),    8 deem 13 uni3046.847uni3063.876uni3068.881uni308A.915 (u,Q,t,o,r,i), uni304D.854uni3085.910uni30FC.660uni3093.924 (ky,u,,N), uni304D.854uni3085.910uni3093.924 (ky,u,N) uni3064.877uni304F.856uni3065.878uni304F.856 (t,u,k,u,t,u,k,u),    9 feel delight 6 uni3046.847uni3057.864uni3046.847uni3057.864 (u,s,i,u,s,i ), uni304D.854uni3083.908uni3074.893uni304D.854uni3083.908uni3074.893 (ky,a,h,i,ky,a,h,i) uni3046.847uni306F.888uni3046.847uni306F.888 (u,,h,a,,u,,h,a), uni307B.900uni3044.845uni307B.900uni3044.845 (h,o,i,h,o,i), uni308B.916uni3093.924uni308B.916uni3093.924 (r,u,N,r,u,N),    10 balk 7 uni3044.845uni3058.865uni3044.845uni3058.865 (i,s,i,i,s,i), uni3046.847uni3058.865uni3046.847uni3058.865 (u,s,i,u,s,i), uni304A.851uni305A.867uni304A.851uni305A.867 (o,s,u,o,s,u) uni3050.857uni3060.873uni3050.857uni3060.873 (k,u,t,a,k,u,t,a), uni3082.907uni3058.865uni3082.907uni3058.865 (m,o,s,i,m,o,s,i),    Total 273 marked with bracket denotes phonetic sequences consisting of consonants and vowels." ></td>
	<td class="line x" title="131:191	We retrieved co-occurrences of ono words shown in Table 2 using the search engine, Google." ></td>
	<td class="line x" title="132:191	We applied Newman clustering to the input words." ></td>
	<td class="line x" title="133:191	For comparison, we implemen ted standard kmeans which is often used as a baseline, as it is one of the simplest unsupervised clustering algorithms, and compare d the results to those obtained by our method." ></td>
	<td class="line x" title="134:191	We used Euclidean distance (L 2 norm) as a distance metric used in the k-means." ></td>
	<td class="line x" title="135:191	For evaluation of classication, we used Precisio n(P rec), Recall(Rec), and F-measur e which is a measure that balances precision and recall (Bilenko et al., 2004)." ></td>
	<td class="line x" title="136:191	The precise denitions of these measur es are given below: P rec = #P airsCorrectlyP redictedInSamecluster #T otalP airsP redictedInSameCluster (8) Rec = #P airsCorrectlyP redictedInSameCluster #T otalP airsInSameCluster (9) F  measure = 2  P rec Rec (P rec + Rec) (10) 4.2 Results The results are shown in Table 3." ></td>
	<td class="line x" title="137:191	Co-occ . & Sounds  in Data refers to the results obtained by our method." ></td>
	<td class="line x" title="138:191	Co-oc c. denotes the results obtained by a single measur e, co-occurrence based distributional similarity measure, and Sound s shows the results obtained by orthographic similarity." ></td>
	<td class="line x" title="139:191	 in Table 3 shows a paramet er  used in the Newman clustering." ></td>
	<td class="line x" title="140:191	3 Table 3 shows best performance of each method against  values." ></td>
	<td class="line x" title="141:191	The best result was obtained when we used phrase-based search and a combin ed measure of co-occurrence(MI) and sounds (cos), and F-score was 0.451." ></td>
	<td class="line x" title="142:191	4.2.1 AND vs phrase-based search Table 3 shows that overall the results using phrase-based search were better than those of AND search, and the maximum difference of Fscore betwee n them was 20.6% when we used a combin ed measure . We note that AND boolean search did not consider the position of a word in a document, while our assumpt ion was that semantica lly similar words appeared in similar contexts." ></td>
	<td class="line x" title="143:191	As a result, two ono words which were not semant ically similar were often retrieved by AND boolean search." ></td>
	<td class="line x" title="144:191	For example , consider two antonymous words, a,h,a,h ,a (grinning broadly) and w,a,,N (Wah, Wah)." ></td>
	<td class="line x" title="145:191	The co-occurrence frequency obtained by AND was 5,640, while that of phrase-based search was only one." ></td>
	<td class="line x" title="146:191	The observation shows that we nd phrase-based search to be a good choice." ></td>
	<td class="line x" title="147:191	3 In case of k-means, we used the weights which satises network density." ></td>
	<td class="line x" title="148:191	37 Table 3: Classication results Data Algo." ></td>
	<td class="line x" title="149:191	Sim (Co-occ.)" ></td>
	<td class="line x" title="150:191	Sim (Sounds) Search method  Prec Rec F # of clusters  2 AND .050 .134 .799 .229 10 cos Phrase .820 .137 .880 .236 10 MI AND .050 .134 .562 .216 10 k-means Phrase .150 .190 .618 .289 10  2 AND .680 .134 .801 .229 10 div Phrase .280 .138 .882 .238 10 MI AND .040 .134 .602 .219 10 Co-occ." ></td>
	<td class="line o" title="151:191	& Sounds Phrase .140 .181 .677 .285 10  2 AND .170 .182 .380 .246 9 cos Phrase .100 .322 .288 .304 14 MI AND .050 .217 .282 .245 13 Newman Phrase .080 .397 .520 .451 7  2 AND .130 .212 .328 .258 9 div Phrase .090 .414 .298 .347 17 MI AND .090 .207 .325 .253 6 Phrase .160 .372 .473 .417 8  2 AND .460 .138 .644 .227 10 k-means  Phrase .110 .136 .870 .236 10 MI AND .040 .134 .599 .219 10 Co-occ." ></td>
	<td class="line o" title="152:191	Phrase .150 .191 .588 .286 10  2 AND .700 .169 .415 .240 8 Newman  Phrase .190 .301 .273 .286 14 MI AND .590 .159 .537 .245 3 Phrase .140 .275 .527 .361 5 k-means  cos  .050 .145 .321 .199 10 Sounds div  .020 .126 .545 .204 10 Newman  cos  .270 .151 .365 .213 4 div  .350 .138 .408 .206 3 4.2.2 A single vs combined similarity measure To examine the effectiveness of the combined similarity measure, we used a single measur e as a quality function of the Newman clustering, and compar ed these results with those obtained by our method . As shown in Table 3, the results with combin ing similarity measur es improved overall performance." ></td>
	<td class="line x" title="153:191	In the phrase-based search, for example, the F-score using a combine d measur e Coocc(MI) & Sounds( cos) was 23.8% better than the baseline single measure Sounds(cos), and 9.0% better a single measure Co-occ(MI)." ></td>
	<td class="line x" title="154:191	Figure 1 shows F-score by Co-occ(MI) & Sounds (cos) and Co-occ (MI) against changes in ." ></td>
	<td class="line x" title="155:191	These curves were obtained by phrasebased search." ></td>
	<td class="line x" title="156:191	We can see from Figure 1 that the F-score by a combined measure Co-occ(MI) & Sounds (cos) was better than Co-occ (MI) with  value ranged from .001 to .25." ></td>
	<td class="line x" title="157:191	One possible reason for the difference of F-score between them is the edges selected by varying ." ></td>
	<td class="line x" title="158:191	Figure 2 shows the results obtained by each single measure , and a combin ed measur e to examine how the edges selected by varying  affect overall performance, Fmeasur e. Precision in Figure 2 refers to the ratio of correct ono word pairs (edges) divided by the total number of edges." ></td>
	<td class="line x" title="159:191	Here, correct ono word pairs were created by using the Japanese ono dictionary, i.e., we extracted word pairs within the same sense of the dictionary." ></td>
	<td class="line o" title="160:191	Surprisingly, there were no signicant difference between a combine d measure Co-occ (MI) & Sounds(cos) and a single measure Co-oc c(MI) curves, while the precision of a single measur e Sounds was constantly worse than that obtained by a combin ed measure . Another possible reason for the difference of F-score is due to product of MI and Cos in Eq." ></td>
	<td class="line x" title="161:191	(7)." ></td>
	<td class="line x" title="162:191	Further work is needed to analyze these results in detail." ></td>
	<td class="line x" title="163:191	4.2.3 k-means vs Newman algorithms We examined the results obtained by standard kmeans and Newman clustering algorithms." ></td>
	<td class="line x" title="164:191	As can be seen clearly from Table 3, the results with Newman clustering were better than those of the standard k-means at all search and similarity measures , especially the result obtained by Newman clustering showed a 16.2 % improvement over the kmeans when we used Co-occ.( MI) & Sounds(cos) & phrase-based search." ></td>
	<td class="line x" title="165:191	We recall that we used 273 ono words for clustering." ></td>
	<td class="line x" title="166:191	However, Newman clustering is applicable for a large number of nodes and edges without decreasing accuracy too much, as it does not simply calculate the number of short38  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 F-measure Lower bound values Co-occ(MI) Co-occ(MI) & Sounds(cos) Figure 1: F-score against  values  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 Precision Lower bound values Co-occ(MI) Co-occ(MI) & Sounds(cos) Sounds(cos) Figure 2: Precision against  values est paths between pairs of nodes, but instead calculates a simple quality function." ></td>
	<td class="line x" title="167:191	Quantitative evaluation by applying the method to larger data from the Web is worth trying for future work." ></td>
	<td class="line x" title="168:191	4.3 Qualitati ve Analysis of Errors Finally, to provide feedback for further development of our classication approach, we performed a qualitative analysis of errors." ></td>
	<td class="line x" title="169:191	Consider the following clusters (the Newman output for Co-occ .(MI), Sounds(cos) and phrase-based search), where each parenthetic sequences denotes ono word: A1: (t,o,Q) (t,o,Q,t,o) (t,o,Q,k, i,N,t,o,Q,k ,i,N) A2: (o,h,o,h,o), (e,h,e,h,e), (h,e,h,e,h,e), (o,-,o,-) A3: (u,s,i,u,s, i), (m,o,s,i,m,o,s,i), (m,o,s,o,m,o,s,o) Three main error types were identied: 1." ></td>
	<td class="line x" title="170:191	Morpho logical idiosyncrasy: This was the most frequent error type, exemplied in A1, where (t,o,Q,k,i,N,t,o,Q,k,i,N) (pain sense) was incorrectly clustered with other two words (laugh sense) merely because orthographic similarit y between them was large, as the phonetics sequences of (t,o,Q,k,i,N,t,o,Q,k,i,N) included t and o." ></td>
	<td class="line x" title="171:191	2." ></td>
	<td class="line x" title="172:191	Sparse data: Many of the low frequency ono words performed poorly." ></td>
	<td class="line x" title="173:191	In A2, (o,-,o,-) (cry sense) was classied with other three words (laugh sense) because it occurred few in our data." ></td>
	<td class="line x" title="174:191	3." ></td>
	<td class="line x" title="175:191	Problems of polysemy: In A3, (m,o,s,o,m,o,s,o) (pain sense) was clustered with other two words (balk sense) of its gold standard class." ></td>
	<td class="line x" title="176:191	However, the ono word has another sense, balk sense when it co-occurred with action verbs." ></td>
	<td class="line x" title="177:191	5 Conclusion We have focused on onomatopoetic words, and proposed a method for classifying them into a set of semant ically similar words." ></td>
	<td class="line x" title="178:191	We used a graphbased clustering algorithm, called Newman clustering with a combined different similar ity measures." ></td>
	<td class="line x" title="179:191	The results obtained by using the Web data showed a 9.0% improvement over the baseline single distributional similarity measure." ></td>
	<td class="line x" title="180:191	There are number of interesting directions for future research." ></td>
	<td class="line x" title="181:191	The distributional similarit y measure we used is the basis of the ono words, while other content words such as verbs and adverbs are also effective for classifying ono words." ></td>
	<td class="line x" title="182:191	In the future, we plan to investigate the use of these words and work on improving the accuracy of classication." ></td>
	<td class="line x" title="183:191	As shown in Table 2, many of the ono words consist of duplicative character sequences such as h and a of a,h,a,h,a, and h and i of i,h,i,h,i . Moreover, characters which consist of ono words within the same class match." ></td>
	<td class="line x" title="184:191	For example, the hiragana character uni306F.888 (h,a) frequently appears in laugh sense class." ></td>
	<td class="line x" title="185:191	These observations indicate that integrating edit-distance and our current similar ity measure will improve overall performance." ></td>
	<td class="line x" title="186:191	Another interesting direction is a problem of polysemy." ></td>
	<td class="line x" title="187:191	It clearly supports the classication of (Ono, 2007) to insist that some ono words belong to more than one cluster." ></td>
	<td class="line x" title="188:191	For example, (i,s,o,i,s,o)  has at least two senses, panic and feel delight sense." ></td>
	<td class="line x" title="189:191	In order to accommodate this, we 39 should apply an appropriate soft clustering technique (Tishby et al., 1999; Reichardt and Bornholdt, 2006; Zhang et al., 2007)." ></td>
	<td class="line x" title="190:191	Acknowledgment s We would like to thank the anonymous reviewers for their helpful suggestions." ></td>
	<td class="line x" title="191:191	This material is supported in part by the Grant-in-aid for the Japan Society for the Promotion of Science(JSPS)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1051
Collocation Extraction Using Monolingual Word Alignment Method
Liu, Zhanyi;Wang, Haifeng;Wu, Hua;Li, Sheng;"></td>
	<td class="line x" title="1:237	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 487495, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:237	c 2009 ACL and AFNLP Collocation Extraction Using Monolingual Word Alignment Method  Zhanyi Liu 1,2 , Haifeng Wang 2 , Hua Wu 2 , Sheng Li 1 1 Harbin Institute of Technology, Harbin, China 2 Toshiba (China) Research and Development Center, Beijing, China {liuzhanyi,wanghaifeng,wuhua}@rdc.toshiba.com.cn lisheng@hit.edu.cn    Abstract Statistical bilingual word alignment has been well studied in the context of machine translation." ></td>
	<td class="line x" title="3:237	This paper adapts the bilingual word alignment algorithm to monolingual scenario to extract collocations from monolingual corpus." ></td>
	<td class="line x" title="4:237	The monolingual corpus is first replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language." ></td>
	<td class="line x" title="5:237	Then the monolingual word alignment algorithm is employed to align the potentially collocated words in the monolingual sentences." ></td>
	<td class="line x" title="6:237	Finally the aligned word pairs are ranked according to refined alignment probabilities and those with higher scores are extracted as collocations." ></td>
	<td class="line x" title="7:237	We conducted experiments using Chinese and English corpora individually." ></td>
	<td class="line x" title="8:237	Compared with previous approaches, which use association measures to extract collocations from the co-occurring word pairs within a given window, our method achieves higher precision and recall." ></td>
	<td class="line x" title="9:237	According to human evaluation in terms of precision, our method achieves absolute improvements of 27.9% on the Chinese corpus and 23.6% on the English corpus, respectively." ></td>
	<td class="line x" title="10:237	Especially, we can extract collocations with longer spans, achieving a high precision of 69% on the long-span (>6) Chinese collocations." ></td>
	<td class="line x" title="11:237	1 Introduction Collocation is generally defined as a group of words that occur together more often than by chance (McKeown and Radev, 2000)." ></td>
	<td class="line x" title="12:237	In this paper, a collocation is composed of two words occurring as either a consecutive word sequence or an interrupted word sequence in sentences, such as 'by accident' or 'take  advice'." ></td>
	<td class="line x" title="13:237	The collocations in this paper include phrasal verbs (e.g. 'put on'), proper nouns (e.g. 'New York'), idioms (e.g. 'dry run'), compound nouns (e.g. 'ice cream'), correlative conjunctions (e.g. 'either  or'), and the other commonly used combinations in following types: verb+noun, adjective+noun, adverb+verb, adverb+adjective and adjective+preposition (e.g. 'break rules', 'strong tea', 'softly whisper', 'fully aware', and 'fond of')." ></td>
	<td class="line oc" title="14:237	Many studies on collocation extraction are carried out based on co-occurring frequencies of the word pairs in texts (Choueka et al., 1983; Church and Hanks, 1990; Smadja, 1993; Dunning, 1993; Pearce, 2002; Evert, 2004)." ></td>
	<td class="line o" title="15:237	These approaches use association measures to discover collocations from the word pairs in a given window." ></td>
	<td class="line o" title="16:237	To avoid explosion, these approaches generally limit the window size to a small number." ></td>
	<td class="line n" title="17:237	As a result, long-span collocations can not be extracted 1 . In addition, since the word pairs in the given window are regarded as potential collocations, lots of false collocations exist." ></td>
	<td class="line n" title="18:237	Although these approaches used different association measures to filter those false collocations, the precision of the extracted collocations is not high." ></td>
	<td class="line x" title="19:237	The above problems could be partially solved by introducing more resources into collocation extraction, such as chunker (Wermter and Hahn, 2004), parser (Lin, 1998; Seretan and Wehrli, 2006) and WordNet (Pearce, 2001)." ></td>
	<td class="line x" title="20:237	This paper proposes a novel monolingual word alignment (MWA) method to extract collocation of higher quality and with longer spans only from monolingual corpus, without using any additional resources." ></td>
	<td class="line x" title="21:237	The difference between MWA and bilingual word alignment (Brown et al., 1993) is that the MWA method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment." ></td>
	<td class="line x" title="22:237	The  1  Here, 'span of collocation' means the distance of two words in a collocation." ></td>
	<td class="line x" title="23:237	For example, if the span of the collocation (w 1 , w 2 ) is 6, it means there are 5 words interrupting between w 1  and w 2  in a sentence." ></td>
	<td class="line x" title="24:237	487 monolingual corpus is replicated to generate a parallel corpus, where each sentence pair consists of two identical sentences in the same language, instead of a sentence in one language and its translation in another language." ></td>
	<td class="line x" title="25:237	We adapt the bilingual word alignment algorithm to the monolingual scenario to align the potentially collocated word pairs in the monolingual sentences, with the constraint that a word is not allowed to be aligned with itself in a sentence." ></td>
	<td class="line x" title="26:237	In addition, we propose a ranking method to finally extract the collocations from the aligned word pairs." ></td>
	<td class="line x" title="27:237	This method assigns scores to the aligned word pairs by using alignment probabilities multiplied by a factor derived from the exponential function on the frequencies of the aligned word pairs." ></td>
	<td class="line x" title="28:237	The pairs with higher scores are selected as collocations." ></td>
	<td class="line x" title="29:237	The main contribution of this paper is that the well studied bilingual statistical word alignment method is successfully adapted to monolingual scenario for collocation extraction." ></td>
	<td class="line x" title="30:237	Compared with the previous approaches, which use association measures to extract collocations, our method achieves much higher precision and slightly higher recall." ></td>
	<td class="line x" title="31:237	The MWA method has the following three advantages." ></td>
	<td class="line x" title="32:237	First, it explicitly models the co-occurring frequencies and position information of word pairs, which are integrated into a model to search for the potentially collocated word pairs in a sentence." ></td>
	<td class="line x" title="33:237	Second, a new feature, fertility, is employed to model the number of words that a word can collocate with in a sentence." ></td>
	<td class="line x" title="34:237	Finally, our method can obtain the longspan collocations." ></td>
	<td class="line x" title="35:237	Human evaluations on the extracted Chinese collocations show that 69% of the long-span (>6) collocations are correct." ></td>
	<td class="line x" title="36:237	Although the previous methods could also extract long-span collocations by setting the larger window size, the precision is very low." ></td>
	<td class="line x" title="37:237	In the remainder of this paper, Section 2 describes the MWA model for collocation extraction." ></td>
	<td class="line x" title="38:237	Section 3 describes the initial experimental results." ></td>
	<td class="line x" title="39:237	In Section 4, we propose a method to improve the MWA models." ></td>
	<td class="line x" title="40:237	Further experiments are shown in Sections 5 and 6, followed by a discussion in Section 7." ></td>
	<td class="line x" title="41:237	Finally, the conclusions are presented in Section 8." ></td>
	<td class="line x" title="42:237	2 Collocation Extraction With Monolingual Word Alignment Method 2.1 Monolingual Word Alignment Given a bilingual sentence pair, a source language word can be aligned with its correspond Figure 1." ></td>
	<td class="line x" title="43:237	Bilingual word alignment ing target language word." ></td>
	<td class="line x" title="44:237	Figure 1 shows an example of Chinese-to-English word alignment." ></td>
	<td class="line x" title="45:237	In Figure 1, a word in one language is aligned with its counterpart in the other language." ></td>
	<td class="line x" title="46:237	For examples, the Chinese word ' /tuan-dui' is aligned with its English translation 'team', while the Chinese word ' /fu-ze-ren' is aligned with its English translation 'leader'." ></td>
	<td class="line x" title="47:237	In the Chinese sentence in Figure 1, there are some Chinese collocations, such as ( /tuandui,  /fu-ze-ren)." ></td>
	<td class="line x" title="48:237	There are also some English collocations in the English sentence, such as (team, leader)." ></td>
	<td class="line x" title="49:237	We separately illustrate the collocations in the Chinese sentence and the English sentence in Figure 2, where the collocated words are aligned with each other." ></td>
	<td class="line x" title="50:237	(a) Collocations in the Chinese sentence  (b) Collocations in the English sentence Figure 2." ></td>
	<td class="line x" title="51:237	Word alignments of collocations in sentence Comparing the alignments in Figures 1 and 2, we can see that the task of monolingual collocations construction is similar to that of bilingual word alignment." ></td>
	<td class="line x" title="52:237	In a bilingual sentence pair, a source word is aligned with its corresponding target word, while in a monolingual sentence, a word is aligned with its collocates." ></td>
	<td class="line x" title="53:237	Therefore, it is reasonable to regard collocation construction as a task of aligning the collocated words in monolingual sentences." ></td>
	<td class="line x" title="54:237	                tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . The team leader plays a key role in the project undertaking . The team leader plays a key role in the project undertaking." ></td>
	<td class="line x" title="55:237	The team leader plays a key role in the project undertaking." ></td>
	<td class="line x" title="56:237	                tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong .                  tuan-dui fu-ze-ren zai xiang-mu jin-xing zhong qi guan-jian zuo-yong . 488 Statistical bilingual word alignment method, which has been well studied in the context of machine translation, can extract the aligned bilingual word pairs from a bilingual corpus." ></td>
	<td class="line x" title="57:237	This paper adapts the bilingual word alignment algorithm to monolingual scenario to align the collocated words in a monolingual corpus." ></td>
	<td class="line x" title="58:237	Given a sentence with l words },,{ 1 l wwS = , the word alignments ]},1[|),{( liaiA i =  can be obtained by maximizing the word alignment probability of the sentence, according to Eq." ></td>
	<td class="line x" title="59:237	(1)." ></td>
	<td class="line x" title="60:237	)|(maxarg SApA A =                      (1) Where Aai i ),(  means that the word i w  is aligned with the word i a w . In a monolingual sentence, a word never collocates with itself." ></td>
	<td class="line x" title="61:237	Thus the alignment set is denoted as }&],1[|),{( ialiaiA ii = . We adapt the bilingual word alignment model, IBM Model 3 (Brown et al., 1993), to monolingual word alignment." ></td>
	<td class="line x" title="62:237	The probability of the alignment sequence is calculated using Eq." ></td>
	<td class="line x" title="63:237	(2)." ></td>
	<td class="line x" title="64:237	 == l j jaj l i ii lajdwwtwnSAp j 11 ),|()|()|()|(    (2) Where i   denotes the number of words that are aligned with i w . Three kinds of probabilities are involved: Word collocation probability )|( j aj wwt , which describes the possibility of w j  collocating with j a w ; Position collocation probability d(j, a j , l), which describes the probability of a word in position a j  collocating with another word in position j; Fertility probability )|( ii wn  , which describes the probability of the number of words that a word w i  can collocate with (refer to subsection 7.1 for further discussion)." ></td>
	<td class="line x" title="65:237	Figure 3 shows an example of word alignment on the English sentence in Figure 2 (b) with the MWA method." ></td>
	<td class="line x" title="66:237	In the sentence, the 7 th  word 'role' collocates with both the 4 th  word 'play' and the 6 th  word 'key'." ></td>
	<td class="line x" title="67:237	Thus, )|( 74 wwt  and )|( 76 wwt  describe the probabilities that the word 'role' collocates with 'play' and 'key',  Figure 3." ></td>
	<td class="line x" title="68:237	Results of MWA method respectively." ></td>
	<td class="line x" title="69:237	)12,7|4(d  and )12,7|6(d  describe the probabilities that the word in position 7 collocates with the words in position 4 and 6 in a sentence with 12 words." ></td>
	<td class="line x" title="70:237	For the word 'role', 7  is 2, which indicates that the word 'role' collocates with two words in the sentence." ></td>
	<td class="line x" title="71:237	To train the MWA model, we implement a MWA tool for collocation extraction, which uses similar training methods for bilingual word alignment, except that a word can not be aligned to itself." ></td>
	<td class="line x" title="72:237	2.2 Collocation Extraction Given a monolingual corpus, we use the trained MWA model to align the collocated words in each sentence." ></td>
	<td class="line x" title="73:237	As a result, we can generate a set of aligned word pairs on the corpus." ></td>
	<td class="line x" title="74:237	According to the alignment results, we calculate the frequency for two words aligned in the corpus, denoted as ),( ji wwfreq . In our method, we filtered those aligned word pairs whose frequencies are lower than 5." ></td>
	<td class="line x" title="75:237	Based on the alignment frequency, we estimate the alignment probabilities for each aligned word pair as shown in Eq." ></td>
	<td class="line x" title="76:237	(3) and (4)." ></td>
	<td class="line x" title="77:237	  = w j ji ji wwfreq wwfreq wwp ),( ),( )|( (3)   = w i ji ij wwfreq wwfreq wwp ),( ),( )|( (4) With alignment probabilities, we assign scores to the aligned word pairs and those with higher scores are selected as collocations, which are estimated as shown in Eq." ></td>
	<td class="line x" title="78:237	(5)." ></td>
	<td class="line x" title="79:237	2 )|()|( ),( ijji ji wwpwwp wwp + =      (5) 3 Initial Experiments In this experiment, we used the method as described in Section 2 for collocation extraction." ></td>
	<td class="line x" title="80:237	Since our method does not use any linguistic information, we compared our method with the The team leader plays a key role in the project undertaking ." ></td>
	<td class="line x" title="81:237	(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)               (12) The team leader plays a key role in the project undertaking ." ></td>
	<td class="line x" title="82:237	(1)        (2)           (3)           (4)      (5)   (6)      (7)      (8)    (9)        (10)               (11)              (12) 489 0 2 4 6 8 10 12 0 20 40 60 80 100 120 140 160 180 200 Top-N collocations (K) P r e c is io n  ( % ) Our method (Probability) Log-likelihood ratio  Figure 4." ></td>
	<td class="line x" title="83:237	Precision of collocations baseline methods without using linguistic knowledge." ></td>
	<td class="line x" title="84:237	These baseline methods take all cooccurring word pairs within a given window as collocation candidates, and then use association measures to rank the candidates." ></td>
	<td class="line x" title="85:237	Those candidates with higher association scores are extracted as collocations." ></td>
	<td class="line x" title="86:237	In this paper, the window size is set to [-6, +6]." ></td>
	<td class="line x" title="87:237	3.1 Data The experiments were carried out on a Chinese corpus, which consists of one year (2004) of the Xinhua news corpus from LDC 2 , containing about 28 millions of Chinese words." ></td>
	<td class="line x" title="88:237	Since punctuations are rarely used to construct collocations, they were removed from the corpora." ></td>
	<td class="line x" title="89:237	To automatically estimate the precision of extracted collocations on the Chinese corpus, we built a gold set by collecting Chinese collocations from handcrafted collocation dictionaries, containing 56,888 collocations." ></td>
	<td class="line x" title="90:237	3.2 Results The precision is automatically calculated against the gold set according to Eq." ></td>
	<td class="line x" title="91:237	(6)." ></td>
	<td class="line x" title="92:237	)(# )(# Top goldTop N N C CC precision   = I            (6) Where C Top-N  and C gold  denote the top collocations in the N-best list and the collocations in the gold set, respectively." ></td>
	<td class="line x" title="93:237	We compared our method with several baseline methods using different association measures 3 : co-occurring frequency, log-likelihood  2  Available at: http://www.ldc.upenn.edu/Catalog/Catalog Entry.jsp?catalogId=LDC2007T03 3  The definitions of these measures can be found in Manning and Schtze (1999)." ></td>
	<td class="line x" title="94:237	0 20 40 60 80 100 0.0 2.5 3.7 4.8 5.8 6.8 7.8 8.9 log(frequency) (%) Precision Alignment Probability  Figure 5." ></td>
	<td class="line x" title="95:237	Frequency vs. precision/alignment probability ratio, chi-square test, mutual information, and ttest." ></td>
	<td class="line x" title="96:237	Among them, the log-likelihood ratio measure achieves the best performance." ></td>
	<td class="line x" title="97:237	Thus, in this paper, we only show the performance of the loglikelihood ratio measure." ></td>
	<td class="line x" title="98:237	Figure 4 shows the precisions of the top N collocations as N steadily increases with an increment of 1K, which are extracted by our method and the baseline method using log-likelihood ratio as the association measure." ></td>
	<td class="line x" title="99:237	The absolute precision of collocations is not high in the figure." ></td>
	<td class="line x" title="100:237	For example, among the top 200K collocations, about 4% of the collocations are correct." ></td>
	<td class="line x" title="101:237	This is because our gold set contains only about 57K collocations." ></td>
	<td class="line x" title="102:237	Even if all collocations in the gold set are included in the 200Kbest list, the precision is only 28%." ></td>
	<td class="line x" title="103:237	Thus, it is more useful to compare precision curves for collocations in the N-best lists extracted by different methods." ></td>
	<td class="line x" title="104:237	In addition, since this gold set only includes a small number of collocations, the precision curves of our method and the baseline method are getting closer, as N increases." ></td>
	<td class="line x" title="105:237	For example, when N is set to 200K, our method and the baseline method achieved precisions of 4.09% and 3.12%, respectively." ></td>
	<td class="line x" title="106:237	And when N is set to 400K, they achieved 2.78% and 2.26%, respectively." ></td>
	<td class="line x" title="107:237	For convenience of comparison, we set N up to 200K in the experiments." ></td>
	<td class="line x" title="108:237	From the results, it can also be seen that, among the N-best lists with N less than 20K, the precision of the collocations extracted by our method is lower than that of the collocations extracted by the baseline, and became higher when N is larger than 20K." ></td>
	<td class="line x" title="109:237	In order to analyze the possible reasons, we investigated the relationships among the frequencies of the aligned word pairs, the alignment 490 x y b =4 b =2  Figure 6." ></td>
	<td class="line x" title="110:237	xb ey / = probabilities, and precisions of collocations, which are shown in Figure 5." ></td>
	<td class="line x" title="111:237	From the figure, we can see (1) that the lower the frequencies of the aligned word pairs are, the higher the alignment probabilities are; and (2) that the precisions of the aligned word pairs with lower frequencies is lower." ></td>
	<td class="line x" title="112:237	According to the above observations, we conclude that it is the word pairs with lower frequencies but higher probabilities that caused the lower precision of the top 20K collocations extracted by our method." ></td>
	<td class="line x" title="113:237	4 Improved MWA Method According to the analysis in subsection 3.2, we need to penalize the aligned word pairs with lower frequencies." ></td>
	<td class="line x" title="114:237	In order to achieve the above goal, we need to refine the alignment probabilities by using a penalization factor derived from a function on the frequencies of the aligned word pairs." ></td>
	<td class="line x" title="115:237	This function )(xfy =  should satisfy the following two conditions, where x  represents the log function of frequencies." ></td>
	<td class="line x" title="116:237	(1) The function is monotonic." ></td>
	<td class="line x" title="117:237	When x  is set to a smaller number, y  is also small." ></td>
	<td class="line x" title="118:237	This results in the penalization on the aligned word pairs with lower frequencies." ></td>
	<td class="line x" title="119:237	(2) When x , y  is set to 1." ></td>
	<td class="line x" title="120:237	This means that we dont penalize the aligned word pairs with higher frequencies." ></td>
	<td class="line x" title="121:237	According to the above descriptions, we propose to use the exponential function in Eq." ></td>
	<td class="line x" title="122:237	(7)." ></td>
	<td class="line x" title="123:237	xb ey / =  (7) Figure 6 describes this function." ></td>
	<td class="line x" title="124:237	The constant b in the function is used to adjust the shape of the line." ></td>
	<td class="line x" title="125:237	The line is sharp with b set to a small number, while the line is flat with b set to a larger number." ></td>
	<td class="line x" title="126:237	In our case, if b is set to a larger number, 0 5 10 15 20 25 0 20 40 60 80 100 120 140 160 180 200 Top-N collocations (K) Pr e c i s i on ( % ) Refined probability Probability Baseline (Log-likelihood ratio)  Figure 7." ></td>
	<td class="line x" title="127:237	Precision of collocations extracted by the improved method we assign a larger penalization weight to those aligned word pairs with lower frequencies." ></td>
	<td class="line x" title="128:237	According to the above discussion, we can use the following measure to assign scores to the aligned words pairs generated by the MWA method." ></td>
	<td class="line x" title="129:237	)),(log(  2 )|()|( ),( ji wwfreq b ijji jir e wwpwwp wwp   + =   (8) Where w i  and w j  are two aligned words." ></td>
	<td class="line x" title="130:237	p(w i |w j ) and p(w j |w i ) are alignment probabilities as shown in Eq." ></td>
	<td class="line x" title="131:237	(3) and (4)." ></td>
	<td class="line x" title="132:237	)),(log( ji wwfreq  is the log function of the frequencies of the aligned word pairs (w i , w j )." ></td>
	<td class="line x" title="133:237	5 Evaluation on Chinese corpus We used the same Chinese corpus described in Section 3 to evaluate the improved method as shown in Section 4." ></td>
	<td class="line x" title="134:237	In the experiments, b  was tuned by using a development set and set to 25." ></td>
	<td class="line x" title="135:237	5.1 Precision In this section, we evaluated the extracted collocations in terms of precision using both automatic evaluation and human evaluation." ></td>
	<td class="line x" title="136:237	Automatic Evaluation Figure 7 shows the precisions of the collocations in the N-best lists extracted by our method and the baseline method against the gold set in Section 3." ></td>
	<td class="line x" title="137:237	For our methods, we used two different measures to rank the aligned word pairs: alignment probabilities in Eq." ></td>
	<td class="line x" title="138:237	(5) and refined 491  Our method Baseline True 569 290 A 25 16 B 5 4 C 240 251 False D 161 439 Table 1." ></td>
	<td class="line x" title="139:237	Manual evaluation of the top 1K Chinese collocations." ></td>
	<td class="line x" title="140:237	The precisions of our method and the baseline method are 56.9% and 29.0%, respectively." ></td>
	<td class="line x" title="141:237	alignment probabilities in Eq." ></td>
	<td class="line x" title="142:237	(8)." ></td>
	<td class="line x" title="143:237	From the results, it can be seen that with the refined alignment probabilities, our method achieved the highest precision on the N-best lists, which greatly outperforms the best baseline method." ></td>
	<td class="line x" title="144:237	For example, in the top 1K list, our method achieves a precision of 20.6%, which is much higher than the precision of the baseline method (11.7%)." ></td>
	<td class="line x" title="145:237	This indicates that the exponential function used to penalize the alignment probabilities plays a key role in demoting most of the aligned word pairs with low frequencies." ></td>
	<td class="line x" title="146:237	Human Evaluation In automatic evaluation, the gold set only contains collocations in the existing dictionaries." ></td>
	<td class="line x" title="147:237	Some collocations related to specific corpora are not included in the set." ></td>
	<td class="line x" title="148:237	Therefore, we selected the top 1K collocations extracted by our improved method to manually estimate the precision." ></td>
	<td class="line x" title="149:237	During human evaluation, the true collocations are denoted as 'True' in our experiments." ></td>
	<td class="line x" title="150:237	The false collocations were further classified into the following classes." ></td>
	<td class="line x" title="151:237	A: The candidate consists of two words that are semantically related, such as (  doctor,   nurse)." ></td>
	<td class="line x" title="152:237	B: The candidate is a part of the multi-word ( 3) collocation." ></td>
	<td class="line x" title="153:237	For example, (  self,  mechanism) is a part of the three-word collocation (  self,   regulating,   mechanism)." ></td>
	<td class="line x" title="154:237	C: The candidates consist of the adjacent words that frequently occur together, such as ( he,   say) and (  very,   good)." ></td>
	<td class="line x" title="155:237	D: Two words in the candidates have no relationship with each other, but occur together frequently, such as (  Beijing,   month) and (  and,   for)." ></td>
	<td class="line x" title="156:237	Table 1 shows the evaluation results." ></td>
	<td class="line x" title="157:237	Our method extracted 569 true collocations, which 0 2 4 6 8 10 12 0 1 2 3 4 5 6 7 8 9 10 11 12 Training corpus (Months) Pr e c i s i o n  ( % ) Our method Baseline  Figure 8." ></td>
	<td class="line x" title="158:237	Corpus size vs. precision are much more than those extracted by the baseline method." ></td>
	<td class="line x" title="159:237	Further analysis shows that, in addition to extracting short-span collocations, our method extracted collocations with longer spans as compared with the baseline method." ></td>
	<td class="line x" title="160:237	For example, ( in,  state) and (  because,   so) are two long-span collocations." ></td>
	<td class="line x" title="161:237	Among the 1K collocations, there are 48 collocation candidates whose spans are larger than 6, which are not covered by the baseline method since the window size is set to 6." ></td>
	<td class="line x" title="162:237	And 33 of them are true collocations, with a higher precision of 69%." ></td>
	<td class="line x" title="163:237	Classes C and D account for the most part of the false collocations." ></td>
	<td class="line x" title="164:237	Although the words in these two classes co-occur frequently, they can not be regarded as collocations." ></td>
	<td class="line x" title="165:237	And we also found out that the errors in class D produced by the baseline method are much more than that of those produced by our method." ></td>
	<td class="line x" title="166:237	This indicates that our MWA method can remove much more noise from the frequently occurring word pairs." ></td>
	<td class="line x" title="167:237	In Class A, the two words are semantically related and occur together in the corpus." ></td>
	<td class="line x" title="168:237	These kinds of collocations can not be distinguished from the true collocations by our method without additional resources." ></td>
	<td class="line x" title="169:237	Since only bigram collocations were extracted by our method, the multi-word ( 3) collocations were split into bigram collocations, which caused the error collocations in Class B 4 . Corpus size vs. precision Here, we investigated the effect of the corpus size on the precision of the extracted collocations." ></td>
	<td class="line x" title="170:237	We evaluated the precision against the gold set as shown in the automatic evaluation." ></td>
	<td class="line x" title="171:237	First, the whole corpus (one year of newspaper) was split into 12 parts according to the published months." ></td>
	<td class="line x" title="172:237	Then we calculated the precisions as the training  4  Since only a very small faction of collocations contain more than two words, a few error collocations belong to Class B. 492 0 20 40 60 80 100 0 20 40 60 80 100 120 140 160 180 200 Top-N collocations (K) Re c a l l  (%) Our method Baseline  Figure 9." ></td>
	<td class="line x" title="173:237	Recall on the Chinese corpus corpus increases part by part." ></td>
	<td class="line x" title="174:237	The top 20K collocations were selected for evaluation." ></td>
	<td class="line x" title="175:237	Figure 8 shows the experimental results." ></td>
	<td class="line x" title="176:237	The precision of collocations extracted by our method is obviously higher than that of collocations extracted by the baseline method." ></td>
	<td class="line x" title="177:237	When the size of the training corpus became larger, the difference between our method and the baseline method also became bigger." ></td>
	<td class="line x" title="178:237	When the training corpus contains more than 9 months of corpora, the precision of collocations extracted by the baseline method did not increase anymore." ></td>
	<td class="line x" title="179:237	However, the precision of collocations extracted by our method kept on increasing." ></td>
	<td class="line x" title="180:237	This indicates the MWA method can extract more true collocations of higher quality when it is trained with larger size of training data." ></td>
	<td class="line x" title="181:237	5.2 Recall Recall was evaluated on a manually labeled subset of the training corpus." ></td>
	<td class="line x" title="182:237	The subset contains 100 sentences that were randomly selected from the whole corpus." ></td>
	<td class="line x" title="183:237	The sentence average length is 24." ></td>
	<td class="line x" title="184:237	All true collocations (660) were labeled manually." ></td>
	<td class="line x" title="185:237	The recall was calculated according to Eq." ></td>
	<td class="line x" title="186:237	(9)." ></td>
	<td class="line x" title="187:237	)(# )(# subset subsetTop C CC recall N I  =               (9) Here, C Top-N  denotes the top collocations in the N-best list and C subset  denotes the true collocations in the subset." ></td>
	<td class="line x" title="188:237	Figure 9 shows the recalls of collocations extracted by our method and the baseline method on the labeled subset." ></td>
	<td class="line x" title="189:237	The results show that our method can extract more true collocations than the baseline method." ></td>
	<td class="line x" title="190:237	0 20 40 60 80 100 0 20 40 60 80 100 120 140 160 180 200 Top-N collocations (K) Re c a l l  (%) Our method Baseline  Figure 10." ></td>
	<td class="line x" title="191:237	Recall on the English corpus  Our method Baseline True 591 355 A 11 4 B 19 20 C 200 136 False D 179 485 Table 2." ></td>
	<td class="line x" title="192:237	Manual evaluation of the top 1K English collocations." ></td>
	<td class="line x" title="193:237	The precisions of our method and the baseline method are 59.1% and 35.5%, respectively." ></td>
	<td class="line x" title="194:237	In our experiments, the baseline method extracts about 20 millions of collocation candidates, while our method only extracts about 3 millions of collocation candidates 5 . Although the collocations of our method are much less than that of the baseline, the experiments show that the recall of our method is higher." ></td>
	<td class="line x" title="195:237	This again proved that our method has the stronger ability to distinguish true collocations from false collocations." ></td>
	<td class="line x" title="196:237	6 Evaluation on English corpus We also manually evaluated the proposed method on an English corpus, which is a subset randomly extracted from the British National Corpus 6 . The English corpus contains about 20 millions of words." ></td>
	<td class="line x" title="197:237	6.1 Precision We estimated the precision of the top 1K collocations." ></td>
	<td class="line x" title="198:237	Table 2 shows the results." ></td>
	<td class="line x" title="199:237	The classification of the false collocations is the same as that in Table 1." ></td>
	<td class="line x" title="200:237	The results show that our methods outperformed the baseline method using log 5  We set the threshold to 7.88 with a confidence level  of 005.0=  (cf.page 174 of Chapter 5 in (McKeown and Radev, 2000) for more details)." ></td>
	<td class="line x" title="202:237	6  Available at: http://www.hcu.ox.ac.uk/BNC/ 493 0 5 10 15 20 0 20 40 60 80 100 120 140 160 180 Top-N collocation (K) P r ec is io n  ( % )  Figure 11." ></td>
	<td class="line x" title="203:237	Fertility vs. precision likelihood ratio." ></td>
	<td class="line x" title="204:237	And the distribution of the false collocations is similar to that on the Chinese corpus." ></td>
	<td class="line x" title="205:237	6.2 Recall We used the method described in subsection 5.2 to calculate the recall." ></td>
	<td class="line x" title="206:237	100 English sentences were labeled manually, obtaining 205 true collocations." ></td>
	<td class="line x" title="207:237	Figure 10 shows the recall of the collocations in the N-best lists." ></td>
	<td class="line x" title="208:237	From the figure, it can be seen that the trend on the English corpus is similar to that on the Chinese corpus, which indicates that our method is language-independent." ></td>
	<td class="line x" title="209:237	7 Discussion 7.1 The Effect of Fertility In the MWA model as described in subsection 2.1, i   denotes the number of words that can align with i w . Since a word only collocates with a few other words in a sentence, we should set a maximum number for  , denote as max  . In order to set max  , we examined the true collocations in the manually labeled set described in subsection 5.2." ></td>
	<td class="line x" title="210:237	We found that 78% of words collocate with only one word, and 17% of words collocate with two words." ></td>
	<td class="line x" title="211:237	In sum, 95% of words in the corpus can only collocate with at most two words." ></td>
	<td class="line x" title="212:237	According to the above observation, we set max   to 2." ></td>
	<td class="line x" title="213:237	In order to further examine the effect of max  on collocation extraction, we used several different max   in our experiments." ></td>
	<td class="line x" title="214:237	The comparison 0 1 2 3 4 5 6 7 8 0 2040608010 Span of collocation l og(#( a l i g ne d  w ord pa i r s ) )  Figure 12." ></td>
	<td class="line x" title="215:237	Distribution of spans results are shown in Figure 11." ></td>
	<td class="line x" title="216:237	The highest precision is achieved when max   is set to 2." ></td>
	<td class="line x" title="217:237	This result verifies our observation on the corpus." ></td>
	<td class="line x" title="218:237	7.2 Span of Collocation One of the advantages of our method is that long-span collocations can be reliably extracted." ></td>
	<td class="line x" title="219:237	In this subsection, we investigate the distribution of the span of the aligned word pairs." ></td>
	<td class="line x" title="220:237	For the aligned word pairs occurring more than once, we calculated the average span as shown in Eq." ></td>
	<td class="line x" title="221:237	(10)." ></td>
	<td class="line x" title="222:237	),( );,( ),( ji corpuss ji ji wwfreq swwSpan wwAveSpan  =   (10) Where, );,( swwSpan ji  is the span of the words w i  and w j  in the sentence s; ),( ji wwAveSpan  is the average span." ></td>
	<td class="line x" title="223:237	The distribution is shown in Figure 12." ></td>
	<td class="line x" title="224:237	It can be seen that the number of the aligned word pairs decreased exponentially as the average span increased." ></td>
	<td class="line x" title="225:237	About 17% of the aligned word pairs have spans longer than 6." ></td>
	<td class="line x" title="226:237	According to the human evaluation result for precision in subsection 5.1, the precision of the long-span collocations is even higher than that of the short-span collocations." ></td>
	<td class="line x" title="227:237	This indicates that our method can extract reliable collocations with long spans." ></td>
	<td class="line x" title="228:237	8 Conclusion We have presented a monolingual word alignment method to extract collocations from monolingual corpus." ></td>
	<td class="line x" title="229:237	We first replicated the monolingual corpus to generate a parallel corpus, in which each sentence pair consists of the two identical sentences in the same language." ></td>
	<td class="line x" title="230:237	Then we adapted the bilingual word alignment algorithm to the monolingual scenario to align the 10 3 2 1 max max max max = = = =     494 potentially collocated word pairs in the monolingual sentences." ></td>
	<td class="line x" title="231:237	In addition, a ranking method was proposed to finally extract the collocations from the aligned word pairs." ></td>
	<td class="line x" title="232:237	It scores collocation candidates by using alignment probabilities multiplied by a factor derived from the exponential function on the frequencies." ></td>
	<td class="line x" title="233:237	Those with higher scores are selected as collocations." ></td>
	<td class="line x" title="234:237	Both Chinese and English collocation extraction experiments indicate that our method outperforms previous approaches in terms of both precision and recall." ></td>
	<td class="line x" title="235:237	For example, according to the human evaluations on the Chinese corpus, our method achieved a precision of 56.9%, which is much higher than that of the baseline method (29.0%)." ></td>
	<td class="line x" title="236:237	Moreover, we can extract collocations with longer span." ></td>
	<td class="line x" title="237:237	Human evaluation on the extracted Chinese collocations shows that 69% of the long-span (>6) collocations are correct." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J09-3004
Articles: Bootstrapping Distributional Feature Vector Quality
Zhitomirsky-Geffet, Maayan;Dagan, Ido;"></td>
	<td class="line x" title="1:376	Bootstrapping Distributional Feature Vector Quality MaayanZhitomirsky-Geffet  Bar-Ilan University IdoDagan  Bar-Ilan University This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity." ></td>
	<td class="line x" title="2:376	The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment." ></td>
	<td class="line x" title="3:376	Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufcient quality of the word feature vectors, caused by decient feature weighting." ></td>
	<td class="line x" title="4:376	This observation led to the denition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors." ></td>
	<td class="line x" title="5:376	The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted." ></td>
	<td class="line x" title="6:376	This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space." ></td>
	<td class="line x" title="7:376	The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset." ></td>
	<td class="line x" title="8:376	These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions." ></td>
	<td class="line x" title="9:376	Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector." ></td>
	<td class="line x" title="10:376	Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied." ></td>
	<td class="line x" title="11:376	1." ></td>
	<td class="line x" title="12:376	Introduction 1.1 Motivation Distributionalwordsimilarityhaslongbeenanactiveresearcharea(Hindle1990;Ruge 1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and  Department ofInformationScience, Bar-IlanUniversity, Ramat-Gan,Israel." ></td>
	<td class="line x" title="13:376	E-mail:zhitomim@mail.biu.ac.il." ></td>
	<td class="line x" title="14:376	 Department ofComputerScience, Bar-IlanUniversity, Ramat-Gan,Israel." ></td>
	<td class="line x" title="15:376	E-mail:dagan@cs.biu.ac.il." ></td>
	<td class="line x" title="16:376	Submissionreceived: 6 December 2006;revised submissionreceived: 9July2008;accepted forpublication: 21November2008." ></td>
	<td class="line x" title="17:376	2009AssociationforComputationalLinguistics ComputationalLinguistics Volume35,Number3 Weir 2005)." ></td>
	<td class="line x" title="18:376	This paradigm is inspired by Harriss distributional hypothesis (Harris 1968), which states that semantically similar words tend to appear in similar contexts." ></td>
	<td class="line x" title="19:376	Inacomputationalrealization,eachwordischaracterizedbyaweightedfeaturevector, wherefeaturestypicallycorrespondtootherwordsthatco-occurwiththecharacterized wordinthecontext.Distributionalsimilaritymeasuresquantifythedegreeofsimilarity between a pair of such feature vectors." ></td>
	<td class="line x" title="20:376	It is then assumed that two words that occur within similar contexts, as measured by similarity of their context vectors, are indeed semantically similar." ></td>
	<td class="line x" title="21:376	The distributional word similarity measures were often applied for two types of inferences." ></td>
	<td class="line x" title="22:376	The rst type is making similarity-based generalizations for smoothing word co-occurrence probabilities, in applications such as language modeling and disambiguation." ></td>
	<td class="line x" title="23:376	For example, assume that we need to estimate the likelihood of the verb objectco-occurrencepair visitcountry,althoughitdidnotappearinoursamplecorpus." ></td>
	<td class="line x" title="24:376	Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city,andregion, however, do appear in the corpus." ></td>
	<td class="line x" title="25:376	Consequently, we may infer that visitcountry is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005)." ></td>
	<td class="line x" title="26:376	The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts." ></td>
	<td class="line x" title="27:376	A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution." ></td>
	<td class="line x" title="28:376	Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) summarization, need to recognize that one word can be substituted by another one in a given context while preserving, or entailing the original meaning." ></td>
	<td class="line x" title="29:376	Naturally, recognizing such substitutable lexical entailments is a prominent component within the textual entailment recognition paradigm, which models semantic inference as an applicationindependent task (Dagan, Glickman, and Magnini 2006)." ></td>
	<td class="line x" title="30:376	Accordingly, several textual entailmentsystemsdidutilizetheoutputofdistributionalsimilaritymeasurestomodel entailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al. 2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006)." ></td>
	<td class="line x" title="31:376	In some of these papers the distributional information typically complements manual lexical resources in textual entailment systems, most notably WordNet (Fellbaum 1998)." ></td>
	<td class="line x" title="32:376	Lexical substitution typically requires that the meaning of one word entails the meaning of the other." ></td>
	<td class="line x" title="33:376	For instance, in question answering, the word company in a question can be substituted in an answer text by rm, automaker,orsubsidiary, whose meanings entail the meaning of company." ></td>
	<td class="line x" title="34:376	However, as it turns out, traditional distributional similarity measures do not capture well such lexical substitution relationships, but rather capture a somewhat broader (and looser) notion of semantic similarity." ></td>
	<td class="line x" title="35:376	For example, quite distant co-hyponyms such as party and company also come out as distributionally similar to country, due to a partial overlap of their semantic properties." ></td>
	<td class="line x" title="36:376	Clearly, the meanings of these words do not entail each other." ></td>
	<td class="line x" title="37:376	Motivated by these observations, our long-term goal is to investigate whether the distributionalsimilarityschememaybeimprovedtoyieldtightersemanticsimilarities, and eventually better approximation of lexical entailments." ></td>
	<td class="line x" title="38:376	This article presents one component of this research plan, which focuses on improving the underlying semantic 436 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality quality of distributional word feature vectors." ></td>
	<td class="line x" title="39:376	The article describes the methodology, denitions, and analysis of our investigation and the resulting bootstrapping scheme for feature weighting which yielded improved empirical performance." ></td>
	<td class="line x" title="40:376	1.2 Main Contributions and Outline As a starting point for our investigation, an operational denition was needed for evaluating the correctness of candidate pairs of similar words." ></td>
	<td class="line x" title="41:376	Following the lexical substitution motivation, in Section 3weformulate the substitutable lexical entailment relation (or lexical entailment, for brevity), rening earlier denitions in Geffet and Dagan (2004, 2005)." ></td>
	<td class="line x" title="42:376	Generally speaking, this relation holds for a pair of words if a possiblemeaningofonewordentailsameaningoftheother,andtheentailingwordcan substitute the entailed one in some typical contexts." ></td>
	<td class="line x" title="43:376	Lexical entailment overlaps partly with traditional lexical semantic relationships, while capturing more generally the lexical substitution needs of applications." ></td>
	<td class="line x" title="44:376	Empirically, high inter-annotator agreement was obtained when judging the output of distributional similarity measures for lexical entailment." ></td>
	<td class="line x" title="45:376	Next,weanalyzedthetypicalbehaviorofexistingwordsimilaritymeasuresrelative to the lexical entailment criterion." ></td>
	<td class="line x" title="46:376	Choosing the commonly used measure of Lin (1998) as a representative case, the analysis shows that quite noisy feature vectors are a major cause for generating rather loose semantic similarities." ></td>
	<td class="line x" title="47:376	On the other hand, one may expect that features which seem to be most characteristic for a words meaning should receive the highest feature weights." ></td>
	<td class="line oc" title="48:376	This does not seem to be the case, however, for common feature weighting functions, such as Point-wise Mutual Information (Church and Patrick 1990; Hindle 1990)." ></td>
	<td class="line x" title="49:376	Followingtheseobservations,wedevelopedabootstrappingformulathatimproves the original feature weights (Section 4), leading to better feature vectors and better similarity predictions." ></td>
	<td class="line x" title="50:376	The general idea is to promote the weights of features that are common for semantically similar words, since these features are likely to be most characteristicforthewordsmeaning.Thisideaisimplementedbyabootstrappingscheme, where the initial (and cruder) similarity measure provides an initial approximation for semantic word similarity." ></td>
	<td class="line x" title="51:376	The bootstrapping method yields a high concentration of semantically characteristic features among the top-ranked features of the vector, which also allows aggressive feature reduction." ></td>
	<td class="line x" title="52:376	The bootstrapping scheme was evaluated in two experimental settings, which correspond to the two types of applications for distributional similarity." ></td>
	<td class="line x" title="53:376	First, it achieved signicant improvements in predicting lexical entailment as assessed by human judgments, when applied over several base similarity measures (Section 5)." ></td>
	<td class="line x" title="54:376	Additional analysis relative to the lexical entailment dataset revealed cleaner and more characteristic feature vectors for the bootstrapping method." ></td>
	<td class="line x" title="55:376	To obtain a quantitative analysis of this behavior, we dened a measure called average common-feature rank ratio." ></td>
	<td class="line x" title="56:376	This measure captures the idea that a prominent feature for a word is expected to be prominentalsoforsemanticallysimilarwords,whilebeinglessprominentforunrelated words.Tothebestofourknowledgethisistherstproposedmeasurefordirectanalysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure." ></td>
	<td class="line x" title="57:376	As a second evaluation, we applied the bootstrapping scheme for similarity-based predictionofco-occurrencelikelihoodwithinatypicalpseudo-wordsensedisambiguation experiment, obtaining substantial error reductions (Section 7)." ></td>
	<td class="line x" title="58:376	Section 8 concludes 437 ComputationalLinguistics Volume35,Number3 this article, suggesting the relevance of our analysis and bootstrapping scheme for the general use ofdistributional feature vectors." ></td>
	<td class="line x" title="59:376	1 2." ></td>
	<td class="line x" title="60:376	Background: Distributional Similarity Models This section reviews the components of the distributional similarity approach and species the measures and functions that were utilized byour work." ></td>
	<td class="line x" title="61:376	The Distributional Hypothesis assumes that semantically similar words appear in similar contexts, suggesting that semantic similarity can be detected by comparing contexts of words." ></td>
	<td class="line x" title="62:376	This is the underlying principle of the vector-based distributional similaritymodel,whichcomprisestwophases.First,contextfeaturesforeachwordare constructed and assigned weights; then, the weighted feature vectors of pairs of words are compared by a vector similarity measure." ></td>
	<td class="line x" title="63:376	The following two subsections review typical methods for each phase." ></td>
	<td class="line x" title="64:376	2.1 Features and Weighting Functions In the typical computational setting, word contexts are represented by feature vectors." ></td>
	<td class="line x" title="65:376	A feature represents another word (or term) w prime with which w co-occurs, and possibly speciesalsothesyntacticrelationshipbetweenthetwowords,asinGrefenstette(1994), Lin (1998), and Weeds and Weir (2005)." ></td>
	<td class="line x" title="66:376	Thus, a word (or term) w is represented by a feature vector, where each entry in the vector corresponds to a feature f.Padoand Lapata (2007) demonstrate that using syntactic dependency-based features helps to distinguish among classes of lexical relations, which seems to be more difcult when using bag of words features that are based on co-occurrence in a textwindow." ></td>
	<td class="line x" title="67:376	A syntactic-based feature f for aword w is dened as atriple: fw, syn rel, f role where fw is a context word (or term) that co-occurs with w under the syntactic dependencyrelationsyn rel.Thefeaturerole(f role)correspondstotheroleofthefeatureword fw in the syntactic dependency, being either the head (denoted h) or the modier (denoted m)oftherelation.Forexample,giventhewordcompany,thefeatureearnings, gen, hcorrespondstothegenitiverelationship companys earnings,andinvestor, pcomp of, m corresponds to the prepositional complement relationship the company of the investor." ></td>
	<td class="line x" title="68:376	2 Throughout this article we use syntactic dependency relationships generated by the Minipar dependency parser (Lin 1993)." ></td>
	<td class="line x" title="69:376	Table 1 lists common Minipar dependency relations involving nouns." ></td>
	<td class="line x" title="70:376	Minipar also identies multi-word expressions, which is 1Apreliminaryversion ofthebootstrappingmethodwaspresented inGeffetand Dagan (2004).That paperpresented initialresults forthe bootstrappingscheme, when applied onlyoverLins measure and testedby themanuallyjudged datasetoflexicalentailment." ></td>
	<td class="line x" title="71:376	Thecurrent research extends ourinitial resultsin many respects." ></td>
	<td class="line x" title="72:376	Itrenes the denitionoflexicalentailment; utilizesarevised testset oflarger scope andhigher quality,annotatedbythree assessors; extends the experiments totwoadditional similaritymeasures; provides comparativequalitativeandquantitativeanalysis ofthe bootstrapped vectors, whileemploying ourproposedaverage common-featurerankratio;andpresents an additional evaluationbased onapseudo-WSD task." ></td>
	<td class="line x" title="73:376	2 Followingacommonpractice,weconsider the relationshipbetweenahead noun (company inthe example)andthe nominalcomplementofamodifying prepositionalphrase (investor) as asingle direct dependency relationship." ></td>
	<td class="line x" title="74:376	Theprepositionitselfis encoded inthe dependency relationname, witha distinctrelationforeach preposition." ></td>
	<td class="line x" title="75:376	438 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality Table 1 Commongrammaticalrelationsof Miniparinvolvingnouns." ></td>
	<td class="line x" title="76:376	Relation Description appo apposition comp1rst complement det determiner gen genitivemarker mod therelationshipbetweena wordandits adjunctmodier pnmod post nominalmodier pcomp nominalcomplement of prepositions post post determiner vrel passive verbmodierof nouns obj object ofverbs obj2 second object of ditransitiveverbs subj subject of verbs s surfacesubject advantageous for detecting distributional similarity for such terms." ></td>
	<td class="line x" title="77:376	For example, Curran (2004) reports that multi-word expressions make up between 1425% of the synonyms in a gold-standard thesaurus." ></td>
	<td class="line x" title="78:376	Thus, in our representation the corpus is rst transformed to a set S of dependency relationship instances of the form w,f , where each pair corresponds to a single cooccurrence of w and f in the corpus." ></td>
	<td class="line x" title="79:376	f is termed as a feature of w. Then, a word w is represented by a feature vector, where each entry in the vector corresponds to one feature f. The value of the entry is determined by a feature weighting function weight(w,f),whichquantiesthedegreeofstatisticalassociationbetween w and f inthe set S. For example, some feature weighting functions are based on the logarithm of the wordfeatureco-occurrencefrequency(Ruge1992),orontheconditionalprobabilityof thefeaturegiventheword(Pereira,Tishby,andLee1993;Dagan,Lee,andPereira1999; Lee 1999)." ></td>
	<td class="line pc" title="80:376	Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch, Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), dened by: weight MI (w,f)=log 2 P(w,f) P(w)P(f) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight MI (w,f)=log 2 count(w,f) nrels count(w) count(f) (2) where count(w,f) is the frequency of the co-occurrence pair w,f  in S, count(w)and count(f) are the independent frequencies of w and f in S,andnrels is the size of S.High MI weights are assumed to correspond to strong wordfeature associations." ></td>
	<td class="line x" title="81:376	439 ComputationalLinguistics Volume35,Number3 Curran and Moens (2002) argue that, generally, informative features are statistically correlated with their corresponding headword." ></td>
	<td class="line x" title="82:376	Thus, they suggest that any statistical test used for collocations is a good starting point for improving featureweightfunctions.Intheirexperimentsthet-test-basedmetricyieldedthebestempirical performance." ></td>
	<td class="line n" title="83:376	However, a known weakness of MI and most of the other statistical weighting functions used for collocation extraction, including t-test and  2 , is their tendency to inate the weights for rare features (Dunning 1993)." ></td>
	<td class="line x" title="84:376	In addition, a major property of lexical collocations is their non-substitutability, as termed in Manning and Schutze (1999)." ></td>
	<td class="line x" title="85:376	That is, typically neither a headword nor a modier in the collocation can be substituted by their synonyms or other related terms." ></td>
	<td class="line x" title="86:376	This implies that using modiers within strong collocations as features for a head word would provide a rather small amount of common features for semantically similar words." ></td>
	<td class="line x" title="87:376	Hence, these functions seem less suitable for learning broader substitutability relationships, such as lexical entailment." ></td>
	<td class="line p" title="88:376	Similarity measures that utilize MI weights showed good performance, however." ></td>
	<td class="line x" title="89:376	In particular, a common practice is to lter out features by minimal frequency and weightthresholds.Then,awordsvectorisconstructedfromtheremaining(notltered) features that are strongly associated with the word." ></td>
	<td class="line x" title="90:376	These features are denoted here as active features." ></td>
	<td class="line o" title="91:376	In the current work we use MI for data analysis, and for the evaluations of vector quality and word similarity performance." ></td>
	<td class="line x" title="92:376	2.2 Vector Similarity Measures Once feature vectors have been constructed the similarity between two words is dened by some vector similarity measure." ></td>
	<td class="line x" title="93:376	Similarity measures which have been used in the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992), and various information theoretic measures, as introduced and reviewed in Lee (1997, 1999)." ></td>
	<td class="line x" title="94:376	In the current work we experiment with the following three popular similarity measures." ></td>
	<td class="line x" title="95:376	1." ></td>
	<td class="line x" title="96:376	The basic Jaccard measure compares the number of common features with the overall number offeatures for apair ofwords." ></td>
	<td class="line x" title="97:376	One of the weighted generalizations of this scheme to non-binary values replaces intersection with minimum weight, union with maximum weight, and set cardinality with summation." ></td>
	<td class="line x" title="98:376	This measure is commonly referred to as weighted Jaccard (WJ)(Grefenstette 1994; Dagan, Marcus, and Markovitch 1995; Dagan 2000; Gasperin and Vieira 2004), dened as follows: sim WJ (w,v)= summationtext fF(w)F(v) min(weight(w,f),weight(v,f)) summationtext fF(w)F(v) max(weight(w,f),weight(v,f)) (3) where F(w)andF(v) are the sets of active features of the two words w and v. The appealing property of this measure is that itconsiders the association weights rather than just the number of common features." ></td>
	<td class="line x" title="99:376	2." ></td>
	<td class="line x" title="100:376	The standard Cosine measure (COS), which is popularly employed for information retrieval (Salton and McGill 1983) and also utilized for 440 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality learning distributionally similar words (Ruge 1992; Caraballo 1999; Gauch, Wang,and Rachakonda 1999; Panteland Ravichandran 2004), isdened as follows: sim COS (w,v)= summationtext f (weight(w,f)weight(v,f))  summationtext f (weight(w,f)) 2   summationtext f (weight(v,f)) 2 (4) This measure computes the cosine of the angle between the two feature vectors, which normalizes the vector lengths and thus avoids inated discrimination between vectors ofsignicantly different lengths." ></td>
	<td class="line x" title="101:376	3." ></td>
	<td class="line x" title="102:376	A popular state of the art measure has been developed by Lin (1998), motivated byInformation Theory principles." ></td>
	<td class="line o" title="103:376	This measure behaves quite similarly to the weighted Jaccard measure (Weeds, Weir, and McCarthy 2004), and is dened as follows: sim LIN (w,v)= summationtext fF(w)F(v) (weight MI (w,f)+weight MI (v,f)) summationtext fF(w) weight MI (w,f)+ summationtext fF(v) weight MI (v,f) (5) where F(w)andF(v) are the active features ofthe two words." ></td>
	<td class="line o" title="104:376	Theweight function used originally by Lin is MI (Equation 1)." ></td>
	<td class="line x" title="105:376	It is interesting to note that a relatively recent work by Weeds and Weir (2005) investigates a more generic similarity framework." ></td>
	<td class="line x" title="106:376	Within their framework, the similarity of two nouns is viewed as the ability to predict the distribution of one of them based on thatoftheother.Theirproposedformulacombinestheprecisionandrecallofapotential retrieval of similar words based on the features of the target word." ></td>
	<td class="line x" title="107:376	The precision of wspredictionof vsfeaturedistributionindicateshowmanyofthefeaturesoftheword w co-occurred with the word v. The recall of ws prediction of vs features indicates how many of the features of v co-occurred with w. Words with both high precision and high recall can be obtained by computing their harmonic mean, mh (or F-score), and a weighted arithmetic mean." ></td>
	<td class="line x" title="108:376	However, after empirical tuning of weights for the arithmetic mean, Weeds and Weirs formula practically reduces to Lins measure, as was anticipated bytheir own analysis (in Section 4of their paper)." ></td>
	<td class="line x" title="109:376	Consequently,wechoosetheLinmeasure(Equation5)(henceforthdenotedas LIN) as representative for the state of the art and utilize it for data analysis and as a starting point for improvement." ></td>
	<td class="line x" title="110:376	To further explore and evaluate our new weighting scheme, independently of a single similarity measure, we conduct evaluations also with the other two similarity measures of weighted Jaccard and Cosine." ></td>
	<td class="line x" title="111:376	3." ></td>
	<td class="line x" title="112:376	Substitutable Lexical Entailment AsmentionedintheIntroduction,thelongtermresearchgoalwhichinspiredourwork is modeling meaningentailing lexical substitution." ></td>
	<td class="line x" title="113:376	Motivated by this goal, we proposed in earlier work (Geffet and Dagan 2004, 2005) a new type of lexical relationship which aims to capture such lexical substitution needs." ></td>
	<td class="line x" title="114:376	Here we adopt that approach and formulate a rened denition for this relationship, termed substitutable lexical entailment.Inthecontextofthecurrentarticle,utilizingaconcretetargetnotion of word similarity enabled us to apply direct human judgment for the correctness (relative to the dened notion) of candidate word pairs suggested by distributional similarity." ></td>
	<td class="line x" title="115:376	Utilizing these judgments we could analyze the behavior of alternative 441 ComputationalLinguistics Volume35,Number3 distributional vector representations and, in particular, conduct error analysis for word pair candidates that were judged negatively." ></td>
	<td class="line x" title="116:376	The discussion in the Introduction suggested that multiple text understanding applications need to identify term pairs whose meanings are both entailing and substitutable." ></td>
	<td class="line x" title="117:376	Such pairs seem to be most appropriate for lexical substitution in a meaning preservingscenario.Tomodelthisgoalwepresentanoperationaldenitionforalexical semanticrelationshipthatintegratesthetwoaspectsofentailmentandsubstitutability, 3 which is termed substitutable lexical entailment (or lexical entailment, for brevity)." ></td>
	<td class="line x" title="118:376	This relationship holds for a given directional pair of terms (w,v), saying that w entails v, if the following twoconditions are fullled: 1." ></td>
	<td class="line x" title="119:376	Word meaning entailment:the meaning of apossible sense of w implies a possible sense of v; 2." ></td>
	<td class="line x" title="120:376	Substitutability: w cansubstitutefor v insomenaturallyoccurringsentence, such that the meaning ofthe modied sentence would entail the meaning of the original one." ></td>
	<td class="line x" title="121:376	To operationally assess the rst condition (by annotators) we propose considering the meaning of terms by existential statements of the form there exists an instance of the meaning of the term w in some context (notice that, unlike propositions, it is not intuitiveforannotatorstoassigntruthvaluestoterms).Forexample,theword company would correspond to the existential statement there exists an instance of the concept company in some context. Thus, if in some context there is a company (in the sense of commercial organization) then necessarily there is a rm in that context (in the corresponding sense)." ></td>
	<td class="line x" title="122:376	Therefore, we conclude that the meaning of company implies the meaningof rm.Ontheotherhand,thereisan organizationdoesnotnecessarilyimply theexistenceof company,sinceorganization mightstandforsomenon-protassociation, as well." ></td>
	<td class="line x" title="123:376	Therefore, we conclude that organization does not entail company." ></td>
	<td class="line x" title="124:376	To assess the second condition, the annotators need to identify some natural context in which the lexical substitution would satisfy entailment between the modied sentence and the original one." ></td>
	<td class="line x" title="125:376	Practically, in our experiments presented in Section 5the humanassessorscouldconsultexternallexicalresourcesandtheentireWebtoobtainall thesensesofthewordsandpossiblesentencesforsubstitution.Wenotethatthetaskof identifyingthecommonsenseoftwogivenwordsisquiteeasysincetheymutuallydisambiguateeachother,andoncethecommonsenseisknownitnaturallyhelpsndinga corresponding common context." ></td>
	<td class="line x" title="126:376	We note that this condition is important, in particular, in order to eliminate cases of anaphora and co-reference in contexts, where two words quite different in their meaning can sometimes appear in the same contexts only due to the text pragmatics in a particular situation." ></td>
	<td class="line x" title="127:376	For example, in some situations worker and demonstrator could be used interchangeably in text, but clearly it is a discourse coreference rather than common meaning that makes the substitution possible." ></td>
	<td class="line x" title="128:376	Instead, we are interested in identifying word pairs in which one words meaning provides a reference to the entailed words meaning." ></td>
	<td class="line x" title="129:376	This purpose is exactly captured by the existential propositions ofthe rst criterion above." ></td>
	<td class="line x" title="130:376	3 TheWordNetdenitionofthelexicalentailmentrelationis specied onlyforverbs and, therefore,is not felicitousforgeneralpurposes: Averb XentailsYifXcannot bedoneunless Yis, orhas been, done (e.g., snore and sleep)." ></td>
	<td class="line x" title="131:376	442 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality As reported further in Section 5.1, we observed that assessing these two conditions forcandidatewordsimilaritypairswasquiteintuitiveforannotators,andyieldedgood cross-annotator agreement." ></td>
	<td class="line x" title="132:376	Overall, substitutable lexical entailment captures directly thetypicallexicalsubstitutionscenariointextunderstandingapplications,aswellasin generictextualentailmentmodeling.Infact,thisrelationpartiallyoverlapswithseveral traditional lexical semantic relations that are known as relevant for lexical substitution, such as synonymy, hyponymy, and some cases of meronymy." ></td>
	<td class="line x" title="133:376	For example, we say that the meaning of company is lexically entailed by the meaning of rm (synonym) or automaker (hyponym), while the word government entails minister (meronym) as The government voted for the new law entails A minister in the government voted for the new law." ></td>
	<td class="line x" title="134:376	Ontheotherhand,lexicalentailmentisnotjustasupersetofotherknownrelations, butitisratherdesignedtoselectthosesub-casesofotherlexicalrelationsthatareneeded forappliedentailmentinference.Forexample,lexicalentailmentdoesnotcoverallcases of meronyms (e.g., division does not entail company), but only some sub-cases of partwholerelationshipmentionedherein.Inaddition,someotherrelationsarealsocovered by lexical entailment, like ocean and water and murder and death, which do not seem to directly correspond to meronymy or hyponymy relations." ></td>
	<td class="line x" title="135:376	Notice also that whereas lexical entailment is a directional relation that species which word of the pair entails the other, the relation may hold in both directions for a pair of words, as is the case for synonyms." ></td>
	<td class="line x" title="136:376	More detailed motivations for the substitutable lexical entailment relation and analysis of its relationship to traditional lexical semantic relations appear in Geffet(2006) and Geffetand Dagan (2004, 2005)." ></td>
	<td class="line x" title="137:376	4." ></td>
	<td class="line x" title="138:376	Bootstrapping Feature Weights To gain a better understanding of distributional similarity behavior we rst analyzed the output of the LIN measure, as a representative case for the state of the art, and regarding lexical entailment as a reference evaluation criterion." ></td>
	<td class="line x" title="139:376	We judge as correct, with respect to lexical entailment, those candidate pairs of the distributional similarity method for which entailment holds at least in one direction." ></td>
	<td class="line x" title="140:376	For example, the word area is entailed by country, since the existence of country entailstheexistenceofarea,andthesentenceThere is no rain in subtropical countries during the summer periodentailsthesentence There is no rain in subtropical areas during the summer period." ></td>
	<td class="line x" title="141:376	Asanother example, democracy isatypeof country inthe political sense, thus the existence entailment holds and also the sentence Israel is a democracy in the Middle East entails Israel is a country in the Middle East." ></td>
	<td class="line x" title="142:376	Ontheotherhand,ouranalysisrevealedthatmanycandidatewordsimilaritypairs suggested by distributional similarity measures do not correspond to tight semantic relationships." ></td>
	<td class="line x" title="143:376	In particular, many word pairs suggested by the LIN measure do not satisfy the lexical entailment relation, as demonstrated in Table 2." ></td>
	<td class="line x" title="144:376	A deeper look at the corresponding word feature vectors reveals typical reasons for these lexical entailment prediction errors." ></td>
	<td class="line x" title="145:376	Most relevant for the scope of the current article, in many cases highly ranked features in a word vector (when sorting the features by their weight) do not seem very characteristic for the word meaning." ></td>
	<td class="line x" title="146:376	This is demonstrated in Table 3, which shows the top 10 features in the vector for country." ></td>
	<td class="line x" title="147:376	As can be seen, some of the top features are either too specic (landlocked, airspace), and are thus less reliable, or too general (destination, ambition), thus not indicative and may co-occur with many different types of words." ></td>
	<td class="line x" title="148:376	On the other hand, intuitively more characteristic features of country, like population and governor, occur further down the 443 ComputationalLinguistics Volume35,Number3 Table 2 Thetop 20most similarwordsfor country (and theirranks) inthesimilaritylist of LIN,followed bythenextfourwordsinthesimilaritylistthatwerejudgedasentailingatleastinonedirection." ></td>
	<td class="line x" title="149:376	nation 1 *city 7 economy 13 *company 19 region 2 territory 8 *neighbor 14 *industry 20 state 3 area 9 *sector 15 kingdom 30 *world 4 *town 10 *member 16 place 35 *island 5 republic 11 *party 17 colony 41 *province 6 *north 12 government 18 democracy 82 Twelve out of 20 top similarities (60%) were judged as mutually non-entailing and are marked withanasterisk.Thesimilaritydatawasproducedas described inSection5." ></td>
	<td class="line x" title="150:376	Table 3 Thetop 10rankedfeaturesfor country produced by MI, theweightingfunctionemployedinthe LIN method." ></td>
	<td class="line o" title="151:376	Feature weight MI Commercial bank,gen, h 8.08 Destination,pcomp of, m 7.97 Airspace, pcomp of, h 7.83 Landlocked,mod, m 7.79 Tradebalance,gen, h 7.78 Sovereignty,pcomp of, h 7.78 Ambition,nn, h 7.77 Bourse,gen, h 7.72 Politician,gen, h 7.54 Border,pcomp of, h 7.53 sorted feature list, at positions 461and 832." ></td>
	<td class="line x" title="152:376	Overall, features that seem to characterize the word meaning well are scattered across the ranked feature list, while many nonindicative features receive high weights." ></td>
	<td class="line x" title="153:376	This behavior often yields high similarity scores for word pairs whose semantic similarity is rather loose while missing some much tighter similarities." ></td>
	<td class="line x" title="154:376	Furthermore, we observed that characteristic features for a word w, which should receive higher weights, are expected to be common for w and other words that are semantically similar to it." ></td>
	<td class="line x" title="155:376	This observation suggests a computational scheme which wouldpromotetheweightsoffeaturesthatarecommonforsemanticallysimilarwords." ></td>
	<td class="line x" title="156:376	Ofcourse,thereisaninherentcircularityinsuchascheme:todeterminewhichfeatures should receive high weights we need to know which words are semantically similar, while computing distributional semantic similarity already requires pre-determined feature weights." ></td>
	<td class="line x" title="157:376	This kind of circularity can be approached by a bootstrapping scheme." ></td>
	<td class="line x" title="158:376	We rst compute initial distributional similarity values, based on an initial feature weighting function." ></td>
	<td class="line x" title="159:376	Then, to learn more accurate feature weights for a word w,wepromote features that characterize other words that are initially known to be similar to w.By the same rationale, features that do not characterize many words that are sufciently similar to w are demoted." ></td>
	<td class="line x" title="160:376	Even if such features happen to have a strong direct statistical association with w they would not be considered reliable, because they are not supported by additional words that have asimilar meaning to that of w. 444 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality 4.1 Bootstrapped Feature Weight Denition The bootstrapped feature weight is dened as follows." ></td>
	<td class="line x" title="161:376	First, some standard word similarity measure sim is computed to obtain an initial approximation of the similarity space.Then,wedenethe word set ofafeature f,denotedby WS(f),asthesetofwords forwhich f isanactivefeature.RecallfromSection2.2thatanactivefeatureisafeature that is strongly associated with the word, that is, its (initial) weight is higher than an empiricallypredenedthreshold,  weight .Thesemantic neighborhood of w,denotedby N(w), is dened as the set of all words v which are considered sufciently similar to w, satisfying sim(w,v) > sim , where  sim is a second empirically determined threshold." ></td>
	<td class="line x" title="162:376	Thebootstrapped feature weight, denoted weight B ,is then dened by: weight B (w,f)= summationtext vWS(f)N(w) sim(w,v) (6) That is, we identify all words v that are in the semantic neighborhood of w and are also characterized by f,and then sum the values of their similarities to w. Intuitively, summing these similarity values captures simultaneously a desired balance between feature specicity and generality, addressing the observations in the beginning of this section." ></td>
	<td class="line x" title="163:376	Some features might characterize just a single word that is verysimilarto w,butthenthesumofsimilaritieswillincludeasingleelement,yielding a relatively low weight." ></td>
	<td class="line x" title="164:376	This is why the sum of similarities is used rather than an average value, which might become too high by chance when computed over just a single element (or very few elements)." ></td>
	<td class="line x" title="165:376	Relatively generic features, which occur with many words and are thus less indicative, may characterize more words within N(w) but then on average the similarity values of these words with w is likely to be lower, contributingsmallervaluestothesum.Toreceiveahighoverallweightareliablefeature has to characterize multiple words that are highly similar to w. We note that the bootstrapped weight is a sum of word similarity values rather than a direct function of wordfeature association values, which is the more common approach." ></td>
	<td class="line x" title="166:376	It thus does not depend on the exact statistical co-occurrence level between w and f. Instead, it depends on a more global assessment of the association between f and the semantic vicinity of w. We notice that the bootstrapped weight is determined separately relative to each individual word." ></td>
	<td class="line x" title="167:376	This differs from measures that are global word-independent functions of the feature, such as the feature entropy used in Grefenstette (1994) and the feature term strength relative to a predened class as employed in Pekar, Krkoska, and Staab (2004) for supervised word classication." ></td>
	<td class="line x" title="168:376	4.2 Feature Reduction and Similarity Re-Computation Oncethebootstrappedweightshavebeencomputed,theiraccuracyissufcienttoallow foraggressivefeaturereduction.Asshowninthefollowingsection,inourexperiments it sufced to use only the top 100 features for each word in order to obtain optimal word similarity results, because the most informative features now receive the highest weights." ></td>
	<td class="line x" title="169:376	Finally,similaritybetweenwordsisre-computedoverthereducedvectorsusingthe sim functionwith weight B replacingtheoriginalfeatureweights.Theresultingsimilarity measure is further referred to as sim B . 445 ComputationalLinguistics Volume35,Number3 5." ></td>
	<td class="line x" title="170:376	Evaluation by Lexical Entailment To test the effectiveness of the bootstrapped weighting scheme, we rst evaluated whether it contributes to better prediction of lexical entailment." ></td>
	<td class="line x" title="171:376	This evaluation was based on gold-standard annotations determined by human judgments of the substitutable lexical entailment relation, as dened in Section 3." ></td>
	<td class="line x" title="172:376	The new similarity scheme, sim B , based on the bootstrapped weights, was rst computed using the standard LIN method as the initial similarity measure." ></td>
	<td class="line x" title="173:376	The resulting similarity lists of sim LIN (the original LIN method) and sim B LIN (Bootstrapped LIN) schemes were evaluated for a sample of nouns (Section 5.2)." ></td>
	<td class="line x" title="174:376	Then, the evaluation was extended (Section 5.3) to apply the bootstrapping scheme over the two additional similarity measures that were presented in Section 2.2, sim WJ (weighted Jaccard) and sim COS (Cosine)." ></td>
	<td class="line x" title="175:376	Along with these lexical entailment evaluations we also analyzed directly the quality of the bootstrapped feature vectors, according to the average common-feature rank ratio measure, which was dened in Section 6." ></td>
	<td class="line x" title="176:376	5.1 Experimental Setting Ourexperimentswereconductedusingstatisticsfroman18milliontokensubsetofthe Reuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 199608-20 to 1997-08-19), parsed by Lins Minipar dependency parser (Lin 1993)." ></td>
	<td class="line x" title="177:376	The test set of candidate word similarity pairs was constructed for a sample of 30randomlyselectednounswhosecorpusfrequencyexceeds500.Inourprimaryexperiment we computed the top 40 most similar words for each noun by the sim LIN and by sim B LIN measures,yielding1,200pairsforeachmethod,and2,400pairsaltogether.About 800 of these pairs were common for the two methods, therefore leaving approximately 1,600 distinct candidate word similarity pairs." ></td>
	<td class="line x" title="178:376	Because the lexical entailment relation is directional, eachcandidate pairwasduplicatedtocreatetwodirectionalpairs,yielding atestsetof3,200pairs.Thus,foreachpairofwords,wandv,thetwoorderedpairs(w,v) and (v,w)were created tobejudged separately forentailment in thespecied direction (whether the rst word entails the other)." ></td>
	<td class="line x" title="179:376	Consequently, a non-directional candidate similaritypair w,v isconsideredasacorrectentailmentifitwasassessedasanentailing pair at least in one direction." ></td>
	<td class="line x" title="180:376	The assessors were only provided with a list of word pairs without any contextual information and could consult any available dictionary, WordNet, and the Web." ></td>
	<td class="line x" title="181:376	The judgment criterion follows the criterion presented in Section 3." ></td>
	<td class="line x" title="182:376	In particular, the judges were asked to apply the two operational conditions, existence and substitutability in context, to each given pair." ></td>
	<td class="line x" title="183:376	Prior to performing the nal test of the annotation experiment, the judges were presented with an annotated set of entailing and non-entailing pairs along with the existential statements and sample sentences for substitution, demonstrating how the two conditions could be applied in different cases of entailment." ></td>
	<td class="line x" title="184:376	In addition, they had to judge a training set of several dozen pairs and thendiscusstheirjudgmentdecisionswitheachothertogainabetterunderstandingof the two criteria." ></td>
	<td class="line x" title="185:376	The following example illustrates this process." ></td>
	<td class="line x" title="186:376	Given a non-directional pair {company, organization} two directional pairs are created: (company, organization) and (organization, company)." ></td>
	<td class="line x" title="187:376	The former pair is judged as a correct entailment: the existence of a company entails the existence of an organization, and the meaning of the sentence: John works for a large company entails the meaning of the sentence with substitution: John works for a large organization." ></td>
	<td class="line x" title="188:376	Hence, company lexically entails organization,butnot 446 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality viceversa(asshowninSection3.3),thereforethesecondpairisjudgedasnotentailing." ></td>
	<td class="line x" title="189:376	Eventually, the non-directional pair {company, organization} is considered as a correct entailment." ></td>
	<td class="line x" title="190:376	Finally, the test set of 3,200 pairs was split into three disjoint subsets that were judged by three native English speaking assessors, each of whom possessed a Bachelors degree in English Linguistics." ></td>
	<td class="line x" title="191:376	For each subset a different pair of assessors was assigned, each person judging the entire subset." ></td>
	<td class="line x" title="192:376	The judges were grouped into three different pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII)." ></td>
	<td class="line x" title="193:376	Each pair was assigned initially to judge all the word similarities in each subset, and the third assessor was employed in cases of disagreement between the rst two." ></td>
	<td class="line x" title="194:376	The majority vote was taken as the nal decision." ></td>
	<td class="line x" title="195:376	Hence, each assessor had to fully annotate two thirds of the data and for a third subset she only had to judge the pairs for which there wasdisagreementbetweentheothertwojudges.Thiswasdoneinordertomeasurethe agreement achieved for different pairs ofannotators." ></td>
	<td class="line x" title="196:376	Theoutputpairsfrombothmethodsweremixedsotheassessorscouldnotassociate a pair with the method that proposed it." ></td>
	<td class="line x" title="197:376	We note that this evaluation methodology, in which human assessors judge the correctness of candidate pairs by some semantic substitutabilitycriterion,issimilartocommonevaluationmethodologiesusedforparaphrase acquisition (Barzilay and McKeown 2001; Lin and Pantel 2001; Szpektor et al. 2004)." ></td>
	<td class="line x" title="198:376	Measuring human agreement level for this task, the proportions of matching decisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and 91.2%forJudgeIIandJudgeIII.Thecorrespondingkappavaluesare0.83,0.80,and0.80, which is regarded as very good agreement (Landis and Koch 1997)." ></td>
	<td class="line x" title="199:376	It is interesting to note that after some discussion most of the disagreements were settled, and the few remaining mismatches were due to different understandings of word meanings." ></td>
	<td class="line x" title="200:376	These ndingsseemtohaveasimilaravortothehumanagreementndingsreportedforthe RecognizingTextualEntailmentchallenges(Bar-Haimetal.2006;Dagan,Glickman,and Magnini2006),inwhichentailmentwasjudgedforpairsofsentences.Infact,thekappa values obtained in our evaluation are substantially higher than reported for sentencelevel textual entailment, which suggests that it is easier to make entailment judgments at the lexical level than at the full sentence level." ></td>
	<td class="line x" title="201:376	The parameter values of the algorithms were tuned using a development set of similarity pairs generated for 10 additional nouns, distinct from the 30 nouns used for the test set." ></td>
	<td class="line x" title="202:376	The parameters were optimized by running the algorithm systematically with various values across the parameter scales and judging a sample subset of the results." ></td>
	<td class="line o" title="203:376	weight MI =4wasfoundastheoptimalMI threshold for active feature weights (featuresincludedinthefeaturevectors),yieldinga10%precisionincreaseofsim LIN and removing over 50% of the data relative to no feature ltering." ></td>
	<td class="line x" title="204:376	Accordingly, this value also serves as the  weight threshold in the bootstrapping scheme (Section 4)." ></td>
	<td class="line x" title="205:376	As for the  sim parameter, the best results on the development set were obtained for  sim =0.04,  sim =0.02, and  sim =0.01when bootstrapping over the initial similarity measures LIN, WJ,andCOS, respectively." ></td>
	<td class="line x" title="206:376	5.2 Evaluation Results for sim B LIN We measured the contribution of the improved feature vectors to the resulting precision of sim LIN and sim B LIN in predicting lexical entailment." ></td>
	<td class="line x" title="207:376	The results are presented in Table 4, where precision and error reduction values were computed for the top 20, 30, and 40 word similarity pairs produced by each method." ></td>
	<td class="line x" title="208:376	It can be seen that the 447 ComputationalLinguistics Volume35,Number3 Table 4 Lexical entailmentprecisionvaluesfortop-nsimilar wordsbythe Bootstrapped LIN and the original LIN method." ></td>
	<td class="line x" title="209:376	Top-n Correct Error Rate Words Entailments(%) Reduction(%) sim LIN sim B LIN Top 20 52.0 57.9 12.3 Top 30 48.2 56.2 15.4 Top 40 41.0 49.7 14.7 Bootstrapped LIN method outperformed the original LIN approach by 69 precision points at all top-n levels." ></td>
	<td class="line x" title="210:376	As expected, the precision for the shorter top 20 list is higher for both methods, thus leaving a bitless room forimprovement." ></td>
	<td class="line x" title="211:376	Overall, the Bootstrapped LIN method extracted 104 (21%) more correct similarity pairs than the other measure and reduced the number of errors by almost 15%." ></td>
	<td class="line x" title="212:376	We also computed the relative recall, which shows the percentage of correct word similarities found by each method relative to the joint set of similarities that were extracted by both methods." ></td>
	<td class="line x" title="213:376	The overall relative recall of the Bootstrapped LIN was quite high (94%), exceeding LINs relative recall (of 78%) by 16 percentage points." ></td>
	<td class="line x" title="214:376	We found that the bootstrappedmethodcoversover90%ofthecorrectsimilaritieslearnedbytheoriginal method, while also identifying many additional correct pairs." ></td>
	<td class="line x" title="215:376	It should be noted at this point that the current limited precision levels are determinednotjustbythequalityofthefeaturevectorsbutsignicantlybythenatureofthe vector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard and Cosine as reported in Section 5.3)." ></td>
	<td class="line x" title="216:376	It was observed in other work (Geffet and Dagan 2005) that these common types of vector comparison schemes exhibit certain aws in predicting lexical entailment." ></td>
	<td class="line x" title="217:376	Our present work thus shows that the bootstrapping method yields a signicant improvement in feature vector quality, but future research is needed to investigate improved vector comparison schemes." ></td>
	<td class="line x" title="218:376	An additional indication of the improved vector quality is the massive feature reduction allowed by having the most characteristic features concentrated at the top ranks of the vectors." ></td>
	<td class="line x" title="219:376	The vectors of active features of LIN, as constructed after standard feature ltering (Section 5.1), could be further reduced by the bootstrapped weighting to about one third of their size." ></td>
	<td class="line x" title="220:376	As illustrated in Figure 1, changing the vector size signicantly affects the similarity results." ></td>
	<td class="line x" title="221:376	In sim B LIN the best result was obtained with the top 100 features per word, while using less than 100 or more than 150 features caused a 510% decrease in performance." ></td>
	<td class="line n" title="222:376	On the other hand, an attempt to cut off the lower ranked features of the MI weighting always resulted in a noticeable decrease in precision." ></td>
	<td class="line n" title="223:376	These results show that for MI weighting many important features appear further down in the ranked vectors, while for the bootstrapped weighting adding too many features adds mostly noise, since most characteristic features are concentrated at the top ranks." ></td>
	<td class="line x" title="224:376	Thus, in addition to better feature weighting, the bootstrapping step provides effective feature reduction, which improves vector quality and consequently the similarity results." ></td>
	<td class="line x" title="225:376	We note that the optimal vector size we obtained conforms to previous results for example, by Widdows (2003), Curran (2004), and Curran and Moens (2002)who also used reduced vectors of up to 100 features as optimal for learning hyponymy and synonymy, respectively." ></td>
	<td class="line x" title="226:376	In Widdows the known SVD method for dimension reduction 448 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality Figure 1 Percentageofcorrect entailmentswithinthetop40 candidatepairsof each ofthemethods, LIN and Bootstrapped LIN (denotedas LINB inthegure),whenusingvaryingnumbersof top-rankedfeaturesinthefeaturevector.Thevalueof Allcorrespondstothefullsizeof vectorsandis typicallyin therangeof 300400 features." ></td>
	<td class="line x" title="227:376	of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, only the strongly associated verbs (direct and indirect objects of the noun) are selected as canonical features that are expected to be shared by true synonyms." ></td>
	<td class="line x" title="228:376	Finally, we tried executing an additional bootstrapping iteration of weight B calculation over the similarity results of sim B LIN . The resulting increase in precision was much smaller, of about 2%, showing that most of the potential benet is exploited in the rst bootstrapping iteration (which is not uncommon for natural language data)." ></td>
	<td class="line x" title="229:376	On the other hand, computing the bootstrapping weight twice increases computation time signicantly, which led us to suggest a single bootstrapping iteration as a reasonable cost-effectiveness tradeoff for our data." ></td>
	<td class="line x" title="230:376	5.3 Evaluation for sim B WJ and sim B COS Tofurthervalidatethebehaviorofthebootstrappingschemeweexperimentedwithtwo additionalsimilaritymeasures,weightedJaccard(sim WJ )andCosine(sim COS )(described in Section 2.2)." ></td>
	<td class="line o" title="231:376	For each of the additional measures the experiment repeats the main three steps described in Section 4: Initially, the basic similarity lists are calculated for each of the measures using MI weighting; then, the bootstrapped weighting, weight B ,is computed based on the initial similarities, yielding new word feature vectors; nally, the similarity values are recomputed by the same vector similarity measure using the new feature vectors." ></td>
	<td class="line o" title="232:376	To assess the effectiveness of weight B we computed the four alternative output similarity lists, using the sim WJ and sim COS similarity measures, each with the weight MI 449 ComputationalLinguistics Volume35,Number3 Table 5 Comparativeprecisionvalues forthetop20 similaritylistsof thethreeselected similarity measures, with MI and Bootstrappedfeatureweightingfor each." ></td>
	<td class="line x" title="233:376	Measure LINLIN B WJWJ B COSCOS B Correct Similarities(%) 52.057.9 51.054.8 46.150.9 and weight B weighting functions." ></td>
	<td class="line x" title="234:376	The four lists were judged for lexical entailment by three assessors, according to the same procedure described in Section 5.1." ></td>
	<td class="line x" title="235:376	To make the additionalmanualevaluationaffordablewejudgedthetop20similarwordsineachlist for each of the 30 target nouns of Section 5.1." ></td>
	<td class="line o" title="236:376	Table 5 summarizes the precision values achieved by LIN, WJ,andCOS with both weight MI and weight B . As shown in the table, bootstrapped weighting consistently contributed between 46 points to the accuracy of each method in the top 20 similarity list." ></td>
	<td class="line x" title="237:376	We view the results as quite positive, considering that improving over top 20 similaritiesisamuchmorechallengingtaskthanimprovingoverlongersimilaritylists, while the improvement was achieved only by modifying the feature vectors without changing the similarity measure itself (as hinted in Section 5.2)." ></td>
	<td class="line x" title="238:376	Our results are also compatible with previous ndings in the literature (Dagan, Lee, and Pereira 1999; Weeds, Weir, and McCarthy 2004) that found LIN and WJ to be more accurate for similarity acquisition than COS." ></td>
	<td class="line x" title="239:376	Overall, the results demonstrate that the bootstrapped weighting scheme consistently produces improved results." ></td>
	<td class="line x" title="240:376	An interesting behavior of the bootstrapping process is that the most prominent featuresforagiventargetwordconvergeacrossthedifferentinitialsimilaritymeasures, as exemplied in Table 6." ></td>
	<td class="line x" title="241:376	In particular, although the initial similarity lists overlap only partly, 4 the overlap of the top 30 features for our 30-word sample was ranging between 88% and 100%." ></td>
	<td class="line x" title="242:376	This provides additional evidence that the quality of the bootstrapped weighting is quite similar for various initial similarity measures." ></td>
	<td class="line x" title="243:376	6." ></td>
	<td class="line p" title="244:376	Analyzing the Bootstrapped Feature Vector Quality In this section we provide an in-depth analysis of the bootstrapping feature weighting quality compared tothe state-of-the-art MI weighting function." ></td>
	<td class="line x" title="245:376	6.1 Qualitative Observations The problematic feature ranking noticed at the beginning of Section 4 can be revealed moreobjectivelybyexaminingthecommonfeatureswhichcontributemosttotheword similarityscores.Tothatend,weexaminethecommonfeaturesofthetwogivenwords and sort them by the sum of their weights in both word vectors." ></td>
	<td class="line x" title="246:376	Table 7 shows the top 10commonfeaturesbythissortingforapairoftrulysimilar(lexicallyentailing)words (countrystate), and for a pair of non-entailing words (countryparty)." ></td>
	<td class="line x" title="247:376	For each common feature the table shows its two corresponding ranks in the feature vectors of the two words." ></td>
	<td class="line x" title="248:376	4 Overlapratewasabout40%between COS and WJ or LIN, and 70%between WJ and LIN.Theoverlap wascomputedfollowingtheprocedure ofWeeds,Weir,and McCarthy(2004),disregarding the orderof thesimilarwordsinthe lists." ></td>
	<td class="line x" title="249:376	Interestingly, they obtainedroughlysimilargures, of28%overlapfor COS and WJ,32%overlapfor COS and LIN, and 81%overlapbetween LIN and WJ." ></td>
	<td class="line x" title="250:376	450 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality Table 6 Top30 featuresof town bybootstrappedweightingbased on LIN, WJ,andCOS asinitial similarities.Thethreesets ofwordsarealmost identical,withrelativelyminorranking differences." ></td>
	<td class="line x" title="251:376	LIN B WJ B COS B southern southern northern northern northern southern ofce ofce remote eastern ofcial eastern remote coastal ofcial ofcial eastern based troop northeastern northeastern northeastern remote ofce people troop coastal coastal people northwestern attack based people based populated attack populated attack troop northwestern home home base northwestern south home south western south western city west west populated western resident base neighboring neighboring resident resident house north plant city west police base neighboring held trip trip locate camp surrounding trip held police city north held site locate locate camp surrounding house surrounding police camp It can be observed in Table 7 that for both word pairs the common features are scattered across the pair of feature vectors, making it difcult to distinguish between the truly similar and the non-similar pairs." ></td>
	<td class="line x" title="252:376	We suggest, on the other hand, that the desired behavior of effective feature weighting is that the common features of truly similar words would be concentrated at the top ranks of both word vectors." ></td>
	<td class="line x" title="253:376	In other words, if the two words are semantically similar then we expect them to share their most characteristic features, which are in turn expected to appear at the higher ranks of each feature vector." ></td>
	<td class="line x" title="254:376	The common features for non-similar words are expected to be scattered all across each of the vectors." ></td>
	<td class="line x" title="255:376	In fact, these expectations correspond exactly to the rationale behind distributional similarity measures: Such measures are designed to assign higher similarity scores for vector pairs that share highly weighted features." ></td>
	<td class="line x" title="256:376	Comparatively,weillustratethebehaviorofthe Bootstrapped LIN methodrelativeto the observations regarding the original LIN method, using the same running example." ></td>
	<td class="line x" title="257:376	Table 8 shows the top 10 features of country." ></td>
	<td class="line x" title="258:376	We observe that the list now contains features that are intuitively quite indicative and reliable, while many too specic or idiomaticfeatures,andtoogeneralones,weredemoted(comparewithTable3).Table9 showsthatmostofthetop10commonfeaturesfor countrystate arenowrankedhighly 451 ComputationalLinguistics Volume35,Number3 for both words." ></td>
	<td class="line x" title="259:376	On the other hand, there are only two common features (among the top100features)fortheincorrectpaircountryparty,bothwithquitelowranks(compare withTable7),whiletherestofthecommonfeaturesforthispairdidnotpassthetop100 cutoff." ></td>
	<td class="line x" title="260:376	Consequently, Table 10 demonstrates a much more accurate similarity list for country,wheremanyincorrect(non-entailing)wordsimilarities,likepartyandcompany,were demoted." ></td>
	<td class="line x" title="261:376	Instead, additional correct similarities, like kingdom and land, were promoted (compare with Table 2)." ></td>
	<td class="line x" title="262:376	In this particular case all the remaining errors correspond to words that are related quite closely to country, denoting geographic concepts." ></td>
	<td class="line x" title="263:376	Many of these errors are context dependent entailments which might be substitutable in some cases, but they violate the word meaning entailment condition (e.g., countryneighbor, countryport)." ></td>
	<td class="line x" title="264:376	Apparently, these words tend to occur in contexts that are typical for country in the Reuters corpus." ></td>
	<td class="line x" title="265:376	Some errors violating the substitutability condition of lexical entailment were identied as well, such as industryproduct." ></td>
	<td class="line x" title="266:376	These cases are quite hard to differentiate from correct entailments, since the two words are usually closely related to each other and also share highly ranked features, because they often appear in similar characteristic contexts." ></td>
	<td class="line x" title="267:376	It may therefore be difcult to lter out such Table 7 LIN (MI) weighting:Thetop10 commonfeaturesfor countrystate and countryparty,alongwith theircorrespondingranksineach ofthetwofeaturevectors." ></td>
	<td class="line x" title="268:376	Thefeaturesaresorted bythesum of theirfeatureweightswithbothwords." ></td>
	<td class="line x" title="269:376	CountryState Ranks CountryParty Ranks Broadcast,pcomp in, h 24 50 Brass,nn, h 64 22 Goods,mod, h 140 16 Concluding,pcomp of, h 73 20 Civilservant,gen, h 64 54 Representation,pcomp of, h 82 27 Bloc,gen, h 30 77 Patriarch,pcomp of, h 128 28 Nonaligned,mod, m 55 60 Friendly,mod, m 58 83 Neighboring,mod, m 15 165 Expel,pcomp from, h 59 30 Statistic,pcomp on, h 165 43 Heartland,pcomp of, h 102 23 Border,pcomp of, h 10 247 Surprising,pcomp of, h 114 38 Northwest,mod, h 41174 Issue, pcomp between, h 103 51 Trip, pcomp to, h 105 34 Contravention,pcomp in, m 129 43 Table 8 Top 10featuresof country bytheBootstrappedfeatureweighting." ></td>
	<td class="line x" title="270:376	Feature Weight B Industry,gen, h 1.21 Airport,gen, h 1.16 Visit, pcomp to, h 1.06 Neighboring,mod, m 1.04 Law,gen, h 1.02 Economy,gen, h 1.02 Population,gen, h 0.93 Stockmarket,gen, h 0.92 Governor,pcomp of, h 0.92 Parliament,gen, h 0.91 452 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality Table 9 Bootstrappedweighting:top10commonfeaturesfor countrystate and countryparty alongwith theircorrespondingranksinthetwo(sorted)featurevectors." ></td>
	<td class="line x" title="271:376	CountryState Ranks CountryParty Ranks Neighboring,mod, m 31Relation,pcompwith, h 1226 Industry,gen, h 111Minister,pcomp from, h 77 49 Impoverished,mod, m 88 Governor,pcomp of, h 109 Population,gen, h 616 City,gen, h 1718 Economy,gen, h 515 Parliament,gen, h 1022 Citizen,pcomp of, h 1425 Law,gen, h 433 Table 10 Top20 most similarwordsfor country and theirranksinthesimilaritylist bythe Bootstrapped LIN measure." ></td>
	<td class="line x" title="272:376	nation 1 territory 6 *province 11 zone 16 state 2 *neighbor 7 *city 12 land 17 *island 3 colony 8 *town 13 place 18 region 4 *port 9 kingdom 14 economy 19 area 5 republic 10 *district 15 *world 20 Note that four of the incorrect similarities from Table 2 were replaced with correct entailments resultingin a20%increase of precision(reaching60%)." ></td>
	<td class="line x" title="273:376	non-substitutable similarities merely by the standard distributional similarity scheme, suggesting that additional mechanisms and data types would be required." ></td>
	<td class="line x" title="274:376	6.2 The Average Common-Feature Rank Ratio It should be noted at this point that these observations regarding feature weight behavior are based on subjective intuition of how characteristic features are for a word meaning, which is quite difcult to assess systematically." ></td>
	<td class="line x" title="275:376	Therefore, we next propose a quantitative measure for analyzing the quality offeature vector weights." ></td>
	<td class="line x" title="276:376	More formally, given a pair of feature vectors for words w and v we rst dene theiraverage common-feature rankwithrespecttothetop-ncommonfeatures,denoted acfr n ,as follows: acfr n (w,v)= 1 n summationtext ftopn(F(w)F(v)) 1 2 [rank(w,f)+rank(v,f)] (7) where rank(w,f) is the rank of feature f in the vector of the word w when features are sorted by their weight, and F(w)isthesetoffeaturesinws vector." ></td>
	<td class="line x" title="277:376	top-n is the set of top n common features to consider, where common features are sorted by the sum of their weights in the two word vectors (the same sorting as in Table 7)." ></td>
	<td class="line x" title="278:376	In other words, acfr n (w,v)istheaveragerankinthetwofeaturevectorsoftheirtop n commonfeatures." ></td>
	<td class="line x" title="279:376	453 ComputationalLinguistics Volume35,Number3 Using this measure, we expect that a good feature weighting function would typically yield lower values of acfr n for truly similar words (as low ranking values correspondtohigherpositionsinthevectors)thanfornon-similarwords.Hence,given a pre-judged test set of pairs of similar and non-similar words, we dene the ratio, acfr-ratio, between the average acfr n of the set of all the non-similar words, denoted as Non-Sim,andtheaverage acfr n ofthesetofalltheknownpairsofsimilarwords, Sim,to be an objective measure for feature weighting quality, as follows: acfr n  ratio = 1 |NonSim| summationtext w,vNonSim acfr n (w,v) 1 |Sim| summationtext w,vSim acfr n (w,v) (8) As an illustration, the two word pairs in Table 7 yielded acfr 10 (country,state)=78 and acfr 10 (country,party)=64." ></td>
	<td class="line x" title="280:376	Both values are quite high, showing no principal difference between the tighter lexically entailing similarity versus a pair of non-similar (or rather loosely related) words." ></td>
	<td class="line n" title="281:376	This behavior indicates the deciency of the MI feature weighting function in this case." ></td>
	<td class="line x" title="282:376	On the other hand, the corresponding values for the two pairs produced by the Bootstrapped LIN method (for the features in Table 9) are acfr 10 (country,state)=12andacfr 10 (country,party)=41." ></td>
	<td class="line x" title="283:376	These gures clearly reect the desired distinction between similar and non-similar words, showing that the common features of the similar words are indeed concentrated at much higher ranks in the vectors than the common features of the non-similar words." ></td>
	<td class="line x" title="284:376	In recent work on distributional similarity (Curran 2004; Weeds and Weir 2005) a varietyofalternativeweightingfunctionswerecompared.However,thequalityofthese weighting functions was evaluated only through their impact on the performance of a particular word similarity measure, as we did in Section 5." ></td>
	<td class="line x" title="285:376	Our acfr-ratio measure providestherstattempttoanalyzethequalityofweightingfunctionsdirectly,relative to apre-judged word similarity set,without reference to aconcrete similarity measure." ></td>
	<td class="line o" title="286:376	6.3 An Empirical Assessment of the acfr-ratio Inthissubsectionwereportanempiricalcomparisonoftheacfr-ratioobtainedfortheMI and BootstrappedLIN weighting functions." ></td>
	<td class="line x" title="287:376	To that end, we have run the Minipar system on the full Reuters RCV1corpus, which contains 2.5 GB of English news stories, and thencalculatedtheMI-weightedfeaturevectors.Theoptimizedthresholdonthefeature weights,  weight , was set to 0.2." ></td>
	<td class="line x" title="288:376	Further, to compute the Bootstrapped LIN feature weights a  sim of 0.02 was applied to the LIN similarity values." ></td>
	<td class="line x" title="289:376	In this experiment we employed the full bootstrapped vectors (i.e., without applying feature reduction by the top 100 cutoff)." ></td>
	<td class="line x" title="290:376	This was done to avoid the effect of the feature vector size on the acfr n metric, which tends to naturally assign higher scores to shorter vectors." ></td>
	<td class="line x" title="291:376	As computing the acfr-ratio requires a pre-judged sample of candidate word similaritypairs,weutilizedtheannotatedtestsampleofcandidatepairsofwordsimilarities described in Section 5, which contains both entailing and non-entailing pairs." ></td>
	<td class="line o" title="292:376	First, we computed the average common-feature rank scores (acfr n ) (with varying valuesofn)forweight MI andforweight B overallthepairsinthetestsample.Interestingly, the mean acfr n scores for weight B range within 110264 for n = 10100, while the correspondingrangefor weight MI isbyanorderofmagnitudehigher:7801,254,despite the insignicant differences in vector sizes." ></td>
	<td class="line x" title="293:376	Therefore, we conclude that the common features that are relevant to establishing distributional similarity in general (regardless of entailment) are much more scattered across the vectors by MI weighting, while with bootstrapping they tend to appear at higher positions in the vectors." ></td>
	<td class="line x" title="294:376	These gures 454 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality reect a desired behavior of the bootstrapping function which concentrates most of the prominentcommonfeaturesforallthedistributionallysimilarwords(whetherentailing or not) at the lower ranks of their vectors." ></td>
	<td class="line x" title="295:376	In particular, this explains the ability of our method to perform a massive feature reduction as demonstrated in Section 5, and to produce more informative vectors, while demoting and eliminating much of the noise in the original vectors." ></td>
	<td class="line x" title="296:376	Next, we aim to measure the discriminative power of the compared methods to distinguish between entailing and non-entailing pairs." ></td>
	<td class="line x" title="297:376	To this end we calculated the acfr-ratio, which captures the difference in the average common feature ranks between entailing vs. non-entailing pairs, for both the MI-based and bootstrapped vectors." ></td>
	<td class="line x" title="298:376	The obtained results are presented in Figure 2." ></td>
	<td class="line o" title="299:376	As can be seen the acfr-ratio values are consistently higher for Bootstrapped LIN than for MI." ></td>
	<td class="line o" title="300:376	That is, the bootstrapping method assigns much higher acfr n scores to entailing words than to non-entailing ones, whereas for MI the corresponding acfr n scores for entailing and non-entailing pairs are roughly equal." ></td>
	<td class="line x" title="301:376	In particular, we notice that the largest gaps in acfr-ratio occur for lower numbers of top common features, whose weights are indeed the most important and inuential in distributional similarity measures." ></td>
	<td class="line x" title="302:376	Thus, these ndings suggest a direct indication of an improved quality of the bootstrapped feature vectors." ></td>
	<td class="line x" title="303:376	7." ></td>
	<td class="line x" title="304:376	A Pseudo-Word Sense Disambiguation Evaluation Thelexicalentailmentevaluationreportedhereincorrespondstothelexicalsubstitution applicationofdistributionalsimilarity.Theothertypeofapplication,asreviewedinthe Introduction, is similarity-based prediction of word co-occurrence likelihood, needed for disambiguation applications." ></td>
	<td class="line x" title="305:376	Comparative evaluations of distributional similarity methods for this type of application were commonly conducted using a pseudo-word sense disambiguation scheme, which is replicated here." ></td>
	<td class="line x" title="306:376	In the next subsections we rst describe how distributional similarity can help improve word sense disambiguation (WSD)." ></td>
	<td class="line o" title="307:376	Then we describe how the pseudo-word sense disambiguation task, which Figure 2 Comparisonbetween the acfr-ratio for MI and Bootstrapped LIN methods,when usingvarying numbersof commontop-rankedfeaturesinthewordsfeaturevectors." ></td>
	<td class="line x" title="308:376	455 ComputationalLinguistics Volume35,Number3 corresponds to the general WSD setting, was used to evaluate the co-occurrence likelihood predictions obtained byalternative similarity methods." ></td>
	<td class="line x" title="309:376	7.1 Similarity Modeling for Word Sense Disambiguation WSD methods need to identify the correct sense of an ambiguous word in a given context." ></td>
	<td class="line x" title="310:376	For example, a test instance for the verb save might be presented in the context saving Private Ryan." ></td>
	<td class="line x" title="311:376	The disambiguation method must decide whether save in this particular context means rescue, preserve, keep, lay aside, or some other alternative." ></td>
	<td class="line x" title="312:376	Sense recognition is typically based on context features collected from a senseannotated training corpus." ></td>
	<td class="line x" title="313:376	For example, the system might learn from the annotated training data that the word soldier is a typical object for the rescuing sense of save,asin: They saved the soldier." ></td>
	<td class="line x" title="314:376	In this setting, distributional similarity is used to reduce the data sparseness problem via similarity-based generalization." ></td>
	<td class="line x" title="315:376	The general idea is to predict the likelihood of unobserved word co-occurrences based on observed co-occurrences of distributionally similar words." ></td>
	<td class="line x" title="316:376	For example, assume that the noun private did not occur as a direct object of save in the training data." ></td>
	<td class="line x" title="317:376	Yet, some of the words that are distributionally similar to private,like soldier or sergeant,mighthaveoccurred with save." ></td>
	<td class="line x" title="318:376	Thus, a WSD system may infer that the co-occurrence save private is more likely for the rescuing sense of save because private is distributionally similar to soldier, which did cooccurwiththissenseof save intheannotatedtrainingcorpus.Ingeneralterms,theWSD method estimates the co-occurrence likelihood for the target sense and a given context word based on training data for words that are distributionally similar to the context word." ></td>
	<td class="line x" title="319:376	This idea of similarity-based estimation of co-occurrence likelihood was applied in Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machine translation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed a Latent Semantic Analysis (LSA)-based kernel function as a similarity-based representation for WSD." ></td>
	<td class="line x" title="320:376	Other works employed the same idea for pseudo-word sense disambiguation, as explained in the next subsection." ></td>
	<td class="line x" title="321:376	7.2 The Pseudo-Word Sense Disambiguation Setting Sense disambiguation typically requires annotated training data, created with considerable human effort." ></td>
	<td class="line x" title="322:376	Yarowsky (1992) suggested that when using WSD as a test bed for comparative algorithmic evaluation it is possible to set up a pseudo-word sense disambiguation scheme." ></td>
	<td class="line x" title="323:376	This scheme was later adopted in several experiments, and was popular for comparative evaluations of similarity-based co-occurrence likelihood estimation(Dagan,Lee,andPereira1999;Lee1999;WeedsandWeir2005).Wefollowed closely the same experimental scheme, as described subsequently." ></td>
	<td class="line x" title="324:376	First,alistofpseudo-wordsisconstructedbymergingpairsofwordsintoasingle pseudo word." ></td>
	<td class="line x" title="325:376	In our experiment each pseudo-word constitutes a pair of randomly chosen verbs, (v,v prime ), where each verb represents an alternative sense of the pseudoword." ></td>
	<td class="line x" title="326:376	The two verbs are chosen to have almost identical probability of occurrence, which avoids a word frequency bias on the co-occurrence likelihood predictions." ></td>
	<td class="line x" title="327:376	Next, we consider occurrences of pairs of the form n, (v, v) , where (v,v prime )isa pseudo-word and n is a noun representing the object of the pseudo-word." ></td>
	<td class="line x" title="328:376	Such pairs are constructed from all co-occurrences of either v or v prime with the object n in the corpus." ></td>
	<td class="line x" title="329:376	For example, given the pseudo-word (rescue, keep) and the verbobject co-occurrence in the corpus rescueprivate we construct the pair private, (rescue, keep)." ></td>
	<td class="line x" title="330:376	Given such a test 456 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality pair, the disambiguation task is to decide which of the two verbs is more likely to cooccur with the given object noun, aiming to recover the original verb from which this pairwasconstructed.Inthisexamplewewouldliketopredictthat rescue ismorelikely toco-occur with private as an object than keep." ></td>
	<td class="line x" title="331:376	In our experiment 80% of the constructed pairs were used for training, providing the co-occurrence statistics for the original known verb in each pair (i.e., either n, v or n, v)." ></td>
	<td class="line x" title="332:376	From the remaining 20% of the pairs those occurring in the training corpus werediscarded,leavingasatestsetonlypairswhichdonotappearinthetrainingpart." ></td>
	<td class="line x" title="333:376	Thus, predicting the co-occurrence likelihood of the noun with each of the two verbs cannot rely on direct frequency estimation for the co-occurrences, but rather only on similarity-based information." ></td>
	<td class="line x" title="334:376	To make the similarity-based predictions we rst compute the distributional similarity scores for all pairs of nouns based on the training set statistics, where the cooccurring verbs serve as the features in the distributional vectors of the nouns." ></td>
	<td class="line x" title="335:376	Then, given a test pair (v,v), n our task is to predict which of the two verbs is more likely to co-occur with n. This verb is thus predicted as being the original verb from which the pair was constructed." ></td>
	<td class="line x" title="336:376	To this end, the noun n is substituted in turn with each of its k distributionally most similar nouns, n i , and then both of the obtained similar pairs n i ,v and n i ,v prime  are sought in the training set." ></td>
	<td class="line x" title="337:376	Next,wewouldliketopredictthatthemorelikelyco-occurrencebetweenn, vand n, v is the one for which more pairs of similar words were found in the training set." ></td>
	<td class="line x" title="338:376	Several approaches were used in the literature to quantify this decision procedure and we have followed the most recent one from Weeds and Weir (2005)." ></td>
	<td class="line x" title="339:376	Each similar noun n i is given a vote, which is equal to the difference between the frequencies of the two co-occurrences (n i ,v)and(n i ,v prime ), and which it casts to the verb with which it co-occurs more frequently." ></td>
	<td class="line x" title="340:376	The votes for each of the two verbs are summed over all k similar nouns n i and the one with most votes wins." ></td>
	<td class="line x" title="341:376	The winning verb is considered correct if it is indeed the original verb from which the pair was constructed, and a tie is recorded if the votes for both verbs are equal." ></td>
	<td class="line x" title="342:376	Finally, the overall performance of the prediction method is calculated byits error rate: error = 1 T (#of incorrect choices+ #of ties 2 )(9) where T is the number of test instances." ></td>
	<td class="line x" title="343:376	In the experiment, we used the 1,000 most frequent nouns in our subset of the Reuters corpus (of Section 5.1)." ></td>
	<td class="line x" title="344:376	The training and test data were created as described herein, using the Minipar parser (Lin 1993) to produce verbobject co-occurrence pairs." ></td>
	<td class="line x" title="345:376	The k=40most similar nouns for each test noun were computed by each of the three examined similarity measures LIN, WJ,andCOS (as in Section 5), with and without bootstrapping." ></td>
	<td class="line x" title="346:376	The six similarity lists were utilized in turn for the pseudo-word sense disambiguation task, calculating the corresponding error rate." ></td>
	<td class="line x" title="347:376	7.3 Results Table11shows theerrorrateimprovements afterapplyingthebootstrapped weighting for each of the three similarity measures." ></td>
	<td class="line x" title="348:376	The largest error reduction, by over 15%, was obtainedforthe LIN method,withquitesimilarresultsfor WJ.Thisresultisbetterthan the one reported by Weeds and Weir (2005), who achieved about 6% error reduction compared to LIN." ></td>
	<td class="line x" title="349:376	457 ComputationalLinguistics Volume35,Number3 Table 11 Thecomparativeerrorratesof thepseudo-disambiguationtask forthethreeexaminedsimilarity measures, withand withoutapplyingthebootstrappedweightingforeach of them." ></td>
	<td class="line x" title="350:376	Measure LINLIN B WJWJ B COSCOS B Errorrate 0.1570.133 0.1500.132 0.1550.145 This experiment shows that learning tighter semantic similarities, based on the improved bootstrapped feature vectors, correlates also with better similarity-based inferenceforco-occurrencelikelihoodprediction.Furthermore,wehaveseenonceagainthat the bootstrapping scheme does not depend on a specic similarity measure, reducing the error rates for all three measures." ></td>
	<td class="line x" title="351:376	8." ></td>
	<td class="line x" title="352:376	Conclusions The primary contribution of this article is the proposal of a bootstrapping method that substantially improves the quality of distributional feature vectors, as needed for statistical word similarity." ></td>
	<td class="line x" title="353:376	The main idea is that features which are common for similar words are also most characteristic for their meanings and thus should be promoted." ></td>
	<td class="line x" title="354:376	In fact, beyond its intuitive appeal, this idea corresponds to the underlying rationale of the distributional similarity scheme: Semantically similar words are expected to share exactly those context features which are most characteristic for their meaning." ></td>
	<td class="line x" title="355:376	The superior empirical performance of the resulting vectors was assessed in the context of the two primary applications of distributional word similarity." ></td>
	<td class="line x" title="356:376	The rst is lexical substitution, which was represented in our work by a human gold standard for the substitutable lexical entailment relation." ></td>
	<td class="line x" title="357:376	The second is co-occurrence likelihood prediction, which was assessed by the automatically computed scores of the common pseudo-wordsensedisambiguationevaluation.Anadditionaloutcomeoftheimproved feature weighting is massive feature reduction." ></td>
	<td class="line x" title="358:376	Experimenting with three prominent similarity measures showed that the bootstrapping scheme is robust and performs well when applied over different measures." ></td>
	<td class="line x" title="359:376	Notably, our experiments show that the underlying assumption behind the bootstrapping scheme is valid, that is, available similarity metrics do provide a reasonable approximation of the semantic similarity space which can be then exploited via bootstrapping." ></td>
	<td class="line x" title="360:376	Themethodologyofourinvestigationhasyieldedseveraladditionalcontributions: 1." ></td>
	<td class="line x" title="361:376	Utilizing arened denition ofsubstitutable lexical entailment both as an end goal and as an analysis vehicle for distributional similarity." ></td>
	<td class="line x" title="362:376	It was shownthatthereneddenitioncanbejudgeddirectlybyhumansubjects with very good agreement." ></td>
	<td class="line x" title="363:376	Overall, lexical entailment is suggested as a useful model for lexical substitution needs in semantic-oriented applications." ></td>
	<td class="line x" title="364:376	2." ></td>
	<td class="line x" title="365:376	A thorough error analysis of state of the art distributional similarity performance was conducted." ></td>
	<td class="line x" title="366:376	The main observation was decient quality of the feature vectors, which reduces the eventual quality ofsimilarity measures." ></td>
	<td class="line x" title="367:376	458 Zhitomirsky-GeffetandDagan BootstrappingDistributionalFeatureVector Quality 3." ></td>
	<td class="line x" title="368:376	Inspired by the qualitative analysis, we proposed anew analytic measure for feature vector quality, namely average common-feature rank ratio (acfr-ratio),which is based on the common ranks of the features for pairs of words.Thismeasureestimatestheabilityofafeatureweightingmethodto distinguish between pairs of similar vs. non-similar words." ></td>
	<td class="line x" title="369:376	To the best of our knowledge this is the rstproposed measure for direct analysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure." ></td>
	<td class="line x" title="370:376	Theabilitytoidentifythemostcharacteristicfeaturesofwordscanhaveadditionalbenets, beyond their impact on traditional word similarity measures (as evaluated in this article)." ></td>
	<td class="line x" title="371:376	A demonstration of such potential appears in Geffet and Dagan (2005), which presents a novel feature inclusion scheme for vector comparison." ></td>
	<td class="line x" title="372:376	That scheme utilizes our bootstrapping method to identify the most characteristic features of a word and then tests whether these particular features co-occur also with a hypothesized entailed word.Theempiricalsuccessreportedinthatpaperprovidesadditionalevidenceforthe utilityof the bootstrapping method." ></td>
	<td class="line x" title="373:376	More generally, our motivation and methodology can be extended in several directions by future work on acquiring lexical entailment or other lexical-semantic relations." ></td>
	<td class="line x" title="374:376	One direction is to explore better vector comparison methods that will utilize the improved feature weighting, as shown in Geffet and Dagan (2005)." ></td>
	<td class="line x" title="375:376	Another direction is to integrate distributional similarity and pattern-based acquisition approaches, whichwereshowntoprovidelargelycomplementaryinformation(Mirkin,Dagan,and Geffet2006).Anadditionalpotentialistointegrateautomaticallyacquiredrelationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the outputofautomaticacquisitionmethods.Asaparalleldirection,futureresearchshould explore in detail the impact of different lexical-semantic acquisition methods on text understanding applications." ></td>
	<td class="line x" title="376:376	Finally, our proposed bootstrapping scheme seems to have a general appeal for improvingfeaturevectorqualityinadditionalunsupervisedsettings.Wethushopethat this idea will be explored further in other NLP and machine learning contexts." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1032
Domain Adaptation with Latent Semantic Association for Named Entity Recognition
Guo, Hong Lei;Zhu, Huijia;Guo, Zhili;Zhang, Xiaoxun;Wu, Xian;Su, Zhong;"></td>
	<td class="line x" title="1:250	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 281289, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:250	c 2009 Association for Computational Linguistics Domain Adaptation with Latent Semantic Association for Named Entity Recognition Honglei Guo Huijia Zhu Zhili Guo Xiaoxun Zhang Xian Wu and Zhong Su IBM China Research Laboratory Beijing, P. R. China {guohl, zhuhuiji, guozhili, zhangxx, wuxian, suzhong}@cn.ibm.com Abstract Domain adaptation is an important problem in named entity recognition (NER)." ></td>
	<td class="line x" title="3:250	NER classifiers usually lose accuracy in the domain transfer due to the different data distribution between the source and the target domains." ></td>
	<td class="line x" title="4:250	The major reason for performance degrading is that each entity type often has lots of domainspecific term representations in the different domains." ></td>
	<td class="line x" title="5:250	The existing approaches usually need an amount of labeled target domain data for tuning the original model." ></td>
	<td class="line x" title="6:250	However, it is a labor-intensive and time-consuming task to build annotated training data set for every target domain." ></td>
	<td class="line x" title="7:250	We present a domain adaptation method with latent semantic association (LaSA)." ></td>
	<td class="line x" title="8:250	This method effectively overcomes the data distribution difference without leveraging any labeled target domain data." ></td>
	<td class="line x" title="9:250	LaSA model is constructed to capture latent semantic association among words from the unlabeled corpus." ></td>
	<td class="line x" title="10:250	It groups words into a set of concepts according to the related context snippets." ></td>
	<td class="line x" title="11:250	In the domain transfer, the original term spaces of both domains are projected to a concept space using LaSA model at first, then the original NER model is tuned based on the semantic association features." ></td>
	<td class="line x" title="12:250	Experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly enhances the performance of NER." ></td>
	<td class="line x" title="13:250	1 Introduction Named entities (NE) are phrases that contain names of persons, organizations, locations, etc. NER is an important task in information extraction and natural language processing (NLP) applications." ></td>
	<td class="line x" title="14:250	Supervised learning methods can effectively solve NER problem by learning a model from manually labeled data (Borthwick, 1999; Sang and Meulder, 2003; Gao et al., 2005; Florian et al., 2003)." ></td>
	<td class="line x" title="15:250	However, empirical study shows that NE types have different distribution across domains (Guo et al., 2006)." ></td>
	<td class="line x" title="16:250	Trained NER classifiers in the source domain usually lose accuracy in a new target domain when the data distribution is different between both domains." ></td>
	<td class="line x" title="17:250	Domain adaptation is a challenge for NER and other NLP applications." ></td>
	<td class="line x" title="18:250	In the domain transfer, the reason for accuracy loss is that each NE type often has various specific term representations and context clues in the different domains." ></td>
	<td class="line x" title="19:250	For example, {economist, singer, dancer, athlete, player, philosopher, } are used as context clues for NER." ></td>
	<td class="line x" title="20:250	However, the distribution of these representations are varied with domains." ></td>
	<td class="line x" title="21:250	We expect to do better domain adaptation for NER by exploiting latent semantic association among words from different domains." ></td>
	<td class="line x" title="22:250	Some approaches have been proposed to group words into topics to capture important relationships between words, such as Latent Semantic Indexing (LSI) (Deerwester et al., 1990), probabilistic Latent Semantic Indexing (pLSI) (Hofmann, 1999), Latent Dirichlet Allocation (LDA) (Blei et al., 2003)." ></td>
	<td class="line x" title="23:250	These models have been successfully employed in topic modeling, dimensionality reduction for text categorization (Blei et al., 2003), ad hoc IR (Wei and Croft., 2006), and so on." ></td>
	<td class="line x" title="24:250	In this paper, we present a domain adaptation method with latent semantic association." ></td>
	<td class="line x" title="25:250	We focus 281 on capturing the hidden semantic association among words in the domain adaptation." ></td>
	<td class="line x" title="26:250	We introduce the LaSA model to overcome the distribution difference between the source domain and the target domain." ></td>
	<td class="line x" title="27:250	LaSA model is constructed from the unlabeled corpus at first." ></td>
	<td class="line x" title="28:250	It learns latent semantic association among words from their related context snippets." ></td>
	<td class="line x" title="29:250	In the domain transfer, words in the corpus are associated with a low-dimension concept space using LaSA model, then the original NER model is tuned using these generated semantic association features." ></td>
	<td class="line x" title="30:250	The intuition behind our method is that words in one concept set will have similar semantic features or latent semantic association, and share syntactic and semantic context in the corpus." ></td>
	<td class="line x" title="31:250	They can be considered as behaving in the same way for discriminative learning in the source and target domains." ></td>
	<td class="line x" title="32:250	The proposed method associates words from different domains on a semantic level rather than by lexical occurrence." ></td>
	<td class="line x" title="33:250	It can better bridge the domain distribution gap without any labeled target domain samples." ></td>
	<td class="line x" title="34:250	Experimental results on English and Chinese corpus show that LaSA-based adaptation significantly enhances NER performance across domains." ></td>
	<td class="line x" title="35:250	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="36:250	Section 2 briefly describes the related works." ></td>
	<td class="line x" title="37:250	Section 3 presents a domain adaptation method based on latent semantic association." ></td>
	<td class="line x" title="38:250	Section 4 illustrates how to learn LaSA model from the unlabeled corpus." ></td>
	<td class="line x" title="39:250	Section 5 shows experimental results on large-scale English and Chinese corpus across domains, respectively." ></td>
	<td class="line x" title="40:250	The conclusion is given in Section 6." ></td>
	<td class="line x" title="41:250	2 Related Works Some domain adaptation techniques have been employed in NLP in recent years." ></td>
	<td class="line x" title="42:250	Some of them focus on quantifying the generalizability of certain features across domains." ></td>
	<td class="line x" title="43:250	Roark and Bacchiani (2003) use maximum a posteriori (MAP) estimation to combine training data from the source and target domains." ></td>
	<td class="line x" title="44:250	Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data." ></td>
	<td class="line x" title="45:250	Daume III and Marcu (2006) use an empirical Bayes model to estimate a latent variable model grouping instances into domain-specific or common across both domains." ></td>
	<td class="line x" title="46:250	Daume III (2007) further augments the feature space on the instances of both domains." ></td>
	<td class="line x" title="47:250	Jiang and Zhai (2006) exploit the domain structure contained in the training examples to avoid over-fitting the training domains." ></td>
	<td class="line x" title="48:250	Arnold et al.(2008) exploit feature hierarchy for transfer learning in NER." ></td>
	<td class="line x" title="50:250	Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation." ></td>
	<td class="line x" title="51:250	Most of these approaches need the labeled target domain samples for the model estimation in the domain transfer." ></td>
	<td class="line x" title="52:250	Obviously, they require much efforts for labeling the target domain samples." ></td>
	<td class="line x" title="53:250	Some approaches exploit the common structure of related problems." ></td>
	<td class="line x" title="54:250	Ando et al.(2005) learn predicative structures from multiple tasks and unlabeled data." ></td>
	<td class="line x" title="56:250	Blitzer et al.(2006, 2007) employ structural corresponding learning (SCL) to infer a good feature representation from unlabeled source and target data sets in the domain transfer." ></td>
	<td class="line x" title="58:250	We present LaSA model to overcome the data gap across domains by capturing latent semantic association among words from unlabeled source and target data." ></td>
	<td class="line x" title="59:250	In addition, Miller et al.(2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain." ></td>
	<td class="line x" title="61:250	Li and McCallum (2005) present a semi-supervised sequence modeling with syntactic topic models." ></td>
	<td class="line x" title="62:250	In this paper, we focus on capturing hidden semantic association among words in the domain adaptation." ></td>
	<td class="line x" title="63:250	3 Domain Adaptation Based on Latent Semantic Association The challenge in domain adaptation is how to capture latent semantic association from the source and target domain data." ></td>
	<td class="line x" title="64:250	We present a LaSA-based domain adaptation method in this section." ></td>
	<td class="line x" title="65:250	NER can be considered as a classification problem." ></td>
	<td class="line x" title="66:250	Let X be a feature space to represent the observed word instances, and let Y be the set of class labels." ></td>
	<td class="line x" title="67:250	Let ps(x,y) and pt(x,y) be the true underlying distributions for the source and the target domains, respectively." ></td>
	<td class="line x" title="68:250	In order to minimize the efforts required in the domain transfer, we often expect to use ps(x,y) to approximate pt(x,y)." ></td>
	<td class="line x" title="69:250	However, data distribution are often varied with the domains." ></td>
	<td class="line x" title="70:250	For example, in the economics-to282 entertainment domain transfer, although many NE triggers (e.g. company and Mr.) are used in both domains, some are totally new, like dancer, singer." ></td>
	<td class="line x" title="71:250	Moreover, many useful words (e.g. economist) in the economics NER are useless in the entertainment domain." ></td>
	<td class="line x" title="72:250	The above examples show that features could change behavior across domains." ></td>
	<td class="line x" title="73:250	Some useful predictive features from one domain are not predictive or do not appear in another domain." ></td>
	<td class="line x" title="74:250	Although some triggers (e.g. singer, economist) are completely distinct for each domain, they often appear in the similar syntactic and semantic context." ></td>
	<td class="line x" title="75:250	For example, triggers of person entity often appear as the subject of visited, said, etc, or are modified by excellent, popular, famous etc. Such latent semantic association among words provides useful hints for overcoming the data distribution gap of both domains." ></td>
	<td class="line x" title="76:250	Hence, we present a LaSA model s,t to capture latent semantic association among words in the domain adaptation." ></td>
	<td class="line x" title="77:250	s,t is learned from the unlabeled source and target domain data." ></td>
	<td class="line x" title="78:250	Each instance is characterized by its co-occurred context distribution in the learning." ></td>
	<td class="line x" title="79:250	Semantic association feature in s,t is a hidden random variable that is inferred from data." ></td>
	<td class="line x" title="80:250	In the domain adaptation, we transfer the problem of semantic association mapping to a posterior inference task using LaSA model." ></td>
	<td class="line x" title="81:250	Latent semantic concept association set of a word instance x (denoted by SA(x)) is generated by s,t. Instances in the same concept set are considered as behaving in the same way for discriminative learning in both domains." ></td>
	<td class="line x" title="82:250	Even though word instances do not appear in a training corpus (or appear rarely) but are in similar context, they still might have relatively high probability in the same semantic concept set." ></td>
	<td class="line x" title="83:250	Obviously, SA(x) can better bridge the gap between the two distributions ps(y|x) and pt(y|x)." ></td>
	<td class="line x" title="84:250	Hence, LaSA model can enhance the estimate of the source domain distribution ps(y|x;s,t) to better approximate the target domain distribution pt(y|x;s,t)." ></td>
	<td class="line x" title="85:250	4 Learning LaSA Model from Virtual Context Documents In the domain adaptation, LaSA model is employed to find the latent semantic association structures of words in a text corpus." ></td>
	<td class="line x" title="86:250	We will illustrate how to build LaSA model from words and their context snippets in this section." ></td>
	<td class="line x" title="87:250	LaSA model actually can be considered as a general probabilistic topic model." ></td>
	<td class="line x" title="88:250	It can be learned on the unlabeled corpus using the popular hidden topic models such as LDA or pLSI." ></td>
	<td class="line x" title="89:250	4.1 Virtual Context Document The distribution of content words (e.g. nouns, adjectives) is usually varied with domains." ></td>
	<td class="line x" title="90:250	Hence, in the domain adaptation, we focus on capturing the latent semantic association among content words." ></td>
	<td class="line x" title="91:250	In order to learn latent relationships among words from the unlabeled corpus, each content word is characterized by a virtual context document as follows." ></td>
	<td class="line x" title="92:250	Given a content word xi, the virtual context document of xi (denoted by vdxi) consists of all the context units around xi in the corpus." ></td>
	<td class="line x" title="93:250	Let n be the total number of the sentences which contain xi in the corpus." ></td>
	<td class="line x" title="94:250	vdxi is constructed as follows." ></td>
	<td class="line x" title="95:250	vdxi = {F(xs1i ),,F(xski ),,F(xsni )} where, F(xski ) denotes the context feature set of xi in the sentence sk, 1  k  n. Given the context window size {-t, t} (i.e. previous t words and next t words around xi in sk)." ></td>
	<td class="line x" title="96:250	F(xski ) usually consists of the following features." ></td>
	<td class="line x" title="97:250	1." ></td>
	<td class="line x" title="98:250	Anchor unit AxiC : the current focused word unit xi." ></td>
	<td class="line x" title="99:250	2." ></td>
	<td class="line x" title="100:250	Left adjacent unit AxiL : The nearest left adjacent unit xi1 around xi, denoted by AL(xi1)." ></td>
	<td class="line x" title="101:250	3." ></td>
	<td class="line x" title="102:250	Right adjacent unit AxiR : The nearest right adjacent unit xi+1 around xi, denoted by AR(xi+1)." ></td>
	<td class="line x" title="103:250	4." ></td>
	<td class="line x" title="104:250	Left context set CxiL : the other left adjacent units {xit, , xij, , xi2} (2 j t) around xi, denoted by {CL(xit), , CL(xij), , CL(xi2)}." ></td>
	<td class="line x" title="105:250	5." ></td>
	<td class="line x" title="106:250	Right context set CxiR : the other right adjacent units {xi+2, , xi+j, , xi+t} (2 j  t ) around xi, denoted by {CR(xi+2), , CR(xi+j), , CR(xi+t)}." ></td>
	<td class="line x" title="107:250	For example, given xi=singer, sk=This popular new singer attended the new year party." ></td>
	<td class="line x" title="108:250	Let the context window size be {-3,3}." ></td>
	<td class="line x" title="109:250	F(singer) = {singer, AL(new), AR(attend(ed)), CL(this), CL(popular), CR(the), CR(new) }." ></td>
	<td class="line x" title="110:250	vdxi actually describes the semantic and syntactic feature distribution of xi in the domains." ></td>
	<td class="line x" title="111:250	We construct the feature vector of xi with all the observed context features in vdxi." ></td>
	<td class="line x" title="112:250	Given vdxi = 283 {f1,,fj,,fm}, fj denotes jth context feature around xi, 1  j  m, m denotes the total number of features in vdxi." ></td>
	<td class="line oc" title="113:250	The value of fj is calculated by Mutual Information (Church and Hanks, 1990) between xi and fj." ></td>
	<td class="line x" title="114:250	Weight(fj,xi) = log2 P(fj,xi)P(f j)P(xi) (1) where, P(fj,xi) is the joint probability of xi and fj co-occurred in the corpus, P(fj) is the probability of fj occurred in the corpus." ></td>
	<td class="line x" title="115:250	P(xi) is the probability of xi occurred in the corpus." ></td>
	<td class="line x" title="116:250	4.2 Learning LaSA Model Topic models are statistical models of text that posit a hidden space of topics in which the corpus is embedded (Blei et al., 2003)." ></td>
	<td class="line x" title="117:250	LDA (Blei et al., 2003) is a probabilistic model that can be used to model and discover underlying topic structures of documents." ></td>
	<td class="line x" title="118:250	LDA assumes that there are K topics, multinomial distributions over words, which describes a collection." ></td>
	<td class="line x" title="119:250	Each document exhibits multiple topics, and each word in each document is associated with one of them." ></td>
	<td class="line x" title="120:250	LDA imposes a Dirichlet distribution on the topic mixture weights corresponding to the documents in the corpus." ></td>
	<td class="line x" title="121:250	The topics derived by LDA seem to possess semantic coherence." ></td>
	<td class="line x" title="122:250	Those words with similar semantics are likely to occur in the same topic." ></td>
	<td class="line x" title="123:250	Since the number of LDA model parameters depends only on the number of topic mixtures and vocabulary size, LDA is less prone to over-fitting and is capable of estimating the probability of unobserved test documents." ></td>
	<td class="line x" title="124:250	LDA is already successfully applied to enhance document representations in text classification (Blei et al., 2003), information retrieval (Wei and Croft., 2006)." ></td>
	<td class="line x" title="125:250	In the following, we illustrate how to construct LDA-style LaSA model s,t on the virtual context documents." ></td>
	<td class="line x" title="126:250	Algorithm 1 describes LaSA model training method in detail, where, Function AddTo(data,Set) denotes that data is added to Set." ></td>
	<td class="line x" title="127:250	Given a large-scale unlabeled data set Du which consists of the source and target domain data, virtual context document for each candidate content word is extracted from Du at first, then the value of each feature in a virtual context document is calculated using its Mutual Information ( see Equation 1 in Section 4.1) instead of the counts when running Algorithm 1: LaSA Model Training Inputs:1  Unlabeled data set: Du;2 Outputs:3 LaSA model: s,t;4 Initialization:5  Virtual context document set: VDs,t = ;6  Candidate content word set: Xs,t = ;7 Steps:8 begin9 foreach content word xi Du do10 if Frequency(xi) the predefined threshold then11 AddTo(xi,Xs,t);12 foreach xk Xs,t do13 foreach sentence Si Du do14 if xk Si then15 F(xSik ) 16 {xk,AxkL ,AxkR ,CxkL ,CxkR }; AddTo(F(xSik ),vdxk); AddTo(vdxk,VDs,t);17  Generate LaSA model s,t with Dirichlet distribution on VDs,t.18 end19 LDA." ></td>
	<td class="line x" title="128:250	LaSA model s,t with Dirichlet distribution is generated on the virtual context document set VDs,t using the algorithm presented by Blei et al (2003)." ></td>
	<td class="line x" title="129:250	1 2 3 4 5 customer theater company Beijing music president showplace government Hongkong film singer courtyard university China arts manager center community Japan concert economist city team Singapore party policeman gymnasium enterprise New York Ballet reporter airport bank Vienna dance director square market America song consumer park organization Korea band dancer building agency international opera Table 1: Top 10 nouns from 5 randomly selected topics computed on the economics and entertainment domains LaSA model learns the posterior distribution to decompose words and their corresponding virtual context documents into topics." ></td>
	<td class="line x" title="130:250	Table 1 lists top 10 nouns from a random selection of 5 topics computed on the unlabeled economics and entertainment domain data." ></td>
	<td class="line x" title="131:250	As shown, words in the same topic are representative nouns." ></td>
	<td class="line x" title="132:250	They actually are grouped into broad concept sets." ></td>
	<td class="line x" title="133:250	For example, set 1, 3 and 4 correspond to nominal person, nominal organization and location, respectively." ></td>
	<td class="line x" title="134:250	With a large-scale unlabeled corpus, we will have enough words assigned to each topic concept to better approximate the underlying semantic association distribution." ></td>
	<td class="line x" title="135:250	In LDA-style LaSA model, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all the virtual context docu284 ments." ></td>
	<td class="line x" title="136:250	Hence, given a word xi in the corpus, we may perform posterior inference to determine the conditional distribution of the hidden topic feature variables associated with xi." ></td>
	<td class="line x" title="137:250	Latent semantic association set of xi (denoted by SA(xi)) is generated using Algorithm 2." ></td>
	<td class="line x" title="138:250	Here, Multinomial(s,t(vdxi)) refers to sample from the posterior distribution over topics given a virtual document vdxi." ></td>
	<td class="line x" title="139:250	In the domain adaptation, we do semantic association inference on the source domain training data using LaSA model at first, then the original source domain NER model is tuned on the source domain training data set by incorporating these generated semantic association features." ></td>
	<td class="line x" title="140:250	Algorithm 2: Generate Latent Semantic Association Set of Word xi Using K-topic LaSA Model Inputs:1 s,t: LaSA model with multinomial distribution;2 Dirichlet(): Dirichlet distribution with parameter ;3 xi: Content word;4 Outputs:5 SA(xi): Latent semantic association set of xi;6 Steps:7 begin8  Extract vdxi from the corpus.9  Draw topic weights s,t(vdxi) from Dirichlet();10  foreach fj in vdxi do11 draw a topic zj{ 1,,K} from Multinomial(s,t(vdxi));12 AddTo(zj,Topics(vdxi));13  Rank all the topics in Topics(vdxi);14 SA(xi) top n topics in Topics(vdxi);15 end16 LaSA model better models latent semantic association distribution in the source and the target domains." ></td>
	<td class="line x" title="141:250	By grouping words into concepts, we effectively overcome the data distribution difference of both domains." ></td>
	<td class="line x" title="142:250	Thus, we may reduce the number of parameters required to model the target domain data, and improve the quality of the estimated parameters in the domain transfer." ></td>
	<td class="line x" title="143:250	LaSA model extends the traditional bag-of-words topic models to context-dependence concept association model." ></td>
	<td class="line x" title="144:250	It has potential use for concept grouping." ></td>
	<td class="line x" title="145:250	5 Experiments We evaluate LaSA-based domain adaptation method on both English and Chinese corpus in this section." ></td>
	<td class="line x" title="146:250	In the experiments, we focus on recognizing person (PER), location (LOC) and organization (ORG) in the given four domains, including economics (Eco), entertainment (Ent), politics (Pol) and sports (Spo)." ></td>
	<td class="line x" title="147:250	5.1 Experimental setting In the NER domain adaptation, nouns and adjectives make a significant impact on the performance." ></td>
	<td class="line x" title="148:250	Thus, we focus on capturing latent semantic association for high-frequency nouns and adjectives (i.e. occurrence count  50 ) in the unlabeled corpus." ></td>
	<td class="line x" title="149:250	LaSA models for nouns and adjectives are learned from the unlabeled corpus using Algorithm 1 (see section 4.2), respectively." ></td>
	<td class="line x" title="150:250	Our empirical study shows that better adaptation is obtained with a 50-topic LaSA model." ></td>
	<td class="line x" title="151:250	Therefore, we set the number of topics N as 50, and define the context view window size as {3,3} (i.e. previous 3 words and next 3 words) in the LaSA model learning." ></td>
	<td class="line x" title="152:250	LaSA features for other irrespective words (e.g. token unit the) are assigned with a default topic value N+1." ></td>
	<td class="line x" title="153:250	All the basic NER models are trained on the domain-specific training data using RRM classifier (Guo et al., 2005)." ></td>
	<td class="line x" title="154:250	RRM is a generalization Winnow learning algorithm (Zhang et al., 2002)." ></td>
	<td class="line x" title="155:250	We set the context view window size as {-2,2} in NER." ></td>
	<td class="line x" title="156:250	Given a word instance x, we employ local linguistic features (e.g. word unit, part of speech) of x and its context units ( i.e. previous 2 words and next 2 words ) in NER." ></td>
	<td class="line x" title="157:250	All Chinese texts in the experiments are automatically segmented into words using HMM." ></td>
	<td class="line x" title="158:250	In LaSA-based domain adaptation, the semantic association features of each unit in the observation window{-2,2}are generated by LaSA model at first, then the basic source domain NER model is tuned on the original source domain training data set by incorporating the semantic association features." ></td>
	<td class="line x" title="159:250	For example, given the sentence This popular new singer attended the new year party, Figure 1 illustrates various features and views at the current word wi= singer in LaSA-based adaptation." ></td>
	<td class="line x" title="160:250	 Tagging  Position wi2 wi1 wi wi+1 wi+2 Word popular new singer attend the POS adj adj noun verb article SA SA(popular) SA(new) SA(singer) SA(attend) SA(the)  Tag ti2 ti1 ti Figure 1: Feature window in LaSA-based adaptation In the viewing window at the word singer (see Figure 1), each word unit around singer is codified with a set of primitive features (e.g. POS, SA, Tag), together with its relative position to singer." ></td>
	<td class="line x" title="161:250	285 Here, SA denotes semantic association feature set which is generated by LaSA model." ></td>
	<td class="line x" title="162:250	Tag denotes NE tags labeled in the data set." ></td>
	<td class="line x" title="163:250	Given the input vector constructed with the above features, RRM method is then applied to train linear weight vectors, one for each possible class-label." ></td>
	<td class="line x" title="164:250	In the decoding stage, the class with the maximum confidence is then selected for each token unit." ></td>
	<td class="line x" title="165:250	In our evaluation, only NEs with correct boundaries and correct class labels are considered as the correct recognition." ></td>
	<td class="line x" title="166:250	We use the standard Precision (P), Recall (R), and F-measure (F = 2PRP+R) to measure the performance of NER models." ></td>
	<td class="line x" title="167:250	5.2 Data We built large-scale English and Chinese annotated corpus." ></td>
	<td class="line x" title="168:250	English corpus are generated from wikipedia while Chinese corpus are selected from Chinese newspapers." ></td>
	<td class="line x" title="169:250	Moreover, test data do not overlap with training data and unlabeled data." ></td>
	<td class="line x" title="170:250	5.2.1 Generate English Annotated Corpus from Wikipedia Wikipedia provides a variety of data resources for NER and other NLP research (Richman and Schone, 2008)." ></td>
	<td class="line x" title="171:250	We generate all the annotated English corpus from wikipedia." ></td>
	<td class="line x" title="172:250	With the limitation of efforts, only PER NEs in the corpus are automatically tagged using an English person gazetteer." ></td>
	<td class="line x" title="173:250	We automatically extract an English Person gazetteer from wikipedia at first." ></td>
	<td class="line x" title="174:250	Then we select the articles from wikipedia and tag them using this gazetteer." ></td>
	<td class="line x" title="175:250	In order to build the English Person gazetteer from wikipdedia, we manually selected several key phrases, including births, deaths, surname, given names and human names at first." ></td>
	<td class="line x" title="176:250	For each article title of interest, we extracted the categories to which that entry was assigned." ></td>
	<td class="line x" title="177:250	The entry is considered as a person name if its related explicit category links contain any one of the key phrases, such as Category: human names." ></td>
	<td class="line x" title="178:250	We totally extracted 25,219 person name candidates from 204,882 wikipedia articles." ></td>
	<td class="line x" title="179:250	And we expanded this gazetteer by adding the other available common person names." ></td>
	<td class="line x" title="180:250	Finally, we obtained a large-scale gazetteer of 51,253 person names." ></td>
	<td class="line x" title="181:250	All the articles selected from wikipedia are further tagged using the above large-scale gazetteer." ></td>
	<td class="line x" title="182:250	Since human annotated set were not available, we held out more than 100,000 words of text from the automatically tagged corpus to as a test set in each domain." ></td>
	<td class="line x" title="183:250	Table 2 shows the data distribution of the training and test data sets." ></td>
	<td class="line x" title="184:250	Domains Training Data Set Test Data Set Size PERs Size PERs Pol 0.45M 9,383 0.23M 6,067 Eco 1.06M 21,023 0.34M 6,951 Spo 0.47M 17,727 0.20M 6,075 Ent 0.36M 12,821 0.15M 5,395 Table 2: English training and test data sets We also randomly select 17M unlabeled English data (see Table 3) from Wikipedia." ></td>
	<td class="line x" title="185:250	These unlabeled data are used to build the English LaSA model." ></td>
	<td class="line x" title="186:250	All Domain Pol Eco Spo Ent Data Size(M) 17.06 7.36 2.59 3.65 3.46 Table 3: Domain distribution in the unlabeled English data set 5.2.2 Chinese Data We built a large-scale high-quality Chinese NE annotated corpus." ></td>
	<td class="line x" title="187:250	All the data are news articles from several Chinese newspapers in 2001 and 2002." ></td>
	<td class="line x" title="188:250	All the NEs (i.e. PER, LOC and ORG ) in the corpus are manually tagged." ></td>
	<td class="line x" title="189:250	Cross-validation checking is employed to ensure the quality of the annotated corpus." ></td>
	<td class="line x" title="190:250	Domain Size NEs in the training data set (M) PER ORG LOC Total Pol 0.90 11,388 6,618 14,350 32,356 Eco 1.40 6,821 18,827 14,332 39,980 Spo 0.60 11,647 8,105 7,468 27,220 Ent 0.60 12,954 2,823 4,665 20,442 Domain Size NEs in the test data set (M) PER ORG LOC Total Pol 0.20 2,470 1,528 2,540 6,538 Eco 0.26 1,098 2,971 2,362 6,431 Spo 0.10 1,802 1,323 1,246 4,371 Ent 0.10 2,458 526 738 3,722 Table 4: Chinese training and test data sets All the domain-specific training and test data are selected from this annotated corpus according to the domain categories (see Table 4)." ></td>
	<td class="line x" title="191:250	8.46M unlabeled Chinese data (see Table 5) are randomly selected from this corpus to build the Chinese LaSA model." ></td>
	<td class="line x" title="192:250	5.3 Experimental Results All the experiments are conducted on the above large-scale English and Chinese corpus." ></td>
	<td class="line x" title="193:250	The overall performance enhancement of NER by LaSA-based 286 All Domain Pol Eco Spo Ent Data Size(M) 8.46 2.34 1.99 2.08 2.05 Table 5: Domain distribution in the unlabeled Chinese data set domain adaptation is evaluated at first." ></td>
	<td class="line x" title="194:250	Since the distribution of each NE type is different across domains, we also analyze the performance enhancement on each entity type by LaSA-based adaptation." ></td>
	<td class="line x" title="195:250	5.3.1 Performance Enhancement of NER by LaSA-based Domain Adaptation Table 6 and 7 show the experimental results for all pairs of domain adaptation on both English and Chinese corpus, respectively." ></td>
	<td class="line x" title="196:250	In the experiment, the basic source domain NER model Ms is learned from the specific domain training data set Ddom (see Table 2 and 4 in Section 5.2)." ></td>
	<td class="line x" title="197:250	Here, dom  {Eco,Ent,Pol,Spo}." ></td>
	<td class="line x" title="198:250	Findom denotes the top-line F-measure of Ms in the source trained domain dom." ></td>
	<td class="line x" title="199:250	When Ms is directly applied in a new target domain, its F-measure in this basic transfer is considered as baseline (denoted by FBase)." ></td>
	<td class="line x" title="200:250	FLaSA denotes F-measure of Ms achieved in the target domain with LaSA-based domain adaptation." ></td>
	<td class="line x" title="201:250	(F) = FLaSAFBase FBase , which denotes the relative F-measureenhancement by LaSA-based domain adaptation." ></td>
	<td class="line x" title="202:250	Source  Performance in the domain transfer Target FBase FLaSA (F) (loss) FTop EcoEnt 57.61% 59.22% +2.79% 17.87% FinEnt=66.62% PolEnt 57.5 % 59.83% +4.05% 25.55% FinEnt=66.62% SpoEnt 58.66% 62.46% +6.48% 47.74% FinEnt=66.62% EntEco 70.56 % 72.46% +2.69% 19.33% FinEco=80.39% PolEco 63.62% 68.1% +7.04% 26.71% FinEco=80.39% SpoEco 70.35% 72.85% +3.55% 24.90% FinEco=80.39% EcoPol 50.59% 52.7% +4.17% 15.81% FinPol=63.94% EntPol 56.12% 59.82% +6.59% 47.31% FinPol=63.94% SpoPol 60.22% 62.6% +3.95% 63.98% FinPol=63.94% EcoSpo 60.28% 61.21% +1.54% 9.93% FinSpo=69.65% EntSpo 60.28% 62.68% +3.98% 25.61% FinSpo=69.65% PolSpo 56.94% 60.48% +6.22% 27.85% FinSpo=69.65% Table 6: Experimental results on English corpus Experimental results on English and Chinese corpus indicate that the performance of Ms significantly degrades in each basic domain transfer without using LaSA model (see Table 6 and 7)." ></td>
	<td class="line x" title="203:250	For example, in the EcoEnt transfer on Chinese corpus (see Table 7), Fineco of Ms is 82.28% while FBase of Ms is 60.45% in the entertainment domain." ></td>
	<td class="line x" title="204:250	Fmeasure of Ms significantly degrades by 21.83 perSource  Performance in the domain transfer Target FBase FLaSA (F) (loss) FTop EcoEnt 60.45% 66.42% +9.88% 26.29% FinEnt=83.16% PolEnt 69.89% 73.07% +4.55% 23.96% FinEnt =83.16% SpoEnt 68.66% 70.89% +3.25% 15.38% FinEnt =83.16% EntEco 58.50% 61.35% + 4.87% 11.98% FinEco=82.28% PolEco 62.89% 64.93% +3.24% 10.52% FinEco=82.28% SpoEco 60.44% 63.20% + 4.57 % 12.64% FinEco=82.28% EcoPol 67.03% 70.90 % +5.77% 27.78% FinPol=80.96% EntPol 66.64 % 68.94 % +3.45% 16.06% FinPol=80.96% SpoPol 65.40% 67.20% +2.75% 11.57% FinPol=80.96% EcoSpo 67.20% 70.77% +5.31% 15.47% FinSpo=90.24% EntSpo 70.05% 72.20% +3.07% 10.64% FinSpo=90.24% PolSpo 70.99% 73.86% +4.04% 14.91% FinSpo=90.24% Table 7: Experimental results on Chinese corpus cent points in this basic transfer." ></td>
	<td class="line x" title="205:250	Significant performance degrading of Ms is observed in all the basic transfer." ></td>
	<td class="line x" title="206:250	It shows that the data distribution of both domains is very different in each possible transfer." ></td>
	<td class="line x" title="207:250	Experimental results on English corpus show that LaSA-based adaptation effectively enhances the performance in each domain transfer (see Table 6)." ></td>
	<td class="line x" title="208:250	For example, in the PolEco transfer, FBase is 63.62% while FLaSA achieves 68.10%." ></td>
	<td class="line x" title="209:250	Compared with FBase, LaSA-based method significantly enhances F-measure by 7.04%." ></td>
	<td class="line x" title="210:250	We perform t-tests on F-measure of all the comparison experiments on English corpus." ></td>
	<td class="line x" title="211:250	The p-value is 2.44E-06, which shows that the improvement is statistically significant." ></td>
	<td class="line x" title="212:250	Table 6 also gives the accuracy loss due to transfer in each domain adaptation on English corpus." ></td>
	<td class="line x" title="213:250	The accuracy loss is defined as loss = 1  FFin dom . And the relative reduction in error is defined as (loss)= |1  lossLaSAlossBase |." ></td>
	<td class="line x" title="214:250	Experimental results indicate that the relative reduction in error is above 9.93% with LaSA-based transfer in each test on English corpus." ></td>
	<td class="line x" title="215:250	LaSA model significantly decreases the accuracy loss by 29.38% in average." ></td>
	<td class="line x" title="216:250	Especially for SpoPol transfer, (loss) achieves 63.98% with LaSA-based adaptation." ></td>
	<td class="line x" title="217:250	All the above results show that LaSA-based adaptation significantly reduces the accuracy loss in the domain transfer for English NER without any labeled target domain samples." ></td>
	<td class="line x" title="218:250	Experimental results on Chinese corpus also show that LaSA-based adaptation effectively increases the accuracy in all the tests (see Table 7)." ></td>
	<td class="line x" title="219:250	For example, in the EcoEnt transfer, compared with FBase, LaSA-based adaptation significantly increases Fmeasure by 9.88%." ></td>
	<td class="line x" title="220:250	We also perform t-tests on F287 measure of 12 comparison experiments on Chinese corpus." ></td>
	<td class="line x" title="221:250	The p-value is 1.99E-06, which shows that the enhancement is statistically significant." ></td>
	<td class="line x" title="222:250	Moreover, the relative reduction in error is above 10% with LaSA-based method in each test." ></td>
	<td class="line x" title="223:250	LaSA model decreases the accuracy loss by 16.43% in average." ></td>
	<td class="line x" title="224:250	Especially for the EcoEnt transfer (see Table 7), (loss) achieves 26.29% with LaSA-based method." ></td>
	<td class="line x" title="225:250	All the above experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly decreases the accuracy loss in the transfer without any labeled target domain data." ></td>
	<td class="line x" title="226:250	Although automatically tagging introduced some errors in English source training data, the relative reduction in errors in English NER adaptation seems comparable to that one in Chinese NER adaptation." ></td>
	<td class="line x" title="227:250	5.3.2 Accuracy Enhancement for Each NE Type Recognition Our statistic data (Guo et al., 2006) show that the distribution of NE types varies with domains." ></td>
	<td class="line x" title="228:250	Each NE type has different domain features." ></td>
	<td class="line x" title="229:250	Thus, the performance stability of each NE type recognition is very important in the domain transfer." ></td>
	<td class="line x" title="230:250	Figure 2 gives F-measure of each NE type recognition achieved by LaSA-based adaptation on English and Chinese corpus." ></td>
	<td class="line x" title="231:250	Experimental results show that LaSA-based adaptation effectively increases the accuracy of each NE type recognition in the most of the domain transfer tests." ></td>
	<td class="line x" title="232:250	We perform t-tests on F-measure of the comparison experiments on each NE type, respectively." ></td>
	<td class="line x" title="233:250	All the p-value is less than 0.01, which shows that the improvement on each NE type recognition is statistically significant." ></td>
	<td class="line x" title="234:250	Especially, the p-value of English and Chinese PER is 2.44E-06 and 9.43E-05, respectively, which shows that the improvement on PER recognition is very significant." ></td>
	<td class="line x" title="235:250	For example, in the EcoPol transfer on Chinese corpus, compared with FBase, LaSA-based adaptation enhances F-measure of PER recognition by 9.53 percent points." ></td>
	<td class="line x" title="236:250	Performance enhancement for ORG recognition is less than that one for PER and LOC recognition using LaSA model since ORG NEs usually contain much more domainspecific information than PER and LOC." ></td>
	<td class="line x" title="237:250	The major reason for error reduction is that external context and internal units are better semantically associated using LaSA model." ></td>
	<td class="line x" title="238:250	For example, LaSA Figure 2: PER, LOC and ORG recognition in the transfer model better groups various titles from different domains (see Table 1 in Section 4.2)." ></td>
	<td class="line x" title="239:250	Various industry terms in ORG NEs are also grouped into the semantic sets." ></td>
	<td class="line x" title="240:250	These semantic associations provide useful hints for detecting the boundary of NEs in the new target domain." ></td>
	<td class="line x" title="241:250	All the above results show that LaSA model better compensates for the feature distribution difference of each NE type across domains." ></td>
	<td class="line x" title="242:250	6 Conclusion We present a domain adaptation method with LaSA model in this paper." ></td>
	<td class="line x" title="243:250	LaSA model captures latent semantic association among words from the unlabeled corpus." ></td>
	<td class="line x" title="244:250	It better groups words into a set of concepts according to the related context snippets." ></td>
	<td class="line x" title="245:250	LaSAbased domain adaptation method projects words to a low-dimension concept feature space in the transfer." ></td>
	<td class="line x" title="246:250	It effectively overcomes the data distribution gap across domains without using any labeled target domain data." ></td>
	<td class="line x" title="247:250	Experimental results on English and Chinese corpus show that LaSA-based domain adaptation significantly enhances the performance of NER across domains." ></td>
	<td class="line x" title="248:250	Especially, LaSA model effectively increases the accuracy of each NE type recognition in the domain transfer." ></td>
	<td class="line x" title="249:250	Moreover, LaSA-based domain adaptation method works well across languages." ></td>
	<td class="line x" title="250:250	To further reduce the accuracy loss, we will explore informative sampling to capture fine-grained data difference in the domain transfer." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1072
Quantitative modeling of the neural representation of adjective-noun phrases to account for fMRI activation
Chang, Kai-min K.;Cherkassky, Vladimir L.;Mitchell, Tom M.;Just, Marcel Adam;"></td>
	<td class="line x" title="1:222	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 638646, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:222	c2009 ACL and AFNLP Quantitative modeling of the neural representation of adjective-noun phrases to account for fMRI activation   Kai-min K. Chang 1    Vladimir L. Cherkassky 2    Tom M. Mitchell 3    Marcel Adam Just 2 Language Technologies Institute 1 Center for Cognitive Brain Imaging 2 Machine Learning Department 3 Carnegie Mellon University Pittsburgh, PA 15213, U.S.A. {kkchang,cherkassky,tom.mitchell,just}@cmu.edu    Abstract  Recent advances in functional Magnetic Resonance Imaging (fMRI) offer a significant new approach to studying semantic representations in humans by making it possible to directly observe brain activity while people comprehend words and sentences." ></td>
	<td class="line x" title="3:222	In this study, we investigate how humans comprehend adjective-noun phrases (e.g. strong dog) while their neural activity is recorded." ></td>
	<td class="line x" title="4:222	Classification analysis shows that the distributed pattern of neural activity contains sufficient signal to decode differences among phrases." ></td>
	<td class="line x" title="5:222	Furthermore, vector-based semantic models can explain a significant portion of systematic variance in the observed neural activity." ></td>
	<td class="line x" title="6:222	Multiplicative composition models of the two-word phrase outperform additive models, consistent with the assumption that people use adjectives to modify the meaning of the noun, rather than conjoining the meaning of the adjective and noun." ></td>
	<td class="line x" title="7:222	1 Introduction How humans represent meanings of individual words and how lexical semantic knowledge is combined to form complex concepts are issues fundamental to the study of human knowledge." ></td>
	<td class="line x" title="8:222	There have been a variety of approaches from different scientific communities trying to characterize semantic representations." ></td>
	<td class="line x" title="9:222	Linguists have tried to characterize the meaning of a word with feature-based approaches, such as semantic roles (Kipper et al., 2006), as well as word-relation approaches, such as WordNet (Miller, 1995)." ></td>
	<td class="line oc" title="10:222	Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church & Hanks, 1990)." ></td>
	<td class="line x" title="11:222	Psychologists have studied word meaning through feature-norming studies (Cree & McRae, 2003) in which human participants are asked to list the features they associate with various words." ></td>
	<td class="line x" title="12:222	There are also efforts to recover the latent semantic structure from text corpora using techniques such as LSA (Landauer & Dumais, 1997) and topic models (Blei et al., 2003)." ></td>
	<td class="line x" title="13:222	Recent advances in functional Magnetic Resonance Imaging (fMRI) provide a significant new approach to studying semantic representations in humans by making it possible to directly observe brain activity while people comprehend words and sentences." ></td>
	<td class="line x" title="14:222	fMRI measures the hemodynamic response (changes in blood flow and blood oxygenation) related to neural activity in the human brain." ></td>
	<td class="line x" title="15:222	Images can be acquired at good spatial resolution and reasonable temporal resolution  the activity level of 15,000 20,000 brain volume elements (voxels) of about 50 mm 3  each can be measured every 1 second." ></td>
	<td class="line x" title="16:222	Recent multivariate analyses of fMRI activity have shown that classifiers can be trained to decode which of several visually presented objects or object categories a person is contemplating, given the persons fMRImeasured neural activity (Cox and Savoy, 2003; O'Toole et al., 2005; Haynes and Rees, 2006; Mitchell et al., 2004)." ></td>
	<td class="line x" title="17:222	Furthermore, Mitchell et al.(2008) showed that word features computed from the occurrences of stimulus words (within a trillion-token Google text corpus that captures the typical use of words in English text) can predict the brain activity associated with the 638 meaning of these words." ></td>
	<td class="line x" title="19:222	They developed a generative model that is capable of predicting fMRI neural activity well enough that it can successfully match words it has not yet encountered to their previously unseen fMRI images with accuracies far above chance level." ></td>
	<td class="line x" title="20:222	The distributed pattern of neural activity encodes the meanings of words, and the models success indicates some initial access to the encoding." ></td>
	<td class="line x" title="21:222	Given these early succesess in using fMRI to discriminate categorial information and to model lexical semantic representations of individual words, it is interesting to ask whether a similar approach can be used to study the representation of adjective-noun phrases." ></td>
	<td class="line x" title="22:222	In this study, we applied the vector-based models of semantic composition used in computational linguistics to model neural activation patterns obtained while subjects comprehended adjective-noun phrases." ></td>
	<td class="line x" title="23:222	In an object-contemplation task, human participants were presented with 12 text labels of objects (e.g. dog) and were instructed to think of the same properties of the stimulus object consistently during multiple presentations of each item." ></td>
	<td class="line x" title="24:222	The participants were also shown adjective-noun phrases, where adjectives were used to modify the meaning of nouns (e.g. strong dog)." ></td>
	<td class="line x" title="25:222	Mitchell and Lapata (2008) presented a framework for representing the meaning of phrases and sentences in vector space." ></td>
	<td class="line x" title="26:222	They discussed how an additive model, a multiplicative model, a weighted additive model, a Kintsch (2001) model, and a model which combines multiplicative and additive models can be used to model human behavior in similiarity judgements when human participants were presented with a reference containing a subjectverb phrase (e.g., horse ran) and two landmarks (e.g., galloped and dissolved) and asked to choose which landmark was most similiar to the reference (in this case, galloped)." ></td>
	<td class="line x" title="27:222	They compared the composition models to human similarity ratings and found that all models were statistically significantly correlated with human judgements." ></td>
	<td class="line x" title="28:222	Moreover, the multiplicative and combined model performed signficantlly better than the non-compositional models." ></td>
	<td class="line x" title="29:222	Our approach is similar to that of Mitchell and Lapata (2008) in that we compared additive and multiplicative models to non-compositional models in terms of their ability to model human data." ></td>
	<td class="line x" title="30:222	Our work differs from these efforts because we focus on modeling neural activity while people comprehend adjective-noun phrases." ></td>
	<td class="line x" title="31:222	In section 2, we describe the experiment and how functional brain images were acquired." ></td>
	<td class="line x" title="32:222	In section 3, we apply classifier analysis to see if the distributed pattern of neural activity contains sufficient signal to discriminate among phrases." ></td>
	<td class="line x" title="33:222	In section 4, we discuss a vector-based approach to modeling the lexical semantic knowledge using word occurrence measures in a text corpus." ></td>
	<td class="line x" title="34:222	Two composition models, namely the additive and the multiplicative models, along with two non-composition models, namely the adjective and the noun models, are used to explain the systematic variance in neural activation." ></td>
	<td class="line x" title="35:222	Section 5 distinguishes between two types of adjectives that are used in our stimuli: attribute-specifying adjectives and object-modifying adjectives." ></td>
	<td class="line x" title="36:222	Classifier analysis suggests people interpret the two types of adjectives differently." ></td>
	<td class="line x" title="37:222	Finally, we discuss some of the implications of our work and suggest some future studies." ></td>
	<td class="line x" title="38:222	2 Brain Imaging Experiments on Adjective-Noun Comprehension 2.1 Experimental Paradigm Nineteen right-handed adults (aged between 18 and 32) from the Carnegie Mellon community participated and gave informed consent approved by the University of Pittsburgh and Carnegie Mellon Institutional Review Boards." ></td>
	<td class="line x" title="39:222	Four additional participants were excluded from the analysis due to head motion greater than 2.5 mm." ></td>
	<td class="line x" title="40:222	The stimuli were text labels of 12 concrete nouns from 4 semantic categories with 3 exemplars per category." ></td>
	<td class="line x" title="41:222	The 12 nouns were bear, cat, dog (animal); bottle, cup, knife (utensil); carrot, corn, tomato (vegetable); airplane, train, and truck (vehicle; see Table 1)." ></td>
	<td class="line x" title="42:222	The fMRI neural signatures of these objects have been found in previous studies to elicit different neural activity." ></td>
	<td class="line x" title="43:222	The participants were also shown each of the 12 nouns paired with an adjective, where the adjectives are expected to emphasize certain semantic properties of the nouns." ></td>
	<td class="line x" title="44:222	For instance, in the case of strong dog, the adjective is used to emphasize the visual or physical aspect (e.g. muscular) of a dog, as opposed to the behavioral aspects (e.g. play, eat, petted) that people more often associate with the term." ></td>
	<td class="line x" title="45:222	Notice that the last three adjectives in Table 1 are marked by asterisks to denote they are object-modifying adjectives." ></td>
	<td class="line x" title="46:222	These adjectives appear to behave differently from the ordinary attribute-specifying adjectives." ></td>
	<td class="line x" title="47:222	Section 5 is devoted to discussing the different adjective types in more detail." ></td>
	<td class="line x" title="48:222	639  Adjective Noun Category Soft Bear Animal Large Cat Animal Strong Dog Animal Plastic Bottle Utensil Small Cup Utensil Sharp Knife Utensil Hard Carrot Vegetable Cut Corn Vegetable Firm Tomato Vegetable Paper* Airplane Vehicle Model* Train Vehicle Toy* Truck Vehicle Table 1." ></td>
	<td class="line x" title="49:222	Word stimuli." ></td>
	<td class="line x" title="50:222	Asterisks mark the object-modifying adjectives, as opposed to the ordinary attribute-specifying adjectives." ></td>
	<td class="line x" title="51:222	To ensure that participants had a consistent set of properties to think about, they were each asked to generate and write a set of properties for each exemplar in a session prior to the scanning session (such as 4 legs, house pet, fed by me for dog)." ></td>
	<td class="line x" title="52:222	However, nothing was done to elicit consistency across participants." ></td>
	<td class="line x" title="53:222	The entire set of 24 stimuli was presented 6 times during the scanning session, in a different random order each time." ></td>
	<td class="line x" title="54:222	Participants silently viewed the stimuli and were asked to think of the same item properties consistently across the 6 presentations of the items." ></td>
	<td class="line x" title="55:222	Each stimulus was presented for 3s, followed by a 7s rest period, during which the participants were instructed to fixate on an X displayed in the center of the screen." ></td>
	<td class="line x" title="56:222	There were two additional presentations of fixation, 31s each, at the beginning and end of each session, to provide a baseline measure of activity." ></td>
	<td class="line x" title="57:222	2.2 Data Acquisition and Processing Functional images were acquired on a Siemens Allegra 3.0T scanner (Siemens, Erlangen, Germany) at the Brain Imaging Research Center of Carnegie Mellon University and the University of Pittsburgh using a gradient echo EPI pulse sequence with TR = 1000 ms, TE = 30 ms, and a 60 flip angle." ></td>
	<td class="line x" title="58:222	Seventeen 5-mm thick oblique-axial slices were imaged with a gap of 1mm between slices." ></td>
	<td class="line x" title="59:222	The acquisition matrix was 64 x 64 with 3.125 x 3.125 x 5-mm voxels." ></td>
	<td class="line x" title="60:222	Data processing were performed with Statistical Parametric Mapping software (SPM2, Wellcome Department of Cognitive Neurology, London, UK; Friston, 2005)." ></td>
	<td class="line x" title="61:222	The data were corrected for slice timing, motion, and linear trend, and were temporally smoothed with a high-pass filter using a 190s cutoff." ></td>
	<td class="line x" title="62:222	The data were normalized to the MNI template brain image using a 12parameter affine transformation and resampled to 3 x 3 x 6-mm 3  voxels." ></td>
	<td class="line x" title="63:222	The percent signal change (PSC) relative to the fixation condition was computed for each item presentation at each voxel." ></td>
	<td class="line x" title="64:222	The mean of the four images (mean PSC) acquired within a 4s window, offset 4s from the stimulus onset (to account for the delay in hemodynamic response), provided the main input measure for subsequent analysis." ></td>
	<td class="line x" title="65:222	The mean PSC data for each word presentation were further normalized to have mean zero and variance one to equate the variation between participants over exemplars." ></td>
	<td class="line x" title="66:222	Due to the inherent limitations in the temporal properties of fMRI data, we consider here only the spatial distribution of the neural activity after the stimuli are comprehended and do not attempt to model the cogntive process of comprehension." ></td>
	<td class="line x" title="67:222	3 Does the distribution of neural activity encode sufficient signal to classify adjective-noun phrases?" ></td>
	<td class="line x" title="68:222	3.1 Classifier Analysis We are interested in whether the distribution of neural activity encodes sufficient signal to decode both nouns and adjective-noun phrases." ></td>
	<td class="line x" title="69:222	Given the observed neural activity when participants comprehended the adjective-noun phrases, Gaussian Nave Bayes classifiers were trained to identify cognitive states associated with viewing stimuli from the evoked patterns of functional activity (mean PSC)." ></td>
	<td class="line x" title="70:222	For instance, the classifier would predict which of the 24 exemplars the participant was viewing and thinking about." ></td>
	<td class="line x" title="71:222	Separate classifiers were also trained for classifying the isolated nouns, the phrases, and the 4 semantic categories." ></td>
	<td class="line x" title="72:222	Since fMRI acquires the neural activity at 15,000  20,000 distinct voxel locations, many of which might not exhibit neural activity that encodes word or phrase meaning, the classifier analysis selected the voxels whose responses to the 24 different items were most stable across presentations." ></td>
	<td class="line x" title="73:222	Voxel stability was computed as the average pairwise correlation between 24 item vectors across presentations." ></td>
	<td class="line x" title="74:222	The focus on the most stable voxels effectively increased the signal-to-noise ratio in the data and facilitated further analysis by classifiers." ></td>
	<td class="line x" title="75:222	Many of our previous analyses have indicated that 120 voxels is a set size suitable for our purposes." ></td>
	<td class="line x" title="76:222	640 Classification results were evaluated using 6fold cross validation, where one of the 6 repetitions was left out for each fold." ></td>
	<td class="line x" title="77:222	The voxel selection procedure was performed separately inside each fold, using only the training data." ></td>
	<td class="line x" title="78:222	Since multiple classes were involved, rank accuracy was used (Mitchell et al., 2004) to evaluate the classifier." ></td>
	<td class="line x" title="79:222	Given a new fMRI image to classify, the classifier outputs a rank-ordered list of possible class labels from most to least likely." ></td>
	<td class="line x" title="80:222	The rank accuracy is defined as the percentile rank of the correct class in this ordered output list." ></td>
	<td class="line x" title="81:222	Rank accuracy ranges from 0 to 1." ></td>
	<td class="line x" title="82:222	Classification analysis was performed separately for each participant, and the mean rank accuracy was computed over the participants." ></td>
	<td class="line x" title="83:222	3.2 Results and Discussion Table 2 shows the results of the exemplar-level classification analysis." ></td>
	<td class="line x" title="84:222	All classification accuracies were significantly higher than chance (p < 0.05), where the chance level for each classification is determined based on the empirical distribution of rank accuracies for randomly generated null models." ></td>
	<td class="line x" title="85:222	One hundred null models were generated by permuting the class labels." ></td>
	<td class="line x" title="86:222	The classifier was able to distinguish among the 24 exemplars with mean rank accuracies close to 70%." ></td>
	<td class="line x" title="87:222	We also determined the classification accuracies separately for nouns only and phrases only." ></td>
	<td class="line x" title="88:222	Distinct classifiers were trained." ></td>
	<td class="line x" title="89:222	Classification accuracies were significantly higher (p < 0.05) for the nouns, calculated with a paired t-test." ></td>
	<td class="line x" title="90:222	For 3 participants, the classifier did not achieve reliable classification accuracies for the phrase stimuli." ></td>
	<td class="line x" title="91:222	Moreover, we determined the classification accuracies separately for each semantic category of stimuli." ></td>
	<td class="line x" title="92:222	There were no significant differences in accuracy across categories, except for the difference between vegetables and vehicles." ></td>
	<td class="line x" title="93:222	Classifier Racc All 24 exemplars 0.69 Nouns 0.71 Phrases 0.64 Animals 0.67 Tools 0.66 Vegetables 0.65 Vehicles 0.69 Table 2." ></td>
	<td class="line x" title="94:222	Rank accuracies for classifiers." ></td>
	<td class="line x" title="95:222	Distinct classifiers were trained to distinguish all 24 examples, nouns only, phrases only, and only words within each of the 4 semantic categories." ></td>
	<td class="line x" title="96:222	High classification accuracies indicate that the distributed pattern of neural activity does encode sufficient signal to discriminate differences among stimuli." ></td>
	<td class="line x" title="97:222	The classification accuracy for the nouns was on par with previous research, providing a replication of previous findings (Mitchell et al, 2004)." ></td>
	<td class="line x" title="98:222	The classifiers performed better on the nouns than the phrases, consistent with our expectation that characterizing phrases is more difficult than characterizing nouns in isolation." ></td>
	<td class="line x" title="99:222	It is easier for participants to recall properties associated with a familiar object than to comprehend a noun whose meaning is further modified by an adjective." ></td>
	<td class="line x" title="100:222	The classification analysis also helps us to identify participants whose mental representations for phrases are consistent across phrase presentations." ></td>
	<td class="line x" title="101:222	Subsequent regression analysis on phrase activation will be based on subjects who perform the phrase task well." ></td>
	<td class="line oc" title="102:222	4 Using vector-based models of semantic representation to account for the systematic variances in neural activity 4.1 Lexical Semantic Representation Computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990)." ></td>
	<td class="line x" title="103:222	Consequently, Mitchell et al.(2008) encoded the meaning of a word as a vector of intermediate semantic features computed from the co-occurrences with stimulus words within the Google trillion-token text corpus that captures the typical use of words in English text." ></td>
	<td class="line x" title="105:222	Motivated by existing conjectures regarding the centrality of sensory-motor features in neural representations of objects (Caramazza and Shelton, 1998), they selected a set of 25 semantic features defined by 25 verbs: see, hear, listen, taste, smell, eat, touch, rub, lift, manipulate, run, push, fill, move, ride, say, fear, open, approach, near, enter, drive, wear, break, and clean." ></td>
	<td class="line x" title="106:222	These verbs generally correspond to basic sensory and motor activities, actions performed on objects, and actions involving changes in spatial relationships." ></td>
	<td class="line x" title="107:222	Because there are only 12 stimuli in our experiment, we consider only 5 sensory verbs (see hear, smell, eat and touch) to avoid overfitting with the full set of 25 verbs." ></td>
	<td class="line x" title="108:222	Following the work of Bullinaria and Levy (2007), we consider the basic semantic vector which normalizes n(c,t), the count of times context word c occurs within a window of 5 words around the target word t. The 641 basic semantic vector is thus the vector of conditional probabilities,  () () () () ()  == c tcn tcn tp tcp tcp , ,, |  where all components are positive and sum to one." ></td>
	<td class="line x" title="109:222	Table 3 shows the semantic representation for strong and dog." ></td>
	<td class="line x" title="110:222	Notice that strong is heavily loaded on see and smell, whereas dog is heavily loaded on eat and see, consistent with the intuitive interpretation of these two words." ></td>
	<td class="line x" title="111:222	See Hear Smell Eat Touch Strong 0.63 0.06 0.26 0.03 0.03 Dog 0.34 0.06 0.05 0.54 0.02 Table 3." ></td>
	<td class="line x" title="112:222	The lexical semantic representation for strong and dog." ></td>
	<td class="line x" title="113:222	4.2 Semantic Composition We adopt the vector-based semantic composition models discussed in Mitchell and Lapata (2008)." ></td>
	<td class="line x" title="114:222	Let u and v denote the meaning of the adjective and noun, respectively, and let p denote the composition of the two words in vector space." ></td>
	<td class="line x" title="115:222	We consider two non-composition models, the adjective model and the noun model, as well as two composition models, the additive model and the multplicative model." ></td>
	<td class="line x" title="116:222	The adjective model assumes that the meaning of the composition is the same as the adjective:  up =  The noun model assumes that the meaning of the composition is the same as the noun:  vp =  The adjective model and the noun model correspond to the assumption that when people comprehend phrases, they focus exclusively on one of the two words." ></td>
	<td class="line x" title="117:222	This serves as a baseline for comparison to other models." ></td>
	<td class="line x" title="118:222	The additive model assumes the meaning of the composition is a linear combination of the adjective and noun vector:  vBuAp +=  where A and B are vectors of weighting coefficients." ></td>
	<td class="line x" title="119:222	The multiplicative model assumes the meaning of the composition is the element-wise product of the two vectors:  vuCp =  Mitchell and Lapata (2008) fitted the parameters of the weighting vectors A, B, and C, though we assume A = B = C = 1, since we are interested in the model comparison." ></td>
	<td class="line x" title="120:222	Also, there are no model complexity issues, since the number of parameters in the four models is the same." ></td>
	<td class="line x" title="121:222	More critically, the additive model and multiplicative model correspond to different cognitive processes." ></td>
	<td class="line x" title="122:222	On the one hand, the additive model assumes that people concatenate the meanings of the two words when comprehending phrases." ></td>
	<td class="line x" title="123:222	On the other hand, the multiplicative model assumes that the contribution of u is scaled to its relevance to v, or vice versa." ></td>
	<td class="line x" title="124:222	Notice that the former assumption of the multiplicative model corresponds to the modifier-head interpretation where adjectives are used to modify the meaning of nouns." ></td>
	<td class="line x" title="125:222	To foreshadow our results, we found the modifier-head interpretation of the multiplicative model to best account for the neural activity observed in adjective-noun phrase data." ></td>
	<td class="line x" title="126:222	Table 4 shows the semantic representation for strong dog under each of the four models." ></td>
	<td class="line x" title="127:222	Although the multiplicative model appears to have small loadings on all features, the relative distribution of loadings still encodes sufficient information, as our later analysis will show." ></td>
	<td class="line x" title="128:222	Notice how the additive model concatenates the meaning of two words and is heavily loaded on see, eat, and smell, whereas the multiplicative model zeros out unshared features like eat and smell." ></td>
	<td class="line x" title="129:222	As a result, the multiplicative model predicts that the visual aspects will be emphasized when a participant is thinking about strong dog, while the additive model predicts that, in addition, the behavioral aspects (e.g., eat, smell, and hear) of dog will be emphasized." ></td>
	<td class="line x" title="130:222	See Hear Smell Eat Touch Adj 0.63 0.06 0.26 0.03 0.03 Noun 0.34 0.06 0.05 0.54 0.02 Add 0.96 0.12 0.31 0.57 0.04 Multi 0.21 0.00 0.01 0.01 0.00 Table 4." ></td>
	<td class="line x" title="131:222	The semantic representation for strong dog under the adjective, noun, additive, and multiplicative models." ></td>
	<td class="line x" title="132:222	642 Notice that these 4 vector-based semantic composition models ignore word order." ></td>
	<td class="line x" title="133:222	This corresponds to the bag-of-words assumption, such that the representation for strong dog will be the same as that of dog strong." ></td>
	<td class="line x" title="134:222	The bag-of-words model is used as a simplifying assumption in several semantic models, including LSA (Landauer & Dumais, 1997) and topic models (Blei et al., 2003)." ></td>
	<td class="line x" title="135:222	There were two main hypotheses that we tested." ></td>
	<td class="line x" title="136:222	First, people usually regard the noun in the adjective-noun pair as the linguistic head." ></td>
	<td class="line x" title="137:222	Therefore, meaning associated with the noun should be more evoked." ></td>
	<td class="line x" title="138:222	Thus, we predicted that the noun model would outperform the adjective model." ></td>
	<td class="line x" title="139:222	Second, people make more interpretations that use adjectives to modify the meaning of the noun, rather than disjunctive interpretations that add together or take the union of the semantic features of the two words." ></td>
	<td class="line x" title="140:222	Thus, we predicted that the multiplicative model would outperform the additive model." ></td>
	<td class="line x" title="141:222	4.3 Regression Fit In this analysis, we train a regression model to fit the activation profile for the 12 phrase stimuli." ></td>
	<td class="line x" title="142:222	We focused on subjects for whom the classifier established reliable classification accuracies for the phrase stimuli." ></td>
	<td class="line x" title="143:222	The regression model examined to what extent the semantic feature vectors (explanatory variables) can account for the variation in neural activity (response variable) across the 12 stimuli." ></td>
	<td class="line x" title="144:222	All explanatory variables were entered into the regression model simultaneously." ></td>
	<td class="line x" title="145:222	More precisely, the predicted activity a v  at voxel v in the brain for word w is given by  ()  = += n i viviv wfa 1   where f i (w) is the value of the i th  intermediate semantic feature for word w,  vi  is the regression coefficient that specifies the degree to which the i th  intermediate semantic feature activates voxel v, and  v  is the models error term that represents the unexplained variation in the response variable." ></td>
	<td class="line x" title="146:222	Least squares estimates of  vi  were obtained to minimize the sum of squared errors in reconstructing the training fMRI images." ></td>
	<td class="line x" title="147:222	An L2 regularization with lambda = 1.0 was added to prevent overfitting given the high parameter-todata-points ratios." ></td>
	<td class="line x" title="148:222	A regression model was trained for each of the 120 voxels and the reported R 2  is the average across the 120 voxels." ></td>
	<td class="line x" title="149:222	R 2  measures the amount of systematic variance explained by the model." ></td>
	<td class="line x" title="150:222	Regression results were evaluated using 6-fold cross validation, where one of the 6 repetitions was left out for each fold." ></td>
	<td class="line x" title="151:222	Linear regression assumes a linear dependency among the variables and compares the variance due to the independent variables against the variance due to the residual errors." ></td>
	<td class="line x" title="152:222	While the linearity assumption may be overly simplistic, it reflects the assumption that fMRI activity often reflects a superimposition of contributions from different sources, and has provided a useful first order approximation in the field (Mitchell et al., 2008)." ></td>
	<td class="line x" title="153:222	4.4 Results and Discussion The second column of Table 5 shows the R 2  regression fit (averaged across 120 voxels) of the adjective, noun, additive, and multiplicative model to the neural activity observed in adjective-noun phrase data." ></td>
	<td class="line x" title="154:222	The noun model significantly (p < 0.05) outperformed the adjective model, estimated with a paired t-test." ></td>
	<td class="line x" title="155:222	Moreover, the difference between the additive and adjective models was not significant, whereas the difference between the additive and noun models was significant (p < 0.05)." ></td>
	<td class="line x" title="156:222	The multiplicative model significantly (p < 0.05) outperformed both of the non-compositional models, as well as the additive model." ></td>
	<td class="line x" title="157:222	More importantly, the two hypotheses that we were testing were both verified." ></td>
	<td class="line x" title="158:222	Notice Table 5 supports our hypothesis that the noun model should outperform the adjective model based on the assumption that the noun is generally more central to the phrase meaning than is the adjective." ></td>
	<td class="line x" title="159:222	Table 5 also supports our hypothesis that the multiplicative model should outperform the additive model, based on the assumption that adjectives are used to emphasize particular semantic features that will already be represented in the semantic feature vector of the noun." ></td>
	<td class="line x" title="160:222	Our findings here are largely consistent with Mitchell and Lapata (2008)." ></td>
	<td class="line x" title="161:222	R 2 Racc Adjective 0.34 0.57 Noun 0.36 0.61 Additive 0.35 0.60 Multiplicative 0.42 0.62 Table 5." ></td>
	<td class="line x" title="162:222	Regression fit and regression-based classification rank accuracy of the adjective, noun, additive, and multiplicative models for phrase stimuli." ></td>
	<td class="line x" title="163:222	643 Following Mitchell et al.(2008), the regression model can be used to decode mental states." ></td>
	<td class="line x" title="165:222	Specifically, for each regression model, the estimated regression weights can be used to generate the predicted activity for each word." ></td>
	<td class="line x" title="166:222	Then, a previously unseen neural activation vector is identified with the class of the predicted activation that had the highest correlation with the given observed neural activation vector." ></td>
	<td class="line x" title="167:222	Notice that, unlike Mitchell et al.(2008), where the regression model was used to make predictions for items outside the training set, here we are just showing that the regression model can be used for classification purposes." ></td>
	<td class="line x" title="169:222	The third column of Table 5 shows the rank accuracies classifying mental concepts using the predicted activation from the adjective, noun, additive, and multiplicative models." ></td>
	<td class="line x" title="170:222	All rank accuracies were significantly higher (p < 0.05) than chance, where the chance level for each classification is again determined by permutation testing." ></td>
	<td class="line x" title="171:222	More importantly, here we observe a ranking of these four models similar to that observed for the regression analysis." ></td>
	<td class="line x" title="172:222	Namely, the noun model performs significantly better (p < 0.05) than the adjective model, and the multiplicative model performs significantly better (p < 0.05) than the additive model." ></td>
	<td class="line x" title="173:222	However, the difference between the multiplicative model and the noun model is not statistically significant in this case." ></td>
	<td class="line x" title="174:222	5 Comparing the attribute-specifying adjectives with the object-modifying adjectives Some of the phrases contained adjectives that changed the meaning of the noun." ></td>
	<td class="line x" title="175:222	In the case of vehicle nouns, adjectives were chosen to modify the manipulability of the nouns (e.g., to make an airplane more manipulable, paper was chosen as the modifier)." ></td>
	<td class="line x" title="176:222	This type of modifier raises two issues." ></td>
	<td class="line x" title="177:222	First, these modifiers (e.g. paper, model, toy) more typically assume the part of speech (POS) tag of nouns, unlike our other modifiers (e.g., soft, large, strong) whose typical POS tag is adjective." ></td>
	<td class="line x" title="178:222	Second, these modifiers combine with the noun to denote a very different object from the noun in isolation (paper airplane, model train, toy truck), in comparison to other cases where the adjective simply specifies an attribute of the noun (soft bear, large cat, strong dog, etc.)." ></td>
	<td class="line x" title="179:222	In order to study this difference, we performed classification analysis separately for the attribute-specifying adjectives and the objectmodifying adjectives." ></td>
	<td class="line x" title="180:222	Our hypothesis is that the phrases with attribute-specifying adjectives will be much more difficult to distinguish from the original nouns than the adjectives that change the referent." ></td>
	<td class="line x" title="181:222	For instance, we hypothesize that it is much more difficult to distinguish the neural representation for strong dog versus dog than it is to distinguish the neural representation for paper airplane versus airplane." ></td>
	<td class="line x" title="182:222	To verify this, Gaussian Nave Bayes classifiers were trained to discriminate between each of the 12 pairs of nouns and adjective-noun phrases." ></td>
	<td class="line x" title="183:222	The average classification for phrases with object-modifying adjectives is 0.76, whereas classification accuracies for phrases with attribute-specifying adjectives are 0.68." ></td>
	<td class="line x" title="184:222	The difference is statistically significant at p < 0.05." ></td>
	<td class="line x" title="185:222	This result supports our hypothesis." ></td>
	<td class="line x" title="186:222	Furthermore, we performed regression-based classification separately for the two types of adjectives." ></td>
	<td class="line x" title="187:222	Notice that the number of phrases with object-modifying adjectives is much less than the number of phrases with attribute-specifying adjectives (3 vs. 9)." ></td>
	<td class="line x" title="188:222	This affects the parameter-todata-points ratio in our regression model." ></td>
	<td class="line x" title="189:222	Consequently, an L2 regularization with lambda = 10.0 was used to prevent overfitting." ></td>
	<td class="line x" title="190:222	Table 6 shows a pattern similar to that seen in section 4 is observed for the attribute-specifying adjectives." ></td>
	<td class="line x" title="191:222	That is, the noun model outperformed the adjective model and the multiplicative model outperformed the additive model when using attributespecifying adjectives." ></td>
	<td class="line x" title="192:222	However, for the objectmodifying adjectives, the noun model no longer outperformed the adjective model." ></td>
	<td class="line x" title="193:222	Moreover, the additive model performed better than the noun model." ></td>
	<td class="line x" title="194:222	Although neither difference is statistically significant, this clearly shows a pattern different from the attribute-specifying adjectives." ></td>
	<td class="line x" title="195:222	This result suggests that when interpreting phrases like paper airplane, it is more important to consider contributions from the adjectives, compared to when interpreting phrases like strong dog, where the contribution from the adjective is simply to specify a property of the item typically referred to by the noun in isolation." ></td>
	<td class="line x" title="196:222	Attributespecifying Objectmodifying Adjective 0.57 0.65 Noun 0.62 0.64 Additive 0.61 0.65 Multiplicative 0.63 0.67 Table 6." ></td>
	<td class="line x" title="197:222	Separate regression-based classification rank accuracy for phrases with attributespecifying or object-modifying adjectives." ></td>
	<td class="line x" title="198:222	644 In light of this observation, we plan to extend our analysis of adjective-nouns phrases to nounnoun phrases, where participants will be shown noun phrases (e.g. carrot knife) and instructed to think of a likely meaning for the phrases." ></td>
	<td class="line x" title="199:222	Unlike adjective-noun phrases, where a single interpretation often dominates, noun-noun combinations allow multiple interpretations (e.g., carrot knife can be interpreted as a knife that is specifically used to cut carrots or a knife carved out of carrots)." ></td>
	<td class="line x" title="200:222	There exists an extensive literature on the conceptual combination of noun-noun phrases." ></td>
	<td class="line x" title="201:222	Costello and Keane (1997) provide extensive studies on the polysemy of conceptual combination." ></td>
	<td class="line x" title="202:222	More importantly, they outline different rules of combination, including property mapping, relational mapping, hybrid mapping, etc. It will be interesting to see if different composition models better account for neural activation when different kinds of combination rules are used." ></td>
	<td class="line x" title="203:222	6 Contribution and Conclusion Experimental results have shown that the distributed pattern of neural activity while people are comprehending adjective-noun phrases does contain sufficient information to decode the stimuli with accuracies significantly above chance." ></td>
	<td class="line x" title="204:222	Furthermore, vector-based semantic models can explain a significant portion of systematic variance in observed neural activity." ></td>
	<td class="line x" title="205:222	Multiplicative composition models outperform additive models, a trend that is consistent with the assumption that people use adjectives to modify the meaning of the noun, rather than conjoining the meaning of the adjective and noun." ></td>
	<td class="line x" title="206:222	In this study, we represented the meaning of both adjectives and nouns in terms of their cooccurrences with 5 sensory verbs." ></td>
	<td class="line x" title="207:222	While this type of representation might be justified for concrete nouns (hypothesizing that their neural representations are largely grounded in sensorymotor features), it might be that a different representation is needed for adjectives." ></td>
	<td class="line x" title="208:222	Further research is needed to investigate alternative representations for both nouns and adjectives." ></td>
	<td class="line x" title="209:222	Moreover, the composition models that we presented here are overly simplistic in a number of ways." ></td>
	<td class="line x" title="210:222	We look forward to future research to extend the intermediate representation and to experiment with different modeling methodologies." ></td>
	<td class="line x" title="211:222	An alternative approach is to model the semantic representation as a hidden variable using a generative probabilistic model that describes how neural activity is generated from some latent semantic representation." ></td>
	<td class="line x" title="212:222	We are currently exploring the infinite latent semantic feature model (ILFM; Griffiths & Ghahramani, 2005), which assumes a non-parametric Indian Buffet prior to the binary feature vector and models neural activation with a linear Gaussian model." ></td>
	<td class="line x" title="213:222	The basic proposition of the model is that the human semantic knowledge system is capable of storing an infinite list of features (or semantic components) associated with a concept; however, only a subset is actively recalled during any given task (contextdependent)." ></td>
	<td class="line x" title="214:222	Thus, a set of latent indicator variables is introduced to indicate whether a feature is actively recalled at any given task." ></td>
	<td class="line x" title="215:222	We are investigating if the compositional models also operate in the learned latent semantic space." ></td>
	<td class="line x" title="216:222	The premise of our research relies on advancements in the fields of computational linguistics and cognitive neuroimaging." ></td>
	<td class="line x" title="217:222	Indeed, we are at an especially opportune time in the history of the study of language, when linguistic corpora allow word meanings to be computed from the distribution of word co-occurrence in a trilliontoken text corpus, and brain imaging technology allows us to directly observe and model neural activity associated with the conceptual combination of lexical items." ></td>
	<td class="line x" title="218:222	An improved understanding of language processing in the brain could yield a more biologically-informed model of semantic representation of lexical knowledge." ></td>
	<td class="line x" title="219:222	We therefore look forward to further brain imaging studies shedding new light on the nature of human representation of semantic knowledge." ></td>
	<td class="line x" title="220:222	Acknowledgements This research was supported by the National Science Foundation, Grant No." ></td>
	<td class="line x" title="221:222	IIS-0835797, and by the W. M. Keck Foundation." ></td>
	<td class="line x" title="222:222	We would like to thank Jennifer Moore for help in preparation of the manuscript." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0211
A Non-negative Tensor Factorization Model for Selectional Preference Induction
Van de Cruys, Tim;"></td>
	<td class="line x" title="1:165	Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 8390, Athens, Greece, 31 March 2009." ></td>
	<td class="line x" title="2:165	c2009 Association for Computational Linguistics A Non-negative Tensor Factorization Model for Selectional Preference Induction Tim Van de Cruys University of Groningen The Netherlands t.van.de.cruys@rug.nl Abstract Distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity." ></td>
	<td class="line x" title="3:165	Up till now, most algorithms use two-way cooccurrence data to compute the meaning of words." ></td>
	<td class="line x" title="4:165	Co-occurrence frequencies, however, need not be pairwise." ></td>
	<td class="line x" title="5:165	One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond." ></td>
	<td class="line x" title="6:165	This paper will investigate a tensor factorization method called non-negative tensor factorization to build a model of three-way cooccurrences." ></td>
	<td class="line x" title="7:165	The approach is applied to the problem of selectional preference induction, and automatically evaluated in a pseudo-disambiguation task." ></td>
	<td class="line x" title="8:165	The results show that non-negative tensor factorization is a promising tool for NLP." ></td>
	<td class="line x" title="9:165	1 Introduction Distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity." ></td>
	<td class="line x" title="10:165	Theaggregateofawordscontextsgenerally provides enough information to compute its meaning, viz." ></td>
	<td class="line x" title="11:165	its semantic similarity or relatedness to other words." ></td>
	<td class="line x" title="12:165	Up till now, most algorithms use two-way cooccurrence data to compute the meaning of words." ></td>
	<td class="line x" title="13:165	Awordsmeaningmightforexamplebecomputed by looking at:  the various documents that the word appears in (words  documents);  a bag of words context window around the word (words  context words);  the dependency relations that the word appears with (words  dependency relations)." ></td>
	<td class="line x" title="14:165	The extracted data  representing the cooccurrence frequencies of two different entities  is encoded in a matrix." ></td>
	<td class="line x" title="15:165	Co-occurrence frequencies, however, need not be pairwise." ></td>
	<td class="line x" title="16:165	One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond." ></td>
	<td class="line x" title="17:165	In an information retrieval context, one such situation might be the investigation of words  documents  authors." ></td>
	<td class="line x" title="18:165	In an NLP context,onemightwanttoinvestigatewordsdependency relations  bag of word context words, or verbs  subjects  direct objects." ></td>
	<td class="line x" title="19:165	Note that it is not possible to investigate the three-way co-occurrences in a matrix representation form." ></td>
	<td class="line x" title="20:165	It is possible to capture the cooccurrence frequencies of a verb with its subjects and its direct objects, but one cannot capture the co-occurrence frequencies of the verb appearing with the subject and the direct object at the same time." ></td>
	<td class="line x" title="21:165	When the actual three-way cooccurrence data is matricized, valuable information is thrown-away." ></td>
	<td class="line x" title="22:165	To be able to capture the mutual dependencies among the three modes, we will make use of a generalized tensor representation." ></td>
	<td class="line x" title="23:165	Two-way co-occurrence models (such as latentsemanticanalysis)haveoftenbeenaugmented with some form of dimensionality reduction in ordertocounternoiseandovercomedatasparseness." ></td>
	<td class="line x" title="24:165	We will also make use of a dimensionality reduction algorithm appropriate for tensor representations." ></td>
	<td class="line x" title="25:165	2 Previous Work 2.1 Selectional Preferences & Verb Clustering Selectional preferences have been a popular research subject in the NLP community." ></td>
	<td class="line x" title="26:165	One of the first to automatically induce selectional preferences from corpora was Resnik (1996)." ></td>
	<td class="line x" title="27:165	Resnik generalizes among nouns by using WordNet noun 83 synsets as clusters." ></td>
	<td class="line x" title="28:165	He then calculates the selectional preference strength of a specific verb in a particular relation by computing the KullbackLeibler divergence between the cluster distribution of the verb and the aggregate cluster distribution." ></td>
	<td class="line x" title="29:165	The selectional association is then the contribution of the cluster to the verbs preference strength." ></td>
	<td class="line x" title="30:165	The models generalization relies entirely on WordNet; there is no generalization among the verbs." ></td>
	<td class="line x" title="31:165	The research in this paper is related to previous work on clustering." ></td>
	<td class="line x" title="32:165	Pereira et al.(1993) use an information-theoretic based clustering approach, clustering nouns according to their distribution as direct objects among verbs." ></td>
	<td class="line x" title="34:165	Their model is a onesided clustering model: only the direct objects are clustered, there is no clustering among the verbs." ></td>
	<td class="line x" title="35:165	Rooth et al.(1999) use an EM-based clustering technique to induce a clustering based on the co-occurrence frequencies of verbs with their subjects and direct objects." ></td>
	<td class="line x" title="37:165	As opposed to the method of Pereira et al.(1993), their model is two-sided: the verbs as well as the subjects/direct objects are clustered." ></td>
	<td class="line x" title="39:165	We will use a similar model for evaluation purposes." ></td>
	<td class="line x" title="40:165	Recent approaches using distributional similarity methods for the induction of selectional preferences are the ones by Erk (2007), Bhagat et al.(2007) and Basili et al.(2007)." ></td>
	<td class="line x" title="43:165	This research differs from the approaches mentioned above by its use of multi-way data: where the approaches above limit themselves to two-way co-occurrences, this research will focus on cooccurrences for multi-way data." ></td>
	<td class="line x" title="44:165	2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901))." ></td>
	<td class="line x" title="45:165	PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions." ></td>
	<td class="line x" title="46:165	Singular value decomposition (SVD) is the generalization of the eigenvalue decomposition used in PCA (Wall et al., 2003)." ></td>
	<td class="line x" title="47:165	In information retrieval, singular value decomposition has been applied in latent semantic analysis (LSA, Landauer and Dumais (1997), Landauer et al.(1998))." ></td>
	<td class="line x" title="49:165	In LSA, a term-document matrix is created, containing the frequency of each word in a specific document." ></td>
	<td class="line x" title="50:165	This matrix is then decomposed into three other matrices with SVD." ></td>
	<td class="line x" title="51:165	The most important dimensions that come out of the SVD allegedly represent latent semantic dimensions, according to which nouns and documents can be represented more efficiently." ></td>
	<td class="line x" title="52:165	LSA has been criticized for a number of reasons, one of them being the fact that the factorization contains negative numbers." ></td>
	<td class="line x" title="53:165	It is not clear what negativity on a semantic scale should designate." ></td>
	<td class="line x" title="54:165	Subsequent methods such as probabilistic latent semantic analysis (PLSA, Hofmann (1999)) and non-negative matrix factorization (NMF, Lee and Seung (2000)) remedy these problems, and indeed get much more clear-cut semantic dimensions." ></td>
	<td class="line x" title="55:165	2.2.2 Three-way Factorizations To be able to cope with three-way data, several algorithms have been developed as multilinear generalizations of the SVD." ></td>
	<td class="line x" title="56:165	In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001))." ></td>
	<td class="line x" title="57:165	The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), CarrollandChang(1970))andthree-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al.(2000))." ></td>
	<td class="line x" title="59:165	Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002)." ></td>
	<td class="line x" title="60:165	In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006)." ></td>
	<td class="line x" title="61:165	One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005))." ></td>
	<td class="line x" title="62:165	NTF is a generalization of non-negative matrix factorization, and can be considered an extension of the PARAFAC model with the constraint of non-negativity (cfr." ></td>
	<td class="line x" title="63:165	infra)." ></td>
	<td class="line x" title="64:165	One of the few papers that has investigated the application of tensor factorization for NLP is Turney (2007), in which a three-mode tensor is used to compute the semantic similarity of words." ></td>
	<td class="line x" title="65:165	The method achieves 83.75% accuracy on the TOEFL synonym questions." ></td>
	<td class="line x" title="66:165	84 3 Methodology 3.1 Tensors Distributional similarity methods usually represent co-occurrence data in the form of a matrix." ></td>
	<td class="line x" title="67:165	This form is perfectly suited to represent two-way co-occurrence data, but for co-occurrence data beyond two modes, we need a more general representation." ></td>
	<td class="line x" title="68:165	The generalization of a matrix is called a tensor." ></td>
	<td class="line x" title="69:165	A tensor is able to encode co-occurrence data of any n modes." ></td>
	<td class="line x" title="70:165	Figure 1 shows a graphical comparison of a matrix and a tensor with three modes  although a tensor can easily be generalized to more than three modes." ></td>
	<td class="line x" title="71:165	Figure 1: Matrix representation vs. tensor representation 3.2 Non-negative Tensor Factorization Inordertocreateasuccinctandgeneralizedmodel of the extracted data, a statistical dimensionality reduction technique called non-negative tensor factorization (NTF) is applied to the data." ></td>
	<td class="line x" title="72:165	The NTF model is similar to the PARAFAC analysis  popular in areas such as psychology and bio-chemistry  with the constraint that all data needs to be nonnegative (i.e. 0)." ></td>
	<td class="line x" title="73:165	Parallel factor analysis (PARAFAC) is a multilinear analogue of the singular value decomposition (SVD) used in latent semantic analysis." ></td>
	<td class="line x" title="74:165	The key idea is to minimize the sum of squares between the original tensor and the factorized model of the tensor." ></td>
	<td class="line x" title="75:165	For the three mode case of a tensor T RD1D2D3 this gives equation 1, where k is the number of dimensions in the factorized model and  denotes the outer product." ></td>
	<td class="line x" title="76:165	min xiRD1,yiRD2,ziRD3 bardblT  k i=1 xiyizi bardbl2F (1) With non-negative tensor factorization, the nonnegativity constraint is enforced, yielding a model like the one in equation 2: min xiRD10,yiRD20,ziRD30 bardblT  k i=1 xiyizi bardbl2F (2) The algorithm results in three matrices, indicating the loadings of each mode on the factorized dimensions." ></td>
	<td class="line x" title="77:165	The model is represented graphically in figure 2, visualizing the fact that the PARAFAC decomposition consists of the summation over the outer products of n (in this case three) vectors." ></td>
	<td class="line x" title="78:165	Figure 2: Graphical representation of the NTF as the sum of outer products Computationally, the non-negative tensor factorization model is fitted by applying an alternating least-squares algorithm." ></td>
	<td class="line x" title="79:165	In each iteration, two of the modes are fixed and the third one is fitted in a least squares sense." ></td>
	<td class="line x" title="80:165	This process is repeated until convergence.1 3.3 Applied to Language Data Themodelcanstraightforwardlybeappliedtolanguage data." ></td>
	<td class="line x" title="81:165	In this part, we describe the factorization of verbs  subjects  direct objects co-occurrences, but the example can easily be substituted with other co-occurrence information." ></td>
	<td class="line x" title="82:165	Moreover, the model need not be restricted to 3 modes; it is very well possible to go to 4 modes and beyond  as long as the computations remain feasible." ></td>
	<td class="line x" title="83:165	The NTF decomposition for the verbs  subjectsdirect objects co-occurrencesintothethree loadings matrices is represented graphically in figure 3." ></td>
	<td class="line x" title="84:165	By applying the NTF model to three-way (s,v,o) co-occurrences, we want to extract a generalized selectional preference model, and eventually even induce some kind of frame semantics (in the broad sense of the word)." ></td>
	<td class="line x" title="85:165	In the resulting factorization, each verb, subject and direct object gets a loading value for each factor dimension in the corresponding loadings matrix." ></td>
	<td class="line x" title="86:165	The original value for a particular (s,v,o) 1The algorithm has been implemented in MATLAB, using the Tensor Toolbox for sparse tensor calculations (Bader and Kolda, 2007)." ></td>
	<td class="line x" title="87:165	85 Figure 3: Graphical representation of the NTF for language data triple xsvo can then be reconstructed with equation 3." ></td>
	<td class="line x" title="88:165	xsvo = k i=1 ssivviooi (3) To reconstruct the selectional preference value for the triple (man,bite,dog), for example, we look up the subject vector for man, the verb vector for bite and the direct object vector for dog." ></td>
	<td class="line x" title="89:165	Then, for each dimension i in the model, we multiply the ith value of the three vectors." ></td>
	<td class="line x" title="90:165	The sum of these values is the final preference value." ></td>
	<td class="line x" title="91:165	4 Results 4.1 Setup Theapproachdescribedintheprevioussectionhas been applied to Dutch, using the Twente Nieuws Corpus (Ordelman, 2002), a 500M words corpus of Dutch newspaper texts." ></td>
	<td class="line x" title="92:165	The corpus has been parsed with the Dutch dependency parser Alpino (van Noord, 2006), and three-way co-occurrences of verbs with their respective subject and direct object relations have been extracted." ></td>
	<td class="line x" title="93:165	As dimension sizes, the 1K most frequent verbs were used, together with the 10K most frequent subjects and 10K most frequent direct objects, yielding a tensor of 1K  10K  10K." ></td>
	<td class="line x" title="94:165	The resulting tensor is very sparse, with only 0.0002% of the values being non-zero." ></td>
	<td class="line oc" title="95:165	The tensor has been adapted with a straightforward extension of pointwise mutual information (Church and Hanks, 1990) for three-way cooccurrences, following equation 4." ></td>
	<td class="line x" title="96:165	Negative values are set to zero.2 2This is not just an ad hoc conversion to enforce nonnegativity." ></td>
	<td class="line x" title="97:165	Negative values indicate a smaller co-occurrence probability than the expected number of co-occurrences." ></td>
	<td class="line x" title="98:165	Setting those values to zero proves beneficial for similarity calculations (see e.g. Bullinaria and Levy (2007))." ></td>
	<td class="line x" title="99:165	MI3(x,y,z) = log p(x,y,z)p(x)p(y)p(z) (4) The resulting matrix has been factorized into k dimensions (varying between 50 and 300) with the NTF algorithm described in section 3.2." ></td>
	<td class="line x" title="100:165	4.2 Examples Table 1, 2 and 3 show example dimensions that have been found by the algorithm with k = 100." ></td>
	<td class="line x" title="101:165	Each example gives the top 10 subjects, verbs and direct objects for a particular dimension, together with the score for that particular dimension." ></td>
	<td class="line x" title="102:165	Table 1 shows the induction of a police action frame, with police authorities as subjects, police actions as verbs and patients of the police actions as direct objects." ></td>
	<td class="line x" title="103:165	In table 2, a legislation dimension is induced, with legislative bodies as subjects3, legislative actionsasverbs, andmostlylaw(proposals)asdirect objects." ></td>
	<td class="line x" title="104:165	Note that some direct objects (e.g. minister) also designate personsthat can be the object of a legislative act." ></td>
	<td class="line x" title="105:165	Table 3, finally, is clearly an exhibition dimension, with verbs describing actions of display and trade that art institutions (subjects) can do with works of art (objects)." ></td>
	<td class="line x" title="106:165	These are not the only sensible dimensions that have been found by the algorithm." ></td>
	<td class="line x" title="107:165	A quick qualitative evaluation indicates that about 44 dimensions contain similar, framelike semantics." ></td>
	<td class="line x" title="108:165	In another 43 dimensions, the semantics are less clearcut (single verbs account for one dimension, or differentsensesofaverbgetmixedup)." ></td>
	<td class="line x" title="109:165	13dimensionsarenotsomuchbasedonsemanticcharacteristics, but rather on syntax (e.g. fixed expressions and pronomina)." ></td>
	<td class="line x" title="110:165	4.3 Evaluation The results of the NTF model have been quantitatively evaluated in a pseudo-disambiguation task, similar to the one used by Rooth et al.(1999)." ></td>
	<td class="line x" title="112:165	It is used to evaluate the generalization capabilities of the algorithm." ></td>
	<td class="line x" title="113:165	The task is to judge which subject (s or sprime) and direct object (o or oprime) is more likely for a particular verb v, where (s,v,o) is a combination drawn from the corpus, and sprime and oprime are a subject and direct object randomly drawn from the corpus." ></td>
	<td class="line x" title="114:165	A triple is considered correct if the algorithm prefers both s and o over their counterparts 3Note that VVD, D66, PvdA and CDA are Dutch political parties." ></td>
	<td class="line x" title="115:165	86 subjects sus verbs vs objects objs politie police .99 houd aan arrest .64 verdachte suspect .16 agent policeman .07 arresteer arrest .63 man man .16 autoriteit authority .05 pak op run in .41 betoger demonstrator .14 Justitie Justice .05 schiet dood shoot .08 relschopper rioter .13 recherche detective force .04 verdenk suspect .07 raddraaiers instigator .13 marechaussee military police .04 tref aan find .06 overvaller raider .13 justitie justice .04 achterhaal overtake .05 Roemeen Romanian .13 arrestatieteam special squad .03 verwijder remove .05 actievoerder campaigner .13 leger army .03 zoek search .04 hooligan hooligan .13 douane customs .02 spoor op track .03 Algerijn Algerian .13 Table 1: Top 10 subjects, verbs and direct objects for the police action dimension subjects sus verbs vs objects objs meerderheid majority .33 steun support .83 motie motion .63 VVD .28 dien in submit .44 voorstel proposal .53 D66 .25 neem aan pass .23 plan plan .28 Kamermeerderheid Chamber majority .25 wijs af reject .17 wetsvoorstel bill .19 fractie party .24 verwerp reject .14 hem him .18 PvdA .23 vind think .08 kabinet cabinet .16 CDA .23 aanvaard accepts .05 minister minister .16 Tweede Kamer Second Chamber .21 behandel treat .05 beleid policy .13 partij party .20 doe do .04 kandidatuur candidature .11 Kamer Chamber .20 keur goed pass .03 amendement amendment .09 Table 2: Top 10 subjects, verbs and direct objects for the legislation dimension sprime and oprime (so the (s,v,o) triple  that appears in the test corpus  is preferred over the triples (sprime,v,oprime), (sprime,v,o) and (s,v,oprime))." ></td>
	<td class="line x" title="116:165	Table 4 shows three examples from the pseudo-disambiguation task." ></td>
	<td class="line x" title="117:165	s v o sprime oprime jongere drink bier coalitie aandeel youngster drink beer coalition share werkgever riskeer boete doel kopzorg employer risk fine goal worry directeur zwaai scepter informateur vodka manager sway sceptre informer wodka Table 4: Three examples from the pseudodisambiguation evaluation tasks test set Four different models have been evaluated." ></td>
	<td class="line x" title="118:165	The first two models are tensor factorization models." ></td>
	<td class="line x" title="119:165	The first model is the NTF model, as described in section 3.2." ></td>
	<td class="line x" title="120:165	The second model is the original PARAFAC model, without the non-negativity constraints." ></td>
	<td class="line x" title="121:165	The other two models are matrix factorization models." ></td>
	<td class="line x" title="122:165	The third model is the non-negative matrix factorization (NMF) model, and the fourth model is the singular value decomposition (SVD)." ></td>
	<td class="line x" title="123:165	For these models, a matrix has been constructed that contains the pairwise co-occurrence frequencies of verbs by subjects as well as direct objects." ></td>
	<td class="line x" title="124:165	This gives a matrix of 1K verbs by 10K subjects + 10K direct objects (1K  20K)." ></td>
	<td class="line x" title="125:165	The matrix has been adapted with pointwise mutual information." ></td>
	<td class="line x" title="126:165	The models have been evaluated with 10-fold cross-validation." ></td>
	<td class="line x" title="127:165	The corpus contains 298,540 different (s,v,o) co-occurrences." ></td>
	<td class="line x" title="128:165	Those have been randomly divided into 10 equal parts." ></td>
	<td class="line x" title="129:165	So in each fold, 268,686 co-occurrences have been used for training, and 29,854 have been used for testing." ></td>
	<td class="line x" title="130:165	The accuracy results of the evaluation are given in table 5." ></td>
	<td class="line x" title="131:165	The results clearly indicate that the NTF model outperforms all the other models." ></td>
	<td class="line x" title="132:165	The model achieves the best result with 300 dimensions, but the differences between the different NTF models are not very large  all attaining scores around 90%." ></td>
	<td class="line x" title="133:165	87 subjects sus verbs vs objects objs tentoonstelling exhibition .50 toon display .72 schilderij painting .47 expositie exposition .49 omvat cover .63 werk work .46 galerie gallery .36 bevat contain .18 tekening drawing .36 collectie collection .29 presenteer present .17 foto picture .33 museum museum .27 laat let .07 sculptuur sculpture .25 oeuvre oeuvre .22 koop buy .07 aquarel aquarelle .20 Kunsthal .19 bezit own .06 object object .19 kunstenaar artist .15 zie see .05 beeld statue .12 dat that .12 koop aan acquire .05 overzicht overview .12 hij he .10 in huis heb own .04 portret portrait .11 Table 3: Top 10 subjects, verbs and direct objects for the exhibition dimension dimensions 50 (%) 100 (%) 300 (%) NTF 89.52  0.18 90.43  0.14 90.89  0.16 PARAFAC 85.57  0.25 83.58  0.59 80.12  0.76 NMF 81.79  0.15 78.83  0.40 75.74  0.63 SVD 69.60  0.41 62.84  1.30 45.22  1.01 Table 5: Results of the 10-fold cross-validation for the NTF, PARAFAC, NMF and SVD model for 50, 100 and 300 dimensions (averages and standard deviation) The PARAFAC results indicate the fitness of tensor factorization for the induction of three-way selectional preferences." ></td>
	<td class="line x" title="134:165	Even without the constraint of non-negativity, the model outperforms the matrixfactorizationmodels,reachingascoreofabout 85%." ></td>
	<td class="line x" title="135:165	The model deteriorates when more dimensions are used." ></td>
	<td class="line x" title="136:165	Both matrix factorization models perform worse than their tensor factorization counterparts." ></td>
	<td class="line x" title="137:165	The NMF still scores reasonably well, indicating the positive effect of the non-negativity constraint." ></td>
	<td class="line x" title="138:165	The simple SVD model performs worst, reaching a score of about 70% with 50 dimensions." ></td>
	<td class="line x" title="139:165	5 Conclusion and Future Work This paper has presented a novel method that is able to investigate three-way co-occurrences." ></td>
	<td class="line x" title="140:165	Other distributional methods deal almost exclusively with pairwise co-occurrences." ></td>
	<td class="line x" title="141:165	The ability to keep track of multi-way co-occurrences opens up new possibilities and brings about interesting results." ></td>
	<td class="line x" title="142:165	The method uses a factorization model  non-negative tensor factorization  that is suitable for three way data." ></td>
	<td class="line x" title="143:165	The model is able to generalize among the data and overcome data sparseness." ></td>
	<td class="line x" title="144:165	The method has been applied to the problem of selectional preference induction." ></td>
	<td class="line x" title="145:165	The results indicate that the algorithm is able to induce selectional preferences, leading to a broad kind of frame semantics." ></td>
	<td class="line x" title="146:165	The quantitative evaluation shows that use of three-way data is clearly beneficialfortheinductionofthree-wayselectionalpreferences." ></td>
	<td class="line x" title="147:165	The tensor models outperform the simple matrix models in the pseudo-disambiguation task." ></td>
	<td class="line x" title="148:165	The results also indicate the positive effect of the non-negativity constraint: both models with non-negative constraints outperform their non-constrained counterparts." ></td>
	<td class="line x" title="149:165	The results as well as the evaluation indicate that the method presented here is a promising tool for the investigation of NLP topics, although more research and thorough evaluation are desirable." ></td>
	<td class="line x" title="150:165	There is quite some room for future work." ></td>
	<td class="line x" title="151:165	First of all, we want to further investigate the usefulness of the method for selectional preference induction." ></td>
	<td class="line x" title="152:165	This includes a deeper quantitative evaluation and a comparison to other methods for selectional preference induction." ></td>
	<td class="line x" title="153:165	We also want to include other dependency relations in our model, apart from subjects and direct objects." ></td>
	<td class="line x" title="154:165	Secondly, there is room for improvement and further research with regard to the tensor factorization model." ></td>
	<td class="line x" title="155:165	The model presented here minimizes the sum of squared distance." ></td>
	<td class="line x" title="156:165	This is, however, not the only objective function possible." ></td>
	<td class="line x" title="157:165	Another possibility is the minimization of the Kullback-Leibler divergence." ></td>
	<td class="line x" title="158:165	Minimizing the sum of squared distance assumes normally distributed data, and languagephenomenaare rarelynormally distributed." ></td>
	<td class="line x" title="159:165	Otherobjectivefunctionssuchasthe minimization of the Kullback-Leibler divergence  might be able to capture the language structures 88 much more adequately." ></td>
	<td class="line x" title="160:165	We specifically want to stress this second line of future research as one of the most promising and exciting ones." ></td>
	<td class="line x" title="161:165	Finally, the model presented here is not only suitable for selectional preference induction." ></td>
	<td class="line x" title="162:165	There are many problems in NLP that involve three-way co-occurrences." ></td>
	<td class="line x" title="163:165	In future work, we want to apply the NTF model presented here to other problems in NLP, the most important one being word sense discrimination." ></td>
	<td class="line x" title="164:165	Acknowledgements BrettBaderkindlyprovidedhisimplementationof non-negative tensor factorization for sparse matrices, from which this research has substantially benefited." ></td>
	<td class="line x" title="165:165	The three anonymous reviewers provided fruitful comments and remarks, which considerably improved the quality of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0304
Evaluating the Pairwise String Alignment of Pronunciations
Wieling, Martijn;Prokić, Jelena;Nerbonne, John;"></td>
	<td class="line x" title="1:170	Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage, Social Sciences, Humanities, and Education LaTeCH  SHELT&R 2009, pages 2634, Athens, Greece, 30 March 2009." ></td>
	<td class="line x" title="2:170	c2009 Association for Computational Linguistics Evaluatingthepairwisestringalignmentof pronunciations MartijnWieling Universityof Groningen TheNetherlands m.b.wieling@rug.nl JelenaProkic Universityof Groningen TheNetherlands j.prokic@rug.nl JohnNerbonne Universityof Groningen TheNetherlands j.nerbonne@rug.nl Abstract Pairwisestring alignment(PSA)is an important general technique for obtaining a measureof similaritybetweentwo strings, used e.g., in dialectology, historical linguistics, transliteration,and in evaluating name distinctiveness." ></td>
	<td class="line x" title="3:170	The current study focuseson evaluatingdifferentPSAmethods at the alignment level instead of via thedistancesit induces." ></td>
	<td class="line x" title="4:170	About3.5million pairwisealignmentsofBulgarianphonetic dialect data are used to compare four algorithms with a manually corrected gold standard." ></td>
	<td class="line x" title="5:170	The algorithms evaluated include three variantsof the LevenshteinalgorithmaswellasthePairHiddenMarkov Model." ></td>
	<td class="line x" title="6:170	Our results show that while all algorithms perform very well and align around 95% of all alignments correctly, thereare specificqualitative differencesin the (mis)alignmentsof the different algorithms." ></td>
	<td class="line x" title="7:170	1 Introduction Our cultural heritage is not only accessible through museums, libraries, archives and their digital portals, it is alive and well in the varied culturalhabitspracticedtodayby the variouspeoples of the world." ></td>
	<td class="line x" title="8:170	To researchand understandthis culturalheritagewerequireinstrumentswhichare sensitive to its signals,and, in particularsensitive to signals of common provenance." ></td>
	<td class="line x" title="9:170	The present paper focuses on speech habits which even today bear signals of common provenance in the various dialects of the worlds languages, and which have also been recorded and preserved in major archivesoffolkcultureinternationally." ></td>
	<td class="line x" title="10:170	Wepresent work in a research line which seeks to develop digital instrumentscapable of detecting common provenanceamongpronunciation habits,focusing in this paperon the issue of evaluatingthe quality of theseinstruments." ></td>
	<td class="line x" title="11:170	Pairwise string alignment (PSA) methods, like the popular Levenshtein algorithm (Levenshtein, 1965) which uses insertions(alignmentsof a segmentagainstagap),deletions(alignmentsofagap against a segment) and substitutions (alignments of two segments) often form the basis of determining the distance between two strings." ></td>
	<td class="line x" title="12:170	Since there are many alignmentalgorithmsand specific settings for each algorithm influencing the distance between two strings (Nerbonne and Kleiweg, 2007), evaluationis very importantin determiningthe effectivenessof the distancemethods." ></td>
	<td class="line x" title="13:170	Determining the distance (or similarity) between two phoneticstrings is an importantaspect of dialectometry, and alignmentquality is important in applications in which string alignment is a goal in itself, for example, determining if two words are likely to be cognate (Kondrak, 2003), detecting confusable drug names (Kondrak and Dorr, 2003), or determining whether a string is the transliterationof the same name from another writingsystem(Pouliquen,2008)." ></td>
	<td class="line x" title="14:170	In this paper we evaluate string distance measures on the basis of data from dialectology." ></td>
	<td class="line x" title="15:170	We thereforeexplaina bit moreof the intendeduse of the pronunciationdistancemeasure." ></td>
	<td class="line x" title="16:170	Dialect atlases normally contain a large number of pronunciationsof the sameword in various places throughout a language area." ></td>
	<td class="line x" title="17:170	All pairs of pronunciationsof correspondingwords are compared in order to obtain a measure of the aggregate linguisticdistancebetweendialectalvarieties (Heeringa, 2004)." ></td>
	<td class="line x" title="18:170	It is clearthat the qualityof the measurementis of crucialimportance." ></td>
	<td class="line x" title="19:170	Almostall evaluationmethodsin dialectometry focus on the aggregate results and ignore the individualword-pairdistancesand individualalignments on which the distances are based." ></td>
	<td class="line x" title="20:170	The focus on the aggregate distance of 100 or so word 26 pairs effectively hides many differences between methods.For example,Heeringa et al.(2006)find no significantdifferences in the degrees to which severalpairwisestringdistancemeasurescorrelate withperceptualdistanceswhenexaminedatanaggregate level." ></td>
	<td class="line x" title="22:170	Wieling et al.(2007) and Wieling and Nerbonne(2007)also reportalmostno difference betweendifferent PSA algorithmsat the aggregate level." ></td>
	<td class="line x" title="24:170	It is importantto be ableto evaluate the differenttechniquesmoresensitively, whichis why this paper examinesalignmentquality at the segmentlevel." ></td>
	<td class="line x" title="25:170	Kondrak (2003) applies a PSA algorithm to align words in different languagesin order to detect cognates automatically." ></td>
	<td class="line x" title="26:170	Exceptionally, he doesprovideanevaluationofthestringalignments generatedby differentalgorithms.But he restricts his examinationto a set of only 82 gold standard pairwisealignmentsandheonlydistinguishescorrect and incorrectalignmentsand doesnot lookat misalignedphones." ></td>
	<td class="line x" title="27:170	In the current study we introduce and evaluate several alignment algorithmsmore extensively at the alignment level." ></td>
	<td class="line x" title="28:170	The algorithms we evaluate include the Levenshtein algorithm(with syllabicity constraint), which is one of the most popular alignmentmethodsandhassuccessfullybeenused in determining pronunciationdifferences in phonetic strings (Kessler, 1995; Heeringa, 2004)." ></td>
	<td class="line x" title="29:170	In addition we look at two adaptations of the Levenshtein algorithm." ></td>
	<td class="line oc" title="30:170	The first adaptation includes theswap-operation(WagnerandLowrance,1975), whilethesecondadaptationincludesphoneticsegment distances, which are generated by applying an iterative pointwise mutual information (PMI) procedure(Churchand Hanks, 1990)." ></td>
	<td class="line x" title="31:170	Finallywe include alignments generated with the Pair HiddenMarkov Model(PHMM)as introducedto language studies by Mackay and Kondrak (2005)." ></td>
	<td class="line x" title="32:170	They reportedthatthePairHiddenMarkov Model outperformed ALINE, the best performing algorithmat the alignmentlevel in the aforementioned study of Kondrak (2003)." ></td>
	<td class="line x" title="33:170	The PHMM has also successfullybeenusedin dialectologyby Wieling et al.(2007)." ></td>
	<td class="line x" title="35:170	2 Dataset The dataset used in this study consists of 152 words collectedfrom197 sitesequallydistributed over Bulgaria." ></td>
	<td class="line x" title="36:170	The transcribed word pronunciations includediacriticsand suprasegmentals(e.g., intonation).Thetotalnumberofdifferentphonetic types(or segments)is 98.1 The gold standard pairwise alignment was automatically generated from a manually corrected gold standard set of N multiple alignments (see Prokic et al., 2009) in the followingway:  Every individual string (including gaps) in the multiple alignment is aligned with every other string of the same word." ></td>
	<td class="line x" title="37:170	With 152 words and 197 sites and in some cases more than one pronunciations per site for a certainword,thetotalnumberofpairwisealignmentsis about3.5 million." ></td>
	<td class="line x" title="38:170	 If a resulting pairwise alignment contains a gap in both strings at the same position (a gap-gap alignment),these gaps are removed fromthe pairwisealignment." ></td>
	<td class="line x" title="39:170	We justifythis, reasoning that no alignment algorithm may be expected to detect parallel deletions in a singlepairofwords." ></td>
	<td class="line x" title="40:170	Thereisnoevidencefor this in the singlepair." ></td>
	<td class="line x" title="41:170	To make this clear, considerthe multiplealignment of three Bulgarian dialectal variants of the word I (as in I am): j 'A s 'A z i j 'A Using the procedure above, the three generated pairwisealignmentsare: j 'A s j 'A s 'A z i 'A z i j 'A j 'A 3 Algorithms Four algorithmsare evaluated with respect to the quality of their alignments, including three variants of the Levenshtein algorithm and the Pair HiddenMarkov Model." ></td>
	<td class="line x" title="42:170	3.1 TheVC-sensitive Levenshteinalgorithm The Levenshtein algorithmis a very efficient dynamicprogrammingalgorithm,whichwasfirstintroducedby Kessler(1995)as a tool for computationallycomparingdialects." ></td>
	<td class="line x" title="43:170	The Levenshteindistancebetweentwo stringsis determinedby counting the minimum number of edit operations (i.e. insertions, deletions and substitutions)needed to transformone stringintothe other." ></td>
	<td class="line x" title="44:170	1The dataset is available online at the website http://www.bultreebank.org/BulDialects/ 27 For example,the Levenshteindistancebetween [j'As] and ['Azi], two Bulgarian dialectal variants of the word I (as in I am),is 3: j'As deletej 1 'As subst." ></td>
	<td class="line x" title="45:170	s/z 1 'Az inserti 1 'Azi 3 Thecorrespondingalignmentis: j 'A s 'A z i 1 1 1 The Levenshtein distance has been used frequently and successfully in measuring linguistic distancesin several languages,includingIrish (Kessler, 1995),Dutch(Heeringa, 2004)and Norwegian (Heeringa, 2004)." ></td>
	<td class="line x" title="46:170	Additionally, the Levenshtein distance has been shown to yield aggregate results that are consistent (Cronbachs  = 0.99) and valid when compared to dialect speakers judgementsof similarity(r  0.7; Heeringa et al., 2006)." ></td>
	<td class="line x" title="47:170	Following Heeringa (2004), we have adapted the Levenshteinalgorithmslightly, so that it does not allow alignments of vowels with consonants." ></td>
	<td class="line x" title="48:170	We refer to this adapted algorithm as the VCsensitive Levenshteinalgorithm." ></td>
	<td class="line x" title="49:170	3.2 TheLevenshteinalgorithmwiththeswap operation Because metathesis (i.e. transpositionof sounds) occurs relatively frequently in the Bulgarian dialect data (in 21 of 152 words), we extend the VC-sensitive Levenshtein algorithm as described insection3.1toincludetheswap-operation(Wagner and Lowrance, 1975), which allows two adjacent characters to be interchanged." ></td>
	<td class="line x" title="50:170	The swapoperationis also known as a transposition,which was introducedwith respect to detecting spelling errors by Damerau(1964)." ></td>
	<td class="line x" title="51:170	As a consequencethe Dameraudistance refers to the minimumnumber of insertions,deletions,substitutionsand transpositions required to transform one string into the other." ></td>
	<td class="line x" title="52:170	In contrastto Wagnerand Lowrance(1975) and in line with Damerau (1964) we restrict the swap operation to be only allowed for string X and Y when xi = yi+1 and yi = xi+1 (with xi beingthe token at positioni in stringX): xi xi+1 yi yi+1 >< 1 Note that a swap-operationin the alignmentis indicatedbythesymbol><." ></td>
	<td class="line x" title="53:170	Thefirstnumberfollowingthis symbolindicatesthe cost of the swapoperation." ></td>
	<td class="line x" title="54:170	Consider the alignment of [vr'7] and [v'7r],2 twoBulgariandialectalvariantsofthewordpeak (mountain)." ></td>
	<td class="line x" title="55:170	The alignment involves a swap and resultsin a totalLevenshteindistanceof 1: v r '7 v '7 r >< 1 However, the alignmentof the transcription[vr'7] withanotherdialectaltranscription[v'ar] doesnot allow a swap and yields a total Levenshtein distanceof 2: v r '7 v 'a r 1 1 Including just the option of swapping identical segments in the implementation of the Levenshtein algorithm is relatively easy." ></td>
	<td class="line x" title="56:170	We set the cost of the swap operationto one3 plus twice the cost of substituting xi with yi+1 plus twice the cost of substitutingyi with xi+1." ></td>
	<td class="line x" title="57:170	In this way the swap operationwill be preferredwhen xi = yi+1 and yi = xi+1, but not when xi negationslash= yi+1 and/or yi negationslash= xi+1." ></td>
	<td class="line x" title="58:170	In the first case the cost of the swap operation is 1, which is less than the cost of the alternative oftwo substitutions.Inthesecondcase the cost is either 3 (if xi negationslash= yi+1 or yi negationslash= xi+1) or 5 (if xi negationslash= yi+1 and yi negationslash= xi+1), which is higher than the cost of using insertions,deletionsand/or substitutions." ></td>
	<td class="line x" title="59:170	Just as in the previous section,we do not allow vowelstoalignwithconsonants(exceptinthecase of a swap)." ></td>
	<td class="line x" title="60:170	3.3 TheLevenshteinalgorithmwith generatedsegmentdistances The VC-sensitive Levenshtein algorithm as described in section 3.1 only distinguishesbetween vowels and consonants." ></td>
	<td class="line x" title="61:170	However, more sensitive segmentdistancesare alsopossible.Heeringa (2004) experimented with specifying phonetic segment distancesbased on phoneticfeaturesand 2We use transcriptions in which stress is marked on stressedvowels instead of before stressedsyllables." ></td>
	<td class="line x" title="62:170	We follow in this the Bulgarian convention insteadof the IPA convention." ></td>
	<td class="line x" title="63:170	3Actually the cost is set to 0.999 to prefer an alignment involvingaswapoveranalternativealignmentinvolvingonly regulareditoperations." ></td>
	<td class="line x" title="64:170	28 also based on acoustic differences derived from spectrograms,but he did not obtain improved resultsat the aggregate level." ></td>
	<td class="line x" title="65:170	Insteadof using segment distancesas these are (incompletely) suggested by phonetic or phonological theory, we tried to determine the sound distances automatically based on the available data." ></td>
	<td class="line oc" title="66:170	We used pointwise mutual information (PMI; Church and Hanks, 1990) to obtain these distances." ></td>
	<td class="line o" title="67:170	It generates segment distances by assessing the degree of statistical dependence betweenthe segmentsx andy: PMI(x,y) = log2 parenleftbigg p(x,y) p(x)p(y) parenrightbigg (1) Where:  p(x,y): the number of times x and y occur at the same position in two aligned strings X and Y, divided by the total number of alignedsegments(i.e.the relative occurrence of the alignedsegmentsx and y in the whole dataset)." ></td>
	<td class="line x" title="68:170	Note that eitherx or y can be a gap in the caseof insertionor deletion." ></td>
	<td class="line x" title="69:170	 p(x) and p(y): the numberof times x (or y) occurs, divided by the total number of segmentoccurrences(i.e.therelative occurrence of x or y in the whole dataset)." ></td>
	<td class="line x" title="70:170	Dividing by this term normalizesthe empiricalfrequency with respect to the frequency expected if x andy are statisticallyindependent." ></td>
	<td class="line x" title="71:170	ThegreaterthePMIvalue,themoresegmentstend to cooccurin correspondences.Negative PMIvalues indicatethat segments do not tend to cooccur in correspondences,whilepositive PMIvaluesindicatethatsegmentstendto cooccurin correspondences." ></td>
	<td class="line o" title="72:170	The segment distances can therefore be generatedbysubtractingthePMIvaluefrom0and adding the maximum PMI value (i.e. lowest distance is 0)." ></td>
	<td class="line x" title="73:170	In that way correspondingsegments obtainthe lowestdistance." ></td>
	<td class="line o" title="74:170	Based on the PMI value and its conversion to segmentdistances,we developedan iterative procedure to automatically obtain the segment distances: 1." ></td>
	<td class="line o" title="75:170	Thestringalignmentsaregeneratedusingthe VC-sensitiveLevenshteinalgorithm(seesection3.1).4 4We also used the Levenshtein algorithm without the vowel-consonantrestrictionto generate the PMI values, but this had a negative effecton the performance." ></td>
	<td class="line x" title="76:170	2." ></td>
	<td class="line x" title="77:170	The PMI value for every segmentpair is calculated according to (1) and subsequently transformed to a segment distance by subtracting it from zero and adding the maximumPMIvalue." ></td>
	<td class="line x" title="78:170	3." ></td>
	<td class="line x" title="79:170	The Levenshtein algorithm using these segment distances is applied to generate a new set of alignments." ></td>
	<td class="line x" title="80:170	4." ></td>
	<td class="line x" title="81:170	Step2and3arerepeateduntilthealignments of two consecutive iterations do not differ (i.e.convergenceis reached)." ></td>
	<td class="line x" title="82:170	The potential merit of using PMI-generated segmentdistancescanbemadeclearbythefollowing example." ></td>
	<td class="line x" title="83:170	Considerthestrings[v'7n] and[v'7k@], Bulgarian dialectalvariantsof the word outside." ></td>
	<td class="line x" title="84:170	The VC-sensitive Levenshtein algorithm yields the following(correct)alignment: v '7 n v '7  k @ 1 1 1 But alsothe alternative (incorrect)alignment: v '7 n v '7  k @ 1 1 1 The VC-sensitive Levenshtein algorithm generatestheerroneousalignmentbecauseithasnoway to identify that the consonant [n] is nearer to the consonant [] than to the consonant [k]." ></td>
	<td class="line x" title="85:170	In contrast, the Levenshtein algorithm which uses the PMI-generatedsegment distances only generates the correct first alignment,becausethe [n] occurs relatively more often aligned with [] than with [k] so that the distance between [n] and [] will be lower than the distance between [n] and [k]." ></td>
	<td class="line x" title="86:170	The idea behind this procedure is similar to Ristadssuggestiontolearnsegmentdistancesforedit distanceusing an expectationmaximizationalgorithm (Ristad and Yianilos, 1998)." ></td>
	<td class="line x" title="87:170	Our approach differs from their approach in that we only learn segmentdistancesbasedon the alignmentsgenerated by the VC-sensitive Levenshtein algorithm, while Ristad and Yianilos (1998) learn segment distancesbyconsideringallpossiblealignmentsof two strings." ></td>
	<td class="line x" title="88:170	3.4 ThePairHiddenMarkov Model The Pair Hidden Markov Model (PHMM) also generatesalignmentsbasedon automaticallygenerated segment distances and has been used suc29 Figure 1: Pair Hidden Markov Model." ></td>
	<td class="line x" title="89:170	Image courtesyof Mackayand Kondrak(2005)." ></td>
	<td class="line x" title="90:170	cessfully in language studies (Mackay and Kondrak,2005;Wielinget al., 2007)." ></td>
	<td class="line x" title="91:170	A Hidden Markov Model (HMM) is a probabilisticfinite-statetransducerthatgeneratesanobservation sequence by starting in an initial state, goingfrom state to state basedon transitionprobabilities and emitting an output symbol in each state based on the emission probabilities in that state for that output symbol(Rabiner, 1989)." ></td>
	<td class="line x" title="92:170	The PHMM was originally proposed by Durbin et al.(1998) for aligning biologicalsequencesand was first used in linguistics by Mackay and Kondrak (2005) to identify cognates." ></td>
	<td class="line x" title="94:170	The PHMM differs from the regular HMM in that it outputs two observation streams (i.e. a series of alignments of pairsof individualsegments)insteadof only a series of single symbols." ></td>
	<td class="line x" title="95:170	The PHMM displayed in Figure1 has threeemittingstates: the substitution (match)state(M) whichemitstwo alignedsymbols, the insertionstate (Y) whichemitsa symbol and a gap, and the deletionstate (X) which emits a gap and a symbol." ></td>
	<td class="line x" title="96:170	The following example shows the state sequenceforthepronunciations[j'As] and['Azi] (EnglishI): j 'A s 'A z i X M M Y Before generatingthe alignments,all probabilities of the PHMM have to be estimated." ></td>
	<td class="line x" title="97:170	These probabilitiesconsist of the 5 transition probabilities shown in Figure 1: epsilon1, , , XY and M. In additionthereare98emissionprobabilitiesforthe insertion state and the deletion state (one for every segment) and 9604 emission probabilitiesfor thesubstitutionstate." ></td>
	<td class="line x" title="98:170	Theprobabilityofstartingin oneofthethreestatesissetequaltotheprobability of goingfromthesubstitutionstateto thatparticularstate." ></td>
	<td class="line x" title="99:170	TheBaum-Welchexpectationmaximization algorithm(Baum et al., 1970) can be used to iterativelyreestimatetheseprobabilitiesuntila local optimumis found." ></td>
	<td class="line x" title="100:170	To prevent order effects in training,every word pair is consideredtwice (e.g., wa wb and wb  wa)." ></td>
	<td class="line x" title="101:170	Theresultinginsertionanddeletionprobabilitiesarethereforethesame(foreachsegment),and the probabilityof substitutingx for y is equal to the probabilityof substitutingy for x, effectively yielding4802distinctsubstitutionprobabilities." ></td>
	<td class="line x" title="102:170	Wieling et al.(2007) showed that using Dutch dialect data for training, sensible segment distances were obtained; acoustic vowel distances on the basis of spectrograms correlated significantly (r = 0.72) with the vowel substitution probabilitiesof the PHMM." ></td>
	<td class="line x" title="104:170	Additionally, probabilities of substituting a symbol with itself were much higher than the probabilities of substituting an arbitrary vowel with another non-identical vowel (mutatis mutandis for consonants), which were in turn muchhigherthan the probabilitiesof substitutinga vowel for a consonant." ></td>
	<td class="line x" title="105:170	Aftertraining,thewellknownViterbialgorithm canbeusedtoobtainthebestalignments(Rabiner, 1989)." ></td>
	<td class="line x" title="106:170	4 Evaluation As described in section 2, we use the generated pairwisealignmentsfromagoldstandardofmultiplealignmentsforevaluation.Inaddition,welook at theperformanceof a baselineof pairwisealignments,whichisconstructedbyaligningthestrings according to the Hamming distance (i.e. only allowingsubstitutionsandnoinsertionsordeletions; Hamming,1950)." ></td>
	<td class="line x" title="107:170	Theevaluationprocedureconsistsofcomparing the alignments of the previously discussed algorithms includingthe baselinewith the alignments of the gold standard." ></td>
	<td class="line x" title="108:170	For the comparisonwe use the standard Levenshtein algorithm without any restrictions.Theevaluationproceedsas follows: 1." ></td>
	<td class="line x" title="109:170	The pairwise alignments of the four algorithms,thebaselineandthegoldstandardare generatedand standardized(see section4.1)." ></td>
	<td class="line x" title="110:170	When multiple equal-scoringalignmentsare 30 generatedby an algorithm,only one (i.e. the final)alignmentis selected." ></td>
	<td class="line x" title="111:170	2." ></td>
	<td class="line x" title="112:170	In each alignment, we convert each pair of alignedsegmentstoasingletoken,sothatevery alignmentof two stringsis convertedto a singlestringof segmentpairs." ></td>
	<td class="line x" title="113:170	3." ></td>
	<td class="line x" title="114:170	Foreveryalgorithmthesetransformedstrings are aligned with the transformed strings of the gold standard using the standard Levenshteinalgorithm." ></td>
	<td class="line x" title="115:170	4." ></td>
	<td class="line x" title="116:170	The Levenshtein distances for all these strings are summed up resulting in the total distance between every alignment algorithm and the gold standard." ></td>
	<td class="line x" title="117:170	Only if individual segmentsmatchcompletelythe segmentdistanceis 0, otherwiseit is 1." ></td>
	<td class="line x" title="118:170	Toillustratethisprocedure,considerthefollowing goldstandardalignmentof[vl'7k] and[v'7lk], two Bulgariandialectalvariantsof the word wolf: v l '7 k v '7 l k Everyalignedsegmentpairisconvertedtoasingle token by adding the symbol / between the segments and using the symbol - to indicate a gap." ></td>
	<td class="line x" title="119:170	Thisyieldsthe followingtransformedstring: v/v l/'7 '7/l k/k Suppose another algorithm generates the following alignment(notdetectingthe swap): v l '7 k v '7 l k Thetransformedstringfor this alignmentis: v/v l/'7/'7 -/l k/k To evaluate this alignment,we align this string to thetransformedstringofthegoldstandardandobtaina Levenshteindistanceof 3: v/v l/'7 '7/l k/k v/v l/'7/'7 -/l k/k 1 1 1 By repeatingthisprocedurefor all alignmentsand summing up all distances, we obtain total distancesbetweenthe gold standardand every alignmentalgorithm." ></td>
	<td class="line x" title="120:170	Algorithmswhichgeneratehighquality alignmentswill have a low distance from thegoldstandard,whilethedistancewillbehigher for algorithms which generate low-quality alignments." ></td>
	<td class="line x" title="121:170	4.1 Standardization The gold standard contains a number of alignments which have alternative equivalent alignments, most notably an alignment containing an insertion followed by a deletion (which is equal to the deletion followed by the insertion), or an alignmentcontaininga syllabicconsonantsuchas [''], which in fact matches both a vowel and a neighboringr-like consonantand can thereforebe alignedwith eitherthe vowel or the consonant." ></td>
	<td class="line x" title="122:170	In order to prevent punishing the algorithms which do not match the exact gold standard in these cases, the alignmentsof the gold standardand all alignmentalgorithmsare transformedto one standardformin all relevant cases." ></td>
	<td class="line x" title="123:170	For example, considerthe correct alignmentof [v'iA] and [v'ij], two Bulgariandialectalvariations of the Englishpluralpronounyou: v 'i A v 'i j Of course,this alignmentis as reasonableas: v 'i A v 'i j To avoid punishing the first, we transform all insertions followed by deletions to deletions followed by insertions, effectively scoring the two alignmentsthe same." ></td>
	<td class="line x" title="124:170	For the syllabic consonants we transform all alignments to a form in which the syllabic consonant is followed by a gap and not vice versa." ></td>
	<td class="line x" title="125:170	For instance,aligning[v''x] with[v'Arx] (English: peak)yields: v '' x v 'A r x Whichis transformedto the equivalentalignment: v '' x v 'A r x 5 Results We will report both quantitative results using the evaluation method discussed in the previous section, as well as the qualitative results, where we focusoncharacteristicerrorsofthedifferentalignmentalgorithms." ></td>
	<td class="line x" title="126:170	5.1 Quantitative results Becausethere are two algorithmswhich use generatedsegmentdistances(or probabilities)in their alignments,we first check if these values are sensibleand comparableto eachother." ></td>
	<td class="line o" title="127:170	31 5.1.1 Comparisonof segmentdistances With respect to the PMI results (convergence was reached after 7 iterations, taking less than 5 CPU minutes), we indeed found sensible results: the averagedistancebetweenidenticalsymbols was significantlylower than the distancebetween pairs of different vowels and consonants (t < 13,p < .001)." ></td>
	<td class="line x" title="128:170	Because we did not allow vowel-consonants alignments in the Levenshtein algorithm,noPMIvaluesweregeneratedforthose segmentpairs." ></td>
	<td class="line x" title="129:170	Just as Wieling et al.(2007), we found sensible PHMM substitution probabilities (convergence was reached after 1675 iterations, taking about 7 CPU hours): the probabilityof matching a symbol with itself was significantlyhigher than the probability of substituting one vowel for another(similarlyforconsonants),whichinturnwas higherthanthe probabilityof substitutinga vowel witha consonant(allts > 9,p < .001)." ></td>
	<td class="line o" title="131:170	To allow a fair comparisonbetweenthePHMM probabilities and the PMI distances, we transformed the PHMM probabilities to log-odds scores (i.e. dividing the probability by the relative frequency of the segments and subsequently taking the log)." ></td>
	<td class="line x" title="132:170	Because the residues after the linear regression between the PHMM similarities and PMI distanceswere not normallydistributed, we used Spearmans rank correlation coefficient to assess the relationship between the two variables." ></td>
	<td class="line x" title="133:170	We found a highly significant Spearmans  = .965(p < .001), which means that the relationshipbetweenthePHMMsimilaritiesandthe PMIdistancesis verystrong." ></td>
	<td class="line x" title="134:170	Whenlookingat the insertionsanddeletionswealsofoundasignificant relationship:Spearmans  = .736(p < .001)." ></td>
	<td class="line x" title="135:170	5.1.2 Evaluationagainstthegoldstandard Usingtheproceduredescribedinsection4,wecalculated the distances between the gold standard and the alignment algorithms." ></td>
	<td class="line x" title="136:170	Besides reporting the totalnumberof misalignedtokens,we alsodivided this number by the total number of aligned segments in the gold standard (about 16 million) to get an idea of the error rate." ></td>
	<td class="line x" title="137:170	Note that the error rateis0intheperfectcase,butmightrisetonearly 2 in theworstcase,whichis an alignmentconsisting of only insertionsand deletionsand therefore up to twice as long as the alignmentsin the gold standard." ></td>
	<td class="line x" title="138:170	Finally, we also report the total number of alignments(word pairs) which are not exactly equalto the alignmentsof the goldstandard." ></td>
	<td class="line x" title="139:170	The results are shown in Table 1." ></td>
	<td class="line x" title="140:170	We can clearly see that all algorithms beat the baseline and align about 95% of all string pairs correctly." ></td>
	<td class="line x" title="141:170	Whilethe LevenshteinPMIalgorithmalignsmost strings perfectly, it misaligns slightly more individual segments than the PHMM and the Levenshtein algorithm with the swap operation (i.e. it makes more segment alignment errors per word pair)." ></td>
	<td class="line x" title="142:170	The VC-sensitive Levenshtein algorithm in general performs slightly worse than the other threealgorithms." ></td>
	<td class="line x" title="143:170	5.2 Qualitative results Let us first note that it is almost impossible for anyalgorithmtoachieveaperfectoverlapwiththe goldstandard,becausethe goldstandardwas generated from multiplealignmentsand thereforeincorporatesotherconstraints.For example,while a certainpairwisealignmentcouldappearcorrectin aligning two consonants, the multiple alignment could show contextual support (from pronunciations in other varieties) for separating the consonants." ></td>
	<td class="line x" title="144:170	Consequently, all algorithmsdiscussedbelow make errorsof this kind." ></td>
	<td class="line x" title="145:170	In general, the specific errors of the VCsensitive Levenshtein algorithm can be separated into three cases." ></td>
	<td class="line x" title="146:170	First, as we illustratedin section 3.3, the VC-sensitive Levenshtein algorithm has no way to distinguish between aligning a consonant with one of two neighboringconsonantsand sometimeschoosesthe wrongone (thisalso holds for vowels)." ></td>
	<td class="line x" title="147:170	Second,it does not allow alignments of vowels with consonants and therefore cannot detectcorrectvowel-consonantalignmentssuchas correspondencesof [u] with [v] initially." ></td>
	<td class="line x" title="148:170	Third, for the same reason the VC-sensitive Levenshtein algorithm is also not able to detect metathesis of vowelswithconsonants." ></td>
	<td class="line x" title="149:170	The misalignments of the Levenshtein algorithm with the swap-operationcan also be split in three cases." ></td>
	<td class="line x" title="150:170	It suffers from the same two problemsastheVC-sensitive Levenshteinalgorithmin choosingtoaligna consonantincorrectlywithone of two neighboringconsonantsand not beingable to align a vowel with a consonant." ></td>
	<td class="line x" title="151:170	Third, even thoughit alignssome of the metathesiscases correctly, it alsomakessomeerrorsbyincorrectlyapplyingthe swap-operation.For example,consider the alignment of [s'irjIni] and [s'irjnI], two Bulgariandialectalvariationsof the word cheese,in whichthe swap-operationis applied: 32 Algorithm Misalignedsegments(error rate) Incorrectalignments(%) Baseline(Hammingalgorithm) 2510094 (0.1579) 726844 (20.92%) VC-sens.Levenshteinalgorithm 490703 (0.0309) 191674 (5.52%) LevenshteinPMIalgorithm 399216 (0.0251) 156440 (4.50%) Levenshteinswap algorithm 392345 (0.0247) 161834 (4.66%) Pair HiddenMarkov Model 362423 (0.0228) 160896 (4.63%) Table1: Comparisonto goldstandardalignments.All differencesare significant(p < 0.01)." ></td>
	<td class="line x" title="152:170	s 'i rj I n i s 'i rj n I 0 0 0 >< 1 1 However, thetwo Isarenotrelatedandshouldnot beswapped,whichisreflectedinthegoldstandard alignment: s 'i rj I n i s 'i rj n I 0 0 0 1 0 1 The incorrect alignments of the Levenshtein algorithm with the PMI-generated segment distances are mainly caused by its inability to align vowels with consonantsand therefore, just as the VC-sensitive Levenshteinalgorithm,it fails to detect metathesis." ></td>
	<td class="line x" title="153:170	On the other hand, using segmentdistancesoftensolves the problemof selecting whichof two plausibleneighborsa consonant shouldbe alignedwith." ></td>
	<td class="line x" title="154:170	Because the PHMM employs segment substitution probabilities,it also often solves the problem of aligning a consonantto one of two neighbors." ></td>
	<td class="line x" title="155:170	In addition, the PHMM often correctly aligns metathesisinvolving equal as well as similar symbols,even realizingan improvement over the Levenshtein swap algorithm." ></td>
	<td class="line x" title="156:170	Unfortunately, many wrong alignments of the PHMM are also caused by allowing vowel-consonantalignments." ></td>
	<td class="line x" title="157:170	Since the PHMM does not take context into account,it also alignsvowels and consonantswhich oftenplaya rolein metathesiswhennometathesis is involved." ></td>
	<td class="line x" title="158:170	6 Discussion This study provides an alternative evaluation of stringdistancealgorithmsby focusingon theireffectiveness in aligning segments." ></td>
	<td class="line x" title="159:170	We proposed, implemented,and tested the new procedure on a substantialbodyof data." ></td>
	<td class="line x" title="160:170	Thisprovidesa new perspective on the quality of distance and alignment algorithmsas they have beenusedin dialectology, where aggregate comparisons had been at times frustratinglyinconclusive." ></td>
	<td class="line p" title="161:170	In addition, we introduced the PMI weighting within the Levenshtein algorithm as a simple means of obtaining segment distances, and showed that it improves on the popular Levenshtein algorithm with respect to alignment accuracy." ></td>
	<td class="line x" title="162:170	WhiletheresultsindicatedthatthePHMMmisaligned the fewest segments, training the PHMM is a lengthy process lasting several hours." ></td>
	<td class="line x" title="163:170	Considering that the Levenshtein algorithm with the swap operation and the Levenshtein algorithm with the PMI-generated segment distances are much quicker to (train and) apply, and that they have onlyslightlylowerperformancewithrespect to the segment alignments,we actuallyprefer using those methods." ></td>
	<td class="line x" title="164:170	Anotherargument in favor of using one of these Levenshtein algorithmsis that it is a prioriclearerwhat type of alignmenterrors to expect from them, while the PHMMalgorithm is lesspredictableand harderto comprehend." ></td>
	<td class="line x" title="165:170	While our results are an indicationof the good qualityoftheevaluatedalgorithms,weonlyevaluatedthe algorithmson a singledatasetfor whicha goldstandardwasavailable." ></td>
	<td class="line x" title="166:170	Ideallywewouldlike to verify theseresultson otherdatasets,for which gold standards consisting of multiple or pairwise alignmentsare available." ></td>
	<td class="line x" title="167:170	Acknowledgements We aregratefulto PeterKleiweg forextendingthe LevenshteinalgorithmintheL04packagewiththe swap-operation." ></td>
	<td class="line x" title="168:170	We also thank Greg Kondrakfor providingtheoriginalsourcecodeofthePairHidden Markov Models." ></td>
	<td class="line x" title="169:170	Finally, we thank Therese Leinonenand SebastianKurschnerof the Universityof GroningenandEsteve Valls i Alechaof the University of Barcelonafor their useful feedback on our ideas." ></td>
	<td class="line x" title="170:170	33" ></td>
</tr></table>
</div
</body></html>
