<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
P02-1053 <div class="dstPaperTitle">Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews</div><div class="dstPaperAuthors">Turney, Peter D.;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W02-1011
Thumbs Up? Sentiment Classification Using Machine Learning Techniques
Pang, Bo;Lee, Lillian;Vaithyanathan, Shivakumar;"></td>
	<td class="line x" title="1:181	Thumbs up?" ></td>
	<td class="line x" title="2:181	Sentiment Classiflcation using Machine Learning Techniques Bo Pang and Lillian Lee Department of Computer Science Cornell University Ithaca, NY 14853 USA fpabo,lleeg@cs.cornell.edu Shivakumar Vaithyanathan IBM Almaden Research Center 650 Harry Rd. San Jose, CA 95120 USA shiv@almaden.ibm.com Abstract We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative." ></td>
	<td class="line x" title="3:181	Using movie reviews as data, we flnd that standard machine learning techniques deflnitively outperform human-produced baselines." ></td>
	<td class="line x" title="4:181	However, the three machine learning methods we employed (Naive Bayes, maximum entropy classiflcation, and support vector machines) do not perform as well on sentiment classiflcation as on traditional topic-based categorization." ></td>
	<td class="line x" title="5:181	We conclude by examining factors that make the sentiment classiflcation problem more challenging." ></td>
	<td class="line x" title="6:181	1 Introduction Today, very large amounts of information are available in on-line documents." ></td>
	<td class="line x" title="7:181	As part of the efiort to better organize this information for users, researchers have been actively investigating the problem of automatic text categorization." ></td>
	<td class="line x" title="8:181	The bulk of such work has focused on topical categorization, attempting to sort documents according to their subject matter (e.g. , sports vs. politics)." ></td>
	<td class="line x" title="9:181	However, recent years have seen rapid growth in on-line discussion groups and review sites (e.g. , the New York Times Books web page) where a crucial characteristic of the posted articles is their sentiment, or overall opinion towards the subject matter | for example, whether a product review is positive or negative." ></td>
	<td class="line x" title="10:181	Labeling these articles with their sentiment would provide succinct summaries to readers; indeed, these labels are part of the appeal and value-add of such sites as www.rottentomatoes.com, which both labels movie reviews that do not contain explicit rating indicators and normalizes the difierent rating schemes that individual reviewers use." ></td>
	<td class="line x" title="11:181	Sentiment classiflcation would also be helpful in business intelligence applications (e.g. MindfulEyes Lexant system1) and recommender systems (e.g. , Terveen et al.(1997), Tatemura (2000)), where user input and feedback could be quickly summarized; indeed, in general, free-form survey responses given in natural language format could be processed using sentiment categorization." ></td>
	<td class="line x" title="13:181	Moreover, there are also potential applications to message flltering; for example, one might be able to use sentiment information to recognize and discard \ ames'(Spertus, 1997)." ></td>
	<td class="line x" title="14:181	In this paper, we examine the efiectiveness of applying machine learning techniques to the sentiment classiflcation problem." ></td>
	<td class="line x" title="15:181	A challenging aspect of this problem that seems to distinguish it from traditional topic-based classiflcation is that while topics are often identiflable by keywords alone, sentiment can be expressed in a more subtle manner." ></td>
	<td class="line x" title="16:181	For example, the sentence \How could anyone sit through this movie?' contains no single word that is obviously negative." ></td>
	<td class="line x" title="17:181	(See Section 7 for more examples)." ></td>
	<td class="line x" title="18:181	Thus, sentiment seems to require more understanding than the usual topic-based classiflcation." ></td>
	<td class="line x" title="19:181	So, apart from presenting our results obtained via machine learning techniques, we also analyze the problem to gain a better understanding of how dicult it is. 2 Previous Work This section brie y surveys previous work on nontopic-based text categorization." ></td>
	<td class="line x" title="20:181	One area of research concentrates on classifying documents according to their source or source style, with statistically-detected stylistic variation (Biber, 1988) serving as an important cue." ></td>
	<td class="line x" title="21:181	Examples include author, publisher (e.g. , the New York Times vs. The Daily News), native-language background, and \brow' (e.g. , high-brow vs. \popular', or low-brow) (Mosteller and Wallace, 1984; Argamon-Engelson et 1http://www.mindfuleye.com/about/lexant.htm Association for Computational Linguistics." ></td>
	<td class="line x" title="22:181	Language Processing (EMNLP), Philadelphia, July 2002, pp." ></td>
	<td class="line x" title="23:181	79-86." ></td>
	<td class="line x" title="24:181	Proceedings of the Conference on Empirical Methods in Natural al. , 1998; Tomokiyo and Jones, 2001; Kessler et al. , 1997)." ></td>
	<td class="line x" title="25:181	Another, more related area of research is that of determining the genre of texts; subjective genres, such as \editorial', are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al. , 1997; Finn et al. , 2002)." ></td>
	<td class="line x" title="26:181	Other work explicitly attempts to flnd features indicating that subjective language is being used (Hatzivassiloglou and Wiebe, 2000; Wiebe et al. , 2001)." ></td>
	<td class="line x" title="27:181	But, while techniques for genre categorization and subjectivity detection can help us recognize documents that express an opinion, they do not address our speciflc classiflcation task of determining what that opinion actually is. Most previous research on sentiment-based classiflcation has been at least partially knowledge-based." ></td>
	<td class="line oc" title="28:181	Some of this work focuses on classifying the semantic orientation of individual words or phrases, using linguistic heuristics or a pre-selected set of seed words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2002)." ></td>
	<td class="line x" title="29:181	Past work on sentiment-based categorization of entire documents has often involved either the use of models inspired by cognitive linguistics (Hearst, 1992; Sack, 1994) or the manual or semi-manual construction of discriminant-word lexicons (Huettner and Subasic, 2000; Das and Chen, 2001; Tong, 2001)." ></td>
	<td class="line x" title="30:181	Interestingly, our baseline experiments, described in Section 4, show that humans may not always have the best intuition for choosing discriminating words." ></td>
	<td class="line oc" title="31:181	Turneys (2002) work on classiflcation of reviews is perhaps the closest to ours.2 He applied a speciflc unsupervised learning technique based on the mutual information between document phrases and the words \excellent' and \poor', where the mutual information is computed using statistics gathered by a search engine." ></td>
	<td class="line x" title="32:181	In contrast, we utilize several completely prior-knowledge-free supervised machine learning methods, with the goal of understanding the inherent diculty of the task." ></td>
	<td class="line x" title="33:181	3 The Movie-Review Domain For our experiments, we chose to work with movie reviews." ></td>
	<td class="line x" title="34:181	This domain is experimentally convenient because there are large on-line collections of such reviews, and because reviewers often summarize their overall sentiment with a machine-extractable rating indicator, such as a number of stars; hence, we did not need to hand-label the data for supervised learning or evaluation purposes." ></td>
	<td class="line oc" title="35:181	We also note that Turney (2002) found movie reviews to be the most 2Indeed, although our choice of title was completely independent of his, our selections were eerily similar." ></td>
	<td class="line x" title="36:181	dicult of several domains for sentiment classiflcation, reporting an accuracy of 65.83% on a 120document set (random-choice performance: 50%)." ></td>
	<td class="line x" title="37:181	But we stress that the machine learning methods and features we use are not speciflc to movie reviews, and should be easily applicable to other domains as long as sucient training data exists." ></td>
	<td class="line x" title="38:181	Our data source was the Internet Movie Database (IMDb) archive of the rec.arts.movies.reviews newsgroup.3 We selected only reviews where the author rating was expressed either with stars or some numerical value (other conventions varied too widely to allow for automatic processing)." ></td>
	<td class="line x" title="39:181	Ratings were automatically extracted and converted into one of three categories: positive, negative, or neutral." ></td>
	<td class="line x" title="40:181	For the work described in this paper, we concentrated only on discriminating between positive and negative sentiment." ></td>
	<td class="line x" title="41:181	To avoid domination of the corpus by a small number of proliflc reviewers, we imposed a limit of fewer than 20 reviews per author per sentiment category, yielding a corpus of 752 negative and 1301 positive reviews, with a total of 144 reviewers represented." ></td>
	<td class="line x" title="42:181	This dataset will be available on-line at http://www.cs.cornell.edu/people/pabo/movie-review-data/ (the URL contains hyphens only around the word \review')." ></td>
	<td class="line x" title="43:181	4 A Closer Look At the Problem Intuitions seem to difier as to the diculty of the sentiment detection problem." ></td>
	<td class="line x" title="44:181	An expert on using machine learning for text categorization predicted relatively low performance for automatic methods." ></td>
	<td class="line x" title="45:181	On the other hand, it seems that distinguishing positive from negative reviews is relatively easy for humans, especially in comparison to the standard text categorization problem, where topics can be closely related." ></td>
	<td class="line x" title="46:181	One might also suspect that there are certain words people tend to use to express strong sentiments, so that it might suce to simply produce a list of such words by introspection and rely on them alone to classify the texts." ></td>
	<td class="line x" title="47:181	To test this latter hypothesis, we asked two graduate students in computer science to (independently) choose good indicator words for positive and negative sentiments in movie reviews." ></td>
	<td class="line x" title="48:181	Their selections, shown in Figure 1, seem intuitively plausible." ></td>
	<td class="line x" title="49:181	We then converted their responses into simple decision procedures that essentially count the number of the proposed positive and negative words in a given document." ></td>
	<td class="line x" title="50:181	We applied these procedures to uniformlydistributed data, so that the random-choice baseline result would be 50%." ></td>
	<td class="line x" title="51:181	As shown in Figure 1, the 3http://reviews.imdb.com/Reviews/ Proposed word lists Accuracy Ties Human 1 positive: dazzling, brilliant, phenomenal, excellent, fantastic 58% 75% negative: suck, terrible, awful, unwatchable, hideous Human 2 positive: gripping, mesmerizing, riveting, spectacular, cool, 64% 39% awesome, thrilling, badass, excellent, moving, exciting negative: bad, cliched, sucks, boring, stupid, slow Figure 1: Baseline results for human word lists." ></td>
	<td class="line x" title="52:181	Data: 700 positive and 700 negative reviews." ></td>
	<td class="line x" title="53:181	Proposed word lists Accuracy Ties Human 3 + stats positive: love, wonderful, best, great, superb, still, beautiful 69% 16% negative: bad, worst, stupid, waste, boring, ?, ! Figure 2: Results for baseline using introspection and simple statistics of the data (including test data)." ></td>
	<td class="line x" title="54:181	accuracy | percentage of documents classifled correctly | for the human-based classiflers were 58% and 64%, respectively.4 Note that the tie rates | percentage of documents where the two sentiments were rated equally likely | are quite high5 (we chose a tie breaking policy that maximized the accuracy of the baselines)." ></td>
	<td class="line x" title="55:181	While the tie rates suggest that the brevity of the human-produced lists is a factor in the relatively poor performance results, it is not the case that size alone necessarily limits accuracy." ></td>
	<td class="line x" title="56:181	Based on a very preliminary examination of frequency counts in the entire corpus (including test data) plus introspection, we created a list of seven positive and seven negative words (including punctuation), shown in Figure 2." ></td>
	<td class="line x" title="57:181	As that flgure indicates, using these words raised the accuracy to 69%." ></td>
	<td class="line x" title="58:181	Also, although this third list is of comparable length to the other two, it has a much lower tie rate of 16%." ></td>
	<td class="line x" title="59:181	We further observe that some of the items in this third list, such as \'?" ></td>
	<td class="line x" title="60:181	or \still', would probably not have been proposed as possible candidates merely through introspection, although upon re ection one sees their merit (the question mark tends to occur in sentences like \What was the director thinking?'; \still' appears in sentences like \Still, though, it was worth seeing')." ></td>
	<td class="line x" title="61:181	We conclude from these preliminary experiments that it is worthwhile to explore corpus-based techniques, rather than relying on prior intuitions, to select good indicator features and to perform sentiment classiflcation in general." ></td>
	<td class="line x" title="62:181	These experiments also provide us with baselines for experimental comparison; in particular, the third baseline of 69% might actually be considered somewhat dicult to beat, since it was achieved by examination of the test data (although our examination was rather cursory; we do 4Later experiments using these words as features for machine learning methods did not yield better results." ></td>
	<td class="line x" title="63:181	5This is largely due to 0-0 ties." ></td>
	<td class="line x" title="64:181	not claim that our list was the optimal set of fourteen words)." ></td>
	<td class="line x" title="65:181	5 Machine Learning Methods Our aim in this work was to examine whether it sufflces to treat sentiment classiflcation simply as a special case of topic-based categorization (with the two \topics' being positive sentiment and negative sentiment), or whether special sentiment-categorization methods need to be developed." ></td>
	<td class="line x" title="66:181	We experimented with three standard algorithms: Naive Bayes classiflcation, maximum entropy classiflcation, and support vector machines." ></td>
	<td class="line x" title="67:181	The philosophies behind these three algorithms are quite difierent, but each has been shown to be efiective in previous text categorization studies." ></td>
	<td class="line x" title="68:181	To implement these machine learning algorithms on our document data, we used the following standard bag-of-features framework." ></td>
	<td class="line x" title="69:181	Let ff1;:::;fmg be a predeflned set of m features that can appear in a document; examples include the word \still' or the bigram \really stinks'." ></td>
	<td class="line x" title="70:181	Let ni(d) be the number of times fi occurs in document d. Then, each document d is represented by the document vector ~d := (n1(d);n2(d);:::;nm(d))." ></td>
	<td class="line x" title="71:181	5.1 Naive Bayes One approach to text classiflcation is to assign to a given document d the class c = argmaxc P(c j d)." ></td>
	<td class="line x" title="72:181	We derive the Naive Bayes (NB) classifler by flrst observing that by Bayes rule, P(c j d) = P(c)P(d j c)P(d) ; where P(d) plays no role in selecting c." ></td>
	<td class="line x" title="73:181	To estimate the term P(d j c), Naive Bayes decomposes it by assuming the fis are conditionally independent given ds class: PNB(c j d) := P(c) Qm i=1 P(fi j c) ni(d) P(d) : Our training method consists of relative-frequency estimation of P(c) and P(fi j c), using add-one smoothing." ></td>
	<td class="line x" title="74:181	Despite its simplicity and the fact that its conditional independence assumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features." ></td>
	<td class="line x" title="75:181	On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next." ></td>
	<td class="line x" title="76:181	5.2 Maximum Entropy Maximum entropy classiflcation (MaxEnt, or ME, for short) is an alternative technique which has proven efiective in a number of natural language processing applications (Berger et al. , 1996)." ></td>
	<td class="line x" title="77:181	Nigam et al.(1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classiflcation." ></td>
	<td class="line x" title="79:181	Its estimate of P(c j d) takes the following exponential form: PME(c j d) := 1Z(d) exp X i i;cFi;c(d;c) ! ; where Z(d) is a normalization function." ></td>
	<td class="line x" title="80:181	Fi;c is a feature/class function for feature fi and class c, deflned as follows:6 Fi;c(d;c0) := n1; n i(d) > 0 and c0 = c 0 otherwise : For instance, a particular feature/class function might flre if and only if the bigram \still hate' appears and the documents sentiment is hypothesized to be negative.7 Importantly, unlike Naive Bayes, MaxEnt makes no assumptions about the relationships between features, and so might potentially perform better when conditional independence assumptions are not met." ></td>
	<td class="line x" title="81:181	The i;cs are feature-weight parameters; inspection of the deflnition of PME shows that a large i;c means that fi is considered a strong indicator for 6We use a restricted deflnition of feature/class functions so that MaxEnt relies on the same sort of feature information as Naive Bayes." ></td>
	<td class="line x" title="82:181	7The dependence on class is necessary for parameter induction." ></td>
	<td class="line x" title="83:181	See Nigam et al.(1999) for additional motivation." ></td>
	<td class="line x" title="85:181	class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classiflers name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense." ></td>
	<td class="line x" title="86:181	We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al. , 1997) for parameter training (this was a sucient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfltting (Chen and Rosenfeld, 2000)." ></td>
	<td class="line x" title="87:181	5.3 Support Vector Machines Support vector machines (SVMs) have been shown to be highly efiective at traditional text categorization, generally outperforming Naive Bayes (Joachims, 1998)." ></td>
	<td class="line x" title="88:181	They are large-margin, rather than probabilistic, classiflers, in contrast to Naive Bayes and MaxEnt." ></td>
	<td class="line x" title="89:181	In the two-category case, the basic idea behind the training procedure is to flnd a hyperplane, represented by vector ~w, that not only separates the document vectors in one class from those in the other, but for which the separation, or margin, is as large as possible." ></td>
	<td class="line x" title="90:181	This search corresponds to a constrained optimization problem; letting cj 2 f1;1g (corresponding to positive and negative) be the correct class of document dj, the solution can be written as ~w := X j fijcj ~dj; fij  0; where the fijs are obtained by solving a dual optimization problem." ></td>
	<td class="line x" title="91:181	Those ~dj such that fij is greater than zero are called support vectors, since they are the only document vectors contributing to ~w. Classiflcation of test instances consists simply of determining which side of ~ws hyperplane they fall on." ></td>
	<td class="line x" title="92:181	We used Joachims (1999) SVMlight package8 for training and testing, with all parameters set to their default values, after flrst length-normalizing the document vectors, as is standard (neglecting to normalize generally hurt performance slightly)." ></td>
	<td class="line x" title="93:181	6 Evaluation 6.1 Experimental Set-up We used documents from the movie-review corpus described in Section 3." ></td>
	<td class="line x" title="94:181	To create a data set with uniform class distribution (studying the efiect of skewed 8http://svmlight.joachims.org Features # of frequency or NB ME SVM features presence?" ></td>
	<td class="line x" title="95:181	(1) unigrams 16165 freq." ></td>
	<td class="line x" title="96:181	78.7 N/A 72.8 (2) unigrams ' pres." ></td>
	<td class="line x" title="97:181	81.0 80.4 82.9 (3) unigrams+bigrams 32330 pres." ></td>
	<td class="line x" title="98:181	80.6 80.8 82.7 (4) bigrams 16165 pres." ></td>
	<td class="line x" title="99:181	77.3 77.4 77.1 (5) unigrams+POS 16695 pres." ></td>
	<td class="line x" title="100:181	81.5 80.4 81.9 (6) adjectives 2633 pres." ></td>
	<td class="line x" title="101:181	77.0 77.7 75.1 (7) top 2633 unigrams 2633 pres." ></td>
	<td class="line x" title="102:181	80.3 81.0 81.4 (8) unigrams+position 22430 pres." ></td>
	<td class="line x" title="103:181	81.0 80.1 81.6 Figure 3: Average three-fold cross-validation accuracies, in percent." ></td>
	<td class="line x" title="104:181	Boldface: best performance for a given setting (row)." ></td>
	<td class="line x" title="105:181	Recall that our baseline results ranged from 50% to 69%." ></td>
	<td class="line x" title="106:181	class distributions was out of the scope of this study), we randomly selected 700 positive-sentiment and 700 negative-sentiment documents." ></td>
	<td class="line x" title="107:181	We then divided this data into three equal-sized folds, maintaining balanced class distributions in each fold." ></td>
	<td class="line x" title="108:181	(We did not use a larger number of folds due to the slowness of the MaxEnt training procedure)." ></td>
	<td class="line x" title="109:181	All results reported below, as well as the baseline results from Section 4, are the average three-fold cross-validation results on this data (of course, the baseline algorithms had no parameters to tune)." ></td>
	<td class="line x" title="110:181	To prepare the documents, we automatically removed the rating indicators and extracted the textual information from the original HTML document format, treating punctuation as separate lexical items." ></td>
	<td class="line x" title="111:181	No stemming or stoplists were used." ></td>
	<td class="line x" title="112:181	One unconventional step we took was to attempt to model the potentially important contextual efiect of negation: clearly \good' and \not very good' indicate opposite sentiment orientations." ></td>
	<td class="line x" title="113:181	Adapting a technique of Das and Chen (2001), we added the tag NOT to every word between a negation word (\not', \isnt', \didnt', etc)." ></td>
	<td class="line x" title="114:181	and the flrst punctuation mark following the negation word." ></td>
	<td class="line x" title="115:181	(Preliminary experiments indicate that removing the negation tag had a negligible, but on average slightly harmful, effect on performance.)" ></td>
	<td class="line x" title="116:181	For this study, we focused on features based on unigrams (with negation tagging) and bigrams." ></td>
	<td class="line x" title="117:181	Because training MaxEnt is expensive in the number of features, we limited consideration to (1) the 16165 unigrams appearing at least four times in our 1400document corpus (lower count cutofis did not yield signiflcantly difierent results), and (2) the 16165 bigrams occurring most often in the same data (the selected bigrams all occurred at least seven times)." ></td>
	<td class="line x" title="118:181	Note that we did not add negation tags to the bigrams, since we consider bigrams (and n-grams in general) to be an orthogonal way to incorporate context." ></td>
	<td class="line x" title="119:181	6.2 Results Initial unigram results The classiflcation accuracies resulting from using only unigrams as features are shown in line (1) of Figure 3." ></td>
	<td class="line x" title="120:181	As a whole, the machine learning algorithms clearly surpass the random-choice baseline of 50%." ></td>
	<td class="line x" title="121:181	They also handily beat our two human-selected-unigram baselines of 58% and 64%, and, furthermore, perform well in comparison to the 69% baseline achieved via limited access to the test-data statistics, although the improvement in the case of SVMs is not so large." ></td>
	<td class="line x" title="122:181	On the other hand, in topic-based classiflcation, all three classiflers have been reported to use bagof-unigram features to achieve accuracies of 90% and above for particular categories (Joachims, 1998; Nigam et al. , 1999)9 | and such results are for settings with more than two classes." ></td>
	<td class="line x" title="123:181	This provides suggestive evidence that sentiment categorization is more dicult than topic classiflcation, which corresponds to the intuitions of the text categorization expert mentioned above.10 Nonetheless, we still wanted to investigate ways to improve our sentiment categorization results; these experiments are reported below." ></td>
	<td class="line x" title="124:181	Feature frequency vs. presence Recall that we represent each document d by a feature-count vector (n1(d);:::;nm(d))." ></td>
	<td class="line x" title="125:181	However, the deflnition of the 9Joachims (1998) used stemming and stoplists; in some of their experiments, Nigam et al.(1999), like us, did not." ></td>
	<td class="line x" title="127:181	10We could not perform the natural experiment of attempting topic-based categorization on our data because the only obvious topics would be the fllm being reviewed; unfortunately, in our data, the maximum number of reviews per movie is 27, too small for meaningful results." ></td>
	<td class="line x" title="128:181	MaxEnt feature/class functions Fi;c only re ects the presence or absence of a feature, rather than directly incorporating feature frequency." ></td>
	<td class="line x" title="129:181	In order to investigate whether reliance on frequency information could account for the higher accuracies of Naive Bayes and SVMs, we binarized the document vectors, setting ni(d) to 1 if and only feature fi appears in d, and reran Naive Bayes and SVMlight on these new vectors.11 As can be seen from line (2) of Figure 3, better performance (much better performance for SVMs) is achieved by accounting only for feature presence, not feature frequency." ></td>
	<td class="line x" title="130:181	Interestingly, this is in direct opposition to the observations of McCallum and Nigam (1998) with respect to Naive Bayes topic classiflcation." ></td>
	<td class="line x" title="131:181	We speculate that this indicates a difierence between sentiment and topic categorization | perhaps due to topic being conveyed mostly by particular content words that tend to be repeated | but this remains to be verifled." ></td>
	<td class="line x" title="132:181	In any event, as a result of this flnding, we did not incorporate frequency information into Naive Bayes and SVMs in any of the following experiments." ></td>
	<td class="line x" title="133:181	Bigrams In addition to looking speciflcally for negation words in the context of a word, we also studied the use of bigrams to capture more context in general." ></td>
	<td class="line x" title="134:181	Note that bigrams and unigrams are surely not conditionally independent, meaning that the feature set they comprise violates Naive Bayes conditional-independence assumptions; on the other hand, recall that this does not imply that Naive Bayes will necessarily do poorly (Domingos and Pazzani, 1997)." ></td>
	<td class="line x" title="135:181	Line (3) of the results table shows that bigram information does not improve performance beyond that of unigram presence, although adding in the bigrams does not seriously impact the results, even for Naive Bayes." ></td>
	<td class="line x" title="136:181	This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be efiective features for word sense disambiguation." ></td>
	<td class="line x" title="137:181	However, comparing line (4) to line (2) shows that relying just on bigrams causes accuracy to decline by as much as 5.8 percentage points." ></td>
	<td class="line x" title="138:181	Hence, if context is in fact important, as our intuitions suggest, bigrams are not efiective at capturing it in our setting." ></td>
	<td class="line x" title="139:181	11Alternatively, we could have tried integrating frequency information into MaxEnt." ></td>
	<td class="line x" title="140:181	However, feature/class functions are traditionally deflned as binary (Berger et al. , 1996); hence, explicitly incorporating frequencies would require difierent functions for each count (or count bin), making training impractical." ></td>
	<td class="line x" title="141:181	But cf.(Nigam et al. , 1999)." ></td>
	<td class="line x" title="143:181	Parts of speech We also experimented with appending POS tags to every word via Oliver Masons Qtag program.12 This serves as a crude form of word sense disambiguation (Wilks and Stevenson, 1998): for example, it would distinguish the difierent usages of \love' in \I love this movie' (indicating sentiment orientation) versus \This is a love story' (neutral with respect to sentiment)." ></td>
	<td class="line x" title="144:181	However, the efiect of this information seems to be a wash: as depicted in line (5) of Figure 3, the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged." ></td>
	<td class="line oc" title="145:181	Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone." ></td>
	<td class="line x" title="146:181	Intuitively, we might expect that adjectives carry a great deal of information regarding a documents sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech." ></td>
	<td class="line x" title="147:181	Yet, the results, shown in line (6) of Figure 3, are relatively poor: the 2633 adjectives provide less useful information than unigram presence." ></td>
	<td class="line x" title="148:181	Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2))." ></td>
	<td class="line x" title="149:181	This may imply that applying explicit feature-selection algorithms on unigrams could improve performance." ></td>
	<td class="line x" title="150:181	Position An additional intuition we had was that the position of a word in the text might make a difference: movie reviews, in particular, might begin with an overall sentiment statement, proceed with a plot discussion, and conclude by summarizing the authors views." ></td>
	<td class="line x" title="151:181	As a rough approximation to determining this kind of structure, we tagged each word according to whether it appeared in the flrst quarter, last quarter, or middle half of the document14." ></td>
	<td class="line x" title="152:181	The results (line (8)) didnt difier greatly from using unigrams alone, but more reflned notions of position might be more successful." ></td>
	<td class="line x" title="153:181	7 Discussion The results produced via machine learning techniques are quite good in comparison to the humangenerated baselines discussed in Section 4." ></td>
	<td class="line oc" title="154:181	In terms of relative performance, Naive Bayes tends to do the worst and SVMs tend to do the best, although the 12http://www.english.bham.ac.uk/stafi/oliver/software/tagger/index.htm 13Turneys (2002) unsupervised algorithm uses bigrams containing an adjective or an adverb." ></td>
	<td class="line x" title="155:181	14We tried a few other settings, e.g., flrst third vs. last third vs middle third, and found them to be less efiective." ></td>
	<td class="line x" title="156:181	difierences arent very large." ></td>
	<td class="line x" title="157:181	On the other hand, we were not able to achieve accuracies on the sentiment classiflcation problem comparable to those reported for standard topic-based categorization, despite the several difierent types of features we tried." ></td>
	<td class="line x" title="158:181	Unigram presence information turned out to be the most efiective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated." ></td>
	<td class="line x" title="159:181	Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classiflcation work (McCallum and Nigam, 1998)." ></td>
	<td class="line x" title="160:181	What accounts for these two difierences | difflculty and types of information proving useful | between topic and sentiment classiflcation, and how might we improve the latter?" ></td>
	<td class="line x" title="161:181	To answer these questions, we examined the data further." ></td>
	<td class="line x" title="162:181	(All examples below are drawn from the full 2053-document corpus.)" ></td>
	<td class="line x" title="163:181	As it turns out, a common phenomenon in the documents was a kind of \thwarted expectations' narrative, where the author sets up a deliberate contrast to earlier discussion: for example, \This fllm should be brilliant." ></td>
	<td class="line x" title="164:181	It sounds like a great plot, the actors are flrst grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance." ></td>
	<td class="line x" title="165:181	However, it cant hold up' or \I hate the Spice Girls." ></td>
	<td class="line x" title="166:181	[3 things the author hates about them] Why I saw this movie is a really, really, really long story, but I did, and one would think Id despise every minute of it." ></td>
	<td class="line x" title="167:181	But Okay, Im really ashamed of it, but I enjoyed it." ></td>
	<td class="line x" title="168:181	I mean, I admit its a really awful moviethe ninth oor of hellThe plot is such a mess that its terrible." ></td>
	<td class="line x" title="169:181	But I loved it'." ></td>
	<td class="line x" title="170:181	15 In these examples, a human would easily detect the true sentiment of the review, but bag-of-features classiflers would presumably flnd these instances difflcult, since there are many words indicative of the opposite sentiment to that of the entire review." ></td>
	<td class="line x" title="171:181	Fundamentally, it seems that some form of discourse analysis is necessary (using more sophisticated tech15This phenomenon is related to another common theme, that of \a good actor trapped in a bad movie': \AN AMERICAN WEREWOLF IN PARIS is a failed attempt Julie Delpy is far too good for this movie." ></td>
	<td class="line x" title="172:181	She imbues Seraflne with spirit, spunk, and humanity." ></td>
	<td class="line x" title="173:181	This isnt necessarily a good thing, since it prevents us from relaxing and enjoying AN AMERICAN WEREWOLF IN PARIS as a completely mindless, campy entertainment experience." ></td>
	<td class="line x" title="174:181	Delpys injection of class into an otherwise classless production raises the specter of what this fllm could have been with a better script and a better cast  She was radiant, charismatic, and efiective ' niques than our positional feature mentioned above), or at least some way of determining the focus of each sentence, so that one can decide when the author is talking about the fllm itself." ></td>
	<td class="line oc" title="175:181	(Turney (2002) makes a similar point, noting that for reviews, \the whole is not necessarily the sum of the parts')." ></td>
	<td class="line x" title="176:181	Furthermore, it seems likely that this thwarted-expectations rhetorical device will appear in many types of texts (e.g. , editorials) devoted to expressing an overall opinion about some topic." ></td>
	<td class="line x" title="177:181	Hence, we believe that an important next step is the identiflcation of features indicating whether sentences are on-topic (which is a kind of co-reference problem); we look forward to addressing this challenge in future work." ></td>
	<td class="line x" title="178:181	Acknowledgments We thank Joshua Goodman, Thorsten Joachims, Jon Kleinberg, Vikas Krishna, John Lafierty, Jussi Myllymaki, Phoebe Sengers, Richard Tong, Peter Turney, and the anonymous reviewers for many valuable comments and helpful suggestions, and Hubie Chen and Tony Faradjian for participating in our baseline experiments." ></td>
	<td class="line x" title="179:181	Portions of this work were done while the flrst author was visiting IBM Almaden." ></td>
	<td class="line x" title="180:181	This paper is based upon work supported in part by the National Science Foundation under ITR/IM grant IIS0081334." ></td>
	<td class="line x" title="181:181	Any opinions, flndings, and conclusions or recommendations expressed above are those of the authors and do not necessarily re ect the views of the National Science Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N03-1025
Language And Task Independent Text Categorization With Simple Language Models
Peng, Fuchun;Schuurmans, Dale;Wang, Shaojun;"></td>
	<td class="line x" title="1:184	Language and Task Independent Text Categorization with Simple Language Models Fuchun Peng Dale Schuurmans Shaojun Wang School of Computer Science, University of Waterloo 200 University Avenue West, Waterloo, Ontario, Canada, N2L 3G1 ff3peng, dale, sjwangg@cs.uwaterloo.ca Abstract We present a simple method for language independent and task independent text categorization learning, based on character-level n-gram language models." ></td>
	<td class="line x" title="2:184	Our approach uses simple information theoretic principles and achieves effective performance across a variety of languages and tasks without requiring feature selection or extensive pre-processing." ></td>
	<td class="line x" title="3:184	To demonstrate the language and task independence of the proposed technique, we present experimental results on several languagesGreek, English, Chinese and Japanesein several text categorization problemslanguage identification, authorship attribution, text genre classification, and topic detection." ></td>
	<td class="line x" title="4:184	Our experimental results show that the simple approach achieves state of the art performance in each case." ></td>
	<td class="line x" title="5:184	1 Introduction Text categorization concerns the problem of automatically assigning given text passages (paragraphs or documents) into predefined categories." ></td>
	<td class="line x" title="6:184	Due to the rapid explosion of texts in digital form, text categorization has become an important area of research owing to the need to automatically organize and index large text collections in various ways." ></td>
	<td class="line oc" title="7:184	Such techniques are currently being applied in many areas, including language identification, authorship attribution (Stamatatos et al. , 2000), text genre classification (Kesseler et al. , 1997; Stamatatos et al. , 2000), topic identification (Dumais et al. , 1998; Lewis, 1992; McCallum, 1998; Yang, 1999), and subjective sentiment classification (Turney, 2002)." ></td>
	<td class="line x" title="8:184	Many standard machine learning techniques have been applied to automated text categorization problems, such as naive-Bayes classifiers, support vector machines, linear least squares models, neural networks, and K-nearest neighbor classifiers (Yang, 1999; Sebastiani, 2002)." ></td>
	<td class="line x" title="9:184	A common aspect of these approaches is that they treat text categorization as a standard classification problem, and thereby reduce the learning process to two simple steps: feature engineering, and classification learning over the feature space." ></td>
	<td class="line x" title="10:184	Of these two steps, feature engineering is critical to achieving good performance in text categorization problems." ></td>
	<td class="line x" title="11:184	Once good features are identified, almost any reasonable technique for learning a classifier seems to perform well (Scott, 1999)." ></td>
	<td class="line x" title="12:184	Unfortunately, the standard classification learning methodology has several drawbacks for text categorization." ></td>
	<td class="line x" title="13:184	First, feature construction is usually language dependent." ></td>
	<td class="line x" title="14:184	Various techniques such as stop-word removal or stemming require language specific knowledge to design adequately." ></td>
	<td class="line x" title="15:184	Moreover, whether one can use a purely word-level approach is itself a language dependent issue." ></td>
	<td class="line x" title="16:184	In many Asian languages such as Chinese or Japanese, identifying words from character sequences is hard, and any word-based approach must suffer added complexity in coping with segmentation errors." ></td>
	<td class="line x" title="17:184	Second, feature selection is task dependent." ></td>
	<td class="line x" title="18:184	For example, tasks like authorship attribution or genre classification require attention to linguistic style markers (Stamatatos et al. , 2000), whereas topic detection systems rely more heavily on bag of words features." ></td>
	<td class="line x" title="19:184	Third, there are an enormous number of possible features to consider in text categorization problems, and standard feature selection approaches do not always cope well in such circumstances." ></td>
	<td class="line x" title="20:184	For example, given an enormous number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less information than common features individually." ></td>
	<td class="line x" title="21:184	Consequently, throwing away uncommon features is usually not an appropriate strategy in this domain (Aizawa, 2001)." ></td>
	<td class="line x" title="22:184	Another problem is that feature selection normally uses indirect tests, such as 2 or mutual information, which involve setting arbiEdmonton, May-June 2003 Main Papers, pp." ></td>
	<td class="line x" title="23:184	110-117 Proceedings of HLT-NAACL 2003 trary thresholds and conducting a heuristic greedy search to find good feature sets." ></td>
	<td class="line x" title="24:184	Finally, by treating text categorization as a classical classification problem, standard approaches can ignore the fact that texts are written in natural language, meaning that they have many implicit regularities that can be well modeled with specific tools from natural language processing." ></td>
	<td class="line x" title="25:184	In this paper, we propose a straightforward text categorization learning method based on learning categoryspecific, character-level, n-gram language models." ></td>
	<td class="line x" title="26:184	Although this is a very simple approach, it has not yet been systematically investigated in the literature." ></td>
	<td class="line x" title="27:184	We find that, surprisingly, we obtain competitive (and often superior) results to more sophisticated learning and feature construction techniques, while requiring almost no feature engineering or pre-processing." ></td>
	<td class="line x" title="28:184	In fact, the overall approach requires almost no language specific or task specific pre-processing to achieve effective performance." ></td>
	<td class="line x" title="29:184	The success of this simple method, we think, is due to the effectiveness of well known statistical language modeling techniques, which surprisingly have had little significant impact on the learning algorithms normally applied to text categorization." ></td>
	<td class="line x" title="30:184	Nevertheless, statistical language modeling is also concerned with modeling the semantic, syntactic, lexicographical and phonological regularities of natural languageand would seem to provide a natural foundation for text categorization problems." ></td>
	<td class="line x" title="31:184	One interesting difference, however, is that instead of explicitly pre-computing features and selecting a subset based on arbitrary decisions, the language modeling approach simply considers all character (or word) subsequences occurring in the text as candidate features, and implicitly considers the contribution of every feature in the final model." ></td>
	<td class="line x" title="32:184	Thus, the language modeling approach completely avoids a potentially error-prone feature selection process." ></td>
	<td class="line x" title="33:184	Also, by applying character-level language models, one also avoids the word segmentation problems that arise in many Asian languages, and thereby achieves a language independent method for constructing accurate text categorizers." ></td>
	<td class="line x" title="34:184	2 n-Gram Language Modeling The dominant motivation for language modeling has traditionally come from speech recognition, but language models have recently become widely used in many other application areas." ></td>
	<td class="line x" title="35:184	The goal of language modeling is to predict the probability of naturally occurring word sequences, s = w1w2:::wN; or more simply, to put high probability on word sequences that actually occur (and low probability on word sequences that never occur)." ></td>
	<td class="line x" title="36:184	Given a word sequence w1w2:::wN to be used as a test corpus, the quality of a language model can be measured by the empirical perplexity and entropy scores on this corpus Perplexity = N vu ut NY i=1 1 Pr(wijw1:::wi1) (1) Entropy = log2 Perplexity (2) where the goal is to minimize these measures." ></td>
	<td class="line x" title="37:184	The simplest and most successful approach to language modeling is still based on the n-gram model." ></td>
	<td class="line x" title="38:184	By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2:::wN) = NY i=1 Pr(wijw1:::wi1) (3) An n-gram model approximates this probability by assuming that the only words relevant to predicting Pr(wijw1:::wi1) are the previous n1 words; i.e. Pr(wijw1:::wi1) = Pr(wijwin+1:::wi1) A straightforward maximum likelihood estimate of ngram probabilities from a corpus is given by the observed frequency of each of the patterns Pr(wijwin+1:::wi1) = #(win+1:::wi)#(w in+1:::wi1) (4) where #()." ></td>
	<td class="line x" title="39:184	denotes the number of occurrences of a specified gram in the training corpus." ></td>
	<td class="line x" title="40:184	Although one could attempt to use simple n-gram models to capture long range dependencies in language, attempting to do so directly immediately creates sparse data problems: Using grams of length up to n entails estimating the probability of Wn events, where W is the size of the word vocabulary." ></td>
	<td class="line x" title="41:184	This quickly overwhelms modern computational and data resources for even modest choices of n (beyond 3 to 6)." ></td>
	<td class="line x" title="42:184	Also, because of the heavy tailed nature of language (i.e. Zipfs law) one is likely to encounter novel n-grams that were never witnessed during training in any test corpus, and therefore some mechanism for assigning non-zero probability to novel n-grams is a central and unavoidable issue in statistical language modeling." ></td>
	<td class="line x" title="43:184	One standard approach to smoothing probability estimates to cope with sparse data problems (and to cope with potentially missing n-grams) is to use some sort of back-off estimator." ></td>
	<td class="line x" title="44:184	Pr(wijwin+1:::wi1) = 8> >< >>: Pr(wijwin+1:::wi1); if #(win+1:::wi) > 0 fl(win+1:::wi1)Pr(wijwin+2:::wi1); otherwise (5) where Pr(wijwin+1:::wi1) = discount#(win+1:::wi) #(win+1:::wi1) (6) is the discounted probability and fl(win+1:::wi1) is a normalization constant fl(win+1:::wi1) = 1 X x2(win+1:::wi1x) Pr(xjwin+1:::wi1) 1 X x2(win+1:::wi1x) Pr(xjwin+2:::wi1) (7) The discounted probability (6) can be computed with different smoothing techniques, including absolute smoothing, Good-Turing smoothing, linear smoothing, and Witten-Bell smoothing (Chen and Goodman, 1998)." ></td>
	<td class="line x" title="45:184	The details of the smoothing techniques are omitted here for simplicity." ></td>
	<td class="line x" title="46:184	The language models described above use individual words as the basic unit, although one could instead consider models that use individual characters as the basic unit." ></td>
	<td class="line x" title="47:184	The remaining details remain the same in this case." ></td>
	<td class="line x" title="48:184	The only difference is that the character vocabulary is always much smaller than the word vocabulary, which means that one can normally use a much higher order, n, in a character-level n-gram model (although the text spanned by a character model is still usually less than that spanned by a word model)." ></td>
	<td class="line x" title="49:184	The benefits of the character-level model in the context of text classification are several-fold: it avoids the need for explicit word segmentation in the case of Asian languages, it captures important morphological properties of an authors writing, it models the typos and misspellings that are common in informal texts, it can still discover useful inter-word and inter-phrase features, and it greatly reduces the sparse data problems associated with large vocabulary models." ></td>
	<td class="line x" title="50:184	In this paper, we experiment with character-level models to achieve flexibility and language independence." ></td>
	<td class="line x" title="51:184	3 Language Models as Text Classifiers Our approach to applying language models to text categorization is to use Bayesian decision theory." ></td>
	<td class="line x" title="52:184	Assume we wish to classify a text D into a category c 2 C = fc1;:::;cjCjg." ></td>
	<td class="line x" title="53:184	A natural choice is to pick the category c that has the largest posterior probability given the text." ></td>
	<td class="line x" title="54:184	That is, c = argmax c2C fPr(cjD)g (8) Using Bayes rule, this can be rewritten as c = argmax c2C fPr(Djc)Pr(c)g (9) = argmax c2C fPr(Djc)g (10) = argmax c2C n NY i=1 Prc(wijwin+1:::wi1) o (11) where deducing Eq." ></td>
	<td class="line x" title="55:184	(10) from Eq." ></td>
	<td class="line x" title="56:184	(9) assumes uniformly weighted categories (since we have no other prior knowledge)." ></td>
	<td class="line x" title="57:184	Here, Pr(Djc) is the likelihood of D under category c, which can be computed by Eq." ></td>
	<td class="line x" title="58:184	(11)." ></td>
	<td class="line x" title="59:184	Likelihood is related to perplexity and entropy by Eq." ></td>
	<td class="line x" title="60:184	(1) and Eq." ></td>
	<td class="line x" title="61:184	(2)." ></td>
	<td class="line x" title="62:184	Therefore, our approach is to learn a separate language model for each category, by training on a data set from that category." ></td>
	<td class="line x" title="63:184	Then, to categorize a new text D, we supply D to each language model, evaluate the likelihood (or entropy) of D under the model, and pick the winning category according to Eq." ></td>
	<td class="line x" title="64:184	(10)." ></td>
	<td class="line x" title="65:184	The inference of an n-gram based text classifier is very similar to a naive-Bayes classifier." ></td>
	<td class="line x" title="66:184	In fact, n-gram classifiers are a straightforward generalization of naiveBayes: A uni-gram classifier with Laplace smoothing corresponds exactly to the traditional naive-Bayes classifier." ></td>
	<td class="line x" title="67:184	However, n-gram language models, for larger n, possess many advantages over naive-Bayes classifiers, including modeling longer context and applying superior smoothing techniques in the presence of sparse data." ></td>
	<td class="line x" title="68:184	4 Experimental Comparison We now proceed to present our results on several text categorization problems on different languages." ></td>
	<td class="line x" title="69:184	Specifically, we consider language identification, Greek authorship attribution, Greek genre classification, English topic detection, Chinese topic detection and Japanese topic detection." ></td>
	<td class="line x" title="70:184	For the sake of consistency with previous research (Aizawa, 2001; He et al. , 2000; Stamatatos et al. , 2000), we measure categorization performance by the overall accuracy, which is the number of correctly identified texts divided by the total number of texts considered." ></td>
	<td class="line x" title="71:184	We also measure the performance with Macro Fmeasure, which is the average of the F-measures across all categories." ></td>
	<td class="line x" title="72:184	F-measure is a combination of precision and recall (Yang, 1999)." ></td>
	<td class="line x" title="73:184	4.1 Language Identification The first text categorization problem we examined was language identificationa useful pre-processing step in information retrieval." ></td>
	<td class="line x" title="74:184	Language identification is probably the easiest text classification problem because of the significant morphological differences between languages, n Absolute Good-Turing Linear Witten-Bell Acc." ></td>
	<td class="line x" title="75:184	F-Mac Acc." ></td>
	<td class="line x" title="76:184	F-Mac Acc." ></td>
	<td class="line x" title="77:184	F-Mac Acc." ></td>
	<td class="line x" title="78:184	F-Mac 1 0.57 0.53 0.55 0.49 0.55 0.49 0.55 0.49 2 0.85 0.84 0.80 0.75 0.84 0.83 0.84 0.82 3 0.90 0.89 0.79 0.72 0.89 0.88 0.89 0.87 4 0.87 0.85 0.79 0.72 0.85 0.82 0.88 0.86 5 0.86 0.85 0.79 0.72 0.87 0.85 0.86 0.83 6 0.86 0.83 0.79 0.73 0.87 0.85 0.86 0.83 Table 1: Results on Greek authorship attribution even when they are based on the same character set.1 In our experiments, we considered one chapter of Bible that had been translated into 6 different languages: English, French, German, Italian, Latin and Spanish." ></td>
	<td class="line x" title="79:184	In each case, we reserved twenty sentences from each language for testing and used the remainder for training." ></td>
	<td class="line x" title="80:184	For this task, with only bi-gram character-level models and any smoothing technique, we achieved 100% accuracy." ></td>
	<td class="line x" title="81:184	4.2 Authorship Attribution The second text categorization problem we examined was author attribution." ></td>
	<td class="line x" title="82:184	A famous example is the case of the Federalist Papers, of which twelve instances are claimed to have been written both by Alexander Hamilton and James Madison (Holmes and Forsyth, 1995)." ></td>
	<td class="line x" title="83:184	Authorship attribution is more challenging than language identification because the difference among the authors is much more subtle than that among different languages." ></td>
	<td class="line x" title="84:184	We considered a data set used by (Stamatatos et al. , 2000) consisting of 20 texts written by 10 different modern Greek authors (totaling 200 documents)." ></td>
	<td class="line x" title="85:184	In each case, 10 texts from each author were used for training and the remaining 10 for testing." ></td>
	<td class="line x" title="86:184	The results using different orders of n-gram models and different smoothing techniques are shown in Table 1." ></td>
	<td class="line x" title="87:184	With 3-grams and absolute smoothing, we observe 90% accuracy." ></td>
	<td class="line x" title="88:184	This result compares favorably to the 72% accuracy reported in (Stamatatos et al. , 2000) which is based on linear least square fit (LLSF)." ></td>
	<td class="line x" title="89:184	4.3 Text Genre Classification The third problem we examined was text genre classification, which is an important application in information retrieval (Kesseler et al. , 1997; Lee et al. , 2002)." ></td>
	<td class="line x" title="90:184	We considered a Greek data set used by (Stamatatos et al. , 2000) consisting of 20 texts of 10 different styles extracted from various sources (200 documents total)." ></td>
	<td class="line x" title="91:184	For each style, we used 10 texts as training data and the remaining 10 as testing." ></td>
	<td class="line x" title="92:184	1Language identification from speech is much harder." ></td>
	<td class="line x" title="93:184	n Absolute Good-Turing Linear Witten-Bell Acc." ></td>
	<td class="line x" title="94:184	F-Mac Acc." ></td>
	<td class="line x" title="95:184	F-Mac Acc." ></td>
	<td class="line x" title="96:184	F-Mac Acc." ></td>
	<td class="line x" title="97:184	F-Mac 1 0.31 0.55 0.30 0.54 0.30 0.54 0.30 0.54 2 0.86 0.86 0.60 0.52 0.82 0.81 0.86 0.86 3 0.77 0.75 0.65 0.59 0.79 0.77 0.85 0.85 4 0.69 0.65 0.58 0.50 0.74 0.69 0.76 0.74 5 0.66 0.61 0.56 0.49 0.69 0.66 0.73 0.70 6 0.62 0.57 0.49 0.53 0.67 0.63 0.71 0.68 7 0.63 0.58 0.49 0.53 0.66 0.62 0.70 0.68 Table 2: Results on Greek text genre classification The results of learning an n-gram based text classifier are shown in Table 2." ></td>
	<td class="line x" title="98:184	The 86% accuracy obtained with bi-gram models compares favorably to the 82% reported in (Stamatatos et al. , 2000), which again is based on a much deeper NLP analysis." ></td>
	<td class="line x" title="99:184	4.4 Topic Detection The fourth problem we examined was topic detection in text, which is a heavily researched text categorization problem (Dumais et al. , 1998; Lewis, 1992; McCallum, 1998; Yang, 1999; Sebastiani, 2002)." ></td>
	<td class="line x" title="100:184	Here we demonstrate the language independence of the language modeling approach by considering experiments on English, Chinese and Japanese data sets." ></td>
	<td class="line x" title="101:184	4.4.1 English Data The English 20 Newsgroup data has been widely used in topic detection research (McCallum, 1998; Rennie, 2001).2 This collection consists of 19,974 non-empty documents distributed evenly across 20 newsgroups." ></td>
	<td class="line x" title="102:184	We use the newsgroups to form our categories, and randomly select 80% of the documents to be used for training and set aside the remaining 20% for testing." ></td>
	<td class="line x" title="103:184	In this case, as before, we merely considered text to be a sequence of characters, and learned character-level ngram models." ></td>
	<td class="line x" title="104:184	The resulting classification accuracies are reported in in Table 3." ></td>
	<td class="line x" title="105:184	With 3-gram (or higher order) models, we consistently obtain accurate performance, peaking at 89% accuracy in the case of 6-gram models with Witten-Bell smoothing." ></td>
	<td class="line x" title="106:184	(We note that word-level models were able to achieve 88% accuracy in this case.)" ></td>
	<td class="line x" title="107:184	These results compare favorably to the state of the art result of 87.5% accuracy reported in (Rennie, 2001), which was based on a combination of an SVM with error correct output coding (ECOC)." ></td>
	<td class="line x" title="108:184	4.4.2 Chinese Data Chinese topic detection is often thought to be more challenging than English, because words are not whitespace delimited in Chinese text." ></td>
	<td class="line x" title="109:184	This fact seems to 2http://www.ai.mit.edu/ jrennie/20Newsgroups/ n Absolute Good-Turing Linear Witten-Bell Acc." ></td>
	<td class="line x" title="110:184	F-Mac Acc." ></td>
	<td class="line x" title="111:184	F-Mac Acc." ></td>
	<td class="line x" title="112:184	F-Mac Acc." ></td>
	<td class="line x" title="113:184	F-Mac 1 0.22 0.21 0.22 0.21 0.22 0.21 0.22 0.21 2 0.68 0.66 0.69 0.67 0.68 0.67 0.67 0.65 3 0.86 0.86 0.86 0.86 0.86 0.85 0.86 0.86 4 0.88 0.88 0.88 0.87 0.87 0.87 0.89 0.88 5 0.89 0.88 0.87 0.87 0.88 0.88 0.89 0.89 6 0.89 0.88 0.88 0.88 0.88 0.88 0.89 0.89 7 0.89 0.88 0.88 0.87 0.88 0.88 0.89 0.89 8 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89 9 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89 Table 3: Topic detection results on English 20 Newsgroup data n Absolute Good-Turing Linear Witten-Bell Acc." ></td>
	<td class="line x" title="114:184	F-Mac Acc." ></td>
	<td class="line x" title="115:184	F-Mac Acc." ></td>
	<td class="line x" title="116:184	F-Mac Acc." ></td>
	<td class="line x" title="117:184	F-Mac 1 0.77 0.77 0.76 0.77 0.76 0.76 0.77 0.77 2 0.80 0.80 0.80 0.80 0.79 0.79 0.80 0.80 3 0.80 0.80 0.81 0.81 0.80 0.80 0.80 0.80 4 0.80 0.80 0.81 0.81 0.81 0.80 0.80 0.80 Table 4: Chinese topic detection results require word segmentation to be performed as a preprocessing step before further classification (He et al. , 2000)." ></td>
	<td class="line x" title="118:184	However, we avoid the need for explicit segmentation by simply using a character level n-gram classifier." ></td>
	<td class="line x" title="119:184	For Chinese topic detection we considered a data set investigated in (He et al. , 2000)." ></td>
	<td class="line x" title="120:184	The corpus in this case is a subset of the TREC-5 data set created for research on Chinese text retrieval." ></td>
	<td class="line x" title="121:184	To make the data set suitable for text categorization, documents were first clustered into 101 groups that shared the same headline (as indicated by an SGML tag) and the six most frequent groups were selected to make a Chinese text categorization data set." ></td>
	<td class="line x" title="122:184	In each group, 500 documents were randomly selected for training and 100 documents were reserved for testing." ></td>
	<td class="line x" title="123:184	We observe over 80% accuracy for this task, using bigram (2 Chinese characters) or higher order models." ></td>
	<td class="line x" title="124:184	This is the same level of performance reported in (He et al. , 2000) for an SVM approach using word segmentation and feature selection." ></td>
	<td class="line x" title="125:184	4.4.3 Japanese Data Japanese poses the same word segmentation issues as Chinese." ></td>
	<td class="line x" title="126:184	Word segmentation is also thought to be necessary for Japanese text categorization (Aizawa, 2001), but we avoid the need again by considering character level language models." ></td>
	<td class="line x" title="127:184	We consider the Japanese topic detection data investigated by (Aizawa, 2001)." ></td>
	<td class="line x" title="128:184	This data set was conn Absolute Good-Turing Linear Witten-Bell Acc." ></td>
	<td class="line x" title="129:184	F-Mac Acc." ></td>
	<td class="line x" title="130:184	F-Mac Acc." ></td>
	<td class="line x" title="131:184	F-Mac Acc." ></td>
	<td class="line x" title="132:184	F-Mac 1 0.33 0.29 0.34 0.29 0.34 0.29 0.34 0.29 2 0.66 0.62 0.66 0.61 0.66 0.63 0.66 0.62 3 0.75 0.72 0.75 0.72 0.76 0.73 0.75 0.72 4 0.81 0.77 0.81 0.76 0.82 0.76 0.81 0.77 5 0.83 0.77 0.83 0.76 0.83 0.76 0.83 0.77 6 0.84 0.76 0.83 0.75 0.83 0.75 0.84 0.77 7 0.84 0.75 0.83 0.74 0.83 0.74 0.84 0.76 8 0.83 0.74 0.83 0.73 0.83 0.73 0.84 0.76 Table 5: Japanese topic detection results verted from the NTCIR-J1 data set originally created for Japanese text retrieval research." ></td>
	<td class="line x" title="133:184	The data has 24 categories." ></td>
	<td class="line x" title="134:184	The testing set contains 10,000 documents distributed unevenly between categories (with a minimum of 56 and maximum of 2696 documents per category)." ></td>
	<td class="line x" title="135:184	This imbalanced distribution causes some difficulty since we assumed a uniform prior over categories." ></td>
	<td class="line x" title="136:184	Although this is easily remedied, we did not fix the problem here." ></td>
	<td class="line x" title="137:184	Nevertheless, we obtain experimental results in Table 5 that still show an 84% accuracy rate on this problem (for 6-gram or higher order models)." ></td>
	<td class="line x" title="138:184	This is the same level of performance as that reported in (Aizawa, 2001), which uses an SVM approach with word segmentation, morphology analysis and feature selection." ></td>
	<td class="line x" title="139:184	5 Analysis The perplexity of a test document under a language model depends on several factors." ></td>
	<td class="line x" title="140:184	The two most influential factors are the order, n, of the n-gram model and the smoothing technique used." ></td>
	<td class="line x" title="141:184	Different choices will result in different perplexities, which could influence the final decision in using Eq." ></td>
	<td class="line x" title="142:184	(10)." ></td>
	<td class="line x" title="143:184	We now experimentally assess the influence of each of these factors below." ></td>
	<td class="line x" title="144:184	5.1 Effects of n-Gram Order The order n is a key factor in n-gram language models." ></td>
	<td class="line x" title="145:184	If n is too small then the model will not capture enough context." ></td>
	<td class="line x" title="146:184	However, if n is too large then this will create severe sparse data problems." ></td>
	<td class="line x" title="147:184	Both extremes result in a larger perplexity than the optimal context length." ></td>
	<td class="line x" title="148:184	Figures 1 and 2 illustrate the influence of order n on classification performance and on language model quality in the previous five experiments (all using absolute smoothing)." ></td>
	<td class="line x" title="149:184	Note that in this case the entropy (bits per character) is the average entropy across all testing documents." ></td>
	<td class="line x" title="150:184	From the curves, one can see that as the order increases, classification accuracy increases and testing entropy decreases, presumably because the longer context better captures the regularities of the text." ></td>
	<td class="line x" title="151:184	However, at some point accu1 2 3 4 5 6 7 80.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 order n or ngram model Overall accuracy Greek authorship attributionGreek genre classification English Topic DetectionChinese Topic Detection Japanese Topic Detection Figure 1: Influence of the order n on the classification performance 1 2 3 4 5 6 7 8 93 4 5 6 7 8 9 10 11 order n or ngram model Entropy Greek authorship attributionGreek genre classification English Topic Detection Chinese Topic Detection Japanese Topic Detection Figure 2: The entropy of different n-gram models racy begins to decrease and entropy begins to increase as the sparse data problems begin to set in." ></td>
	<td class="line x" title="152:184	Interestingly, the effect is more pronounced in some experiments (Greek genre classification) but less so in other experiments (topic detection under any language)." ></td>
	<td class="line x" title="153:184	The sensitivity in the Greek genre case could still be attributed to the sparse data problem (the over-fitting problem in genre classification could be more serious than the other problems, as seen from the entropy curves)." ></td>
	<td class="line x" title="154:184	5.2 Effects of Smoothing Technique Another key factor affecting the performance of a language model is the smoothing technique used." ></td>
	<td class="line x" title="155:184	Figures 3 and 4 show the effects of smoothing techniques on classification accuracy and testing entropy (Chinese topic detection and Japanese topic detection are not shown in the figure to save space)." ></td>
	<td class="line x" title="156:184	Here we find that, in most cases, the smoothing technique does not have a significant effect on text categorization accuracy, because of the small vocabulary size of 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 60.4 0.6 0.8 1 Greek authorship attribution Overall Accuracy 1 2 3 4 5 6 70.2 0.4 0.6 0.8 1 Greek Genre Clasification Overall Accuracy 1 2 3 4 5 6 7 8 90.2 0.4 0.6 0.8 1 English Topic Detection Overall Accuracy order n of ngram models AbsoluteGoodTuring LinearWittenBell Figure 3: Influence of smoothing on accuracy 1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 63 3.5 4 4.5 5 Greek authorship attribution Entropy 1 2 3 4 5 6 73.5 4 4.5 5 5.5 Greek Genre Clasification Entropy 1 2 3 4 5 6 7 8 93 3.5 4 4.5 5 English Topic Detection Entropy order n of ngram models Absolute GoodTuring Linear WittenBell Figure 4: The entropy of different smoothing character level n-gram models." ></td>
	<td class="line x" title="157:184	However, there are two exceptionsGreek authorship attribution and Greek text genre classificationwhere Good-Turing smoothing is not as effective as other techniques, even though it gives better test entropy than some others." ></td>
	<td class="line x" title="158:184	Since our goal is to make a final decision based on the ranking of perplexities, not just their absolute values, a superior smoothing method in the sense of perplexity reduction (i.e. from the perspective of classical language modeling) does not necessarily lead to a better decision from the perspective of categorization accuracy." ></td>
	<td class="line x" title="159:184	In fact, in all our experiments we have found that it is Witten-Bell smoothing, not Good-Turing smoothing, that gives the best results in terms of classification accuracy." ></td>
	<td class="line x" title="160:184	Our observation is consistent with previous research which reports that WittenBell smoothing achieves benchmark performance in character level text compression (Bell et al. , 1990)." ></td>
	<td class="line x" title="161:184	For the most part, however, one can use any standard smoothing technique in these problems and obtain comparable performance, since the rankings they produce are almost always the same." ></td>
	<td class="line x" title="162:184	5.3 Relation to Previous Research In principle, any language model can be used to perform text categorization based on Eq." ></td>
	<td class="line x" title="163:184	(10)." ></td>
	<td class="line x" title="164:184	However, n-gram models are extremely simple and have been found to be effective in many applications." ></td>
	<td class="line x" title="165:184	For example, character level n-gram language models can be easily applied to any language, and even non-language sequences such as DNA and music." ></td>
	<td class="line x" title="166:184	Character level n-gram models are widely used in text compressione.g. , the PPM model (Bell et al. , 1990)and have recently been found to be effective in text classification problems as well (Teahan and Harper, 2001)." ></td>
	<td class="line x" title="167:184	The PPM model is a weighted linear interpolation n-gram models and has been set as a benchmark in text compression for decades." ></td>
	<td class="line x" title="168:184	Building an adaptive PPM model is expensive however (Bell et al. , 1990), and our back-off models are relatively much simpler." ></td>
	<td class="line x" title="169:184	Using compression techniques for text categorization has also been investigated in (Benedetto et al. , 2002), where the authors seek a model that yields the minimum compression rate increase when a new test document is introduced." ></td>
	<td class="line x" title="170:184	However, this method is found not to be generally effective nor efficient (Goodman, 2002)." ></td>
	<td class="line x" title="171:184	In our approach, we evaluate the perplexity (or entropy) directly on test documents, and find the outcome to be both effective and efficient." ></td>
	<td class="line x" title="172:184	Many previous researchers have realized the importance of n-gram models in designing language independent text categorization systems (Cavnar and Trenkle, 1994; Damashek, 1995)." ></td>
	<td class="line x" title="173:184	However, they have used ngrams as features for a traditional feature selection process, and then deployed classifiers based on calculating feature-vector similarities." ></td>
	<td class="line x" title="174:184	Feature selection in such a classical approach is critical, and many required procedures, such as stop word removal, are actually language dependent." ></td>
	<td class="line x" title="175:184	In our approach, all n-grams are considered as features and their importance is implicitly weighted by their contribution to perplexity." ></td>
	<td class="line x" title="176:184	Thus we avoid an error prone preliminary feature selection step." ></td>
	<td class="line x" title="177:184	6 Conclusion We have presented an extremely simple approach for language and task independent text categorization based on character level n-gram language modeling." ></td>
	<td class="line x" title="178:184	The approach is evaluated on four different languages and four different text categorization problems." ></td>
	<td class="line x" title="179:184	Surprisingly, we observe state of the art or better performance in each case." ></td>
	<td class="line x" title="180:184	We have also experimentally analyzed the influence of two factors that can affect the accuracy of this approach, and found that for the most part the results are robust to perturbations of the basic method." ></td>
	<td class="line x" title="181:184	The wide applicability and simplicity of this approach makes it immediately applicable to any sequential data (such as natural language, music, DNA) and yields effective baseline performance." ></td>
	<td class="line oc" title="182:184	We are currently investigating more challenging problems like multiple category classification using the Reuters-21578 data set (Lewis, 1992) and subjective sentiment classification (Turney, 2002)." ></td>
	<td class="line x" title="183:184	To us, these results suggest that basic statistical language modeling ideas might be more relevant to other areas of natural language processing than commonly perceived." ></td>
	<td class="line x" title="184:184	7 Acknowledgments Research supported by Bell University Labs and MITACS." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-0404
Learning Subjective Nouns Using Extraction Pattern Bootstrapping
Riloff, Ellen;Wiebe, Janyce M.;Wilson, Theresa;"></td>
	<td class="line x" title="1:226	Learning Subjective Nouns using Extraction Pattern Bootstrapping Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112 riloff@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Theresa Wilson Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 twilson@cs.pitt.edu Abstract We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms." ></td>
	<td class="line x" title="2:226	The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences." ></td>
	<td class="line x" title="3:226	First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns." ></td>
	<td class="line x" title="4:226	Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research." ></td>
	<td class="line x" title="5:226	The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision." ></td>
	<td class="line x" title="6:226	1 Introduction Many natural language processing applications could benefit from being able to distinguish between factual and subjective information." ></td>
	<td class="line x" title="7:226	Subjective remarks come in a variety of forms, including opinions, rants, allegations, accusations, suspicions, and speculation." ></td>
	<td class="line x" title="8:226	Ideally, information extraction systems should be able to distinguish between factual information (which should be extracted) and non-factual information (which should be discarded or labeled as uncertain)." ></td>
	<td class="line x" title="9:226	Question answering systems should distinguish between factual and speculative answers." ></td>
	<td class="line x" title="10:226	Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources." ></td>
	<td class="line x" title="11:226	MultiThis work was supported in part by the National Science Foundation under grants IIS-0208798 and IRI-9704240." ></td>
	<td class="line x" title="12:226	The data preparation was performed in support of the Northeast Regional Reseach Center (NRRC) which is sponsored by the Advanced Research and Development Activity (ARDA), a U.S. Government entity which sponsors and promotes research of import to the Intelligence Community which includes but is not limited to the CIA, DIA, NSA, NIMA, and NRO." ></td>
	<td class="line x" title="13:226	document summarization systems need to summarize different opinions and perspectives." ></td>
	<td class="line x" title="14:226	Spam filtering systems must recognize rants and emotional tirades, among other things." ></td>
	<td class="line x" title="15:226	In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information." ></td>
	<td class="line x" title="16:226	Subjective language has been previously studied in fields such as linguistics, literary theory, psychology, and content analysis." ></td>
	<td class="line x" title="17:226	Some manually-developed knowledge resources exist, but there is no comprehensive dictionary of subjective language." ></td>
	<td class="line x" title="18:226	Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002) are bootstrapping algorithms that use automatically generated extraction patterns to identify words belonging to a semantic category." ></td>
	<td class="line x" title="19:226	We hypothesized that extraction patterns could also identify subjective words." ></td>
	<td class="line x" title="20:226	For example, the pattern expressed <direct object> often extracts subjective nouns, such as concern, hope, and support." ></td>
	<td class="line x" title="21:226	Furthermore, these bootstrapping algorithms require only a handful of seed words and unannotated texts for training; no annotated data is needed at all." ></td>
	<td class="line x" title="22:226	In this paper, we use the Meta-Bootstrapping and Basilisk algorithms to learn lists of subjective nouns from a large collection of unannotated texts." ></td>
	<td class="line x" title="23:226	Then we train a subjectivity classifier on a small set of annotated data, using the subjective nouns as features along with some other previously identified subjectivity features." ></td>
	<td class="line x" title="24:226	Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al. , 1999)." ></td>
	<td class="line x" title="25:226	2 Subjectivity Data 2.1 The Annotation Scheme In 2002, an annotation scheme was developed for a U.S. government-sponsored project with a team of 10 researchers (the annotation instructions and project reports are available on the Web at http://www.cs.pitt.edu/wiebe/pubs/ardasummer02/)." ></td>
	<td class="line x" title="26:226	Edmonton, May-June 2003 held at HLT-NAACL 2003, pp." ></td>
	<td class="line x" title="27:226	25-32 Proceeings of the Seventh CoNLL conference The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982)." ></td>
	<td class="line x" title="28:226	The scheme is more detailed and comprehensive than previous ones." ></td>
	<td class="line x" title="29:226	We mention only those aspects of the annotation scheme relevant to this paper." ></td>
	<td class="line x" title="30:226	The goal of the annotation scheme is to identify and characterize expressions of private states in a sentence." ></td>
	<td class="line x" title="31:226	Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. , 1985)." ></td>
	<td class="line x" title="32:226	For example, in sentence (1) the writer is expressing a negative evaluation." ></td>
	<td class="line x" title="33:226	(1) The time has come, gentlemen, for Sharon, the assassin, to realize that injustice cannot last long. Sentence (2) reflects the private state of Western countries." ></td>
	<td class="line x" title="34:226	Mugabes use of overwhelmingly also reflects a private state, his positive reaction to and characterization of his victory." ></td>
	<td class="line x" title="35:226	(2) Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwes presidential election. Annotators are also asked to judge the strength of each private state." ></td>
	<td class="line x" title="36:226	A private state can have low, medium, high or extreme strength." ></td>
	<td class="line x" title="37:226	2.2 Corpus and Agreement Results Our data consists of English-language versions of foreign news documents from FBIS, the U.S. Foreign Broadcast Information Service." ></td>
	<td class="line x" title="38:226	The data is from a variety of publications and countries." ></td>
	<td class="line x" title="39:226	The annotated corpus used to train and test our subjectivity classifiers (the experiment corpus) consists of 109 documents with a total of 2197 sentences." ></td>
	<td class="line x" title="40:226	We used a separate, annotated tuning corpus of 33 documents with a total of 698 sentences to establish some experimental parameters.1 Each document was annotated by one or both of two annotators, A and T. To allow us to measure interannotator agreement, the annotators independently annotated the same 12 documents with a total of 178 sentences." ></td>
	<td class="line x" title="41:226	We began with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private-state expression, of any strength, anywhere in the sentence." ></td>
	<td class="line x" title="42:226	If so, the sentence should be subjective." ></td>
	<td class="line x" title="43:226	Otherwise, it is objective." ></td>
	<td class="line x" title="44:226	Table 1 shows the contingency table." ></td>
	<td class="line x" title="45:226	The percentage agreement is 88%, and the value is 0.71." ></td>
	<td class="line x" title="46:226	1The annotated data will be available to U.S. government contractors this summer." ></td>
	<td class="line x" title="47:226	We are working to resolve copyright issues to make it available to the wider research community." ></td>
	<td class="line x" title="48:226	Tagger T Subj Obj Tagger A Subj nyy = 112 nyn = 16 Obj nny = 6 nnn = 44 Table 1: Agreement for sentence-level annotations Tagger T Subj Obj Tagger A Subj nyy = 106 nyn = 9 Obj nny = 0 nnn = 44 Table 2: Agreement for sentence-level annotations, lowstrength cases removed One would expect that there are clear cases of objective sentences, clear cases of subjective sentences, and borderline sentences in between." ></td>
	<td class="line x" title="49:226	The agreement study supports this." ></td>
	<td class="line x" title="50:226	In terms of our annotations, we define a sentence as borderline if it has at least one privatestate expression identified by at least one annotator, and all strength ratings of private-state expressions are low." ></td>
	<td class="line x" title="51:226	Table 2 shows the agreement results when such borderline sentences are removed (19 sentences, or 11% of the agreement test corpus)." ></td>
	<td class="line x" title="52:226	The percentage agreement increases to 94% and the value increases to 0.87." ></td>
	<td class="line x" title="53:226	As expected, the majority of disagreement cases involve low-strength subjectivity." ></td>
	<td class="line x" title="54:226	The annotators consistently agree about which are the clear cases of subjective sentences." ></td>
	<td class="line x" title="55:226	This leads us to define the gold-standard that we use in our experiments." ></td>
	<td class="line x" title="56:226	A sentence is subjective if it contains at least one private-state expression of medium or higher strength." ></td>
	<td class="line x" title="57:226	The second class, which we call objective, consists of everything else." ></td>
	<td class="line x" title="58:226	Thus, sentences with only mild traces of subjectivity are tossed into the objective category, making the systems goal to find the clearly subjective sentences." ></td>
	<td class="line x" title="59:226	3 Using Extraction Patterns to Learn Subjective Nouns In the last few years, two bootstrapping algorithms have been developed to create semantic dictionaries by exploiting extraction patterns: Meta-Bootstrapping (Riloff and Jones, 1999) and Basilisk (Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="60:226	Extraction patterns were originally developed for information extraction tasks (Cardie, 1997)." ></td>
	<td class="line x" title="61:226	They represent lexico-syntactic expressions that typically rely on shallow parsing and syntactic role assignment." ></td>
	<td class="line x" title="62:226	For example, the pattern <subject> was hired would apply to sentences that contain the verb hired in the passive voice." ></td>
	<td class="line x" title="63:226	The subject would be extracted as the hiree." ></td>
	<td class="line x" title="64:226	Meta-Bootstrapping and Basilisk were designed to learn words that belong to a semantic category (e.g. , truck is a VEHICLE and seashore is a LOCATION)." ></td>
	<td class="line x" title="65:226	Both algorithms begin with unannotated texts and seed words that represent a semantic category." ></td>
	<td class="line x" title="66:226	A bootstrapping process looks for words that appear in the same extraction patterns as the seeds and hypothesizes that those words belong to the same semantic class." ></td>
	<td class="line x" title="67:226	The principle behind this approach is that words of the same semantic class appear in similar pattern contexts." ></td>
	<td class="line x" title="68:226	For example, the phrases lived in and traveled to will co-occur with many noun phrases that represent LOCATIONS." ></td>
	<td class="line x" title="69:226	In our research, we want to automatically identify words that are subjective." ></td>
	<td class="line x" title="70:226	Subjective terms have many different semantic meanings, but we believe that the same contextual principle applies to subjectivity." ></td>
	<td class="line x" title="71:226	In this section, we briefly overview these bootstrapping algorithms and explain how we used them to generate lists of subjective nouns." ></td>
	<td class="line x" title="72:226	3.1 Meta-Bootstrapping The Meta-Bootstrapping (MetaBoot) process (Riloff and Jones, 1999) begins with a small set of seed words that represent a targeted semantic category (e.g. , 10 words that represent LOCATIONS) and an unannotated corpus." ></td>
	<td class="line x" title="73:226	First, MetaBoot automatically creates a set of extraction patterns for the corpus by applying and instantiating syntactic templates." ></td>
	<td class="line x" title="74:226	This process literally produces thousands of extraction patterns that, collectively, will extract every noun phrase in the corpus." ></td>
	<td class="line x" title="75:226	Next, MetaBoot computes a score for each pattern based upon the number of seed words among its extractions." ></td>
	<td class="line x" title="76:226	The best pattern is saved and all of its extracted noun phrases are automatically labeled as the targeted semantic category.2 MetaBoot then re-scores the extraction patterns, using the original seed words as well as the newly labeled words, and the process repeats." ></td>
	<td class="line x" title="77:226	This procedure is called mutual bootstrapping." ></td>
	<td class="line x" title="78:226	A second level of bootstrapping (the meta- bootstrapping part) makes the algorithm more robust." ></td>
	<td class="line x" title="79:226	When the mutual bootstrapping process is finished, all nouns that were put into the semantic dictionary are reevaluated." ></td>
	<td class="line x" title="80:226	Each noun is assigned a score based on how many different patterns extracted it." ></td>
	<td class="line x" title="81:226	Only the five best nouns are allowed to remain in the dictionary." ></td>
	<td class="line x" title="82:226	The other entries are discarded, and the mutual bootstrapping process starts over again using the revised semantic dictionary." ></td>
	<td class="line x" title="83:226	3.2 Basilisk Basilisk (Thelen and Riloff, 2002) is a more recent bootstrapping algorithm that also utilizes extraction patterns to create a semantic dictionary." ></td>
	<td class="line x" title="84:226	Similarly, Basilisk begins with an unannotated text corpus and a small set of 2Our implementation of Meta-Bootstrapping learns individual nouns (vs. noun phrases) and discards capitalized words." ></td>
	<td class="line x" title="85:226	seed words for a semantic category." ></td>
	<td class="line x" title="86:226	The bootstrapping process involves three steps." ></td>
	<td class="line x" title="87:226	(1) Basilisk automatically generates a set of extraction patterns for the corpus and scores each pattern based upon the number of seed words among its extractions." ></td>
	<td class="line x" title="88:226	This step is identical to the first step of Meta-Bootstrapping." ></td>
	<td class="line x" title="89:226	Basilisk then puts the best patterns into a Pattern Pool." ></td>
	<td class="line x" title="90:226	(2) All nouns3 extracted by a pattern in the Pattern Pool are put into a Candidate Word Pool." ></td>
	<td class="line x" title="91:226	Basilisk scores each noun based upon the set of patterns that extracted it and their collective association with the seed words." ></td>
	<td class="line x" title="92:226	(3) The top 10 nouns are labeled as the targeted semantic class and are added to the dictionary." ></td>
	<td class="line x" title="93:226	The bootstrapping process then repeats, using the original seeds and the newly labeled words." ></td>
	<td class="line x" title="94:226	The main difference between Basilisk and MetaBootstrapping is that Basilisk scores each noun based on collective information gathered from all patterns that extracted it." ></td>
	<td class="line x" title="95:226	In contrast, Meta-Bootstrapping identifies a single best pattern and assumes that everything it extracted belongs to the same semantic class." ></td>
	<td class="line x" title="96:226	The second level of bootstrapping smoothes over some of the problems caused by this assumption." ></td>
	<td class="line x" title="97:226	In comparative experiments (Thelen and Riloff, 2002), Basilisk outperformed Meta-Bootstrapping." ></td>
	<td class="line x" title="98:226	But since our goal of learning subjective nouns is different from the original intent of the algorithms, we tried them both." ></td>
	<td class="line x" title="99:226	We also suspected they might learn different words, in which case using both algorithms could be worthwhile." ></td>
	<td class="line x" title="100:226	3.3 Experimental Results The Meta-Bootstrapping and Basilisk algorithms need seed words and an unannotated text corpus as input." ></td>
	<td class="line x" title="101:226	Since we did not need annotated texts, we created a much larger training corpus, the bootstrapping corpus, by gathering 950 new texts from the FBIS source mentioned in Section 2.2." ></td>
	<td class="line x" title="102:226	To find candidate seed words, we automatically identified 850 nouns that were positively correlated with subjective sentences in another data set." ></td>
	<td class="line x" title="103:226	However, it is crucial that the seed words occur frequently in our FBIS texts or the bootstrapping process will not get off the ground." ></td>
	<td class="line x" title="104:226	So we searched for each of the 850 nouns in the bootstrapping corpus, sorted them by frequency, and manually selected 20 high-frequency words that we judged to be strongly subjective." ></td>
	<td class="line x" title="105:226	Table 3 shows the 20 seed words used for both Meta-Bootstrapping and Basilisk." ></td>
	<td class="line x" title="106:226	We ran each bootstrapping algorithm for 400 iterations, generating 5 words per iteration." ></td>
	<td class="line x" title="107:226	Basilisk generated 2000 nouns and Meta-Bootstrapping generated 1996 nouns.4 Table 4 shows some examples of extraction pat3Technically, each head noun of an extracted noun phrase." ></td>
	<td class="line x" title="108:226	4Meta-Bootstrapping will sometimes produce fewer than 5 words per iteration if it has low confidence in its judgements." ></td>
	<td class="line x" title="109:226	cowardice embarrassment hatred outrage crap fool hell slander delight gloom hypocrisy sigh disdain grievance love twit dismay happiness nonsense virtue Table 3: Subjective Seed Words Extraction Patterns Examples of Extracted Nouns expressed <dobj> condolences, hope, grief, views, worries, recognition indicative of <np> compromise, desire, thinking inject <dobj> vitality, hatred reaffirmed <dobj> resolve, position, commitment voiced <dobj> outrage, support, skepticism, disagreement, opposition, concerns, gratitude, indignation show of <np> support, strength, goodwill, solidarity, feeling <subject> was shared anxiety, view, niceties, feeling Table 4: Extraction Pattern Examples terns that were discovered to be associated with subjective nouns." ></td>
	<td class="line x" title="110:226	Meta-Bootstrapping and Basilisk are semi-automatic lexicon generation tools because, although the bootstrapping process is 100% automatic, the resulting lexicons need to be reviewed by a human.5 So we manually reviewed the 3996 words proposed by the algorithms." ></td>
	<td class="line x" title="111:226	This process is very fast; it takes only a few seconds to classify each word." ></td>
	<td class="line x" title="112:226	The entire review process took approximately 3-4 hours." ></td>
	<td class="line x" title="113:226	One author did this labeling; this person did not look at or run tests on the experiment corpus." ></td>
	<td class="line x" title="114:226	Strong Subjective Weak Subjective tyranny scum aberration plague smokescreen bully allusion risk apologist devil apprehensions drama barbarian liar beneficiary trick belligerence pariah resistant promise condemnation venom credence intrigue sanctimonious diatribe distortion unity exaggeration mockery eyebrows failures repudiation anguish inclination tolerance insinuation fallacies liability persistent antagonism evil assault trust atrocities genius benefit success denunciation goodwill blood spirit exploitation injustice controversy slump humiliation innuendo likelihood sincerity ill-treatment revenge peaceful eternity sympathy rogue pressure rejection Table 5: Examples of Learned Subjective Nouns 5This is because NLP systems expect dictionaries to have high integrity." ></td>
	<td class="line x" title="115:226	Even if the algorithms could achieve 90% accuracy, a dictionary in which 1 of every 10 words is defined incorrectly would probably not be desirable." ></td>
	<td class="line x" title="116:226	B M B \ M B [ M StrongSubj 372 192 110 454 WeakSubj 453 330 185 598 Total 825 522 295 1052 Table 6: Subjective Word Lexicons after Manual Review (B=Basilisk, M=MetaBootstrapping) 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 200 400 600 800 1000 1200 1400 1600 1800 2000 % of Words Subjective Number of Words Generated Basilisk MetaBoot Figure 1: Accuracy during Bootstrapping We classified the words as StrongSubjective, WeakSubjective, or Objective." ></td>
	<td class="line x" title="117:226	Objective terms are not subjective at all (e.g. , chair or city)." ></td>
	<td class="line x" title="118:226	StrongSubjective terms have strong, unambiguously subjective connotations, such as bully or barbarian." ></td>
	<td class="line x" title="119:226	WeakSubjective was used for three situations: (1) words that have weak subjective connotations, such as aberration which implies something out of the ordinary but does not evoke a strong sense of judgement, (2) words that have multiple senses or uses, where one is subjective but the other is not." ></td>
	<td class="line x" title="120:226	For example, the word plague can refer to a disease (objective) or an onslaught of something negative (subjective), (3) words that are objective by themselves but appear in idiomatic expressions that are subjective." ></td>
	<td class="line x" title="121:226	For example, the word eyebrows was labeled WeakSubjective because the expression raised eyebrows probably occurs more often in our corpus than literal references to eyebrows." ></td>
	<td class="line x" title="122:226	Table 5 shows examples of learned words that were classified as StrongSubjective or WeakSubjective." ></td>
	<td class="line x" title="123:226	Once the words had been manually classified, we could go back and measure the effectiveness of the algorithms." ></td>
	<td class="line x" title="124:226	The graph in Figure 1 tracks their accuracy as the bootstrapping progressed." ></td>
	<td class="line x" title="125:226	The X-axis shows the number of words generated so far." ></td>
	<td class="line x" title="126:226	The Y-axis shows the percentage of those words that were manually classified as subjective." ></td>
	<td class="line x" title="127:226	As is typical of bootstrapping algorithms, accuracy was high during the initial iterations but tapered off as the bootstrapping continued." ></td>
	<td class="line x" title="128:226	After 20 words, both algorithms were 95% accurate." ></td>
	<td class="line x" title="129:226	After 100 words Basilisk was 75% accurate and MetaBoot was 81% accurate." ></td>
	<td class="line x" title="130:226	After 1000 words, accuracy dropped to about 28% for MetaBoot, but Basilisk was still performing reasonably well at 53%." ></td>
	<td class="line x" title="131:226	Although 53% accuracy is not high for a fully automatic process, Basilisk depends on a human to review the words so 53% accuracy means that the human is accepting every other word, on average." ></td>
	<td class="line x" title="132:226	Thus, the reviewers time was still being spent productively even after 1000 words had been hypothesized." ></td>
	<td class="line x" title="133:226	Table 6 shows the size of the final lexicons created by the bootstrapping algorithms." ></td>
	<td class="line x" title="134:226	The first two columns show the number of subjective terms learned by Basilisk and Meta-Bootstrapping." ></td>
	<td class="line x" title="135:226	Basilisk was more prolific, generating 825 subjective terms compared to 522 for MetaBootstrapping." ></td>
	<td class="line x" title="136:226	The third column shows the intersection between their word lists." ></td>
	<td class="line x" title="137:226	There was substantial overlap, but both algorithms produced many words that the other did not." ></td>
	<td class="line x" title="138:226	The last column shows the results of merging their lists." ></td>
	<td class="line x" title="139:226	In total, the bootstrapping algorithms produced 1052 subjective nouns." ></td>
	<td class="line x" title="140:226	4 Creating Subjectivity Classifiers To evaluate the subjective nouns, we trained a Naive Bayes classifier using the nouns as features." ></td>
	<td class="line x" title="141:226	We also incorporated previously established subjectivity clues, and added some new discourse features." ></td>
	<td class="line x" title="142:226	In this section, we describe all the feature sets and present performance results for subjectivity classifiers trained on different combinations of these features." ></td>
	<td class="line x" title="143:226	The threshold values and feature representations used in this section are the ones that produced the best results on our separate tuning corpus." ></td>
	<td class="line x" title="144:226	4.1 Subjective Noun Features We defined four features to represent the sets of subjective nouns produced by the bootstrapping algorithms." ></td>
	<td class="line x" title="145:226	BA-Strong: the set of StrongSubjective nouns generated by Basilisk BA-Weak: the set of WeakSubjective nouns generated by Basilisk MB-Strong: the set of StrongSubjective nouns generated by Meta-Bootstrapping MB-Weak: the set of WeakSubjective nouns generated by Meta-Bootstrapping For each set, we created a three-valued feature based on the presence of 0, 1, or 2 words from that set." ></td>
	<td class="line x" title="146:226	We used the nouns as feature sets, rather than define a separate feature for each word, so the classifier could generalize over the set to minimize sparse data problems." ></td>
	<td class="line x" title="147:226	We will refer to these as the SubjNoun features." ></td>
	<td class="line x" title="148:226	4.2 Previously Established Features Wiebe, Bruce, & OHara (1999) developed a machine learning system to classify subjective sentences." ></td>
	<td class="line x" title="149:226	We experimented with the features that they used, both to compare their results to ours and to see if we could benefit from their features." ></td>
	<td class="line x" title="150:226	We will refer to these as the WBO features." ></td>
	<td class="line x" title="151:226	WBO includes a set of stems positively correlated with the subjective training examples (subjStems) and a set of stems positively correlated with the objective training examples (objStems)." ></td>
	<td class="line x" title="152:226	We defined a three-valued feature for the presence of 0, 1, or 2 members of subjStems in a sentence, and likewise for objStems." ></td>
	<td class="line x" title="153:226	For our experiments, subjStems includes stems that appear 7 times in the training set, and for which the precision is 1.25 times the baseline word precision for that training set." ></td>
	<td class="line x" title="154:226	objStems contains the stems that appear 7 times and for which at least 50% of their occurrences in the training set are in objective sentences." ></td>
	<td class="line x" title="155:226	WBO also includes a binary feature for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not." ></td>
	<td class="line x" title="156:226	We also added manually-developed features found by other researchers." ></td>
	<td class="line x" title="157:226	We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981), some Framenet lemmas with frame element experiencer (Baker et al. , 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and some subjectivity clues listed in (Wiebe, 1990)." ></td>
	<td class="line x" title="158:226	We represented each set as a three-valued feature based on the presence of 0, 1, or 2 members of the set." ></td>
	<td class="line x" title="159:226	We will refer to these as the manual features." ></td>
	<td class="line x" title="160:226	4.3 Discourse Features We created discourse features to capture the density of clues in the text surrounding a sentence." ></td>
	<td class="line x" title="161:226	First, we computed the average number of subjective clues and objective clues per sentence, normalized by sentence length." ></td>
	<td class="line x" title="162:226	The subjective clues, subjClues, are all sets for which 3-valued features were defined above (except objStems)." ></td>
	<td class="line x" title="163:226	The objective clues consist only of objStems." ></td>
	<td class="line x" title="164:226	For sentence S, let ClueRatesubj(S) = jsubjClues in SjjSj and ClueRateobj(S) = jobjStems in SjjSj." ></td>
	<td class="line x" title="165:226	Then we define AvgClueRatesubj to be the average of ClueRate(S) over all sentences S and similarly for AvgClueRateobj." ></td>
	<td class="line x" title="166:226	Next, we characterize the number of subjective and objective clues in the previous and next sentences as: higher-than-expected (high), lower-than-expected (low), or expected (medium)." ></td>
	<td class="line x" title="167:226	The value for ClueRatesubj(S) is high if ClueRatesubj(S) AvgClueRatesubj 1:3; low if ClueRatesubj(S) AvgClueRatesubj=1:3; otherwise it is medium." ></td>
	<td class="line x" title="168:226	The values for ClueRateobj(S) are defined similarly." ></td>
	<td class="line x" title="169:226	Using these definitions we created four features: ClueRatesubj for the previous and following sentences, and ClueRateobj for the previous and following sentences." ></td>
	<td class="line x" title="170:226	We also defined a feature for sentence length." ></td>
	<td class="line x" title="171:226	Let AvgSentLen be the average sentence length." ></td>
	<td class="line x" title="172:226	SentLen(S) is high if length(S) AvgSentLen 1:3; low if length(S) AvgSentLen=1:3; and medium otherwise." ></td>
	<td class="line x" title="173:226	4.4 Classification Results We conducted experiments to evaluate the performance of the feature sets, both individually and in various combinations." ></td>
	<td class="line x" title="174:226	Unless otherwise noted, all experiments involved training a Naive Bayes classifier using a particular set of features." ></td>
	<td class="line x" title="175:226	We evaluated each classifier using 25fold cross validation on the experiment corpus and used paired t-tests to measure significance at the 95% confidence level." ></td>
	<td class="line x" title="176:226	As our evaluation metrics, we computed accuracy (Acc) as the percentage of the systems classifications that match the gold-standard, and precision (Prec) and recall (Rec) with respect to subjective sentences." ></td>
	<td class="line x" title="177:226	Acc Prec Rec (1) Bag-Of-Words 73.3 81.7 70.9 (2) WBO 72.1 76.0 77.4 (3) Most-Frequent 59.0 59.0 100.0 Table 7: Baselines for Comparison Table 7 shows three baseline experiments." ></td>
	<td class="line x" title="178:226	Row (3) represents the common baseline of assigning every sentence to the most frequent class." ></td>
	<td class="line x" title="179:226	The Most-Frequent baseline achieves 59% accuracy because 59% of the sentences in the gold-standard are subjective." ></td>
	<td class="line x" title="180:226	Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al. , 1999)." ></td>
	<td class="line x" title="181:226	Row (1) shows a Naive Bayes classifier that uses unigram bag-ofwords features, with one binary feature for the absence or presence in the sentence of each word that appeared during training." ></td>
	<td class="line x" title="182:226	Pang et al.(2002) reported that a similar experiment produced their best results on a related classification task." ></td>
	<td class="line x" title="184:226	The difference in accuracy between Rows (1) and (2) is not statistically significant (Bag-of-Words higher precision is balanced by WBOs higher recall)." ></td>
	<td class="line x" title="185:226	Next, we trained a Naive Bayes classifier using only the SubjNoun features." ></td>
	<td class="line x" title="186:226	This classifier achieved good precision (77%) but only moderate recall (64%)." ></td>
	<td class="line x" title="187:226	Upon further inspection, we discovered that the subjective nouns are good subjectivity indicators when they appear, but not every subjective sentence contains one of them." ></td>
	<td class="line x" title="188:226	And, relatively few sentences contain more than one, making it difficult to recognize contextual effects (i.e. , multiple clues in a region)." ></td>
	<td class="line x" title="189:226	We concluded that the appropriate way to benefit from the subjective nouns is to use them in tandem with other subjectivity clues." ></td>
	<td class="line x" title="190:226	Acc Prec Rec (1) 76.1 81.3 77.4 WBO+SubjNoun+ manual+discourse (2) 74.3 78.6 77.8 WBO+SubjNoun (3) 72.1 76.0 77.4 WBO Table 8: Results with New Features Table 8 shows the results of Naive Bayes classifiers trained with different combinations of features." ></td>
	<td class="line x" title="191:226	The accuracy differences between all pairs of experiments in Table 8 are statistically significant." ></td>
	<td class="line x" title="192:226	Row (3) uses only the WBO features (also shown in Table 7 as a baseline)." ></td>
	<td class="line x" title="193:226	Row (2) uses the WBO features as well as the SubjNoun features." ></td>
	<td class="line x" title="194:226	There is a synergy between these feature sets: using both types of features achieves better performance than either one alone." ></td>
	<td class="line x" title="195:226	The difference is mainly precision, presumably because the classifier found more and better combinations of features." ></td>
	<td class="line x" title="196:226	In Row (1), we also added the manual and discourse features." ></td>
	<td class="line x" title="197:226	The discourse features explicitly identify contexts in which multiple clues are found." ></td>
	<td class="line x" title="198:226	This classifier produced even better performance, achieving 81.3% precision with 77.4% recall." ></td>
	<td class="line x" title="199:226	The 76.1% accuracy result is significantly higher than the accuracy results for all of the other classifiers (in both Table 8 and Table 7)." ></td>
	<td class="line x" title="200:226	Finally, higher precision classification can be obtained by simply classifying a sentence as subjective if it contains any of the StrongSubjective nouns." ></td>
	<td class="line x" title="201:226	On our data, this method produces 87% precision with 26% recall." ></td>
	<td class="line x" title="202:226	This approach could support applications for which precision is paramount." ></td>
	<td class="line x" title="203:226	5 Related Work Several types of research have involved document-level subjectivity classification." ></td>
	<td class="line oc" title="204:226	Some work identifies inflammatory texts (e.g. , (Spertus, 1997)) or classifies reviews as positive or negative ((Turney, 2002; Pang et al. , 2002))." ></td>
	<td class="line x" title="205:226	Tongs system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time." ></td>
	<td class="line x" title="206:226	Research in genre classification may include recognition of subjective genres such as editorials (e.g. , (Karlgren and Cutting, 1994; Kessler et al. , 1997; Wiebe et al. , 2001))." ></td>
	<td class="line x" title="207:226	In contrast, our work classifies individual sentences, as does the research in (Wiebe et al. , 1999)." ></td>
	<td class="line x" title="208:226	Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences." ></td>
	<td class="line x" title="209:226	For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al. , 2001) reported that, in their corpus, 44% of sentences (in articles that are not editorials or reviews) were subjective." ></td>
	<td class="line x" title="210:226	Some previous work has focused explicitly on learning subjective words and phrases." ></td>
	<td class="line x" title="211:226	(Hatzivassiloglou and McKeown, 1997) describes a method for identifying the semantic orientation of words, for example that beautiful expresses positive sentiments." ></td>
	<td class="line oc" title="212:226	Researchers have focused on learning adjectives or adjectival phrases (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000) and verbs (Wiebe et al. , 2001), but no previous work has focused on learning nouns." ></td>
	<td class="line x" title="213:226	A unique aspect of our work is the use of bootstrapping methods that exploit extraction patterns." ></td>
	<td class="line oc" title="214:226	(Turney, 2002) used patterns representing part-of-speech sequences, (Hatzivassiloglou and McKeown, 1997) recognized adjectival phrases, and (Wiebe et al. , 2001) learned N-grams." ></td>
	<td class="line x" title="215:226	The extraction patterns used in our research are linguistically richer patterns, requiring shallow parsing and syntactic role assignment." ></td>
	<td class="line x" title="216:226	In recent years several techniques have been developed for semantic lexicon creation (e.g. , (Hearst, 1992; Riloff and Shepherd, 1997; Roark and Charniak, 1998; Caraballo, 1999))." ></td>
	<td class="line x" title="217:226	Semantic word learning is different from subjective word learning, but we have shown that MetaBootstrapping and Basilisk could be successfully applied to subjectivity learning." ></td>
	<td class="line x" title="218:226	Perhaps some of these other methods could also be used to learn subjective words." ></td>
	<td class="line x" title="219:226	6 Conclusions This research produced interesting insights as well as performance results." ></td>
	<td class="line x" title="220:226	First, we demonstrated that weakly supervised bootstrapping techniques can learn subjective terms from unannotated texts." ></td>
	<td class="line x" title="221:226	Subjective features learned from unannotated documents can augment or enhance features learned from annotated training data using more traditional supervised learning techniques." ></td>
	<td class="line x" title="222:226	Second, Basilisk and Meta-Bootstrapping proved to be useful for a different task than they were originally intended." ></td>
	<td class="line x" title="223:226	By seeding the algorithms with subjective words, the extraction patterns identified expressions that are associated with subjective nouns." ></td>
	<td class="line x" title="224:226	This suggests that the bootstrapping algorithms should be able to learn not only general semantic categories, but any category for which words appear in similar linguistic phrases." ></td>
	<td class="line x" title="225:226	Third, our best subjectivity classifier used a wide variety of features." ></td>
	<td class="line x" title="226:226	Subjectivity is a complex linguistic phenomenon and our evidence suggests that reliable subjectivity classification requires a broad array of features." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1014
Learning Extraction Patterns For Subjective Expressions
Riloff, Ellen;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:210	Learning Extraction Patterns for Subjective Expressions Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112 riloff@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Abstract This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions." ></td>
	<td class="line x" title="2:210	High-precision classifiers label unannotated data to automatically create a large training set, which is then given to an extraction pattern learning algorithm." ></td>
	<td class="line x" title="3:210	The learned patterns are then used to identify more subjective sentences." ></td>
	<td class="line x" title="4:210	The bootstrapping process learns many subjective patterns and increases recall while maintaining high precision." ></td>
	<td class="line x" title="5:210	1 Introduction Many natural language processing applications could benefit from being able to distinguish between factual and subjective information." ></td>
	<td class="line x" title="6:210	Subjective remarks come in a variety of forms, including opinions, rants, allegations, accusations, suspicions, and speculations." ></td>
	<td class="line x" title="7:210	Ideally, information extraction systems should be able to distinguish between factual information (which should be extracted) and non-factual information (which should be discarded or labeled as uncertain)." ></td>
	<td class="line x" title="8:210	Question answering systems should distinguish between factual and speculative answers." ></td>
	<td class="line x" title="9:210	Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources." ></td>
	<td class="line x" title="10:210	Multidocument summarization systems need to summarize different opinions and perspectives." ></td>
	<td class="line x" title="11:210	Spam filtering systems This work was supported by the National Science Foundation under grants IIS-0208798, IIS-0208985, and IRI-9704240." ></td>
	<td class="line x" title="12:210	The data preparation was performed in support of the Northeast Regional Research Center (NRRC) which is sponsored by the Advanced Research and Development Activity (ARDA), a U.S. Government entity which sponsors and promotes research of import to the Intelligence Community which includes but is not limited to the CIA, DIA, NSA, NIMA, and NRO." ></td>
	<td class="line x" title="13:210	must recognize rants and emotional tirades, among other things." ></td>
	<td class="line x" title="14:210	In general, nearly any system that seeks to identify information could benefit from being able to separate factual and subjective information." ></td>
	<td class="line oc" title="15:210	Some existing resources contain lists of subjective words (e.g. , Levins desire verbs (1993)), and some empirical methods in NLP have automatically identified adjectives, verbs, and N-grams that are statistically associated with subjective language (e.g. , (Turney, 2002; Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Wiebe et al. , 2001))." ></td>
	<td class="line x" title="16:210	However, subjective language can be exhibited by a staggering variety of words and phrases." ></td>
	<td class="line x" title="17:210	In addition, many subjective terms occur infrequently, such as strongly subjective adjectives (e.g. , preposterous, unseemly) and metaphorical or idiomatic phrases (e.g. , dealt a blow, swept off ones feet)." ></td>
	<td class="line x" title="18:210	Consequently, we believe that subjectivity learning systems must be trained on extremely large text collections before they will acquire a subjective vocabulary that is truly broad and comprehensive in scope." ></td>
	<td class="line x" title="19:210	To address this issue, we have been exploring the use of bootstrapping methods to allow subjectivity classifiers to learn from a collection of unannotated texts." ></td>
	<td class="line x" title="20:210	Our research uses high-precision subjectivity classifiers to automatically identify subjective and objective sentences in unannotated texts." ></td>
	<td class="line x" title="21:210	This process allows us to generate a large set of labeled sentences automatically." ></td>
	<td class="line x" title="22:210	The second emphasis of our research is using extraction patterns to represent subjective expressions." ></td>
	<td class="line x" title="23:210	These patterns are linguistically richer and more flexible than single words or N-grams." ></td>
	<td class="line x" title="24:210	Using the (automatically) labeled sentences as training data, we apply an extraction pattern learning algorithm to automatically generate patterns representing subjective expressions." ></td>
	<td class="line x" title="25:210	The learned patterns can be used to automatically identify more subjective sentences, which grows the training set, and the entire process can then be bootstrapped." ></td>
	<td class="line x" title="26:210	Our experimental results show that this bootstrapping process increases the recall of the highprecision subjective sentence classifier with little loss in precision." ></td>
	<td class="line x" title="27:210	We also find that the learned extraction patterns capture subtle connotations that are more expressive than the individual words by themselves." ></td>
	<td class="line x" title="28:210	This paper is organized as follows." ></td>
	<td class="line x" title="29:210	Section 2 discusses previous work on subjectivity analysis and extraction pattern learning." ></td>
	<td class="line x" title="30:210	Section 3 overviews our general approach, describes the high-precision subjectivity classifiers, and explains the algorithm for learning extraction patterns associated with subjectivity." ></td>
	<td class="line x" title="31:210	Section 4 describes the data that we use, presents our experimental results, and shows examples of patterns that are learned." ></td>
	<td class="line x" title="32:210	Finally, Section 5 summarizes our findings and conclusions." ></td>
	<td class="line o" title="33:210	2 Background 2.1 Subjectivity Analysis Much previous work on subjectivity recognition has focused on document-level classification." ></td>
	<td class="line oc" title="34:210	For example, (Spertus, 1997) developed a system to identify inflammatory texts and (Turney, 2002; Pang et al. , 2002) developed methods for classifying reviews as positive or negative." ></td>
	<td class="line x" title="35:210	Some research in genre classification has included the recognition of subjective genres such as editorials (e.g. , (Karlgren and Cutting, 1994; Kessler et al. , 1997; Wiebe et al. , 2001))." ></td>
	<td class="line x" title="36:210	In contrast, the goal of our work is to classify individual sentences as subjective or objective." ></td>
	<td class="line x" title="37:210	Document-level classification can distinguish between subjective texts, such as editorials and reviews, and objective texts, such as newspaper articles." ></td>
	<td class="line x" title="38:210	But in reality, most documents contain a mix of both subjective and objective sentences." ></td>
	<td class="line x" title="39:210	Subjective texts often include some factual information." ></td>
	<td class="line x" title="40:210	For example, editorial articles frequently contain factual information to back up the arguments being made, and movie reviews often mention the actors and plot of a movie as well as the theatres where its currently playing." ></td>
	<td class="line x" title="41:210	Even if one is willing to discard subjective texts in their entirety, the objective texts usually contain a great deal of subjective information in addition to facts." ></td>
	<td class="line x" title="42:210	For example, newspaper articles are generally considered to be relatively objective documents, but in a recent study (Wiebe et al. , 2001) 44% of sentences in a news collection were found to be subjective (after editorial and review articles were removed)." ></td>
	<td class="line x" title="43:210	One of the main obstacles to producing a sentencelevel subjectivity classifier is a lack of training data." ></td>
	<td class="line x" title="44:210	To train a document-level classifier, one can easily find collections of subjective texts, such as editorials and reviews." ></td>
	<td class="line x" title="45:210	For example, (Pang et al. , 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g. , number of stars) given by the reviewer." ></td>
	<td class="line x" title="46:210	It is much harder to obtain collections of individual sentences that can be easily identified as subjective or objective." ></td>
	<td class="line x" title="47:210	Previous work on sentence-level subjectivity classification (Wiebe et al. , 1999) used training corpora that had been manually annotated for subjectivity." ></td>
	<td class="line x" title="48:210	Manually producing annotations is time consuming, so the amount of available annotated sentence data is relatively small." ></td>
	<td class="line x" title="49:210	The goal of our research is to use high-precision subjectivity classifiers to automatically identify subjective and objective sentences in unannotated text corpora." ></td>
	<td class="line x" title="50:210	The high-precision classifiers label a sentence as subjective or objective when they are confident about the classification, and they leave a sentence unlabeled otherwise." ></td>
	<td class="line x" title="51:210	Unannotated texts are easy to come by, so even if the classifiers can label only 30% of the sentences as subjective or objective, they will still produce a large collection of labeled sentences." ></td>
	<td class="line x" title="52:210	Most importantly, the high-precision classifiers can generate a much larger set of labeled sentences than are currently available in manually created data sets." ></td>
	<td class="line x" title="53:210	2.2 Extraction Patterns Information extraction (IE) systems typically use lexicosyntactic patterns to identify relevant information." ></td>
	<td class="line x" title="54:210	The specific representation of these patterns varies across systems, but most patterns represent role relationships surrounding noun and verb phrases." ></td>
	<td class="line x" title="55:210	For example, an IE system designed to extract information about hijackings might use the pattern hijacking of <x>, which looks for the noun hijacking and extracts the object of the preposition of as the hijacked vehicle." ></td>
	<td class="line x" title="56:210	The pattern <x> was hijacked would extract the hijacked vehicle when it finds the verb hijacked in the passive voice, and the pattern <x> hijacked would extract the hijacker when it finds the verb hijacked in the active voice." ></td>
	<td class="line x" title="57:210	One of our hypotheses was that extraction patterns would be able to represent subjective expressions that have noncompositional meanings." ></td>
	<td class="line x" title="58:210	For example, consider the common expression drives (someone) up the wall, which expresses the feeling of being annoyed with something." ></td>
	<td class="line x" title="59:210	The meaning of this expression is quite different from the meanings of its individual words (drives, up, wall)." ></td>
	<td class="line x" title="60:210	Furthermore, this expression is not a fixed word sequence that could easily be captured by N-grams." ></td>
	<td class="line x" title="61:210	It is a relatively flexible construction that may be more generally represented as <x> drives <y> up the wall, where x and y may be arbitrary noun phrases." ></td>
	<td class="line x" title="62:210	This pattern would match many different sentences, such as George drives me up the wall, She drives the mayor up the wall, or The nosy old man drives his quiet neighbors up the wall. We also wondered whether the extraction pattern representation might reveal slight variations of the same verb or noun phrase that have different connotations." ></td>
	<td class="line x" title="63:210	For example, you can say that a comedian bombed last night, which is a subjective statement, but you cant express this sentiment with the passive voice of bombed." ></td>
	<td class="line x" title="64:210	In Section 3.2, we will show examples of extraction patterns representing subjective expressions which do in fact exhibit both of these phenomena." ></td>
	<td class="line x" title="65:210	A variety of algorithms have been developed to automatically learn extraction patterns." ></td>
	<td class="line x" title="66:210	Most of these algorithms require special training resources, such as texts annotated with domain-specific tags (e.g. , AutoSlog (Riloff, 1993), CRYSTAL (Soderland et al. , 1995), RAPIER (Califf, 1998), SRV (Freitag, 1998), WHISK (Soderland, 1999)) or manually defined keywords, frames, or object recognizers (e.g. , PALKA (Kim and Moldovan, 1993) and LIEP (Huffman, 1996))." ></td>
	<td class="line x" title="67:210	AutoSlog-TS (Riloff, 1996) takes a different approach, requiring only a corpus of unannotated texts that have been separated into those that are related to the target domain (the relevant texts) and those that are not (the irrelevant texts)." ></td>
	<td class="line x" title="68:210	Most recently, two bootstrapping algorithms have been used to learn extraction patterns." ></td>
	<td class="line x" title="69:210	Metabootstrapping (Riloff and Jones, 1999) learns both extraction patterns and a semantic lexicon using unannotated texts and seed words as input." ></td>
	<td class="line x" title="70:210	ExDisco (Yangarber et al. , 2000) uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input." ></td>
	<td class="line x" title="71:210	For our research, we adopted a learning process very similar to that used by AutoSlog-TS, which requires only relevant texts and irrelevant texts as its input." ></td>
	<td class="line x" title="72:210	We describe this learning process in more detail in the next section." ></td>
	<td class="line x" title="73:210	3 Learning and Bootstrapping Extraction Patterns for Subjectivity We have developed a bootstrapping process for subjectivity classification that explores three ideas: (1) highprecision classifiers can be used to automatically identify subjective and objective sentences from unannotated texts, (2) this data can be used as a training set to automatically learn extraction patterns associated with subjectivity, and (3) the learned patterns can be used to grow the training set, allowing this entire process to be bootstrapped." ></td>
	<td class="line x" title="74:210	Figure 1 shows the components and layout of the bootstrapping process." ></td>
	<td class="line x" title="75:210	The process begins with a large collection of unannotated text and two high precision subjectivity classifiers." ></td>
	<td class="line x" title="76:210	One classifier searches the unannotated corpus for sentences that can be labeled as subjective with high confidence, and the other classifier searches for sentences that can be labeled as objective with high confidence." ></td>
	<td class="line x" title="77:210	All other sentences in the corpus are left unlabeled." ></td>
	<td class="line x" title="78:210	The labeled sentences are then fed to an extraction pattern learner, which produces a set of extraction patterns that are statistically correlated with the subjective sentences (we will call these the subjective patterns)." ></td>
	<td class="line x" title="79:210	These patterns are then used to identify more sentences within the unannotated texts that can be classified as subjective." ></td>
	<td class="line x" title="80:210	The extraction pattern learner can then retrain using the larger training set and the process repeats." ></td>
	<td class="line x" title="81:210	The subjective patterns can also be added to the highprecision subjective sentence classifier as new features to improve its performance." ></td>
	<td class="line x" title="82:210	The dashed lines in Figure 1 represent the parts of the process that are bootstrapped." ></td>
	<td class="line x" title="83:210	In this section, we will describe the high-precision sentence classifiers, the extraction pattern learning process, and the details of the bootstrapping process." ></td>
	<td class="line x" title="84:210	3.1 High-Precision Subjectivity Classifiers The high-precision classifiers (HP-Subj and HP-Obj) use lists of lexical items that have been shown in previous work to be good subjectivity clues." ></td>
	<td class="line x" title="85:210	Most of the items are single words, some are N-grams, but none involve syntactic generalizations as in the extraction patterns." ></td>
	<td class="line x" title="86:210	Any data used to develop this vocabulary does not overlap with the test sets or the unannotated data used in this paper." ></td>
	<td class="line x" title="87:210	Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981), Framenet lemmas with frame element experiencer (Baker et al. , 1998), adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997), and subjectivity clues listed in (Wiebe, 1990)." ></td>
	<td class="line x" title="88:210	Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al. , 2003)." ></td>
	<td class="line x" title="89:210	The subjectivity clues are divided into those that are strongly subjective and those that are weakly subjective, using a combination of manual review and empirical results on a small training set of manually annotated data." ></td>
	<td class="line x" title="90:210	As the terms are used here, a strongly subjective clue is one that is seldom used without a subjective meaning, whereas a weakly subjective clue is one that commonly has both subjective and objective uses." ></td>
	<td class="line x" title="91:210	The high-precision subjective classifier classifies a sentence as subjective if it contains two or more of the strongly subjective clues." ></td>
	<td class="line x" title="92:210	On a manually annotated test set, this classifier achieves 91.5% precision and 31.9% recall (that is, 91.5% of the sentences that it selected are subjective, and it found 31.9% of the subjective sentences in the test set)." ></td>
	<td class="line x" title="93:210	This test set consists of 2197 sentences, 59% of which are subjective." ></td>
	<td class="line x" title="94:210	The high-precision objective classifier takes a different approach." ></td>
	<td class="line x" title="95:210	Rather than looking for the presence of lexical items, it looks for their absence." ></td>
	<td class="line x" title="96:210	It classifies a sentence as objective if there are no strongly subjective clues and at most one weakly subjective clue in the current, previous, and next sentence combined." ></td>
	<td class="line x" title="97:210	Why doesnt the objective classifier mirror the subjective classifier, and consult its own list of strongly objective clues?" ></td>
	<td class="line x" title="98:210	There are certainly lexical items that are statistically correlated with the obKnown Subjective Vocabulary HighPrecision Objective Sentence Classifier (HPObj) HighPrecision Subjective Sentence Classifier (HPSubj) Unannotated Text Collection unlabeled sentences unlabeled sentences unlabeled sentences Patternbased Subjective Sentence Classifier Extraction Pattern Learner subjective sentences subjective sentences objective sentences subjective patterns subjective patterns Figure 1: Bootstrapping Process jective class (examples are cardinal numbers (Wiebe et al. , 1999), and words such as per, case, market, and total), but the presence of such clues does not readily lead to high precision objective classification." ></td>
	<td class="line x" title="99:210	Add sarcasm or a negative evaluation to a sentence about a dry topic such as stock prices, and the sentence becomes subjective." ></td>
	<td class="line x" title="100:210	Conversely, add objective topics to a sentence containing two strongly subjective words such as odious and scumbag, and the sentence remains subjective." ></td>
	<td class="line x" title="101:210	The performance of the high-precision objective classifier is a bit lower than the subjective classifier: 82.6% precision and 16.4% recall on the test set mentioned above (that is, 82.6% of the sentences selected by the objective classifier are objective, and the objective classifier found 16.4% of the objective sentences in the test set)." ></td>
	<td class="line x" title="102:210	Although there is room for improvement, the performance proved to be good enough for our purposes." ></td>
	<td class="line x" title="103:210	3.2 Learning Subjective Extraction Patterns To automatically learn extraction patterns that are associated with subjectivity, we use a learning algorithm similar to AutoSlog-TS (Riloff, 1996)." ></td>
	<td class="line x" title="104:210	For training, AutoSlogTS uses a text corpus consisting of two distinct sets of texts: relevant texts (in our case, subjective sentences) and irrelevant texts (in our case, objective sentences)." ></td>
	<td class="line x" title="105:210	A set of syntactic templates represents the space of possible extraction patterns." ></td>
	<td class="line x" title="106:210	The learning process has two steps." ></td>
	<td class="line x" title="107:210	First, the syntactic templates are applied to the training corpus in an exhaustive fashion, so that extraction patterns are generated for (literally) every possible instantiation of the templates that appears in the corpus." ></td>
	<td class="line x" title="108:210	The left column of Figure 2 shows the syntactic templates used by AutoSlog-TS." ></td>
	<td class="line x" title="109:210	The right column shows a specific extraction pattern that was learned during our subjectivity experiments as an instantiation of the syntactic form on the left." ></td>
	<td class="line x" title="110:210	For example, the pattern <subj> was satisfied1 will match any sentence where the verb satisfied appears in the passive voice." ></td>
	<td class="line x" title="111:210	The pattern <subj> dealt blow represents a more complex expression that will match any sentence that contains a verb phrase with head=dealt followed by a direct object with head=blow." ></td>
	<td class="line x" title="112:210	This would match sentences such as The experience dealt a stiff blow to his pride. It is important to recognize that these patterns look for specific syntactic constructions produced by a (shallow) parser, rather than exact word sequences." ></td>
	<td class="line x" title="113:210	SYNTACTIC FORM EXAMPLE PATTERN <subj> passive-verb <subj> was satisfied <subj> active-verb <subj> complained <subj> active-verb dobj <subj> dealt blow <subj> verb infinitive <subj> appear to be <subj> aux noun <subj> has position active-verb <dobj> endorsed <dobj> infinitive <dobj> to condemn <dobj> verb infinitive <dobj> get to know <dobj> noun aux <dobj> fact is <dobj> noun prep <np> opinion on <np> active-verb prep <np> agrees with <np> passive-verb prep <np> was worried about <np> infinitive prep <np> to resort to <np> Figure 2: Syntactic Templates and Examples of Patterns that were Learned 1This is a shorthand notation for the internal representation." ></td>
	<td class="line x" title="114:210	PATTERN FREQ %SUBJ <subj> was asked 11 100% <subj> asked 128 63% <subj> is talk 5 100% talk of <np> 10 90% <subj> will talk 28 71% <subj> put an end 10 90% <subj> put 187 67% <subj> is going to be 11 82% <subj> is going 182 67% was expected from <np> 5 100% <subj> was expected 45 42% <subj> is fact 38 100% fact is <dobj> 12 100% Figure 3: Patterns with Interesting Behavior The second step of AutoSlog-TSs learning process applies all of the learned extraction patterns to the training corpus and gathers statistics for how often each pattern occurs in subjective versus objective sentences." ></td>
	<td class="line x" title="115:210	AutoSlog-TS then ranks the extraction patterns using a metric called RlogF (Riloff, 1996) and asks a human to review the ranked list and make the final decision about which patterns to keep." ></td>
	<td class="line x" title="116:210	In contrast, for this work we wanted a fully automatic process that does not depend on a human reviewer, and we were most interested in finding patterns that can identify subjective expressions with high precision." ></td>
	<td class="line x" title="117:210	So we ranked the extraction patterns using a conditional probability measure: the probability that a sentence is subjective given that a specific extraction pattern appears in it." ></td>
	<td class="line x" title="118:210	The exact formula is: Pr(subjective j patterni) = subjfreq(patterni)freq(patterni) where subjfreq(patterni) is the frequency of patterni in subjective training sentences, and freq(patterni) is the frequency of patterni in all training sentences." ></td>
	<td class="line x" title="119:210	(This may also be viewed as the precision of the pattern on the training data)." ></td>
	<td class="line x" title="120:210	Finally, we use two thresholds to select extraction patterns that are strongly associated with subjectivity in the training data." ></td>
	<td class="line x" title="121:210	We choose extraction patterns for which freq(patterni) 1 and Pr(subjective j patterni) 2." ></td>
	<td class="line x" title="122:210	Figure 3 shows some patterns learned by our system, the frequency with which they occur in the training data (FREQ) and the percentage of times they occur in subjective sentences (%SUBJ)." ></td>
	<td class="line x" title="123:210	For example, the first two rows show the behavior of two similar expressions using the verb asked." ></td>
	<td class="line x" title="124:210	100% of the sentences that contain asked in the passive voice are subjective, but only 63% of the sentences that contain asked in the active voice are subjective." ></td>
	<td class="line x" title="125:210	A human would probably not expect the active and passive voices to behave so differently." ></td>
	<td class="line x" title="126:210	To understand why this is so, we looked in the training data and found that the passive voice is often used to query someone about a specific opinion." ></td>
	<td class="line x" title="127:210	For example, here is one such sentence from our training set: Ernest Bai Koroma of RITCORP was asked to address his supporters on his views relating to full blooded Temne to head APC. In contrast, many of the sentences containing asked in the active voice are more general in nature, such as The mayor asked a newly formed JR about his petition. Figure 3 also shows that expressions using talk as a noun (e.g. , Fred is the talk of the town) are highly correlated with subjective sentences, while talk as a verb (e.g. , The mayor will talk about) are found in a mix of subjective and objective sentences." ></td>
	<td class="line x" title="128:210	Not surprisingly, longer expressions tend to be more idiomatic (and subjective) than shorter expressions (e.g. , put an end (to) vs. put; is going to be vs. is going; was expected from vs. was expected)." ></td>
	<td class="line x" title="129:210	Finally, the last two rows of Figure 3 show that expressions involving the noun fact are highly correlated with subjective expressions!" ></td>
	<td class="line x" title="130:210	These patterns match sentences such as The fact is and is a fact, which apparently are often used in subjective contexts." ></td>
	<td class="line x" title="131:210	This example illustrates that the corpus-based learning method can find phrases that might not seem subjective to a person intuitively, but that are reliable indicators of subjectivity." ></td>
	<td class="line x" title="132:210	4 Experimental Results 4.1 Subjectivity Data The text collection that we used consists of Englishlanguage versions of foreign news documents from FBIS, the U.S. Foreign Broadcast Information Service." ></td>
	<td class="line x" title="133:210	The data is from a variety of countries." ></td>
	<td class="line x" title="134:210	Our system takes unannotated data as input, but we needed annotated data to evaluate its performance." ></td>
	<td class="line x" title="135:210	We briefly describe the manual annotation scheme used to create the gold-standard, and give interannotator agreement results." ></td>
	<td class="line x" title="136:210	In 2002, a detailed annotation scheme (Wilson and Wiebe, 2003) was developed for a government-sponsored project." ></td>
	<td class="line x" title="137:210	We only mention aspects of the annotation scheme relevant to this paper." ></td>
	<td class="line x" title="138:210	The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982)." ></td>
	<td class="line x" title="139:210	The goal is to identify and characterize expressions of private states in a sentence." ></td>
	<td class="line x" title="140:210	Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. , 1985)." ></td>
	<td class="line x" title="141:210	For example, in sentence (1) the writer is expressing a negative evaluation." ></td>
	<td class="line x" title="142:210	(1) The time has come, gentlemen, for Sharon, the assassin, to realize that injustice cannot last long. Sentence (2) reflects the private state of Western countries." ></td>
	<td class="line x" title="143:210	Mugabes use of overwhelmingly also reflects a private state, his positive reaction to and characterization of his victory." ></td>
	<td class="line x" title="144:210	(2) Western countries were left frustrated and impotent after Robert Mugabe formally declared that he had overwhelmingly won Zimbabwes presidential election. Annotators are also asked to judge the strength of each private state." ></td>
	<td class="line x" title="145:210	A private state may have low, medium, high or extreme strength." ></td>
	<td class="line x" title="146:210	To allow us to measure interannotator agreement, three annotators (who are not authors of this paper) independently annotated the same 13 documents with a total of 210 sentences." ></td>
	<td class="line x" title="147:210	We begin with a strict measure of agreement at the sentence level by first considering whether the annotator marked any private-state expression, of any strength, anywhere in the sentence." ></td>
	<td class="line x" title="148:210	If so, the sentence is subjective." ></td>
	<td class="line x" title="149:210	Otherwise, it is objective." ></td>
	<td class="line x" title="150:210	The average pairwise percentage agreement is 90% and the average pairwise value is 0.77." ></td>
	<td class="line x" title="151:210	One would expect that there are clear cases of objective sentences, clear cases of subjective sentences, and borderline sentences in between." ></td>
	<td class="line x" title="152:210	The agreement study supports this." ></td>
	<td class="line x" title="153:210	In terms of our annotations, we define a sentence as borderline if it has at least one private-state expression identified by at least one annotator, and all strength ratings of private-state expressions are low." ></td>
	<td class="line x" title="154:210	On average, 11% of the corpus is borderline under this definition." ></td>
	<td class="line x" title="155:210	When those sentences are removed, the average pairwise percentage agreement increases to 95% and the average pairwise value increases to 0.89." ></td>
	<td class="line x" title="156:210	As expected, the majority of disagreement cases involve low-strength subjectivity." ></td>
	<td class="line x" title="157:210	The annotators consistently agree about which are the clear cases of subjective sentences." ></td>
	<td class="line x" title="158:210	This leads us to define the gold-standard that we use when evaluating our results." ></td>
	<td class="line x" title="159:210	A sentence is subjective if it contains at least one private-state expression of medium or higher strength." ></td>
	<td class="line x" title="160:210	The second class, which we call objective, consists of everything else." ></td>
	<td class="line x" title="161:210	4.2 Evaluation of the Learned Patterns Our pool of unannotated texts consists of 302,163 individual sentences." ></td>
	<td class="line x" title="162:210	The HP-Subj classifier initially labeled roughly 44,300 of these sentences as subjective, and the HP-Obj classifier initially labeled roughly 17,000 sentences as objective." ></td>
	<td class="line x" title="163:210	In order to keep the training set relatively balanced, we used all 17,000 objective sentences and 17,000 of the subjective sentences as training data for the extraction pattern learner." ></td>
	<td class="line x" title="164:210	17,073 extraction patterns were learned that have frequency 2 and Pr(subjective j patterni) .60 on the training data." ></td>
	<td class="line x" title="165:210	We then wanted to determine whether the extraction patterns are, in fact, good indicators of subjectivity." ></td>
	<td class="line x" title="166:210	To evaluate the patterns, we applied different subsets of them to a test set to see if they consistently occur in subjective sentences." ></td>
	<td class="line x" title="167:210	This test set consists of 3947 Figure 4: Evaluating the Learned Patterns on Test Data sentences, 54% of which are subjective." ></td>
	<td class="line x" title="168:210	Figure 4 shows sentence recall and pattern (instancelevel) precision for the learned extraction patterns on the test set." ></td>
	<td class="line x" title="169:210	In this figure, precision is the proportion of pattern instances found in the test set that are in subjective sentences, and recall is the proportion of subjective sentences that contain at least one pattern instance." ></td>
	<td class="line x" title="170:210	We evaluated 18 different subsets of the patterns, by selecting the patterns that pass certain thresholds in the training data." ></td>
	<td class="line x" title="171:210	We tried all combinations of 1 = f2,10g and 2 = f.60,.65,.70,.75,.80,.85,.90,.95,1.0g." ></td>
	<td class="line x" title="172:210	The data points corresponding to 1=2 are shown on the upper line in Figure 4, and those corresponding to 1=10 are shown on the lower line." ></td>
	<td class="line x" title="173:210	For example, the data point corresponding to 1=10 and 2=.90 evaluates only the extraction patterns that occur at least 10 times in the training data and with a probability .90 (i.e. , at least 90% of its occurrences are in subjective training sentences)." ></td>
	<td class="line x" title="174:210	Overall, the extraction patterns perform quite well." ></td>
	<td class="line x" title="175:210	The precision ranges from 71% to 85%, with the expected tradeoff between precision and recall." ></td>
	<td class="line x" title="176:210	This experiment confirms that the extraction patterns are effective at recognizing subjective expressions." ></td>
	<td class="line x" title="177:210	4.3 Evaluation of the Bootstrapping Process In our second experiment, we used the learned extraction patterns to classify previously unlabeled sentences from the unannotated text collection." ></td>
	<td class="line x" title="178:210	The new subjective sentences were then fed back into the Extraction Pattern Learner to complete the bootstrapping cycle depicted by the rightmost dashed line in Figure 1." ></td>
	<td class="line x" title="179:210	The Patternbased Subjective Sentence Classifier classifies a sentence as subjective if it contains at least one extraction pattern with 1 5 and 2 1.0 on the training data." ></td>
	<td class="line x" title="180:210	This process produced approximately 9,500 new subjective sentences that were previously unlabeled." ></td>
	<td class="line x" title="181:210	Since our bootstrapping process does not learn new objective sentences, we did not want to simply add the new subjective sentences to the training set, or it would become increasingly skewed toward subjective sentences." ></td>
	<td class="line x" title="182:210	Since HP-Obj had produced roughly 17,000 objective sentences used for training, we used the 9,500 new subjective sentences along with 7,500 of the previously identified subjective sentences as our new training set." ></td>
	<td class="line x" title="183:210	In other words, the training set that we used during the second bootstrapping cycle contained exactly the same objective sentences as the first cycle, half of the same subjective sentences as the first cycle, and 9,500 brand new subjective sentences." ></td>
	<td class="line x" title="184:210	On this second cycle of bootstrapping, the extraction pattern learner generated many new patterns that were not discovered during the first cycle." ></td>
	<td class="line x" title="185:210	4,248 new patterns were found that have 1 2 and 2 .60." ></td>
	<td class="line x" title="186:210	If we consider only the strongest (most subjective) extraction patterns, 308 new patterns were found that had 1 10 and 2 1.0." ></td>
	<td class="line x" title="187:210	This is a substantial set of new extraction patterns that seem to be very highly correlated with subjectivity." ></td>
	<td class="line x" title="188:210	An open question was whether the new patterns provide additional coverage." ></td>
	<td class="line x" title="189:210	To assess this, we did a simple test: we added the 4,248 new patterns to the original set of patterns learned during the first bootstrapping cycle." ></td>
	<td class="line x" title="190:210	Then we repeated the same analysis that we depict in Figure 4." ></td>
	<td class="line x" title="191:210	In general, the recall numbers increased by about 2-4% while the precision numbers decreased by less, from 0.5-2%." ></td>
	<td class="line x" title="192:210	In our third experiment, we evaluated whether the learned patterns can improve the coverage of the highprecision subjectivity classifier (HP-Subj), to complete the bootstrapping loop depicted in the top-most dashed line of Figure 1." ></td>
	<td class="line x" title="193:210	Our hope was that the patterns would allow more sentences from the unannotated text collection to be labeled as subjective, without a substantial drop in precision." ></td>
	<td class="line x" title="194:210	For this experiment, we selected the learned extraction patterns that had 1 10 and 2 1.0 on the training set, since these seemed likely to be the most reliable (high precision) indicators of subjectivity." ></td>
	<td class="line x" title="195:210	We modified the HP-Subj classifier to use extraction patterns as follows." ></td>
	<td class="line x" title="196:210	All sentences labeled as subjective by the original HP-Subj classifier are also labeled as subjective by the new version." ></td>
	<td class="line x" title="197:210	For previously unlabeled sentences, the new version classifies a sentence as subjective if (1) it contains two or more of the learned patterns, or (2) it contains one of the clues used by the original HPSubj classifier and at least one learned pattern." ></td>
	<td class="line x" title="198:210	Table 1 shows the performance results on the test set mentioned in Section 3.1 (2197 sentences) for both the original HPSubj classifier and the new version that uses the learned extraction patterns." ></td>
	<td class="line x" title="199:210	The extraction patterns produce a 7.2 percentage point gain in coverage, and only a 1.1 percentage point drop in precision." ></td>
	<td class="line x" title="200:210	This result shows that the learned extraction patterns do improve the performance of the high-precision subjective sentence classifier, allowing it to classify more sentences as subjective with nearly the same high reliability." ></td>
	<td class="line x" title="201:210	HP-Subj HP-Subj w/Patterns Recall Precision Recall Precision 32.9 91.3 40.1 90.2 Table 1: Bootstrapping the Learned Patterns into the High-Precision Sentence Classifier Table 2 gives examples of patterns used to augment the HP-Subj classifier which do not overlap in non-function words with any of the clues already known by the original system." ></td>
	<td class="line x" title="202:210	For each pattern, we show an example sentence from our corpus that matches the pattern." ></td>
	<td class="line x" title="203:210	5 Conclusions This research explored several avenues for improving the state-of-the-art in subjectivity analysis." ></td>
	<td class="line x" title="204:210	First, we demonstrated that high-precision subjectivity classification can be used to generate a large amount of labeled training data for subsequent learning algorithms to exploit." ></td>
	<td class="line x" title="205:210	Second, we showed that an extraction pattern learning technique can learn subjective expressions that are linguistically richer than individual words or fixed phrases." ></td>
	<td class="line x" title="206:210	We found that similar expressions may behave very differently, so that one expression may be strongly indicative of subjectivity but the other may not." ></td>
	<td class="line x" title="207:210	Third, we augmented our original high-precision subjective classifier with these newly learned extraction patterns." ></td>
	<td class="line x" title="208:210	This bootstrapping process resulted in substantially higher recall with a minimal loss in precision." ></td>
	<td class="line x" title="209:210	In future work, we plan to experiment with different configurations of these classifiers, add new subjective language learners in the bootstrapping process, and address the problem of how to identify new objective sentences during bootstrapping." ></td>
	<td class="line x" title="210:210	6 Acknowledgments We are very grateful to Theresa Wilson for her invaluable programming support and help with data preparation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W03-1017
Towards Answering Opinion Questions: Separating Facts From Opinions And Identifying The Polarity Of Opinion Sentences
Yu, Hong;Hatzivassiloglou, Vasileios;"></td>
	<td class="line x" title="1:165	Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences Hong Yu Department of Computer Science Columbia University New York, NY 10027, USA hongyu@cs.columbia.edu Vasileios Hatzivassiloglou Department of Computer Science Columbia University New York, NY 10027, USA vh@cs.columbia.edu Abstract Opinion question answering is a challenging task for natural language processing." ></td>
	<td class="line x" title="2:165	In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level." ></td>
	<td class="line x" title="3:165	We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level." ></td>
	<td class="line x" title="4:165	We also present a first model for classifying opinion sentences as positive or negative in terms of the main perspective being expressed in the opinion." ></td>
	<td class="line x" title="5:165	Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy)." ></td>
	<td class="line x" title="6:165	1 Introduction Newswire articles include those that mainly present opinions or ideas, such as editorials and letters to the editor, and those that mainly report facts such as daily news articles." ></td>
	<td class="line x" title="7:165	Text materials from many other sources also contain mixed facts and opinions." ></td>
	<td class="line x" title="8:165	For many natural language processing applications, the ability to detect and classify factual and opinion sentences offers distinct advantages in deciding what information to extract and how to organize and present this information." ></td>
	<td class="line x" title="9:165	For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives." ></td>
	<td class="line x" title="10:165	At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g. , filtering or retrieving only editorials in favor of a particular policy decision)." ></td>
	<td class="line x" title="11:165	Our motivation for building the opinion detection and classification system described in this paper is the need for organizing information in the context of question answering for complex questions." ></td>
	<td class="line x" title="12:165	Unlike questions like Who was the first man on the moon? which can be answered with a simple phrase, more intricate questions such as What are the reasons for the US-Iraq war? require long answers that must be constructed from multiple sources." ></td>
	<td class="line x" title="13:165	In such a context, it is imperative that the question answering system can discriminate between opinions and facts, and either use the appropriate type depending on the question or combine them in a meaningful presentation." ></td>
	<td class="line x" title="14:165	Perspective information can also help highlight contrasts and contradictions between different sourcesthere will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example." ></td>
	<td class="line x" title="15:165	Fully analyzing and classifying opinions involves tasks that relate to some fairly deep semantic and syntactic analysis of the text." ></td>
	<td class="line x" title="16:165	These include not only recognizing that the text is subjective, but also determining who the holder of the opinion is, what the opinion is about, and which of many possible positions the holder of the opinion expresses regarding that subject." ></td>
	<td class="line x" title="17:165	In this paper, we are presenting three of the components of our opinion detection and organization subsystem, which have already been integrated into our larger question-answering system." ></td>
	<td class="line x" title="18:165	These components deal with the initial tasks of classifying articles as mostly subjective or objective, finding opinion sentences in both kinds of articles, and determining, in general terms and without reference to a specific subject, if the opinions are positive or negative." ></td>
	<td class="line x" title="19:165	The three modules of the system discussed here provide the basis for ongoing work for further classification of opinions according to subject and opinion holder and for refining the original positive/negative attitude determination." ></td>
	<td class="line x" title="20:165	We review related work in Section 2, and then present our document-level classifier for opinion or factual articles (Section 3), three implemented techniques for detecting opinions at the sentence level (Section 4), and our approach for rating an opinion as positive or negative (Section 5)." ></td>
	<td class="line x" title="21:165	We have evaluated these methods using a large collection of news articles without additional annotation (Section 6) and an evaluation corpus of 400 sentences annotated for opinion classifications (Section 7)." ></td>
	<td class="line x" title="22:165	The results, presented in Section 8, indicate that we achieve very high performance (more than 97%) at document-level classification and respectable performance (8691%) at detecting opinion sentences and classifying them according to orientation." ></td>
	<td class="line x" title="23:165	2 Related Work Much of the earlier research in automated opinion detection has been performed by Wiebe and colleagues (Bruce and Wiebe, 1999; Wiebe et al. , 1999; Hatzivassiloglou and Wiebe, 2000; Wiebe, 2000; Wiebe et al. , 2002), who proposed methods for discriminating between subjective and objective text at the document, sentence, and phrase levels." ></td>
	<td class="line x" title="24:165	Bruce and Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al.(1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position." ></td>
	<td class="line x" title="26:165	Subsequently, Hatzivassiloglou and Wiebe (2000) showed that automatically detected gradable adjectives are a useful feature for subjectivity classification, while Wiebe (2000) introduced lexical features in addition to the presence/absence of syntactic categories." ></td>
	<td class="line x" title="27:165	More recently, Wiebe et al.(2002) report on document-level subjectivity classification, using a k-nearest neighbor algorithm based on the total count of subjective words and phrases within each document." ></td>
	<td class="line x" title="29:165	Psychological studies (Bradley and Lang, 1999) found measurable associations between words and human emotions." ></td>
	<td class="line x" title="30:165	Hatzivassiloglou and McKeown (1997) described an unsupervised learning method for obtaining positively and negatively oriented adjectives with accuracy over 90%, and demonstrated that this semantic orientation, or polarity, is a consistent lexical property with high inter-rater agreement." ></td>
	<td class="line oc" title="31:165	Turney (2002) showed that it is possible to use only a few of those semantically oriented words (namely, excellent and poor) to label other phrases co-occuring with them as positive or negative." ></td>
	<td class="line o" title="32:165	He then used these phrases to automatically separate positive and negative movie and product reviews, with accuracy of 6684%." ></td>
	<td class="line x" title="33:165	Pang et al.(2002) adopted a more direct approach, using supervised machine learning with words and n-grams as features to predict orientation at the document level with up to 83% precision." ></td>
	<td class="line x" title="35:165	Our approach to document and sentence classification of opinions builds upon the earlier work by using extended lexical models with additional features." ></td>
	<td class="line x" title="36:165	Unlike the work cited above, we do not rely on human annotations for training but only on weak metadata provided at the document level." ></td>
	<td class="line x" title="37:165	Our sentence-level classifiers introduce additional criteria for detecting subjective material (opinions), including methods based on sentence similarity within a topic and an approach that relies on multiple classifiers." ></td>
	<td class="line x" title="38:165	At the document level, our classifier uses the same document labels that the method of (Wiebe et al. , 2002) does, but automatically detects the words and phrases of importance without further analysis of the text." ></td>
	<td class="line oc" title="39:165	For determining whether an opinion sentence is positive or negative, we have used seed words similar to those produced by (Hatzivassiloglou and McKeown, 1997) and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by (Turney, 2002)." ></td>
	<td class="line oc" title="40:165	Our focus is on the sentence level, unlike (Pang et al. , 2002) and (Turney, 2002); we employ a significantly larger set of seed words, and we explore as indicators of orientation words from syntactic classes other than adjectives (nouns, verbs, and adverbs)." ></td>
	<td class="line x" title="41:165	3 Document Classification To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes1, a commonly used supervised machine-learning algorithm." ></td>
	<td class="line x" title="42:165	This approach presupposes the availability of at least a collection of articles with pre-assigned opinion and fact labels at the document level; fortunately, Wall Street Journal articles contain such metadata by identifying the type of each article as Editorial, Letter to editor, Business and News." ></td>
	<td class="line x" title="43:165	These labels are used only to provide the correct classification labels during training and evaluation, and are not included in the feature space." ></td>
	<td class="line x" title="44:165	We used as features single words, without stemming or stopword removal." ></td>
	<td class="line x" title="45:165	Naive Bayes assigns a document a0 to the class a1 that maximizes a2a4a3a5a1a7a6 a0a9a8 by applying Bayes rule a2a4a3a5a1a7a6 a0a9a8a11a10a13a12a15a14a17a16a19a18a20a12a15a14a17a21a23a22a16a19a18 a12a15a14a17a21a24a18 and assuming conditional independence of the features." ></td>
	<td class="line x" title="46:165	Although Naive Bayes can be outperformed in text classification tasks by more complex methods such as SVMs, Pang et al.(2002) report similar performance for Naive Bayes and other machine learning techniques for a similar task, that of distinguishing between positive and negative reviews at the document level." ></td>
	<td class="line x" title="48:165	Further, we achieved such high performance with Naive Bayes (see Section 8) that exploring additional techniques for this task seemed unnecessary." ></td>
	<td class="line x" title="49:165	4 Finding Opinion Sentences We developed three different approaches to classify opinions from facts at the sentence level." ></td>
	<td class="line x" title="50:165	To avoid the need for obtaining individual sentence annotations for training and evaluation, we rely instead on the expectation that documents classified as opinion on the whole (e.g. , editorials) will tend to have mostly opinion sentences, and conversely documents placed in the factual category will tend to have mostly factual sentences." ></td>
	<td class="line x" title="51:165	Wiebe et al.(2002) report that this expectation is borne out 75% of the time for opinion documents and 56% of the time for factual documents." ></td>
	<td class="line x" title="53:165	4.1 Similarity Approach Our first approach to classifying sentences as opinions or facts explores the hypothesis that, within a given topic, opinion sentences will be more similar to other opinion sentences than to factual sen1Using the Rainbow implementation, available from www." ></td>
	<td class="line x" title="54:165	cs.cmu.edu/mccallum/bow/rainbow." ></td>
	<td class="line x" title="55:165	tences." ></td>
	<td class="line x" title="56:165	We used SIMFINDER (Hatzivassiloglou et al. , 2001), a state-of-the-art system for measuring sentence similarity based on shared words, phrases, and WordNet synsets." ></td>
	<td class="line x" title="57:165	To measure the overall similarity of a sentence to the opinion or fact documents, we first select the documents that are on the same topic as the sentence in question." ></td>
	<td class="line x" title="58:165	We obtain topics as the results of IR queries (for example, by searching our document collection for welfare reform)." ></td>
	<td class="line x" title="59:165	We then average its SIMFINDER-provided similarities with each sentence in those documents." ></td>
	<td class="line x" title="60:165	Then we assign the sentence to the category for which the average is higher (we call this approach the score variant)." ></td>
	<td class="line x" title="61:165	Alternatively, for the frequency variant, we do not use the similarity scores themselves but instead we count how many of them, for each category, exceed a predetermined threshold (empirically set to 0.65)." ></td>
	<td class="line x" title="62:165	4.2 Naive Bayes Classifier Our second method trains a Naive Bayes classifier (see Section 3), using the sentences in opinion and fact documents as the examples of the two categories." ></td>
	<td class="line x" title="63:165	The features include words, bigrams, and trigrams, as well as the parts of speech in each sentence." ></td>
	<td class="line x" title="64:165	In addition, the presence of semantically oriented (positive and negative) words in a sentence is an indicator that the sentence is subjective (Hatzivassiloglou and Wiebe, 2000)." ></td>
	<td class="line x" title="65:165	Therefore, we include in our features the counts of positive and negative words in the sentence (which are obtained with the method of Section 5.1), as well as counts of the polarities of sequences of semantically oriented words (e.g. , ++ for two consecutive positively oriented words)." ></td>
	<td class="line x" title="66:165	We also include the counts of parts of speech combined with polarity information (e.g. , JJ+ for positive adjectives), as well as features encoding the polarity (if any) of the head verb, the main subject, and their immediate modifiers." ></td>
	<td class="line x" title="67:165	Syntactic structure was obtained with Charniaks statistical parser (Charniak, 2000)." ></td>
	<td class="line x" title="68:165	Finally, we used as one of the features the average semantic orientation score of the words in the sentence." ></td>
	<td class="line x" title="69:165	4.3 Multiple Naive Bayes Classifiers Our designation of all sentences in opinion or factual articles as opinion or fact sentences is an approximation." ></td>
	<td class="line x" title="70:165	To address this, we apply an algorithm using multiple classifiers, each relying on a different subset of our features." ></td>
	<td class="line x" title="71:165	The goal is to reduce the training set to the sentences that are most likely to be correctly labeled, thus boosting classification accuracy." ></td>
	<td class="line x" title="72:165	Given separate sets of features a0a2a1a4a3a5a0a7a6a8a3a4a9a8a9a4a9a10a3a5a0a12a11, we train separate Naive Bayes classifiers a13a14a1a15a3a16a13a17a6, a9a4a9a8a9a18a3a16a13a19a11 corresponding to each feature set." ></td>
	<td class="line x" title="73:165	Assuming as ground truth the information provided by the document labels and that all sentences inherit the status of their document as opinions or facts, we first traina13 a1 on the entire training set, then use the resulting classifier to predict labels for the training set." ></td>
	<td class="line x" title="74:165	The sentences that receive a label different from the assumed truth are then removed, and we train a13 a6 on the remaining sentences." ></td>
	<td class="line x" title="75:165	This process is repeated iteratively until no more sentences can be removed." ></td>
	<td class="line x" title="76:165	We report results using five feature sets, starting from words alone and adding in bigrams, trigrams, part-of-speech, and polarity." ></td>
	<td class="line x" title="77:165	5 Identifying the Polarity of Opinion Sentences Having distinguished whether a sentence is a fact or opinion, we separate positive, negative, and neutral opinions into three classes." ></td>
	<td class="line x" title="78:165	We base this decision on the number and strength of semantically oriented words (either positive or negative) in the sentence." ></td>
	<td class="line x" title="79:165	We first discuss how such words are automatically found by our system, and then describe the method by which we aggregate this information across the sentence." ></td>
	<td class="line x" title="80:165	5.1 Semantically Oriented Words To determine which words are semantically oriented, in what direction, and the strength of their orientation, we measured their co-occurrence with words from a known seed set of semantically oriented words." ></td>
	<td class="line oc" title="81:165	The approach is based on the hypothesis that positive words co-occur more than expected by chance, and so do negative words; this hypothesis was validated, at least for strong positive/negative words, in (Turney, 2002)." ></td>
	<td class="line x" title="82:165	As seed words, we used subsets of the 1,336 adjectives that were manually classified as positive (657) or negative (679) by Hatzivassiloglou and McKeown (1997)." ></td>
	<td class="line oc" title="83:165	In earlier work (Turney, 2002) only singletons were used as seed words; varying their number allows us to test whether multiple seed words have a positive effect in detection performance." ></td>
	<td class="line x" title="84:165	We experimented with seed sets containing 1, 20, 100 and over 600 positive and negative pairs of adjectives." ></td>
	<td class="line x" title="85:165	For a given seed set size, we denote the set of positive seeds as ADJa20 and the set of negative seeds as ADJa21." ></td>
	<td class="line x" title="86:165	We then calculate a modified log-likelihood ratio a22 a3a24a23a26a25a27a3 POSa28 a8 for a word a23 a25 with part of speech POSa28 (a29 can be adjective, adverb, noun or verb) as the ratio of its collocation frequency with ADJa20 and ADJa21 within a sentence, a22 a3a30a23 a25a3 POSa28 a8 a10a32a31a34a33a36a35 a37a38 Freq a14a40a39a19a41a43a42POS a44 a42ADJ a45 a18a47a46a49a48 Freqa14a50a39 alla42POSa44a42ADJa45 a18 Freqa14a50a39a51a41a30a42POSa44a42ADJa52 a18a53a46a54a48 Freqa14a50a39 alla42POSa44a42ADJa52 a18 a55a56 where Freqa3a30a23 alla3 POSa28a3 ADJa20 a8 represents the collocation frequency of all wordsa23 all of part of speech POSa28 with ADJa20 and a57 is a smoothing constant (a57 a10a59a58 a9a61a60 in our case)." ></td>
	<td class="line x" title="87:165	We used Brills tagger (Brill, 1995) to obtain part-of-speech information." ></td>
	<td class="line x" title="88:165	5.2 Sentence Polarity Tagging As our measure of semantic orientation across an entire sentence we used the average per word loglikelihood scores defined in the preceding section." ></td>
	<td class="line x" title="89:165	To determine the orientation of an opinion sentence, all that remains is to specify cutoffs a62a47a20 and a62a63a21 so that sentences for which the average log-likelihood score exceeds a62a53a20 are classified as positive opinions, sentences with scores lower than a62a63a21 are classified as negative opinions, and sentences with in-between scores are treated as neutral opinions." ></td>
	<td class="line x" title="90:165	Optimal values fora62a20 anda62a21 are obtained from the training data via density estimationusing a small, hand-labeled subset of sentences we estimate the proportion of sentences that are positive or negative." ></td>
	<td class="line x" title="91:165	The values of the average log-likelihood score that correspond to the appropriate tails of the score distribution are then determined via Monte Carlo analysis of a much larger sample of unlabeled training data." ></td>
	<td class="line x" title="92:165	6 Data We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles." ></td>
	<td class="line x" title="93:165	The aggregate collection covers six different newswire sources including 173,252 Wall Street 2http://trec.nist.gov/." ></td>
	<td class="line x" title="94:165	Journal (WSJ) articles from 1987 to 1992." ></td>
	<td class="line x" title="95:165	Some of the WSJ articles have structured headings that include Editorial, Letter to editor, Business, and News (2,877, 1,695, 2,009 and 3,714 articles, respectively)." ></td>
	<td class="line x" title="96:165	We randomly selected 2,000 articles3 from each category so that our data set was approximate evenly divided between fact and opinion articles." ></td>
	<td class="line x" title="97:165	Those articles were used for both document and sentence level opinion/fact classification." ></td>
	<td class="line x" title="98:165	7 Evaluation Metrics and Gold Standard For classification tasks (i.e. , classifying between facts and opinions and identifying the semantic orientation of sentences), we measured our systems performance by standard recall and precision." ></td>
	<td class="line x" title="99:165	We evaluated the quality of semantically oriented words by mapping the extracted words and labels to an external gold standard." ></td>
	<td class="line x" title="100:165	We took the subset of our output containing words that appear in the standard, and measured the accuracy of our output as the portion of that subset that was assigned the correct label." ></td>
	<td class="line x" title="101:165	A gold standard for document-level classification is readily available, since each article in our Wall Street Journal collection comes with an article type label (see Section 6)." ></td>
	<td class="line x" title="102:165	We mapped article types News and Business to facts, and article types Editorial and Letter to the Editor to opinions." ></td>
	<td class="line x" title="103:165	We cannot automatically select a sentence-level gold standard discriminating between facts and opinions, or between positive and negative opinions." ></td>
	<td class="line x" title="104:165	We therefore asked human evaluators to classify a set of sentences between facts and opinions as well as determine the type of opinions." ></td>
	<td class="line x" title="105:165	Since we have implemented our methods in an opinion question answering system, we selected four different topics (gun control, illegal aliens, social security, and welfare reform)." ></td>
	<td class="line x" title="106:165	For each topic, we randomly selected 25 articles from the entire combined TREC corpus (not just the WSJ portion); these were articles matching the corresponding topical phrase given above as determined by the Lucene search engine.4 From each of these documents we randomly selected four sentences." ></td>
	<td class="line x" title="107:165	If a document happened to have less than four sentences, additional 3Except for Letters to Editor, for which we included all 1,695 articles available." ></td>
	<td class="line x" title="108:165	4http://www.jguru.com/faq/Lucene." ></td>
	<td class="line x" title="109:165	Label A B Agreement Fact 123 16 46% Opinion 258 65 77% Uncertain 19 1 33% Breakdown of opinion labels Positive 33 4 29% Negative 131 27 51% No orientation 45 6 26% Mixed orientation 8 0 0% Uncertain orientation 41 1 7% Table 1: Statistics of gold standards A and B. documents from the same topic were retrieved to supply the missing sentences." ></td>
	<td class="line x" title="110:165	The resulting a0a2a1 a3 a60a4a1a5a0 a10 a0 a58a36a58 sentences were then interleaved so that successive sentences came from different topics and documents and divided into ten 50-sentence blocks." ></td>
	<td class="line x" title="111:165	Each block shares ten sentences with the preceding and following block (the last block is considered to precede the first one), so that 100 of the 400 sentences appear in two blocks." ></td>
	<td class="line x" title="112:165	Each of ten human evaluators (all with graduate training in computational linguistics) was presented with one block and asked to select a label for each sentence among the following: fact, positive opinion, negative opinion, neutral opinion, sentence contains both positive and negative opinions, opinion but cannot determine orientation, and uncertain.5 Since we have one judgment for 300 sentences and two judgments for 100 sentences, we created two gold standards for sentence classification." ></td>
	<td class="line x" title="113:165	The first (Standard A) includes the 300 sentences with one judgment and a single judgment for the remaining 100 sentences.6 The second standard (Standard B) contains the subset of the 100 sentences for which we obtained identical labels." ></td>
	<td class="line x" title="114:165	Statistics of these two standards are given in Table 1." ></td>
	<td class="line x" title="115:165	We measured the pairwise agreement among the 100 sentences that were judged by two evaluators, as the ratio of sentences that receive a label a6 from both evaluators divided by the total number of sentences receiving label a6 from any evaluator." ></td>
	<td class="line x" title="116:165	The agreement across 5The full instructions can be viewed online at http: //www1.cs.columbia.edu/hongyu/research/ emnlp03/opinion-eval-instructions.html." ></td>
	<td class="line x" title="117:165	6In order to assign a unique label, we arbitrarily chose the first evaluator for those sentences." ></td>
	<td class="line x" title="118:165	F-measure News vs. Editorial 0.96 News+Business vs. Editorial+Letter 0.97 Table 2: Document-level fact/opinion classification by Naive Bayes algorithm." ></td>
	<td class="line x" title="119:165	the 100 sentences for all seven choices was 55%; if we group together the five subtypes of opinion sentences, the overall agreement rises to 82%." ></td>
	<td class="line x" title="120:165	The low agreement for some labels was not surprising because there is much ambiguity between facts and opinions." ></td>
	<td class="line x" title="121:165	An example of an arguable sentence is A lethal guerrilla war between poachers and wardens now rages in central and eastern Africa, which one rater classified as fact and another rater classified as opinion." ></td>
	<td class="line x" title="122:165	Finally, for evaluating the quality of extracted words with semantic orientation labels, we used two distinct manually labeled collections as gold standards." ></td>
	<td class="line x" title="123:165	One set consists of the previously described 657 positive and 679 negative adjectives (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="124:165	We also used the ANEW list which was constructed during psycholinguistic experiments (Bradley and Lang, 1999) and contains 1,031 words of all four open classes." ></td>
	<td class="line x" title="125:165	As described in (Bradley and Lang, 1999), humans assigned valence scores to each score according to dimensions such as pleasure, arousal, and dominance; following heuristics proposed in psycholinguistics7 we obtained 284 positive and 272 negative words from the valence scores." ></td>
	<td class="line x" title="126:165	8 Results and Discussion Document Classification We trained our Bayes classifier for documents on 4,000 articles from the WSJ portion of our combined TREC collection, and evaluated on 4,000 other articles also from the WSJ part." ></td>
	<td class="line x" title="127:165	Table 2 lists the F-measure scores (the harmonic mean of precision and recall) of our Bayesian classifier for document-level opinion/fact classification." ></td>
	<td class="line x" title="128:165	The results show the classifier achieved 97% F-measure, which is comparable or higher than the 93% accuracy reported by (Wiebe et al. , 2002), who evaluated their work based on a similar set of WSJ articles." ></td>
	<td class="line x" title="129:165	The high classification performance 7http://www.sci.sdsu.edu/CAL/wordlist/." ></td>
	<td class="line x" title="130:165	Variant Class Standard A Standard B Score Fact a0 0.61,0.34a1 a0 1.00,0.27 a1Opinion a0 0.30,0.49a1 a0 0.16,0.64 a1 Frequency Fact a0 0.82,0.32a1 a0 0.89,0.19 a1Opinion a0 0.17,0.55a1 a0 0.28,0.55 a1 Table 3: a0 Recall, precision a1 of similarity classifier." ></td>
	<td class="line x" title="131:165	is also consistent with a high inter-rater agreement (kappa=0.95) for document-level fact/opinion annotation (Wiebe et al. , 2002)." ></td>
	<td class="line x" title="132:165	Note that we trained and evaluated only on WSJ articles for which we can obtain article class metadata, so the classifier may perform less accurately when used for other newswire articles." ></td>
	<td class="line x" title="133:165	Sentence Classification Table 3 shows the recall and precision of the similarity-based approach, while Table 4 lists the recall and precision of naive Bayes (single and multiple classifiers) for sentencelevel opinion/fact classification." ></td>
	<td class="line x" title="134:165	In both cases, the results are better when we evaluate against Standard B, containing the sentences for which two humans assign the same label; obviously, it is easier for the automatic system to produce the correct label in these more clear-cut cases." ></td>
	<td class="line x" title="135:165	Our Naive Bayes classifier has a higher recall and precision (8090%) for detecting opinions than for facts (around 50%)." ></td>
	<td class="line x" title="136:165	While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al.(1999)." ></td>
	<td class="line x" title="138:165	In general, the additional features helped the classifier; the best performance is achieved when words, bigrams, trigrams, part-of-speech, and polarity are included in the feature set." ></td>
	<td class="line x" title="139:165	Further, using multiple classifiers to automatically identify an appropriate subset of the data for training slightly increases performance." ></td>
	<td class="line x" title="140:165	Polarity Classification Using the method of Section 5.1, we automatically identified a total of 39,652 (65,773), 3,128 (4,426), 144,238 (195,984), and 22,279 (30,609) positive (negative) adjectives, adverbs, nouns, and verbs, respectively." ></td>
	<td class="line x" title="141:165	Extracted positive words include inspirational, truly, luck, and achieve." ></td>
	<td class="line x" title="142:165	Negative ones include depraved, disastrously, problem, and depress." ></td>
	<td class="line x" title="143:165	Figure 1 plots the Features Class Standard A Standard BSingle Multiple Single Multiple Features from (Wiebe et al. , 1999) Fact a0 0.03,0.38 a1 a0 0.03,0.38 a1 a0 0.06,1.00a1 a0 0.06,1.00 a1Opinion a0 0.97,0.69 a1 a0 0.97,0.69 a1 a0 1.00,0.80a1 a0 1.00,0.80 a1 Words only Fact a0 0.14,0.39 a1 a0 0.12,0.42 a1 a0 0.28,0.42a1 a0 0.28,0.45 a1Opinion a0 0.90,0.69 a1 a0 0.92,0.69 a1 a0 0.90,0.82a1 a0 0.91,0.83 a1 Words and Bigrams Fact a0 0.15,0.39 a1 a0 0.12,0.43 a1 a0 0.16,0.25a1 a0 0.16,0.25 a1Opinion a0 0.89,0.69 a1 a0 0.92,0.69 a1 a0 0.87,0.79a1 a0 0.87,0.79 a1 Words, Bigrams, and Trigrams Fact a0 0.18,0.44 a1 a0 0.13,0.41 a1 a0 0.26,0.50a1 a0 0.26,0.50 a1Opinion a0 0.89,0.70 a1 a0 0.91,0.69 a1 a0 0.93,0.82a1 a0 0.93,0.82 a1 Words, Bigrams, Trigrams, and Part-of-Speech Fact a0 0.17,0.42 a1 a0 0.13,0.40 a1 a0 0.18,0.49a1 a0 0.27,0.44 a1 Opinion a0 0.89,0.70 a1 a0 0.91,0.69 a1 a0 0.92,0.70a1 a0 0.85,0.84 a1 Words, Bigrams, Trigrams, Part-of-Speech, and Polarity Fact a0 0.15,0.43 a1 a0 0.13,0.42 a1 a0 0.44,0.50a1 a0 0.44,0.53 a1 Opinion a0 0.91,0.69 a1 a0 0.92,0.70 a1 a0 0.88,0.86a1 a0 0.91,0.86 a1 Table 4: a0 Recall, precisiona1 of opinion/fact sentence classification using different features and either a single or multiple (data cleaning) classifiers." ></td>
	<td class="line x" title="144:165	Figure 1: Recall and precision (1,336 manually labeled positive and negative adjectives as gold standard) of extracted adjectives using 1, 20, and 100 positive and negative adjective pairs as seeds." ></td>
	<td class="line x" title="145:165	recall and precision of extracted adjectives by using randomly selected seed sets of 1, 20, and 100 pairs of positive and negative adjectives from the list of (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="146:165	Both recall and precision increase as the seed set becomes larger." ></td>
	<td class="line x" title="147:165	We obtained similar results with the ANEW list of adjectives (Section 7)." ></td>
	<td class="line x" title="148:165	As an additional experiment, we tested the effect of ignoring sentences with negative particles, obtaining a small increase in precision and recall." ></td>
	<td class="line x" title="149:165	We subsequently used the automatically extracted polarity score for each word to assign an aggregate Parts-of-speech Used A B Adjectives 0.49 0.55 Adverbs 0.37 0.46 Nouns 0.54 0.52 Verbs 0.54 0.52 Adjectives and Adverbs 0.55 0.84 Adjectives, Adverbs, and Verbs 0.68 0.90 Adjectives, Adverbs, Nouns, and Verbs 0.62 0.74 Table 5: Accuracy of sentence polarity tagging on gold standards A and B for different sets of parts-ofspeech." ></td>
	<td class="line x" title="150:165	polarity to opinion sentences." ></td>
	<td class="line x" title="151:165	Table 5 lists the accuracy of our sentence-level tagging process." ></td>
	<td class="line x" title="152:165	We experimented with different combinations of part-ofspeech classes for calculating the aggregate polarity scores, and found that the combined evidence from adjectives, adverbs, and verbs achieves the highest accuracy (90% over a baseline of 48%)." ></td>
	<td class="line x" title="153:165	As in the case of sentence-level classification between opinion and fact, we also found the performance to be higher on Standard B, for which humans exhibited consistent agreement." ></td>
	<td class="line x" title="154:165	9 Conclusions We presented several models for distinguishing between opinions and facts, and between positive and negative opinions." ></td>
	<td class="line x" title="155:165	At the document level, a fairly straightforward Bayesian classifier using lexical information can distinguish between mostly factual and mostly opinion documents with very high precision and recall (F-measure of 97%)." ></td>
	<td class="line x" title="156:165	The task is much harder at the sentence level." ></td>
	<td class="line x" title="157:165	For that case, we described three novel techniques for opinion/fact classification achieving up to 91% precision and recall on the detection of opinion sentences." ></td>
	<td class="line x" title="158:165	We also examined an automatic method for assigning polarity information to single words and sentences, accurately discriminating between positive, negative, and neutral opinions in 90% of the cases." ></td>
	<td class="line x" title="159:165	Our work so far has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about." ></td>
	<td class="line x" title="160:165	While we have found presenting information organized in separate opinion and fact classes useful, our goal is to introduce further analysis of each sentence so that opinion sentences can be linked to particular perspectives on a specific subject." ></td>
	<td class="line x" title="161:165	We intend to cluster together sentences from the same perspective and present them in summary form as answers to subjective questions." ></td>
	<td class="line x" title="162:165	Acknowledgments We wish to thank Eugene Agichtein, Sasha BlairGoldensohn, Roy Byrd, John Chen, Noemie Elhadad, Kathy McKeown, Becky Passonneau, and the anonymous reviewers for valuable input on earlier versions of this paper." ></td>
	<td class="line x" title="163:165	We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions." ></td>
	<td class="line x" title="164:165	This work was supported by ARDA under AQUAINT project MDA908-02-C-0008." ></td>
	<td class="line x" title="165:165	Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect ARDAs views." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1071
Deeper Sentiment Analysis Using Machine Translation Technology
Kanayama, Hiroshi;Nasukawa, Tetsuya;Watanabe, Hideo;"></td>
	<td class="line x" title="1:218	Deeper Sentiment Analysis Using Machine Translation Technology KANAYAMA Hiroshi NASUKAWA Tetsuya WATANABE Hideo Tokyo Research Laboratory, IBM Japan, Ltd. 1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan fhkana,nasukawa,hiwatg@jp.ibm.com Abstract This paper proposes a new paradigm for sentiment analysis: translation from text documents to a set of sentiment units." ></td>
	<td class="line x" title="2:218	The techniques of deep language analysis for machine translation are applicable also to this kind of text mining task." ></td>
	<td class="line x" title="3:218	We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based machine translation engine." ></td>
	<td class="line x" title="4:218	1 Introduction Sentiment analysis (SA) (Nasukawa and Yi, 2003; Yi et al. , 2003) is a task to obtain writers feelings as expressed in positive or negative comments, questions, and requests, by analyzing large numbers of documents." ></td>
	<td class="line x" title="5:218	SA is becoming a useful tool for the commercial activities of both companies and individual consumers, because they want to sort out opinions about products, services, or brands that are scattered in online texts such as product review articles, replies given to questionnaires, and messages in bulletin boards on the WWW." ></td>
	<td class="line x" title="6:218	This paper describes a method to extract a set of sentiment units from sentences, which is the key component of SA." ></td>
	<td class="line x" title="7:218	A sentiment unit is a tuple of a sentiment1, a predicate, and its arguments." ></td>
	<td class="line x" title="8:218	For example, these sentences in a customers review of a digital camera (1) contained three sentiment units (1a), (1b), and (1c)." ></td>
	<td class="line x" title="9:218	Apparently these units indicate that the camera has good features in its lens and recharger, and a bad feature in its price." ></td>
	<td class="line x" title="10:218	It has excellent lens, but the price is too high." ></td>
	<td class="line x" title="11:218	I dont think the quality of the recharger has any problem." ></td>
	<td class="line x" title="12:218	9= ; (1) [favorable] excellent (lens) (1a) [unfavorable] high (price) (1b) [favorable] problematic+neg (recharger) (1c) The extraction of these sentiment units is not a trivial task because many syntactic and semantic operations are required." ></td>
	<td class="line x" title="13:218	First, the structure of a predicate and its arguments may be changed from the 1Possible values of a sentiment are favorable, unfavorable, question, and request." ></td>
	<td class="line x" title="14:218	In this paper the discussion is mostly focused on the first two values." ></td>
	<td class="line x" title="15:218	syntactic form as in (1a) and (1c)." ></td>
	<td class="line x" title="16:218	Also modal, aspectual, and negation information must be handled, as in (1c)." ></td>
	<td class="line x" title="17:218	Second, a sentiment unit should be constructed as the smallest possible informative unit so that it is easy to handle for the organizing processes after extraction." ></td>
	<td class="line x" title="18:218	In (1b) the degree adverb too is omitted to normalize the expression." ></td>
	<td class="line x" title="19:218	For (1c), the predicate problematic has the argument recharger instead of the head word of the noun phrase the quality of the recharger, because just using quality is not informative to describe the sentiment of the attribute of a real-world object." ></td>
	<td class="line x" title="20:218	Moreover, disambiguation of sentiments is necessary: in (1b) the adjective high has the unfavorable feature, but high can be treated as favorable in the expression resolution is high." ></td>
	<td class="line x" title="21:218	We regard this task as translation from text to sentiment units, because we noticed that the deep language analysis techniques which are required for the extraction of sentiment units are analogous to those which have been studied for the purpose of language translation." ></td>
	<td class="line x" title="22:218	We implemented an accurate sentiment analyzer by making use of an existing transfer-based machine translation engine (Watanabe, 1992), replacing the translation patterns and bilingual lexicons with sentiment patterns and a sentiment polarity lexicon." ></td>
	<td class="line x" title="23:218	Although we used many techniques for deep language analysis, the system was implemented at a surprisingly low development cost because the techniques for machine translation could be reused in the architecture described in this paper." ></td>
	<td class="line x" title="24:218	We aimed at the high precision extraction of sentiment units." ></td>
	<td class="line x" title="25:218	In other words, our SA system attaches importance to each individual sentiment expression, rather than to the quantitative tendencies of reputation." ></td>
	<td class="line x" title="26:218	This is in order to meet the requirement of the SA users who want to know not only the overall goodness of an object, but also the breakdown of opinions." ></td>
	<td class="line x" title="27:218	For example, when there are many positive opinions and only one negative opinion, the negative one should not be ignored because of its low percentage, but should be investigated thoroughly since valuable knowledge is often found in such a minority opinion." ></td>
	<td class="line x" title="28:218	Figure 1 illustrates an image of the SA output." ></td>
	<td class="line x" title="29:218	The outliner organizes positive and negative opinions by topic words, and provides references to the original text." ></td>
	<td class="line x" title="30:218	Favorable Unfavorable battery long life battery (3) good battery (2) : not good battery (1) s lens : nice lens (2) : (original document) When I bought this camera, I thought the battery was not good, but the problem was solved after I replaced it with new one." ></td>
	<td class="line x" title="31:218	Figure 1: An image of an outliner which uses SA output." ></td>
	<td class="line x" title="32:218	Users can refer to the original text by clicking on the document icons." ></td>
	<td class="line x" title="33:218	MT SA Japanese sentence ?parser Japanese tree structure  transfer j English tree structure Sentiment fragments ? generator ?English sentence Sentiment units Transfer patterns Fragment patterns Bilingual lexicon Polarity lexicon Figure 2: The concept of the machine translation engine and the sentiment analyzer." ></td>
	<td class="line x" title="34:218	Some components are shared between them." ></td>
	<td class="line x" title="35:218	Also other components are similar between MT and SA." ></td>
	<td class="line x" title="36:218	This means that the approach for SA should be switched from the rather shallow analysis techniques used for text mining (Hearst, 1999; Nasukawa and Nagano, 2001), where some errors can be treated as noise, into deep analysis techniques such as those used for machine translation (MT) where all of the syntactic and semantic phenomena must be handled." ></td>
	<td class="line x" title="37:218	We implemented a Japanese SA system using a Japanese to English translation engine." ></td>
	<td class="line x" title="38:218	Figure 2 illustrates our SA system, which utilizes a MT engine, where techniques for parsing and pattern matching on the tree structures are shared between MT and SA." ></td>
	<td class="line x" title="39:218	Section 2 reviews previous studies of sentiment analysis." ></td>
	<td class="line x" title="40:218	In Section 3 we define the sentiment unit to be extracted for sentiment analysis." ></td>
	<td class="line x" title="41:218	Section 4 presents the implementation of our system, comparing the operations and resources with those used for machine translation." ></td>
	<td class="line x" title="42:218	Our system is evaluated in Section 5." ></td>
	<td class="line x" title="43:218	In the rest of paper we mainly use Japanese examples because some of the operations depend on the Japanese language, but we also use English examples to express the sentiment units and some language-independent issues, for understandability." ></td>
	<td class="line nc" title="44:218	2 Previous work on Sentiment Analysis Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Turney, 2002; Pang et al. , 2002) where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal." ></td>
	<td class="line x" title="45:218	Other work (Subasic and Huettner, 2001; Morinaga et al. , 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of favorability." ></td>
	<td class="line x" title="46:218	Automatic acquisition of sentiment expressions have also been studied (Hatzivassiloglou and McKeown, 1997), but limited to adjectives, and only one sentiment could be assigned to each word." ></td>
	<td class="line x" title="47:218	Yi et al.(2003) pointed out that the multiple sentiment aspects in a document should be extracted." ></td>
	<td class="line x" title="49:218	This paper follows that approach, but exploits deeper analysis in order to avoid the analytic failures reported by Nasukawa and Yi (2003), which occurred when they used a shallow parser and only addressed a limited number of syntactic phenomena." ></td>
	<td class="line x" title="50:218	In our in-depth approach described in the next section, two types of errors out of the four reported by Nasukawa and Yi (2003) were easily removed2." ></td>
	<td class="line x" title="51:218	3 Sentiment Unit This section describes the sentiment units which are extracted from text, and their roles in the sentiment analysis and its applications." ></td>
	<td class="line x" title="52:218	A sentiment unit consists of a sentiment, a predicate, its one or more arguments, and a surface form." ></td>
	<td class="line x" title="53:218	Formally it is expressed as in Figure 3." ></td>
	<td class="line x" title="54:218	The sentiment feature categorizes a sentiment unit into four types: favorable [fav], unfavorable [unf], question [qst], and request [req]." ></td>
	<td class="line x" title="55:218	A predicate is a word, typically a verb or an adjective, which conveys the main notion of the sentiment unit." ></td>
	<td class="line x" title="56:218	An argument is also a word, typically a noun, which modifies the predicate with a case postpositional in Japanese." ></td>
	<td class="line x" title="57:218	They roughly correspond to a subject and an object of the predicate in English." ></td>
	<td class="line x" title="58:218	For example, from the sentence (2)3, the extracted sentiment unit is (2a)." ></td>
	<td class="line x" title="59:218	ABC123-ha renzu-ga subarashii." ></td>
	<td class="line x" title="60:218	ABC123-TOPIC lens-NOM excellent ABC123 has an excellent lens. (2) [fav] excellent h ABC123, lens i (2a) The sentiment unit (2a) stands for the sentiment is favorable, the predicate is excellent and its arguments are ABC123 and lens." ></td>
	<td class="line x" title="61:218	In this case, both ABC123 and lens are counted as words which are associated with a favorable sentiment." ></td>
	<td class="line x" title="62:218	Arguments are used as the keywords in the outliner, as in the leftmost column in Figure 1." ></td>
	<td class="line x" title="63:218	Predicates with no argument are ignored, because they have no effects on the view and often become noise." ></td>
	<td class="line x" title="64:218	2Though this paper handles Japanese SA, we also implemented an English version of SA using English-French translation techniques, and that system solved the problems which were mentioned in Nasukawa and Yis paper." ></td>
	<td class="line x" title="65:218	3ABC123 is a fictitious product name." ></td>
	<td class="line x" title="66:218	<sentiment unit> ::= <sentiment> <predicate> <argument>+ <surface> <sentiment> ::= favorable | unfavorable | question | request <predicate> ::= <word> <feature>* <argument> ::= <word> <feature>* <surface> ::= <string> Figure 3: The definition of a sentiment unit." ></td>
	<td class="line x" title="67:218	The predicate and its arguments can be different from the surface form in the original text." ></td>
	<td class="line x" title="68:218	Semantically similar representations should be aggregated to organize extracted sentiments, so the examples in this paper use English canonical forms to represent predicates and arguments, while the actual implementation uses Japanese expressions." ></td>
	<td class="line x" title="69:218	Predicates may have features, such as negation, facility, difficulty, etc. For example, ABC123 doesnt have an excellent lens. brings a sentiment unit [unf] excellent+neg h ABC123, lens i." ></td>
	<td class="line x" title="70:218	Also the facility/difficulty feature affects the sentiments such as [unf] break+facil for easy to break and [unf] learn+diff difficult to learn." ></td>
	<td class="line x" title="71:218	The surface string is the corresponding part in the original text." ></td>
	<td class="line x" title="72:218	It is used for reference in the view of the output of SA, because the surface string is the most understandable notation of each sentiment unit for humans." ></td>
	<td class="line x" title="73:218	We use the term sentiment polarity for the selection of the two sentiments [fav] and [unf]." ></td>
	<td class="line x" title="74:218	The other two sentiments, [qst] and [req] are important in applications, e.g. the automatic creation of FAQ." ></td>
	<td class="line x" title="75:218	Roughly speaking, [qst] is extracted from an interrogative sentence, and [req] is used for imperative sentences or expressions such as I want and Id like you to ." ></td>
	<td class="line x" title="76:218	From a pragmatic point of view it is difficult to distinguish between them4, but we classify them using simple rules." ></td>
	<td class="line x" title="77:218	4 Implementation This section describes operations and resources designed for the extraction of sentiment units." ></td>
	<td class="line x" title="78:218	There are many techniques analogous to those for machine translation, so first we show the architecture of the transfer-based machine translation engine which is used as the basis of the extraction of sentiment units." ></td>
	<td class="line x" title="79:218	4.1 Transfer-based Machine Translation Engine As illustrated on the left side of Figure 2, the transfer-based machine translation system consists of three parts: a source language syntactic parser, a bilingual transfer which handles the syntactic tree structures, and a target language generator." ></td>
	<td class="line x" title="80:218	Here the flow of the Japanese to English translation is shown with the following example sentence (3)." ></td>
	<td class="line x" title="81:218	4For example, the interrogative sentence Would you read it? implies a request." ></td>
	<td class="line x" title="82:218	kare hon ki iru watashi ha wo ni no Figure 4: The Japanese syntactic tree for the sentence (3)." ></td>
	<td class="line x" title="83:218	Kare-ha watashi-no He-TOPIC I-GEN hon-wo ki-ni iru." ></td>
	<td class="line x" title="84:218	book-ACC mind-DAT enter He likes my book. (3) First the syntactic parser parses the sentence (3) to create the tree structure as shown in Figure 4." ></td>
	<td class="line x" title="85:218	Next, the transfer converts this Japanese parse tree into an English one by applying the translation patterns as in Figure 5." ></td>
	<td class="line x" title="86:218	A translation pattern consists of a tree of the source language, a tree of the target language, and the word correspondences between both languages." ></td>
	<td class="line x" title="87:218	The patterns (a) and (b) in Figure 5 match with the subtrees in Figure 4, as Figure 6 illustrates." ></td>
	<td class="line x" title="88:218	Thismatchingoperationisverycomplicatedbecause there can be an enormous number of possible combinations of patterns." ></td>
	<td class="line x" title="89:218	The fitness of the pattern combinations is calculated according to the similarity of the source tree and the left side of the translation pattern, the specificity of the translation pattern, and so on." ></td>
	<td class="line x" title="90:218	This example also shows the process of matching the Japanese case markers (postpositional particles)." ></td>
	<td class="line x" title="91:218	The source tree and the pattern (a) match even though the postpositional particles are different (ha and ga)." ></td>
	<td class="line x" title="92:218	This process may be much more complicated when a verb is transformed into special forms e.g. passive or causative." ></td>
	<td class="line x" title="93:218	Besides this there are many operations to handle syntactic and semantic phenomena, but here we take them for granted because of space constraints." ></td>
	<td class="line x" title="94:218	Now the target fragments have been created as in Figure 6, using the right side of the matched translation patterns as in Figure 5." ></td>
	<td class="line x" title="95:218	The two fragments are attached at the shared node  noun2 , and lexicalized by using the bilingual lexicon." ></td>
	<td class="line x" title="96:218	Finally the target sentence He likes my book. is generated by the target language generator." ></td>
	<td class="line x" title="97:218	iru noun noun ki ga wo ni like noun noun SUBJ OBJ (a) noun no watashi noun my (b) Figure 5: Two examples of Japanese-English translation patterns." ></td>
	<td class="line x" title="98:218	The left side and the right side are Japanese and English syntactic trees, respectively." ></td>
	<td class="line x" title="99:218	The  noun  works as a wildcard which matches with any noun." ></td>
	<td class="line x" title="100:218	Curves stand for correspondences between Japanese and English words." ></td>
	<td class="line x" title="101:218	kare hon ki iru watashi ha wo ni no (a) (b) like noun1 noun2 SUBJ OBJ noun2 my Figure 6: Transferring the Japanese tree in Figure 4 intotheEnglishtree." ></td>
	<td class="line x" title="102:218	ThepatternsinFigure5create two English fragments, and they are attached at the nodes  noun2  which share the same correspondent node in the source language tree." ></td>
	<td class="line x" title="103:218	4.2 Techniques Required for Sentiment Analysis Our aim is to extract sentiment units with high precision." ></td>
	<td class="line x" title="104:218	Moreover, the set of arguments of each predicate should be selected necessarily and sufficiently." ></td>
	<td class="line x" title="105:218	Here we show that the techniques to meet these requirements are analogous to the techniques for machine translation which have been reviewed in Section 4.1." ></td>
	<td class="line x" title="106:218	4.2.1 Full parsing and top-down tree matching Full syntactic parsing plays an important role to extract sentiments correctly, because the local structures obtained by a shallow parser are not always reliable." ></td>
	<td class="line x" title="107:218	For example, expressions such as I dont think X is good, I hope that X is good are not favorable opinions about X, even though X is good appears on the surface." ></td>
	<td class="line x" title="108:218	Therefore we use top-down pattern matching on the tree structures from the full parsing in order to find each sentiment fragment, that is potentially a part of a sentiment unit." ></td>
	<td class="line x" title="109:218	In our method, initially the top node is examined to see whether or not the node and its combination of children nodes match with one of the patterns in the pattern repository." ></td>
	<td class="line x" title="110:218	In this top-down manner, the nodes dont think and hope in the above examples are examined before X is good, and thus the above expressions wont be misunderstood to express favorable sentiments." ></td>
	<td class="line x" title="111:218	There are three types of patterns: principal patterns, auxiliary patterns, and nominal patterns." ></td>
	<td class="line x" title="112:218	Figure 7 illustrates examples of principal patterns: the noun warui ga [unf] badh noun i (c) noun iru ki wo ni [fav] likeh noun i (d) Figure 7: Examples of principal patterns." ></td>
	<td class="line x" title="113:218	declinable to omowa nai unit +neg (e) declinable monono declinable unit unit (f) Figure 8: Examples of auxiliary patterns." ></td>
	<td class="line x" title="114:218	 declinable  denotes a verb or an adjective in Japanese." ></td>
	<td class="line x" title="115:218	Note that the two unit s on the right side of (f) are not connected." ></td>
	<td class="line x" title="116:218	This means two separated sentiment units can be obtained." ></td>
	<td class="line x" title="117:218	pattern (c) converts a Japanese expression  noun ga warui to a sentiment unit [unf] bad h noun i." ></td>
	<td class="line x" title="118:218	The pattern (d) converts an expression  noun -wo ki-ni iru to a sentiment unit [fav] like h noun i, where the subject (the noun preceding the postpositional ga) is excluded from the arguments because the subject of like is usually the author, who is not the target of sentiment analysis." ></td>
	<td class="line x" title="119:218	Another type is the auxiliary pattern, which expands the scope of matching." ></td>
	<td class="line x" title="120:218	Figure 8 has two examples." ></td>
	<td class="line x" title="121:218	The pattern (e) matches with phrases such as X-wa yoi-to omowa-nai." ></td>
	<td class="line x" title="122:218	((I) dont think X is good.) and produces a sentiment unit with the negation feature." ></td>
	<td class="line x" title="123:218	When this pattern is attached to a principal pattern, its favorability is inverted." ></td>
	<td class="line x" title="124:218	The pattern (f) allows us to obtain two separate sentiment units from sentences such as Dezain-ga waruimonono, sousasei-ha yoi." ></td>
	<td class="line x" title="125:218	(The design is bad, but the usability is good.)." ></td>
	<td class="line x" title="126:218	4.2.2 Informative noun phrase The third type of pattern is a nominal pattern." ></td>
	<td class="line x" title="127:218	Figure 9 shows three examples." ></td>
	<td class="line x" title="128:218	The pattern (g) is used to avoid a formal noun (nominalizer) being an argument." ></td>
	<td class="line x" title="129:218	Using this pattern, from the sentence Kawaii no-ga suki-da." ></td>
	<td class="line x" title="130:218	((I) like pretty things), [fav] like h pretty i can be extracted instead of [fav] like h thing i." ></td>
	<td class="line x" title="131:218	The pattern (h) is used to convert a noun phrase renzu-no shitsu (quality of the lens) into just lens." ></td>
	<td class="line x" title="132:218	Due to this operation, from Sentence (4), an informative sentiment unit (4a) can be obtained instead of a less informative one (4b)." ></td>
	<td class="line x" title="133:218	Renzu-no shitsu-ga yoi." ></td>
	<td class="line x" title="134:218	lens-GEN quality-NOM good The quality of the lens is good. (4) [fav] good h lens i (4a) ? [fav] good h quality i (4b) adj no adj (g) noun no shitsu noun (h) noun noun noun noun (i) Figure 9: Examples of nominal patterns." ></td>
	<td class="line x" title="135:218	The pattern (i) is for compound nouns such as juuden jikan (recharging time)." ></td>
	<td class="line x" title="136:218	A sentiment unit long h time i is not informative, but long h recharging time i can be regarded as a [unf] sentiment." ></td>
	<td class="line x" title="137:218	4.2.3 Disambiguation of sentiment polarity Some adjectives and verbs may be used for both favorable and unfavorable predicates." ></td>
	<td class="line x" title="138:218	This variation of sentiment polarity can be disambiguated naturally in the same manner as the word sense disambiguation in machine translation." ></td>
	<td class="line x" title="139:218	The adjective takai (high) is a typical example, as in (5a) and (5b)." ></td>
	<td class="line x" title="140:218	In this case the sentiment polarity depends on the noun preceding the postpositional particle ga: favorable if the noun is kaizoudo (resolution), unfavorable if the noun is a product name." ></td>
	<td class="line x" title="141:218	The semantic category assigned to a noun holds the information used for this type of disambiguation." ></td>
	<td class="line x" title="142:218	Kaizoudo-ga takai." ></td>
	<td class="line x" title="143:218	resolution-NOM high The resolution is high. ! [fav] (5a) ABC123-ga takai." ></td>
	<td class="line x" title="144:218	ABC123-NOM high (price) ABC123 is expensive. ! [unf] (5b) 4.2.4 Aggregation of synonymous expressions In contrast to disambiguation, aggregation of synonymous expressions is important to organize extracted sentiment units." ></td>
	<td class="line x" title="145:218	If the different expressions which convey the same (or similar) meanings are aggregated into a canonical one, the frequency increases and one can easily find frequently mentioned opinions." ></td>
	<td class="line x" title="146:218	Using the translation architecture, any forms can be chosen as the predicates and arguments by adjusting the patterns and lexicons." ></td>
	<td class="line x" title="147:218	That is, monolingual word translation is done in our method." ></td>
	<td class="line x" title="148:218	4.3 Resources for Sentiment Analysis We prepared the following resources for sentiment analysis: Principal patterns: The verbal and adjectival patterns for machine translation were converted to principal patterns for sentiment analysis." ></td>
	<td class="line x" title="149:218	The left sides of the patterns are compatible with the source language parts of the original patterns, so we just assigned a sentiment polarity to each word." ></td>
	<td class="line x" title="150:218	A total of 3752 principal patterns were created." ></td>
	<td class="line x" title="151:218	Auxiliary/Nominal patterns: A total of 95 auxiliary patterns and 36 nominal patterns were created manually." ></td>
	<td class="line x" title="152:218	Polarity lexicon: Some nouns were assigned sentiment polarity, e.g. [unf] for noise." ></td>
	<td class="line x" title="153:218	This polarity is used in expressions such as  ga ooi." ></td>
	<td class="line x" title="154:218	(There are many )." ></td>
	<td class="line x" title="155:218	This lexicon is also used for the aggregation of words." ></td>
	<td class="line x" title="156:218	Some patterns and lexicons are domaindependent." ></td>
	<td class="line x" title="157:218	The situation is the same as in machine translation." ></td>
	<td class="line x" title="158:218	Fortunately the translation engine used here has a function to selectively use domain-dependent dictionaries, and thus we can prepare patterns which are especially suited for the messages on bulletin boards, or for the domain of digital cameras." ></td>
	<td class="line x" title="159:218	For example, The size is small. is a desirable feature of a digital camera." ></td>
	<td class="line x" title="160:218	We can assign the appropriate sentiment (in this case, [fav]) by using a domain-specific principal pattern." ></td>
	<td class="line x" title="161:218	5 Evaluation We conducted two experiments on the extraction of sentiment units from bulletin boards on the WWW that are discussing digital cameras." ></td>
	<td class="line x" title="162:218	A total of 200 randomly selected sentences were analyzed by our system." ></td>
	<td class="line x" title="163:218	The resources were created by looking at other parts of the same domain texts, and therefore this experiment is an open test." ></td>
	<td class="line x" title="164:218	Experiment 1 measured the precision of the sentiment polarity, and Experiment 2 evaluated the informativeness of the sentiment units." ></td>
	<td class="line x" title="165:218	In this section we handled only the sentiments [fav] and [unf] sentiments, thus the other two sentiments [qst] and [req] were not evaluated." ></td>
	<td class="line x" title="166:218	5.1 Experiment 1: Precision and Recall In order to see the reliability of the extracted sentiment polarities, we evaluated the following three metrics: Weak precision: The coincidence rate of the sentiment polarity between the systems output and manual output when both the system and the human evaluators assigned either a favorable or unfavorable sentiment." ></td>
	<td class="line x" title="167:218	Strong precision: The coincidence rate of the sentiment polarity between the systems output and manual output when the system assigned either a favorable or unfavorable sentiment." ></td>
	<td class="line x" title="168:218	Recall: The detection rate of sentiment units within the manual output." ></td>
	<td class="line x" title="169:218	These metrics are measured by using two methods: (A) our proposed method based on the machine translation engine, and (B) the lexicon-only method, which emulates the shallow parsing approach." ></td>
	<td class="line x" title="170:218	The latter method used the simple polarity lexicon of adjectives and verbs, where an adjective or a verb had only one sentiment polarity, then no disambiguation was done." ></td>
	<td class="line x" title="171:218	Except for the direct negation of (A) MT (B) Lexicon only Weak prec." ></td>
	<td class="line x" title="172:218	100% (31/31) 80% (41/51) Strong prec." ></td>
	<td class="line x" title="173:218	89% (31/35) 44% (41/93) Recall 43% (31/72) 57% (41/72) Table 1: Precision and recall for the extraction of sentiment units from 200 sentences." ></td>
	<td class="line x" title="174:218	(A) MT Manual f n u f 20 3 0 System n 27 14 u 0 1 11 (B) Lexicon only Manual f n u f 26 19 6 System n 14 7 u 4 23 15 Table 2: The breakdown of the results of Experiment 1." ></td>
	<td class="line x" title="175:218	The columns and rows show the manual output and the system output, respectively (f: favorable, n: non-sentiment, u: unfavorable)." ></td>
	<td class="line x" title="176:218	The sum of the bold numbers equals the numerators of the precision and recall." ></td>
	<td class="line x" title="177:218	an adjective or a verb5, no translation patterns were used." ></td>
	<td class="line x" title="178:218	Instead of the top-down pattern matching, sentiment units were extracted from any part of the tree structures (the results of full-parsing were used also here)." ></td>
	<td class="line x" title="179:218	Table 1 shows the results." ></td>
	<td class="line x" title="180:218	With the MT framework, the weak precision was perfect, and also the strong precision was much higher, while the recall was lower than for the lexicon-only method." ></td>
	<td class="line x" title="181:218	Their breakdowns in the two parts of Table 2 indicate that most of errors where the system wrongly assigned either of sentiments (i.e. human regarded an expression as non-sentiment) have been reduced with the MT framework." ></td>
	<td class="line x" title="182:218	All of the above results are consistent with intuition." ></td>
	<td class="line x" title="183:218	The MT method outputs a sentiment unit only when the expression is reachable from the root node of the syntactic tree through the combination of sentiment fragments, while the lexicon-only method picks up sentiment units from any node in the syntactic tree." ></td>
	<td class="line x" title="184:218	The sentence (6) is an example where the lexicon-only method output the wrong sentiment unit (6a)." ></td>
	<td class="line x" title="185:218	The MT method did not output this sentiment unit, thus the precision values of the MT method did not suffer from this example." ></td>
	<td class="line x" title="186:218	gashitsu-ga kirei-da-to iu hyouka-ha uke-masen-deshi-ta." ></td>
	<td class="line x" title="187:218	(6) There was no opinion that the picture was sharp.  [fav] clear h picture i (6a) In the lexicon-only method, some errors occurred due to the ambiguity in sentiment polarity of an adjective or a verb, e.g. Kanousei-ga takai." ></td>
	<td class="line x" title="188:218	(Capabilities are high.) since takai (high/expensive) is always assigned the [unf] feature." ></td>
	<td class="line x" title="189:218	5He doesnt like it. is regarded as negation, but I dont think it is good. is not." ></td>
	<td class="line x" title="190:218	declinable noun noun noun ga wo ni Figure 10: A nave predicate-argument structure used by the system (C)." ></td>
	<td class="line x" title="191:218	Nouns preceding three major postpositional particles ga, wo, and ni are supported as the slots of arguments." ></td>
	<td class="line x" title="192:218	On the other hand, in the system (A), there are over 3,000 principal patterns that have information on appropriate combinations for each verb and adjective." ></td>
	<td class="line x" title="193:218	(A) MT (C) Nave Less redundant 2/35 0/35 More informative 13/35 1/35 Both 1/35 0/35 Table 3: Comparison of scope of sentiment units." ></td>
	<td class="line x" title="194:218	The numbers mean the counts of the better output for each system among 35 sentiment units." ></td>
	<td class="line x" title="195:218	The remainder is the outputs that were the same in both systems." ></td>
	<td class="line x" title="196:218	The recall was not so high, especially in the MT method, but according to our error analysis the recall can be increased by adding auxiliary patterns." ></td>
	<td class="line x" title="197:218	Ontheotherhand, itisalmostimpossibletoincrease the precision without our deep analysis techniques." ></td>
	<td class="line x" title="198:218	Consequently, ourproposedmethodoutperformsthe shallow (lexicon-only) approach." ></td>
	<td class="line x" title="199:218	5.2 Experiment 2: Scope of Sentiment Unit We also compared the appropriateness of the scope of the extracted sentiment units between (A) the proposed method with the MT framework and (C) a method that supports only nave predicateargument structures as in Figure 10 and doesnt use any nominal patterns." ></td>
	<td class="line x" title="200:218	According to the results shown in Table 3, the MT method produced less redundant or more informative sentiment units than did relying on the nave predicate-argument structures in about half of the cases among the 35 extracted sentiment units." ></td>
	<td class="line x" title="201:218	The following example (7) is a case where the sentiment unit output by the MT method (7a) was less redundant than that output by the nave method (7b)." ></td>
	<td class="line x" title="202:218	The translation engine understood that the phrase kyonen-no 5-gatsu-ni (last May) held temporal information, therefore it was excluded from the arguments of the predicate enhance, while both functionandMayweretheargumentsofenhance in (7b)." ></td>
	<td class="line x" title="203:218	Apparently the argument May is not necessary here." ></td>
	<td class="line x" title="204:218	kyonen-no 5-gatsu-ni kinou-ga kairyou-sare-ta you-desu." ></td>
	<td class="line x" title="205:218	(7) It seems the function was enhanced last May. [fav] enhance h function i (7a) ? [fav] enhance h function, May i (7b) Example (8) is another case where the sentiment unit output by the MT method (8a) was more informative than that output by the nave method (8b)." ></td>
	<td class="line x" title="206:218	Than the Japanese functional noun hou, its modifier zoom was more informative." ></td>
	<td class="line x" title="207:218	The MT method successfully selected the noun zoom as the argument of desirable." ></td>
	<td class="line x" title="208:218	zuum-no hou-ga nozomashii." ></td>
	<td class="line x" title="209:218	(8) A zoom is more desirable. [fav] desirable h zoom i (8a) ? [fav] desirable h hou i (8b) The only one case we encountered where the MT method extracted a less informative sentiment unit was the sentence Botan-ga satsuei-ni pittaridesu (The shutter is suitable for taking photos)." ></td>
	<td class="line x" title="210:218	The nave method could produce the sentiment unit [fav] suitable h shutter, photo i, but the MT method created [fav] suitable h shutter i." ></td>
	<td class="line x" title="211:218	This is due to the lack of a noun phrase preceding the postpositional particle ni in the principal pattern." ></td>
	<td class="line x" title="212:218	Such problems can be avoided by modifying the patterns, and thus the effect of the combination of patterns for SA has been shown here." ></td>
	<td class="line x" title="213:218	6 Conclusion This paper has proposed a new approach to sentiment analysis: the translation from text to a set of semantic fragments." ></td>
	<td class="line x" title="214:218	We have shown that the deep syntactic and semantic analysis makes possible the reliable extraction of sentiment units, and the outlining of sentiments became useful because of the aggregation of the variations in expressions, and the informative outputs of the arguments." ></td>
	<td class="line x" title="215:218	The experimental results have shown that the precision of the sentiment polarity was much higher than for the conventional methods, and the sentiment units created by our system were less redundant and more informative than when using nave predicate-argument structures." ></td>
	<td class="line x" title="216:218	Even though we exploited many advantages of deep analysis, we could create a sentiment analysis system at a very low development cost, becausemanyofthetechniquesformachinetranslation can be reused naturally when we regard the extraction of sentiment units as a kind of translation." ></td>
	<td class="line x" title="217:218	Many techniques which have been studied for the purpose of machine translation, such as word sense disambiguation (Dagan and Itai, 1994; Yarowsky, 1995), anaphora resolution (Mitamura et al. , 2002), and automatic pattern extraction from corpora (Watanabe et al. , 2003), can accelerate the further enhancement of sentiment analysis, or other NLP tasks." ></td>
	<td class="line x" title="218:218	Therefore this work is the first step towards the integration of shallow and wide NLP, with deep NLP." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1121
Sentiment Classification On Customer Feedback Data: Noisy Data, Large Feature Vectors, And The Role Of Linguistic Analysis
Gamon, Michael;"></td>
	<td class="line x" title="1:115	Sentiment classification on customer feedback data: noisy data, large feature vectors, and the role of linguistic analysis Michael Gamon Microsoft Research One Microsoft Way Redmond, WA 98052 mgamon@microsoft.com Abstract We demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data." ></td>
	<td class="line x" title="2:115	We show that by using large feature vectors in combination with feature reduction, we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator." ></td>
	<td class="line x" title="3:115	We also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain." ></td>
	<td class="line x" title="4:115	1 Introduction Software companies typically receive high volumes of electronic customer feedback every day, some of it in the form of elicited surveys, some of it in the form of unsolicited comments, suggestions, criticism." ></td>
	<td class="line x" title="5:115	In order to react to that feedback quickly, and to direct it to the appropriate channels inside the company, it is desirable to provide intelligent and automatic classification of the feedback along two dimensions: What is the feedback about?" ></td>
	<td class="line x" title="6:115	Is the feedback positive or negative?" ></td>
	<td class="line x" title="7:115	The first question is addressed by text mining tools." ></td>
	<td class="line x" title="8:115	Automatic sentiment classification addresses the second question." ></td>
	<td class="line x" title="9:115	Text mining tools can help make large quantities of feedback more manageable by splitting them into clusters based on keywords or topics." ></td>
	<td class="line x" title="10:115	Sentiment analysis, which is the focus of this paper, adds a second dimension to the analysis." ></td>
	<td class="line x" title="11:115	It makes it possible to focus the text mining on areas in need of improvement (negative feedback) or on areas of success (positive feedback)." ></td>
	<td class="line x" title="12:115	Sentiment classification is a special case of text categorization, where the criterion of classification is the attitude expressed in the text, rather than the content or topic." ></td>
	<td class="line x" title="13:115	Faced with the task of having to automatically classify a piece of text as expressing positive or negative sentiment, a reasonable first approach would consist of paying special attention to words that tend to express a positive or negative attitude." ></td>
	<td class="line x" title="14:115	Pang et al.(2002) have demonstrated, however, that this is not as straightforward as one may think, given that sentiment is often expressed in more subtle and indirect ways." ></td>
	<td class="line x" title="16:115	The literature on sentiment classification can be divided into approaches that rely on semantic resources, such as a sentiment or affect lexicon (Nasukawa and Yi 2003, Subasic and Huettner 2001), or a large scale knowledge base (Liu et al 2003) on the one hand, and approaches that try to learn patterns directly from tagged data, without additional resources (Dave et al 2003, Pang et al. 2003)." ></td>
	<td class="line oc" title="17:115	Much research is also being directed at acquiring affect lexica automatically (Turney 2002, Turney and Littman 2002)." ></td>
	<td class="line x" title="18:115	There is also a considerable amount of research on classification of text as subjective or objective (Wiebe et al 2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback." ></td>
	<td class="line x" title="19:115	In many studies, research on sentiment classification is conducted on review-type data, such as movie or restaurant reviews." ></td>
	<td class="line x" title="20:115	These data often consist of relatively well-formed, coherent and at least paragraph-length pieces of text." ></td>
	<td class="line x" title="21:115	The results we present in this paper are based on customer feedback data from web surveys, which, as we will discuss below, are particularly noisy and fragmentary." ></td>
	<td class="line x" title="22:115	For our purpose of automatic classification of customer feedback, we decided to use machinelearning directly on the customer feedback, instead of relying on additional semantic resources of any kind." ></td>
	<td class="line x" title="23:115	This decision was motivated by practical considerations: first, the customer feedback data we are facing are often very short and sometimes very incoherent." ></td>
	<td class="line x" title="24:115	This makes it seem unlikely that a detailed semantic resource would be of particular help." ></td>
	<td class="line x" title="25:115	Second, we believe that an appropriately chosen machine-learning technique will be able to draw its own conclusions from the distribution of lexical elements in a piece of feedback." ></td>
	<td class="line x" title="26:115	We conducted our sentiment classification experiments using support vector machines." ></td>
	<td class="line x" title="27:115	Support vector machines (SVMs) have a good track record in text classification (Joachims 1998, Dumais et al. 1998), they can be trained using a large number of features, and both training and classification for linear SVMs are fast with optimized learning algorithms." ></td>
	<td class="line x" title="28:115	For our experiments we use John Platts Sequential Minimal Optimization (SMO) tool (Platt 1999)." ></td>
	<td class="line x" title="29:115	In the absence of any evidence that would suggest a more complicated kernel function such as a polynomial or an RBF kernel, we have decided to train linear SVMs for our classification task (see also the results in Joachims 1998)." ></td>
	<td class="line x" title="30:115	The procedure, as is standard in supervised machine learning tasks, consists of training a classifier on pretagged training data and then evaluating the performance of the classifier on a held-out set of test data." ></td>
	<td class="line x" title="31:115	The two main questions we wanted to assess with our experiments are: 1." ></td>
	<td class="line x" title="32:115	which features and feature sets are relevant for sentiment classification on customer feedback?" ></td>
	<td class="line x" title="33:115	2." ></td>
	<td class="line x" title="34:115	what is the maximum classification accuracy that can be achieved on this data set?" ></td>
	<td class="line x" title="35:115	2 Data Our data consists of 11399 feedback items from a Global Support Services survey, and 29485 feedback items from a Knowledge Base survey for a total of 40884 items." ></td>
	<td class="line x" title="36:115	We excluded pieces of feedback without any verbatim from the data." ></td>
	<td class="line x" title="37:115	Along with the verbatim, customers provided a numeric satisfaction score on a scale from 1 (not satisfied) to 4 (very satisfied) for each of those pieces of feedback." ></td>
	<td class="line x" title="38:115	The numeric score served as the target tag in our experiments, making it unnecessary to perform any costly human evaluation and tagging." ></td>
	<td class="line x" title="39:115	The distribution of items across numerical scores is given in Table 1." ></td>
	<td class="line x" title="40:115	Category 1 2 3 4 Number of documents 8596 9060 14573 8655 Table 1: number of documents in each satisfaction category The data is extremely noisy, and a human evaluation of a random set of 200 pieces of feedback could only assign a positive or negative sentiment to 117 (58.5%) items, the rest was either balanced (16 cases or 8%), expressed no sentiment (50 cases or 25%), or too incoherent or random to be classified (17 cases or 8.5%)." ></td>
	<td class="line x" title="41:115	Amongst the 117 classifiable cases, the human evaluator assigned the category positive: to 26 cases (or 22.2%) and the category negative to 91 cases (or 77.8%)." ></td>
	<td class="line x" title="42:115	After automatic sentence breaking into one sentence per line, the individual files contained an average of 2.56 lines." ></td>
	<td class="line x" title="43:115	For our experiments we split the data 90/10 into training and held-out test data." ></td>
	<td class="line x" title="44:115	We performed 10-fold cross validation for each of the experiments reported in this paper." ></td>
	<td class="line x" title="45:115	For each of the various classification tasks, we trained a linear SVM using the standard settings of the SMO tool, and calculated accuracy, precision and recall numbers on the held-out test data, averaging them across the 10-fold cross validation." ></td>
	<td class="line x" title="46:115	3 Features 3.1 Feature vectors We experimented with a range of different feature sets." ></td>
	<td class="line x" title="47:115	Most importantly, we wanted to establish whether we would gain any significant advantage in the sentiment classification task by using features based on deep linguistic analysis or whether surface-based features would suffice." ></td>
	<td class="line x" title="48:115	Previous results in authorship attribution and style classification experiments had indicated that linguistic features contribute to the overall accuracy of the classifiers, although our null hypothesis based on a review of the relevant literature for sentiment classification was that we would not gain much by using these features." ></td>
	<td class="line x" title="49:115	The surface features we used were lemma unigrams, lemma bigrams, and lemma trigrams." ></td>
	<td class="line x" title="50:115	For the linguistic features, we performed a linguistic analysis of the data with the NLPWin natural language processing system developed in Microsoft Research (an overview can be found in Heidorn 2000)." ></td>
	<td class="line x" title="51:115	NLPWin provides us with a phrase structure tree and a logical form for each string, from which we can extract an additional set of features:  part-of-speech trigrams  constituent specific length measures (length of sentence, clauses, adverbial/adjectival phrases, and noun phrases)  constituent structure in the form of context free phrase structure patterns for each constituent in a parse tree." ></td>
	<td class="line x" title="52:115	Example: DECL::NP VERB NP (a declarative sentence consisting of a noun phrase a verbal head and a second noun phrase)  Part of speech information coupled with semantic relations (e.g. Verb Subject Noun indicating a nominal subject to a verbal predicate)  Logical form features provided by NLPWin, such as transitivity of a predicate, tense information etc. For each of these features, except for the length features, we extract a binary value, corresponding to the presence or absence of that feature in a given document." ></td>
	<td class="line x" title="53:115	Using binary values for presence/absence as opposed to frequency values is motivated by the rather extreme brevity of these documents." ></td>
	<td class="line x" title="54:115	3.2 Feature reduction Feature reduction is an important part of optimizing the performance of a (linear) classifier by reducing the feature vector to a size that does not exceed the number of training cases as a starting point." ></td>
	<td class="line x" title="55:115	Further reduction of vector size can lead to more improvements if the features are noisy or redundant." ></td>
	<td class="line x" title="56:115	Reducing the number of features in the feature vector can be done in two different ways:  reduction to the top ranking n features based on some criterion of predictiveness  reduction by elimination of sets of features (e.g. elimination of linguistic analysis features etc)." ></td>
	<td class="line x" title="57:115	Experimenting with the elimination of feature sets provides an answer to the question as to which qualitative sets of features play a significant role in the classification task Of course these methods can also be combined, for example by eliminating sets of features and then taking the top ranking n features from the remaining set." ></td>
	<td class="line x" title="58:115	We used both techniques (and their combinations) in our experiments." ></td>
	<td class="line x" title="59:115	The measure of predictiveness we employed is log likelihood ratio with respect to the target variable (Dunning 1993)." ></td>
	<td class="line x" title="60:115	In the experiments described below, n (in the n top-ranked features) ranged from 1000 to 40,000." ></td>
	<td class="line x" title="61:115	The different feature set combinations we used were:  all features  no linguistic features (only word ngrams)  surface features (word ngrams, function word frequencies and POS ngrams)  linguistic features only (no word ngrams) 4 Results Given the four different rankings associated by users with their feedback, we experimented with two distinct classification scenarios: 1." ></td>
	<td class="line x" title="62:115	classification of documents as belonging to category 1 versus category 4 2." ></td>
	<td class="line x" title="63:115	classification of documents as belonging to categories 1 or 2 on the one hand, and 3 or 4 on the other Two additional scenarios can be envisioned." ></td>
	<td class="line x" title="64:115	In the first, two classifiers (1 versus 2/3/4 and 4 versus 1/2/3) would be trained and their votes would be combined either through weighted probability voting or other classifier combination methods (Dietterich 1997)." ></td>
	<td class="line x" title="65:115	A second possibility is to learn a three-way distinction 1 versus 2/3 versus 4." ></td>
	<td class="line x" title="66:115	In this paper we restrict ourselves to the scenarios 1 and 2 above." ></td>
	<td class="line x" title="67:115	Initial experiments suggest that the combination of two classifiers yields only minimal improvements." ></td>
	<td class="line x" title="68:115	4.1 Classification of category 1 versus category 4 Figure 1 below illustrates the accuracy of the 1 versus 4 classifier at different feature reduction cutoffs and with different feature sets." ></td>
	<td class="line x" title="69:115	The accuracy differences are statistically significant at the.99 confidence level, based on the 10fold cross validation scenario." ></td>
	<td class="line x" title="70:115	Figure 2and Figure 3 show the F1-measure for target value 4 (good sentiment) and target value 1 (bad sentiment) respectively." ></td>
	<td class="line x" title="71:115	The baseline for this experiment is 50.17% (choosing category 4 as the value for the target feature by default)." ></td>
	<td class="line x" title="72:115	Accuracy peaks at 77.5% when the top 2000 features in terms of log likelihood ratio are used, and when the feature set is not restricted, i.e. when these top 2000 features are drawn from linguistic and surface features." ></td>
	<td class="line x" title="73:115	We will return to the role of linguistic features in section 4.4." ></td>
	<td class="line x" title="74:115	F1-measure for both target 4 (Figure 2) and target 1 (Figure 3) exhibit a similar picture, again we achieve maximum performance by using the top 2000 features from the complete pool of features." ></td>
	<td class="line x" title="75:115	Accuracy 1 versus 4 70 71 72 73 74 75 76 77 78 79 80 20k 10k 5k 2k 1k number of features ac cu r a c y all features no linguistic features surface features linguistic features only Figure 1: Accuracy of the 1 versus 4 classifier Target 4: F-measure 1 vs 4 classifier 70 71 72 73 74 75 76 77 78 79 80 20k 10k 5k 2k 1k number of features F m e asu r e all features no linguistic features surface features linguistic features only Figure 2: F1-measure for target category 4 Target 1: F-measure 1 vs 4 classifier 70 71 72 73 74 75 76 77 78 79 80 20k 10k 5k 2k 1k number of features Fm e a s ur e all features no linguistic features surface features linguistic features only Figure 3: F1-measure for target category 1 4.2 Classification of categories 1 and 2 versus 3 and 4 Accuracy and F1-measure results for the 1/2 versus 3/4 task are shown in Figure 4, Figure 5 and Figure 6." ></td>
	<td class="line x" title="76:115	Again, the accuracy differences are statistically significant." ></td>
	<td class="line x" title="77:115	The baseline in this scenario is at 56.81% (choosing category 3/4 for the target feature by default)." ></td>
	<td class="line x" title="78:115	Classification accuracy is lower than in the 1 versus 4 scenario, as can be expected since the fuzzy categories 2 and 3 are included in the training and test data." ></td>
	<td class="line x" title="79:115	Similarly to the 1 versus 4 classification, accuracy is maximal at 69.48% when the top 2000 features from the complete feature set are used." ></td>
	<td class="line x" title="80:115	The F1-measure for the target value 1/2 peaks at the same feature reduction cutoff, whereas the F1measure for the target value 3/4 benefits from more drastic feature reduction to a set of only the topranked 1000 features." ></td>
	<td class="line x" title="81:115	Accuracy 1/2 versus 3/4 65 66 67 68 69 70 71 72 73 74 75 20k 10k 5k 2k 1k number of features acc u r a c y all features no linguistic features surface features linguistic features only Figure 4: Accuracy of the 1/2 versus 3/4 classifier Target 3/4: F-measure 1/2 versus 3/4 classifier 71 71.5 72 72.5 73 73.5 74 74.5 75 75.5 76 20k 10k 5k 2k 1k number of features F m e asu r e all features no linguistic features surface features linguistic features only Figure 5: F1-measure for target category 3/4 Target 1/2: F-measure 1/2 versus 3/4 classifier 55 56 57 58 59 60 61 62 63 64 65 20k 10k 5k 2k 1k number of features Fm e a s ur e all features no linguistic features surface features linguistic features only Figure 6: F1-measure for target category 1/2 4.3 Results compared to human classification The numbers reported in the previous sections are substantially lower than results that have been reported on other data sets such as movie or restaurant reviews." ></td>
	<td class="line x" title="82:115	Pang et al.(2002), for example, report a maximum accuracy of 82.9% on movie reviews." ></td>
	<td class="line x" title="84:115	As we have observed in section 2, the data that we are dealing with here are extremely noisy." ></td>
	<td class="line x" title="85:115	Recall that on a random sample of 200 pieces of feedback even a human evaluator could only assign a sentiment classification to 117 of the documents, the remaining 83 being either balanced in their sentiment, or too unclear or too short to be classifiable at all." ></td>
	<td class="line x" title="86:115	In order to assess performance of our classifiers on cleaner data, we used the 117 humanly classifiable pieces of customer feedback as a test set for the best performing classifier scenario." ></td>
	<td class="line x" title="87:115	For that purpose, we retrained both 1 versus 4 and 1/2 versus 3/4 classifiers with the top-ranked 2000 features on our data set, with the humanly evaluated cases removed from the training set." ></td>
	<td class="line x" title="88:115	Results are shown in Table 2, the baseline in this experiment is at 77.78% (choosing the bad sentiment as a default)." ></td>
	<td class="line x" title="89:115	1 versus 4 using top 2k features 1/2 versus 3/4 using top 2k features Accuracy 85.47 69.23 F-measure good 74.62 58.14 F-measure bad 89.82 75.67 Table 2: Results of the two best classifiers on humanly classifiable data Accuracy of 85.47% as achieved by the 1 versus 4 scenario is in line with accuracy numbers reported for less noisy domains." ></td>
	<td class="line x" title="90:115	4.4 The role of linguistic analysis features Figure 1 through Figure 6 also show the effect of eliminating whole feature sets from the training process." ></td>
	<td class="line x" title="91:115	A result that came as a surprise to us is the fact that the presence of very abstract linguistic analysis features based on constituent structure and semantic dependency graphs improves the performance of the classifiers." ></td>
	<td class="line x" title="92:115	The only exception to this observation is the F1-measure for the good sentiment case in the 1/2 versus 3/4 scenario (Figure 5), where the different feature sets yield very much similar performance across the feature reduction spectrum, with the no linguistic features even outperforming the other feature sets by a very small margin (0.18%)." ></td>
	<td class="line x" title="93:115	While the improvement in practice may be too small to warrant the overhead of linguistic analysis, it is very interesting from a linguistic point of view that even in a domain as noisy as this one, there seem to be robust stylistic and linguistic correlates with sentiment." ></td>
	<td class="line x" title="94:115	Note that in the 1 versus 4 scenario we can achieve classification accuracy of 74.5% by using only linguistic features (Figure 1), without the use of any word n-gram features (or any other word-based information) at all." ></td>
	<td class="line x" title="95:115	This clearly indicates that affect and style are linked in a more significant way than has been previously suggested in the literature." ></td>
	<td class="line x" title="96:115	4.5 Relevant features Given that linguistic features play a consistent role in the experiments described here, we inspected the models to see which features play a particularly big role as indicated by their associated weights in the linear svm." ></td>
	<td class="line x" title="97:115	This is particularly interesting in light of the fact that in previous research on sentiment classification, affect lexica or other special semantic resources have served as a source for features (see references in section 1)." ></td>
	<td class="line x" title="98:115	When looking at the top 100 weighted features in the best classifier (1 versus 4), we found an interesting mix of the obvious, and the not-so-obvious." ></td>
	<td class="line x" title="99:115	Amongst the obviously affect-charged terms and features in the top 100 are: +Neg 1, unable to, thanks, the good, easy to, ease of, lack of, not find, not work, no help, much accurate, a simple On the other hand, there are many features that carry high weights, but are not what one would intuitively think of as a typical affect indicator: try the, of, off, ++Univ 2, ADV PRON PREP 3, NP::PRON:CHAR 4, @@Adj Props Verb Tsub Pron 5, AUXP::VERB, your We conclude from this inspection of individual features that within a specific domain it is not necessarily advisable to start out with a resource that has been geared towards containing particularly affect-charged terminology." ></td>
	<td class="line x" title="100:115	See Pang et al.(2002) for a similar argument." ></td>
	<td class="line x" title="102:115	As our numbers and feature sets suggest, there are many terms (and grammatical patterns) associated with sentiment in a given domain that may not fall into a typical affect class." ></td>
	<td class="line x" title="103:115	We believe that these results show that as with many other classification tasks in the machine learning literature, it is preferable to start without an artificially limited hand-crafted set of features." ></td>
	<td class="line x" title="104:115	By using large feature sets which are derived from the data, and by paring down the number of features through a feature reduction procedure if necessary, relevant patterns in the data can be identified that may not have been obvious to the human intuition." ></td>
	<td class="line x" title="105:115	5 Conclusion We have shown that in the very noisy domain of customer feedback, it is nevertheless possible to perform sentiment classification." ></td>
	<td class="line x" title="106:115	This can be achieved by using large initial feature vectors combined with feature reduction based on log 1 this semantic feature indicates a negated context." ></td>
	<td class="line x" title="107:115	2 Universal quantification." ></td>
	<td class="line x" title="108:115	3 part of speech trigram." ></td>
	<td class="line x" title="109:115	4 An NP consisting of a pronoun followed by a punctuation character." ></td>
	<td class="line x" title="110:115	5 An adjectival semantic node modified by a verbal proposition and a pronominal subject." ></td>
	<td class="line x" title="111:115	This is in fact the representation for a copular construction of the form pronoun be adjective to verb as in I am happy to report likelihood ratio." ></td>
	<td class="line x" title="112:115	A second, more surprising result is that the use of abstract linguistic analysis features consistently contributes to the classification accuracy in sentiment classification." ></td>
	<td class="line x" title="113:115	While results like this have been reported in the area of style classification (Baayen et al. 1996, Gamon 2004), they are noteworthy in a domain where stylistic markers have not been considered in the past, indicating the need for more research into the stylistic correlations of affect in text." ></td>
	<td class="line x" title="114:115	6 Acknowledgements We thank Anthony Aue and Eric Ringger (Microsoft Research) and Hang Li (Microsoft Research Asia) for helpful comments and discussions, and Chris Moore (Microsoft Product Support Services UK) for the initial request for sentiment classification based on the needs of Support Services at Microsoft." ></td>
	<td class="line x" title="115:115	Thanks also go to Karin Berghoefer of the Butler-Hill group for manually annotating a subset of the data." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1145
Morpheme-Based Derivation Of Bipolar Semantic Orientation Of Chinese Words
Yuen, Raymond W. M.;Chan, Terence Y. W.;Lai, Tom Bong-Yeung;Kwong, Oi Yee;Tsou, Benjamin K.;"></td>
	<td class="line x" title="1:195	Morpheme-based Derivation of Bipolar Semantic Orientation of Chinese Words Raymond W.M. Yuen, Terence Y.W. Chan, Tom B.Y. Lai, O.Y. Kwong, Benjamin K.Y. T'sou Language Information Sciences Research Centre, the City University of Hong Kong 83 Tat Chee Avenue, Hong Kong { wmyuen, dcywchan, cttomlai, rlolivia, rlbtsou}@cityu.edu.hk Abstract The evaluative character of a word is called its semantic orientation (SO)." ></td>
	<td class="line x" title="2:195	A positive SO indicates desirability (e.g. Good, Honest) and a negative SO indicates undesirability (e.g. , Bad, Ugly)." ></td>
	<td class="line x" title="3:195	This paper presents a method, based on Turney (2003), for inferring the SO of a word from its statistical association with strongly-polarized words and morphemes in Chinese." ></td>
	<td class="line x" title="4:195	It is noted that morphemes are much less numerous than words, and that also a small number of fundamental morphemes may be used in the modified system to great advantage." ></td>
	<td class="line x" title="5:195	The algorithm was tested on 1,249 words (604 positive and 645 negative) in a corpus of 34 million words, and was run with 20 and 40 polarized words respectively, giving a high precision (79.96% to 81.05%), but a low recall (45.56% to 59.57%)." ></td>
	<td class="line x" title="6:195	The algorithm was then run with 20 polarized morphemes, or single characters, in the same corpus, giving a high precision of 80.23% and a high recall of 85.03%." ></td>
	<td class="line x" title="7:195	We concluded that morphemes in Chinese, as in any language, constitute a distinct sub-lexical unit which, though small in number, has greater linguistic significance than words, as seen by the significant enhancement of results with a much smaller corpus than that required by Turney." ></td>
	<td class="line x" title="8:195	1." ></td>
	<td class="line x" title="9:195	Introduction The semantic orientation (SO) of a word indicates the direction in which the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974)." ></td>
	<td class="line x" title="10:195	Words that encode a desirable state (e.g. , beautiful) have a positive SO, while words that represent undesirable states (e.g. absurd) have a negative SO (Hatzivassiloglou and Wiebe, 2000)." ></td>
	<td class="line x" title="11:195	Hatzivassiloglou and Mckeown (1997) used the words and, or, and but as linguistic cues to extract adjective pairs." ></td>
	<td class="line x" title="12:195	Turney (2003) assessed the SO of words using their occurrences near stronglypolarized words like excellent and poor with accuracy from 61% to 82%, subject to corpus size." ></td>
	<td class="line x" title="13:195	Turneys algorithm requires a colossal corpus (hundred billion words) indexed by the AltaVista search engine in his experiment." ></td>
	<td class="line x" title="14:195	Undoubtedly, internet texts have formed a very large and easilyaccessible corpus." ></td>
	<td class="line x" title="15:195	However, Chinese texts in internet are not segmented so it is not costeffective to use them." ></td>
	<td class="line x" title="16:195	This paper presents a general strategy for inferring SO for Chinese words from their association with some strongly-polarized morphemes." ></td>
	<td class="line x" title="17:195	The modified system of using morphemes was proved to be more effective than strongly-polarized words in a much smaller corpus." ></td>
	<td class="line x" title="18:195	Related work and potential applications of SO are discussed in section 2." ></td>
	<td class="line x" title="19:195	Section 3 illustrates one of the methods of Turneys model for inferring SO, namely, Pointwise Mutual Information (PMI), based on the hypothesis that the SO of a word tends to correspond to the SO of its neighbours." ></td>
	<td class="line x" title="20:195	The experiment with polarized words is presented in section 4." ></td>
	<td class="line x" title="21:195	The test set includes 1,249 words (604 positive and 645 negative)." ></td>
	<td class="line x" title="22:195	In a corpus of 34 million word tokens, 410k word types, the algorithm is run with 20 and 40 polarized words, giving a precision of 79.96% and 81.05%, and a recall of 45.56% and 59.57%, respectively." ></td>
	<td class="line x" title="23:195	The system is further modified by using polarized morphemes in section 5." ></td>
	<td class="line x" title="24:195	We first evaluate the distinction of Chinese morphemes to justify why the modification can probably give simpler and better results, and then introduce a more scientific selection of polarized morphemes." ></td>
	<td class="line x" title="25:195	A high precision of 80.23% and a greatly increased recall of 85.03% are yielded." ></td>
	<td class="line x" title="26:195	In section 6, the algorithm is run with 14, 10 and 6 morphemes, giving a precision of 79.15%, 79.89% and 75.65%, and a recall of 79.50%, 73.26% and 66.29% respectively." ></td>
	<td class="line x" title="27:195	It shows that the algorithm can be also effectively run with 6 to 10 polarized morphemes in a smaller corpus." ></td>
	<td class="line x" title="28:195	The conclusion and future work are discussed in section 7." ></td>
	<td class="line x" title="29:195	2. Related Work and Applications Hatzivassiloglou and Mckeown (1997) presented a method for automatically assigning a + or  orientation label to adjectives known to have some SO by the linguistic constraints on the use of adjectives in conjunctions." ></td>
	<td class="line x" title="30:195	For example, and links adjectives that have the same SO, while but links adjectives that have opposite SO." ></td>
	<td class="line x" title="31:195	They devised an algorithm based on such constraints to evaluate 1,336 manually-labeled adjectives (657 positive and 679 negative) with 97% accuracy in a corpus of 21 million words." ></td>
	<td class="line x" title="32:195	Turney (2003) introduced a method for automatically inferring the direction and intensity of the SO of a word from its statistical association with a set of positive and negative paradigm words, i.e., strongly-polarized words." ></td>
	<td class="line x" title="33:195	The algorithm was evaluated on 3,596 words (1,614 positive and 1,982 negative) including adjectives, adverbs, nouns, and verbs." ></td>
	<td class="line x" title="34:195	An accuracy of 82.8% was attained in a corpus of hundred billion words." ></td>
	<td class="line oc" title="35:195	SO can be used to classify reviews (e.g. , movie reviews) as positive or negative (Turney, 2002), and applied to subjectivity analysis such as recognizing hostile messages, classifying emails, mining reviews (Wiebe et al. , 2001)." ></td>
	<td class="line o" title="36:195	The first step of those applications is to recognize that the text is subjective and then the second step, naturally, is to determine the SO of the subjective text." ></td>
	<td class="line x" title="37:195	Also, it can be used to summarize argumentative articles like editorials of news media." ></td>
	<td class="line x" title="38:195	A summarization system would benefit from distinguishing sentences intended to present factual materials from those intended to present opinions, since many summaries are meant to include only facts." ></td>
	<td class="line x" title="39:195	3." ></td>
	<td class="line x" title="40:195	SO from Association-PMI Turney (2003) examined SO-PMI (Pointwise Mutual Information) and SO-LSA (Latent Semantic Analysis)." ></td>
	<td class="line x" title="41:195	SO-PMI will be our focus in the following parts." ></td>
	<td class="line x" title="42:195	PMI is defined as: PMI(word1, word2)=log2( )()( )&( 21 21 wordpwordp wordwordp ) where p(word1 & word2) is the probability that word1 and word2 co-occur." ></td>
	<td class="line x" title="43:195	If the words are statistically independent, the probability that they co-occur is given by the product p(word1) p(word2)." ></td>
	<td class="line x" title="44:195	The ratio between p(word1 & word2) and p(word1) p(word2) is a measure of the degree of statistical dependence between the words." ></td>
	<td class="line x" title="45:195	The SO of a given word is calculated from the strength of its association with a set of positive words, minus the strength of its association with a set of negative words." ></td>
	<td class="line x" title="46:195	Thus the SO of a word, word, is calculated by SO-PMI as follows: SO-PMI(word) =" ></td>
	<td class="line x" title="47:195	Pwordspword pwordwordPMI ),( -" ></td>
	<td class="line x" title="48:195	Nwordsnword nwordwordPMI ),( where Pwords is a set of 7 positive paradigm words (good, nice, excellent, positive, fortunate, correct, and superior) and Nwords is a set of 7 negative paradigm words (bad, nasty, poor, negative, unfortunate, wrong, and inferior)." ></td>
	<td class="line x" title="49:195	Those 14 words were chosen by intuition and based on opposing pairs (good/bad, excellent/poor, etc.)." ></td>
	<td class="line x" title="50:195	The words are rather insensitive to context, i.e., excellent is positive in almost all contexts." ></td>
	<td class="line x" title="51:195	A word, word, is classified as having a positive SO when SO-PMI(word) is positive and a negative SO when SO-PMI(word) is negative." ></td>
	<td class="line x" title="52:195	Turney (2003) used the Alta Vista Advanced search engine with a NEAR operator, which constrains the search to documents that contain the words within ten words of one another, in either order." ></td>
	<td class="line x" title="53:195	Three corpora were tested." ></td>
	<td class="line x" title="54:195	AV-ENG is the largest corpus covering 350 million web pages (English only) indexed by Alta Vista." ></td>
	<td class="line x" title="55:195	The medium corpus is a 2% subset of AV-ENG corpus called AV-CA (Canadian domain only)." ></td>
	<td class="line x" title="56:195	The smallest corpus TASA is about 0.5% of AV-CA and contains various short documents." ></td>
	<td class="line x" title="57:195	One of the lexicons used in Turneys experiment is the GI lexicon (Stone et al. , 1966), which consists of 3,596 adjectives, adverbs, nouns, and verbs, 1,614 positive and 1,982 negative." ></td>
	<td class="line x" title="58:195	Table 1 shows the precision of SO-PMI with the GI lexicon in the three corpora." ></td>
	<td class="line x" title="59:195	Precision Percent of full test set Size of test set AV-ENG AV-CA TASA 100% 3596 82.84% 76.06% 61.26% 75% 2697 90.66% 81.76% 63.92% 50% 1798 95.49% 87.26% 47.33% 25% 899 97.11% 89.88% 68.74% Approx." ></td>
	<td class="line x" title="60:195	no. of words 1x10 11 2x109 1x107 Table 1: The precision of SO-PMI with the GI lexicon The strength (absolute value) of the SO was used as a measure of confidence that the words will be correctly classified." ></td>
	<td class="line x" title="61:195	Test set words were sorted in descending order of the absolute value of their SO and the top ranked words (the highest confidence words) were then classified." ></td>
	<td class="line x" title="62:195	For example, the second row (starting with 75%) in table 1 shows the precision when the top 75% were classified and the last 25% (with lowest confidence) were ignored." ></td>
	<td class="line x" title="63:195	We will employ this measure of confidence in the following experiments." ></td>
	<td class="line x" title="64:195	Turney concluded that SO-PMI requires a large corpus (hundred billion words), but it is simple, easy to implement, unsupervised, and it is not restricted to adjectives." ></td>
	<td class="line x" title="65:195	4." ></td>
	<td class="line x" title="66:195	Experiment with Chinese Words In the following experiments, we applied Turneys method to Chinese." ></td>
	<td class="line x" title="67:195	The algorithm was run with 20 and then 40 paradigm words for comparison." ></td>
	<td class="line x" title="68:195	The experiment details include: NEAR Operator: it was applied to constrain the search to documents that contain the words within ten words of one another, in either order." ></td>
	<td class="line x" title="69:195	Corpus: the LIVAC synchronous corpus (Tsou et al. , 2000, http://www.livac.org) was used." ></td>
	<td class="line x" title="70:195	It covers 9-year news reports of Chinese communities including Hong Kong, Beijing and Taiwan, and we used a sub-corpus with about 34 million word tokens and 410k word types." ></td>
	<td class="line x" title="71:195	Test Set Words: a combined set of two dictionaries of polarized words (Guo, 1999, Wang, 2001) was used to evaluate the results." ></td>
	<td class="line x" title="72:195	While LIVAC is an enormous Chinese corpus, its size is still far from the hundred-billion-word corpus used by Turney." ></td>
	<td class="line x" title="73:195	It is likely that some words in the combined set are not used in the 9-year corpus." ></td>
	<td class="line x" title="74:195	To avoid a skewed recall, the number of test set words used in the corpus is given in table 2." ></td>
	<td class="line x" title="75:195	In other words, the recall can be calculated by the total number of words used in the corpus, but not by that recorded in the dictionaries." ></td>
	<td class="line x" title="76:195	The difference between two numbers is just 100." ></td>
	<td class="line x" title="77:195	Polarity Total no." ></td>
	<td class="line x" title="78:195	of the test set words Words used in the 9-year corpus Positive 629 604 Negative 721 645 Total 1350 1249 Table 2: Number of the test set words Paradigm words: The paradigm words were chosen using intuition and based on opposing pairs, as Turney (2003) did." ></td>
	<td class="line x" title="79:195	The first experiment was conducted with 10 positive and 10 negative paradigm words, as follows, Pwords:  (honest), # (clever),  (sufficient), (lucky), % (right),  (excellent), (prosperous), (kind),  (brave), P (humble) Nwords: (hypocritical), a (foolish),  R (deficient), l (unlucky), Q (wrong), m (adverse), (unsuccessful), v J (violent),  (cowardly),  (arrogant) The experiment was then repeated by increasing the number of paradigm words to 40." ></td>
	<td class="line x" title="80:195	The paradigm words added are: Pwords: 5 I (mild),  (favourable),  (successful), % (positive),  (active), W (optimistic),  (benign),  (attentive), ; l (promising), e (incorrupt) Nwords:  ^ (radical), l (unfavourable), (failed), (negative),  (passive), (pessimistic),  (malignant), (inattentive), (indifferent), M (corrupt) 4.1 Results Tables 3 and 4 show the precision and recall of SO-PMI by two sets of paradigm words." ></td>
	<td class="line x" title="81:195	% of test set 100% 75% 50% 25% Size of test set 1249 937 625 312 Extracted Set 569 427 285 142 Precision 79.96% 86.17% 86.99% 90.16% Recall 45.56% Table 3: Precision and Recall of the SO-PMI of the 20 paradigm word test set % of test set 100% 75% 50% 25% Size of test set 1249 937 625 312 Extracted Set 744 558 372 186 Precision 81.05% 86.02% 88.71% 94.09% Recall 59.57% Table 4: Precision and Recall of the SO-PMI of the 40 paradigm word test set The results of both sets gave a satisfactory precision of 80% even in 100% confidence." ></td>
	<td class="line x" title="82:195	However, the recall was just 45.56% under the 20word condition, and rose to 59.57% under the 40word condition." ></td>
	<td class="line x" title="83:195	The 15% rise was noted." ></td>
	<td class="line x" title="84:195	To further improve the recall performance, we experimented with a modified algorithm based on the distinct features of Chinese morphemes." ></td>
	<td class="line x" title="85:195	5." ></td>
	<td class="line x" title="86:195	Experiment with Chinese Morphemes Taking morphemes to be smallest linguistic meaningful unit, Chinese morphemes are mostly monosyllabic and single characters, although there are some exceptional poly-syllabic morphemes like *`* (grape), (coffee), which are mostly loanwords." ></td>
	<td class="line x" title="87:195	In the following discussion, we consider morphemes to be monosyllabic and represented by single characters." ></td>
	<td class="line x" title="88:195	It is observed that many poly-syllabic words with the same SO incorporate a common set of morphemes." ></td>
	<td class="line x" title="89:195	The fact suggests the possibility of using paradigm morphemes instead of words." ></td>
	<td class="line x" title="90:195	Unlike English, the constituent morphemes of a Chinese word are often free-standing monosyllabic words." ></td>
	<td class="line x" title="91:195	It is note-worthy that words in ancient Chinese were much more mono-morphemic than modern Chinese." ></td>
	<td class="line x" title="92:195	The evolution from monosyllabic word to disyllabic word may have its origin in the phonological simplification which has given rise to homophony, and which has affected the efficacy of communication." ></td>
	<td class="line x" title="93:195	To compensate for this, many more related disyllabic words have appeared in modern Chinese (Tsou, 1976)." ></td>
	<td class="line x" title="94:195	There are three basic constructions for deriving disyllabic words in Chinese, including: (1) combination of synonyms or near synonyms ( 5 f, warm, genial, 5=warm, mild, f =warm, genial) (2) combination of semantically related morphemes ( P , P=affair, =circumstances) (3) The affixation of minor suffixes which serve no primary grammatical function ( 6, =village, 6=zi, suffix) The three processes for deriving disyllabic morphemes in Chinese outlined here should be viewed as historical processes." ></td>
	<td class="line x" title="95:195	The extent to which such processes may be realized by native speakers to be productive synchronically bears further exploration." ></td>
	<td class="line x" title="96:195	Of the three processes, the first two, i.e., synonym and near-synonym compounding, are used frequently by speakers for purposes of disambiguation." ></td>
	<td class="line x" title="97:195	In view of this development, the evolution from monosyllabic words in ancient Chinese to disyllabic words in modern Chinese does not change the inherent meaning of the morphemes (words in ancient Chinese) in many cases." ></td>
	<td class="line x" title="98:195	The SO of a word often conforms to that of its morphemes." ></td>
	<td class="line x" title="99:195	In English, there are affixal morphemes like dis-, un(negation prefix), or less (suffix meaning short-age), -ful (suffix meaning to have a property of), we can say careful or careless to expand the meaning of care." ></td>
	<td class="line x" title="100:195	However, it is impossible to construct a word like *ful-care, *less-care." ></td>
	<td class="line x" title="101:195	However, in Chinese, the position of a morpheme in many disyllabic words is far more flexible in the formation of synonym and near-synonym compound words." ></td>
	<td class="line x" title="102:195	For instance,  b(honor) is a part of two similar word  b(& (honor-bright) and  v b(outstanding-honor)." ></td>
	<td class="line x" title="103:195	Morphemes in Chinese are like a zipped file of the same file types." ></td>
	<td class="line x" title="104:195	When it unzips, all the words released have the same SO." ></td>
	<td class="line x" title="105:195	5.1 Probability of Constituent Morphemes of Words with the Same SO Most morphemes can contribute to positive or negative words, regardless of their inherent meaning." ></td>
	<td class="line x" title="106:195	For example,   (luck) has inherently a positive meaning, but it can construct both positive word  1  (lucky) or a negative word   (unlucky)." ></td>
	<td class="line x" title="107:195	Thus it is not easy to define the paradigm set simply by intuition." ></td>
	<td class="line x" title="108:195	But we can assign a probability value for a morpheme in forming polarized words on the basis of corpus data." ></td>
	<td class="line x" title="109:195	The first step is to come up with possible paradigm morphemes by intuition in a large set of polarized words." ></td>
	<td class="line x" title="110:195	With the LIVAC synchronous corpus, the types and tokens of the words constructed by the selected morphemes can easily be extracted." ></td>
	<td class="line x" title="111:195	The word types, excluding proper nouns, are then manually-labeled as negative, neutral or positive." ></td>
	<td class="line x" title="112:195	Then to obtain the probability that a polar morpheme generates words with the same SO, the tokens of the polarized word types carrying the morpheme are divided by the tokens of all word types carrying the morpheme." ></td>
	<td class="line x" title="113:195	For example, given a negative morpheme, m1, the probability that it appears in negative words in token, P(m1, -ve) is given by: 1m Carrying WordtypesAll of Tokens 1m Carrying rdtypesNegativeWo of Tokens Positive morphemes can be done likewise." ></td>
	<td class="line x" title="114:195	Ten negative morphemes and ten positive morphemes were chosen as in table 5." ></td>
	<td class="line x" title="115:195	Their values of P(morpheme, orientation) are all above 0.95." ></td>
	<td class="line x" title="116:195	+ve Morpheme -ve Morpheme 1 { (gift) (hurt) 2  (win)      3 (good) (doubt) 4 (secure) : (difficult) 5 (rich)  (rush) 6 (health)    7 (happy) h (explode) 8  (honor) (ban) 9 (hardworking) (collapse) 10 (smooth)  (reject) Derived Types 7383 2048 Tokens 247249 166335 Table 5: Selected positive and negative morphemes Those morphemes were extracted from a 5-year subset of the LIVAC corpus." ></td>
	<td class="line x" title="117:195	A morpheme, free to construct new words, may construct hundreds of words but those words with extremely low frequency can be regarded as noise." ></td>
	<td class="line x" title="118:195	The noise may be creative use or even incorrect use." ></td>
	<td class="line x" title="119:195	Thus, the number of ready-to-label word types formed from a particular morpheme was limited to 50, but it must cover 80% of the tokens of all word types carrying the morpheme in the corpus (i.e. , 80% dominance)." ></td>
	<td class="line x" title="120:195	For example, if the morpheme m1 constructs 120 word types with 10,000 tokens, and the first 50 high-frequency words can reach 8,000 tokens, then the remaining 70 low-frequency word types, or noise, are discarded." ></td>
	<td class="line x" title="121:195	Otherwise, the number of sampled words would be expanded to a number (over 50) fulfilling 80% dominance." ></td>
	<td class="line x" title="122:195	5.2 Results and Evaluation In table 6, the precision of 80.23% is slightly better than 79.96% of the 20-word condition, and just 1% lower than that of the 40-word condition." ></td>
	<td class="line x" title="123:195	However, the recall drastically increases from 45.56%, or 59.57% under the 40-word condition, to 85.03%." ></td>
	<td class="line x" title="124:195	In other words, the algorithm run with 20 Chinese paradigm morphemes resulted not only in high precision but also much higher recall than Chinese paradigm words in the same corpus." ></td>
	<td class="line x" title="125:195	% of test set 100% 75% 50% 25% Size of test set 1249  937  625  312  Extracted Set 1062  797  531  266  Precision 80.23%  85.44%  90.96%  96.61%  Recall 85.03% Table 6: Precision and Recall of SO-PMI of the 20 paradigm morpheme test set Since the morphemes were chosen from a subset of the corpus for evaluation, we repeated the experiment in a separate 1-year corpus (20012002)." ></td>
	<td class="line x" title="126:195	The results in table 7 reflect a similar pattern in the two corpora  both words and morphemes can get high precision, but morphemes can double the recall of words." ></td>
	<td class="line x" title="127:195	40 Words 20 Morphemes Size of test set 1065 Extracted Set 333 671 Precision (Full Set) 75.38% 73.62% Recall 31.27% 63.00% Table 7: Precision (full test set only) and Recall of SO-PMI of 40 paradigm words and 20 paradigm morphemes in 1-year corpus It is assumed that a smaller corpus easily leads to the algorithms low recall because many lowfrequency words in the test set barely associate with the paradigm words." ></td>
	<td class="line x" title="128:195	To examine the assumption, the results were further analyzed with the frequency of the test set words." ></td>
	<td class="line x" title="129:195	First, the occurrence of the test set words in the 9-year corpus was counted, then the median of the frequency, 44 in this case, was taken." ></td>
	<td class="line x" title="130:195	The results were divided into two sections from the median value, and the recall of two sections was calculated respectively, as in table 8." ></td>
	<td class="line x" title="131:195	                  `      `         `      `     Table 8: Morpheme-based and word-based recall of high-frequency and low-frequency words The results showed that high-frequency words could be largely extracted by the algorithm with both morphemes (99.80% recall) and words (89.45% recall)." ></td>
	<td class="line x" title="132:195	However, paradigm words gave 26.55% recall of low-frequency words, whereas paradigm morphemes gave 67.66%." ></td>
	<td class="line x" title="133:195	They showed that morphemes outperform words in the retrieval of low-frequency words." ></td>
	<td class="line x" title="134:195	Colossal corpora like Turneys hundred-billionword corpus can compensate for the low performance of paradigm words in low-frequency words." ></td>
	<td class="line x" title="135:195	Such a large corpus has been easilyaccessible since the emergence of internet, but it is not cost-effective to use the Chinese texts from the internet because those texts are not segmented." ></td>
	<td class="line x" title="136:195	Another way of compensation is the expansion of paradigm words, but doubling the number of paradigm words just raised the recall from 45.56% to 59.57%, as shown in section 4." ></td>
	<td class="line x" title="137:195	The supervised cost is not reasonable if the number of paradigm words is further expanded." ></td>
	<td class="line x" title="138:195	Morphemes, or single characters in Chinese, naturally occur more frequently than words in an article, so 20 morphemes can be more discretelydistributed over texts than 20 or even 40 words." ></td>
	<td class="line x" title="139:195	The results show that some morphemes always retain their inherent SO when becoming constituents in other derived words." ></td>
	<td class="line x" title="140:195	Such morphemes are like a zipped file of the same SO, when the algorithm is run with 20 paradigm morphemes, it is actually run by thousands of paradigm words." ></td>
	<td class="line x" title="141:195	Consequently, the recall could double while the high precision was not affected." ></td>
	<td class="line x" title="142:195	It may be argued that the labour cost of defining the SO of 20 morphemes is not sufficiently low either." ></td>
	<td class="line x" title="143:195	The following experiments will demonstrate that decreasing the number of morphemes can also give satisfactory results." ></td>
	<td class="line x" title="144:195	6." ></td>
	<td class="line x" title="145:195	Experiment with different number of morphemes The following experiments were done respectively by decreasing the number of morphemes, i.e., 14 and 10 morphemes, chosen from table 5." ></td>
	<td class="line x" title="146:195	The algorithm was then run with 3 groups of 6 different morphemes, in which the morphemes were different, and the combination of morphemes in each group was random." ></td>
	<td class="line x" title="147:195	The morphemes in each group are shown in table 9." ></td>
	<td class="line x" title="148:195	Other conditions for the experiments were unchanged." ></td>
	<td class="line x" title="149:195	6.1 Results and Evaluation Table 10 shows the results with different number of morphemes, and table 11 shows those for different groups of 6 morphemes." ></td>
	<td class="line x" title="150:195	For convenient comparison, the tables only show the results of the full test set, i.e., no threshold filtering." ></td>
	<td class="line x" title="151:195	It is shown that the recall falls as the number of morphemes is reduced." ></td>
	<td class="line x" title="152:195	However, even the average recall 66.29% under the 6-morpheme condition is still higher than that under the 40-word condition (59.57%)." ></td>
	<td class="line x" title="153:195	In section 5, it was evaluated that low recall could be attributed to the low frequency of test set words." ></td>
	<td class="line x" title="154:195	Therefore, 6 to 10 morphemes are already ideal for deducing the SO of highfrequency words." ></td>
	<td class="line x" title="155:195	Number of morphemes used Morpheme 20 14 10 6 (Gp1) 6 (Gp2) 6 (Gp3) P { (gift) 1 1 P (good) 1 1 1 1 P (happy) 1 1 1 P (rich) 1 1 1 P  (honor) 1 1 1 1 P (smooth) 1 1 1 1 P  (win) 1 1 P (secure) 1 1 P (health) 1 1 1 1 P (hardworking) 1 1 1 N (doubt) 1 1 1 1 N h (explode) 1 1 1 N (ban) 1 1 1 1 N (rash) 1 1 1 1 N (greedy) 1 1 1 1 N : (difficult) 1 1 1 1 N (hurt) 1 1 1 N  (rush) 1 1 N (collapse) 1 1 N  (reject) 1 Table 9: Morphemes selected for different experimental sets, P=+ve, N=-ve, 1=selected, Gp= Group Number of morphemes used No of morphemes 20 14 10 Size of test set 1249 1249 1249 Extracted Set 1062 993 915 Precision (%) 80.23 79.15 79.89 Recall (%) 85.03 79.50 73.26 Table 10: Precision and Recall of SO-PMI of the test set words with different no." ></td>
	<td class="line x" title="156:195	of morphemes Group of Morphemes Group 1 Group 2 Group 3 Average Size of test set 1249 1249 1249 1249 Extracted Set 837 776 871 828  Precision (%) 79.69 78.48 68.77 75.65  Recall (%) 67.01 62.13 69.74 66.29  Table 11: Precision and Recall of SO-PMI of the test set words with 3 different groups of 6 morphemes The precision remains high from 20 morphemes to 6 morphemes, but from table 10 the precision varies with different sets of morphemes." ></td>
	<td class="line x" title="157:195	Group 3 gave the lowest precision of 68.77%, whereas other groups gave a high precision close to 80%." ></td>
	<td class="line x" title="158:195	The limited space of this paper cannot allow a detailed investigation into the reasons for this result, only some suggestions can be made." ></td>
	<td class="line x" title="159:195	The precision may be related to the dominant lexical types of the words constructed by the morphemes and those of the test set words." ></td>
	<td class="line x" title="160:195	Lexical types should be carefully considered in the algorithm for Chinese because Chinese is an isolating language no form change." ></td>
	<td class="line x" title="161:195	For example, the word  !currency1 (recover) can appear in different positions of a sentence, such as the following examples extracted from the corpus: (1) &  !currency19 currency1& (American economy is gradually recovering) (2)  '1  q & !currency10/ - (most people is now pessimistic about the economy recovery) (3)  4 !currency19 =  J5 ." ></td>
	<td class="line x" title="162:195	(decelerates the recovery, but also makes the future unpredictable)." ></td>
	<td class="line x" title="163:195	English allows different forms of recovery, like recovery, recovering, recovered but Chinese does not." ></td>
	<td class="line x" title="164:195	Lexical types are thus an important factor for the precision performance." ></td>
	<td class="line x" title="165:195	Another way of solving the problems of lexical types is the automatic extraction of meaningful units (Danielsson, 2003)." ></td>
	<td class="line x" title="166:195	Simply, meaningful units are some frequently-used patterns which consist of two or more words." ></td>
	<td class="line x" title="167:195	It is useful to automatically extract the meaningful units with SO in future." ></td>
	<td class="line x" title="168:195	Syntactic markers like negation, and creative uses like ironical expression of adding quotation marks can also affect the precision." ></td>
	<td class="line x" title="169:195	Here is an example from the corpus: (   q (HONEST BUSINESSMAN)." ></td>
	<td class="line x" title="170:195	The quotation mark (  in English) is to actually express the opposite meaning of words within the mark, i.e., HONEST means DISHONEST in this case." ></td>
	<td class="line x" title="171:195	Such markers should further be handled, just as with the use of so-called." ></td>
	<td class="line x" title="172:195	6 Conclusion and Future Work This paper presents an algorithm based on Turneys model (2003) for inferring SO of Chinese words from their association with stronglypolarized Chinese morphemes." ></td>
	<td class="line x" title="173:195	The algorithm was run with 20 and 40 strongly-polarized Chinese words respectively in a corpus of 34 million words, giving a high precision of 79.96% and 81.05%, but a low recall of 45.56% and 59.57%." ></td>
	<td class="line x" title="174:195	The algorithm was then run with 20 Chinese polarized morphemes, or single characters, in the same corpus, giving a high precision of 80.23% and an even high recall of 85.03%." ></td>
	<td class="line x" title="175:195	The algorithm was further run with just 14, 10 and 6 morphemes, giving a precision of 79.15%, 79.89% and 75.65%, and a recall of 79.50%, 73.26% and 66.29% respectively." ></td>
	<td class="line x" title="176:195	Thus, conveniently defined morphemes in Chinese enhance the effectiveness of the algorithm by simplifying processing and yielding better results even in a smaller corpus compared with what Turney (2003) used." ></td>
	<td class="line x" title="177:195	Just 6 to 10 morphemes can give satisfactory results in a smaller corpus." ></td>
	<td class="line x" title="178:195	The efficient application of Turneys algorithm with help of colossal corpus like hundred-billionword corpus is matched by the ready availability of internet texts." ></td>
	<td class="line x" title="179:195	However, the same convenience is not available to Chinese because of the heavy cost of word segmentation." ></td>
	<td class="line x" title="180:195	The efficient application of Turneys algorithm with help of colossal corpus like hundred-billionword corpus is matched by the ready availability of internet texts." ></td>
	<td class="line x" title="181:195	However, the same convenience is not available to Chinese because of the heavy cost of word segmentation." ></td>
	<td class="line x" title="182:195	In our experiment, all syntactic markers are ignored." ></td>
	<td class="line x" title="183:195	Better results can be expected if syntactic markers are taken into consideration." ></td>
	<td class="line x" title="184:195	An obvious example is negation (not, never) which can counteract the polarity of a word." ></td>
	<td class="line x" title="185:195	In future, we will try to handle negation and other syntactic markers." ></td>
	<td class="line x" title="186:195	The lists of the probability of morphemes forming polarized words in section 5.2 can be handled by the concept of decision list (Yarowsky, 2000) which has not been applied in this paper for simplification." ></td>
	<td class="line x" title="187:195	In the future, decision lists can be employed to systematically include the loaded features of morphemes." ></td>
	<td class="line x" title="188:195	The experiment can be conducted with different sets of paradigm morphemes, and on corpora of different sizes." ></td>
	<td class="line x" title="189:195	With the LIVAC synchronous corpus (Tsou et al. , 2000), it should be possible to compare the SO of some words in different communities like Beijing, Hong Kong and Taipei." ></td>
	<td class="line x" title="190:195	The data would be valuable for cultural studies if the SO of some words fluctuates in different communities." ></td>
	<td class="line x" title="191:195	SO from association can be also applied to the judgment of news articles like editorials on celebrities." ></td>
	<td class="line x" title="192:195	Given a celebrity name or organization name, we can calculate, using SO-PMI, the strength of SO of the given word, i.e., the name." ></td>
	<td class="line x" title="193:195	Then we would be able to tell whether the news about the target is positive or negative." ></td>
	<td class="line x" title="194:195	For example, we tried to calculate the SO-PMI of the name George W Bush, the U.S. President, with thousands of polarized Chinese words in the corpus, it was found that the SO-PMI of Bush was about -200 from January to February, 2003, and plunged to -500 from March to April, 2003, when U.S. launched an unauthorized war against Iraq." ></td>
	<td class="line x" title="195:195	Such useful applications will be further investigated in future." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C04-1200
Determining The Sentiment Of Opinions
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:153	Determining the Sentiment of Opinions Soo-Min Kim Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292-6695 skim@isi.edu Eduard Hovy Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu Abstract Identifying sentiments (the affective parts of opinions) is a challenging problem." ></td>
	<td class="line x" title="2:153	We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion." ></td>
	<td class="line x" title="3:153	The system contains a module for determining word sentiment and another for combining sentiments within a sentence." ></td>
	<td class="line x" title="4:153	We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results." ></td>
	<td class="line x" title="5:153	1 Introduction What is an opinion?" ></td>
	<td class="line x" title="6:153	The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003)." ></td>
	<td class="line oc" title="7:153	Recent computational work either focuses on sentence subjectivity (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives." ></td>
	<td class="line x" title="8:153	We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003)." ></td>
	<td class="line x" title="9:153	Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion." ></td>
	<td class="line x" title="10:153	For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief." ></td>
	<td class="line x" title="11:153	For example, the following opinions contain Claims but no Sentiments: I believe the world is flat The Gap is likely to go bankrupt Bin Laden is hiding in Pakistan Water always flushes anti-clockwise in the southern hemisphere Like Yu and Hatzivassiloglou (2003), we want to automatically identify Sentiments, which in this work we define as an explicit or implicit expression in text of the Holders positive, negative, or neutral regard toward the Claim about the Topic." ></td>
	<td class="line x" title="12:153	(Other sentiments we plan to study later)." ></td>
	<td class="line x" title="13:153	Sentiments always involve the Holders emotions or desires, and may be present explicitly or only implicitly: I think that attacking Iraq would put the US in a difficult position (implicit) The US attack on Iraq is wrong (explicit) I like Ike (explicit) We should decrease our dependence on oil (implicit) Reps. Tom Petri and William F. Goodling asserted that counting illegal aliens violates citizens basic right to equal representation (implicit) In this paper we address the following challenge problem." ></td>
	<td class="line x" title="14:153	Given a Topic (e.g. , Should abortion be banned?) and a set of texts about the topic, find the Sentiments expressed about (claims about) the Topic (but not its supporting subtopics) in each text, and identify the people who hold each sentiment." ></td>
	<td class="line x" title="15:153	To avoid the problem of differentiating between shades of sentiments, we simplify the problem to: identify just expressions of positive, negative, or neutral sentiments, together with their holders." ></td>
	<td class="line x" title="16:153	In addition, for sentences that do not express a sentiment but simply state that some sentiment(s) exist(s), return these sentences in a separate set." ></td>
	<td class="line x" title="17:153	For example, given the topic What should be done with Medicare? the sentence After years of empty promises, Congress has rolled out two Medicare prescription plans, one from House Republicans and the other from the Democratic Sentence POS Tagger verbs nounsAdjectives Adjective Sentiment classifier sentiment sentiment Sentence sentiment classifier Opinion region + polarity + holder Holder finder Named Entity Tagger Sentence Sentence texts + topic sentiment sentiment sentiment Verbs Verb Sentiment classifier Nouns Noun Sentiment classifier WordNet Sentence : Figure 1: System architecture." ></td>
	<td class="line x" title="18:153	Sens. Bob Graham of Florida and Zell Miller of Georgia should be returned in the separate set." ></td>
	<td class="line x" title="19:153	We approach the problem in stages, starting with words and moving on to sentences." ></td>
	<td class="line x" title="20:153	We take as unit sentiment carrier a single word, and first classify each adjective, verb, and noun by its sentiment." ></td>
	<td class="line x" title="21:153	We experimented with several classifier models." ></td>
	<td class="line x" title="22:153	But combining sentiments requires additional care, as Table 1 shows." ></td>
	<td class="line x" title="23:153	California Supreme Court agreed that the states new term-limit law was constitutional." ></td>
	<td class="line x" title="24:153	California Supreme Court disagreed that the states new term-limit law was constitutional." ></td>
	<td class="line x" title="25:153	California Supreme Court agreed that the states new term-limit law was unconstitutional." ></td>
	<td class="line x" title="26:153	California Supreme Court disagreed that the states new term-limit law was unconstitutional." ></td>
	<td class="line x" title="27:153	Table 1: Combining sentiments." ></td>
	<td class="line x" title="28:153	A sentence might even express opinions of different people." ></td>
	<td class="line x" title="29:153	When combining word-level sentiments, we therefore first determine for each Holder a relevant region within the sentence and then experiment with various models for combining word sentiments." ></td>
	<td class="line x" title="30:153	We describe our models and algorithm in Section 2, system experiments and discussion in Section 3, and conclude in Section 4." ></td>
	<td class="line x" title="31:153	2 Algorithm Given a topic and a set of texts, the system operates in four steps." ></td>
	<td class="line x" title="32:153	First it selects sentences that contain both the topic phrase and holder candidates." ></td>
	<td class="line x" title="33:153	Next, the holder-based regions of opinion are delimited." ></td>
	<td class="line x" title="34:153	Then the sentence sentiment classifier calculates the polarity of all sentiment-bearing words individually." ></td>
	<td class="line x" title="35:153	Finally, the system combines them to produce the holders sentiment for the whole sentence." ></td>
	<td class="line x" title="36:153	Figure 1 shows the overall system architecture." ></td>
	<td class="line x" title="37:153	Section 2.1 describes the word sentiment classifier and Section 2.2 describes the sentence sentiment classifier." ></td>
	<td class="line x" title="38:153	2.1 Word Sentiment Classifier 2.1.1 Word Classification Models For word sentiment classification we developed two models." ></td>
	<td class="line x" title="39:153	The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two listspositive and negativeand then to grow this by adding words obtained from WordNet (Miller et al. 1993; Fellbaum et al. 1993)." ></td>
	<td class="line x" title="40:153	We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word good has synonyms virtuous, honorable, righteous and antonyms evil, disreputable, unrighteous." ></td>
	<td class="line x" title="41:153	Antonyms of negative words are added to the positive list, and synonyms to the negative one." ></td>
	<td class="line x" title="42:153	To start the seed lists we selected verbs (23 positive and 21 negative) and adjectives (15 positive and 19 negative), adding nouns later." ></td>
	<td class="line x" title="43:153	Since adjectives and verbs are structured differently in WordNet, we obtained from it synonyms and antonyms for adjectives but only synonyms for verbs." ></td>
	<td class="line x" title="44:153	For each seed word, we extracted from WordNet its expansions and added them back into the appropriate seed lists." ></td>
	<td class="line x" title="45:153	Using these expanded lists, we extracted an additional cycle of words from WordNet, to obtain finally 5880 positive adjectives, 6233 negative adjectives, 2840 positive verbs, and 3239 negative verbs." ></td>
	<td class="line x" title="46:153	However, not all synonyms and antonyms could be used: some had opposite sentiment or were neutral." ></td>
	<td class="line x" title="47:153	In addition, some common words such as great, strong, take, and get occurred many times in both positive and negative categories." ></td>
	<td class="line x" title="48:153	This indicated the need to develop a measure of strength of sentiment polarity (the alternative was simply to discard such ambiguous words)to determine how strongly a word is positive and also how strongly it is negative." ></td>
	<td class="line x" title="49:153	This would enable us to discard sentiment-ambiguous words but retain those with strengths over some threshold." ></td>
	<td class="line x" title="50:153	Armed with such a measure, we can also assign strength of sentiment polarity to as yet unseen words." ></td>
	<td class="line x" title="51:153	Given a new word, we use WordNet again to obtain a synonym set of the unseen word to determine how it interacts with our sentiment seed lists." ></td>
	<td class="line x" title="52:153	That is, we compute (1) ),|(maxarg )|(maxarg 21 n c c synsynsyncP wcP  where c is a sentiment category (positive or negative), w is the unseen word, and syn n are the WordNet synonyms of w. To compute Equation (1), we tried two different models: (2) )|()(maxarg )|()(maxarg )|()(maxarg)|(maxarg 1 ))(,(3 2 1  = = = = m k wsynsetfcount k c n c cc k cfPcP csynsynsynsynPcP cwPcPwcP where f k is the k th feature (list word) of sentiment class c which is also a member of the synonym set of w, and count(f k,synset(w)) is the total number of occurrences of f k in the synonym set of w. P(c) is the number of words in class c divided by the total number of words considered." ></td>
	<td class="line x" title="53:153	This model derives from document classification." ></td>
	<td class="line x" title="54:153	We used the synonym and antonym lists obtained from Wordnet instead of learning word sets from a corpus, since the former is simpler and does not require manually annotated data for training." ></td>
	<td class="line x" title="55:153	Equation (3) shows the second model for a word sentiment classifier." ></td>
	<td class="line x" title="56:153	(3) )( ),( )(maxarg )|()(maxarg)|(maxarg 1 ccount csyncount cP cwPcPwcP n i i c cc  = = = To compute the probability P(w|c) of word w given a sentiment class c, we count the occurrence of ws synonyms in the list of c. The intuition is that the more synonyms occuring in c, the more likely the word belongs." ></td>
	<td class="line x" title="57:153	We computed both positive and negative sentiment strengths for each word and compared their relative magnitudes." ></td>
	<td class="line x" title="58:153	Table 2 shows several examples of the system output, computed with Equation (2), in which + represents positive category strength and - negative." ></td>
	<td class="line x" title="59:153	The word amusing, for example, was classified as carrying primarily positive sentiment, and blame as primarily negative." ></td>
	<td class="line x" title="60:153	The absolute value of each category represents the strength of its sentiment polarity." ></td>
	<td class="line x" title="61:153	For instance, afraid with strength -0.99 represents strong negavitity while abysmal with strength -0.61 represents weaker negativity." ></td>
	<td class="line x" title="62:153	abysmal : NEGATIVE [+ : 0.3811][: 0.6188] adequate : POSITIVE [+ : 0.9999][: 0.0484e-11] afraid : NEGATIVE [+ : 0.0212e-04][: 0.9999] ailing : NEGATIVE [+ : 0.0467e-8][: 0.9999] amusing : POSITIVE [+ : 0.9999][: 0.0593e-07] answerable : POSITIVE [+ : 0.8655][: 0.1344] apprehensible: POSITIVE [+ : 0.9999][: 0.0227e-07] averse : NEGATIVE [+ : 0.0454e-05][: 0.9999] blame : NEGATIVE [+ : 0.2530][: 0.7469] Table 2: Sample output of word sentiment classifier." ></td>
	<td class="line x" title="63:153	2.2 Sentence Sentiment Classifier As shows in Table 1, combining sentiments in a sentence can be tricky." ></td>
	<td class="line x" title="64:153	We are interested in the sentiments of the Holder about the Claim." ></td>
	<td class="line x" title="65:153	Manual analysis showed that such sentiments can be found most reliably close to the Holder; without either Holder or Topic/Claim nearby as anchor points, even humans sometimes have trouble reliably determining the source of a sentiment." ></td>
	<td class="line x" title="66:153	We therefore included in the algorithm steps to identify the Topic (through direct matching, since we took it as given) and any likely opinion Holders (see Section 2.2.1)." ></td>
	<td class="line x" title="67:153	Near each Holder we then identified a region in which sentiments would be considered; any sentiments outside such a region we take to be of undetermined origin and ignore (Section 2.2.2)." ></td>
	<td class="line x" title="68:153	We then defined several models for combining the sentiments expressed within a region (Section 2.2.3)." ></td>
	<td class="line x" title="69:153	2.2.1 Holder Identification We used BBNs named entity tagger IdentiFinder to identify potential holders of an opinion." ></td>
	<td class="line x" title="70:153	We considered PERSON and ORGANIZATION as the only possible opinion holders." ></td>
	<td class="line x" title="71:153	For sentences with more than one Holder, we chose the one closest to the Topic phrase, for simplicity." ></td>
	<td class="line x" title="72:153	This is a very crude step." ></td>
	<td class="line x" title="73:153	A more sophisticated approach would employ a parser to identify syntactic relationships between each Holder and all dependent expressions of sentiment." ></td>
	<td class="line x" title="74:153	2.2.2 Sentiment Region Lacking a parse of the sentence, we were faced with a dilemma: How large should a region be?" ></td>
	<td class="line x" title="75:153	We therefore defined the sentiment region in various ways (see Table 3) and experimented with their effectiveness, as reported in Section 3." ></td>
	<td class="line x" title="76:153	Window1: full sentence Window2: words between Holder and Topic Window3: window2  2 words Window4: window2 to the end of sentence Table 3: Four variations of region size." ></td>
	<td class="line x" title="77:153	2.2.3 Classification Models We built three models to assign a sentiment category to a given sentence, each combining the individual sentiments of sentiment-bearing words, as described above, in a different way." ></td>
	<td class="line x" title="78:153	Model 0 simply considers the polarities of the sentiments, not the strengths: Model 0:  (signs in region) The intuition here is something like negatives cancel one another out." ></td>
	<td class="line x" title="79:153	Here the system assigns the same sentiment to both the California Supreme Court agreed that the states new term-limit law was constitutional and the California Supreme Court disagreed that the states new term-limit law was unconstitutional." ></td>
	<td class="line x" title="80:153	For this model, we also included negation words such as not and never to reverse the sentiment polarity." ></td>
	<td class="line x" title="81:153	Model 1 is the harmonic mean (average) of the sentiment strengths in the region: Model 1: cwcp wcp cn scP ij n i i = =  = )|(argmax if,)|( )( 1 )|( j 1 Here n(c) is the number of words in the region whose sentiment category is c. If a region contains more and stronger positive than negative words, the sentiment will be positive." ></td>
	<td class="line x" title="82:153	Model 2 is the geometric mean: Model 2: cwcpif wcpscP ij n i i cn = =  =  )|(argmax,)|(10)|( j 1 1)( 2.2.4 Examples The following are two example outputs." ></td>
	<td class="line x" title="83:153	Public officials throughout California have condemned a U.S. Senate vote Thursday to exclude illegal aliens from the 1990 census, saying the action will shortchange California in Congress and possibly deprive the state of millions of dollars of federal aid for medical emergency services and other programs for poor people." ></td>
	<td class="line x" title="84:153	TOPIC : illegal alien HOLDER : U.S. Senate OPINION REGION: vote/NN Thursday/NNP to/TO exclude/VB illegal/JJ aliens/NNS from/IN the/DT 1990/CD census,/NN SENTIMENT_POLARITY: negative For that reason and others, the Constitutional Convention unanimously rejected term limits and the First Congress soundly defeated two subsequent term-limit proposals." ></td>
	<td class="line x" title="85:153	TOPIC : term limit HOLDER : First Congress OPINION REGION: soundly/RB defeated/VBD two/CD subsequent/JJ term-limit/JJ proposals./NN SENTIMENT_POLARITY: negative 3 Experiments The first experiment examines the two word sentiment classifier models and the second the three sentence sentiment classifier models." ></td>
	<td class="line x" title="86:153	3.1 Word Sentiment Classifier For test material, we asked three humans to classify data." ></td>
	<td class="line x" title="87:153	We started with a basic English word list for foreign students preparing for the TOEFL test and intersected it with an adjective list containing 19748 English adjectives and a verb list of 8011 verbs to obtain common adjectives and verbs." ></td>
	<td class="line x" title="88:153	From this we randomly selected 462 adjectives and 502 verbs for human classification." ></td>
	<td class="line x" title="89:153	Human1 and human2 each classified 462 adjectives, and human2 and human3 502 verbs." ></td>
	<td class="line x" title="90:153	The classification task is defined as assigning each word to one of three categories: positive, negative, and neutral." ></td>
	<td class="line x" title="91:153	3.1.1 HumanHuman Agreement Adjectives Verbs Human1 : Human2 Human1 : Human3 Strict 76.19% 62.35% Lenient 88.96% 85.06% Table 4: Inter-human classification agreement." ></td>
	<td class="line x" title="92:153	Table 4 shows inter-human agreement." ></td>
	<td class="line x" title="93:153	The strict measure is defined over all three categories, whereas the lenient measure is taken over only two categories, where positive and neutral have been merged, should we choose to focus only on differentiating words of negative sentiment." ></td>
	<td class="line x" title="94:153	3.1.2 HumanMachine Agreement Table 5 shows results, using Equation (2) of Section 2.1.1, compared against a baseline that randomly assigns a sentiment category to each word (averaged over 10 iterations)." ></td>
	<td class="line x" title="95:153	The system achieves lower agreement than humans but higher than the random process." ></td>
	<td class="line x" title="96:153	Of the test data, the algorithm classified 93.07% of adjectives and 83.27% of verbs as either positive and negative." ></td>
	<td class="line x" title="97:153	The remainder of adjectives and verbs failed to be classified, since they did not overlap with the synonym set of adjectives and verbs." ></td>
	<td class="line x" title="98:153	In Table 5, the seed list included just a few manually selected seed words (23 positive and 21 negative verbs and 15 and 19 adjectives, repectively)." ></td>
	<td class="line x" title="99:153	We decided to investigate the effect of more seed words." ></td>
	<td class="line x" title="100:153	After collecting the annotated data, we added half of it (231 adjectives and 251 verbs) to the training set, retaining the other half for the test." ></td>
	<td class="line x" title="101:153	As Table 6 shows, agreement of both adjectives and verbs with humans improves." ></td>
	<td class="line x" title="102:153	Recall is also improved." ></td>
	<td class="line x" title="103:153	Adjective (Train: 231 Test : 231) Verb (Train: 251 Test : 251) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall 75.66% 77.88% 97.84% 81.20% 79.06% 93.23% Table 6: Results including manual data." ></td>
	<td class="line x" title="104:153	3.2 Sentence Sentiment Classifier 3.2.1 Data 100 sentences were selected from the DUC 2001 corpus with the topics illegal alien, term limits, gun control, and NAFTA." ></td>
	<td class="line x" title="105:153	Two humans annotated the 100 sentences with three categories (positive, negative, and N/A)." ></td>
	<td class="line x" title="106:153	To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988)." ></td>
	<td class="line x" title="107:153	The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable." ></td>
	<td class="line x" title="108:153	3.2.2 Test on Human Annotated Data We experimented on Section 2.2.3s 3 models of sentiment classifiers, using the 4 different window definitions and 4 variations of word-level classifiers (the two word sentiment equations introduced in Section 2.1.1, first with and then without normalization, to compare performance)." ></td>
	<td class="line x" title="109:153	Since Model 0 considers not probabilities of words but only their polarities, the two wordlevel classifier equations yield the same results." ></td>
	<td class="line x" title="110:153	Consequently, Model 0 has 8 combinations and Models 1 and 2 have 16 each." ></td>
	<td class="line x" title="111:153	To test the identification of opinion Holder, we first ran models with holders that were annotated by humans then ran the same models with the automatic holder finding strategies." ></td>
	<td class="line x" title="112:153	The results appear in Figures 2 and 3." ></td>
	<td class="line x" title="113:153	The models are numbered as follows: m0 through m4 represent 4 sentence classifier models, Table 5." ></td>
	<td class="line x" title="114:153	Agreement between humans and system." ></td>
	<td class="line x" title="115:153	Adjective (test: 231 adjectives) Verb (test : 251 verbs) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall Random selection (average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% p1/p2 and p3/p4 represent the word classifier models in Equation (2) and Equation (3) with normalization and without normalization respectively." ></td>
	<td class="line x" title="116:153	0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu r acy Window1 Window2 Window3 Window4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu r a c y Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machine Figure 2: Results with manually annotated Holder." ></td>
	<td class="line x" title="117:153	0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 accu r acy Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 accu r acy Window 1 Window 2 Window 3 Window 4 Human 1 : Machine Human 2 : Machi ne Figure 3: Results with automatic Holder detection." ></td>
	<td class="line x" title="118:153	Correctness of an opinion is determined when the system finds both a correct holder and the appropriate sentiment within the sentence." ></td>
	<td class="line x" title="119:153	Since human1 classified 33 sentences positive and 33 negative, random classification gives 33 out of 66 sentences." ></td>
	<td class="line x" title="120:153	Similarly, since human2 classified 29 positive and 34 negative, random classification gives 34 out of 63 when the system blindly marks all sentences as negative and 29 out of 63 when it marks all as positive." ></td>
	<td class="line x" title="121:153	The systems best model performed at 81% accuracy with the manually provided holder and at 67% accuracy with automatic holder detection." ></td>
	<td class="line x" title="122:153	3.3 Problems 3.3.1 Word Sentiment Classification As mentioned, some words have both strong positive and negative sentiment." ></td>
	<td class="line x" title="123:153	For these words, it is difficult to pick one sentiment category without considering context." ></td>
	<td class="line x" title="124:153	Second, a unigram model is not sufficient: common words without much sentiment alone can combine to produce reliable sentiment." ></td>
	<td class="line x" title="125:153	For example, in Term limits really hit at democracy, says Prof. Fenno, the common and multi-meaning word hit was used to express a negative point of view about term limits." ></td>
	<td class="line x" title="126:153	If such combinations occur adjacently, we can use bigrams or trigrams in the seed word list." ></td>
	<td class="line x" title="127:153	When they occur at a distance, however, it is more difficult to identify the sentiment correctly, especially if one of the words falls outside the sentiment region." ></td>
	<td class="line x" title="128:153	3.3.2 Sentence Sentiment Classification Even in a single sentence, a holder might express two different opinions." ></td>
	<td class="line x" title="129:153	Our system only detects the closest one." ></td>
	<td class="line x" title="130:153	Another difficult problem is that the models cannot infer sentiments from facts in a sentence." ></td>
	<td class="line x" title="131:153	She thinks term limits will give women more opportunities in politics expresses a positive opinion about term limits but the absence of adjective, verb, and noun sentiment-words prevents a classification." ></td>
	<td class="line x" title="132:153	Although relatively easy task for people, detecting an opinion holder is not simple either." ></td>
	<td class="line x" title="133:153	As a result, our system sometimes picks a wrong holder when there are multiple plausible opinion holder candidates present." ></td>
	<td class="line x" title="134:153	Employing a parser to delimit opinion regions and more accurately associate them with potential holders should help." ></td>
	<td class="line x" title="135:153	3.4 Discussion Which combination of models is best?" ></td>
	<td class="line x" title="136:153	The best overall performance is provided by Model 0." ></td>
	<td class="line x" title="137:153	Apparently, the mere presence of negative words is more important than sentiment strength." ></td>
	<td class="line x" title="138:153	For manually tagged holder and topic, Model 0 has the highest single performance, though Model 1 averages best." ></td>
	<td class="line x" title="139:153	Which is better, a sentence or a region?" ></td>
	<td class="line x" title="140:153	With manually identified topic and holder, the region window4 (from Holder to sentence end) performs better than other regions." ></td>
	<td class="line x" title="141:153	How do scores differ from manual to automatic holder identification?" ></td>
	<td class="line x" title="142:153	Table 7 compares the average results with automatic holder identification to manually annotated holders in 40 different models." ></td>
	<td class="line x" title="143:153	Around 7 more sentences (around 11%) were misclassified by the automatic detection method." ></td>
	<td class="line x" title="144:153	positive negative total Human1 5.394 1.667 7.060 Human2 4.984 1.714 6.698 Table 7: Average difference between manual and automatic holder detection." ></td>
	<td class="line x" title="145:153	How does adding the neutral sentiment as a separate category affect the score?" ></td>
	<td class="line x" title="146:153	It is very confusing even for humans to distinguish between a neutral opinion and nonopinion bearing sentences." ></td>
	<td class="line x" title="147:153	In previous research, we built a sentence subjectivity classifier." ></td>
	<td class="line x" title="148:153	Unfortunately, in most cases it classifies neutral and weak sentiment sentences as non-opinion bearing sentences." ></td>
	<td class="line x" title="149:153	4 Conclusion Sentiment recognition is a challenging and difficult part of understanding opinions." ></td>
	<td class="line x" title="150:153	We plan to extend our work to more difficult cases such as sentences with weak-opinion-bearing words or sentences with multiple opinions about a topic." ></td>
	<td class="line x" title="151:153	To improve identification of the Holder, we plan to use a parser to associate regions more reliably with holders." ></td>
	<td class="line x" title="152:153	We plan to explore other learning techniques, such as decision lists or SVMs." ></td>
	<td class="line x" title="153:153	Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1034
The Sentimental Factor: Improving Review Classification Via Human-Provided Information
Beineke, Philip;Hastie, Trevor;Vaithyanathan, Shivakumar;"></td>
	<td class="line x" title="1:179	The Sentimental Factor: Improving Review Classification via Human-Provided Information Philip Beineke and Trevor Hastie Dept. of Statistics Stanford University Stanford, CA 94305 Shivakumar Vaithyanathan IBM Almaden Research Center 650 Harry Rd. San Jose, CA 95120-6099 Abstract Sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion (favorable or unfavorable)." ></td>
	<td class="line x" title="2:179	In approaching this problem, a model builder often has three sources of information available: a small collection of labeled documents, a large collection of unlabeled documents, and human understanding of language." ></td>
	<td class="line x" title="3:179	Ideally, a learning method will utilize all three sources." ></td>
	<td class="line x" title="4:179	To accomplish this goal, we generalize an existing procedure that uses the latter two." ></td>
	<td class="line x" title="5:179	We extend this procedure by re-interpreting it as a Naive Bayes model for document sentiment." ></td>
	<td class="line x" title="6:179	Viewed as such, it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment." ></td>
	<td class="line x" title="7:179	This perspective allows us to improve upon previous methods, primarily through two strategies: incorporating additional derived features into the model and, where possible, using labeled data to estimate their relative influence." ></td>
	<td class="line x" title="8:179	1 Introduction Text documents are available in ever-increasing numbers, making automated techniques for information extraction increasingly useful." ></td>
	<td class="line x" title="9:179	Traditionally, most research effort has been directed towards objective information, such as classification according to topic; however, interest is growing in producing information about the opinions that a document contains; for instance, Morinaga et al.(2002)." ></td>
	<td class="line x" title="11:179	In March, 2004, the American Association for Artificial Intelligence held a symposium in this area, entitled Exploring Affect and Attitude in Text. One task in opinion extraction is to label a review document d according to its prevailing sentiment s 2 f 1; 1g (unfavorable or favorable)." ></td>
	<td class="line x" title="12:179	Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents, e.g. Pang et al.(2002), Dave et al.(2003)." ></td>
	<td class="line x" title="15:179	By learning models from labeled data, one can apply familiar, powerful techniques directly; however, in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately." ></td>
	<td class="line oc" title="16:179	A contrasting approach (Turney, 2002) relies only upon documents whose labels are unknown." ></td>
	<td class="line o" title="17:179	This makes it possible to use a large underlying corpus  in this case, the entire Internet as seen through the AltaVista search engine." ></td>
	<td class="line o" title="18:179	As a result, estimates for model parameters are subject to a relatively small amount of random variation." ></td>
	<td class="line n" title="19:179	The corresponding drawback to such an approach is that its predictions are not validated on actual documents." ></td>
	<td class="line x" title="20:179	In machine learning, it has often been effective to use labeled and unlabeled examples in tandem, e.g. Nigam et al.(2000)." ></td>
	<td class="line o" title="22:179	Turneys model introduces the further consideration of incorporating human-provided knowledge about language." ></td>
	<td class="line x" title="23:179	In this paper we build models that utilize all three sources: labeled documents, unlabeled documents, and human-provided information." ></td>
	<td class="line o" title="24:179	The basic concept behind Turneys model is quite simple." ></td>
	<td class="line x" title="25:179	The sentiment orientation (Hatzivassiloglou and McKeown, 1997) of a pair of words is taken to be known." ></td>
	<td class="line x" title="26:179	These words serve as anchors for positive and negative sentiment." ></td>
	<td class="line x" title="27:179	Words that co-occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment." ></td>
	<td class="line x" title="28:179	As a result, information about a pair of words is generalized to many words, and then to documents." ></td>
	<td class="line o" title="29:179	In the following section, we relate this model with Naive Bayes classification, showing that Turneys classifier is a pseudo-supervised approach: it effectively generates a new corpus of labeled documents, upon which it fits a Naive Bayes classifier." ></td>
	<td class="line x" title="30:179	This insight allows the procedure to be represented as a probability model that is linear on the logistic scale, which in turn suggests generalizations that are developed in subsequent sections." ></td>
	<td class="line x" title="31:179	2 A Logistic Model for Sentiment 2.1 Turneys Sentiment Classifier In Turneys model, the sentiment orientation of word w is estimated as follows." ></td>
	<td class="line x" title="32:179	^ (w) = log N(w;excellent)=NexcellentN (w;poor)=Npoor (1) Here, Na is the total number of sites on the Internet that contain an occurrence of a  a feature that can be a word type or a phrase." ></td>
	<td class="line x" title="33:179	N(w;a) is the number of sites in which features w and a appear near each other, i.e. in the same passage of text, within a span of ten words." ></td>
	<td class="line x" title="34:179	Both numbers are obtained from the hit count that results from a query of the AltaVista search engine." ></td>
	<td class="line x" title="35:179	The rationale for this estimate is that words that express similar sentiment often co-occur, while words that express conflicting sentiment cooccur more rarely." ></td>
	<td class="line x" title="36:179	Thus, a word that co-occurs more frequently with excellent than poor is estimated to have a positive sentiment orientation." ></td>
	<td class="line x" title="37:179	To extrapolate from words to documents, the estimated sentiment ^s 2 f 1; 1g of a review document d is the sign of the average sentiment orientation of its constituent features.1 To represent this estimate formally, we introduce the following notation: W is a dictionary of features: (w1;::: ;wp)." ></td>
	<td class="line x" title="38:179	Each features respective sentiment orientation is represented as an entry in the vector ^ of length p: ^ j = ^ (wj) (2) Given a collection of n review documents, the i-th each di is also represented as a vector of length p, with dij equal to the number of times that feature wj occurs in di." ></td>
	<td class="line x" title="39:179	The length of a document is its total number of features, jdij = Ppj=1 dij." ></td>
	<td class="line o" title="40:179	Turneys classifier for the i-th documents sentiment si can now be written: ^si = sign Pp j=1 ^ jdij jdij !" ></td>
	<td class="line o" title="41:179	(3) Using a carefully chosen collection of features, this classifier produces correct results on 65.8% of a collection of 120 movie reviews, where 60 are labeled positive and 60 negative." ></td>
	<td class="line n" title="42:179	Although this is not a particularly encouraging result, movie reviews tend to be a difficult domain." ></td>
	<td class="line oc" title="43:179	Accuracy on sentiment classification in other domains exceeds 80% (Turney, 2002)." ></td>
	<td class="line x" title="44:179	1Note that not all words or phrases need to be considered as features." ></td>
	<td class="line oc" title="45:179	In Turney (2002), features are selected according to part-of-speech labels." ></td>
	<td class="line x" title="46:179	2.2 Naive Bayes Classification Bayes Theorem provides a convenient framework for predicting a binary response s 2 f 1; 1g from a feature vector x: Pr(s = 1jx) = Pr(xjs = 1) 1P k2f 1;1g Pr(xjs = k) k (4) For a labeled sample of data (xi;si);i = 1;:::;n, a classs marginal probability k can be estimated trivially as the proportion of training samples belonging to the class." ></td>
	<td class="line x" title="47:179	Thus the critical aspect of classification by Bayes Theorem is to estimate the conditional distribution of x given s. Naive Bayes simplifies this problem by making a naive assumption: within a class, the different feature values are taken to be independent of one another." ></td>
	<td class="line x" title="48:179	Pr(xjs) = Y j Pr(xjjs) (5) As a result, the estimation problem is reduced to univariate distributions." ></td>
	<td class="line x" title="49:179	Naive Bayes for a Multinomial Distribution We consider a bag of words model for a document that belongs to class k, where features are assumed to result from a sequence of jdij independent multinomial draws with outcome probability vector qk = (qk1;::: ;qkp)." ></td>
	<td class="line x" title="50:179	Given a collection of documents with labels, (di;si);i = 1;::: ;n, a natural estimate for qkj is the fraction of all features in documents of class k that equal wj: ^qkj = P i:si=k dijP i:si=k jdij (6) In the two-class case, the logit transformation provides a revealing representation of the class posterior probabilities of the Naive Bayes model." ></td>
	<td class="line x" title="51:179	dlogit(sjd), log cPr(s = 1jd)c Pr(s = 1jd) (7) = log ^ 1^ 1 + pX j=1 dj log ^q1j^q 1j (8) = ^ 0 + pX j=1 dj ^ j (9) where ^ 0 = log ^ 1^ 1 (10) ^ j = log ^q1j^q 1j (11) Observe that the estimate for the logit in Equation 9 has a simple structure: it is a linear function of d. Models that take this form are commonplace in classification." ></td>
	<td class="line o" title="52:179	2.3 Turneys Classifier as Naive Bayes Although Naive Bayes classification requires a labeled corpus of documents, we show in this section that Turneys approach corresponds to a Naive Bayes model." ></td>
	<td class="line x" title="53:179	The necessary documents and their corresponding labels are built from the spans of text that surround the anchor words excellent and poor." ></td>
	<td class="line x" title="54:179	More formally, a labeled corpus may be produced by the following procedure: 1." ></td>
	<td class="line x" title="55:179	For a particular anchor ak, locate all of the sites on the Internet where it occurs." ></td>
	<td class="line x" title="56:179	2." ></td>
	<td class="line x" title="57:179	From all of the pages within a site, gather the features that occur within ten words of an occurrence of ak, with any particular feature included at most once." ></td>
	<td class="line x" title="58:179	This list comprises a new document, representing that site.2 3." ></td>
	<td class="line x" title="59:179	Label this document +1 if ak = excellent, -1 if ak = poor." ></td>
	<td class="line x" title="60:179	When a Naive Bayes model is fit to the corpus described above, it results in a vector ^ of length p, consisting of coefficient estimates for all features." ></td>
	<td class="line o" title="61:179	In Propositions 1 and 2 below, we show that Turneys estimates of sentiment orientation ^ are closely related to ^, and that both estimates produce identical classifiers." ></td>
	<td class="line o" title="62:179	Proposition 1 ^ = C1^ (12) where C1 = Nexc:= P i:si=1 jdij Npoor=Pi:si= 1 jdij (13) Proof: Because a feature is restricted to at most one occurrence in a document, X i:si=k dij = N(w;ak) (14) Then from Equations 6 and 11: ^ j = log ^q1j^q 1j (15) = log N(w;exc:)= P i:si=1 jdij N(w;poor)=Pi:si= 1 jdij (16) = C1^ j (17) 2 2If both anchors occur on a site, then there will actually be two documents, one for each sentiment Proposition 2 Turneys classifier is identical to a Naive Bayes classifier fit on this corpus, with 1 = 1 = 0:5." ></td>
	<td class="line x" title="63:179	Proof: A Naive Bayes classifier typically assigns an observation to its most probable class." ></td>
	<td class="line x" title="64:179	This is equivalent to classifying according to the sign of the estimated logit." ></td>
	<td class="line x" title="65:179	So for any document, we must show that both the logit estimate and the average sentiment orientation are identical in sign." ></td>
	<td class="line x" title="66:179	When 1 = 0:5, 0 = 0." ></td>
	<td class="line o" title="67:179	Thus the estimated logit is dlogit(sjd) = pX j=1 ^ jdj (18) = C1 pX j=1 ^ jdj (19) This is a positive multiple of Turneys classifier (Equation 3), so they clearly match in sign." ></td>
	<td class="line o" title="68:179	2 3 A More Versatile Model 3.1 Desired Extensions By understanding Turneys model within a Naive Bayes framework, we are able to interpret its output as a probability model for document classes." ></td>
	<td class="line x" title="69:179	In the presence of labeled examples, this insight also makes it possible to estimate the intercept term 0." ></td>
	<td class="line x" title="70:179	Further, we are able to view this model as a member of a broad class: linear estimates for the logit." ></td>
	<td class="line x" title="71:179	This understanding facilitates further extensions, in particular, utilizing the following: 1." ></td>
	<td class="line x" title="72:179	Labeled documents 2." ></td>
	<td class="line x" title="73:179	More anchor words The reason for using labeled documents is straightforward; labels offer validation for any chosen model." ></td>
	<td class="line x" title="74:179	Using additional anchors is desirable in part because it is inexpensive to produce lists of words that are believed to reflect positive sentiment, perhaps by reference to a thesaurus." ></td>
	<td class="line x" title="75:179	In addition, a single anchor may be at once too general and too specific." ></td>
	<td class="line x" title="76:179	An anchor may be too general in the sense that many common words have multiple meanings, and not all of them reflect a chosen sentiment orientation." ></td>
	<td class="line x" title="77:179	For example, poor can refer to an objective economic state that does not necessarily express negative sentiment." ></td>
	<td class="line x" title="78:179	As a result, a word such as income appears 4.18 times as frequently with poor as excellent, even though it does not convey negative sentiment." ></td>
	<td class="line x" title="79:179	Similarly, excellent has a technical meaning in antiquity trading, which causes it to appear 3.34 times as frequently with furniture." ></td>
	<td class="line x" title="80:179	An anchor may also be too specific, in the sense that there are a variety of different ways to express sentiment, and a single anchor may not capture them all." ></td>
	<td class="line x" title="81:179	So a word like pretentious carries a strong negative sentiment but co-occurs only slightly more frequently (1.23 times) with excellent than poor." ></td>
	<td class="line x" title="82:179	Likewise, fascination generally reflects a positive sentiment, yet it appears slightly more frequently (1.06 times) with poor than excellent." ></td>
	<td class="line x" title="83:179	3.2 Other Sources of Unlabeled Data The use of additional anchors has a drawback in terms of being resource-intensive." ></td>
	<td class="line x" title="84:179	A feature set may contain many words and phrases, and each of them requires a separate AltaVista query for every chosen anchor word." ></td>
	<td class="line x" title="85:179	In the case of 30,000 features and ten queries per minute, downloads for a single anchor word require over two days of data collection." ></td>
	<td class="line x" title="86:179	An alternative approach is to access a large collection of documents directly." ></td>
	<td class="line x" title="87:179	Then all cooccurrences can be counted in a single pass." ></td>
	<td class="line x" title="88:179	Although this approach dramatically reduces the amount of data available, it does offer several advantages." ></td>
	<td class="line x" title="89:179	Increased Query Options Search engine queries of the form phrase NEAR anchor may not produce all of the desired cooccurrence counts." ></td>
	<td class="line x" title="90:179	For instance, one may wish to run queries that use stemmed words, hyphenated words, or punctuation marks." ></td>
	<td class="line x" title="91:179	One may also wish to modify the definition of NEAR, or to count individual co-occurrences, rather than counting sites that contain at least one co-occurrence." ></td>
	<td class="line x" title="92:179	Topic Matching Across the Internet as a whole, features may not exhibit the same correlation structure as they do within a specific domain." ></td>
	<td class="line x" title="93:179	By restricting attention to documents within a domain, one may hope to avoid cooccurrences that are primarily relevant to other subjects." ></td>
	<td class="line x" title="94:179	Reproducibility On a fixed corpus, counts of word occurrences produce consistent results." ></td>
	<td class="line x" title="95:179	Due to the dynamic nature of the Internet, numbers may fluctuate." ></td>
	<td class="line x" title="96:179	3.3 Co-Occurrences and Derived Features The Naive Bayes coefficient estimate ^ j may itself be interpreted as an intercept term plus a linear combination of features of the form log N(wj;ak)." ></td>
	<td class="line x" title="97:179	Num." ></td>
	<td class="line x" title="98:179	of Labeled Occurrences Correlation 1 5 0.022 6 10 0.082 11 25 0.113 26 50 0.183 51 75 0.283 76 100 0.316 Figure 1: Correlation between Supervised and Unsupervised Coefficient Estimates ^ j = log N(j;exc:)= P i:si=1 jdij N(j;pr:)=Pi:si= 1 jdij (20) = log C1 + log N(j;exc:) log N(j;pr:) (21) We generalize this estimate as follows: for a collection of K different anchor words, we consider a general linear combination of logged co-occurrence counts." ></td>
	<td class="line x" title="99:179	^ j = KX k=1 k log N(wj;ak) (22) In the special case of a Naive Bayes model, k = 1 when the k-th anchor word ak conveys positive sentiment, 1 when it conveys negative sentiment." ></td>
	<td class="line x" title="100:179	Replacing the logit estimate in Equation 9 with an estimate of this form, the model becomes: dlogit(sjd) = ^ 0 + pX j=1 dj ^ j (23) = ^ 0 + pX j=1 KX k=1 dj k log N(wj;ak) (24) = 0 + KX k=1 k pX j=1 dj log N(wj;ak) (25) (26) This model has only K + 1 parameters: 0; 1;::: ; K. These can be learned straightforwardly from labeled documents by a method such as logistic regression." ></td>
	<td class="line x" title="101:179	Observe that a document receives a score for each anchor word Ppj=1 dj log N(wj;ak)." ></td>
	<td class="line x" title="102:179	Effectively, the predictor variables in this model are no longer counts of the original features dj." ></td>
	<td class="line x" title="103:179	Rather, they are 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 3 2 1 0 1 2 3 4 Traditional Naive Bayes Coefs." ></td>
	<td class="line o" title="104:179	Turney Naive Bayes Coefs." ></td>
	<td class="line x" title="105:179	Unsupervised vs. Supervised Coefficients Figure 2: Unsupervised versus Supervised Coefficient Estimates inner products between the entire feature vector d and the logged co-occurence vector N(w;ak)." ></td>
	<td class="line x" title="106:179	In this respect, the vector of logged co-occurrences is used to produce derived feature." ></td>
	<td class="line x" title="107:179	4 Data Analysis 4.1 Accuracy of Unsupervised Coefficients By means of a Perl script that uses the Lynx browser, Version 2.8.3rel.1, we download AltaVista hit counts for queries of the form target NEAR anchor. The initial list of targets consists of 44,321 word types extracted from the Pang corpus of 1400 labeled movie reviews." ></td>
	<td class="line x" title="108:179	After preprocessing, this number is reduced to 28,629.3 In Figure 1, we compare estimates produced by two Naive Bayes procedures." ></td>
	<td class="line o" title="109:179	For each feature wj, we estimate j by using Turneys procedure, and by fitting a traditional Naive Bayes model to the labeled documents." ></td>
	<td class="line x" title="110:179	The traditional estimates are smoothed by assuming a Beta prior distribution that is equivalent to having four previous observations of wj in documents of each class." ></td>
	<td class="line x" title="111:179	^q1j ^q 1j = C2 4 +Pi:si=1 dij 4 +Pi:si= 1 dij (27) where C2 = 4p + P i:si=1 jdij 4p +Pi:si= 1 jdij (28) Here, dij is used to indicate feature presence: dij = 1 if w j appears in di 0 otherwise (29) 3We eliminate extremely rare words by requiring each target to co-occur at least once with each anchor." ></td>
	<td class="line x" title="112:179	In addition, certain types, such as words containing hyphens, apostrophes, or other punctuation marks, do not appear to produce valid counts, so they are discarded." ></td>
	<td class="line x" title="113:179	Positive Negative best awful brilliant bad excellent pathetic spectacular poor wonderful worst Figure 3: Selected Anchor Words We choose this fitting procedure among several candidates because it performs well in classifying test documents." ></td>
	<td class="line x" title="114:179	In Figure 1, each entry in the right-hand column is the observed correlation between these two estimates over a subset of features." ></td>
	<td class="line x" title="115:179	For features that occur in five documents or fewer, the correlation is very weak (0.022)." ></td>
	<td class="line x" title="116:179	This is not surprising, as it is difficult to estimate a coefficient from such a small number of labeled examples." ></td>
	<td class="line x" title="117:179	Correlations are stronger for more common features, but never strong." ></td>
	<td class="line x" title="118:179	As a baseline for comparison, Naive Bayes coefficients can be estimated using a subset of their labeled occurrences." ></td>
	<td class="line x" title="119:179	With two independent sets of 51-75 occurrences, Naive Bayes coefficient estimates had a correlation of 0.475." ></td>
	<td class="line x" title="120:179	Figure 2 is a scatterplot of the same coefficient estimates for word types that appear in 51 to 100 documents." ></td>
	<td class="line x" title="121:179	The great majority of features do not have large coefficients, but even for the ones that do, there is not a tight correlation." ></td>
	<td class="line x" title="122:179	4.2 Additional Anchors We wish to learn how our model performance depends on the choice and number of anchor words." ></td>
	<td class="line x" title="123:179	Selecting from WordNet synonym lists (Fellbaum, 1998), we choose five positive anchor words and five negative (Figure 3)." ></td>
	<td class="line x" title="124:179	This produces a total of 25 different possible pairs for use in producing coefficient estimates." ></td>
	<td class="line x" title="125:179	Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data." ></td>
	<td class="line x" title="126:179	Coefficients ^ j are estimated as described in Equation 22." ></td>
	<td class="line x" title="127:179	Several different experimental conditions are applied." ></td>
	<td class="line x" title="128:179	The methods labeled Count use the original un-normalized coefficients, while those labeled Norm. have been normalized so that the number of co-occurrences with each anchor have identical variance." ></td>
	<td class="line x" title="129:179	Results are shown when rare words (with three or fewer occurrences in the labeled corpus) are included and omitted." ></td>
	<td class="line x" title="130:179	The methods pair and 10 describe whether all ten anchor coefficients are used at once, or just the ones that correspond to a single pair of Method Feat." ></td>
	<td class="line x" title="131:179	Misclass." ></td>
	<td class="line x" title="132:179	St.Dev Count Pair >3 39.6% 2.9% Norm." ></td>
	<td class="line x" title="133:179	Pair >3 38.4% 3.0% Count Pair all 37.4% 3.1% Norm." ></td>
	<td class="line x" title="134:179	Pair all 37.3% 3.0% Count 10 > 3 36.4%  Norm." ></td>
	<td class="line x" title="135:179	10 > 3 35.4%  Count 10 all 34.6%  Norm." ></td>
	<td class="line x" title="136:179	10 all 34.1%  Figure 4: Classification Error Rates for Different Unsupervised Approaches anchor words." ></td>
	<td class="line x" title="137:179	For anchor pairs, the mean error across all 25 pairs is reported, along with its standard deviation." ></td>
	<td class="line x" title="138:179	Patterns are consistent across the different conditions." ></td>
	<td class="line x" title="139:179	A relatively large improvement comes from using all ten anchor words." ></td>
	<td class="line x" title="140:179	Smaller benefits arise from including rare words and from normalizing model coefficients." ></td>
	<td class="line x" title="141:179	Models that use the original pair of anchor words, excellent and poor, perform slightly better than the average pair." ></td>
	<td class="line x" title="142:179	Whereas mean performance ranges from 37.3% to 39.6%, misclassification rates for this pair of anchors ranges from 37.4% to 38.1%." ></td>
	<td class="line x" title="143:179	4.3 A Smaller Unlabeled Corpus As described in Section 3.2, there are several reasons to explore the use of a smaller unlabeled corpus, rather than the entire Internet." ></td>
	<td class="line x" title="144:179	In our experiments, we use additional movie reviews as our documents." ></td>
	<td class="line x" title="145:179	For this domain, Pang makes available 27,886 reviews.4 Because this corpus offers dramatically fewer instances of anchor words, we modify our estimation procedure." ></td>
	<td class="line x" title="146:179	Rather than discarding words that rarely co-occur with anchors, we use the same feature set as before and regularize estimates by the same procedure used in the Naive Bayes procedure described earlier." ></td>
	<td class="line x" title="147:179	Using all features, and ten anchor words with normalized scores, test error is 35.0%." ></td>
	<td class="line x" title="148:179	This suggests that comparable results can be attained while referring to a considerably smaller unlabeled corpus." ></td>
	<td class="line x" title="149:179	Rather than requiring several days of downloads, the count of nearby co-occurrences was completed in under ten minutes." ></td>
	<td class="line x" title="150:179	Because this procedure enables fast access to counts, we explore the possibility of dramatically enlarging our collection of anchor words." ></td>
	<td class="line x" title="151:179	We col4This corpus is freely available on the following website: http://www.cs.cornell.edu/people/pabo/movie-review-data/." ></td>
	<td class="line x" title="152:179	100 200 300 400 500 600 0.30 0.32 0.34 0.36 0.38 0.40 Num." ></td>
	<td class="line x" title="153:179	of Labeled Documents Classif." ></td>
	<td class="line x" title="154:179	Error Misclassification versus Sample Size Figure 5: Misclassification with Labeled Documents." ></td>
	<td class="line x" title="155:179	The solid curve represents a latent factor model with estimated coefficients." ></td>
	<td class="line x" title="156:179	The dashed curve uses a Naive Bayes classifier." ></td>
	<td class="line x" title="157:179	The two horizontal lines represent unsupervised estimates; the upper one is for the original unsupervised classifier, and the lower is for the most successful unsupervised method." ></td>
	<td class="line x" title="158:179	lect data for the complete set of WordNet synonyms for the words good, best, bad, boring, and dreadful." ></td>
	<td class="line x" title="159:179	This yields a total of 83 anchor words, 35 positive and 48 negative." ></td>
	<td class="line x" title="160:179	When all of these anchors are used in conjunction, test error increases to 38.3%." ></td>
	<td class="line x" title="161:179	One possible difficulty in using this automated procedure is that some synonyms for a word do not carry the same sentiment orientation." ></td>
	<td class="line x" title="162:179	For instance, intense is listed as a synonym for bad, even though its presence in a movie review is a strongly positive indication.5 4.4 Methods with Supervision As demonstrated in Section 3.3, each anchor word ak is associated with a coefficient k. In unsupervised models, these coefficients are assumed to be known." ></td>
	<td class="line x" title="163:179	However, when labeled documents are available, it may be advantageous to estimate them." ></td>
	<td class="line x" title="164:179	Figure 5 compares the performance of a model with estimated coefficient vector, as opposed to unsupervised models and a traditional supervised approach." ></td>
	<td class="line x" title="165:179	When a moderate number of labeled documents are available, it offers a noticeable improvement." ></td>
	<td class="line x" title="166:179	The supervised method used for reference in this case is the Naive Bayes model that is described in section 4.1." ></td>
	<td class="line x" title="167:179	Naive Bayes classification is of particular interest here because it converges faster to its asymptotic optimum than do discriminative methods (Ng, A. Y. and Jordan, M. , 2002)." ></td>
	<td class="line x" title="168:179	Further, with 5In the labeled Pang corpus, intense appears in 38 positive reviews and only 6 negative ones." ></td>
	<td class="line x" title="169:179	a larger number of labeled documents, its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models (Pang et al. , 2002)." ></td>
	<td class="line x" title="170:179	The coefficient vector is estimated by regularized logistic regression." ></td>
	<td class="line x" title="171:179	This method has been used in other text classification problems, as in Zhang and Yang (2003)." ></td>
	<td class="line x" title="172:179	In our case, the regularization6 is introduced in order to enforce the beliefs that: 1 2, if a1, a2 synonyms (30) 1 2, if a1, a2 antonyms (31) For further information on regularized model fitting, see for instance, Hastie et al.(2001)." ></td>
	<td class="line x" title="174:179	5 Conclusion In business settings, there is growing interest in learning product reputations from the Internet." ></td>
	<td class="line x" title="175:179	For such problems, it is often difficult or expensive to obtain labeled data." ></td>
	<td class="line x" title="176:179	As a result, a change in modeling strategies is needed, towards approaches that require less supervision." ></td>
	<td class="line x" title="177:179	In this paper we provide a framework for allowing human-provided information to be combined with unlabeled documents and labeled documents." ></td>
	<td class="line x" title="178:179	We have found that this framework enables improvements over existing techniques, both in terms of the speed of model estimation and in classification accuracy." ></td>
	<td class="line x" title="179:179	As a result, we believe that this is a promising new approach to problems of practical importance." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-1035
A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based On Minimum Cuts
Pang, Bo;Lee, Lillian;"></td>
	<td class="line x" title="1:149	A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts Bo Pang and Lillian Lee Department of Computer Science Cornell University Ithaca, NY 14853-7501 {pabo,llee}@cs.cornell.edu Abstract Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as thumbs up or thumbs down." ></td>
	<td class="line x" title="2:149	To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document." ></td>
	<td class="line x" title="3:149	Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints." ></td>
	<td class="line x" title="4:149	1 Introduction The computational treatment of opinion, sentiment, and subjectivity has recently attracted a great deal of attention (see references), in part because of its potential applications." ></td>
	<td class="line x" title="5:149	For instance, informationextraction and question-answering systems could flag statements and queries regarding opinions rather than facts (Cardie et al. , 2003)." ></td>
	<td class="line x" title="6:149	Also, it has proven useful for companies, recommender systems, and editorial sites to create summaries of peoples experiences and opinions that consist of subjective expressions extracted from reviews (as is commonly done in movie ads) or even just a reviews polarity  positive (thumbs up) or negative (thumbs down)." ></td>
	<td class="line x" title="7:149	Document polarity classification poses a significant challenge to data-driven methods, resisting traditional text-categorization techniques (Pang, Lee, and Vaithyanathan, 2002)." ></td>
	<td class="line x" title="8:149	Previous approaches focused on selecting indicative lexical features (e.g. , the word good), classifying a document according to the number of such features that occur anywhere within it." ></td>
	<td class="line x" title="9:149	In contrast, we propose the following process: (1) label the sentences in the document as either subjective or objective, discarding the latter; and then (2) apply a standard machine-learning classifier to the resulting extract." ></td>
	<td class="line x" title="10:149	This can prevent the polarity classifier from considering irrelevant or even potentially misleading text: for example, although the sentence The protagonist tries to protect her good name contains the word good, it tells us nothing about the authors opinion and in fact could well be embedded in a negative movie review." ></td>
	<td class="line x" title="11:149	Also, as mentioned above, subjectivity extracts can be provided to users as a summary of the sentiment-oriented content of the document." ></td>
	<td class="line x" title="12:149	Our results show that the subjectivity extracts we create accurately represent the sentiment information of the originating documents in a much more compact form: depending on choice of downstream polarity classifier, we can achieve highly statistically significant improvement (from 82.8% to 86.4%) or maintain the same level of performance for the polarity classification task while retaining only 60% of the reviews words." ></td>
	<td class="line x" title="13:149	Also, we explore extraction methods based on a minimum cut formulation, which provides an efficient, intuitive, and effective means for integrating inter-sentencelevel contextual information with traditional bag-ofwords features." ></td>
	<td class="line x" title="14:149	2 Method 2.1 Architecture One can consider document-level polarity classification to be just a special (more difficult) case of text categorization with sentimentrather than topic-based categories." ></td>
	<td class="line x" title="15:149	Hence, standard machinelearning classification techniques, such as support vector machines (SVMs), can be applied to the entire documents themselves, as was done by Pang, Lee, and Vaithyanathan (2002)." ></td>
	<td class="line x" title="16:149	We refer to such classification techniques as default polarity classifiers." ></td>
	<td class="line x" title="17:149	However, as noted above, we may be able to improve polarity classification by removing objective sentences (such as plot summaries in a movie review)." ></td>
	<td class="line x" title="18:149	We therefore propose, as depicted in Figure 1, to first employ a subjectivity detector that determines whether each sentence is subjective or not: discarding the objective ones creates an extract that should better represent a reviews subjective content to a default polarity classifier." ></td>
	<td class="line x" title="19:149	s1s2 s3 s4 s_n +/s4s1 subjectivity detector yes no no yes nsentence reviewsubjectivesentence?msentence extract(m<=n) review?positive or negative default classifierpolarity subjectivity extraction Figure 1: Polarity classification via subjectivity detection." ></td>
	<td class="line x" title="20:149	To our knowledge, previous work has not integrated sentence-level subjectivity detection with document-level sentiment polarity." ></td>
	<td class="line x" title="21:149	Yu and Hatzivassiloglou (2003) provide methods for sentencelevel analysis and for determining whether a document is subjective or not, but do not combine these two types of algorithms or consider document polarity classification." ></td>
	<td class="line x" title="22:149	The motivation behind the singlesentence selection method of Beineke et al.(2004) is to reveal a documents sentiment polarity, but they do not evaluate the polarity-classification accuracy that results." ></td>
	<td class="line x" title="24:149	2.2 Context and Subjectivity Detection As with document-level polarity classification, we could perform subjectivity detection on individual sentences by applying a standard classification algorithm on each sentence in isolation." ></td>
	<td class="line x" title="25:149	However, modeling proximity relationships between sentences would enable us to leverage coherence: text spans occurring near each other (within discourse boundaries) may share the same subjectivity status, other things being equal (Wiebe, 1994)." ></td>
	<td class="line x" title="26:149	We would therefore like to supply our algorithms with pair-wise interaction information, e.g., to specify that two particular sentences should ideally receive the same subjectivity label but not state which label this should be." ></td>
	<td class="line x" title="27:149	Incorporating such information is somewhat unnatural for classifiers whose input consists simply of individual feature vectors, such as Naive Bayes or SVMs, precisely because such classifiers label each test item in isolation." ></td>
	<td class="line x" title="28:149	One could define synthetic features or feature vectors to attempt to overcome this obstacle." ></td>
	<td class="line x" title="29:149	However, we propose an alternative that avoids the need for such feature engineering: we use an efficient and intuitive graph-based formulation relying on finding minimum cuts." ></td>
	<td class="line x" title="30:149	Our approach is inspired by Blum and Chawla (2001), although they focused on similarity between items (the motivation being to combine labeled and unlabeled data), whereas we are concerned with physical proximity between the items to be classified; indeed, in computer vision, modeling proximity information via graph cuts has led to very effective classification (Boykov, Veksler, and Zabih, 1999)." ></td>
	<td class="line x" title="31:149	2.3 Cut-based classification Figure 2 shows a worked example of the concepts in this section." ></td>
	<td class="line x" title="32:149	Suppose we have n items x1,,xn to divide into two classes C1 and C2, and we have access to two types of information:  Individual scores indj(xi): non-negative estimates of each xis preference for being in Cj based on just the features of xi alone; and  Association scores assoc(xi,xk): non-negative estimates of how important it is that xi and xk be in the same class.1 We would like to maximize each items net happiness: its individual score for the class it is assigned to, minus its individual score for the other class." ></td>
	<td class="line x" title="33:149	But, we also want to penalize putting tightlyassociated items into different classes." ></td>
	<td class="line x" title="34:149	Thus, after some algebra, we arrive at the following optimization problem: assign the xis to C1 and C2 so as to minimize the partition cost summationdisplay xC1 ind2(x)+ summationdisplay xC2 ind1(x)+ summationdisplay xiC1, xkC2 assoc(xi,xk)." ></td>
	<td class="line x" title="35:149	The problem appears intractable, since there are 2n possible binary partitions of the xis. However, suppose we represent the situation in the following manner." ></td>
	<td class="line x" title="36:149	Build an undirected graph G with vertices {v1,,vn,s,t}; the last two are, respectively, the source and sink." ></td>
	<td class="line x" title="37:149	Add n edges (s,vi), each with weight ind1(xi), and n edges (vi,t), each with weight ind2(xi)." ></td>
	<td class="line x" title="38:149	Finally, add parenleftbign2parenrightbig edges (vi,vk), each with weight assoc(xi,xk)." ></td>
	<td class="line x" title="39:149	Then, cuts in G are defined as follows: Definition 1 A cut (S,T) of G is a partition of its nodes into sets S = {s}  Sprime and T = {t}  Tprime, where s negationslash Sprime,t negationslash Tprime." ></td>
	<td class="line x" title="40:149	Its cost cost(S,T) is the sum of the weights of all edges crossing from S to T. A minimum cut of G is one of minimum cost." ></td>
	<td class="line x" title="41:149	1Asymmetry is allowed, but we used symmetric scores." ></td>
	<td class="line x" title="42:149	[ ] s t Y M N 2ind (Y)[.2]1ind (Y)[.8] 2ind (M)[.5]1ind (M)[.5] [.1]assoc(Y,N) 2ind (N)[.9]1ind (N) assoc(M,N) assoc(Y,M) [.2] [1.0] [.1] C1 Individual Association Cost penalties penalties {Y,M}.2 + .5 + .1 .1 + .2 1.1 (none) .8 + .5 + .1 0 1.4 {Y,M,N} .2 + .5 + .9 0 1.6 {Y} .2 + .5 + .1 1.0 + .1 1.9 {N} .8 + .5 + .9 .1 + .2 2.5 {M} .8 + .5 + .1 1.0 + .2 2.6 {Y,N} .2 + .5 + .9 1.0 + .2 2.8 {M,N} .8 + .5 + .9 1.0 + .1 3.3 Figure 2: Graph for classifying three items." ></td>
	<td class="line x" title="43:149	Brackets enclose example values; here, the individual scores happen to be probabilities." ></td>
	<td class="line x" title="44:149	Based on individual scores alone, we would put Y (yes) in C1, N (no) in C2, and be undecided about M (maybe)." ></td>
	<td class="line x" title="45:149	But the association scores favor cuts that put Y and M in the same class, as shown in the table." ></td>
	<td class="line x" title="46:149	Thus, the minimum cut, indicated by the dashed line, places M together with Y in C1." ></td>
	<td class="line x" title="47:149	Observe that every cut corresponds to a partition of the items and has cost equal to the partition cost." ></td>
	<td class="line x" title="48:149	Thus, our optimization problem reduces to finding minimum cuts." ></td>
	<td class="line x" title="49:149	Practical advantages As we have noted, formulating our subjectivity-detection problem in terms of graphs allows us to model item-specific and pairwise information independently." ></td>
	<td class="line x" title="50:149	Note that this is a very flexible paradigm." ></td>
	<td class="line x" title="51:149	For instance, it is perfectly legitimate to use knowledge-rich algorithms employing deep linguistic knowledge about sentiment indicators to derive the individual scores." ></td>
	<td class="line x" title="52:149	And we could also simultaneously use knowledgelean methods to assign the association scores." ></td>
	<td class="line x" title="53:149	Interestingly, Yu and Hatzivassiloglou (2003) compared an individual-preference classifier against a relationship-based method, but didnt combine the two; the ability to coordinate such algorithms is precisely one of the strengths of our approach." ></td>
	<td class="line x" title="54:149	But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximum-flow algorithms with polynomial asymptotic running times  and near-linear running times in practice  to exactly compute the minimumcost cut(s), despite the apparent intractability of the optimization problem (Cormen, Leiserson, and Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2 In contrast, other graph-partitioning problems that have been previously used to formulate NLP classification problems3 are NP-complete (Hatzivassiloglou and McKeown, 1997; Agrawal et al. , 2003; Joachims, 2003)." ></td>
	<td class="line x" title="55:149	2Code available at http://www.avglab.com/andrew/soft.html." ></td>
	<td class="line x" title="56:149	3Graph-based approaches to general clustering problems are too numerous to mention here." ></td>
	<td class="line x" title="57:149	3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative, an appealing task for several reasons." ></td>
	<td class="line x" title="58:149	First, as mentioned in the introduction, providing polarity information about reviews is a useful service: witness the popularity of www.rottentomatoes.com." ></td>
	<td class="line oc" title="59:149	Second, movie reviews are apparently harder to classify than reviews of other products (Turney, 2002; Dave, Lawrence, and Pennock, 2003)." ></td>
	<td class="line x" title="60:149	Third, the correct label can be extracted automatically from rating information (e.g. , number of stars)." ></td>
	<td class="line x" title="61:149	Our data4 contains 1000 positive and 1000 negative reviews all written before 2002, with a cap of 20 reviews per author (312 authors total) per category." ></td>
	<td class="line x" title="62:149	We refer to this corpus as the polarity dataset." ></td>
	<td class="line x" title="63:149	Default polarity classifiers We tested support vector machines (SVMs) and Naive Bayes (NB)." ></td>
	<td class="line x" title="64:149	Following Pang et al.(2002), we use unigram-presence features: the ith coordinate of a feature vector is 1 if the corresponding unigram occurs in the input text, 0 otherwise." ></td>
	<td class="line x" title="66:149	(For SVMs, the feature vectors are length-normalized)." ></td>
	<td class="line x" title="67:149	Each default documentlevel polarity classifier is trained and tested on the extracts formed by applying one of the sentencelevel subjectivity detectors to reviews in the polarity dataset." ></td>
	<td class="line x" title="68:149	Subjectivity dataset To train our detectors, we need a collection of labeled sentences." ></td>
	<td class="line x" title="69:149	Riloff and Wiebe (2003) state that It is [very hard] to obtain collections of individual sentences that can be easily identified as subjective or objective; the polarity-dataset sentences, for example, have not 4Available at www.cs.cornell.edu/people/pabo/moviereview-data/ (review corpus version 2.0)." ></td>
	<td class="line x" title="70:149	been so annotated.5 Fortunately, we were able to mine the Web to create a large, automaticallylabeled sentence corpus6." ></td>
	<td class="line x" title="71:149	To gather subjective sentences (or phrases), we collected 5000 moviereview snippets (e.g. , bold, imaginative, and impossible to resist) from www.rottentomatoes.com." ></td>
	<td class="line x" title="72:149	To obtain (mostly) objective data, we took 5000 sentences from plot summaries available from the Internet Movie Database (www.imdb.com)." ></td>
	<td class="line x" title="73:149	We only selected sentences or snippets at least ten words long and drawn from reviews or plot summaries of movies released post-2001, which prevents overlap with the polarity dataset." ></td>
	<td class="line x" title="74:149	Subjectivity detectors As noted above, we can use our default polarity classifiers as basic sentencelevel subjectivity detectors (after retraining on the subjectivity dataset) to produce extracts of the original reviews." ></td>
	<td class="line x" title="75:149	We also create a family of cut-based subjectivity detectors; these take as input the set of sentences appearing in a single document and determine the subjectivity status of all the sentences simultaneously using per-item and pairwise relationship information." ></td>
	<td class="line x" title="76:149	Specifically, for a given document, we use the construction in Section 2.2 to build a graph wherein the source s and sink t correspond to the class of subjective and objective sentences, respectively, and each internal node vi corresponds to the documents ith sentence si." ></td>
	<td class="line x" title="77:149	We can set the individual scores ind1(si) to PrNBsub (si) and ind2(si) to 1  PrNBsub (si), as shown in Figure 3, where PrNBsub (s) denotes Naive Bayes estimate of the probability that sentence s is subjective; or, we can use the weights produced by the SVM classifier instead.7 If we set all the association scores to zero, then the minimum-cut classification of the sentences is the same as that of the basic subjectivity detector." ></td>
	<td class="line x" title="78:149	Alternatively, we incorporate the degree of proximity between pairs of sentences, controlled by three parameters." ></td>
	<td class="line x" title="79:149	The threshold T specifies the maximum distance two sentences can be separated by and still be considered proximal." ></td>
	<td class="line x" title="80:149	The 5We therefore could not directly evaluate sentenceclassification accuracy on the polarity dataset." ></td>
	<td class="line x" title="81:149	6Available at www.cs.cornell.edu/people/pabo/moviereview-data/, sentence corpus version 1.0." ></td>
	<td class="line x" title="82:149	7We converted SVM output di, which is a signed distance (negative=objective) from the separating hyperplane, to nonnegative numbers by ind1(si) def= braceleftBigg1 d i > 2; (2 + di)/4 2  di  2; 0 di < 2." ></td>
	<td class="line x" title="83:149	and ind2(si) = 1  ind1(si)." ></td>
	<td class="line x" title="84:149	Note that scaling is employed only for consistency; the algorithm itself does not require probabilities for individual scores." ></td>
	<td class="line x" title="85:149	non-increasing function f(d) specifies how the influence of proximal sentences decays with respect to distance d; in our experiments, we tried f(d) = 1, e1d, and 1/d2." ></td>
	<td class="line x" title="86:149	The constant c controls the relative influence of the association scores: a larger c makes the minimum-cut algorithm more loath to put proximal sentences in different classes." ></td>
	<td class="line x" title="87:149	With these in hand8, we set (for j > i) assoc(si,sj) def= braceleftBigf(j i) c if (j i)  T; 0 otherwise." ></td>
	<td class="line x" title="88:149	4 Experimental Results Below, we report average accuracies computed by ten-fold cross-validation over the polarity dataset." ></td>
	<td class="line x" title="89:149	Section 4.1 examines our basic subjectivity extraction algorithms, which are based on individualsentence predictions alone." ></td>
	<td class="line x" title="90:149	Section 4.2 evaluates the more sophisticated form of subjectivity extraction that incorporates context information via the minimum-cut paradigm." ></td>
	<td class="line x" title="91:149	As we will see, the use of subjectivity extracts can in the best case provide satisfying improvement in polarity classification, and otherwise can at least yield polarity-classification accuracies indistinguishable from employing the full review." ></td>
	<td class="line x" title="92:149	At the same time, the extracts we create are both smaller on average than the original document and more effective as input to a default polarity classifier than the same-length counterparts produced by standard summarization tactics (e.g. , firstor last-N sentences)." ></td>
	<td class="line x" title="93:149	We therefore conclude that subjectivity extraction produces effective summaries of document sentiment." ></td>
	<td class="line x" title="94:149	4.1 Basic subjectivity extraction As noted in Section 3, both Naive Bayes and SVMs can be trained on our subjectivity dataset and then used as a basic subjectivity detector." ></td>
	<td class="line x" title="95:149	The former has somewhat better average ten-fold cross-validation performance on the subjectivity dataset (92% vs. 90%), and so for space reasons, our initial discussions will focus on the results attained via NB subjectivity detection." ></td>
	<td class="line x" title="96:149	Employing Naive Bayes as a subjectivity detector (ExtractNB) in conjunction with a Naive Bayes document-level polarity classifier achieves 86.4% accuracy.9 This is a clear improvement over the 82.8% that results when no extraction is applied 8Parameter training is driven by optimizing the performance of the downstream polarity classifier rather than the detector itself because the subjectivity datasets sentences come from different reviews, and so are never proximal." ></td>
	<td class="line x" title="97:149	9This result and others are depicted in Figure 5; for now, consider only the y-axis in those plots." ></td>
	<td class="line x" title="98:149	subsubNB NBs1 s2 s3 s4 s_n a0a1a0a1a0a1a0a1a0a2a1a2a1a2a1a2a1a2 constructgraph computemin." ></td>
	<td class="line x" title="99:149	cut a3a1a3a1a3a1a3a1a3 a4a1a4a1a4a1a4a1a4extract create s1 s4 msentence extract(m<=n) a5a1a5a1a5 a5a1a5a1a5 a6a1a6 a6a1a6 a7a1a7a1a7 a7a1a7a1a7 a8a1a8 a8a1a8 a9a1a9a1a9 a9a1a9a1a9 a10a1a10 a10a1a10 nsentence review v1 v2s v3 edge crossing the cut v2 v3 v1 ts vn t vn proximity link individual subjectivityprobability linkPr 1Pr (s1)Pr (s1) a11a1a11a1a11a1a11a1a11a12a1a12a1a12a1a12a1a12 Figure 3: Graph-cut-based creation of subjective extracts." ></td>
	<td class="line x" title="100:149	(Full review); indeed, the difference is highly statistically significant (p < 0.01, paired t-test)." ></td>
	<td class="line x" title="101:149	With SVMs as the polarity classifier instead, the Full review performance rises to 87.15%, but comparison via the paired t-test reveals that this is statistically indistinguishable from the 86.4% that is achieved by running the SVM polarity classifier on ExtractNB input." ></td>
	<td class="line x" title="102:149	(More improvements to extraction performance are reported later in this section.)" ></td>
	<td class="line x" title="103:149	These findings indicate10 that the extracts preserve (and, in the NB polarity-classifier case, apparently clarify) the sentiment information in the originating documents, and thus are good summaries from the polarity-classification point of view." ></td>
	<td class="line x" title="104:149	Further support comes from a flipping experiment: if we give as input to the default polarity classifier an extract consisting of the sentences labeled objective, accuracy drops dramatically to 71% for NB and 67% for SVMs." ></td>
	<td class="line x" title="105:149	This confirms our hypothesis that sentences discarded by the subjectivity extraction process are indeed much less indicative of sentiment polarity." ></td>
	<td class="line x" title="106:149	Moreover, the subjectivity extracts are much more compact than the original documents (an important feature for a summary to have): they contain on average only about 60% of the source reviews words." ></td>
	<td class="line x" title="107:149	(This word preservation rate is plotted along the x-axis in the graphs in Figure 5)." ></td>
	<td class="line x" title="108:149	This prompts us to study how much reduction of the original documents subjectivity detectors can perform and still accurately represent the texts sentiment information." ></td>
	<td class="line x" title="109:149	We can create subjectivity extracts of varying lengths by taking just the N most subjective sentences11 from the originating review." ></td>
	<td class="line x" title="110:149	As one base10Recall that direct evidence is not available because the polarity datasets sentences lack subjectivity labels." ></td>
	<td class="line x" title="111:149	11These are the N sentences assigned the highest probability by the basic NB detector, regardless of whether their probabilline to compare against, we take the canonical summarization standard of extracting the first N sentences  in general settings, authors often begin documents with an overview." ></td>
	<td class="line x" title="112:149	We also consider the last N sentences: in many documents, concluding material may be a good summary, and www.rottentomatoes.com tends to select snippets from the end of movie reviews (Beineke et al. , 2004)." ></td>
	<td class="line x" title="113:149	Finally, as a sanity check, we include results from the N least subjective sentences according to Naive Bayes." ></td>
	<td class="line x" title="114:149	Figure 4 shows the polarity classifier results as N ranges between 1 and 40." ></td>
	<td class="line x" title="115:149	Our first observation is that the NB detector provides very good bang for the buck: with subjectivity extracts containing as few as 15 sentences, accuracy is quite close to what one gets if the entire review is used." ></td>
	<td class="line x" title="116:149	In fact, for the NB polarity classifier, just using the 5 most subjective sentences is almost as informative as the Full review while containing on average only about 22% of the source reviews words." ></td>
	<td class="line x" title="117:149	Also, it so happens that at N = 30, performance is actually slightly better than (but statistically indistinguishable from) Full review even when the SVM default polarity classifier is used (87.2% vs. 87.15%).12 This suggests potentially effective extraction alternatives other than using a fixed probability threshold (which resulted in the lower accuracy of 86.4% reported above)." ></td>
	<td class="line x" title="118:149	Furthermore, we see in Figure 4 that the N mostsubjective-sentences method generally outperforms the other baseline summarization methods (which perhaps suggests that sentiment summarization cannot be treated the same as topic-based summarizaities exceed 50% and so would actually be classified as subjective by Naive Bayes." ></td>
	<td class="line x" title="119:149	For reviews with fewer than N sentences, the entire review will be returned." ></td>
	<td class="line x" title="120:149	12Note that roughly half of the documents in the polarity dataset contain more than 30 sentences (average=32.3, standard deviation 15)." ></td>
	<td class="line x" title="121:149	55 60 65 70 75 80 85 90 1 5 10 15 20 25 30 35 40 Average accuracy N Accuracy for N-sentence abstracts (def = NB) most subjective N sentenceslast N sentences first N sentencesleast subjective N sentences Full review 55 60 65 70 75 80 85 90 1 5 10 15 20 25 30 35 40 Average accuracy N Accuracy for N-sentence abstracts (def = SVM) most subjective N sentenceslast N sentences first N sentencesleast subjective N sentences Full review Figure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers." ></td>
	<td class="line x" title="122:149	83 83.5 84 84.5 85 85.5 86 86.5 87 0.6 0.7 0.8 0.9 1 1.1 Average accuracy % of words extracted Accuracy for subjective abstracts (def = NB) difference in accuracy ExtractSVM+Prox ExtractNB+ProxExtract NB ExtractSVM not statistically significant Full Reviewindicates statistically significantimprovement in accuracy 83 83.5 84 84.5 85 85.5 86 86.5 87 0.6 0.7 0.8 0.9 1 1.1 Average accuracy % of words extracted Accuracy for subjective abstracts (def = SVM) difference in accuracy ExtractNB+Prox ExtractSVM+Prox ExtractSVM ExtractNB not statistically significant Full Review improvement in accuracyindicates statistically significant Figure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers." ></td>
	<td class="line x" title="123:149	Also indicated are results for some statistical significance tests." ></td>
	<td class="line x" title="124:149	tion, although this conjecture would need to be verified on other domains and data)." ></td>
	<td class="line x" title="125:149	Its also interesting to observe how much better the last N sentences are than the first N sentences; this may reflect a (hardly surprising) tendency for movie-review authors to place plot descriptions at the beginning rather than the end of the text and conclude with overtly opinionated statements." ></td>
	<td class="line x" title="126:149	4.2 Incorporating context information The previous section demonstrated the value of subjectivity detection." ></td>
	<td class="line x" title="127:149	We now examine whether context information, particularly regarding sentence proximity, can further improve subjectivity extraction." ></td>
	<td class="line x" title="128:149	As discussed in Section 2.2 and 3, contextual constraints are easily incorporated via the minimum-cut formalism but are not natural inputs for standard Naive Bayes and SVMs." ></td>
	<td class="line x" title="129:149	Figure 5 shows the effect of adding in proximity information." ></td>
	<td class="line x" title="130:149	ExtractNB+Prox and ExtractSVM+Prox are the graph-based subjectivity detectors using Naive Bayes and SVMs, respectively, for the individual scores; we depict the best performance achieved by a single setting of the three proximity-related edge-weight parameters over all ten data folds13 (parameter selection was not a focus of the current work)." ></td>
	<td class="line x" title="131:149	The two comparisons we are most interested in are ExtractNB+Prox versus ExtractNB and ExtractSVM+Prox versus ExtractSVM." ></td>
	<td class="line x" title="132:149	We see that the context-aware graph-based subjectivity detectors tend to create extracts that are more informative (statistically significant so (paired t-test) for SVM subjectivity detectors only), although these extracts are longer than their contextblind counterparts." ></td>
	<td class="line x" title="133:149	We note that the performance 13Parameters are chosen from T  {1,2,3}, f(d)  {1,e1d,1/d2}, and c  [0,1] at intervals of 0.1." ></td>
	<td class="line x" title="134:149	enhancements cannot be attributed entirely to the mere inclusion of more sentences regardless of whether they are subjective or not  one counterargument is that Full review yielded substantially worse results for the NB default polarity classifier and at any rate, the graph-derived extracts are still substantially more concise than the full texts." ></td>
	<td class="line x" title="135:149	Now, while incorporating a bias for assigning nearby sentences to the same category into NB and SVM subjectivity detectors seems to require some non-obvious feature engineering, we also wish to investigate whether our graph-based paradigm makes better use of contextual constraints that can be (more or less) easily encoded into the input of standard classifiers." ></td>
	<td class="line x" title="136:149	For illustrative purposes, we consider paragraph-boundary information, looking only at SVM subjectivity detection for simplicitys sake." ></td>
	<td class="line x" title="137:149	It seems intuitively plausible that paragraph boundaries (an approximation to discourse boundaries) loosen coherence constraints between nearby sentences." ></td>
	<td class="line x" title="138:149	To capture this notion for minimum-cutbased classification, we can simply reduce the association scores for all pairs of sentences that occur in different paragraphs by multiplying them by a cross-paragraph-boundary weight w  [0,1]." ></td>
	<td class="line x" title="139:149	For standard classifiers, we can employ the trick of having the detector treat paragraphs, rather than sentences, as the basic unit to be labeled." ></td>
	<td class="line x" title="140:149	This enables the standard classifier to utilize coherence between sentences in the same paragraph; on the other hand, it also (probably unavoidably) poses a hard constraint that all of a paragraphs sentences get the same label, which increases noise sensitivity.14 Our experiments reveal the graph-cut formulation to be the better approach: for both default polarity classifiers (NB and SVM), some choice of parameters (including w) for ExtractSVM+Prox yields statistically significant improvement over its paragraphunit non-graph counterpart (NB: 86.4% vs. 85.2%; SVM: 86.15% vs. 85.45%)." ></td>
	<td class="line x" title="141:149	5 Conclusions We examined the relation between subjectivity detection and polarity classification, showing that subjectivity detection can compress reviews into much shorter extracts that still retain polarity information at a level comparable to that of the full review." ></td>
	<td class="line x" title="142:149	In fact, for the Naive Bayes polarity classifier, the subjectivity extracts are shown to be more effective input than the originating document, which suggests 14For example, in the data we used, boundaries may have been missed due to malformed html." ></td>
	<td class="line x" title="143:149	that they are not only shorter, but also cleaner representations of the intended polarity." ></td>
	<td class="line x" title="144:149	We have also shown that employing the minimum-cut framework results in the development of efficient algorithms for sentiment analysis." ></td>
	<td class="line x" title="145:149	Utilizing contextual information via this framework can lead to statistically significant improvement in polarity-classification accuracy." ></td>
	<td class="line x" title="146:149	Directions for future research include developing parameterselection techniques, incorporating other sources of contextual cues besides sentence proximity, and investigating other means for modeling such information." ></td>
	<td class="line x" title="147:149	Acknowledgments We thank Eric Breck, Claire Cardie, Rich Caruana, Yejin Choi, Shimon Edelman, Thorsten Joachims, Jon Kleinberg, Oren Kurland, Art Munson, Vincent Ng, Fernando Pereira, Ves Stoyanov, Ramin Zabih, and the anonymous reviewers for helpful comments." ></td>
	<td class="line x" title="148:149	This paper is based upon work supported in part by the National Science Foundation under grants ITR/IM IIS-0081334 and IIS-0329064, a Cornell Graduate Fellowship in Cognitive Studies, and by an Alfred P. Sloan Research Fellowship." ></td>
	<td class="line x" title="149:149	Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or Sloan Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P04-3025
Incorporating Topic Information Into Semantic Analysis Models
Mullen, Tony;Collier, Nigel;"></td>
	<td class="line x" title="1:77	Incorporating topic information into sentiment analysis models Tony Mullen National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku Tokyo 101-8430, Japan, mullen@nii.ac.jp Nigel Collier National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku Tokyo 101-8430, Japan, collier@nii.ac.jp Abstract This paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic." ></td>
	<td class="line x" title="2:77	The results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information." ></td>
	<td class="line x" title="3:77	1 Introduction There are a number of challenging aspects in recognizing the favorability of opinion-based texts, the task known as sentiment analysis." ></td>
	<td class="line x" title="4:77	Opinions in natural language are very often expressed in subtle and complex ways, presenting challenges which may not be easily addressed by simple text categorization approaches such as n-gram or keyword identification approaches." ></td>
	<td class="line x" title="5:77	Although such approaches have been employed effectively (Pang et al. , 2002), there appears to remain considerable room for improvement." ></td>
	<td class="line x" title="6:77	Moving beyond these approaches can involve addressing the task at several levels." ></td>
	<td class="line x" title="7:77	Negative reviews may contain many apparently positive phrases even while maintaining a strongly negative tone, and the opposite is also common." ></td>
	<td class="line x" title="8:77	This paper attempts to address this issue using Support Vector Machines (SVMs), a well-known and powerful tool for classification of vectors of real-valued features (Vapnik, 1998)." ></td>
	<td class="line x" title="9:77	The present approach emphasizes the use of a variety of diverse information sources." ></td>
	<td class="line x" title="10:77	In particular, several classes of features based upon the proximity of the topic with phrases which have been assigned favorability values are described in order to take advantage of situations in which the topic of the text may be explicitly identified." ></td>
	<td class="line oc" title="11:77	2 Motivation In the past, work has been done in the area of characterizing words and phrases according to their emotive tone (Turney and Littman, 2003; Turney, 2002; Kamps et al. , 2002; Hatzivassiloglou and Wiebe, 2000; Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual phrases may bear little relation to the overall sentiment expressed by the text." ></td>
	<td class="line x" title="12:77	Pang et al.(2002)s treatment of the task as analogous to topic-classification underscores the difference between the two tasks." ></td>
	<td class="line x" title="14:77	A number of rhetorical devices, such as the drawing of contrasts between the reviewed entity and other entities or expectations, sarcasm, understatement, and digressions, all of which are used in abundance in many discourse domains, create challenges for these approaches." ></td>
	<td class="line x" title="15:77	It is hoped that incorporating topic information along the lines suggested in this paper will be a step towards solving some of these problems." ></td>
	<td class="line x" title="16:77	3 Methods 3.1 Semantic orientation with PMI Here, the term semantic orientation (SO) (Hatzivassiloglou and McKeown, 2002) refers to a real number measure of the positive or negative sentiment expressed by a word or phrase." ></td>
	<td class="line oc" title="17:77	In the present work, the approach taken by Turney (2002) is used to derive such values for selected phrases in the text." ></td>
	<td class="line o" title="18:77	For the purposes of this paper, these phrases will be referred to as value phrases, since they will be the sources of SO values." ></td>
	<td class="line x" title="19:77	Once the desired value phrases have been extracted from the text, each one is assigned an SO value." ></td>
	<td class="line x" title="20:77	The SO of a phrase is determined based upon the phrases pointwise mutual information (PMI) with the words excellent and poor." ></td>
	<td class="line x" title="21:77	PMI is defined by Church and Hanks (1989) as follows: a0a2a1a4a3a6a5a8a7a10a9a12a11a13a7a15a14a17a16a19a18a21a20a23a22a25a24 a14a27a26a29a28 a5a8a7a10a9a19a30a31a7a15a14a17a16 a28 a5a8a7 a9 a16 a28 a5a8a7 a14 a16a33a32 (1) where a28 a5a8a7a10a9a19a30a31a7a15a14a12a16 is the probability that a7a34a9 and a7a35a14 co-occur." ></td>
	<td class="line x" title="22:77	The SO for a a28a37a36a39a38a41a40a29a42a44a43 is the difference between its PMI with the word excellent and its PMI with the word poor. The method used to derive these values takes advantage of the possibility of using the World Wide Web as a corpus, similarly to work such as (Keller and Lapata, 2003)." ></td>
	<td class="line x" title="23:77	The probabilities are estimated by querying the AltaVista Advanced Search engine1 for counts." ></td>
	<td class="line x" title="24:77	The search engines NEAR operator, representing occurrences of the two queried words within ten words of each other in a text, is used to define co-occurrence." ></td>
	<td class="line x" title="25:77	The final SO equation is a45a39a46 a5 a28a37a36a47a38a25a40a29a42a33a43 a16a19a18 a48a50a49a33a51 a14a35a52a54a53a56a55 a57a59a58a61a60a62a17a63a6a64a13a65a67a66a61a68a70a69a72a71a74a73a37a75a77a76a8a78a80a79a67a81a82a78a84a83a85a83a85a78a84a86a87a57a8a88a88a90a89a90a53a91a55 a57a8a58a82a60a80a76a8a92a12a93a94a93a13a95a8a88a88a23a89 a53a56a55 a57a59a58a61a60a62a17a63a6a64a13a65a67a66a61a68a70a69a72a71a74a73a37a75a77a76a8a92a12a93a94a93a13a95 a88a88 a89a90a53a91a55 a57a8a58a82a60a80a76a8a78a80a79a67a81a82a78a84a83a85a83a85a78a84a86a87a57 a88a88 a89a67a96 Intuitively, this yields values above zero for phrases with greater PMI with the word excellent and below zero for greater PMI with poor." ></td>
	<td class="line o" title="26:77	A SO value of zero would indicate a completely neutral semantic orientation." ></td>
	<td class="line x" title="27:77	3.2 Osgood semantic differentiation with WordNet Further feature types are derived using the method of Kamps and Marx (2002) of using WordNet relationships to derive three values pertinent to the emotive meaning of adjectives." ></td>
	<td class="line x" title="28:77	The three values correspond to the potency (strong or weak), activity (active or passive) and the evaluative (good or bad) factors introduced in Charles Osgoods Theory of Semantic Differentiation (Osgood et al. , 1957)." ></td>
	<td class="line x" title="29:77	These values are derived by measuring the relative minimal path length (MPL) in WordNet between the adjective in question and the pair of words appropriate for the given factor." ></td>
	<td class="line x" title="30:77	In the case of the evaluative factor (EVA) for example, the comparison is between the MPL between the adjective and good and the MPL between the adjective and bad." ></td>
	<td class="line x" title="31:77	Only adjectives connected by synonymy to each of the opposites are considered." ></td>
	<td class="line x" title="32:77	The method results in a list of 5410 adjectives, each of which is given a value for each of the three factors referred to as EVA, POT, and ACT." ></td>
	<td class="line x" title="33:77	Each of these factors values are averaged over all the adjectives in a text, yielding three real-valued feature values for the text, which will be added to the SVM model." ></td>
	<td class="line x" title="34:77	3.3 Topic proximity and syntactic-relation features In some application domains, it is known in advance what the topic is toward which sentiment is to be evaluated." ></td>
	<td class="line x" title="35:77	Incorporating this information is done by creating several classes of features based upon the semantic orientation values of phrases given their position in relation to the topic of the text." ></td>
	<td class="line x" title="36:77	The approach allows secondary information to be incorporated where available, in this case, the primary information is the specific record being reviewed and the secondary information identified is the artist." ></td>
	<td class="line x" title="37:77	Texts were annotated by hand using the Open Ontology Forge annotation tool (Collier et al. , 2003)." ></td>
	<td class="line x" title="38:77	In each record review, references (including co-reference) to the record being reviewed were tagged as THIS WORK and references to the artist under review were tagged as THIS ARTIST." ></td>
	<td class="line x" title="39:77	With these entities tagged, a number of classes of features may be extracted, representing various relationships between topic entities and value phrases similar to those described in section 3.1." ></td>
	<td class="line o" title="40:77	The classes looked at in this work are as follows: Turney Value The average value of all value phrases SO values for the text." ></td>
	<td class="line o" title="41:77	Classification by this feature alone is not the equivalent of Turneys approach, since the present approach involves retraining in a supervised model." ></td>
	<td class="line x" title="42:77	In sentence with THIS WORK The average value of all value phrases which occur in the same sentence as a reference to the work being reviewed." ></td>
	<td class="line x" title="43:77	1www.altavista.com Following THIS WORK The average value of all value phrases which follow a reference to the work being reviewed directly, or separated only by the copula or a preposition." ></td>
	<td class="line x" title="44:77	Preceding THIS WORK The average value of all value phrases which precede a reference to the work being reviewed directly, or separated only by the copula or a preposition." ></td>
	<td class="line x" title="45:77	In sentence with THIS ARTIST As above, but with reference to the artist." ></td>
	<td class="line x" title="46:77	Following THIS ARTIST As above, but with reference to the artist." ></td>
	<td class="line x" title="47:77	Preceding THIS ARTIST As above, but with reference to the artist." ></td>
	<td class="line x" title="48:77	The features used which make use of adjectives with WordNet derived Osgood values include the following: Text-wide EVA The average EVA value of all adjectives in a text." ></td>
	<td class="line x" title="49:77	Text-wide POT The average POT value of all adjectives in a text." ></td>
	<td class="line x" title="50:77	Text-wide ACT The average ACT value of all adjectives in a text." ></td>
	<td class="line x" title="51:77	TOPIC-sentence EVA The average EVA value of all adjectives which share a sentence with the topic of the text." ></td>
	<td class="line x" title="52:77	TOPIC-sentence POT The average POT value of all adjectives which share a sentence with the topic of the text." ></td>
	<td class="line x" title="53:77	TOPIC-sentence ACT The average ACT value of all adjectives which share a sentence with the topic of the text." ></td>
	<td class="line x" title="54:77	The grouping of these classes should reflect some common degree of reliability of features within a given class, but due to data sparseness what might have been more natural class groupingsfor example including value-phrase preposition topic-entity as a distinct classoften had to be conflated in order to get features with enough occurrences to be representative." ></td>
	<td class="line x" title="55:77	4 Experiments The dataset consists of 100 record reviews from the Pitchfork Media online record review publication,2 topic-annotated by hand." ></td>
	<td class="line o" title="56:77	Features used include word unigrams and lemmatized unigrams3 as well as the features described in 3.3 which make use of topic information, namely the broader PMI derived SO values and the topic-sentence Osgood values." ></td>
	<td class="line x" title="57:77	Due to the relatively small size of this dataset, test suites were created using 100, 20, 10, and 5-fold cross validation, to maximize the amount of data available for training and the accuracy of the results." ></td>
	<td class="line x" title="58:77	SVMs were built using Kudos TinySVM software implementation.4 5 Results Experimental results may be seen in figure 1." ></td>
	<td class="line x" title="59:77	It must be noted that this dataset is very small,and although the results are not conclusive they are promising insofar as they suggest that the use of incorporating PMI values towards the topic yields some improvement in modeling." ></td>
	<td class="line x" title="60:77	They also suggest that the best way to incorporate such features is in the form of a separate SVM which may then be combined with the lemma-based model to create a hybrid." ></td>
	<td class="line o" title="61:77	2http://www.pitchforkmedia.com 3We employ the Conexor FDG parser (Tapanainen and Jarvinen, 1997) for POS tagging and lemmatization 4http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM Model 5 folds 10 folds 20 folds 100 folds All (THIS WORK and THIS ARTIST) PMI 70% 70% 68% 69% THIS WORK PMI 72% 69% 70% 71% All Osgood 64% 64% 65% 64% All PMI and Osgood 74% 71% 74% 72% Unigrams 79% 80% 78% 82% Unigrams, PMI, Osgood 81% 80% 82% 82% Lemmas 83% 85% 84% 84% Lemmas and Osgood 83% 84% 84% 84% Lemmas and Turney 84% 85% 84% 84% Lemmas, Turney, text-wide Osgood 84% 85% 84% 84% Lemmas, PMI, Osgood 84% 85% 84% 86% Lemmas and PMI 84% 85% 85% 86% Hybrid SVM (PMI/Osgood and Lemmas) 86% 87% 84% 89% Figure 1: Accuracy results (percent of texts correctly classed) for 5, 10, 20 and 100-fold cross-validation tests with Pitchforkmedia.com record review data, hand-annotated for topic." ></td>
	<td class="line x" title="62:77	5.1 Discussion At the level of the phrasal SO assignment, it would seem that some improvement could be gained by adding domain context to the AltaVista Search." ></td>
	<td class="line x" title="63:77	Manyperhaps mostterms favorability content depends to some extent on their context." ></td>
	<td class="line o" title="64:77	As Turney notes, unpredictable, is generally positive when describing a movie plot, and negative when describing an automobile or a politician." ></td>
	<td class="line x" title="65:77	Likewise, such terms as devastating might be generally negative, but in the context of music or art may imply an emotional engagement which is usually seen as positive." ></td>
	<td class="line x" title="66:77	Likewise, using excellent and poor as the poles in assessing this value seems somewhat arbitrary, especially given the potentially misleading economic meaning of poor. Nevertheless, cursory experiments in adjusting the search have not yielded improvements." ></td>
	<td class="line x" title="67:77	One problem with limiting the domain (such as adding AND music or some disjunction of such constraints to the query) is that the resultant hit count is greatly diminished." ></td>
	<td class="line x" title="68:77	The data sparseness which results from added restrictions appears to cancel out any potential gain." ></td>
	<td class="line x" title="69:77	It is to be hoped that in the future, as search engines continue to improve and the Internet continues to grow, more possibilities will open up in this regard." ></td>
	<td class="line x" title="70:77	As it is, Google returns more hits than AltaVista, but its query syntax lacks a NEAR operator, making it unsuitable for this task." ></td>
	<td class="line x" title="71:77	As to why using excellent and poor works better than, for example good and bad, it is not entirely clear." ></td>
	<td class="line p" title="72:77	Again, cursory investigations have thus far supported Turneys conclusion that the former are the appropriate terms to use for this task." ></td>
	<td class="line x" title="73:77	It also seems likely that the topic-relations aspect of the present research only scratches the surface of what should be possible." ></td>
	<td class="line x" title="74:77	Although performance in the mid-80s is not bad, there is still considerable room for improvement." ></td>
	<td class="line x" title="75:77	The present models may also be further expanded with features representing other information sources, which may include other types of semantic annotation (Wiebe, 2002; Wiebe et al. , 2002), or features based on more sophisticated grammatical or dependency relations, or perhaps upon such things as zoning (e.g. do opinions become more clearly stated towards the end of a text?)." ></td>
	<td class="line x" title="76:77	In any case, it is hoped that the present work may help to indicate how various information sources pertinent to the task may be brought together." ></td>
	<td class="line x" title="77:77	6 Conclusion Further investigation using larger datasets is necessary for the purposes of fully exploiting topic information where it is available, but the present results suggest that this is a worthwhile direction to investigate." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1043
Extracting Product Features And Opinions From Reviews
Popescu, Ana-Maria;Etzioni, Oren;"></td>
	<td class="line x" title="1:242	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 339346, Vancouver, October 2005." ></td>
	<td class="line x" title="2:242	c2005 Association for Computational Linguistics Extracting Product Features and Opinions from Reviews Ana-Maria Popescu and Oren Etzioni Department of Computer Science and Engineering University of Washington Seattle, WA 98195-2350 {amp, etzioni}@cs.washington.edu Abstract Consumers are often forced to wade through many on-line reviews in order to make an informed product choice." ></td>
	<td class="line x" title="3:242	This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products." ></td>
	<td class="line x" title="4:242	Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task." ></td>
	<td class="line x" title="5:242	OPINEs novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity." ></td>
	<td class="line x" title="6:242	1 Introduction The Web contains a wealth of opinions about products, politicians, and more, which are expressed in newsgroup posts, review sites, and elsewhere." ></td>
	<td class="line oc" title="7:242	As a result, the problem of opinion mining has seen increasing attention over the last three years from (Turney, 2002; Hu and Liu, 2004) and many others." ></td>
	<td class="line x" title="8:242	This paper focuses on product reviews, though our methods apply to a broader range of opinions." ></td>
	<td class="line x" title="9:242	Product reviews on Web sites such as amazon.com and elsewhere often associate meta-data with each review indicating how positive (or negative) it is using a 5-star scale, and also rank products by how they fare in the reviews at the site." ></td>
	<td class="line x" title="10:242	However, the readers taste may differ from the reviewers." ></td>
	<td class="line x" title="11:242	For example, the reader may feel strongly about the quality of the gym in a hotel, whereas many reviewers may focus on other aspects of the hotel, such as the decor or the location." ></td>
	<td class="line x" title="12:242	Thus, the reader is forced to wade through a large number of reviews looking for information about particular features of interest." ></td>
	<td class="line x" title="13:242	We decompose the problem of review mining into the following main subtasks: I. Identify product features." ></td>
	<td class="line x" title="14:242	II." ></td>
	<td class="line x" title="15:242	Identify opinions regarding product features." ></td>
	<td class="line x" title="16:242	III." ></td>
	<td class="line x" title="17:242	Determine the polarity of opinions." ></td>
	<td class="line x" title="18:242	IV." ></td>
	<td class="line x" title="19:242	Rank opinions based on their strength." ></td>
	<td class="line x" title="20:242	This paper introduces OPINE, an unsupervised information extraction system that embodies a solution to each of the above subtasks." ></td>
	<td class="line x" title="21:242	OPINE is built on top of the KnowItAll Web information-extraction system (Etzioni et al. , 2005) as detailed in Section 3." ></td>
	<td class="line x" title="22:242	Given a particular product and a corresponding set of reviews, OPINE solves the opinion mining tasks outlined above and outputs a set of product features, each accompanied by a list of associated opinions which are ranked based on strength (e.g. , abominable is stronger than bad)." ></td>
	<td class="line x" title="23:242	This output information can then be used to generate various types of opinion summaries." ></td>
	<td class="line x" title="24:242	This paper focuses on the first 3 review mining subtasks and our contributions are as follows: 1." ></td>
	<td class="line x" title="25:242	We introduce OPINE, a review-mining system whose novel components include the use of relaxation labeling to find the semantic orientation of words in the context of given product features and sentences." ></td>
	<td class="line x" title="26:242	2." ></td>
	<td class="line x" title="27:242	We compare OPINE with the most relevant previous review-mining system (Hu and Liu, 2004) and find that OPINEs precision on the feature extraction task is 22% better though its recall is 3% lower on Hus data sets." ></td>
	<td class="line x" title="28:242	We show that 1/3 of this increase in precision comes from using OPINEs feature assessment mechanism on review data while the rest is due to Web PMI statistics." ></td>
	<td class="line x" title="29:242	3." ></td>
	<td class="line x" title="30:242	While many other systems have used extracted opinion phrases in order to determine the polarity of sentences or documents, OPINE is the first to report its precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity determination in the context of known product features and sentences." ></td>
	<td class="line x" title="31:242	On the first task, OPINE has a precision of 79% and a recall of 76%." ></td>
	<td class="line x" title="32:242	On the second task, OPINE has a precision of 86% and a recall of 89%." ></td>
	<td class="line x" title="33:242	339 Input: product class C, reviews R. Output: set of [feature, ranked opinion list] tuples RparseReviews(R); EfindExplicitFeatures(R, C); OfindOpinions(R, E); COclusterOpinions(O); IfindImplicitFeatures(CO, E); ROrankOpinions(CO); {(f, oi,oj)}outputTuples(RO, IE); Figure 1: OPINE Overview." ></td>
	<td class="line x" title="34:242	The remainder of this paper is organized as follows: Section 2 introduces the basic terminology, Section 3 gives an overview of OPINE, describes and evaluates its main components, Section 4 describes related work and Section 5 presents our conclusion." ></td>
	<td class="line x" title="35:242	2 Terminology A product class (e.g. , Scanner) is a set of products (e.g. , Epson1200)." ></td>
	<td class="line x" title="36:242	OPINE extracts the following types of product features: properties, parts, features of product parts, related concepts, parts and properties of related concepts (see Table 1 for examples of such features in the Scanner domains)." ></td>
	<td class="line x" title="37:242	Related concepts are concepts relevant to the customers experience with the main product (e.g. , the company that manufactures a scanner)." ></td>
	<td class="line x" title="38:242	The relationships between the main product and related concepts are typically expressed as verbs (e.g. , Epson manufactures scanners) or prepositions (scanners from Epson)." ></td>
	<td class="line x" title="39:242	Features can be explicit (good scan quality) or implicit (good scans implies good ScanQuality)." ></td>
	<td class="line x" title="40:242	OPINE also extracts opinion phrases, which are adjective, noun, verb or adverb phrases representing customer opinions." ></td>
	<td class="line x" title="41:242	Opinions can be positive or negative and vary in strength (e.g. , fantastic is stronger than good)." ></td>
	<td class="line x" title="42:242	3 OPINE Overview This section gives an overview of OPINE (see Figure 1) and describes its components and their experimental evaluation." ></td>
	<td class="line x" title="43:242	Goal Given product class C with instances I and reviews R, OPINEs goal is to find a set of (feature, opinions) tuples(f,oi,oj)}s.t. f F and oi,oj O, where: a) F is the set of product class features in R. b) O is the set of opinion phrases in R. c) f is a feature of a particular product instance." ></td>
	<td class="line x" title="44:242	d) o is an opinion about f in a particular sentence." ></td>
	<td class="line x" title="45:242	d) the opinions associated with each feature f are ranked based on their strength." ></td>
	<td class="line x" title="46:242	Solution The steps of our solution are outlined in Figure 1 above." ></td>
	<td class="line x" title="47:242	OPINE parses the reviews using MINIPAR (Lin, 1998) and applies a simple pronoun-resolution module to parsed review data." ></td>
	<td class="line x" title="48:242	OPINE then uses the data to find explicit product features (E)." ></td>
	<td class="line x" title="49:242	OPINEs Feature Assessor and its use of Web PMI statistics are vital for the extraction of high-quality features (see 3.2)." ></td>
	<td class="line x" title="50:242	OPINE then identifies opinion phrases associated with features in E and finds their polarity." ></td>
	<td class="line x" title="51:242	OPINEs novel use of relaxationlabeling techniques for determining the semantic orientation of potential opinion words in the context of given features and sentences leads to high precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity extraction (see 3.3)." ></td>
	<td class="line x" title="52:242	In this paper, we only focus on the extraction of explicit features, identifying corresponding customer opinions about these features and determining their polarity." ></td>
	<td class="line x" title="53:242	We omit the descriptions of the opinion clustering, implicit feature generation and opinion ranking algorithms." ></td>
	<td class="line x" title="54:242	3.0.1 The KnowItAll System." ></td>
	<td class="line x" title="55:242	OPINE is built on top of KnowItAll, a Web-based, domain-independent information extraction system (Etzioni et al. , 2005)." ></td>
	<td class="line x" title="56:242	Given a set of relations of interest, KnowItAll instantiates relation-specific generic extraction patterns into extraction rules which find candidate facts." ></td>
	<td class="line x" title="57:242	KnowItAlls Assessor then assigns a probability to each candidate." ></td>
	<td class="line x" title="58:242	The Assessor uses a form of Point-wise Mutual Information (PMI) between phrases that is estimated from Web search engine hit counts (Turney, 2001)." ></td>
	<td class="line x" title="59:242	It computes the PMI between each fact and automatically generated discriminator phrases (e.g. , is a scanner for the isA() relationship in the context of the Scanner class)." ></td>
	<td class="line x" title="60:242	Given fact f and discriminator d, the computed PMI score is: PMI(f,d) = Hits(d+f)Hits(d)Hits(f) The PMI scores are converted to binary features for a Naive Bayes Classifier, which outputs a probability associated with each fact (Etzioni et al. , 2005)." ></td>
	<td class="line x" title="61:242	3.1 Finding Explicit Features OPINE extracts explicit features for the given product class from parsed review data." ></td>
	<td class="line x" title="62:242	First, the system recursively identifies both the parts and the properties of the given product class and their parts and properties, in turn, continuing until no candidates are found." ></td>
	<td class="line x" title="63:242	Then, the system finds related concepts as described in (Popescu et al. , 2004) and extracts their parts and properties." ></td>
	<td class="line x" title="64:242	Table 1 shows that each feature type contributes to the set of final features (averaged over 7 product classes)." ></td>
	<td class="line x" title="65:242	Explicit Features Examples % Total Properties ScannerSize 7% Parts ScannerCover 52% Features of Parts BatteryLife 24% Related Concepts ScannerImage 9% Related Concepts Features ScannerImageSize 8% Table 1: Explicit Feature Information 340 In order to find parts and properties, OPINE first extracts the noun phrases from reviews and retains those with frequency greater than an experimentally set threshold." ></td>
	<td class="line x" title="66:242	OPINEs Feature Assessor, which is an instantiation of KnowItAlls Assessor, evaluates each noun phrase by computing the PMI scores between the phrase and meronymy discriminators associated with the product class (e.g. , of scanner, scanner has, scanner comes with, etc. for the Scanner class)." ></td>
	<td class="line x" title="67:242	OPINE distinguishes parts from properties using WordNets IS-A hierarchy (which enumerates different kinds of properties) and morphological cues (e.g. , -iness, -ity suffixes)." ></td>
	<td class="line x" title="68:242	3.2 Experiments: Explicit Feature Extraction In our experiments we use sets of reviews for 7 product classes (1621 total reviews) which include the publicly available data sets for 5 product classes from (Hu and Liu, 2004)." ></td>
	<td class="line x" title="69:242	Hus system is the review mining system most relevant to our work." ></td>
	<td class="line x" title="70:242	It uses association rule mining to extract frequent review noun phrases as features." ></td>
	<td class="line x" title="71:242	Frequent features are used to find potential opinion words (only adjectives) and the system uses WordNet synonyms/antonyms in conjunction with a set of seed words in order to find actual opinion words." ></td>
	<td class="line x" title="72:242	Finally, opinion words are used to extract associated infrequent features." ></td>
	<td class="line x" title="73:242	The system only extracts explicit features." ></td>
	<td class="line x" title="74:242	On the 5 datasets in (Hu and Liu, 2004), OPINEs precision is 22% higher than Hus at the cost of a 3% recall drop." ></td>
	<td class="line x" title="75:242	There are two important differences between OPINE and Hus system: a) OPINEs Feature Assessor uses PMI assessment to evaluate each candidate feature and b) OPINE incorporates Web PMI statistics in addition to review data in its assessment." ></td>
	<td class="line x" title="76:242	In the following, we quantify the performance gains from a) and b)." ></td>
	<td class="line x" title="77:242	a) In order to quantify the benefits of OPINEs Feature Assessor, we use it to evaluate the features extracted by Hus algorithm on review data (Hu+A/R)." ></td>
	<td class="line x" title="78:242	The Feature Assessor improves Hus precision by 6%." ></td>
	<td class="line x" title="79:242	b) In order to evaluate the impact of using Web PMI statistics, we assess OPINEs features first on reviews (OP/R) and then on reviews in conjunction with the Web (the corresponding methods are Hu+A/R+W and OPINE)." ></td>
	<td class="line x" title="80:242	Web PMI statistics increase precision by an average of 14.5%." ></td>
	<td class="line x" title="81:242	Overall, 1/3 of OPINEs precision increase over Hus system comes from using PMI assessment on reviews and the other 2/3 from the use of the Web PMI statistics." ></td>
	<td class="line x" title="82:242	In order to show that OPINEs performance is robust across multiple product classes, we used two sets of reviews downloaded from tripadvisor.com for Hotels and amazon.com for Scanners." ></td>
	<td class="line x" title="83:242	Two annotators labeled a set of unique 450 OPINE extractions as correct or incorrect." ></td>
	<td class="line x" title="84:242	The inter-annotator agreement was 86%." ></td>
	<td class="line x" title="85:242	The extractions on which the annotators agreed were used to compute OPINEs precision, which was 89%." ></td>
	<td class="line x" title="86:242	FurData Explicit Feature Extraction: Precision Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.75 +0.05 +0.17 +0.07 +0.19 D2 0.71 +0.03 +0.19 +0.08 +0.22 D3 0.72 +0.03 +0.25 +0.09 +0.23 D4 0.69 +0.06 +0.22 +0.08 +0.25 D5 0.74 +0.08 +0.19 +0.04 +0.21 Avg 0.72 +0.06 + 0.20 +0.07 +0.22 Table 2: Precision Comparison on the Explicit FeatureExtraction Task." ></td>
	<td class="line x" title="87:242	OPINEs precision is 22% better than Hus precision; Web PMI statistics are responsible for 2/3 of the precision increase." ></td>
	<td class="line x" title="88:242	All results are reported with respect to Hus. Data Explicit Feature Extraction: Recall Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.82 -0.16 -0.08 -0.14 -0.02 D2 0.79 -0.17 -0.09 -0.13 -0.06 D3 0.76 -0.12 -0.08 -0.15 -0.03 D4 0.82 -0.19 -0.04 -0.17 -0.03 D5 0.80 -0.16 -0.06 -0.12 -0.02 Avg 0.80 -0.16 -0.07 -0.14 -0.03 Table 3: Recall Comparison on the Explicit FeatureExtraction Task." ></td>
	<td class="line x" title="89:242	OPINEs recall is 3% lower than the recall of Hus original system (precision level = 0.8)." ></td>
	<td class="line x" title="90:242	All results are reported with respect to Hus. thermore, the annotators extracted explicit features from 800 review sentences (400 for each domain)." ></td>
	<td class="line x" title="91:242	The interannotator agreement was 82%." ></td>
	<td class="line x" title="92:242	OPINEs recall on the set of 179 features on which both annotators agreed was 73%." ></td>
	<td class="line x" title="93:242	3.3 Finding Opinion Phrases and Their Polarity This subsection describes how OPINE extracts potential opinion phrases, distinguishes between opinions and nonopinions, and finds the polarity of each opinion in the context of its associated feature in a particular review sentence." ></td>
	<td class="line x" title="94:242	3.3.1 Extracting Potential Opinion Phrases OPINE uses explicit features to identify potential opinion phrases." ></td>
	<td class="line x" title="95:242	Our intuition is that an opinion phrase associated with a product feature will occur in its vicinity." ></td>
	<td class="line x" title="96:242	This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser." ></td>
	<td class="line x" title="97:242	Our intuition is embodied by 10 extraction rules, some of which are shown in Table 4." ></td>
	<td class="line x" title="98:242	If an explicit feature is found in a sentence, OPINE applies the extraction rules in order to find the heads of potential opinion phrases." ></td>
	<td class="line x" title="99:242	Each head word together with its modi341 fiers is returned as a potential opinion phrase1." ></td>
	<td class="line x" title="100:242	Extraction Rules Examples if(M,NP = f)po = M (expensive) scanner if(S = f,P,O)po = O lamp has (problems) if(S,P,O = f)po = P I (hate) this scanner if(S = f,P,O)po = P program (crashed) Table 4: Examples of Domain-independent Rules for the Extraction of Potential Opinion Phrases." ></td>
	<td class="line x" title="101:242	Notation: po=potential opinion, M=modifier, NP=noun phrase, S=subject, P=predicate, O=object." ></td>
	<td class="line x" title="102:242	Extracted phrases are enclosed in parentheses." ></td>
	<td class="line x" title="103:242	Features are indicated by the typewriter font." ></td>
	<td class="line x" title="104:242	The equality conditions on the left-hand side use pos head." ></td>
	<td class="line x" title="105:242	Rule Templates Rules dep(w,wprime) m(w,wprime) v s.t. dep(w,v),dep(v,wprime) v s.t. m(w,v),o(v,wprime) v s.t. dep(w,v),dep(wprime,v) v s.t. m(w,v),o(wprime,v) Table 5: Dependency Rule Templates For Finding Words w, w with Related SO Labels." ></td>
	<td class="line x" title="106:242	OPINE instantiates these templates in order to obtain extraction rules." ></td>
	<td class="line x" title="107:242	Notation: dep=dependent, m=modifier, o=object, v,w,w=words." ></td>
	<td class="line x" title="108:242	OPINE examines the potential opinion phrases in order to identify the actual opinions." ></td>
	<td class="line x" title="109:242	First, the system finds the semantic orientation for the lexical head of each potential opinion phrase." ></td>
	<td class="line x" title="110:242	Every phrase whose head word has a positive or negative semantic orientation is then retained as an opinion phrase." ></td>
	<td class="line x" title="111:242	In the following, we describe how OPINE finds the semantic orientation of words." ></td>
	<td class="line x" title="112:242	3.3.2 Word Semantic Orientation OPINE finds the semantic orientation of a word w in the context of an associated feature f and sentence s. We restate this task as follows: Task Given a set of semantic orientation (SO) labels ({positive,negative,neutral}), a set of reviews and a set of tuples (w, f, s), where w is a potential opinion word associated with feature f in sentence s, assign a SO label to each tuple (w, f, s)." ></td>
	<td class="line x" title="113:242	For example, the tuple (sluggish, driver, I am not happy with this sluggish driver) would be assigned a negative SO label." ></td>
	<td class="line x" title="114:242	Note: We use word to refer to a potential opinion word w and feature to refer to the word or phrase which represents the explicit feature f. Solution OPINE uses the 3-step approach below: 1." ></td>
	<td class="line x" title="115:242	Given the set of reviews, OPINE finds a SO label for each word w. 2." ></td>
	<td class="line x" title="116:242	Given the set of reviews and the set of SO labels for words w, OPINE finds a SO label for each (w, f) pair." ></td>
	<td class="line x" title="117:242	1The (S,P,O) tuples in Table 4 are automatically generated from MINIPARs output." ></td>
	<td class="line x" title="118:242	3." ></td>
	<td class="line x" title="119:242	Given the set of SO labels for (w, f) pairs, OPINE finds a SO label for each (w, f, s) input tuple." ></td>
	<td class="line x" title="120:242	Each of these subtasks is cast as an unsupervised collective classification problem and solved using the same mechanism." ></td>
	<td class="line x" title="121:242	In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of labels to objects." ></td>
	<td class="line x" title="122:242	In each case, OPINE makes use of local constraints on label assignments (e.g. , conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997))." ></td>
	<td class="line x" title="123:242	A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tuples while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computer vision (e.g. , model-based matching)." ></td>
	<td class="line x" title="124:242	OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983), in order to solve the three subtasks described above." ></td>
	<td class="line x" title="125:242	3.3.3 Relaxation Labeling Overview Relaxation labeling is an unsupervised classification technique which takes as input: a) a set of objects (e.g. , words) b) a set of labels (e.g. , SO labels) c) initial probabilities for each objects possible labels d) the definition of an object os neighborhood (a set of other objects which influence the choice of os label) e) the definition of neighborhood features f) the definition of a support function for an object label The influence of an object os neighborhood on its label L is quantified using the support function." ></td>
	<td class="line x" title="126:242	The support function computes the probability of the label L being assigned to o as a function of os neighborhood features." ></td>
	<td class="line x" title="127:242	Examples of features include the fact that a certain local constraint is satisfied (e.g. , the word nice participates in the conjunction and together with some other word whose SO label is estimated to be positive)." ></td>
	<td class="line x" title="128:242	Relaxation labeling is an iterative procedure whose output is an assignment of labels to objects." ></td>
	<td class="line x" title="129:242	At each iteration, the algorithm uses an update equation to reestimate the probability of an object label based on its previous probability estimate and the features of its neighborhood." ></td>
	<td class="line x" title="130:242	The algorithm stops when the global label assignment stays constant over multiple consecutive iterations." ></td>
	<td class="line x" title="131:242	We employ relaxation labeling for the following reasons: a) it has been extensively used in computer-vision with good results b) its formalism allows for many types of constraints on label assignments to be used simultaneously." ></td>
	<td class="line x" title="132:242	As mentioned before, constraints are integrated into the algorithm as neighborhood features which influence the assignment of a particular label to a particular object." ></td>
	<td class="line x" title="133:242	OPINE uses the following sources of constraints: 342 a) conjunctions and disjunctions in the review text b) manually-supplied syntactic dependency rule templates (see Table 5)." ></td>
	<td class="line x" title="134:242	The templates are automatically instantiated by our system with different dependency relationships (premodifier, postmodifier, subject, etc)." ></td>
	<td class="line x" title="135:242	in order to obtain syntactic dependency rules which find words with related SO labels." ></td>
	<td class="line x" title="136:242	c) automatically derived morphological relationships (e.g. , wonderful and wonderfully are likely to have similar SO labels)." ></td>
	<td class="line x" title="137:242	d) WordNet-supplied synonymy, antonymy, IS-A and morphological relationships between words." ></td>
	<td class="line x" title="138:242	For example, clean and neat are synonyms and so they are likely to have similar SO labels." ></td>
	<td class="line x" title="139:242	Each of the SO label assignment subtasks previously identified is solved using a relaxation labeling step." ></td>
	<td class="line x" title="140:242	In the following, we describe in detail how relaxation labeling is used to find SO labels for words in the given review sets." ></td>
	<td class="line x" title="141:242	3.3.4 Finding SO Labels for Words For many words, a word sense or set of senses is used throughout the review corpus with a consistently positive, negative or neutral connotation (e.g. , great, awful, etc.)." ></td>
	<td class="line x" title="142:242	Thus, in many cases, a word ws SO label in the context of a feature f and sentence s will be the same as its SO label in the context of other features and sentences." ></td>
	<td class="line x" title="143:242	In the following, we describe how OPINEs relaxation labeling mechanism is used to find a words dominant SO label in a set of reviews." ></td>
	<td class="line x" title="144:242	For this task, a words neighborhood is defined as the set of words connected to it through conjunctions, disjunctions and all other relationships previously introduced as sources of constraints." ></td>
	<td class="line x" title="145:242	RL uses an update equation to re-estimate the probability of a word label based on its previous probability estimate and the features of its neighborhood (see Neighborhood Features)." ></td>
	<td class="line x" title="146:242	At iteration m, let q(w,L)(m) denote the support function for label L of w and let P(l(w) = L)(m) denote the probability that L is the label of w. P(l(w) = L)(m+1) is computed as follows: RL Update Equation (Rangarajan, 2000) P(l(w) = L)(m+1) = P(l(w) = L)(m)(1+ q(w,L)(m))P Lprime P(l(w) = Lprime)(m)(1+ q(w,Lprime)(m)) where Lprime  {pos,neg,neutral} and  > 0 is an experimentally set constant keeping the numerator and probabilities positive." ></td>
	<td class="line x" title="147:242	RLs output is an assignment of dominant SO labels to words." ></td>
	<td class="line x" title="148:242	In the following, we describe in detail the initialization step, the derivation of the support function formula and the use of neighborhood features." ></td>
	<td class="line x" title="149:242	RL Initialization Step OPINE uses a version of Turneys PMI-based approach (Turney, 2003) in order to derive the initial probability estimates (P(l(w) = L)(0)) for a subset S of the words." ></td>
	<td class="line x" title="150:242	OPINE computes a SO score so(w) for each w in S as the difference between the PMI of w with positive keywords (e.g. , excellent) and the PMI of w with negative keywords (e.g. , awful)." ></td>
	<td class="line x" title="151:242	When so(w) is small, or w rarely co-occurs with the keywords, w is classified as neutral." ></td>
	<td class="line x" title="152:242	If so(w) > 0, then w is positive, otherwise w is negative." ></td>
	<td class="line x" title="153:242	OPINE then uses the labeled S set in order to compute prior probabilities P(l(w) = L), L {pos,neg,neutral} by computing the ratio between the number of words in S labeled L and |S|." ></td>
	<td class="line x" title="154:242	Such probabilities are used as initial probability estimates associated with the labels of the remaining words." ></td>
	<td class="line x" title="155:242	Support Function The support function computes the probability of each label for word w based on the labels of objects in ws neighborhood N. Let Ak = {(wj,Lj)|wj  N}, 0 < k  3|N| represent one of the potential assignments of labels to the words in N. Let P(Ak)(m) denote the probability of this particular assignment at iteration m. The support for label L of word w at iteration m is : q(w,L)(m) = 3|N|X k=1 P(l(w) = L|Ak)(m)  P(Ak)(m) We assume that the labels of ws neighbors are independent of each other and so the formula becomes: q(w,L)(m) = 3|N|X k=1 P(l(w) = L|Ak)(m) |N|Y j=1 P(l(wj) = Lj)(m) Every P(l(wj) = Lj)(m) term is the estimate for the probability that l(wj) = Lj (which was computed at iteration m using the RL update equation)." ></td>
	<td class="line x" title="156:242	The P(l(w) = L|Ak)(m) term quantifies the influence of a particular label assignment to ws neighborhood over ws label." ></td>
	<td class="line x" title="157:242	In the following, we describe how we estimate this term." ></td>
	<td class="line x" title="158:242	Neighborhood Features Each type of word relationship which constrains the assignment of SO labels to words (synonymy, antonymy, etc)." ></td>
	<td class="line x" title="159:242	is mapped by OPINE to a neighborhood feature." ></td>
	<td class="line x" title="160:242	This mapping allows OPINE to use simultaneously use multiple independent sources of constraints on the label of a particular word." ></td>
	<td class="line x" title="161:242	In the following, we formalize this mapping." ></td>
	<td class="line x" title="162:242	Let T denote the type of a word relationship in R (synonym, antonym, etc)." ></td>
	<td class="line x" title="163:242	and let Ak,T represent the labels assigned by Ak to neighbors of a word w which are connected to w through a relationship of type T . We have Ak =uniontextT Ak,T and P(l(w) = L|Ak)(m) = P(l(w) = L| [ T Ak,T)(m) For each relationship type T, OPINE defines a neighborhood feature fT(w,L,Ak,T) which computes P(l(w) = L|Ak,T), the probability that ws label is L given Ak,T (see below)." ></td>
	<td class="line x" title="164:242	P(l(w) = L|uniontextT Ak,T)(m) is estimated combining the information from various features about ws label using the sigmoid function (): 343 P(l(w) = L|Ak)(m) = ( jX i=1 fi(w,L,Ak,i)(m)  ci) where c0,cj are weights whose sum is 1 and which reflect OPINE s confidence in each type of feature." ></td>
	<td class="line x" title="165:242	Given word w, label L, relationship type T and neighborhood label assignment Ak, let NT represent the subset of ws neighbors connected to w through a type T relationship." ></td>
	<td class="line x" title="166:242	The feature fT computes the probability that ws label is L given the labels assigned by Ak to words in NT . Using Bayess Law and assuming that these labels are independent given l(w), we have the following formula for fT at iteration m: fT(w,L,Ak,T)(m) = P(l(w) = L)(m) |NT|Y j=1 P(Lj|l(w) = L) P(Lj|l(w) = L) is the probability that word wj has label Lj if wj and w are linked by a relationship of type T and w has label L. We make the simplifying assumption that this probability is constant and depends only of T, L and Lprime, not of the particular words wj and w. For each tuple (T, L, Lj), L,Lj {pos,neg,neutral}, OPINE builds a probability table using a small set of bootstrapped positive, negative and neutral words." ></td>
	<td class="line x" title="167:242	3.3.5 Finding (Word, Feature) SO Labels This subtask is motivated by the existence of frequent words which change their SO label based on associated features, but whose SO labels in the context of the respective features are consistent throughout the reviews (e.g. , in the Hotel domain, hot water has a consistently positive connotation, whereas hot room has a negative one)." ></td>
	<td class="line x" title="168:242	In order to solve this task, OPINE first assigns each (w,f) pair an initial SO label which is ws SO label." ></td>
	<td class="line x" title="169:242	The system then executes a relaxation labeling step during which syntactic relationships between words and, respectively, between features, are used to update the default SO labels whenever necessary." ></td>
	<td class="line x" title="170:242	For example, (hot, room) appears in the proximity of (broken, fan)." ></td>
	<td class="line x" title="171:242	If roomand fan are conjoined by and, this suggests that hot and broken have similar SO labels in the context of their respective features." ></td>
	<td class="line x" title="172:242	If broken has a strongly negative semantic orientation, this fact contributes to OPINEs belief that hot may also be negative in this context." ></td>
	<td class="line x" title="173:242	Since (hot, room) occurs in the vicinity of other such phrases (e.g. , stifling kitchen), hot acquires a negative SO label in the context of room." ></td>
	<td class="line x" title="174:242	3.3.6 Finding (Word, Feature, Sentence) SO Labels This subtask is motivated by the existence of (w,f) pairs (e.g. , (big, room)) for which ws orientation changes based on the sentence in which the pair appears (e.g. ,  I hated the big, drafty room because I ended up freezing. vs. We had a big, luxurious room.)" ></td>
	<td class="line x" title="175:242	In order to solve this subtask, OPINE first assigns each (w,f,s) tuple an initial label which is simply the SO label for the (w,f) pair." ></td>
	<td class="line x" title="176:242	The system then uses syntactic relationships between words and, respectively, features in order to update the SO labels when necessary." ></td>
	<td class="line x" title="177:242	For example, in the sentence I hated the big, drafty room because I ended up freezing., big and hate satisfy condition 2 in Table 5 and therefore OPINE expects them to have similar SO labels." ></td>
	<td class="line x" title="178:242	Since hate has a strong negative connotation, big acquires a negative SO label in this context." ></td>
	<td class="line x" title="179:242	In order to correctly update SO labels in this last step, OPINE takes into consideration the presence of negation modifiers." ></td>
	<td class="line x" title="180:242	For example, in the sentence I dont like a large scanner either, OPINE first replaces the positive (w,f) pair (like, scanner) with the negative labeled pair (not like, scanner) and then infers that large is likely to have a negative SO label in this context." ></td>
	<td class="line x" title="181:242	3.3.7 Identifying Opinion Phrases After OPINE has computed the most likely SO labels for the head words of each potential opinion phrase in the context of given features and sentences, OPINE can extract opinion phrases and establish their polarity." ></td>
	<td class="line x" title="182:242	Phrases whose head words have been assigned positive or negative labels are retained as opinion phrases." ></td>
	<td class="line x" title="183:242	Furthermore, the polarity of an opinion phrase o in the context of a feature f and sentence s is given by the SO label assigned to the tuple (head(o),f,s) (3.3.6 shows how OPINE takes into account negation modifiers)." ></td>
	<td class="line x" title="184:242	3.4 Experiments In this section we evaluate OPINEs performance on the following tasks: finding SO labels of words in the context of known features and sentences (SO label extraction); distinguishing between opinion and non-opinion phrases in the context of known features and sentences (opinion phrase extraction); finding the correct polarity of extracted opinion phrases in the context of known features and sentences (opinion phrase polarity extraction)." ></td>
	<td class="line nc" title="185:242	While other systems, such as (Hu and Liu, 2004; Turney, 2002), have addressed these tasks to some degree, OPINE is the first to report results." ></td>
	<td class="line x" title="186:242	We first ran OPINE on 13841 sentences and 538 previously extracted features." ></td>
	<td class="line x" title="187:242	OPINE searched for a SO label assignment for 1756 different words in the context of the given features and sentences." ></td>
	<td class="line x" title="188:242	We compared OPINE against two baseline methods, PMI++ and Hu++." ></td>
	<td class="line oc" title="189:242	PMI++ is an extended version of (Turney, 2002)s method for finding the SO label of a phrase (as an attempt to deal with context-sensitive words)." ></td>
	<td class="line x" title="190:242	For a given (word, feature, sentence) tuple, PMI++ ignores the sentence, generates a phrase based on the word and the feature (e.g. , (clean, room): clean room) and finds its SO label using PMI statistics." ></td>
	<td class="line x" title="191:242	If unsure of the label, PMI++ tries to find the orientation of the potential opinion word instead." ></td>
	<td class="line x" title="192:242	The search engine queries use domain-specific keywords (e.g. , scanner), which are dropped if they 344 lead to low counts." ></td>
	<td class="line x" title="193:242	Hu++ is a WordNet-based method for finding a words context-independent semantic orientation." ></td>
	<td class="line x" title="194:242	It extends Hus adjective labeling method in a number of ways in order to handle nouns, verbs and adverbs in addition to adjectives and in order to improve coverage." ></td>
	<td class="line x" title="195:242	Hus method starts with two sets of positive and negative words and iteratively grows each one by including synonyms and antonyms from WordNet." ></td>
	<td class="line x" title="196:242	The final sets are used to predict the orientation of an incoming word." ></td>
	<td class="line x" title="197:242	Type PMI++ Hu++ OPINE P R P R P R adj 0.73 0.91 +0.02 -0.17 +0.07 -0.03 nn 0.63 0.92 +0.04 -0.24 +0.11 -0.08 vb 0.71 0.88 +0.03 -0.12 +0.01 -0.01 adv 0.82 0.92 +0.02 -0.01 +0.06 +0.01 Avg 0.72 0.91 +0.03 -0.14 +0.06 -0.03 Table 6: Finding SO Labels of Potential Opinion Words in the Context of Given Product Features and Sentences." ></td>
	<td class="line x" title="198:242	OPINEs precision is higher than that of PMI++ and Hu++." ></td>
	<td class="line x" title="199:242	All results are reported with respect to PMI++ . Notation: adj=adjectives, nn=nouns, vb=verbs, adv=adverbs 3.4.1 Experiments: SO Labels On the task of finding SO labels for words in the context of given features and review sentences, OPINE obtains higher precision than both baseline methods at a small loss in recall with respect to PMI++." ></td>
	<td class="line x" title="200:242	As described below, this result is due in large part to OPINEs ability to handle context-sensitive opinion words." ></td>
	<td class="line x" title="201:242	We randomly selected 200 (word, feature, sentence) tuples for each word type (adjective, adverb, etc)." ></td>
	<td class="line x" title="202:242	and obtained a test set containing 800 tuples." ></td>
	<td class="line x" title="203:242	Two annotators assigned positive, negative and neutral labels to each tuple (the inter-annotator agreement was 78%)." ></td>
	<td class="line x" title="204:242	We retained the tuples on which the annotators agreed as the gold standard." ></td>
	<td class="line x" title="205:242	We ran PMI++ and Hu++ on the test data and compared the results against OPINEs results on the same data." ></td>
	<td class="line x" title="206:242	In order to quantify the benefits of each of the three steps of our method for finding SO labels, we also compared OPINE with a version which only finds SO labels for words and a version which finds SO labels for words in the context of given features, but doesnt take into account given sentences." ></td>
	<td class="line x" title="207:242	We have learned from this comparison that OPINEs precision gain over PMI++ and Hu++ is mostly due to to its ability to handle contextsensitive words in a large number of cases." ></td>
	<td class="line x" title="208:242	Although Hu++ does not handle context-sensitive SO label assignment, its average precision was reasonable (75%) and better than that of PMI++." ></td>
	<td class="line x" title="209:242	Finding a words SO label is good enough in the case of strongly positive or negative opinion words, which account for the majority of opinion instances." ></td>
	<td class="line x" title="210:242	The methods loss in recall is due to not recognizing words absent from WordNet (e.g. , depth-adjustable) or not having enough information to classify some words in WordNet." ></td>
	<td class="line x" title="211:242	PMI++ typically does well in the presence of strongly positive or strongly negative words." ></td>
	<td class="line x" title="212:242	Its high recall is correlated with decreased precision, but overall this simple approach does well." ></td>
	<td class="line x" title="213:242	PMI++s main shortcoming is misclassifying terms such as basic or visible which change orientation based on context." ></td>
	<td class="line x" title="214:242	3.4.2 Experiments: Opinion Phrases In order to evaluate OPINE on the tasks of opinion phrase extraction and opinion phrase polarity extraction in the context of known features and sentences, we used a set of 550 sentences containing previously extracted features." ></td>
	<td class="line x" title="215:242	The sentences were annotated with the opinion phrases corresponding to the known features and with the opinion polarity." ></td>
	<td class="line x" title="216:242	We compared OPINE with PMI++ and Hu++ on the tasks of interest." ></td>
	<td class="line x" title="217:242	We found that OPINE had the highest precision on both tasks at a small loss in recall with respect to PMI++." ></td>
	<td class="line x" title="218:242	OPINEs ability to identify a words SO label in the context of a given feature and sentence allows the system to correctly extract opinions expressed by words such as big or small, whose semantic orientation varies based on context." ></td>
	<td class="line x" title="219:242	Measure PMI++ Hu++ OPINE OP Extraction: Precision 0.71 +0.06 +0.08 OP Extraction: Recall 0.78 -0.08 -0.02 OP Polarity: Precision 0.80 -0.04 +0.06 OP Polarity: Recall 0.93 +0.07 -0.04 Table 7: Extracting Opinion Phrases and Opinion Phrase Polarity Corresponding to Known Features and Sentences." ></td>
	<td class="line x" title="220:242	OPINEs precision is higher than that of PMI++ and of Hu++." ></td>
	<td class="line x" title="221:242	All results are reported with respect to PMI++." ></td>
	<td class="line x" title="222:242	4 Related Work The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words." ></td>
	<td class="line x" title="223:242	The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al. , 2004)." ></td>
	<td class="line x" title="224:242	Both identify product features from reviews, but OPINE significantly improves on both." ></td>
	<td class="line x" title="225:242	(Hu and Liu, 2004) doesnt assess candidate features, so its precision is lower than OPINEs." ></td>
	<td class="line x" title="226:242	(Kobayashi et al. , 2004) employs an iterative semi-automatic approach which requires human input at every iteration." ></td>
	<td class="line x" title="227:242	Neither model explicitly addresses composite (feature of feature) or implicit features." ></td>
	<td class="line x" title="228:242	Other systems (Morinaga et al. , 2002; Kushal et al. , 2003) also look at Web product reviews but they do not extract 345 opinions about particular product features." ></td>
	<td class="line x" title="229:242	OPINEs use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004)." ></td>
	<td class="line x" title="230:242	Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al. , 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="231:242	Most recently, (Takamura et al. , 2005) reports on the use of spin models to infer the semantic orientation of words." ></td>
	<td class="line x" title="232:242	The papers global optimization approach and use of multiple sources of constraints on a words semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information." ></td>
	<td class="line oc" title="233:242	Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al. , 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative." ></td>
	<td class="line x" title="234:242	So far, OPINEs focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity." ></td>
	<td class="line x" title="235:242	5 Conclusion OPINE is an unsupervised information extraction system which extracts fine-grained features, and associated opinions, from reviews." ></td>
	<td class="line x" title="236:242	OPINEs use of the Web as a corpus helps identify product features with improved precision compared with previous work." ></td>
	<td class="line x" title="237:242	OPINE uses a novel relaxation-labeling technique to determine the semantic orientation of potential opinion words in the context of the extracted product features and specific review sentences; this technique allows the system to identify customer opinions and their polarity with high precision and recall." ></td>
	<td class="line x" title="238:242	6 Acknowledgments We would like to thank the KnowItAll project and the anonymous reviewers for their comments." ></td>
	<td class="line x" title="239:242	Michael Gamon, Costas Boulis and Adam Carlson have also provided valuable feedback." ></td>
	<td class="line x" title="240:242	We thank Minquing Hu and Bing Liu for providing their data sets and for their comments." ></td>
	<td class="line x" title="241:242	Finally, we are grateful to Bernadette Minton and Fetch Technologies for their help in collecting additional reviews." ></td>
	<td class="line x" title="242:242	This research was supported in part by NSF grant IIS-0312988, DARPA contract NBCHD030010, ONR grant N00014-02-1-0324 as well as gifts from Google and the Turing Center." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1044
Recognizing Contextual Polarity In Phrase-Level Sentiment Analysis
Wilson, Theresa;Wiebe, Janyce M.;Hoffmann, Paul;"></td>
	<td class="line x" title="1:197	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 347354, Vancouver, October 2005." ></td>
	<td class="line x" title="2:197	c2005 Association for Computational Linguistics Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis Theresa Wilson Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 twilson@cs.pitt.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Paul Hoffmann Intelligent Systems Program University of Pittsburgh Pittsburgh, PA 15260 hoffmanp@cs.pitt.edu Abstract This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions." ></td>
	<td class="line x" title="3:197	With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline." ></td>
	<td class="line x" title="4:197	1 Introduction Sentiment analysis is the task of identifying positive and negative opinions, emotions, and evaluations." ></td>
	<td class="line x" title="5:197	Most work on sentiment analysis has been done at the document level, for example distinguishing positive from negative reviews." ></td>
	<td class="line x" title="6:197	However, tasks such as multi-perspective question answering and summarization, opinion-oriented information extraction, and mining product reviews require sentence-level or even phrase-level sentiment analysis." ></td>
	<td class="line x" title="7:197	For example, if a question answering system is to successfully answer questions about peoples opinions, it must be able to pinpoint expressions of positive and negative sentiments, such as we find in the sentences below: (1) African observers generally approved+ of his victory while Western governments denounced it." ></td>
	<td class="line x" title="8:197	(2) A succession of officers filled the TV screen to say they supported+ the people and that the killings were not tolerable. (3) We donprimet hate+ the sinner, he says, but we hate the sin. A typical approach to sentiment analysis is to start with a lexicon of positive and negative words and phrases." ></td>
	<td class="line x" title="9:197	In these lexicons, entries are tagged with their a priori prior polarity: out of context, does the word seem to evoke something positive or something negative." ></td>
	<td class="line x" title="10:197	For example, beautiful has a positive prior polarity, and horrid has a negative prior polarity." ></td>
	<td class="line x" title="11:197	However, the contextual polarity of the phrase in which a word appears may be different from the words prior polarity." ></td>
	<td class="line x" title="12:197	Consider the underlined polarity words in the sentence below: (4) Philip Clapp, president of the National Environment Trust, sums up well the general thrust of the reaction of environmental movements: There is no reason at all to believe that the polluters are suddenly going to become reasonable. Of these words, Trust, well, reason, and reasonable have positive prior polarity, but they are not all being used to express positive sentiments." ></td>
	<td class="line x" title="13:197	The word reason is negated, making the contextual polarity negative." ></td>
	<td class="line x" title="14:197	The phrase no reason at all to believe changes the polarity of the proposition that follows; because reasonable falls within this proposition, its contextual polarity becomes negative." ></td>
	<td class="line x" title="15:197	The word Trust is simply part of a referring expression and is not being used to express a sentiment; thus, its contextual polarity is neutral." ></td>
	<td class="line x" title="16:197	Similarly for polluters: in the context of the article, it simply refers to companies that pollute." ></td>
	<td class="line x" title="17:197	Only well has the same prior and contextual polarity." ></td>
	<td class="line x" title="18:197	Many things must be considered in phrase-level sentiment analysis." ></td>
	<td class="line x" title="19:197	Negation may be local (e.g. , not good), or involve longer-distance dependencies such as the negation of the proposition (e.g. , does not look very good) or the negation of the subject (e.g. , 347 no one thinks that its good)." ></td>
	<td class="line x" title="20:197	In addition, certain phrases that contain negation words intensify rather than change polarity (e.g. , not only good but amazing)." ></td>
	<td class="line x" title="21:197	Contextual polarity may also be influenced by modality (e.g. , whether the proposition is asserted to be real (realis) or not real (irrealis)  no reason at all to believe is irrealis, for example); word sense (e.g. , Environmental Trust versus He has won the peoples trust); the syntactic role of a word in the sentence (e.g. , polluters are versus they are polluters); and diminishers such as little (e.g. , little truth, little threat)." ></td>
	<td class="line x" title="22:197	(See (Polanya and Zaenen, 2004) for a more detailed discussion of contextual polarity influencers.)" ></td>
	<td class="line x" title="23:197	This paper presents new experiments in automatically distinguishing prior and contextual polarity." ></td>
	<td class="line x" title="24:197	Beginning with a large stable of clues marked with prior polarity, we identify the contextual polarity of the phrases that contain instances of those clues in the corpus." ></td>
	<td class="line x" title="25:197	We use a two-step process that employs machine learning and a variety of features." ></td>
	<td class="line x" title="26:197	The first step classifies each phrase containing a clue as neutral or polar." ></td>
	<td class="line x" title="27:197	The second step takes all phrases marked in step one as polar and disambiguates their contextual polarity (positive, negative, both, or neutral)." ></td>
	<td class="line x" title="28:197	With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline." ></td>
	<td class="line x" title="29:197	In addition, we describe new manual annotations of contextual polarity and a successful inter-annotator agreement study." ></td>
	<td class="line x" title="30:197	2 Manual Annotation Scheme To create a corpus for the experiments below, we added contextual polarity judgments to existing annotations in the Multi-perspective Question Answering (MPQA) Opinion Corpus1, namely to the annotations of subjective expressions2." ></td>
	<td class="line x" title="31:197	A subjective expression is any word or phrase used to express an opinion, emotion, evaluation, stance, speculation, 1The MPQA Corpus is described in (Wiebe et al. , 2005) and available at nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="32:197	2In the MPQA Corpus, subjective expressions are direct subjective expressions with non-neutral expression intensity, plus all the expressive subjective elements." ></td>
	<td class="line x" title="33:197	Please see (Wiebe et al. , 2005) for more details on the existing annotations in the MPQA Corpus." ></td>
	<td class="line x" title="34:197	etc. A general covering term for such states is private state (Quirk et al. , 1985)." ></td>
	<td class="line x" title="35:197	In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases." ></td>
	<td class="line x" title="36:197	For this work, our focus is on sentiment expressions  positive and negative expressions of emotions, evaluations, and stances." ></td>
	<td class="line x" title="37:197	As these are types of subjective expressions, to create the corpus, we just needed to manually annotate the existing subjective expressions with their contextual polarity." ></td>
	<td class="line x" title="38:197	In particular, we developed an annotation scheme3 for marking the contextual polarity of subjective expressions." ></td>
	<td class="line x" title="39:197	Annotators were instructed to tag the polarity of subjective expressions as positive, negative, both, or neutral." ></td>
	<td class="line x" title="40:197	The positive tag is for positive emotions (Im happy), evaluations (Great idea!), and stances (She supports the bill)." ></td>
	<td class="line x" title="41:197	The negative tag is for negative emotions (Im sad), evaluations (Bad idea!), and stances (Shes against the bill)." ></td>
	<td class="line x" title="42:197	The both tag is applied to sentiment expressions that have both positive and negative polarity." ></td>
	<td class="line x" title="43:197	The neutral tag is used for all other subjective expressions: those that express a different type of subjectivity such as speculation, and those that do not have positive or negative polarity." ></td>
	<td class="line x" title="44:197	Below are examples of contextual polarity annotations." ></td>
	<td class="line x" title="45:197	The tags are in boldface, and the subjective expressions with the given tags are underlined." ></td>
	<td class="line x" title="46:197	(5) Thousands of coup supporters celebrated (positive) overnight, waving flags, blowing whistles." ></td>
	<td class="line x" title="47:197	(6) The criteria set by Rice are the following: the three countries in question are repressive (negative) and grave human rights violators (negative) . . ." ></td>
	<td class="line x" title="48:197	(7) Besides, politicians refer to good and evil (both) only for purposes of intimidation and exaggeration." ></td>
	<td class="line x" title="49:197	(8) Jerome says the hospital feels (neutral) no different than a hospital in the states." ></td>
	<td class="line x" title="50:197	The annotators were asked to judge the contextual polarity of the sentiment that is ultimately being conveyed by the subjective expression, i.e., once the sentence has been fully interpreted." ></td>
	<td class="line x" title="51:197	Thus, the subjective expression, they have not succeeded, and 3The annotation instructions are available at http://www.cs.pitt.edu/twilson." ></td>
	<td class="line x" title="52:197	348 will never succeed, was marked as positive in the sentence, They have not succeeded, and will never succeed, in breaking the will of this valiant people." ></td>
	<td class="line x" title="53:197	The reasoning is that breaking the will of a valiant people is negative; hence, not succeeding in breaking their will is positive." ></td>
	<td class="line x" title="54:197	3 Agreement Study To measure the reliability of the polarity annotation scheme, we conducted an agreement study with two annotators, using 10 documents from the MPQA Corpus." ></td>
	<td class="line x" title="55:197	The 10 documents contain 447 subjective expressions." ></td>
	<td class="line x" title="56:197	Table 1 shows the contingency table for the two annotators judgments." ></td>
	<td class="line x" title="57:197	Overall agreement is 82%, with a Kappa () value of 0.72." ></td>
	<td class="line x" title="58:197	Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, : 0.72) For 18% of the subjective expressions, at least one annotator used an uncertain tag when marking polarity." ></td>
	<td class="line x" title="59:197	If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 90% and Kappa rises to 0.84." ></td>
	<td class="line x" title="60:197	Thus, the annotator agreement is especially high when both are certain." ></td>
	<td class="line x" title="61:197	(Note that all annotations are included in the experiments described below.)" ></td>
	<td class="line x" title="62:197	4 Corpus In total, 15,991 subjective expressions from 425 documents (8,984 sentences) were annotated with contextual polarity as described above." ></td>
	<td class="line x" title="63:197	Of these sentences, 28% contain no subjective expressions, 25% contain only one, and 47% contain two or more." ></td>
	<td class="line x" title="64:197	Of the 4,247 sentences containing two or more subjective expressions, 17% contain mixtures of positive and negative expressions, and 62% contain mixtures of polar (positive/negative/both) and neutral subjective expressions." ></td>
	<td class="line x" title="65:197	The annotated documents are divided into two sets." ></td>
	<td class="line x" title="66:197	The first (66 documents/1,373 sentences/2,808 subjective expressions) is a development set, used for data exploration and feature development." ></td>
	<td class="line x" title="67:197	We use the second set (359 documents/7,611 sentences/13,183 subjective expressions) in 10-fold cross-validation experiments, described below." ></td>
	<td class="line x" title="68:197	5 Prior-Polarity Subjectivity Lexicon For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues." ></td>
	<td class="line x" title="69:197	Subjectivity clues are words and phrases that may be used to express private states, i.e., they have subjective usages (though they may have objective usages as well)." ></td>
	<td class="line x" title="70:197	For this work, only single-word clues are used." ></td>
	<td class="line x" title="71:197	To compile the lexicon, we began with a list of subjectivity clues from (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="72:197	The words in this list were grouped in previous work according to their reliability as subjectivity clues." ></td>
	<td class="line x" title="73:197	Words that are subjective in most contexts were marked strongly subjective (strongsubj), and those that may only have certain subjective usages were marked weakly subjective (weaksubj)." ></td>
	<td class="line x" title="74:197	We expanded the list using a dictionary and a thesaurus, and also added words from the General Inquirer positive and negative word lists (GeneralInquirer, 2000) which we judged to be potentially subjective." ></td>
	<td class="line x" title="75:197	We also gave the new words reliability tags, either strongsubj or weaksubj." ></td>
	<td class="line x" title="76:197	The next step was to tag the clues in the lexicon with their prior polarity." ></td>
	<td class="line x" title="77:197	For words that came from positive and negative word lists (General-Inquirer, 2000; Hatzivassiloglou and McKeown, 1997), we largely retained their original polarity, either positive or negative." ></td>
	<td class="line x" title="78:197	We assigned the remaining words one of the tags positive, negative, both or neutral." ></td>
	<td class="line x" title="79:197	By far, the majority of clues, 92.8%, are marked as having either positive (33.1%) or negative (59.7%) prior polarity." ></td>
	<td class="line x" title="80:197	Only a small number of clues (0.3%) are marked as having both positive and negative polarity." ></td>
	<td class="line x" title="81:197	6.9% of the clues in the lexicon are marked as neutral." ></td>
	<td class="line x" title="82:197	Examples of these are verbs such as feel, look, and think, and intensifiers such as deeply, entirely, and practically." ></td>
	<td class="line x" title="83:197	These words are included because, although their prior polarity is neutral, they are good clues that a sentiment is being expressed (e.g. , feels slighted, look forward to)." ></td>
	<td class="line x" title="84:197	Including them increases the coverage of the system." ></td>
	<td class="line x" title="85:197	349 6 Experiments The goal of the experiments described below is to classify the contextual polarity of the expressions that contain instances of the subjectivity clues in our lexicon." ></td>
	<td class="line x" title="86:197	What the system specifically does is give each clue instance its own label." ></td>
	<td class="line x" title="87:197	Note that the system does not try to identify expression boundaries." ></td>
	<td class="line x" title="88:197	Doing so might improve performance and is a promising avenue for future research." ></td>
	<td class="line x" title="89:197	6.1 Definition of the Gold Standard We define the gold standard used to train and test the system in terms of the manual annotations described in Section 2." ></td>
	<td class="line x" title="90:197	The gold standard class of a clue instance that is not in a subjective expression is neutral: since the clue is not even in a subjective expression, it is not contained in a sentiment expression." ></td>
	<td class="line x" title="91:197	Otherwise, if a clue instance appears in just one subjective expression (or in multiple subjective expressions with the same contextual polarity), then the class assigned to the clue instance is the class of the subjective expression(s)." ></td>
	<td class="line x" title="92:197	If a clue appears in at least one positive and one negative subjective expression (or in a subjective expression marked as both), then its class is both." ></td>
	<td class="line x" title="93:197	If it is in a mixture of negative and neutral subjective expressions, its class is negative; if it is in a mixture of positive and neutral subjective expressions, its class is positive." ></td>
	<td class="line x" title="94:197	6.2 Performance of a Prior-Polarity Classifier An important question is how useful prior polarity alone is for identifying contextual polarity." ></td>
	<td class="line x" title="95:197	To answer this question, we create a classifier that simply assumes that the contextual polarity of a clue instance is the same as the clues prior polarity, and we explore the classifiers performance on the development set." ></td>
	<td class="line x" title="96:197	This simple classifier has an accuracy of 48%." ></td>
	<td class="line x" title="97:197	From the confusion matrix given in Table 2, we see that 76% of the errors result from words with nonneutral prior polarity appearing in phrases with neutral contextual polarity." ></td>
	<td class="line x" title="98:197	6.3 Contextual Polarity Disambiguation The fact that words with non-neutral prior polarity so frequently appear in neutral contexts led us to Prior-Polarity Classifier Neut Pos Neg Both Total Neut 798 784 698 4 2284 Pos 81 371 40 0 492 Gold Neg 149 181 622 0 952 Both 4 11 13 5 33 Total 1032 1347 1373 9 3761 Table 2: Confusion matrix for the prior-polarity classifier on the development set." ></td>
	<td class="line x" title="99:197	adopt a two-step approach to contextual polarity disambiguation." ></td>
	<td class="line x" title="100:197	For the first step, we concentrate on whether clue instances are neutral or polar in context (where polar in context refers to having a contextual polarity that is positive, negative or both)." ></td>
	<td class="line x" title="101:197	For the second step, we take all clue instances marked as polar in step one, and focus on identifying their contextual polarity." ></td>
	<td class="line x" title="102:197	For both steps, we develop classifiers using the BoosTexter AdaBoost.HM (Schapire and Singer, 2000) machine learning algorithm with 5000 rounds of boosting." ></td>
	<td class="line x" title="103:197	The classifiers are evaluated in 10-fold cross-validation experiments." ></td>
	<td class="line x" title="104:197	6.3.1 Neutral-Polar Classification The neutral-polar classifier uses 28 features, listed in Table 3." ></td>
	<td class="line x" title="105:197	Word Features: Word context is a bag of three word tokens: the previous word, the word itself, and the next word." ></td>
	<td class="line x" title="106:197	The prior polarity and reliability class are indicated in the lexicon." ></td>
	<td class="line x" title="107:197	Modification Features: These are binary relationship features." ></td>
	<td class="line x" title="108:197	The first four involve relationships with the word immediately before or after: if the word is a noun preceded by an adjective, if the preceding word is an adverb other than not, if the preceding word is an intensifier, and if the word itself is an intensifier." ></td>
	<td class="line x" title="109:197	A word is considered an intensifier if it appears in a list of intensifiers and if it precedes a word of the appropriate part-of-speech (e.g. , an intensifier adjective must come before a noun)." ></td>
	<td class="line x" title="110:197	The modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence (Collins, 1997) and then converting the tree into its dependency representation (Xia and Palmer, 2001)." ></td>
	<td class="line x" title="111:197	In a dependency representation, every node in the tree structure is a surface word (i.e. , there are no abstract nodes such as NP or VP)." ></td>
	<td class="line x" title="112:197	The edge between a parent and a child specifies the grammatical relationship between the two words." ></td>
	<td class="line x" title="113:197	Figure 1 shows 350 Word Features Sentence Features Structure Features word token strongsubj clues in current sentence: count in subject: binary word part-of-speech strongsubj clues in previous sentence: count in copular: binary word context strongsubj clues in next sentence: count in passive: binary prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count Modification Features weaksubj clues in next sentence: count Document Feature preceeded by adjective: binary adjectives in sentence: count document topic preceeded by adverb (other than not): binary adverbs in sentence (other than not): count preceeded by intensifier: binary cardinal number in sentence: binary is intensifier: binary pronoun in sentence: binary modifies strongsubj: binary modal in sentence (other than will): binary modifies weaksubj: binary modified by strongsubj: binary modified by weaksubj: binary Table 3: Features for neutral-polar classification T h e h u m a n r i g h t s r e p o r t a p o s e s s u b s t a n t i a l c h a l l e n g e t o U St h e i n t e r p r e t a t i o n o f g o o d a n d e v i l d e t d e t d e t a d j a d j o b j s u b j m o d m o d c o n j c o n j p o b j p o b j p p ( p o s ) ( n e g ) ( p o s ) ( n e g ) ( p o s ) Figure 1: The dependency tree for the sentence The human rights report poses a substantial challenge to the US interpretation of good and evil." ></td>
	<td class="line x" title="114:197	Prior polarity is marked in parentheses for words that match clues from the lexicon." ></td>
	<td class="line x" title="115:197	an example." ></td>
	<td class="line x" title="116:197	The modifies strongsubj/weaksubj features are true if the word and its parent share an adj, mod or vmod relationship, and if its parent is an instance of a clue from the lexicon with strongsubj/weaksubj reliability." ></td>
	<td class="line x" title="117:197	The modified by strongsubj/weaksubj features are similar, but look for relationships and clues in the words children." ></td>
	<td class="line x" title="118:197	Structure Features: These are binary features that are determined by starting with the word instance and climbing up the dependency parse tree toward the root, looking for particular relationships, words, or patterns." ></td>
	<td class="line x" title="119:197	The in subject feature is true if we find a subj relationship." ></td>
	<td class="line x" title="120:197	The in copular feature is true if in subject is false and if a node along the path is both a main verb and a copular verb." ></td>
	<td class="line x" title="121:197	The in passive features is true if a passive verb pattern is found on the climb." ></td>
	<td class="line x" title="122:197	Sentence Features: These are features that were found useful for sentence-level subjectivity classification by Wiebe and Riloff (2005)." ></td>
	<td class="line x" title="123:197	They include counts of strongsubj and weaksubj clues in the current, previous and next sentences, counts of adjectives and adverbs other than not in the current sentence, and binary features to indicate whether the sentence contains a pronoun, a cardinal number, and a modal other than will." ></td>
	<td class="line x" title="124:197	Document Feature: There is one document feature representing the topic of the document." ></td>
	<td class="line x" title="125:197	A document may belong to one of 15 topics ranging from specific (e.g. , the 2002 presidential election in Zimbabwe) to more general (e.g. , economics) topics." ></td>
	<td class="line x" title="126:197	Table 4 gives neutral-polar classification results for the 28-feature classifier and two simpler classifiers that provide our baselines." ></td>
	<td class="line x" title="127:197	The first row in the table lists the results for a classifier that uses just one feature, the word token." ></td>
	<td class="line x" title="128:197	The second row shows the results for a classifier that uses both the word token and the words prior polarity as features." ></td>
	<td class="line x" title="129:197	The results for the 28-feature classifier are listed in the last row." ></td>
	<td class="line x" title="130:197	The 28-feature classifier performs significantly better (1-tailed t-test, p  .05) than the two simpler classifiers, as measured by accuracy, polar F-measure, and neutral F-measure ( = 1)." ></td>
	<td class="line x" title="131:197	It has an accuracy of 75.9%, with a polar F-measure of 63.4 and a neutral F-measure of 82.1." ></td>
	<td class="line x" title="132:197	Focusing on the metrics for polar expressions, its interesting to note that using just the word token as a feature produces a classifier with a precision slightly better than the 28-feature classifier, but with a recall that is 20% lower." ></td>
	<td class="line x" title="133:197	Adding a feature for the prior 351 Word Features word token word prior polarity: positive, negative, both, neutral Polarity Features negated: binary negated subject: binary modifies polarity: positive, negative, neutral, both, notmod modified by polarity: positive, negative, neutral, both, notmod conj polarity: positive, negative, neutral, both, notmod general polarity shifter: binary negative polarity shifter: binary positive polarity shifter: binary Table 6: Features for polarity classification polarity improves recall so that it is only 4.4% lower, but this hurts precision, which drops to 4.2% lower than the 28-feature classifiers precision." ></td>
	<td class="line x" title="134:197	It is only with all the features that we get the best result, good precision with the highest recall." ></td>
	<td class="line x" title="135:197	The clues in the prior-polarity lexicon have 19,506 instances in the test set." ></td>
	<td class="line x" title="136:197	According to the 28-feature neutral-polar classifier, 5,671 of these instances are polar in context." ></td>
	<td class="line x" title="137:197	It is these clue instances that are passed on to the second step in the contextual disambiguation process, polarity classification." ></td>
	<td class="line x" title="138:197	6.3.2 Polarity Classification Ideally, this second step in the disambiguation process would be a three-way classification task, determining whether the contextual polarity is positive, negative or both." ></td>
	<td class="line x" title="139:197	However, although the majority of neutral expressions have been filtered out by the neutral-polar classification in step one, a number still remain." ></td>
	<td class="line x" title="140:197	So, for this step, the polarity classification task remains four-way: positive, negative, both, and neutral." ></td>
	<td class="line x" title="141:197	Table 6 lists the features used by the polarity classifier." ></td>
	<td class="line x" title="142:197	Word token and word prior polarity are unchanged from the neutral-polar classifier." ></td>
	<td class="line x" title="143:197	Negated is a binary feature that captures whether the word is being locally negated: its value is true if a negation word or phrase is found within the four preceeding words or in any of the words children in the dependency tree, and if the negation word is not in a phrase that intensifies rather than negates (e.g. , not only)." ></td>
	<td class="line x" title="144:197	The negated subject feature is true if the subject of the clause containing the word is negated." ></td>
	<td class="line x" title="145:197	The modifies polarity, modified by polarity, and conj polarity features capture specific relationships between the word instance and other polarity words it may be related to." ></td>
	<td class="line x" title="146:197	If the word and its parent in the dependency tree share an obj, adj, mod, or vmod relationship, the modifies polarity feature is set to the prior polarity of the words parent (if the parent is not in our prior-polarity lexicon, its prior polarity is set to neutral)." ></td>
	<td class="line x" title="147:197	The modified by polarity feature is similar, looking for adj, mod, and vmod relationships and polarity clues within the words children." ></td>
	<td class="line x" title="148:197	The conj polarity feature determines if the word is in a conjunction." ></td>
	<td class="line x" title="149:197	If so, the value of this feature is its siblings prior polarity (as above, if the sibling is not in the lexicon, its prior polarity is neutral)." ></td>
	<td class="line x" title="150:197	Figure 1 helps to illustrate these features: modifies polarity is negative for the word substantial, modified by polarity is positive for the word challenge, and conj polarity is negative for the word good. The last three polarity features look in a window of four words before, searching for the presence of particular types of polarity influencers." ></td>
	<td class="line x" title="151:197	General polarity shifters reverse polarity (e.g. , little truth, little threat)." ></td>
	<td class="line x" title="152:197	Negative polarity shifters typically make the polarity of an expression negative (e.g. , lack of understanding)." ></td>
	<td class="line x" title="153:197	Positive polarity shifters typically make the polarity of an expression positive (e.g. , abate the damage)." ></td>
	<td class="line x" title="154:197	The polarity classification results for this second step in the contextual disambiguation process are given in Table 5." ></td>
	<td class="line x" title="155:197	Also listed in the table are results for the two simple classifiers that provide our baselines." ></td>
	<td class="line x" title="156:197	The first line in Table 5 lists the results for the classifier that uses just one feature, the word token. The second line shows the results for the classifier that uses both the word token and the words prior polarity as features." ></td>
	<td class="line x" title="157:197	The last line shows the results for the polarity classifier that uses all 10 features from Table 6." ></td>
	<td class="line x" title="158:197	Mirroring the results from step one, the more complex classifier performs significantly better than the simpler classifiers, as measured by accuracy and all of the F-measures." ></td>
	<td class="line x" title="159:197	The 10-feature classifier achieves an accuracy of 65.7%, which is 4.3% higher than the more challenging baseline provided by the word + prior polarity classifier." ></td>
	<td class="line x" title="160:197	Positive Fmeasure is 65.1 (5.7% higher); negative F-measure is 77.2 (2.3% higher); and neutral F-measure is 46.2 (13.5% higher)." ></td>
	<td class="line x" title="161:197	Focusing on the metrics for positive and negative expressions, we again see that the simpler classifiers 352 Acc Polar Rec Polar Prec Polar F Neut Rec Neut Prec Neut F word token 73.6 45.3 72.2 55.7 89.9 74.0 81.2 word+priorpol 74.2 54.3 68.6 60.6 85.7 76.4 80.7 28 features 75.9 56.8 71.6 63.4 87.0 77.7 82.1 Table 4: Results for Step 1 Neutral-Polar Classification Positive Negative Both Neutral Acc Rec Prec F Rec Prec F Rec Prec F Rec Prec F word token 61.7 59.3 63.4 61.2 83.9 64.7 73.1 9.2 35.2 14.6 30.2 50.1 37.7 word+priorpol 63.0 69.4 55.3 61.6 80.4 71.2 75.5 9.2 35.2 14.6 33.5 51.8 40.7 10 features 65.7 67.1 63.3 65.1 82.1 72.9 77.2 11.2 28.4 16.1 41.4 52.4 46.2 Table 5: Results for Step 2 Polarity Classification." ></td>
	<td class="line x" title="162:197	Experiment Features Removed AB1 negated, negated subject AB2 modifies polarity, modified by polarity AB3 conj polarity AB4 general, negative, and positive polarity shifters Table 7: Features for polarity classification take turns doing better or worse for precision and recall." ></td>
	<td class="line x" title="163:197	Using just the word token, positive precision is slightly higher than for the 10-feature classifier, but positive recall is 11.6% lower." ></td>
	<td class="line x" title="164:197	Add the prior polarity, and positive recall improves, but at the expense of precision, which is 12.6% lower than for the 10-feature classifier." ></td>
	<td class="line x" title="165:197	The results for negative expressions are similar." ></td>
	<td class="line x" title="166:197	The word-token classifier does well on negative recall but poorly on negative precision." ></td>
	<td class="line x" title="167:197	When prior polarity is added, negative recall improves but negative precision drops." ></td>
	<td class="line x" title="168:197	It is only with the addition of the polarity features that we achieve both higher precisions and higher recalls." ></td>
	<td class="line x" title="169:197	To explore how much the various polarity features contribute to the performance of the polarity classifier, we perform four experiments." ></td>
	<td class="line x" title="170:197	In each experiment, a different set of polarity features is excluded, and the polarity classifier is retrained and evaluated." ></td>
	<td class="line x" title="171:197	Table 7 lists the features that are removed for each experiment." ></td>
	<td class="line x" title="172:197	The only significant difference in performance in these experiments is neutral F-measure when the modification features (AB2) are removed." ></td>
	<td class="line x" title="173:197	These ablation experiments show that the combination of features is needed to achieve significant results over baseline for polarity classification." ></td>
	<td class="line oc" title="174:197	7 Related Work Much work on sentiment analysis classifies documents by their overall sentiment, for example determining whether a review is positive or negative (e.g. , (Turney, 2002; Dave et al. , 2003; Pang and Lee, 2004; Beineke et al. , 2004))." ></td>
	<td class="line x" title="175:197	In contrast, our experiments classify individual words and phrases." ></td>
	<td class="line oc" title="176:197	A number of researchers have explored learning words and phrases with prior positive or negative polarity (another term is semantic orientation) (e.g. , (Hatzivassiloglou and McKeown, 1997; Kamps and Marx, 2002; Turney, 2002))." ></td>
	<td class="line x" title="177:197	In contrast, we begin with a lexicon of words with established prior polarities, and identify the contextual polarity of phrases in which instances of those words appear in the corpus." ></td>
	<td class="line o" title="178:197	To make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity are included in our prior-polarity lexicon (General Inquirer lists (General-Inquirer, 2000) used for evaluation by Turney, and lists of manually identified positive and negative adjectives, used for evaluation by Hatzivassiloglou and McKeown)." ></td>
	<td class="line x" title="179:197	Some research classifies the sentiments of sentences." ></td>
	<td class="line x" title="180:197	Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.(2001)4 all begin by first creating prior-polarity lexicons." ></td>
	<td class="line x" title="182:197	Yu and Hatzivassiloglou then assign a sentiment to a sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence." ></td>
	<td class="line x" title="183:197	Thus, they do not identify the contextual polarity of individual phrases containing clues, as we 4In (Grefenstette et al. , 2001), the units that are classified are fixed windows around named entities rather than sentences." ></td>
	<td class="line x" title="184:197	353 do in this paper." ></td>
	<td class="line x" title="185:197	Kim and Hovy, Hu and Liu, and Grefenstette et al. multiply or count the prior polarities of clue instances in the sentence." ></td>
	<td class="line x" title="186:197	They also consider local negation to reverse polarity." ></td>
	<td class="line x" title="187:197	However, they do not use the other types of features in our experiments, and they restrict their tags to positive and negative (excluding our both and neutral categories)." ></td>
	<td class="line x" title="188:197	In addition, their systems assign one sentiment per sentence; our system assigns contextual polarity to individual expressions." ></td>
	<td class="line x" title="189:197	As seen above, sentences often contain more than one sentiment expression." ></td>
	<td class="line x" title="190:197	Nasukawa, Yi, and colleagues (Nasukawa and Yi, 2003; Yi et al. , 2003) classify the contextual polarity of sentiment expressions, as we do." ></td>
	<td class="line x" title="191:197	Thus, their work is probably most closely related to ours." ></td>
	<td class="line x" title="192:197	They classify expressions that are about specific items, and use manually developed patterns to classify polarity." ></td>
	<td class="line x" title="193:197	These patterns are high-quality, yielding quite high precision, but very low recall." ></td>
	<td class="line x" title="194:197	Their system classifies a much smaller proportion of the sentiment expressions in a corpus than ours does." ></td>
	<td class="line x" title="195:197	8 Conclusions In this paper, we present a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions." ></td>
	<td class="line x" title="196:197	With this approach, we are able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline." ></td>
	<td class="line x" title="197:197	9 Acknowledgments This work was supported in part by the NSF under grant IIS-0208798 and by the Advanced Research and Development Activity (ARDA)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1045
Identifying Sources Of Opinions With Conditional Random Fields And Extraction Patterns
Choi, Yejin;Cardie, Claire;Riloff, Ellen;Patwardhan, Siddharth;"></td>
	<td class="line x" title="1:194	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 355362, Vancouver, October 2005." ></td>
	<td class="line x" title="2:194	c2005 Association for Computational Linguistics Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns Yejin Choi and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14853 {ychoi,cardie}@cs.cornell.edu Ellen Riloff and Siddharth Patwardhan School of Computing University of Utah Salt Lake City, UT 84112 {riloff,sidd}@cs.utah.edu Abstract Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g. , detecting polarity and strength)." ></td>
	<td class="line x" title="3:194	We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments." ></td>
	<td class="line x" title="4:194	We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al. , 2001) and a variation of AutoSlog (Riloff, 1996a)." ></td>
	<td class="line x" title="5:194	While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns." ></td>
	<td class="line x" title="6:194	Our results show that the combination of these two methods performs better than either one alone." ></td>
	<td class="line x" title="7:194	The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure." ></td>
	<td class="line x" title="8:194	1 Introduction In recent years, there has been a great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text." ></td>
	<td class="line oc" title="9:194	Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g. , Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line x" title="12:194	Other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g. , Dave et al.(2003), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="15:194	Many applications could benefit from these opinion analyzers, including product reputation tracking (e.g. , Morinaga et al.(2002), Yi et al.(2003)), opinion-oriented summarization (e.g. , Cardie et al.(2004)), and question answering (e.g. , Bethard et al.(2004), Yu and Hatzivassiloglou (2003))." ></td>
	<td class="line x" title="20:194	We focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions." ></td>
	<td class="line x" title="21:194	Identifying opinion sources will be especially critical for opinion-oriented questionanswering systems (e.g. , systems that answer questions of the form How does [X] feel about [Y]?) and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text." ></td>
	<td class="line x" title="22:194	To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence 1In related work, we investigate methods to identify the opinion expressions (e.g. , Riloff and Wiebe (2003), Wiebe and Riloff (2005), Wilson et al.(2005)) and the nesting structure of sources (e.g. , Breck and Cardie (2004))." ></td>
	<td class="line x" title="24:194	The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus." ></td>
	<td class="line x" title="25:194	355 S2: According to the report, the human rights record in China is horrendous." ></td>
	<td class="line x" title="26:194	S3: International officers believe that the EU will prevail." ></td>
	<td class="line x" title="27:194	S4: International officers said US officials want the EU to prevail." ></td>
	<td class="line x" title="28:194	In S1, the phrase Taiwan-born voters is the direct (i.e. , first-hand) source of the favoring sentiment." ></td>
	<td class="line x" title="29:194	In S2, the report is the direct source of the opinion about Chinas human rights record." ></td>
	<td class="line x" title="30:194	In S3, International officers are the direct source of an opinion regarding the EU." ></td>
	<td class="line x" title="31:194	The same phrase in S4, however, denotes an indirect (i.e. , second-hand, third-hand, etc)." ></td>
	<td class="line x" title="32:194	source of an opinion whose direct source is US officials." ></td>
	<td class="line x" title="33:194	In this paper, we view source identification as an information extraction task and tackle the problem using sequence tagging and pattern matching techniques simultaneously." ></td>
	<td class="line x" title="34:194	Using syntactic, semantic, and orthographic lexical features, dependency parse features, and opinion recognition features, we train a linear-chain Conditional Random Field (CRF) (Lafferty et al. , 2001) to identify opinion sources." ></td>
	<td class="line x" title="35:194	In addition, we employ features based on automatically learned extraction patterns and perform feature induction on the CRF model." ></td>
	<td class="line x" title="36:194	We evaluate our hybrid approach using the NRRC corpus (Wiebe et al. , 2005), which is manually annotated with direct and indirect opinion source information." ></td>
	<td class="line x" title="37:194	Experimental results show that the CRF model performs well, and that both the extraction patterns and feature induction produce performance gains." ></td>
	<td class="line x" title="38:194	The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure." ></td>
	<td class="line x" title="39:194	2 The Big Picture The goal of information extraction (IE) systems is to extract information about events, including the participants of the events." ></td>
	<td class="line x" title="40:194	This task goes beyond Named Entity recognition (e.g. , Bikel et al.(1997)) because it requires the recognition of role relationships." ></td>
	<td class="line x" title="42:194	For example, an IE system that extracts information about corporate acquisitions must distinguish between the company that is doing the acquiring and the company that is being acquired." ></td>
	<td class="line x" title="43:194	Similarly, an IE system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim." ></td>
	<td class="line x" title="44:194	We hypothesized that IE techniques would be wellsuited for source identification because an opinion statement can be viewed as a kind of speech event with the source as the agent." ></td>
	<td class="line x" title="45:194	We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning." ></td>
	<td class="line x" title="46:194	In particular, we consider Conditional Random Fields (Lafferty et al. , 2001) and a variation of AutoSlog (Riloff, 1996a)." ></td>
	<td class="line x" title="47:194	CRFs have been used successfully for Named Entity recognition (e.g. , McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a)." ></td>
	<td class="line x" title="48:194	While CRFs treat source identification as a sequence tagging task, AutoSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence." ></td>
	<td class="line x" title="49:194	We hypothesized that a combination of the two techniques would perform better than either one alone." ></td>
	<td class="line x" title="50:194	Section 3 describes the CRF approach to identifying opinion sources and the features that the system uses." ></td>
	<td class="line x" title="51:194	Section 4 then presents a new variation of AutoSlog, AutoSlog-SE, which generates IE patterns to extract sources." ></td>
	<td class="line x" title="52:194	Section 5 describes the hybrid system: we encode the IE patterns as additional features in the CRF model." ></td>
	<td class="line x" title="53:194	Finally, Section 6 presents our experimental results and error analysis." ></td>
	<td class="line x" title="54:194	3 Semantic Tagging via Conditional Random Fields We defined the problem of opinion source identification as a sequence tagging task via CRFs as follows." ></td>
	<td class="line x" title="55:194	Given a sequence of tokens, x = x1x2xn, we need to generate a sequence of tags, or labels, y = y1y2yn." ></td>
	<td class="line x" title="56:194	We define the set of possible label values as S, T, -, where S is the first token (or Start) of a source, T is a non-initial token (i.e. , a conTinuation) of a source, and -is a token that is not part of any source.2 A detailed description of CRFs can be found in 2This is equivalent to the IOB tagging scheme used in syntactic chunkers (Ramshaw and Marcus, 1995)." ></td>
	<td class="line x" title="57:194	356 Lafferty et al.(2001)." ></td>
	<td class="line x" title="59:194	For our sequence tagging problem, we create a linear-chain CRF based on an undirected graph G = (V,E), where V is the set of random variables Y = fYij1 i ng, one for each of n tokens in an input sentence; and E = f(Yi1,Yi)j1 < i ng is the set of n 1 edges forming a linear chain." ></td>
	<td class="line x" title="60:194	For each sentence x, we define a non-negative clique potential exp(summationtextKk=1 kfk(yi1,yi,x)) for each edge, and exp(summationtextKprimek=1 primekfprimek(yi,x)) for each node, where fk() is a binary feature indicator function, k is a weight assigned for each feature function, and K and Kprime are the number of features defined for edges and nodes respectively." ></td>
	<td class="line x" title="61:194	Following Lafferty et al.(2001), the conditional probability of a sequence of labels y given a sequence of tokens x is: P(y|x) = 1Z x exp X i,k k fk(yi1, yi, x)+ X i,k primek fprimek(yi, x)  (1) Zx = X y exp X i,k k fk(yi1, yi, x) + X i,k primek fprimek(yi, x)  (2) where Zx is a normalization constant for each x. Given the training data D, a set of sentences paired with their correct ST- source label sequences, the parameters of the model are trained to maximize the conditional log-likelihoodproducttext (x,y)D P(yjx)." ></td>
	<td class="line x" title="63:194	For inference, given a sentence x in the test data, the tagging sequence y is given by argmaxyprimeP(yprimejx)." ></td>
	<td class="line x" title="64:194	3.1 Features To develop features, we considered three properties of opinion sources." ></td>
	<td class="line x" title="65:194	First, the sources of opinions are mostly noun phrases." ></td>
	<td class="line x" title="66:194	Second, the source phrases should be semantic entities that can bear or express opinions." ></td>
	<td class="line x" title="67:194	Third, the source phrases should be directly related to an opinion expression." ></td>
	<td class="line x" title="68:194	When considering only the first and second criteria, this task reduces to named entity recognition." ></td>
	<td class="line x" title="69:194	Because of the third condition, however, the task requires the recognition of opinion expressions and a more sophisticated encoding of sentence structure to capture relationships between source phrases and opinion expressions." ></td>
	<td class="line x" title="70:194	With these properties in mind, we define the following features for each token/word xi in an input sentence." ></td>
	<td class="line x" title="71:194	For pedagogical reasons, we will describe some of the features as being multi-valued or categorical features." ></td>
	<td class="line x" title="72:194	In practice, however, all features are binarized for the CRF model." ></td>
	<td class="line x" title="73:194	Capitalization features We use two boolean features to represent the capitalization of a word: all-capital, initial-capital." ></td>
	<td class="line x" title="74:194	Part-of-speech features Based on the lexical categories produced by GATE (Cunningham et al. , 2002), each token xi is classified into one of a set of coarse part-of-speech tags: noun, verb, adverb, wh-word, determiner, punctuation, etc. We do the same for neighboring words in a [ 2, +2] window in order to assist noun phrase segmentation." ></td>
	<td class="line x" title="75:194	Opinion lexicon features For each token xi, we include a binary feature that indicates whether or not the word is in our opinion lexicon  a set of words that indicate the presence of an opinion." ></td>
	<td class="line x" title="76:194	We do the same for neighboring words in a [ 1, +1] window." ></td>
	<td class="line x" title="77:194	Additionally, we include for xi a feature that indicates the opinion subclass associated with xi, if available from the lexicon." ></td>
	<td class="line x" title="78:194	(e.g., bless is classified as moderately subjective according to the lexicon, while accuse and berate are classified more specifically as judgments)." ></td>
	<td class="line x" title="79:194	The lexicon is initially populated with approximately 500 opinion words 3 from (Wiebe et al. , 2002), and then augmented with opinion words identified in the training data." ></td>
	<td class="line x" title="80:194	The training data contains manually produced phrase-level annotations for all expressions of opinions, emotions, etc.(Wiebe et al. , 2005)." ></td>
	<td class="line x" title="82:194	We collected all content words that occurred in the training set such that at least 50% of their occurrences were in opinion annotations." ></td>
	<td class="line x" title="83:194	Dependency tree features For each token xi, we create features based on the parse tree produced by the Collins (1999) dependency parser." ></td>
	<td class="line x" title="84:194	The purpose of the features is to (1) encode structural information, and (2) indicate whether xi is involved in any grammatical relations with an opinion word." ></td>
	<td class="line x" title="85:194	Two pre-processing steps are required before features can be constructed: 3Some words are drawn from Levin (1993); others are from Framenet lemmas (Baker et al. 1998) associated with communication verbs." ></td>
	<td class="line x" title="86:194	357 1." ></td>
	<td class="line x" title="87:194	Syntactic chunking." ></td>
	<td class="line x" title="88:194	We traverse the dependency tree using breadth-first search to identify and group syntactically related nodes, producing a flatter, more concise tree." ></td>
	<td class="line x" title="89:194	Each syntactic chunk is also assigned a grammatical role (e.g. , subject, object, verb modifier, time, location, of-pp, by-pp) based on its constituents." ></td>
	<td class="line x" title="90:194	Possessives (e.g. , Clintons idea) and the phrase according to X are handled as special cases in the chunking process." ></td>
	<td class="line x" title="91:194	2." ></td>
	<td class="line x" title="92:194	Opinion word propagation." ></td>
	<td class="line x" title="93:194	Although the opinion lexicon contains only content words and no multi-word phrases, actual opinions often comprise an entire phrase, e.g., is really willing or in my opinion." ></td>
	<td class="line x" title="94:194	As a result, we mark as an opinion the entire chunk that contains an opinion word." ></td>
	<td class="line x" title="95:194	This allows each token in the chunk to act as an opinion word for feature encoding." ></td>
	<td class="line x" title="96:194	After syntactic chunking and opinion word propagation, we create the following dependency tree features for each token xi: the grammatical role of its chunk the grammatical role of xi1s chunk whether the parent chunk includes an opinion word whether xis chunk is in an argument position with respect to the parent chunk whether xi represents a constituent boundary Semantic class features We use 7 binary features to encode the semantic class of each word xi: authority, government, human, media, organizationor company, proper name, and other." ></td>
	<td class="line x" title="97:194	The other class captures 13 semantic classes that cannot be sources, such as vehicle and time." ></td>
	<td class="line x" title="98:194	Semantic class information is derived from named entity and semantic class labels assigned to xi by the Sundance shallow parser (Riloff, 2004)." ></td>
	<td class="line x" title="99:194	Sundance uses named entity recognition rules to label noun phrases as belonging to named entity classes, and assigns semantic tags to individual words based on a semantic dictionary." ></td>
	<td class="line x" title="100:194	Table 1 shows the hierarchy that Sundance uses for semantic classes associated with opinion sources." ></td>
	<td class="line x" title="101:194	Sundance is also used to recognize and instantiate the source extraction patterns PROPER NAMEAUTHORITY LOCATION CITY COUNTRY PLANET PROVINCE PERSON NAME PERSON DESC NATIONALITY TITLE COMPANY GOVERNMENT MEDIA ORGANIZATION HUMAN SOURCE Figure 1: The semantic hierarchy for opinion sources that are learned by AutoSlog-SE, which is described in the next section." ></td>
	<td class="line x" title="102:194	4 Semantic Tagging via Extraction Patterns We also learn patterns to extract opinion sources using a statistical adaptation of the AutoSlog IE learning algorithm." ></td>
	<td class="line x" title="103:194	AutoSlog (Riloff, 1996a) is a supervised extraction pattern learner that takes a training corpus of texts and their associated answer keys as input." ></td>
	<td class="line x" title="104:194	A set of heuristics looks at the context surrounding each answer and proposes a lexicosyntactic pattern to extract that answer from the text." ></td>
	<td class="line x" title="105:194	The heuristics are not perfect, however, so the resulting set of patterns needs to be manually reviewed by a person." ></td>
	<td class="line x" title="106:194	In order to build a fully automatic system that does not depend on manual review, we combined AutoSlogs heuristics with statistics from the annotated training data to create a fully automatic supervised learner." ></td>
	<td class="line x" title="107:194	We will refer to this learner as AutoSlog-SE (Statistically Enhanced variation of AutoSlog)." ></td>
	<td class="line x" title="108:194	AutoSlog-SEs learning process has three steps: Step 1: AutoSlogs heuristics are applied to every noun phrase (NP) in the training corpus." ></td>
	<td class="line x" title="109:194	This generates a set of extraction patterns that, collectively, can extract every NP in the training corpus." ></td>
	<td class="line x" title="110:194	Step 2: The learned patterns are augmented with selectional restrictions that semantically constrain the types of noun phrases that are legitimate extractions for opinion sources." ></td>
	<td class="line x" title="111:194	We used 358 the semantic classes shown in Figure 1 as selectional restrictions." ></td>
	<td class="line x" title="112:194	Step 3: The patterns are applied to the training corpus and statistics are gathered about their extractions." ></td>
	<td class="line x" title="113:194	We count the number of extractions that match annotations in the corpus (correct extractions) and the number of extractions that do not match annotations (incorrect extractions)." ></td>
	<td class="line x" title="114:194	These counts are then used to estimate the probability that the pattern will extract an opinion source in new texts: P(source | patterni) = correct sourcescorrect sources + incorrect sources This learning process generates a set of extraction patterns coupled with probabilities." ></td>
	<td class="line x" title="115:194	In the next section, we explain how these extraction patterns are represented as features in the CRF model." ></td>
	<td class="line x" title="116:194	5 Extraction Pattern Features for the CRF The extraction patterns provide two kinds of information." ></td>
	<td class="line x" title="117:194	SourcePatt indicates whether a word activates any source extraction pattern." ></td>
	<td class="line x" title="118:194	For example, the word complained activates the pattern <subj> complained because it anchors the expression." ></td>
	<td class="line x" title="119:194	SourceExtrindicates whether a word is extracted by any source pattern." ></td>
	<td class="line x" title="120:194	For example, in the sentence President Jacques Chirac frequently complained about Frances economy, the words President, Jacques, and Chirac would all be extracted by the <subj> complained pattern." ></td>
	<td class="line x" title="121:194	Each extraction pattern has frequency and probability values produced by AutoSlog-SE, hence we create four IE pattern-based features for each token xi: SourcePatt-Freq, SourceExtr-Freq, SourcePatt-Prob, and SourceExtr-Prob, where the frequency values are divided into three ranges: f0, 1, 2+g and the probability values are divided into five ranges of equal size." ></td>
	<td class="line x" title="122:194	6 Experiments We used the Multi-Perspective Question Answering (MPQA) corpus4 for our experiments." ></td>
	<td class="line x" title="123:194	This corpus 4The MPQA corpus can be freely obtained at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="124:194	consists of 535 documents that have been manually annotated with opinion-related information including direct and indirect sources." ></td>
	<td class="line x" title="125:194	We used 135 documents as a tuning set for model development and feature engineering, and used the remaining 400 documents for evaluation, performing 10-fold cross validation." ></td>
	<td class="line x" title="126:194	These texts are English language versions of articles that come from many countries and cover many topics.5 We evaluate performance using 3 measures: overlap match (OL), head match (HM), and exact match (EM)." ></td>
	<td class="line x" title="127:194	OL is a lenient measure that considers an extraction to be correct if it overlaps with any of the annotated words." ></td>
	<td class="line x" title="128:194	HM is a more conservative measure that considers an extraction to be correct if its head matches the head of the annotated source." ></td>
	<td class="line x" title="129:194	We report these somewhat loose measures because the annotators vary in where they place the exact boundaries of a source." ></td>
	<td class="line x" title="130:194	EM is the strictest measure that requires an exact match between the extracted words and the annotated words." ></td>
	<td class="line x" title="131:194	We use three evaluation metrics: recall, precision, and F-measure with recall and precision equally weighted." ></td>
	<td class="line x" title="132:194	6.1 Baselines We developed three baseline systems to assess the difficulty of our task." ></td>
	<td class="line x" title="133:194	Baseline-1 labels as sources all phrases that belong to the semantic categories authority, government, human, media, organizationor company, proper name." ></td>
	<td class="line x" title="134:194	Table 1 shows that the precision is poor, suggesting that the third condition described in Section 3.1 (opinion recognition) does play an important role in source identification." ></td>
	<td class="line x" title="135:194	The recall is much higher but still limited due to sources that fall outside of the semantic categories or are not recognized as belonging to these categories." ></td>
	<td class="line x" title="136:194	Baseline-2 labels a noun phrase as a source if any of the following are true: (1) the NP is the subject of a verb phrase containing an opinion word, (2) the NP follows according to, (3) the NP contains a possessive and is preceded by an opinion word, or (4) the NP follows by and attaches to an opinion word." ></td>
	<td class="line x" title="137:194	Baseline-2s heuristics are designed to address the first and the third conditions in Section 3.1." ></td>
	<td class="line x" title="138:194	Table 1 shows that Baseline-2 is substantially better than Baseline-1." ></td>
	<td class="line x" title="139:194	Baseline-3 5This data was obtained from the Foreign Broadcast Information Service (FBIS), a U.S. government agency." ></td>
	<td class="line x" title="140:194	359 Recall Prec F1 OL 77.3 28.8 42.0 Baseline-1 HM 71.4 28.6 40.8 EM 65.4 20.9 31.7 OL 62.4 60.5 61.4 Baseline-2 HM 59.7 58.2 58.9 EM 50.8 48.9 49.8 OL 49.9 72.6 59.2 Baseline-3 HM 47.4 72.5 57.3 EM 44.3 58.2 50.3 OL 48.5 81.3 60.8 Extraction Patterns HM 46.9 78.5 58.7 EM 41.9 70.2 52.5 CRF: OL 56.1 81.0 66.3 basic features HM 55.1 79.2 65.0 EM 50.0 72.4 59.2 CRF: OL 59.1 82.4 68.9 basic + IE pattern HM 58.1 80.5 67.5 features EM 52.5 73.3 61.2 CRF-FI: OL 57.7 80.7 67.3 basic features HM 56.8 78.8 66.0 EM 51.7 72.4 60.3 CRF-FI: OL 60.6 81.2 69.4 basic + IE pattern HM 59.5 79.3 68.0 features EM 54.1 72.7 62.0 Table 1: Source identification performance table labels a noun phrase as a source if it satisfies both Baseline-1 and Baseline-2s conditions (this should satisfy all three conditions described in Section 3.1)." ></td>
	<td class="line x" title="141:194	As shown in Table 1, the precision of this approach is the best of the three baselines, but the recall is the lowest." ></td>
	<td class="line x" title="142:194	6.2 Extraction Pattern Experiment We evaluated the performance of the learned extraction patterns on the source identification task." ></td>
	<td class="line x" title="143:194	The learned patterns were applied to the test data and the extracted sources were scored against the manual annotations.6 Table 1 shows that the extraction patterns produced lower recall than the baselines, but with considerably higher precision." ></td>
	<td class="line x" title="144:194	These results show that the extraction patterns alone can identify 6These results were obtained using the patterns that had a probability >.50 and frequency > 1." ></td>
	<td class="line x" title="145:194	nearly half of the opinion sources with good accuracy." ></td>
	<td class="line x" title="146:194	6.3 CRF Experiments We developed our CRF model using the MALLET code from McCallum (2002)." ></td>
	<td class="line x" title="147:194	For training, we used a Gaussian prior of 0.25, selected based on the tuning data." ></td>
	<td class="line x" title="148:194	We evaluate the CRF using the basic features from Section 3, both with and without the IE pattern features from Section 5." ></td>
	<td class="line x" title="149:194	Table 1 shows that the CRF with basic features outperforms all of the baselines as well as the extraction patterns, achieving an F-measure of 66.3 using the OL measure, 65.0 using the HM measure, and 59.2 using the EM measure." ></td>
	<td class="line x" title="150:194	Adding the IE pattern features further increases performance, boosting recall by about 3 points for all of the measures and slightly increasing precision as well." ></td>
	<td class="line x" title="151:194	CRF with feature induction." ></td>
	<td class="line x" title="152:194	One limitation of log-linear function models like CRFs is that they cannot form a decision boundary from conjunctions of existing features, unless conjunctions are explicitly given as part of the feature vector." ></td>
	<td class="line x" title="153:194	For the task of identifying opinion sources, we observed that the model could benefit from conjunctive features." ></td>
	<td class="line x" title="154:194	For instance, instead of using two separate features, HUMAN and PARENT-CHUNK-INCLUDESOPINION-EXPRESSION, the conjunction of the two is more informative." ></td>
	<td class="line x" title="155:194	For this reason, we applied the CRF feature induction approach introduced by McCallum (2003)." ></td>
	<td class="line x" title="156:194	As shown in Table 1, where CRF-FI stands for the CRF model with feature induction, we see consistent improvements by automatically generating conjunctive features." ></td>
	<td class="line x" title="157:194	The final system, which combines the basic features, the IE pattern features, and feature induction achieves an F-measure of 69.4 (recall=60.6%, precision=81.2%) for the OL measure, an F-measure of 68.0 (recall=59.5%, precision=79.3%) for the HM measure, and an F-measure of 62.0 (recall=54.1%, precision=72.7%) for the EM measure." ></td>
	<td class="line x" title="158:194	6.4 Error Analysis An analysis of the errors indicated some common mistakes: Some errors resulted from error propagation in 360 our subsystems." ></td>
	<td class="line x" title="159:194	Errors from the sentence boundary detector in GATE (Cunningham et al. , 2002) were especially problematic because they caused the Collins parser to fail, resulting in no dependency tree information." ></td>
	<td class="line x" title="160:194	Some errors were due to complex and unusual sentence structure, which our rather simple feature encoding for CRF could not capture well." ></td>
	<td class="line x" title="161:194	Some errors were due to the limited coverage of the opinion lexicon." ></td>
	<td class="line x" title="162:194	We failed to recognize some cases when idiomatic or vague expressions were used to express opinions." ></td>
	<td class="line x" title="163:194	Below are some examples of errors that we found interesting." ></td>
	<td class="line x" title="164:194	Doubly underlined phrases indicate incorrectly extracted sources (either false positives or false negatives)." ></td>
	<td class="line x" title="165:194	Opinion words are singly underlined." ></td>
	<td class="line x" title="166:194	False positives: (1) Actually, these three countries do have one common denominator, i.e., that their values and policies do not agree with those of the United States and none of them are on good terms with the United States." ></td>
	<td class="line x" title="167:194	(2) Perhaps this is why Fidel Castro has not spoken out against what might go on in Guantanamo." ></td>
	<td class="line x" title="168:194	In (1), their values and policies seems like a reasonable phrase to extract, but the annotation does not mark this as a source, perhaps because it is somewhat abstract." ></td>
	<td class="line x" title="169:194	In (2), spoken out is negated, which means that the verb phrase does not bear an opinion, but our system failed to recognize the negation." ></td>
	<td class="line x" title="170:194	False negatives: (3) And for this reason, too, they have a moral duty to speak out, as Swedish Foreign Minister Anna Lindh, among others, did yesterday." ></td>
	<td class="line x" title="171:194	(4) In particular, Iran and Iraq are at loggerheads with each other to this day." ></td>
	<td class="line x" title="172:194	Example (3) involves a complex sentence structure that our system could not deal with." ></td>
	<td class="line x" title="173:194	(4) involves an uncommon opinion expression that our system did not recognize." ></td>
	<td class="line x" title="174:194	7 Related Work To our knowledge, our research is the first to automatically identify opinion sources using the MPQA opinion annotation scheme." ></td>
	<td class="line x" title="175:194	The most closely related work on opinion analysis is Bethard et al.(2004), who use machine learning techniques to identify propositional opinions and their holders (sources)." ></td>
	<td class="line x" title="177:194	However, their work is more limited in scope than ours in several ways." ></td>
	<td class="line x" title="178:194	Their work only addresses propositional opinions, which are localized in the propositional argument of certain verbs such as believe or realize." ></td>
	<td class="line x" title="179:194	In contrast, our work aims to find sources for all opinions, emotions, and sentiments, including those that are not related to a verb at all." ></td>
	<td class="line x" title="180:194	Furthermore, Berthard et al.s task definition only requires the identification of direct sources, while our task requires the identification of both direct and indirect sources." ></td>
	<td class="line x" title="181:194	Bethard et al. evaluate their system on manually annotated FrameNet (Baker et al. , 1998) and PropBank (Palmer et al. , 2005) sentences and achieve 48% recall with 57% precision." ></td>
	<td class="line x" title="182:194	Our IE pattern learner can be viewed as a cross between AutoSlog (Riloff, 1996a) and AutoSlogTS (Riloff, 1996b)." ></td>
	<td class="line x" title="183:194	AutoSlog is a supervised learner that requires annotated training data but does not compute statistics." ></td>
	<td class="line x" title="184:194	AutoSlog-TS is a weakly supervised learner that does not require annotated data but generates coarse statistics that measure each patterns correlation with relevant and irrelevant documents." ></td>
	<td class="line x" title="185:194	Consequently, the patterns learned by both AutoSlog and AutoSlog-TS need to be manually reviewed by a person to achieve good accuracy." ></td>
	<td class="line x" title="186:194	In contrast, our IE learner, AutoSlog-SE, computes statistics directly from the annotated training data, creating a fully automatic variation of AutoSlog." ></td>
	<td class="line x" title="187:194	8 Conclusion We have described a hybrid approach to the problem of extracting sources of opinions in text." ></td>
	<td class="line x" title="188:194	We cast this problem as an information extraction task, using both CRFs and extraction patterns." ></td>
	<td class="line x" title="189:194	Our research is the first to identify both direct and indirect sources for all types of opinions, emotions, and sentiments." ></td>
	<td class="line x" title="190:194	Directions for future work include trying to increase recall by identifying relationships between opinions and sources that cross sentence boundaries, and relationships between multiple opinion expressions by the same source." ></td>
	<td class="line x" title="191:194	For example, the fact that a coreferring noun phrase was marked as a source in one sentence could be a useful clue for extracting the source from another sentence." ></td>
	<td class="line x" title="192:194	The probability or the strength of an opinion expression may also play a useful role in encouraging or suppressing source extraction." ></td>
	<td class="line x" title="193:194	361 9 Acknowledgments We thank the reviewers for their many helpful comments, and the Cornell NLP group for their advice and suggestions for improvement." ></td>
	<td class="line x" title="194:194	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0208028 and IIS-0208985, and by the Xerox Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1071
KnowItNow: Fast, Scalable Information Extraction From The Web
Cafarella, Michael J.;Downey, Doug;Soderland, Stephen;Etzioni, Oren;"></td>
	<td class="line x" title="1:232	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 563570, Vancouver, October 2005." ></td>
	<td class="line x" title="2:232	c2005 Association for Computational Linguistics KnowItNow: Fast, Scalable Information Extraction from the Web Michael J. Cafarella, Doug Downey, Stephen Soderland, Oren Etzioni Department of Computer Science and Engineering University of Washington Seattle, WA 98195-2350 {mjc,ddowney,soderlan,etzioni}@cs.washington.edu Abstract Numerous NLP applications rely on search-engine queries, both to extract information from and to compute statistics over the Web corpus." ></td>
	<td class="line x" title="3:232	But search engines often limit the number of available queries." ></td>
	<td class="line x" title="4:232	As a result, query-intensive NLP applications such as Information Extraction (IE) distribute their query load over several days, making IE a slow, offline process." ></td>
	<td class="line x" title="5:232	This paper introduces a novel architecture for IE that obviates queries to commercial search engines." ></td>
	<td class="line x" title="6:232	The architecture is embodied in a system called KNOWITNOW that performs high-precision IE in minutes instead of days." ></td>
	<td class="line x" title="7:232	We compare KNOWITNOW experimentally with the previouslypublished KNOWITALL system, and quantify the tradeoff between recall and speed." ></td>
	<td class="line x" title="8:232	KNOWITNOWs extraction rate is two to three orders of magnitude higher than KNOWITALLs. 1 Background and Motivation Numerous modern NLP applications use the Web as their corpus and rely on queries to commercial search engines to support their computation (Turney, 2001; Etzioni et al. , 2005; Brill et al. , 2001)." ></td>
	<td class="line x" title="9:232	Search engines are extremely helpful for several linguistic tasks, such as computing usage statistics or nding a subset of web documents to analyze in depth; however, these engines were not designed as building blocks for NLP applications." ></td>
	<td class="line x" title="10:232	As a result, the applications are forced to issue literally millions of queries to search engines, which limits the speed, scope, and scalability of the applications." ></td>
	<td class="line x" title="11:232	Further, the applications must often then fetch some web documents, which at scale can be very time-consuming." ></td>
	<td class="line x" title="12:232	In response to heavy programmatic search engine use, Google has created the Google API to shunt programmatic queries away from Google.com and has placed hard quotas on the number of daily queries a program can issue to the API." ></td>
	<td class="line x" title="13:232	Other search engines have also introduced mechanisms to limit programmatic queries, forcing applications to introduce courtesy waits between queries and to limit the number of queries they issue." ></td>
	<td class="line x" title="14:232	To understand these ef ciency problems in more detail, consider the KNOWITALL information extraction system (Etzioni et al. , 2005)." ></td>
	<td class="line x" title="15:232	KNOWITALL has a generateand-test architecture that extracts information in two stages." ></td>
	<td class="line x" title="16:232	First, KNOWITALL utilizes a small set of domainindependent extraction patterns to generate candidate facts (cf.(Hearst, 1992))." ></td>
	<td class="line x" title="18:232	For example, the generic pattern NP1 such as NPList2 indicates that the head of each simple noun phrase (NP) in NPList2 is a member of the class named in NP1." ></td>
	<td class="line x" title="19:232	By instantiating the pattern for class City, KNOWITALL extracts three candidate cities from the sentence: We provide tours to cities such as Paris, London, and Berlin." ></td>
	<td class="line x" title="20:232	Note that it must also fetch each document that contains a potential candidate." ></td>
	<td class="line x" title="21:232	Next, extending the PMI-IR algorithm (Turney, 2001), KNOWITALL automatically tests the plausibility of the candidate facts it extracts using pointwise mutual information (PMI) statistics computed from search-engine hit counts." ></td>
	<td class="line x" title="22:232	For example, to assess the likelihood that Yakima is a city, KNOWITALL will compute the PMI between Yakima and a set of k discriminator phrases that tend to have high mutual information with city names (e.g. , the simple phrase city )." ></td>
	<td class="line x" title="23:232	Thus, KNOWITALL requires at least k search-engine queries for every candidate extraction it assesses." ></td>
	<td class="line x" title="24:232	Due to KNOWITALLs dependence on search-engine queries, large-scale experiments utilizing KNOWITALL take days and even weeks to complete, which makes research using KNOWITALL slow and cumbersome." ></td>
	<td class="line x" title="25:232	Private access to Google-scale infrastructure would provide 563 suf cient access to search queries, but at prohibitive cost, and the problem of fetching documents (even if from a cached copy) would remain (as we discuss in Section 2.1)." ></td>
	<td class="line x" title="26:232	Is there a feasible alternative Web-based IE system?" ></td>
	<td class="line x" title="27:232	If so, what size Web index and how many machines are required to achieve reasonable levels of precision/recall?" ></td>
	<td class="line x" title="28:232	What would the architecture of this IE system look like, and how fast would it run?" ></td>
	<td class="line x" title="29:232	To address these questions, this paper introduces a novel architecture for web information extraction." ></td>
	<td class="line x" title="30:232	It consists of two components that supplant the generateand-test mechanisms in KNOWITALL." ></td>
	<td class="line x" title="31:232	To generate extractions rapidly we utilize our own specialized search engine, called the Bindings Engine (or BE), which efciently returns bindings in response to variabilized queries." ></td>
	<td class="line x" title="32:232	For example, in response to the query Cities such as ProperNoun(Head(hNounPhrasei)), BE will return a list of proper nouns likely to be city names." ></td>
	<td class="line x" title="33:232	To assess these extractions, we use URNS, a combinatorial model, which estimates the probability that each extraction is correct without using any additional search engine queries.1 For further ef ciency, we introduce an approximation to URNS, based on frequency of extractions occurrence in the output of BE, and show that it achieves comparable precision/recall to URNS." ></td>
	<td class="line x" title="34:232	Our contributions are as follows: 1." ></td>
	<td class="line x" title="35:232	We present a novel architecture for Information Extraction (IE), embodied in the KNOWITNOW system, which does not depend on Web search-engine queries." ></td>
	<td class="line x" title="36:232	2." ></td>
	<td class="line x" title="37:232	We demonstrate experimentally that KNOWITNOW is the rst system able to extract tens of thousands of facts from the Web in minutes instead of days." ></td>
	<td class="line x" title="38:232	3." ></td>
	<td class="line x" title="39:232	We show that KNOWITNOWs extraction rate is two to three orders of magnitude greater than KNOWITALLs, but this increased ef ciency comes at the cost of reduced recall." ></td>
	<td class="line x" title="40:232	We quantify this tradeoff for KNOWITNOWs 60,000,000 page index and extrapolate how the tradeoff would change with larger indices." ></td>
	<td class="line x" title="41:232	Our recent work has described the BE search engine in detail (Cafarella and Etzioni, 2005), and also analyzed the URNS models ability to compute accurate probability estimates for extractions (Downey et al. , 2005)." ></td>
	<td class="line x" title="42:232	However, this is the rst paper to investigate the composition of these components to create a fast IE system, and to compare it experimentally to KNOWITALL in terms of time, 1In contrast, PMI-IR, which is built into KNOWITALL, requires multiple search engine queries to assess each potential extraction." ></td>
	<td class="line x" title="43:232	recall, precision, and extraction rate." ></td>
	<td class="line x" title="44:232	The frequencybased approximation to URNS and the demonstration of its success are also new." ></td>
	<td class="line x" title="45:232	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="46:232	Section 2 provides an overview of BEs design." ></td>
	<td class="line x" title="47:232	Section 3 describes the URNS model and introduces an efcient approximation to URNS that achieves similar precision/recall." ></td>
	<td class="line x" title="48:232	Section 4 presents experimental results." ></td>
	<td class="line x" title="49:232	We conclude with related and future work in Sections 5 and 6." ></td>
	<td class="line x" title="50:232	2 The Bindings Engine This section explains how relying on standard search engines leads to a bottleneck for NLP applications, and provides a brief overview of the Bindings Engine (BE) our solution to this problem." ></td>
	<td class="line x" title="51:232	A comprehensive description of BE appears in (Cafarella and Etzioni, 2005)." ></td>
	<td class="line x" title="52:232	Standard search engines are computationally expensive for IE and other NLP tasks." ></td>
	<td class="line x" title="53:232	IE systems issue multiple queries, downloading all pages that potentially match an extraction rule, and performing expensive processing on each page." ></td>
	<td class="line x" title="54:232	For example, such systems operate roughly as follows on the query ( cities such as hNounPhrasei ): 1." ></td>
	<td class="line x" title="55:232	Perform a traditional search engine query to nd all URLs containing the non-variable terms (e.g. , cities such as ) 2." ></td>
	<td class="line x" title="56:232	For each such URL: (a) obtain the document contents, (b) nd the searched-for terms ( cities such as ) in the document text, (c) run the noun phrase recognizer to determine whether text following cities such as satis es the linguistic type requirement, (d) and if so, return the string We can divide the algorithm into two stages: obtaining the list of URLs from a search engine, and then processing them to nd the hNounPhrasei bindings." ></td>
	<td class="line x" title="57:232	Each stage poses its own scalability and speed challenges." ></td>
	<td class="line x" title="58:232	The rst stage makes a query to a commercial search engine; while the number of available queries may be limited, a single one executes relatively quickly." ></td>
	<td class="line x" title="59:232	The second stage fetches a large number of documents, each fetch likely resulting in a random disk seek; this stage executes slowly." ></td>
	<td class="line x" title="60:232	Naturally, this disk access is slow regardless of whether it happens on a locally-cached copy or on a remote document server." ></td>
	<td class="line x" title="61:232	The observation that the second stage is slow, even if it is executed locally, is important because it shows that merely operating a private search engine does not solve the problem (see Section 2.1)." ></td>
	<td class="line x" title="62:232	The Bindings Engine supports queries containing typed variables (such as NounPhrase) and 564 string-processing functions (such as head(X) or ProperNoun(X) ) as well as standard query terms." ></td>
	<td class="line x" title="63:232	BE processes a variable by returning every possible string in the corpus that has a matching type, and that can be substituted for the variable and still satisfy the users query." ></td>
	<td class="line x" title="64:232	If there are multiple variables in a query, then all of them must simultaneously have valid substitutions." ></td>
	<td class="line x" title="65:232	(So, for example, the query <NounPhrase> is located in <NounPhrase> only returns strings when noun phrases are found on both sides of is located in )." ></td>
	<td class="line x" title="66:232	We call a string that meets these requirements a binding for the variable in question." ></td>
	<td class="line x" title="67:232	These queries, and the bindings they elicit, can usefully serve as part of an information extraction system or other common NLP tasks (such as gathering usage statistics)." ></td>
	<td class="line x" title="68:232	Figure 1 illustrates some of the queries that BE can handle." ></td>
	<td class="line x" title="69:232	president Bush <Verb> cities such as ProperNoun(Head(<NounPhrase>)) <NounPhrase> is the CEO of <NounPhrase> Figure 1: Examples of queries that can be handled by BE." ></td>
	<td class="line x" title="70:232	Queries that include typed variables and stringprocessing functions allow NLP tasks to be done efciently without downloading the original document during query processing." ></td>
	<td class="line x" title="71:232	BEs novel neighborhood index enables it to process these queries with O(k) random disk seeks and O(k) serial disk reads, where k is the number of non-variable terms in its query." ></td>
	<td class="line x" title="72:232	As a result, BE can yield orders of magnitude speedup as shown in the asymptotic analysis later in this section." ></td>
	<td class="line x" title="73:232	The neighborhood index is an augmented inverted index structure." ></td>
	<td class="line x" title="74:232	For each term in the corpus, the index keeps a list of documents in which the term appears and a list of positions where the term occurs, just as in a standard inverted index (Baeza-Yates and RibeiroNeto, 1999)." ></td>
	<td class="line x" title="75:232	In addition, the neighborhood index keeps a list of left-hand and right-hand neighbors at each position." ></td>
	<td class="line x" title="76:232	These are adjacent text strings that satisfy a recognizer for one of the target types, such as NounPhrase." ></td>
	<td class="line x" title="77:232	As with a standard inverted index, a terms list is processed from start to nish, and can be kept on disk as a contiguous piece." ></td>
	<td class="line x" title="78:232	The relevant string for a variable binding is included directly in the index, so there is no need to fetch the source document (thus causing a disk seek)." ></td>
	<td class="line x" title="79:232	Expensive processing such as part-of-speech tagging or shallow syntactic parsing is performed only once, while building the index, and is not needed at query time." ></td>
	<td class="line x" title="80:232	It is important to note that simply preprocessing the corpus and placing the results in a database would not avoid disk seeks, as we would still have to explicitly fetch these results." ></td>
	<td class="line x" title="81:232	The run-time ef ciency of the neighborhood index Query Time Index Space BE O(k) O(N) Standard engine O(k + B) O(N) Table 1: BE yields considerable savings in query time over a standard search engine." ></td>
	<td class="line x" title="82:232	k is the number of concrete terms in the query, B is the number of variable bindings found in the corpus, and N is the number of documents in the corpus." ></td>
	<td class="line x" title="83:232	N and B are typically extremely large, while k is small." ></td>
	<td class="line x" title="84:232	comes from integrating the results of corpus processing with the inverted index (which determines which of those results are relevant)." ></td>
	<td class="line x" title="85:232	The neighborhood index avoids the need to return to the original corpus, but it can consume a large amount of disk space, as parts of the corpus text are folded into the index several times." ></td>
	<td class="line x" title="86:232	To conserve space, we perform simple dictionary-lookup compression of strings in the index." ></td>
	<td class="line x" title="87:232	The storage penalty will, of course, depend on the exact number of different types added to the index." ></td>
	<td class="line x" title="88:232	In our experiments, we created a useful IE system with a small number of types (including NounPhrase) and found that the neighborhood index increased disk space only four times that of a standard inverted index." ></td>
	<td class="line x" title="89:232	Asymptotic Analysis: In our asymptotic analysis of BEs behavior, we count query time as a function of the number of random disk seeks, since these seeks dominate all other processing tasks." ></td>
	<td class="line x" title="90:232	Index space is simply the number of bytes needed to store the index (not including the corpus itself)." ></td>
	<td class="line x" title="91:232	Table 1 shows that BE requires only O(k) random disk seeks to process queries with an arbitrary number of variables whereas a standard engine takes O(k + B), where k is the number of concrete query terms, and B is the number of bindings found in a corpus of N documents." ></td>
	<td class="line x" title="92:232	Thus, BEs performance is the same as that of a standard search engine for queries containing only concrete terms." ></td>
	<td class="line x" title="93:232	For variabilized queries, N may be in the billions and B will tend to grow with N. In our experiments, eliminating the B term from our query processing time has resulted in speedups of two to three orders of magnitude over a standard search engine." ></td>
	<td class="line x" title="94:232	The speedup is at the price of a small constant multiplier to index size." ></td>
	<td class="line x" title="95:232	2.1 Discussion While BE has some attractive properties for NLP computations, is it necessary?" ></td>
	<td class="line x" title="96:232	Could fast, large-scale information extraction be achieved merely by operating a private search engine?" ></td>
	<td class="line x" title="97:232	The release of open-source search engines such as Nutch2, coupled with the dropping price of CPUs and 2http://lucene.apache.org/nutch/ 565 8.16 0.06 0 1 2 3 4 5 6 7 8 9 10 BE Nutch E l a p s e d m i n u t e s Figure 2: Average time to return the relevant bindings in response to a set of queries was 0.06 CPU minutes for BE, compared to 8.16 CPU minutes for the comparable processing on Nutch." ></td>
	<td class="line x" title="98:232	This is a 134-fold speed up." ></td>
	<td class="line x" title="99:232	The CPU resources, network, and index size were the same for both systems." ></td>
	<td class="line x" title="100:232	disks, makes it feasible for NLP researchers to operate their own large-scale search engines." ></td>
	<td class="line x" title="101:232	For example, Turney operates a search engine with a terabyte-sized index of Web pages, running on a local eight-machine Beowulf cluster (Turney, 2004)." ></td>
	<td class="line x" title="102:232	Private search engines have two advantages." ></td>
	<td class="line x" title="103:232	First, there is no query quota or need for courtesy waits between queries." ></td>
	<td class="line x" title="104:232	Second, since the engine is local, network latency is minimal." ></td>
	<td class="line x" title="105:232	However, to support IE, we must also execute the second stage of the algorithm (see the beginning of this section)." ></td>
	<td class="line x" title="106:232	In this stage, each document that matches a query has to be retrieved from an arbitrary location on a disk.3 Thus, the number of random disk seeks scales linearly with the number of documents retrieved." ></td>
	<td class="line x" title="107:232	Moreover, many NLP applications require the extraction of strings matching particular syntactic or semantic types from each page." ></td>
	<td class="line x" title="108:232	The lack of linguistic data in the search engines index means that many pages are fetched only to be discarded as irrelevant." ></td>
	<td class="line x" title="109:232	To quantify the speedup due to BE, we compared it to a standard search index built on the open-source Nutch engine." ></td>
	<td class="line x" title="110:232	All of our Nutch and BE experiments were carried out on the same corpus of 60 million Web pages and were run on a cluster of 23 dual-Xeon machines, each with two local 140 Gb disks and 4 Gb of RAM." ></td>
	<td class="line x" title="111:232	We set all con guration values to be exactly the same for both Nutch and BE." ></td>
	<td class="line x" title="112:232	BE gave a 134-fold speed up on average query processing time when compared to the same queries with the Nutch index, as shown in Figure 2." ></td>
	<td class="line x" title="113:232	3Moving the disk head to an arbitrary location on the disk is a mechanical operation that takes about 5 milliseconds on average." ></td>
	<td class="line x" title="114:232	3 The URNS Model To realize the speedup from BE, KNOWITNOW must also avoid issuing search engine queries to validate the correctness of each extraction, as required by PMI computation." ></td>
	<td class="line x" title="115:232	We have developed a probabilistic model obviating search-engine queries for assessment." ></td>
	<td class="line x" title="116:232	The intuition behind this model is that correct instances of a class or relation are likely to be extracted repeatedly, while random errors by an IE system tend to have low frequency for each distinct incorrect extraction." ></td>
	<td class="line x" title="117:232	Our probabilistic model, which we call URNS, takes the form of a classic balls-and-urns model from combinatorics." ></td>
	<td class="line x" title="118:232	We think of IE abstractly as a generative process that maps text to extractions." ></td>
	<td class="line x" title="119:232	Each extraction is modeled as a labeled ball in an urn." ></td>
	<td class="line x" title="120:232	A label represents either an instance of the target class or relation, or represents an error." ></td>
	<td class="line x" title="121:232	The information extraction process is modeled as repeated draws from the urn, with replacement." ></td>
	<td class="line x" title="122:232	Formally, the parameters that characterize an urn are: C the set of unique target labels; jCj is the number of unique target labels in the urn." ></td>
	<td class="line x" title="123:232	E the set of unique error labels; jEj is the number of unique error labels in the urn." ></td>
	<td class="line x" title="124:232	num(b) the function giving the number of balls labeled by b where b 2 C [ E. num(B) is the multi-set giving the number of balls for each label b 2 B. The goal of an IE system is to discern which of the labels it extracts are in fact elements of C, based on repeated draws from the urn." ></td>
	<td class="line x" title="125:232	Thus, the central question we are investigating is: given that a particular label x was extracted k times in a set of n draws from the urn, what is the probability that x 2 C?" ></td>
	<td class="line x" title="126:232	We can express the probability that an element extracted k of n times is of the target relation as follows." ></td>
	<td class="line x" title="127:232	P(x 2 Cjx appearsk times in n draws) =summationtext rnum(C)( r s) k(1 r s) nk summationtext rprimenum(CE)( rprime s )k(1 rprime s )nk (1) where s is the total number of balls in the urn, and the sum is taken over possible repetition rates r. A few numerical examples illustrate the behavior of this equation." ></td>
	<td class="line x" title="128:232	Let jCj = jEj = 2, 000 and assume for simplicity that all labels are repeated on the same number of balls (num(ci) = RC for all ci 2 C, and num(ei) = RE for all ei 2 E)." ></td>
	<td class="line x" title="129:232	Assume that the extraction rules have precision p = 0.9, which means that RC = 9 RE target balls are nine times as common in the urn as error balls." ></td>
	<td class="line x" title="130:232	Now, for k = 3 and n = 10, 000 we have P(x 2 C) = 93.0%." ></td>
	<td class="line x" title="131:232	Thus, we see that a small number of repetitions can yield high con dence in an extraction." ></td>
	<td class="line x" title="132:232	However, when the sample size increases so that 566 n = 20, 000, and the other parameters are unchanged, then P(x 2 C) drops to 19.6%." ></td>
	<td class="line x" title="133:232	On the other hand, if C balls repeat much more frequently than E balls, say RC = 90 RE (with jEj set to 20,000, so that p remains unchanged), then P(x 2 C) rises to 99.9%." ></td>
	<td class="line x" title="134:232	The above examples enable us to illustrate the advantages of URNS over the noisy-or model used in previous work." ></td>
	<td class="line x" title="135:232	The noisy-or model assumes that each extraction is an independent assertion that the extracted label is true, an assertion that is correct a fraction p of the time." ></td>
	<td class="line x" title="136:232	The noisy-or model assigns the following probability to extractions: Pnoisyor(x 2 Cjx appearsk times) = 1 (1 p)k Therefore, the noisy-or model will assign the same probability 99.9% in all three of the above examples, although this is only correct in the case for which n = 10, 000 and RC = 90 RE." ></td>
	<td class="line x" title="137:232	As the other two examples show, for different sample sizes or repetition rates, the noisy-or model can be highly inaccurate." ></td>
	<td class="line x" title="138:232	This is not surprising given that the noisy-or model ignores the sample size and the repetition rates." ></td>
	<td class="line x" title="139:232	URNS uses an EM algorithm to estimate its parameters, and currently the algorithm takes roughly three minutes to terminate.4 Fortunately, we determined experimentally that we can approximate URNSs precision and recall using a far simpler frequency-based assessment method." ></td>
	<td class="line x" title="140:232	This is true because good precision and recall merely require an appropriate ordering of the extractions for each relation, and not accurate probabilities for each extraction." ></td>
	<td class="line x" title="141:232	For unary relations, we use the simple approximation that items extracted more often are more likely to be true, and order the extractions from most to least extracted." ></td>
	<td class="line x" title="142:232	For binary relations like CapitalOf(X,y), in which we extract several different candidate capitals y for each known country X, we use a smoothed frequency estimate to order the extractions." ></td>
	<td class="line x" title="143:232	Let freq(R(X,y)) denote the number of times that the binary relation R(X,y) is extracted; we de ne: smoothed freq(R(X,y)) = freq(R(X,y))max yprime freq(R(X,yprime)) + 1 We found that sorting by smoothed frequency (in descending order) performed better than simply sorting by freq for relations R(X,y) in which different known X values may have widely varying Web presence." ></td>
	<td class="line x" title="144:232	Unlike URNS, our frequency-based assessment does not yield accurate probabilities to associate with each extraction, but for the purpose of returning a ranked list of high-quality extractions it is comparable to URNS (see 4This code has not been optimized at all." ></td>
	<td class="line x" title="145:232	We believe that we can easily reduce its running time to less than a minute on average, and perhaps substantially more." ></td>
	<td class="line x" title="146:232	0.75 0.8 0.85 0.9 0.95 1 0 50 100 150 200 250 Correct Extractions P r e c i s i o n KnowItNow-freq KnowItNow-URNS KnowItAll-PMI Figure 3: Country: KNOWITALL maintains somewhat higher precision than KNOWITNOW throughout the recall-precision curve." ></td>
	<td class="line x" title="147:232	Figures 3 through 6), and it has the advantage of being much faster." ></td>
	<td class="line x" title="148:232	Thus, in the experiments reported on below, we use frequency-based assessment as part of KNOWITNOW." ></td>
	<td class="line x" title="149:232	4 Experimental Results This section contrasts the performance of KNOWITNOW and KNOWITALL experimentally." ></td>
	<td class="line x" title="150:232	Before considering the experiments in detail, we note that a key advantage of KNOWITNOW is that it does not make any queries to Web search engines." ></td>
	<td class="line x" title="151:232	As a result, KNOWITNOWs scale is not limited by a query quota, though it is limited by the size of its index." ></td>
	<td class="line x" title="152:232	We report on the following metrics: Recall: how many distinct extractions does each system return at high precision?5 Time: how long did each system take to produce and rank its extractions?" ></td>
	<td class="line x" title="153:232	Extraction Rate: how many distinct high-quality extractions does the system return per minute?" ></td>
	<td class="line x" title="154:232	The extraction rate is simply recall divided by time." ></td>
	<td class="line x" title="155:232	We contrast KNOWITALL and KNOWITNOWs precision/recall curves in Figures 3 through 6." ></td>
	<td class="line x" title="156:232	We compared KNOWITNOW with KNOWITALL on four relations: Corp, Country, CeoOf(Corp,Ceo), and CapitalOf(Country,City)." ></td>
	<td class="line x" title="157:232	The unary relations were chosen to examine the difference between a relation with a small number of correct instances (Country) and one with a large number of extractions (Corp)." ></td>
	<td class="line x" title="158:232	The binary relations were chosen to cover both functional relations (CapitalOf) and set-valued relations (CeoOf we treat former CEOs as correct instances of the relation)." ></td>
	<td class="line x" title="159:232	5Since we cannot compute true recall for most relations on the Web, the paper uses the term recall to refer to the size of the set of facts extracted." ></td>
	<td class="line x" title="160:232	567 0.75 0.8 0.85 0.9 0.95 1 0 50 100 150 200 Correct Extractions P r e c i s i o n KnowItNow-freq KnowItNow-URNS KnowItAll-PMI Figure 4: CapitalOf: KNOWITNOW does nearly as well as KNOWITALL, but has more dif culty than KNOWITALL with sparse data for capitals of more obscure countries." ></td>
	<td class="line x" title="161:232	For the two unary relations, both systems created extraction rules from eight generic patterns." ></td>
	<td class="line x" title="162:232	These are hyponym patterns like NP1 f,g such as NPList2 or NP2 f,g and other NP1, which extract members of NPList2 or NP2 as instances of NP1." ></td>
	<td class="line x" title="163:232	For the binary relations, the systems instantiated rules from four generic patterns." ></td>
	<td class="line x" title="164:232	These are patterns for a generic of relation." ></td>
	<td class="line x" title="165:232	They are NP1, rel of NP2, NP1 the rel of NP2, rel of NP2, NP1, and NP2 rel NP1." ></td>
	<td class="line x" title="166:232	When rel is instantiated for CeoOf, these patterns become NP1, CEO of NP2 and so forth." ></td>
	<td class="line x" title="167:232	Both KNOWITNOW and KNOWITALL merge extractions with slight variants in the name, such as those differing only in punctuation or whitespace, or in the presence or absence of a corporate designator." ></td>
	<td class="line x" title="168:232	For binary extractions, CEOs with the same last name and same company were also merged." ></td>
	<td class="line x" title="169:232	Both systems rely on the OpenNlp maximum-entropy part-of-speech tagger and chunker (Ratnaparkhi, 1996), but KNOWITALL applies them to pages downloaded from the Web based on the results of Google queries, whereas KNOWITNOW applies them once to crawled and indexed pages.6 Overall, each of the above elements of KNOWITALL and KNOWITNOW are the same to allow for controlled experiments." ></td>
	<td class="line x" title="170:232	Whereas KNOWITNOW runs a small number of variabilized queries (one for each extraction pattern, for each relation), KNOWITALL requires a stopping criterion." ></td>
	<td class="line x" title="171:232	Otherwise, KNOWITALL will continue to query Google and download URLs found in its result pages over many days and even weeks." ></td>
	<td class="line x" title="172:232	We allowed a total of 6 days of search time for KNOWITALL, allocating more search for the relations that continued to be most productive." ></td>
	<td class="line x" title="173:232	For CeoOf KNOWITNOW returned all pairs of Corp,Ceo 6Our time measurements for KNOWITALL are not affected by the tagging and chunking time because it is dominated by time required to query Google, waiting a second between queries." ></td>
	<td class="line x" title="174:232	0.75 0.8 0.85 0.9 0.95 1 0 5,000 10,000 15,000 20,000 25,000 Correct Extractions P r e c i s i o n KnowItNow-freq KnowItNow-URNS KnowItAll-PMI Figure 5: Corp: KNOWITALLs PMI assessment maintains high precision." ></td>
	<td class="line x" title="175:232	KNOWITNOW has low recall up to precision 0.85, then catches up with KNOWITALL." ></td>
	<td class="line x" title="176:232	in its corpus; KNOWITALL searched for CEOs of a random selection of 10% of the corporations it found, and we projected the total extractions and search effort for all corporations." ></td>
	<td class="line x" title="177:232	For CapitalOf, both KNOWITNOW and KNOWITALL looked for capitals of a set of 195 countries." ></td>
	<td class="line x" title="178:232	Table 2 shows the number of queries, search time, distinct correct extractions at precision 0.8, and extraction rate for each relation." ></td>
	<td class="line x" title="179:232	Search time for KNOWITNOW is measured in seconds and search time for KNOWITALL is measured in hours." ></td>
	<td class="line x" title="180:232	The number of extractions per minute counts the distinct correct extractions." ></td>
	<td class="line x" title="181:232	Since we limit KNOWITALL to one Google query per second, the time for KNOWITALL is proportional to the number of queries." ></td>
	<td class="line x" title="182:232	KNOWITNOWs extraction rate is from 275 to 4,707 times that of KNOWITALL at this level of precision." ></td>
	<td class="line x" title="183:232	While the number of distinct correct extractions from KNOWITNOW at precision 0.8 is roughly comparable to that of 6 days search effort from KNOWITALL, the situation is different at precision 0.9." ></td>
	<td class="line x" title="184:232	KNOWITALLs PMI assessor is able to maintain higher precision than KNOWITNOWs frequency-based assessor." ></td>
	<td class="line x" title="185:232	The number of correct corporations for KNOWITNOW drops from 23,128 at precision 0.8 to 1,116 at precision 0.9." ></td>
	<td class="line x" title="186:232	KNOWITALL is able to identify 17,620 correct corporations at precision 0.9." ></td>
	<td class="line x" title="187:232	Even with the drop in recall, KNOWITNOWs extraction rate is still 305 times higher than KNOWITALLs. The reason for KNOWITNOWs dif culty at precision 0.9 is due to extraction errors that occur with high frequency, particularly generic references to companies ( the Seller is a corporation  , corporations such as Banks, etc.) and truncation of certain company names by the extraction rules." ></td>
	<td class="line x" title="188:232	The more expensive PMI-based assessment was not fooled by these systematic extraction errors." ></td>
	<td class="line x" title="189:232	Figures 3 through 6 show the recall-precision curves for KNOWITNOW with URNS assessment, KNOWITNOW with the simpler frequency-based assessment, and 568 Google Queries Time Extractions Extractions per minute NOW ALL NOW (sec) ALL (hrs) NOW ALL NOW ALL ratio Corp 0 (16) 201,878 42 56.1 23,128 23,617 33,040 7.02 4,707 Country 0 (16) 35,480 42 9.9 161 203 230 0.34 672 CeoOf 0 (6) 263,646 51 73.2 2,402 5,823 2,836 1.33 2,132 CapitalOf 0 (6) 17,216 55 4.8 169 192 184 0.67 275 Table 2: Comparison of KNOWITNOW with KNOWITALL for four relations, showing number of Google queries (local BE queries in parentheses), search time, correct extractions at precision 0.8, and extraction rate (the number of correct extractions at precision 0.8 per minute of search)." ></td>
	<td class="line x" title="190:232	Overall, KNOWITNOW took a total of slightly over 3 minutes as compared to a total of 6 days of search for KNOWITALL." ></td>
	<td class="line x" title="191:232	0.75 0.8 0.85 0.9 0.95 1 0 2,000 4,000 6,000 Correct Extractions P r e c i s i o n KnowItNow-freq KnowitNow-URNS KnowItAll-PMI Figure 6: CeoOf: KNOWITNOW has dif culty distinguishing low frequency correct extractions from noise." ></td>
	<td class="line x" title="192:232	KNOWITALL is able to cope with the sparse data more effectively." ></td>
	<td class="line x" title="193:232	KNOWITALL with PMI-based assessment." ></td>
	<td class="line x" title="194:232	For each of the four relations, PMI is able to maintain a higher precision than either frequency-based or URNS assessment." ></td>
	<td class="line x" title="195:232	URNS and frequency-based assessment give roughly the same levels of precision." ></td>
	<td class="line x" title="196:232	For the relations with a small number of correct instances, Country and CapitalOf, KNOWITNOW is able to identify 70-80% as many instances as KNOWITALL at precision 0.9." ></td>
	<td class="line x" title="197:232	In contrast, Corp and CeoOf have a huge number of correct instances and a long tail of low frequency extractions that KNOWITNOW has difculty distinguishing from noise." ></td>
	<td class="line x" title="198:232	Over one fourth of the corporations found by KNOWITALL had Google hit counts less than 10,500, a sparseness problem that was exacerbated by KNOWITNOWs limited index size." ></td>
	<td class="line x" title="199:232	Figure 7 shows projected recall from larger KNOWITNOW indices, tting a sigmoid curve to the recall from index size of 10M, 20M, up to 60M pages." ></td>
	<td class="line x" title="200:232	The curve was tted using logistic regression, and is restricted to asymptote at the level reported for Google-based KNOWITALL for each relation." ></td>
	<td class="line x" title="201:232	We report recall at precision 0.9 for capitals of 195 countries and CEOs of a random selection of the top 5,000 corporations as ranked by PMI." ></td>
	<td class="line x" title="202:232	Recall is de ned as the percent of countries with a 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 KnowItNow index size (millions) R e c a l l a t 0 . 9 p r e c i s i o n KnowItNow CeoOf Google CeoOf KnowItNow CapitalOf Google CapitalOf Figure 7: Projections of recall (at precision 0.9) as a function of KNOWITNOW index size." ></td>
	<td class="line x" title="203:232	At 400 million pages, KNOWITNOWs recall rapidly approaches the recall achieved by KNOWITALL using roughly 300,000 Google queries." ></td>
	<td class="line x" title="204:232	correct capital or the number of correct CEOs divided by the number of corporations." ></td>
	<td class="line x" title="205:232	The curve for CeoOf is rising steeply enough that a 400 million page KNOWITNOW index may approach the same level of recall yielded by KNOWITALL when it uses 300,000 Google queries." ></td>
	<td class="line x" title="206:232	As shown in Table 2, KNOWITALL takes slightly more than three days to generate these results." ></td>
	<td class="line x" title="207:232	KNOWITNOW would operate over a corpus 6.7 times its current one, but the number of required random disk seeks (and the asymptotic run time analysis) would remain the same." ></td>
	<td class="line x" title="208:232	We thus expect that with a larger corpus we can construct a KNOWITNOW system that reproduces KNOWITALL levels of precision and recall while still executing in the order of a few minutes." ></td>
	<td class="line x" title="209:232	5 Related Work There has been very little work published on how to make NLP computations such as PMI-IR and IE fast for large corpora." ></td>
	<td class="line x" title="210:232	Indeed, extraction rate is not a metric typically used to evaluate IE systems, but we believe it is an important metric if IE is to scale." ></td>
	<td class="line x" title="211:232	Hobbs et al. point out the advantage of fast text processing for rapid system development (Hobbs et al. , 1992)." ></td>
	<td class="line x" title="212:232	They could test each change to system parameters 569 and domain-speci c patterns on a large sample of documents, having moved from a system that took 36 hours to process 100 documents to FASTUS, which took only 11 minutes." ></td>
	<td class="line x" title="213:232	This allowed them to develop one of the highest performing MUC-4 systems in only one month." ></td>
	<td class="line x" title="214:232	While there has been extensive work in the IR and Web communities on improvements to the standard inverted index scheme, there has been little work on ef cient large-scale search to support natural language applications." ></td>
	<td class="line x" title="215:232	One exception is Resniks Linguists Search Engine (Elkiss and Resnik, 2004), a tool for searching large corpora of parse trees." ></td>
	<td class="line x" title="216:232	There is little published information about its indexing system, but the user manual suggests its corpus is a combination of indexed sentences and user-speci c document collections driven by the users AltaVista queries." ></td>
	<td class="line x" title="217:232	In contrast, the BE system has a single index, constructed just once, that serves all queries." ></td>
	<td class="line x" title="218:232	There is no published performance data available for Resniks system." ></td>
	<td class="line oc" title="219:232	6 Conclusions and Future Directions In previous work, statistical NLP computation over large corpora has been a slow, of ine process, as in KNOWITALL (Etzioni et al. , 2005) and also in PMI-IR applications such as sentiment classi cation (Turney, 2002)." ></td>
	<td class="line x" title="220:232	Technology trends, and open source search engines such as Nutch, have made it feasible to create private search engines that index large collections of documents; but as shown in Figure 2, ring large numbers of queries at private search engines is still slow." ></td>
	<td class="line x" title="221:232	This paper described a novel and practical approach towards substantially speeding up IE." ></td>
	<td class="line x" title="222:232	We described KNOWITNOW, which extracts thousands of facts in minutes instead of days." ></td>
	<td class="line x" title="223:232	Furthermore, we sketched URNS, a probabilistic model that both obviates the need for search-engine queries and outputs more accurate probabilities than PMI-IR." ></td>
	<td class="line x" title="224:232	Finally, we introduced a simple, ef cient approximation to URNS, whose probability estimates are not as good, but which has comparable precision/recall to URNS, making it an appropriate assessor for KNOWITNOW." ></td>
	<td class="line x" title="225:232	The speed and massively improved extraction rate of KNOWITNOW come at the cost of reduced recall." ></td>
	<td class="line x" title="226:232	We quanti ed this tradeoff in Table 2, and also argued that as KNOWITNOWs index size increases from 60 million to 400 million pages, KNOWITNOW would achieve in minutes the same precision/recall that takes KNOWITALL days to obtain." ></td>
	<td class="line x" title="227:232	Of course, a hybrid approach is possible where KNOWITNOW has, say, a 100 million page index and, when necessary, augments its results with a limited number of queries to Google." ></td>
	<td class="line x" title="228:232	Investigating the extraction-rate/recall tradeoff in such a hybrid system is a natural next step." ></td>
	<td class="line x" title="229:232	While our experiments have used the Web corpus, our approach transfers readily to other large corpora; experimentation with other corpora is another topic for future work." ></td>
	<td class="line x" title="230:232	In conclusion, we believe that our techniques transform IE from a slow, of ine process to an online one." ></td>
	<td class="line x" title="231:232	They could open the door to a new class of interactive IE applications, of which KNOWITNOW is merely the rst." ></td>
	<td class="line x" title="232:232	7 Acknowledgments This research was supported in part by NSF grant IIS0312988, DARPA contract NBCHD030010, ONR grant N00014-02-1-0324, and gifts from Google and the Turing Center." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1073
Emotions From Text: Machine Learning For Text-Based Emotion Prediction
Alm, Cecilia Ovesdotter;Roth, Dan;Sproat, Richard W.;"></td>
	<td class="line x" title="1:192	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 579586, Vancouver, October 2005." ></td>
	<td class="line x" title="2:192	c2005 Association for Computational Linguistics Emotions from text: machine learning for text-based emotion prediction Cecilia Ovesdotter Alm Dept. of Linguistics UIUC Illinois, USA ebbaalm@uiuc.edu Dan Roth Dept. of Computer Science UIUC Illinois, USA danr@uiuc.edu Richard Sproat Dept. of Linguistics Dept. of Electrical Eng." ></td>
	<td class="line x" title="3:192	UIUC Illinois, USA rws@uiuc.edu Abstract In addition to information, text contains attitudinal, and more specifically, emotional content." ></td>
	<td class="line x" title="4:192	This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture." ></td>
	<td class="line x" title="5:192	The goal is to classify the emotional affinity of sentences in the narrative domain of childrens fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis." ></td>
	<td class="line x" title="6:192	Initial experiments on a preliminary data set of 22 fairy tales show encouraging results over a nave baseline and BOW approach for classification of emotional versus non-emotional contents, with some dependency on parameter tuning." ></td>
	<td class="line x" title="7:192	We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations." ></td>
	<td class="line x" title="8:192	In addition, we present plans for a more cognitively sound sequential model, taking into consideration a larger set of basic emotions." ></td>
	<td class="line x" title="9:192	1 Introduction Text does not only communicate informative contents, but also attitudinal information, including emotional states." ></td>
	<td class="line x" title="10:192	The following reports on an empirical study of text-based emotion prediction." ></td>
	<td class="line x" title="11:192	Section 2 gives a brief overview of the intended application area, whereas section 3 summarizes related work." ></td>
	<td class="line x" title="12:192	Next, section 4 explains the empirical study, including the machine learning model, the corpus, the feature set, parameter tuning, etc. Section 5 presents experimental results from two classification tasks and feature set modifications." ></td>
	<td class="line x" title="13:192	Section 6 describes the agenda for refining the model, before presenting concluding remarks in 7." ></td>
	<td class="line x" title="14:192	2 Application area: Text-to-speech Narrative text is often especially prone to having emotional contents." ></td>
	<td class="line x" title="15:192	In the literary genre of fairy tales, emotions such as HAPPINESS and ANGER and related cognitive states, e.g. LOVE or HATE, become integral parts of the story plot, and thus are of particular importance." ></td>
	<td class="line x" title="16:192	Moreover, the story teller reading the story interprets emotions in order to orally convey the story in a fashion which makes the story come alive and catches the listeners attention." ></td>
	<td class="line x" title="17:192	In speech, speakers effectively express emotions by modifying prosody, including pitch, intensity, and durational cues in the speech signal." ></td>
	<td class="line x" title="18:192	Thus, in order to make text-to-speech synthesis sound as natural and engaging as possible, it is important to convey the emotional stance in the text." ></td>
	<td class="line x" title="19:192	However, this implies first having identified the appropriate emotional meaning of the corresponding text passage." ></td>
	<td class="line x" title="20:192	Thus, an application for emotional text-to-speech synthesis has to solve two basic problems." ></td>
	<td class="line x" title="21:192	First, what emotion or emotions most appropriately describe a certain text passage, and second, given a text passage and a specified emotional mark-up, how to render the prosodic contour in order to convey the emotional content, (Cahn, 1990)." ></td>
	<td class="line x" title="22:192	The text-based emotion prediction task (TEP) addresses the first of these two problems." ></td>
	<td class="line x" title="23:192	579 3 Previous work For a complete general overview of the field of affective computing, see (Picard, 1997)." ></td>
	<td class="line x" title="24:192	(Liu, Lieberman and Selker, 2003) is a rare study in textbased inference of sentence-level emotional affinity." ></td>
	<td class="line x" title="25:192	The authors adopt the notion of basic emotions, cf.(Ekman, 1993), and use six emotion categories: ANGER, DISGUST, FEAR, HAPPINESS, SADNESS, SURPRISE." ></td>
	<td class="line x" title="27:192	They critique statistical NLP for being unsuccessful at the small sentence level, and instead use a database of common-sense knowledge and create affect models which are combined to form a representation of the emotional affinity of a sentence." ></td>
	<td class="line x" title="28:192	At its core, the approach remains dependent on an emotion lexicon and hand-crafted rules for conceptual polarity." ></td>
	<td class="line x" title="29:192	In order to be effective, emotion recognition must go beyond such resources; the authors note themselves that lexical affinity is fragile." ></td>
	<td class="line x" title="30:192	The method was tested on 20 users preferences for an email-client, based on user-composed text emails describing short but colorful events." ></td>
	<td class="line x" title="31:192	While the users preferred the emotional client, this evaluation does not reveal emotion classification accuracy, nor how well the model generalizes on a large data set." ></td>
	<td class="line x" title="32:192	Whereas work on emotion classification from the point of view of natural speech and humancomputer dialogues is fairly extensive, e.g.(Scherer, 2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthesis (TTS)." ></td>
	<td class="line x" title="34:192	A short study by (Sugimoto et al. , 2004) addresses sentence-level emotion recognition for Japanese TTS." ></td>
	<td class="line x" title="35:192	Their model uses a composition assumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence." ></td>
	<td class="line x" title="36:192	They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects and compute words emotional strength based on the ratio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects." ></td>
	<td class="line x" title="37:192	Additionally, they conducted an interactive experiment concerning the acoustic rendering of emotion, using manual tuning of prosodic parameters for Japanese sentences." ></td>
	<td class="line x" title="38:192	While the authors actually address the two fundamental problems of emotional TTS, their approach is impractical and most likely cannot scale up for a real corpus." ></td>
	<td class="line x" title="39:192	Again, while lexical items with clear emotional meaning, such as happy or sad, matter, emotion classification probably needs to consider additional inference mechanisms." ></td>
	<td class="line x" title="40:192	Moreover, a nave compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation." ></td>
	<td class="line oc" title="41:192	Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g.(Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts attitudinal valence, e.g.(Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003)." ></td>
	<td class="line x" title="44:192	Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in childrens stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application." ></td>
	<td class="line x" title="45:192	This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al. , 2003)." ></td>
	<td class="line x" title="46:192	4 Empirical study This part covers the experimental study with a formal problem definition, computational implementation, data, features, and a note on parameter tuning." ></td>
	<td class="line x" title="47:192	4.1 Machine learning model Determining emotion of a linguistic unit can be cast as a multi-class classification problem." ></td>
	<td class="line x" title="48:192	For the flat case, let T denote the text, and s an embedded linguistic unit, such as a sentence, where s  T. Let k be the number of emotion classes E = {em1,em2, ,emk}, where em1 denotes the special case of neutrality, or absence of emotion." ></td>
	<td class="line x" title="49:192	The goal is to determine a mapping function f : s  emi, such that we obtain an ordered labeled pair (s,emi)." ></td>
	<td class="line x" title="50:192	The mapping is based on F = {f1,f2, ,fn}, where F contains the features derived from the text." ></td>
	<td class="line x" title="51:192	Furthermore, if multiple emotion classes can characterize s, then given E  E, the target of the mapping function becomes the ordered pair (s,Eprime)." ></td>
	<td class="line x" title="52:192	Finally, as further discussed in section 6, the hierarchical case of label assignment requires a sequen580 tial model that further defines levels of coarse versus fine-grained classifiers, as done by (Li and Roth, 2002) for the question classification problem." ></td>
	<td class="line x" title="53:192	4.2 Implementation Whereas our goal is to predict finer emotional meaning distinctions according to emotional categories in speech; in this study, we focus on the basic task of recognizing emotional passages and on determining their valence (i.e. positive versus negative) because we currently do not have enough training data to explore finer-grained distinctions." ></td>
	<td class="line x" title="54:192	The goal here is to get a good understanding of the nature of the TEP problem and explore features which may be useful." ></td>
	<td class="line x" title="55:192	We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carlson et al. , 1999),1 which learns a linear classifier in feature space, and has been successful in several NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005)." ></td>
	<td class="line x" title="56:192	In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N,E}." ></td>
	<td class="line x" title="57:192	In the second case, E has been incremented with emotional distinctions according to the valence, i.e. E = {N,PE,NE}." ></td>
	<td class="line x" title="58:192	Experiments used 10-fold cross-validation, with 90% train and 10% test data.2 4.3 Data The goal of our current data annotation project is to annotate a corpus of approximately 185 children stories, including Grimms, H.C. Andersens and B. Potters stories." ></td>
	<td class="line x" title="59:192	So far, the annotation process proceeds as follows: annotators work in pairs on the same stories." ></td>
	<td class="line x" title="60:192	They have been trained separately and work independently in order to avoid any annotation bias and get a true understanding of the task difficulty." ></td>
	<td class="line x" title="61:192	Each annotator marks the sentence level with one of eight primary emotions, see table 1, reflecting an extended set of basic emotions (Ekman, 1993)." ></td>
	<td class="line x" title="62:192	In order to make the annotation process more focused, emotion is annotated from the point of view of the text, i.e. the feeler in the sentence." ></td>
	<td class="line x" title="63:192	While the primary emotions are targets, the sentences are also 1Available from http://l2r.cs.uiuc.edu/cogcomp/ 2Experiments were also run for Perceptron, however the results are not included." ></td>
	<td class="line x" title="64:192	Overall, Perceptron performed worse." ></td>
	<td class="line x" title="65:192	marked for other affective contents, i.e. background mood, secondary emotions via intensity, feeler, and textual cues." ></td>
	<td class="line x" title="66:192	Disagreements in annotations are resolved by a second pass of tie-breaking by the first author, who chooses one of the competing labels." ></td>
	<td class="line x" title="67:192	Eventually, the completed annotations will be made available." ></td>
	<td class="line x" title="68:192	Table 1: Basic emotions used in annotation Abbreviation Emotion class A ANGRY D DISGUSTED F FEARFUL H HAPPY Sa SAD Su+ POSITIVELY SURPRISED SuNEGATIVELY SURPRISED Emotion annotation is hard; interannotator agreement currently range at  =.24.51, with the ratio of observed annotation overlap ranging between 45-64%, depending on annotator pair and stories assigned." ></td>
	<td class="line x" title="69:192	This is expected, given the subjective nature of the annotation task." ></td>
	<td class="line x" title="70:192	The lack of a clear definition for emotion vs. non-emotion is acknowledged across the emotion literature, and contributes to dynamic and shifting annotation targets." ></td>
	<td class="line x" title="71:192	Indeed, a common source of confusion is NEUTRAL, i.e. deciding whether or not a sentence is emotional or non-emotional." ></td>
	<td class="line x" title="72:192	Emotion perception also depends on which characters point-of-view the annotator takes, and on extratextual factors such as annotators personality or mood." ></td>
	<td class="line x" title="73:192	It is possible that by focusing more on the training of annotator pairs, particularly on joint training, agreement might improve." ></td>
	<td class="line x" title="74:192	However, that would also result in a bias, which is probably not preferable to actual perception." ></td>
	<td class="line x" title="75:192	Moreover, what agreement levels are needed for successful expressive TTS remains an empirical question." ></td>
	<td class="line x" title="76:192	The current data set consisted of a preliminary annotated and tie-broken data set of 1580 sentence, or 22 Grimms tales." ></td>
	<td class="line x" title="77:192	The label distribution is in table 2." ></td>
	<td class="line x" title="78:192	NEUTRAL was most frequent with 59.94%." ></td>
	<td class="line x" title="79:192	Table 2: Percent of annotated labels A D F H 12.34% 0.89% 7.03% 6.77% N SA SU+ SU.59.94% 7.34% 2.59% 3.10% 581 Table 3: % EMOTIONAL vs. NEUTRAL examples E N 40.06% 59.94% Table 4: % POSITIVE vs. NEGATIVE vs. NEUTRAL PE NE N 9.87% 30.19% 59.94% Next, for the purpose of this study, all emotional classes, i.e. A, D, F, H, SA, SU+, SU-, were combined into one emotional superclass E for the first experiment, as shown in table 3." ></td>
	<td class="line x" title="80:192	For the second experiment, we used two emotional classes, i.e. positive versus negative emotions; PE={H, SU+} and NE={A, D, F, SA, SU-}, as seen in table 4." ></td>
	<td class="line x" title="81:192	4.4 Feature set The feature extraction was written in python." ></td>
	<td class="line x" title="82:192	SNoW only requires active features as input, which resulted in a typical feature vector size of around 30 features." ></td>
	<td class="line x" title="83:192	The features are listed below." ></td>
	<td class="line x" title="84:192	They were implemented as boolean values, with continuous values represented by ranges." ></td>
	<td class="line x" title="85:192	The ranges generally overlapped, in order to get more generalization coverage." ></td>
	<td class="line x" title="86:192	1." ></td>
	<td class="line x" title="87:192	First sentence in story 2." ></td>
	<td class="line x" title="88:192	Conjunctions of selected features (see below) 3." ></td>
	<td class="line x" title="89:192	Direct speech (i.e. whole quote) in sentence 4." ></td>
	<td class="line x" title="90:192	Thematic story type (3 top and 15 sub-types) 5." ></td>
	<td class="line x" title="91:192	Special punctuation (!" ></td>
	<td class="line x" title="92:192	and ?) 6." ></td>
	<td class="line x" title="93:192	Complete upper-case word 7." ></td>
	<td class="line x" title="94:192	Sentence length in words (0-1, 2-3, 4-8, 9-15, 16-25, 26-35, >35) 8." ></td>
	<td class="line x" title="95:192	Ranges of story progress (5-100%, 15-100%, 80-100%, 90-100%) 9." ></td>
	<td class="line x" title="96:192	Percent of JJ, N, V, RB (0%, 1-100%, 50100%, 80-100%) 10." ></td>
	<td class="line x" title="97:192	V count in sentence, excluding participles (0-1, 0-3, 0-5, 0-7, 0-9, > 9) 11." ></td>
	<td class="line x" title="98:192	Positive and negative word counts (  1,  2,  3,  4,  5,  6) 12." ></td>
	<td class="line x" title="99:192	WordNet emotion words 13." ></td>
	<td class="line x" title="100:192	Interjections and affective words 14." ></td>
	<td class="line x" title="101:192	Content BOW: N, V, JJ, RB words by POS Feature conjunctions covered pairings of counts of positive and negative words with range of story progress or interjections, respectively." ></td>
	<td class="line x" title="102:192	Feature groups 1, 3, 5, 6, 7, 8, 9, 10 and 14 are extracted automatically from the sentences in the stories; with the SNoW POS-tagger used for features 9, 10, and 14." ></td>
	<td class="line x" title="103:192	Group 10 reflects how many verbs are active in a sentence." ></td>
	<td class="line x" title="104:192	Together with the quotation and punctuation, verb domination intends to capture the assumption that emotion is often accompanied by increased action and interaction." ></td>
	<td class="line x" title="105:192	Feature group 4 is based on Finish scholar Antti Aarnes classes of folk-tale types according to their informative thematic contents (Aarne, 1964)." ></td>
	<td class="line x" title="106:192	The current tales have 3 top story types (ANIMAL TALES, ORDINARY FOLK-TALES, and JOKES AND ANECDOTES), and 15 subtypes (e.g. supernatural helpers is a subtype of the ORDINARY FOLK-TALE)." ></td>
	<td class="line x" title="107:192	This feature intends to provide an idea about the storys general affective personality (Picard, 1997), whereas the feature reflecting the story progress is hoped to capture that some emotions may be more prevalent in certain sections of the story (e.g. the happy end)." ></td>
	<td class="line x" title="108:192	For semantic tasks, words are obviously important." ></td>
	<td class="line x" title="109:192	In addition to considering content words, we also explored specific word lists." ></td>
	<td class="line x" title="110:192	Group 11 uses 2 lists of 1636 positive and 2008 negative words, obtained from (Di Cicco et al. , online)." ></td>
	<td class="line x" title="111:192	Group 12 uses lexical lists extracted from WordNet (Fellbaum, 1998), on the basis of the primary emotion words in their adjectival and nominal forms." ></td>
	<td class="line x" title="112:192	For the adjectives, Py-WordNets (Steele et al. , 2004) SIMILAR feature was used to retrieve similar items of the primary emotion adjectives, exploring one additional level in the hierarchy (i.e. similar items of all senses of all words in the synset)." ></td>
	<td class="line x" title="113:192	For the nouns and any identical verbal homonyms, synonyms and hyponyms were extracted manually.3 Feature group 13 used a short list of 22 interjections collected manually by browsing educational ESL sites, whereas the affective word list of 771 words consisted of a combination of the non-neutral words from (JohnsonLaird and Oatley, 1989) and (Siegle, online)." ></td>
	<td class="line x" title="114:192	Only a subset of these lexical lists actually occurred.4 3Multi-words were transformed to hyphenated form." ></td>
	<td class="line x" title="115:192	4At this point, neither stems and bigrams nor a list of onomatopoeic words contribute to accuracy." ></td>
	<td class="line x" title="116:192	Intermediate resource processing inserted some feature noise." ></td>
	<td class="line x" title="117:192	582 The above feature set is henceforth referred to as all features, whereas content BOW is just group 14." ></td>
	<td class="line x" title="118:192	The content BOW is a more interesting baseline than the nave one, P(Neutral), i.e. always assigning the most likely NEUTRAL category." ></td>
	<td class="line x" title="119:192	Lastly, emotions blend and transform (Liu, Lieberman and Selker, 2003)." ></td>
	<td class="line x" title="120:192	Thus, emotion and background mood of immediately adjacent sentences, i.e. the sequencing, seems important." ></td>
	<td class="line x" title="121:192	At this point, it is not implemented automatically." ></td>
	<td class="line x" title="122:192	Instead, it was extracted from the manual emotion and mood annotations." ></td>
	<td class="line x" title="123:192	If sequencing seemed important, an automatic method using sequential target activation could be added next." ></td>
	<td class="line x" title="124:192	4.5 Parameter tuning The Winnow parameters that were tuned included promotional , demotional , activation threshold , initial weights , and the regularization parameter, S, which implements a margin between positive and negative examples." ></td>
	<td class="line x" title="125:192	Given the currently fairly limited data, results from 2 alternative tuning methods, applied to all features, are reported." ></td>
	<td class="line x" title="126:192	 For the condition called sep-tune-eval, 50% of the sentences were randomly selected and set aside to be used for the parameter tuning process only." ></td>
	<td class="line x" title="127:192	Of this subset, 10% were subsequently randomly chosen as test set with the remaining 90% used for training during the automatic tuning process, which covered 4356 different parameter combinations." ></td>
	<td class="line x" title="128:192	Resulting parameters were:  = 1.1,  = 0.5,  = 5,  = 1.0, S = 0.5." ></td>
	<td class="line x" title="129:192	The remaining half of the data was used for training and testing in the 10-fold cross-validation evaluation." ></td>
	<td class="line x" title="130:192	(Also, note the slight change for P(Neutral) in table 5, due to randomly splitting the data.)" ></td>
	<td class="line x" title="131:192	 Given that the data set is currently small, for the condition named same-tune-eval, tuning was performed automatically on all data using a slightly smaller set of combinations, and then manually adjusted against the 10-fold crossvalidation process." ></td>
	<td class="line x" title="132:192	Resulting parameters were:  = 1.2,  = 0.9,  = 4,  = 1, S = 0.5." ></td>
	<td class="line x" title="133:192	All data was used for evaluation." ></td>
	<td class="line x" title="134:192	Emotion classification was sensitive to the selected tuning data." ></td>
	<td class="line x" title="135:192	Generally, a smaller tuning set resulted in pejorative parameter settings." ></td>
	<td class="line x" title="136:192	The random selection could make a difference, but was not explored." ></td>
	<td class="line x" title="137:192	5 Results and discussion This section first presents the results from experiments with the two different confusion sets described above, as well as feature experimentation." ></td>
	<td class="line x" title="138:192	5.1 Classification results Average accuracy from 10-fold cross validation for the first experiment, i.e. classifying sentences as either NEUTRAL or EMOTIONAL, are included in table 5 and figure 1 for the two tuning conditions on the main feature sets and baselines." ></td>
	<td class="line x" title="139:192	As expected, Table 5: Mean classification accuracy: N vs. E, 2 conditions same-tune-eval sep-tune-eval P(Neutral) 59.94 60.05 Content BOW 61.01 58.30 All features except BOW 64.68 63.45 All features 68.99 63.31 All features + sequencing 69.37 62.94 degree of success reflects parameter settings, both for content BOW and all features." ></td>
	<td class="line x" title="140:192	Nevertheless, under these circumstances, performance above a nave baseline and a BOW approach is obtained." ></td>
	<td class="line x" title="141:192	Moreover, sequencing shows potential for contributing in one case." ></td>
	<td class="line x" title="142:192	However, observations also point to three issues: first, the current data set appears to be too small." ></td>
	<td class="line x" title="143:192	Second, the data is not easily separable." ></td>
	<td class="line x" title="144:192	This comes as no surprise, given the subjective nature of the task, and the rather low interannotator agreement, reported above." ></td>
	<td class="line x" title="145:192	Moreover, despite the schematic narrative plots of childrens stories, tales still differ in their overall affective orientation, which increases data complexity." ></td>
	<td class="line x" title="146:192	Third and finally, the EMOTION class is combined by basic emotion labels, rather than an original annotated label." ></td>
	<td class="line x" title="147:192	More detailed averaged results from 10-fold cross-validation are included in table 6 using all features and the separated tuning and evaluation data condition sep-tune-eval." ></td>
	<td class="line x" title="148:192	With these parameters, approximately 3% improvement in accuracy over the nave baseline P(Neutral) was recorded, and 5% over the content BOW, which obviously did poorly with these parameters." ></td>
	<td class="line x" title="149:192	Moreover, precision is 583 0 10 20 30 40 50 60 70 same-tune-eval sep-tune-eval Tuning sets % Accuracy P(Neutral) Content BOWAll features except BOWAll features All features + sequencing Figure 1: Accuracy under different conditions (in %) Table 6: Classifying N vs. E (all features, sep-tune-eval) Measure N E Averaged accuracy 0.63 0.63 Averaged error 0.37 0.37 Averaged precision 0.66 0.56 Averaged recall 0.75 0.42 Averaged F-score 0.70 0.47 higher than recall for the combined EMOTION class." ></td>
	<td class="line x" title="150:192	In comparison, with the same-tune-eval procedure, the accuracy improved by approximately 9% over P(Neutral) and by 8% over content BOW." ></td>
	<td class="line x" title="151:192	In the second experiment, the emotion category was split into two classes: emotions with positive versus negative valence." ></td>
	<td class="line x" title="152:192	The results in terms of precision, recall, and F-score are included in table 7, using all features and the sep-tune-eval condition." ></td>
	<td class="line x" title="153:192	The decrease in performance for the emotion classes mirrors the smaller amounts of data available for each class." ></td>
	<td class="line x" title="154:192	As noted in section 4.3, only 9.87% of the sentences were annotated with a positive emotion, and the results for this class are worse." ></td>
	<td class="line x" title="155:192	Thus, performance seems likely to improve as more annotated story data becomes available; at this point, we are experimenting with merely around 12% of the total texts targeted by the data annotation project." ></td>
	<td class="line x" title="156:192	5.2 Feature experiments Emotions are poorly understood, and it is especially unclear which features may be important for their recognition from text." ></td>
	<td class="line x" title="157:192	Thus, we experimented Table 7: N, PE, and NE (all features, sep-tune-eval) N NE PE Averaged precision 0.64 0.45 0.13 Averaged recall 0.75 0.27 0.19 Averaged F-score 0.69 0.32 0.13 Table 8: Feature group members Word lists interj., WordNet, affective lists, pos/neg Syntactic length ranges, % POS, V-count ranges Story-related % story-progress, 1st sent., story type Orthographic punctuation, upper-case words, quote Conjunctions Conjunctions with pos/neg Content BOW Words (N,V,Adj, Adv) with different feature configurations." ></td>
	<td class="line x" title="160:192	Starting with all features, again using 10-fold cross-validation for the separated tuning-evaluation condition sep-tuneeval, one additional feature group was removed until none remained." ></td>
	<td class="line x" title="161:192	The feature groups are listed in table 8." ></td>
	<td class="line x" title="162:192	Figure 2 on the next page shows the accuracy at each step of the cumulative subtraction process." ></td>
	<td class="line x" title="163:192	While some feature groups, e.g. syntactic, appeared less important, the removal order mattered; e.g. if syntactic features were removed first, accuracy decreased." ></td>
	<td class="line x" title="164:192	This fact also illustrated that features work together; removing any group degraded performance because features interact and there is no true independence." ></td>
	<td class="line x" title="165:192	It was observed that features contributions were sensitive to parameter tuning." ></td>
	<td class="line x" title="166:192	Clearly, further work on developing features which fit the TEP problem is needed." ></td>
	<td class="line x" title="167:192	6 Refining the model This was a first pass of addressing TEP for TTS." ></td>
	<td class="line x" title="168:192	At this point, the annotation project is still on-going, and we only had a fairly small data set to draw on." ></td>
	<td class="line x" title="169:192	Nevertheless, results indicate that our learning approach benefits emotion recognition." ></td>
	<td class="line x" title="170:192	For example, the following instances, also labeled with the same valence by both annotators, were correctly classified both in the binary (N vs. E) and the tripartite polarity task (N, NE, PE), given the separated tuning and evaluation data condition, and using all features: (1a) E/NE: Then he offered the dwarfs money, and prayed and besought them to let him take her away; but they said, We will not part with her for all the gold in the world. 584 Cumulative removal of feature groups 61.81 63.31 62.57 57.95 58.3058.93 59.56 55 60 65 Allfeature s -Wordlis ts -Syntacti c -Story-rel ated -Orthogra phic -Conjunc tions -Content words % Ac curac y All featuresP(Neutral)BOW Figure 2: Averaged effect of feature group removal, using sep-tune-eval (1b) N: And so the little girl really did grow up; her skin was as white as snow, her cheeks as rosy as the blood, and her hair as black as ebony; and she was called Snowdrop." ></td>
	<td class="line x" title="171:192	(2a) E/NE: Ah, she answered, have I not reason to weep?" ></td>
	<td class="line x" title="172:192	(2b) N: Nevertheless, he wished to try him first, and took a stone in his hand and squeezed it together so that water dropped out of it." ></td>
	<td class="line x" title="173:192	Cases (1a) and (1b) are from the well-known FOLK TALE Snowdrop, also called Snow White." ></td>
	<td class="line x" title="174:192	(1a) and (1b) are also correctly classified by the simple content BOW approach, although our approach has higher prediction confidence for E/NE (1a); it also considers, e.g. direct speech, a fairly high verb count, advanced story progress, connotative words and conjunctions thereof with story progress features, all of which the BOW misses." ></td>
	<td class="line x" title="175:192	In addition, the simple content BOW approach makes incorrect predictions at both the bipartite and tripartite levels for examples (2a) and (2b) from the JOKES AND ANECDOTES stories Clever Hans and The Valiant Little Tailor, while our classifier captures the affective differences by considering, e.g. distinctions in verb count, interjection, POS, sentence length, connotations, story subtype, and conjunctions." ></td>
	<td class="line x" title="176:192	Next, we intend to use a larger data set to conduct a more complete study to establish mature findings." ></td>
	<td class="line x" title="177:192	We also plan to explore finer emotional meaning distinctions, by using a hierarchical sequential model which better corresponds to different levels of cognitive difficulty in emotional categorization by humans, and to classify the full set of basic level emotional categories discussed in section 4.3." ></td>
	<td class="line x" title="178:192	Sequential modeling of simple classifiers has been successfully employed to question classification, for example by (Li and Roth, 2002)." ></td>
	<td class="line x" title="179:192	In addition, we are working on refining and improving the feature set, and given more data, tuning can be improved on a sufficiently large development set." ></td>
	<td class="line x" title="180:192	The three subcorpora in the annotation project can reveal how authorship affects emotion perception and classification." ></td>
	<td class="line x" title="181:192	Moreover, arousal appears to be an important dimension for emotional prosody (Scherer, 2003), especially in storytelling (Alm and Sproat, 2005)." ></td>
	<td class="line x" title="182:192	Thus, we are planning on exploring degrees of emotional intensity in a learning scenario, i.e. a problem similar to measuring strength of opinion clauses (Wilson, Wiebe and Hwa, 2004)." ></td>
	<td class="line x" title="183:192	Finally, emotions are not discrete objects; rather they have transitional nature, and blend and overlap along the temporal dimension." ></td>
	<td class="line x" title="184:192	For example, (Liu, Lieberman and Selker, 2003) include parallel estimations of emotional activity, and include smooth585 ing techniques such as interpolation and decay to capture sequential and interactive emotional activity." ></td>
	<td class="line x" title="185:192	Observations from tales indicate that some emotions are more likely to be prolonged than others." ></td>
	<td class="line x" title="186:192	7 Conclusion This paper has discussed an empirical study of the text-based emotion prediction problem in the domain of childrens fairy tales, with child-directed expressive text-to-speech synthesis as goal." ></td>
	<td class="line x" title="187:192	Besides reporting on encouraging results in a first set of computational experiments using supervised machine learning, we have set forth a research agenda for tackling the TEP problem more comprehensively." ></td>
	<td class="line x" title="188:192	8 Acknowledgments We are grateful to the annotators, in particular A. Rasmussen and S. Siddiqui." ></td>
	<td class="line x" title="189:192	We also thank two anonymous reviewers for comments." ></td>
	<td class="line x" title="190:192	This work was funded by NSF under award ITR-#0205731, and NS ITR IIS-0428472." ></td>
	<td class="line x" title="191:192	The annotation is supported by UIUCs Research Board." ></td>
	<td class="line x" title="192:192	The authors take sole responsibility for the work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="H05-1116
Multi-Perspective Question Answering Using The OpQA Corpus
Stoyanov, Veselin;Cardie, Claire;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:241	Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 923930, Vancouver, October 2005." ></td>
	<td class="line x" title="2:241	c2005 Association for Computational Linguistics Multi-Perspective Question Answering Using the OpQA Corpus Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14850, USA {ves,cardie}@cs.cornell.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260, USA wiebe@cs.pitt.edu Abstract We investigate techniques to support the answering of opinion-based questions." ></td>
	<td class="line x" title="3:241	We first present the OpQA corpus of opinion questions and answers." ></td>
	<td class="line x" title="4:241	Using the corpus, we compare and contrast the properties of fact and opinion questions and answers." ></td>
	<td class="line x" title="5:241	Based on the disparate characteristics of opinion vs. fact answers, we argue that traditional fact-based QA approaches may have difficulty in an MPQA setting without modification." ></td>
	<td class="line x" title="6:241	As an initial step towards the development of MPQA systems, we investigate the use of machine learning and rule-based subjectivity and opinion source filters and show that they can be used to guide MPQA systems." ></td>
	<td class="line x" title="7:241	1 Introduction Much progress has been made in recent years in automatic, open-domain question answering (e.g. , Voorhees (2001), Voorhees (2002), Voorhees and Buckland (2003))." ></td>
	<td class="line x" title="8:241	The bulk of the research in this area, however, addresses fact-based questions like: When did McDonalds open its first restaurant? or What is the Kyoto Protocol?." ></td>
	<td class="line x" title="9:241	To date, however, relatively little research been done in the area of Multi-Perspective Question Answering (MPQA), which targets questions of the following sort:  How is Bushs decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?" ></td>
	<td class="line x" title="10:241	 How do the Chinese regard the human rights record of the United States?" ></td>
	<td class="line x" title="11:241	In comparison to fact-based question answering (QA), researchers understand far less about the properties of questions and answers in MPQA, and have yet to develop techniques to exploit knowledge of those properties." ></td>
	<td class="line x" title="12:241	As a result, it is unclear whether approaches that have been successful in the domain of fact-based QA will work well for MPQA." ></td>
	<td class="line x" title="13:241	We first present the OpQA corpus of opinion questions and answers." ></td>
	<td class="line x" title="14:241	Using the corpus, we compare and contrast the properties of fact and opinion questions and answers." ></td>
	<td class="line x" title="15:241	We find that text spans identified as answers to opinion questions: (1) are approximately twice as long as those of fact questions, (2) are much more likely (37% vs. 9%) to represent partial answers rather than complete answers, (3) vary much more widely with respect to syntactic category  covering clauses, verb phrases, prepositional phrases, and noun phrases; in contrast, fact answers are overwhelming associated with noun phrases, and (4) are roughly half as likely to correspond to a single syntactic constituent type (16-38% vs. 31-53%)." ></td>
	<td class="line x" title="16:241	Based on the disparate characteristics of opinion vs. fact answers, we argue that traditional fact-based QA approaches may have difficulty in an MPQA setting without modification." ></td>
	<td class="line x" title="17:241	As one such modification, we propose that MPQA systems should rely on natural language processing methods to identify information about opinions." ></td>
	<td class="line x" title="18:241	In experiments in opinion question answering using the OpQA corpus, we find that filtering potential answers using machine learning and rule-based NLP opinion filters substantially improves the performance of an end-to-end MPQA system according to both a mean reciprocal rank (MRR) measure (0.59 vs. a baseline of 0.42) 923 and a metric that determines the mean rank of the first correct answer (MRFA) (26.2 vs. a baseline of 61.3)." ></td>
	<td class="line x" title="19:241	Further, we find that requiring opinion answers to match the requested opinion source (e.g. , does <source> approve of the Kyoto Protocol) dramatically improves the performance of the MPQA system on the hardest questions in the corpus." ></td>
	<td class="line x" title="20:241	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="21:241	In the next section we summarize related work." ></td>
	<td class="line x" title="22:241	Section 3 describes the OpQA corpus." ></td>
	<td class="line x" title="23:241	Section 4 uses the OpQA corpus to identify potentially problematic issues for handling opinion vs. fact questions." ></td>
	<td class="line x" title="24:241	Section 5 briefly describes an opinion annotation scheme used in the experiments." ></td>
	<td class="line x" title="25:241	Sections 6 and 7 explore the use of opinion information in the design of MPQA systems." ></td>
	<td class="line x" title="26:241	2 Related Work There is a growing interest in methods for the automatic identification and extraction of opinions, emotions, and sentiments in text." ></td>
	<td class="line oc" title="27:241	Much of the relevant research explores sentiment classification, a text categorization task in which the goal is to assign to a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line x" title="30:241	Other research has concentrated on analyzing opinions at, or below, the sentence level." ></td>
	<td class="line x" title="31:241	Recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to a reasonable degree of accuracy (e.g. Dave et al.(2003), Riloff and Wiebe (2003), Bethard et al.(2004), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="35:241	Related work in the area of corpus development includes Wiebe et al.s (2005) opinion annotation scheme to identify subjective expressions  expressions used to express opinions, emotions, sentiments and other private states in text." ></td>
	<td class="line x" title="36:241	Wiebe et al. have applied the annotation scheme to create the MPQA corpus consisting of 535 documents manually annotated for phrase-level expressions of opinion." ></td>
	<td class="line x" title="37:241	In addition, the NIST-sponsored TREC evaluation has begun to develop data focusing on opinions  the 2003 Novelty Track features a task that requires systems to identify opinion-oriented documents w.r.t. a specific issue (Voorhees and Buckland, 2003)." ></td>
	<td class="line x" title="38:241	While all of the above work begins to bridge the gap between text categorization and question answering, none of the approaches have been employed or evaluated in the context of MPQA." ></td>
	<td class="line x" title="39:241	3 OpQA Corpus To support our research in MPQA, we created the OpQA corpus of opinion and fact questions and answers." ></td>
	<td class="line x" title="40:241	Additional details on the construction of the corpus as well as results of an interannotator agreement study can be found in Stoyanov et al.(2004)." ></td>
	<td class="line x" title="42:241	3.1 Documents and Questions The OpQA corpus consists of 98 documents that appeared in the world press between June 2001 and May 2002." ></td>
	<td class="line x" title="43:241	All documents were taken from the aforementioned MPQA corpus (Wilson and Wiebe, 2003)1 and are manually annotated with phraselevel opinion information, following the annotation scheme of Wiebe et al.(2005), which is briefly summarized in Section 5." ></td>
	<td class="line x" title="45:241	The documents cover four general (and controversial) topics: President Bushs alternative to the Kyoto protocol (kyoto); the US annual human rights report (humanrights); the 2002 coup detat in Venezuela (venezuela); and the 2002 elections in Zimbabwe and Mugabes reelection (mugabe)." ></td>
	<td class="line x" title="46:241	Each topic is covered by between 19 and 33 documents that were identified automatically via IR methods." ></td>
	<td class="line x" title="47:241	Both fact and opinion questions for each topic were added to the OpQA corpus by a volunteer not associated with the current project." ></td>
	<td class="line x" title="48:241	The volunteer was provided with a set of instructions for creating questions together with two documents on each topic selected at random." ></td>
	<td class="line x" title="49:241	He created between six and eight questions on each topic, evenly split between fact and opinion." ></td>
	<td class="line x" title="50:241	The 30 questions are given in Table 1 sorted by topic." ></td>
	<td class="line x" title="51:241	3.2 Answer annotations Answer annotations were added to the corpus by two annotators according to a set of annotation instruc1The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="52:241	The OpQA corpus is available upon request." ></td>
	<td class="line x" title="53:241	924 Kyoto 1f WhatistheKyotoProtocolabout?" ></td>
	<td class="line x" title="54:241	2f WhenwastheKyotoProtocoladopted?" ></td>
	<td class="line x" title="55:241	3f WhoisthepresidentoftheKikoNetwork?" ></td>
	<td class="line x" title="56:241	4f WhatistheKikoNetwork?" ></td>
	<td class="line x" title="57:241	5o DoesthepresidentoftheKikoNetworkapproveoftheUSactionconcerningtheKyotoProtocol?" ></td>
	<td class="line x" title="58:241	6o AretheJapaneseunanimousintheiropinionofBushspositionontheKyotoProtocol?" ></td>
	<td class="line x" title="59:241	7o HowisBushsdecisionnottoratifytheKyotoProtocollookeduponbyJapanandotherUSallies?" ></td>
	<td class="line x" title="60:241	8o HowdoEuropeanUnioncountriesfeelabouttheUSoppositiontotheKyotoprotocol?" ></td>
	<td class="line x" title="61:241	HumanRights 1f WhatisthemurderrateintheUnitedStates?" ></td>
	<td class="line x" title="62:241	2f WhatcountryissuesanannualreportonhumanrightsintheUnitedStates?" ></td>
	<td class="line x" title="63:241	3o HowdotheChineseregardthehumanrightsrecordoftheUnitedStates?" ></td>
	<td class="line x" title="64:241	4f WhoisAndrewWelsdan?" ></td>
	<td class="line x" title="65:241	5o WhatfactorsinfluencethewayinwhichtheUSregardsthehumanrightsrecordsofothernations?" ></td>
	<td class="line x" title="66:241	6o IstheUSAnnualHumanRightsReportreceivedwithuniversalapprovalaroundtheworld?" ></td>
	<td class="line x" title="67:241	Venezuela 1f WhendidHugoChavezbecomePresident?" ></td>
	<td class="line x" title="68:241	2f DidanyprominentAmericansplantovisitVenezuelaimmediatelyfollowingthe2002coup?" ></td>
	<td class="line x" title="69:241	3o DidanythingsurprisinghappenwhenHugoChavezregainedpowerinVenezuelaafterhewasremovedbyacoup?" ></td>
	<td class="line x" title="70:241	4o DidmostVenezuelanssupportthe2002coup?" ></td>
	<td class="line x" title="71:241	5f WhichgovernmentalinstitutionsinVenezuelaweredissolvedbytheleadersofthe2002coup?" ></td>
	<td class="line x" title="72:241	6o HowdidordinaryVenezuelansfeelaboutthe2002coupandsubsequentevents?" ></td>
	<td class="line x" title="73:241	7o DidAmericasupporttheVenezuelanforeignpolicyfollowedbyChavez?" ></td>
	<td class="line x" title="74:241	8f WhoisVice-PresidentofVenezuela?" ></td>
	<td class="line x" title="75:241	Mugabe 1o WhatwastheAmericanandBritishreactiontothereelectionofMugabe?" ></td>
	<td class="line x" title="76:241	2f WheredidMugabevoteinthe2002presidentialelection?" ></td>
	<td class="line x" title="77:241	3f AtwhichprimaryschoolhadMugabebeenexpectedtovoteinthe2002presidentialelection?" ></td>
	<td class="line x" title="78:241	4f HowlonghasMugabeheadedhiscountry?" ></td>
	<td class="line x" title="79:241	5f WhowasexpectingMugabeatMhofuSchoolforthe2002election?" ></td>
	<td class="line x" title="80:241	6o WhatisthebasisfortheEuropeanUnionandUScriticalattitudeandadversarialactiontowardMugabe?" ></td>
	<td class="line x" title="81:241	7o WhatdidSouthAfricawantMugabetodoafterthe2002election?" ></td>
	<td class="line x" title="82:241	8o WhatisMugabesopinionabouttheWests attitudeandactionstowardsthe2002Zimbabweelec-tion?" ></td>
	<td class="line x" title="83:241	Table 1: Questions in the OpQA collection by topic." ></td>
	<td class="line x" title="84:241	f in column 1 indicates a fact question; o, an opinion question." ></td>
	<td class="line x" title="85:241	tions.2 Every text segment that contributes to an answer to any of the 30 questions is annotated as an answer." ></td>
	<td class="line x" title="86:241	In particular, answer annotations include segments that constitute a partial answer." ></td>
	<td class="line x" title="87:241	Partial answers either (1) lack the specificity needed to constitute a full answer (e.g. , before May 2004 partially answers the question When was the Kyoto protocol ratified?" ></td>
	<td class="line x" title="88:241	when a specific date is known) or (2) need to be combined with at least one additional answer segment to fully answer the question (e.g. , the question Are the Japanese unanimous in their opposition of Bushs position on the Kyoto protocol?" ></td>
	<td class="line x" title="89:241	is answered only partially by a segment expressing a single opinion)." ></td>
	<td class="line x" title="90:241	In addition, annotators mark the minimum answer spans (e.g. , a Tokyo organization, vs. a Tokyo organization representing about 150 Japanese groups)." ></td>
	<td class="line x" title="91:241	4 Characteristics of opinion answers Next, we use the OpQA corpus to analyze and compare the characteristics of fact vs. opinion questions." ></td>
	<td class="line x" title="92:241	Based on our findings, we believe that QA systems based solely on traditional QA techniques are likely 2The annotation instructions are available at http://www.cs.cornell.edu/ ves/ Publications/publications.htm." ></td>
	<td class="line x" title="93:241	to be less effective at MPQA than they are at traditional fact-based QA." ></td>
	<td class="line x" title="94:241	4.1 Traditional QA architectures Despite the wide variety of approaches implied by modern QA systems, almost all systems rely on the following two steps (subsystems), which have empirically proven to be effective:  IR module." ></td>
	<td class="line x" title="95:241	The QA system invokes an IR subsystem that employs traditional text similarity measures (e.g. , tf/idf) to retrieve and rank document fragments (sentences or paragraphs) w.r.t. the question (query)." ></td>
	<td class="line x" title="96:241	 Linguistic filters." ></td>
	<td class="line x" title="97:241	QA systems employ a set of filters and text processing components to discard some document fragments." ></td>
	<td class="line x" title="98:241	The following filters have empirically proven to be effective and are used universally: Semantic filters prefer an answer segment that matches the semantic class(es) associated with the question type (e.g. , date or time for when questions; person or organization for who questions)." ></td>
	<td class="line x" title="99:241	Syntactic filters are also configured on the type of question." ></td>
	<td class="line x" title="100:241	The most common and effective syntactic filters select a specific constituent (e.g. , noun phrase) according to the question type (e.g. , who question)." ></td>
	<td class="line x" title="101:241	QA systems typically interleave the above two subsystems with a variety of different processing steps of both the question and the answer." ></td>
	<td class="line x" title="102:241	The goal of the processing is to identify text fragments that contain an answer to the question." ></td>
	<td class="line x" title="103:241	Typical QA systems do not perform any further text processing; they return the text fragment as it occurred in the text." ></td>
	<td class="line x" title="104:241	3 4.2 Corpus-based analysis of opinion answers We hypothesize that QA systems that conform to this traditional architecture will have difficulty handling opinion questions without non-trivial modification." ></td>
	<td class="line x" title="105:241	In support of this hypothesis, we provide statistics from the OpQA corpus to illustrate some of the characteristics that distinguish answers to opinion vs. fact questions, and discuss their implications for a traditional QA system architecture." ></td>
	<td class="line x" title="106:241	Answer length." ></td>
	<td class="line x" title="107:241	We see in Table 2 that the average length of opinion answers in the OpQA corpus 3This architecture is seen mainly in QA systems designed for TRECs factoid and list QA tracks." ></td>
	<td class="line x" title="108:241	Systems competing in the relatively new definition or other tracks have begun to introduce new approaches." ></td>
	<td class="line x" title="109:241	However, most such systems still rely on the IR step and return the text fragment as it occurred in the text." ></td>
	<td class="line x" title="110:241	925 Number of answers Length Number of partials fact 124 5.12 12 (9.68%) opinion 415 9.24 154 (37.11%) Table 2: Number of answers, average answer length (in tokens), and number of partial answers for fact/opinion questions." ></td>
	<td class="line x" title="111:241	is 9.24 tokens, almost double that of fact answers." ></td>
	<td class="line x" title="112:241	Unfortunately, longer answers could present problems for some traditional QA systems." ></td>
	<td class="line x" title="113:241	In particular, some of the more sophisticated algorithms that perform additional processing steps such as logical verifiers (Moldovan et al. , 2002) may be less accurate or computationally infeasible for longer answers." ></td>
	<td class="line x" title="114:241	More importantly, longer answers are likely to span more than a single syntactic constituent, rendering the syntactic filters, and very likely the semantic filters, less effective." ></td>
	<td class="line x" title="115:241	Partial answers." ></td>
	<td class="line x" title="116:241	Table 2 also shows that over 37% of the opinion answers were marked as partial vs. 9.68% of the fact answers." ></td>
	<td class="line x" title="117:241	The implications of partial answers for the traditional QA architecture are substantial: an MPQA system will require an answer generator to (1) distinguish between partial and full answers; (2) recognize redundant partial answers; (3) identify which subset of the partial answers, if any, constitutes a full answer; (4) determine whether additional documents need to be examined to find a complete answer; and (5) asemble the final answer from partial pieces of information." ></td>
	<td class="line x" title="118:241	Syntactic constituent of the answer." ></td>
	<td class="line x" title="119:241	As discussed in Section 4.1, traditional QA systems rely heavily on the predicted syntactic and semantic class of the answer." ></td>
	<td class="line x" title="120:241	Based on answer lengths, we speculated that opinion answers are unlikely to span a single constituent and/or semantic class." ></td>
	<td class="line x" title="121:241	This speculation is confirmed by examining the phrase type associated with OpQA answers using Abneys (1996) CASS partial parser.4 For each question, we count the number of times an answer segment for the question (in the manual annotations) matches each constituent type." ></td>
	<td class="line x" title="122:241	We consider four constituent types  noun phrase (n), verb phrase (v), prepositional phrase (p), and clause (c)  and three matching criteria: 4The parser is available from http://www.vinartus.net/spa/." ></td>
	<td class="line x" title="123:241	Fact Opinion Ques#of MatchingCriteria syn Ques#of MatchingCriteria syn tion answers ex up up/dn type tion answers ex up up/dn type H1 1 0 0 0 H3 15 5 5 5 c H2 4 2 2 2 n H5 24 5 5 10 n H4 1 0 0 0 H6 123 17 23 52 n K1 48 13 14 24 n K5 3 0 0 1 K2 38 13 13 19 n K6 34 6 5 12 c K3 1 1 1 1 cn K7 55 9 8 19 c K4 2 1 1 1 n K8 25 4 4 10 v M2 3 0 0 1 M1 74 10 12 29 v M3 1 0 0 1 M6 12 3 5 7 n M4 10 2 2 5 n M7 1 0 0 0 M5 3 1 1 2 c M8 3 0 0 1 V1 4 3 3 4 n V3 1 1 0 1 c V2 1 1 1 1 n V4 13 2 2 2 c V5 3 0 1 1 V6 9 2 2 5 cn V8 4 2 4 4 n V7 23 3 1 5 Cov124 39 43 66 Cov415 67 70 159 erage 31% 35% 53% erage 16% 17% 38% Table 3: Syntactic Constituent Type for Answers in the OpQA Corpus 1." ></td>
	<td class="line x" title="124:241	The exact match criterion is satisfied only by answer segments whose spans exactly correspond to a constituent in the CASS output." ></td>
	<td class="line x" title="125:241	2." ></td>
	<td class="line x" title="126:241	The up criterion considers an answer to match a CASS constituent if the constituent completely contains the answer and no more than three additional (non-answer) tokens." ></td>
	<td class="line x" title="127:241	3." ></td>
	<td class="line x" title="128:241	The up/dn criterion considers an answer to match a CASS constituent if it matches according to the up criterion or if the answer completely contains the constituent and no more than three additional tokens." ></td>
	<td class="line x" title="129:241	The counts for the analysis of answer segment syntactic type for fact vs. opinion questions are summarized in Table 3." ></td>
	<td class="line x" title="130:241	Results for the 15 fact questions are shown in the left half of the table, and for the 15 opinion questions in the right half." ></td>
	<td class="line x" title="131:241	The leftmost column in each half provides the question topic and number, and the second column indicates the total number of answer segments annotated for the question." ></td>
	<td class="line x" title="132:241	The next three columns show, for each of the ex, up, and up/dn matching criteria, respectively, the number of annotated answer segments that match the majority syntactic type among answer segments for that question/criterion pair." ></td>
	<td class="line x" title="133:241	Using a traditional QA architecture, the MPQA system might filter answers based on this majority type." ></td>
	<td class="line x" title="134:241	The syn type column indicates the majority syntactic type using the exact match criterion; two values in the column indicate a tie for majority syntactic type, and an empty syntactic type indicates that no answer exactly matched any of the four constituent types." ></td>
	<td class="line x" title="135:241	With only a few exceptions, the up and up/dn matching criteria agreed in majority syntactic type." ></td>
	<td class="line x" title="136:241	Results in Table 3 show a significant disparity between fact and opinion questions." ></td>
	<td class="line x" title="137:241	For fact ques926 tions, the syntactic type filter would keep 31%, 35%, or 53% of the correct answers, depending on the matching criterion." ></td>
	<td class="line x" title="138:241	For opinion questions, there is unfortunately a two-fold reduction in the percentage of correct answers that would remain after filtering  only 16%, 17% or 38%, depending on the matching criterion." ></td>
	<td class="line x" title="139:241	More importantly, the majority syntactic type among answers for fact questions is almost always a noun phrase, while no single constituent type emerges as a useful syntactic filter for opinion questions (see the syn phrase columns in Table 3)." ></td>
	<td class="line x" title="140:241	Finally, because semantic class information is generally tied to a particular syntactic category, the effectiveness of traditional semantic filters in the MPQA setting is unclear." ></td>
	<td class="line x" title="141:241	In summary, identifying answers to questions in an MPQA setting within a traditional QA architecture will be difficult." ></td>
	<td class="line x" title="142:241	First, the implicit and explicit assumptions inherent in standard linguistic filters are consistent with the characteristics of factrather than opinion-oriented QA." ></td>
	<td class="line x" title="143:241	In addition, the presence of relatively long answers and partial answers will require a much more complex answer generator than is typically present in current QA systems." ></td>
	<td class="line x" title="144:241	In Sections 6 and 7, we propose initial steps towards modifying the traditional QA architecture for use in MPQA." ></td>
	<td class="line x" title="145:241	In particular, we propose and evaluate two types of opinion filters for MPQA: subjectivity filters and opinion source filters." ></td>
	<td class="line x" title="146:241	Both types of linguistic filters rely on phrase-level and sentencelevel opinion information, which has been manually annotated for our corpus; the next section briefly describes the opinion annotation scheme." ></td>
	<td class="line x" title="147:241	5 Manual Opinion Annotations Documents in our OpQA corpus come from the larger MPQA corpus, which contains manual opinion annotations." ></td>
	<td class="line x" title="148:241	The annotation framework is described in detail in (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="149:241	Here we give a high-level overview." ></td>
	<td class="line x" title="150:241	The annotation framework provides a basis for subjective expressions: expressions used to express opinions, emotions, and sentiments." ></td>
	<td class="line x" title="151:241	The framework allows for the annotation of both directly expressed private states (e.g. , afraid in the sentence John is afraid that Sue might fall,) and opinions expressed by the choice of words and style of language (e.g. , it is about time and oppression in the sentence It is about time that we end Saddams oppression)." ></td>
	<td class="line x" title="152:241	In addition, the annotations include several attributes, including the intensity (with possible values low, medium, high, and extreme) and the source of the private state." ></td>
	<td class="line x" title="153:241	The source of a private state is the person or entity who holds or experiences it." ></td>
	<td class="line x" title="154:241	6 Subjectivity Filters for MPQA Systems This section describes three subjectivity filters based on the above opinion annotation scheme." ></td>
	<td class="line x" title="155:241	Below (in Section 6.3), the filters are used to remove fact sentences from consideration when answering opinion questions, and the OpQA corpus is used to evaluate their effectiveness." ></td>
	<td class="line x" title="156:241	6.1 Manual Subjectivity Filter Much previous research on automatic extraction of opinion information performed classifications at the sentence level." ></td>
	<td class="line x" title="157:241	Therefore, we define sentence-level opinion classifications in terms of the phrase-level annotations." ></td>
	<td class="line x" title="158:241	For our gold standard of manual opinion classifications (dubbed MANUAL for the rest of the paper) we will follow Riloff and Wiebes (2003) convention (also used by Wiebe and Riloff (2005)) and consider a sentence to be opinion if it contains at least one opinion of intensity medium or higher, and to be fact otherwise." ></td>
	<td class="line x" title="159:241	6.2 Two Automatic Subjectivity Filters As discussed in section 2, several research efforts have attempted to perform automatic opinion classification on the clause and sentence level." ></td>
	<td class="line x" title="160:241	We investigate whether such information can be useful for MPQA by using the automatic sentence level opinion classifiers of Riloff and Wiebe (2003) and Wiebe and Riloff (2005)." ></td>
	<td class="line x" title="161:241	Riloff and Wiebe (2003) use a bootstrapping algorithm to perform a sentence-based opinion classification on the MPQA corpus." ></td>
	<td class="line x" title="162:241	They use a set of high precision subjectivity and objectivity clues to identify subjective and objective sentences." ></td>
	<td class="line x" title="163:241	This data is then used in an algorithm similar to AutoSlogTS (Riloff, 1996) to automatically identify a set of extraction patterns." ></td>
	<td class="line x" title="164:241	The acquired patterns are then used iteratively to identify a larger set of subjective and objective sentences." ></td>
	<td class="line x" title="165:241	In our experiments we use 927 precision recall F MPQAcorpus RULEBASED 90.4 34.2 46.6 NAIVE BAYES 79.4 70.6 74.7 Table 4: Precision, recall, and F-measure for the two classifiers." ></td>
	<td class="line x" title="166:241	the classifier that was created by the reimplementation of this bootstrapping process in Wiebe and Riloff (2005)." ></td>
	<td class="line x" title="167:241	We will use RULEBASED to denote the opinion information output by this classifier." ></td>
	<td class="line x" title="168:241	In addition, Wiebe and Riloff used the RULEBASED classifier to produce a labeled data set for training." ></td>
	<td class="line x" title="169:241	They trained a Naive Bayes subjectivity classifier on the labeled set." ></td>
	<td class="line x" title="170:241	We will use NAIVE BAYES to refer to Wiebe and Riloffs naive Bayes classifier.5 Table 4 shows the performance of the two classifiers on the MPQA corpus as reported by Wiebe and Riloff." ></td>
	<td class="line x" title="171:241	6.3 Experiments We performed two types of experiments using the subjectivity filters." ></td>
	<td class="line x" title="172:241	6.3.1 Answer rank experiments Our hypothesis motivating the first type of experiment is that subjectivity filters can improve the answer identification phase of an MPQA system." ></td>
	<td class="line x" title="173:241	We implement the IR subsystem of a traditional QA system, and apply the subjectivity filters to the IR results." ></td>
	<td class="line x" title="174:241	Specifically, for each opinion question in the corpus 6, we do the following: 1." ></td>
	<td class="line x" title="175:241	Split all documents in our corpus into sentences." ></td>
	<td class="line x" title="176:241	2." ></td>
	<td class="line x" title="177:241	Run an information retrieval algorithm7 on the set of all sentences using the question as the query to obtain a ranked list of sentences." ></td>
	<td class="line x" title="178:241	3." ></td>
	<td class="line x" title="179:241	Apply a subjectivity filter to the ranked list to remove all fact sentences from the ranked list." ></td>
	<td class="line x" title="180:241	We test each of the MANUAL, RULEBASED, and NAIVE BAYES subjectivity filters." ></td>
	<td class="line x" title="181:241	We compare the rank of the first answer to each question in the 5Specifically, the one they label Naive Bayes 1." ></td>
	<td class="line x" title="182:241	6We do not evaluate the opinion filters on the 15 fact questions." ></td>
	<td class="line x" title="183:241	Since opinion sentences are defined as containing at least one opinion of intensity medium or higher, opinion sentences can contain factual information and sentence-level opinion filters are not likely to be effective for fact-based QA." ></td>
	<td class="line x" title="184:241	7We use the Lemur toolkits standard tf.idf implementation available from http://www.lemurproject.org/." ></td>
	<td class="line x" title="185:241	Topic Qnum Baseline Manual NaiveBayes Rulebased Kyoto 5 1 1 1 1 6 5 4 4 3 7 1 1 1 1 8 1 1 1 1 Human 3 1 1 1 1 Rights 5 10 6 7 5 6 1 1 1 1 Venezuela 3 106 81 92 35 4 3 2 3 1 6 1 1 1 1 7 3 3 3 2 Mugabe 1 2 2 2 2 6 7 5 5 4 7 447 291 317 153 8 331 205 217 182 MRR: 0.4244 0.5189 0.5078 0.5856 MRFA: 61.3333 40.3333 43.7333 26.2 Table 5: Results for the subjectivity filters." ></td>
	<td class="line x" title="186:241	ranked list before the filter is applied, with the rank of the first answer to the question in the ranked list after the filter is applied." ></td>
	<td class="line x" title="187:241	Results." ></td>
	<td class="line x" title="188:241	Results for the opinion filters are compared to a simple baseline, which performs the information retrieval step with no filtering." ></td>
	<td class="line x" title="189:241	Table 5 gives the results on the 15 opinion questions for the baseline and each of the three subjectivity filters." ></td>
	<td class="line x" title="190:241	The table shows two cumulative measures  the mean reciprocal rank (MRR) 8 and the mean rank of the first answer (MRFA)." ></td>
	<td class="line x" title="191:241	9 Table 5 shows that all three subjectivity filters outperform the baseline: for all three filters, the first answer in the filtered results for all 15 questions is ranked at least as high as in the baseline." ></td>
	<td class="line x" title="192:241	As a result, the three subjectivity filters outperform the baseline in both MRR and MRFA." ></td>
	<td class="line x" title="193:241	Surprisingly, the best performing subjectivity filter is RULEBASED, surpassing the gold standard MANUAL, both in MRR (0.59 vs. 0.52) and MRFA (40.3 vs. 26.2)." ></td>
	<td class="line x" title="194:241	Presumably, the improvement in performance comes from the fact that RULEBASED identifies subjective sentences with the highest precision (and lowest recall)." ></td>
	<td class="line x" title="195:241	Thus, the RULEBASED subjectivity filter discards non-subjective sentences most aggressively." ></td>
	<td class="line x" title="196:241	6.3.2 Answer probability experiments The second experiment, answer probability, begins to explore whether opinion information can be 8The MRR is computed as the average of 1/r, where r is the rank of the first answer." ></td>
	<td class="line x" title="197:241	9MRR has been accepted as the standard performance measure in QA, since MRFA can be strongly affected by outlier questions." ></td>
	<td class="line x" title="198:241	However, the MRR score is dominated by the results in the high end of the ranking." ></td>
	<td class="line x" title="199:241	Thus, MRFA may be more appropriate for our experiments because the filters are an intermediate step in the processing, the results of which other MPQA components may improve." ></td>
	<td class="line x" title="200:241	928 sentence fact opinion Manual fact 56(46.67%) 64(53.33%) opinion 42(10.14%) 372(89.86%) question NaiveBayes fact 49(40.83%) 71(59.17%) opinion 57(13.77%) 357(86.23%) Rulebased fact 96(80.00%) 24(20.00%) opinion 184(44.44%) 230(55.56%) Table 6: Answer probability results." ></td>
	<td class="line x" title="201:241	used in an answer generator." ></td>
	<td class="line x" title="202:241	This experiment considers correspondences between (1) the classes (i.e. , opinion or fact) assigned by the subjectivity filters to the sentences containing answers, and (2) the classes of the questions the answers are responses to (according to the OpQA annotations)." ></td>
	<td class="line x" title="203:241	That is, we compute the probabilities (where ans = answer): P(ans is in a C1 sentence | ans is the answer to a C2 question) for all four combinations of C1=opinion, fact and C2=opinion, fact." ></td>
	<td class="line x" title="204:241	Results." ></td>
	<td class="line x" title="205:241	Results for the answer probability experiment are given in Table 6." ></td>
	<td class="line x" title="206:241	The rows correspond to the classes of the questions the answers respond to, and the columns correspond to the classes assigned by the subjectivity filters to the sentences containing the answers." ></td>
	<td class="line x" title="207:241	The first two rows, for instance, give the results for the MANUAL criterion." ></td>
	<td class="line x" title="208:241	MANUAL placed 56 of the answers to fact questions in fact sentences (46.67% of all answers to fact questions) and 64 (53.33%) of the answers to fact questions in opinion sentences." ></td>
	<td class="line x" title="209:241	Similarly, MANUAL placed 42 (10.14%) of the answers to opinion questions in fact sentences, and 372 (89.86%) of the answers to opinion questions in opinion sentences." ></td>
	<td class="line x" title="210:241	The answer probability experiment sheds some light on the subjectivity filter experiments." ></td>
	<td class="line x" title="211:241	All three subjectivity filters place a larger percentage of answers to opinion questions in opinion sentences than they place in fact sentences." ></td>
	<td class="line x" title="212:241	However, the different filters exhibit different degrees of discrimination." ></td>
	<td class="line x" title="213:241	Answers to opinion questions are almost always placed in opinion sentences by MANUAL (89.86%) and NAIVE BAYES (86.23%)." ></td>
	<td class="line x" title="214:241	While that aspect of their performance is excellent, MANUAL and NAIVE BAYES place more answers to fact questions in opinion rather than fact sentences (though the percentages are in the 50s)." ></td>
	<td class="line x" title="215:241	This is to be expected, because MANUAL and NAIVE BAYES are more conservative and err on the side of classifying sentences as opinions: for MANUAL, the presence of any subjective expression makes the entire sentence opinion, even if parts of the sentence are factual; NAIVE BAYES shows high recall but lower precision in recognizing opinion sentences (see Table 4)." ></td>
	<td class="line x" title="216:241	Conversely, RULEBASED places 80% of the fact answers in fact sentences and only 56% of the opinion answers in opinion sentences." ></td>
	<td class="line x" title="217:241	Again, the lower number of assignments to opinion sentences is to be expected, given the high precision and low recall of the classifier." ></td>
	<td class="line x" title="218:241	But the net result is that, for RULEBASED, the offdiagonals are all less than 50%: it places more answers to fact questions in fact rather than opinion sentences (80%), and more answers to opinion questions in opinion rather than fact sentences (56%)." ></td>
	<td class="line x" title="219:241	This is consistent with its superior performance in the subjectivity filtering experiment." ></td>
	<td class="line x" title="220:241	In addition to explaining the performance of the subjectivity filters, the answer rank experiment shows that the automatic opinion classifiers can be used directly in an answer generator module." ></td>
	<td class="line x" title="221:241	The two automatic classifiers rely on evidence in the sentence to predict the class (the information extraction patterns used by RULEBASED and the features used by NAIVE BAYES)." ></td>
	<td class="line x" title="222:241	In ongoing work we investigate ways to use this evidence to extract and summarize the opinions expressed in text, which is a task similar to that of an answer generator module." ></td>
	<td class="line x" title="223:241	7 Opinion Source Filters for MPQA Systems In addition to subjectivity filters, we also define an opinion source filter based on the manual opinion annotations." ></td>
	<td class="line x" title="224:241	This filter removes all sentences that do not have an opinion annotation with a source that matches the source of the question10." ></td>
	<td class="line x" title="225:241	For this filter we only used the MANUAL source annotations since we did not have access to automatically extracted source information." ></td>
	<td class="line x" title="226:241	We employ the same Answer Rank experiment as in 6.3.1, substituting the source filter for a subjectivity filter." ></td>
	<td class="line x" title="227:241	Results." ></td>
	<td class="line x" title="228:241	Results for the source filter are mixed." ></td>
	<td class="line x" title="229:241	The filter outperforms the baseline on some questions and performs worst on others." ></td>
	<td class="line x" title="230:241	As a result the MRR for the source filter is worse than the base10We manually identified the sources of each of the 15 opinion questions." ></td>
	<td class="line x" title="231:241	929 line (0.4633 vs. 0.4244)." ></td>
	<td class="line x" title="232:241	However, the source filter exhibits by far the best results using the MRFA measure, a value of 11.267." ></td>
	<td class="line x" title="233:241	The performance improvement is due to the filters ability to recognize the answers to the hardest questions, for which the other filters have the most trouble (questions mugabe 7 and 8)." ></td>
	<td class="line x" title="234:241	For these questions, the rank of the first answer improves from 153 to 21, and from 182 to 11, respectively." ></td>
	<td class="line x" title="235:241	With the exception of question venezuela 3, which does not contain a clear source (and is problematic altogether because there is only a single answer in the corpus and the questions qualification as opinion is not clear) the source filter always ranked an answer within the first 25 answers." ></td>
	<td class="line x" title="236:241	Thus, source filters can be especially useful in systems that rely on the presence of an answer within the first few ranked answer segments and then invoke more sophisticated analysis in the additional processing phase." ></td>
	<td class="line x" title="237:241	8 Conclusions We began by giving a high-level overview of the OpQA corpus." ></td>
	<td class="line x" title="238:241	Using the corpus, we compared the characteristics of answers to fact and opinion questions." ></td>
	<td class="line x" title="239:241	Based on the different characteristics, we surmise that traditional QA approaches may not be as effective for MPQA as they have been for fact-based QA." ></td>
	<td class="line x" title="240:241	Finally, we investigated the use of machine learning and rule-based opinion filters and showed that they can be used to guide MPQA systems." ></td>
	<td class="line x" title="241:241	Acknowledgments We would like to thank Diane Litman for her work eliciting the questions for the OpQA corpus, and the anonymous reviewers for their helpful comments.This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0208028 and IIS0208798, by the Xerox Foundation, and by a NSF Graduate Research Fellowship to the first author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I05-2011
Automatic Detection of Opinion Bearing Words and Sentences
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:161	Automatic Detection of Opinion Bearing Words and Sentences Soo-Min Kim and Eduard Hovy Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292-6695 {skim, hovy}@isi.edu Abstract We describe a sentence-level opinion detection system." ></td>
	<td class="line x" title="2:161	We first define what an opinion means in our research and introduce an effective method for obtaining opinion-bearing and nonopinion-bearing words." ></td>
	<td class="line x" title="3:161	Then we describe recognizing opinion-bearing sentences using these words We test the system on 3 different test sets: MPQA data, an internal corpus, and the TREC2003 Novelty track data." ></td>
	<td class="line x" title="4:161	We show that our automatic method for obtaining opinion-bearing words can be used effectively to identify opinion-bearing sentences." ></td>
	<td class="line x" title="5:161	1 Introduction Sophisticated language processing in recent years has made possible increasingly complex challenges for text analysis." ></td>
	<td class="line x" title="6:161	One such challenge is recognizing, classifying, and understanding opinionated text." ></td>
	<td class="line x" title="7:161	This ability is desirable for various tasks, including filtering advertisements, separating the arguments in online debate or discussions, and ranking web documents cited as authorities on contentious topics." ></td>
	<td class="line x" title="8:161	The challenge is made very difficult by a general inability to define opinion." ></td>
	<td class="line x" title="9:161	Our preliminary reading of a small selection of the available literature (Aristotle, 1954; Toulmin et al. , 1979; Perelman, 1970; Wallace, 1975), as well as our own text analysis, indicates that a profitable approach to opinion requires a system to know and/or identify at least the following elements: the topic (T), the opinion holder (H), the belief (B), and the opinion valence (V)." ></td>
	<td class="line x" title="10:161	For the purposes of the various interested communities, neutral-valence opinions (such as we believe the sun will rise tomorrow; Susan believes that John has three children) is of less interest; more relevant are opinions in which the valence is positive or negative." ></td>
	<td class="line x" title="11:161	Such valence often falls together with the actual belief, as in going to Mars is a waste of money; in which the word waste signifies both the belief a lot [of money] and the valence bad/undesirable, but need not always do so: Smith[the holder] believes that abortion should be permissible[the topic] although he thinks that is a bad thing[the valence]." ></td>
	<td class="line x" title="12:161	As the core first step of our research, we would like an automated system to identify, given an opinionated text, all instances of the [Holder/Topic/Valence] opinion triads it contains 1." ></td>
	<td class="line x" title="13:161	Exploratory manual work has shown this to be a difficult task." ></td>
	<td class="line x" title="14:161	We therefore simplify the task as follows." ></td>
	<td class="line x" title="15:161	We build a classifier that simply identifies in a text all the sentences expressing a valence." ></td>
	<td class="line x" title="16:161	Such a two-way classification is simple to set up and evaluate, since enough testing data has been created." ></td>
	<td class="line x" title="17:161	As primary indicators, we note from newspaper editorials and online exhortatory text that certain modal verbs (should, must) and adjectives and adverbs (better, best, unfair, ugly, nice, desirable, nicely, luckily) are strong markers of opinion." ></td>
	<td class="line x" title="18:161	Section 3 describes our construction of a series of increasingly large collections of such marker words." ></td>
	<td class="line x" title="19:161	Section 4 describes our methods for organizing and combining them and using them to identify valence-bearing sentences." ></td>
	<td class="line x" title="20:161	The evaluation is reported in Section 5." ></td>
	<td class="line x" title="21:161	2 Past Computational Studies There has been a spate of research on identifying sentence-level subjectivity in general and opinion in particular." ></td>
	<td class="line x" title="22:161	The Novelty track 1 In the remainder of the paper, we will mostly use opinion in place of valence." ></td>
	<td class="line x" title="23:161	We will no longer discuss Belief, Holder, or Topic." ></td>
	<td class="line x" title="24:161	61 (Soboroff and Harman, 2003) of the TREC-2003 competition included a task of recognizing opinion-bearing sentences (see Section 5.2)." ></td>
	<td class="line x" title="25:161	Wilson and Wiebe (2003) developed an annotation scheme for so-called subjective sentences (opinions and other private states) as part of a U.S. government-sponsored project (ARDA AQUAINT NRRC) in 2002." ></td>
	<td class="line x" title="26:161	They created a corpus, MPQA, containing news articles manually annotated." ></td>
	<td class="line o" title="27:161	Several other approaches have been applied for learning words and phrases that signal subjectivity." ></td>
	<td class="line oc" title="28:161	Turney (2002) and Wiebe (2000) focused on learning adjectives and adjectival phrases and Wiebe et al.(2001) focused on nouns." ></td>
	<td class="line x" title="30:161	Riloff et al.(2003) extracted nouns and Riloff and Wiebe (2003) extracted patterns for subjective expressions using a bootstrapping process." ></td>
	<td class="line x" title="32:161	3 Data Sources We developed several collections of opinionbearing and non-opinion-bearing words." ></td>
	<td class="line x" title="33:161	One is accurate but small; another is large but relatively inaccurate." ></td>
	<td class="line x" title="34:161	We combined them to obtain a more reliable list." ></td>
	<td class="line x" title="35:161	We obtained an additional list from Columbia University." ></td>
	<td class="line x" title="36:161	3.1 Collection 1: Using WordNet In pursuit of accuracy, we first manually collected a set of opinion-bearing words (34 adjectives and 44 verbs)." ></td>
	<td class="line x" title="37:161	Early classification trials showed that precision was very high (the system found only opinion-bearing sentences), but since the list was so small, recall was very low (it missed many)." ></td>
	<td class="line x" title="38:161	We therefore used this list as seed words for expansion using WordNet." ></td>
	<td class="line x" title="39:161	Our assumption was that synonyms and antonyms of an opinion-bearing word could be opinionbearing as well, as for example nice, virtuous, pleasing, well-behaved, gracious, honorable, righteous as synonyms for good, or bad, evil, disreputable, unrighteous as antonyms." ></td>
	<td class="line x" title="40:161	However, not all synonyms and antonyms could be used: some such words seemed to exhibit both opinion-bearing and non-opinion-bearing senses, such as solid, hot, full, ample for good." ></td>
	<td class="line x" title="41:161	This indicated the need for a scale of valence strength." ></td>
	<td class="line x" title="42:161	If we can measure the opinion-based closeness of a synonym or antonym to a known opinion bearer, then we can determine whether to include it in the expanded set." ></td>
	<td class="line x" title="43:161	To develop such a scale, we first created a non-opinion-bearing word list manually and produced related words for it using WordNet." ></td>
	<td class="line x" title="44:161	To avoid collecting uncommon words, we started with a basic/common English word list compiled for foreign students preparing for the TOEFL test." ></td>
	<td class="line x" title="45:161	From this we randomly selected 462 adjectives and 502 verbs for human annotation." ></td>
	<td class="line x" title="46:161	Human1 and human2 annotated 462 adjectives and human3 and human2 annotated 502 verbs, labeling each word as either opinionbearing or non-opinion-bearing." ></td>
	<td class="line x" title="47:161	OP NonOP word Synonym set of OP Synonym set of NonOP Synonym set of a given word O P : O pinion-bearing w ords NonOP : Non-Opinion-bearing words Figure 1." ></td>
	<td class="line x" title="48:161	Automatic word expansion using WordNet Now, to obtain a measure of opinion/nonopinion strength, we measured the WordNet distance of a target (synonym or antonym) word to the two sets of manually selected seed words plus their current expansion words (see Figure 1)." ></td>
	<td class="line x" title="49:161	We assigned the new word to the closer category." ></td>
	<td class="line x" title="50:161	The following equation represents this approach: (1) ),|(maxarg )|(maxarg 21 n c c synsynsyncP wcP  where c is a category (opinion-bearing or nonopinion-bearing), w is the target word, and syn n is the synonyms or antonyms of the given word by WordNet." ></td>
	<td class="line x" title="51:161	To compute equation (1), we built a classification model, equation (2): (2) )|()(maxarg )|()(maxarg )|()(maxarg)|(maxarg 1 ))(,( 3 2 1  = = = = m k wsynsetfcount k c n c cc k cfPcP csynsynsynsynPcP cwPcPwcP where k f is the k th feature of category c which is also a member of the synonym set of the target word w, and count(f k, synset(w)) means the total number of occurrences of f k in the synonym set of w. The motivation for this model is document classification." ></td>
	<td class="line x" title="52:161	(Although we used the synonym set of seed words achieved by WordNet, we could instead have obtained word features from a corpus)." ></td>
	<td class="line x" title="53:161	After expansion, we obtained 2682 62 opinion-bearing and 2548 non-opinion-bearing adjectives, and 1329 opinion-bearing and 1760 non-opinion-bearing verbs, with strength values." ></td>
	<td class="line x" title="54:161	By using these words as features we built a Naive bayesian classifier and we finally classified 32373 words." ></td>
	<td class="line x" title="55:161	3.2 Collection 2: WSJ Data Experiments with the above set did not provide very satisfactory results on arbitrary text." ></td>
	<td class="line x" title="56:161	For one reason, WordNets synonym connections are simply not extensive enough." ></td>
	<td class="line x" title="57:161	However, if we know the relative frequency of a word in opinion-bearing texts compared to non-opinionbearing text, we can use the statistical information instead of lexical information." ></td>
	<td class="line x" title="58:161	For this, we collected a huge amount of data in order to make up for the limitations of collection 1." ></td>
	<td class="line x" title="59:161	Following the insight of Yu and Hatzivassiloglou (2003), we made the basic and rough assumption that words that appear more often in newspaper editorials and letters to the editor than in non-editorial news articles could be potential opinion-bearing words (even though editorials contain sentences about factual events as well)." ></td>
	<td class="line x" title="60:161	We used the TREC collection to collect data, extracting and classifying all Wall Street Journal documents from it either as Editorial or nonEditorial based on the occurrence of the keywords Letters to the Editor, Letter to the Editor or Editorial present in its headline." ></td>
	<td class="line x" title="61:161	This produced in total 7053 editorial documents and 166025 non-editorial documents." ></td>
	<td class="line x" title="62:161	We separated out opinion from non-opinion words by considering their relative frequency in the two collections, expressed as a probability, using SRILM, SRIs language modeling toolkit (http://www.speech.sri.com/projects/srilm/)." ></td>
	<td class="line x" title="63:161	For every word W occurring in either of the document sets, we computed the followings: documents Editorialin wordstotal documents Editorialin W # )(Pr =WobEditorial docs alnonEditoriin wordstotal docs alnonEditoriin W # )(Pr =WobalnonEditori We used Kneser-Ney smoothing (Kneser and Ney, 1995) to handle unknown/rare words." ></td>
	<td class="line x" title="64:161	Having obtained the above probabilities we calculated the score of W as the following ratio: alProb(W)nonEditori rob(W)EditorialP )( =WScore Score(W) gives an indication of the bias of each word towards editorial or non-editorial texts." ></td>
	<td class="line x" title="65:161	We computed scores for 86,674,738 word tokens." ></td>
	<td class="line x" title="66:161	Naturally, words with scores close to 1 were untrustworthy markers of valence." ></td>
	<td class="line x" title="67:161	To eliminate these words we applied a simple filter as follows." ></td>
	<td class="line x" title="68:161	We divided the Editorial and the non-Editorial collections each into 3 subsets." ></td>
	<td class="line x" title="69:161	For each word in each {Editorial, non-Editorial} subset pair we calculated Score(W)." ></td>
	<td class="line x" title="70:161	We retained only those words for which the scores in all three subset pairs were all greater than 1 or all less than 1." ></td>
	<td class="line x" title="71:161	In other words, we only kept words with a repeated bias towards Editorial or nonEditorial." ></td>
	<td class="line x" title="72:161	This procedure helped eliminate some of the noisy words, resulting in 15568 words." ></td>
	<td class="line x" title="73:161	3.3 Collection 3: With Columbia Wordlist Simply partitioning WSJ articles into Editorial/non-Editorial is a very crude differentiation." ></td>
	<td class="line x" title="74:161	In order to compare the effectiveness of our implementation of this idea with the implementation by Yu and Hatzivassiloglou of Columbia University, we requested their word list, which they kindly provided." ></td>
	<td class="line x" title="75:161	Their list contained 167020 adjectives, 72352 verbs, 168614 nouns, and 9884 adverbs." ></td>
	<td class="line x" title="76:161	However, this figure is significantly inflated due to redundant counting of words with variations in capitalization and a punctuation.We merged this list and ours to obtain collection 4." ></td>
	<td class="line x" title="77:161	Among these words, we only took top 2000 opinion bearing words and top 2000 non-opinion-bearing words for the final word list." ></td>
	<td class="line x" title="78:161	3.4 Collection 4: Final Merger So far, we have classified words as either opinion-bearing or non-opinion-bearing by two different methods." ></td>
	<td class="line x" title="79:161	The first method calculates the degrees of closeness to manually chosen sets of opinion-bearing and non-opinion-bearing words in WordNet and decides its class and strength." ></td>
	<td class="line x" title="80:161	When the word is equally close to both classes, it is hard to decide its subjectivity, and when WordNet doesnt contain a word or its synonyms, such as the word antihomosexsual, we fail to classify it." ></td>
	<td class="line x" title="81:161	The second method, classification of words using WSJ texts, is less reliable than the lexical method." ></td>
	<td class="line x" title="82:161	However, it does for example successfully handle antihomosexual." ></td>
	<td class="line x" title="83:161	Therefore, we combined the results of the two methods (collections 1 and 2), since their different characteris63 tics compensate for each other." ></td>
	<td class="line x" title="84:161	Later we also combine 4000 words from the Columbia word list to our final 43700 word list." ></td>
	<td class="line x" title="85:161	Since all three lists include a strength between 0 and 1, we simply averaged them, and normalized the valence strengths to the range from -1 to +1, with greater opinion valence closer to 1 (see Table 1)." ></td>
	<td class="line x" title="86:161	Obviously, words that had a high valence strength in all three collections had a high overall positive strength." ></td>
	<td class="line x" title="87:161	When there was a conflict vote among three for a word, it aotomatically got weak strength." ></td>
	<td class="line x" title="88:161	Table 2 shows the distribution of words according to their sources: Collection1(C1), Collection2(C2) and Collection3(C3)." ></td>
	<td class="line x" title="89:161	4 Measuring Sentence Valence 4.1 Two Models We are now ready to automatically identify opinion-bearing sentences." ></td>
	<td class="line x" title="90:161	We defined several models, combining valence scores in different ways, and eventually kept two: Model 1: Total valence score of all words in a sentence Model 2: Presence of a single strong valence word The intuition underlying Model 1 is that sentences in which opinion-bearing words dominate tend to be opinion-bearing, while Model 2 reflects the idea that even one strong valence word is enough." ></td>
	<td class="line x" title="91:161	After experimenting with these models, we decided to use Model 2." ></td>
	<td class="line x" title="92:161	How strong is strong enough?" ></td>
	<td class="line x" title="93:161	To determine the cutoff threshold () on the opinionbearing valence strength of words, we experimented on human annotated data." ></td>
	<td class="line x" title="94:161	4.2 Gold Standard Annotation We built two sets of human annotated sentence subjectivity data." ></td>
	<td class="line x" title="95:161	Test set A contains 50 sentences about welfare reform, of which 24 sentences are opinion-bearing." ></td>
	<td class="line x" title="96:161	Test set B contains 124 sentences on two topics (illegal aliens and term limits), of which 53 sentences are opinionbearing." ></td>
	<td class="line x" title="97:161	Three humans classified the sentences as either opinion or non-opinion bearing." ></td>
	<td class="line x" title="98:161	We calculated agreement for each pair of humans and for all three together." ></td>
	<td class="line x" title="99:161	Simple pairwise agreement averaged at 0.73, but the kappa score was only 0.49." ></td>
	<td class="line x" title="100:161	Table 3 shows the results of experimenting with different combinations of Model 1, Model 2, and several cutoff values." ></td>
	<td class="line x" title="101:161	Recall, precision, Fscore, and accuracy are defined in the normal way." ></td>
	<td class="line x" title="102:161	Generally, as the cutoff threshold increases, fewer opinion markers are included in the lists, and precision increases while recall drops." ></td>
	<td class="line x" title="103:161	The best F-core is obtained on Test set A, Model 2, with =0.1 or 0.2 (i.e. , being rather liberal)." ></td>
	<td class="line x" title="104:161	Table 1." ></td>
	<td class="line x" title="105:161	Examples of opinion-bearing/non-opinionbearing words Adjectives Final score Verbs Final score Careless 0.63749 Harm 0.61715 wasteful 0.49999 Hate 0.53847 Unpleasant 0.15263 Yearn 0.50000 Southern -0.2746 Enter -0.4870 Vertical -0.4999 Crack -0.4999 Scored -0.5874 combine -0.5852 Table 2." ></td>
	<td class="line x" title="106:161	Distribution of words C1 C2 C3 # words %  25605 58.60  8202 18.77  2291 5.24   5893 13.49   834 1.90   236 0.54    639 1.46 Total # 32373 15568 4000 43700 100 Table 3." ></td>
	<td class="line x" title="107:161	Determining  and performance for various models on gold standard data [: cutoff parameter, R: recall, P: precision, F: F-score, A: accuracy] Development Test set A Development Test set B Model1 Model2 Model1 Model2  R P F A R P F A R P F A R P F A 0.1 0.54 0.61 0.57 0.62 0.91 0.55 0.69 0.6 0.43 0.36 0.39 0.43 0.94 0.45 0.61 0.48 0.2 0.54 0.61 0.57 0.62 0.91 0.56 0.69 0.62 0.39 0.35 0.37 0.42 0.86 0.45 0.59 0.49 0.3 0.58 0.6 0.59 0.62 0.83 0.55 0.66 0.6 0.43 0.39 0.41 0.47 0.77 0.45 0.57 0.05 0.4 0.33 0.8 0.47 0.64 0.33 0.8 0.47 0.64 0.45 0.36 0.4 0.42 0.45 0.36 0.4 0.42 0.5 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.32 0.3 0.31 0.4 0.32 0.3 0.31 0.4 0.6 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.2 0.22 0.21 0.35 0.2 0.22 0.21 0.35 64 Table 4." ></td>
	<td class="line x" title="108:161	Test on MPQA data Accuracy Precision Recall C Ours All C Ours All C Ours All t=1 0.55 0.63 0.59 0.55 0.61 0.58 0.97 0.85 0.91 t=2 0.57 0.65 0.63 0.56 0.70 0.63 0.92 0.62 0.75 t=3 0.58 0.61 0.62 0.58 0.77 0.69 0.84 0.40 0.56 t=4 0.59 0.55 0.60 0.60 0.83 0.74 0.74 0.22 0.39 t=5 0.59 0.51 0.55 0.62 0.87 0.78 0.63 0.12 0.25 t=6 0.58 0.48 0.52 0.64 0.91 0.82 0.53 0.06 0.15 random 0.50 0.54 0.50 C: Columbia word list(top 10682 words), Ours : C1+C2 (top 10682 words), All: C+Ours (top 19947 words) 5 Results We tested our system on three different data sets." ></td>
	<td class="line x" title="109:161	First, we ran the system on MPQA data provided by ARDA." ></td>
	<td class="line x" title="110:161	Second, we participated in the novelty track of TREC 2003." ></td>
	<td class="line x" title="111:161	Third, we ran it on our own test data described in Section 4.2." ></td>
	<td class="line x" title="112:161	5.1 MPQA Test The MPQA corpus contains news articles manually annotated using an annotation scheme for subjectivity (opinions and other private states that cannot be directly observed or verified." ></td>
	<td class="line x" title="113:161	(Quirk et al. , 1985), such as beliefs, emotions, sentiment, speculation, etc.)." ></td>
	<td class="line x" title="114:161	This corpus was collected and annotated as part of the summer 2002 NRRC Workshop on Multi-Perspective Question Answering (MPQA) (Wiebe et al. , 2003) sponsored by ARDA." ></td>
	<td class="line x" title="115:161	It contains 535 documents and 10,657 sentences." ></td>
	<td class="line x" title="116:161	The annotation scheme contains two main components: a type of explicit private state and speech event, and a type of expressive subjective element." ></td>
	<td class="line x" title="117:161	Several detailed attributes and strengths are annotated as well." ></td>
	<td class="line x" title="118:161	More details are provided in (Riloff et al. , 2003)." ></td>
	<td class="line x" title="119:161	Subjective sentences are defined according to their attributes and strength." ></td>
	<td class="line x" title="120:161	In order to apply our system at the sentence level, we followed their definition of subjective sentences." ></td>
	<td class="line x" title="121:161	The annotation GATE_on is used to mark speech events and direct expressions of private states." ></td>
	<td class="line x" title="122:161	The onlyfactive attribute is used to indicate whether the source of the private state or speech event is indeed expressing an emotion, opinion or other private state." ></td>
	<td class="line x" title="123:161	GATE_expressivesubjectivity annotation marks words and phrases that indirectly express a private state." ></td>
	<td class="line x" title="124:161	In our experiments, our system performed relatively well in both precision and recall." ></td>
	<td class="line x" title="125:161	We interpret our opinion markers as coinciding with (enough of) the subjective words of MPQA." ></td>
	<td class="line x" title="126:161	In order to see the relationship between the number of opinion-bearing words in a sentence and its classification by MPQA as subjective, we varied the threshold number of opinionbearing words required for subjectivity." ></td>
	<td class="line x" title="127:161	Table 4 shows accuracy, precision, and recall according to the list used and the threshold value t. The random row shows the average of ten runs of randomly assigning sentences as either subjective or objective." ></td>
	<td class="line x" title="128:161	As we can see from Table 4, our word list which is the combination of the Collection1 and Collection2, achieved higher accuracy and precision than the Columbia list." ></td>
	<td class="line x" title="129:161	However, the Columbia list achieved higher recall than ours." ></td>
	<td class="line x" title="130:161	For a fair comparison, we took top 10682 opinion-bearing words from each side and ran the same sentence classifier system." ></td>
	<td class="line x" title="131:161	2 5.2 TREC data Opinion sentence recognition was a part of the novelty track of TREC 2003 (Soboroff and Harman, 2003)." ></td>
	<td class="line x" title="132:161	The task was as follows." ></td>
	<td class="line x" title="133:161	Given a TREC topic and an ordered list of 25 documents relevant to the topic, find all the opinion-bearing sentences." ></td>
	<td class="line x" title="134:161	No definition of opinion was provided by TREC; their assessors intuitions were considered final." ></td>
	<td class="line x" title="135:161	In 2003, there were 22 opinion topics containing 21115 sentences in total." ></td>
	<td class="line x" title="136:161	The opinion topics generally related to the pros and cons of some controversial subject, such as, partial birth abortion ban, Microsoft antitrust charges, Cuban child refugee Elian Gonzalez, marijuana legalization, Clinton relationship with Lewinsky, death penalty, adoption same-sex partners, and etc. For the opinion topics, a sentence is relevant if it contains an opinion about that subject, as decided by the assessor." ></td>
	<td class="line x" title="137:161	There was no categorizing of polarity of opinion or ranking of sentences by likelihood that they contain an opinion." ></td>
	<td class="line x" title="138:161	F-score was used to measure system performance." ></td>
	<td class="line x" title="139:161	We submitted 5 separate runs, using different models." ></td>
	<td class="line x" title="140:161	Our best model among the five was Model 2." ></td>
	<td class="line x" title="141:161	It performed the second best of the 55 runs in the task, submitted by 14 participating 2 In comparison, the HP-Subj (height precision subjectivity classifier) (Riloff, 2003) produced recall 40.1 and precision 90.2 on test data using text patterns, and recall 32.9 and precision 91.3 without patterns." ></td>
	<td class="line x" title="142:161	These figures are comparable with ours." ></td>
	<td class="line x" title="143:161	65 institutions." ></td>
	<td class="line x" title="144:161	(Interestingly, and perhaps disturbingly, RUN3, which simply returned every sentence as opinion-bearing, fared extremely well, coming in 11th." ></td>
	<td class="line x" title="145:161	This model now provides a baseline for future research)." ></td>
	<td class="line x" title="146:161	After the TREC evaluation data was made available, we tested Model 1 and Model 2 further." ></td>
	<td class="line x" title="147:161	Table 5 shows the performance of each model with the two bestperforming cutoff values." ></td>
	<td class="line x" title="148:161	Table 5." ></td>
	<td class="line x" title="149:161	System performance with different models and cutoff values on TREC 2003 data Model System Parameter  F-score 0.2 0.398 Model1 0.3 0.425 0.2 0.514 Model2 0.3 0.464 5.3 Test with Our Data Section 4.2 described our manual data annotation by 3 humans." ></td>
	<td class="line x" title="150:161	Here we used the work of one human as development test data for parameter tuning." ></td>
	<td class="line x" title="151:161	The other set with 62 sentences on the topic of gun control we used as blind test data." ></td>
	<td class="line x" title="152:161	Although the TREC and MPQA data sets are larger and provide comparisons with others work, and despite the low kappa agreement values, we decided to obtain cutoff values on this data too." ></td>
	<td class="line x" title="153:161	The graphs in Figure 3 show the performance of Models 1 and 2 with different values." ></td>
	<td class="line x" title="154:161	6 Conclusions and Future Work In this paper, we described an efficient automatic algorithm to produce opinion-bearing words by combining two methods." ></td>
	<td class="line x" title="155:161	The first method used only a small set of humanannotated data." ></td>
	<td class="line x" title="156:161	We showed that one can find productive synonyms and antonyms of an opinion-bearing word through automatic expansion in WordNet and use them as feature sets of a classifier." ></td>
	<td class="line x" title="157:161	To determine a words closeness to opinion-bearing or non-opinion-bearing synoym set, we also used all synonyms of a given word as well as the word itself." ></td>
	<td class="line x" title="158:161	An additional method, harvesting words from WSJ, can compensate the first method." ></td>
	<td class="line x" title="159:161	Using the resulting list, we experimented with different cutoff thresholds in the opinion/nonopinion sentence classification on 3 different test data sets." ></td>
	<td class="line x" title="160:161	Especially on the TREC 2003 Novelty Track, the system performed well." ></td>
	<td class="line x" title="161:161	We plan in future work to pursue the automated analysis of exhortatory text in order to produce detailed argument graphs reflecting their authors argumentation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I05-2030
Opinion Extraction Using a Learning-Based Anaphora Resolution Technique
Kobayashi, Nozomi;Iida, Ryu;Inui, Kentaro;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:148	Opinion Extraction Using a Learning-Based Anaphora Resolution Technique Nozomi Kobayashi Ryu Iida Kentaro Inui Yuji Matsumoto Nara Institute of Science and Technology Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,ryu-i,inui,matsu}@is.naist.jp Abstract This paper addresses the task of extracting opinions from a given document collection." ></td>
	<td class="line x" title="2:148	Assuming that an opinion can be represented as a tuple Subject, Attribute, Value, we propose a computational method to extract such tuples from texts." ></td>
	<td class="line x" title="3:148	In this method, the main task is decomposed into (a) the process of extracting Attribute-Value pairs from a given text and (b) the process of judging whether an extracted pair expresses an opinion of the author." ></td>
	<td class="line x" title="4:148	We apply machine-learning techniques to both subtasks." ></td>
	<td class="line x" title="5:148	We also report on the results of our experiments and discuss future directions." ></td>
	<td class="line x" title="6:148	1 Introduction The explosive spread of communication on the Web has attracted increasing interest in technologies for automatically mining large numbers of message boards and blog pages for opinions and recommendations." ></td>
	<td class="line o" title="7:148	Previous approaches to the task of mining a large-scale document collection for opinions can be classified into two groups: the document classification approach and the information extraction approach." ></td>
	<td class="line oc" title="8:148	In the document classification approach, researchers have been exploring techniques for classifying documents according to semantic/sentiment orientation such as positive vs. negative (e.g.(Dave et al. , 2003; Pang and Lee, 2004; Turney, 2002))." ></td>
	<td class="line x" title="10:148	The information extraction approach, on the other hand, focuses on the task of extracting elements which constitute opinions (e.g.(Kanayama and Nasukawa, 2004; Hu and Liu, 2004; Tateishi et al. , 2001))." ></td>
	<td class="line x" title="12:148	The aim of this paper is to extract opinions that represent an evaluation of a products together with the evidence." ></td>
	<td class="line x" title="13:148	To achieve this, we consider our task from the information extraction viewpoint." ></td>
	<td class="line x" title="14:148	We term the above task opinion extraction in this paper." ></td>
	<td class="line x" title="15:148	While they can be linguistically realized in many ways, opinions on a product are in fact often expressed in the form of an attribute-value pair." ></td>
	<td class="line x" title="16:148	An attribute represents one aspect of a subject and the value is a specific language expression that qualifies or quantifies the aspect." ></td>
	<td class="line x" title="17:148	Given this observation, we approach our goal by reducing the task to a general problem of extracting four-tuples Product, Attribute, Value, Evaluation from a large-scale text collection." ></td>
	<td class="line x" title="18:148	Technology for this opinion extraction task would be useful for collecting and summarizing latent opinions from the Web." ></td>
	<td class="line x" title="19:148	A straightforward application might be generation of radar charts from collected opinions as suggested by Tateishi et al.(2004)." ></td>
	<td class="line x" title="21:148	Consider an example from the automobile domain, I am very satisfied with the powerful engine (of a car)." ></td>
	<td class="line x" title="22:148	We can extract the four-tuple CAR, engine, powerful, satisfied from this sentence." ></td>
	<td class="line x" title="23:148	Note that the distinction between Value and Evaluation is not easy." ></td>
	<td class="line x" title="24:148	Many expressions used to express a Value can also be used to express an Evaluation." ></td>
	<td class="line x" title="25:148	For this reason, we do not distinguish value and evaluation, and therefore consider the task of extracting triplets Product, Attribute, Value." ></td>
	<td class="line x" title="26:148	Another problem with opinion extraction is that we want to get only subjective opinions." ></td>
	<td class="line x" title="27:148	Given this setting, the opinion extraction task can be decomposed into two subtasks: extraction of attributevalue pairs related to a product and determination of its subjectivity." ></td>
	<td class="line x" title="28:148	As we discuss in section 3, an attribute and its value may not appear in a fixed expression and may be separated." ></td>
	<td class="line x" title="29:148	In some cases, the attribute may be missing from a sentence." ></td>
	<td class="line x" title="30:148	In this respect, finding the attribute of a value is similar to finding the missing antecedent of an anaphoric expression." ></td>
	<td class="line x" title="31:148	In this paper, we discuss the similarities and differences between opinion extraction and anaphora resolution." ></td>
	<td class="line x" title="32:148	Then, we apply a machine learning-based method used for anaphora reso173 lution to the opinion extraction problem and report on our experiments conducted on a domainrestricted set of Japanese texts excerpted from review pages on the Web." ></td>
	<td class="line x" title="33:148	2 Related work In this section, we discuss previous approaches to the opinion extraction problem." ></td>
	<td class="line x" title="34:148	In the patternbased approach (Murano and Sato, 2003; Tateishi et al. , 2001), pre-defined extraction patterns and a list of evaluative expressions are used." ></td>
	<td class="line x" title="35:148	These extraction patterns and the list of evaluation expressions need to be manually created." ></td>
	<td class="line x" title="36:148	However, as is the case in information extraction, manual construction of rules may require considerable cost to provide sufficient coverage and accuracy." ></td>
	<td class="line x" title="37:148	Hu and Liu (2004) attempt to extract the attributes of target products on which customers have expressed their opinions using association mining, and to determine whether the opinions are positive or negative." ></td>
	<td class="line x" title="38:148	Their aim is quite similar to our aim, however, our work differs from theirs in that they do not identify the value corresponding to an attribute." ></td>
	<td class="line x" title="39:148	Their aim is to extract the attributes and their semantic orientations." ></td>
	<td class="line x" title="40:148	Taking the semantic parsing-based approach, Kanayama and Nasukawa (2004) apply the idea of transfer-based machine translation to the extraction of attribute-value pairs." ></td>
	<td class="line x" title="41:148	They regard the extraction task as translation from a text to a sentiment unit which consists of a sentiment value, a predicate, and its arguments." ></td>
	<td class="line x" title="42:148	Their idea is to replace the translation patterns and bilingual lexicons with sentiment expression patterns and a lexicon that specifies the polarity of expressions." ></td>
	<td class="line x" title="43:148	Their method first analyzes the predicateargument structure of a given input sentence making use of the sentence analysis component of an existing machine translation engine, and then extracts a sentiment unit from it, if any, using the transfer component." ></td>
	<td class="line x" title="44:148	One important problem the semantic parsing approach encounters is that opinion expressions often appear with anaphoric expressions and ellipses, which need to be resolved to accomplish the opinion extraction task." ></td>
	<td class="line x" title="45:148	Our investigation of an opinion-tagged Japanese corpus (described below) showed that 30% of the attribute-value pairs we found did not have a direct syntactic dependency relation within the sentence, mostly due to ellipsis." ></td>
	<td class="line x" title="46:148	For example 1, dezain-wa a hen-daga watashi-wa -ga suki-da v design a weird I [it] like v (The design is weird, but I like it.)" ></td>
	<td class="line x" title="47:148	This type of case accounted for 46 out of 100 pairs that did not have direct dependency relations." ></td>
	<td class="line x" title="48:148	To analyze predicate argument structure robustly, we have to solve this problem." ></td>
	<td class="line x" title="49:148	In the next section, we discuss the similarity between the anaphora resolution task and the opinion extraction task and propose to apply to opinion extraction a method used for anaphora resolution." ></td>
	<td class="line x" title="50:148	3 Method for opinion extraction 3.1 Analogy with anaphora resolution We consider the task of extracting opinion tuples Product, Attribute, Value from review sites and message boards on the Web dedicated to providing and exchanging information about retail goods." ></td>
	<td class="line x" title="51:148	On these Web pages, products are often specified clearly and so it is frequently a trivial job to extract the information for the Product slot." ></td>
	<td class="line x" title="52:148	We therefore in this paper focus on the problem of extracting Attribute, Value pairs." ></td>
	<td class="line x" title="53:148	In the process of attribute-value pair identification for opinion extraction, we need to deal with the following two cases: (a) both a value and its corresponding attribute appear in the text, and (b) a value appears in the text while its attribute is missing since it is inferable form the value expression and the context." ></td>
	<td class="line x" title="54:148	The upper half of Figure 1 illustrates these two cases in the automobile domain." ></td>
	<td class="line x" title="55:148	In (b), the writer is talking about the size of the car, but the expression size is not explicitly mentioned in the text." ></td>
	<td class="line x" title="56:148	In addition, (b) includes the case where the writer evaluates the product itself." ></td>
	<td class="line x" title="57:148	For example, Im very satisfied with my car!: in this case, a value expression satisfied evaluates the product as a whole, therefore a corresponding attribute does not exists." ></td>
	<td class="line x" title="58:148	For the case (a), we first identify a value expression (like in Figure 1) in a given text and then look for the corresponding attribute in the text." ></td>
	<td class="line x" title="59:148	Since we also see the case (b), on the other hand, we additionally need to consider the problem of whether the corresponding attribute of the identified value expression appears in the text or not." ></td>
	<td class="line x" title="60:148	The structure of these problems is analogous to that of anaphora resolution; namely, there are exactly two cases in anaphora resolution that have a clear correspondence with the above two cases as illustrated in Figure 1: in (a) the noun phrase (NP) is anaphoric; namely, the NPs antecedent appears in the text, and in (b) the noun phrase is non-anaphoric." ></td>
	<td class="line x" title="61:148	A non-anaphoric NP is either ex1  a denotes the word sequence corresponding to the Attribute." ></td>
	<td class="line x" title="62:148	Likewise, we also use  v for the Value." ></td>
	<td class="line x" title="63:148	174 Taro-wa shisetsu-wog17128g15458-gag17129shirabe-te houkokusho-o sakusei-shita (a) (b) Dezain-wa hen-desuga watashi-wa g17128g15458-gag17129 suki-desu g16877g16877g16877g16877g16877 (g15458-ga) Ookii-kedo atsukai-yasui ( it ) large but easy to handle (a) (b) anaphora resolution opinion extraction anaphorantecedent Attribute Value (The design is weird, but I like it.)" ></td>
	<td class="line x" title="64:148	omitted Attribute (It is large, but easy to handle) Tar-NOM attendance-ACC noted report-ACC wrote (Taro noted the attendance and wrote a report.)" ></td>
	<td class="line x" title="65:148	design-NOM weird I-NOM ( it ) like Value Onaka-ga hetta-node kaerouto (g15458-ga) omou hungry go home (I) exophora anaphor (I think Ill go home because Im hungry.)" ></td>
	<td class="line x" title="66:148	Figure 1: Similarity between opinion extraction and anaphora resolution ophoric (i.e. the NP has an implicit referent) or indefinite." ></td>
	<td class="line x" title="67:148	While the figure shows Japanese examples, the similarity between anaphora resolution and opinion extraction is language independent." ></td>
	<td class="line x" title="68:148	This analogy naturally leads us to think of applying existing techniques for anaphora resolution to our opinion extraction task since anaphora resolution has been studied for a considerably longer period in a wider range of disciplines as we briefly review below." ></td>
	<td class="line x" title="69:148	3.2 Existing techniques for anaphora resolution Corpus-based empirical approaches to anaphora resolution have been reasonably successful." ></td>
	<td class="line x" title="70:148	This approach, as exemplified by (Soon et al. , 2001; Iida et al. , 2003; Ng, 2004), is cost effective, while achieving a better performance than the best-performing rule-based systems for the test sets of MUC-6 and MUC-7 2." ></td>
	<td class="line x" title="71:148	As suggested by Figure 1, anaphora resolution can be decomposed into two subtasks: anaphoricity determination and antecedent identification." ></td>
	<td class="line x" title="72:148	Anaphoricity determination is the task of judging whether a given NP is anaphoric or nonanaphoric." ></td>
	<td class="line x" title="73:148	Recent research advances have provided several important findings as follows:  Learning-based methods for antecedent identification can also benefit from the use of linguistic clues inspired by Centering Theory (Grosz et al. , 1995)." ></td>
	<td class="line x" title="74:148	 One useful clue for anaphoricity determination is the availability of a plausible candidate for the antecedent." ></td>
	<td class="line x" title="75:148	If an appropriate candidate for the antecedent is found in the preceding discourse context, the NP is likely to be anaphoric." ></td>
	<td class="line x" title="76:148	For these reasons, an anaphora resolution model performs best if it carries out the following pro2 The 7th Message Understanding Conference (1998): www.itl.nist.gov/iaui/894.02/related projects/muc/ interia seki Dezain-wa hen-desuga watashi-wa suki-desu g16877g16877g16877 interior seat design-NOM weird I-NOM like candidates design like interior like seat like design like candidate attributes real attribute Select the best candidate attribute Decide whether the candidate attribute stands for the real attribute or not design likedesign like real attribute pairedness determination attribute identification opinionhood determination Judge whether the pair expresses an opinion or not opinion Attribute dictionary Value dictionary interior seat design like good ." ></td>
	<td class="line x" title="77:148	target value initialization pair extraction Figure 2: Process of opinion extraction cess in the given order (Iida et al. , 2005): (1) Antecedent identification: Given an NP, identify the best candidate antecedent for it, and (2) Anaphoricity determination: Judge whether the candidate really stands for the true antecedent of the NP." ></td>
	<td class="line x" title="78:148	3.3 An opinion extraction model inspired by analogy with anaphora resolution As illustrated in Figure 2, an opinion extraction model derived from the aforementioned analogy with anaphora resolution as follows: 1." ></td>
	<td class="line x" title="79:148	Initialization: Identify attribute and value candidates by dictionary lookup 2." ></td>
	<td class="line x" title="80:148	Attribute identification: Select a value and identify the best candidate attribute corresponding to the value 3." ></td>
	<td class="line x" title="81:148	Pairedness determination: Decide whether the candidate attribute stands for the real attribute of the value or not (i.e. the value has no explicit corresponding attribute in the text) 4." ></td>
	<td class="line x" title="82:148	Opinionhood determination: Judge whether the obtained attribute-value pair 3 expresses an opinion or not Here, the attribute identification and pairedness determination processes respectively correspond to the antecedent identification and anaphoricity determination processes in anaphora resolution." ></td>
	<td class="line x" title="83:148	Note that our opinion extraction task requires an additional subtask, opinionhood determination  an attribute-value pair appearing in a text does not necessarily constitute an opinion." ></td>
	<td class="line x" title="84:148	We elaborate on the notion of opinionhood in section 4.1." ></td>
	<td class="line x" title="85:148	From the above discussion, we can expect that the findings for anaphora resolution mentioned in 3.2 stated above apply to opinion extraction as well." ></td>
	<td class="line x" title="86:148	In fact, the information about the candidate 3 For simplicity, we call a value both with and without an attribute uniformly by the term attribute-value pair unless the distinction is important." ></td>
	<td class="line x" title="87:148	175 attribute is likely to be useful for pairedness determination." ></td>
	<td class="line x" title="88:148	We therefore expect that carrying out attribute identification before pairedness determination should outperform the counterpart model which executes the two subtasks in the reversed order." ></td>
	<td class="line x" title="89:148	The same analogy also applies to opinionhood determination; namely, we expect that opinion determination is bet performed after attribute determination." ></td>
	<td class="line x" title="90:148	Furthermore, our opinion extraction model also can be implemented in a totally machine learning-based fashion." ></td>
	<td class="line x" title="91:148	4 Evaluation We conducted experiments with Japanese Web documents to empirically evaluate the performance of our opinion extraction model, focusing particularly on the validity of the analogy discussed in the previous section." ></td>
	<td class="line x" title="92:148	4.1 Opinionhood In these experiments, we define an opinion as follows: An opinion is a description that expresses the writers subjective evaluation of a particular subject or a certain aspect of it." ></td>
	<td class="line x" title="93:148	By this definition, we exclude requests, factual or counter-factual descriptions and hearsay evidence from our target opinions." ></td>
	<td class="line x" title="94:148	For example, The engine is powerful is an opinion, while a counterfactual sentence such as If only the engine were more powerful is not regarded as opinion." ></td>
	<td class="line x" title="95:148	4.2 Opinion-tagged corpus We created an opinion-tagged Japanese corpus consisting of 288 review articles in the automobile domain (4,442 sentences)." ></td>
	<td class="line x" title="96:148	While it is not easy to judge whether an expression is a value or an attribute, we asked the annotator to identify attribute and value expressions according to their subjective judgment." ></td>
	<td class="line x" title="97:148	If some attributes are in a hierarchical relation with each other, we asked the annotator to choose the attribute lowest in the hierarchy as the attribute of the value." ></td>
	<td class="line x" title="98:148	For example, in a sound system with poor sound, only sound is annotated as the attribute of the value poor." ></td>
	<td class="line x" title="99:148	The corpus contains 2,191 values with an attribute and 420 values without an attribute." ></td>
	<td class="line x" title="100:148	Most of the attributes appear in the same sentence as their corresponding values or in the immediately preceding sentence (99% of the total number of pairs)." ></td>
	<td class="line x" title="101:148	Therefore, we extract attributes and their corresponding values from the same sentence or from the preceding sentence." ></td>
	<td class="line x" title="102:148	4.3 Experimental method As preprocessing, we analyzed the opiniontagged corpus using the Japanese morphological analyzer ChaSen 4 and the Japanese dependency structure analyzer CaboCha 5 . We used Support Vector Machines to train the models for attribute identification, pairedness determination and opinionhood determination." ></td>
	<td class="line x" title="103:148	We used the 2nd order polynomial kernel as the kernel function for SVMs." ></td>
	<td class="line x" title="104:148	Evaluation was performed by 10-fold cross validation using all the data." ></td>
	<td class="line x" title="105:148	4.3.1 Dictionaries We use dictionaries for identification of attribute and value candidates." ></td>
	<td class="line x" title="106:148	We constructed a attribute dictionary and a value dictionary from review articles about automobiles (230,000 sentences in total) using the semi-automatic method proposed by Kobayashi et al.(2004)." ></td>
	<td class="line x" title="108:148	The data used in this process was different from the opinion-tagged corpus." ></td>
	<td class="line x" title="109:148	Furthermore, we added to the dictionaries expressions which frequently appearing in the opinion-tagged corpus." ></td>
	<td class="line x" title="110:148	The final size of the dictionaries was 3,777 attribute expressions and 3,950 value expressions." ></td>
	<td class="line x" title="111:148	4.3.2 Order of model application To examine the effects of appropriately choosing the order of model application we mentioned in the previous section, we conducted four experiments using different orders (AI indicates attribute identification, PD indicates pairedness determination and OD indicates opinion determination): Proc.1: ODPDAI, Proc.2: ODAIPD Proc.3: AIODPD, Proc.4: AIPDOD Note that Proc.4 is our proposed ordering." ></td>
	<td class="line x" title="112:148	In addition to these models, we adopted a baseline model." ></td>
	<td class="line x" title="113:148	In this model, if the candidate value and a candidate attribute are connected via a dependency relation, the candidate value is judged to have an attribute." ></td>
	<td class="line x" title="114:148	When none of the candidate attributes have a dependency relation, the candidate value is judged not to have an attribute." ></td>
	<td class="line x" title="115:148	We adopted the tournament model for attribute identification (Iida et al. , 2003)." ></td>
	<td class="line x" title="116:148	This model implements a pairwise comparison (i.e. a match) between two candidates in reference to the given value treating it as a binary classification problem, and conducting a tournament which consists of a series of matches, in which the one that prevails through to the final round is declared the 4 http://chasen.naist.jp/ 5 http://chasen.org/taku/software/cabocha/ 176 winner, namely, it is identified as the most likely candidate attribute." ></td>
	<td class="line x" title="117:148	Each of the matches is conducted as a binary classification task in which one or other of the candidate wins." ></td>
	<td class="line x" title="118:148	The pairedness determination task and the opinionhood determination task are also binary classification tasks." ></td>
	<td class="line x" title="119:148	In Proc.1, since pair identification is conducted before finding the best candidate attribute, we used Soon et al.s model (Soon et al. , 2001) for pairedness determination." ></td>
	<td class="line x" title="120:148	This model picks up each possible candidate attribute for a value and determines if it is the attribute for that value." ></td>
	<td class="line x" title="121:148	If all the candidates are determined not to be the attribute, the value is judged not to have an attribute." ></td>
	<td class="line x" title="122:148	In Proc.4, we can use the information about whether the value has a corresponding attribute or not for opinionhood determination." ></td>
	<td class="line x" title="123:148	We therefore create two separate models for when the value does and does not have an attribute." ></td>
	<td class="line x" title="124:148	4.3.3 Features We extracted the following two types of features from the candidate attribute and the candidate value: (a) surface spelling and part-of-speech of the target value expression, as well as those of its dependent phrase and those in its depended phrase(s) (b) relation between the target value and candidate attribute (distance between them, existence of dependency, existence of a cooccurrence relation) We extracted (b) if the model could use both the attribute and the value information." ></td>
	<td class="line x" title="125:148	Existence of a co-occurrence relation is determined by reference to a predefined co-occurrence list that contains attribute-value pair information such as height of vehicle  low." ></td>
	<td class="line x" title="126:148	We created the list from the 230,000 sentences described in section 4.3.1 by applying the attribute and value dictionary and extracting attribute-value pairs if there is a dependency relation between the attribute and the value." ></td>
	<td class="line x" title="127:148	The number of pairs we extracted was about 48,000." ></td>
	<td class="line x" title="128:148	4.4 Results Table 1 shows the results of opinion extraction." ></td>
	<td class="line x" title="129:148	We evaluated the results by recall R and precision P defined as follows (For simplicity, we substitute A-V for attribute-value pair): R = correctly extracted A-V opinions total number of A-V opinions, P = correctly extracted A-V opinions total number of A-V opinions found by the system . In order to demonstrate the effectiveness of the information about the candidate attribute, we evaluated the results of pair extraction and opinionhood determination separately." ></td>
	<td class="line x" title="130:148	Table 2 shows the results." ></td>
	<td class="line x" title="131:148	In the pair extraction, we assume that the value is given, and evaluate how successfully attribute-value pairs are extracted." ></td>
	<td class="line x" title="132:148	4.5 Discussions As Table 1 shows, our proposed ordering is outperformed on the recall in Proc.3, however, the precision is higher than Proc.3 and get the best Fmeasure." ></td>
	<td class="line x" title="133:148	In what follows, we discuss the results of pair extraction and opinionhood determination." ></td>
	<td class="line x" title="134:148	Pair extraction From Table 2, we can see that carrying out attribute identification before pairedness determination outperforms the reverse ordering by 11% better precision and 3% better recall." ></td>
	<td class="line x" title="135:148	This result supports our expectation that knowledge of attribute information assists attributevalue pair extraction." ></td>
	<td class="line x" title="136:148	Focusing on the rows labeled (dependency) and (no dependency) in Table 2, while 80% of the attribute-value pairs in a direct dependency relation are successfully extracted with high precision, the model achieves only 51.7% recall with 61.7% precision for the cases where an attribute and value are not in a direct dependency relation." ></td>
	<td class="line x" title="137:148	According to our error analysis, a major source of errors lies in the attribute identification task." ></td>
	<td class="line x" title="138:148	In this experiment, the precision of attribute identification is 78%." ></td>
	<td class="line x" title="139:148	A major reason for this problem was that the true attributes did not exist in our dictionary." ></td>
	<td class="line x" title="140:148	In addition, a major cause of error in the pair determination stage is cases where an attribute appearing in the preceding sentence causes a false decision." ></td>
	<td class="line x" title="141:148	We need to conduct further investigations in order to resolve these problems." ></td>
	<td class="line x" title="142:148	Opinionhood determination Table 2 also shows that carrying out attribute identification followed by opinionhood determination outperforms the reverse ordering, which supports our expectation that knowing the attribute information aids opinionhood determination." ></td>
	<td class="line x" title="143:148	While it produces better results, our proposed method still has room for improvement in both precision and recall." ></td>
	<td class="line x" title="144:148	Our current error analysis has not identified particular error patterns  the types of errors are very diverse." ></td>
	<td class="line x" title="145:148	However, we need to at least address the issue of modifying the feature set to make the model more sensitive to modality-oriented distinctions such as subjunctive and conditional expressions." ></td>
	<td class="line x" title="146:148	177 Table 1: The precision and the recall for opinion extraction procedure value with attribute value without attribute attribute-value pairs baseline precision 60.5% (1130/1869) 10.6% (249/2340) 32.8% (1379/4209) recall 51.6% (1130/2191) 59.3% (249/420) 52.8% (1379/2611) F-measure 55.7 21.0 40.5 Proc.1 precision 47.3% (864/1828) 21.6% ( 86/399) 42.7% ( 950/2227) recall 39.4% (864/2191) 20.5% ( 86/420) 36.4% ( 950/2611) F-measure 43.0 21.0 39.3 Proc.2 precision 63.0% (1074/1706) 38.0% (198/521) 57.1% (1272/2227) recall 49.0% (1074/2191) 47.1% (198/420) 48.7% (1272/2611) F-measure 55.1 42.0 52.6 Proc.3 precision 74.9% (1277/1632) 29.1% (151/519) 63.8% (1373/2151) recall 55.8% (1222/2191) 36.0% (151/420) 52.6% (1373/2611) F-measure 64.0 32.2 57.7 Proc.4 precision 80.5% (1175/1460) 30.2% (150/497) 67.7% (1325/1957) recall 53.6% (1175/2191) 35.7% (150/420) 50.7% (1325/2611) F-measure 64.4 32.7 58.0 Table 2: The result of pair extraction and opinionhood determination procedure precision recall pair extraction baseline (dependency) 71.1% (1385/1929) 63.2% (1385/2191) PDAI 65.3% (1579/2419) 72.1% (1579/2191) AIPD 76.6% (1645/2148) 75.1% (1645/2191) (dependency) 87.7% (1303/1486) 79.6% (1303/1637) (no dependency) 51.7% ( 342/ 662) 61.7% ( 342/ 554) opinionhood determination OD 74.0% (1554/2101) 60.2% (1554/2581) AIOD 82.2% (1709/2078) 66.2% (1709/2581) 5 Conclusion In this paper, we have proposed a machine learning-based method for the extraction of opinions on consumer products by reducing the problem to that of extracting attribute-value pairs from texts." ></td>
	<td class="line x" title="147:148	We have pointed out the similarity between the tasks of anaphora resolution and opinion extraction, and have applied the machine learningbased method designed for anaphora resolution to opinion extraction." ></td>
	<td class="line x" title="148:148	The experimental results reported in this paper show that identifying the corresponding attribute for a given value expression is effective in both pairedness determination and opinionhood determination." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-1015
Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales
Pang, Bo;Lee, Lillian;"></td>
	<td class="line x" title="1:189	Proceedings of the 43rd Annual Meeting of the ACL, pages 115124, Ann Arbor, June 2005." ></td>
	<td class="line x" title="2:189	c2005 Association for Computational Linguistics Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales Bo Panga0a2a1a3 and Lillian Leea0a2a1a4a5a1a3 (1) Department of Computer Science, Cornell University (2) Language Technologies Institute, Carnegie Mellon University (3) Computer Science Department, Carnegie Mellon University Abstract We address the rating-inference problem, wherein rather than simply decide whether a review is thumbs up or thumbs down, as in previous sentiment analysis work, one must determine an authors evaluation with respect to a multi-point scale (e.g. , one to ve stars )." ></td>
	<td class="line x" title="3:189	This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, three stars is intuitively closer to four stars than to one star." ></td>
	<td class="line x" title="4:189	We rst evaluate human performance at the task." ></td>
	<td class="line x" title="5:189	Then, we apply a metaalgorithm, based on a metric labeling formulation of the problem, that alters a given a6 -ary classi ers output in an explicit attempt to ensure that similar items receive similar labels." ></td>
	<td class="line x" title="6:189	We show that the meta-algorithm can provide signi cant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem." ></td>
	<td class="line x" title="7:189	1 Introduction There has recently been a dramatic surge of interest in sentiment analysis, as more and more people become aware of the scienti c challenges posed and the scope of new applications enabled by the processing of subjective language." ></td>
	<td class="line x" title="8:189	(The papers collected by Qu, Shanahan, and Wiebe (2004) form a representative sample of research in the area)." ></td>
	<td class="line oc" title="9:189	Most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs. negative (Turney, 2002; Pang, Lee, and Vaithyanathan, 2002; Dave, Lawrence, and Pennock, 2003; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="10:189	But it is often helpful to have more information than this binary distinction provides, especially if one is ranking items by recommendation or comparing several reviewers opinions: example applications include collaborative ltering and deciding which conference submissions to accept." ></td>
	<td class="line x" title="11:189	Therefore, in this paper we consider generalizing to ner-grained scales: rather than just determine whether a review is thumbs up or not, we attempt to infer the authors implied numerical rating, such as three stars or four stars . Note that this differs from identifying opinion strength (Wilson, Wiebe, and Hwa, 2004): rants and raves have the same strength but represent opposite evaluations, and referee forms often allow one to indicate that one is very con dent (high strength) that a conference submission is mediocre (middling rating)." ></td>
	<td class="line x" title="12:189	Also, our task differs from ranking not only because one can be given a single item to classify (as opposed to a set of items to be ordered relative to one another), but because there are settings in which classi cation is harder than ranking, and vice versa." ></td>
	<td class="line x" title="13:189	One can apply standarda6 -ary classi ers or regression to this rating-inference problem; independent work by Koppel and Schler (2005) considers such 115 methods." ></td>
	<td class="line x" title="14:189	But an alternative approach that explicitly incorporates information about item similarities together with label similarity information (for instance, one star is closer to two stars than to four stars ) is to think of the task as one of metric labeling (Kleinberg and Tardos, 2002), where label relations are encoded via a distance metric." ></td>
	<td class="line x" title="15:189	This observation yields a meta-algorithm, applicable to both semi-supervised (via graph-theoretic techniques) and supervised settings, that alters a given a6 -ary classi ers output so that similar items tend to be assigned similar labels." ></td>
	<td class="line x" title="16:189	In what follows, we rst demonstrate that humans can discern relatively small differences in (hidden) evaluation scores, indicating that rating inference is indeed a meaningful task." ></td>
	<td class="line x" title="17:189	We then present three types of algorithms one-vs-all, regression, and metric labeling that can be distinguished by how explicitly they attempt to leverage similarity between items and between labels." ></td>
	<td class="line x" title="18:189	Next, we consider what item similarity measure to apply, proposing one based on the positive-sentence percentage." ></td>
	<td class="line x" title="19:189	Incorporating this new measure within the metriclabeling framework is shown to often provide signi cant improvements over the other algorithms." ></td>
	<td class="line x" title="20:189	We hope that some of the insights derived here might apply to other scales for text classifcation that have been considered, such as clause-level opinion strength (Wilson, Wiebe, and Hwa, 2004); affect types like disgust (Subasic and Huettner, 2001; Liu, Lieberman, and Selker, 2003); reading level (Collins-Thompson and Callan, 2004); and urgency or criticality (Horvitz, Jacobs, and Hovel, 1999)." ></td>
	<td class="line x" title="21:189	2 Problem validation and formulation We rst ran a small pilot study on human subjects in order to establish a rough idea of what a reasonable classi cation granularity is: if even people cannot accurately infer labels with respect to a ve-star scheme with half stars, say, then we cannot expect a learning algorithm to do so." ></td>
	<td class="line x" title="22:189	Indeed, some potential obstacles to accurate rating inference include lack of calibration (e.g. , what an understated author intends as high praise may seem lukewarm), author inconsistency at assigning ne-grained ratings, and Rating diff." ></td>
	<td class="line x" title="23:189	Pooled Subject 1 Subject 2 a7 or more 100% 100% (35) 100% (15) 2 (e.g. , 1 star) 83% 77% (30) 100% (11) 1 (e.g. , a0a4 star) 69% 65% (57) 90% (10) 0 55% 47% (15) 80% ( 5) Table 1: Human accuracy at determining relative positivity." ></td>
	<td class="line x" title="24:189	Rating differences are given in notches . Parentheses enclose the number of pairs attempted." ></td>
	<td class="line x" title="25:189	ratings not entirely supported by the text1." ></td>
	<td class="line x" title="26:189	For data, we rst collected Internet movie reviews in English from four authors, removing explicit rating indicators from each documents text automatically." ></td>
	<td class="line x" title="27:189	Now, while the obvious experiment would be to ask subjects to guess the rating that a review represents, doing so would force us to specify a xed rating-scale granularity in advance." ></td>
	<td class="line x" title="28:189	Instead, we examined peoples ability to discern relative differences, because by varying the rating differences represented by the test instances, we can evaluate multiple granularities in a single experiment." ></td>
	<td class="line x" title="29:189	Speci cally, at intervals over a number of weeks, we authors (a non-native and a native speaker of English) examined pairs of reviews, attemping to determine whether the rst review in each pair was (1) more positive than, (2) less positive than, or (3) as positive as the second." ></td>
	<td class="line x" title="30:189	The texts in any particular review pair were taken from the same author to factor out the effects of cross-author divergence." ></td>
	<td class="line x" title="31:189	As Table 1 shows, both subjects performed perfectly when the rating separation was at least 3 notches in the original scale (we de ne a notch as a half star in a fouror ve-star scheme and 10 points in a 100-point scheme)." ></td>
	<td class="line x" title="32:189	Interestingly, although human performance drops as rating difference decreases, even at a one-notch separation, both subjects handily outperformed the random-choice baseline of 33%." ></td>
	<td class="line x" title="33:189	However, there was large variation in accuracy between subjects.2 1For example, the critic Dennis Schwartz writes that sometimes the review itself [indicates] the letter grade should have been higher or lower, as the review might fail to take into consideration my overall impression of the lm which I hope to capture in the grade (http://www.sover.net/ ozus/cinema.htm)." ></td>
	<td class="line x" title="34:189	2One contributing factor may be that the subjects viewed disjoint document sets, since we wanted to maximize experimental coverage of the types of document pairs within each difference class." ></td>
	<td class="line x" title="35:189	We thus cannot report inter-annotator agreement, 116 Because of this variation, we de ned two different classi cation regimes." ></td>
	<td class="line x" title="36:189	From the evidence above, a three-class task (categories 0, 1, and 2 essentially negative, middling, and positive, respectively) seems like one that most people would do quite well at (but we should not assume 100% human accuracy: according to our one-notch results, people may misclassify borderline cases like 2.5 stars)." ></td>
	<td class="line x" title="37:189	Our study also suggests that people could do at least fairly well at distinguishing full stars in a zeroto four-star scheme." ></td>
	<td class="line x" title="38:189	However, when we began to construct ve-category datasets for each of our four authors (see below), we found that in each case, either the most negative or the most positive class (but not both) contained only about 5% of the documents." ></td>
	<td class="line x" title="39:189	To make the classes more balanced, we folded these minority classes into the adjacent class, thus arriving at a four-class problem (categories 0-3, increasing in positivity)." ></td>
	<td class="line x" title="40:189	Note that the four-class problem seems to offer more possibilities for leveraging class relationship information than the three-class setting, since it involves more class pairs." ></td>
	<td class="line oc" title="41:189	Also, even the two-category version of the rating-inference problem for movie reviews has proven quite challenging for many automated classi cation techniques (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002)." ></td>
	<td class="line x" title="42:189	We applied the above two labeling schemes to a scale dataset3 containing four corpora of movie reviews." ></td>
	<td class="line x" title="43:189	All reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences; the motivation for the latter step is that it has previously aided positive vs. negative classi cation (Pang and Lee, 2004)." ></td>
	<td class="line x" title="44:189	All of the 1770, 902, 1307, or 1027 documents in a given corpus were written by the same author." ></td>
	<td class="line x" title="45:189	This decision facilitates interpretation of the results, since it factors out the effects of different choices of methods for calibrating authors scales.4 We point out that but since our goal is to recover a reviewers true recommendation, reader-author agreement is more relevant." ></td>
	<td class="line x" title="46:189	While another factor might be degree of English uency, in an informal experiment (six subjects viewing the same three pairs), native English speakers made the only two errors." ></td>
	<td class="line x" title="47:189	3Available at http://www.cs.cornell.edu/People/pabo/moviereview-data as scale dataset v1.0." ></td>
	<td class="line x" title="48:189	4From the Rotten Tomatoes websites FAQ: star systems are not consistent between critics." ></td>
	<td class="line x" title="49:189	For critics like Roger Ebert and James Berardinelli, 2.5 stars or lower out of 4 stars is always negative." ></td>
	<td class="line x" title="50:189	For other critics, 2.5 stars can either be positive it is possible to gather author-speci c information in some practical applications: for instance, systems that use selected authors (e.g. , the Rotten Tomatoes movie-review website where, we note, not all authors provide explicit ratings) could require that someone submit rating-labeled samples of newlyadmitted authors work." ></td>
	<td class="line x" title="51:189	Moreover, our results at least partially generalize to mixed-author situations (see Section 5.2)." ></td>
	<td class="line x" title="52:189	3 Algorithms Recall that the problem we are considering is multicategory classi cation in which the labels can be naturally mapped to a metric space (e.g. , points on a line); for simplicity, we assume the distance metric a8a10a9a12a11a14a13a15a11a17a16a19a18a21a20a23a22a11a25a24a26a11a17a16a27a22 throughout." ></td>
	<td class="line x" title="53:189	In this section, we present three approaches to this problem in order of increasingly explicit use of pairwise similarity information between items and between labels." ></td>
	<td class="line x" title="54:189	In order to make comparisons between these methods meaningful, we base all three of them on Support Vector Machines (SVMs) as implemented in Joachims (1999) a28a30a29a32a31a34a33a36a35a38a37a40a39a42a41 package." ></td>
	<td class="line x" title="55:189	3.1 One-vs-all The standard SVM formulation applies only to binary classi cation." ></td>
	<td class="line x" title="56:189	One-vs-all (OVA) (Rifkin and Klautau, 2004) is a common extension to the a6 -ary case." ></td>
	<td class="line x" title="57:189	Training consists of building, for each label a11, an SVM binary classi er distinguishing label a11 from not-a11 . We consider the nal output to be a label preference function a43a45a44a27a46a2a47 a9a49a48a50a13a15a11a51a18, de ned as the signed distance of (test) item a48 to the a11 side of the a11 vs. not-a11 decision plane." ></td>
	<td class="line x" title="58:189	Clearly, OVA makes no explicit use of pairwise label or item relationships." ></td>
	<td class="line x" title="59:189	However, it can perform well if each class exhibits suf ciently distinct language; see Section 4 for more discussion." ></td>
	<td class="line x" title="60:189	3.2 Regression Alternatively, we can take a regression perspective by assuming that the labels come from a discretization of a continuous function a52 mapping from the or negative." ></td>
	<td class="line x" title="61:189	Even though Eric Lurio uses a 5 star system, his grading is very relaxed." ></td>
	<td class="line x" title="62:189	So, 2 stars can be positive." ></td>
	<td class="line x" title="63:189	Thus, calibration may sometimes require strong familiarity with the authors involved, as anyone who has ever needed to reconcile con icting referee reports probably knows." ></td>
	<td class="line x" title="64:189	117 feature space to a metric space.5 If we choose a52 from a family of suf ciently gradual functions, then similar items necessarily receive similar labels." ></td>
	<td class="line x" title="65:189	In particular, we consider linear, a53 -insensitive SVM regression (Vapnik, 1995; Smola and Schcurrency1olkopf, 1998); the idea is to nd the hyperplane that best ts the training data, but where training points whose labels are within distance a53 of the hyperplane incur no loss." ></td>
	<td class="line x" title="66:189	Then, for (test) instance a48, the label preference function a43a55a54a12a56a58a57 a9a49a48a59a13a15a11a60a18 is the negative of the distance between a11 and the value predicted for a48 by the tted hyperplane function." ></td>
	<td class="line x" title="67:189	Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods." ></td>
	<td class="line x" title="68:189	However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms." ></td>
	<td class="line x" title="69:189	3.3 Metric labeling Regression implicitly encodes the similar items, similar labels heuristic, in that one can restrict consideration to gradual functions." ></td>
	<td class="line x" title="70:189	But we can also think of our task as a metric labeling problem (Kleinberg and Tardos, 2002), a special case of the maximum a posteriori estimation problem for Markov random elds, to explicitly encode our desideratum." ></td>
	<td class="line x" title="71:189	Suppose we have an initial label preference function a43 a9a49a48a50a13a15a11a51a18, perhaps computed via one of the two methods described above." ></td>
	<td class="line x" title="72:189	Also, let a8 be a distance metric on labels, and let a6a55a6a62a61 a9a49a48a63a18 denote the a64 nearest neighbors of item a48 according to some item-similarity function a65a5a66a12a67 . Then, it is quite natural to pose our problem as nding a mapping of instances a48 to labels a11a60a68 (respecting the original labels of the training instances) that minimizes a69 a68a14a70 test a71a72 a24 a43 a9a49a48a50a13a15a11 a68 a18a45a73a26a74 a69 a75 a70a77a76a78a76a80a79a42a81a82a68a51a83a78a84 a9a85a8a86a9a12a11 a68 a13a15a11 a75 a18a2a18 a65a5a66a12a67 a9a49a48a50a13a2a87a88a18a27a89a90a91a13 where a84 is monotonically increasing (we chose a84 a9a85a8a92a18a93a20a94a8 unless otherwise speci ed) and a74 is a trade-off and/or scaling parameter." ></td>
	<td class="line x" title="73:189	(The inner summation is familiar from work in locally-weighted 5We discuss the ordinal regression variant in Section 6." ></td>
	<td class="line x" title="74:189	learning6 (Atkeson, Moore, and Schaal, 1997))." ></td>
	<td class="line x" title="75:189	In a sense, we are using explicit item and label similarity information to increasingly penalize the initial classi er as it assigns more divergent labels to similar items." ></td>
	<td class="line x" title="76:189	In this paper, we only report supervised-learning experiments in which the nearest neighbors for any given test item were drawn from the training set alone." ></td>
	<td class="line x" title="77:189	In such a setting, the labeling decisions for different test items are independent, so that solving the requisite optimization problem is simple." ></td>
	<td class="line x" title="78:189	Aside: transduction The above formulation also allows for transductive semi-supervised learning as well, in that we could allow nearest neighbors to come from both the training and test sets." ></td>
	<td class="line x" title="79:189	We intend to address this case in future work, since there are important settings in which one has a small number of labeled reviews and a large number of unlabeled reviews, in which case considering similarities between unlabeled texts could prove quite helpful." ></td>
	<td class="line x" title="80:189	In full generality, the corresponding multi-label optimization problem is intractable, but for many families of a84 functions (e.g. , convex) there exist practical exact or approximation algorithms based on techniques for nding minimum s-t cuts in graphs (Ishikawa and Geiger, 1998; Boykov, Veksler, and Zabih, 1999; Ishikawa, 2003)." ></td>
	<td class="line x" title="81:189	Interestingly, previous sentiment analysis research found that a minimum-cut formulation for the binary subjective/objective distinction yielded good results (Pang and Lee, 2004)." ></td>
	<td class="line x" title="82:189	Of course, there are many other related semi-supervised learning algorithms that we would like to try as well; see Zhu (2005) for a survey." ></td>
	<td class="line x" title="83:189	4 Class struggle: nding a label-correlated item-similarity function We need to specify an item similarity function a65a95a66a49a67 to use the metric-labeling formulation described in Section 3.3." ></td>
	<td class="line x" title="84:189	We could, as is commonly done, employ a term-overlap-based measure such as the cosine between term-frequency-based document vectors (henceforth TO(cos) )." ></td>
	<td class="line x" title="85:189	However, Table 2 6If we ignore the a96a98a97a100a99a92a101a49a102a15a103 term, different choices of a104 correspond to different versions of nearest-neighbor learning, e.g., majority-vote, weighted average of labels, or weighted median of labels." ></td>
	<td class="line x" title="86:189	118 Label difference: 1 2 3 Three-class data 37% 33% Four-class data 34% 31% 30% Table 2: Average over authors and class pairs of between-class vocabulary overlap as the class labels of the pair grow farther apart." ></td>
	<td class="line x" title="87:189	shows that in aggregate, the vocabularies of distant classes overlap to a degree surprisingly similar to that of the vocabularies of nearby classes." ></td>
	<td class="line x" title="88:189	Thus, item similarity as measured by TO(cos) may not correlate well with similarity of the items true labels." ></td>
	<td class="line x" title="89:189	We can potentially develop a more useful similarity metric by asking ourselves what, intuitively, accounts for the label relationships that we seek to exploit." ></td>
	<td class="line x" title="90:189	A simple hypothesis is that ratings can be determined by the positive-sentence percentage (PSP) of a text, i.e., the number of positive sentences divided by the number of subjective sentences." ></td>
	<td class="line oc" title="91:189	(Termbased versions of this premise have motivated much sentiment-analysis work for over a decade (Das and Chen, 2001; Tong, 2001; Turney, 2002))." ></td>
	<td class="line n" title="92:189	But counterexamples are easy to construct: reviews can contain off-topic opinions, or recount many positive aspects before describing a fatal aw." ></td>
	<td class="line x" title="93:189	We therefore tested the hypothesis as follows." ></td>
	<td class="line x" title="94:189	To avoid the need to hand-label sentences as positive or negative, we rst created a sentence polarity dataset7 consisting of 10,662 movie-review snippets (a striking extract usually one sentence long) downloaded from www.rottentomatoes.com; each snippet was labeled with its source reviews label (positive or negative) as provided by Rotten Tomatoes." ></td>
	<td class="line x" title="95:189	Then, we trained a Naive Bayes classi er on this data set and applied it to our scale dataset to identify the positive sentences (recall that objective sentences were already removed)." ></td>
	<td class="line x" title="96:189	Figure 1 shows that all four authors tend to exhibit a higher PSP when they write a more positive review, and we expect that most typical reviewers would follow suit." ></td>
	<td class="line x" title="97:189	Hence, PSP appears to be a promising basis for computing document similarity for our rating-inference task." ></td>
	<td class="line x" title="98:189	In particular, 7Available at http://www.cs.cornell.edu/People/pabo/moviereview-data as sentence polarity dataset v1.0." ></td>
	<td class="line x" title="99:189	we de ned a24a12a24a88a24a105a24a88a24a38a106 a107 a28 a107 a9a49a48a55a18 to be the two-dimensional vector a9a107 a28 a107 a9a49a48a55a18a95a13a60a108a109a24 a107 a28 a107 a9a49a48a63a18a2a18, and then set the itemsimilarity function required by the metric-labeling optimization function (Section 3.3) to a65a5a66a12a67 a9a49a48a50a13a2a87a88a18a110a20 a111a17a112a114a113a116a115 a24a49a24a105a24a88a24a105a24a82a106 a107 a28 a107 a9a49a48a63a18a17a13 a24a88a24a88a24a105a24a117a106 a107 a28 a107 a9a49a87a105a18a119a118a121a1208 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 2 4 6 8 10 mean and standard deviation of PSP rating (in notches) Positive-sentence percentage (PSP) statistics Author a Author b Author c Author d Figure 1: Average and standard deviation of PSP for reviews expressing different ratings." ></td>
	<td class="line x" title="100:189	But before proceeding, we note that it is possible that similarity information might yield no extra bene t at all." ></td>
	<td class="line x" title="101:189	For instance, we dont need it if we can reliably identify each class just from some set of distinguishing terms." ></td>
	<td class="line x" title="102:189	If we de ne such terms as frequent ones (a6a23a122 a123a14a124 ) that appear in a single class 50% or more of the time, then we do nd many instances; some examples for one author are: meaningless, disgusting (class 0); pleasant, uneven (class 1); and oscar, gem (class 2) for the three-class case, and, in the four-class case, at, tedious (class 1) versus straightforward, likeable (class 2)." ></td>
	<td class="line x" title="103:189	Some unexpected distinguishing terms for this author are lion for class 2 (threeclass case), and for class 2 in the four-class case, jennifer, for a wide variety of Jennifers." ></td>
	<td class="line x" title="104:189	5 Evaluation This section compares the accuracies of the approaches outlined in Section 3 on the four corpora comprising our scale dataset." ></td>
	<td class="line x" title="105:189	(Results using a125 a0 error were qualitatively similar)." ></td>
	<td class="line x" title="106:189	Throughout, when 8While admittedly we initially chose this function because it was convenient to work with cosines, post hoc analysis revealed that the corresponding metric space stretched certain distances in a useful way." ></td>
	<td class="line x" title="107:189	119 we refer to something as signi cant, we mean statistically so with respect to the paireda126 -test,a127a129a128 a120a124a114a130 . The results that follow are based on a28a92a29a131a31 a33a36a35a38a37a40a39a42a41 s default parameter settings for SVM regression and OVA." ></td>
	<td class="line x" title="108:189	Preliminary analysis of the effect of varying the regression parameter a53 in the four-class case revealed that the default value was often optimal." ></td>
	<td class="line x" title="109:189	The notation Aa73 B denotes metric labeling where method A provides the initial label preference function a43 and B serves as similarity measure." ></td>
	<td class="line x" title="110:189	To train, we rst select the meta-parameters a64 and a74 by running 9-fold cross-validation within the training set." ></td>
	<td class="line x" title="111:189	Fixing a64 and a74 to those values yielding the best performance, we then re-train A (but with SVM parameters xed, as described above) on the whole training set." ></td>
	<td class="line x" title="112:189	At test time, the nearest neighbors of each item are also taken from the full training set." ></td>
	<td class="line x" title="113:189	5.1 Main comparison Figure 2 summarizes our average 10-fold crossvalidation accuracy results." ></td>
	<td class="line x" title="114:189	We rst observe from the plots that all the algorithms described in Section 3 always de nitively outperform the simple baseline of predicting the majority class, although the improvements are smaller in the four-class case." ></td>
	<td class="line x" title="115:189	Incidentally, the data was distributed in such a way that the absolute performance of the baseline itself does not change much between the threeand four-class case (which implies that the three-class datasets were relatively more balanced); and Author cs datasets seem noticeably easier than the others." ></td>
	<td class="line x" title="116:189	We now examine the effect of implicitly using label and item similarity." ></td>
	<td class="line x" title="117:189	In the four-class case, regression performed better than OVA (signi cantly so for two authors, as shown in the righthand table); but for the three-category task, OVA signi cantly outperforms regression for all four authors." ></td>
	<td class="line x" title="118:189	One might initially interprete this ip as showing that in the four-class scenario, item and label similarities provide a richer source of information relative to class-speci c characteristics, especially since for the non-majority classes there is less data available; whereas in the three-class setting the categories are better modeled as quite distinct entities." ></td>
	<td class="line x" title="119:189	However, the three-class results for metric labeling on top of OVA and regression (shown in Figure 2 by black versions of the corresponding icons) show that employing explicit similarities always improves results, often to a signi cant degree, and yields the best overall accuracies." ></td>
	<td class="line x" title="120:189	Thus, we can in fact effectively exploit similarities in the three-class case." ></td>
	<td class="line x" title="121:189	Additionally, in both the threeand fourclass scenarios, metric labeling often brings the performance of the weaker base method up to that of the stronger one (as indicated by the disappearance of upward triangles in corresponding table rows), and never hurts performance signi cantly." ></td>
	<td class="line x" title="122:189	In the four-class case, metric labeling and regression seem roughly equivalent." ></td>
	<td class="line x" title="123:189	One possible interpretation is that the relevant structure of the problem is already captured by linear regression (and perhaps a different kernel for regression would have improved its three-class performance)." ></td>
	<td class="line x" title="124:189	However, according to additional experiments we ran in the four-class situation, the test-set-optimal parameter settings for metric labeling would have produced signi cant improvements, indicating there may be greater potential for our framework." ></td>
	<td class="line x" title="125:189	At any rate, we view the fact that metric labeling performed quite well for both rating scales as a de nitely positive result." ></td>
	<td class="line x" title="126:189	5.2 Further discussion Q: Metric labeling looks like its just combining SVMs with nearest neighbors, and classi er combination often improves performance." ></td>
	<td class="line x" title="127:189	Couldnt we get the same kind of results by combining SVMs with any other reasonable method?" ></td>
	<td class="line x" title="128:189	A: No." ></td>
	<td class="line x" title="129:189	For example, if we take the strongest base SVM method for initial label preferences, but replace PSP with the term-overlap-based cosine (TO(cos)), performance often drops signi cantly." ></td>
	<td class="line x" title="130:189	This result, which is in accordance with Section 4s data, suggests that choosing an item similarity function that correlates well with label similarity is important." ></td>
	<td class="line x" title="131:189	(ovaa73 PSP a132a80a132a80a132a80a132 ovaa73 TO(cos) [3c]; rega73 PSP a132 rega73 TO(cos) [4c]) Q: Could you explain that notation, please?" ></td>
	<td class="line x" title="132:189	A: Triangles point toward the signi cantly better algorithm for some dataset." ></td>
	<td class="line x" title="133:189	For instance, M a132a80a132a80a133 N [3c] means, In the 3-class task, method M is signi cantly better than N for two author datasets and signi cantly worse for one dataset (so the algorithms were statistically indistinguishable on the remaining dataset) . When the algorithms being compared are statistically indistinguishable on 120 Average accuracies, three-class data Average accuracies, four-class data 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Author a Author b Author c Author d majority ova ova+PSP reg reg+PSP 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Author a Author b Author c Author d majority ova ova+PSP reg reg+PSP Average ten-fold cross-validation accuracies." ></td>
	<td class="line x" title="134:189	Open icons: SVMs in either one-versus-all (square) or regression (circle) mode; dark versions: metric labeling using the corresponding SVM together with the positive-sentence percentage (PSP)." ></td>
	<td class="line x" title="135:189	The a87 -axes of the two plots are aligned." ></td>
	<td class="line x" title="136:189	Signi cant differences, three-class data Signi cant differences, four-class data ova ova+PSP reg reg+PSP a b c d a b c d a b c d a b c d ova a134a86a134a63a134 . a132a63a132a63a132a86a132 .a132 . . ova+PSP a135a63a135a63a135 . a132a63a132a63a132a86a132 a132a63a132a63a132 . reg a134a63a134a63a134a86a134 a134a86a134a63a134a63a134 .a134 .a134 reg+PSP .a134 . . a134a86a134a63a134 . .a135 .a135 ova ova+PSP reg reg+PSP a b c d a b c d a b c d a b c d ova .a134a63a134a63a134 a134a63a134 . . a134 . .a134 ova+PSP .a135a63a135a63a135 a134 . . ." ></td>
	<td class="line x" title="137:189	a134 . . ." ></td>
	<td class="line x" title="138:189	reg a132a63a132 . . a132 . . ." ></td>
	<td class="line x" title="139:189	reg+PSP a132 . .a132 a132 . . ." ></td>
	<td class="line x" title="140:189	Triangles point towards signi cantly better algorithms for the results plotted above." ></td>
	<td class="line x" title="141:189	Speci cally, if the difference between a row and a column algorithm for a given author dataset (a, b, c, or d) is signi cant, a triangle points to the better one; otherwise, a dot ()." ></td>
	<td class="line x" title="142:189	is shown." ></td>
	<td class="line x" title="143:189	Dark icons highlight the effect of adding PSP information via metric labeling." ></td>
	<td class="line x" title="144:189	Figure 2: Results for main experimental comparisons." ></td>
	<td class="line x" title="145:189	all four datasets (the no triangles case), we indicate this with an equals sign ( = )." ></td>
	<td class="line x" title="146:189	Q: Thanks." ></td>
	<td class="line x" title="147:189	Doesnt Figure 1 show that the positive-sentence percentage would be a good classi er even in isolation, so metric labeling isnt necessary?" ></td>
	<td class="line x" title="148:189	A: No." ></td>
	<td class="line x" title="149:189	Predicting class labels directly from the PSP value via trained thresholds isnt as effective (ovaa73 PSP a132a80a132a80a132a80a132 threshold PSP [3c]; rega73 PSP a132a80a132 threshold PSP [4c])." ></td>
	<td class="line x" title="150:189	Alternatively, we could use only the PSP component of metric labeling by setting the label preference function to the constant function 0, but even with test-set-optimal parameter settings, doing so underperforms the trained metric labeling algorithm with access to an initial SVM classi er (ovaa73 PSP a132a80a132a80a132a80a132 0a73 a107 a28 a107a137a136 [3c]; rega73 PSP a132a80a132 0a73 a107 a28 a107a138a136 [4c])." ></td>
	<td class="line x" title="151:189	Q: What about using PSP as one of the features for input to a standard classi er?" ></td>
	<td class="line x" title="152:189	A: Our focus is on investigating the utility of similarity information." ></td>
	<td class="line x" title="153:189	In our particular rating-inference setting, it so happens that the basis for our pairwise similarity measure can be incorporated as an 121 item-speci c feature, but we view this as a tangential issue." ></td>
	<td class="line x" title="154:189	That being said, preliminary experiments show that metric labeling can be better, barely (for test-set-optimal parameter settings for both algorithms: signi cantly better results for one author, four-class case; statistically indistinguishable otherwise), although one needs to determine an appropriate weight for the PSP feature to get good performance." ></td>
	<td class="line x" title="155:189	Q: You de ned the metric transformation function a84 as the identity function a84 a9a85a8a30a18a139a20a140a8, imposing greater loss as the distance between labels assigned to two similar items increases." ></td>
	<td class="line x" title="156:189	Can you do just as well if you penalize all non-equal label assignments by the same amount, or does the distance between labels really matter?" ></td>
	<td class="line x" title="157:189	A: Youre asking for a comparison to the Potts model, which sets a84 to the function a141 a84 a9a85a8a30a18 a20 a108 if a8 a142 a124, a124 otherwise." ></td>
	<td class="line x" title="158:189	In the one setting in which there is a signi cant difference between the two, the Potts model does worse (ovaa73 PSP a132 ova a141 a73 PSP [3c])." ></td>
	<td class="line x" title="159:189	Also, employing the Potts model generally leads to fewer signi cant improvements over a chosen base method (compare Figure 2s tables with: reg a141 a73 PSP a132 reg [3c]; ova a141 a73 PSP a132a80a132 ova [3c]; ova a141 a73 PSP a20 ova [4c]; but note that reg a141 a73 PSP a132 reg [4c])." ></td>
	<td class="line x" title="160:189	We note that optimizing the Potts model in the multi-label case is NPhard, whereas the optimal metric labeling with the identity metric-transformation function can be ef ciently obtained (see Section 3.3)." ></td>
	<td class="line x" title="161:189	Q: Your datasets had many labeled reviews and only one author each." ></td>
	<td class="line x" title="162:189	Is your work relevant to settings with many authors but very little data for each?" ></td>
	<td class="line x" title="163:189	A: As discussed in Section 2, it can be quite difcult to properly calibrate different authors scales, since the same number of stars even within what is ostensibly the same rating system can mean different things for different authors." ></td>
	<td class="line x" title="164:189	But since you ask: we temporarily turned a blind eye to this serious issue, creating a collection of 5394 reviews by 496 authors with at most 80 reviews per author, where we pretended that our rating conversions mapped correctly into a universal rating scheme." ></td>
	<td class="line x" title="165:189	Preliminary results on this dataset were actually comparable to the results reported above, although since we are not con dent in the class labels themselves, more work is needed to derive a clear analysis of this setting." ></td>
	<td class="line x" title="166:189	(Abusing notation, since were already playing fast and loose: [3c]: baseline 52.4%, reg 61.4%, rega73 PSP 61.5%, ova (65.4%) a133 ovaa73 PSP (66.3%); [4c]: baseline 38.8%, reg (51.9%) a133 rega73 PSP (52.7%), ova (53.8%) a133 ovaa73 PSP (54.6%)) In future work, it would be interesting to determine author-independent characteristics that can be used on (or suitably adapted to) data for speci c authors." ></td>
	<td class="line x" title="167:189	Q: How about trying A: Yes, there are many alternatives." ></td>
	<td class="line x" title="168:189	A few that we tested are described in the Appendix, and we propose some others in the next section." ></td>
	<td class="line x" title="169:189	We should mention that we have not yet experimented with all-vs.-all (AVA), another standard binary-tomulti-category classi er conversion method, because we wished to focus on the effect of omitting pairwise information." ></td>
	<td class="line x" title="170:189	In independent work on 3-category rating inference for a different corpus, Koppel and Schler (2005) found that regression outperformed AVA, and Rifkin and Klautau (2004) argue that in principle OVA should do just as well as AVA." ></td>
	<td class="line x" title="171:189	But we plan to try it out." ></td>
	<td class="line x" title="172:189	6 Related work and future directions In this paper, we addressed the rating-inference problem, showing the utility of employing label similarity and (appropriate choice of) item similarity either implicitly, through regression, or explicitly and often more effectively, through metric labeling." ></td>
	<td class="line x" title="173:189	In the future, we would like to apply our methods to other scale-based classi cation problems, and explore alternative methods." ></td>
	<td class="line x" title="174:189	Clearly, varying the kernel in SVM regression might yield better results." ></td>
	<td class="line x" title="175:189	Another choice is ordinal regression (McCullagh, 1980; Herbrich, Graepel, and Obermayer, 2000), which only considers the ordering on labels, rather than any explicit distances between them; this approach could work well if a good metric on labels is lacking." ></td>
	<td class="line x" title="176:189	Also, one could use mixture models (e.g. , combine positive and negative language models) to capture class relationships (McCallum, 1999; Schapire and Singer, 2000; Takamura, Matsumoto, and Yamada, 2004)." ></td>
	<td class="line x" title="177:189	We are also interested in framing multi-class but non-scale-based categorization problems as metric 122 labeling tasks." ></td>
	<td class="line x" title="178:189	For example, positive vs. negative vs. neutral sentiment distinctions are sometimes considered in which neutral means either objective (Engstrcurrency1om, 2004) or a con ation of objective with a rating of mediocre (Das and Chen, 2001)." ></td>
	<td class="line x" title="179:189	(Koppel and Schler (2005) in independent work also discuss various types of neutrality)." ></td>
	<td class="line x" title="180:189	In either case, we could apply a metric in which positive and negative are closer to objective (or objective+mediocre) than to each other." ></td>
	<td class="line x" title="181:189	As another example, hierarchical label relationships can be easily encoded in a label metric." ></td>
	<td class="line x" title="182:189	Finally, as mentioned in Section 3.3, we would like to address the transductive setting, in which one has a small amount of labeled data and uses relationships between unlabeled items, since it is particularly well-suited to the metric-labeling approach and may be quite important in practice." ></td>
	<td class="line x" title="183:189	Acknowledgments We thank Paul Bennett, Dave Blei, Claire Cardie, Shimon Edelman, Thorsten Joachims, Jon Kleinberg, Oren Kurland, John Lafferty, Guy Lebanon, Pradeep Ravikumar, Jerry Zhu, and the anonymous reviewers for many very useful comments and discussion." ></td>
	<td class="line x" title="184:189	We learned of Moshe Koppel and Jonathan Schlers work while preparing the cameraready version of this paper; we thank them for so quickly answering our request for a pre-print." ></td>
	<td class="line x" title="185:189	Our descriptions of their work are based on that pre-print; we apologize in advance for any inaccuracies in our descriptions that result from changes between their pre-print and their nal version." ></td>
	<td class="line x" title="186:189	We also thank CMU for its hospitality during the year." ></td>
	<td class="line x" title="187:189	This paper is based upon work supported in part by the National Science Foundation (NSF) under grant no." ></td>
	<td class="line x" title="188:189	IIS-0329064 and CCR-0122581; SRI International under subcontract no. 03-000211 on their project funded by the Department of the Interiors National Business Center; and by an Alfred P. Sloan Research Fellowship." ></td>
	<td class="line x" title="189:189	Any opinions, ndings, and conclusions or recommendations expressed are those of the authors and do not necessarily re ect the views or of cial policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P05-2008
Using Emoticons To Reduce Dependency In Machine Learning Techniques For Sentiment Classification
Read, Jonathon;"></td>
	<td class="line x" title="1:156	Proceedings of the ACL Student Research Workshop, pages 4348, Ann Arbor, Michigan, June 2005." ></td>
	<td class="line x" title="2:156	c2005 Association for Computational Linguistics Using Emoticons to reduce Dependency in Machine Learning Techniques for Sentiment Classification Jonathon Read Department of Informatics University of Sussex United Kingdom j.l.read@sussex.ac.uk Abstract Sentiment Classification seeks to identify a piece of text according to its authors general feeling toward their subject, be it positive or negative." ></td>
	<td class="line x" title="3:156	Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic." ></td>
	<td class="line x" title="4:156	This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time." ></td>
	<td class="line x" title="5:156	1 Introduction Recent years have seen an increasing amount of research effort expended in the area of understanding sentiment in textual resources." ></td>
	<td class="line x" title="6:156	A sub-topic of this research is that of Sentiment Classification." ></td>
	<td class="line x" title="7:156	That is, given a problem text, can computational methods determine if the text is generally positive or generally negative?" ></td>
	<td class="line x" title="8:156	Several diverse applications exist for this potential technology, ranging from the automatic filtering of abusive messages (Spertus, 1997) to an in-depth analysis of market trends and consumer opinions (Dave et al. , 2003)." ></td>
	<td class="line x" title="9:156	This is a complex and challenging task for a computer to achieve  consider the difficulties involved in instructing a computer to recognise sarcasm, for example." ></td>
	<td class="line x" title="10:156	Previous work has shown that traditional text classification approaches can be quite effective when applied to the sentiment analysis problem." ></td>
	<td class="line x" title="11:156	Models such as Nave Bayes (NB), Maximum Entropy (ME) and Support Vector Machines (SVM) can determine the sentiment of texts." ></td>
	<td class="line x" title="12:156	Pang et al.(2002) used a bagof-features framework (based on unigrams and bigrams) to train these models from a corpus of movie reviews labelled as positive or negative." ></td>
	<td class="line x" title="14:156	The best accuracy achieved was 82.9%, using an SVM trained on unigram features." ></td>
	<td class="line x" title="15:156	A later study (Pang and Lee, 2004) found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective." ></td>
	<td class="line x" title="16:156	However, Engstrom (2004) showed that the bagof-features approach is topic-dependent." ></td>
	<td class="line x" title="17:156	A classifier trained on movie reviews is unlikely to perform as well on (for example) reviews of automobiles." ></td>
	<td class="line oc" title="18:156	Turney (2002) noted that the unigram unpredictable might have a positive sentiment in a movie review (e.g. unpredictable plot), but could be negative in the review of an automobile (e.g. unpredictable steering)." ></td>
	<td class="line x" title="19:156	In this paper, we demonstrate how the models are also domain-dependent  how a classifier trained on product reviews is not effective when evaluating the sentiment of newswire articles, for example." ></td>
	<td class="line x" title="20:156	Furthermore, we show how the models are temporally-dependent  how classifiers are biased by the trends of sentiment apparent during the time-period represented by the training data." ></td>
	<td class="line x" title="21:156	We propose a novel source of training data based on the language used in conjunction with emoticons in Usenet newsgroups." ></td>
	<td class="line x" title="22:156	Training a classifier using this data provides a breadth of features that, while it 43 Testing FIN M&A MIX Training NB FIN 80.3 75.5 74.0 M&A 77.5 75.3 75.8 MIX 70.7 62.9 84.6 SVM FIN 78.8 72.7 68.9 M&A 74.5 75.5 75.5 MIX 72.0 68.9 81.1 Figure 1: Topic dependency in sentiment classification." ></td>
	<td class="line x" title="23:156	Accuracies, in percent." ></td>
	<td class="line x" title="24:156	Best performance on a test set for each model is highlighted in bold." ></td>
	<td class="line x" title="25:156	does not perform to the state-of-the-art, could function independent of domain, topic and time." ></td>
	<td class="line x" title="26:156	2 Dependencies in Sentiment Classification 2.1 Experimental Setup In this section, we describe experiments we have carried out to determine the influence of domain, topic and time on machine learning based sentiment classification." ></td>
	<td class="line x" title="27:156	The experiments use our own implementation of a Nave Bayes classifier and Joachims (1999) SVMlight implementation of a Support Vector Machine classifier." ></td>
	<td class="line x" title="28:156	The models were trained using unigram features, accounting for the presence of feature types in a document, rather than the frequency, as Pang et al.(2002) found that this is the most effective strategy for sentiment classification." ></td>
	<td class="line x" title="30:156	When training and testing on the same set, the mean accuracy is determined using three-fold crossvalidation." ></td>
	<td class="line x" title="31:156	In each case, we use a paired-sample t-test over the set of test documents to determine whether the results produced by one classifier are statistically significantly better than those from another, at a confidence interval of at least 95%." ></td>
	<td class="line x" title="32:156	2.2 Topic Dependency Engstrom (2004) demonstrated how machinelearning techniques for sentiment classification can be topic dependent." ></td>
	<td class="line x" title="33:156	However, that study focused on a three-way classification (positive, negative and neutral)." ></td>
	<td class="line x" title="34:156	In this paper, for uniformity across different data sets, we focus on only positive and negative sentiment." ></td>
	<td class="line x" title="35:156	This experiment also provides an opportunity to evaluate the Nave Bayes classifier as the previous work used SVMs." ></td>
	<td class="line x" title="36:156	We use subsets of a Newswire dataset (kindly proTesting Newswire Polarity 1.0 Training NB Newswire 78.2 57.6 Polarity 1.0 53.2 78.9 SVM Newswire 78.2 63.2 Polarity 1.0 63.6 81.5 Figure 2: Domain dependency in sentiment classification." ></td>
	<td class="line x" title="37:156	Accuracies, in percent." ></td>
	<td class="line x" title="38:156	Best performance on a test set for each model is highlighted in bold." ></td>
	<td class="line x" title="39:156	vided by Roy Lipski of Infonic Ltd)." ></td>
	<td class="line x" title="40:156	that relate to the topics of Finance (FIN), Mergers and Aquisitions (M&A) and a mixture of both topics (MIX)." ></td>
	<td class="line x" title="41:156	Each subset contains further subsets of articles of positive and negative sentiment (selected by independent trained annotators), each containing 100 stories." ></td>
	<td class="line x" title="42:156	We trained a model on a dataset relating to one topic and tested that model using the other topics." ></td>
	<td class="line x" title="43:156	Figure 1 shows the results of this experiment." ></td>
	<td class="line x" title="44:156	The tendency seems to be that performance in a given topic is best if the training data is from the same topic." ></td>
	<td class="line x" title="45:156	For example, the Finance-trained SVM classifier achieved an accuracy of 78.8% against articles from Finance, but only 72.7% when predicting the sentiment of articles from M&A. However, statistical testing showed that the results are not significantly different when training on one topic and testing on another." ></td>
	<td class="line x" title="46:156	It is interesting to note, though, that providing a dataset of mixed topics (the sub-corpus MIX) does not necessarily reduce topic dependency." ></td>
	<td class="line x" title="47:156	Indeed, the performance of the classifiers suffers a great deal when training on mixed data (confidence interval 95%)." ></td>
	<td class="line x" title="48:156	2.3 Domain Dependency We conducted an experiment to compare the accuracy when training a classifier on one domain (newswire articles or movie reviews from the Polarity 1.0 dataset used by Pang et al.(2002)) and testing on the other domain." ></td>
	<td class="line x" title="50:156	In Figure 2, we see a clear indication that models trained on one domain do not perform as well on another domain." ></td>
	<td class="line x" title="51:156	All differences are significant at a confidence interval of 99.9%." ></td>
	<td class="line x" title="52:156	2.4 Temporal Dependency To investigate the effect of time on sentiment classification, we constructed a new set of movie re44 Testing Polarity 1.0 Polarity 2004 Training NB Polarity 1.0 78.9 71.8 Polarity 2004 63.2 76.5 SVM Polarity 1.0 81.5 77.5 Polarity 2004 76.5 80.8 Figure 3: Temporal dependency in sentiment classification." ></td>
	<td class="line x" title="53:156	Accuracies, in percent." ></td>
	<td class="line x" title="54:156	Best performance on a test set for each model is highlighted in bold." ></td>
	<td class="line x" title="55:156	views, following the same approach used by Pang et al.(2002) when they created the Polarity 1.0 dataset." ></td>
	<td class="line x" title="57:156	The data source was the Internet Movie Review Database archive1 of movie reviews." ></td>
	<td class="line x" title="58:156	The reviews were categorised as positive or negative using automatically extracted ratings." ></td>
	<td class="line x" title="59:156	A review was ignored if it was not written in 2003 or 2004 (ensuring that the review was written after any in the Polarity 1.0 dataset)." ></td>
	<td class="line x" title="60:156	This procedure yielded a corpus of 716 negative and 2,669 positive reviews." ></td>
	<td class="line x" title="61:156	To create the Polarity 20042 dataset we randomly selected 700 negative reviews and 700 positive reviews, matching the size and distribution of the Polarity 1.0 dataset." ></td>
	<td class="line x" title="62:156	The next experiment evaluated the performance of the models first against movie reviews from the same time-period as the training set and then against reviews from the other time-period." ></td>
	<td class="line x" title="63:156	Figure 3 shows the resulting accuracies." ></td>
	<td class="line x" title="64:156	These results show that while the models perform well on reviews from the same time-period as the training set, they are not so effective on reviews from other time-periods (confidence interval 95%)." ></td>
	<td class="line x" title="65:156	It is also apparent that the Polarity 2004 dataset performs worse than the Polarity 1.0 dataset (confidence interval 99.9%)." ></td>
	<td class="line x" title="66:156	A possible reason for this is that Polarity 2004 data is from a much smaller time-period than that represented by Polarity 1.0." ></td>
	<td class="line x" title="67:156	3 Sentiment Classification using Emoticons One way of overcoming the domain, topic and time problems we have demonstrated above would be to find a source of much larger and diverse amounts of general text, annotated for sentiment." ></td>
	<td class="line x" title="68:156	Users of 1http://reviews.imdb.com/Reviews/ 2The new datasets described in this paper are available at http://www.sussex.ac.uk/Users/jlr24/data Glyph Meaning Frequency :-) smile 3.8739 ;-) wink 2.4350 :-( frown 0.4961 :-D wide grin 0.1838 :-P tongue sticking out 0.1357 :-O surprise 0.0171 :-| disappointed 0.0146 :( crying 0.0093 :-S confused 0.0075 :-@ angry 0.0038 :-$ embarrassed 0.0007 Figure 4: Examples of emoticons and the frequency of usage observed in Usenet articles, in percent." ></td>
	<td class="line x" title="69:156	For example, 2.435% of downloaded Usenet articles contained a wink emoticon." ></td>
	<td class="line x" title="70:156	electronic methods of communication have developed visual cues that are associated with emotional states in an attempt to state the emotion that their text represents." ></td>
	<td class="line x" title="71:156	These have become known as smileys or emoticons and are glyphs constructed using the characters available on a standard keyboard, representing a facial expression of emotion  see Figure 4 for some examples." ></td>
	<td class="line x" title="72:156	When the author of an electronic communication uses an emoticon, they are effectively marking up their own text with an emotional state." ></td>
	<td class="line x" title="73:156	This marked-up text can be used to train a sentiment classifier if we assume that a smile indicates generally positive text and a frown indicates generally negative text." ></td>
	<td class="line x" title="74:156	3.1 Emoticon Corpus Construction We collected a corpus of text marked-up with emoticons by downloading Usenet newsgroups and saving an article if it contained an emoticon listed in Figure 4." ></td>
	<td class="line x" title="75:156	This process resulted in 766,730 articles being stored, from 10,682,455 messages in 49,759 newsgroups inspected." ></td>
	<td class="line x" title="76:156	Figure 4 also lists the percentage of documents containing each emoticon type, as observed in the Usenet newsgroups." ></td>
	<td class="line x" title="77:156	We automatically extracted the paragraph(s) containing the emoticon of interest (a smile or a frown) from each message and removed any superfluous formatting characters (such as those used to indicate article quotations in message threads)." ></td>
	<td class="line x" title="78:156	In order to prevent quoted text from being considered more than once, any paragraph that began with exactly the same thirty characters as a previously observed paragraph was disregarded." ></td>
	<td class="line x" title="79:156	Finally, we used the classifier developed by Cavnar and Trenkle (1994) to filter 45 Finance M&A Mixed NB 46.0  2.1 55.8  3.8 49.0  1.6 SVM 50.3  1.7 57.8  6.5 55.5  2.7 Figure 5: Performance of Emoticon-trained classifier across topics." ></td>
	<td class="line x" title="80:156	Mean accuracies with standard deviation, in percent." ></td>
	<td class="line x" title="81:156	Newswire Polarity 1.0 NB 50.3  2.2 56.8  1.8 SVM 54.4  2.8 54.0  0.8 Figure 6: Performance of Emoticon-trained classifiers across domains." ></td>
	<td class="line x" title="82:156	Mean accuracies with standard deviation, in percent." ></td>
	<td class="line x" title="83:156	out any paragraphs of non-English text." ></td>
	<td class="line x" title="84:156	This process yielded a corpus of 13,000 article extracts containing frown emoticons." ></td>
	<td class="line x" title="85:156	As investigating skew between positive and negative distributions is outside the scope of this work, we also extracted 13,000 article extracts containing smile emoticons." ></td>
	<td class="line x" title="86:156	The dataset is referred to throughout this paper as Emoticons and contains 748,685 words." ></td>
	<td class="line x" title="87:156	3.2 Emoticon-trained Sentiment Classification This section describes how the Emoticons corpus3 was optimised for use as sentiment classification training data." ></td>
	<td class="line x" title="88:156	2,000 articles containing smiles and 2,000 articles containing frowns were held-out as optimising test data." ></td>
	<td class="line x" title="89:156	We took increasing amounts of articles from the remaining dataset (from 2,000 to 22,000 in increments of 1,000, an equal number being taken from the positive and negative sets) as optimising training data." ></td>
	<td class="line x" title="90:156	For each set of training data we extracted a context of an increasing number of tokens (from 10 to 1,000 in increments of 10) both before and in a window4 around the smile or frown emoticon." ></td>
	<td class="line x" title="91:156	The models were trained using this extracted context and tested on the held-out dataset." ></td>
	<td class="line x" title="92:156	The optimisation process revealed that the bestperforming settings for the Nave Bayes classifier was a window context of 130 tokens taken from the largest training set of 22,000 articles." ></td>
	<td class="line x" title="93:156	Similarly, the best performance for the SVM classifier was found using a window context of 150 tokens taken from 3Note that in these experiments the emoticons are used as anchors from which context is extracted, but are removed from texts before they are used as training or test data." ></td>
	<td class="line x" title="94:156	4Context taken after an emoticon was also investigated, but was found to be inferior." ></td>
	<td class="line x" title="95:156	This is because approximately twothirds of article extracts end in an emoticon so when using aftercontext few features are extracted." ></td>
	<td class="line x" title="96:156	Polarity 1.0 Polarity 2004 NB 56.8  1.8 56.7  2.2 SVM 54.0  0.8 57.8  1.8 Figure 7: Performance of Emoticon-trained classifier across time-periods." ></td>
	<td class="line x" title="97:156	Mean accuracies with standard deviation, in percent." ></td>
	<td class="line x" title="98:156	20,000 articles." ></td>
	<td class="line x" title="99:156	The classifiers performance in predicting the smiles and frowns of article extracts was verified using these optimised parameters and ten-fold crossvalidation." ></td>
	<td class="line x" title="100:156	The mean accuracy of the Nave Bayes classifier was 61.5%, while the SVM classifier was 70.1%." ></td>
	<td class="line x" title="101:156	Using these same classifiers to predict the sentiment of movie reviews in Polarity 1.0 resulted in accuracies of 59.1% (Nave Bayes) and 52.1% (SVM)." ></td>
	<td class="line x" title="102:156	We repeated the optimisation process using a held-out set of 100 positive and 100 negative reviews from the Polarity 1.0 dataset, as it is possible that this test needs different parameter settings." ></td>
	<td class="line x" title="103:156	This revealed an optimum context of a window of 50 tokens taken from a training set of 21,000 articles for the Nave Bayes classifier." ></td>
	<td class="line x" title="104:156	Interestingly, the optimum context for the SVM classifier appeared to be a window of only 20 tokens taken from a mere 2,000 training examples." ></td>
	<td class="line x" title="105:156	This is clearly an anomaly, as these parameters resulted in an accuracy of 48.9% when testing against the reserved reviews of Polarity 1.0." ></td>
	<td class="line x" title="106:156	We attribute this to the presence of noise, both in the training set and in the held-out set, and discuss this below (Section 4.2)." ></td>
	<td class="line x" title="107:156	The second-best parameters according to the optimisation process were a context of 510 tokens taken before an emoticon, from a training set of 20,000 examples." ></td>
	<td class="line x" title="108:156	We used these optimised parameters to evaluate the sentiments of texts in the test sets used to evaluate dependency in Section 2." ></td>
	<td class="line x" title="109:156	Figures 5, 6 and 7 show the final, optimised results across topics, domains and time-periods respectively." ></td>
	<td class="line x" title="110:156	These tables report the average accuracies over three folds, with the standard deviation as a measure of error." ></td>
	<td class="line x" title="111:156	4 Discussion The emoticon-trained classifiers perform well (up to 70% accuracy) when predicting the sentiment of article extracts from the Emoticons dataset, which is encouraging when one considers the high level of 46 Training Testing Coverage Polarity 1.0 Polarity 1.0 69.8 (three-fold cross-validation) Emoticons FIN 54.9 M&A 58.1 MIX 60.2 Newswire 46.1 Polarity 1.0 41.1 Polarity 2004 42.6 Figure 8: Coverage of classifiers, in percent." ></td>
	<td class="line x" title="112:156	noise that is likely to be present in the dataset." ></td>
	<td class="line x" title="113:156	However, they perform only a little better than one would expect by chance when classifying movie reviews, and are not effective in predicting the sentiment of newswire articles." ></td>
	<td class="line x" title="114:156	This is perhaps due to the nature of the datasets  one would expect language to be informal in movie reviews, and even more so in Usenet articles." ></td>
	<td class="line x" title="115:156	In contrast, language in newswire articles is far more formal." ></td>
	<td class="line x" title="116:156	We might therefore infer a further type of dependence in sentiment classification, that of language-style dependency." ></td>
	<td class="line x" title="117:156	Also, note that neither machine-learning model consistently out-performs the other." ></td>
	<td class="line x" title="118:156	We speculate that this, and the generally mediocre performance of the classifiers, is due (at least) to two factors; poor coverage of the features found in the test domains and a high level of noise found in Usenet article extracts." ></td>
	<td class="line x" title="119:156	We investigate these factors below." ></td>
	<td class="line x" title="120:156	4.1 Coverage Figure 8 shows the coverage of the Emoticon-trained classifiers on the various test sets." ></td>
	<td class="line x" title="121:156	In these experiments, we are interested in the coverage in terms of unique token types rather than the frequency of features, as this more closely reflects the training of the models (see Section 2.1)." ></td>
	<td class="line x" title="122:156	The mean coverage of the Polarity 1.0 dataset during three-fold crossvalidation is also listed as an example of the coverage one would expect from a better-performing sentiment classifier." ></td>
	<td class="line x" title="123:156	The Emoticon-trained classifier has much worse coverage in the test sets." ></td>
	<td class="line x" title="124:156	We analysed the change in coverage of the Emoticon-trained classifiers on the Polarity 1.0 dataset." ></td>
	<td class="line x" title="125:156	We found that the coverage continued to improve as more training data was provided; the coverage of unique token types was improving by about 0.6% per 1,000 training examples when the Emoti48 50 52 54 56 58 60 3000 6000 9000 12000 15000 18000Training Size 100 200 300 400 500 600 700 800 900 1000 Context Size 48 50 52 54 56 58 60 Accuracy (%) Figure 9: Change in Performance of the SVM Classifier on held-out reviews from Polarity 1.0, varying training set size and window context size." ></td>
	<td class="line x" title="126:156	The datapoints represent 2,200 experiments in total." ></td>
	<td class="line x" title="127:156	cons dataset was exhausted." ></td>
	<td class="line x" title="128:156	It appears possible that more training data will improve the performance of the Emoticon-trained classifiers by increasing the coverage." ></td>
	<td class="line x" title="129:156	Potential sources for this include online bulletin boards, chat forums, and further newsgroup data from Usenet and Google Groups5." ></td>
	<td class="line x" title="130:156	Future work will utilise these sources to collect more examples of emoticon use and analyse any improvement in coverage and accuracy." ></td>
	<td class="line x" title="131:156	4.2 Noise in Usenet Article Extracts The article extracts collected in the Emoticons dataset may be noisy with respect to sentiment." ></td>
	<td class="line x" title="132:156	The SVM classifier seems particularly affected by this noise." ></td>
	<td class="line x" title="133:156	Figure 9 depicts the change in performance of the SVM classifier when varying the training set size and size of context extracted." ></td>
	<td class="line x" title="134:156	There are significant spikes apparent for the training sizes of 2,000, 3,000 and 6,000 article extracts (as noted in Section 3.2), where the accuracy suddenly increases for the training set size, then quickly decreases for the next set size." ></td>
	<td class="line x" title="135:156	This implies that the classifier is discovering features that are useful in classifying the heldout set, but the addition of more, noisy, texts soon makes the information redundant." ></td>
	<td class="line x" title="136:156	Some examples of noise taken from the Emoticons dataset are: mixed sentiment, e.g. 5http://groups.google.com 47 Sorry about venting my frustration here but I just lost it." ></td>
	<td class="line x" title="137:156	:-( Happy thanks giving everybody :-), sarcasm, e.g. Thank you so much, thats really encouraging :-(, and spelling mistakes, e.g. The movies where for me a major desapointment :-(." ></td>
	<td class="line x" title="138:156	In future work we will investigate ways to remove noisy data from the Emoticons dataset." ></td>
	<td class="line x" title="139:156	5 Conclusions and Future Work This paper has demonstrated that dependency in sentiment classification can take the form of domain, topic, temporal and language style." ></td>
	<td class="line x" title="140:156	One might suppose that dependency is occurring because classifiers are learning the semantic sentiment of texts rather than the general sentiment of language used." ></td>
	<td class="line x" title="141:156	That is, the classifiers could be learning authors sentiment towards named entities (e.g. actors, directors, companies, etc.)." ></td>
	<td class="line x" title="142:156	However, this does not seem to be the case." ></td>
	<td class="line x" title="143:156	In a small experiment, we part-ofspeech tagged the Polarity 2004 dataset and automatically replaced proper nouns with placeholders." ></td>
	<td class="line x" title="144:156	Retraining on this modified text did not significantly affect performance." ></td>
	<td class="line x" title="145:156	But it may be that something more subtle is happening." ></td>
	<td class="line x" title="146:156	Possibly, the classifiers are learning the words associated with the semantic sentiment of entities." ></td>
	<td class="line x" title="147:156	For example, suppose that there has been a well-received movie about mountaineering." ></td>
	<td class="line x" title="148:156	During this movie, there is a particularly stirring scene involving an ice-axe and most of the reviewers mention this scene." ></td>
	<td class="line x" title="149:156	During training, the word ice-axe would become associated with a positive sentiment, whereas one would suppose that this word does not in general express any kind of sentiment." ></td>
	<td class="line x" title="150:156	In future work we will perform further tests to determine the nature of dependency in machine learning techniques for sentiment classification." ></td>
	<td class="line x" title="151:156	One way of evaluating the ice-axe effect could be to build a pseudo-ontology of the movie reviews  a map of the sentiment-bearing relations that would enable the analysis of the dependencies created by the training process." ></td>
	<td class="line x" title="152:156	Other extensions of this work are to collect more text marked-up with emoticons, and to experiment with techniques to automatically remove noisy examples from the training data." ></td>
	<td class="line x" title="153:156	Acknowledgements This research was funded by a UK EPSRC studentship." ></td>
	<td class="line x" title="154:156	I am very grateful to Thorsten Joachims, Roy Lipski, Bo Pang and John Trenkle for kindly making their data or software available, and to the anonymous reviewers for their constructive comments." ></td>
	<td class="line x" title="155:156	Thanks also to Nick Jacobi for his discussion of the ice-axe effect." ></td>
	<td class="line x" title="156:156	Special thanks to my supervisor, John Carroll, for his continued advice and encouragement." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W05-0408
Automatic Identification Of Sentiment Vocabulary: Exploiting Low Association With Known Sentiment Terms
Gamon, Michael;Aue, Anthony;"></td>
	<td class="line x" title="1:174	Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 5764, Ann Arbor, June 2005." ></td>
	<td class="line oc" title="2:174	c2005 Association for Computational Linguistics Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms Michael Gamon Anthony Aue Natural Language Processing Group Natural Language Processing Group Microsoft Research Microsoft Research mgamon@microsoft.com anthaue@microsoft.com Abstract We describe an extension to the technique for the automatic identification and labeling of sentiment terms described in Turney (2002) and Turney and Littman (2002)." ></td>
	<td class="line x" title="3:174	Their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level." ></td>
	<td class="line x" title="4:174	We add a second assumption, namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level." ></td>
	<td class="line x" title="5:174	This additional assumption allows us to identify sentiment-bearing terms very reliably." ></td>
	<td class="line x" title="6:174	We then use these newly identified terms in various scenarios for the sentiment classification of sentences." ></td>
	<td class="line n" title="7:174	We show that our approach outperforms Turneys original approach." ></td>
	<td class="line x" title="8:174	Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance." ></td>
	<td class="line x" title="9:174	We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data." ></td>
	<td class="line oc" title="10:174	1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others)." ></td>
	<td class="line x" title="11:174	The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification." ></td>
	<td class="line x" title="12:174	Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writers sentiment toward the topic." ></td>
	<td class="line oc" title="13:174	Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002)." ></td>
	<td class="line x" title="14:174	Typically, these reviews are classified at the document level, and the class labels are positive and negative." ></td>
	<td class="line x" title="15:174	In this work, in contrast, we narrow the scope of investigation to the sentence level and expand the set of labels, making a threefold distinction between positive, neutral, and negative." ></td>
	<td class="line x" title="16:174	The narrowing of scope is motivated by the fact that for realistic text mining on customer feedback, the document level is too coarse, as described in Gamon et al.(2005)." ></td>
	<td class="line x" title="18:174	The expansion of the label set is also motivated by real-world concerns; while it is a given that review text expresses positive or negative sentiment, in many cases it is necessary to also identify the cases that dont carry strong expressions of sentiment at all." ></td>
	<td class="line x" title="19:174	Traditional approaches to text classification require large amounts of labeled training data." ></td>
	<td class="line x" title="20:174	Acquisition of such data can be costly and timeconsuming." ></td>
	<td class="line x" title="21:174	Due to the highly domain-specific nature of the sentiment classification task, moving from one domain to another typically requires the acquisition of a new set of training data." ></td>
	<td class="line x" title="22:174	For this reason, unsupervised or very weakly supervised methods for sentiment classification are especially 57 desirable." ></td>
	<td class="line x" title="23:174	1 Our focus, therefore, is on methods that require very little data annotation." ></td>
	<td class="line x" title="24:174	We describe a method to automatically identify the sentiment vocabulary in a domain." ></td>
	<td class="line x" title="25:174	This method rests on three special properties of the sentiment domain: 1." ></td>
	<td class="line x" title="26:174	the presence of certain words can serve as a proxy for the class label 2." ></td>
	<td class="line x" title="27:174	sentiment terms of similar orientation tend to co-occur 3." ></td>
	<td class="line x" title="28:174	sentiment terms of opposite orientation tend to not co-occur at the sentence level." ></td>
	<td class="line oc" title="29:174	Turney (2002) and Turney and Littman (2002) exploit the first two generalizations for unsupervised sentiment classification of movie reviews." ></td>
	<td class="line o" title="30:174	They use the two terms excellent and poor as seed terms to determine the semantic orientation of other terms." ></td>
	<td class="line o" title="31:174	These seed terms can be viewed as proxies for the class labels positive and negative, allowing for the exploitation of otherwise unlabeled data: Terms that tend to co-occur with excellent in documents tend to be of positive orientation, and vice versa for poor." ></td>
	<td class="line oc" title="32:174	Turney (2002) starts from a small (2 word) set of terms with known orientation (excellent and poor)." ></td>
	<td class="line oc" title="33:174	Given a set of terms with unknown sentiment orientation, Turney (2002) then uses the PMI-IR algorithm (Turney 2001) to issue queries to the web and determine, for each of these terms, its pointwise mutual information (PMI) with the two seed words across a large set of documents." ></td>
	<td class="line x" title="34:174	Term candidates are constrained to be adjectives, which tend to be the strongest bearers of sentiment." ></td>
	<td class="line x" title="35:174	The sentiment orientation (SO) of a term is then determined by the difference between its association (PMI) with the positive seed term excellent and its association with the negative seed term poor." ></td>
	<td class="line x" title="36:174	The resulting list of terms and associated sentiment orientations can then be used to implement a classifier: semantic orientation of the terms in a document of unknown sentiment is added up, and if the overall score is positive, the document is classified as being of positive sentiment, otherwise it is classified as negative." ></td>
	<td class="line x" title="37:174	Yu and Hatzivassiloglou (2003) extend this approach by (1) applying it at the sentence level (instead of the document-level), (2) taking into account non-adjectival parts-of-speech, and (3) 1 For domain-specificity of sentiment classification see Engstrm (2004) and Aue and Gamon (2005)." ></td>
	<td class="line x" title="38:174	using larger sets of seed words." ></td>
	<td class="line x" title="39:174	Their classification goal also differs from Turneys: it is to distinguish opinion sentences from factual statements." ></td>
	<td class="line o" title="40:174	Turney et al.s approach is based on the assumption that sentiment terms of similar orientation tend to co-occur in documents." ></td>
	<td class="line x" title="41:174	Our approach takes advantage of a second assumption: At the sentence level, sentiment terms of opposite orientation tend not to co-occur." ></td>
	<td class="line x" title="42:174	This is, of course, an assumption that will only hold in general, with exceptions." ></td>
	<td class="line x" title="43:174	Basically, the assumption is that sentences of the following form: I dislike X. I really like X. are more frequent than mixed sentiment sentences such as I dislike X but I really like Y. It has been our experience that this generalization does hold often enough to be useful." ></td>
	<td class="line x" title="44:174	We propose to utilize this assumption to identify a set of sentiment terms in a domain." ></td>
	<td class="line x" title="45:174	We select the terms that have the lowest PMI scores on the sentence level with respect to a set of manually selected seed words." ></td>
	<td class="line x" title="46:174	If our assumption about low association at the sentence level is correct, this set of low-scoring terms will be particularly rich in sentiment terms." ></td>
	<td class="line oc" title="47:174	We can then use this newly identified set to: (1) use Turneys method to find the orientation for the terms and employ the terms and their scores in a classifier, and (2) use Turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration As opposed to Turney (2002), we do not use the web as a resource to find associations, rather we apply the method directly to in-domain data." ></td>
	<td class="line o" title="48:174	This has the disadvantage of not being able to apply the classification to any arbitrary domain." ></td>
	<td class="line oc" title="49:174	It is worth noting, however, that even in Turney (2002) the choice of seed words is explicitly motivated by domain properties of movie reviews." ></td>
	<td class="line x" title="50:174	In the remainder of the paper we will describe results from various experiments based on this assumption." ></td>
	<td class="line x" title="51:174	We also show how we can combine this method with a Naive Bayes bootstrapping approach that takes further advantage of the unlabeled data (Nigam et al. 2000)." ></td>
	<td class="line x" title="52:174	58 2 Data For our experiments we used a set of car reviews from the MSN Autos web site." ></td>
	<td class="line x" title="53:174	The data consist of 406,818 customer car reviews written over a fouryear period." ></td>
	<td class="line x" title="54:174	Aside from filtering out examples containing profanity, the data was not edited." ></td>
	<td class="line x" title="55:174	The reviews range in length from a single sentence (56% of all cases) to 50 sentences (a single review)." ></td>
	<td class="line x" title="56:174	Less than 1% of reviews contain ten or more sentences." ></td>
	<td class="line x" title="57:174	There are almost 900,000 sentences in total." ></td>
	<td class="line x" title="58:174	When customers submitted reviews to the website, they were asked for a recommendation on a scale of 1 (negative) to 10 (positive)." ></td>
	<td class="line x" title="59:174	The average score was very high, at 8.3, yielding a strong skew in favor of positive class labels." ></td>
	<td class="line x" title="60:174	We annotated a randomlyselected sample of 3,000 sentences for sentiment." ></td>
	<td class="line x" title="61:174	Each sentence was viewed in isolation and classified as positive, negative or neutral." ></td>
	<td class="line x" title="62:174	The neutral category was applied to sentences with no discernible sentiment, as well as to sentences that expressed both positive and negative sentiment." ></td>
	<td class="line x" title="63:174	Three annotators had pair-wise agreement scores (Cohens Kappa score, Cohen 1960) of 70.10%, 71.78% and 79.93%, suggesting that the task of sentiment classification on the sentence level is feasible but difficult even for people." ></td>
	<td class="line x" title="64:174	This set of data was split into a development test set of 400 sentences and a blind test set of 2600 sentences." ></td>
	<td class="line x" title="65:174	Sentences are represented as vectors of binary unigram features." ></td>
	<td class="line x" title="66:174	The total number of observed unigram features is 72988." ></td>
	<td class="line x" title="67:174	In order to restrict the number of features to a manageable size, we disregard features that occur less than 10 times in the corpus." ></td>
	<td class="line x" title="68:174	With this restriction we obtain a reduced feature set of 13317 features." ></td>
	<td class="line x" title="69:174	3 Experimental Setup Our experiments were performed as follows: We started with a small set of manually-selected and annotated seed terms." ></td>
	<td class="line x" title="70:174	We used 4 positive and 6 negative seed terms." ></td>
	<td class="line x" title="71:174	We decided to use a few more negative seed words because of the inherent positive skew in the data that makes the identification of negative sentences particularly hard." ></td>
	<td class="line x" title="72:174	The terms we used are: positive: negative: good bad excellent lousy love terrible happy hate suck unreliable There was no tuning of the set of initial seed terms; the 10 words were originally chosen intuitively, as words that we observed frequently when manually inspecting the data." ></td>
	<td class="line o" title="73:174	We then used these seed terms in two basic ways: (1) We used them as seeds for a Turneystyle determination of the semantic orientation of words in the corpus (semantic orientation, or SO method)." ></td>
	<td class="line o" title="74:174	As mentioned above, this process is based on the assumption that terms of similar orientation tend to co-occur." ></td>
	<td class="line x" title="75:174	(2) We used them to mine sentiment vocabulary from the unlabeled data using the additional assumption that sentiment terms of opposite orientation tend not to co-occur at the sentence level (sentiment mining, or SM method)." ></td>
	<td class="line x" title="76:174	This method yields a set of sentiment terms, but no orientation for that set of terms." ></td>
	<td class="line x" title="77:174	We continue by using the SO method to find the semantic orientation for this set of sentiment terms, effectively using SM as a feature selection method for sentiment terminology." ></td>
	<td class="line x" title="78:174	Pseudo-code for the SO and SM approaches is provided in Figure 1 and Figure 2." ></td>
	<td class="line x" title="79:174	As a first step for both SO and SM methods (not shown in the pseudocode), PMI needs to be calculated for each pair (f, s) of feature f and seed word s over the collection of feature vectors." ></td>
	<td class="line x" title="80:174	Figure 1: SO method for determining semantic orientation 59 Figure 2: SM method for mining sentiment terms In the first scenario (using straightforward SO), features F range over all observed features in the data (modulo the aforementioned count cutoff of 10)." ></td>
	<td class="line x" title="81:174	In the second scenario (SM + SO), features F range over the n% of features with the lowest PMI scores with respect to any of the seed words that were identified using the sentiment mining technique in Figure 2." ></td>
	<td class="line x" title="82:174	The result of both SO and SM+SO is a list of unigram features which have an associated semantic orientation score, indicating their sentiment orientation: the higher the score, the more positive a term, and vice versa." ></td>
	<td class="line x" title="83:174	This list of features and associated scores can be used to construct a simple classifier: for each sentence with unknown sentiment, we take the sum of the semantic orientation scores for all of the unigrams in that sentence." ></td>
	<td class="line x" title="84:174	This overall score determines the classification of the sentence as positive, neutral or negative as shown in Figure 3." ></td>
	<td class="line x" title="85:174	Scoring and classifying sentence vectors: (1) assigning a sentence score: FOREACH feature f in sentence vector v: Score(v) = Score(v) + SO(f) (2) assigning a class label based on the sentence score: IF Score(v) > threshold1: Class(v) = positive ELSE IF Score(v) < threshold1 AND Score(v) > threshold2: Class(v) = neutral ELSE Class(v) = negative Figure 3: Using SO scores for sentence scoring and classification The two thresholds used in classification need to be determined empirically by taking the distribution of class values in the corpus into account." ></td>
	<td class="line x" title="86:174	For our experiments we simply took the distribution of class labels in the 400 sentence development test set as an approximation of the overall class label distribution: we determined that distribution to be 15.5% for negative sentences, 21.5% for neutral sentences, and 63.0% for positive sentences." ></td>
	<td class="line x" title="87:174	Scores for all sentence vectors in the corpus are then collected using the scoring part of the algorithm in Figure 3." ></td>
	<td class="line x" title="88:174	The scores are sorted and the thresholds are determined as the cutoffs for the top 63% and bottom 15.5% of scores respectively." ></td>
	<td class="line x" title="89:174	4 Results 4.1." ></td>
	<td class="line x" title="90:174	Comparing SO and SM+SO In our first set of experiments we manipulated the following parameters: 1." ></td>
	<td class="line x" title="91:174	the choice of SO or SM+SO method 2." ></td>
	<td class="line x" title="92:174	the choice of n when selecting the n% semantic terms with lowest PMI score in the SM method The tables below show the results of classifying sentence vectors using the unigram features and associated scores produced by SO and SO+SM." ></td>
	<td class="line x" title="93:174	We used the 2,600-sentence manually-annotated test set described previously to establish these numbers." ></td>
	<td class="line x" title="94:174	Since the data exhibit a strong skew in favor of the positive class label, we measure performance not in terms of accuracy but in terms of average precision and recall across the three class labels, as suggested in (Manning and Schtze 2002)." ></td>
	<td class="line x" title="95:174	Avg precision Avg recall SO 0.4481 0.4511 Table 1: Using the SO approach." ></td>
	<td class="line x" title="96:174	Table 1 shows results of using the SO method on the data." ></td>
	<td class="line x" title="97:174	Table 2 presents the results of combining the SM and SO methods for different values of n. The best results are shown in boldface." ></td>
	<td class="line x" title="98:174	As a comparison between Table 1 and Table 2 shows, the highest average precision and recall scores were obtained by combining the SM and SO methods." ></td>
	<td class="line x" title="99:174	Using SM as a feature selection mechanism also reduces the number of features significantly." ></td>
	<td class="line x" title="100:174	While the SO method employed on sentence-level vectors uses 13,000 features, the best-performing SM+SO combination uses only 20% of this feature set, indicating that SM is indeed effective in selecting the most important sentiment-bearing terms." ></td>
	<td class="line x" title="101:174	60 We also determined that the positive impact of SM is not just a matter of reducing the number of features." ></td>
	<td class="line x" title="102:174	If SO without the SM feature selection step is reduced to a comparable number of features by taking the top features according to absolute score, average precision is at 0.4445 and average recall at 0.4464." ></td>
	<td class="line x" title="103:174	N=10 N=20 N=30 N=40 N=50 Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec Avg prec Avg rec SM+SO SO from document level 0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475 Table 2: combining SM and SO." ></td>
	<td class="line x" title="104:174	Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO terms excellent, terrible, broke, junk, alright, bargain, grin, highest, exceptional, exceeded, horrible, loved, waste, ok, death, leaking, outstanding, cracked, rebate, warped, hooked, sorry, refuses, excellant, satisfying, died, biggest, competitive, delight, avoid, awful, garbage, loud, okay, competent, upscale, dated, mistake, sucks, superior, high, kill, neither excellent, happy, stylish, sporty, smooth, love, quiet, overall, pleased, plenty, dependable, solid, roomy, safe, good, easy, smaller, luxury, comfortable, style, loaded, space, classy, handling, joy, small, comfort, size, perfect, performance, room, choice, recommended, package, compliments, awesome, unique, fun, holds, comfortably, extremely, value, free, satisfied, little, recommend, limited, great, pleasure Non sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO terms alternative, wont, below, surprisingly, maintained, choosing, comparing, legal, vibration, seemed, claim, demands, assistance, knew, engineering, accelleration, ended, salesperson, performed, started, midsize, site, gonna, lets, plugs, industry, alternator, month, told, vette, 180, powertrain, write, mos, walk, causing, lift, es, segment, $250, 300m, wanna, february, mod, $50, nhtsa, suburbans, manufactured, tiburon, $10, f150, 5000, posted, tt, him, saw, jan, condition, very, handles, milage, definitely, definately, far, drives, shape, color, price, provides, options, driving, rides, sports, heated, ride, sport, forward, expected, fairly, anyone, test, fits, storage, range, family, sedan, trunk, young, weve, black, college, suv, midsize, coupe, 30, shopping, kids, player, saturn, bose, truck, town, am, leather, stereo, car, husband Table 3: the top 100 terms identified by SM and SO Table 3 shows the top 100 terms that were identified by each SM and SO methods." ></td>
	<td class="line x" title="105:174	The terms are categorized into sentiment-bearing and nonsentiment bearing terms by human judgment." ></td>
	<td class="line x" title="106:174	The two sets seem to differ in both strength and orientation of the identified terms." ></td>
	<td class="line x" title="107:174	The SM-identified words have a higher density of negative terms (22 out of 43 versus 2 out of 49 for the SO-identified terms)." ></td>
	<td class="line x" title="108:174	The SM-identified terms also express sentiment more strongly, but this conclusion is more tentative since it may be a consequence of the higher density of negative terms." ></td>
	<td class="line x" title="109:174	4.2." ></td>
	<td class="line x" title="110:174	Multiple iterations: increasing the number of seed features by SM+SO In a second set of experiments, we assessed the question of whether it is possible to use multiple iterations of the SM+SO method to gradually build the list of seed words." ></td>
	<td class="line x" title="111:174	We do this by adding the top n% of features selected by SM, along with their orientation as determined by SO, to the initial set of seed words." ></td>
	<td class="line x" title="112:174	The procedure for this round of experiments is as follows:  take the top n% of features identified by SM (we used n=1 for the reported re61 sults, since preliminary experiments with other values for n did not improve results)  perform SO for these features to determine their orientation  take the top 15.5% negative and top 63% positive (according to class label distribution in the development test set) of the features and add them as negative/positive seed features respectively This iteration increases the number of seed features from the original 10 manually-selected features to a total of 111 seed features." ></td>
	<td class="line x" title="113:174	With this enhanced set of seed features we then re-ran a subset of the experiments in Table 2." ></td>
	<td class="line x" title="114:174	Results are shown in Table 4." ></td>
	<td class="line x" title="115:174	Increasing the number of seed features through the SM feature selection method increases precision and recall by several percentage points." ></td>
	<td class="line x" title="116:174	In particular, precision and recall for negative sentences are boosted." ></td>
	<td class="line x" title="117:174	Avg precision Avg recall SM + SO, n=10, SO from document vectors 0.4826 0.48.76 SM + SO, n=30, SO from document vectors 0.4957 0.4995 SM + SO, n=50, SO from document vectors 0.4914 0.4952 Table 4: Using 2 iterations to increase the seed feature set We also confirmed that these results are truly attributable to the use of the SM method for the first iteration." ></td>
	<td class="line x" title="118:174	If we take an equivalent number of features with strongest semantic orientation according to the SO method and add them to the list of seed features, our results degrade significantly (the resulting classifier performance is significantly different at the 99.9% level as established by the McNemar test)." ></td>
	<td class="line x" title="119:174	This is further evidence that SM is indeed an effective method for selecting sentiment terms." ></td>
	<td class="line x" title="120:174	4.3." ></td>
	<td class="line x" title="121:174	Using the SO classifier to bootstrap a Naive Bayes classifier In a third set of experiments, we tried to improve on the results of the SO classifier by combining it with the bootstrapping approach described in (Nigam et al. 2000)." ></td>
	<td class="line x" title="122:174	The basic idea here is to use the SO classifier to label a subset of the data DL." ></td>
	<td class="line x" title="123:174	This labeled subset of the data is then used to bootstrap a Naive Bayes (NB) classifier on the remaining unlabeled data D U using the Expectation Maximization (EM) algorithm: (1) An initial naive Bayes classifier with parameters  is trained on the documents in DL." ></td>
	<td class="line x" title="124:174	(2) This initial classifier is used to estimate a probability distribution over all classes for each of the documents in DU." ></td>
	<td class="line x" title="125:174	(EStep) (3) The labeled and unlabeled data are then used to estimate parameters for a new classifier." ></td>
	<td class="line x" title="126:174	(M-Step) Steps 2 and 3 are repeated until convergence is achieved when the difference in the joint probability of the data and the parameters falls below the configurable threshold  between iterations." ></td>
	<td class="line x" title="127:174	Another free parameter, , can be used to control how much weight is given to the unlabeled data." ></td>
	<td class="line x" title="128:174	For our experiments we used classifiers from the best SM+SO combination (2 iterations at n=30) from Table 4 above to label 30% of the total data." ></td>
	<td class="line x" title="129:174	Table 5 shows the average precision and recall numbers for the converged NB classifier." ></td>
	<td class="line x" title="130:174	2 In addition to improving average precision and recall, the resulting classifier also has the advantage of producing class probabilities instead of simple scores." ></td>
	<td class="line x" title="131:174	3 Avg precision Avg recall Bootstrapped NB classifier 0.5167 0.52 Table 5: Results obtained by bootstrapping a NB classifier 4.4." ></td>
	<td class="line x" title="132:174	Results from supervised learning: using small sets of labeled data Given infinite resources, we can always annotate enough data to train a classifier using a supervised algorithm that will outperform unsupervised or weakly-supervised methods." ></td>
	<td class="line x" title="133:174	Which approach to take depends entirely on how much time and money are available and on the accuracy requirements for the task at hand." ></td>
	<td class="line x" title="134:174	2 In this experiment,  was set to 0.1 and  was set to 0.05." ></td>
	<td class="line x" title="135:174	3 We also experimented with labeling the whole data set with the best of our SO score classifiers, and then training a linear Support Vector Machine classifier on the data." ></td>
	<td class="line x" title="136:174	The results were considerably worse than any of the reported numbers, so they are not included in this paper." ></td>
	<td class="line x" title="137:174	62 To help situate the precision and recall numbers presented in the tables above, we trained Support Vector Machines (SVMs) using small amounts of labeled data." ></td>
	<td class="line x" title="138:174	SVMs were trained with 500, 1000, 2000, and 2500 labeled sentences." ></td>
	<td class="line x" title="139:174	Annotating 2500 sentences represents approximately eight person-hours of work." ></td>
	<td class="line x" title="140:174	The results can be found in Table 5." ></td>
	<td class="line x" title="141:174	We were pleasantly surprised at how well the unsupervised classifiers described above perform in comparison to state-of-the-art supervised methods (albeit trained on small amounts of data)." ></td>
	<td class="line x" title="142:174	Labeled examples Avg." ></td>
	<td class="line x" title="143:174	Precision Avg." ></td>
	<td class="line x" title="144:174	Recall 500.4878 .4967 1000 .5161 .5105 2000 .5297 .5256 2500 .5017 .5083 Table 6: Average precision and recall for SVMs for small numbers of labeled examples 4.5." ></td>
	<td class="line x" title="145:174	Results on the movie domain We also performed a small set of experiments on the movie domain using Pang and Lees 2004 data set." ></td>
	<td class="line x" title="146:174	This set consists of 2000 reviews, 1000 each of very positive and very negative reviews." ></td>
	<td class="line x" title="147:174	Since this data set is balanced and the task is only a two-way classification between positive and negative reviews, we only report accuracy numbers here." ></td>
	<td class="line oc" title="148:174	accuracy Training data Turney (2002) 66% unsupervised Pang & Lee (2004) 87.15% supervised Aue & Gamon (2005) 91.4% supervised SO 73.95% unsupervised SM+SO to increase seed words, then SO 74.85% weakly supervised Table 7: Classification accuracy on the movie review domain Turney (2002) achieves 66% accuracy on the movie review domain using the PMI-IR algorithm to gather association scores from the web." ></td>
	<td class="line x" title="149:174	Pang and Lee (2004) report 87.15% accuracy using a unigram-based SVM classifier combined with subjectivity detection." ></td>
	<td class="line x" title="150:174	Aue and Gamon (2005) use a simple linear SVM classifier based on unigrams, combined with LLR-based feature reduction, to achieve 91.4% accuracy." ></td>
	<td class="line o" title="151:174	Using the Turney SO method on in-domain data instead of web data achieves 73.95% accuracy (using the same two seed words that Turney does)." ></td>
	<td class="line x" title="152:174	Using one iteration of SM+SO to increase the number of seed words, followed by finding SO scores for all words with respect to the enhanced seed word set, yields a slightly higher accuracy of 74.85%." ></td>
	<td class="line x" title="153:174	With additional parameter tuning, this number can be pushed to 76.4%, at which point we achieve statistical significance at the 0.95 level according to the McNemar test, indicating that there is more room here for improvement." ></td>
	<td class="line x" title="154:174	Any reduction of the number of overall features in this domain leads to decreased accuracy, contrary to what we observed in the car review domain." ></td>
	<td class="line x" title="155:174	We attribute this observation to the smaller data set." ></td>
	<td class="line x" title="156:174	5 Discussion 5.1 A note on statistical significance We used the McNemar test to assess whether two classifiers are performing significantly differently." ></td>
	<td class="line x" title="157:174	This test establishes whether the accuracy of two classifiers differs significantly it does not guarantee significance for precision and recall differences." ></td>
	<td class="line x" title="158:174	For the latter, other tests have been proposed (e.g. Chinchor 1995), but time constraints prohibited us from implementing any of those more computationally costly tests." ></td>
	<td class="line x" title="159:174	For the results presented in the previous sections the McNemar test established statistical significance at the 0.99 level over baseline (i.e. the SO results in Table 1) for the multiple iterations results (Table 4) and the bootstrapping approach (Table 5), but not for the SM+SO approach (Table 2)." ></td>
	<td class="line x" title="160:174	5.2 Future work This exploratory set of experiments indicates a number of interesting directions for future work." ></td>
	<td class="line x" title="161:174	A shortcoming of the present work is the manual tuning of cutoff parameters." ></td>
	<td class="line x" title="162:174	This problem could be alleviated in at least two possible ways: First, using a general combination of the ranking of terms according to SM and SO." ></td>
	<td class="line x" title="163:174	In other words, calculate the semantic weight of a term as a combination of SO and its rank in the SM scores." ></td>
	<td class="line x" title="164:174	63 Secondly, following a suggestion by an anonymous reviewer, the Naive Bayes bootstrapping approach could be used in a feedback loop to inform the SO score estimation in the absence of a manually annotated parameter tuning set." ></td>
	<td class="line x" title="165:174	5.3 Summary Our results demonstrate that the SM method can serve as a valid tool to mine sentiment-rich vocabulary in a domain." ></td>
	<td class="line x" title="166:174	SM will yield a list of terms that are likely to have a strong sentiment orientation." ></td>
	<td class="line x" title="167:174	SO can then be used to find the polarity for the selected features by association with the sentiment terms of known polarity in the seed word list." ></td>
	<td class="line x" title="168:174	Performing this process iteratively by first enhancing the set of seed words through SM+SO yields the best results." ></td>
	<td class="line x" title="169:174	While this approach does not compare to the results that can be achieved by supervised learning with large amounts of labeled data, it does improve on results obtained by using SO alone." ></td>
	<td class="line x" title="170:174	We believe that this result is relevant in two respects." ></td>
	<td class="line x" title="171:174	First, by improving average precision and recall on the classification task, we move closer to the goal of unsupervised sentiment classification." ></td>
	<td class="line x" title="172:174	This is a very important goal in itself given the need for out of the box sentiment techniques in business intelligence and the notorious difficulty of rapidly adapting to a new domain (Engstrm 2004, Aue and Gamon 2005)." ></td>
	<td class="line x" title="173:174	Second, the exploratory results reported here may indicate a general source of information for feature selection in natural language tasks: features that have a tendency to be in complementary distribution (especially in smaller linguistic units such as sentences) may often form a class that shares certain properties." ></td>
	<td class="line x" title="174:174	In other words, it is not only the strong association scores that should be exploited but also the particularly weak (negative) associations." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1025
Determining Term Subjectivity And Term Orientation For Opinion Mining
Esuli, Andrea;Sebastiani, Fabrizio;"></td>
	<td class="line x" title="1:169	Determining Term Subjectivity and Term Orientation for Opinion Mining Andrea Esuli1 and Fabrizio Sebastiani2 (1) Istituto di Scienza e Tecnologie dellInformazione  Consiglio Nazionale delle Ricerche Via G Moruzzi, 1  56124 Pisa, Italy andrea.esuli@isti.cnr.it (2) Dipartimento di Matematica Pura e Applicata  Universit`a di Padova Via GB Belzoni, 7  35131 Padova, Italy fabrizio.sebastiani@unipd.it Abstract Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses." ></td>
	<td class="line x" title="2:169	To aid the extraction of opinions from text, recent work has tackled the issue of determining the orientation of subjective terms contained in text, i.e. deciding whether a term that carries opinionated content has a positive or a negative connotation." ></td>
	<td class="line x" title="3:169	This is believed to be of key importance for identifying the orientation of documents, i.e. determining whether a document expresses a positive or negative opinion about its subject matter." ></td>
	<td class="line x" title="4:169	We contend that the plain determination of the orientation of terms is not a realistic problem, since it starts from the nonrealistic assumption that we already know whether a term is subjective or not; this would imply that a linguistic resource that marks terms as subjective or objective is available, which is usually not the case." ></td>
	<td class="line x" title="5:169	In this paper we confront the task of deciding whether a given term has a positive connotation, or a negative connotation, or has no subjective connotation at all; this problem thus subsumes the problem of determining subjectivity and the problem of determining orientation." ></td>
	<td class="line x" title="6:169	We tackle this problem by testing three different variants of a semi-supervised method previously proposed for orientation detection." ></td>
	<td class="line x" title="7:169	Our results show that determining subjectivity and orientation is a much harder problem than determining orientation alone." ></td>
	<td class="line x" title="8:169	1 Introduction Opinion mining is a recent subdiscipline of computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses." ></td>
	<td class="line x" title="9:169	Opinion-driven content management has several important applications, such as determining critics opinions about a given product by classifying online product reviews, or tracking the shifting attitudes ofthegeneral public towardapolitical candidate by mining online forums." ></td>
	<td class="line x" title="10:169	Within opinion mining, several subtasks can be identified, all of them having to do with tagging a given document according to expressed opinion: 1." ></td>
	<td class="line x" title="11:169	determining document subjectivity, as in deciding whether a given text has a factual nature (i.e. describes a given situation or event, without expressing a positive or a negative opinion on it) or expresses an opinion on its subject matter." ></td>
	<td class="line x" title="12:169	This amounts to performing binary text categorization under categories Objective and Subjective (Pang and Lee, 2004; Yu and Hatzivassiloglou, 2003); 2." ></td>
	<td class="line oc" title="13:169	determining document orientation (or polarity), as in deciding if a given Subjective text expresses a Positive or a Negative opinion on its subject matter (Pang and Lee, 2004; Turney, 2002); 3." ></td>
	<td class="line x" title="14:169	determining the strength of document orientation, as in deciding e.g. whether the Positive opinion expressed by a text on its subject matter is Weakly Positive, Mildly Positive, or Strongly Positive (Wilson et al. , 2004)." ></td>
	<td class="line x" title="15:169	To aid these tasks, recent work (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Kamps et al. , 2004; Kim and Hovy, 2004; Takamura et al. , 2005; Turney and Littman, 2003) has tackled the issue of identifying the orientation ofsubjective terms contained in text, i.e.determiningwhether atermthat carries opinionated content has a positive or a negative connotation (e.g. deciding that  using Turney and Littmans (2003) examples  honest and intrepid have a positive connotation while disturbing and superfluous have a negative connotation)." ></td>
	<td class="line x" title="16:169	193 This is believed to be of key importance for identifying the orientation of documents, since it is by considering the combined contribution of these terms that one may hope to solve Tasks 1, 2 and 3 above." ></td>
	<td class="line nc" title="17:169	The conceptually simplest approach to this latter problem is probably Turneys (2002), who has obtained interesting results on Task 2 by considering the algebraic sum of the orientations of terms as representative of the orientation of the document they belong to; but more sophisticated approaches arealsopossible (Hatzivassiloglou and Wiebe, 2000; Riloff et al. , 2003; Wilson et al. , 2004)." ></td>
	<td class="line x" title="18:169	Implicit in most works dealing with term orientation is the assumption that, for many languages for which one would like to perform opinion mining, there is no available lexical resource where terms are tagged as having either a Positive or a Negative connotation, and that in the absence of such a resource the only available route is to generate such a resource automatically." ></td>
	<td class="line x" title="19:169	However, we think this approach lacks realism, since it is also true that, for the very same languages, there is no available lexical resource where terms are tagged as having either a Subjective or an Objective connotation." ></td>
	<td class="line x" title="20:169	Thus, the availability of an algorithm that tags Subjective terms as being either Positive or Negative is of little help, since determining if a term is Subjective is itself non-trivial." ></td>
	<td class="line x" title="21:169	In this paper we confront the task of determining whether a given term has a Positive connotation (e.g. honest, intrepid), or a Negative connotation (e.g. disturbing, superfluous), or has instead no Subjective connotation at all (e.g. white, triangular); this problem thus subsumes the problem of deciding between Subjective and Objective and the problem of deciding between Positive and Negative." ></td>
	<td class="line x" title="22:169	We tackle this problem by testing three different variants of the semi-supervised method for orientation detection proposed in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="23:169	Our results show that determining subjectivity and orientation is amuch harder problem than determining orientation alone." ></td>
	<td class="line x" title="24:169	1.1 Outline of the paper The rest of the paper is structured as follows." ></td>
	<td class="line x" title="25:169	Section 2 reviews related work dealing with term orientation and/or subjectivity detection." ></td>
	<td class="line x" title="26:169	Section 3 briefly reviews the semi-supervised method for orientation detection presented in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="27:169	Section 4 describes in detail three different variants of itwe propose for determining, at the same time, subjectivity and orientation, and describes the general setup of our experiments." ></td>
	<td class="line x" title="28:169	In Section 5 we discuss the results we have obtained." ></td>
	<td class="line x" title="29:169	Section 6 concludes." ></td>
	<td class="line x" title="30:169	2 Related work 2.1 Determining term orientation Most previous works dealing with the properties of terms within an opinion mining perspective have focused on determining term orientation." ></td>
	<td class="line x" title="31:169	Hatzivassiloglou and McKeown (1997) attempt to predict the orientation of subjective adjectives by analysing pairs of adjectives (conjoined by and,or,but,either-or,orneither-nor) extracted from a large unlabelled document set." ></td>
	<td class="line x" title="32:169	The underlying intuition is that the act of conjoining adjectives is subject to linguistic constraints on the orientation of the adjectives involved; e.g. and usually conjoins adjectives of equal orientation, while but conjoins adjectives of opposite orientation." ></td>
	<td class="line x" title="33:169	The authors generate a graph where terms are nodes connected by equal-orientation or opposite-orientation edges, depending on the conjunctions extracted from the document set." ></td>
	<td class="line x" title="34:169	A clustering algorithm then partitions the graph into a Positive cluster and a Negative cluster, based on a relation of similarity induced by the edges." ></td>
	<td class="line x" title="35:169	Turney and Littman (2003) determine term orientation by bootstrapping from two small sets of subjective seed terms (with the seed set for Positive containing terms such as good and nice, and the seed set for Negative containing terms such as bad and nasty)." ></td>
	<td class="line x" title="36:169	Their method is based on computing the pointwise mutual information (PMI) of the target term t with each seed term ti as a measure of their semantic association." ></td>
	<td class="line x" title="37:169	Given a target term t, its orientation value O(t) (where positive value means positive orientation, and higher absolute value means stronger orientation) is given by the sum of the weights of its semantic association with the seed positive terms minus the sum of the weights of its semantic association with the seed negative terms." ></td>
	<td class="line x" title="38:169	For computing PMI, term frequencies and co-occurrence frequencies are measured by querying a document set by means of the AltaVista search engine1 with a t query, a ti query, and a t NEARti query, and using the number of matching documents returned by the search engine as estimates of the probabilities needed for the computation of PMI." ></td>
	<td class="line x" title="39:169	Kamps et al.(2004) consider instead the graph defined on adjectives by the WordNet2 synonymy relation, and determine the orientation of a target 1http://www.altavista.com/ 2http://wordnet.princeton.edu/ 194 adjective t contained in the graph by comparing the lengths of (i) the shortest path between t and the seed term good, and (ii) the shortest path between t and the seed term bad: if the former is shorter than the latter, than t is deemed to be Positive, otherwise it is deemed to be Negative." ></td>
	<td class="line x" title="41:169	Takamura et al.(2005) determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration." ></td>
	<td class="line x" title="43:169	The authors equate terms with electrons and term orientation to spin direction." ></td>
	<td class="line x" title="44:169	They build a neighbourhood matrix connecting each pair of terms if one appears in the gloss ofthe other, and iteratively apply thespin model on the matrix until a minimum energy configuration is reached." ></td>
	<td class="line x" title="45:169	The orientation assigned to a term then corresponds to the spin direction assigned to electrons." ></td>
	<td class="line x" title="46:169	ThesystemofKimandHovy(2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others." ></td>
	<td class="line x" title="47:169	Their system starts from a set of positive and negative seed terms, and expands the positive (resp." ></td>
	<td class="line x" title="48:169	negative) seed set by adding to it the synonyms of positive (resp." ></td>
	<td class="line x" title="49:169	negative) seed termsandtheantonyms ofnegative(resp." ></td>
	<td class="line x" title="50:169	positive) seed terms." ></td>
	<td class="line x" title="51:169	The system classifies then a target term t into either Positive or Negative by means of two alternative learning-free methods based on the probabilities that synonyms of t also appear in the respective expanded seed sets." ></td>
	<td class="line x" title="52:169	A problem with this method is that it can classify only terms that share somesynonyms withtheexpanded seed sets." ></td>
	<td class="line x" title="53:169	Kim and Hovy also report an evaluation of human inter-coder agreement." ></td>
	<td class="line x" title="54:169	We compare this evaluation with our results in Section 5." ></td>
	<td class="line x" title="55:169	The approach we have proposed for determining term orientation (Esuli and Sebastiani, 2005) is described in more detail in Section 3, since it will be extensively used in this paper." ></td>
	<td class="line x" title="56:169	All these works evaluate the performance of the proposed algorithms by checking them against precompiled sets of Positive and Negative terms, i.e. checking how good the algorithms are at classifying a term known to be subjective into either Positive or Negative." ></td>
	<td class="line x" title="57:169	When tested on the same benchmarks, the methods of (Esuli and Sebastiani, 2005; Turney and Littman, 2003) have performed with comparable accuracies (however, the method of (Esuli and Sebastiani, 2005) is much more efficient than the one of (Turney and Littman, 2003)), and have outperformed the method of (Hatzivassiloglou and McKeown, 1997) by a wide margin and the one by (Kamps et al. , 2004) by a very wide margin." ></td>
	<td class="line x" title="58:169	The methods described in (Hatzivassiloglou and McKeown, 1997) is also limited by the fact that it can only decide the orientation of adjectives, while the method of (Kamps et al. , 2004) is further limited in that it can only work on adjectives that are present in WordNet." ></td>
	<td class="line x" title="59:169	The methods of (Kim and Hovy, 2004; Takamura et al. , 2005) are instead difficult to compare with the other ones since they were not evaluated on publicly available datasets." ></td>
	<td class="line x" title="60:169	2.2 Determining term subjectivity Riloff et al.(2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms." ></td>
	<td class="line x" title="62:169	The method identifies patterns for the extraction of subjective nouns from text, bootstrapping from a seed set of 20 terms that the authors judge to be strongly subjective and have found to have high frequency in the text collection from which the subjective nouns must be extracted." ></td>
	<td class="line x" title="63:169	The results of this method are not easy to compare with the ones we present in this paper because of the different evaluation methodologies." ></td>
	<td class="line x" title="64:169	While we adopt the evaluation methodology used in all of the papers reviewed so far (i.e. checking how good our system is at replicating an existing, independently motivated lexical resource), the authors do not test their method on an independently identified set of labelled terms, butonthesetoftermsthatthealgorithm itself extracts." ></td>
	<td class="line x" title="65:169	This evaluation methodology only allows to test precision, and not accuracy tout court, since no quantification can be made of false negatives (i.e. the subjective terms that the algorithm should have spotted but has not spotted)." ></td>
	<td class="line x" title="66:169	In Section 5 this will prevent us from drawing comparisons between this method and our own." ></td>
	<td class="line x" title="67:169	Baroni and Vegnaduzzo (2004) apply the PMI method, first used by Turney and Littman (2003) to determine term orientation, to determine term subjectivity." ></td>
	<td class="line x" title="68:169	Their method uses a small set Ss of 35 adjectives, marked as subjective by human judges, toassign asubjectivity score toeachadjective to be classified." ></td>
	<td class="line x" title="69:169	Therefore, their method, unlike our own, does not classify terms (i.e. take firm classification decisions), but ranks them according to a subjectivity score, on which they evaluate precision at various level of recall." ></td>
	<td class="line x" title="70:169	195 3 Determining term subjectivity and term orientation by semi-supervised learning The method we use in this paper for determining term subjectivity and term orientation is a variant of the method proposed in (Esuli and Sebastiani, 2005) for determining term orientation alone." ></td>
	<td class="line x" title="71:169	This latter method relies on training, in a semisupervised way, a binary classifier that labels terms as either Positive or Negative." ></td>
	<td class="line x" title="72:169	A semisupervised method is a learning process whereby only a small subset L  Tr of the training data Tr are human-labelled." ></td>
	<td class="line x" title="73:169	In origin the training data in U = Tr  L are instead unlabelled; it is the process itself that labels them, automatically, by using L (with the possible addition of other publicly available resources) as input." ></td>
	<td class="line x" title="74:169	The method of (Esuli and Sebastiani, 2005) starts from two small seed (i.e. training) sets Lp and Ln of known Positive and Negativeterms, respectively, and expands them into the two final training sets Trp  Lp andTrn  Ln byadding them new sets of terms Up and Un found by navigating the WordNet graph along the synonymy and antonymy relations3." ></td>
	<td class="line x" title="75:169	This process is based on the hypothesis that synonymy and antonymy, in addition to defining a relation of meaning, also define a relation of orientation, i.e. that two synonyms typically have the same orientation and two antonyms typically have opposite orientation." ></td>
	<td class="line x" title="76:169	The method is iterative, generating two sets Trkp and Trkn at each iteration k, where Trkp  Trk1p   Tr1p = Lp and Trkn  Trk1n    Tr1n = Ln." ></td>
	<td class="line x" title="77:169	At iteration k, Trkp is obtained by adding to Trk1p all synonyms of terms in Trk1p and all antonyms of terms in Trk1n ; similarly, Trkn is obtained by adding to Trk1n all synonyms of terms in Trk1n and allantonyms oftermsinTrk1p . Ifatotal ofK iterations are performed, then Tr = TrKp TrKn . The second main feature of the method presented in (Esuli and Sebastiani, 2005) is that terms are given vectorial representations based on their WordNet glosses (i.e. textual definitions)." ></td>
	<td class="line x" title="78:169	For each term ti in TrTe (Te being the test set, i.e. thesetoftermstobeclassified), atextual representation of ti is generated by collating all the glosses of ti as found in WordNet4." ></td>
	<td class="line x" title="79:169	Each such represen3Several other WordNet lexical relations, and several combinations of them, are tested in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="80:169	In the present paper we only use the best-performing such combination, as described in detail in Section 4.2." ></td>
	<td class="line x" title="81:169	The version of WordNet used here and in (Esuli and Sebastiani, 2005) is 2.0." ></td>
	<td class="line x" title="82:169	4In general a term ti may have more than one gloss, since tation is converted into vectorial form by standard text indexing techniques (in (Esuli and Sebastiani, 2005) and in the present work, stop words are removed and the remaining words are weighted by cosine-normalized tfidf; no stemming is performed)5." ></td>
	<td class="line x" title="83:169	This representation method is based on the assumption that terms with a similar orientation tend to have similar glosses: for instance, that the glosses of honest and intrepid will both contain appreciative expressions, while the glosses of disturbing and superfluous will both contain derogative expressions." ></td>
	<td class="line x" title="84:169	Note that this method allows to classify any term, independently of its POS, provided there is a gloss for it in the lexical resource." ></td>
	<td class="line x" title="85:169	Once the vectorial representations for all terms inTrTehavebeengenerated, thosefortheterms in Tr are fed to a supervised learner, which thus generates a binary classifier." ></td>
	<td class="line x" title="86:169	This latter, once fed with the vectorial representations of the terms in Te, classifies each of them as either Positive or Negative." ></td>
	<td class="line x" title="87:169	4 Experiments In this paper we extend the method of (Esuli and Sebastiani, 2005) tothedetermination oftermsubjectivity and term orientation altogether." ></td>
	<td class="line x" title="88:169	4.1 Test sets The benchmark (i.e. test set) we use for our experiments is the General Inquirer (GI) lexicon (Stone et al. , 1966)." ></td>
	<td class="line x" title="89:169	This is a lexicon of terms labelled according to a large set of categories6, each one denoting the presence of a specific trait in the term." ></td>
	<td class="line x" title="90:169	The two main categories, and the ones we will be concerned with, are Positive/Negative, which contain 1,915/2,291 terms having a positive/negative orientation (in what follows we will also refer to the category Subjective, which we define as the union of the two categories Positive and Negative)." ></td>
	<td class="line x" title="91:169	In opinion mining research the GI was first used by Turney and Littman (2003), who reduced the list of terms to 1,614/1,982 entries afit may have more than one sense; dictionaries normally associate one gloss to each sense." ></td>
	<td class="line x" title="92:169	5Several combinations of subparts of a WordNet gloss are tested as textual representations of terms in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="93:169	Of all those combinations, in the present paper we always use the DGS combination, since this is the one that has been shown to perform best in (Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="94:169	DGS corresponds to using the entire gloss and performing negation propagation on itstext, i.e. replacing allthe terms that occur after a negation in a sentence with negated versions of the term (see (Esuli and Sebastiani, 2005) for details)." ></td>
	<td class="line x" title="95:169	6The definitions of all such categories are available at http://www.webuse.umd.edu:9090/ 196 terremoving 17termsappearing inboth categories (e.g. deal) and reducing all the multiple entries of the same term in a category, caused by multiple senses, to a single entry." ></td>
	<td class="line x" title="96:169	Likewise, we take all the 7,582 GI terms that are not labelled as either Positive or Negative, as being (implicitly) labelled as Objective, and reduce them to 5,009 terms after combining multiple entries of the same term, caused by multiple senses, to a single entry." ></td>
	<td class="line x" title="97:169	The effectiveness of our classifiers will thus be evaluated in terms of their ability to assign the total 8,605 GI terms to the correct category among Positive, Negative, and Objective7." ></td>
	<td class="line x" title="98:169	4.2 Seed sets and training sets Similarly to (Esuli and Sebastiani, 2005), our training set is obtained by expanding initial seed sets by means of WordNet lexical relations." ></td>
	<td class="line x" title="99:169	The main difference is that our training set is now the union of three sets of training terms Tr = TrKp TrKn TrKo obtained byexpanding, through K iterations, three seed sets Tr1p,Tr1n,Tr1o, one for each of the categories Positive, Negative, and Objective, respectively." ></td>
	<td class="line x" title="100:169	Concerning categories Positive and Negative, we have used the seed sets, expansion policy, and number of iterations, that have performed best in the experiments of (Esuli and Sebastiani, 2005), i.e. the seed sets Tr1p = {good} and Tr1n = {bad} expanded by using the union of synonymy and indirect antonymy, restricting the relations only to terms with the same POS of the original terms (i.e. adjectives), for a total of K = 4 iterations." ></td>
	<td class="line x" title="101:169	The final expanded sets contain 6,053 Positive terms and 6,874 Negative terms." ></td>
	<td class="line x" title="102:169	Concerning the category Objective, the process we have followed is similar, but with a few key differences." ></td>
	<td class="line x" title="103:169	These are motivated by the fact that the Objective category coincides with the complement of the union of Positive and Negative; therefore, Objective terms are more varied and diverse in meaning than the terms in the other two categories." ></td>
	<td class="line x" title="104:169	To obtain a representative expanded set TrKo, we have chosen the seed set Tr1o = {entity} and we have expanded it by using, along with synonymy and antonymy, the WordNet relation of hyponymy (e.g. vehicle / car),andwithout imposing the restriction that the two related terms must have the same POS." ></td>
	<td class="line x" title="105:169	These choices are strictly related to each other: the term entityis the root term of the largest generalization hierarchy in WordNet, with more than 40,000 7We make this labelled term set available for download at http://patty.isti.cnr.it/esuli/software/ SentiGI.tgz." ></td>
	<td class="line x" title="106:169	terms (Devitt and Vogel, 2004), thus allowing to reach a very large number of terms by using the hyponymy relation8." ></td>
	<td class="line x" title="107:169	Moreover, it seems reasonable to assume that terms that refer to entities are likely to have an objective nature, and that hyponyms (and also synonyms and antonyms) of an objective term are also objective." ></td>
	<td class="line x" title="108:169	Note that, at each iteration k, a given term t is added to Trko only if it does not already belong to either Trp or Trn." ></td>
	<td class="line x" title="109:169	We experiment with two different choices for the Tro set, corresponding to the sets generated in K = 3 and K = 4 iterations, respectively; this yields sets Tr3o and Tr4o consisting of 8,353 and 33,870 training terms, respectively." ></td>
	<td class="line x" title="110:169	4.3 Learning approaches and evaluation measures We experiment with three philosophically different learning approaches to the problem of distinguishing between Positive, Negative, and Objective terms." ></td>
	<td class="line x" title="111:169	Approach I is a two-stage method which consists in learning two binary classifiers: the first classifier places terms into either Subjective or Objective, while the second classifier places terms that have been classified as Subjective by thefirstclassifier into either Positive orNegative." ></td>
	<td class="line x" title="112:169	In the training phase, the terms in TrKp TrKn are used as training examples of category Subjective." ></td>
	<td class="line x" title="113:169	Approach II is again based on learning two binary classifiers." ></td>
	<td class="line x" title="114:169	Here, one of them must discriminate between terms that belong to the Positive category and ones that belong to its complement (not Positive), while the other must discriminate between terms that belong to the Negative category and ones that belong to its complement (not Negative)." ></td>
	<td class="line x" title="115:169	Terms that have been classified both into Positive by the former classifier and into (not Negative) by the latter are deemed to be positive, and terms that have been classified both into (not Positive) by the former classifier and into Negative by the latter are deemed to be negative." ></td>
	<td class="line x" title="116:169	The terms that have been classified (i) into both (not Positive) and (not Negative), or (ii) into both Positive and Negative, are taken to be Objective." ></td>
	<td class="line x" title="117:169	In the training phase of Approach II, the terms in TrKn  TrKo are used as training examples of category (not Positive), and the terms in TrKp TrKo are used as training examples of category (not Negative)." ></td>
	<td class="line x" title="118:169	Approach III consists instead in viewing Positive, Negative, and Objective as three categories 8The synonymy relation connects instead only 10,992 terms at most (Kamps et al. , 2004)." ></td>
	<td class="line x" title="119:169	197 with equal status, and in learning a ternary classifier that classifies each term into exactly one among the three categories." ></td>
	<td class="line x" title="120:169	There are several differences among these three approaches." ></td>
	<td class="line x" title="121:169	A first difference, of a conceptual nature, is that only Approaches I and III view Objective as a category, or concept, in its own right, while Approach II views objectivity as a nonexistent entity, i.e. as the absence of subjectivity (in fact, in Approach II the training examples of Objective are only used as training examples of the complements of Positive and Negative)." ></td>
	<td class="line x" title="122:169	Asecond difference isthatApproaches Iand II are based on standard binary classification technology, while Approach III requires multiclass (i.e. 1-of-m) classification." ></td>
	<td class="line x" title="123:169	As a consequence, while for the former we use well-known learners for binary classification (the naive Bayesian learner using the multinomial model (McCallum and Nigam, 1998), support vector machines using linear kernels (Joachims, 1998), the Rocchio learner, and its PrTFIDFprobabilistic version (Joachims, 1997)), for Approach III we use their multiclass versions9." ></td>
	<td class="line x" title="124:169	Before running our learners we make a pass of feature selection, with the intent of retaining only those features that are good at discriminating our categories, while discarding those which are not." ></td>
	<td class="line x" title="125:169	Feature selection is implemented by scoring each feature fk (i.e. each term that occurs in the glosses of at least one training term) by means of the mutual information (MI) function, defined as MI(fk) = summationdisplay c{c1,,cm}, f{fk,fk} Pr(f,c)  log Pr(f,c)Pr(f)Pr(c) (1) and discarding the x% features fk that minimize it." ></td>
	<td class="line x" title="126:169	We will call x% the reduction factor." ></td>
	<td class="line x" title="127:169	Note that thesetc1,,cm}fromEquation 1isinterpreted differently in Approaches I to III, and always consistently with who the categories at stake are." ></td>
	<td class="line x" title="128:169	Since the task we aim to solve is manifold, we will evaluate our classifiers according to two evaluation measures:  SO-accuracy, i.e. the accuracy of a classifier inseparating SubjectivefromObjective,i.e. in deciding term subjectivity alone;  PNO-accuracy, the accuracy of a classifier in discriminating among Positive, Negative, 9The naive Bayesian, Rocchio, and PrTFIDF learners we have used are from Andrew McCallums Bow package (http://www-2.cs.cmu.edu/mccallum/bow/), while the SVMs learner we have used is Thorsten Joachims SV Mlight (http://svmlight.joachims.org/), version 6.01." ></td>
	<td class="line x" title="129:169	Both packages allow the respective learners to be run in multiclass fashion." ></td>
	<td class="line x" title="130:169	Table 1: Average and best accuracy values over the four dimensions analysed in the experiments." ></td>
	<td class="line x" title="131:169	Dimension SO-accuracy PNO-accuracy Avg () Best Avg () Best Approach I .635 (.020) .668 .595 (.029) .635 II .636 (.033) .676 .614 (.037) .660 III .635 (.036) .674 .600 (.039) .648 Learner NB .653 (.014) .674 .619 (.022) .647 SVMs .627 (.033) .671 .601 (.037) .658 Rocchio .624 (.030) .654 .585 (.033) .616 PrTFIDF .637 (.031) .676 .606 (.042) .660 TSR 0% .649 (.025) .676 .619 (.027) .660 50% .650 (.022) .670 .622 (.022) .657 80% .646 (.023) .674 .621 (.021) .647 90% .642 (.024) .667 .616 (.024) .651 95% .635 (.027) .671 .606 (.031) .658 99% .612 (.036) .661 .570 (.049) .647 TrKo set Tr3o .645 (.006) .676 .608 (.007) .658 Tr4o .633 (.013) .674 .610 (.018) .660 and Objective, i.e. in deciding both term orientation and subjectivity." ></td>
	<td class="line x" title="132:169	5 Results We present results obtained from running every combination of (i) the three approaches to classification described in Section 4.3, (ii) the four learners mentioned in the same section, (iii) five different reduction factors for feature selection (0%, 50%, 90%, 95%, 99%), and (iv) the two different training sets (Tr3o and Tr4o) for Objective mentioned in Section 4.2." ></td>
	<td class="line x" title="133:169	We discuss each of these four dimensions of the problem individually, for each one reporting results averaged across all the experiments we have run (see Table 1)." ></td>
	<td class="line x" title="134:169	The first and most important observation is that, with respect to a pure term orientation task, accuracy drops significantly." ></td>
	<td class="line x" title="135:169	In fact, the best SOaccuracy and the best PNO-accuracy results obtained across the 120 different experiments are .676 and .660, respectively (these were obtained by using Approach II with the PrTFIDF learner and no feature selection, with Tro = Tr3o for the .676 SO-accuracy result and Tro = Tr4o for the .660 PNO-accuracy result); this contrasts sharply with the accuracy obtained in (Esuli and Sebastiani, 2005) on discriminating Positive from Negative (where the best run obtained .830 accuracy), on the same benchmarks and essentially the same algorithms." ></td>
	<td class="line x" title="136:169	This suggests that good performance at orientation detection (as e.g. in (Esuli and Sebastiani, 2005; Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003)) may not be a 198 Table 2: Human inter-coder agreement values reported by Kim and Hovy (2004)." ></td>
	<td class="line x" title="137:169	Agreement Adjectives (462) Verbs (502) measure Hum1 vs Hum2 Hum2 vs Hum3 Strict .762 .623 Lenient .890 .851 guarantee of good performance at subjectivity detection, quite evidently a harder (and, as we have suggested, more realistic) task." ></td>
	<td class="line x" title="138:169	This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels." ></td>
	<td class="line x" title="139:169	The authors define two measures of such agreement: strict agreement, equivalent to our PNO-accuracy, and lenient agreement, which measures the accuracy at telling Negative against the rest." ></td>
	<td class="line x" title="140:169	For any experiment, strict agreement values are then going to be, by definition, lower or equal than the corresponding lenient ones." ></td>
	<td class="line x" title="141:169	Theauthors use two sets of 462 adjectives and 502 verbs, respectively, randomly extracted from the basic English word list of the TOEFL test." ></td>
	<td class="line x" title="142:169	The intercoder agreement results (see Table 2) show a deterioration in agreement (from lenient to strict) of 16.77% for adjectives and 36.42% for verbs." ></td>
	<td class="line x" title="143:169	Following this, we evaluated our best experiment according to these measures, and obtained a strict accuracy value of .660 and a lenient accuracy value of .821, with a relative deterioration of 24.39%, in line with Kim and Hovys observation10." ></td>
	<td class="line x" title="144:169	This confirms that determining subjectivity and orientation is a much harder task than determining orientation alone." ></td>
	<td class="line x" title="145:169	The second important observation is that there is very little variance in the results: across all 120 experiments, average SO-accuracy and PNOaccuracy results were .635 (with standard deviation  = .030) and .603 ( = .036), a mere 6.06% and 8.64% deterioration from the best results reported above." ></td>
	<td class="line x" title="146:169	This seems to indicate that the levels of performance obtained may be hard to improve upon, especially if working in a similar framework." ></td>
	<td class="line x" title="147:169	Let us analyse the individual dimensions of the problem." ></td>
	<td class="line x" title="148:169	Concerning the three approaches to classification described in Section 4.3, Approach II outperforms the other two, but by an extremely narrow margin." ></td>
	<td class="line x" title="149:169	As for the choice of learners, on average the best performer is NB, but again by a very small margin wrt the others." ></td>
	<td class="line x" title="150:169	On average, the 10We observed this trend in all of our experiments." ></td>
	<td class="line x" title="151:169	best reduction factor for feature selection turns out to be 50%, but the performance drop we witness in approaching 99% (a dramatic reduction factor) is extremely graceful." ></td>
	<td class="line x" title="152:169	As for the choice of TrKo, we note that Tr3o and Tr4o elicit comparable levels of performance, with the former performing best at SO-accuracy and the latter performing best at PNO-accuracy." ></td>
	<td class="line x" title="153:169	An interesting observation on the learners we have used is that NB, PrTFIDF and SVMs, unlike Rocchio, generate classifiers that depend on P(ci), the prior probabilities of the classes, which are normally estimated as the proportion of training documents that belong to ci." ></td>
	<td class="line x" title="154:169	In many classification applications this is reasonable, as we may assume that the training data are sampled from the samedistribution fromwhichthetestdataaresampled, and that these proportions are thus indicative of the proportions that we are going to encounter in the test data." ></td>
	<td class="line x" title="155:169	However, in our application this is not the case, since we do not have a natural sample of training terms." ></td>
	<td class="line x" title="156:169	What we have is one human-labelled training term for each category in {Positive,Negative,Objective}, and as many machine-labelled terms as we deem reasonable to include, in possibly different numbers for the different categories; and we have no indication whatsoever as to what the natural proportions among the three might be." ></td>
	<td class="line x" title="157:169	This means that the proportions of Positive, Negative, and Objective terms we decide to include in the training set will strongly bias the classification results if the learner is one of NB, PrTFIDF and SVMs." ></td>
	<td class="line x" title="158:169	We may notice this by looking at Table 3, which shows the average proportion of test terms classified as Objective by each learner, depending on whether we have chosen Tro to coincide with Tr3o or Tr4o; note that the former (resp." ></td>
	<td class="line x" title="159:169	latter) choice means having roughly as many (resp." ></td>
	<td class="line x" title="160:169	roughly five times as many) Objective training terms as there are Positive and Negative ones." ></td>
	<td class="line x" title="161:169	Table 3 shows that, the more Objective training terms there are, the more test terms NB, PrTFIDF and (in particular) SVMs will classify as Objective; this is not true for Rocchio, which is basically unaffected by the variation in size of Tro." ></td>
	<td class="line x" title="162:169	6 Conclusions We have presented a method for determining both term subjectivity and term orientation for opinion mining applications." ></td>
	<td class="line x" title="163:169	This is a valuable advance with respect to the state of the art, since past work in this area had mostly confined to determining term orientation alone, a task that (as we have ar199 Table 3: Average proportion of test terms classified as Objective, for each learner and for each choice of the TrKo set." ></td>
	<td class="line x" title="164:169	Learner Tr3o Tr4o Variation NB .564 ( = .069) .693 (.069) +23.0% SVMs .601 (.108) .814 (.083) +35.4% Rocchio .572 (.043) .544 (.061) -4.8% PrTFIDF .636 (.059) .763 (.085) +20.0% gued) has limited practical significance in itself, given the generalized absence of lexical resources that tag terms as being either Subjective or Objective." ></td>
	<td class="line x" title="165:169	Our algorithms have tagged by orientation and subjectivity the entire General Inquirer lexicon, a complete general-purpose lexicon that is the de facto standard benchmark for researchers in this field." ></td>
	<td class="line x" title="166:169	Our results thus constitute, for this task, the first baseline for other researchers to improve upon." ></td>
	<td class="line x" title="167:169	Unfortunately, our results have shown that an algorithm that had shown excellent, stateof-the-art performance in deciding term orientation (Esuli and Sebastiani, 2005), once modified for the purposes of deciding term subjectivity, performs more poorly." ></td>
	<td class="line x" title="168:169	This has been shown by testing several variants of the basic algorithm, some of them involving radically different supervised learning policies." ></td>
	<td class="line x" title="169:169	The results suggest that deciding term subjectivity is a substantially harder task that deciding term orientation alone." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1026
Latent Variable Models For Semantic Orientations Of Phrases
Takamura, Hiroya;Inui, Takashi;Okumura, Manabu;"></td>
	<td class="line x" title="1:189	Latent Variable Models for Semantic Orientations of Phrases Hiroya Takamura Precision and Intelligence Laboratory Tokyo Institute of Technology takamura@pi.titech.ac.jp Takashi Inui Japan Society of the Promotion of Science tinui@lr.pi.titech.ac.jp Manabu Okumura Precision and Intelligence Laboratory Tokyo Institute of Technology oku@pi.titech.ac.jp Abstract We propose models for semantic orientations of phrases as well as classification methods based on the models." ></td>
	<td class="line x" title="2:189	Although eachphraseconsistsofmultiplewords, the semantic orientation of the phrase is not a mere sum of the orientations of the component words." ></td>
	<td class="line x" title="3:189	Some words can invert the orientation." ></td>
	<td class="line x" title="4:189	In order to capture the property of such phrases, we introduce latent variables into the models." ></td>
	<td class="line x" title="5:189	Through experiments, we show that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy." ></td>
	<td class="line x" title="6:189	1 Introduction Technologyforaffectanalysisoftextshasrecently gained attention in both academic and industrial areas." ></td>
	<td class="line x" title="7:189	It can be applied to, for example, a survey of new products or a questionnaire analysis." ></td>
	<td class="line x" title="8:189	Automatic sentiment analysis enables a fast and comprehensive investigation." ></td>
	<td class="line x" title="9:189	The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: desirable or undesirable (positive or negative)." ></td>
	<td class="line x" title="10:189	For example, the word beautiful is positive, while the word dirty is negative." ></td>
	<td class="line x" title="11:189	Many researchers have developed several methods for this purpose and obtained good results (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al. , 2004; Takamura et al. , 2005; Kobayashi et al. , 2001)." ></td>
	<td class="line x" title="12:189	One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions." ></td>
	<td class="line x" title="13:189	No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words." ></td>
	<td class="line x" title="14:189	The purpose of this paperistoproposecomputationalmodelsforphrases with semantic orientations as well as classification methods based on the models." ></td>
	<td class="line x" title="15:189	Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the most basic orientations of phrases." ></td>
	<td class="line x" title="16:189	We believe that we can use the obtained basic orientations of phrases for affect analysis of higher linguistic units such as sentences and documents." ></td>
	<td class="line x" title="17:189	The semantic orientation of a phrase is not a mere sum of its component words." ></td>
	<td class="line x" title="18:189	Semantic orientations can emerge out of combinations of non-oriented words." ></td>
	<td class="line x" title="19:189	For example, light laptopcomputer is positively oriented although neither light nor laptop-computer has a positive orientation." ></td>
	<td class="line x" title="20:189	Besides, some words can invert the orientation of a neighboring word, such as low in low risk, where the negative orientation of risk is inverted to a positive by the adjective low." ></td>
	<td class="line x" title="21:189	This kind of non-compositional operation has to be incorporated into the model." ></td>
	<td class="line x" title="22:189	We focus on noun+adjective in this paper, since this type of phrase contains most of interesting properties of phrases, such as emergence or inversion of semantic orientations." ></td>
	<td class="line x" title="23:189	In order to capture the properties of semantic orientations of phrases, we introduce latent variables into the models, where one random variable corresponds to nouns and another random variable corresponds to adjectives." ></td>
	<td class="line x" title="24:189	The words that are similar in terms of semantic orientations, such as risk and mortality (i.e. , the positive orientation emerges when they are low), make a cluster in these models." ></td>
	<td class="line x" title="25:189	Our method is language201 independent in the sense that it uses only cooccurrence data of words and semantic orientations." ></td>
	<td class="line x" title="26:189	2 Related Work We briefly explain related work from two viewpoints: the classification of word pairs and the identification of semantic orientation." ></td>
	<td class="line x" title="27:189	2.1 Classification of Word Pairs Torisawa (2001) used a probabilistic model to identify the appropriate case for a pair of words constituting a noun and a verb with the case of the noun-verb pair unknown." ></td>
	<td class="line x" title="28:189	Their model is the same as Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 2001), which is a generative probability model of two random variables." ></td>
	<td class="line x" title="29:189	Torisawas method is similar to ours in that a latent variable model is used for word pairs." ></td>
	<td class="line x" title="30:189	However, Torisawas objective is different from ours." ></td>
	<td class="line x" title="31:189	In addition, we used not the original PLSI, but its expanded version, which is more suitable for this task of semantic orientation classification of phrases." ></td>
	<td class="line x" title="32:189	Fujita et al.(2004) addressed the task of the detection of incorrect case assignment in automatically paraphrased sentences." ></td>
	<td class="line x" title="34:189	They reduced the task to a problem of classifying pairs of a verb and a noun with a case into correct or incorrect." ></td>
	<td class="line x" title="35:189	They first obtained a latent semantic space with PLSI and adopted the nearest-neighbors method, in which they used latent variables as features." ></td>
	<td class="line x" title="36:189	Fujita et al.s method is different from ours, and also from Torisawas, in that a probabilistic model is used for feature extraction." ></td>
	<td class="line x" title="37:189	2.2 Identification of Semantic Orientations The semantic orientation classification of words has been pursued by several researchers (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al. , 2004; Takamura et al. , 2005)." ></td>
	<td class="line x" title="38:189	However, no computational model for semantically oriented phrases has been proposed to date although research for a similar purpose has been proposed." ></td>
	<td class="line x" title="39:189	Some researchers used sequences of words as features in document classification according to semantic orientation." ></td>
	<td class="line x" title="40:189	Pang et al.(2002) used bigrams." ></td>
	<td class="line x" title="42:189	Matsumoto et al.(2005) used sequential patterns and tree patterns." ></td>
	<td class="line x" title="44:189	Although such patterns were proved to be effective in document classification, the semantic orientations of the patterns themselves are not considered." ></td>
	<td class="line x" title="45:189	Suzuki et al.(2006) used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3-term evaluative expressions." ></td>
	<td class="line x" title="47:189	They focused on the utilization of context information such as neighboring words and emoticons." ></td>
	<td class="line oc" title="48:189	Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, whichhadoriginallybeendevelopedforwordsentiment classification." ></td>
	<td class="line o" title="49:189	In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g. , phrase NEAR good) is used to determine the orientation." ></td>
	<td class="line x" title="50:189	Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences." ></td>
	<td class="line o" title="51:189	Their method is similar to Turneys in the sense that cooccurrence with seed words is used." ></td>
	<td class="line o" title="52:189	The three methods above are based on context information." ></td>
	<td class="line x" title="53:189	In contrast, our method exploits the internal structure of the semantic orientations of phrases." ></td>
	<td class="line x" title="54:189	Inui (2004) introduced an attribute plus/minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus/minus attribute values and the positive/negative attribute values of the component words." ></td>
	<td class="line x" title="55:189	For example, a rule [negative+minus=positive] determines low (minus) risk (negative) to be positive." ></td>
	<td class="line x" title="56:189	Wilson et al.(2005) worked on phrase-level semantic orientations." ></td>
	<td class="line x" title="58:189	They introduced a polarity shifter, which is almost equivalent to the plus/minus attribute above." ></td>
	<td class="line x" title="59:189	They manually created the list of polarity shifters." ></td>
	<td class="line x" title="60:189	The method that we propose in this paper is an automatic version of Inuis or Wilson et al.s idea, in the sense that the method automatically creates word clusters and their polarity shifters." ></td>
	<td class="line x" title="61:189	3 Latent Variable Models for Semantic Orientations of Phrases As mentioned in the Introduction, the semantic orientation of a phrase is not a mere sum of its component words." ></td>
	<td class="line x" title="62:189	If we know that low risk is positive, and that risk and mortality, in some sense, belong to the same semantic cluster, we can infer that low mortality is also positive." ></td>
	<td class="line x" title="63:189	Therefore, we propose to use latent variable models to extract such latent semantic clusters and to realize an accurate classification of phrases (we focus 202 N Z A N A C N Z A C N Z A C N Z A C (a) (b) (c) (d) (e) Figure 1: Graphical representations:(a) PLSI, (b) naive bayes, (c) 3-PLSI, (d) triangle, (e) U-shaped; Each node indicates a random variable." ></td>
	<td class="line x" title="64:189	Arrows indicate statistical dependency between variables." ></td>
	<td class="line x" title="65:189	N, A, Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations." ></td>
	<td class="line x" title="66:189	on two-term phrases in this paper)." ></td>
	<td class="line x" title="67:189	The models adopted in this paper are also used for collaborative filtering by Hofmann (2004)." ></td>
	<td class="line x" title="68:189	With these models, the nouns (e.g. , risk and mortality) that become positive by reducing their degree or amount would make a cluster." ></td>
	<td class="line x" title="69:189	On the other hand, the adjectives or verbs (e.g. , reduce and decrease) that are related to reduction would also make a cluster." ></td>
	<td class="line x" title="70:189	Figure 1 shows graphical representations of statistical dependencies of models with a latent variable." ></td>
	<td class="line x" title="71:189	N, A, Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations." ></td>
	<td class="line x" title="72:189	Figure 1-(a) is the PLSI model, which cannot be used in this task due to the absence of a variable for semantic orientations." ></td>
	<td class="line x" title="73:189	Figure 1-(b) is the naive bayes model, in which nouns and adjectives are statistically independent of each other given the semantic orientation." ></td>
	<td class="line x" title="74:189	Figure 1-(c) is, what we call, the 3-PLSI model, which is the 3observable variable version of the PLSI." ></td>
	<td class="line x" title="75:189	We call Figure 1-(d) the triangle model, since three of its four variables make a triangle." ></td>
	<td class="line x" title="76:189	We call Figure 1(e) the U-shaped model." ></td>
	<td class="line x" title="77:189	In the triangle model and the U-shaped model, adjectives directly influence semantic orientations (rating categories) through the probability P(c|az)." ></td>
	<td class="line x" title="78:189	While nouns and adjectives are associated with the same set of clusters Z in the 3-PLSI and the triangle models, only nouns are clustered in the U-shaped model." ></td>
	<td class="line x" title="79:189	In the following, we construct a probability model for the semantic orientations of phrases using each model of (b) to (e) in Figure 1." ></td>
	<td class="line x" title="80:189	We explain in detail the triangle model and the U-shaped model, which we will propose to use for this task." ></td>
	<td class="line x" title="81:189	3.1 Triangle Model Suppose that a set D of tuples of noun n, adjective a (predicate, generally) and the rating c is given : D = {(n1,a1,c1),,(n|D|,a|D|,c|D|)}, (1) where c  {1,0,1}, for example." ></td>
	<td class="line x" title="82:189	This can be easily expanded to the case ofc  {1,,5}." ></td>
	<td class="line x" title="83:189	Our purposeistopredicttheratingcforunknownpairs of n and a. According to Figure 1-(d), the generative probability of n,a,c,z is the following : P(nacz) = P(z|n)P(a|z)P(c|az)P(n)." ></td>
	<td class="line x" title="84:189	(2) Remember that for the original PLSI model, P(naz) = P(z|n)P(a|z)P(n)." ></td>
	<td class="line x" title="85:189	We use the Expectation-Maximization (EM) algorithm (Dempster et al. , 1977) to estimate the parameters of the model." ></td>
	<td class="line x" title="86:189	According to the theory of the EM algorithm, we can increase the likelihood of the model with latent variables by iteratively increasing the Q-function." ></td>
	<td class="line x" title="87:189	The Q-function (i.e. , the expected log-likelihood of the joint probability of complete data with respect to the conditional posterior of the latent variable) is expressed as : Q() = summationdisplay nac fnac summationdisplay z P(z|nac)logP(nazc|), (3) where  denotes the set of the new parameters." ></td>
	<td class="line x" title="88:189	fnac denotes the frequency of a tuple n,a,c in the data." ></td>
	<td class="line x" title="89:189	P represents the posterior computed using the current parameters." ></td>
	<td class="line x" title="90:189	The E-step (expectation step) corresponds to simple posterior computation : P(z|nac) = P(z|n)P(a|z)P(c|az)summationtext z P(z|n)P(a|z)P(c|az)." ></td>
	<td class="line x" title="91:189	(4) For derivation of update rules in the M-step (maximization step), we use a simple Lagrange method for this optimization problem with constraints : z,summationtextn P(n|z) = 1, z,summationtexta P(a|z) = 1, and a,z,summationtextc P(c|az) = 1." ></td>
	<td class="line x" title="92:189	We obtain the following update rules : P(z|n) = summationtext ac fnac P(z|nac)summationtext ac fnac, (5) 203 P(y|z) = summationtext nc fnac P(z|nac)summationtext nac fnac P(z|nac), (6) P(c|az) = summationtext n fnac P(z|nac)summationtext nc fnac P(z|nac)." ></td>
	<td class="line x" title="93:189	(7) These steps are iteratively computed until convergence." ></td>
	<td class="line x" title="94:189	IfthedifferenceofthevaluesofQ-function before and after an iteration becomes smaller than a threshold, we regard it as converged." ></td>
	<td class="line x" title="95:189	For classification of an unknown pair n,a, we compare the values of P(c|na) = summationtext z P(z|n)P(a|z)P(c|az)summationtext cz P(z|n)P(a|z)P(c|az) ." ></td>
	<td class="line x" title="96:189	(8) Then the rating categorycthat maximizeP(c|na) is selected." ></td>
	<td class="line x" title="97:189	3.2 U-shaped Model We suppose that the conditional probability of c and z given n and a is expressed as : P(cz|na) = P(c|az)P(z|n)." ></td>
	<td class="line x" title="98:189	(9) We compute parameters above using the EM algorithm with the Q-function : Q() = summationdisplay nac fnac summationdisplay z P(z|nac)logP(cz|na,).(10) We obtain the following update rules : E step P(z|nac) = P(c|az)P(z|n)summationtext z P(c|az)P(z|n), (11) M step P(c|az) = summationtext n fnac P(z|nac)summationtext nc fnac P(z|nac), (12) P(z|n) = summationtext ac fnac P(z|nac)summationtext ac fnac ." ></td>
	<td class="line x" title="99:189	(13) For classification, we use the formula : P(c|na) = summationdisplay z P(c|az)P(z|n)." ></td>
	<td class="line x" title="100:189	(14) 3.3 Other Models for Comparison We will also test the 3-PLSI model corresponding to Figure 1-(c)." ></td>
	<td class="line x" title="101:189	In addition to the latent models, we test a baseline classifier, which uses the posterior probability : P(c|na)  P(n|c)P(a|c)P(c)." ></td>
	<td class="line x" title="102:189	(15) This baseline model is equivalent to the 2-term naivebayes classifier(Mitchell, 1997)." ></td>
	<td class="line x" title="103:189	Thegraphical representation of the naive bayes model is (b) in Figure 1." ></td>
	<td class="line x" title="104:189	The parameters are estimated as : P(n|c) = 1 + fnc|N| + f c, (16) P(a|c) = 1 + fac|A| + f c, (17) where |N| and |A| are the numbers of the words for n and a, respectively." ></td>
	<td class="line x" title="105:189	Thus, we have four different models : naive bayes (baseline), 3-PLSI, triangle, and U-shaped." ></td>
	<td class="line x" title="106:189	3.4 Discussions on the EM computation, the Models and the Task In the actual EM computation, we use the tempered EM (Hofmann, 2001) instead of the standard EM explained above, because the tempered EM can avoid an inaccurate estimation of the model caused by over-confidence in computing the posterior probabilities." ></td>
	<td class="line x" title="107:189	The tempered EM can be realized by a slight modification to the E-step, which results in a new E-step : P(z|nac) = parenleftbigP(c|az)P(z|n)parenrightbig summationtext z parenleftbigP(c|az)P(z|n)parenrightbig, (18) for the U-shaped model, where  is a positive hyper-parameter, called the inverse temperature." ></td>
	<td class="line x" title="108:189	The new E-steps for the other models are similarly expressed." ></td>
	<td class="line x" title="109:189	Now we have two hyper-parameters : inverse temperature , and the number of possible values M of latent variables." ></td>
	<td class="line x" title="110:189	We determine the values of these hyper-parameters by splitting the giventrainingdatasetintotwodatasets(thetemporary training dataset 90% and the held-out dataset 10%), and by obtaining the classification accuracy for the held-out dataset, which is yielded by the classifier with the temporary training dataset." ></td>
	<td class="line x" title="111:189	We should also note that Z (or any variable) should not have incoming arrows simultaneously from N and A, because the model with such arrows has P(z|na), which usually requires an excessively large memory." ></td>
	<td class="line x" title="112:189	To work with numerical scales of the rating variable (i.e. , the difference between c = 1 and c = 1 should be larger than that of c = 1 and c = 0), Hofmann (2004) used also a GaussiandistributionforP(c|az)incollaborativefiltering." ></td>
	<td class="line x" title="113:189	However, we do not employ a Gaussian, becauseinourdataset,thenumberofratingclassesis 204 only 3, which is so small that a Gaussian distribution cannot be a good approximation of the actual probability density function." ></td>
	<td class="line x" title="114:189	We conducted preliminary experiments with the model with Gaussians, but failed to obtain good results." ></td>
	<td class="line x" title="115:189	For other datasets with more classes, Gaussians might be a good model for P(c|az)." ></td>
	<td class="line x" title="116:189	The task we address in this paper is somewhat similar to the trigram prediction task, in the sense that both are classification tasks given two words." ></td>
	<td class="line x" title="117:189	However, we should note the difference between these two tasks." ></td>
	<td class="line x" title="118:189	In our task, the actual answer given two specific words are fixed as illustrated by the fact high+salary is always positive, while the answer for the trigram prediction task is randomly distributed." ></td>
	<td class="line x" title="119:189	We are therefore interested in thesemanticorientations ofunseen pairsof words, while the main purpose of the trigram prediction is accurately estimate the probability of (possibly seen) word sequences." ></td>
	<td class="line x" title="120:189	In the proposed models, only the words that appeared in the training dataset can be classified." ></td>
	<td class="line x" title="121:189	An attempt to deal with the unseen words is an interesting task." ></td>
	<td class="line x" title="122:189	For example, we could extend our models to semi-supervised models by regarding C as a partially observable variable." ></td>
	<td class="line x" title="123:189	We could also use distributional similarity of words (e.g. , based on window-size cooccurrence) to find an observed wordthatismostsimilartothegivenunseenword." ></td>
	<td class="line x" title="124:189	However, such methods would not work for the semantic orientation classification, because those methods are designed for simple cooccurrence and cannotdistinguishsurvival-ratefrominfectionrate." ></td>
	<td class="line x" title="125:189	In fact, the similarity-based method mentioned above failed to work efficiently in our preliminary experiments." ></td>
	<td class="line x" title="126:189	To solve the problem of unseen words, we would have to use other linguistic resources such as a thesaurus or a dictionary." ></td>
	<td class="line x" title="127:189	4 Experiments 4.1 Experimental Settings We extracted pairs of a noun (subject) and an adjective (predicate), from Mainichi newspaper articles (1995) written in Japanese, and annotated the pairs with semantic orientation tags : positive, neutral or negative." ></td>
	<td class="line x" title="128:189	We thus obtained the labeled dataset consisting of 12066 pair instances (7416 different pairs)." ></td>
	<td class="line x" title="129:189	The dataset contains 4459 negative instances, 4252 neutral instances, and 3355 positiveinstances." ></td>
	<td class="line x" title="130:189	Thenumberofdistinctnounsis 4770 and the number of distinct adjectives is 384." ></td>
	<td class="line x" title="131:189	To check the inter-annotator agreement between two annotators, we calculated  statistics, which was 0.640." ></td>
	<td class="line x" title="132:189	This value is allowable, but not quite high." ></td>
	<td class="line x" title="133:189	However, positive-negative disagreement is observedforonly0.7%ofthedata." ></td>
	<td class="line x" title="134:189	Inotherwords, thisstatisticsmeansthatthetaskofextractingneutral examples, which has hardly been explored, is intrinsically difficult." ></td>
	<td class="line x" title="135:189	We employ 10-fold cross-validation to obtain the average value of the classification accuracy." ></td>
	<td class="line x" title="136:189	We split the dataset such that there is no overlapping pair (i.e. , any pair in the training dataset does not appear in the test dataset)." ></td>
	<td class="line x" title="137:189	If either of the two words in a pair in the test dataset does not appear in the training dataset, we excluded the pair from the test dataset since the problem of unknown words is not in the scope of this research." ></td>
	<td class="line x" title="138:189	Therefore, we evaluate the pairs that are not in the training dataset, but whose component words appear in the training dataset." ></td>
	<td class="line x" title="139:189	In addition to the original dataset, which we call the standard dataset, we prepared another dataset inordertoexaminethepowerofthelatentvariable model." ></td>
	<td class="line x" title="140:189	The new dataset, which we call the hard dataset, consists only of examples with 17 difficult adjectives such as high, low, large, small, heavy, and light." ></td>
	<td class="line x" title="141:189	1 The semantic orientations of pairs including these difficult words often shift depending on the noun they modify." ></td>
	<td class="line x" title="142:189	Thus, the harddatasetisasubsetofthestandarddataset." ></td>
	<td class="line x" title="143:189	The size of the hard dataset is 4787." ></td>
	<td class="line x" title="144:189	Please note that the hard dataset is used only as a test dataset." ></td>
	<td class="line x" title="145:189	For training, we always use the standard dataset in our experiments." ></td>
	<td class="line x" title="146:189	We performed experiments with all the values of  in {0.1,0.2,,1.0} and with all the values of M in {10,30,50,70,100,200,300,500}, and predicted the best values of the hyper-parameters with the held-out method in Section 3.4." ></td>
	<td class="line x" title="147:189	4.2 Results The classification accuracies of the four methods with  and M predicted by the held-out method are shown in Table 1." ></td>
	<td class="line x" title="148:189	Please note that the naive bayes method is irrelevant of  and M. The table shows that the triangle model and the U-shaped 1The complete list of the 17 Japanese adjectives with their English counterparts are : takai (high), hikui (low), ookii (large), chiisai (small), omoi (heavy), karui (light), tsuyoi (strong), yowai (weak), ooi (many), sukunai (few/little), nai (no), sugoi (terrific), hageshii (terrific), hukai (deep), asai (shallow), nagai (long), mizikai (short)." ></td>
	<td class="line x" title="149:189	205 Table 1: Accuracies with predicted  and M standard hard accuracy  M accuracy  M Naive Bayes 73.40   65.93   3-PLSI 67.02 0.73 91.7 60.51 0.80 87.4 Triangle model 81.39 0.60 174.0 77.95 0.60 191.0 U-shaped model 81.94 0.64 60.0 75.86 0.65 48.3 model achieved high accuracies and outperformed the naive bayes method." ></td>
	<td class="line x" title="150:189	This result suggests that we succeeded in capturing the internal structure of semantically oriented phrases by way of latent variables." ></td>
	<td class="line x" title="151:189	The more complex structure of the triangle model resulted in the accuracy that is higher than that of the U-shaped model." ></td>
	<td class="line x" title="152:189	The performance of the 3-PLSI method is even worse than the baseline method." ></td>
	<td class="line x" title="153:189	This result shows thatweshoulduseamodelinwhichadjectivescan directly influence the rating category." ></td>
	<td class="line x" title="154:189	Figures 2, 3, 4 show cross-validated accuracy valuesforvariousvaluesof, respectivelyyielded by the 3-PLSI model, the triangle model and the U-shapedmodelwithdifferentnumbersM ofpossible states for the latent variable." ></td>
	<td class="line x" title="155:189	As the figures show, the classification performance is sensitive to thevalueof." ></td>
	<td class="line x" title="156:189	M = 100 andM = 300 aremostly better than M = 10." ></td>
	<td class="line x" title="157:189	However, this is a tradeoff between classification performance and training time, since large values of M demand heavy computation." ></td>
	<td class="line x" title="158:189	In that sense, the U-shaped model is useful in many practical situations, since it achieved a good accuracy even with a relatively small M. To observe the overall tendency of errors, we show the contingency table of classification by the U-shaped model with the predicted values of hyperparameters, in Table 2." ></td>
	<td class="line x" title="159:189	As this table shows, most of the errors are caused by the difficulty of classifying neutral examples." ></td>
	<td class="line x" title="160:189	Only 2.26% of the errors are mix-ups of the positive orientation and the negative orientation." ></td>
	<td class="line x" title="161:189	We next investigate the causes of errors by observing those mix-ups of the positive orientation and the negative orientation." ></td>
	<td class="line x" title="162:189	One type of frequent errors is illustrated by the pair food (s price) is high, in which the word price is omitted in the actual example 2." ></td>
	<td class="line x" title="163:189	As in this expression, the attribute (price, in this case) of an example is sometimes omitted or not correctly 2This kind of ellipsis often occurs in Japanese." ></td>
	<td class="line x" title="164:189	62 64 66 68 70 72 74 76 78 80 82 84 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy (%) beta M=300 M=100 M=10 Figure 2: 3-PLSI model with standard dataset 62 64 66 68 70 72 74 76 78 80 82 84 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy (%) beta M=300 M=100 M=10 Figure 3: Triangle model with standard dataset 62 64 66 68 70 72 74 76 78 80 82 84 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy (%) beta M=300 M=100 M=10 Figure 4: U-shaped model with standard dataset 206 Table 2: Contingency table of classification result by the U-shaped model U-shaped model positive neutral negative sum positive 1856 281 69 2206 Gold standard neutral 202 2021 394 2617 negative 102 321 2335 2758 sum 2160 2623 2798 7581 identified." ></td>
	<td class="line x" title="165:189	To tackle these examples, we will need methods for correctly identifying attributes and objects." ></td>
	<td class="line x" title="166:189	Some researchers are starting to work on this problem (e.g. , Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="167:189	We succeeded in addressing the data-sparseness problem by introducing a latent variable." ></td>
	<td class="line x" title="168:189	However, this problem still causes some errors." ></td>
	<td class="line x" title="169:189	Precise statistics cannot be obtained for infrequent words." ></td>
	<td class="line oc" title="170:189	This problem will be solved by incorporating other resources such as thesaurus or a dictionary,orcombiningourmethodwithothermethods using external wider contexts (Suzuki et al. , 2006; Turney, 2002; Baron and Hirst, 2004)." ></td>
	<td class="line x" title="171:189	4.3 Examples of Obtained Clusters Next, wequalitativelyevaluatetheproposedmethods." ></td>
	<td class="line x" title="172:189	For several clusters z, we extract the words that occur more than twice in the whole dataset and are in top 50 according to P(z|n)." ></td>
	<td class="line x" title="173:189	The model used here as an example is the U-shaped model." ></td>
	<td class="line x" title="174:189	The experimental settings are  = 0.6 and M = 60." ></td>
	<td class="line x" title="175:189	Although some elements of clusters are composed of multiple words in English, the original Japanese counterparts are single words." ></td>
	<td class="line x" title="176:189	Cluster 1 trouble, objection, disease, complaint, anxiety, anamnesis, relapse Cluster 2 risk, mortality, infection rate, onset rate Cluster 3 bond, opinion, love, meaning, longing, will Cluster 4 vote, application, topic, supporter Cluster 5 abuse, deterioration, shock, impact, burden Cluster 6 deterioration, discrimination, load, abuse Cluster 7 relative importance, degree of influence, number, weight, sense of belonging, wave, reputation These obtained clusters match our intuition." ></td>
	<td class="line x" title="177:189	For example, in cluster 2 are the nouns that are negative when combined with high, and positive when combined with low." ></td>
	<td class="line x" title="178:189	In fact, the posterior probabilities of semantic orientations for cluster 2 are as follows : P(negative|high,cluster 2) = 0.995, P(positive|low,cluster 2) = 0.973." ></td>
	<td class="line x" title="179:189	With conventional clustering methods based on the cooccurrence of two words, cluster 2 would include the words resulting in the opposite orientation, such as success rate." ></td>
	<td class="line x" title="180:189	We succeeded in obtaining the clusters that are suitable for our task, by incorporating the new variable c for semantic orientation in the EM computation." ></td>
	<td class="line x" title="181:189	5 Conclusion We proposed models for phrases with semantic orientations as well as a classification method based on the models." ></td>
	<td class="line x" title="182:189	We introduced a latent variable into the models to capture the properties of phrases." ></td>
	<td class="line x" title="183:189	Through experiments, we showed that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy." ></td>
	<td class="line x" title="184:189	We should also note that our method is language-independent although evaluation was on a Japanese dataset." ></td>
	<td class="line x" title="185:189	We plan next to adopt a semi-supervised learning method in order to correctly classify phrases with infrequent words, as mentioned in Section 4.2." ></td>
	<td class="line x" title="186:189	We would also like to extend our method to 3or more term phrases." ></td>
	<td class="line x" title="187:189	We can also use the obtained latent variables as features for another classifier, as Fujita et al.(2004) used latent variables of PLSI for the k-nearest neighbors method." ></td>
	<td class="line x" title="189:189	One important and promising task would be the use of semantic orientations of words for phrase level classification." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="J06-3003
Similarity of Semantic Relations
Turney, Peter D.;"></td>
	<td class="line x" title="1:729	Similarity of Semantic Relations Peter D. Turney  National Research Council Canada There are at least two kinds of similarity." ></td>
	<td class="line x" title="2:729	Relational similarity is correspondence between relations, in contrast with attributional similarity, which is correspondence between attributes." ></td>
	<td class="line x" title="3:729	When two words have a high degree of attributional similarity, we call them synonyms.When two pairs of words have a high degree of relational similarity, we say that their relations are analogous." ></td>
	<td class="line x" title="4:729	For example, the word pair mason:stone is analogous to the pair carpenter:wood." ></td>
	<td class="line x" title="5:729	This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity." ></td>
	<td class="line x" title="6:729	LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval." ></td>
	<td class="line x" title="7:729	Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions." ></td>
	<td class="line x" title="8:729	In the VSM approach, the relation between a pair of words is characterized by a vector of frequencies of predefined patterns in a large corpus." ></td>
	<td class="line x" title="9:729	LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs." ></td>
	<td class="line x" title="10:729	LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%." ></td>
	<td class="line x" title="11:729	On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM." ></td>
	<td class="line x" title="12:729	1." ></td>
	<td class="line x" title="13:729	Introduction There are at least two kinds of similarity." ></td>
	<td class="line x" title="14:729	Attributional similarity is correspondence between attributes and relational similarity is correspondence between relations (Medin, Goldstone, and Gentner 1990)." ></td>
	<td class="line x" title="15:729	When two words have a high degree of attributional similarity, we call them synonyms." ></td>
	<td class="line x" title="16:729	When two word pairs have a high degree of relational similarity, we say they are analogous." ></td>
	<td class="line x" title="17:729	Verbal analogies are often written in the form A:B::C:D, meaning AistoBasCisto D; for example, traffic:street::water:riverbed." ></td>
	<td class="line x" title="18:729	Traffic flows over a street; water flows over a riverbed." ></td>
	<td class="line x" title="19:729	A street carries traffic; a riverbed carries water." ></td>
	<td class="line x" title="20:729	There is a high degree of relational similarity between the word pair traffic:street and the word pair water:riverbed." ></td>
	<td class="line x" title="21:729	In fact, this analogy is the basis of several mathematical theories of traffic flow (Daganzo 1994)." ></td>
	<td class="line x" title="22:729	In Section 2, we look more closely at the connections between attributional and relational similarity." ></td>
	<td class="line x" title="23:729	In analogies such as mason:stone::carpenter:wood, it seems that  Institute for Information Technology, National Research Council Canada, M-50 Montreal Road, Ottawa, Ontario, Canada K1A 0R6." ></td>
	<td class="line x" title="24:729	E-mail: peter.turney@nrc-cnrc.gc.ca." ></td>
	<td class="line x" title="25:729	Submission received: 30 March 2005; revised submission received: 10 November 2005; accepted for publication: 27 February 2006." ></td>
	<td class="line x" title="26:729	 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood." ></td>
	<td class="line x" title="27:729	In general, this reduction fails." ></td>
	<td class="line x" title="28:729	Consider the analogy traffic:street::water:riverbed." ></td>
	<td class="line x" title="29:729	Traffic and water are not attributionally similar." ></td>
	<td class="line x" title="30:729	Street and riverbed are only moderately attributionally similar." ></td>
	<td class="line x" title="31:729	Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003)." ></td>
	<td class="line oc" title="32:729	Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986)." ></td>
	<td class="line x" title="33:729	On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known." ></td>
	<td class="line x" title="34:729	Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity." ></td>
	<td class="line x" title="35:729	We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3." ></td>
	<td class="line x" title="36:729	This article builds on the Vector Space Model (VSM) of information retrieval." ></td>
	<td class="line x" title="37:729	Given a query, a search engine produces a ranked list of documents." ></td>
	<td class="line x" title="38:729	The documents are ranked in order of decreasing attributional similarity between the query and each document." ></td>
	<td class="line x" title="39:729	Almost all modern search engines measure attributional similarity using the VSM (Baeza-Yates and Ribeiro-Neto 1999)." ></td>
	<td class="line x" title="40:729	Turney and Littman (2005) adapt the VSM approach to measuring relational similarity." ></td>
	<td class="line x" title="41:729	They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words." ></td>
	<td class="line x" title="42:729	Section 4 presents the VSM approach to measuring similarity." ></td>
	<td class="line x" title="43:729	In Section 5, we present an algorithm for measuring relational similarity, which we call Latent Relational Analysis (LRA)." ></td>
	<td class="line x" title="44:729	The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision." ></td>
	<td class="line x" title="45:729	LRA extends the VSM approach of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived automatically from the corpus, instead of using a fixed set of patterns." ></td>
	<td class="line x" title="46:729	(2) Singular Value Decomposition (SVD) is used to smooth the frequency data." ></td>
	<td class="line x" title="47:729	(3) Given a word pair such as traffic:street, LRA considers transformations of the word pair, generated by replacing one of the words by synonyms, such as traffic:road or traffic:highway." ></td>
	<td class="line x" title="48:729	Section 6 presents our experimental evaluation of LRA with a collection of 374 multiple-choice word analogy questions from the SAT college entrance exam." ></td>
	<td class="line x" title="49:729	1 An example of a typical SAT question appears in Table 1." ></td>
	<td class="line x" title="50:729	In the educational testing literature, the first pair (mason:stone) is called the stem of the analogy." ></td>
	<td class="line x" title="51:729	The correct choice is called the solution and the incorrect choices are distractors." ></td>
	<td class="line x" title="52:729	We evaluate LRA by testing its ability to select the solution and avoid the distractors." ></td>
	<td class="line x" title="53:729	The average performance of collegebound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%." ></td>
	<td class="line x" title="54:729	LRA achieves an accuracy of about 56%." ></td>
	<td class="line x" title="55:729	On these same questions, the VSM attained 47%." ></td>
	<td class="line x" title="56:729	1 The College Board eliminated analogies from the SAT in 2005, apparently because it was believed that analogy questions discriminate against minorities, although it has been argued by liberals (Goldenberg 2005) that dropping analogy questions has increased discrimination against minorities and by conservatives (Kurtz 2002) that it has decreased academic standards." ></td>
	<td class="line x" title="57:729	Analogy questions remain an important component in many other tests, such as the GRE." ></td>
	<td class="line x" title="58:729	380 Turney Similarity of Semantic Relations Table 1 An example of a typical SAT question, from the collection of 374 questions." ></td>
	<td class="line x" title="59:729	Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood One application for relational similarity is classifying semantic relations in nounmodifier pairs (Turney and Littman 2005)." ></td>
	<td class="line x" title="60:729	In Section 7, we evaluate the performance of LRA with a set of 600 noun-modifier pairs from Nastase and Szpakowicz (2003)." ></td>
	<td class="line x" title="61:729	The problem is to classify a noun-modifier pair, such as laser printer, according to the semantic relation between the head noun (printer) and the modifier (laser)." ></td>
	<td class="line x" title="62:729	The 600 pairs have been manually labeled with 30 classes of semantic relations." ></td>
	<td class="line x" title="63:729	For example, laser printer is classified as instrument; the printer uses the laser as an instrument for printing." ></td>
	<td class="line x" title="64:729	We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem." ></td>
	<td class="line x" title="65:729	The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its single nearest neighbor in the training set." ></td>
	<td class="line x" title="66:729	LRA is used to measure distance (i.e., similarity, nearness)." ></td>
	<td class="line x" title="67:729	LRA achieves an accuracy of 39.8% on the 30-class problem and 58.0% on the 5-class problem." ></td>
	<td class="line x" title="68:729	On the same 600 noun-modifier pairs, the VSM had accuracies of 27.8% (30-class) and 45.7% (5-class) (Turney and Littman 2005)." ></td>
	<td class="line x" title="69:729	We discuss the experimental results, limitations of LRA, and future work in Section 8 and we conclude in Section 9." ></td>
	<td class="line x" title="70:729	2. Attributional and Relational Similarity In this section, we explore connections between attributional and relational similarity." ></td>
	<td class="line x" title="71:729	2.1 Types of Similarity Medin, Goldstone, and Gentner (1990) distinguish attributes and relations as follows: Attributes are predicates taking one argument (e.g., X is red, X is large), whereas relations are predicates taking two or more arguments (e.g., X collides with Y, X is larger than Y)." ></td>
	<td class="line x" title="72:729	Attributes are used to state properties of objects; relations express relations between objects or propositions." ></td>
	<td class="line x" title="73:729	Gentner (1983) notes that what counts as an attribute or a relation can depend on the context." ></td>
	<td class="line x" title="74:729	For example, large can be viewed as an attribute of X, LARGE(X), or a relation between X and some standard Y, LARGER THAN(X, Y)." ></td>
	<td class="line x" title="75:729	The amount of attributional similarity between two words, A and B, depends on the degree of correspondence between the properties of A and B. A measure of attributional similarity is a function that maps two words, A and B, to a real number, 381 Computational Linguistics Volume 32, Number 3 sim a (A, B) Rfractur." ></td>
	<td class="line x" title="76:729	The more correspondence there is between the properties of A and B, the greater their attributional similarity." ></td>
	<td class="line x" title="77:729	For example, dog and wolf have a relatively high degree of attributional similarity." ></td>
	<td class="line x" title="78:729	The amount of relational similarity between two pairs of words, A:B and C:D, depends on the degree of correspondence between the relations between A and B and the relations between C and D. A measure of relational similarity is a function that maps two pairs, A:B and C:D, to a real number, sim r (A:B, C:D) Rfractur." ></td>
	<td class="line x" title="79:729	The more correspondence there is between the relations of A:B and C:D, the greater their relational similarity." ></td>
	<td class="line x" title="80:729	For example, dog:bark and cat:meow have a relatively high degree of relational similarity." ></td>
	<td class="line x" title="81:729	Cognitive scientists distinguish words that are semantically associated (beehoney) from words that are semantically similar (deerpony), although they recognize that some words are both associated and similar (doctornurse) (Chiarello et al. 1990)." ></td>
	<td class="line x" title="82:729	Both of these are types of attributional similarity, since they are based on correspondence between attributes (e.g., bees and honey are both found in hives; deer and ponies are both mammals)." ></td>
	<td class="line x" title="83:729	Budanitsky and Hirst (2001) describe semantic relatedness as follows: Recent research on the topic in computational linguistics has emphasized the perspective of semantic relatedness of two lexemes in a lexical resource, or its inverse, semantic distance." ></td>
	<td class="line x" title="84:729	Its important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by virtue of their likeness (banktrust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (carwheel) and antonymy (hotcold), or just by any kind of functional relationship or frequent association (pencilpaper, penguinAntarctica)." ></td>
	<td class="line x" title="85:729	As these examples show, semantic relatedness is the same as attributional similarity (e.g., hot and cold are both kinds of temperature, pencil and paper are both used for writing)." ></td>
	<td class="line x" title="86:729	Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity." ></td>
	<td class="line x" title="87:729	The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion." ></td>
	<td class="line x" title="88:729	Resnik (1995) describes semantic similarity as follows: Semantic similarity represents a special case of semantic relatedness: for example, cars and gasoline would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar." ></td>
	<td class="line x" title="89:729	Rada et al.(1989) suggest that the assessment of similarity in semantic networks can in fact be thought of as involving just taxonomic (IS-A) links, to the exclusion of other link types; that view will also be taken here, although admittedly it excludes some potentially useful information." ></td>
	<td class="line x" title="91:729	Thus semantic similarity is a specific type of attributional similarity." ></td>
	<td class="line x" title="92:729	The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity." ></td>
	<td class="line x" title="93:729	To avoid confusion, we will use the terms attributional similarity and relational similarity, following Medin, Goldstone, and Gentner (1990)." ></td>
	<td class="line x" title="94:729	Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity." ></td>
	<td class="line x" title="95:729	We interpret synonymy as a high degree of attributional similarity." ></td>
	<td class="line x" title="96:729	Analogy is a high degree of relational similarity." ></td>
	<td class="line x" title="97:729	382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003)." ></td>
	<td class="line x" title="98:729	Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus." ></td>
	<td class="line x" title="99:729	However, experiments do not support this intuition." ></td>
	<td class="line x" title="100:729	Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL)." ></td>
	<td class="line x" title="101:729	An example of one of the 80 TOEFL questions appears in Table 2." ></td>
	<td class="line x" title="102:729	Table 3 shows the best performance on the TOEFL questions for each type of attributional similarity algorithm." ></td>
	<td class="line x" title="103:729	The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy." ></td>
	<td class="line x" title="104:729	2.3 Using Attributional Similarity to Solve Analogies We may distinguish near analogies (mason:stone::carpenter:wood)fromfar analogies (traffic:street::water:riverbed) (Gentner 1983; Medin, Goldstone, and Gentner 1990)." ></td>
	<td class="line x" title="105:729	In an analogy A:B::C:D, where there is a high degree of relational similarity between A:B and C:D, if there is also a high degree of attributional similarity between A and C, and between B and D, then A:B::C:D is a near analogy; otherwise, it is a far analogy." ></td>
	<td class="line x" title="106:729	It seems possible that SAT analogy questions might consist largely of near analogies, in which case they can be solved using attributional similarity measures." ></td>
	<td class="line x" title="107:729	We could score each candidate analogy by the average of the attributional similarity, sim a , between A and C and between B and D: score(A:B::C:D) = 1 2 (sim a (A, C) + sim a (B, D)) (1) This kind of approach was used in two of the thirteen modules in Turney et al.(2003) (see Section 3.1)." ></td>
	<td class="line x" title="109:729	Table 2 An example of a typical TOEFL question, from the collection of 80 questions." ></td>
	<td class="line x" title="110:729	Stem: Levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed 383 Computational Linguistics Volume 32, Number 3 Table 3 Performance of attributional similarity measures on the 80 TOEFL questions." ></td>
	<td class="line x" title="111:729	(The average non-English US college applicants performance is included in the bottom row, for comparison.)" ></td>
	<td class="line x" title="112:729	Reference Description Percent correct Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75 Terra and Clarke (2003) Best corpus-based algorithm 81.25 Turney et al.(2003) Best hybrid algorithm 97.50 Landauer and Dumais (1997) Average human score 64.50 To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions." ></td>
	<td class="line x" title="114:729	The performance of the algorithms was measured by precision, recall, and F, defined as follows: precision = number of correct guesses total number of guesses made (2) recall = number of correct guesses maximum possible number correct (3) F = 2  precision  recall precision + recall (4) Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general)." ></td>
	<td class="line x" title="115:729	Table 4 shows the experimental results for our set of 374 analogy questions." ></td>
	<td class="line x" title="116:729	For example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered correctly, 224 incorrectly, and 30 questions were skipped." ></td>
	<td class="line x" title="117:729	When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped." ></td>
	<td class="line x" title="118:729	The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30)." ></td>
	<td class="line x" title="119:729	The first five algorithms in Table 4 are implemented in Pedersens WordNetSimilarity package." ></td>
	<td class="line x" title="120:729	2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003)." ></td>
	<td class="line x" title="121:729	The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="122:729	However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence." ></td>
	<td class="line x" title="123:729	We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity." ></td>
	<td class="line x" title="124:729	2 See http://www.d.umn.edu/tpederse/similarity.html." ></td>
	<td class="line x" title="125:729	384 Turney Similarity of Semantic Relations 3." ></td>
	<td class="line x" title="126:729	Related Work This section is a brief survey of the many problems that involve semantic relations and could potentially make use of an algorithm for measuring relational similarity." ></td>
	<td class="line x" title="127:729	3.1 Recognizing Word Analogies The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem." ></td>
	<td class="line x" title="128:729	This problem was first attempted by a system called Argus (Reitman 1965), using a small hand-built semantic network." ></td>
	<td class="line x" title="129:729	Argus could only solve the limited set of analogy questions that its programmer had anticipated." ></td>
	<td class="line x" title="130:729	Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity." ></td>
	<td class="line x" title="131:729	Turney et al.(2003) combined 13 independent modules to answer SAT questions." ></td>
	<td class="line x" title="133:729	The final output of the system was based on a weighted combination of the outputs of each individual module." ></td>
	<td class="line x" title="134:729	The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005)." ></td>
	<td class="line x" title="135:729	The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%." ></td>
	<td class="line x" title="136:729	In contrast with the corpus-based approach of Turney and Littman (2005), Veale (2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score of 43%." ></td>
	<td class="line x" title="137:729	Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths in WordNet, joining A to B and C to D. The quality measure was based on the similarity between the A:B paths and the C:D paths." ></td>
	<td class="line x" title="138:729	Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version of the VSM approach, which reached 56% on the 374 SAT questions." ></td>
	<td class="line x" title="139:729	Here we go beyond Turney (2005) by describing LRA in more detail, performing more extensive experiments, and analyzing the algorithm and related work in more depth." ></td>
	<td class="line x" title="140:729	3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making." ></td>
	<td class="line x" title="141:729	The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions." ></td>
	<td class="line x" title="142:729	Precision, recall, and F are reported as percentages." ></td>
	<td class="line x" title="143:729	(The bottom two rows are not attributional similarity measures." ></td>
	<td class="line x" title="144:729	They are included for comparison.)" ></td>
	<td class="line x" title="145:729	Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies." ></td>
	<td class="line x" title="146:729	SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target." ></td>
	<td class="line x" title="147:729	The domains are given structured propositional representations, using predicate logic." ></td>
	<td class="line x" title="148:729	These descriptions include attributes, relations, and higher-order relations (expressing relations between relations)." ></td>
	<td class="line x" title="149:729	The analogical mapping connects source domain relations to target domain relations." ></td>
	<td class="line x" title="150:729	For example, there is an analogy between the solar system and Rutherfords model of the atom (Falkenhainer, Forbus, and Gentner 1989)." ></td>
	<td class="line x" title="151:729	The solar system is the source domain and Rutherfords model of the atom is the target domain." ></td>
	<td class="line x" title="152:729	The basic objects in the source model are the planets and the sun." ></td>
	<td class="line x" title="153:729	The basic objects in the target model are the electrons and the nucleus." ></td>
	<td class="line x" title="154:729	The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet)." ></td>
	<td class="line x" title="155:729	Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron)." ></td>
	<td class="line x" title="156:729	SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron)." ></td>
	<td class="line x" title="157:729	Each individual connection (e.g., from revolve(planet, sun) to revolve(electron, nucleus)) in an analogical mapping implies that the connected relations are similar; thus, SMT requires a measure of relational similarity in order to form maps." ></td>
	<td class="line x" title="158:729	Early versions of SME only mapped identical relations, but later versions of SME allowed similar, nonidentical relations to match (Falkenhainer 1990)." ></td>
	<td class="line x" title="159:729	However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary." ></td>
	<td class="line x" title="160:729	We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME." ></td>
	<td class="line x" title="161:729	Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME." ></td>
	<td class="line x" title="162:729	3.3 Metaphor Metaphorical language is very common in our daily life, so common that we are usually unaware of it (Lakoff and Johnson 1980)." ></td>
	<td class="line x" title="163:729	Gentner et al.(2001) argue that novel metaphors are understood using analogy, but conventional metaphors are simply recalled from memory." ></td>
	<td class="line x" title="165:729	A conventional metaphor is a metaphor that has become entrenched in our language (Lakoff and Johnson 1980)." ></td>
	<td class="line x" title="166:729	Dolan (1995) describes an algorithm that can recognize conventional metaphors, but is not suited to novel metaphors." ></td>
	<td class="line x" title="167:729	This suggests that it may be fruitful to combine Dolans (1995) algorithm for handling conventional metaphorical language with LRA and SME for handling novel metaphors." ></td>
	<td class="line x" title="168:729	Lakoff and Johnson (1980) give many examples of sentences in support of their claim that metaphorical language is ubiquitous." ></td>
	<td class="line x" title="169:729	The metaphors in their sample sentences can be expressed using SAT-style verbal analogies of the form A:B::C:D.Thefirst column in Table 5 is a list of sentences from Lakoff and Johnson (1980) and the second column shows how the metaphor that is implicit in each sentence may be made explicit as a verbal analogy." ></td>
	<td class="line x" title="170:729	3.4 Classifying Semantic Relations The task of classifying semantic relations is to identify the relation between a pair of words." ></td>
	<td class="line x" title="171:729	Often the pairs are restricted to noun-modifier pairs, but there are many 386 Turney Similarity of Semantic Relations Table 5 Metaphorical sentences from Lakoff and Johnson (1980), rendered as SAT-style verbal analogies." ></td>
	<td class="line x" title="172:729	Metaphorical sentence SAT-style verbal analogy He shot down all of my arguments." ></td>
	<td class="line x" title="173:729	aircraft:shoot down::argument:refute I demolished his argument." ></td>
	<td class="line x" title="174:729	building:demolish::argument:refute You need to budget your time." ></td>
	<td class="line x" title="175:729	money:budget::time:schedule Ive invested a lot of time in her." ></td>
	<td class="line x" title="176:729	money:invest::time:allocate My mind just isnt operating today." ></td>
	<td class="line x" title="177:729	machine:operate::mind:think Life has cheated me. charlatan:cheat::life:disappoint Inflation is eating up our profits." ></td>
	<td class="line x" title="178:729	animal:eat::inflation:reduce interesting relations, such as antonymy, that do not occur in noun-modifier pairs." ></td>
	<td class="line x" title="179:729	However, noun-modifier pairs are interesting due to their high frequency in English." ></td>
	<td class="line x" title="180:729	For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms." ></td>
	<td class="line x" title="181:729	Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each noun-modifier pair with a feature vector." ></td>
	<td class="line x" title="182:729	They trained a neural network to distinguish 13 classes of semantic relations." ></td>
	<td class="line x" title="183:729	Nastase and Szpakowicz (2003) explore a similar approach to classifying general noun-modifier pairs (i.e., not restricted to a particular domain, such as medicine), using WordNet and Rogets Thesaurus as lexical resources." ></td>
	<td class="line x" title="184:729	Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs." ></td>
	<td class="line x" title="185:729	None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent." ></td>
	<td class="line x" title="186:729	Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used a measure of relational similarity, but their measure was based on literal matching, which limited its ability to generalize." ></td>
	<td class="line x" title="187:729	Moldovan et al.(2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet." ></td>
	<td class="line x" title="189:729	The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet." ></td>
	<td class="line x" title="190:729	Turney and Littman (2005) used the VSM (as a component in a single nearest neighbor learning algorithm) to measure relational similarity." ></td>
	<td class="line x" title="191:729	We take the same approach here, substituting LRA for the VSM, in Section 7." ></td>
	<td class="line x" title="192:729	Lauer (1995) used a corpus-based approach (using the BNC) to paraphrase noun modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about." ></td>
	<td class="line x" title="193:729	For example, reptile haven was paraphrased as haven for reptiles." ></td>
	<td class="line x" title="194:729	Lapata and Keller (2004) achieved improved results on this task by using the database of AltaVistas search engine as a corpus." ></td>
	<td class="line x" title="195:729	3.5 Word Sense Disambiguation We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text." ></td>
	<td class="line x" title="196:729	If we can identify the semantic 387 Computational Linguistics Volume 32, Number 3 relations between the given word and its context, then we can disambiguate the given word." ></td>
	<td class="line x" title="197:729	Yarowskys (1993) observation that collocations are almost always monosemous is evidence for this view." ></td>
	<td class="line x" title="198:729	Federici, Montemagni, and Pirrelli (1997) present an analogybased approach to word sense disambiguation." ></td>
	<td class="line x" title="199:729	For example, consider the word plant." ></td>
	<td class="line x" title="200:729	Out of context, plant could refer to an industrial plant or a living organism." ></td>
	<td class="line x" title="201:729	Suppose plant appears in some text near food.A typical approach to disambiguating plant would compare the attributional similarity of food and industrial plant to the attributional similarity of food and living organism (Lesk 1986; Banerjee and Pedersen 2003)." ></td>
	<td class="line x" title="202:729	In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food." ></td>
	<td class="line x" title="203:729	It would be very helpful to know the relation between food and plant in this example." ></td>
	<td class="line x" title="204:729	In the phrase food for the plant, the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food." ></td>
	<td class="line x" title="205:729	In the text food at the plant, the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations." ></td>
	<td class="line x" title="206:729	Thus, an algorithm for classifying semantic relations (as in Section 7) should be helpful for word sense disambiguation." ></td>
	<td class="line x" title="207:729	3.6 Information Extraction The problem of relation extraction is, given an input document and a specific relation R, to extract all pairs of entities (if any) that have the relation R in the document." ></td>
	<td class="line x" title="208:729	The problem was introduced as part of the Message Understanding Conferences (MUC) in 1998." ></td>
	<td class="line x" title="209:729	Zelenko, Aone, and Richardella (2003) present a kernel method for extracting the relations personaffiliation and organizationlocation." ></td>
	<td class="line x" title="210:729	For example, in the sentence John Smith is the chief scientist of the Hardcom Corporation, there is a personaffiliation relation between John Smith and Hardcom Corporation (Zelenko, Aone, and Richardella 2003)." ></td>
	<td class="line x" title="211:729	This is similar to the problem of classifying semantic relations (Section 3.4), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text." ></td>
	<td class="line x" title="212:729	Therefore an algorithm for classifying semantic relations should be useful for information extraction." ></td>
	<td class="line x" title="213:729	In the VSM approach to classifying semantic relations (Turney and Littman 2005), we would have a training set of labeled examples of the relation personaffiliation, for instance." ></td>
	<td class="line x" title="214:729	Each example would be represented by a vector of pattern frequencies." ></td>
	<td class="line x" title="215:729	Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then measure the relational similarity between this unlabeled vector and each of our labeled training vectors." ></td>
	<td class="line x" title="216:729	It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, but the new unlabeled vector for John Smith and Hardcom Corporation would be very sparse, since these entities might be mentioned only once in the given document." ></td>
	<td class="line x" title="217:729	However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval." ></td>
	<td class="line x" title="218:729	A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector." ></td>
	<td class="line x" title="219:729	There are well-known techniques in information retrieval for coping with this disparity, such as weighting schemes for query vectors that are different from the weighting schemes for document vectors (Salton and Buckley 1988)." ></td>
	<td class="line x" title="220:729	388 Turney Similarity of Semantic Relations 3.7 Question Answering In their article on classifying semantic relations, Moldovan et al.(2004) suggest that an important application of their work is question answering (QA)." ></td>
	<td class="line x" title="222:729	As defined in the Text Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as Where have nuclear incidents occurred?, by retrieving a relevant document from a large corpus and then extracting a short string from the document, such as The Three Mile Island nuclear incident caused a DOE policy crisis." ></td>
	<td class="line x" title="223:729	Moldovan et al.(2004) propose to map a given question to a semantic relation and then search for that relation in a corpus of semantically tagged text." ></td>
	<td class="line x" title="225:729	They argue that the desired semantic relation can easily be inferred from the surface form of the question." ></td>
	<td class="line x" title="226:729	A question of the form Where? is likely to be looking for entities with a location relation and a question of the form What did  make? is likely to be looking for entities with a product relation." ></td>
	<td class="line x" title="227:729	In Section 7, we show how LRA can recognize relations such as location and product (see Table 19)." ></td>
	<td class="line x" title="228:729	3.8 Automatic Thesaurus Generation Hearst (1992) presents an algorithm for learning hyponym (type of ) relations from a corpus and Berland and Charniak (1999) describe how to learn meronym (part of ) relations from a corpus." ></td>
	<td class="line x" title="229:729	These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy." ></td>
	<td class="line x" title="230:729	WordNet distinguishes more than a dozen semantic relations between words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for noun-modifier pairs." ></td>
	<td class="line x" title="231:729	Hearst and Berland and Charniak (1999) use manually generated rules to mine text for semantic relations." ></td>
	<td class="line x" title="232:729	Turney and Littman (2005) also use a manually generated set of 64 patterns." ></td>
	<td class="line x" title="233:729	LRA does not use a predefined set of patterns; it learns patterns from a large corpus." ></td>
	<td class="line x" title="234:729	Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations." ></td>
	<td class="line x" title="235:729	A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data." ></td>
	<td class="line x" title="236:729	Girju, Badulescu, and Moldovan (2003) present an algorithm for learning meronym relations from a corpus." ></td>
	<td class="line x" title="237:729	Like Hearst (1992) and Berland and Charniak (1999), they use manually generated rules to mine text for their desired relation." ></td>
	<td class="line x" title="238:729	However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules." ></td>
	<td class="line x" title="239:729	3.9 Information Retrieval Veale (2003) has developed an algorithm for recognizing certain types of word analogies, based on information in WordNet." ></td>
	<td class="line x" title="240:729	He proposes to use the algorithm for analogical information retrieval." ></td>
	<td class="line x" title="241:729	For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas." ></td>
	<td class="line x" title="242:729	The algorithm was designed with a focus on analogies of the form adjective:noun::adjective:noun, such as Christian:church::Muslim:mosque." ></td>
	<td class="line x" title="243:729	A measure of relational similarity is applicable to this task." ></td>
	<td class="line x" title="244:729	Given a pair of words, A and B, the task is to return another pair of words, X and Y, such that there is high relational similarity between the pair A:X and the pair Y:B. For example, given 389 Computational Linguistics Volume 32, Number 3 A = Muslim and B = church,returnX = mosque and Y = Christian." ></td>
	<td class="line x" title="245:729	(The pair Muslim:mosque has a high relational similarity to the pair Christian:church.)" ></td>
	<td class="line x" title="246:729	Marx et al.(2002) developed an unsupervised algorithm for discovering analogies by clustering words from two different corpora." ></td>
	<td class="line x" title="248:729	Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus." ></td>
	<td class="line x" title="249:729	For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents." ></td>
	<td class="line x" title="250:729	A cluster of words such as {Hindu, Mahayana, Zen, } from the Buddhist corpus was coupled with a cluster of words such as {Catholic, Protestant, } from the Christian corpus." ></td>
	<td class="line x" title="251:729	Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions." ></td>
	<td class="line x" title="252:729	This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words." ></td>
	<td class="line x" title="253:729	3.10 Identifying Semantic Roles A semantic frame for an event such as judgement contains semantic roles such as judge, evaluee,andreason, whereas an event such as statement contains roles such as speaker, addressee,andmessage (Gildea and Jurafsky 2002)." ></td>
	<td class="line x" title="254:729	The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles." ></td>
	<td class="line x" title="255:729	We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic roles." ></td>
	<td class="line x" title="256:729	Moldovan et al.(2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech." ></td>
	<td class="line x" title="258:729	4." ></td>
	<td class="line x" title="259:729	The Vector Space Model This section examines past work on measuring attributional and relational similarity using the VSM." ></td>
	<td class="line x" title="260:729	4.1 Measuring Attributional Similarity with the Vector Space Model The VSM was first developed for information retrieval (Salton and McGill 1983; Salton and Buckley 1988; Salton 1989) and it is at the core of most modern search engines (Baeza-Yates and Ribeiro-Neto 1999)." ></td>
	<td class="line x" title="261:729	In the VSM approach to information retrieval, queries and documents are represented by vectors." ></td>
	<td class="line x" title="262:729	Elements in these vectors are based on the frequencies of words in the corresponding queries and documents." ></td>
	<td class="line x" title="263:729	The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989)." ></td>
	<td class="line x" title="264:729	The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors." ></td>
	<td class="line x" title="265:729	For a given query, the search engine sorts the matching documents in order of decreasing cosine." ></td>
	<td class="line x" title="266:729	The VSM approach has also been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002)." ></td>
	<td class="line x" title="267:729	Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM." ></td>
	<td class="line x" title="268:729	Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning." ></td>
	<td class="line x" title="269:729	Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997)." ></td>
	<td class="line x" title="270:729	SVD improves both document-query attributional similarity measures (Deerwester et al. 1990; Dumais 1993) and wordword attributional similarity measures (Landauer and Dumais 1997)." ></td>
	<td class="line x" title="271:729	LRA also uses SVD to smooth vectors, as we discuss in Section 5." ></td>
	<td class="line x" title="272:729	4.2 Measuring Relational Similarity with the Vector Space Model Let R 1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R 2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R 1 and R 2 .TherelationsR 1 and R 2 are not given to us; our task is to infer these hidden (latent) relations and then compare them." ></td>
	<td class="line x" title="273:729	In the VSM approach to relational similarity (Turney and Littman 2005), we create vectors, r 1 and r 2 , that represent features of R 1 and R 2 , and then measure the similarity of R 1 and R 2 by the cosine of the angle  between r 1 and r 2 : r 1 =r 1,1 ,, r 1,n  (5) r 2 =r 2,1 ,r 2,n  (6) cosine() = n summationtext i=1 r 1,i  r 2,i radicalBigg n summationtext i=1 (r 1,i ) 2  n summationtext i=1 (r 2,i ) 2 = r 1 r 2  r 1 r 1   r 2 r 2 = r 1 r 2 bardblr 1 bardblbardblr 2 bardbl (7) We create a vector, r, to characterize the relationship between two words, X and Y, by counting the frequencies of various short phrases containing X and Y. Turney and Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that contain X and Y, such as XofY, YofX, XforY, YforX, XtoY,andYtoX." ></td>
	<td class="line x" title="274:729	These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query." ></td>
	<td class="line x" title="275:729	This process yields a vector of 128 numbers." ></td>
	<td class="line x" title="276:729	If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1)." ></td>
	<td class="line x" title="277:729	Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin 1998b)." ></td>
	<td class="line x" title="278:729	Turney and Littman (2005) evaluated the VSM approach by its performance on 374 SAT analogy questions, achieving a score of 47%." ></td>
	<td class="line x" title="279:729	Since there are five choices for each question, the expected score for random guessing is 20%." ></td>
	<td class="line x" title="280:729	To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair." ></td>
	<td class="line x" title="281:729	The best guess is the choice pair with the highest cosine." ></td>
	<td class="line x" title="282:729	We use the same set of analogy questions to evaluate LRA in Section 6." ></td>
	<td class="line x" title="283:729	391 Computational Linguistics Volume 32, Number 3 The VSM was also evaluated by its performance as a distance (nearness) measure in a supervised nearest neighbor classifier for noun-modifier semantic relations (Turney and Littman 2005)." ></td>
	<td class="line x" title="284:729	The evaluation used 600 hand-labeled noun-modifier pairs from Nastase and Szpakowicz (2003)." ></td>
	<td class="line x" title="285:729	A testing pair is classified by searching for its single nearest neighbor in the labeled training data." ></td>
	<td class="line x" title="286:729	The best guess is the label for the training pair with the highest cosine." ></td>
	<td class="line x" title="287:729	LRA is evaluated with the same set of noun-modifier pairs in Section 7." ></td>
	<td class="line x" title="288:729	Turney and Littman (2005) used the AltaVista search engine to obtain the frequency information required to build vectors for the VSM." ></td>
	<td class="line x" title="289:729	Thus their corpus was the set of all Web pages indexed by AltaVista." ></td>
	<td class="line x" title="290:729	At the time, the English subset of this corpus consisted of about 5  10 11 words." ></td>
	<td class="line x" title="291:729	Around April 2004, AltaVista made substantial changes to their search engine, removing their advanced search operators." ></td>
	<td class="line x" title="292:729	Their search engine no longer supports the asterisk operator, which was used by Turney and Littman (2005) for stemming and wild-card searching." ></td>
	<td class="line x" title="293:729	AltaVista also changed their policy toward automated searching, which is now forbidden." ></td>
	<td class="line x" title="294:729	3 Turney and Littman (2005) used AltaVistas hit count, which is the number of documents (Web pages) matching a given query, but LRA uses the number of passages (strings) matching a query." ></td>
	<td class="line x" title="295:729	In our experiments with LRA (Sections 6 and 7), we use a local copy of the Waterloo MultiText System (WMTS) (Clarke, Cormack, and Palmer 1998; Terra and Clarke 2003), running on a 16 CPU Beowulf Cluster, with a corpus of about 5  10 10 English words." ></td>
	<td class="line x" title="296:729	The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval)." ></td>
	<td class="line x" title="297:729	The text and index require approximately one terabyte of disk space." ></td>
	<td class="line x" title="298:729	Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages." ></td>
	<td class="line x" title="299:729	Turney et al.(2003) combine 13 independent modules to answer SAT questions." ></td>
	<td class="line x" title="301:729	The performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module." ></td>
	<td class="line x" title="302:729	Since the VSM module had the best performance of the 13 modules (Turney et al. 2003), the following experiments focus on comparing VSM and LRA." ></td>
	<td class="line x" title="303:729	5." ></td>
	<td class="line x" title="304:729	Latent Relational Analysis LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs." ></td>
	<td class="line x" title="305:729	LRA relies on three resources, a search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms, and an efficient implementation of SVD." ></td>
	<td class="line x" title="306:729	We first present a short description of the core algorithm." ></td>
	<td class="line x" title="307:729	Later, in the following subsections, we will give a detailed description of the algorithm, as it is applied in the experiments in Sections 6 and 7." ></td>
	<td class="line x" title="308:729	a114 Given a set of word pairs as input, look in a thesaurus for synonyms for each word in each word pair." ></td>
	<td class="line x" title="309:729	For each input pair, make alternate pairs by replacing the original words with their synonyms." ></td>
	<td class="line x" title="310:729	The alternate pairs are 3 See http://www.altavista.com/robots.txt for AltaVistas current policy toward robots (software for automatically gathering Web pages or issuing batches of queries)." ></td>
	<td class="line x" title="311:729	The protocol of the robots.txt file is explained in http://www.robotstxt.org/wc/robots.html." ></td>
	<td class="line x" title="312:729	392 Turney Similarity of Semantic Relations intended to form near analogies with the corresponding original pairs (see Section 2.3)." ></td>
	<td class="line x" title="313:729	a114 Filter out alternate pairs that do not form near analogies by dropping alternate pairs that co-occur rarely in the corpus." ></td>
	<td class="line x" title="314:729	In the preceding step, if a synonym replaced an ambiguous original word, but the synonym captures the wrong sense of the original word, it is likely that there is no significant relation between the words in the alternate pair, so they will rarely co-occur." ></td>
	<td class="line x" title="315:729	a114 For each original and alternate pair, search in the corpus for short phrases that begin with one member of the pair and end with the other." ></td>
	<td class="line x" title="316:729	These phrases characterize the relation between the words in each pair." ></td>
	<td class="line x" title="317:729	a114 For each phrase from the previous step, create several patterns, by replacing words in the phrase with wild cards." ></td>
	<td class="line x" title="318:729	a114 Build a pairpattern frequency matrix, in which each cell represents the number of times that the corresponding pair (row) appears in the corpus with the corresponding pattern (column)." ></td>
	<td class="line x" title="319:729	The number will usually be zero, resulting in a sparse matrix." ></td>
	<td class="line x" title="320:729	a114 Apply the SVD to the matrix." ></td>
	<td class="line x" title="321:729	This reduces noise in the matrix and helps with sparse data." ></td>
	<td class="line x" title="322:729	a114 Suppose that we wish to calculate the relational similarity between any two of the original pairs." ></td>
	<td class="line x" title="323:729	Start by looking for the two row vectors in the pairpattern frequency matrix that correspond to the two original pairs." ></td>
	<td class="line x" title="324:729	Calculate the cosine of the angle between these two row vectors." ></td>
	<td class="line x" title="325:729	Then merge the cosine of the two original pairs with the cosines of their corresponding alternate pairs, as follows." ></td>
	<td class="line x" title="326:729	If an analogy formed with alternate pairs has a higher cosine than the original pairs, we assume that we have found a better way to express the analogy, but we have not significantly changed its meaning." ></td>
	<td class="line x" title="327:729	If the cosine is lower, we assume that we may have changed the meaning by inappropriately replacing words with synonyms." ></td>
	<td class="line x" title="328:729	Filter out inappropriate alternates by dropping all analogies formed of alternates, such that the cosines are less than the cosine for the original pairs." ></td>
	<td class="line x" title="329:729	The relational similarity between the two original pairs is then calculated as the average of all of the remaining cosines." ></td>
	<td class="line x" title="330:729	The motivation for the alternate pairs is to handle cases where the original pairs cooccur rarely in the corpus." ></td>
	<td class="line x" title="331:729	The hope is that we can find near analogies for the original pairs, such that the near analogies co-occur more frequently in the corpus." ></td>
	<td class="line x" title="332:729	The danger is that the alternates may have different relations from the originals." ></td>
	<td class="line x" title="333:729	The filtering steps above aim to reduce this risk." ></td>
	<td class="line x" title="334:729	5.1 Input and Output In our experiments, the input set contains from 600 to 2,244 word pairs." ></td>
	<td class="line x" title="335:729	The output similarity measure is based on cosines, so the degree of similarity can range from 1 (dissimilar;  = 180  )to+1 (similar;  = 0  )." ></td>
	<td class="line x" title="336:729	Before applying SVD, the vectors are 393 Computational Linguistics Volume 32, Number 3 completely non-negative, which implies that the cosine can only range from 0 to+1, but SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments." ></td>
	<td class="line x" title="337:729	5.2 Search Engine and Corpus In the following experiments, we use a local copy of the WMTS (Clarke, Cormack, and Palmer 1998; Terra and Clarke 2003)." ></td>
	<td class="line x" title="338:729	4 The corpus consists of about 5  10 10 English words, gathered by a Web crawler, mainly from US academic Web sites." ></td>
	<td class="line x" title="339:729	The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill." ></td>
	<td class="line x" title="340:729	The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax." ></td>
	<td class="line x" title="341:729	5.3 Thesaurus As a source of synonyms, we use Lins (1998a) automatically generated thesaurus." ></td>
	<td class="line x" title="342:729	This thesaurus is available through an on-line interactive demonstration or it can be downloaded." ></td>
	<td class="line x" title="343:729	5 We used the on-line demonstration, since the downloadable version seems to contain fewer words." ></td>
	<td class="line x" title="344:729	For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms." ></td>
	<td class="line x" title="345:729	As a courtesy to other users of Lins on-line system, we insert a 20-second delay between each two queries." ></td>
	<td class="line x" title="346:729	Lins thesaurus was generated by parsing a corpus of about 5  10 7 English words, consisting of text from the Wall Street Journal, San Jose Mercury,andAP Newswire (Lin 1998a)." ></td>
	<td class="line x" title="347:729	The parser was used to extract pairs of words and their grammatical relations." ></td>
	<td class="line x" title="348:729	Words were then clustered into synonym sets, based on the similarity of their grammatical relations." ></td>
	<td class="line x" title="349:729	Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words." ></td>
	<td class="line x" title="350:729	Given a word and its part of speech, Lins thesaurus provides a list of words, sorted in order of decreasing attributional similarity." ></td>
	<td class="line x" title="351:729	This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest." ></td>
	<td class="line x" title="352:729	WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses." ></td>
	<td class="line x" title="353:729	WordNets sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003)." ></td>
	<td class="line x" title="354:729	5.4 Singular Value Decomposition We use Rohdes SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992)." ></td>
	<td class="line x" title="355:729	6 In LRA, SVD is used to reduce noise and compensate for sparseness." ></td>
	<td class="line x" title="356:729	4 See http://multitext.uwaterloo.ca/." ></td>
	<td class="line x" title="357:729	5 The online demonstration is at http://www.cs.ualberta.ca/lindek/demos/depsim.htm and the downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz." ></td>
	<td class="line x" title="358:729	6 SVDLIBC is available at http://tedlab.mit.edu/dr/SVDLIBC/ and SVDPACKC is available at http://www.netlib.org/svdpack/." ></td>
	<td class="line x" title="359:729	394 Turney Similarity of Semantic Relations 5.5 The Algorithm We will go through each step of LRA, using an example to illustrate the steps." ></td>
	<td class="line x" title="360:729	Assume that the input to LRA is the 374 multiple-choice SAT word analogy questions of Turney and Littman (2005)." ></td>
	<td class="line x" title="361:729	Since there are six word pairs per question (the stem and five choices), the input consists of 2,244 word pairs." ></td>
	<td class="line x" title="362:729	Lets suppose that we wish to calculate the relational similarity between the pair quart:volume and the pair mile:distance, taken from the SAT question in Table 6." ></td>
	<td class="line x" title="363:729	The LRA algorithm consists of the following 12 steps: 1." ></td>
	<td class="line x" title="364:729	Find alternates: For each word pair A:B in the input set, look in Lins (1998a) thesaurus for the top num sim words (in the following experiments, num sim is 10) that are most similar to A. For each A prime that is similar to A, make a new word pair A prime :B. Likewise, look for the top num sim words that are most similar to B, and for each B prime , make a new word pair A:B prime . A:B is called the original pair and each A prime :B or A:B prime is an alternate pair." ></td>
	<td class="line x" title="365:729	The intent is that alternates should have almost the same semantic relations as the original." ></td>
	<td class="line x" title="366:729	For each input pair, there will now be 2  num sim alternate pairs." ></td>
	<td class="line x" title="367:729	When looking for similar words in Lins (1998a) thesaurus, avoid words that seem unusual (e.g., hyphenated words, words with three characters or less, words with non-alphabetical characters, multiword phrases, and capitalized words)." ></td>
	<td class="line x" title="368:729	The first column in Table 7 shows the alternate pairs that are generated for the original pair quart:volume." ></td>
	<td class="line x" title="369:729	2." ></td>
	<td class="line x" title="370:729	Filter alternates: For each original pair A:B,filterthe2 num sim alternates as follows." ></td>
	<td class="line x" title="371:729	For each alternate pair, send a query to the WMTS, to find the frequency of phrases that begin with one member of the pair and end with the other." ></td>
	<td class="line x" title="372:729	The phrases cannot have more than max phrase words (we use max phrase = 5)." ></td>
	<td class="line x" title="373:729	Sort the alternate pairs by the frequency of their phrases." ></td>
	<td class="line x" title="374:729	Select the top num filter most frequent alternates and discard the remainder (we use num filter = 3, so 17 alternates are dropped)." ></td>
	<td class="line x" title="375:729	This step tends to eliminate alternates that have no clear semantic relation." ></td>
	<td class="line x" title="376:729	The third column in Table 7 shows the frequency with which each pair co-occurs in a window of max phrase words." ></td>
	<td class="line x" title="377:729	The last column in Table 7 shows the pairs that are selected." ></td>
	<td class="line x" title="378:729	3." ></td>
	<td class="line x" title="379:729	Find phrases: For each pair (originals and alternates), make a list of phrases in the corpus that contain the pair." ></td>
	<td class="line x" title="380:729	Query the WMTS for all phrases that begin with one member of the pair and end with the other (in either order)." ></td>
	<td class="line x" title="381:729	We ignore suffixes when searching for phrases that match Table 6 This SAT question, from Claman (2000), is used to illustrate the steps in the LRA algorithm." ></td>
	<td class="line x" title="382:729	Stem: quart:volume Choices: (a) day:night (b) mile:distance (c) decade:century (d) friction:heat (e) part:whole Solution: (b) mile:distance 395 Computational Linguistics Volume 32, Number 3 Table 7 Alternate forms of the original pair quart:volume." ></td>
	<td class="line x" title="383:729	The first column shows the original pair and the alternate pairs." ></td>
	<td class="line x" title="384:729	The second column shows Lins similarity score for the alternate word compared to the original word." ></td>
	<td class="line x" title="385:729	For example, the similarity between quart and pint is 0.210." ></td>
	<td class="line x" title="386:729	The third column shows the frequency of the pair in the WMTS corpus." ></td>
	<td class="line x" title="387:729	The fourth column shows the pairs that pass the filtering step (i.e., step 2)." ></td>
	<td class="line x" title="388:729	Word pair Similarity Frequency Filtering step quart:volume NA 632 Accept (original pair) pint:volume 0.210 372 gallon:volume 0.159 1500 Accept (top alternate) liter:volume 0.122 3323 Accept (top alternate) squirt:volume 0.084 54 pail:volume 0.084 28 vial:volume 0.084 373 pumping:volume 0.073 1386 Accept (top alternate) ounce:volume 0.071 430 spoonful:volume 0.070 42 tablespoon:volume 0.069 96 quart:turnover 0.229 0 quart:output 0.225 34 quart:export 0.206 7 quart:value 0.203 266 quart:import 0.186 16 quart:revenue 0.185 0 quart:sale 0.169 119 quart:investment 0.161 11 quart:earnings 0.156 0 quart:profit 0.156 24 a given pair." ></td>
	<td class="line x" title="389:729	The phrases cannot have more than max phrase words and there must be at least one word between the two members of the word pair." ></td>
	<td class="line x" title="390:729	These phrases give us information about the semantic relations between the words in each pair." ></td>
	<td class="line x" title="391:729	A phrase with no words between the two members of the word pair would give us very little information about the semantic relations (other than that the words occur together with a certain frequency in a certain order)." ></td>
	<td class="line x" title="392:729	Table 8 gives some examples of phrases in the corpus that match the pair quart:volume." ></td>
	<td class="line x" title="393:729	4." ></td>
	<td class="line x" title="394:729	Find patterns: For each phrase found in the previous step, build patterns from the intervening words." ></td>
	<td class="line x" title="395:729	A pattern is constructed by replacing any or all or none of the intervening words with wild cards (one wild card can Table 8 Some examples of phrases that contain quart:volume." ></td>
	<td class="line x" title="396:729	Suffixes are ignored when searching for matching phrases in the WMTS corpus." ></td>
	<td class="line x" title="397:729	At least one word must occur between quart and volume." ></td>
	<td class="line x" title="398:729	At most max phrase words can appear in a phrase." ></td>
	<td class="line x" title="399:729	quarts liquid volume volume in quarts quarts of volume volume capacity quarts quarts in volume volume being about two quarts quart total volume volume of milk in quarts quart of spray volume volume include measures like quart 396 Turney Similarity of Semantic Relations replace only one word)." ></td>
	<td class="line x" title="400:729	If a phrase is n words long, there are n  2 intervening words between the members of the given word pair (e.g., between quart and volume)." ></td>
	<td class="line x" title="401:729	Thus a phrase with n words generates 2 (n2) patterns." ></td>
	<td class="line x" title="402:729	(We use max phrase = 5, so a phrase generates at most eight patterns.)" ></td>
	<td class="line x" title="403:729	For each pattern, count the number of pairs (originals and alternates) with phrases that match the pattern (a wild card must match exactly one word)." ></td>
	<td class="line x" title="404:729	Keep the top num patterns most frequent patterns and discard the rest (we use num patterns = 4, 000)." ></td>
	<td class="line x" title="405:729	Typically there will be millions of patterns, so it is not feasible to keep them all." ></td>
	<td class="line x" title="406:729	5." ></td>
	<td class="line x" title="407:729	Map pairs to rows: In preparation for building the matrix X,createa mapping of word pairs to row numbers." ></td>
	<td class="line x" title="408:729	For each pair A:B, create a row for A:B and another row for B:A. This will make the matrix more symmetrical, reflecting our knowledge that the relational similarity between A:B and C:D should be the same as the relational similarity between B:A and D:C. This duplication of rows is examined in Section 6.6." ></td>
	<td class="line x" title="409:729	6. Map patterns to columns: Create a mapping of the top num patterns patterns to column numbers." ></td>
	<td class="line x" title="410:729	For each pattern P, create a column for word 1 Pword 2  and another column for word 2 Pword 1 . Thus there will be 2  num patterns columns in X. This duplication of columns is examined in Section 6.6." ></td>
	<td class="line x" title="411:729	7. Generate a sparse matrix: Generate a matrix X in sparse matrix format, suitable for input to SVDLIBC." ></td>
	<td class="line x" title="412:729	The value for the cell in row i and column j is the frequency of the jth pattern (see step 6) in phrases that contain the ith word pair (see step 5)." ></td>
	<td class="line x" title="413:729	Table 9 gives some examples of pattern frequencies for quart:volume." ></td>
	<td class="line x" title="414:729	8." ></td>
	<td class="line x" title="415:729	Calculate entropy: Apply log and entropy transformations to the sparse matrix (Landauer and Dumais 1997)." ></td>
	<td class="line x" title="416:729	These transformations have been found to be very helpful for information retrieval (Harman 1986; Dumais 1990)." ></td>
	<td class="line x" title="417:729	Let x i,j bethecellinrowi and column j of the matrix X from step 7." ></td>
	<td class="line x" title="418:729	Let m be the number of rows in X and let n be the number of columns." ></td>
	<td class="line x" title="419:729	We wish to weight the cell x i,j by the entropy of the jth column." ></td>
	<td class="line x" title="420:729	To calculate the entropy of the column, we need to convert the column into a vector of probabilities." ></td>
	<td class="line x" title="421:729	Let p i,j be the probability of x i,j , calculated by normalizing the column vector so that the sum of the elements is one, p i,j = x i,j / summationtext m k=1 x k,j . The entropy of the jth column is then H j = summationtext m k=1 p k,j log(p k,j )." ></td>
	<td class="line x" title="422:729	Entropy is at its maximum when p i,j is a uniform distribution, p i,j = 1/m, in which case H j = log(m)." ></td>
	<td class="line x" title="423:729	Entropy is at its minimum when p i,j is 1 for some value of i and 0 for all other values of i, in which case H j = 0." ></td>
	<td class="line x" title="424:729	We want to give Table 9 Frequencies of various patterns for quart:volume." ></td>
	<td class="line x" title="425:729	The asterisk * represents the wildcard." ></td>
	<td class="line x" title="426:729	Suffixes are ignored, so quart matches quarts." ></td>
	<td class="line x" title="427:729	For example, quarts in volume is one of the four phrases that match quart P volume when P is in." ></td>
	<td class="line x" title="428:729	P =in P =*of P =of* P =** freq(quart P volume) 4 1 5 19 freq(volume P quart) 10 0 2 16 397 Computational Linguistics Volume 32, Number 3 more weight to columns (patterns) with frequencies that vary substantially from one row (word pair) to the next, and less weight to columns that are uniform." ></td>
	<td class="line x" title="429:729	Therefore we weight the cell x i,j by w j = 1  H j /log(m), which varies from 0 when p i,j is uniform to 1 when entropy is minimal." ></td>
	<td class="line x" title="430:729	We also apply the log transformation to frequencies, log(x i,j + 1)." ></td>
	<td class="line x" title="431:729	(Entropy is calculated with the original frequency values, before the log transformation is applied.)" ></td>
	<td class="line x" title="432:729	For all i and all j, replace the original value x i,j in X by the new value w j log(x i,j + 1)." ></td>
	<td class="line x" title="433:729	This is an instance of the Term Frequency-Inverse Document Frequency (TF-IDF) family of transformations, which is familiar in information retrieval (Salton and Buckley 1988; Baeza-Yates and Ribeiro-Neto 1999): log(x i,j + 1) is the TF term and w j is the IDF term." ></td>
	<td class="line x" title="434:729	9." ></td>
	<td class="line x" title="435:729	Apply SVD: After the log and entropy transformations have been applied to the matrix X, run SVDLIBC." ></td>
	<td class="line x" title="436:729	SVD decomposes a matrix X into a product of three matrices UV T , where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length: U T U = V T V = I) and  is a diagonal matrix of singular values (hence SVD) (Golub and Van Loan 1996)." ></td>
	<td class="line x" title="437:729	If X is of rank r, then  is also of rank r.Let k , where k < r, be the diagonal matrix formed from the top k singular values, and let U k and V k be the matrices produced by selecting the corresponding columns from U and V. The matrix U k  k V T k is the matrix of rank k that best approximates the original matrix X, in the sense that it minimizes the approximation errors." ></td>
	<td class="line x" title="438:729	That is,  X = U k  k V T k minimizes vextenddouble vextenddouble X  X vextenddouble vextenddouble F over all matrices  X of rank k, where bardblbardbl F denotes the Frobenius norm (Golub and Van Loan 1996)." ></td>
	<td class="line x" title="439:729	We may think of this matrix U k  k V T k as a smoothed or compressed version of the original matrix." ></td>
	<td class="line x" title="440:729	In the subsequent steps, we will be calculating cosines for row vectors." ></td>
	<td class="line x" title="441:729	For this purpose, we can simplify calculations by dropping V. The cosine of two vectors is their dot product, after they have been normalized to unit length." ></td>
	<td class="line x" title="442:729	The matrix XX T contains the dot products of all of the row vectors." ></td>
	<td class="line x" title="443:729	We can find the dot product of the ith and jth row vectors by looking at thecellinrowi, column j of the matrix XX T . Since V T V = I, we have XX T = UV T (UV T ) T = UV T V T U T = U(U) T , which means that we can calculate cosines with the smaller matrix U, instead of using X = UV T (Deerwester et al. 1990)." ></td>
	<td class="line x" title="444:729	10." ></td>
	<td class="line x" title="445:729	Projection: Calculate U k  k (we use k = 300)." ></td>
	<td class="line x" title="446:729	This matrix has the same number of rows as X, but only k columns (instead of 2  num patterns columns; in our experiments, that is 300 columns instead of 8,000)." ></td>
	<td class="line x" title="447:729	We can compare two word pairs by calculating the cosine of the corresponding row vectors in U k  k . The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space." ></td>
	<td class="line x" title="448:729	The value k = 300 is recommended by Landauer and Dumais (1997) for measuring the attributional similarity between words." ></td>
	<td class="line x" title="449:729	We investigate other values in Section 6.4." ></td>
	<td class="line x" title="450:729	11." ></td>
	<td class="line x" title="451:729	Evaluate alternates: Let A:B and C:D be any two word pairs in the input set." ></td>
	<td class="line x" title="452:729	From step 2, we have (num filter + 1) versions of A:B, the original and num filter alternates." ></td>
	<td class="line x" title="453:729	Likewise, we have (num filter + 1) versions of C:D. 398 Turney Similarity of Semantic Relations Therefore we have (num filter + 1) 2 ways to compare a version of A:B with a version of C:D. Look for the row vectors in U k  k that correspond to the versions of A:B and the versions of C:D and calculate the (num filter + 1) 2 cosines (in our experiments, there are 16 cosines)." ></td>
	<td class="line x" title="454:729	For example, suppose A:B is quart:volume and C:D is mile:distance." ></td>
	<td class="line x" title="455:729	Table 10 gives the cosines for the sixteen combinations." ></td>
	<td class="line x" title="456:729	12." ></td>
	<td class="line x" title="457:729	Calculate relational similarity: The relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1) 2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. The requirement that the cosine must be greater than or equal to the original cosine is a way of filtering out poor analogies, which may be introduced in step 1 and may have slipped through the filtering in step 2." ></td>
	<td class="line x" title="458:729	Averaging the cosines, as opposed to taking their maximum, is intended to provide some resistance to noise." ></td>
	<td class="line x" title="459:729	For quart:volume and mile:distance, the third column in Table 10 shows which alternates are used to calculate the average." ></td>
	<td class="line x" title="460:729	For these two pairs, the average of the selected cosines is 0.677." ></td>
	<td class="line x" title="461:729	In Table 7, we see that pumping:volume has slipped through the filtering in step 2, although it is not a good alternate for quart:volume." ></td>
	<td class="line x" title="462:729	However, Table 10 shows that all four analogies that involve pumping:volume are dropped here, in step 12." ></td>
	<td class="line x" title="463:729	Steps 11 and 12 can be repeated for each two input pairs that are to be compared." ></td>
	<td class="line x" title="464:729	This completes the description of LRA." ></td>
	<td class="line x" title="465:729	Table 11 gives the cosines for the sample SAT question." ></td>
	<td class="line x" title="466:729	The choice pair with the highest average cosine (the choice with the largest value in column 1), choice (b), is the solution for this question; LRA answers the question correctly." ></td>
	<td class="line x" title="467:729	For comparison, column 2 gives the cosines for the original pairs and column 3 gives the highest cosine." ></td>
	<td class="line x" title="468:729	Table 10 The 16 combinations and their cosines." ></td>
	<td class="line x" title="469:729	A:B::C:D expresses the analogy AistoBasCistoD.The third column indicates those combinations for which the cosine is greater than or equal to the cosine of the original analogy, quart:volume::mile:distance." ></td>
	<td class="line x" title="470:729	Word pairs Cosine Cosine  original pairs quart:volume::mile:distance 0.525 Yes (original pairs) quart:volume::feet:distance 0.464 quart:volume::mile:length 0.634 Yes quart:volume::length:distance 0.499 liter:volume::mile:distance 0.736 Yes liter:volume::feet:distance 0.687 Yes liter:volume::mile:length 0.745 Yes liter:volume::length:distance 0.576 Yes gallon:volume::mile:distance 0.763 Yes gallon:volume::feet:distance 0.710 Yes gallon:volume::mile:length 0.781 Yes (highest cosine) gallon:volume::length:distance 0.615 Yes pumping:volume::mile:distance 0.412 pumping:volume::feet:distance 0.439 pumping:volume::mile:length 0.446 pumping:volume::length:distance 0.491 399 Computational Linguistics Volume 32, Number 3 For this particular SAT question, there is one choice that has the highest cosine for all three columns, choice (b), although this is not true in general." ></td>
	<td class="line x" title="471:729	Note that the gap between the first choice (b) and the second choice (d) is largest for the average cosines (column 1)." ></td>
	<td class="line x" title="472:729	This suggests that the average of the cosines (column 1) is better at discriminating the correct choice than either the original cosine (column 2) or the highest cosine (column 3)." ></td>
	<td class="line x" title="473:729	6." ></td>
	<td class="line x" title="474:729	Experiments with Word Analogy Questions This section presents various experiments with 374 multiple-choice SAT word analogy questions." ></td>
	<td class="line x" title="475:729	6.1 Baseline LRA System Table 12 shows the performance of the baseline LRA system on the 374 SAT questions, using the parameter settings and configuration described in Section 5." ></td>
	<td class="line x" title="476:729	LRA correctly answered 210 of the 374 questions; 160 questions were answered incorrectly and 4 questions were skipped, because the stem pair and its alternates were represented by zero vectors." ></td>
	<td class="line x" title="477:729	The performance of LRA is significantly better than the lexicon-based approach of Veale (2004) (see Section 3.1) and the best performance using attributional similarity (see Section 2.3), with 95% confidence, according to the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="478:729	As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency." ></td>
	<td class="line x" title="479:729	The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation." ></td>
	<td class="line x" title="480:729	This strategy is signifcantly worse than random guessing." ></td>
	<td class="line x" title="481:729	The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not significantly)." ></td>
	<td class="line x" title="482:729	It appears that the designers of the SAT questions deliberately chose distractors that would thwart these two strategies." ></td>
	<td class="line x" title="483:729	Table 11 Cosines for the sample SAT question given in Table 6." ></td>
	<td class="line x" title="484:729	Column 1 gives the averages of the cosines that are greater than or equal to the original cosines (e.g., the average of the cosines that are marked Yes in Table 10 is 0.677; see choice (b) in column 1)." ></td>
	<td class="line x" title="485:729	Column 2 gives the cosine for the original pairs (e.g., the cosine for the first pair in Table 10 is 0.525; see choice (b) in column 2)." ></td>
	<td class="line x" title="486:729	Column 3 gives the maximum cosine for the 16 possible analogies (e.g., the maximum cosine in Table 10 is 0.781; see choice (b) in column 3)." ></td>
	<td class="line x" title="487:729	Stem: quart:volume Average Original Highest cosines cosines cosines 123 Choices: (a) day:night 0.374 0.327 0.443 (b) mile:distance 0.677 0.525 0.781 (c) decade:century 0.389 0.327 0.470 (d) friction:heat 0.428 0.336 0.552 (e) part:whole 0.370 0.330 0.408 Solution: (b) mile:distance 0.677 0.525 0.781 Gap: (b)(d) 0.249 0.189 0.229 400 Turney Similarity of Semantic Relations Table 12 Performance of LRA on the 374 SAT questions." ></td>
	<td class="line x" title="488:729	Precision, recall, and F are reported as percentages." ></td>
	<td class="line x" title="489:729	(The bottom five rows are included for comparison.)" ></td>
	<td class="line x" title="490:729	Algorithm Precision Recall F LRA 56.8 56.1 56.5 Veale (2004) 42.8 42.8 42.8 Best attributional similarity 35.0 35.0 35.0 Random guessing 20.0 20.0 20.0 Lowest co-occurrence frequency 16.8 16.8 16.8 Highest co-occurrence frequency 11.8 11.8 11.8 With 374 questions and six word pairs per question (one stem and five choices), there are 2,244 pairs in the input set." ></td>
	<td class="line x" title="491:729	In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs." ></td>
	<td class="line x" title="492:729	In step 5, for each pair A:B,weadd B:A, yielding 17,952 pairs." ></td>
	<td class="line x" title="493:729	However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five words in the WMTS corpus)." ></td>
	<td class="line x" title="494:729	Also, a few words do not appear in Lins thesaurus, and some word pairs appear twice in the SAT questions (e.g., lion:cat)." ></td>
	<td class="line x" title="495:729	The sparse matrix (step 7) has 17,232 rows (word pairs) and 8,000 columns (patterns), with a density of 5.8% (percentage of nonzero values)." ></td>
	<td class="line x" title="496:729	Table 13 gives the time required for each step of LRA, a total of almost 9 days." ></td>
	<td class="line x" title="497:729	All of the steps used a single CPU on a desktop computer, except step 3, finding the phrases for each word pair, which used a 16 CPU Beowulf cluster." ></td>
	<td class="line x" title="498:729	Most of the other steps are parallelizable; with a bit of programming effort, they could also be executed on the Beowulf cluster." ></td>
	<td class="line x" title="499:729	All CPUs (both desktop and cluster) were 2.4 GHz Intel Xeons." ></td>
	<td class="line x" title="500:729	The desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM." ></td>
	<td class="line x" title="501:729	6.2 LRA versus VSM Table 14 compares LRA to the VSM with the 374 analogy questions." ></td>
	<td class="line x" title="502:729	VSM-AV refers to the VSM using AltaVistas database as a corpus." ></td>
	<td class="line x" title="503:729	The VSM-AV results are taken Table 13 LRA elapsed run time." ></td>
	<td class="line x" title="504:729	Step Description Time H:M:S Hardware 1 Find alternates 24:56:00 1 CPU 2 Filter alternates 0:00:02 1 CPU 3 Find phrases 109:52:00 16 CPUs 4 Find patterns 33:41:00 1 CPU 5 Map pairs to rows 0:00:02 1 CPU 6 Map patterns to columns 0:00:02 1 CPU 7 Generate a sparse matrix 38:07:00 1 CPU 8 Calculate entropy 0:11:00 1 CPU 9 Apply SVD 0:43:28 1 CPU 10 Projection 0:08:00 1 CPU 11 Evaluate alternates 2:11:00 1 CPU 12 Calculate relational similarity 0:00:02 1 CPU Total 209:49:36 401 Computational Linguistics Volume 32, Number 3 from Turney and Littman (2005)." ></td>
	<td class="line x" title="505:729	As mentioned in Section 4.2, we estimate this corpus contained about 5  10 11 English words at the time the VSM-AV experiments took place." ></td>
	<td class="line x" title="506:729	VSM-WMTS refers to the VSM using the WMTS, which contains about 5  10 10 English words." ></td>
	<td class="line x" title="507:729	We generated the VSM-WMTS results by adapting the VSM to the WMTS." ></td>
	<td class="line x" title="508:729	The algorithm is slightly different from Turney and Littmans (2005), because we used passage frequencies instead of document frequencies." ></td>
	<td class="line x" title="509:729	All three pairwise differences in recall in Table 14 are statistically significant with 95% confidence, using the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="510:729	The pairwise differences in precision between LRA and the two VSM variations are also significant, but the difference in precision between the two VSM variations (42.4% vs. 47.7%) is not significant." ></td>
	<td class="line x" title="511:729	Although VSM-AV has a corpus 10 times larger than LRAs, LRA still performs better than VSM-AV." ></td>
	<td class="line x" title="512:729	Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of the VSM, but much of the drop is due to the larger number of questions that were skipped (34 for VSM-WMTS versus 5 for VSM-AV)." ></td>
	<td class="line x" title="513:729	With the smaller corpus, many more of the input word pairs simply do not appear together in short phrases in the corpus." ></td>
	<td class="line x" title="514:729	LRA is able to answer as many questions as VSM-AV, although it uses the same corpus as VSM-WMTS, because Lins thesaurus allows LRA to substitute synonyms for words that are not in the corpus." ></td>
	<td class="line x" title="515:729	VSM-AV required 17 days to process the 374 analogy questions (Turney and Littman 2005), compared to 9 days for LRA." ></td>
	<td class="line x" title="516:729	As a courtesy to AltaVista, Turney and Littman (2005) inserted a 5-second delay between each two queries." ></td>
	<td class="line x" title="517:729	Since the WMTS is running locally, there is no need for delays." ></td>
	<td class="line x" title="518:729	VSM-WMTS processed the questions in only one day." ></td>
	<td class="line x" title="519:729	6.3 Human Performance The average performance of college-bound senior high school students on verbal SAT questions corresponds to a recall (percent correct) of about 57% (Turney and Littman 2005)." ></td>
	<td class="line x" title="520:729	The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry)." ></td>
	<td class="line x" title="521:729	Analogy questions are only a subset of the 78 verbal SAT questions." ></td>
	<td class="line x" title="522:729	If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions." ></td>
	<td class="line x" title="523:729	Of our 374 SAT questions, 190 are from a collection of ten official SAT tests (Claman 2000)." ></td>
	<td class="line x" title="524:729	On this subset of the questions, LRA has a recall of 61.1%, compared to a recall of 51.1% on the other 184 questions." ></td>
	<td class="line x" title="525:729	The 184 questions that are not from Claman (2000) seem to be more difficult." ></td>
	<td class="line x" title="526:729	This indicates that we may be underestimating how well LRA performs, relative to college-bound senior high school students." ></td>
	<td class="line x" title="527:729	Claman (2000) suggests that the analogy questions may be somewhat harder than other verbal SAT Table 14 LRA versus VSM with 374 SAT analogy questions." ></td>
	<td class="line x" title="528:729	Algorithm Correct Incorrect Skipped Precision Recall F VSM-AV 176 193 5 47.7 47.1 47.4 VSM-WMTS 144 196 34 42.4 38.5 40.3 LRA 210 160 4 56.8 56.1 56.5 402 Turney Similarity of Semantic Relations questions, so we may be slightly overestimating the mean human score on the analogy questions." ></td>
	<td class="line x" title="529:729	Table 15 gives the 95% confidence intervals for LRA, VSM-AV, and VSM-WMTS, calculated by the Binomial Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="530:729	There is no significant difference between LRA and human performance, but VSM-AV and VSM-WMTS are significantly below human-level performance." ></td>
	<td class="line x" title="531:729	6.4 Varying the Parameters in LRA There are several parameters in the LRA algorithm (see Section 5.5)." ></td>
	<td class="line x" title="532:729	The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside." ></td>
	<td class="line x" title="533:729	Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions." ></td>
	<td class="line x" title="534:729	We hypothesized that LRA is relatively insensitive to the values of the parameters." ></td>
	<td class="line x" title="535:729	Table 16 shows the variation in the performance of LRA as the parameter values are adjusted." ></td>
	<td class="line x" title="536:729	We take the baseline parameter settings (given in Section 5.5) and vary each parameter, one at a time, while holding the remaining parameters fixed at their baseline values." ></td>
	<td class="line x" title="537:729	None of the precision and recall values are significantly different from the baseline, according to the Fisher Exact Test (Agresti 1990), at the 95% confidence level." ></td>
	<td class="line x" title="538:729	This supports the hypothesis that the algorithm is not sensitive to the parameter values." ></td>
	<td class="line x" title="539:729	Although a full run of LRA on the 374 SAT questions takes 9 days, for some of the parameters it is possible to reuse cached data from previous runs." ></td>
	<td class="line x" title="540:729	We limited the experiments with num sim and max phrase because caching was not as helpful for these parameters, so experimenting with them required several weeks." ></td>
	<td class="line x" title="541:729	6.5 Ablation Experiments As mentioned in the introduction, LRA extends the VSM approach of Turney and Littman (2005) by (1) exploring variations on the analogies by replacing words with synonyms (step 1), (2) automatically generating connecting patterns (step 4), and (3) smoothing the data with SVD (step 9)." ></td>
	<td class="line x" title="542:729	In this subsection, we ablate each of these three components to assess their contribution to the performance of LRA." ></td>
	<td class="line x" title="543:729	Table 17 shows the results." ></td>
	<td class="line x" title="544:729	Table 15 Comparison with human SAT performance." ></td>
	<td class="line x" title="545:729	The last column in the table indicates whether (YES) or not (NO) the average human performance (57%) falls within the 95% confidence interval of the corresponding algorithms performance." ></td>
	<td class="line x" title="546:729	The confidence intervals are calculated using the Binomial Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="547:729	System Recall 95% confidence Human-level (% correct) interval for recall (57%) VSM-AV 47.1 42.252.5 NO VSM-WMTS 38.5 33.543.6 NO LRA 56.1 51.061.2 YES 403 Computational Linguistics Volume 32, Number 3 Table 16 Variation in performance with different parameter values." ></td>
	<td class="line x" title="548:729	The Baseline column marks the baseline parameter values." ></td>
	<td class="line x" title="549:729	The Step column gives the step number in Section 5.5 where each parameter is discussed." ></td>
	<td class="line x" title="550:729	Parameter Baseline Value Step Precision Recall F num sim 5 1 54.2 53.5 53.8 num sim  10 1 56.8 56.1 56.5 num sim 15 1 54.1 53.5 53.8 max phrase 4 2 55.8 55.1 55.5 max phrase  5 2 56.8 56.1 56.5 max phrase 6 2 56.2 55.6 55.9 num filter 1 2 54.3 53.7 54.0 num filter 2 2 55.7 55.1 55.4 num filter  3 2 56.8 56.1 56.5 num filter 4 2 55.7 55.1 55.4 num filter 5 2 54.3 53.7 54.0 num patterns 1000 4 55.9 55.3 55.6 num patterns 2000 4 57.6 57.0 57.3 num patterns 3000 4 58.4 57.8 58.1 num patterns  4000 4 56.8 56.1 56.5 num patterns 5000 4 57.0 56.4 56.7 num patterns 6000 4 57.0 56.4 56.7 num patterns 7000 4 58.1 57.5 57.8 k 100 10 55.7 55.1 55.4 k  300 10 56.8 56.1 56.5 k 500 10 57.6 57.0 57.3 k 700 10 56.5 55.9 56.2 k 900 10 56.2 55.6 55.9 Without SVD (compare column 1 to 2 in Table 17), performance drops, but the drop is not statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="551:729	However, we hypothesize that the drop in performance would be significant with a larger set of word pairs." ></td>
	<td class="line x" title="552:729	More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution." ></td>
	<td class="line x" title="553:729	Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage." ></td>
	<td class="line x" title="554:729	For example, Landauer and Dumais (1997) apply SVD to a matrix of 30,473 columns by 60,768 rows, but our matrix Table 17 Results of ablation experiments." ></td>
	<td class="line x" title="555:729	LRA LRA Baseline LRA LRA No SVD, system No SVD No synonyms no synonyms VSM-WMTS 12 3 4 5 Correct 210 198 185 178 144 Incorrect 160 172 167 173 196 Skipped 4 4 22 23 34 Precision 56.8 53.5 52.6 50.7 42.4 Recall 56.1 52.9 49.5 47.6 38.5 F 56.5 53.2 51.0 49.1 40.3 404 Turney Similarity of Semantic Relations here is 8,000 columns by 17,232 rows." ></td>
	<td class="line x" title="556:729	We are currently gathering more SAT questions to test this hypothesis." ></td>
	<td class="line x" title="557:729	Without synonyms (compare column 1 to 3 in Table 17), recall drops significantly (from 56.1% to 49.5%), but the drop in precision is not significant." ></td>
	<td class="line x" title="558:729	When the synonym component is dropped, the number of skipped questions rises from 4 to 22, which demonstrates the value of the synonym component of LRA for compensating for sparse data." ></td>
	<td class="line x" title="559:729	When both SVD and synonyms are dropped (compare column 1 to 4 in Table 17), the decrease in recall is significant, but the decrease in precision is not significant." ></td>
	<td class="line x" title="560:729	Again, we believe that a larger sample size would show that the drop in precision is significant." ></td>
	<td class="line x" title="561:729	If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from VSM-WMTS is the patterns (step 4)." ></td>
	<td class="line x" title="562:729	The VSM approach uses a fixed list of 64 patterns to generate 128 dimensional vectors (Turney and Littman 2005), whereas LRA uses a dynamically generated set of 4,000 patterns, resulting in 8,000 dimensional vectors." ></td>
	<td class="line x" title="563:729	We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5)." ></td>
	<td class="line x" title="564:729	The difference in both precision and recall is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="565:729	The ablation experiments support the value of the patterns (step 4) and synonyms (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness." ></td>
	<td class="line x" title="566:729	Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5)." ></td>
	<td class="line x" title="567:729	6.6 Matrix Symmetry We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as carpenter is to wood implies stone is to mason as wood is to carpenter." ></td>
	<td class="line x" title="568:729	Therefore, a good measure of relational similarity, sim r , should obey the following equation: sim r (A:B, C:D) = sim r (B:A, D:C)(8) In steps 5 and 6 of the LRA algorithm (Section 5.5), we ensure that the matrix X is symmetrical, so that equation (8) is necessarily true for LRA." ></td>
	<td class="line x" title="569:729	The matrix is designed so that the row vector for A:B is different from the row vector for B:A only by a permutation of the elements." ></td>
	<td class="line x" title="570:729	The same permutation distinguishes the row vectors for C:D and D:C. Therefore the cosine of the angle between A:B and C:D must be identical to the cosine of the angle between B:A and D:C (see equation (7))." ></td>
	<td class="line x" title="571:729	To discover the consequences of this design decision, we altered steps 5 and 6 so that symmetry is no longer preserved." ></td>
	<td class="line x" title="572:729	In step 5, for each word pair A:B that appears in the input set, we only have one row." ></td>
	<td class="line x" title="573:729	There is no row for B:A unless B:A also appears in the input set." ></td>
	<td class="line x" title="574:729	Thus the number of rows in the matrix dropped from 17,232 to 8,616." ></td>
	<td class="line x" title="575:729	In step 6, we no longer have two columns for each pattern P,oneforword 1 Pword 2  and another for word 2 Pword 1 . However, to be fair, we kept the total number of columns at 8,000." ></td>
	<td class="line x" title="576:729	In step 4, we selected the top 8,000 patterns (instead of the top 4,000), distinguishing the pattern word 1 Pword 2  from the pattern word 2 Pword 1 (insteadof considering them equivalent)." ></td>
	<td class="line x" title="577:729	Thus a pattern P with a high frequency is likely to appear in two columns, in both possible orders, but a lower frequency pattern might appear in only one column, in only one possible order." ></td>
	<td class="line x" title="578:729	405 Computational Linguistics Volume 32, Number 3 These changes resulted in a slight decrease in performance." ></td>
	<td class="line x" title="579:729	Recall dropped from 56.1% to 55.3% and precision dropped from 56.8% to 55.9%." ></td>
	<td class="line x" title="580:729	The decrease is not statistically significant." ></td>
	<td class="line x" title="581:729	However, the modified algorithm no longer obeys equation (8)." ></td>
	<td class="line x" title="582:729	Although dropping symmetry appears to cause no significant harm to the performance of the algorithm on the SAT questions, we prefer to retain symmetry, to ensure that equation (8) is satisfied." ></td>
	<td class="line x" title="583:729	Note that, if A:B::C:D,itdoesnot follow that B:A::C:D. For example, it is false that stone is to mason as carpenter is to wood. In general (except when the semantic relations between A and B are symmetrical), we have the following inequality: sim r (A:B, C:D) negationslash= sim r (B:A, C:D)(9) Therefore we do not want A:B and B:A to be represented by identical row vectors, although it would ensure that equation (8) is satisfied." ></td>
	<td class="line x" title="584:729	6.7 All Alternates versus Better Alternates In step 12 of LRA, the relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1) 2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. That is, the average includes only those alternates that are better than the originals." ></td>
	<td class="line x" title="585:729	Taking all alternates instead of the better alternates, recall drops from 56.1% to 40.4% and precision drops from 56.8% to 40.8%." ></td>
	<td class="line x" title="586:729	Both decreases are statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="587:729	6.8 Interpreting Vectors Suppose a word pair A:B corresponds to a vector r in the matrix X. It would be convenient if inspection of r gave us a simple explanation or description of the relation between A and B. For example, suppose the word pair ostrich:bird maps to the row vector r. It would be pleasing to look in r and find that the largest element corresponds to the pattern is the largest (i.e., ostrich is the largest bird)." ></td>
	<td class="line x" title="588:729	Unfortunately, inspection of r reveals no such convenient patterns." ></td>
	<td class="line x" title="589:729	We hypothesize that the semantic content of a vector is distributed over the whole vector; it is not concentrated in a few elements." ></td>
	<td class="line x" title="590:729	To test this hypothesis, we modified step 10 of LRA." ></td>
	<td class="line x" title="591:729	Instead of projecting the 8,000 dimensional vectors into the 300 dimensional space U k  k , we use the matrix U k  k V T k . This matrix yields the same cosines as U k  k , but preserves the original 8,000 dimensions, making it easier to interpret the row vectors." ></td>
	<td class="line x" title="592:729	For each row vector in U k  k V T k , we select the N largest values and set all other values to zero." ></td>
	<td class="line x" title="593:729	The idea here is that we will only pay attention to the N most important patterns in r; the remaining patterns will be ignored." ></td>
	<td class="line x" title="594:729	This reduces the length of the row vectors, but the cosine is the dot product of normalized vectors (all vectors are normalized to unit length; see equation (7)), so the change to the vector lengths has no impact; only the angle of the vectors is important." ></td>
	<td class="line x" title="595:729	If most of the semantic content is in the N largest elements of r, then setting the remaining elements to zero should have relatively little impact." ></td>
	<td class="line x" title="596:729	Table 18 shows the performance as N varies from 1 to 3,000." ></td>
	<td class="line x" title="597:729	The precision and recall are significantly below the baseline LRA until N  300 (95% confidence, Fisher Exact 406 Turney Similarity of Semantic Relations Test)." ></td>
	<td class="line x" title="598:729	In other words, for a typical SAT analogy question, we need to examine the top 300 patterns to explain why LRA selected one choice instead of another." ></td>
	<td class="line x" title="599:729	We are currently working on an extension of LRA that will explain with a single pattern why one choice is better than another." ></td>
	<td class="line x" title="600:729	We have had some promising results, but this work is not yet mature." ></td>
	<td class="line x" title="601:729	However, we can confidently claim that interpreting the vectors is not trivial." ></td>
	<td class="line x" title="602:729	6.9 Manual Patterns versus Automatic Patterns Turney and Littman (2005) used 64 manually generated patterns, whereas LRA uses 4,000 automatically generated patterns." ></td>
	<td class="line x" title="603:729	We know from Section 6.5 that the automatically generated patterns are significantly better than the manually generated patterns." ></td>
	<td class="line x" title="604:729	It may be interesting to see how many of the manually generated patterns appear within the automatically generated patterns." ></td>
	<td class="line x" title="605:729	If we require an exact match, 50 of the 64 manual patterns can be found in the automatic patterns." ></td>
	<td class="line x" title="606:729	If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns." ></td>
	<td class="line x" title="607:729	This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns." ></td>
	<td class="line x" title="608:729	Turney and Littman (2005) point out that some of their 64 patterns have been used by other researchers." ></td>
	<td class="line x" title="609:729	For example, Hearst (1992) used the pattern such as to discover hyponyms and Berland and Charniak (1999) used the pattern of the to discover meronyms." ></td>
	<td class="line x" title="610:729	Both of these patterns are included in the 4,000 patterns automatically generated by LRA." ></td>
	<td class="line x" title="611:729	The novelty in Turney and Littman (2005) is that their patterns are not used to mine text for instances of word pairs that fit the patterns (Hearst 1992; Berland and Charniak 1999); instead, they are used to gather frequency data for building vectors that represent the relation between a given pair of words." ></td>
	<td class="line x" title="612:729	The results in Section 6.8 show that a vector contains more information than any single pattern or small set of patterns; a vector is a distributed representation." ></td>
	<td class="line x" title="613:729	LRA is distinct from Hearst (1992) and Berland and Charniak (1999) in its focus on distributed representations, which it shares with Turney and Littman (2005), but LRA goes beyond Turney and Littman (2005) by finding patterns automatically." ></td>
	<td class="line x" title="614:729	Riloff and Jones (1999) and Yangarber (2003) also find patterns automatically, but their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and Table 18 Performance as a function of N. N Correct Incorrect Skipped Precision Recall F 1 114 179 81 38.9 30.5 34.2 3 146 206 22 41.5 39.0 40.2 10 167 201 6 45.4 44.7 45.0 30 174 196 4 47.0 46.5 46.8 100 178 192 4 48.1 47.6 47.8 300 192 178 4 51.9 51.3 51.6 1000 198 172 4 53.5 52.9 53.2 3000 207 163 4 55.9 55.3 55.6 407 Computational Linguistics Volume 32, Number 3 Berland and Charniak (1999)." ></td>
	<td class="line x" title="615:729	Because LRA uses patterns to build distributed vector representations, it can exploit patterns that would be much too noisy and unreliable for the kind of text mining instance extraction that is the objective of Hearst (1992), Berland and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003)." ></td>
	<td class="line x" title="616:729	Therefore LRA can simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003)." ></td>
	<td class="line x" title="617:729	7." ></td>
	<td class="line x" title="618:729	Experiments with Noun-Modifier Relations This section describes experiments with 600 noun-modifier pairs, hand-labeled with 30 classes of semantic relations (Nastase and Szpakowicz 2003)." ></td>
	<td class="line x" title="619:729	In the following experiments, LRA is used with the baseline parameter values, exactly as described in Section 5.5." ></td>
	<td class="line x" title="620:729	No adjustments were made to tune LRA to the noun-modifier pairs." ></td>
	<td class="line x" title="621:729	LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm." ></td>
	<td class="line x" title="622:729	7.1 Classes of Relations The following experiments use the 600 labeled noun-modifier pairs of Nastase and Szpakowicz (2003)." ></td>
	<td class="line x" title="623:729	This data set includes information about the part of speech and WordNet synset (synonym set; i.e., word sense tag) of each word, but our algorithm does not use this information." ></td>
	<td class="line x" title="624:729	Table 19 lists the 30 classes of semantic relations." ></td>
	<td class="line x" title="625:729	The table is based on Appendix A of Nastase and Szpakowicz (2003), with some simplifications." ></td>
	<td class="line x" title="626:729	The original table listed several semantic relations for which there were no instances in the data set." ></td>
	<td class="line x" title="627:729	These were relations that are typically expressed with longer phrases (three or more words), rather than noun-modifier word pairs." ></td>
	<td class="line x" title="628:729	For clarity, we decided not to include these relations in Table 19." ></td>
	<td class="line x" title="629:729	In this table, H represents the head noun and M represents the modifier." ></td>
	<td class="line x" title="630:729	For example, in flu virus, the head noun (H)isvirus and the modifier (M)isflu (*)." ></td>
	<td class="line x" title="631:729	In English, the modifier (typically a noun or adjective) usually precedes the head noun." ></td>
	<td class="line x" title="632:729	In the description of purpose, V represents an arbitrary verb." ></td>
	<td class="line x" title="633:729	In concert hall, the hall is for presenting concerts (V is present) or holding concerts (V is hold)()." ></td>
	<td class="line x" title="634:729	Nastase and Szpakowicz (2003) organized the relations into groups." ></td>
	<td class="line x" title="635:729	The five capitalized terms in the Relation column of Table 19 are the names of five groups of semantic relations." ></td>
	<td class="line x" title="636:729	(The original table had a sixth group, but there are no examples of this group in the data set.)" ></td>
	<td class="line x" title="637:729	We make use of this grouping in the following experiments." ></td>
	<td class="line x" title="638:729	7.2 Baseline LRA with Single Nearest Neighbor The following experiments use single nearest neighbor classification with leave-one-out cross-validation." ></td>
	<td class="line x" title="639:729	For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaining noun-modifiers." ></td>
	<td class="line x" title="640:729	The data set is split 600 times, so that each noun-modifier gets a turn as the testing word pair." ></td>
	<td class="line x" title="641:729	The predicted class of the testing pair is the class of the single nearest neighbor in the training set." ></td>
	<td class="line x" title="642:729	As the measure of nearness, we use LRA to calculate the relational similarity between the testing pair and the training pairs." ></td>
	<td class="line x" title="643:729	The single nearest neighbor algorithm is a supervised learning algorithm (i.e., it requires a training set of labeled 408 Turney Similarity of Semantic Relations Table 19 Classes of semantic relations, from Nastase and Szpakowicz (2003)." ></td>
	<td class="line x" title="644:729	Relation Abbr." ></td>
	<td class="line x" title="645:729	Example phrase Description CAUSALITY cause cs flu virus (*) H makes M occur or exist, H is necessary and sufficient effect eff exam anxiety M makes H occur or exist, M is necessary and sufficient purpose prp concert hall () H is for V-ing M, M does not necessarily occur or exist detraction detr headache pill H opposes M, H is not sufficient to prevent M TEMPORALITY frequency freq daily exercise H occurs every time M occurs time at tat morning exercise H occurs when M occurs time through tthr six-hour meeting H existed while M existed, M is an interval of time SPATIAL direction dir outgoing mail H is directed towards M, M is not the final point location loc home town H is the location of M location at lat desert storm H is located at M location from lfr foreign capital H originates at M PARTICIPANT agent ag student protest M performs H, M is animate or natural phenomenon beneficiary ben student discount M benefits from H instrument inst laser printer H uses M object obj metal separator M is acted upon by H object property obj prop sunken ship H underwent M part part printer tray H is part of M possessor posr national debt M has H property prop blue book H is M product prod plum tree H produces M source src olive oil M is the source of H stative st sleeping dog H is in a state of M whole whl daisy chain M is part of H QUALITY container cntr film music M contains H content cont apple cake M is contained in H equative eq player coach H is also M material mat brick house H is made of M measure meas expensive book M is a measure of H topic top weather report H is concerned with M type type oak tree M is a type of H 409 Computational Linguistics Volume 32, Number 3 data), but we are using LRA to measure the distance between a pair and its potential neighbors, and LRA is itself determined in an unsupervised fashion (i.e., LRA does not need labeled data)." ></td>
	<td class="line x" title="646:729	Each SAT question has five choices, so answering 374 SAT questions required calculating 374  5  16 = 29, 920 cosines." ></td>
	<td class="line x" title="647:729	The factor of 16 comes from the alternate pairs, step 11 in LRA." ></td>
	<td class="line x" title="648:729	With the noun-modifier pairs, using leave-one-out cross-validation, each test pair has 599 choices, so an exhaustive application of LRA would require calculating 600  599  16 = 5, 750, 400 cosines." ></td>
	<td class="line x" title="649:729	To reduce the amount of computation required, we first find the 30 nearest neighbors for each pair, ignoring the alternate pairs (600  599 = 359, 400 cosines), and then apply the full LRA, including the alternates, to just those 30 neighbors (600  30  16 = 288, 000 cosines), which requires calculating only 359, 400 + 288, 000 = 647, 400 cosines." ></td>
	<td class="line x" title="650:729	There are 600 word pairs in the input set for LRA." ></td>
	<td class="line x" title="651:729	In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 2,400 pairs." ></td>
	<td class="line x" title="652:729	In step 5, for each pair A:B,weaddB:A, yielding 4,800 pairs." ></td>
	<td class="line x" title="653:729	However, some pairs are dropped because they correspond to zero vectors and a few words do not appear in Lins thesaurus." ></td>
	<td class="line x" title="654:729	The sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8.4%." ></td>
	<td class="line x" title="655:729	Following Turney and Littman (2005), we evaluate the performance by accuracy and also by the macroaveraged F measure (Lewis 1991)." ></td>
	<td class="line x" title="656:729	Macroaveraging calculates the precision, recall, and F for each class separately, and then calculates the average across all classes." ></td>
	<td class="line x" title="657:729	Microaveraging combines the true positive, false positive, and false negative counts for all of the classes, and then calculates precision, recall, and F from the combined counts." ></td>
	<td class="line x" title="658:729	Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes." ></td>
	<td class="line x" title="659:729	We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the classes in a real corpus." ></td>
	<td class="line x" title="660:729	Classification with 30 distinct classes is a hard problem." ></td>
	<td class="line x" title="661:729	To make the task easier, we can collapse the 30 classes to 5 classes, using the grouping that is given in Table 19." ></td>
	<td class="line x" title="662:729	For example, agent and beneficiary both collapse to participant." ></td>
	<td class="line x" title="663:729	On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39.8% (239/600) and a macroaveraged F of 36.6%." ></td>
	<td class="line x" title="664:729	Always guessing the majority class would result in an accuracy of 8.2% (49/600)." ></td>
	<td class="line x" title="665:729	On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54.6%." ></td>
	<td class="line x" title="666:729	Always guessing the majority class would give an accuracy of 43.3% (260/600)." ></td>
	<td class="line x" title="667:729	For both the 30 class and 5 class problems, LRAs accuracy is significantly higher than guessing the majority class, with 95% confidence, according to the Fisher Exact Test (Agresti 1990)." ></td>
	<td class="line x" title="668:729	7.3 LRA versus VSM Table 20 shows the performance of LRA and VSM on the 30 class problem." ></td>
	<td class="line x" title="669:729	VSM-AV is VSM with the AltaVista corpus and VSM-WMTS is VSM with the WMTS corpus." ></td>
	<td class="line x" title="670:729	The results for VSM-AV are taken from Turney and Littman (2005)." ></td>
	<td class="line x" title="671:729	All three pairwise differences in the three F measures are statistically significant at the 95% level, according to the Paired t-Test (Feelders and Verkooijen 1995)." ></td>
	<td class="line x" title="672:729	The accuracy of LRA is significantly higher than the accuracies of VSM-AV and VSM-WMTS, according to the Fisher Exact Test (Agresti 1990), but the difference between the two VSM accuracies is not significant." ></td>
	<td class="line x" title="673:729	Table 21 compares the performance of LRA and VSM on the 5 class problem." ></td>
	<td class="line x" title="674:729	The accuracy and F measure of LRA are significantly higher than the accuracies and 410 Turney Similarity of Semantic Relations Table 20 Comparison of LRA and VSM on the 30 class problem." ></td>
	<td class="line x" title="675:729	VSM-AV VSM-WMTS LRA Correct 167 148 239 Incorrect 433 452 361 Total 600 600 600 Accuracy 27.8 24.7 39.8 Precision 27.9 24.0 41.0 Recall 26.8 20.9 35.9 F 26.5 20.3 36.6 Table 21 Comparison of LRA and VSM on the 5 class problem." ></td>
	<td class="line x" title="676:729	VSM-AV VSM-WMTS LRA Correct 274 264 348 Incorrect 326 336 252 Total 600 600 600 Accuracy 45.7 44.0 58.0 Precision 43.4 40.2 55.9 Recall 43.1 41.4 53.6 F 43.2 40.6 54.6 F measures of VSM-AV and VSM-WMTS, but the differences between the two VSM accuracies and F measures are not significant." ></td>
	<td class="line x" title="677:729	8." ></td>
	<td class="line x" title="678:729	Discussion The experimental results in Sections 6 and 7 demonstrate that LRA performs significantly better than the VSM, but it is also clear that there is room for improvement." ></td>
	<td class="line x" title="679:729	The accuracy might not yet be adequate for practical applications, although past work has shown that it is possible to adjust the trade-off of precision versus recall (Turney and Littman 2005)." ></td>
	<td class="line x" title="680:729	For some of the applications, such as information extraction, LRA might be suitable if it is adjusted for high precision, at the expense of low recall." ></td>
	<td class="line x" title="681:729	Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy questions." ></td>
	<td class="line x" title="682:729	However, with progress in computer hardware, speed will gradually become less of a concern." ></td>
	<td class="line x" title="683:729	Also, the software has not been optimized for speed; there are several places where the efficiency could be increased and many operations are parallelizable." ></td>
	<td class="line x" title="684:729	It may also be possible to precompute much of the information for LRA, although this would require substantial changes to the algorithm." ></td>
	<td class="line x" title="685:729	The difference in performance between VSM-AV and VSM-WMTS shows that VSM is sensitive to the size of the corpus." ></td>
	<td class="line x" title="686:729	Although LRA is able to surpass VSM-AV when the WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA would perform better with a larger corpus." ></td>
	<td class="line x" title="687:729	The WMTS corpus requires one terabyte of hard disk space, but progress in hardware will likely make 10 or even 100 terabytes affordable in the relatively near future." ></td>
	<td class="line x" title="688:729	For noun-modifier classification, more labeled data should yield performance improvements." ></td>
	<td class="line x" title="689:729	With 600 noun-modifier pairs and 30 classes, the average class has only 411 Computational Linguistics Volume 32, Number 3 20 examples." ></td>
	<td class="line x" title="690:729	We expect that the accuracy would improve substantially with 5 or 10 times more examples." ></td>
	<td class="line x" title="691:729	Unfortunately, it is time consuming and expensive to acquire hand-labeled data." ></td>
	<td class="line x" title="692:729	Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations." ></td>
	<td class="line x" title="693:729	The 30 classes of Nastase and Szpakowicz (2003) might not be the best scheme." ></td>
	<td class="line x" title="694:729	Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002)." ></td>
	<td class="line x" title="695:729	It seems likely that some schemes are easier for machine learning than others." ></td>
	<td class="line x" title="696:729	For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient." ></td>
	<td class="line x" title="697:729	LRA, like VSM, is a corpus-based approach to measuring relational similarity." ></td>
	<td class="line x" title="698:729	Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003)." ></td>
	<td class="line x" title="699:729	In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al.(2003)." ></td>
	<td class="line x" title="701:729	SVD is only one of many methods for handling sparse, noisy data." ></td>
	<td class="line x" title="702:729	We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000)." ></td>
	<td class="line x" title="703:729	We had some interesting results with small matrices (around 2,000 rows by 1,000 columns), but none of these methods seemed substantially better than SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows and 8,000 columns; see Section 6.1)." ></td>
	<td class="line x" title="704:729	In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns." ></td>
	<td class="line x" title="705:729	Perhaps a more sophisticated selection algorithm would improve the performance of LRA." ></td>
	<td class="line x" title="706:729	We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on performance." ></td>
	<td class="line x" title="707:729	We hypothesize that the distributed vector representation is not sensitive to the selection method, but it is possible that future work will find a method that yields significant improvement in performance." ></td>
	<td class="line x" title="708:729	9." ></td>
	<td class="line x" title="709:729	Conclusion This article has introduced a new method for calculating relational similarity, Latent Relational Analysis." ></td>
	<td class="line x" title="710:729	The experiments demonstrate that LRA performs better than the VSM approach, when evaluated with SAT word analogy questions and with the task of classifying noun-modifier expressions." ></td>
	<td class="line x" title="711:729	The VSM approach represents the relation between a pair of words with a vector, in which the elements are based on the frequencies of 64 hand-built patterns in a large corpus." ></td>
	<td class="line x" title="712:729	LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs." ></td>
	<td class="line x" title="713:729	With the WMTS corpus (about 5  10 10 English words), LRA achieves an F of 56.5%, whereas the F of VSM is 40.3%." ></td>
	<td class="line x" title="714:729	We have presented several examples of the many potential applications for measures of relational similarity." ></td>
	<td class="line x" title="715:729	Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used." ></td>
	<td class="line x" title="716:729	Gentner et al.(2001) argue that relational similarity is essential to understanding novel metaphors (as opposed to conventional metaphors)." ></td>
	<td class="line oc" title="718:729	Many 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking (Lakoff and Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner et al. 2001; French 2002)." ></td>
	<td class="line x" title="719:729	We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence." ></td>
	<td class="line x" title="720:729	In future work, we plan to investigate some potential applications for LRA." ></td>
	<td class="line x" title="721:729	It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging." ></td>
	<td class="line x" title="722:729	Acknowledgments Thanks to Michael Littman for sharing the 374 SAT analogy questions and for inspiring me to tackle them." ></td>
	<td class="line x" title="723:729	Thanks to Vivi Nastase and Stan Szpakowicz for sharing their 600 classified noun-modifier phrases." ></td>
	<td class="line x" title="724:729	Thanks to Egidio Terra, Charlie Clarke, and the School of Computer Science of the University of Waterloo, for giving us a copy of the Waterloo MultiText System and their Terabyte Corpus." ></td>
	<td class="line x" title="725:729	Thanks to Dekang Lin for making his Dependency-Based Word Similarity lexicon available online." ></td>
	<td class="line x" title="726:729	Thanks to Doug Rohde for SVDLIBC and Michael Berry for SVDPACK." ></td>
	<td class="line x" title="727:729	Thanks to Ted Pedersen for making his WordNet::Similarity package available." ></td>
	<td class="line x" title="728:729	Thanks to Joel Martin for comments on the article." ></td>
	<td class="line x" title="729:729	Thanks to the anonymous reviewers of Computational Linguistics for their very helpful comments and suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1026
Identifying And Analyzing Judgment Opinions
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:213	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 200207, New York, June 2006." ></td>
	<td class="line x" title="2:213	c2006 Association for Computational Linguistics Identifying and Analyzing Judgment Opinions Soo-Min Kim and Eduard Hovy USC Information Sciences Institute 4676 Admiralty Way, Marina del Rey, CA 90292 {skim, hovy}@ISI.EDU Abstract In this paper, we introduce a methodology for analyzing judgment opinions." ></td>
	<td class="line x" title="3:213	We define a judgment opinion as consisting of a valence, a holder, and a topic." ></td>
	<td class="line x" title="4:213	We decompose the task of opinion analysis into four parts: 1) recognizing the opinion; 2) identifying the valence; 3) identifying the holder; and 4) identifying the topic." ></td>
	<td class="line x" title="5:213	In this paper, we address the first three parts and evaluate our methodology using both intrinsic and extrinsic measures." ></td>
	<td class="line x" title="6:213	1 Introduction Recently, many researchers and companies have explored the area of opinion detection and analysis." ></td>
	<td class="line x" title="7:213	With the increased immersion of Internet users has come a proliferation of opinions available on the web." ></td>
	<td class="line x" title="8:213	Not only do we read more opinions from the web, such as in daily news editorials, but also we post more opinions through mechanisms such as governmental web sites, product review sites, news group message boards and personal blogs." ></td>
	<td class="line x" title="9:213	This phenomenon has opened the door for massive opinion collection, which has potential impact on various applications such as public opinion monitoring and product review summary systems." ></td>
	<td class="line o" title="10:213	Although in its infancy, many researchers have worked in various facets of opinion analysis." ></td>
	<td class="line oc" title="11:213	Pang et al.(2002) and Turney (2002) classified sentiment polarity of reviews at the document level." ></td>
	<td class="line x" title="13:213	Wiebe et al.(1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features." ></td>
	<td class="line x" title="15:213	Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process." ></td>
	<td class="line x" title="16:213	Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words." ></td>
	<td class="line o" title="17:213	These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles 1." ></td>
	<td class="line o" title="18:213	These researchers use lists of opinion-bearing clue words and phrases, and then apply various additional techniques and refinements." ></td>
	<td class="line x" title="19:213	Along with many opinion researchers, we participated in a large pilot study, sponsored by NIST, which concluded that it is very difficult to define what an opinion is in general." ></td>
	<td class="line x" title="20:213	Moreover, an expression that is considered as an opinion in one domain might not be an opinion in another." ></td>
	<td class="line x" title="21:213	For example, the statement The screen is very big might be a positive review for a wide screen desktop review, but it could be a mere fact in general newspaper text." ></td>
	<td class="line x" title="22:213	This implies that it is hard to apply opinion bearing words collected from one domain to an application for another domain." ></td>
	<td class="line x" title="23:213	One might therefore need to collect opinion clues within individual domains." ></td>
	<td class="line x" title="24:213	In case we cannot simply find training data from existing sources, such as news article analysis, we need to manually annotate data first." ></td>
	<td class="line x" title="25:213	Most opinions are of two kinds: 1) beliefs about the world, with values such as true, false, possible, unlikely, etc. and 2) judgments about the world, with values such as good, bad, neutral, wise, foolish, virtuous, etc. Statements like I believe that he is smart and Stock prices will rise soon are examples of beliefs whereas I like the new policy on social security and Unfortunately this really was his year: despite a stagnant economy, he still won his re-election are examples of judgment opinions." ></td>
	<td class="line x" title="26:213	However, judgment opinions and beliefs are not necessarily mutually exclusive." ></td>
	<td class="line x" title="27:213	For example, I think it is an outrage or I believe that he is smart carry both a belief and a judgment." ></td>
	<td class="line x" title="28:213	In the NIST pilot study, it was apparent that human annotators often disagreed on whether a belief statement was or was not an opinion." ></td>
	<td class="line x" title="29:213	However, high annotator agreement was seen on judg1 TREC novelty track 2003 and 2004 200 ment opinions." ></td>
	<td class="line x" title="30:213	In this paper, we therefore focus our analysis on judgment opinions only." ></td>
	<td class="line x" title="31:213	We hope that future work yields a more precise definition of belief opinions on which human annotators can agree." ></td>
	<td class="line x" title="32:213	We define a judgment opinion as consisting of three elements: a valence, a holder, and a topic." ></td>
	<td class="line x" title="33:213	The valence, which applies specifically to judgment opinions and not beliefs, is the value of the judgment." ></td>
	<td class="line x" title="34:213	In our framework, we consider the following valences: positive, negative, and neutral." ></td>
	<td class="line x" title="35:213	The holder of an opinion is the person, organization or group whose opinion is expressed." ></td>
	<td class="line x" title="36:213	Finally, the topic is the event or entity about which the opinion is held." ></td>
	<td class="line x" title="37:213	In previous work, Choi et al.(2005) identify opinion holders (sources) using Conditional Random Fields (CRF) and extraction patterns." ></td>
	<td class="line x" title="39:213	They define the opinion holder identification problem as a sequence tagging task: given a sequence of words ( n xxxL 21 ) in a sentence, they generate a sequence of labels ( n yyyL 21 ) indicating whether the word is a holder or not." ></td>
	<td class="line x" title="40:213	However, there are many cases where multiple opinions are expressed in a sentence each with its own holder." ></td>
	<td class="line x" title="41:213	In those cases, finding opinion holders for each individual expression is necessary." ></td>
	<td class="line x" title="42:213	In the corpus they used, 48.5% of the sentences which contain an opinion have more than one opinion expression with multiple opinion holders." ></td>
	<td class="line x" title="43:213	This implies that multiple opinion expressions in a sentence occur significantly often." ></td>
	<td class="line x" title="44:213	A major challenge of our work is therefore not only to focus on sentence with only one opinion, but also to identify opinion holders when there is more than one opinion expressed in a sentence." ></td>
	<td class="line x" title="45:213	For example, consider the sentence In relation to Bushs axis of evil remarks, the German Foreign Minister also said, Allies are not satellites, and the French Foreign Minister caustically criticized that the United States unilateral, simplistic worldview poses a new threat to the world." ></td>
	<td class="line x" title="46:213	Here, the German Foreign Minister should be the holder for the opinion Allies are not satellites and the French Foreign Minister should be the holder for caustically criticized." ></td>
	<td class="line x" title="47:213	In this paper, we introduce a methodology for analyzing judgment opinions." ></td>
	<td class="line x" title="48:213	We decompose the task into four parts: 1) recognizing the opinion; 2) identifying the valence; 3) identifying the holder; and 4) identifying the topic." ></td>
	<td class="line x" title="49:213	For the purposes of this paper, we address the first three parts and leave the last for future work." ></td>
	<td class="line x" title="50:213	Opinions can be extracted from various granularities such as a word, a sentence, a text, or even multiple texts." ></td>
	<td class="line x" title="51:213	Each is important, but we focus our attention on wordlevel opinion detection (Section 2.1) and the detection of opinions in short emails (Section 3)." ></td>
	<td class="line x" title="52:213	We evaluate our methodology using intrinsic and extrinsic measures." ></td>
	<td class="line x" title="53:213	The remainder of the paper is organized as follows." ></td>
	<td class="line x" title="54:213	In the next section, we describe our methodology addressing the three steps described above, and in Section 4 we present our experimental results." ></td>
	<td class="line x" title="55:213	We conclude with a discussion of future work." ></td>
	<td class="line x" title="56:213	2 Analysis of Judgment Opinions In this section, we first describe our methodology for detecting opinion bearing words and for identifying their valence, which is described in Section 2.1." ></td>
	<td class="line x" title="57:213	Then, in Section 2.2, we describe our algorithm for identifying opinion holders." ></td>
	<td class="line x" title="58:213	In Section 3, we show how to use our methodology for detecting opinions in short emails." ></td>
	<td class="line x" title="59:213	2.1 Detecting Opinion-Bearing Words and Identifying Valence We introduce an algorithm to classify a word as being positive, negative, or neutral classes." ></td>
	<td class="line x" title="60:213	This classifier can be used for any set of words of interest and the resulting words with their valence tags can help in developing new applications such as a public opinion monitoring system." ></td>
	<td class="line x" title="61:213	We define an opinion-bearing word as a word that carries a positive or negative sentiment directly such as good, bad, foolish, virtuous, etc. In other words, this is the smallest unit of opinion that can thereafter be used as a clue for sentence-level or text-level opinion detection." ></td>
	<td class="line x" title="62:213	We treat word sentiment classification into Positive, Negative, and Neutral as a three-way classification problem instead of a two-way classification problem of Positive and Negative." ></td>
	<td class="line x" title="63:213	By adding the third class, Neutral, we can prevent the classifier from assigning either positive or negative sentiment to weak opinion-bearing words." ></td>
	<td class="line x" title="64:213	For example, the word central that Hatzivassiloglou and McKeown (1997) included as a positive adjective is not classified as positive in our system." ></td>
	<td class="line x" title="65:213	Instead 201 we mark it as neutral since it is a weak clue for an opinion." ></td>
	<td class="line x" title="66:213	If an unknown word has a strong relationship with the neutral class, we can therefore classify it as neutral even if it has some small connotation of Positive or Negative as well." ></td>
	<td class="line x" title="67:213	Approach: We built a word sentiment classifier using WordNet and three sets of positive, negative, and neutral words tagged by hand." ></td>
	<td class="line x" title="68:213	Our insight is that synonyms of positive words tend to have positive sentiment." ></td>
	<td class="line x" title="69:213	We expanded those manually selected seed words of each sentiment class by collecting synonyms from WordNet." ></td>
	<td class="line x" title="70:213	However, we cannot simply assume that all the synonyms of positive words are positive since most words could have synonym relationships with all three sentiment classes." ></td>
	<td class="line x" title="71:213	This requires us to calculate the closeness of a given word to each category and determine the most probable class." ></td>
	<td class="line x" title="72:213	The following formula describes our model for determining the category of a word: (1) ),|(maxarg)|(maxarg 21 n cc synsynsyncPwcP  where c is a category (Positive, Negative, or Neutral) and w is a given word; syn n is a WordNet synonym of the word w. We calculate this closeness as follows; (2) )|()(maxarg )|()(maxarg )|()(maxarg)|(maxarg 1 ))(,( 3 2 1  = = = = m k wsynsetfcount k c n c cc k cfPcP csynsynsynsynPcP cwPcPwcP where k f is the k th feature of class c which is also a member of the synonym set of the given word w. count(f k,synset(w)) is the total number of occurrences of the word feature f k in the synonym set of word w. In section 4.1, we describe our manually annotated dataset which we used for seed words and for our evaluation." ></td>
	<td class="line x" title="73:213	2.2 Identifying Opinion Holders Despite successes in identifying opinion expressions and subjective words/phrases (See Section 1), there has been less achievement on the factors closely related to subjectivity and polarity, such as identifying the opinion holder." ></td>
	<td class="line x" title="74:213	However, our research indicates that without this information, it is difficult, if not impossible, to define opinion accurately enough to obtain reasonable interannotator agreement." ></td>
	<td class="line x" title="75:213	Since these factors co-occur and mutually reinforce each other, the question Who is the holder of this opinion? is as important as Is this an opinion? or What kind of opinion is expressed here?." ></td>
	<td class="line x" title="76:213	In this section, we describe the automated identification for opinion holders." ></td>
	<td class="line x" title="77:213	We define an opinion holder as an entity (person, organization, country, or special group of people) who expresses explicitly or implicitly the opinion contained in the sentence." ></td>
	<td class="line x" title="78:213	Previous work that is related to opinion holder identification is (Bethard et al. 2004) who identify opinion propositions and holders." ></td>
	<td class="line x" title="79:213	However, their opinion is restricted to propositional opinion and mostly to verbs." ></td>
	<td class="line x" title="80:213	Another related work is (Choi et al. 2005) who use the MPQA corpus 2 to learn patterns of opinion sources using a graphical model and extraction pattern learning." ></td>
	<td class="line x" title="81:213	However, they have a different task definition from ours." ></td>
	<td class="line x" title="82:213	They define the task as identifying opinion sources (holders) given a sentence, whereas we define it as identifying opinion sources given an opinion expression in a sentence." ></td>
	<td class="line x" title="83:213	We discussed their work in Section 1." ></td>
	<td class="line x" title="84:213	Data: As training data, we used the MPQA corpus (Wilson and Wiebe, 2003), which contains news articles manually annotated by 5 trained annotators." ></td>
	<td class="line x" title="85:213	They annotated 10657 sentences from 535 documents, in four different aspects: agent, expressive-subjectivity, on, and inside." ></td>
	<td class="line x" title="86:213	Expressivesubjectivity marks words and phrases that indirectly express a private state that is defined as a term for opinions, evaluations, emotions, and speculations." ></td>
	<td class="line x" title="87:213	The on annotation is used to mark speech events and direct expressions of private states." ></td>
	<td class="line x" title="88:213	As for the holder, we use the agent of the selected private states or speech events." ></td>
	<td class="line x" title="89:213	While there are many possible ways to define what opinion means, intuitively, given an opinion, it is clear what the opinion holder means." ></td>
	<td class="line x" title="90:213	Table 1 shows an example of the annotation." ></td>
	<td class="line x" title="91:213	In this example, we consider the expression the U.S. government is the source of evil in the world with an expres2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ Sentence Iraqi Vice President Taha Yassin Ramadan, responding to Bushs axis of evil remark, said the U.S. government is the source of evil in the world." ></td>
	<td class="line x" title="92:213	Expressive subjectivity the U.S. government is the source of evil in the world Strength Extreme Source Iraqi Vice President Taha Yassin Ramadan Table 1: Annotation example 202 sive-subjectivity tag as an opinion of the holder Iraqi Vice President Taha Yassin Ramadan." ></td>
	<td class="line x" title="93:213	Approach: Since more than one opinion may be expressed in a sentence, we have to find an opinion holder for each opinion expression." ></td>
	<td class="line x" title="94:213	For example, in a sentence A thinks Bs criticism of T is wrong, B is the holder of the criticism of T, whereas A is the person who has an opinion that Bs criticism is wrong." ></td>
	<td class="line x" title="95:213	Therefore, we define our task as finding an opinion holder, given an opinion expression." ></td>
	<td class="line x" title="96:213	Our earlier work (ref suppressed) focused on identifying opinion expressions within text." ></td>
	<td class="line x" title="97:213	We employ that system in tandem with the one described here." ></td>
	<td class="line x" title="98:213	To learn opinion holders automatically, we use a Maximum Entropy model." ></td>
	<td class="line x" title="99:213	Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible (Berger et al. 1996)." ></td>
	<td class="line x" title="100:213	There are two ways to model the problem with ME: classification and ranking." ></td>
	<td class="line x" title="101:213	Classification allocates each holder candidate to one of a set of predefined classes while ranking selects a single candidate as answer." ></td>
	<td class="line x" title="102:213	This means that classification modeling 3 can select many candidates as answers as long as they are marked as true, and does not select any candidate if every one is marked as false." ></td>
	<td class="line x" title="103:213	In contrast, ranking always selects the most probable candidate as an answer, which suits our task better." ></td>
	<td class="line x" title="104:213	Our earlier experiments showed poor performance with classification modeling, an experience also reported for Question Answering (Ravichandran et al. 2003)." ></td>
	<td class="line x" title="105:213	We modeled the problem to choose the most probable candidate that maximizes a given conditional probability distribution, given a set of holder candidates h 1 h 2 h N and opinion expression e. The conditional probability P h h 1 h 2 h N,e can be calculated based on K feature functions f k h, h 1 h 2 h N,e . We write a decision rule for the ranking as follows: {} {}]e),hhh(h,f[= e)],hhh|[P(hh K =k Nkk h N h  = 1 21 21 argmax argmax Each k  is a model parameter indicating the weight of its feature function." ></td>
	<td class="line x" title="106:213	3 In our task, there are two classes: holder and non-holder." ></td>
	<td class="line x" title="107:213	Figure 1 illustrates our holder identification system." ></td>
	<td class="line x" title="108:213	First, the system generates all possible holder candidates, given a sentence and an opinion expression <E>." ></td>
	<td class="line x" title="109:213	After parsing the sentence, it extracts features such as the syntactic path information between each candidate <H> and the expression <E> and a distance between <H> and <E>." ></td>
	<td class="line x" title="110:213	Then it ranks holder candidates according to the score obtained by the ME ranking model." ></td>
	<td class="line x" title="111:213	Finally the system picks the candidate with the highest score." ></td>
	<td class="line x" title="112:213	Below, we describe in turn how to select holder candidates and how to select features for the training model." ></td>
	<td class="line x" title="113:213	Holder Candidate Selection: Intuitively, one would expect most opinion holders to be named entities (PERSON or ORGANIZATION) 4 . However, other common noun phrases can often be opinion holders, such as the leader, three nations, and the Arab and Islamic world." ></td>
	<td class="line x" title="114:213	Sometimes, pronouns like he, she, and they that refer to a PERSON, or it that refers to an ORGANIZATION or country, can be an opinion holder." ></td>
	<td class="line x" title="115:213	In our study, we consider all noun phrases, including common noun phrases, named entities, and pronouns, as holder candidates." ></td>
	<td class="line x" title="116:213	Feature Selection: Our hypothesis is that there exists a structural relation between a holder <H> and an expression <E> that can help to identify opinion holders." ></td>
	<td class="line x" title="117:213	This relation may be represented by lexical-level patterns between <H> and <E>, but anchoring on surface words might run into the data sparseness problem." ></td>
	<td class="line x" title="118:213	For example, if we see the lexical pattern <H> recently criticized <E> in the training data, it is impossible to match the expression <H> yesterday condemned <E>." ></td>
	<td class="line x" title="119:213	These, however, have the same syntactic features in our 4 We use BBNs named entity tagger IdentiFinder to collect named entities." ></td>
	<td class="line x" title="120:213	Sentence : w1 w2 w3 w4 w5 w6 w7 w8 w9  wn Opinion expression <E> : w6 w7 w8  w2  w4 w5 w6 w7 w8  w11 w12 w13  w18  w23 w24 w25  C1 C2 <E> C3 C4 C5 given Candidate holder selection Feature extraction: Parsing C1 C2 <E> C3 C4 C5 Rank the candidates by ME model 1.C1 2." ></td>
	<td class="line x" title="121:213	C5 3.C3 4.C2 5.C4 Pick the best candidate as a holder C1 Figure 1: Overall system architecture 203 model." ></td>
	<td class="line x" title="122:213	We therefore selected structural features from a deep parse, using the Charniak parser." ></td>
	<td class="line x" title="123:213	After parsing the sentence, we search for the lowest common parent node of the words in <H> and <E> respectively (<H> and <E> are mostly expressed with multiple words)." ></td>
	<td class="line x" title="124:213	A lowest common parent node is a non-terminal node in a parse tree that covers all the words in <H> and <E>." ></td>
	<td class="line x" title="125:213	Figure 2 shows a parsed example of a sentence with the holder Chinas official Xinhua news agency and the opinion expression accusing." ></td>
	<td class="line x" title="126:213	In this example, the lowest common parent of words in <H> is the bold NP and the lowest common parent of <E> is the bold VBG." ></td>
	<td class="line x" title="127:213	We name these nodes Hhead and Ehead respectively." ></td>
	<td class="line x" title="128:213	After finding these nodes, we label them by subscript (e.g. , NP H and VBG E ) to indicate they cover <H> and <E>." ></td>
	<td class="line x" title="129:213	In order to see how Hhead and Ehead are related to each other in the parse tree, we define another node, HEhead, which covers both Hhead and Ehead." ></td>
	<td class="line x" title="130:213	In the example, HEhead is S at the top of the parse tree since it covers both NP H and VBG E . We also label S by subscript as S HE . To express tree structure for ME training, we extract path information between <H> and <E>." ></td>
	<td class="line x" title="131:213	In the example, the complete path from Hhead to Ehead is <H> NP S VP S S VP VBG <E>." ></td>
	<td class="line x" title="132:213	However, representing each complete path as a single feature produces so many different paths with low frequencies that the ME system would learn poorly." ></td>
	<td class="line x" title="133:213	Therefore, we split the path into three parts: HEpath, Hpath an Epath." ></td>
	<td class="line x" title="134:213	HEpath is defined as a path from HEhead to its left and right child nodes that are also parents of Hhead and Ehead." ></td>
	<td class="line x" title="135:213	Hpath is a path from Hhead and one of its ancestor nodes that is a child of HEhead." ></td>
	<td class="line x" title="136:213	Similarly, Epath is defined as a path from Ehead to one of its ancestors that is also a child of HEhead." ></td>
	<td class="line x" title="137:213	With this splitting, the system can work when any of HEpath, Hpath or Epath appeared in the training data, even if the entire path from <H> to <E> is unseen." ></td>
	<td class="line x" title="138:213	Table 2 summarizes these concepts with two holder candidate examples in the parse tree of Figure 2." ></td>
	<td class="line x" title="139:213	We also include two non-structural features." ></td>
	<td class="line x" title="140:213	The first is the type of the candidate, with values NP, PERSON, ORGANIZATION, and LOCATION." ></td>
	<td class="line x" title="141:213	The second feature is the distance between <H> and <E>, counted in parse tree words." ></td>
	<td class="line x" title="142:213	This is motivated by the intuition that holder candidates tend to lie closer to their opinion expression." ></td>
	<td class="line x" title="143:213	All features are listed in Table 3." ></td>
	<td class="line x" title="144:213	We describe the performance of the system in Section 4." ></td>
	<td class="line x" title="145:213	Candidate 1 Candidate 2 Chinas official Xinuhua news agency Bush Hhead NP H NNP H Ehead VBG E VBG E HEhead S HE VP HE Hpath NP H NNP H NP H NP H NP H PP H Epath VBG E VP E S E S E VP E VBG E VP E S E S E HEpath S HE NP H VP E VP HE PP H S E Table 2: Heads and paths for the Figure 2 example Features Description F1 Type of <H> F2 HEpath F3 Hpath F4 Epath F5 Distance between <H> and <E> Table 3: Features for ME training NP ADVP VP S . NP JJ NNP NNNN NNP POS RB VBD PP PP, NPIN NNP NPIN PPNP NNNP NPIN S S official China s Xinhua news agency also weighed in sunday on NNP POS choice Bush s of NNS words VP VBG PP NP accusing the DT NN IN S president of VP VBG NP PP orchestrating public opinion JJ NN In advance of possible strikes against the three countries in an expansion of the war against terrorism Figure 2: A parsing example 204 Model 1  Translate a German email to English  Apply English opinion-bearing words Model 2  Translate English opinion-bearing words to German  Analyze a German email using the German opinion-bearing words." ></td>
	<td class="line x" title="146:213	Table 4: Two models of German Email opinion analysis system 3 Applying our Methodology to German Emails In this section, we describe a German email analysis system into which we included the opinionbearing words from Section 2.1 to detect opinions expressed in emails." ></td>
	<td class="line x" title="147:213	This system is part of a collaboration with the EU-funded project QUALEG (Quality of Service and Legitimacy in eGovernment) which aims at enabling local governments to manage their policies in a transparent and trustable way 5 . For this purpose, local governments should be able to measure the performance of the services they offer, by assessing the satisfaction of its citizens." ></td>
	<td class="line x" title="148:213	This need makes a system that can monitor and analyze citizens emails essential." ></td>
	<td class="line x" title="149:213	The goal of our system is to classify emails as neutral or as bearing a positive or negative opinion." ></td>
	<td class="line x" title="150:213	To generate opinion bearing words, we ran the word sentiment classifier from Section 2.1 on 8011 verbs to classify them into 807 positive, 785 negative, and 6149 neutral." ></td>
	<td class="line x" title="151:213	For 19748 adjectives, the system classified them into 3254 positive, 303 negative, and 16191 neutral." ></td>
	<td class="line x" title="152:213	Since our opinionbearing words are in English and our target system is in German, we also applied a statistical word alignment technique, GIZA++ 6 (Och and Ney 2000)." ></td>
	<td class="line x" title="153:213	Running it on version two of the European Parliament corpus, we obtained statistics for 678,340 German-English word pairs and 577,362 English-German word pairs." ></td>
	<td class="line x" title="154:213	Obtaining these two lists of translation pairs allows us to convert English words to German, and German to English, without a full document translation system." ></td>
	<td class="line x" title="155:213	To utilize our English opinion-bearing words in a German opinion analysis system, we developed two models, 5 http://www.qualeg.eupm.net/my_spip/index.php 6 http://www.fjoch.com/GIZA++.html outlined in Table 4, each of which is triggered at different points in the system." ></td>
	<td class="line x" title="156:213	In both models, however, we still need to decide how to apply opinion-bearing words as clues to determine the sentiment of a whole email." ></td>
	<td class="line x" title="157:213	Our previous work on sentence level sentiment classification (ref suppressed) shows that the presence of any negative words is a reasonable indication of a negative sentence." ></td>
	<td class="line x" title="158:213	Since our emails are mostly short (the average number of words in each email is 19.2) and we avoided collecting weak negative opinion clue words, we hypothesize that our previous sentence sentiment classification study works on the email sentiment analysis." ></td>
	<td class="line x" title="159:213	This implies that an email is negative if it contains more than certain number of strong negative words." ></td>
	<td class="line x" title="160:213	We tune this parameter using our training data." ></td>
	<td class="line x" title="161:213	Conversely, if an email contains mostly positive opinion-bearing words, we classify it as a positive email." ></td>
	<td class="line x" title="162:213	We assign neutral if an email does not contain any strong opinion-bearing words." ></td>
	<td class="line x" title="163:213	Manually annotated email data was provided by our joint research site." ></td>
	<td class="line x" title="164:213	This data contains 71 emails from citizens regarding a German festival." ></td>
	<td class="line x" title="165:213	26 of them contained negative complaints, for example, the lack of parking space, and 24 of them were positive with complimentary comments to the organization." ></td>
	<td class="line x" title="166:213	The rest of them were marked as questions such as how to buy festival tickets, only text of simple comments, fuzzy, and difficult." ></td>
	<td class="line x" title="167:213	So, we carried system experiments on positive and negative emails with precision and recall." ></td>
	<td class="line x" title="168:213	We report system results in Section 4." ></td>
	<td class="line x" title="169:213	4 Experiment Results In this section, we evaluate the three systems described in Sections 2 and 3: detecting opinionbearing words and identifying valence, identifying opinion holders, and the German email opinion analysis system." ></td>
	<td class="line x" title="170:213	4.1 Detecting Opinion-bearing Words We described a word classification system to detect opinion-bearing words in Section 2.1." ></td>
	<td class="line x" title="171:213	To examine its effectiveness, we annotated 2011 verbs and 1860 adjectives, which served as a gold standard 7 . These words were randomly selected from a 7 Although nouns and adverbs may also be opinion-bearing, we focus only on verbs and adjectives for this study." ></td>
	<td class="line x" title="172:213	205 collection of 8011 English verbs and 19748 English adjectives." ></td>
	<td class="line x" title="173:213	We use training data as seed words for the WordNet expansion part of our algorithm (described in Section 2.1)." ></td>
	<td class="line x" title="174:213	Table 5 shows the distribution of each semantic class." ></td>
	<td class="line x" title="175:213	In both verb and adjective annotation, neutral class has much more words than the positive or negative classes." ></td>
	<td class="line x" title="176:213	We measured the precision, recall, and F-score of our system using 10-fold cross validation." ></td>
	<td class="line x" title="177:213	Table 6 shows the results with 95% confidence bounds." ></td>
	<td class="line x" title="178:213	Overall (combining positive, neutral and negative), our system achieved 77.7%  1.2% accuracy on verbs and 69.1%  2.1% accuracy on adjectives." ></td>
	<td class="line x" title="179:213	The system has very high precision in the neutral category for both verbs (97.2%) and adjectives (89.5%), which we interpret to mean that our system is really good at filtering non-opinion bearing words." ></td>
	<td class="line x" title="180:213	Recall is high in all cases but precision varies; very high for neutral and relatively high for negative but low for positive." ></td>
	<td class="line x" title="181:213	4.2 Opinion Holder Identification We conducted experiments on 2822 <sentence; opinion expression; holder> triples and divided the data set into 10 <training; test> sets for cross validation." ></td>
	<td class="line x" title="182:213	For evaluation, we consider to match either fully or partially with the holder marked in the test data." ></td>
	<td class="line x" title="183:213	The holder matches fully if it is a single entity (e.g. , Bush)." ></td>
	<td class="line x" title="184:213	The holder matches partially when it is part of the multiple entities that make up the marked holder." ></td>
	<td class="line x" title="185:213	For example, given a marked holder Michel Sidibe, Director of the Country and Regional Support Department of UNAIDS, we consider both Michel Sidibe and Director of the Country and Regional Support Department of UNAIDS as acceptable answers." ></td>
	<td class="line x" title="186:213	Our experiments consist of two parts based on the candidate selection method." ></td>
	<td class="line x" title="187:213	Besides the selection method we described in Section 2.2, we also conducted a separate experiment by excluding pronouns from the candidate list." ></td>
	<td class="line x" title="188:213	With the second method, the system always produces a nonpronoun holder as an answer." ></td>
	<td class="line x" title="189:213	This selection method is useful in some Information Extraction application that only cares non-pronoun holders." ></td>
	<td class="line x" title="190:213	We report accuracy (the percentage of correct answers the system found in the test set) to evaluate our system." ></td>
	<td class="line x" title="191:213	We also report how many correct answers were found within the top2 and top3 system answers." ></td>
	<td class="line x" title="192:213	Tables 7 and 8 show the system accuracy with and without considering pronouns as alias candidates, respectively." ></td>
	<td class="line x" title="193:213	Table 8 mostly shows lower accuracies than Table 7 because test data often has only a non-pronoun entity as a holder and the system picks a pronoun as its answer." ></td>
	<td class="line x" title="194:213	Even if the pronoun refers the same entity marked in the test data, the evaluation system counts it as wrong because it does not match the hand annotated holder." ></td>
	<td class="line x" title="195:213	To evaluate the effectiveness of our system, we set the baseline as a system choosing the closest candidate to the expression as a holder without the Maximum Entropy decision." ></td>
	<td class="line x" title="196:213	The baseline system had an accuracy of only 21.3% for candidate selection over all noun phrases and 23.2% for candidate selection excluding pronouns." ></td>
	<td class="line x" title="197:213	The results show that detecting opinion holders is a hard problem, but adopting syntactic features (F2, F3, and F4) helps to improve the system." ></td>
	<td class="line x" title="198:213	A promising avenue of future work is to investigate the use of semantic features to eliminate noun Positive Negative Neutral Total Verb 69 151 1791 2011 Adjective 199 304 1357 1860 Table 5: Word distribution in our gold standard Precision Recall F-score V 20.5%  3.5% 82.4%  7.5% 32.3%  4.6% P A 32.4%  3.8% 75.5%  6.1% 45.1%  4.4% V 97.2%  0.6% 77.6%  1.4% 86.3%  0.7% X A 89.5%  1.7% 67.1%  2.7% 76.6%  2.1% V 37.8%  4.9% 76.2%  8.0% 50.1%  5.6% N A 60.0%  4.1% 78.5%  4.9% 67.8%  3.8% Table 6: Precision, recall, and F-score on word valence categorization for Positive (P), Negative (N) and Neutral (X) verbs (V) and adjectives (A) (with 95% confidence intervals) Baseline F5 F15 F234 F12345 Top1 23.2% 21.8% 41.6% 50.8% 52.7% Top2 39.7% 61.9% 66.3% 67.9% Top3 52.2% 72.5% 77.1% 77.8% Table 7: Opinion holder identification results (excluding pronouns from candidates) Baseline F5 F15 F234 F12345 Top1 21.3% 18.9% 41.8% 47.9% 50.6% Top2 37.9% 61.6% 64.8% 66.7% Top3 51.2% 72.3% 75.3% 76.0% Table 8: Opinion holder identification results (All noun phrases as candidates) 206 phrases such as cheap energy subsidies or possible strikes from the candidate set before we run our ME model, since they are less likely to be an opinion holder than noun phrases like three nations or Palestine people. 4.3 German Emails For our experiment, we performed 7-fold cross validation on a set of 71 emails." ></td>
	<td class="line x" title="199:213	Table 9 shows the average precision, recall, and F-score." ></td>
	<td class="line x" title="200:213	Results show that our system identifies negative emails (complaints) better than praise." ></td>
	<td class="line x" title="201:213	When we chose a system parameter for the focus, we intended to find negative emails rather than positive emails because officials who receive these emails need to act to solve problems when people complain but they have less need to react to compliments." ></td>
	<td class="line x" title="202:213	By highlighting high recall of negative emails, we may misclassify a neutral email as negative but there is also less chance to neglect complaints." ></td>
	<td class="line x" title="203:213	Category Model1 Model2 Precision 0.72 0.55 Recall 0.40 0.65 Positive (P) F-score 0.51 0.60 Precision 0.55 0.61 Recall 0.80 0.42 Negative (N) F-score 0.65 0.50 Table 9: German email opinion analysis system results 5 Conclusion and Future Work In this paper, we presented a methodology for analyzing judgment opinions, which we define as opinions consisting of a valence, a holder, and a topic." ></td>
	<td class="line x" title="204:213	We presented models for recognizing sentences containing judgment opinions, identifying the valence of the opinion, and identifying the holder of the opinion." ></td>
	<td class="line x" title="205:213	Remaining is to also finally identify the topic of the opinion." ></td>
	<td class="line x" title="206:213	Past tests with human annotators indicate that the accuracy of identifying valence, holder and topic is much increased when all three are being done simultaneously." ></td>
	<td class="line x" title="207:213	We plan to investigate a joint model to verify this intuition." ></td>
	<td class="line x" title="208:213	Our past work indicated that, for newspaper texts, it is feasible for annotators to identify judgment opinion sentences and for them to identify their holders and judgment valences." ></td>
	<td class="line x" title="209:213	It is encouraging to see that we achieved good results on a new genre  emails sent from citizens to a city counsel  and in a new language, German." ></td>
	<td class="line x" title="210:213	This paper presents a computational framework for analyzing judgment opinions." ></td>
	<td class="line x" title="211:213	Even though these are the most common opinions, it is a pity that the research community remains unable to define belief opinions (i.e. , those opinions that have values such as true, false, possible, unlikely, etc)." ></td>
	<td class="line x" title="212:213	with high enough inter-annotator agreement." ></td>
	<td class="line x" title="213:213	Only once we properly define belief opinion will we be capable of building a complete opinion analysis system." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1034
Learning To Generate Naturalistic Utterances Using Reviews In Spoken Dialogue Systems
Higashinaka, Ryuichiro;Prasad, Rashmi;Walker, Marilyn A.;"></td>
	<td class="line x" title="1:231	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 265272, Sydney, July 2006." ></td>
	<td class="line x" title="2:231	c2006 Association for Computational Linguistics Learning to Generate Naturalistic Utterances Using Reviews in Spoken Dialogue Systems Ryuichiro Higashinaka NTT Corporation rh@cslab.kecl.ntt.co.jp Rashmi Prasad University of Pennsylvania rjprasad@linc.cis.upenn.edu Marilyn A. Walker University of Sheffield walker@dcs.shef.ac.uk Abstract Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts." ></td>
	<td class="line x" title="3:231	Dictionary creation is a costly process; it is currently done by hand for each dialogue domain." ></td>
	<td class="line x" title="4:231	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain, and test it on restaurant reviews." ></td>
	<td class="line x" title="5:231	We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision." ></td>
	<td class="line x" title="6:231	Experimental analyses show that the mappings learned cover most of the domain ontology, and provide good linguistic variation." ></td>
	<td class="line x" title="7:231	A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline." ></td>
	<td class="line x" title="8:231	1 Introduction One obstacle to the widespread deployment of spoken dialogue systems is the cost involved withhand-craftingthespoken languagegeneration module." ></td>
	<td class="line x" title="9:231	Spoken language generation requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts." ></td>
	<td class="line x" title="10:231	Dictionary creation is a costly process: an automatic method for creating them would make dialogue technology more scalable." ></td>
	<td class="line x" title="11:231	A secondary benefit is that a learned dictionary may produce more natural and colloquial utterances." ></td>
	<td class="line x" title="12:231	We propose a novel method for mining user reviews to automatically acquire a domain specific generation dictionary for information presentation in a dialogue system." ></td>
	<td class="line x" title="13:231	Our hypothesis is that reviews that provide individual ratings for various distinguished attributes of review entities can be used to map review sentences to a semantic repAn example user review (we8there.com) Ratings Food=5, Service=5, Atmosphere=5, Value=5, Overall=5 Review comment The best Spanish food in New York." ></td>
	<td class="line x" title="14:231	I am from Spain and I had my 28th birthday there and we all had a great time." ></td>
	<td class="line x" title="15:231	Salud!" ></td>
	<td class="line x" title="16:231	 Review commentafter named entity recognition The best {NE=foodtype, string=Spanish}{NE=food, string=food, rating=5} in {NE=location, string=New York}  Mappingbetweena semantic representation(a set of relations) and a syntactic structure(DSyntS)  Relations: RESTAURANT has FOODTYPE RESTAURANT has foodquality=5 RESTAURANT has LOCATION ([foodtype,food=5, location] for shorthand.)" ></td>
	<td class="line x" title="17:231	 DSyntS:                     lexeme : food class : common noun number : sg article : def ATTR bracketleftBig lexeme : best class : adjective bracketrightBig ATTR   lexeme : FOODTYPE class : common noun number : sg article : no-art   ATTR      lexeme : in class : preposition II   lexeme : LOCATION class : proper noun number : sg article : no-art                            Figure 1: Example of procedure for acquiring a generation dictionary mapping." ></td>
	<td class="line x" title="18:231	resentation." ></td>
	<td class="line x" title="19:231	Figure 1 shows a user review in the restaurant domain, where we hypothesize that the user rating food=5 indicates that the semantic representation for the sentence The best Spanish food in New York includes the relation RESTAURANT has foodquality=5. We apply the method to extract 451 mappings from restaurant reviews." ></td>
	<td class="line x" title="20:231	Experimental analyses show that the mappings learned cover most of the domainontology,and providegoodlinguisticvariation." ></td>
	<td class="line x" title="21:231	A subjective user evaluation indicates that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is significantly higher than a hand-crafted baseline." ></td>
	<td class="line x" title="22:231	265 Section 2 provides a step-by-step description of the method." ></td>
	<td class="line x" title="23:231	Sections 3 and 4 present the evaluation results." ></td>
	<td class="line x" title="24:231	Section 5 covers related work." ></td>
	<td class="line x" title="25:231	Section 6 summarizes and discusses future work." ></td>
	<td class="line x" title="26:231	2 Learning a Generation Dictionary Our automatically created generation dictionary consists of triples (U,R,S) representing a mapping between the original utterance U in the user review, its semantic representation R(U), and its syntactic structure S(U)." ></td>
	<td class="line x" title="27:231	Although templates are widely used in many practicalsystems (Seneff and Polifroni, 2000; Theune, 2003), we derive syntactic structures to represent the potential realizations, in order to allow aggregation, and other syntactic transformations of utterances, as well as contextspecificprosodyassignment(Walkeretal., 2003; Moore et al. , 2004)." ></td>
	<td class="line x" title="29:231	The method is outlined briefly in Fig." ></td>
	<td class="line x" title="30:231	1 and described below." ></td>
	<td class="line x" title="31:231	It comprises the following steps: 1." ></td>
	<td class="line x" title="32:231	Collect user reviews on the web to create a population of utterances U. 2." ></td>
	<td class="line x" title="33:231	To derive semantic representations R(U):  Identify distinguished attributes and construct a domain ontology;  Specify lexicalizations of attributes;  Scrape webpages structured data for named-entities;  Tag named-entities." ></td>
	<td class="line x" title="34:231	3." ></td>
	<td class="line x" title="35:231	Derive syntactic representations S(U)." ></td>
	<td class="line x" title="36:231	4." ></td>
	<td class="line x" title="37:231	Filter inappropriate mappings." ></td>
	<td class="line x" title="38:231	5." ></td>
	<td class="line x" title="39:231	Add mappings (U,R,S) to dictionary." ></td>
	<td class="line x" title="40:231	2.1 Creating the corpus We created a corpus of restaurant reviews by scraping 3,004 user reviews of 1,810 restaurants posted at we8there.com (http://www.we8there.com/), where each individual review includes a 1-to-5 Likert-scale rating of different restaurant attributes." ></td>
	<td class="line x" title="41:231	The corpus consists of 18,466 sentences." ></td>
	<td class="line x" title="42:231	2.2 Deriving semantic representations The distinguished attributes are extracted from the webpages for each restaurant entity." ></td>
	<td class="line x" title="43:231	They include attributes that the users are asked to rate, i.e. food, service, atmosphere, value,andoverall, which have scalar values." ></td>
	<td class="line x" title="44:231	In addition, other attributes are extracted from the webpage, such as the name, foodtype and location of the restaurant, which have categorical values." ></td>
	<td class="line x" title="45:231	The name attribute is assumed to correspond to the restaurant entity." ></td>
	<td class="line x" title="46:231	Given the distinguished attributes, a Dist. Attr." ></td>
	<td class="line x" title="47:231	Lexicalization food food, meal service service, staff, waitstaff, wait staff, server, waiter, waitress atmosphere atmosphere, decor, ambience, decoration value value, price, overprice, pricey, expensive, inexpensive, cheap, affordable, afford overall recommend, place, experience, establishment Table 1: Lexicalizations for distinguished attributes." ></td>
	<td class="line x" title="48:231	simple domain ontology can be automatically derived by assuming that a meronymy relation, represented by the predicate has, holds between the entity type (RESTAURANT) and the distinguished attributes." ></td>
	<td class="line x" title="49:231	Thus, the domain ontology consists of the relations:                RESTAURANT has foodquality RESTAURANT has servicequality RESTAURANT has valuequality RESTAURANT has atmospherequality RESTAURANT has overallquality RESTAURANT has foodtype RESTAURANT has location We assume that, although users may discuss other attributes of the entity, at least some of the utterancesin the reviewsrealizethe relations specified in the ontology." ></td>
	<td class="line x" title="50:231	Our problem then is to identify these utterances." ></td>
	<td class="line x" title="51:231	We test the hypothesis that, if an utterance U contains named-entities corresponding to the distinguished attributes, thatR for that utterance includes the relation concerning that attribute in the domain ontology." ></td>
	<td class="line x" title="52:231	We define named-entities for lexicalizations of the distinguished attributes, starting with the seed word for that attribute on the webpage (Table 1)." ></td>
	<td class="line x" title="53:231	1 Fornamed-entityrecognition, weuseGATE(Cunningham et al. , 2002), augmented with namedentity lists for locations, food types, restaurant names, and food subtypes (e.g. pizza), scraped from the we8there webpages." ></td>
	<td class="line x" title="54:231	We also hypothesizethat the rating givenfor the distinguished attribute specifies the scalar value of the relation." ></td>
	<td class="line x" title="55:231	For example, a sentence containing food or meal is assumed to realize the relation RESTAURANT has foodquality.,andthe value of the foodquality attribute is assumed to be the value specified in the user rating for that attribute, e.g. RESTAURANT has foodquality = 5 in Fig." ></td>
	<td class="line x" title="56:231	1." ></td>
	<td class="line x" title="57:231	Similarly, the other relations in Fig." ></td>
	<td class="line x" title="58:231	1 are assumed to be realized by the utterance The best Spanish food in New York because it contains 1 In future, we will investigate other techniques for bootstrapping these lexicalizations from the seed word on the webpage." ></td>
	<td class="line x" title="59:231	266 filter filtered retained No Relations Filter 7,947 10,519 Other Relations Filter 5,351 5,168 Contextual Filter 2,973 2,195 Unknown Words Filter 1,467 728 Parsing Filter 216 512 Table 2: Filtering statistics: the number of sentences filtered and retained by each filter." ></td>
	<td class="line x" title="60:231	one FOODTYPE named-entity and one LOCATION named-entity." ></td>
	<td class="line x" title="61:231	Values of categorical attributes are replaced by variables representing their type before the learned mappings are added to the dictionary, as shown in Fig." ></td>
	<td class="line x" title="62:231	1." ></td>
	<td class="line x" title="63:231	2.3 Parsing and DSyntS conversion We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can be realized by the fast portable realizer RealPro (Lavoie and Rambow, 1997)." ></td>
	<td class="line x" title="64:231	Since DSyntSs are a type of dependency structure, we first process the sentences with Minipar (Lin, 1998), and then convert Minipars representation into DSyntS." ></td>
	<td class="line x" title="65:231	Since user reviews are different from the newspaper articles on which Minipar was trained, the output of Minipar can be inaccurate, leading to failure in conversion." ></td>
	<td class="line x" title="66:231	We check whether conversion is successful in the filtering stage." ></td>
	<td class="line x" title="67:231	2.4 Filtering The goal of filtering is to identify U that realize the distinguished attributes and to guarantee high precision for the learned mappings." ></td>
	<td class="line x" title="68:231	Recall is less important since systems need to convey requested information as accurately as possible." ></td>
	<td class="line x" title="69:231	Our procedurefor derivingsemanticrepresentationsis based on the hypothesisthat ifU containsnamed-entities that realizethe distinguishedattributes, thatRwill include the relevant relation in the domain ontology." ></td>
	<td class="line x" title="70:231	We also assume that if U contains namedentities that are not covered by the domain ontology, or words indicating that the meaning of U depends on the surrounding context, that R will not completely characterizesthe meaning ofU,andso U should be eliminated." ></td>
	<td class="line x" title="71:231	We also require an accurate S for U. Therefore, the filters described below eliminate U that (1) realize semantic relations not in the ontology; (2) contain words indicating that its meaning depends on the context; (3) contain unknown words; or (4) cannot be parsed accurately." ></td>
	<td class="line x" title="72:231	No Relations Filter: The sentence does not contain any named-entities for the distinguished attributes." ></td>
	<td class="line x" title="73:231	Other Relations Filter: The sentence contains named-entities for food subtypes, person Rating Dist.Attr." ></td>
	<td class="line x" title="74:231	1234 5Total food 5 8 6 18 57 94 service 15 3 6 17 56 97 atmosphere 033831 45 value 001812 21 overall 3 2 5 15 45 70 Total 23 15 21 64 201 327 Table 3: Domain coverage of single scalar-valued relation mappings." ></td>
	<td class="line x" title="75:231	names, country names, dates (e.g. , today, tomorrow, Aug. 26th) or prices (e.g. , 12 dollars), or POS tag CD for numerals." ></td>
	<td class="line x" title="76:231	These indicate relations not in the ontology." ></td>
	<td class="line x" title="77:231	Contextual Filter: The sentence contains indexicals such as I, you, that or cohesive markers of rhetorical relations that connect it to some part of the preceding text, which means that the sentence cannot be interpreted out of context." ></td>
	<td class="line x" title="78:231	These include discourse markers, such as list item markers with LS as the POS tag, that signal the organization structure of the text (Hirschberg and Litman, 1987), as well as discourse connectives that signal semantic and pragmatic relations of the sentence with other parts of the text (Knott, 1996), such as coordinatingconjunctions at the beginning of the utterance like and and but etc. , and conjunct adverbs such as however, also, then." ></td>
	<td class="line x" title="79:231	Unknown Words Filter: The sentence contains words not in WordNet (Fellbaum, 1998) (which includes typographical errors), or POS tags contain NN (Noun), which may indicate an unknown named-entity, or the sentence has more than a fixed length of words, 2 indicating that its meaning may not be estimated solely by named entities." ></td>
	<td class="line x" title="80:231	Parsing Filter: The sentence fails the parsing to DSyntS conversion." ></td>
	<td class="line x" title="81:231	Failures are automatically detected by comparing the original sentence with the one realized by RealPro taking the converted DSyntS as an input." ></td>
	<td class="line x" title="82:231	We apply the filters, in a cascading manner, to the 18,466 sentences with semantic representations." ></td>
	<td class="line x" title="83:231	As a result, we obtain 512 (2.8%) mappings of (U,R,S)." ></td>
	<td class="line x" title="84:231	After removing 61 duplicates, 451 distinct (2.4%) mappings remain." ></td>
	<td class="line x" title="85:231	Table 2 shows the number of sentences eliminated by each filter." ></td>
	<td class="line x" title="86:231	3 Objective Evaluation We evaluate the learned expressions with respect to domain coverage, linguistic variation and generativity." ></td>
	<td class="line x" title="87:231	2 We used 20 as a threshold." ></td>
	<td class="line x" title="88:231	267 # Combination of Dist. Attrs Count 1 food-service 39 2 food-value 21 3 atmosphere-food 14 4 atmosphere-service 10 5 atmosphere-food-service 7 6 food-foodtype 4 7 atmosphere-food-value 4 8 location-overall 3 9 food-foodtype-value 3 10 food-service-value 2 11 food-foodtype-location 2 12 food-overall 2 13 atmosphere-foodtype 2 14 atmosphere-overall 2 15 service-value 1 16 overall-service 1 17 overall-value 1 18 foodtype-overall 1 19 food-foodtype-location-overall 1 20 atmosphere-food-service-value 1 21 atmosphere-food-overallservice-value 1 Total 122 Table 4: Counts for multi-relation mappings." ></td>
	<td class="line x" title="89:231	3.1 Domain Coverage To be usable for a dialogue system, the mappings must have good domain coverage." ></td>
	<td class="line x" title="90:231	Table 3 shows the distribution of the 327 mappings realizing a single scalar-valued relation, categorized by the associatedrating score." ></td>
	<td class="line x" title="91:231	3 For example, there are 57 mappings with R of RESTAURANT has foodquality=5, and a large number of mappings for both the foodquality and servicequality relations." ></td>
	<td class="line x" title="92:231	Although we could not obtain mappings for some relations such as price={1,2}, coverage for expressing a single relation is fairly complete." ></td>
	<td class="line x" title="93:231	There are also mappings that express several relations." ></td>
	<td class="line x" title="94:231	Table 4 shows the counts of mappings for multi-relation mappings, with those containing a food or service relation occurring more frequentlyas inthesingle scalar-valuedrelationmappings." ></td>
	<td class="line x" title="95:231	We found only 21 combinations of relations, which is surprising given the large potential number of combinations (There are 50 combinations if we treat relations with different scalar values differently)." ></td>
	<td class="line x" title="96:231	We also find that most of the mappingshavetwo or threerelations, perhaps suggesting that system utterances should not express too many relations in a single sentence." ></td>
	<td class="line x" title="97:231	3.2 Linguistic Variation We also wish to assess whether the linguistic variation of the learned mappings was greater than what we could easily have generated with a hand-crafted dictionary, or a hand-crafted dictionary augmented with aggregation operators, as in 3 There are two other single-relation but not scalar-valued mappings that concern LOCATION in our mappings." ></td>
	<td class="line x" title="98:231	(Walker et al. , 2003)." ></td>
	<td class="line x" title="99:231	Thus, we first categorized the mappings by the patterns of the DSyntSs." ></td>
	<td class="line x" title="100:231	Table 5 shows the most common syntactic patterns (more than 10 occurrences), indicating that 30% of the learned patterns consist of the simple form X is ADJwhereADJ is an adjective, or X is RB ADJ, where RB is a degree modifier." ></td>
	<td class="line x" title="101:231	Furthermore, up to 55% of the learned mappings could be generated from these basic patterns by the application of a combination operator that coordinates multiple adjectives, or coordinates predications over distinct attributes." ></td>
	<td class="line x" title="102:231	However, there are 137 syntactic patterns in all, 97 with unique syntactic structures and 21 with two occurrences, accounting for 45% of the learned mappings." ></td>
	<td class="line x" title="103:231	Table 6 shows examplesof learned mappingswith distinctsyntactic structures." ></td>
	<td class="line x" title="104:231	It would be surprising to see this type of variety in a hand-crafted generation dictionary." ></td>
	<td class="line x" title="105:231	In addition, the learned mappings contain 275 distinct lexemes, with a minimum of 2, maximum of 15, and mean of 4.63 lexemes per DSyntS, indicating that the method extracts a wide variety of expressions of varying lengths." ></td>
	<td class="line x" title="106:231	Another interesting aspect of the learned mappings is the wide variety of adjectival phrases (APs)inthecommonpatterns." ></td>
	<td class="line x" title="107:231	Tables7and8 show the APs in singlescalar-valuedrelation mappings for food and service categorized by the associated ratings." ></td>
	<td class="line x" title="108:231	Tables for atmosphere, value and overall can be found in the Appendix." ></td>
	<td class="line x" title="109:231	Moreover, the meanings for some of the learned APs are very specific to the particular attribute, e.g. cold and burnt associated with foodquality of 1, attentive and prompt for servicequality of 5, silly and inattentive for servicequality of 1." ></td>
	<td class="line x" title="110:231	and mellow for atmosphere of 5." ></td>
	<td class="line oc" title="111:231	In addition, our method places the adjectival phrases (APs) in the common patterns on a more fine-grained scale of 1 to 5, similar to the strengthclassificationsin (Wilsonet al. , 2004), in contrast to other automatic methods that classify expressions into a binary positive or negative polarity (e.g.(Turney, 2002))." ></td>
	<td class="line x" title="113:231	3.3 Generativity Our motivation for deriving syntactic representations for the learned expressions was the possibility of using an off-the-shelf sentence planner to derive new combinations of relations, and apply aggregation and other syntactic transformations." ></td>
	<td class="line x" title="114:231	We examined how many of the learned DSyntSs can be combined with each other, by taking every pair of DSyntSs in the mappings and applying the built-in merge operation in the SPaRKy generator (Walker et al. , 2003)." ></td>
	<td class="line x" title="115:231	We found that only 306 combinations out of a potential 81,318 268 # syntactic pattern example utterance count ratio accum." ></td>
	<td class="line x" title="116:231	1 NN VB JJ The atmosphere is wonderful." ></td>
	<td class="line x" title="117:231	92 20.4% 20.4% 2 NN VB RB JJ The atmosphere was very nice." ></td>
	<td class="line x" title="118:231	52 11.5% 31.9% 3 JJ NN Bad service." ></td>
	<td class="line x" title="119:231	36 8.0% 39.9% 4 NN VB JJ CC JJ The food was flavorful but cold." ></td>
	<td class="line x" title="120:231	25 5.5% 45.5% 5 RB JJ NN Very trendy ambience." ></td>
	<td class="line x" title="121:231	22 4.9% 50.3% 6 NN VB JJ CC NN VB JJ The food is excellent and the atmosphere is great." ></td>
	<td class="line x" title="122:231	13 2.9% 53.2% 7 NN CC NN VB JJ The food and service were fantastic." ></td>
	<td class="line x" title="123:231	10 2.2% 55.4% Table 5: Common syntactic patterns of DSyntSs, flattened to a POS sequence for readability." ></td>
	<td class="line x" title="124:231	NN, VB, JJ, RB, CC stand for noun, verb, adjective, adverb, and conjunction, respectively." ></td>
	<td class="line x" title="125:231	[overall=1, value=2] Very disappointing experience for the money charged." ></td>
	<td class="line x" title="126:231	[food=5, value=5] The food is excellent and plentiful at a reasonable price." ></td>
	<td class="line x" title="127:231	[food=5, service=5] The food is exquisite as well as the service and setting." ></td>
	<td class="line x" title="128:231	[food=5,service=5]The food was spectacular and so was the service." ></td>
	<td class="line x" title="129:231	[food=5, foodtype, value=5] Best FOODTYPE food with a great value for money." ></td>
	<td class="line x" title="130:231	[food=5, foodtype, value=5] An absolutely outstanding value with fantastic FOODTYPE food." ></td>
	<td class="line x" title="131:231	[food=5, foodtype, location, overall=5] This is the best place to eat FOODTYPE food in LOCATION." ></td>
	<td class="line x" title="132:231	[food=5, foodtype] Simply amazing FOODTYPE food." ></td>
	<td class="line x" title="133:231	[food=5, foodtype] RESTAURANTNAMEis the best of the best for FOODTYPE food." ></td>
	<td class="line x" title="134:231	[food=5] The food is to die for." ></td>
	<td class="line x" title="135:231	[food=5] What incredible food." ></td>
	<td class="line x" title="136:231	[food=4] Very pleasantly surprised by the food." ></td>
	<td class="line x" title="137:231	[food=1] The food has gone downhill." ></td>
	<td class="line x" title="138:231	[atmosphere=5, overall=5] This is a quiet little place with great atmosphere." ></td>
	<td class="line x" title="139:231	[atmosphere=5,food=5, overall=5, service=5, value=5] The food, service and ambience of the place are all fabulous and the prices are downright cheap." ></td>
	<td class="line x" title="140:231	Table 6: Acquired generation patterns (with shorthand for relations in square brackets) whose syntactic patterns occurred only once." ></td>
	<td class="line x" title="141:231	combinations (0.37%) were successful." ></td>
	<td class="line x" title="142:231	This is because the merge operation in SPaRKy requires that the subjects and the verbs of the two DSyntSs are identical, e.g. the subject is RESTAURANT and verb is has, whereas the learned DSyntSs often place the attribute in subject position as a definite noun phrase." ></td>
	<td class="line x" title="143:231	However, the learned DSyntS can be incorporated into SPaRKy using the semantic representations to substitute learned DSyntSs into nodes in the sentence plan tree." ></td>
	<td class="line x" title="144:231	Figure 2 shows some example utterances generated by SPaRKy with its originaldictionary and exampleutterances when the learned mappings are incorporated." ></td>
	<td class="line x" title="145:231	The resulting utterances seem more natural and colloquial; we examine whether this is true in the next section." ></td>
	<td class="line x" title="146:231	4 Subjective Evaluation We evaluate the obtained mappings in two respects: the consistency between the automatically derived semantic representation and the realizafood=1 awful, bad, burnt, cold, very ordinary food=2 acceptable, bad, flavored, not enough, very bland, very good food=3 adequate, bland and mediocre, flavorful but cold, pretty good, rather bland, very good food=4 absolutely wonderful, awesome, decent, excellent, good, good and generous, great, outstanding, rather good, really good, traditional, very fresh and tasty, very good, very very good food=5 absolutely delicious, absolutely fantastic, absolutely great, absolutely terrific, ample, well seasoned and hot, awesome, best, delectable and plentiful, delicious, delicious but simple, excellent, exquisite, fabulous, fancy but tasty, fantastic, fresh, good, great, hot, incredible, just fantastic, large and satisfying, outstanding, plentiful and outstanding, plentiful and tasty, quick and hot, simply great, so delicious, so very tasty, superb, terrific, tremendous, very good, wonderful Table 7: Adjectival phrases (APs) in single scalarvalued relation mappings for foodquality." ></td>
	<td class="line x" title="147:231	tion, and the naturalness of the realization." ></td>
	<td class="line x" title="148:231	For comparison, we used a baseline of handcrafted mappings from (Walker et al. , 2003) except that we changed the word decor to atmosphere and added five mappings for overall." ></td>
	<td class="line x" title="149:231	For scalar relations, this consists of the realization RESTAURANT has ADJ LEX where ADJ is mediocre, decent,good,very good,orexcellent for rating values 1-5, and LEX is food quality, service, atmosphere, value,oroverall depending on the relation." ></td>
	<td class="line x" title="150:231	RESTAURANT is filled with the name of a restaurant at runtime." ></td>
	<td class="line x" title="151:231	For example, RESTAURANT has foodquality=1 is realized as RESTAURANT has mediocre food quality. The location and food type relations are mapped to RESTAURANT is located in LOCATION and RESTAURANT is a FOODTYPE restaurant. The learned mappings include 23 distinct semantic representationsfor a single-relation (22 for scalar-valued relations and one for location) and 50 for multi-relations." ></td>
	<td class="line x" title="152:231	Therefore, using the handcrafted mappings, we first created 23 utterances for the single-relations." ></td>
	<td class="line x" title="153:231	We then created three utterancesforeachof50multi-relationsusingdifferent clause-combining operations from (Walker et al. , 2003)." ></td>
	<td class="line x" title="154:231	This gave a total of 173 baseline utterances, which together with 451 learned mappings, 269 service=1 awful, bad, great, horrendous, horrible, inattentive, forgetful and slow, marginal, really slow, silly and inattentive, still marginal, terrible, young service=2 overly slow, very slow and inattentive service=3 bad, bland and mediocre, friendly and knowledgeable, good, pleasant, prompt, very friendly service=4 all very warm and welcoming, attentive, extremely friendly and good, extremely pleasant, fantastic, friendly, friendly and helpful, good, great, great and courteous, prompt and friendly, really friendly, so nice, swift and friendly, very friendly, very friendly and accommodating service=5 all courteous, excellent, excellent and friendly, extremely friendly, fabulous, fantastic, friendly, friendly and helpful, friendly and very attentive, good, great, great, prompt and courteous, happy and friendly, impeccable, intrusive, legendary, outstanding, pleasant, polite, attentive and prompt, prompt and courteous, prompt and pleasant, quick and cheerful, stupendous, superb, the most attentive, unbelievable, very attentive, very congenial, very courteous, very friendly, very friendly and helpful, very friendly and pleasant, very friendly and totally personal, very friendly and welcoming, very good, very helpful, very timely, warm and friendly, wonderful Table 8: Adjectival phrases (APs) in single scalarvalued relation mappings for servicequality." ></td>
	<td class="line x" title="155:231	yielded 624 utterances for evaluation." ></td>
	<td class="line x" title="156:231	Ten subjects, all native English speakers, evaluated the mappings by reading them from a webpage." ></td>
	<td class="line x" title="157:231	For each system utterance, the subjects were asked to express their degree of agreement, on a scale of 1 (lowest) to 5 (highest), with the statement (a) The meaning of the utterance is consistent with the ratings expressing their semantics, and with the statement (b) The style of the utterance is very natural and colloquial.Theywere asked not to correct their decisions and also to rate each utterance on its own merit." ></td>
	<td class="line x" title="158:231	4.1 Results Table 9 shows the means and standard deviations of thescores forbaselinevs." ></td>
	<td class="line x" title="159:231	learned utterancesfor consistency and naturalness." ></td>
	<td class="line x" title="160:231	A t-test shows that theconsistencyofthe learnedexpressionis significantly lower than the baseline (df=4712, p <.001) but that their naturalness is significantly higher than the baseline (df=3107, p < .001)." ></td>
	<td class="line x" title="161:231	However, consistency is still high." ></td>
	<td class="line x" title="162:231	Only 14 of the learned utterances (shown in Tab." ></td>
	<td class="line x" title="163:231	10) have a mean consistency score lower than 3, which indicates that, by and large, the human judges felt that the inferred semantic representations were consistent with the meaning of the learned expressions." ></td>
	<td class="line x" title="164:231	The correlation coefficient between consistency and naturalness scores is 0.42, which indicates that consisOriginal SPaRKy utterances  Babbo has the best overall quality among the selected restaurants with excellent decor, excellent service and superb food quality." ></td>
	<td class="line x" title="165:231	 Babbo has excellent decor and superb food quality with excellent service." ></td>
	<td class="line x" title="166:231	It has the best overall quality among the selected restaurants." ></td>
	<td class="line x" title="167:231	 Combination of SPaRKy and learnedDSyntS  Because the food is excellent, the wait staff is professional and the decor is beautiful and very comfortable, Babbo has the best overall quality among the selected restaurants." ></td>
	<td class="line x" title="168:231	 Babbo has the best overall quality among the selected restaurants because atmosphere is exceptionally nice, food is excellent and the service is superb." ></td>
	<td class="line x" title="169:231	 Babbo has superb food quality, the service is exceptional and the atmosphere is very creative.Ithasthe best overall quality among the selected restaurants." ></td>
	<td class="line x" title="170:231	Figure 2: Utterances incorporating learned DSyntSs (Bold font) in SPaRKy." ></td>
	<td class="line x" title="171:231	baseline learned stat." ></td>
	<td class="line x" title="172:231	mean sd." ></td>
	<td class="line x" title="173:231	mean sd." ></td>
	<td class="line x" title="174:231	sig." ></td>
	<td class="line x" title="175:231	Consistency 4.714 0.588 4.459 0.890 + Naturalness 4.227 0.852 4.613 0.844 + Table 9: Consistency and naturalness scores averaged over 10 subjects." ></td>
	<td class="line x" title="176:231	tency does not greatly relate to naturalness." ></td>
	<td class="line x" title="177:231	We also performed an ANOVA (ANalysis Of VAriance) of the effect of each relation in R on naturalness and consistency." ></td>
	<td class="line x" title="178:231	There were no significant effects except that mappings combining food, service, and atmosphere were significantly worse (df=1, F=7.79, p=0.005)." ></td>
	<td class="line x" title="179:231	However, there is a trend for mappings to be rated higher for the food attribute (df=1, F=3.14, p=0.08) and the value attribute (df=1, F=3.55, p=0.06) for consistency, suggesting that perhaps it is easier to learn some mappings than others." ></td>
	<td class="line x" title="180:231	5 Related Work Automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corporawith multiple descriptionsof the same events (Barzilay and McKeown, 2001; Barzilay and Lee, 2003)." ></td>
	<td class="line x" title="181:231	Other work finds predicates of similar meanings by using the similarity of contexts around the predicates (Lin and Pantel, 2001)." ></td>
	<td class="line x" title="182:231	However, these studies find a set of sentences with the same meaning, but do not associate a specific meaning with the sentences." ></td>
	<td class="line x" title="183:231	One exception is (Barzilay and Lee, 2002), which derives mappings between semantic representations and realizations using a parallel (but unaligned) corpus consisting of both complex semantic input and corresponding natural language verbalizations for mathemat270 shorthand for relations and utterance score [food=4] The food is delicious and beautifully prepared." ></td>
	<td class="line x" title="184:231	2.9 [overall=4] A wonderful experience." ></td>
	<td class="line x" title="185:231	2.9 [service=3]The service is bland and mediocre." ></td>
	<td class="line x" title="186:231	2.8 [atmosphere=2] The atmosphere here is eclectic." ></td>
	<td class="line x" title="187:231	2.6 [overall=3] Really fancy place." ></td>
	<td class="line x" title="188:231	2.6 [food=3, service=4] Wonderful service and great food." ></td>
	<td class="line x" title="189:231	2.5 [service=4]The service is fantastic." ></td>
	<td class="line x" title="190:231	2.5 [overall=2] The RESTAURANTNAME is once a great place to go and socialize." ></td>
	<td class="line x" title="191:231	2.2 [atmosphere=2] The atmosphere is unique and pleasant." ></td>
	<td class="line x" title="192:231	2.0 [food=5,foodtype] FOODTYPEand FOODTYPE food." ></td>
	<td class="line x" title="193:231	1.8 [service=3] Waitstaff is friendly and knowledgeable." ></td>
	<td class="line x" title="194:231	1.7 [atmosphere=5,food=5, service=5] The atmosphere, food and service." ></td>
	<td class="line x" title="195:231	1.6 [overall=3] Overall, a great experience." ></td>
	<td class="line x" title="196:231	1.4 [service=1]The waiter is great." ></td>
	<td class="line x" title="197:231	1.4 Table 10: The 14 utterances with consistency scores below 3." ></td>
	<td class="line x" title="198:231	ical proofs." ></td>
	<td class="line x" title="199:231	However, our technique does not require parallel corpora or previously existing semantictranscriptsor labeling, and userreviewsare widely available in many different domains (See http://www.epinions.com/)." ></td>
	<td class="line x" title="200:231	There is also significant previous work on mining user reviews." ></td>
	<td class="line x" title="201:231	For example, Hu and Liu (2005) use reviews to find adjectives to describeproducts, and Popescuand Etzioni(2005) automaticallyfind features of a product together with the polarity of adjectives used to describe them." ></td>
	<td class="line x" title="202:231	They both aim at summarizing reviews so that users can make decisions easily." ></td>
	<td class="line x" title="203:231	Our method is also capable of finding polarities of modifying expressions including adjectives, but on a more fine-grained scale of 1 to 5." ></td>
	<td class="line x" title="204:231	However, it might be possible to use their approach to create rating information for raw review texts as in (Pang and Lee, 2005), so that we can create mappings from reviews without ratings." ></td>
	<td class="line x" title="205:231	6 Summary and Future Work We proposed automatically obtaining mappings between semantic representations and realizations from reviews with individual ratings." ></td>
	<td class="line x" title="206:231	The results show that: (1) the learned mappings provide good coverage of the domain ontology and exhibit good linguistic variation; (2) the consistency between the semantic representations and realizations is high; and (3) the naturalnessof the realizations are significantly higher than the baseline." ></td>
	<td class="line x" title="207:231	There are also limitations in our method." ></td>
	<td class="line x" title="208:231	Even though consistency is rated highly by human subjects, this may actually be a judgement of whether the polarity of the learned mapping is correctly placed on the 1 to 5 rating scale." ></td>
	<td class="line x" title="209:231	Thus, alternate ways of expressing, for example foodquality=5, shown in Table 7, cannot be guaranteed to be synonymous, which may be required for use in spoken language generation." ></td>
	<td class="line x" title="210:231	Rather, an examination of the adjectival phrases in Table 7 shows that different aspects of the food are discussed." ></td>
	<td class="line x" title="211:231	For example ample and plentiful refer to the portion size, fancy may refer to the presentation, and delicious describes the flavors." ></td>
	<td class="line x" title="212:231	This suggests that perhaps the ontology would benefit from representing these sub-attributes of the food attribute, and sub-attributes in general." ></td>
	<td class="line x" title="213:231	Another problem with consistency is that the same AP, e.g. very good in Table 7 may appear with multiple ratings." ></td>
	<td class="line x" title="214:231	For example, very good is used for every foodquality rating from 2 to 5." ></td>
	<td class="line x" title="215:231	Thus some further automatic or by-hand analysis is required to refine what is learned before actual use in spoken language generation." ></td>
	<td class="line x" title="216:231	Still, our method could reduce the amount of time a system designer spends developing the spoken language generator, and increase the naturalness of spoken language generation." ></td>
	<td class="line x" title="217:231	Another issue is that the recall appears to be quite low given that all of the sentences concern the same domain: only 2.4% of the sentences could be used to create the mappings." ></td>
	<td class="line x" title="218:231	One way to increase recall might be to automatically augment the list of distinguished attribute lexicalizations, using WordNet or work on automatic identification of synonyms, such as (Lin and Pantel, 2001)." ></td>
	<td class="line x" title="219:231	However, the method here has high precision, and automatic techniques may introduce noise." ></td>
	<td class="line x" title="220:231	A related issue is that the filters are in some cases too strict." ></td>
	<td class="line x" title="221:231	For example the contextual filter is based on POS-tags, so that sentences that do not require the prior context for their interpretation are eliminated, such as sentences containing subordinating conjunctions like because, when, if, whose arguments are both given in the same sentence (Prasad et al. , 2005)." ></td>
	<td class="line x" title="222:231	In addition, recall is affected by the domain ontology, and the automatically constructed domain ontology from the review webpages may not cover all of the domain." ></td>
	<td class="line x" title="223:231	In some review domains, the attributes that get individual ratings are a limited subset of the domain ontology." ></td>
	<td class="line x" title="224:231	Techniques for automatic feature identification (Hu and Liu, 2005; Popescu and Etzioni, 2005) could possibly help here, although these techniques currently have the limitation that they do not automatically identify different lexicalizations of the same feature." ></td>
	<td class="line x" title="225:231	A different type of limitation is that dialogue systems need to generate utterances for information gathering whereas the mappings we obtained 271 can only be used for information presentation." ></td>
	<td class="line x" title="226:231	Thus these would have to be constructed by hand, as in current practice, or perhaps other types of corpora or resources could be utilized." ></td>
	<td class="line x" title="227:231	In addition, the utility of syntactic structures in the mappingsshouldbe furtherexamined, especiallygiven the failures in DSyntS conversion." ></td>
	<td class="line x" title="228:231	An alternative would be to leave some sentences unparsed and usethem astemplateswith hybridgenerationtechniques (White and Caldwell, 1998)." ></td>
	<td class="line x" title="229:231	Finally, while webelievethatthistechniquewillapplyacrossdomains, it wouldbe usefultotest iton domainssuch as movie reviews or product reviews, which have more complex domain ontologies." ></td>
	<td class="line x" title="230:231	Acknowledgments We thank the anonymous reviewers for their helpful comments." ></td>
	<td class="line x" title="231:231	This work was supported by a Royal Society Wolfson award to Marilyn Walker and a research collaboration grant from NTT to the Cognitive Systems Group at the University of Sheffield." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1134
Word Sense And Subjectivity
Wiebe, Janyce M.;Mihalcea, Rada;"></td>
	<td class="line x" title="1:194	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 10651072, Sydney, July 2006." ></td>
	<td class="line x" title="2:194	c2006 Association for Computational Linguistics Word Sense and Subjectivity Janyce Wiebe Department of Computer Science University of Pittsburgh wiebe@cs.pitt.edu Rada Mihalcea Department of Computer Science University of North Texas rada@cs.unt.edu Abstract Subjectivity and meaning are both important properties of language." ></td>
	<td class="line x" title="3:194	This paper explores their interaction, and brings empirical evidence in support of the hypotheses that (1) subjectivity is a property that can be associated with word senses, and (2) word sense disambiguation can directly benefit from subjectivity annotations." ></td>
	<td class="line x" title="4:194	1 Introduction There is growing interest in the automatic extraction of opinions, emotions, and sentiments in text (subjectivity), to provide tools and support for various NLP applications." ></td>
	<td class="line x" title="5:194	Similarly, there is continuous interest in the task of word sense disambiguation, with sense-annotated resources being developed for many languages, and a growing number of research groups participating in large-scale evaluations such as SENSEVAL." ></td>
	<td class="line x" title="6:194	Though both of these areas are concerned with the semantics of a text, over time there has been little interaction, if any, between them." ></td>
	<td class="line x" title="7:194	In this paper, we address this gap, and explore possible interactions between subjectivity and word sense." ></td>
	<td class="line x" title="8:194	There are several benefits that would motivate such a joint exploration." ></td>
	<td class="line x" title="9:194	First, at the resource level, the augmentation of lexical resources such as WordNet (Miller, 1995) with subjectivity labels could support better subjectivity analysis tools, and principled methods for refining word senses and clustering similar meanings." ></td>
	<td class="line x" title="10:194	Second, at the tool level, an explicit link between subjectivity and word sense could help improve methods for each, by integrating features learned from one into the other in a pipeline approach, or through joint simultaneous learning." ></td>
	<td class="line x" title="11:194	In this paper we address two questions about word sense and subjectivity." ></td>
	<td class="line x" title="12:194	First, can subjectivity labels be assigned to word senses?" ></td>
	<td class="line x" title="13:194	To address this question, we perform two studies." ></td>
	<td class="line x" title="14:194	The first (Section 3) investigates agreement between annotators who manually assign the labels subjective, objective, or both to WordNet senses." ></td>
	<td class="line x" title="15:194	The second study (Section 4) evaluates a method for automatic assignment of subjectivity labels to word senses." ></td>
	<td class="line x" title="16:194	We devise an algorithm relying on distributionally similar words to calculate a subjectivity score, and show how it can be used to automatically assess the subjectivity of a word sense." ></td>
	<td class="line x" title="17:194	Second, can automatic subjectivity analysis be used to improve word sense disambiguation?" ></td>
	<td class="line x" title="18:194	To address this question, the output of a subjectivity sentence classifier is input to a word-sense disambiguation system, which is in turn evaluated on the nouns from the SENSEVAL-3 English lexical sample task (Section 5)." ></td>
	<td class="line x" title="19:194	The results of this experiment show that a subjectivity feature can significantly improve the accuracy of a word sense disambiguation system for those words that have both subjective and objective senses." ></td>
	<td class="line x" title="20:194	A third obvious question is, can word sense disambiguation help automatic subjectivity analysis?" ></td>
	<td class="line x" title="21:194	However, due to space limitations, we do not address this question here, but rather leave it for future work." ></td>
	<td class="line x" title="22:194	2 Background Subjective expressions are words and phrases being used to express opinions, emotions, evaluations, speculations, etc.(Wiebe et al. , 2005)." ></td>
	<td class="line x" title="24:194	A general covering term for such states is private state, a state that is not open to objective obser1065 vation or verification (Quirk et al. , 1985).1 There are three main types of subjective expressions:2 (1) references to private states: His alarm grew." ></td>
	<td class="line x" title="25:194	He absorbed the information quickly." ></td>
	<td class="line x" title="26:194	He was boiling with anger." ></td>
	<td class="line x" title="27:194	(2) references to speech (or writing) events expressing private states: UCC/Disciples leaders roundly condemned the Iranian Presidents verbal assault on Israel." ></td>
	<td class="line x" title="28:194	The editors of the left-leaning paper attacked the new House Speaker." ></td>
	<td class="line x" title="29:194	(3) expressive subjective elements: He would be quite a catch." ></td>
	<td class="line x" title="30:194	Whats the catch?" ></td>
	<td class="line x" title="31:194	That doctor is a quack." ></td>
	<td class="line x" title="32:194	Work on automatic subjectivity analysis falls into three main areas." ></td>
	<td class="line oc" title="33:194	The first is identifying words and phrases that are associated with subjectivity, for example, that think is associated with private states and that beautiful is associated with positive sentiments (e.g. , (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Kamps and Marx, 2002; Turney, 2002; Esuli and Sebastiani, 2005))." ></td>
	<td class="line x" title="34:194	Such judgments are made for words." ></td>
	<td class="line x" title="35:194	In contrast, our end task (in Section 4) is to assign subjectivity labels to word senses." ></td>
	<td class="line x" title="36:194	The second is subjectivity classification of sentences, clauses, phrases, or word instances in the context of a particular text or conversation, either subjective/objective classifications or positive/negative sentiment classifications (e.g. ,(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Dave et al. , 2003; Hu and Liu, 2004))." ></td>
	<td class="line oc" title="37:194	The third exploits automatic subjectivity analysis in applications such as review classification (e.g. , (Turney, 2002; Pang and Lee, 2004)), mining texts for product reviews (e.g. , (Yi et al. , 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)), summarization (e.g. , (Kim and Hovy, 2004)), information extraction (e.g. , (Riloff et al. , 2005)), 1Note that sentiment, the focus of much recent work in the area, is a type of subjectivity, specifically involving positive or negative opinion, emotion, or evaluation." ></td>
	<td class="line x" title="38:194	2These distinctions are not strictly needed for this paper, but may help the reader appreciate the examples given below." ></td>
	<td class="line x" title="39:194	and question answering (e.g. , (Yu and Hatzivassiloglou, 2003; Stoyanov et al. , 2005))." ></td>
	<td class="line x" title="40:194	Most manual subjectivity annotation research has focused on annotating words, out of context (e.g. , (Heise, 2001)), or sentences and phrases in the context of a text or conversation (e.g. , (Wiebe et al. , 2005))." ></td>
	<td class="line x" title="41:194	The new annotations in this paper are instead targeting the annotation of word senses." ></td>
	<td class="line x" title="42:194	3 Human Judgment of Word Sense Subjectivity To explore our hypothesis that subjectivity may be associated with word senses, we developed a manual annotation scheme for assigning subjectivity labels to WordNet senses,3 and performed an inter-annotator agreement study to assess its reliability." ></td>
	<td class="line x" title="43:194	Senses are classified as S(ubjective), O(bjective), or B(oth)." ></td>
	<td class="line x" title="44:194	Classifying a sense as S means that, when the sense is used in a text or conversation, we expect it to express subjectivity; we also expect the phrase or sentence containing it to be subjective." ></td>
	<td class="line x" title="45:194	We saw a number of subjective expressions in Section 2." ></td>
	<td class="line x" title="46:194	A subset is repeated here, along with relevant WordNet senses." ></td>
	<td class="line x" title="47:194	In the display of each sense, the first part shows the synset, gloss, and any examples." ></td>
	<td class="line x" title="48:194	The second part (marked with =>) shows the immediate hypernym." ></td>
	<td class="line x" title="49:194	His alarm grew." ></td>
	<td class="line x" title="50:194	alarm, dismay, consternation  (fear resulting from the awareness of danger) => fear, fearfulness, fright  (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)) He was boiling with anger." ></td>
	<td class="line x" title="51:194	seethe, boil  (be in an agitated emotional state; The customer was seething with anger) => be  (have the quality of being; (copula, used with an adjective or a predicate noun); John is rich; This is not a good answer) Whats the catch?" ></td>
	<td class="line x" title="52:194	catch  (a hidden drawback; it sounds good but whats the catch?) => drawback  (the quality of being a hindrance; he pointed out all the drawbacks to my plan) That doctor is a quack." ></td>
	<td class="line x" title="53:194	quack  (an untrained person who pretends to be a physician and who dispenses medical advice) => doctor, doc, physician, MD, Dr. , medico Before specifying what we mean by an objective sense, we give examples." ></td>
	<td class="line x" title="54:194	3All our examples and data used in the experiments are from WordNet 2.0." ></td>
	<td class="line x" title="55:194	1066 The alarm went off." ></td>
	<td class="line x" title="56:194	alarm, warning device, alarm system  (a device that signals the occurrence of some undesirable event) => device  (an instrumentality invented for a particular purpose; the device is small enough to wear on your wrist; a device intended to conserve water) The water boiled." ></td>
	<td class="line x" title="57:194	boil  (come to the boiling point and change from a liquid to vapor; Water boils at 100 degrees Celsius) => change state, turn  (undergo a transformation or a change of position or action; We turned from Socialism to Capitalism; The people turned against the President when he stole the election) He sold his catch at the market." ></td>
	<td class="line x" title="58:194	catch, haul  (the quantity that was caught; the catch was only 10 fish) => indefinite quantity  (an estimated quantity) The ducks quack was loud and brief." ></td>
	<td class="line x" title="59:194	quack  (the harsh sound of a duck) => sound  (the sudden occurrence of an audible event; the sound awakened them) While we expect phrases or sentences containing subjective senses to be subjective, we do not necessarily expect phrases or sentences containing objective senses to be objective." ></td>
	<td class="line x" title="60:194	Consider the following examples: Will someone shut that damn alarm off?" ></td>
	<td class="line x" title="61:194	Cant you even boil water?" ></td>
	<td class="line x" title="62:194	While these sentences contain objective senses of alarm and boil, the sentences are subjective nonetheless." ></td>
	<td class="line x" title="63:194	But they are not subjective due to alarm and boil, but rather to punctuation, sentence forms, and other words in the sentence." ></td>
	<td class="line x" title="64:194	Thus, classifying a sense as O means that, when the sense is used in a text or conversation, we do not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else." ></td>
	<td class="line x" title="65:194	Finally, classifying a sense as B means it covers both subjective and objective usages, e.g.: absorb, suck, imbibe, soak up, sop up, suck up, draw, take in, take up  (take in, also metaphorically; The sponge absorbs water well; She drew strength from the ministers words) Manual subjectivity judgments were added to a total of 354 senses (64 words)." ></td>
	<td class="line x" title="66:194	One annotator, Judge 1 (a co-author), tagged all of them." ></td>
	<td class="line x" title="67:194	A second annotator (Judge 2, who is not a co-author) tagged a subset for an agreement study, presented next." ></td>
	<td class="line x" title="68:194	3.1 Agreement Study For the agreement study, Judges 1 and 2 independently annotated 32 words (138 senses)." ></td>
	<td class="line x" title="69:194	16 words have both S and O senses and 16 do not (according to Judge 1)." ></td>
	<td class="line x" title="70:194	Among the 16 that do not have both S and O senses, 8 have only S senses and 8 have only O senses." ></td>
	<td class="line x" title="71:194	All of the subsets are balanced between nouns and verbs." ></td>
	<td class="line x" title="72:194	Table 1 shows the contingency table for the two annotators judgments on this data." ></td>
	<td class="line x" title="73:194	In addition to S, O, and B, the annotation scheme also permits U(ncertain) tags." ></td>
	<td class="line x" title="74:194	S O B U Total S 39 O O 4 43 O 3 73 2 4 82 B 1 O 3 1 5 U 3 2 O 3 8 Total 46 75 5 12 138 Table 1: Agreement on balanced set (Agreement: 85.5%, : 0.74) Overall agreement is 85.5%, with a Kappa () value of 0.74." ></td>
	<td class="line x" title="75:194	For 12.3% of the senses, at least one annotators tag is U. If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 95% and  rises to 0.90." ></td>
	<td class="line x" title="76:194	Thus, annotator agreement is especially high when both are certain." ></td>
	<td class="line x" title="77:194	Considering only the 16-word subset with both S and O senses (according to Judge 1),  is.75, and for the 16-word subset for which Judge 1 gave only S or only O senses,  is .73." ></td>
	<td class="line x" title="78:194	Thus, the two subsets are of comparable difficulty." ></td>
	<td class="line x" title="79:194	The two annotators also independently annotated the 20 ambiguous nouns (117 senses) of the SENSEVAL-3 English lexical sample task used in Section 5." ></td>
	<td class="line x" title="80:194	For this tagging task, U tags were not allowed, to create a definitive gold standard for the experiments." ></td>
	<td class="line x" title="81:194	Even so, the  value for them is 0.71, which is not substantially lower." ></td>
	<td class="line x" title="82:194	The distributions of Judge 1s tags for all 20 words can be found in Table 3 below." ></td>
	<td class="line x" title="83:194	We conclude this section with examples of disagreements that illustrate sources of uncertainty." ></td>
	<td class="line x" title="84:194	First, uncertainty arises when subjective senses are missing from the dictionary." ></td>
	<td class="line x" title="85:194	The labels for the senses of noun assault are (O:O,O:O,O:O,O:UO).4 For verb assault there is a subjective sense: attack, round, assail, lash out, snipe, assault (attack in speech or writing) The editors of the left-leaning paper attacked the new House Speaker However, there is no corresponding sense for 4I.e., the first three were labeled O by both annotators." ></td>
	<td class="line x" title="86:194	For the fourth sense, the second annotator was not sure but was leaning toward O. 1067 noun assault." ></td>
	<td class="line x" title="87:194	A missing sense may lead an annotator to try to see subjectivity in an objective sense." ></td>
	<td class="line x" title="88:194	Second, uncertainty can arise in weighing hypernym against sense." ></td>
	<td class="line x" title="89:194	It is fine for a synset to imply just S or O, while the hypernym implies both (the synset specializes the more general concept)." ></td>
	<td class="line x" title="90:194	However, consider the following, which was tagged (O:UB)." ></td>
	<td class="line x" title="91:194	attack  (a sudden occurrence of an uncontrollable condition; an attack of diarrhea) => affliction  (a cause of great suffering and distress) While the sense is only about the condition, the hypernym highlights subjective reactions to the condition." ></td>
	<td class="line x" title="92:194	One annotator judged only the sense (giving tag O), while the second considered the hypernym as well (giving tag UB)." ></td>
	<td class="line x" title="93:194	4 Automatic Assessment of Word Sense Subjectivity Encouraged by the results of the agreement study, we devised a method targeting the automatic annotation of word senses for subjectivity." ></td>
	<td class="line x" title="94:194	The main idea behind our method is that we can derive information about a word sense based on information drawn from words that are distributionally similar to the given word sense." ></td>
	<td class="line x" title="95:194	This idea relates to the unsupervised word sense ranking algorithm described in (McCarthy et al. , 2004)." ></td>
	<td class="line x" title="96:194	Note, however, that (McCarthy et al. , 2004) used the information about distributionally similar words to approximate corpus frequencies for word senses, whereas we target the estimation of a property of a given word sense (the subjectivity)." ></td>
	<td class="line x" title="97:194	Starting with a given ambiguous word w, we first find the distributionally similar words using the method of (Lin, 1998) applied to the automatically parsed texts of the British National Corpus." ></td>
	<td class="line x" title="98:194	Let DSW = dsw1, dsw2,  , dswn be the list of top-ranked distributionally similar words, sorted in decreasing order of their similarity." ></td>
	<td class="line x" title="99:194	Next, for each sense wsi of the word w, we determine the similarity with each of the words in the list DSW, using a WordNet-based measure of semantic similarity (wnss)." ></td>
	<td class="line x" title="100:194	Although a large number of such word-to-word similarity measures exist, we chose to use the (Jiang and Conrath, 1997) measure, since it was found both to be efficient and to provide the best results in previous experiments involving word sense ranking (McCarthy et al. , 2004)5." ></td>
	<td class="line x" title="101:194	For distributionally similar words 5Note that unlike the above measure of distributional simAlgorithm 1 Word Sense Subjectivity Score Input: Word sense wi Input: Distributionally similar words DSW = {dswj|j = 1n} Output: Subjectivity score subj(wi) 1: subj(wi) = 0 2: totalsim = 0 3: for j = 1 to n do 4: Instsj = all instances of dswj in the MPQA corpus 5: for k in Instsj do 6: if k is in a subj." ></td>
	<td class="line x" title="102:194	expr." ></td>
	<td class="line x" title="103:194	in MPQA corpus then 7: subj(wi) += sim(wi,dswj) 8: else if k is not in a subj." ></td>
	<td class="line x" title="104:194	expr." ></td>
	<td class="line x" title="105:194	in MPQA corpus then 9: subj(wi) -= sim(wi,dswj) 10: end if 11: totalsim += sim(wi,dswj) 12: end for 13: end for 14: subj(wi) = subj(wi) / totalsim that are themselves ambiguous, we use the sense that maximizes the similarity score." ></td>
	<td class="line x" title="106:194	The similarity scores associated with each word dswj are normalized so that they add up to one across all possible senses of w, which results in a score described by the following formula: sim(wsi, dswj) = wnss(wsi,dswj)summationtext iprimesenses(w) wnss(wsiprime,dswj) where wnss(wsi, dswj) = max ksenses(dswj) wnss(wsi, dswkj ) A selection process can also be applied so that a distributionally similar word belongs only to one sense." ></td>
	<td class="line x" title="107:194	In this case, for a given sense wi we use only those distributionally similar words with whom wi has the highest similarity score across all the senses of w. We refer to this case as similarityselected, as opposed to similarity-all, which refers to the use of all distributionally similar words for all senses." ></td>
	<td class="line x" title="108:194	Once we have a list of similar words associated with each sense wsi and the corresponding similarity scores sim(wsi, dswj), we use an annotated corpus to assign subjectivity scores to the senses." ></td>
	<td class="line x" title="109:194	The corpus we use is the MPQA Opinion Corpus, which consists of over 10,000 sentences from the world press annotated for subjective expressions (all three types of subjective expressions described in Section 2).6 ilarity which measures similarity between words, rather than word senses, here we needed a similarity measure that also takes into account word senses as defined in a sense inventory such as WordNet." ></td>
	<td class="line x" title="110:194	6The MPQA corpus is described in (Wiebe et al. , 2005) and available at www.cs.pitt.edu/mpqa/databaserelease/." ></td>
	<td class="line x" title="111:194	1068 Algorithm 1 is our method for calculating sense subjectivity scores." ></td>
	<td class="line x" title="112:194	The subjectivity score is a value in the interval [-1,+1] with +1 corresponding to highly subjective and -1 corresponding to highly objective." ></td>
	<td class="line x" title="113:194	It is a sum of sim scores, where sim(wi,dswj) is added for each instance of dswj that is in a subjective expression, and subtracted for each instance that is not in a subjective expression." ></td>
	<td class="line x" title="114:194	Note that the annotations in the MPQA corpus are for subjective expressions in context." ></td>
	<td class="line x" title="115:194	Thus, the data is somewhat noisy for our task, because, as discussed in Section 3, objective senses may appear in subjective expressions." ></td>
	<td class="line x" title="116:194	Nonetheless, we hypothesized that subjective senses tend to appear more often in subjective expressions than objective senses do, and use the appearance of words in subjective expressions as evidence of sense subjectivity." ></td>
	<td class="line x" title="117:194	(Wiebe, 2000) also makes use of an annotated corpus, but in a different approach: given a word w and a set of distributionally similar words DSW, that method assigns a subjectivity score to w equal to the conditional probability that any member of DSW is in a subjective expression." ></td>
	<td class="line x" title="118:194	Moreover, the end task of that work was to annotate words, while our end task is the more difficult problem of annotating word senses for subjectivity." ></td>
	<td class="line x" title="119:194	4.1 Evaluation The evaluation of the algorithm is performed against the gold standard of 64 words (354 word senses) using Judge 1s annotations, as described in Section 3." ></td>
	<td class="line x" title="120:194	For each sense of each word in the set of 64 ambiguous words, we use Algorithm 1 to determine a subjectivity score." ></td>
	<td class="line x" title="121:194	A subjectivity label is then assigned depending on the value of this score with respect to a pre-selected threshold." ></td>
	<td class="line x" title="122:194	While a threshold of 0 seems like a sensible choice, we perform the evaluation for different thresholds ranging across the [-1,+1] interval, and correspondingly determine the precision of the algorithm at different points of recall7." ></td>
	<td class="line x" title="123:194	Note that the word senses for which none of the distributionally similar words are found in the MPQA corpus are not 7Specifically, in the list of word senses ranked by their subjectivity score, we assign a subjectivity label to the top N word senses." ></td>
	<td class="line x" title="124:194	The precision is then determined as the number of correct subjectivity label assignments out of all N assignments, while the recall is measured as the correct subjective senses out of all the subjective senses in the gold standard data set." ></td>
	<td class="line x" title="125:194	By varying the value of N from 1 to the total number of senses in the corpus, we can derive precision and recall curves." ></td>
	<td class="line x" title="126:194	included in this evaluation (excluding 82 senses), since in this case a subjectivity score cannot be calculated." ></td>
	<td class="line x" title="127:194	The evaluation is therefore performed on a total of 272 word senses." ></td>
	<td class="line x" title="128:194	As a baseline, we use an informed random assignment of subjectivity labels, which randomly assigns S labels to word senses in the data set, such that the maximum number of S assignments equals the number of correct S labels in the gold standard data set." ></td>
	<td class="line x" title="129:194	This baseline guarantees a maximum recall of 1 (which under true random conditions might not be achievable)." ></td>
	<td class="line x" title="130:194	Correspondingly, given the controlled distribution of S labels across the data set in the baseline setting, the precision is equal for all eleven recall points, and is determined as the total number of correct subjective assignments divided by the size of the data set8." ></td>
	<td class="line x" title="131:194	Number Break-even Algorithm of DSW point similarity-all 100 0.41 similarity-selected 100 0.50 similarity-all 160 0.43 similarity-selected 160 0.50 baseline 0.27 Table 2: Break-even point for different algorithm and parameter settings There are two aspects of the sense subjectivity scoring algorithm that can influence the label assignment, and correspondingly their evaluation." ></td>
	<td class="line x" title="132:194	First, as indicated above, after calculating the semantic similarity of the distributionally similar words with each sense, we can either use all the distributionally similar words for the calculation of the subjectivity score of each sense (similarityall), or we can use only those that lead to the highest similarity (similarity-selected)." ></td>
	<td class="line x" title="133:194	Interestingly, this aspect can drastically affect the algorithm accuracy." ></td>
	<td class="line x" title="134:194	The setting where a distributionally similar word can belong only to one sense significantly improves the algorithm performance." ></td>
	<td class="line x" title="135:194	Figure 1 plots the interpolated precision for eleven points of recall, for similarity-all, similarity-selected, and baseline." ></td>
	<td class="line x" title="136:194	As shown in this figure, the precisionrecall curves for our algorithm are clearly above the informed baseline, indicating the ability of our algorithm to automatically identify subjective word senses." ></td>
	<td class="line x" title="137:194	Second, the number of distributionally similar words considered in the first stage of the algorithm can vary, and might therefore influence the 8In other words, this fraction represents the probability of making the correct subjective label assignment by chance." ></td>
	<td class="line x" title="138:194	1069 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Precision recall curves selected all baseline Figure 1: Precision and recall for automatic subjectivity annotations of word senses (DSW=160)." ></td>
	<td class="line x" title="139:194	output of the algorithm." ></td>
	<td class="line x" title="140:194	We experiment with two different values, namely 100 and 160 top-ranked distributionally similar words." ></td>
	<td class="line x" title="141:194	Table 2 shows the break-even points for the four different settings that were evaluated,9 with results that are almost double compared to the informed baseline." ></td>
	<td class="line x" title="142:194	As it turns out, for weaker versions of the algorithm (i.e. , similarity-all), the size of the set of distributionally similar words can significantly impact the performance of the algorithm." ></td>
	<td class="line x" title="143:194	However, for the already improved similarity-selected algorithm version, this parameter does not seem to have influence, as similar results are obtained regardless of the number of distributionally similar words." ></td>
	<td class="line x" title="144:194	This is in agreement with the finding of (McCarthy et al. , 2004) that, in their word sense ranking method, a larger set of neighbors did not influence the algorithm accuracy." ></td>
	<td class="line x" title="145:194	5 Automatic Subjectivity Annotations for Word Sense Disambiguation The final question we address is concerned with the potential impact of subjectivity on the quality of a word sense classifier." ></td>
	<td class="line x" title="146:194	To answer this question, we augment an existing data-driven word sense disambiguation system with a feature reflecting the subjectivity of the examples where the ambiguous word occurs, and evaluate the performance of the new subjectivity-aware classifier as compared to the traditional context-based sense classifier." ></td>
	<td class="line x" title="147:194	We use a word sense disambiguation system that integrates both local and topical features." ></td>
	<td class="line x" title="148:194	9The break-even point (Lewis, 1992) is a standard measure used in conjunction with precision-recall evaluations." ></td>
	<td class="line x" title="149:194	It represents the value where precision and recall become equal." ></td>
	<td class="line x" title="150:194	Specifically, we use the current word and its partof-speech, a local context of three words to the left and right of the ambiguous word, the parts-ofspeech of the surrounding words, and a global context implemented through sense-specific keywords determined as a list of at most five words occurring at least three times in the contexts defining a certain word sense." ></td>
	<td class="line x" title="151:194	This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of SENSEVAL systems." ></td>
	<td class="line x" title="152:194	The parameters for sense-specific keyword selection were determined through cross-fold validation on the training set." ></td>
	<td class="line x" title="153:194	The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002)." ></td>
	<td class="line x" title="154:194	The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al. , 2004)." ></td>
	<td class="line x" title="155:194	We use the rule-based subjective sentence classifier of (Riloff and Wiebe, 2003) to assign an S, O, or B label to all the training and test examples pertaining to these ambiguous words." ></td>
	<td class="line x" title="156:194	This subjectivity annotation tool targets sentences, rather than words or paragraphs, and therefore the tool is fed with sentences." ></td>
	<td class="line x" title="157:194	We also include a surrounding context of two additional sentences, because the classifier considers some contextual information." ></td>
	<td class="line x" title="158:194	Our hypothesis motivating the use of a sentence-level subjectivity classifier is that instances of subjective senses are more likely to be in subjective sentences, and thus that sentence subjectivity is an informative feature for the disambiguation of words having both subjective and objective senses." ></td>
	<td class="line x" title="159:194	For each ambiguous word, we perform two separate runs: one using the basic disambiguation system described earlier, and another using the subjectivity-aware system that includes the additional subjectivity feature." ></td>
	<td class="line x" title="160:194	Table 3 shows the results obtained for these 20 nouns, including word sense disambiguation accuracy for the two different systems, the most frequent sense baseline, and the subjectivity/objectivity split among the word senses (according to Judge 1)." ></td>
	<td class="line x" title="161:194	The words in the top half of the table are the ones that have both S and O senses, and those in the bottom are the ones that do not." ></td>
	<td class="line x" title="162:194	If we were to use Judge 2s tags instead of Judge 1s, only one word would change: source would move from the top to the bottom of the table." ></td>
	<td class="line x" title="163:194	1070 Sense Data Classifier Word Senses subjectivity train test Baseline basic + subj." ></td>
	<td class="line x" title="164:194	Words with subjective senses argument 5 3-S 2-O 221 111 49.4% 51.4% 54.1% atmosphere 6 2-S 4-O 161 81 65.4% 65.4% 66.7% difference 5 2-S 3-O 226 114 40.4% 54.4% 57.0% difficulty 4 2-S 2-O 46 23 17.4% 47.8% 52.2% image 7 2-S 5-O 146 74 36.5% 41.2% 43.2% interest 7 1-S 5-O 1-B 185 93 41.9% 67.7% 68.8% judgment 7 5-S 2-O 62 32 28.1% 40.6% 43.8% plan 3 1-S 2-O 166 84 81.0% 81.0% 81.0% sort 4 1-S 2-O 1-B 190 96 65.6% 66.7% 67.7% source 9 1-S 8-O 64 32 40.6% 40.6% 40.6% Average 46.6% 55.6% 57.5% Words with no subjective senses arm 6 6-O 266 133 82.0% 85.0% 84.2% audience 4 4-O 200 100 67.0% 74.0% 74.0% bank 10 10-O 262 132 62.6% 62.6% 62.6% degree 7 5-O 2-B 256 128 60.9% 71.1% 71.1% disc 4 4-O 200 100 38.0% 65.6% 66.4% organization 7 7-O 112 56 64.3% 64.3% 64.3% paper 7 7-O 232 117 25.6% 49.6% 48.0% party 5 5-O 230 116 62.1% 62.9% 62.9% performance 5 5-O 172 87 26.4% 34.5% 34.5% shelter 5 5-O 196 98 44.9% 65.3% 65.3% Average 53.3% 63.5% 63.3% Average for all words 50.0% 59.5% 60.4% Table 3: Word Sense Disambiguation with and without subjectivity information, for the set of ambiguous nouns in SENSEVAL-3 For the words that have both S and O senses, the addition of the subjectivity feature alone can bring a significant error rate reduction of 4.3% (p < 0.05 paired t-test)." ></td>
	<td class="line x" title="165:194	Interestingly, no improvements are observed for the words with no subjective senses; on the contrary, the addition of the subjectivity feature results in a small degradation." ></td>
	<td class="line x" title="166:194	Overall for the entire set of ambiguous words, the error reduction is measured at 2.2% (significant at p < 0.1 paired t-test)." ></td>
	<td class="line x" title="167:194	In almost all cases, the words with both S and O senses show improvement, while the others show small degradation or no change." ></td>
	<td class="line x" title="168:194	This suggests that if a subjectivity label is available for the words in a lexical resource (e.g. using Algorithm 1 from Section 4), such information can be used to decide on using a subjectivity-aware system, thereby improving disambiguation accuracy." ></td>
	<td class="line x" title="169:194	One of the exceptions is disc, which had a small benefit, despite not having any subjective senses." ></td>
	<td class="line x" title="170:194	As it happens, the first sense of disc is phonograph record." ></td>
	<td class="line x" title="171:194	phonograph record, phonograph recording, record, disk, disc, platter  (sound recording consisting of a disc with continuous grooves; formerly used to reproduce music by rotating while a phonograph needle tracked in the grooves) The improvement can be explained by observing that many of the training and test sentences containing this sense are labeled subjective by the classifier, and indeed this sense frequently occurs in subjective sentences such as This is anyway a stunning disc. Another exception is the noun plan, which did not benefit from the subjectivity feature, although it does have a subjective sense." ></td>
	<td class="line x" title="172:194	This can perhaps be explained by the data set for this word, which seems to be particularly difficult, as the basic classifier itself could not improve over the most frequent sense baseline." ></td>
	<td class="line x" title="173:194	The other word that did not benefit from the subjectivity feature is the noun source, for which its only subjective sense did not appear in the sense-annotated data, leading therefore to an objective only set of examples." ></td>
	<td class="line x" title="174:194	6 Conclusion and Future Work The questions posed in the introduction concerning the possible interaction between subjectivity and word sense found answers throughout the paper." ></td>
	<td class="line x" title="175:194	As it turns out, a correlation can indeed be established between these two semantic properties of language." ></td>
	<td class="line x" title="176:194	Addressing the first question of whether subjectivity is a property that can be assigned to word senses, we showed that good agreement (=0.74) can be achieved between human annotators labeling the subjectivity of senses." ></td>
	<td class="line x" title="177:194	When uncertain cases are removed, the  value is even higher (0.90)." ></td>
	<td class="line x" title="178:194	Moreover, the automatic subjectivity scoring mechanism that we devised was able to successfully assign subjectivity labels to senses, significantly outperforming an informed baseline associated with the task." ></td>
	<td class="line x" title="179:194	While much work remains to be done, this first attempt has proved the feasibility of correctly assigning subjectivity labels to the fine-grained level of word senses." ></td>
	<td class="line x" title="180:194	The second question was also positively answered: the quality of a word sense disambiguation system can be improved with the addition of subjectivity information." ></td>
	<td class="line x" title="181:194	Section 5 provided evidence that automatic subjectivity classification may improve word sense disambiguation performance, but mainly for words with both subjective and objective senses." ></td>
	<td class="line x" title="182:194	As we saw, performance may even degrade for words that do not." ></td>
	<td class="line x" title="183:194	Tying the pieces of this paper together, once the senses in a dictionary have been assigned subjectivity labels, a word sense disambiguation system could consult them to decide whether it should consider or ignore the subjectivity feature." ></td>
	<td class="line x" title="184:194	There are several other ways our results could impact future work." ></td>
	<td class="line x" title="185:194	Subjectivity labels would be a useful source of information when manually augmenting the lexical knowledge in a dictionary, 1071 e.g., when choosing hypernyms for senses or deciding which senses to eliminate when defining a coarse-grained sense inventory (if there is a subjective sense, at least one should be retained)." ></td>
	<td class="line x" title="186:194	Adding subjectivity labels to WordNet could also support automatic subjectivity analysis." ></td>
	<td class="line x" title="187:194	First, the input corpus could be sense tagged and the subjectivity labels of the assigned senses could be exploited by a subjectivity recognition tool." ></td>
	<td class="line x" title="188:194	Second, a number of methods for subjectivity or sentiment analysis start with a set of seed words and then search through WordNet to find other subjective words (Kamps and Marx, 2002; Yu and Hatzivassiloglou, 2003; Hu and Liu, 2004; Kim and Hovy, 2004; Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="189:194	However, such searches may veer off course down objective paths." ></td>
	<td class="line x" title="190:194	The subjectivity labels assigned to senses could be consulted to keep the search traveling along subjective paths." ></td>
	<td class="line x" title="191:194	Finally, there could be different strategies for exploiting subjectivity annotations and word sense." ></td>
	<td class="line x" title="192:194	While the current setting considered a pipeline approach, where the output of a subjectivity annotation system was fed to the input of a method for semantic disambiguation, future work could also consider the role of word senses as a possible way of improving subjectivity analysis, or simultaneous annotations of subjectivity and word meanings, as done in the past for other language processing problems." ></td>
	<td class="line x" title="193:194	Acknowledgments We would like to thank Theresa Wilson for annotating senses, and the anonymous reviewers for their helpful comments." ></td>
	<td class="line x" title="194:194	This work was partially supported by ARDA AQUAINT and by the NSF (award IIS-0208798)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2059
Automatic Construction Of Polarity-Tagged Corpus From HTML Documents
Kaji, Nobuhiro;Kitsuregawa, Masaru;"></td>
	<td class="line x" title="1:288	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 452459, Sydney, July 2006." ></td>
	<td class="line x" title="2:288	c2006 Association for Computational Linguistics Automatic Construction of Polarity-tagged Corpus from HTML Documents Nobuhiro Kaji and Masaru Kitsuregawa Institute of Industrial Science the University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan CUkaji,kitsureCV@tkl.iis.u-tokyo.ac.jp Abstract This paper proposes a novel method of building polarity-tagged corpus from HTML documents." ></td>
	<td class="line x" title="3:288	The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents." ></td>
	<td class="line x" title="4:288	The idea behind our method is to utilize certain layout structures and linguistic pattern." ></td>
	<td class="line x" title="5:288	By using them, we can automatically extract such sentences that express opinion." ></td>
	<td class="line x" title="6:288	In our experiment, the method could construct a corpus consisting of 126,610 sentences." ></td>
	<td class="line x" title="7:288	1 Introduction Recently, there has been an increasing interest in such applications that deal with opinions (a.k.a. sentiment, reputation etc.)." ></td>
	<td class="line x" title="8:288	For instance, Morinaga et al. developed a system that extracts and analyzes reputations on the Internet (Morinaga et al. , 2002)." ></td>
	<td class="line x" title="9:288	Pang et al. proposed a method of classifying movie reviews into positive and negative ones (Pang et al. , 2002)." ></td>
	<td class="line x" title="10:288	In these applications, one of the most important issue is how to determine the polarity (or semantic orientation) of a given text." ></td>
	<td class="line x" title="11:288	In other words, it is necessary to decide whether a given text conveys positive or negative content." ></td>
	<td class="line x" title="12:288	In order to solve this problem, we intend to take statistical approach." ></td>
	<td class="line x" title="13:288	More specifically, we plan to learn the polarity of texts from a corpus in which phrases, sentences or documents are tagged with labels expressing the polarity (polarity-tagged corpus)." ></td>
	<td class="line x" title="14:288	So far, this approach has been taken by a lot of researchers (Pang et al. , 2002; Dave et al. , 2003; Wilson et al. , 2005)." ></td>
	<td class="line x" title="15:288	In these previous works, polarity-tagged corpus was built in either of the following two ways." ></td>
	<td class="line x" title="16:288	It is built manually, or created from review sites such as AMAZON.COM." ></td>
	<td class="line x" title="17:288	In some review sites, the review is associated with metadata indicating its polarity." ></td>
	<td class="line x" title="18:288	Those reviews can be used as polarity-tagged corpus." ></td>
	<td class="line x" title="19:288	In case of AMAZON.COM, the reviews polarity is represented by using 5-star scale." ></td>
	<td class="line x" title="20:288	However, both of the two approaches are not appropriate for building large polarity-tagged corpus." ></td>
	<td class="line x" title="21:288	Since manual construction of tagged corpus is time-consuming and expensive, it is difficult to build large polarity-tagged corpus." ></td>
	<td class="line x" title="22:288	The method that relies on review sites can not be applied to domains in which large amount of reviews are not available." ></td>
	<td class="line x" title="23:288	In addition, the corpus created from reviews is often noisy as we discuss in Section 2." ></td>
	<td class="line x" title="24:288	This paper proposes a novel method of building polarity-tagged corpus from HTML documents." ></td>
	<td class="line x" title="25:288	The idea behind our method is to utilize certain layout structures and linguistic pattern." ></td>
	<td class="line x" title="26:288	By using them, we can automatically extract sentences that express opinion (opinion sentences) from HTML documents." ></td>
	<td class="line x" title="27:288	Because this method is fully automatic and can be applied to arbitrary HTML documents, it does not suffer from the same problems as the previous methods." ></td>
	<td class="line x" title="28:288	In the experiment, we could construct a corpus consisting of 126,610 sentences." ></td>
	<td class="line x" title="29:288	To validate the quality of the corpus, two human judges assessed a part of the corpus and found that 92% opinion sentences are appropriate ones." ></td>
	<td class="line x" title="30:288	Furthermore, we applied our corpus to opinion sentence classification task." ></td>
	<td class="line x" title="31:288	Naive Bayes classifier was trained on our corpus and tested on three data sets." ></td>
	<td class="line x" title="32:288	The result demonstrated that the classifier achieved more than 80% accuracy in each data set." ></td>
	<td class="line x" title="33:288	The following of this paper is organized as fol452 lows." ></td>
	<td class="line x" title="34:288	Section 2 shows the design of the corpus constructed by our method." ></td>
	<td class="line x" title="35:288	Section 3 gives an overview of our method, and the detail follows in Section 4." ></td>
	<td class="line x" title="36:288	In Section 5, we discuss experimental results, and in Section 6 we examine related works." ></td>
	<td class="line x" title="37:288	Finally we conclude in Section 7." ></td>
	<td class="line x" title="38:288	2 Corpus Design This Section explains the design of our corpus that is built automatically." ></td>
	<td class="line x" title="39:288	Table 1 represents a part of our corpus that was actually constructed in the experiment." ></td>
	<td class="line x" title="40:288	Note that this paper treats Japanese." ></td>
	<td class="line x" title="41:288	The sentences in the Table are translations, and the original sentences are in Japanese." ></td>
	<td class="line x" title="42:288	The followings are characteristics of our corpus: AF Our corpus uses two labels, B7 and A0." ></td>
	<td class="line x" title="43:288	They denote positive and negative sentences respectively." ></td>
	<td class="line x" title="44:288	Other labels such as neutral are not used." ></td>
	<td class="line x" title="45:288	AF Since we do not use neutral label, such sentence that does not convey opinion is not stored in our corpus." ></td>
	<td class="line x" title="46:288	AF The label is assigned to not multiple sentences (or document) but single sentence." ></td>
	<td class="line x" title="47:288	Namely, our corpus is tagged at sentence level rather than document level." ></td>
	<td class="line x" title="48:288	It is important to discuss the reason that we intend to build a corpus tagged at sentence level rather than document level." ></td>
	<td class="line x" title="49:288	The reason is that one document often includes both positive and negative sentences, and hence it is difficult to learn the polarity from the corpus tagged at document level." ></td>
	<td class="line x" title="50:288	Consider the following example (Pang et al. , 2002): This film should be brilliant." ></td>
	<td class="line x" title="51:288	It sounds like a great plot, the actors are first grade, and the supporting cast is good as well, and Stallone is attempting to deliver a good performance." ></td>
	<td class="line x" title="52:288	However, it cant hold up." ></td>
	<td class="line x" title="53:288	This document as a whole expresses negative opinion, and should be labeled negative if it is tagged at document level." ></td>
	<td class="line x" title="54:288	However, it includes several sentences that represent positive attitude." ></td>
	<td class="line x" title="55:288	We would like to point out that polarity-tagged corpus created from reviews prone to be tagged at document-level." ></td>
	<td class="line x" title="56:288	This is because meta-data (e.g. stars in AMAZON.COM) is usually associated with one review rather than individual sentences in a review." ></td>
	<td class="line x" title="57:288	This is one serious problem in previous works." ></td>
	<td class="line x" title="58:288	Table 1: A part of automatically constructed polarity-tagged corpus." ></td>
	<td class="line x" title="59:288	label opinion sentence B7 It has high adaptability." ></td>
	<td class="line x" title="60:288	A0 The cost is expensive." ></td>
	<td class="line x" title="61:288	A0 The engine is powerless and noisy." ></td>
	<td class="line x" title="62:288	B7 The usage is easy to understand." ></td>
	<td class="line x" title="63:288	B7 Above all, the price is reasonable." ></td>
	<td class="line x" title="64:288	3 The Idea This Section briefly explains our basic idea, and the detail of our corpus construction method is represented in the next Section." ></td>
	<td class="line x" title="65:288	Our idea is to use certain layout structures and linguistic pattern in order to extract opinion sentences from HTML documents." ></td>
	<td class="line x" title="66:288	More specifically, we used two kinds of layout structures: the itemization and the table." ></td>
	<td class="line x" title="67:288	In what follows, we explain examples where opinion sentences can be extracted by using the itemization, table and linguistic pattern." ></td>
	<td class="line x" title="68:288	3.1 Itemization The first idea is to extract opinion sentences from the itemization (Figure 1)." ></td>
	<td class="line x" title="69:288	In this Figure, opinions about a music player are itemized and these itemizations have headers such as pros and cons." ></td>
	<td class="line x" title="70:288	By using the headers, we can recognize that opinion sentences are described in these itemizations." ></td>
	<td class="line x" title="71:288	Pros: AF The sound is natural." ></td>
	<td class="line x" title="72:288	AF Music is easy to find." ></td>
	<td class="line x" title="73:288	AF Can enjoy creating my favorite play-lists." ></td>
	<td class="line x" title="74:288	Cons: AF The remote controller does not have an LCD display." ></td>
	<td class="line x" title="75:288	AF The body gets scratched and fingerprinted easily." ></td>
	<td class="line x" title="76:288	AF The battery drains quickly when using the backlight." ></td>
	<td class="line x" title="77:288	Figure 1: Opinion sentences in itemization." ></td>
	<td class="line x" title="78:288	Hereafter, such phrases that indicate the pres453 ence of opinion sentences are called indicators." ></td>
	<td class="line x" title="79:288	Indicators for positive sentences are called positive indicators." ></td>
	<td class="line x" title="80:288	Pros is an example of positive indicator." ></td>
	<td class="line x" title="81:288	Similarly, indicators for negative sentences are called negative indicators." ></td>
	<td class="line x" title="82:288	3.2 Table The second idea is to use the table structure (Figure 2)." ></td>
	<td class="line x" title="83:288	In this Figure, a car review is summarized in the table." ></td>
	<td class="line x" title="84:288	Mileage(urban) 7.0km/litter Mileage(highway) 9.0km/litter Plus This is a four door car, but its so cool." ></td>
	<td class="line x" title="85:288	Minus The seat is ragged and the light is dark." ></td>
	<td class="line x" title="86:288	Figure 2: Opinion sentences in table." ></td>
	<td class="line x" title="87:288	We can predict that there are opinion sentences in this table, because the left column acts as a header and there are indicators (plus and minus) in that column." ></td>
	<td class="line x" title="88:288	3.3 Linguistic pattern The third idea is based on linguistic pattern." ></td>
	<td class="line x" title="89:288	Because we treat Japanese, the pattern that is discussed in this paper depends on Japanese grammar although we think there are similar patterns in other languages including English." ></td>
	<td class="line x" title="90:288	Consider the Japanese sentences attached with English translations (Figure 3)." ></td>
	<td class="line x" title="91:288	Japanese sentences are written in italics and - denotes that the word is followed by postpositional particles." ></td>
	<td class="line x" title="92:288	For example, software-no means that software is followed by postpositional particle no." ></td>
	<td class="line x" title="93:288	Translations of each word and the entire sentence are represented below the original Japanese sentence." ></td>
	<td class="line x" title="94:288	-POST means postpositional particle." ></td>
	<td class="line x" title="95:288	In the examples, we focused on the singly underlined phrases." ></td>
	<td class="line x" title="96:288	Roughly speaking, they correspond to the advantage/weakness is to in English." ></td>
	<td class="line x" title="97:288	In these phrases, indicators (riten (advantage) and ketten (weakness)) are followed by postpositional particle -ha, which is topic marker." ></td>
	<td class="line x" title="98:288	And hence, we can recognize that something good (or bad) is the topic of the sentence." ></td>
	<td class="line x" title="99:288	Based on this observation, we crafted a linguistic pattern that can detect the singly underlined phrases." ></td>
	<td class="line x" title="100:288	And then, we extracted doubly underlined phrases as opinions." ></td>
	<td class="line x" title="101:288	They correspond to run quickly and take too much time." ></td>
	<td class="line x" title="102:288	The detail of this process is discussed in the next Section." ></td>
	<td class="line x" title="103:288	4 Automatic Corpus Construction This Section represents the detail of the corpus construction procedure." ></td>
	<td class="line x" title="104:288	As shown in the previous Section, our idea utilizes the indicator, and it is important to recognize indicators in HTML documents." ></td>
	<td class="line x" title="105:288	To do this, we manually crafted lexicon, in which positive and negative indicators are listed." ></td>
	<td class="line x" title="106:288	This lexicon consists of 303 positive and 433 negative indicators." ></td>
	<td class="line x" title="107:288	Using this lexicon, the polarity-tagged corpus is constructed from HTML documents." ></td>
	<td class="line x" title="108:288	The method consists of the following three steps: 1." ></td>
	<td class="line x" title="109:288	Preprocessing Before extracting opinion sentences, HTML documents are preprocessed." ></td>
	<td class="line x" title="110:288	This process involves separating texts form HTML tags, recognizing sentence boundary, and complementing omitted HTML tags etc. 2." ></td>
	<td class="line x" title="111:288	Opinion sentence extraction Opinion sentences are extracted from HTML documents by using the itemization, table and linguistic pattern." ></td>
	<td class="line x" title="112:288	3." ></td>
	<td class="line x" title="113:288	Filtering Since HTML documents are noisy, some of the extracted opinion sentences are not appropriate." ></td>
	<td class="line x" title="114:288	They are removed in this step." ></td>
	<td class="line x" title="115:288	For the preprocessing, we implemented simple rule-based system." ></td>
	<td class="line x" title="116:288	We cannot explain its detail for lack of space." ></td>
	<td class="line x" title="117:288	In the remainder of this Section, we describe three extraction methods respectively, and then examine filtering technique." ></td>
	<td class="line x" title="118:288	4.1 Extraction based on itemization The first method utilizes the itemization." ></td>
	<td class="line x" title="119:288	In order to extract opinion sentences, first of all, we have to find such itemization as illustrated in Figure 1." ></td>
	<td class="line x" title="120:288	They are detected by using indicator lexicon and HTML tags such as BOh1BQ and BOulBQ etc. After finding the itemizations, the sentences in the items are extracted as opinion sentences." ></td>
	<td class="line x" title="121:288	Their polarity labels are assigned according to whether the header is positive or negative indicator." ></td>
	<td class="line x" title="122:288	From the itemization in Figure 1, three positive sentences and three negative ones are extracted." ></td>
	<td class="line x" title="123:288	The problem here is how to treat such item that has more than one sentences (Figure 4)." ></td>
	<td class="line x" title="124:288	In this itemization, there are two sentences in each of the 454 (1) kono software-no riten-ha hayaku ugoku koto this software-POST advantage-POST quickly run to The advantage of this software is to run quickly." ></td>
	<td class="line x" title="125:288	(2) ketten-ha jikan-ga kakarisugiru koto-desu weakness-POST time-POST take too much to-POST The weakness is to take too much time." ></td>
	<td class="line x" title="126:288	Figure 3: Instances of the linguistic pattern." ></td>
	<td class="line x" title="127:288	third and fourth item." ></td>
	<td class="line x" title="128:288	It is hard to precisely predict the polarity of each sentence in such items, because such item sometimes includes both positive and negative sentences." ></td>
	<td class="line x" title="129:288	For example, in the third item of the Figure, there are two sentences." ></td>
	<td class="line x" title="130:288	One (Has high pixel) is positive and the other (I was not satisfied) is negative." ></td>
	<td class="line x" title="131:288	To get around this problem, we did not use such items." ></td>
	<td class="line x" title="132:288	From the itemization in Figure 4, only two positive sentences are extracted (the color is really good and this camera makes me happy while taking pictures)." ></td>
	<td class="line x" title="133:288	Pros: AF The color is really good." ></td>
	<td class="line x" title="134:288	AF This camera makes me happy while taking pictures." ></td>
	<td class="line x" title="135:288	AF Has high pixel resolution with 4 million pixels." ></td>
	<td class="line x" title="136:288	I was not satisfied with 2 million." ></td>
	<td class="line x" title="137:288	AF EVF is easy to see." ></td>
	<td class="line x" title="138:288	But, compared with SLR, its hard to see." ></td>
	<td class="line x" title="139:288	Figure 4: Itemization where more than one sentences are written in one item." ></td>
	<td class="line x" title="140:288	4.2 Extraction based on table The second method extracts opinion sentences from the table." ></td>
	<td class="line x" title="141:288	Since the combination of BOtableBQ and other tags can represent various kinds of tables, it is difficult to craft precise rules that can deal with any table." ></td>
	<td class="line x" title="142:288	Therefore, we consider only two types of tables in which opinion sentences are described (Figure 5)." ></td>
	<td class="line x" title="143:288	Type A is a table in which the leftmost column acts as a header, and there are indicators in that column." ></td>
	<td class="line x" title="144:288	Similarly, type B is a table in which the first row acts as a header." ></td>
	<td class="line x" title="145:288	The table illustrated in Figure 2 is categorized into type A. The type of the table is decided as follows." ></td>
	<td class="line x" title="146:288	The table is categorized into type A if there are both type A C1 B7 B7 B7 B7 C1 A0 A0 A0 A0 type B yC1 B7 C1 A0 y yB7 A0y yB7 A0y yB7 A0y C1 B7 :positive indicator B7:positive sentence C1 A0 :negative indicator A0:negative sentence Figure 5: Two types of tables." ></td>
	<td class="line x" title="147:288	positive and negative indicators in the leftmost column." ></td>
	<td class="line x" title="148:288	The table is categorized into type B if it is not type A and there are both positive and negative indicators in the first row." ></td>
	<td class="line x" title="149:288	After the type of the table is decided, we can extract opinion sentences from the cells that correspond to B7 and A0 in the Figure 5." ></td>
	<td class="line x" title="150:288	It is obvious which label (positive or negative) should be assigned to the extracted sentence." ></td>
	<td class="line x" title="151:288	We did not use such cell that contains more than one sentences, because it is difficult to reliably predict the polarity of each sentence." ></td>
	<td class="line x" title="152:288	This is similar to the extraction from the itemization." ></td>
	<td class="line x" title="153:288	4.3 Extraction based on linguistic pattern The third method uses linguistic pattern." ></td>
	<td class="line x" title="154:288	The characteristic of this pattern is that it takes dependency structure into consideration." ></td>
	<td class="line x" title="155:288	First of all, we explain Japanese dependency structure." ></td>
	<td class="line x" title="156:288	Figure 6 depicts the dependency representations of the sentences in the Figure 3." ></td>
	<td class="line x" title="157:288	Japanese sentence is represented by a set of dependencies between phrasal units called bunsetsuphrases." ></td>
	<td class="line x" title="158:288	Broadly speaking, bunsetsu-phrase is an unit similar to baseNP in English." ></td>
	<td class="line x" title="159:288	In the Figure, square brackets enclose bunsetsu-phrase and arrows show modifier AX head dependencies between bunsetsu-phrases." ></td>
	<td class="line x" title="160:288	In order to extract opinion sentences from these dependency representations, we crafted the following dependency pattern." ></td>
	<td class="line x" title="161:288	455 [ kono this ][ software-no software-POST ][ riten-ha advantage-POST ][ hayaku quickly ][ ugoku run ][ koto to ] [ ketten-ha weakness-POST ][ jikan-ga time-POST ][ kakari sugiru take too much ][ koto-desu to-POST ] Figure 6: Dependency representations." ></td>
	<td class="line x" title="162:288	[ INDICATOR-ha ][koto-POST*] This pattern matches the singly underlined bunsetsu-phrases in the Figure 6." ></td>
	<td class="line x" title="163:288	In the modifier part of this pattern, the indicator is followed by postpositional particle ha, which is topic marker 1." ></td>
	<td class="line x" title="164:288	In the head part, koto (to) is followed by arbitrary numbers of postpositional particles." ></td>
	<td class="line x" title="165:288	If we find the dependency that matches this pattern, a phrase between the two bunsetsu-phrases is extracted as opinion sentence." ></td>
	<td class="line x" title="166:288	In the Figure 6, the doubly underlined phrases are extracted." ></td>
	<td class="line x" title="167:288	This heuristics is based on Japanese word order constraint." ></td>
	<td class="line x" title="168:288	4.4 Filtering Sentences extracted by the above methods sometimes include noise text." ></td>
	<td class="line x" title="169:288	Such texts have to be filtered out." ></td>
	<td class="line x" title="170:288	There are two cases that need filtering process." ></td>
	<td class="line x" title="171:288	First, some of the extracted sentences do not express opinions." ></td>
	<td class="line x" title="172:288	Instead, they represent objects to which the writers opinion is directed (Table 7)." ></td>
	<td class="line x" title="173:288	From this table, the overall shape and the shape of the taillight are wrongly extracted as opinion sentences." ></td>
	<td class="line x" title="174:288	Since most of the objects are noun phrases, we removed such sentences that have the noun as the head." ></td>
	<td class="line x" title="175:288	Mileage(urban) 10.0km/litter Mileage(highway) 12.0km/litter Plus The overall shape." ></td>
	<td class="line x" title="176:288	Minus The shape of the taillight." ></td>
	<td class="line x" title="177:288	Figure 7: A table describing only objects to which the opinion is directed." ></td>
	<td class="line x" title="178:288	Secondly, we have to treat duplicate opinion sentences because there are mirror sites in the 1 To be exact, some of the indicators such as strong point consists of more than one bunsetsu-phrase, and the modifier part sometimes consists of more than one bunsetsu-phrase." ></td>
	<td class="line x" title="179:288	HTML documents." ></td>
	<td class="line x" title="180:288	When there are more than one sentences that are exactly the same, one of them is held and the others are removed." ></td>
	<td class="line x" title="181:288	5 Experimental Results and Discussion This Section examines the results of corpus construction experiment." ></td>
	<td class="line x" title="182:288	To analyze Japanese sentence we used Juman and KNP 2 . 5.1 Corpus Construction About 120 millions HTML documents were processed, and 126,610 opinion sentences were extracted." ></td>
	<td class="line x" title="183:288	Before the filtering, there were 224,002 sentences in our corpus." ></td>
	<td class="line x" title="184:288	Table2 shows the statistics of our corpus." ></td>
	<td class="line x" title="185:288	The first column represents the three extraction methods." ></td>
	<td class="line x" title="186:288	The second and third column shows the number of positive and negative sentences by extracted each method." ></td>
	<td class="line x" title="187:288	Some examples are illustrated in Table 3." ></td>
	<td class="line x" title="188:288	Table 2: # of sentences in the corpus." ></td>
	<td class="line x" title="189:288	Positive Negative Total Itemization 18,575 15,327 33,902 Table 12,103 11,016 23,119 Linguistic Pattern 34,282 35,307 69,589 Total 64,960 61,650 126,610 The result revealed that more than half of the sentences are extracted by linguistic pattern (see the fourth row)." ></td>
	<td class="line x" title="190:288	Our method turned out to be effective even in the case where only plain texts are available." ></td>
	<td class="line x" title="191:288	5.2 Quality assessment In order to check the quality of our corpus, 500 sentences were randomly picked up and two judges manually assessed whether appropriate labels are assigned to the sentences." ></td>
	<td class="line x" title="192:288	The evaluation procedure is the followings." ></td>
	<td class="line x" title="193:288	2 http://www.kc.t.u-tokyo.ac.jp/nl-resource/top.html 456 Table 3: Examples of opinion sentences." ></td>
	<td class="line x" title="194:288	label opinion sentence B7 cost keisan-ga yoininaru cost computation-POST become easy It becomes easy to compute cost." ></td>
	<td class="line x" title="195:288	B7 kantan-de jikan-ga setsuyakudekiru easy-POST time-POST can save Its easy and can save time." ></td>
	<td class="line x" title="196:288	B7 soup-ha koku-ga ari oishii soup-POST rich flavorful The soup is rich and flavorful." ></td>
	<td class="line x" title="197:288	A0 HTML keishiki-no mail-ni taioshitenai HTML format-POST mail-POST cannot use Cannot use mails in HTML format." ></td>
	<td class="line x" title="198:288	A0 jugyo-ga hijoni tsumaranai lecture-POST really boring The lecture is really boring." ></td>
	<td class="line x" title="199:288	A0 kokoro-ni nokoru ongaku-ga nai impressive music-POST there is no There is no impressive music." ></td>
	<td class="line x" title="200:288	AF Each of the 500 sentences are shown to the two judges." ></td>
	<td class="line x" title="201:288	Throughout this evaluation, We did not present the label automatically tagged by our method." ></td>
	<td class="line x" title="202:288	Similarly, we did not show HTML documents from which the opinion sentences are extracted." ></td>
	<td class="line x" title="203:288	AF The two judges individually categorized each sentence into three groups: positive, negative and neutral/ambiguous." ></td>
	<td class="line x" title="204:288	The sentence is classified into the third group, if it does not express opinion (neutral) or if its polarity depends on the context (ambiguous)." ></td>
	<td class="line x" title="205:288	Thus, two goldstandard sets were created." ></td>
	<td class="line x" title="206:288	AF The precision is estimated using the goldstandard." ></td>
	<td class="line x" title="207:288	In this evaluation, the precision refers to the ratio of sentences where correct labels are assigned by our method." ></td>
	<td class="line x" title="208:288	Since we have two goldstandard sets, we can report two different precision values." ></td>
	<td class="line x" title="209:288	A sentence that is categorized into neutral/ambiguous by the judge is interpreted as being assigned incorrect label by our method, since our corpus does not have a label that corresponds to neutral/ambiguous." ></td>
	<td class="line x" title="210:288	We investigated the two goldstandard sets, and found that the judges agree with each other in 467 out of 500 sentences (93.4%)." ></td>
	<td class="line x" title="211:288	The Kappa value was 0.901." ></td>
	<td class="line x" title="212:288	From this result, we can say that the goldstandard was reliably created by the judges." ></td>
	<td class="line x" title="213:288	Then, we estimated the precision." ></td>
	<td class="line x" title="214:288	The precision was 459/500 (91.5%) when one goldstandard was used, and 460/500 (92%) when the other was used." ></td>
	<td class="line x" title="215:288	Since these values are nearly equal to the agreement between humans (467/500), we can conclude that our method successfully constructed polaritytagged corpus." ></td>
	<td class="line x" title="216:288	After the evaluation, we analyzed errors and found that most of them were caused by the lack of context." ></td>
	<td class="line x" title="217:288	The following is a typical example." ></td>
	<td class="line x" title="218:288	You see, there is much information." ></td>
	<td class="line x" title="219:288	In our corpus this sentence is categorized into positive one." ></td>
	<td class="line x" title="220:288	The below is a part of the original document from which this sentence was extracted." ></td>
	<td class="line x" title="221:288	I recommend this guide book." ></td>
	<td class="line x" title="222:288	The Pros." ></td>
	<td class="line x" title="223:288	of this book is that, you see, there is much information." ></td>
	<td class="line x" title="224:288	On the other hand, both of the two judges categorized the above sentence into neutral/ambiguous, probably because they can easily assume context where much information is not desirable." ></td>
	<td class="line x" title="225:288	You see, there is much information." ></td>
	<td class="line x" title="226:288	But, it is not at all arranged, and makes me confused." ></td>
	<td class="line x" title="227:288	In order to precisely treat this kind of sentences, we think discourse analysis is inevitable." ></td>
	<td class="line x" title="228:288	5.3 Application to opinion classification Next, we applied our corpus to opinion sentence classification." ></td>
	<td class="line x" title="229:288	This is a task of classifying sentences into positive and negative." ></td>
	<td class="line x" title="230:288	We trained a classifier on our corpus and investigated the result." ></td>
	<td class="line x" title="231:288	Classifier and data sets As a classifier, we chose Naive Bayes with bag-of-words features, because it is one of the most popular one in this task." ></td>
	<td class="line x" title="232:288	Negation was processed in a similar way as previous works (Pang et al. , 2002)." ></td>
	<td class="line x" title="233:288	To validate the accuracy of the classifier, three data sets were created from review pages in which the review is associated with meta-data." ></td>
	<td class="line x" title="234:288	To build data sets tagged at sentence level, we used such reviews that contain only one sentence." ></td>
	<td class="line x" title="235:288	Table 4 represents the domains and the number of sentences in each data set." ></td>
	<td class="line x" title="236:288	Note that we confirmed there is no duplicate between our corpus and the these data sets." ></td>
	<td class="line x" title="237:288	The result and discussion Naive Bayes classifier was trained on our corpus and tested on the three data sets (Table 5)." ></td>
	<td class="line x" title="238:288	In the Table, the second column represents the accuracy of the classification in each data set." ></td>
	<td class="line x" title="239:288	The third and fourth 457 Table 5: Classification result." ></td>
	<td class="line x" title="240:288	Accuracy Positive Negative Precision Recall Precision Recall Computer 0.831 0.856 0.804 0.804 0.859 Restaurant 0.849 0.905 0.859 0.759 0.832 Car 0.833 0.860 0.844 0.799 0.819 Table 4: The data sets." ></td>
	<td class="line x" title="241:288	Domain # of sentences Positive Negative Computer 933 910 Restaurant 753 409 Car 1,056 800 columns represent precision and recall of positive sentences." ></td>
	<td class="line x" title="242:288	The remaining two columns show those of negative sentences." ></td>
	<td class="line x" title="243:288	Naive Bayes achieved over 80% accuracy in all the three domains." ></td>
	<td class="line x" title="244:288	In order to compare our corpus with a small domain specific corpus, we estimated accuracy in each data set using 10 fold crossvalidation (Table 6)." ></td>
	<td class="line x" title="245:288	In two domains, the result of our corpus outperformed that of the crossvalidation." ></td>
	<td class="line x" title="246:288	In the other domain, our corpus is slightly better than the crossvalidation." ></td>
	<td class="line x" title="247:288	Table 6: Accuracy comparison." ></td>
	<td class="line x" title="248:288	Our corpus Crossvalidation Computer 0.831 0.821 Restaurant 0.849 0.848 Car 0.833 0.808 One finding is that our corpus achieved good accuracy, although it includes various domains and is not accustomed to the target domain." ></td>
	<td class="line pc" title="249:288	Turney also reported good result without domain customization (Turney, 2002)." ></td>
	<td class="line n" title="250:288	We think these results can be further improved by domain adaptation technique, and it is one future work." ></td>
	<td class="line x" title="251:288	Furthermore, we examined the variance of the accuracy between different domains." ></td>
	<td class="line x" title="252:288	We trained Naive Bayes on each data set and investigate the accuracy in the other data sets (Table 7)." ></td>
	<td class="line x" title="253:288	For example, when the classifier is trained on Computer and tested on Restaurant, the accuracy was 0.757." ></td>
	<td class="line x" title="254:288	This result revealed that the accuracy is quite poor when the training and test sets are in different domains." ></td>
	<td class="line x" title="255:288	On the other hand, when Naive Bayes is trained on our corpus, there are little variance in different domains (Table 5)." ></td>
	<td class="line x" title="256:288	This experiment indicates that our corpus is relatively robust against the change of the domain compared with small domain specific corpus." ></td>
	<td class="line x" title="257:288	We think this is because our corpus is large and balanced." ></td>
	<td class="line x" title="258:288	Since we cannot always get domain specific corpus in real application, this is the strength of our corpus." ></td>
	<td class="line x" title="259:288	Table 7: Cross domain evaluation." ></td>
	<td class="line x" title="260:288	Training Computer Restaurant Car Computer  0.701 0.773 Test Restaurant 0.757  0.755 Car 0.751 0.711  6 Related Works 6.1 Learning the polarity of words There are some works that discuss learning the polarity of words instead of sentences." ></td>
	<td class="line x" title="261:288	Hatzivassiloglou and McKeown proposed a method of learning the polarity of adjectives from corpus (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="262:288	They hypothesized that if two adjectives are connected with conjunctions such as and/but, they have the same/opposite polarity." ></td>
	<td class="line x" title="263:288	Based on this hypothesis, their method predicts the polarity of adjectives by using a small set of adjectives labeled with the polarity." ></td>
	<td class="line x" title="264:288	Other works rely on linguistic resources such as WordNet (Kamps et al. , 2004; Hu and Liu, 2004; Esuli and Sebastiani, 2005; Takamura et al. , 2005)." ></td>
	<td class="line x" title="265:288	For example, Kamps et al. used a graph where nodes correspond to words in the WordNet, and edges connect synonymous words in the WordNet." ></td>
	<td class="line x" title="266:288	The polarity of an adjective is defined by its shortest paths from the node corresponding to good and bad." ></td>
	<td class="line x" title="267:288	Although those researches are closely related to our work, there is a striking difference." ></td>
	<td class="line x" title="268:288	In those researches, the target is limited to the polarity of words and none of them discussed sentences." ></td>
	<td class="line x" title="269:288	In addition, most of the works rely on external resources such as the WordNet, and cannot treat words that are not in the resources." ></td>
	<td class="line x" title="270:288	458 6.2 Learning subjective phrases Some researchers examined the acquisition of subjective phrases." ></td>
	<td class="line x" title="271:288	The subjective phrase is more general concept than opinion and includes both positive and negative expressions." ></td>
	<td class="line x" title="272:288	Wiebe learned subjective adjectives from a set of seed adjectives." ></td>
	<td class="line x" title="273:288	The idea is to automatically identify the synonyms of the seed and to add them to the seed adjectives (Wiebe, 2000)." ></td>
	<td class="line x" title="274:288	Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al. , 2003)." ></td>
	<td class="line x" title="275:288	Their method learns subjective nouns and extraction patterns in turn." ></td>
	<td class="line x" title="276:288	First, given seed subjective nouns, the method learns patterns that can extract subjective nouns from corpus." ></td>
	<td class="line x" title="277:288	And then, the patterns extract new subjective nouns from corpus, and they are added to the seed nouns." ></td>
	<td class="line x" title="278:288	Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="279:288	Similarly, Wiebe also proposes a bootstrapping approach to create subjective and objective classifier (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="280:288	These works are different from ours in a sense that they did not discuss how to determine the polarity of subjective words or phrases." ></td>
	<td class="line oc" title="281:288	6.3 Unsupervised sentiment classification Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line o" title="282:288	The concept behind Turneys model is that positive/negative phrases co-occur with words like excellent/poor." ></td>
	<td class="line x" title="283:288	The cooccurrence statistic is measured by the result of search engine." ></td>
	<td class="line n" title="284:288	Since his method relies on search engine, it is difficult to use rich linguistic information such as dependencies." ></td>
	<td class="line x" title="285:288	7 Conclusion This paper proposed a fully automatic method of building polarity-tagged corpus from HTML documents." ></td>
	<td class="line x" title="286:288	In the experiment, we could build a corpus consisting of 126,610 sentences." ></td>
	<td class="line x" title="287:288	As a future work, we intend to extract more opinion sentences by applying this method to larger HTML document sets and enhancing extraction rules." ></td>
	<td class="line x" title="288:288	Another important direction is to investigate more precise model that can classify or extract opinions, and learn its parameters from our corpus." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2063
Automatic Identification Of Pro And Con Reasons In Online Reviews
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:265	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483490, Sydney, July 2006." ></td>
	<td class="line x" title="2:265	c2006 Association for Computational Linguistics Automatic Identification of Pro and Con Reasons in Online Reviews Soo-Min Kim and Eduard Hovy USC Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 {skim, hovy}@ISI.EDU Abstract In this paper, we present a system that automatically extracts the pros and cons from online reviews." ></td>
	<td class="line x" title="3:265	Although many approaches have been developed for extracting opinions from text, our focus here is on extracting the reasons of the opinions, which may themselves be in the form of either fact or opinion." ></td>
	<td class="line x" title="4:265	Leveraging online review sites with author-generated pros and cons, we propose a system for aligning the pros and cons to their sentences in review texts." ></td>
	<td class="line x" title="5:265	A maximum entropy model is then trained on the resulting labeled set to subsequently extract pros and cons from online review sites that do not explicitly provide them." ></td>
	<td class="line x" title="6:265	Our experimental results show that our resulting system identifies pros and cons with 66% precision and 76% recall." ></td>
	<td class="line x" title="7:265	1 Introduction Many opinions are being expressed on the Web in such settings as product reviews, personal blogs, and news group message boards." ></td>
	<td class="line x" title="8:265	People increasingly participate to express their opinions online." ></td>
	<td class="line x" title="9:265	This trend has raised many interesting and challenging research topics such as subjectivity detection, semantic orientation classification, and review classification." ></td>
	<td class="line x" title="10:265	Subjectivity detection is the task of identifying subjective words, expressions, and sentences." ></td>
	<td class="line x" title="11:265	(Wiebe et al. , 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al, 2003)." ></td>
	<td class="line oc" title="12:265	Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="13:265	Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al. , 2005)." ></td>
	<td class="line oc" title="14:265	Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al. , 2002; Turney, 2002)." ></td>
	<td class="line o" title="15:265	Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers." ></td>
	<td class="line x" title="16:265	(Bethard et al. , 2004; Choi et al. , 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques." ></td>
	<td class="line x" title="17:265	(Wilson et al. , 2004) focused on the strength of opinion clauses, finding strong and weak opinions." ></td>
	<td class="line x" title="18:265	(Chklovski, 2006) presented a system that aggregates and quantifies degree assessment of opinions scattered throughout web pages." ></td>
	<td class="line x" title="19:265	Beyond document level sentiment classification in online product reviews, (Hu and Liu, 2004; Popescu and Etzioni, 2005) concentrated on mining and summarizing reviews by extracting opinion sentences regarding product features." ></td>
	<td class="line x" title="20:265	In this paper, we focus on another challenging yet critical problem of opinion analysis, identifying reasons for opinions, especially for opinions in online product reviews." ></td>
	<td class="line x" title="21:265	The opinion reason identification problem in online reviews seeks to answer the question What are the reasons that the author of this review likes or dislikes the product? For example, in hotel reviews, information such as found 189 positive reviews and 65 negative reviews may not fully satisfy the information needs of different users." ></td>
	<td class="line x" title="22:265	More useful information would be This hotel is great for families with young infants or Elevators are grouped according to floors, which makes the wait short." ></td>
	<td class="line x" title="23:265	This work differs in important ways from studies in (Hu and Liu, 2004) and (Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="24:265	These approaches extract features 483 of products and identify sentences that contain opinions about those features by using opinion words and phrases." ></td>
	<td class="line x" title="25:265	Here, we focus on extracting pros and cons which include not only sentences that contain opinion-bearing expressions about products and features but also sentences with reasons why an author of a review writes the review." ></td>
	<td class="line x" title="26:265	Following are examples identified by our system." ></td>
	<td class="line x" title="27:265	It creates duplicate files." ></td>
	<td class="line x" title="28:265	Video drains battery." ></td>
	<td class="line x" title="29:265	It won't play music from all music stores Even though finding reasons in opinionbearing texts is a critical part of in-depth opinion assessment, no study has been done in this particular vein partly because there is no annotated data." ></td>
	<td class="line x" title="30:265	Labeling each sentence is a timeconsuming and costly task." ></td>
	<td class="line x" title="31:265	In this paper, we propose a framework for automatically identifying reasons in online reviews and introduce a novel technique to automatically label training data for this task." ></td>
	<td class="line x" title="32:265	We assume reasons in an online review document are closely related to pros and cons represented in the text." ></td>
	<td class="line x" title="33:265	We leverage the fact that reviews on some websites such as epinions.com already contain pros and cons written by the same author as the reviews." ></td>
	<td class="line x" title="34:265	We use those pros and cons to automatically label sentences in the reviews on which we subsequently train our classification system." ></td>
	<td class="line x" title="35:265	We then apply the resulting system to extract pros and cons from reviews in other websites which do not have specified pros and cons." ></td>
	<td class="line x" title="36:265	This paper is organized as follows: Section 2 describes a definition of reasons in online reviews in terms of pros and cons." ></td>
	<td class="line x" title="37:265	Section 3 presents our approach to identify them and Section 4 explains our automatic data labeling process." ></td>
	<td class="line x" title="38:265	Section 5 describes experimental and results and finally, in Section 6, we conclude with future work." ></td>
	<td class="line x" title="39:265	2 Pros and Cons in Online Reviews This section describes how we define reasons in online reviews for our study." ></td>
	<td class="line x" title="40:265	First, we take a look at how researchers in Computational Linguistics define an opinion for their studies." ></td>
	<td class="line x" title="41:265	It is difficult to define what an opinion means in a computational model because of the difficulty of determining the unit of an opinion." ></td>
	<td class="line x" title="42:265	In general, researchers study opinion at three different levels: word level, sentence level, and document level." ></td>
	<td class="line x" title="43:265	Word level opinion analysis includes word sentiment classification, which views single lexical items (such as good or bad) as sentiment carriers, allowing one to classify words into positive and negative semantic categories." ></td>
	<td class="line x" title="44:265	Studies in sentence level opinion regard the sentence as a minimum unit of opinion." ></td>
	<td class="line x" title="45:265	Researchers try to identify opinion-bearing sentences, classify their sentiment, and identify opinion holders and topics of opinion sentences." ></td>
	<td class="line x" title="46:265	Document level opinion analysis has been mostly applied to review classification, in which a whole document written for a review is judged as carrying either positive or negative sentiment." ></td>
	<td class="line x" title="47:265	Many researchers, however, consider a whole document as the unit of an opinion to be too coarse." ></td>
	<td class="line x" title="48:265	In our study, we take the approach that a review text has a main opinion (recommendation or not) about a given product, but also includes various reasons for recommendation or nonrecommendation, which are valuable to identify." ></td>
	<td class="line x" title="49:265	Therefore, we focus on detecting those reasons in online product review." ></td>
	<td class="line x" title="50:265	We also assume that reasons in a review are closely related to pros and cons expressed in the review." ></td>
	<td class="line x" title="51:265	Pros in a product review are sentences that describe reasons why an author of the review likes the product." ></td>
	<td class="line x" title="52:265	Cons are reasons why the author doesnt like the product." ></td>
	<td class="line x" title="53:265	Based on our observation in online reviews, most reviews have both pros and cons even if sometimes one of them dominates." ></td>
	<td class="line x" title="54:265	3 Finding Pros and Cons This section describes our approach for finding pro and con sentences given a review text." ></td>
	<td class="line x" title="55:265	We first collect data from epinions.com and automatically label each sentences in the data set." ></td>
	<td class="line x" title="56:265	We then model our system using one of the machine learning techniques that have been successfully applied to various problems in Natural Language Processing." ></td>
	<td class="line x" title="57:265	This section also describes features we used for our model." ></td>
	<td class="line x" title="58:265	3.1 Automatically Labeling Pro and Con Sentences Among many web sites that have product reviews such as amazon.com and epinions.com, some of them (e.g. epinions.com) explicitly state pros and cons phrases in their respective categories by each reviews author along with the review text." ></td>
	<td class="line x" title="59:265	First, we collected a large set of <review text, pros, cons> triplets from epin484 ions.com." ></td>
	<td class="line x" title="60:265	A review document in epinions.com consists of a topic (a product model, restaurant name, travel destination, etc.), pros and cons (mostly a few keywords but sometimes complete sentences), and the review text." ></td>
	<td class="line x" title="61:265	Our automatic labeling system first collects phrases in pro and con fields and then searches the main review text in order to collect sentences corresponding to those phrases." ></td>
	<td class="line x" title="62:265	Figure 1 illustrates the automatic labeling process." ></td>
	<td class="line x" title="63:265	Figure 1." ></td>
	<td class="line x" title="64:265	The automatic labeling process of pros and cons sentences in a review." ></td>
	<td class="line x" title="65:265	The system first extracts comma-delimited phrases from each pro and con field, generating two sets of phrases: {P1, P2, , Pn} for pros and {C1, C2, , Cm} for cons." ></td>
	<td class="line x" title="66:265	In the example in Figure 1, beautiful display can be P i and not something you want to drop can be C j. Then the system compares these phrases to the sentences in the text in the Full Review." ></td>
	<td class="line x" title="67:265	For each phrase in {P1, P2, , Pn} and {C1, C2, , Cm}, the system checks each sentence to find a sentence that covers most of the words in the phrase." ></td>
	<td class="line x" title="68:265	Then the system annotates this sentence with the appropriate pro or con label." ></td>
	<td class="line x" title="69:265	All remaining sentences with neither label are marked as neither." ></td>
	<td class="line x" title="70:265	After labeling all the epinion data, we use it to train our pro and con sentence recognition system." ></td>
	<td class="line x" title="71:265	3.2 Modeling with Maximum Entropy Classification We use Maximum Entropy classification for the task of finding pro and con sentences in a given review." ></td>
	<td class="line x" title="72:265	Maximum Entropy classification has been successfully applied in many tasks in natural language processing, such as Semantic Role labeling, Question Answering, and Information Extraction." ></td>
	<td class="line x" title="73:265	Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible (Berger et al. , 1996)." ></td>
	<td class="line x" title="74:265	We modeled the conditional probability of a class c given a feature vector x as follows: )),(exp( 1 )|(  = i ii x xcf Z xcp  where x Z is a normalization factor which can be calculated by the following:   = ci iix xcfZ )),(exp(  In the first equation, ),( xcf i is a feature function which has a binary value, 0 or 1." ></td>
	<td class="line x" title="75:265	i  is a weight parameter for the feature function ),( xcf i and higher value of the weight indicates that ),( xcf i is an important feature for a class c . For our system development, we used MegaM toolkit 1 which implements the above intuition." ></td>
	<td class="line x" title="76:265	In order to build an efficient model, we separated the task of finding pro and con sentences into two phases, each being a binary classification." ></td>
	<td class="line x" title="77:265	The first is an identification phase and the second is a classification phase." ></td>
	<td class="line x" title="78:265	For this 2-phase model, we defined the 3 classes of c listed in Table 1." ></td>
	<td class="line x" title="79:265	The identification task separates pro and con candidate sentences (CR and PR in Table 1) from sentences irrelevant to either of them (NR)." ></td>
	<td class="line x" title="80:265	The classification task then classifies candidates into pros (PR) and cons (CR)." ></td>
	<td class="line x" title="81:265	Section 5 reports system results of both phases." ></td>
	<td class="line x" title="82:265	1 http://www.isi.edu/~hdaume/megam/index.html Table 1: Classes defined for the classification tasks." ></td>
	<td class="line x" title="83:265	Class symbol Description PR Sentences related to pros in a review CR Sentences related to cons in a review NR Sentences related to neither PR nor CR 485 3.3 Features The classification uses three types of features: lexical features, positional features, and opinionbearing word features." ></td>
	<td class="line x" title="84:265	For lexical features, we use unigrams, bigrams, and trigrams collected from the training set." ></td>
	<td class="line x" title="85:265	They investigate the intuition that there are certain words that are frequently used in pro and con sentences which are likely to represent reasons why an author writes a review." ></td>
	<td class="line x" title="86:265	Examples of such words and phrases are: because and thats why." ></td>
	<td class="line x" title="87:265	For positional features, we first find paragraph boundaries in review texts using html tags such as <br> and <p>." ></td>
	<td class="line x" title="88:265	After finding paragraph boundaries, we add features indicating the first, the second, the last, and the second last sentence in a paragraph." ></td>
	<td class="line x" title="89:265	These features test the intuition used in document summarization that important sentences that contain topics in a text have certain positional patterns in a paragraph (Lin and Hovy, 1997), which may apply because reasons like pros and cons in a review document are most important sentences that summarize the whole point of the review." ></td>
	<td class="line x" title="90:265	For opinion-bearing word features, we used pre-selected opinion-bearing words produced by a combination of two methods." ></td>
	<td class="line x" title="91:265	The first method derived a list of opinion-bearing words from a large news corpus by separating opinion articles such as letters or editorials from news articles which simply reported news or events." ></td>
	<td class="line x" title="92:265	The second method calculated semantic orientations of words based on WordNet 2 synonyms." ></td>
	<td class="line x" title="93:265	In our previous work (Kim and Hovy, 2005), we demonstrated that the list of words produced by a combination of those two methods performed very well in detecting opinion bearing sentences." ></td>
	<td class="line x" title="94:265	Both algorithms are described in that paper." ></td>
	<td class="line x" title="95:265	The motivation for including the list of opinion-bearing words as one of our features is that pro and con sentences are quite likely to contain opinion-bearing expressions (even though some of them are only facts), such as The waiting time was horrible and Their portion size of food was extremely generous! in restaurant reviews." ></td>
	<td class="line x" title="96:265	We presumed pro and con sentences containing only facts, such as The battery lasted 3 hours, not 5 hours like they advertised, would be captured by lexical or positional features." ></td>
	<td class="line x" title="97:265	In Section 5, we report experimental results with different combinations of these features." ></td>
	<td class="line x" title="98:265	2 http://wordnet.princeton.edu/ Table 2 summarizes the features we used for our model and the symbols we will use in the rest of this paper." ></td>
	<td class="line x" title="99:265	4 Data We collected data from two different sources: epinions.com and complaints.com 3 (see Section 3.1 for details about review data in epinion.com)." ></td>
	<td class="line x" title="100:265	Data from epinions.com is mostly used to train the system whereas data from complaints.com is to test how the trained model performs on new data." ></td>
	<td class="line x" title="101:265	Complaints.com includes a large database of publicized consumer complaints about diverse products, services, and companies collected for over 6 years." ></td>
	<td class="line x" title="102:265	Interestingly, reviews in complaint.com are somewhat different from many other web sites which are directly or indirectly linked to Internet shopping malls such as amazon.com and epinions.com." ></td>
	<td class="line x" title="103:265	The purpose of reviews in complaints.com is to share consumers mostly negative experiences and alert businesses to customers feedback." ></td>
	<td class="line x" title="104:265	However, many reviews in Internet shopping mall related reviews are positive and sometimes encourage people to buy more products or to use more services." ></td>
	<td class="line x" title="105:265	Despite its significance, however, there is no hand-annotated data that we can use to build a system to identify reasons of complaints.com." ></td>
	<td class="line x" title="106:265	In order to solve this problem, we assume that reasons in complaints reviews are similar to cons in other reviews and therefore if we are, somehow, able to build a system that can identify cons from 3 http://www.complaints.com/ Table 2: Feature summary." ></td>
	<td class="line x" title="107:265	Feature category Description Symbol Lexical Features unigrams bigrams trigrams Lex Positional Features the first, the second, the last, the second to last sentence in a paragraph Pos Opinionbearing word features pre-selected opinion-bearing words Op 486 reviews, we can apply it to identify reasons in complaints reviews." ></td>
	<td class="line x" title="108:265	Based on this assumption, we learn a system using the data from epinions.com, to which we can apply our automatic data labeling technique, and employ the resulting system to identify reasons from reviews in complaint.com." ></td>
	<td class="line x" title="109:265	The following sections describe each data set." ></td>
	<td class="line x" title="110:265	4.1 Dataset 1: Automatically Labeled Data We collected two different domains of reviews from epinions.com: product reviews and restaurant reviews." ></td>
	<td class="line x" title="111:265	As for the product reviews, we collected 3241 reviews (115029 sentences) about mp3 players made by various manufacturers such as Apple, iRiver, Creative Lab, and Samsung." ></td>
	<td class="line x" title="112:265	We also collected 7524 reviews (194393 sentences) about various types of restaurants such as family restaurants, Mexican restaurants, fast food chains, steak houses, and Asian restaurants." ></td>
	<td class="line x" title="113:265	The average numbers of sentences in a review document are 35.49 and 25.89 respectively." ></td>
	<td class="line x" title="114:265	The purpose of selecting one of electronics products and restaurants as topics of reviews for our study is to test our approach in two extremely different situations." ></td>
	<td class="line x" title="115:265	Reasons why consumers like or dislike a product in electronics reviews are mostly about specific and tangible features." ></td>
	<td class="line x" title="116:265	Also, there are somewhat a fixed set of features of a specific type of product, for example, ease of use, durability, battery life, photo quality, and shutter lag for digital cameras." ></td>
	<td class="line x" title="117:265	Consequently, we can expect that reasons in electronics reviews may share those product feature words and words that describe aspects of features such as short or long for battery life." ></td>
	<td class="line x" title="118:265	This fact might make the reason identification task easy." ></td>
	<td class="line x" title="119:265	On the other hand, restaurant reviewers talk about very diverse aspects and abstract features as reasons." ></td>
	<td class="line x" title="120:265	For example, reasons such as You feel like you are in a train station or a busy amusement park that is ill-staffed to meet demand!, preferential treatment given to large groups, and they don't offer salads of any kind are hard to predict." ></td>
	<td class="line x" title="121:265	Also, they seem rarely share common keyword features." ></td>
	<td class="line x" title="122:265	We first automatically labeled each sentence in those reviews collected from each domain with the features described in Section 3.1." ></td>
	<td class="line x" title="123:265	We divided the data for training and testing." ></td>
	<td class="line x" title="124:265	We then trained our model using the training set and tested it to see if the system can successfully label sentences in the test set." ></td>
	<td class="line x" title="125:265	4.2 Dataset 2: Complaints.com Data From the database 4 in complaints.com, we searched for the same topics of reviews as Dataset 1: 59 complaints reviews about mp3 players and 322 reviews about restaurants 5 . We tested our system on this dataset and compare the results against human judges annotation results." ></td>
	<td class="line x" title="126:265	Subsection 5.2 reports the evaluation results." ></td>
	<td class="line x" title="127:265	5 Experiments and Results We describe two goals in our experiments in this section." ></td>
	<td class="line x" title="128:265	The first is to investigate how well our pro and con detection model with different feature combinations performs on the data we collected from epinions.com." ></td>
	<td class="line x" title="129:265	The second is to see how well the trained model performs on new data from a different source, complaint.com." ></td>
	<td class="line x" title="130:265	For both datasets, we carried out two separate sets of experiments, for the domains of mp3 players and restaurant reviews." ></td>
	<td class="line x" title="131:265	We divided data into 80% for training, 10% for development, and 10% for test for our experiments." ></td>
	<td class="line x" title="132:265	5.1 Experiments on Dataset 1 Identification step: Table 3 and 4 show pros and cons sentences identification results of our system for mp3 player and restaurant reviews respectively." ></td>
	<td class="line x" title="133:265	The first column indicates which combination of features was used for our model (see Table 2 for the meaning of Op, Lex, and Pos feature categories)." ></td>
	<td class="line x" title="134:265	We measure the performance with accuracy (Acc), precision (Prec), recall (Recl), and F-score 6 . The baseline system assigned all sentences as reason and achieved 57.75% and 54.82% of accuracy." ></td>
	<td class="line x" title="135:265	The system performed well when it only used lexical features in mp3 player reviews (76.27% of accuracy in Lex), whereas it performed well with the combination of lexical and opinion features in restaurant reviews (Lex+Op row in Table 4)." ></td>
	<td class="line x" title="136:265	It was very interesting to see that the system achieved a very low score when it only used opinion word features." ></td>
	<td class="line x" title="137:265	We can interpret this phenomenon as supporting our hypothesis that pro and con sentences in reviews are often purely 4 At the time (December 2005), there were total 42593 complaint reviews available in the database." ></td>
	<td class="line x" title="138:265	5 Average numbers of sentences in a complaint is 19.57 for mp3 player reviews and 21.38 for restaurant reviews." ></td>
	<td class="line x" title="139:265	6 We calculated F-score by Recall Precision Recall Precision 2 +  487 factual." ></td>
	<td class="line x" title="140:265	However, opinion features improved both precision and recall when combined with lexical features in restaurant reviews." ></td>
	<td class="line x" title="141:265	It was also interesting that experiments on mp3 players reviews achieved mostly higher scores than restaurants." ></td>
	<td class="line x" title="142:265	Like the observation we described in Subsection 4.1, frequently mentioned keywords of product features (e.g. durability) may have helped performance, especially with lexical features." ></td>
	<td class="line x" title="143:265	Another interesting observation is that the positional features that helped in topic sentence identification did not help much for our task." ></td>
	<td class="line x" title="144:265	Classification step: Tables 5 and 6 show the system results of the pro and con classification task." ></td>
	<td class="line x" title="145:265	The baseline system marked all sentences as pros and achieved 53.87% and 50.71% accuracy for each domain." ></td>
	<td class="line x" title="146:265	All features performed better than the baseline but the results are not as good as in the identification task." ></td>
	<td class="line x" title="147:265	Unlike the identification task, opinion words by themselves achieved the best accuracy in both mp3 player and restaurant domains." ></td>
	<td class="line x" title="148:265	We think opinion words played more important roles in classifying pros and cons than identifying them." ></td>
	<td class="line x" title="149:265	Position features helped recognizing con sentences in mp3 player reviews." ></td>
	<td class="line x" title="150:265	5.2 Experiments on Dataset 2 This subsection reports the evaluation results of our system on Dataset 2." ></td>
	<td class="line x" title="151:265	Since Dataset 2 from complaints.com has no training data, we trained a system on Dataset 1 and applied it to Dataset 2." ></td>
	<td class="line x" title="152:265	Table 3: Pros and cons sentences identification results on mp3 player reviews." ></td>
	<td class="line x" title="153:265	Features used Acc (%) Prec (%) Recl (%) F-score (%) Op 60.15 65.84 57.31 61.28 Lex 76.27 66.18 76.42 70.93 Lex+Pos 63.10 71.14 60.72 65.52 Lex+Op 62.75 70.64 60.07 64.93 Lex+Pos+Op 62.23 70.58 59.35 64.48 Baseline 57.75 Table 4: Reason sentence identification results on restaurant reviews." ></td>
	<td class="line x" title="154:265	Features used Acc (%) Prec (%) Recl (%) F-score (%) Op 61.64 60.76 47.48 53.31 Lex 63.77 67.10 51.20 58.08 Lex+Pos 63.89 67.62 51.70 58.60 Lex+Op 61.66 69.13 54.30 60.83 Lex+Pos+Op 63.13 66.80 50.41 57.46 Baseline 54.82 Table 5: Pros and cons sentences classification results for mp3 player reviews." ></td>
	<td class="line x" title="155:265	Cons Pros Features used Acc (%) Prec (%) Recl (%) F-score (%) Prec (%) Recl (%) F-score (%) Op 57.18 54.43 67.10 60.10 61.18 48.00 53.80 Lex 55.88 55.49 67.45 60.89 56.52 43.88 49.40 Lex+Pos 55.62 55.26 68.12 61.02 56.24 42.62 48.49 Lex+Op 55.60 55.46 64.63 59.70 55.81 46.26 50.59 Lex+Pos+Op 56.68 56.70 62.45 59.44 56.65 50.71 53.52 baseline 53.87 (mark all as pros) Table 6: Pros and cons sentences classification results for restaurant reviews." ></td>
	<td class="line x" title="156:265	Cons Pros Features used Acc (%) Prec (%) Recl (%) F-score (%) Prec (%) Recl (%) F-score (%) Op 57.32 54.78 51.62 53.15 59.32 62.35 60.80 Lex 55.76 55.94 52.52 54.18 55.60 58.97 57.24 Lex+Pos 56.07 56.20 53.33 54.73 55.94 58.78 57.33 Lex+Op 55.88 56.10 52.39 54.18 55.68 59.34 57.45 Lex+Pos+Op 55.79 55.89 53.17 54.50 55.70 58.38 57.01 baseline 50.71 (mark all as pros) 488 A tough question, however, is how to evaluate the system results." ></td>
	<td class="line x" title="157:265	Since it seemed impossible to evaluate the system without involving a human judge, we annotated a small set of data manually for evaluation purposes." ></td>
	<td class="line x" title="158:265	Gold Standard Annotation: Four humans annotated 3 sets of test sets: Testset 1 with 5 complaints (73 sentences), Testset 2 with 7 complaints (105 sentences), and Testset 3 with 6 complaints (85 sentences)." ></td>
	<td class="line x" title="159:265	Testset 1 and 2 are from mp3 player complaints and Testset 3 is from restaurant reviews." ></td>
	<td class="line x" title="160:265	Annotators marked sentences if they describe specific reasons of the complaint." ></td>
	<td class="line x" title="161:265	Each test set was annotated by 2 humans." ></td>
	<td class="line x" title="162:265	The average pair-wise human agreement was 82.1% 7 . System Performance: Like the human annotators, our system also labeled reason sentences." ></td>
	<td class="line x" title="163:265	Since our goal is to identify reason sentences in complaints, we applied a system modeled as in the identification phase described in Subsection 3.2 instead of the classification phase 8 . Table 7 reports the accuracy, precision, and recall of the system on each test set." ></td>
	<td class="line x" title="164:265	We calculated numbers in each A and B column by assuming each annotators answers separately as a gold standard." ></td>
	<td class="line x" title="165:265	In Table 7, accuracies indicate the agreement between the system and human annotators." ></td>
	<td class="line x" title="166:265	The average accuracy 68.0% is comparable with the pair-wise human agreement 82.1% even if there is still a lot of room for improvement 9 . It was interesting to see that Testset 3, which was from restaurant complaints, achieved higher accuracy and recall than the other test sets from mp3 player complaints, suggesting that it would be interesting to further investigate the performance 7 The kappa value was 0.63." ></td>
	<td class="line x" title="167:265	8 In complaints reviews, we believe that it is more important to identify reason sentences than to classify because most reasons in complaints are likely to be cons." ></td>
	<td class="line x" title="168:265	9 The baseline system which assigned the majority class to each sentence achieved 59.9% of average accuracy." ></td>
	<td class="line x" title="169:265	of reason identification in various other review domains such as travel and beauty products in future work." ></td>
	<td class="line x" title="170:265	Also, even though we were somewhat able to measure reason sentence identification in complaint reviews, we agree that we need more data annotation for more precise evaluation." ></td>
	<td class="line x" title="171:265	Finally, the followings are examples of sentences that our system identified as reasons of complaints." ></td>
	<td class="line x" title="172:265	(1) Unfortunately, I find that I am no longer comfortable in your establishment because of the unprofessional, rude, obnoxious, and unsanitary treatment from the employees." ></td>
	<td class="line x" title="173:265	(2) They never get my order right the first time and what really disgusts me is how they handle the food." ></td>
	<td class="line x" title="174:265	(3) The kids play area at Braum's in The Colony, Texas is very dirty." ></td>
	<td class="line x" title="175:265	(4) The only complaint that I have is that the French fries are usually cold." ></td>
	<td class="line x" title="176:265	(5) The cashier there had short changed me on the payment of my bill." ></td>
	<td class="line x" title="177:265	As we can see from the examples, our system was able to detect con sentences which contained opinion-bearing expressions such as in (1), (2), and (3) as well as reason sentences that mostly described mere facts as in (4) and (5)." ></td>
	<td class="line x" title="178:265	6 Conclusions and Future work This paper proposes a framework for identifying one of the critical elements of online product reviews to answer the question, What are reasons that the author of a review likes or dislikes the product? We believe that pro and con sentences in reviews can be answers for this question." ></td>
	<td class="line x" title="179:265	We present a novel technique that automatically labels a large set of pro and con sentences in online reviews using clue phrases for pros and cons in epinions.com in order to train our system." ></td>
	<td class="line x" title="180:265	We applied it to label sentences both on epinions.com and complaints.com." ></td>
	<td class="line x" title="181:265	To investigate the reliability of our system, we tested it on two extremely different review domains, mp3 player reviews and restaurant reviews." ></td>
	<td class="line x" title="182:265	Our system with the best feature selection performs 71% F-score in the reason identification task and 61% F-score in the reason classification task." ></td>
	<td class="line x" title="183:265	Table 7: System results on Complaint.com reviews (A, B: The first and the second annotator of each set) Testset 1 Testset 2 Testset 3 A B A B A B Avg Acc(%) 65.8 63.0 67.6 61.0 77.6 72.9 68.0 Prec(%) 50.0 60.7 68.6 62.9 67.9 60.7 61.8 Recl(%) 56.0 51.5 51.1 44.0 65.5 58.6 54.5 489 The experimental results further show that pro and con sentences are a mixture of opinions and facts, making identifying them in online reviews a distinct problem from opinion sentence identification." ></td>
	<td class="line x" title="184:265	Finally, we also apply the resulting system to another review data in complaints.com in order to analyze reasons of consumers complaints." ></td>
	<td class="line x" title="185:265	In the future, we plan to extend our pro and con identification system on other sorts of opinion texts, such as debates about political and social agenda that we can find on blogs or news group discussions, to analyze why people support a specific agenda and why people are against it." ></td>
	<td class="line x" title="186:265	Reference Berger, Adam L. , Stephen Della Pietra, and Vincent Della Pietra." ></td>
	<td class="line x" title="187:265	1996." ></td>
	<td class="line x" title="188:265	A maximum entropy approach to natural language processing, Computational Linguistics, (22-1)." ></td>
	<td class="line x" title="189:265	Bethard, Steven, Hong Yu, Ashley Thornton, Vasileios Hatzivassiloglou, and Dan Jurafsky." ></td>
	<td class="line x" title="190:265	2004." ></td>
	<td class="line x" title="191:265	Automatic Extraction of Opinion Propositions and their Holders, AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications." ></td>
	<td class="line x" title="192:265	Chklovski, Timothy." ></td>
	<td class="line x" title="193:265	2006." ></td>
	<td class="line x" title="194:265	Deriving Quantitative Overviews of Free Text Assessments on the Web." ></td>
	<td class="line x" title="195:265	Proceedings of 2006 International Conference on Intelligent User Interfaces (IUI06)." ></td>
	<td class="line x" title="196:265	Sydney, Australia." ></td>
	<td class="line x" title="197:265	Choi, Y. , Cardie, C. , Riloff, E. , and Patwardhan, S. 2005." ></td>
	<td class="line x" title="198:265	Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns." ></td>
	<td class="line x" title="199:265	Proceedings of HLT/EMNLP-05." ></td>
	<td class="line x" title="200:265	Esuli, Andrea and Fabrizio Sebastiani." ></td>
	<td class="line x" title="201:265	2005." ></td>
	<td class="line x" title="202:265	Determining the semantic orientation of terms through gloss classification." ></td>
	<td class="line x" title="203:265	Proceedings of CIKM-05, 14th ACM International Conference on Information and Knowledge Management, Bremen, DE, pp." ></td>
	<td class="line x" title="204:265	617-624." ></td>
	<td class="line x" title="205:265	Hatzivassiloglou, Vasileios and Kathleen McKeown." ></td>
	<td class="line x" title="206:265	1997." ></td>
	<td class="line x" title="207:265	Predicting the Semantic Orientation of Adjectives." ></td>
	<td class="line x" title="208:265	Proceedings of 35th Annual Meeting of the Assoc." ></td>
	<td class="line x" title="209:265	for Computational Linguistics (ACL-97): 174-181 Hatzivassiloglou, Vasileios and Janyce Wiebe." ></td>
	<td class="line x" title="210:265	2000." ></td>
	<td class="line x" title="211:265	Effects of Adjective Orientation and Gradability on Sentence Subjectivity." ></td>
	<td class="line x" title="212:265	Proceedings of International Conference on Computational Linguistics (COLING-2000)." ></td>
	<td class="line x" title="213:265	Saarbrcken, Germany." ></td>
	<td class="line x" title="214:265	Hu, Minqing and Bing Liu." ></td>
	<td class="line x" title="215:265	2004." ></td>
	<td class="line x" title="216:265	Mining and summarizing customer reviews'." ></td>
	<td class="line x" title="217:265	Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD2004), Seattle, Washington, USA." ></td>
	<td class="line x" title="218:265	Kim, Soo-Min and Eduard Hovy." ></td>
	<td class="line x" title="219:265	2004." ></td>
	<td class="line x" title="220:265	Determining the Sentiment of Opinions." ></td>
	<td class="line x" title="221:265	Proceedings of COLING-04." ></td>
	<td class="line x" title="222:265	pp." ></td>
	<td class="line x" title="223:265	1367-1373." ></td>
	<td class="line x" title="224:265	Geneva, Switzerland." ></td>
	<td class="line x" title="225:265	Kim, Soo-Min and Eduard Hovy." ></td>
	<td class="line x" title="226:265	2005." ></td>
	<td class="line x" title="227:265	Automatic Detection of Opinion Bearing Words and Sentences." ></td>
	<td class="line x" title="228:265	In the Companion Volume of the Proceedings of IJCNLP-05, Jeju Island, Republic of Korea." ></td>
	<td class="line x" title="229:265	Kim, Soo-Min and Eduard Hovy." ></td>
	<td class="line x" title="230:265	2006." ></td>
	<td class="line x" title="231:265	Identifying and Analyzing Judgment Opinions." ></td>
	<td class="line x" title="232:265	Proceedings of HLT/NAACL-2006, New York City, NY." ></td>
	<td class="line x" title="233:265	Lin, Chin-Yew and Eduard Hovy." ></td>
	<td class="line x" title="234:265	1997." ></td>
	<td class="line x" title="235:265	Identifying Topics by Position." ></td>
	<td class="line x" title="236:265	Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP97)." ></td>
	<td class="line x" title="237:265	Washington, D.C. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan." ></td>
	<td class="line x" title="238:265	2002." ></td>
	<td class="line x" title="239:265	Thumbs up?" ></td>
	<td class="line x" title="240:265	Sentiment Classification using Machine Learning Techniques, Proceedings of EMNLP 2002." ></td>
	<td class="line x" title="241:265	Popescu, Ana-Maria, and Oren Etzioni." ></td>
	<td class="line x" title="242:265	2005." ></td>
	<td class="line x" title="243:265	Extracting Product Features and Opinions from Reviews, Proceedings of HLT-EMNLP 2005." ></td>
	<td class="line x" title="244:265	Riloff, Ellen, Janyce Wiebe, and Theresa Wilson." ></td>
	<td class="line x" title="245:265	2003." ></td>
	<td class="line x" title="246:265	Learning Subjective Nouns Using Extraction Pattern Bootstrapping." ></td>
	<td class="line x" title="247:265	Proceedings of Seventh Conference on Natural Language Learning (CoNLL-03)." ></td>
	<td class="line x" title="248:265	ACL SIGNLL." ></td>
	<td class="line x" title="249:265	Pages 25-32." ></td>
	<td class="line xc" title="250:265	Turney, Peter D. 2002." ></td>
	<td class="line x" title="251:265	Thumbs up or thumbs down?" ></td>
	<td class="line o" title="252:265	Semantic orientation applied to unsupervised classification of reviews, Proceedings of ACL-02, Philadelphia, Pennsylvania, 417-424 Wiebe, Janyce M. , Bruce, Rebecca F. , and O'Hara, Thomas P. 1999." ></td>
	<td class="line x" title="253:265	Development and use of a gold standard data set for subjectivity classifications." ></td>
	<td class="line x" title="254:265	Proceedings of ACL-99." ></td>
	<td class="line x" title="255:265	University of Maryland, June, pp." ></td>
	<td class="line x" title="256:265	246-253." ></td>
	<td class="line x" title="257:265	Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann." ></td>
	<td class="line x" title="258:265	2005." ></td>
	<td class="line x" title="259:265	Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis." ></td>
	<td class="line x" title="260:265	Proceedings of HLT/EMNLP 2005, Vancouver, Canada Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa." ></td>
	<td class="line x" title="261:265	2004." ></td>
	<td class="line x" title="262:265	Just how mad are you?" ></td>
	<td class="line x" title="263:265	Finding strong and weak opinion clauses." ></td>
	<td class="line x" title="264:265	Proceedings of 19th National Conference on Artificial Intelligence (AAAI-2004)." ></td>
	<td class="line x" title="265:265	490" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2079
Examining The Role Of Linguistic Knowledge Sources In The Automatic Identification And Classification Of Reviews
Ng, Vincent;Dasgupta, Sajib;Arifin, S. M. Niaz;"></td>
	<td class="line x" title="1:254	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 611618, Sydney, July 2006." ></td>
	<td class="line x" title="2:254	c2006 Association for Computational Linguistics Examining the Role of Linguistic Knowledge Sources in the Automatic Identification and Classification of Reviews Vincent Ng and Sajib Dasgupta and S. M. Niaz Arifin Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {vince,sajib,arif}@hlt.utdallas.edu Abstract This paper examines two problems in document-level sentiment analysis: (1) determining whether a given document is a review or not, and (2) classifying the polarity of a review as positive or negative." ></td>
	<td class="line x" title="3:254	We first demonstrate that review identification can be performed with high accuracy using only unigrams as features." ></td>
	<td class="line x" title="4:254	We then examine the role of four types of simple linguistic knowledge sources in a polarity classification system." ></td>
	<td class="line x" title="5:254	1 Introduction Sentiment analysis involves the identification of positive and negative opinions from a text segment." ></td>
	<td class="line x" title="6:254	The task has recently received a lot of attention, with applications ranging from multiperspective question-answering (e.g. , Cardie et al.(2004)) to opinion-oriented information extraction (e.g. , Riloff et al.(2005)) and summarization (e.g. , Hu and Liu (2004))." ></td>
	<td class="line x" title="9:254	Research in sentiment analysis has generally proceeded at three levels, aiming to identify and classify opinions from documents, sentences, and phrases." ></td>
	<td class="line x" title="10:254	This paper examines two problems in document-level sentiment analysis, focusing on analyzing a particular type of opinionated documents: reviews." ></td>
	<td class="line x" title="11:254	The first problem, polarity classification, has the goal of determining a reviews polarity positive ( thumbs up ) or negative ( thumbs down )." ></td>
	<td class="line x" title="12:254	Recent work has expanded the polarity classification task to additionally handle documents expressing a neutral sentiment." ></td>
	<td class="line x" title="13:254	Although studied fairly extensively, polarity classification remains a challenge to natural language processing systems." ></td>
	<td class="line x" title="14:254	We will focus on an important linguistic aspect of polarity classification: examining the role of a variety of simple, yet under-investigated, linguistic knowledge sources in a learning-based polarity classification system." ></td>
	<td class="line x" title="15:254	Specifically, we will show how to build a high-performing polarity classifier by exploiting information provided by (1) high order n-grams, (2) a lexicon composed of adjectives manually annotated with their polarity information (e.g. , happy is annotated as positive and terrible as negative), (3) dependency relations derived from dependency parses, and (4) objective terms and phrases extracted from neutral documents." ></td>
	<td class="line x" title="16:254	As mentioned above, the majority of work on document-level sentiment analysis to date has focused on polarity classification, assuming as input a set of reviews to be classified." ></td>
	<td class="line x" title="17:254	A relevant question is: what if we dont know that an input document is a review in the first place?" ></td>
	<td class="line x" title="18:254	The second task we will examine in this paper review identification attempts to address this question." ></td>
	<td class="line x" title="19:254	Specifically, review identification seeks to determine whether a given document is a review or not." ></td>
	<td class="line x" title="20:254	We view both review identification and polarity classification as a classification task." ></td>
	<td class="line x" title="21:254	For review identification, we train a classifier to distinguish movie reviews and movie-related nonreviews (e.g. , movie ads, plot summaries) using only unigrams as features, obtaining an accuracy of over 99% via 10-fold cross-validation." ></td>
	<td class="line x" title="22:254	Similar experiments using documents from the book domain also yield an accuracy as high as 97%." ></td>
	<td class="line x" title="23:254	An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions." ></td>
	<td class="line x" title="24:254	Next, we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets, one assembled by Pang and Lee (2004) and the other by ourselves." ></td>
	<td class="line x" title="25:254	The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.s dataset and ours, respectively." ></td>
	<td class="line x" title="26:254	To our knowledge, our result on Pang et al.s dataset is one of the best reported to date." ></td>
	<td class="line x" title="27:254	Perhaps more importantly, an analysis of these results show that the various types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification." ></td>
	<td class="line x" title="28:254	2 Related Work 2.1 Review Identification As noted in the introduction, while a review can contain both subjective and objective phrases, our non-reviews are essentially factual documents in which subjective expressions can rarely be found." ></td>
	<td class="line x" title="29:254	Hence, review identification can be viewed as an instance of the broader task of classifying whether a document is mostly factual/objective or mostly opinionated/subjective." ></td>
	<td class="line x" title="30:254	There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al.(2004) for details)." ></td>
	<td class="line oc" title="32:254	2.2 Polarity Classification There is a large body of work on classifying the polarity of a document (e.g. , Pang et al.(2002), Turney (2002)), a sentence (e.g. , Liu et al.(2003), Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Gamon et al.(2005)), a phrase (e.g. , Wilson et al.(2005)), and a specific object (such as a product) mentioned in a document (e.g. , Morinaga et al.(2002), Yi et al.(2003), Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="39:254	Below we will center our discussion of related work around the four types of features we will explore for polarity classification." ></td>
	<td class="line x" title="40:254	Higher-order n-grams." ></td>
	<td class="line x" title="41:254	While n-grams offer a simple way of capturing context, previous work has rarely explored the use of n-grams as features in a polarity classification system beyond unigrams." ></td>
	<td class="line x" title="42:254	Two notable exceptions are the work of Dave et al.(2003) and Pang et al.(2002)." ></td>
	<td class="line x" title="45:254	Interestingly, while Dave et al. report good performance on classifying reviews using bigrams or trigrams alone, Pang et al. show that bigrams are not useful features for the task, whether they are used in isolation or in conjunction with unigrams." ></td>
	<td class="line x" title="46:254	This motivates us to take a closer look at the utility of higher-order n-grams in polarity classification." ></td>
	<td class="line x" title="47:254	Manually-tagged term polarity." ></td>
	<td class="line oc" title="48:254	Much work has been performed on learning to identify and classify polarity terms (i.e. , terms expressing a positive sentiment (e.g. , happy) or a negative sentiment (e.g. , terrible)) and exploiting them to do polarity classification (e.g. , Hatzivassiloglou and McKeown (1997), Turney (2002), Kim and Hovy (2004), Whitelaw et al.(2005), Esuli and Sebastiani (2005))." ></td>
	<td class="line n" title="50:254	Though reasonably successful, these (semi-)automatic techniques often yield lexicons that have either high coverage/low precision or low coverage/high precision." ></td>
	<td class="line x" title="51:254	While manually constructed positive and negative word lists exist (e.g. , General Inquirer1), they too suffer from the problem of having low coverage." ></td>
	<td class="line x" title="52:254	This prompts us to manually construct our own polarity word lists2 and study their use in polarity classification." ></td>
	<td class="line x" title="53:254	Dependency relations." ></td>
	<td class="line x" title="54:254	There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g. , Dave et al.(2003), Yi et al.(2003)) or nouns that enjoy a dependency relation with a polarity term (e.g. , Popescu and Etzioni (2005))." ></td>
	<td class="line x" title="57:254	Wilson et al.(2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document." ></td>
	<td class="line x" title="59:254	In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews." ></td>
	<td class="line x" title="60:254	Objective information." ></td>
	<td class="line x" title="61:254	The objective portions of a review do not contain the authors opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task." ></td>
	<td class="line x" title="62:254	Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g. , Pang and Lee (2004))." ></td>
	<td class="line x" title="63:254	Motivated by the work of Koppel and Schler (2005), we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification." ></td>
	<td class="line x" title="64:254	1http://www.wjh.harvard.edu/inquirer/ spreadsheet guid.htm 2Wilson et al.(2005) have also manually tagged a list of terms with their polarity, but this list is not publicly available." ></td>
	<td class="line x" title="66:254	612 Finally, previous work has also investigated features that do not fall into any of the above categories." ></td>
	<td class="line oc" title="67:254	For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turneys (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration." ></td>
	<td class="line x" title="68:254	3 Review Identification Recall that the goal of review identification is to determine whether a given document is a review or not." ></td>
	<td class="line x" title="69:254	Given this definition, two immediate questions come to mind." ></td>
	<td class="line x" title="70:254	First, should this problem be addressed in a domain-specific or domainindependent manner?" ></td>
	<td class="line x" title="71:254	In other words, should a review identification system take as input documents coming from the same domain or not?" ></td>
	<td class="line x" title="72:254	Apparently this is a design question with no definite answer, but our decision is to perform domain-specific review identification." ></td>
	<td class="line x" title="73:254	The reason is that the primary motivation of review identification is the need to identify reviews for further analysis by a polarity classification system." ></td>
	<td class="line x" title="74:254	Since polarity classification has almost exclusively been addressed in a domain-specific fashion, it seems natural that its immediate upstream component review identification should also assume domain specificity." ></td>
	<td class="line x" title="75:254	Note, however, that assuming domain specificity is not a self-imposed limitation." ></td>
	<td class="line x" title="76:254	In fact, we envision that the review identification system will have as its upstream component a text classification system, which will classify documents by topic and pass to the review identifier only those documents that fall within its domain." ></td>
	<td class="line x" title="77:254	Given our choice of domain specificity, the next question is: which documents are non-reviews?" ></td>
	<td class="line x" title="78:254	Here, we adopt a simple and natural definition: a non-review is any document that belongs to the given domain but is not a review." ></td>
	<td class="line x" title="79:254	Dataset." ></td>
	<td class="line x" title="80:254	Now, recall from the introduction that we cast review identification as a classification task." ></td>
	<td class="line x" title="81:254	To train and test our review identifier, we use 2000 reviews and 2000 non-reviews from the movie domain." ></td>
	<td class="line x" title="82:254	The 2000 reviews are taken from Pang et al.s polarity dataset (version 2.0)3, which consists of an equal number of positive and negative reviews." ></td>
	<td class="line x" title="83:254	We collect the non-reviews for the 3Available from http://www.cs.cornell.edu/ people/pabo/movie-review-data." ></td>
	<td class="line x" title="84:254	movie domain from the Internet Movie Database website4, randomly selecting any documents from this site that are on the movie topic but are not reviews themselves." ></td>
	<td class="line x" title="85:254	With this criterion in mind, the 2000 non-review documents we end up with are either movie ads or plot summaries." ></td>
	<td class="line x" title="86:254	Training and testing the review identifier." ></td>
	<td class="line x" title="87:254	We perform 10-fold cross-validation (CV) experiments on the above dataset, using Joachims (1999) SVMlight package5 to train an SVM classifier for distinguishing reviews and non-reviews." ></td>
	<td class="line x" title="88:254	All learning parameters are set to their default values.6 Each document is first tokenized and downcased, and then represented as a vector of unigrams with length normalization.7 Following Pang et al.(2002), we use frequency as presence." ></td>
	<td class="line x" title="90:254	In other words, the ith element of the document vector is 1 if the corresponding unigram is present in the document and 0 otherwise." ></td>
	<td class="line x" title="91:254	The resulting classifier achieves an accuracy of 99.8%." ></td>
	<td class="line x" title="92:254	Classifying neutral reviews and non-reviews." ></td>
	<td class="line x" title="93:254	Admittedly, the high accuracy achieved using such a simple set of features is somewhat surprising, although it is consistent with previous results on document-level subjectivity classification in which accuracies of 94-97% were obtained (Yu and Hatzivassiloglou, 2003; Wiebe et al. , 2004)." ></td>
	<td class="line x" title="94:254	Before concluding that review classification is an easy task, we conduct an additional experiment: we train a review identifier on a new dataset where we keep the same 2000 non-reviews but replace the positive/negative reviews with 2000 neutral reviews (i.e. , reviews with a mediocre rating)." ></td>
	<td class="line x" title="95:254	Intuitively, a neutral review contains fewer terms with strong polarity than a positive/negative review." ></td>
	<td class="line x" title="96:254	Hence, this additional experiment would allow us to investigate whether the lack of strong polarized terms in neutral reviews would increase the difficulty of the learning task." ></td>
	<td class="line x" title="97:254	Our neutral reviews are randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews8 that have either a rating of 2 (on a 4-point scale) or 2.5 (on a 5-point scale)." ></td>
	<td class="line x" title="98:254	Each review then undergoes a semi-automatic preprocessing stage 4See http://www.imdb.com." ></td>
	<td class="line x" title="99:254	5Available from svmlight.joachims.org." ></td>
	<td class="line x" title="100:254	6We tried polynomial and RBF kernels, but none yields better performance than the default linear kernel." ></td>
	<td class="line x" title="101:254	7We observed that not performing length normalization hurts performance slightly." ></td>
	<td class="line x" title="102:254	8Also available from Pangs website." ></td>
	<td class="line x" title="103:254	See Footnote 3." ></td>
	<td class="line x" title="104:254	613 where (1) HTML tags and any header and trailer information (such as date and author identity) are removed; (2) the document is tokenized and downcased; (3) the rating information extracted by regular expressions is removed; and (4) the document is manually checked to ensure that the rating information is successfully removed." ></td>
	<td class="line x" title="105:254	When trained on this new dataset, the review identifier also achieves an accuracy of 99.8%, suggesting that this learning task isnt any harder in comparison to the previous one." ></td>
	<td class="line x" title="106:254	Discussion." ></td>
	<td class="line x" title="107:254	We hypothesized that the high accuracies are attributable to the different vocabulary used in reviews and non-reviews." ></td>
	<td class="line x" title="108:254	As part of our verification of this hypothesis, we plot the learning curve for each of the above experiments.9 We observe that a 99% accuracy was achieved in all cases even when only 200 training instances are used to acquire the review identifier." ></td>
	<td class="line x" title="109:254	The ability to separate the two classes with such a small amount of training data seems to imply that features strongly indicative of one or both classes are present." ></td>
	<td class="line x" title="110:254	To test this hypothesis, we examine the informative features for both classes." ></td>
	<td class="line x" title="111:254	To get these informative features, we rank the features by their weighted log-likelihood ratio (WLLR)10: P (wtjcj) log P (wtjcj)P (w tj:cj), where wt and cj denote the tth word in the vocabulary and the jth class, respectively." ></td>
	<td class="line x" title="112:254	Informally, a feature (in our case a unigram) w will have a high rank with respect to a class c if it appears frequently in c and infrequently in other classes." ></td>
	<td class="line x" title="113:254	This correlates reasonably well with what we think an informative feature should be." ></td>
	<td class="line x" title="114:254	A closer examination of the feature lists sorted by WLLR confirms our hypothesis that each of the two classes has its own set of distinguishing features." ></td>
	<td class="line x" title="115:254	Experiments with the book domain." ></td>
	<td class="line x" title="116:254	To understand whether these good review identification results only hold true for the movie domain, we conduct similar experiments with book reviews and non-reviews." ></td>
	<td class="line x" title="117:254	Specifically, we collect 1000 book reviews (consisting of a mixture of positive, negative, and neutral reviews) from the Barnes 9The curves are not shown due to space limitations." ></td>
	<td class="line x" title="118:254	10Nigam et al.(2000) show that this metric is effective at selecting good features for text classification." ></td>
	<td class="line x" title="120:254	Other commonly-used feature selection metrics are discussed in Yang and Pedersen (1997)." ></td>
	<td class="line x" title="121:254	and Noble website11, and 1000 non-reviews that are on the book topic (mostly book summaries) from Amazon.12 We then perform 10-fold CV experiments using these 2000 documents as before, achieving a high accuracy of 96.8%." ></td>
	<td class="line x" title="122:254	These results seem to suggest that automatic review identification can be achieved with high accuracy." ></td>
	<td class="line x" title="123:254	4 Polarity Classification Compared to review identification, polarity classification appears to be a much harder task." ></td>
	<td class="line x" title="124:254	This section examines the role of various linguistic knowledge sources in our learning-based polarity classification system." ></td>
	<td class="line x" title="125:254	4.1 Experimental Setup Like several previous work (e.g. , Mullen and Collier (2004), Pang and Lee (2004), Whitelaw et al.(2005)), we view polarity classification as a supervised learning task." ></td>
	<td class="line x" title="127:254	As in review identification, we use SVMlight with default parameter settings to train polarity classifiers13, reporting all results as 10-fold CV accuracy." ></td>
	<td class="line x" title="128:254	We evaluate our polarity classifiers on two movie review datasets, each of which consists of 1000 positive reviews and 1000 negative reviews." ></td>
	<td class="line x" title="129:254	The first one, which we will refer to as Dataset A, is the Pang et al. polarity dataset (version 2.0)." ></td>
	<td class="line x" title="130:254	The second one (Dataset B) was created by us, with the sole purpose of providing additional experimental results." ></td>
	<td class="line x" title="131:254	Reviews in Dataset B were randomly chosen from Pang et al.s pool of 27886 unprocessed movie reviews (see Section 3) that have either a positive or a negative rating." ></td>
	<td class="line x" title="132:254	We followed exactly Pang et al.s guideline when determining whether a review is positive or negative.14 Also, we took care to ensure that reviews included in Dataset B do not appear in Dataset A. We applied to these reviews the same four pre-processing steps that we did to the neutral reviews in the previous section." ></td>
	<td class="line x" title="133:254	4.2 Results The baseline classifier." ></td>
	<td class="line x" title="134:254	We can now train our baseline polarity classifier on each of the two 11www.barnesandnoble.com 12www.amazon.com 13We also experimented with polynomial and RBF kernels when training polarity classifiers, but neither yields better results than linear kernels." ></td>
	<td class="line x" title="135:254	14The guidelines come with their polarity dataset." ></td>
	<td class="line x" title="136:254	Brie y, a positive review has a rating of  3.5 (out of 5) or  3 (out of 4), whereas a negative review has a rating of 2 (out of 5) or  1.5 (out of 4)." ></td>
	<td class="line x" title="137:254	614 System Variation Dataset A Dataset B Baseline 87.1 82.7 Adding bigrams 89.2 84.7 and trigrams Adding dependency 89.0 84.5 relations Adding polarity 90.4 86.2 info of adjectives Discarding objective 90.5 86.1 materials Table 1: Polarity classification accuracies." ></td>
	<td class="line x" title="138:254	datasets." ></td>
	<td class="line x" title="139:254	Our baseline classifier employs as features the k highest-ranking unigrams according to WLLR, with k/2 features selected from each class." ></td>
	<td class="line x" title="140:254	Results with k = 10000 are shown in row 1 of Table 1.15 As we can see, the baseline achieves an accuracy of 87.1% and 82.7% on Datasets A and B, respectively." ></td>
	<td class="line x" title="141:254	Note that our result on Dataset A is as strong as that obtained by Pang and Lee (2004) via their subjectivity summarization algorithm, which retains only the subjective portions of a document." ></td>
	<td class="line x" title="142:254	As a sanity check, we duplicated Pang et al.s (2002) baseline in which all unigrams that appear four or more times in the training documents are used as features." ></td>
	<td class="line x" title="143:254	The resulting classifier achieves an accuracy of 87.2% and 82.7% for Datasets A and B, respectively." ></td>
	<td class="line x" title="144:254	Neither of these results are significantly different from our baseline results.16 Adding higher-order n-grams." ></td>
	<td class="line x" title="145:254	The negative results that Pang et al.(2002) obtained when using bigrams as features for their polarity classifier seem to suggest that high-order n-grams are not useful for polarity classification." ></td>
	<td class="line x" title="147:254	However, recent research in the related (but arguably simpler) task of text classification shows that a bigrambased text classifier outperforms its unigrambased counterpart (Peng et al. , 2003)." ></td>
	<td class="line x" title="148:254	This prompts us to re-examine the utility of high-order n-grams in polarity classification." ></td>
	<td class="line x" title="149:254	In our experiments we consider adding bigrams and trigrams to our baseline feature set." ></td>
	<td class="line x" title="150:254	However, since these higher-order n-grams significantly outnumber the unigrams, adding all of them to the feature set will dramatically increase the dimen15We experimented with several values of k and obtained the best result with k = 10000." ></td>
	<td class="line x" title="151:254	16We use two-tailed paired t-tests when performing significance testing, with p set to 0.05 unless otherwise stated." ></td>
	<td class="line x" title="152:254	sionality of the feature space and may undermine the impact of the unigrams in the resulting classifier." ></td>
	<td class="line x" title="153:254	To avoid this potential problem, we keep the number of unigrams and higher-order n-grams equal." ></td>
	<td class="line x" title="154:254	Specifically, we augment the baseline feature set (consisting of 10000 unigrams) with 5000 bigrams and 5000 trigrams." ></td>
	<td class="line x" title="155:254	The bigrams and trigrams are selected based on their WLLR computed over the positive reviews and negative reviews in the training set for each CV run." ></td>
	<td class="line x" title="156:254	Results using this augmented feature set are shown in row 2 of Table 1." ></td>
	<td class="line x" title="157:254	We see that accuracy rises significantly from 87.1% to 89.2% for Dataset A and from 82.7% to 84.7% for Dataset B. This provides evidence that polarity classification can indeed benefit from higher-order n-grams." ></td>
	<td class="line x" title="158:254	Adding dependency relations." ></td>
	<td class="line x" title="159:254	While bigrams and trigrams are good at capturing local dependencies, dependency relations can be used to capture non-local dependencies among the constituents of a sentence." ></td>
	<td class="line x" title="160:254	Hence, we hypothesized that our ngram-based polarity classifier would benefit from the addition of dependency-based features." ></td>
	<td class="line x" title="161:254	Unlike most previous work on polarity classification, which has largely focused on exploiting adjective-noun (AN) relations (e.g. , Dave et al.(2003), Popescu and Etzioni (2005)), we hypothesized that subject-verb (SV) and verb-object (VO) relations would also be useful for the task." ></td>
	<td class="line x" title="163:254	The following (one-sentence) review illustrates why." ></td>
	<td class="line x" title="164:254	While I really like the actors, the plot is rather uninteresting." ></td>
	<td class="line x" title="165:254	A unigram-based polarity classifier could be confused by the simultaneous presence of the positive term like and the negative term uninteresting when classifying this review." ></td>
	<td class="line x" title="166:254	However, incorporating the VO relation (like, actors) as a feature may allow the learner to learn that the author likes the actors and not necessarily the movie." ></td>
	<td class="line x" title="167:254	In our experiments, the SV, VO and AN relations are extracted from each document by the MINIPAR dependency parser (Lin, 1998)." ></td>
	<td class="line x" title="168:254	As with n-grams, instead of using all the SV, VO and AN relations as features, we select among them the best 5000 according to their WLLR and retrain the polarity classifier with our n-gram-based feature set augmented by these 5000 dependencybased features." ></td>
	<td class="line x" title="169:254	Results in row 3 of Table 1 are somewhat surprising: the addition of dependencybased features does not offer any improvements over the simple n-gram-based classifier." ></td>
	<td class="line x" title="170:254	615 Incorporating manually tagged term polarity." ></td>
	<td class="line x" title="171:254	Next, we consider incorporating a set of features that are computed based on the polarity of adjectives." ></td>
	<td class="line x" title="172:254	As noted before, we desire a high-precision, high-coverage lexicon." ></td>
	<td class="line x" title="173:254	So, instead of exploiting a learned lexicon, we manually develop one." ></td>
	<td class="line x" title="174:254	To construct the lexicon, we take Pang et al.s pool of unprocessed documents (see Section 3), remove those that appear in either Dataset A or Dataset B17, and compile a list of adjectives from the remaining documents." ></td>
	<td class="line x" title="175:254	Then, based on heuristics proposed in psycholinguistics18, we handannotate each adjective with its prior polarity (i.e. , polarity in the absence of context)." ></td>
	<td class="line x" title="176:254	Out of the 45592 adjectives we collected, 3599 were labeled as positive, 3204 as negative, and 38789 as neutral." ></td>
	<td class="line x" title="177:254	A closer look at these adjectives reveals that they are by no means domain-dependent despite the fact that they were taken from movie reviews." ></td>
	<td class="line x" title="178:254	Now let us consider a simple procedure P for deriving a feature set that incorporates information from our lexicon: (1) collect all the bigrams from the training set; (2) for each bigram that contains at least one adjective labeled as positive or negative according to our lexicon, create a new feature that is identical to the bigram except that each adjective is replaced with its polarity label19; (3) merge the list of newly generated features with the list of bigrams20 and select the top 5000 features from the merged list according to their WLLR." ></td>
	<td class="line x" title="179:254	We then repeat procedure P for the trigrams and also the dependency features, resulting in a total of 15000 features." ></td>
	<td class="line x" title="180:254	Our new feature set comprises these 15000 features as well as the 10000 unigrams we used in the previous experiments." ></td>
	<td class="line x" title="181:254	Results of the polarity classifier that incorporates term polarity information are encouraging (see row 4 of Table 1)." ></td>
	<td class="line x" title="182:254	In comparison to the classifier that uses only n-grams and dependency-based features (row 3), accuracy increases significantly (p =.1) from 89.2% to 90.4% for Dataset A, and from 84.7% to 86.2% for Dataset B. These results suggest that the classifier has benefited from the 17We treat the test documents as unseen data that should not be accessed for any purpose during system development." ></td>
	<td class="line x" title="183:254	18http://www.sci.sdsu.edu/CAL/wordlist 19Neutral adjectives are not replaced." ></td>
	<td class="line x" title="184:254	20A newly generated feature could be misleading for the learner if the contextual polarity (i.e. , polarity in the presence of context) of the adjective involved differs from its prior polarity (see Wilson et al.(2005))." ></td>
	<td class="line x" title="186:254	The motivation behind merging with the bigrams is to create a feature set that is more robust in the face of potentially misleading generalizations." ></td>
	<td class="line x" title="187:254	use of features that are less sparse than n-grams." ></td>
	<td class="line x" title="188:254	Using objective information." ></td>
	<td class="line x" title="189:254	Some of the 25000 features we generated above correspond to n-grams or dependency relations that do not contain subjective information." ></td>
	<td class="line x" title="190:254	We hypothesized that not employing these objective features in the feature set would improve system performance." ></td>
	<td class="line x" title="191:254	More specifically, our goal is to use procedure P again to generate 25000 subjective features by ensuring that the objective ones are not selected for incorporation into our feature set." ></td>
	<td class="line x" title="192:254	To achieve this goal, we first use the following rote-learning procedure to identify objective material: (1) extract all unigrams that appear in objective documents, which in our case are the 2000 non-reviews used in review identification [see Section 3]; (2) from these objective unigrams, we take the best 20000 according to their WLLR computed over the non-reviews and the reviews in the training set for each CV run; (3) repeat steps 1 and 2 separately for bigrams, trigrams and dependency relations; (4) merge these four lists to create our 80000-element list of objective material." ></td>
	<td class="line x" title="193:254	Now, we can employ procedure P to get a list of 25000 subjective features by ensuring that those that appear in our 80000-element list are not selected for incorporation into our feature set." ></td>
	<td class="line x" title="194:254	Results of our classifier trained using these subjective features are shown in row 5 of Table 1." ></td>
	<td class="line x" title="195:254	Somewhat surprisingly, in comparison to row 4, we see that our method for filtering objective features does not help improve performance on the two datasets." ></td>
	<td class="line x" title="196:254	We will examine the reasons in the following subsection." ></td>
	<td class="line x" title="197:254	4.3 Discussion and Further Analysis Using the four types of knowledge sources previously described, our polarity classifier significantly outperforms a unigram-based baseline classifier." ></td>
	<td class="line x" title="198:254	In this subsection, we analyze some of these results and conduct additional experiments in an attempt to gain further insight into the polarity classification task." ></td>
	<td class="line x" title="199:254	Due to space limitations, we will simply present results on Dataset A below, and show results on Dataset B only in cases where a different trend is observed." ></td>
	<td class="line x" title="200:254	The role of feature selection." ></td>
	<td class="line x" title="201:254	In all of our experiments we used the best k features obtained via WLLR." ></td>
	<td class="line x" title="202:254	An interesting question is: how will these results change if we do not perform feature selection?" ></td>
	<td class="line x" title="203:254	To investigate this question, we conduct two 616 experiments." ></td>
	<td class="line x" title="204:254	First, we train a polarity classifier using all unigrams from the training set." ></td>
	<td class="line x" title="205:254	Second, we train another polarity classifier using all unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="206:254	We obtain an accuracy of 87.2% and 79.5% for the first and second experiments, respectively." ></td>
	<td class="line x" title="207:254	In comparison to our baseline classifier, which achieves an accuracy of 87.1%, we can see that using all unigrams does not hurt performance, but performance drops abruptly with the addition of all bigrams and trigrams." ></td>
	<td class="line x" title="208:254	These results suggest that feature selection is critical when bigrams and trigrams are used in conjunction with unigrams for training a polarity classifier." ></td>
	<td class="line x" title="209:254	The role of bigrams and trigrams." ></td>
	<td class="line x" title="210:254	So far we have seen that training a polarity classifier using only unigrams gives us reasonably good, though not outstanding, results." ></td>
	<td class="line x" title="211:254	Our question, then, is: would bigrams alone do a better job at capturing the sentiment of a document than unigrams?" ></td>
	<td class="line x" title="212:254	To answer this question, we train a classifier using all bigrams (without feature selection) and obtain an accuracy of 83.6%, which is significantly worse than that of a unigram-only classifier." ></td>
	<td class="line x" title="213:254	Similar results were also obtained by Pang et al.(2002)." ></td>
	<td class="line x" title="215:254	It is possible that the worse result is due to the presence of a large number of irrelevant bigrams." ></td>
	<td class="line x" title="216:254	To test this hypothesis, we repeat the above experiment except that we only use the best 10000 bigrams selected according to WLLR." ></td>
	<td class="line x" title="217:254	Interestingly, the resulting classifier gives us a lower accuracy of 82.3%, suggesting that the poor accuracy is not due to the presence of irrelevant bigrams." ></td>
	<td class="line x" title="218:254	To understand why using bigrams alone does not yield a good classification model, we examine a number of test documents and find that the feature vectors corresponding to some of these documents (particularly the short ones) have all zeroes in them." ></td>
	<td class="line x" title="219:254	In other words, none of the bigrams from the training set appears in these reviews." ></td>
	<td class="line x" title="220:254	This suggests that the main problem with the bigram model is likely to be data sparseness." ></td>
	<td class="line x" title="221:254	Additional experiments show that the trigram-only classifier yields even worse results than the bigram-only classifier, probably because of the same reason." ></td>
	<td class="line x" title="222:254	Nevertheless, these higher-order n-grams play a non-trivial role in polarity classification: we have shown that the addition of bigrams and trigrams selected via WLLR to a unigram-based classifier significantly improves its performance." ></td>
	<td class="line x" title="223:254	The role of dependency relations." ></td>
	<td class="line x" title="224:254	In the previous subsection we see that dependency relations do not contribute to overall performance on top of bigrams and trigrams." ></td>
	<td class="line x" title="225:254	There are two plausible reasons." ></td>
	<td class="line x" title="226:254	First, dependency relations are simply not useful for polarity classification." ></td>
	<td class="line x" title="227:254	Second, the higher-order n-grams and the dependency-based features capture essentially the same information and so using either of them would be sufficient." ></td>
	<td class="line x" title="228:254	To test the first hypothesis, we train a classifier using only 10000 unigrams and 10000 dependency-based features (both selected according to WLLR)." ></td>
	<td class="line x" title="229:254	For Dataset A, the classifier achieves an accuracy of 87.1%, which is statistically indistinguishable from our baseline result." ></td>
	<td class="line x" title="230:254	On the other hand, the accuracy for Dataset B is 83.5%, which is significantly better than the corresponding baseline (82.7%) at the p = .1 level." ></td>
	<td class="line x" title="231:254	These results indicate that dependency information is somewhat useful for the task when bigrams and trigrams are not used." ></td>
	<td class="line x" title="232:254	So the first hypothesis is not entirely true." ></td>
	<td class="line x" title="233:254	So, it seems to be the case that the dependency relations do not provide useful knowledge for polarity classification only in the presence of bigrams and trigrams." ></td>
	<td class="line x" title="234:254	This is somewhat surprising, since these n-grams do not capture the non-local dependencies (such as those that may be present in certain SV or VO relations) that should intuitively be useful for polarity classification." ></td>
	<td class="line x" title="235:254	To better understand this issue, we again examine a number of test documents." ></td>
	<td class="line x" title="236:254	Our initial investigation suggests that the problem might have stemmed from the fact that MINIPAR returns dependency relations in which all the verb in ections are removed." ></td>
	<td class="line x" title="237:254	For instance, given the sentence My cousin Paul really likes this long movie, MINIPAR will return the VO relation (like, movie)." ></td>
	<td class="line x" title="238:254	To see why this can be a problem, consider another sentence I like this long movie." ></td>
	<td class="line x" title="239:254	From this sentence, MINIPAR will also extract the VO relation (like, movie)." ></td>
	<td class="line x" title="240:254	Hence, this same VO relation is capturing two different situations, one in which the author himself likes the movie, and in the other, the authors cousin likes the movie." ></td>
	<td class="line x" title="241:254	The overgeneralization resulting from these stemmed relations renders dependency information not useful for polarity classification." ></td>
	<td class="line x" title="242:254	Additional experiments are needed to determine the role of dependency relations when stemming in MINIPAR is disabled." ></td>
	<td class="line x" title="243:254	617 The role of objective information." ></td>
	<td class="line x" title="244:254	Results from the previous subsection suggest that our method for extracting objective materials and removing them from the reviews is not effective in terms of improving performance." ></td>
	<td class="line x" title="245:254	To determine the reason, we examine the n-grams and the dependency relations that are extracted from the nonreviews." ></td>
	<td class="line x" title="246:254	We find that only in a few cases do these extracted objective materials appear in our set of 25000 features obtained in Section 4.2." ></td>
	<td class="line x" title="247:254	This explains why our method is not as effective as we originally thought." ></td>
	<td class="line x" title="248:254	We conjecture that more sophisticated methods would be needed in order to take advantage of objective information in polarity classification (e.g. , Koppel and Schler (2005))." ></td>
	<td class="line x" title="249:254	5 Conclusions We have examined two problems in documentlevel sentiment analysis, namely, review identification and polarity classification." ></td>
	<td class="line x" title="250:254	We first found that review identification can be achieved with very high accuracies (97-99%) simply by training an SVM classifier using unigrams as features." ></td>
	<td class="line x" title="251:254	We then examined the role of several linguistic knowledge sources in polarity classification." ></td>
	<td class="line x" title="252:254	Our results suggested that bigrams and trigrams selected according to the weighted log-likelihood ratio as well as manually tagged term polarity information are very useful features for the task." ></td>
	<td class="line x" title="253:254	On the other hand, no further performance gains are obtained by incorporating dependency-based information or filtering objective materials from the reviews using our proposed method." ></td>
	<td class="line x" title="254:254	Nevertheless, the resulting polarity classifier compares favorably to state-of-the-art sentiment classification systems." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2081
Whose Thumb Is It Anyway? Classifying Author Personality From Weblog Text
Oberlander, Jon;Nowson, Scott;"></td>
	<td class="line x" title="1:215	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 627634, Sydney, July 2006." ></td>
	<td class="line x" title="2:215	c2006 Association for Computational Linguistics Whose thumb is it anyway?" ></td>
	<td class="line x" title="3:215	Classifying author personality from weblog text Jon Oberlander School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW j.oberlander@ed.ac.uk Scott Nowson School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW s.nowson@ed.ac.uk Abstract We report initial results on the relatively novel task of automatic classification of author personality." ></td>
	<td class="line x" title="4:215	Using a corpus of personal weblogs, or blogs, we investigate the accuracy that can be achieved when classifying authors on four important personality traits." ></td>
	<td class="line x" title="5:215	We explore both binary and multiple classification, using differing sets of n-gram features." ></td>
	<td class="line x" title="6:215	Results are promising for all four traits examined." ></td>
	<td class="line x" title="7:215	1 Introduction There is now considerable interest in affective language processing." ></td>
	<td class="line oc" title="8:215	Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al. , 2002; Turney, 2002; Dave et al. , 2003; Liu et al. , 2003; Pang and Lee, 2005; Shanahan et al. , 2005)." ></td>
	<td class="line x" title="9:215	Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging from short-lived feelings, through emotions, to moods, and ultimately to long-lived, slowly-changing personality characteristics." ></td>
	<td class="line x" title="10:215	Within computational linguistics, most work has focussed on sentiment and opinion concerning specific entities or events, and on binary classifications of these." ></td>
	<td class="line oc" title="11:215	For instance, both Pang and Lee (2002) and Turney (2002) consider the thumbs up/thumbs down decision: is a film review positive or negative?" ></td>
	<td class="line x" title="12:215	However, Pang and Lee (2005) point out that ranking items or comparing reviews will benefit from finer-grained classifications, over multiple ordered classes: is a film review twoor threeor four-star?" ></td>
	<td class="line x" title="13:215	And at the same time, some work now considers longer-term affective states." ></td>
	<td class="line x" title="14:215	For example, Mishne (2005) aims to classify the primary mood of weblog postings; the study encompasses both fine-grained (but non-ordered) multiple classification (frustrated/loved/etc)." ></td>
	<td class="line x" title="15:215	and coarse-grained binary classification (active/passive, positive/negative)." ></td>
	<td class="line x" title="16:215	This paper is about the move to finer-grained multiple classifications; and also about weblogs." ></td>
	<td class="line x" title="17:215	But it is also about even more persistent affective states; in particular, it focusses on classifying author personality." ></td>
	<td class="line x" title="18:215	We would argue that ongoing work on sentiment analysis or opinion-mining stands to benefit from progress on personalityclassification." ></td>
	<td class="line x" title="19:215	The reason is that people vary in personality, and they vary in how they appraise eventsand hence, in how strongly they phrase their praise or condemnation." ></td>
	<td class="line x" title="20:215	Reiter and Sripada (2004) suggest that lexical choice may sometimes be determined by a writers idiolecttheir personal language preferences." ></td>
	<td class="line x" title="21:215	We suggest that while idiolect can be a matter of accident or experience, it may also reflect systematic, personality-based differences." ></td>
	<td class="line x" title="22:215	This can help explain why, as Pang and Lee (2005) note, one persons four star review is anothers two-star." ></td>
	<td class="line x" title="23:215	To put it more bluntly, if youre not a very outgoing sort of person, then your thumbs up might be mistaken for someone elses thumbs down." ></td>
	<td class="line x" title="24:215	But how do we distinguish such people?" ></td>
	<td class="line x" title="25:215	Or, if we spot a thumbs-up review, how can we tell whose thumb it is, anyway?" ></td>
	<td class="line x" title="26:215	The paper is structured as follows." ></td>
	<td class="line x" title="27:215	It introduces trait theories of personality, notes work to date on personality classification, and raises some questions." ></td>
	<td class="line x" title="28:215	It then outlines the weblog corpus and the experiments, which compare classification accuracies for four personality dimensions, seven tasks, and five feature selection policies." ></td>
	<td class="line x" title="29:215	We discuss the implications of the results, and related work, and end with suggestions for next steps." ></td>
	<td class="line x" title="30:215	627 2 Background: traits and language Cattells pioneering work led to the isolation of 16 primary personality factors, and later work on secondary factors led to Costa and McCraes fivefactor model, closely related to the Big Five models emerging from lexical research (Costa and McCrae, 1992)." ></td>
	<td class="line x" title="31:215	Each factor gives a continuous dimension for personality scoring." ></td>
	<td class="line x" title="32:215	These are: Extraversion; Neuroticism; Openness; Agreeableness; and Conscientiousness (Matthews et al. , 2003)." ></td>
	<td class="line x" title="33:215	Work has also investigated whether scores on these dimensions correlate with language use (Scherer, 1979; Dewaele and Furnham, 1999)." ></td>
	<td class="line x" title="34:215	Building on the earlier work of Gottschalk and Gleser, Pennebaker and colleagues secured significant results using the Linguistic Inquiry and Word Count text analysis program (Pennebaker et al. , 2001)." ></td>
	<td class="line x" title="35:215	This primarily counts relative frequencies of word-stems in pre-defined semantic and syntactic categories." ></td>
	<td class="line x" title="36:215	It shows, for instance, that high Neuroticism scorers use: more first person singular and negative emotion words; and fewer articles and positive emotion words (Pennebaker and King, 1999)." ></td>
	<td class="line x" title="37:215	So, can a text classifier trained on such features predict the author personality?" ></td>
	<td class="line x" title="38:215	We know of only one published study: Argamon et al.(2005) focussed on Extraversion and Neuroticism, dividing Pennebaker and Kings (1999) population into the topand bottom-third scorers on a dimension, and discarding the middle third." ></td>
	<td class="line x" title="40:215	For both dimensions, using a restricted feature set, they report binary classification accuracy of around 58%: an 8% absolute improvement over their baseline." ></td>
	<td class="line x" title="41:215	Although mood is more malleable, work on it is also relevant (Mishne, 2005)." ></td>
	<td class="line x" title="42:215	Using a more typical feature set (including n-grams of words and parts-of-speech), the best mood classification accuracy was 66%, for confused." ></td>
	<td class="line x" title="43:215	At a coarser grain, moods could be classified with accuracies of 57% (active vs. passive), and 60% (positive vs. negative)." ></td>
	<td class="line x" title="44:215	So, Argamon et al. used a restricted feature set for binary classification on two dimensions: Extraversion and Neuroticism." ></td>
	<td class="line x" title="45:215	Given this, we now pursue three questions." ></td>
	<td class="line x" title="46:215	(1) Can we improve performance on a similar binary classification task?" ></td>
	<td class="line x" title="47:215	(2) How accurate can classification be on the other dimensions?" ></td>
	<td class="line x" title="48:215	(3) How accurate can multiple three-way or five-wayclassification be?" ></td>
	<td class="line x" title="49:215	3 The weblog corpus 3.1 Construction A corpus of personal weblog (blog) text has been gathered (Nowson, 2006)." ></td>
	<td class="line x" title="50:215	Participants were recruited directly via e-mail to suitable candidates, and indirectly by word-of-mouth: many participants wrote about the study in their blogs." ></td>
	<td class="line x" title="51:215	Participants were first required to answer sociobiographic and personality questionnaires." ></td>
	<td class="line x" title="52:215	The personality instrument has specifically been validated for online completion (Buchanan, 2001)." ></td>
	<td class="line x" title="53:215	It was derived from the 50-item IPIP implementation of Costa and McCraes (1992) revised NEO personality inventory; participants rate themselves on 41items using a 5-point Likert scale." ></td>
	<td class="line x" title="54:215	This provides scores for Neuroticism, Extraversion, Openness, Agreeableness and Conscientiousness." ></td>
	<td class="line x" title="55:215	After completing this stage, participants were requested to submit one months worth of prior weblog postings." ></td>
	<td class="line x" title="56:215	The month was pre-specified so as to reduce the effects of an individual choosing what they considered their best or preferred month." ></td>
	<td class="line x" title="57:215	Raw submissions were marked-up using XML so as to automate extraction of the desired text." ></td>
	<td class="line x" title="58:215	Text was also marked-up by post type, such as purely personal, commentary reporting of external matters, or direct posting of internet memes such as quizzes." ></td>
	<td class="line x" title="59:215	The corpus consisted of 71 participants (47 females, 24 males; average ages 27.8 and 29.4, respectively) and only the text marked as personal from each weblog, approximately 410,000 words." ></td>
	<td class="line x" title="60:215	To eliminate undue influence of particularly verbose individuals, the size of each weblog file was truncated at the mean word count plus 2 standard deviations." ></td>
	<td class="line x" title="61:215	3.2 Personality distribution It might be thought that bloggers are more Extravert than most (because they express themselves in public); or perhaps that they are less Extravert (because they keep diaries in the first place)." ></td>
	<td class="line x" title="62:215	In fact, plotting the Extraversion scores for the corpus authors gives an apparently normal distribution; and the same applies for three other dimensions." ></td>
	<td class="line x" title="63:215	However, scores for Openness to experience are not normally distributed." ></td>
	<td class="line x" title="64:215	Perhaps bloggers are more Open than average; or perhaps there is response bias." ></td>
	<td class="line x" title="65:215	Without a comparison sample of matched non-bloggers, one cannot say, and Openness is not discussed further in this paper." ></td>
	<td class="line x" title="66:215	628 4 Experiments We are thus confined to classifying on four personality dimensions." ></td>
	<td class="line x" title="67:215	However, a number of other variables remain: different learning algorithms can be employed; authors in the corpus can be grouped in several ways, leading to various classification tasks; and more or less restricted linguistic feature sets can be used as input to the classifier." ></td>
	<td class="line x" title="68:215	4.1 Algorithms Support Vector Machines (SVM) appear to work well for binary sentiment classification tasks, so Argamon et al.(2003) and Pang and Lee (2005) consider One-vs-All, or All-vs-All, variants on SVM, to permit multiple classifications." ></td>
	<td class="line x" title="70:215	Choice of algorithm is not our focus, but it remains to be seen whether SVM outperforms Nave Bayes (NB) for personality classification." ></td>
	<td class="line x" title="71:215	Thus, we will use both on the binary Tasks 1 to 3 (defined in section 4.2.1), for each of the personality dimensions, and each of the manually-selected feature sets (Levels I to IV, defined in section 4.3)." ></td>
	<td class="line x" title="72:215	Whichever performs better overall is then reported in full, and used for the multiple Tasks 4 to 7 (defined in section 4.2.2)." ></td>
	<td class="line x" title="73:215	Both approaches are applied as implemented in the WEKA toolkit (Witten and Frank, 1999) and use 10-fold cross validation." ></td>
	<td class="line x" title="74:215	4.2 Tasks For any blog, we have available the scores, on continuous scales, of its author on four personality dimensions." ></td>
	<td class="line x" title="75:215	But for the classifier, the task can be made more or less easy, by grouping authors on each of the dimensions." ></td>
	<td class="line x" title="76:215	The simplest tasks are, of course, binary: given the sequence of words from a blog, the classifier simply has to decide whether the author is (for instance) high or low in Agreeableness." ></td>
	<td class="line x" title="77:215	Binary tasks vary in difficulty, depending on whether authors scoring in the middle of a dimension are left out, or not; and if they are left out, what proportion of authors are left out." ></td>
	<td class="line x" title="78:215	More complex tasks will also vary in difficulty depending on who is left out." ></td>
	<td class="line x" title="79:215	But in the cases considered here, middle authors are now included." ></td>
	<td class="line x" title="80:215	For a three-way task, the classifier must decide if an author is high, medium or low; and those authors known to score between these categories may, or may not, be left out." ></td>
	<td class="line x" title="81:215	In the most challenging five-way task, no-one is left out." ></td>
	<td class="line x" title="82:215	The point of considering such tasks is to gradually approximate the most challenging task of all: continuous rating." ></td>
	<td class="line x" title="83:215	4.2.1 Binary classification tasks In these task variants, the goal is to classify authors as either high or low scorers on a dimension: 1." ></td>
	<td class="line x" title="84:215	The easiest approach is to keep the high and low groups as far apart as possible: high scorers (H) are those whose scores fall above 1 SD above the mean; low scorers (L) are those whose scores fall below 1 SD below the mean." ></td>
	<td class="line x" title="85:215	2." ></td>
	<td class="line x" title="86:215	Task-1 creates distinct groups, at the price of excluding over 50% of the corpus from the analysis." ></td>
	<td class="line x" title="87:215	To include more of the corpus, parameters are relaxed: the high group (HH) includes anyone whose score is above.5 SD above the mean; the low group (LL) is similarly placed below." ></td>
	<td class="line x" title="88:215	3." ></td>
	<td class="line x" title="89:215	The most obvious task (but not the easiest) arises from dividing the corpus in half about the mean score." ></td>
	<td class="line x" title="90:215	This creates high (HHH) and low (LLL) groups, covering the entire population." ></td>
	<td class="line x" title="91:215	Inevitably, some HHH scorers will actually have scores much closer to those of LLL scorers than to other HHH scorers." ></td>
	<td class="line x" title="92:215	These sub-groups are tabulated in Table 1, giving the size of each group within each trait." ></td>
	<td class="line x" title="93:215	Note that in Task-2, the standard-deviation-based divisions contain very nearly the top third and bottom third of the population for each dimension." ></td>
	<td class="line x" title="94:215	Hence, Task-2 is closest in proportion to the division by thirds used in Argamon et al.(2005)." ></td>
	<td class="line x" title="96:215	Lowest . . ." ></td>
	<td class="line x" title="97:215	Highest 1 L  H 2 LL  HH 3 LLL HHH N1 12  13 N2 25  22 N3 39 32 E1 11  12 E2 23  24 E3 32 39 A1 11  13 A2 22  21 A3 34 37 C1 11  14 C2 17  27 C3 30 41 Table 1: Binary task groups: division method and author numbers." ></td>
	<td class="line x" title="98:215	N = Neuroticism; E = Extraversion; A = Agreeableness; C = Conscientiousness." ></td>
	<td class="line x" title="99:215	629 4.2.2 Multiple classification tasks 4." ></td>
	<td class="line x" title="100:215	Takes the greatest distinction between high (H) and low (L) groups from Task-1, and adds a medium group, but attempts to reduce the possibility of inter-group confusion by including only the smaller medium (m) group omitted from Task-2." ></td>
	<td class="line x" title="101:215	Not all subjects are therefore included in this analysis." ></td>
	<td class="line x" title="102:215	Since the three groups to be classified are completely distinct, this should be the easiest of the four multiple-class tasks." ></td>
	<td class="line x" title="103:215	5." ></td>
	<td class="line x" title="104:215	Following Task-4, this uses the most distinct high (H) and low (L) groups, but now considers all remaining subjects medium (M)." ></td>
	<td class="line x" title="105:215	6." ></td>
	<td class="line x" title="106:215	Following Task-2, this uses the larger high (hH) and low (Ll) groups, with all those in between forming the medium (m) group." ></td>
	<td class="line x" title="107:215	7." ></td>
	<td class="line x" title="108:215	Using the distinction between the high and low groups of Task-5 and -6, this creates a 5-way split: highest (H), relatively high (h), medium (m), relatively low (l) and lowest (L)." ></td>
	<td class="line x" title="109:215	With the greatest number of classes, this task is the hardest." ></td>
	<td class="line x" title="110:215	These sub-groups are tabulated in Table 2, giving the size of each group within each trait." ></td>
	<td class="line x" title="111:215	Lowest . . ." ></td>
	<td class="line x" title="112:215	Highest 4 L  m  H 5 L M H 6 Ll m hH 7 L l m h H N4 12  24  13 N5 12 46 13 N6 25 24 22 N7 12 13 24 9 13 E4 11  24  12 E5 11 48 12 E6 23 24 24 E7 11 12 24 12 12 A4 11  28  13 A5 11 47 13 A6 22 28 21 A7 11 11 28 8 13 C4 11  27  14 C5 11 46 14 C6 17 27 27 C7 11 6 27 13 14 Table 2: 3-way/5-way task groups: division method and author numbers." ></td>
	<td class="line x" title="113:215	N = Neuroticism; E = Extraversion; A = Agreeableness; C = Conscientiousness." ></td>
	<td class="line x" title="114:215	4.3 Feature selection There are many possible features that can be used for automatic text classification." ></td>
	<td class="line x" title="115:215	These experiments use essentially word-based biand trigrams." ></td>
	<td class="line x" title="116:215	It should be noted, however, that some generalisations have been made: all proper nouns were identified via CLAWS tagging using the WMatrix tool (Rayson, 2003), and replaced with a single marker (NP1); punctuation was collapsed into a single marker (<p>); and additional tags correspond to non-linguistic features of blogs for instance, <SOP> and <EOP> were used the mark the start and end of individual blogs posts." ></td>
	<td class="line x" title="117:215	Word n-gram approaches provide a large feature space with which to work." ></td>
	<td class="line x" title="118:215	But in the general interest of computational tractability, it is useful to reduce the size of the feature set." ></td>
	<td class="line x" title="119:215	There are many automatic approaches to feature selection, exploiting, for instance, information gain (Quinlan, 1993)." ></td>
	<td class="line x" title="120:215	However, manual methods can offer principled ways of both reducing the size of the set and avoiding overfitting." ></td>
	<td class="line x" title="121:215	We therefore explore the effect of different levels of restriction on the feature sets, and compare them with automatic feature selection." ></td>
	<td class="line x" title="122:215	The levels of restriction are as follows: I The least restricted feature set consists of the n-grams most commonly occurring within the blog corpus." ></td>
	<td class="line x" title="123:215	Therefore, the feature set for each personality dimension is to be drawn from the same pool." ></td>
	<td class="line x" title="124:215	The difference lies in the number of features selected: the size of the set will match that of the next level of restriction." ></td>
	<td class="line x" title="125:215	II The next set includes only those n-grams which were distinctive for the two extremes of each personality trait." ></td>
	<td class="line x" title="126:215	Only features with a corpus frequency 5 are included." ></td>
	<td class="line x" title="127:215	This allows accurate log-likelihood G2 statistics to be computed (Rayson, 2003)." ></td>
	<td class="line x" title="128:215	Distinct collocations are identified via a three way comparison between the H and L groups in Task-1 (see section 4.2.1) and a third, neutral group." ></td>
	<td class="line x" title="129:215	This neutral group contains all those individuals who fell in the medium group (M) for all four traits in the study; the resulting group was of comparable size to the H and L groups for each trait." ></td>
	<td class="line x" title="130:215	Hence, this approach selects features using only a subset of the corpus." ></td>
	<td class="line x" title="131:215	Ngram software was used to identify and count collocations within a sub-corpus (Banerjee 630 and Pedersen, 2003)." ></td>
	<td class="line x" title="132:215	For each feature found, its frequency and relative frequency are calculated." ></td>
	<td class="line x" title="133:215	This permits relative frequency ratios and log-likelihood comparisons to be made between High-Low, High-Neutral and LowNeutral." ></td>
	<td class="line x" title="134:215	Only features that prove distinctive for the H or L groups with a significance of p < .01 are included in the feature set." ></td>
	<td class="line x" title="135:215	III The next set takes into account the possibility that, for a group used in Level-II, an ngram may be used relatively frequently, but only because a small number of authors in a group use it very frequently, while others in the same group use it not at all." ></td>
	<td class="line x" title="136:215	To enter the Level-III set, an n-gram meeting the Level-II criteria must also be used by at least 50%1 of the individuals within the subgroup for which it is reported to be distinctive." ></td>
	<td class="line x" title="137:215	IV While Level-III guards against excessive individual influence, it may abstract too far from the fine-grained variation within a personality trait." ></td>
	<td class="line x" title="138:215	The final manual set therefore includes only those n-grams that meet the Level-II criteria with p < .001, meet the Level-III criteria, and also correlate significantly (p < .05) with individual personality trait scores." ></td>
	<td class="line x" title="139:215	V Finally, it is possible to allow the n-gram feature set to be selected automatically during training." ></td>
	<td class="line x" title="140:215	The set to be selected from is the broadest of the manually filtered sets, those n-grams that meet the Level-II criteria." ></td>
	<td class="line x" title="141:215	The approach adopted is to use the defaults within the WEKA toolkit: Best First search with the CfsSubsetEval evaluator (Witten and Frank, 1999)." ></td>
	<td class="line x" title="142:215	Thus, a key question is whenif evera manual feature selection policy outperforms the automatic selection carried out under Level-V." ></td>
	<td class="line x" title="143:215	LevelsII and -III are of particular interest, since they contain features derived from a subset of the corpus." ></td>
	<td class="line x" title="144:215	Since different sub-groups are considered for each personality trait, the feature sets which meet the increasingly stringent criteria vary in size." ></td>
	<td class="line x" title="145:215	Table 3 contains the size of each of the four manuallydetermined feature sets for each of the four personality traits." ></td>
	<td class="line x" title="146:215	Note again that the number of ngrams selected from the most frequent in the cor1Conservatively rounded down in the case of an odd number of subjects." ></td>
	<td class="line x" title="147:215	I II III IV V N 747 747 169 22 19 E 701 701 167 11 20 A 823 823 237 36 34 C 704 704 197 22 25 Table 3: Number of n-grams per set." ></td>
	<td class="line x" title="148:215	Low High [was that] [this year] N [NP1 <p> NP1] [to eat] [<p> after] [slowly <p>] [is that] [and buy] [point in] [and he] E [last night <p>] [cool <p>] [it the] [<p> NP1] [is to] [to her] [thank god] [this is not] A [have any] [<p> it is] [have to] [<p> after] [turn up] [not have] [a few weeks] [by the way] C [case <p>] [<p> i hope] [okay <p>] [how i] [the game] [kind of] Table 4: Examples of significant Low and High n-grams from the Level-IV set." ></td>
	<td class="line x" title="149:215	pus for Level-I matches the size of the set for Level-II." ></td>
	<td class="line x" title="150:215	In addition, the features automatically selected are task-dependent, so the Level-V sets vary in size; here, the Table shows the number of features selected for Task-2." ></td>
	<td class="line x" title="151:215	To illustrate the types of n-grams in the feature sets, Table 4 contains four of the most significant n-grams from Level-IV for each personality class." ></td>
	<td class="line x" title="152:215	5 Results For each of the 60 binary classification tasks (1 to 3), the performance of the two approaches was compared." ></td>
	<td class="line x" title="153:215	Nave Bayes outperformed Support Vector Machines on 41/60, with 14 wins for SVM and 5 draws." ></td>
	<td class="line x" title="154:215	With limited space available, we therefore discuss only the results for NB, and use NB for Task-4 to -7." ></td>
	<td class="line x" title="155:215	The results for the binary tasks are displayed in Table 5." ></td>
	<td class="line x" title="156:215	Those for the multiple tasks are displayed in Table 6." ></td>
	<td class="line x" title="157:215	Baseline is the majority classification." ></td>
	<td class="line x" title="158:215	The most accurate performance of a feature set for each task is highlighted 631 Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V N1 52.0 52.0 92.0 84.0 96.0 92.0 N2 53.2 51.1 63.8 68.1 83.6 85.1 N3 54.9 54.9 60.6 53.5 71.9 83.1 E1 52.2 56.5 91.3 95.7 87.0 100.0 E2 51.1 44.7 74.5 72.3 66.0 93.6 E3 54.9 50.7 53.5 59.2 64.8 85.9 A1 54.2 62.5 100.0 100.0 95.8 100.0 A2 51.2 60.5 81.4 79.1 72.1 97.7 A3 52.1 53.5 60.6 69.0 66.2 93.0 C1 56.0 52.0 100.0 100.0 84.0 92.0 C2 61.2 54.5 77.3 81.8 72.7 93.2 C3 57.7 54.9 63.4 71.8 70.4 84.5 Table 5: Nave Bayes performance on binary tasks." ></td>
	<td class="line x" title="159:215	Raw % accuracy for 4 personality dimensions, 3 tasks, and 5 feature selection policies." ></td>
	<td class="line x" title="160:215	in bold while the second most accurate is marked italic." ></td>
	<td class="line x" title="161:215	6 Discussion Let us consider the results as they bear in turn on the three main questions posed earlier: Can we improve on Argamon et al.s (2005) performance on binary classification for the Extraversion and Neuroticism dimensions?" ></td>
	<td class="line x" title="162:215	How accurately can we classify on the four personality dimensions?" ></td>
	<td class="line x" title="163:215	And how does performance on multiple classification compare with that on binary classification?" ></td>
	<td class="line x" title="164:215	Before addressing these questions, we note the relatively good performance of NB compared with vanilla SVM on the binary classification tasks." ></td>
	<td class="line x" title="165:215	We also note that automatic selection generally outperforms manual selection; however overfitting is very likely when examining just 71 data points." ></td>
	<td class="line x" title="166:215	Therefore, we do not discuss the Level-V results further." ></td>
	<td class="line x" title="167:215	6.1 Extraversion and Neuroticism The first main question relates to the feature sets chosen, because the main issue is whether word ngrams can give reasonable results on the Extraversion and Neuroticism classification tasks." ></td>
	<td class="line x" title="168:215	Of the current binary classification tasks, Task-2 is most closely comparable to Argamon et al.s. Here, the best performance for Extraversion was returned by the manual Level-II feature set, closely followed by Level-III." ></td>
	<td class="line x" title="169:215	The accuracy of 74.5% represents a 23.4% absolute improvement over baseline Task Base Lv.I Lv.II Lv.III Lv.IV Lv.V N4 49.0 49.0 81.6 65.3 77.6 85.7 N5 64.8 60.6 76.1 67.6 67.6 94.4 N6 35.2 31.0 47.9 46.5 66.2 70.4 N7 33.8 31.0 49.3 38.0 42.3 47.9 E4 51.1 44.7 74.5 59.6 53.2 78.7 E5 67.6 60.6 83.1 67.6 54.9 90.1 E6 33.8 23.9 53.5 46.5 46.5 56.3 E7 33.8 44.7 39.4 29.6 38.0 40.8 A4 53.8 51.9 90.4 78.8 67.3 80.8 A5 66.2 59.2 83.1 84.5 74.6 80.3 A6 39.4 31.0 67.6 60.6 56.3 85.9 A7 39.4 33.8 69.8 60.6 50.7 47.9 C4 51.9 53.8 92.3 65.4 67.3 82.7 C5 64.8 62.0 74.6 69.0 62.0 83.1 C6 38.0 39.4 59.2 59.2 50.7 78.9 C7 38.0 36.6 62.0 45.1 45.1 49.3 Table 6: Nave Bayes performance on multiple tasks." ></td>
	<td class="line x" title="170:215	Raw % accuracy for 4 personality dimensions, 4 tasks, and 5 feature selection policies." ></td>
	<td class="line x" title="171:215	(45.8% relative improvement; we report relative improvement over baseline because baseline accuracies vary between tasks)." ></td>
	<td class="line x" title="172:215	The best performance for Neuroticism was returned by Level-IV." ></td>
	<td class="line x" title="173:215	The accuracy of 83.6% represents a 30.4% absolute improvement over baseline (57.1% relative improvement)." ></td>
	<td class="line x" title="174:215	Argamon et al.s feature set combined insights from computational stylometrics (Koppel et al. , 2002; Argamon et al. , 2003) and systemicfunctional grammar." ></td>
	<td class="line x" title="175:215	Their focus on function words and appraisal-related features was intended to provide more general and informative features than the usual n-grams." ></td>
	<td class="line x" title="176:215	Now, it is unlikely that weblogs are easier to categorise than the genres studied by Argamon et al. So there are instead at least two reasons for the improvement we report." ></td>
	<td class="line x" title="177:215	First, although we did not use systemicfunctional linguistic features, we did test n-grams selected according to more or less strict policies." ></td>
	<td class="line x" title="178:215	So, considering the manual policies, it seems that the Level-IV was the best-performing set for Neuroticism." ></td>
	<td class="line x" title="179:215	This might be expected, given that Level-IV potentially overfits, allowing features to be derived from the full corpus." ></td>
	<td class="line x" title="180:215	However, in spite of this, Level-II pproved best for Extraversion." ></td>
	<td class="line x" title="181:215	Secondly, in classifying an individual as high or low on some dimension, Argamon et al. had 632 (for some of their materials) 500 words from that individual, whereas we had approximately 5000 words." ></td>
	<td class="line x" title="182:215	The availability of more words per individual is to likely to help greatly in training." ></td>
	<td class="line x" title="183:215	Additionally, a greater volume of text increases the chances that a long term property such as personality will emerge 6.2 Binary classification of all dimensions The second question concerns the relative ease of classifying the different dimensions." ></td>
	<td class="line x" title="184:215	Across each of Task-1 to -3, we find that classification accuracies for Agreeableness and Conscientiousness tend to be higher than those for Extraversion and Neuroticism." ></td>
	<td class="line x" title="185:215	In all but two cases, the automatically generated feature set (V) performs best." ></td>
	<td class="line x" title="186:215	Putting this to one side, of the manually constructed sets, the unrestricted set (I) performs worst, often below the baseline, while Level-IV is the best for classifying each task of Neuroticism." ></td>
	<td class="line x" title="187:215	Overall, II and III are better than IV, although the difference is not large." ></td>
	<td class="line x" title="188:215	As tasks increase in difficultyas high and low groups become closer together, and the left-out middle shrinksperformance drops." ></td>
	<td class="line x" title="189:215	But accuracy is still respectable." ></td>
	<td class="line x" title="190:215	6.3 Beyond binary classification The final question is about how classification accuracy suffers as the classification task becomes more subtle." ></td>
	<td class="line x" title="191:215	As expected, we find that as we add more categories, the tasks are harder: compare the results in the Tables for Task-1, -5 and -7." ></td>
	<td class="line x" title="192:215	And, as with the binary tasks, if fewer mid-scoring individuals are left out, the task is typically harder: compare results for Task-4 and 5." ></td>
	<td class="line x" title="193:215	It does seem that some personality dimensions respond to task difficulty more robustly than others." ></td>
	<td class="line x" title="194:215	For instance, on the hardest task, the best Extraversion classification accuracy is 10.9% absolute over the baseline (32.2% relative), while the best Agreeableness accuracy is 30.4% absolute over the baseline (77.2% relative)." ></td>
	<td class="line x" title="195:215	It is notable that the feature set which return the best resultsbar the automatic set V tends to be Level-II, excepting for Neuroticism on Task-6, where Level-IV considerably outperforms the other sets." ></td>
	<td class="line x" title="196:215	A supplementary question is how the best classifiers compare with human performance on this task." ></td>
	<td class="line x" title="197:215	Mishne (2005) reports that, for general mood classification on weblogs, the accuracy of his automatic classifier is comparable to human performance." ></td>
	<td class="line x" title="198:215	There are also general results on human personality classification performance in computer-mediated communication, which suggest that at least some dimensions can be accurately judged even when computer-mediated." ></td>
	<td class="line x" title="199:215	Vazire and Gosling (2004) report that for personal websites, relative accuracy of judgment was, in descending order: Openness > Extraversion > Neuroticism > Agreeableness > Conscientiousness." ></td>
	<td class="line x" title="200:215	Similarly, Gill et al.(2006) report that for personal e-mail, Extraversion is more accurately judged than Neuroticism." ></td>
	<td class="line x" title="202:215	The current study does not have a set of human judgments to report." ></td>
	<td class="line x" title="203:215	For now, it is interesting to note that the performance profile for the best classifiers, on the simplest tasks, appears to diverge from the general human profile, instead ranking on raw accuracy: Agreeableness > Conscientiousness > Neuroticism > Extraversion." ></td>
	<td class="line x" title="204:215	7 Conclusion and next steps This paper has reported the first stages of our investigations into classification of author personality from weblog text." ></td>
	<td class="line x" title="205:215	Results are quite promising, and comparable across all four personality traits." ></td>
	<td class="line x" title="206:215	It seems that even a small selection of features found to exhibit an empirical relationship with personality traits can be used to generate reasonably accurate classification results." ></td>
	<td class="line x" title="207:215	Naturally, there are still many paths to explore." ></td>
	<td class="line x" title="208:215	Simple regression analyses are reported in Nowson (2006); however, for classification, a more thorough comparison of different machine learning methodologies is required." ></td>
	<td class="line x" title="209:215	A richer set of features besides n-grams should be checked, and we should not ignore the potential effectiveness of unigrams in this task (Pang et al. , 2002)." ></td>
	<td class="line x" title="210:215	A completely new test set can be gathered, so as to further guard against overfitting, and to explore systematically the effects of the amount of training data available for each author." ></td>
	<td class="line x" title="211:215	And as just discussed, comparison with human personality classification accuracy is potentially very interesting." ></td>
	<td class="line x" title="212:215	However, it does seem that we are making progress towards being able to deal with a realistic task: if we spot a thumbs-up review in a weblog, we should be able to check other text in that weblog, and tell whose thumb it is; or more accurately, what kind of persons thumb it is, anyway." ></td>
	<td class="line x" title="213:215	And that in turn should help tell us how high the thumb is really being held." ></td>
	<td class="line x" title="214:215	633 8 Acknowledgements We are grateful for the helpful advice of Mirella Lapata, and our three anonymous reviewers." ></td>
	<td class="line x" title="215:215	The second author was supported by a studentship from the Economic and Social Research Council." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0301
Extracting Opinions, Opinion Holders, And Topics Expressed In Online News Media Text
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:178	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 18, Sydney, July 2006." ></td>
	<td class="line x" title="2:178	c2006 Association for Computational Linguistics Extracting Opinions, Opinion Holders, and Topics Expressed in Online News Media Text Soo-Min Kim and Eduard Hovy USC Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 {skim, hovy}@ISI.EDU Abstract This paper presents a method for identifying an opinion with its holder and topic, given a sentence from online news media texts." ></td>
	<td class="line x" title="3:178	We introduce an approach of exploiting the semantic structure of a sentence, anchored to an opinion bearing verb or adjective." ></td>
	<td class="line x" title="4:178	This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using data from FrameNet." ></td>
	<td class="line x" title="5:178	We decompose our task into three phases: identifying an opinion-bearing word, labeling semantic roles related to the word in the sentence, and then finding the holder and the topic of the opinion word among the labeled semantic roles." ></td>
	<td class="line x" title="6:178	For a broader coverage, we also employ a clustering technique to predict the most probable frame for a word which is not defined in FrameNet." ></td>
	<td class="line x" title="7:178	Our experimental results show that our system performs significantly better than the baseline." ></td>
	<td class="line x" title="8:178	1 Introduction The challenge of automatically identifying opinions in text automatically has been the focus of attention in recent years in many different domains such as news articles and product reviews." ></td>
	<td class="line x" title="9:178	Various approaches have been adopted in subjectivity detection, semantic orientation detection, review classification and review mining." ></td>
	<td class="line x" title="10:178	Despite the successes in identifying opinion expressions and subjective words/phrases, there has been less achievement on the factors closely related to subjectivity and polarity, such as opinion holder, topic of opinion, and inter-topic/inter-opinion relationships." ></td>
	<td class="line x" title="11:178	This paper addresses the problem of identifying not only opinions in text but also holders and topics of opinions from online news articles." ></td>
	<td class="line x" title="12:178	Identifying opinion holders is important especially in news articles." ></td>
	<td class="line x" title="13:178	Unlike product reviews in which most opinions expressed in a review are likely to be opinions of the author of the review, news articles contain different opinions of different opinion holders (e.g. people, organizations, and countries)." ></td>
	<td class="line x" title="14:178	By grouping opinion holders of different stance on diverse social and political issues, we can have a better understanding of the relationships among countries or among organizations." ></td>
	<td class="line x" title="15:178	An opinion topic can be considered as an object an opinion is about." ></td>
	<td class="line x" title="16:178	In product reviews, for example, opinion topics are often the product itself or its specific features, such as design and quality (e.g. I like the design of iPod video, The sound quality is amazing)." ></td>
	<td class="line x" title="17:178	In news articles, opinion topics can be social issues, governments acts, new events, or someones opinions." ></td>
	<td class="line x" title="18:178	(e.g., Democrats in Congress accused vice president Dick Cheneys shooting accident., Shiite leaders accused Sunnis of a mass killing of Shiites in Madaen, south of Baghdad.) As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews." ></td>
	<td class="line x" title="19:178	In most approaches in product review mining, given a product (e.g. mp3 player), its frequently mentioned features (e.g. sound, screen, and design) are first collected and then used as anchor points." ></td>
	<td class="line x" title="20:178	In this study, we extract opinion topics from news articles." ></td>
	<td class="line x" title="21:178	Also, we do not pre-limit topics in advance." ></td>
	<td class="line x" title="22:178	We first identify an opinion and then find its holder and topic." ></td>
	<td class="line x" title="23:178	We define holder as an entity who holds an opinion, and topic, as what the opinion is about." ></td>
	<td class="line x" title="24:178	In this paper, we propose a novel method that employs Semantic Role Labeling, a task of identifying semantic roles given a sentence." ></td>
	<td class="line x" title="25:178	We de1 compose the overall task into the following steps:  Identify opinions." ></td>
	<td class="line x" title="26:178	 Label semantic roles related to the opinions." ></td>
	<td class="line x" title="27:178	 Find holders and topics of opinions among the identified semantic roles." ></td>
	<td class="line x" title="28:178	 Store <opinion, holder, topic> triples into a database." ></td>
	<td class="line x" title="29:178	In this paper, we focus on the first three subtasks." ></td>
	<td class="line x" title="30:178	The main contribution of this paper is to present a method that identifies not only opinion holders but also opinion topics." ></td>
	<td class="line x" title="31:178	To achieve this goal, we utilize FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics, and then use them for system training." ></td>
	<td class="line x" title="32:178	We demonstrate that investigating semantic relations between an opinion and its holder and topic is crucial in opinion holder and topic identification." ></td>
	<td class="line x" title="33:178	This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling." ></td>
	<td class="line x" title="34:178	Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet 1 data for our task." ></td>
	<td class="line x" title="35:178	Section 4 reports our experiments and results with discussions and finally Section 5 concludes." ></td>
	<td class="line x" title="36:178	2 Related Work This section reviews previous works in both sentiment detection and semantic role labeling." ></td>
	<td class="line x" title="37:178	2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al. , 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al. , 2003)." ></td>
	<td class="line oc" title="38:178	Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al. , 2005), or documents (Pang et al. , 2002; Turney, 2002)." ></td>
	<td class="line o" title="39:178	Building on this work, more sophisticated problems such as opinion holder identification have also been studied." ></td>
	<td class="line x" title="40:178	(Bethard et al. , 2004) identify opinion propositions and holders." ></td>
	<td class="line x" title="41:178	Their 1 http://framenet.icsi.berkeley.edu/ work is similar to ours but different because their opinion is restricted to propositional opinion and mostly to verbs." ></td>
	<td class="line x" title="42:178	Another related works are (Choi et al. , 2005; Kim and Hovy, 2005)." ></td>
	<td class="line x" title="43:178	Both of them use the MPQA corpus 2 but they only identify opinion holders, not topics." ></td>
	<td class="line x" title="44:178	As for opinion topic identification, little research has been conducted, and only in a very limited domain, product reviews." ></td>
	<td class="line x" title="45:178	(Hu and Liu, 2004; Popescu and Etzioni, 2005) present product mining algorithms with extracting certain product features given specific product types." ></td>
	<td class="line x" title="46:178	Our paper aims at extracting topics of opinion in general news media text." ></td>
	<td class="line x" title="47:178	2.2 Semantic Role Labeling Semantic role labeling is the task of identifying semantic roles such as Agent, Patient, Speaker, or Topic, in a sentence." ></td>
	<td class="line x" title="48:178	A statistical approach for semantic role labeling was introduced by (Gildea and Jurafsky, 2002)." ></td>
	<td class="line x" title="49:178	Their system learned semantic relationship among constituents in a sentence from FrameNet, a large corpus of semantically hand-annotated data." ></td>
	<td class="line x" title="50:178	The FrameNet annotation scheme is based on Frame Semantics (Fillmore, 1976)." ></td>
	<td class="line x" title="51:178	Frames are defined as schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles. For example, given a sentence Jack built a new house out of bricks, a semantic role labeling system should identify the roles for the verb built such as [ Agent Jack] built [ Created_entity a new house] [ Component out of bricks] 3." ></td>
	<td class="line x" title="52:178	In our study, we build a semantic role labeling system as an intermediate step to label opinion holders and topics by training it on opinion-bearing frames and their frame elements in FrameNet." ></td>
	<td class="line x" title="53:178	3 Finding Opinions and Their Holders and Topics For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data." ></td>
	<td class="line x" title="54:178	The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence." ></td>
	<td class="line x" title="55:178	Given a sentence and an opinion bearing word, our method identifies frame elements in the sentence and 2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 3 The verb build is defined under the frame Building in which Agent, Created_entity, and Components are defined as frame elements." ></td>
	<td class="line x" title="56:178	2 searches which frame element corresponds to the opinion holder and which to the topic." ></td>
	<td class="line x" title="57:178	The example in Figure 1 shows the intuition of our algorithm." ></td>
	<td class="line x" title="58:178	We decompose our task in 3 subtasks: (1) collect opinion words and opinion-related frames, (2) semantic role labeling for those frames, and (3) finally map semantic roles to holder and topic." ></td>
	<td class="line x" title="59:178	Following subsections describe each subtask." ></td>
	<td class="line x" title="60:178	3.1 Opinion Words and Related Frames We describe the subtask of collecting opinion words and related frames in 3 phases." ></td>
	<td class="line x" title="61:178	Phase 1: Collect Opinion Words In this study, we consider an opinion-bearing (positive/negative) word is a key indicator of an opinion." ></td>
	<td class="line x" title="62:178	Therefore, we first identify opinionbearing word from a given sentence and extract its holder and topic." ></td>
	<td class="line x" title="63:178	Since previous studies indicate that opinion-bearing verbs and adjectives are especially efficient for opinion identification, we focus on creating a set of opinion-bearing verbs and adjectives." ></td>
	<td class="line x" title="64:178	We annotated 1860 adjectives and 2011 verbs 4 by classifying them into positive, negative, and neutral classes." ></td>
	<td class="line x" title="65:178	Words in the positive class carry positive valence whereas 4 These were randomly selected from 8011 English verbs and 19748 English adjectives." ></td>
	<td class="line x" title="66:178	those in negative class carry negative valence." ></td>
	<td class="line x" title="67:178	Words that are not opinion-bearing are classified as neutral." ></td>
	<td class="line x" title="68:178	Note that in our study we treat word sentiment classification as a three-way classification problem instead of a two-way classification problem (i.e. positive and negative)." ></td>
	<td class="line x" title="69:178	By adding the third class, neutral, we can prevent the classifier assigning either positive or negative sentiment to weak opinion-bearing word." ></td>
	<td class="line x" title="70:178	For example, the word central that Hatzivassiloglou and McKeown (1997) marked as a positive adjective is not classified as positive by our system." ></td>
	<td class="line x" title="71:178	Instead we mark it as neutral, since it is a weak clue for an opinion." ></td>
	<td class="line x" title="72:178	For the same reason, we did not consider able classified as a positive word by General Inquirer 5, a sentiment word lexicon, as a positive opinion indicator." ></td>
	<td class="line x" title="73:178	Finally, we collected 69 positive and 151 negative verbs and 199 positive and 304 negative adjectives." ></td>
	<td class="line x" title="74:178	Phase 2: Find Opinion-related Frames We collected frames related to opinion words from the FrameNet corpus." ></td>
	<td class="line x" title="75:178	We used FrameNet II (Baker et al. , 2003) which contains 450 semantic frames and more than 3000 frame elements (FE)." ></td>
	<td class="line x" title="76:178	A frame consists of lexical items, called Lexical Unit (LU), and related frame elements." ></td>
	<td class="line x" title="77:178	For instance, LUs in ATTACK frame are verbs such as assail, assault, and attack, and nouns such as invasion, raid, and strike." ></td>
	<td class="line x" title="78:178	FrameNet II contains 5 http://www.wjh.harvard.edu/~inquirer/homecat.htm Table 1: Example of opinion related frames and lexical units Frame name Lexical units Frame elements Desiring want, wish, hope, eager, desire, interested, Event, Experiencer, Location_of_event Emotion _directed agitated, amused, anguish, ashamed, angry, annoyed, Event, Topic Experiencer, Expressor, Mental _property absurd, brilliant, careless, crazy, cunning, foolish Behavior, Protagonist, Domain, Degree Subject _stimulus delightful, amazing, annoying, amusing, aggravating, Stimulus, Degree Experiencer, Circumstances, Figure 1: An overview of our algorithm 3 approximately 7500 lexical units and over 100,000 annotated sentences." ></td>
	<td class="line x" title="79:178	For each word in our opinion word set described in Phase 1, we find a frame to which the word belongs." ></td>
	<td class="line x" title="80:178	49 frames for verbs and 43 frames for adjectives are collected." ></td>
	<td class="line x" title="81:178	Table 1 shows examples of selected frames with some of the lexical units those frames cover." ></td>
	<td class="line x" title="82:178	For example, our system found the frame Desiring from opinionbearing words want, wish, hope, etc. Finally, we collected 8256 and 11877 sentences related to selected opinion bearing frames for verbs and adjectives respectively." ></td>
	<td class="line x" title="83:178	Phase 3: FrameNet expansion Even though Phase 2 searches for a correlated frame for each verb and adjective in our opinionbearing word list, not all of them are defined in FrameNet data." ></td>
	<td class="line x" title="84:178	Some words such as criticize and harass in our list have associated frames (Case 1), whereas others such as vilify and maltreat do not have those (Case 2)." ></td>
	<td class="line x" title="85:178	For a word in Case 2, we use a clustering algorithms CBC (Clustering By Committee) to predict the closest (most reasonable) frame of undefined word from existing frames." ></td>
	<td class="line x" title="86:178	CBC (Pantel and Lin, 2002) was developed based on the distributional hypothesis (Harris, 1954) that words which occur in the same contexts tend to be similar." ></td>
	<td class="line x" title="87:178	Using CBC, for example, our clustering module computes lexical similarity between the word vilify in Case 2 and all words in Case 1." ></td>
	<td class="line x" title="88:178	Then it picks criticize as a similar word, so that we can use for vilify the frame Judgment_communication to which criticize belongs and all frame elements defined under Judgment_ communication." ></td>
	<td class="line x" title="89:178	3.2 Semantic Role Labeling To find a potential holder and topic of an opinion word in a sentence, we first label semantic roles in a sentence." ></td>
	<td class="line x" title="90:178	Modeling: We follow the statistical approaches for semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et." ></td>
	<td class="line x" title="91:178	al, 2003) which separate the task into two steps: identify candidates of frame elements (Step 1) and assign semantic roles for those candidates (Step 2)." ></td>
	<td class="line x" title="92:178	Like their intuition, we treated both steps as classification problems." ></td>
	<td class="line x" title="93:178	We first collected all constituents of the given sentence by parsing it using the Charniak parser." ></td>
	<td class="line x" title="94:178	Then, in Step 1, we classified candidate constituents of frame elements from non-candidates." ></td>
	<td class="line x" title="95:178	In Step 2, each selected candidate was thus classified into one of frame element types (e.g. Stimulus, Degree, Experiencer, etc.)." ></td>
	<td class="line x" title="96:178	As a learning algorithm for our classification model, we used Maximum Entropy (Berger et al. , 1996)." ></td>
	<td class="line x" title="97:178	For system development, we used MEGA model optimization package 6, an implementation of ME models." ></td>
	<td class="line x" title="98:178	Data: We collected 8256 and 11877 sentences which were associated to opinion bearing frames for verbs and adjectives from FrameNet annotation data." ></td>
	<td class="line x" title="99:178	Each sentence in our dataset contained a frame name, a target predicate (a word whose meaning represents aspects of the frame), and frame elements labeled with element types." ></td>
	<td class="line x" title="100:178	We divided the data into 90% for training and 10% for test." ></td>
	<td class="line x" title="101:178	Features used: Table 2 describes features that we used for our classification model." ></td>
	<td class="line x" title="102:178	The target word is an opinion-bearing verb or adjective which is associated to a frame." ></td>
	<td class="line x" title="103:178	We used the Charniak parser to get a phrase type feature of a frame element and the parse tree path feature." ></td>
	<td class="line x" title="104:178	We determined a head word of a phrase by an algorithm using a tree head table 7, position feature by the order of surface words of a frame element and the target word, and the voice feature by a simple pattern." ></td>
	<td class="line x" title="105:178	Frame name for a target 6 http://www.isi.edu/~hdaume/megam/index.html 7 http://people.csail.mit.edu/mcollins/papers/heads Table 2: Features used for our semantic role labeling model." ></td>
	<td class="line x" title="106:178	Feature Description target word A predicate whose meaning represents the frame (a verb or an adjective in our task) phrase type Syntactic type of the frame element (e.g. NP, PP) head word Syntactic head of the frame element phrase parse tree path A path between the frame element and target word in the parse tree position Whether the element phrase occurs before or after the target word voice The voice of the sentence (active or passive) frame name one of our opinion-related frames 4 word was selected by methods described in Phase 2 and Phase 3 in Subsection 3.1." ></td>
	<td class="line x" title="107:178	3.3 Map Semantic Roles to Holder and Topic After identifying frame elements in a sentence, our system finally selects holder and topic from those frame elements." ></td>
	<td class="line x" title="108:178	In the example in Table 1, the frame Desiring has frame elements such as Event (The change that the Experiencer would like to see), Experiencer (the person or sentient being who wishes for the Event to occur), Location_of_event (the place involved in the desired Event), Focal_participant (entity that the Experiencer wishes to be affected by some Event)." ></td>
	<td class="line x" title="109:178	Among these FEs, we can consider that Experiencer can be a holder and Focal_participant can be a topic (if any exists in a sentence)." ></td>
	<td class="line x" title="110:178	We manually built a mapping table to map FEs to holder or topic using as support the FE definitions in each opinion related frame and the annotated sample sentences." ></td>
	<td class="line x" title="111:178	4 Experimental Results The goal of our experiment is first, to see how our holder and topic labeling system works on the FrameNet data, and second, to examine how it performs on online news media text." ></td>
	<td class="line x" title="112:178	The first data set (Testset 1) consists of 10% of data described in Subsection 3.2 and the second (Testset 2) is manually annotated by 2 humans." ></td>
	<td class="line x" title="113:178	(see Subsection 4.2)." ></td>
	<td class="line x" title="114:178	We report experimental results for both test sets." ></td>
	<td class="line x" title="115:178	4.1 Experiments on Testset 1 Gold Standard: In total, Testset 1 contains 2028 annotated sentences collected from FrameNet data set." ></td>
	<td class="line x" title="116:178	(834 from frames related to opinion verb and 1194 from opinion adjectives) We measure the system performance using precision (the percentage of correct holders/topics among systems labeling results), recall (the percentage of correct holders/topics that system retrieved), and F-score." ></td>
	<td class="line x" title="117:178	Baseline: For the baseline system, we applied two different algorithms for sentences which have opinion-bearing verbs as target words and for those that have opinion-bearing adjectives as target words." ></td>
	<td class="line x" title="118:178	For verbs, baseline system labeled a subject of a verb as a holder and an object as a topic." ></td>
	<td class="line x" title="119:178	(e.g. [ holder He] condemned [ topic the lawyer].) For adjectives, the baseline marked the subject of a predicate adjective as a holder (e.g. [ holder I] was happy)." ></td>
	<td class="line x" title="120:178	For the topics of adjectives, the baseline picks a modified word if the target adjective is a modifier (e.g. That was a stupid [ topic mistake])." ></td>
	<td class="line x" title="121:178	and a subject word if the adjective is a predicate." ></td>
	<td class="line x" title="122:178	([ topic The view] is breathtaking in January)." ></td>
	<td class="line x" title="123:178	Result: Table 3 and 4 show evaluation results of our system and the baseline system respectively." ></td>
	<td class="line x" title="124:178	Our system performed much better than the baseline system in identifying topic and holder for both sets of sentences with verb target words and those with adjectives." ></td>
	<td class="line x" title="125:178	Especially in recognizing topics of target opinion-bearing words, our system improved F-score from 30.4% to 66.5% for verb target words and from 38.2% to 70.3% for adjectives." ></td>
	<td class="line x" title="126:178	It was interesting to see that the intuition that A subject of opinionbearing verb is a holder and an object is a topic which we applied for the baseline achieved relatively good F-score (56.9%)." ></td>
	<td class="line x" title="127:178	However, our system obtained much higher F-score (78.7%)." ></td>
	<td class="line x" title="128:178	Holder identification task achieved higher Fscore than topic identification which implies that identifying topics of opinion is a harder task." ></td>
	<td class="line x" title="129:178	We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics that simple relations such as subject and object relations are not able to capture." ></td>
	<td class="line x" title="130:178	For example, in a sentence Her letter upset me, simply looking for the subjective and objective of the verb upset is not enough to recognize the holder and topic." ></td>
	<td class="line x" title="131:178	It is necessary to see a deeper level of semantic relaTable 3." ></td>
	<td class="line x" title="132:178	Precision (P), Recall (R), and Fscore (F) of Topic and Holder identification for opinion verbs (V) and adjectives (A) on Testset 1." ></td>
	<td class="line x" title="133:178	Topic Holder P (%) R (%) F (%) P (%) R (%) F (%) V 69.1 64.0 66.5 81.9 75.7 78.7 A 67.5 73.4 70.3 66.2 77.9 71.6 Table 4." ></td>
	<td class="line x" title="134:178	Baseline system on Testset 1." ></td>
	<td class="line x" title="135:178	Topic Holder P (%) R (%) F (%) P (%) R (%) F (%) V 85.5 18.5 30.4 73.7 46.4 56.9 A 68.2 26.5 38.2 12.0 49.1 19.3 5 tions: Her letter is a stimulus and me is an experiencer of the verb upset." ></td>
	<td class="line x" title="136:178	4.2 Experiments on Testset 2 Gold Standard: Two humans 8 annotated 100 sentences randomly selected from news media texts." ></td>
	<td class="line x" title="137:178	Those news data is collected from online news sources such as The New York Times, UN Office for the Coordination of Humanitarian Affairs, and BBC News 9, which contain articles about various international affaires." ></td>
	<td class="line x" title="138:178	Annotators identified opinion-bearing sentences with marking opinion word with its holder and topic if they existed." ></td>
	<td class="line x" title="139:178	The inter-annotator agreement in identifying opinion sentences was 82%." ></td>
	<td class="line x" title="140:178	Baseline: In order to identify opinion-bearing sentences for our baseline system, we used the opinion-bearing word set introduced in Phase 1 in Subsection 3.1." ></td>
	<td class="line x" title="141:178	If a sentence contains an opinion-bearing verb or adjective, the baseline system started looking for its holder and topic." ></td>
	<td class="line x" title="142:178	For holder and topic identification, we applied the 8 We refer them as Human1 and Human2 for the rest of this paper." ></td>
	<td class="line x" title="143:178	9 www.nytimes.com, www.irinnews.org, and www.bbc.co.uk same baseline algorithm as described in Subsection 4.1 to Testset 2." ></td>
	<td class="line x" title="144:178	Result: Note that Testset 1 was collected from sentences of opinion-related frames in FrameNet and therefore all sentences in the set contained either opinion-bearing verb or adjective." ></td>
	<td class="line x" title="145:178	(i.e. All sentences are opinion-bearing) However, sentences in Testset 2 were randomly collected from online news media pages and therefore not all of them are opinion-bearing." ></td>
	<td class="line x" title="146:178	We first evaluated the task of opinion-bearing sentence identification." ></td>
	<td class="line x" title="147:178	Table 5 shows the system results." ></td>
	<td class="line x" title="148:178	When we mark all sentences as opinion-bearing, it achieved 43% and 38% of accuracy for the annotation result of Human1 and Human2 respectively." ></td>
	<td class="line x" title="149:178	Our system performance (64% and 55%) is comparable with the unique assignment." ></td>
	<td class="line x" title="150:178	We measured the holder and topic identification system with precision, recall, and F-score." ></td>
	<td class="line x" title="151:178	As we can see from Table 6, our system achieved much higher precision than the baseline system for both Topic and Holder identification tasks." ></td>
	<td class="line x" title="152:178	However, we admit that there is still a lot of room for improvement." ></td>
	<td class="line x" title="153:178	The system achieved higher precision for topic identification, whereas it achieved higher recall for holder identification." ></td>
	<td class="line x" title="154:178	In overall, our system attained higher F-score in holder identification task, including the baseline system." ></td>
	<td class="line x" title="155:178	Based on Fscore, we believe that identifying topics of opinion is much more difficult than identifying holders." ></td>
	<td class="line x" title="156:178	It was interesting to see the same phenomenon that the baseline system mainly assuming that subject and object of a sentence are likely to be opinion holder and topic, achieved lower scores for both holder and topic identification tasks in Testset 2 as in Testset 1." ></td>
	<td class="line x" title="157:178	This implies that more sophisticated analysis of the relationship between opinion words (e.g. verbs and adjectives) and their topics and holders is crucial." ></td>
	<td class="line x" title="158:178	4.3 Difficulties in evaluation We observed several difficulties in evaluating holder and topic identification." ></td>
	<td class="line x" title="159:178	First, the boundary of an entity of holder or topic can be flexible." ></td>
	<td class="line x" title="160:178	For example, in sentence Senator Titus Olupitan who sponsored the bill wants the permission., not only Senator Titus Olupitan but also Senator Titus Olupitan who sponsored the bill is an eligible answer." ></td>
	<td class="line x" title="161:178	Second, some correct holders and topics which our system found were evaluated wrong even if they referred the same entities in the gold standard because human annotators marked only one of them as an answer." ></td>
	<td class="line x" title="162:178	Table 5." ></td>
	<td class="line x" title="163:178	Opinion-bearing sentence identification on Testset 2." ></td>
	<td class="line x" title="164:178	(P: precision, R: recall, F: F-score, A: Accuracy, H1: Human1, H2: Human2) P (%) R (%) F (%) A (%) H1 56.9 67.4 61.7 64.0 H2 43.1 57.9 49.4 55.0 Table 6: Results of Topic and Holder identification on Testset 2." ></td>
	<td class="line x" title="165:178	(Sys: our system, BL: baseline) Topic Holder P(%) R(%) F(%) P(%) R(%) F(%) H1 64.7 20.8 31.5 47.9 34.0 39.8 Sys H2 58.8 7.1 12.7 36.6 26.2 30.5 H1 12.5 9.4 10.7 20.0 28.3 23.4 BL H2 23.2 7.1 10.9 14.0 19.0 16.1 6 In the future, we need more annotated data for improved evaluation." ></td>
	<td class="line x" title="166:178	5 Conclusion and Future Work This paper presented a methodology to identify an opinion with its holder and topic given a sentence in online news media texts." ></td>
	<td class="line x" title="167:178	We introduced an approach of exploiting semantic structure of a sentence, anchored to an opinion bearing verb or adjective." ></td>
	<td class="line x" title="168:178	This method uses semantic role labeling as an intermediate step to label an opinion holder and topic using FrameNet data." ></td>
	<td class="line x" title="169:178	Our method first identifies an opinion-bearing word, labels semantic roles related to the word in the sentence, and then finds a holder and a topic of the opinion word among labeled semantic roles." ></td>
	<td class="line x" title="170:178	There has been little previous study in identifying opinion holders and topics partly because it requires a great amount of annotated data." ></td>
	<td class="line x" title="171:178	To overcome this barrier, we utilized FrameNet data by mapping target words to opinion-bearing words and mapping semantic roles to holders and topics." ></td>
	<td class="line x" title="172:178	However, FrameNet has a limited number of words in its annotated corpus." ></td>
	<td class="line x" title="173:178	For a broader coverage, we used a clustering technique to predict a most probable frame for an unseen word." ></td>
	<td class="line x" title="174:178	Our experimental results showed that our system performs significantly better than the baseline." ></td>
	<td class="line x" title="175:178	The baseline system results imply that opinion holder and topic identification is a hard task." ></td>
	<td class="line x" title="176:178	We believe that there are many complicated semantic relations between opinion-bearing words and their holders and topics which simple relations such as subject and object relations are not able to capture." ></td>
	<td class="line x" title="177:178	In the future, we plan to extend our list of opinion-bearing verbs and adjectives so that we can discover and apply more opinion-related frames." ></td>
	<td class="line x" title="178:178	Also, it would be interesting to see how other types of part of speech such as adverbs and nouns affect the performance of the system." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0302
Toward Opinion Summarization: Linking The Sources
Stoyanov, Veselin;Cardie, Claire;"></td>
	<td class="line x" title="1:190	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 914, Sydney, July 2006." ></td>
	<td class="line x" title="2:190	c2006 Association for Computational Linguistics Toward Opinion Summarization: Linking the Sources Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14850, USA {ves,cardie}@cs.cornell.edu Abstract We target the problem of linking source mentions that belong to the same entity (source coreference resolution), which is needed for creating opinion summaries." ></td>
	<td class="line x" title="3:190	In this paper we describe how source coreference resolution can be transformed into standard noun phrase coreference resolution, apply a state-of-the-art coreference resolution approach to the transformed data, and evaluate on an available corpus of manually annotated opinions." ></td>
	<td class="line x" title="4:190	1 Introduction Sentiment analysis is concerned with the extraction and representation of attitudes, evaluations, opinions, and sentiment from text." ></td>
	<td class="line x" title="5:190	The area of sentiment analysis has been the subject of much recent research interest driven by two primary motivations." ></td>
	<td class="line x" title="6:190	First, there is a desire to provide applications that can extract, represent, and allow the exploration of opinions in the commercial, government, and political domains." ></td>
	<td class="line x" title="7:190	Second, effective sentiment analysis might be used to enhance and improve existing NLP applications such as information extraction, question answering, summarization, and clustering (e.g. Riloff et al.(2005), Stoyanov et al.(2005))." ></td>
	<td class="line x" title="10:190	Several research efforts (e.g. Riloff and Wiebe (2003), Bethard et al.(2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005)) have shown that sentiment information can be extracted at the sentence, clause, or individualopinionexpressionlevel(fine-grainedopinion information)." ></td>
	<td class="line x" title="13:190	However, little has been done to develop methods for combining fine-grained opinion information to form a summary representation in which expressions of opinions from the same source/target1 are grouped together, multiple opinions from a source toward the same target are accumulated into an aggregated opinion, and cumulative statistics are computed for each source/target." ></td>
	<td class="line x" title="14:190	A simple opinion summary2 is shown in Figure 1." ></td>
	<td class="line x" title="15:190	Being able to create opinion summaries is important both for stand-alone applications of sentiment analysis as well as for the potentialusesofsentimentanalysisaspartofother NLP applications." ></td>
	<td class="line x" title="16:190	In this work we address the dearth of approaches for summarizing opinion information." ></td>
	<td class="line x" title="17:190	In particular, we focus on the problem of source coreference resolution, i.e. deciding which source mentions are associated with opinions that belong to the same real-world entity." ></td>
	<td class="line x" title="18:190	In the example from Figure 1 performing source coreference resolution amounts to determining that Stanishev, he, and he refer to the same real-world entities." ></td>
	<td class="line x" title="19:190	Given the associated opinion expressions and their polarity, this source coreference information is the critical knowledgeneededtoproducethesummaryofFigure 1 (although the two target mentions, Bulgaria and our country, would also need to be identified as coreferent)." ></td>
	<td class="line x" title="20:190	Our work is concerned with fine-grained expressions of opinions and assumes that a system can rely on the results of effective opinion and source extractors such as those described in Riloff and Wiebe (2003), Bethard et al.(2004), Wiebe andRiloff(2005)andChoietal.(2005)." ></td>
	<td class="line x" title="22:190	Presented with sources of opinions, we approach the problem of source coreference resolution as the closely 1We use source to denote an opinion holder and target to denote the entity toward which the opinion is directed." ></td>
	<td class="line x" title="23:190	2For simplicity, the example summary does not contain any source/target statistics or combination of multiple opinions from the same source to the same target." ></td>
	<td class="line x" title="24:190	9  [Target Delaying of Bulgarias accession to the EU] would be a serious mistake [Source Bulgarian Prime Minister Sergey Stanishev] said in an interview for the German daily Suddeutsche Zeitung." ></td>
	<td class="line x" title="25:190	[Target Our country] serves as a model and encourages countries from the region to follow despite the difficulties, [Source he] added." ></td>
	<td class="line x" title="26:190	[Target Bulgaria] is criticized by [Source the EU] because of slow reforms in the judiciary branch, the newspaper notes." ></td>
	<td class="line x" title="27:190	Stanishev was elected prime minister in 2005." ></td>
	<td class="line x" title="28:190	Since then, [Source he] has been a prominent supporter of [Target his countrys accession to the EU]." ></td>
	<td class="line x" title="29:190	Stanishev Accession EU Bulgaria Delaying +   + Figure 1: Example of text containing opinions (above) and a summary of the opinions (below)." ></td>
	<td class="line x" title="30:190	In the text, sources and targets of opinions are marked and opinion expressions are shown in italic." ></td>
	<td class="line x" title="31:190	In the summary graph, + stands for positive opinion and for negative." ></td>
	<td class="line x" title="32:190	related task of noun phrase coreference resolution." ></td>
	<td class="line x" title="33:190	However, source coreference resolution differsfromtraditional nounphrase(NP)coreference resolution in two important aspects discussed in Section4." ></td>
	<td class="line x" title="34:190	Nevertheless, asafirstattemptatsource coreference resolution, we employ a state-of-theart machine learning approach to NP coreference resolution developed by Ng and Cardie (2002)." ></td>
	<td class="line x" title="35:190	Using a corpus of manually annotated opinions, we perform an extensive evaluation and obtain strong initial results for the task of source coreference resolution." ></td>
	<td class="line x" title="36:190	2 Related Work Sentiment analysis has been a subject of much recent research." ></td>
	<td class="line x" title="37:190	Several efforts have attempted to automatically extract opinions, emotions, and sentiment from text." ></td>
	<td class="line oc" title="38:190	The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document eitherpositive(thumbsup)ornegative(thumbs down) polarity (e.g. Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003), Pang and Lee (2004))." ></td>
	<td class="line x" title="41:190	In contrast, the problem of fine-grained opinion extraction has concentrated on recognizing opinions at the sentence, clause, or individual opinion expression level." ></td>
	<td class="line x" title="42:190	Recent work has shown that systems can be trained to recognize opinions, their polarity, and their strength at a reasonable degree of accuracy (e.g. Dave et al.(2003), Riloff and Wiebe (2003), Bethard et al.(2004), Pang and Lee (2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="46:190	Additionally, researchers have been able to effectively identify sources of opinions automatically (Bethard et al. , 2004; Choi et al. , 2005; Kim and Hovy, 2005)." ></td>
	<td class="line x" title="47:190	Finally, Liu et al.(2005) summarize automatically generated opinions about products and develop interface that allows the summaries to be vizualized." ></td>
	<td class="line x" title="49:190	Our work also draws on previous work in the area of coreference resolution, which is a relatively well studied NLP problem." ></td>
	<td class="line x" title="50:190	Coreference resolution is the problem of deciding what noun phrases in the text (i.e. mentions) refer to the same real-world entities (i.e. are coreferent)." ></td>
	<td class="line x" title="51:190	Generally, successful approaches have relied machine learning methods trained on a corpus of documents annotated with coreference information (such as the MUC and ACE corpora)." ></td>
	<td class="line x" title="52:190	Our approach to source coreference resolution is inspired by the state-of-the-art performance of the method of Ng and Cardie (2002)." ></td>
	<td class="line x" title="53:190	3 Data set We begin our discussion by describing the data set that we use for development and evaluation." ></td>
	<td class="line x" title="54:190	As noted previously, we desire methods that work with automatically identified opinions and sources." ></td>
	<td class="line x" title="55:190	However, for the purpose of developing and evaluating our approaches we rely on a corpus ofmanuallyannotatedopinionsandsources." ></td>
	<td class="line x" title="56:190	More precisely, we rely on the MPQA corpus (Wilson and Wiebe, 2003)3, which contains 535 manually annotated documents." ></td>
	<td class="line x" title="57:190	Full details about the corpus and the process of corpus creation can be found in Wilson and Wiebe (2003); full details of the opinion annotation scheme can be found in Wiebe et al.(2005)." ></td>
	<td class="line x" title="59:190	For the purposes of the discussion in this paper, the following three points suffice." ></td>
	<td class="line x" title="60:190	First, the corpus is suitable for the domains and genres that we target  all documents have occurred in the world press over an 11-month period, between June 2001 and May 2002." ></td>
	<td class="line x" title="61:190	Therefore, the 3The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="62:190	10 corpus is suitable for the political and government domains as well as a substantial part of the commercial domain." ></td>
	<td class="line x" title="63:190	However, a fair portion of the commercial domain is concerned with opinion extraction from product reviews." ></td>
	<td class="line x" title="64:190	Work described in this paper does not target the genre of reviews, which appears to differ significantly from newspaper articles." ></td>
	<td class="line x" title="65:190	Second, all documents are manually annotated with phrase-level opinion information." ></td>
	<td class="line x" title="66:190	The annotation scheme of Wiebe et al.(2005) includes phrase level opinions, their sources, as well as other attributes, which are not utilized by our approach." ></td>
	<td class="line x" title="68:190	Additionally, the annotations contain information that allows coreference among source mentions to be recovered." ></td>
	<td class="line x" title="69:190	Finally, the MPQA corpus contains no coreference information for general NPs (which are not sources)." ></td>
	<td class="line x" title="70:190	This might present a problem for traditional coreference resolution approaches, as discussed throughout the paper." ></td>
	<td class="line x" title="71:190	4 Source Coreference Resolution In this Section we define the problem of source coreference resolution, describe its challenges, and provide an overview of our general approach." ></td>
	<td class="line x" title="72:190	We define source coreference resolution as the problem of determining which mentions of opinion sources refer to the same real-world entity." ></td>
	<td class="line x" title="73:190	Source coreference resolution differs from traditional supervised NP coreference resolution in two important aspects." ></td>
	<td class="line x" title="74:190	First, sources of opinions do not exactly correspond to the automatic extractors notion of noun phrases (NPs)." ></td>
	<td class="line x" title="75:190	Second, due mainly to the time-consuming nature of coreference annotation, NP coreference information is incomplete in our data set: NP mentions that are not sources of opinion are not annotated with coreference information (even when they are part of a chain that contains source NPs)4." ></td>
	<td class="line x" title="76:190	In this paper we address the former problem via a heuristic method for mapping sources to NPs and give statistics for the accuracy of the mapping process." ></td>
	<td class="line x" title="77:190	We then apply state-of-the-art coreference resolution methods to the NPs to which sources were 4This problem is illustrated in the example of Figure 1 The underlined Stanishev is coreferent with all of the Stanishev references marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, thereferencewouldbeomittedfromtheStanishevsource coreference chain." ></td>
	<td class="line x" title="78:190	Unfortunately, this proper noun might be critical in establishing coreference of the final source reference he with the other mentions of the source Stanishev." ></td>
	<td class="line x" title="79:190	Single Match Multiple Matches No Match Total 7811 3461 50 Exact 6242 1303 0 Table 1: Statistics for matching sources to noun phrases." ></td>
	<td class="line x" title="80:190	mapped (source noun phrases)." ></td>
	<td class="line x" title="81:190	The latter problem of developing methods that can work with incomplete supervisory information is addressed in a subsequent effort (Stoyanov and Cardie, 2006)." ></td>
	<td class="line x" title="82:190	Our general approach to source coreference resolution consists of the following steps: 1." ></td>
	<td class="line x" title="83:190	Preprocessing: We preprocess the corpus by running NLP components such as a tokenizer, sentence splitter, POS tagger, parser, and a base NP finder." ></td>
	<td class="line x" title="84:190	Subsequently, we augment the set of the base NPs found by the base NP finder with the help of a named entity finder." ></td>
	<td class="line x" title="85:190	The preprocessing is done following the NP coreference work by Ng and Cardie (2002)." ></td>
	<td class="line x" title="86:190	From the preprocessing step, we obtain an augmented set of NPs in the text." ></td>
	<td class="line x" title="87:190	2." ></td>
	<td class="line x" title="88:190	Source to noun phrase mapping: The problem of mapping (manually or automatically annotated) sources to NPs is not trivial." ></td>
	<td class="line x" title="89:190	We map sources to NPs using a set of heuristics." ></td>
	<td class="line x" title="90:190	3." ></td>
	<td class="line x" title="91:190	Coreference resolution: Finally, we restrict our attention to the source NPs identified in step 2." ></td>
	<td class="line x" title="92:190	We extract a feature vector for every pair of source NPs from the preprocessed corpus and perform NP coreference resolution." ></td>
	<td class="line x" title="93:190	The next two sections give the details of Steps 2 and 3, respectively." ></td>
	<td class="line x" title="94:190	We follow with the results of an evaluation of our approach in Section 7." ></td>
	<td class="line x" title="95:190	5 Mapping sources to noun phrases Thissectiondescribesourmethodforheuristically mapping sources to NPs." ></td>
	<td class="line x" title="96:190	In the context of source coreference resolution we consider a noun phrase to correspond to (or match) a source if the source and the NP cover the exact same span of text." ></td>
	<td class="line x" title="97:190	Unfortunately, the annotated sources did not always match exactly a single automatically extracted NP." ></td>
	<td class="line x" title="98:190	We discovered the following problems: 1." ></td>
	<td class="line x" title="99:190	Inexact span match." ></td>
	<td class="line x" title="100:190	We discovered that often (in 3777 out of the 11322 source mentions) there is no noun phrase whose span matches exactly the source although there are noun phrases that overlap the source." ></td>
	<td class="line x" title="101:190	In most cases this is due to the way spans of sources are marked in the data." ></td>
	<td class="line x" title="102:190	For instance, in some cases determiners are not included in the source span (e.g. Venezuelan people vs. the Venezuelan people)." ></td>
	<td class="line x" title="103:190	In other cases, differences are due to mistakes by the NP extractor (e.g. Muslims rulers was not recognized, while Muslims and rulers were recognized)." ></td>
	<td class="line x" title="104:190	Yet in other cases, manually marked sources do not match the definition of a noun phrase." ></td>
	<td class="line x" title="105:190	This case is described in more detail next." ></td>
	<td class="line x" title="106:190	11 Measure Overall Method and Instance B3 MUC Positive Identification Actual Pos." ></td>
	<td class="line x" title="107:190	Identification rank parameters selection score Prec." ></td>
	<td class="line x" title="108:190	Recall F1 Prec." ></td>
	<td class="line x" title="109:190	Recall F1 B3 1 svm C10 0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2 400 5 ripper asc L2 soon2 80.7 72.2 74.5 45.2 56.3 55.1 62.1 58.4 Training MUC Score 1 svm C10 0.01 soon1 77.3 74.2 67.4 51.7 58.5 37.8 70.9 49.3 Documents 4 ripper acs L1.5 soon2 78.4 73.6 68.3 49.0 57.0 40.0 69.9 50.9 Positive 1 svm C10 0.05 soon1 72.7 73.9 60.0 57.2 58.6 37.8 71.0 49.3 identification 4 ripper acs L1.5 soon1 78.9 73.6 68.8 48.9 57.2 40.0 69.9 50.9 Actual pos." ></td>
	<td class="line x" title="110:190	1 svm C10 0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2 identification 2 ripper asc L4 soon2 73.9 69.9 81.1 40.2 53.9 69.8 52.5 60.0 B3 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6 9 svm C10 0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9 200 MUC Score 1 svm C1 0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9 Training 5 ripper acs L1 soon1 77.9 0.732 71.4 46.5 56.3 37.7 69.7 48.9 Documents Positive 1 svm C1 0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9 identification 4 ripper acs L1 soon1 75.3 72.4 69.1 48.0 56.7 33.3 72.3 45.6 Actual pos." ></td>
	<td class="line x" title="111:190	1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6 identification 10 svm C10 0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9 Table 2: Performance of the best runs." ></td>
	<td class="line x" title="112:190	For SVMs,  stands for RBF kernel with the shown  parameter." ></td>
	<td class="line x" title="113:190	2." ></td>
	<td class="line x" title="114:190	Multiple NP match." ></td>
	<td class="line x" title="115:190	For 3461 of the 11322 source mentions more than one NP overlaps the source." ></td>
	<td class="line x" title="116:190	In roughly a quarter of these cases the multiple match is due to the presence of nested NPs (introduced by the NP augmentation process introduced in Section 3)." ></td>
	<td class="line x" title="117:190	In other cases the multiple match is caused by source annotations that spanned multiple NPs or included more than only NPs inside its span." ></td>
	<td class="line x" title="118:190	There are three general classes of such sources." ></td>
	<td class="line x" title="119:190	First, some of the marked sourcesareappositivessuchasthecountrysnewpresident, Eduardo Duhalde." ></td>
	<td class="line x" title="120:190	Second, some sources containanNPfollowedbyanattachedprepositionalphrase suchasLatinAmericanleadersatasummitmeetingin Costa Rica." ></td>
	<td class="line x" title="121:190	Third, some sources are conjunctions of NPs such as Britain, Canada and Australia." ></td>
	<td class="line x" title="122:190	Treatment of the latter is still a controversial problem in the context of coreference resolution as it is unclear whether conjunctions represent entities that are distinct fromtheconjuncts." ></td>
	<td class="line x" title="123:190	Forthepurposeofourcurrentwork we do not attempt to address conjunctions." ></td>
	<td class="line x" title="124:190	3." ></td>
	<td class="line x" title="125:190	No matching NP." ></td>
	<td class="line x" title="126:190	Finally, for 50 of the 11322 sources there are no overlapping NPs." ></td>
	<td class="line x" title="127:190	Half of those (25 to be exact) included marking of the word who such as in the sentence Carmona named new ministers, including two military officers who rebelled against Chavez." ></td>
	<td class="line x" title="128:190	From the other 25, 19 included markings of non-NPs including question words, qualifiers, and adjectives such as many, which, and domestically." ></td>
	<td class="line x" title="129:190	The remaining six are rare NPs such as lash and taskforce that are mistakenly not recognized by the NP extractor." ></td>
	<td class="line x" title="130:190	Counts for the different types of matches of sources to NPs are shown in Table 1." ></td>
	<td class="line x" title="131:190	We determine the match in the problematic cases using a set of heuristics: 1." ></td>
	<td class="line x" title="132:190	If a source matches any NP exactly in span, match that source to the NP; do this even if multiple NPs overlap the source  we are dealing with nested NPs. 2." ></td>
	<td class="line x" title="133:190	If no NP matches matches exactly in span then:  If a single NP overlaps the source, then map the sourcetothatNP.Mostlikelywearedealingwith differently marked spans." ></td>
	<td class="line x" title="134:190	 If multiple NPs overlap the source, determine whether the set of overlapping NPs include any non-nested NPs." ></td>
	<td class="line x" title="135:190	If all overlapping NPs are nested with each other, select the NP that is closer in span to the source  we are still dealing with differently marked spans, but now we also have nested NPs." ></td>
	<td class="line x" title="136:190	If there is more than one set of nested NPs, then most likely the source spans more than a single NP." ></td>
	<td class="line x" title="137:190	In this case we select the outermost of the last set of nested NPs before any preposition in the span." ></td>
	<td class="line x" title="138:190	We prefer: the outermost NP because longer NPs contain more information; thelastNPbecauseitislikelytobethehead NP of a phrase (also handles the case of explanation followed by a proper noun); NPs before preposition, because a preposition signals an explanatory prepositional phrase." ></td>
	<td class="line x" title="139:190	3." ></td>
	<td class="line x" title="140:190	If no NP overlaps the source, select the last NP before the source." ></td>
	<td class="line x" title="141:190	In half of the cases we are dealing with the word who, which typically refers to the last preceding NP." ></td>
	<td class="line x" title="142:190	6 Source coreference resolution as coreference resolution Once we isolate the source NPs, we apply coreference resolution using the standard combination of classification and single-link clustering (e.g. Soon et al.(2001) and Ng and Cardie (2002))." ></td>
	<td class="line x" title="144:190	We compute a vector of 57 features for every pair of source noun phrases from the preprocessed corpus." ></td>
	<td class="line x" title="145:190	We use the training set of pairwise instances to train a classifier to predict whether a source NP pair should be classified as positive (the NPs refer to the same entity) or negative (different entities)." ></td>
	<td class="line x" title="146:190	During testing, we use the trained classifier to predict whether a source NP pair is positive and single-link clustering to group together sources that belong to the same entity." ></td>
	<td class="line x" title="147:190	7 Evaluation For evaluation we randomly split the MPQA corpus into a training set consisting of 400 documents 12 and a test set consisting of the remaining 135 documents." ></td>
	<td class="line x" title="148:190	We use the same test set for all evaluations, although not all runs were trained on all 400 training documents as discussed below." ></td>
	<td class="line x" title="149:190	The purpose of our evaluation is to create a strong baseline utilizing the best settings for the NP coreference approach." ></td>
	<td class="line x" title="150:190	As such, we try the two reportedly best machine learning techniques for pairwise classification  RIPPER (for Repeated Incremental Pruning to Produce Error Reduction) (Cohen, 1995) and support vector machines (SVMs) in the SVMlight implementation (Joachims, 1998)." ></td>
	<td class="line x" title="151:190	Additionally, to exclude possible effects of parameter selection, we try many different parameter settings for the two classifiers." ></td>
	<td class="line x" title="152:190	For RIPPER we vary the order of classes and the positive/negative weight ratio." ></td>
	<td class="line x" title="153:190	For SVMs we vary C (themargintradeoff)andthetypeandparameter of the kernel." ></td>
	<td class="line x" title="154:190	In total, we use 24 different settings for RIPPER and 56 for SVMlight." ></td>
	<td class="line x" title="155:190	Additionally, Ng and Cardie reported better results when the training data distribution is balanced through instance selection." ></td>
	<td class="line x" title="156:190	For instance selection they adopt the method of Soon et al.(2001), which selects for each NP the pairs with the n preceding coreferent instances and all intervening non-coreferent pairs." ></td>
	<td class="line x" title="158:190	Following Ng and Cardie (2002), we perform instance selection with n = 1 (soon1 in the results) and n = 2 (soon2)." ></td>
	<td class="line x" title="159:190	With the three different instance selection algorithms (soon1,soon2, and none), the total number of settings is 72 for RIPPER and 168 for SVMa." ></td>
	<td class="line x" title="160:190	However, not all SVM runs completed in the time limit that we set  200 min, so we selected half of the training set (200 documents) at random and trained all classifiers on that set." ></td>
	<td class="line x" title="161:190	We made sure to run to completion on the full training set those SVM settings that produced the best results on the smaller training set." ></td>
	<td class="line x" title="162:190	Table 2 lists the results of the best performing runs." ></td>
	<td class="line x" title="163:190	The upper half of the table gives the results for the runs that were trained on 400 documents and the lower half contains the results for the 200-document training set." ></td>
	<td class="line x" title="164:190	We evaluated using the two widely used performance measures for coreference resolution  MUC score (Vilain et al. , 1995) and B3 (Bagga and Baldwin, 1998)." ></td>
	<td class="line x" title="165:190	In addition, we used performance metrics (precision, recall and F1) on the identification of the positive class." ></td>
	<td class="line x" title="166:190	We compute the latter in two different ways  either by using the pairwise decisions as the classifiers outputs them or by performing the clustering of the source NPs and then considering a pairwise decision to be positive if the two source NPs belong to the same cluster." ></td>
	<td class="line x" title="167:190	The second option (marked actual in Table 2) should be more representative of a good clustering, since coreference decisions are important only in the context of the clusters that they create." ></td>
	<td class="line x" title="168:190	Table 2 shows the performance of the best RIPPER and SVM runs for each of the four evaluation metrics." ></td>
	<td class="line x" title="169:190	The table also lists the rank for each run among the rest of the runs." ></td>
	<td class="line x" title="170:190	7.1 Discussion The absolute B3 and MUC scores for source coreference resolution are comparable to reported state-of-the-art results for NP coreference resolutions." ></td>
	<td class="line x" title="171:190	Results should be interpreted cautiously, however, due to the different characteristics of our data." ></td>
	<td class="line x" title="172:190	Our documents contained 35.34 source NPs per document on average, with coreference chains consisting of only 2.77 NPs on average." ></td>
	<td class="line x" title="173:190	The low average number of NPs per chain may be producing artificially high score for the B3 and MUC scores as the modest results on positive class identification indicate." ></td>
	<td class="line x" title="174:190	From the relative performance of our runs, we observe the following trends." ></td>
	<td class="line x" title="175:190	First, SVMs trained on the full training set outperform RIPPER trained on the same training set as well as the corresponding SVMs trained on the 200-document training set." ></td>
	<td class="line x" title="176:190	The RIPPER runs exhibit the opposite behavior  RIPPER outperforms SVMs on the 200document training set and RIPPER runs trained on the smaller data set exhibit better performance." ></td>
	<td class="line x" title="177:190	Overall, the single best performance is observed by RIPPER using the smaller training set." ></td>
	<td class="line x" title="178:190	Another interesting observation is that the B3 measure correlates well with good actual performance on positive class identification." ></td>
	<td class="line x" title="179:190	In contrast, good MUC performance is associated with runs that exhibit high recall on the positive class." ></td>
	<td class="line x" title="180:190	This confirms some theoretical concerns that MUC score does not reward algorithms that recognize well the absence of links." ></td>
	<td class="line x" title="181:190	In addition, the results confirm our conjecture that actual precision and recall are more indicative of the true performance of coreference algorithms." ></td>
	<td class="line x" title="182:190	13 8 Conclusions As a first step toward opinion summarization we targeted the problem of source coreference resolution." ></td>
	<td class="line x" title="183:190	We showed that the problem can be tackled effectively as noun coreference resolution." ></td>
	<td class="line x" title="184:190	Oneaspectof sourcecoreferenceresolutionthat we do not address is the use of unsupervised information." ></td>
	<td class="line x" title="185:190	The corpus contains many automatically identified non-source NPs, which can be used to benefit source coreference resolution in two ways." ></td>
	<td class="line x" title="186:190	First, a machine learning approach could use the unlabeleddatatoestimatetheoveralldistributions." ></td>
	<td class="line x" title="187:190	Second, some links between sources may be realized through a non-source NPs (see the example of figure 1)." ></td>
	<td class="line x" title="188:190	As a follow-up to the work described in this paper we developed a method that utilizes the unlabeled NPs in the corpus using a structured rule learner (Stoyanov and Cardie, 2006)." ></td>
	<td class="line x" title="189:190	Acknowledgements The authors would like to thank Vincent Ng and Art Munson for providing coreference resolution code, members of the Cornell NLP group (especially Yejin Choi and Art Munson) for many helpful discussions, and the anonymous reviewers for their insightful comments." ></td>
	<td class="line x" title="190:190	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS-0535099 and IIS-0208028, by gifts from Google and the Xerox Foundation, and by an NSF Graduate Research Fellowship to the first author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0305
Annotating Attribution In The Penn Discourse TreeBank
Prasad, Rashmi;Dinesh, Nikhil;Lee, Alan;Joshi, Aravind K.;Webber, Bonnie Lynn;"></td>
	<td class="line x" title="1:177	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 3138, Sydney, July 2006." ></td>
	<td class="line x" title="2:177	c2006 Association for Computational Linguistics Annotating Attribution in the Penn Discourse TreeBank Rashmi Prasad and Nikhil Dinesh and Alan Lee and Aravind Joshi University of Pennsylvania Philadelphia, PA 19104 USA a0 rjprasad,nikhild,aleewk,joshi a1 @linc.cis.upenn.edu Bonnie Webber University of Edinburgh Edinburgh, EH8 9LW Scotland bonnie@inf.ed.ac.uk Abstract An emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source." ></td>
	<td class="line x" title="3:177	Corpus annotation schemes aim to encode such distinctions for NLP applications concerned with such tasks, such as information extraction, question answering, summarization, and generation." ></td>
	<td class="line x" title="4:177	We describe an annotation scheme for marking the attribution of abstract objects such as propositions, facts and eventualities associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank." ></td>
	<td class="line x" title="5:177	The scheme aims to capture the source and degrees of factuality of the abstract objects." ></td>
	<td class="line x" title="6:177	Key aspects of the scheme are annotation of the text spans signalling the attribution, and annotation of features recording the source, type, scopal polarity, and determinacy of attribution." ></td>
	<td class="line x" title="7:177	1 Introduction News articles typically contain a mixture of information presented from several different perspectives, and often in complex ways." ></td>
	<td class="line x" title="8:177	Writers may present information as known to them, or from some other individuals perspective, while further distinguishing between, for example, whether that perspective involves an assertion or a belief." ></td>
	<td class="line x" title="9:177	Recent work has shown the importance of recognizing such perspectivization of information for several NLP applications, such as information extraction, summarization, question answering (Wiebe et al. , 2004; Stoyanov et al. , 2005; Riloff et al. , 2005) and generation (Prasad et al. , 2005)." ></td>
	<td class="line x" title="10:177	Part of the goal of such applications is to distinguish between factual and non-factual information, and to identify the source of the information." ></td>
	<td class="line x" title="11:177	Annotation schemes (Wiebe et al. , 2005; Wilson and Wiebe, 2005; PDTB-Group, 2006) encode such distinctions to facilitate accurate recognition and representation of such perspectivization of information." ></td>
	<td class="line x" title="12:177	This paper describes an extended annotation scheme for marking the attribution of discourse relations and their arguments annotated in the Penn Discourse TreeBank (PDTB) (Miltsakaki et al. , 2004; Prasad et al. , 2004; Webber et al. , 2005), the primary goal being to capture the source and degrees of factuality of abstract objects." ></td>
	<td class="line x" title="13:177	The scheme captures four salient properties of attribution: (a) source, distinguishing between different types of agents to whom AOs are attributed, (b) type, reflecting the degree of factuality of the AO, (c) scopal polarity of attribution, indicating polarity reversals of attributed AOs due to surface negated attributions, and (d) determinacy of attribution, indicating the presence of contexts canceling the entailment of attribution." ></td>
	<td class="line x" title="14:177	The scheme also describes annotation of the text spans signaling the attribution." ></td>
	<td class="line x" title="15:177	The proposed scheme is an extension of the core scheme used for annotating attribution in the first release of the PDTB (Dinesh et al. , 2005; PDTB-Group, 2006)." ></td>
	<td class="line x" title="16:177	Section 2 gives an overview of the PDTB, Section 3 presents the extended annotation scheme for attribution, and Section 4 presents the summary." ></td>
	<td class="line x" title="17:177	2 The Penn Discourse TreeBank (PDTB) The PDTB contains annotations of discourse relations and their arguments on the Wall Street Journal corpus (Marcus et al. , 1993)." ></td>
	<td class="line x" title="18:177	Following the approach towards discourse structure in (Webber et al. , 2003), the PDTB takes a lexicalized ap31 proacha2 towards the annotation of discourse relations, treating discourse connectives as the anchors of the relations, and thus as discourse-level predicates taking two abstract objects (AOs) as their arguments." ></td>
	<td class="line x" title="19:177	For example, in (1), the subordinating conjunction since is a discourse connective that anchors a TEMPORAL relation between the event of the earthquake hitting and a state where no music is played by a certain woman." ></td>
	<td class="line x" title="20:177	(The 4digit number in parentheses at the end of examples gives the WSJ file number of the example.)" ></td>
	<td class="line x" title="21:177	(1) She hasnt played any music since the earthquake hit." ></td>
	<td class="line x" title="22:177	(0766) There are primarily two types of connectives in the PDTB: Explicit and Implicit." ></td>
	<td class="line x" title="23:177	Explicit connectives are identified form four grammatical classes: subordinating conjunctions (e.g. , because, when, only because, particularly since), subordinators (e.g. , in order that), coordinating conjunctions (e.g. , and, or), and discourse adverbials (e.g. , however, otherwise)." ></td>
	<td class="line x" title="24:177	In the examples in this paper, Explicit connectives are underlined." ></td>
	<td class="line x" title="25:177	For sentences not related by an Explicit connective, annotators attempt to infer a discourse relation between them by inserting connectives (called Implicit connectives) that best convey the inferred relations." ></td>
	<td class="line x" title="26:177	For example, in (2), the inferred CAUSAL relation between the two sentences was annotated with because as the Implicit connective." ></td>
	<td class="line x" title="27:177	Implicit connectives together with their sense classification are shown here in small caps." ></td>
	<td class="line x" title="28:177	(2) Also unlike Mr. Ruder, Mr. Breeden appears to be in a position to get somewhere with his agenda." ></td>
	<td class="line x" title="29:177	Implicit=BECAUSE (CAUSE) As a former White House aide who worked closely with Congress, he is savvy in the ways of Washington." ></td>
	<td class="line x" title="30:177	(0955) Cases where a suitable Implicit connective could not be annotated between adjacent sentences are annotated as either (a) EntRel, where the second sentence only serves to provide some further description of an entity in the first sentence (Example 3); (b) NoRel, where no discourse relation or entity-based relation can be inferred; and (c) AltLex, where the insertion of an Implicit connective leads to redundancy, due to the relation being alternatively lexicalized by some nonconnective expression (Example 4)." ></td>
	<td class="line x" title="31:177	(3) C.B. Rogers Jr. was named chief executive officer of this business information concern." ></td>
	<td class="line x" title="32:177	Implicit=EntRel Mr. Rogers, 60 years old, succeeds J.V. White, 64, who will remain chairman and chairman of the executive committee (0929)." ></td>
	<td class="line x" title="33:177	(4) One in 1981 raised to $2,000 a year from $1,500 the amount a person could put, tax-deductible, into the tax-deferred accounts and widened coverage to people under employer retirement plans." ></td>
	<td class="line x" title="34:177	Implicit=AltLex (consequence) [This caused] an explosion of IRA promotions by brokers, banks, mutual funds and others." ></td>
	<td class="line x" title="35:177	(0933) Arguments of connectives are simply labelled Arg2, for the argument appearing in the clause syntactically bound to the connective, and Arg1, for the other argument." ></td>
	<td class="line x" title="36:177	In the examples here, Arg1 appears in italics, while Arg2 appears in bold." ></td>
	<td class="line x" title="37:177	The basic unit for the realization of an AO argument of a connective is the clause, tensed or untensed, but it can also be associated with multiple clauses, within or across sentences." ></td>
	<td class="line x" title="38:177	Nominalizations and discourse deictics (this, that), which can also be interpreted as AOs, can serve as the argument of a connective too." ></td>
	<td class="line x" title="39:177	The current version of the PDTB also contains attribution annotations on discourse relations and their arguments." ></td>
	<td class="line x" title="40:177	These annotations, however, used the earlier core scheme which is subsumed in the extended scheme described in this paper." ></td>
	<td class="line x" title="41:177	The first release of the Penn Discourse TreeBank, PDTB-1.0 (reported in PDTBGroup (2006)), is freely available from http://www.seas.upenn.edu/pdtb." ></td>
	<td class="line x" title="42:177	PDTB-1.0 contains 100 distinct types of Explicit connectives, with a total of 18505 tokens, annotated across the entire WSJ corpus (25 sections)." ></td>
	<td class="line x" title="43:177	Implicit relations have been annotated in three sections (Sections 08, 09, and 10) for the first release, totalling 2003 tokens (1496 Implicit connectives, 19 AltLex relations, 435 EntRel tokens, and 53 NoRel tokens)." ></td>
	<td class="line x" title="44:177	The corpus also includes a broadly defined sense classification for the implicit relations, and attribution annotation with the earlier core scheme." ></td>
	<td class="line x" title="45:177	Subsequent releases of the PDTB will include Implicit relations annotated across the entire corpus, attribution annotation using the extended scheme proposed here, and fine-grained sense classification for both Explicit and Implicit connectives." ></td>
	<td class="line x" title="46:177	3 Annotation of Attribution Recent work (Wiebe et al. , 2005; Prasad et al. , 2005; Riloff et al. , 2005; Stoyanov et al. , 2005), has shown the importance of recognizing and representing the source and factuality of information in certain NLP applications." ></td>
	<td class="line x" title="47:177	Information extraction systems, for example, would perform better 32 bya3 prioritizing the presentation of factual information, and multi-perspective question answering systems would benefit from presenting information from different perspectives." ></td>
	<td class="line oc" title="48:177	Most of the annotation approaches tackling these issues, however, are aimed at performing classifications at either the document level (Pang et al. , 2002; Turney, 2002), or the sentence or word level (Wiebe et al. , 2004; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line o" title="49:177	In addition, these approaches focus primarily on sentiment classification, and use the same for getting at the classification of facts vs. opinions." ></td>
	<td class="line o" title="50:177	In contrast to these approaches, the focus here is on marking attribution on more analytic semantic units, namely the Abstract Objects (AOs) associated with predicate-argument discourse relations annotated in the PDTB, with the aim of providing a compositional classification of the factuality of AOs." ></td>
	<td class="line x" title="51:177	The scheme isolates four key properties of attribution, to be annotated as features: (1) source, which distinguishes between different types of agents (Section 3.1); (2) type, which encodes the nature of relationship between agents and AOs, reflecting the degree of factuality of the AO (Section 3.2); (3) scopal polarity, which is marked when surface negated attribution reverses the polarity of the attributed AO (Section 3.3), and (4) determinacy, which indicates the presence of contexts due to which the entailment of attribution gets cancelled (Section 3.4)." ></td>
	<td class="line x" title="52:177	In addition, to further facilitate the task of identifying attribution, the scheme also aims to annotate the text span complex signaling attribution (Section 3.5) Results from annotations using the earlier attribution scheme (PDTB-Group, 2006) show that a significant proportion (34%) of the annotated discourse relations have some non-Writer agent as the source for either the relation or one or both arguments." ></td>
	<td class="line x" title="53:177	This illustrates the simplest case of the ambiguity inherent for the factuality of AOs, and shows the potential use of the PDTB annotations towards the automatic classification of factuality." ></td>
	<td class="line x" title="54:177	The annotations also show that there are a variety of configurations in which the components of the relations are attributed to different sources, suggesting that recognition of attributions may be a complex task for which an annotated corpus may be useful." ></td>
	<td class="line x" title="55:177	For example, in some cases, a relation together with its arguments is attributed to the writer or some other agent, whereas in other cases, while the relation is attributed to the writer, one or both of its arguments is attributed to different agent(s)." ></td>
	<td class="line x" title="56:177	For Explicit connectives." ></td>
	<td class="line x" title="57:177	there were 6 unique configurations, for configurations containing more than 50 tokens, and 5 unique configurations for Implicit connectives." ></td>
	<td class="line x" title="58:177	3.1 Source The source feature distinguishes between (a) the writer of the text (Wr), (b) some specific agent introduced in the text (Ot for other), and (c) some generic source, i.e., some arbitrary (Arb) individual(s) indicated via a non-specific reference in the text." ></td>
	<td class="line x" title="59:177	The latter two capture further differences in the degree of factuality of AOs with nonwriter sources." ></td>
	<td class="line x" title="60:177	For example, an Arb source for some information conveys a higher degree of factuality than an Ot source, since it can be taken to be a generally accepted view." ></td>
	<td class="line x" title="61:177	Since arguments can get their attribution through the relation between them, they can be annotated with a fourth value Inh, to indicate that their source value is inherited from the relation." ></td>
	<td class="line x" title="62:177	Given this scheme for source, there are broadly two possibilities." ></td>
	<td class="line x" title="63:177	In the first case, a relation and both its arguments are attributed to the same source, either the writer, as in (5), or some other agent (here, Bill Biedermann), as in (6)." ></td>
	<td class="line x" title="64:177	(Attribution feature values assigned to examples are shown below each example; REL stands for the discourse relation denoted by the connective; Attribution text spans are shown boxed.)" ></td>
	<td class="line x" title="65:177	(5) Since the British auto maker became a takeover target last month, its ADRs have jumped about 78%." ></td>
	<td class="line x" title="66:177	(0048) REL Arg1 Arg2 [Source] Wr Inh Inh (6) The public is buying the market when in reality there is plenty of grain to be shipped, said Bill Biedermann a4a5a4a6a4 (0192) REL Arg1 Arg2 [Source] Ot Inh Inh As Example (5) shows, text spans for implicit Writer attributions (corresponding to implicit communicative acts such as I write, or I say), are not marked and are taken to imply Writer attribution by default (see also Section 3.5)." ></td>
	<td class="line x" title="67:177	In the second case, one or both arguments have a different source from the relation." ></td>
	<td class="line x" title="68:177	In (7), for example, the relation and Arg2 are attributed to the writer, whereas Arg1 is attributed to another agent (here, Mr. Green)." ></td>
	<td class="line x" title="69:177	On the other hand, in (8) and (9), the relation and Arg1 are attributed to the writer, whereas Arg2 is attributed to another agent." ></td>
	<td class="line x" title="70:177	33 (7) When Mr. Green won a $240,000 verdict in a land condemnation case against the state in June 1983, he says Judge OKicki unexpectedly awarded him an additional $100,000." ></td>
	<td class="line x" title="71:177	(0267) REL Arg1 Arg2 [Source] Wr Ot Inh (8) Factory orders and construction outlays were largely flat in December while purchasing agents said manufacturing shrank further in October." ></td>
	<td class="line x" title="72:177	(0178) REL Arg1 Arg2 [Source] Wr Inh Ot (9) There, on one of his first shopping trips, Mr. Paul picked up several paintings at stunning prices." ></td>
	<td class="line x" title="73:177	a4a5a4a5a4 Afterward, Mr. Paul is said by Mr. Guterman to have phoned Mr. Guterman, the New York developer selling the collection, and gloated." ></td>
	<td class="line x" title="74:177	(2113) REL Arg1 Arg2 [Source] Wr Inh Ot Example (10) shows an example of a generic source indicated by an agentless passivized attribution on Arg2 of the relation." ></td>
	<td class="line x" title="75:177	Note that passivized attributions can also be associated with a specific source when the agent is explicit, as shown in (9)." ></td>
	<td class="line x" title="76:177	Arb sources are also identified by the occurrences of adverbs like reportedly, allegedly, etc.(10) Although index arbitrage is said to add liquidity to markets, John Bachmann, a4a5a4a5a4 says too much liquidity isnt a good thing." ></td>
	<td class="line x" title="78:177	(0742) REL Arg1 Arg2 [Source] Wr Ot Arb We conclude this section by noting that Ot is used to refer to any specific individual as the source." ></td>
	<td class="line x" title="79:177	That is, no further annotation is provided to indicate who the Ot agent in the text is. Furthermore, as shown in Examples (11-12), multiple Ot sources within the same relation do not indicate whether or not they refer to the same or different agents." ></td>
	<td class="line x" title="80:177	However, we assume that the text span annotations for attribution, together with an independent mechanism for named entity recognition and anaphora resolution can be employed to identify and disambiguate the appropriate references." ></td>
	<td class="line x" title="81:177	(11) Suppression of the book, Judge Oakes observed, would operate as a prior restraint and thus involve the First Amendment." ></td>
	<td class="line x" title="82:177	Moreover, and here Judge Oakes went to the heart of the question, Responsible biographers and historians constantly use primary sources, letters, diaries, and memoranda." ></td>
	<td class="line x" title="83:177	(0944) REL Arg1 Arg2 [Source] Wr Ot Ot (12) The judge was considered imperious, abrasive and ambitious, those who practiced before him say." ></td>
	<td class="line x" title="84:177	Yet, despite the judges imperial bearing, no one ever had reason to suspect possible wrongdoing, says John Bognato, president of Cambria a4a6a4a5a4 .(0267) REL Arg1 Arg2 [Source] Wr Ot Ot 3.2 Type The type feature signifies the nature of the relation between the agent and the AO, leading to different inferences about the degree of factuality of the AO." ></td>
	<td class="line x" title="85:177	In order to capture the factuality of the AOs, we start by making a three-way distinction of AOs into propositions, facts and eventualities (Asher, 1993)." ></td>
	<td class="line x" title="86:177	This initial distinction allows for a more semantic, compositional approach to the annotation and recognition of factuality." ></td>
	<td class="line x" title="87:177	We define the attribution relations for each AO type as follows: (a) Propositions involve attribution to an agent of his/her (varying degrees of) commitment towards the truth of a proposition; (b) Facts involve attribution to an agent of an evaluation towards or knowledge of a proposition whose truth is taken for granted (i.e. , a presupposed proposition); and (c) Eventualities involve attribution to an agent of an intention/attitude towards an eventuality." ></td>
	<td class="line x" title="88:177	In the case of propositions, a further distinction is made to capture the difference in the degree of the agents commitment towards the truth of the proposition, by distinguishing between assertions and beliefs." ></td>
	<td class="line x" title="89:177	Thus, the scheme for the annotation of type ultimately uses a four-way distinction for AOs, namely between assertions, beliefs, facts, and eventualities." ></td>
	<td class="line x" title="90:177	Initial determination of the degree of factuality involves determination of the type of the AO." ></td>
	<td class="line x" title="91:177	AO types can be identified by well-defined semantic classes of verbs/phrases anchoring the attribution." ></td>
	<td class="line x" title="92:177	We consider each of these in turn." ></td>
	<td class="line x" title="93:177	Assertions are identified by assertive predicates or verbs of communication (Levin, 1993) such as say, mention, claim, argue, explain etc. They take the value Comm (for verbs of Communication)." ></td>
	<td class="line x" title="94:177	In Example (13), the Ot attribution on Arg1 takes the value Comm for type." ></td>
	<td class="line x" title="95:177	Implicit writer attributions, as in the relation of (13), also take (the default) Comm." ></td>
	<td class="line x" title="96:177	Note that when an arguments attribution source is not inherited (as in Arg1 in this example) it also takes its own independent value for type." ></td>
	<td class="line x" title="97:177	This example thus conveys that there are two different attributions expressed within the discourse relation, one for the relation and the other for one of its arguments, and that both involve assertion of propositions." ></td>
	<td class="line x" title="98:177	34 (13) When Mr. Green won a $240,000 verdict in a land condemnation case against the state in June 1983, he says Judge OKicki unexpectedly awarded him an additional $100,000." ></td>
	<td class="line x" title="99:177	(0267) REL Arg1 Arg2 [Source] Wr Ot Inh [Type] Comm Comm Null In the absence of an independent occurrence of attribution on an argument, as in Arg2 of Example (13), the Null value is used for the type on the argument, meaning that it needs to be derived by independent (here, undefined) considerations under the scope of the relation." ></td>
	<td class="line x" title="100:177	Note that unlike the Inh value of the source feature, Null does not indicate inheritance." ></td>
	<td class="line x" title="101:177	In a subordinate clause, for example, while the relation denoted by the subordinating conjunction may be asserted, the clause content itself may be presupposed, as seems to be the case for the relation and Arg2 of (13)." ></td>
	<td class="line x" title="102:177	However, we found these differences difficult to determine at times, and consequently leave this undefined in the current scheme." ></td>
	<td class="line x" title="103:177	Beliefs are identified by propositional attitude verbs (Hintikka, 1971) such as believe, think, expect, suppose, imagine, etc. They take the value PAtt (for Propostional Attitude)." ></td>
	<td class="line x" title="104:177	An example of a belief attribution is given in (14)." ></td>
	<td class="line x" title="105:177	(14) Mr. Marcus believes spot steel prices will continue to fall through early 1990 and then reverse themselves." ></td>
	<td class="line x" title="106:177	(0336) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null Facts are identified by the class of factive and semi-factive verbs (Kiparsky and Kiparsky, 1971; Karttunen, 1971) such as regret, forget, remember, know, see, hear etc. They take the value Ftv (for Factive) for type (Example 15)." ></td>
	<td class="line x" title="107:177	In the current scheme, this class does not distinguish between the true factives and semi-factives, the former involving an attitute/evaluation towards a fact, and the latter involving knowledge of a fact." ></td>
	<td class="line x" title="108:177	(15) The other side, he argues knows Giuliani has always been pro-choice, even though he has personal reservations." ></td>
	<td class="line x" title="109:177	(0041) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Ftv Null Null Lastly, eventualities are identified by a class of verbs which denote three kinds of relations between agents and eventualities (Sag and Pollard, 1991)." ></td>
	<td class="line x" title="110:177	The first kind is anchored by verbs of influence like persuade, permit, order, and involve one agent influencing another agent to perform (or not perform) an action." ></td>
	<td class="line x" title="111:177	The second kind is anchored by verbs of commitment like promise, agree, try, intend, refuse, decline, and involve an agent committing to perform (or not perform) an action." ></td>
	<td class="line x" title="112:177	Finally, the third kind is anchored by verbs of orientation like want, expect, wish, yearn, and involve desire, expectation, or some similar mental orientation towards some state(s) of affairs." ></td>
	<td class="line x" title="113:177	These sub-distinctions are not encoded in the annotation, but we have used the definitions as a guide for identifying these predicates." ></td>
	<td class="line x" title="114:177	All these three types are collectively referred to and annotated as verbs of control." ></td>
	<td class="line x" title="115:177	Type for these classes takes the value Ctrl (for Control)." ></td>
	<td class="line x" title="116:177	Note that the syntactic term control is used because these verbs denote uniform structural control properties, but the primary basis for their definition is nevertheless semantic." ></td>
	<td class="line x" title="117:177	An example of the control attribution relation anchored by a verb of influence is given in (16)." ></td>
	<td class="line x" title="118:177	(16) Eward and Whittington had planned to leave the bank earlier, but Mr. Craven had persuaded them to remain until the bank was in a healthy position." ></td>
	<td class="line x" title="119:177	(1949) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Ctrl Null Null Note that while our use of the term source applies literally to agents responsible for the truth of a proposition, we continue to use the same term for the agents for facts and eventualities." ></td>
	<td class="line x" title="120:177	Thus, for facts, the source represents the bearers of attitudes/knowledge, and for considered eventualities, the source represents intentions/attitudes." ></td>
	<td class="line x" title="121:177	3.3 Scopal Polarity The scopal polarity feature is annotated on relations and their arguments to primarily identify cases when verbs of attribution are negated on the surface syntactically (e.g. , didnt say, dont think) or lexically (e.g. , denied), but when the negation in fact reverses the polarity of the attributed relation or argument content (Horn, 1978)." ></td>
	<td class="line x" title="122:177	Example (17) illustrates such a case." ></td>
	<td class="line x" title="123:177	The but clause entails an interpretation such as I think its not a main consideration, for which the negation must take narrow scope over the embedded clause rather than the higher clause." ></td>
	<td class="line x" title="124:177	In particular, the interpretation of the CONTRAST relation denoted by but requires that Arg2 should be interpreted under the scope of negation." ></td>
	<td class="line x" title="125:177	35 (17) Having the dividend increases is a supportive element in the market outlook, but I dont think its a main consideration, he says." ></td>
	<td class="line x" title="126:177	(0090) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Comm Null PAtt [Polarity] Null Null Neg To capture such entailments with surface negations on attribution verbs, an argument of a connective is marked Neg for scopal polarity when the interpretation of the connective requires the surface negation to take semantic scope over the lower argument." ></td>
	<td class="line x" title="127:177	Thus, in Example (17), scopal polarity is marked as Neg for Arg2." ></td>
	<td class="line x" title="128:177	When the neg-lowered interpretations are not present, scopal polarity is marked as the default Null (such as for the relation and Arg1 of Example 17)." ></td>
	<td class="line x" title="129:177	It is also possible for the surface negation of attribution to be interpreted as taking scope over the relation, rather than an argument." ></td>
	<td class="line x" title="130:177	We have not observed this in the corpus yet, so we describe this case with the constructed example in (18)." ></td>
	<td class="line x" title="131:177	What the example shows is that in addition to entailing (18b) in which case it would be annotated parallel to Example (17) above (18a) can also entail (18c), such that the negation is intrepreted as taking semantic scope over the relation (Lasnik, 1975), rather than one of the arguments." ></td>
	<td class="line x" title="132:177	As the scopal polarity annotations for (18c) show, lowering of the surface negation to the relation is marked as Neg for the scopal polarity of the relation." ></td>
	<td class="line x" title="133:177	(18) a. John doesnt think Mary will get cured because she took the medication." ></td>
	<td class="line x" title="134:177	b. a7a8 John thinks that because Mary took the medication, she will not get cured." ></td>
	<td class="line x" title="135:177	REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null [Polarity] Null Neg Null c. a7a8 John thinks that Mary will get cured not because she took the medication (but because she has started practising yoga.)" ></td>
	<td class="line x" title="136:177	REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null [Polarity] Neg Null Null We note that scopal polarity does not capture the appearance of (opaque) internal negation that may appear on arguments or relations themselves." ></td>
	<td class="line x" title="137:177	For example, a modified connective such as not because does not take Neg as the value for scopal polarity, but rather Null." ></td>
	<td class="line x" title="138:177	This is consistent with our goal of marking scopal polarity only for lowered negation, i.e., when surface negation from the attribution is lowered to either the relation or argument for interpretation." ></td>
	<td class="line x" title="139:177	3.4 Determinacy The determinacy feature captures the fact that the entailment of the attribution relation can be made indeterminate in context, for example when it appears syntactically embedded in negated or conditional contexts The annotation attempts to capture such indeterminacy with the value Indet." ></td>
	<td class="line x" title="140:177	Determinate contexts are simply marked as the default Null." ></td>
	<td class="line x" title="141:177	For example, the annotation in (19) conveys the idea that the belief or opinion about the effect of higher salaries on teachers performance is not really attributed to anyone, but is rather only being conjectured as a possibility." ></td>
	<td class="line x" title="142:177	(19) It is silly libel on our teachers to think they would educate our children better if only they got a few thousand dollars a year more." ></td>
	<td class="line x" title="143:177	(1286) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] PAtt Null Null [Polarity] Null Null Null [Determinacy] Indet Null Null 3.5 Attribution Spans In addition to annotating the properties of attribution in terms of the features discussed above, we also propose to annotate the text span associated with the attribution." ></td>
	<td class="line x" title="144:177	The text span is annotated as a single (possibly discontinuous) complex reflecting three of the annotated features, namely source, type and scopal polarity." ></td>
	<td class="line x" title="145:177	The attribution span also includes all non-clausal modifiers of the elements contained in the span, for example, adverbs and appositive NPs." ></td>
	<td class="line x" title="146:177	Connectives, however, are excluded from the span, even though they function as modifiers." ></td>
	<td class="line x" title="147:177	Example (20) shows a discontinuous annotation of the attribution, where the parenthetical he argues is excluded from the attribution phrase the other side knows, corresponding to the factive attribution." ></td>
	<td class="line x" title="148:177	(20) The other side, he argues knows Giuliani has always been pro-choice, even though he has personal reservations." ></td>
	<td class="line x" title="149:177	(0041) REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Ftv Null Null [Polarity] Null Null Null [Determinacy] Null Null Null Inclusion of the fourth feature, determinacy, is not required to be included in the current scheme because the entailment cancelling contexts 36 cana9 be very complex." ></td>
	<td class="line x" title="150:177	For example, in Example (19), the conditional interpretation leading to the indeterminacy of the relation and its arguments is due to the syntactic construction type of the entire sentence." ></td>
	<td class="line x" title="151:177	It is not clear how to annotate the indeterminacy induced by such contexts." ></td>
	<td class="line x" title="152:177	In the example, therefore, the attribution span only includes the anchor for the type of the attribution." ></td>
	<td class="line x" title="153:177	Spans for implicit writer attributions are left unmarked since there is no corresponding text that can be selected." ></td>
	<td class="line x" title="154:177	The absence of a span annotation is simply taken to reflect writer attribution, together with the Wr value on the source feature." ></td>
	<td class="line x" title="155:177	Recognizing attributions is not trivial since they are often left unexpressed in the sentence in which the AO is realized, and have to be inferred from the prior discourse." ></td>
	<td class="line x" title="156:177	For example, in (21), the relation together with its arguments in the third sentence are attributed to Larry Shapiro, but this attribution is implicit and must be inferred from the first sentence." ></td>
	<td class="line x" title="157:177	(21) There are certain cult wines that can command these higher prices, says Larry Shapiro of Martys, a4a5a4a5a4 Whats different is that it is happening with young wines just coming out." ></td>
	<td class="line x" title="158:177	Were seeing it partly because older vintages are growing more scarce. (0071) REL Arg1 Arg2 [Source] Ot Inh Inh The spans for such implicit Ot attributions mark the text that provides the inference of the implicit attribution, which is just the closest occurrence of the explicit attribution phrase in the prior text." ></td>
	<td class="line x" title="159:177	The final aspect of the span annotation is that we also annotate non-clausal phrases as the anchors attribution, such as prepositional phrases like according to X, and adverbs like reportedly, allegedly, supposedly." ></td>
	<td class="line x" title="160:177	One such example is shown in (22)." ></td>
	<td class="line x" title="161:177	(22) No foreign companies bid on the Hiroshima project, according to the bureau . But the Japanese practice of deep discounting often is cited by Americans as a classic barrier to entry in Japans market." ></td>
	<td class="line x" title="162:177	(0501) REL Arg1 Arg2 [Source] Wr Ot Inh [Type] Comm Comm Null [Polarity] Null Null Null [Determinacy] Null Null Null Note that adverbials are free to pick their own type of attribution." ></td>
	<td class="line x" title="163:177	For example, supposedly as an attribution adverb picks PAtt as the value for type." ></td>
	<td class="line x" title="164:177	3.6 Attribution of Implicit Relations Implicit connectives and their arguments in the PDTB are also marked for attribution." ></td>
	<td class="line x" title="165:177	Implicit connectives express relations that are inferred by the reader." ></td>
	<td class="line x" title="166:177	In such cases, the writer intends for the reader to infer a discourse relation." ></td>
	<td class="line x" title="167:177	As with Explicit connectives, implicit relations intended by the writer of the article are distinguished from those intended by some other agent introduced by the writer." ></td>
	<td class="line x" title="168:177	For example, while the implicit relation in Example (23) is attributed to the writer, in Example (24), both Arg1 and Arg2 have been expressed by someone else whose speech is being quoted: in this case, the implicit relation is attributed to the other agent." ></td>
	<td class="line x" title="169:177	(23) The gruff financier recently started socializing in upper-class circles." ></td>
	<td class="line x" title="170:177	Implicit = FOR EXAMPLE (ADD.INFO) Although he says he wasnt keen on going, last year he attended a New York gala where his daughter made her debut." ></td>
	<td class="line x" title="171:177	(0800) REL Arg1 Arg2 [Source] Wr Inh Inh [Type] Comm Null Null [Polarity] Null Null Null [Determinacy] Null Null Null (24) We asked police to investigate why they are allowed to distribute the flag in this way." ></td>
	<td class="line x" title="172:177	Implicit=BECAUSE (CAUSE) It should be considered against the law, said Danny Leish, a spokesman for the association . REL Arg1 Arg2 [Source] Ot Inh Inh [Type] Comm Null Null [Polarity] Null Null Null [Determinacy] Null Null Null For implicit relations, attribution is also annotated for AltLex relations but not for EntRel and NoRel, since the former but not the latter refer to the presense of discourse relations." ></td>
	<td class="line x" title="173:177	4 Summary In this paper, we have proposed and described an annotation scheme for marking the attribution of both explicit and implicit discourse connectives and their arguments in the Penn Discourse TreeBank." ></td>
	<td class="line x" title="174:177	We discussed the role of the annotations for the recognition of factuality in natural language applications, and defined the notion of attribution." ></td>
	<td class="line x" title="175:177	The scheme was presented in detail with examples, outlining the feature-based annotation in terms of the source, type, scopal polarity, and determinacy associated with attribution, and the span annotation to highlight the text reflecting the attribution features." ></td>
	<td class="line x" title="176:177	37 Acknoa10 wledgements The Penn Discourse TreeBank project is partially supported by NSF Grant: Research Resources, EIA 02-24417 to the University of Pennsylvania (PI: A. Joshi)." ></td>
	<td class="line x" title="177:177	We are grateful to Lukasz Abramowicz and the anonymous reviewers for useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0306
Searching For Sentences Expressing Opinions By Using Declaratively Subjective Clues
Hiroshima, Nobuaki;Yamada, Setsuo;Furuse, Osamu;Kataoka, Ryoji;"></td>
	<td class="line x" title="1:190	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 3946, Sydney, July 2006." ></td>
	<td class="line x" title="2:190	c2006 Association for Computational Linguistics Searching for Sentences Expressing Opinions by using Declaratively Subjective Clues Nobuaki Hiroshima, Setsuo Yamada, Osamu Furuse and Ryoji Kataoka NTT Cyber Solutions Laboratories, NTT Corporation 1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan hiroshima.nobuaki@lab.ntt.co.jp Abstract This paper presents a method for searching the web for sentences expressing opinions." ></td>
	<td class="line x" title="3:190	To retrieve an appropriate number of opinions that users may want to read, declaratively subjective clues are used to judge whether a sentence expresses an opinion." ></td>
	<td class="line x" title="4:190	We collected declaratively subjective clues in opinionexpressing sentences from Japanese web pages retrieved with opinion search queries." ></td>
	<td class="line x" title="5:190	These clues were expanded with the semantic categories of the words in the sentences and were used as feature parameters in a Support Vector Machine to classify the sentences." ></td>
	<td class="line x" title="6:190	Our experimental results using retrieved web pages on various topics showed that the opinion expressing sentences identified by the proposed method are congruent with sentences judged by humans to express opinions." ></td>
	<td class="line x" title="7:190	1 Introduction Readers have an increasing number of opportunities to read opinions (personal ideas or beliefs), feelings (mental states), and sentiments (positive or negative judgments) that have been written or posted on web pages such as review sites, personal web sites, blogs, and BBSes." ></td>
	<td class="line x" title="8:190	Such subjective information on the web can often be a useful basis for finding out what people think about a particular topic or making a decision." ></td>
	<td class="line x" title="9:190	A number of studies on automatically extracting and analyzing product reviews or reputations on the web have been conducted (Dave et al. , 2003; Morinaga et al. , 2002; Nasukawa and Yi, 2003; Tateishi et al. , 2004; Kobayashi et al. , 2004)." ></td>
	<td class="line x" title="10:190	These studies focus on using sentiment analysis to extract positive or negative information about a particular product." ></td>
	<td class="line x" title="11:190	Different kinds of subjective information, such as neutral opinions, requests, and judgments, which are not explicitly associated with positive/negative assessments, have not often been considered in previous work." ></td>
	<td class="line x" title="12:190	Although sentiments provide useful information, opinion-expressing sentences like In my opinion this product should be priced around $15, which do not express explicitly positive or negative judgments (unlike sentiments) can also be informative for a user who wants to know others opinions about a product." ></td>
	<td class="line x" title="13:190	When a user wants to collect opinions about an event, project, or social phenomenon, requests and judgments can be useful as well as sentiments." ></td>
	<td class="line x" title="14:190	With open-domain topics, sentences expressing sentiments should not be searched exclusively; other kinds of opinion expressing sentences should be searched as well." ></td>
	<td class="line x" title="15:190	The goal of our research is to achieve a web search engine that locates opinion-expressing sentences about open-domain topics on products, persons, events, projects, and social phenomena." ></td>
	<td class="line x" title="16:190	Sentence-level subjectivity/objectivity classification in some of the previous research (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) can identify subjective statements that include speculation in addition to positive/negative evaluations." ></td>
	<td class="line x" title="17:190	In these efforts, the subjectivity/objectivity of a current sentence is judged based on the existence of subjective/objective clues in both the sentence itself and the neighboring sentences." ></td>
	<td class="line x" title="18:190	The subjective clues, some adjective, some noun, and some verb phrases, as well as other collocations, are learned from corpora (Wiebe, 2000; Wiebe et al. , 2001)." ></td>
	<td class="line x" title="19:190	Some of the clues express subjective meaning unrestricted to positive/negative measurements." ></td>
	<td class="line x" title="20:190	The sentence-level subjectivity ap39 proach suggests a way of searching for opinion expressing sentences in the open domain." ></td>
	<td class="line x" title="21:190	The problem of applying sentence-level subjectivity classification to opinion-expressing sentence searches is the likelihood of collecting too many sentences for a user to read." ></td>
	<td class="line x" title="22:190	According to the work of Wiebe et al.(2001), 70% of sentences in opinion-expressing articles like editorials and 44% of sentences in non-opinion expressing articles like news reports were judged to be subjective." ></td>
	<td class="line oc" title="24:190	In analyzing opinions (Cardie et al. , 2003; Wilson et al. , 2004), judging document-level subjectivity (Pang et al. , 2002; Turney, 2002), and answering opinion questions (Cardie et al. , 2003; Yu and Hatzivassiloglou, 2003), the output of a sentence-level subjectivity classification can be used without modification." ></td>
	<td class="line x" title="25:190	However, in searching opinion-expressing sentences, it is necessary to designate criteria for opinion-expressing sentences that limit the number of retrieved sentences so that a user can survey them without difficulty." ></td>
	<td class="line x" title="26:190	While it is difficult to formally define an opinion, it is possible to practically tailor the definition of an opinion to the purpose of the application (Kim and Hovy, 2004)." ></td>
	<td class="line x" title="27:190	This study introduces the notion of declaratively subjective clues as a criterion for judging whether a sentence expresses an opinion and proposes a method for finding opinionexpressing sentences that uses these clues." ></td>
	<td class="line x" title="28:190	Declaratively subjective clues such as the subjective predicate part of the main clause and subjective sentential adverb phrases suggest that the writer is the source of the opinion." ></td>
	<td class="line x" title="29:190	We hypothesize that a user of such an opinion-expressing sentence search wants to read the writers opinions and that explicitly stated opinions are preferred over quoted or implicational opinions." ></td>
	<td class="line x" title="30:190	We suppose that writers ideas or beliefs are explicitly declared in a sentence with declaratively subjective clues whereas sentences without declaratively subjective clues mainly describe things." ></td>
	<td class="line x" title="31:190	The number of sentences with declaratively subjective clues is estimated to be less than the number of subjective sentences defined in the previous work." ></td>
	<td class="line x" title="32:190	We expect that the opinion expressing sentences identified with our method will be appropriate from the both qualitative and quantitative viewpoints." ></td>
	<td class="line x" title="33:190	Section 2 describes declaratively subjective clues and explains how we collected them from opinion-expressing sentences on Japanese web pages retrieved with opinion search queries." ></td>
	<td class="line x" title="34:190	Section 3 explains our strategy for searching opinion-expressing sentences by using declaratively subjective clues." ></td>
	<td class="line x" title="35:190	Section 4 evaluates the proposed method and shows how the opinionexpressing sentences found by the proposed method are congruent with the sentences judged by humans to be opinions." ></td>
	<td class="line x" title="36:190	2 Declaratively Subjective Clues Declaratively subjective clues are a basic criterion for judging whether a sentence expresses an opinion." ></td>
	<td class="line x" title="37:190	We extracted the declaratively subjective clues from Japanese sentences that evaluators judged to be opinions." ></td>
	<td class="line x" title="38:190	2.1 Opinion-expressing Sentence Judgment We regard a sentence to be opinion expressing if it explicitly declares the writers idea or belief at a sentence level." ></td>
	<td class="line x" title="39:190	We define as a declaratively subjective clue, the part of a sentence that contributes to explicitly conveying the writers idea or belief in the opinion-expressing sentence." ></td>
	<td class="line x" title="40:190	For example, 'I am glad' in the sentence 'I am glad to see you' can convey the writers pleasure to a reader, so we regard the sentence as an opinionexpressing sentence and I am glad as a declaratively subjective clue. Another example of a declaratively subjective clue is the exclamation mark in the sentence 'We got a contract'!" ></td>
	<td class="line x" title="41:190	It conveys the writers emotion about the event to a reader." ></td>
	<td class="line x" title="42:190	If a sentence only describes something abstract or concrete even though it has word-level or phrase-level subjective parts, we do not consider it to be opinion expressing." ></td>
	<td class="line x" title="43:190	On the other hand, some word-level or phrase-level subjective parts can be declaratively subjective clues depending on where they occur in the sentence." ></td>
	<td class="line x" title="44:190	Consider the following two sentences." ></td>
	<td class="line x" title="45:190	(1) This house is beautiful." ></td>
	<td class="line x" title="46:190	(2) We purchased a beautiful house." ></td>
	<td class="line x" title="47:190	Both (1) and (2) contain the word-level subjective part 'beautiful'." ></td>
	<td class="line x" title="48:190	Our criterion would lead us to say that sentence (1) is an opinion, because 'beautiful' is placed in the predicate part and (1) is considered to declare the writers evaluation of the house to a reader." ></td>
	<td class="line x" title="49:190	This is why beautiful in (1) is eligible as a declaratively subjective clue." ></td>
	<td class="line x" title="50:190	On the other hand, sentence (2) is not judged to contain an opinion, because 'beautiful' is placed in the noun phrase, i.e., the object of the verb purchase, and (2) is considered to report the event of the house purchase rather ob40 jectively to a reader." ></td>
	<td class="line x" title="51:190	Sentence (2) partially contains subjective information about the beauty of the house; however this information is unlikely to be what a writer wants to emphasize." ></td>
	<td class="line x" title="52:190	Thus, 'beautiful' in (2) does not work as a declaratively subjective clue." ></td>
	<td class="line x" title="53:190	These two sentences illustrate the fact that the presence of a subjective word (beautiful) does not unconditionally assure that the sentence expresses an opinion." ></td>
	<td class="line x" title="54:190	Additionally, these examples do suggest that sentences containing an opinion can be judged depending on where such wordlevel or phrase-level subjective parts as evaluative adjectives are placed in the predicate part." ></td>
	<td class="line x" title="55:190	Some word-level or phrase-level subjective parts such as subjective sentential adverbs can be declaratively subjective clues depending on where they occur in the sentence." ></td>
	<td class="line x" title="56:190	In sentence (3), amazingly expresses the writers feeling about the event." ></td>
	<td class="line x" title="57:190	Sentence (3) is judged to contain an opinion because there is a subjective sentential adverb in its main clause." ></td>
	<td class="line x" title="58:190	(3) Amazingly, few people came to my party." ></td>
	<td class="line x" title="59:190	The existence of some idiomatic collocations in the main clause also affects our judgment as to what constitutes an opinion-expressing sentence." ></td>
	<td class="line x" title="60:190	For example, sentence (4) can be judged as expressing an opinion because it includes my wish is." ></td>
	<td class="line x" title="61:190	(4) My wish is to go abroad." ></td>
	<td class="line x" title="62:190	Thus, depending on the type of declaratively subjective clue, it is necessary to consider where the expression is placed in the sentence to judge whether the sentence is an opinion." ></td>
	<td class="line x" title="63:190	2.2 Clue Expression Collection We collected declaratively subjective clues in opinion-expressing sentences from Japanese web pages." ></td>
	<td class="line x" title="64:190	Figure 1 illustrates the flow of collection of eligible expressions." ></td>
	<td class="line x" title="65:190	type querys topic Product cell phone, car, beer, cosmetic Entertainment sports, movie, game, animation Facility museum, zoo, hotel, shop Politics diplomacy, election Phenomena diction, social behavior Event firework, festival Culture artwork, book, music Organization company Food cuisine, noodle, ice cream Creature bird Table 1: Topic Examples First, we retrieved Japanese web pages from forty queries covering a wide range of topics such as products, entertainment, facilities, and phenomena, as shown in Table 1." ></td>
	<td class="line x" title="66:190	We used queries on various topics because we wanted to acquire declaratively subjective clues for opendomain opinion web searches." ></td>
	<td class="line x" title="67:190	Most of the queries contain proper nouns." ></td>
	<td class="line x" title="68:190	These queries correspond to possible situations in which a user wants to retrieve opinions from web pages about a particular topic, such as Cell phone X, Y museum, and Football coach Zs ability, where X, Y, and Z are proper nouns." ></td>
	<td class="line x" title="69:190	Next, opinion-expressing sentences were extracted from the top twenty retrieved web pages in each query, 800 pages in total." ></td>
	<td class="line x" title="70:190	There were 75,575 sentences in these pages." ></td>
	<td class="line x" title="71:190	Figure 1: Flow of Clue Expression Collection 41 Three evaluators judged whether each sentence contained an opinion or not." ></td>
	<td class="line x" title="72:190	The 13,363 sentences judged to do so by all three evaluators were very likely to be opinion expressing." ></td>
	<td class="line x" title="73:190	The number of sentences which three evaluators agreed on as non-opinion expressing was 42,346." ></td>
	<td class="line x" title="74:190	1 Out of the 13,363 opinion expressing sentences, 8,425 were then used to extract declaratively subjective clues and learn positive examples in a Support Vector Machine (SVM), and 4,938 were used to assess the performance of opinion expressing sentence search (Section 4)." ></td>
	<td class="line x" title="75:190	Out of the 42,346 non-opinion sentences, 26,340 were used to learn negative examples, and 16,006 were used to assess, keeping the number ratio of the positive and negative example sentences in learning and assessing." ></td>
	<td class="line x" title="76:190	One analyst extracted declaratively subjective clues from 8,425 of the 13,363 opinionexpressing sentences, and another analyst checked the result." ></td>
	<td class="line x" title="77:190	The number of declaratively 1 Note that not all of these opinion-expressing sentences retrieved were closely related to the query because some of the pages described miscellaneous topics." ></td>
	<td class="line x" title="78:190	subjective clues obtained was 2,936." ></td>
	<td class="line x" title="79:190	These clues were classified into fourteen types as shown in Table 2, where the underlined expressions in example sentences are extracted as declaratively subjective clues." ></td>
	<td class="line x" title="80:190	The example sentences in Table 2 are Japanese opinion-expressing sentences and their English translations." ></td>
	<td class="line x" title="81:190	Although some English counterparts of Japanese clue expressions might not be cogent because of the characteristic difference between Japanese and English, the clue types are likely to be language-independent." ></td>
	<td class="line x" title="82:190	We can see that various types of expressions compose opinion-expressing sentences." ></td>
	<td class="line x" title="83:190	As mentioned in Section 2.1, it is important to check where a declaratively subjective clue appears in the sentence in order to apply our criterion of whether the sentence is an opinion or not." ></td>
	<td class="line x" title="84:190	The clues in the types other than (b), (c) and (l) usually appear in the predicate part of a main clause." ></td>
	<td class="line x" title="85:190	The declaratively subjective clues in Japanese examples are placed in the rear parts of sentences except in types (b), (c) and (l)." ></td>
	<td class="line x" title="86:190	This reflects the heuristic rule that Japanese predicate type example sentence (English translation of Japanese sentence) (a) Thought Kono hon wa kare no dato omou." ></td>
	<td class="line x" title="87:190	(I think this book is his)." ></td>
	<td class="line x" title="88:190	(b) Declarative adverb Tabun rainen yooroppa ni iku." ></td>
	<td class="line x" title="89:190	(I will possibly go to Europe next year)." ></td>
	<td class="line x" title="90:190	(c) Interjection Waa, suteki." ></td>
	<td class="line x" title="91:190	(Oh, wonderful)." ></td>
	<td class="line x" title="92:190	(d) Intensifier Karera wa totemo jouzu ni asonda." ></td>
	<td class="line x" title="93:190	(They played extremely well) (e) Impression Kono yougo wa yayakoshii." ></td>
	<td class="line x" title="94:190	(This terminology is confusing)." ></td>
	<td class="line x" title="95:190	(f) Emotion Oai dekite ureshii desu." ></td>
	<td class="line x" title="96:190	(I am glad to see you)." ></td>
	<td class="line x" title="97:190	(g) Positive/negative judgment Anata no oodio kiki wa sugoi." ></td>
	<td class="line x" title="98:190	(Your audio system is terrific)." ></td>
	<td class="line x" title="99:190	(h) Modality about propositional attitude Sono eiga wo miru beki da." ></td>
	<td class="line x" title="100:190	(You should go to the movie)." ></td>
	<td class="line x" title="101:190	(i) Value judgment Kono bun wa imi fumei da." ></td>
	<td class="line x" title="102:190	(This sentence makes no sense)." ></td>
	<td class="line x" title="103:190	(j) Utterance-specific sentence form Towa ittemo,ima wa tada no yume dakedo." ></td>
	<td class="line x" title="104:190	(Though, it's literally just a dream now)." ></td>
	<td class="line x" title="105:190	(k) Symbol Keiyaku wo tottazo!" ></td>
	<td class="line x" title="106:190	(We got a contract!)" ></td>
	<td class="line x" title="107:190	(l) Idiomatic collocation Ii nikui." ></td>
	<td class="line x" title="108:190	(It's hard to say)." ></td>
	<td class="line x" title="109:190	(m) Uncertainty Ohiru ni nani wo tabeyou kanaa." ></td>
	<td class="line x" title="110:190	(I am wondering what I should eat for lunch)." ></td>
	<td class="line x" title="111:190	(n) Imperative Saizen wo tukushi nasai." ></td>
	<td class="line x" title="112:190	(Give it your best)." ></td>
	<td class="line x" title="113:190	Table 2: Clue Types 42 parts are in principle placed in the rear part of a sentence." ></td>
	<td class="line x" title="114:190	3 Opinion-Sentence Extraction In this section, we explain the method of classifying each sentence by using declaratively subjective clues." ></td>
	<td class="line x" title="115:190	The simplest method for automatically judging whether a sentence is an opinion is a rule-based one that extracts sentences that include declaratively subjective clues." ></td>
	<td class="line x" title="116:190	However, as mentioned in Section 2, the existence of declaratively subjective clues does not assure that the sentence expresses an opinion." ></td>
	<td class="line x" title="117:190	It is a daunting task to write rules that describe how each declaratively subjective clue should appear in an opinionexpressing sentence." ></td>
	<td class="line x" title="118:190	A more serious problem is that an insufficient collection of declaratively subjective clues will lead to poor extraction performance." ></td>
	<td class="line x" title="119:190	For that reason, we adopted a learning method that binarily classifies sentences by using declaratively subjective clues and their positions in sentences as feature parameters of an SVM." ></td>
	<td class="line x" title="120:190	With this method, a consistent framework of classification can be maintained even if we add new declaratively subjective clues, and it is possible that we can extract the opinion-expressing sentences which have unknown declaratively subjective clues." ></td>
	<td class="line x" title="121:190	3.1 Augmentation by Semantic Categories Before we can use declaratively subjective clues as feature parameters, we must address two issues:  Cost of building a corpus: It is costly to provide a sufficient amount of tagged corpus of opinion-expressing-sentence labels to ensure that learning achieves a high-performance extraction capability." ></td>
	<td class="line x" title="122:190	 Coverage of words co-occurring with declaratively subjective clues: Many of the declaratively subjective clue expressions have co-occurring words in the opinion-expressing sentence." ></td>
	<td class="line x" title="123:190	Consider the following two sentences." ></td>
	<td class="line x" title="124:190	(5) The sky is high." ></td>
	<td class="line x" title="125:190	(6) The quality of this product is high." ></td>
	<td class="line x" title="126:190	Both (5) and (6) contain the word 'high' in the predicate part." ></td>
	<td class="line x" title="127:190	Sentence (5) is considered to be less of an opinion than (6) because an evaluator might judge (5) to be the objective truth, while all evaluators are likely to judge (6) to be an opinion." ></td>
	<td class="line x" title="128:190	The adjective 'high' in the predicate part can be validated as a declaratively subjective clue depending on co-occurring words." ></td>
	<td class="line x" title="129:190	However, it is not realistic to provide all possible co-occurring words with each declaratively subjective clue expression." ></td>
	<td class="line x" title="130:190	Semantic categories can be of help in dealing with the above two issues." ></td>
	<td class="line x" title="131:190	Declaratively subjective clue expressions can be augmented by semantic categories of the words in the expressions." ></td>
	<td class="line x" title="132:190	An augmentation involving both declaratively subjective clues and co-occurrences will increase feature parameters." ></td>
	<td class="line x" title="133:190	In our implementation, we adopted the semantic categories proposed by Ikehara et al.(1997)." ></td>
	<td class="line x" title="135:190	Utilization of semantic categories has another effect: it improves the extraction performance." ></td>
	<td class="line x" title="136:190	Consider the following two sentence patterns: (7) X is beautiful." ></td>
	<td class="line x" title="137:190	(8) X is pretty." ></td>
	<td class="line x" title="138:190	The words 'beautiful' and 'pretty' are adjectives in the common semantic category, 'appearance', and the degree of declarative subjectivity of these sentences is almost the same regardless of what X is. Therefore, even if 'beautiful' is learned as a declaratively subjective clue but 'pretty' is not, the semantic category 'appearance' that the learned word 'beautiful' belongs to, enables (8) to be judged opinion expressing as well as (7)." ></td>
	<td class="line x" title="139:190	3.2 Feature Parameters to Learn We implemented our opinion-sentence extraction method by using a Support Vector Machine (SVM) because an SVM can efficiently learn the model for classifying sentences into opinionexpressing and non-opinion expressing, based on the combinations of multiple feature parameters." ></td>
	<td class="line x" title="140:190	The following are the crucial feature parameters of our method." ></td>
	<td class="line x" title="141:190	 2,936 declaratively subjective clues  2,715 semantic categories that words in a sentence can fall into If the sentence has a declaratively subjective clue of type (b), (c) or (l) in Table 2, the feature parameter about the clue is assigned a value of 1; if not, it is assigned 0." ></td>
	<td class="line x" title="142:190	If the sentence has declaratively subjective clues belonging to types 43 other than (b), (c) or (l) in the predicate part, the feature parameter about the clue is assigned 1; if not, it is assigned 0." ></td>
	<td class="line x" title="143:190	The feature parameters for the semantic category are used to compensate for the insufficient amount of declaratively subjective clues provided and to consider co-occurring words with clue expressions in the opinion-expressing sentences, as mentioned in Section 3.1." ></td>
	<td class="line x" title="144:190	The following are additional feature parameters." ></td>
	<td class="line x" title="145:190	 150 frequent words  13 parts of speech Each feature parameter is assigned a value of 1 if the sentence has any of the frequent words or parts of speech." ></td>
	<td class="line x" title="146:190	We added these feature parameters based on the hypotheses that some frequent words in Japanese have the function of changing the degree of declarative subjectivity, and that the existence of such parts of speech as adjectives and adverbs possibly influences the declarative subjectivity." ></td>
	<td class="line x" title="147:190	The effectiveness of these additional feature parameters was confirmed in our preliminary experiment." ></td>
	<td class="line x" title="148:190	4 Experiments We conducted three experiments to assess the validity of the proposed method: comparison with baseline methods, effectiveness of position information in SVM feature parameters, and effectiveness of SVM feature parameters such as declaratively subjective clues and semantic categories." ></td>
	<td class="line x" title="149:190	All experiments were performed using the Japanese sentences described in Section 2.1." ></td>
	<td class="line x" title="150:190	We used 8,425 opinion expressing sentences, which were used to collect declaratively subjective clues as a training set, and used 4,938 opinionexpressing sentences as a test set." ></td>
	<td class="line x" title="151:190	We also used 26,340 non-opinion sentences as a training set and used 16,006 non-opinion sentences as a test set." ></td>
	<td class="line x" title="152:190	The test set was divided into ten equal subsets." ></td>
	<td class="line x" title="153:190	The experiments were evaluated with the following measures following the variable scheme in Table 3: ba a P op + = ca a R op + = opop opop op RP RP F + = 2 dc d P opno + = _ db d R opno + = _ opnoopno opnoopno opno RP RP F __ __ _ 2 + = dcba da A +++ + = We evaluated ten subsets with the above measures and took the average of these results." ></td>
	<td class="line x" title="154:190	4.1 Comparison with Baseline Methods We first performed an experiment comparing two baseline methods with our proposed method." ></td>
	<td class="line x" title="155:190	We prepared a baseline method that regards a sentence as an opinion if it contains a number of declaratively subjective clues that exceeds a certain threshold." ></td>
	<td class="line x" title="156:190	The best threshold was set through trial and error at five occurrences." ></td>
	<td class="line x" title="157:190	We also prepared another baseline method that learns a model and classifies a sentence using only features about a bag of words." ></td>
	<td class="line x" title="158:190	The experimental results are shown in Table 4." ></td>
	<td class="line x" title="159:190	It can be seen that our method performs better than the two baseline methods." ></td>
	<td class="line x" title="160:190	Though the difference between our methods results and those of the bag-of-words method seems rather small, the superiority of the proposed method cannot be rejected at the significance level of 5% in t-test." ></td>
	<td class="line x" title="161:190	Answer System Opinion No opinion Opinion a b No opinion c d Opinion No opinion Method Precision Recall F-measure Precision Recall F-measure Accuracy Occurrences of DS clues (baseline 1) 66.4% 35.3% 46.0% 82.6% 94.5% 88.1% 80.5% Bag of words (baseline 2) 80.9% 64.2% 71.6% 89.6% 95.3% 92.4% 88.0% Proposed 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% Table 4: Results for comparison with baseline methods Table 3: Number of sentences in a test set 44 4.2 Feature Parameters with Position Information We inspected the effect of position information of 2,936 declaratively subjective clues based on the heuristic rule that a Japanese predicate part almost always appears in the last ten words in a sentence." ></td>
	<td class="line x" title="162:190	Instead of more precisely identifying predicate position from parsing information, we employed this heuristic rule as a feature parameter in the SVM learner for practical reasons." ></td>
	<td class="line x" title="163:190	Table 5 lists the experimental results." ></td>
	<td class="line x" title="164:190	'All words' indicates that all feature parameters are permitted at any position in the sentence." ></td>
	<td class="line x" title="165:190	'Last 10 words' indicates that all feature parameters are permitted only if they occur within the last ten words in the sentence." ></td>
	<td class="line x" title="166:190	We can see that feature parameters with position information perform better than those without position information in all evaluations." ></td>
	<td class="line x" title="167:190	This result confirms our claim that the position of the feature parameters is important for judging whether a sentence is an opinion or not." ></td>
	<td class="line x" title="168:190	However, the difference did not indicate superiority between the two results at the significance level of 5%." ></td>
	<td class="line x" title="169:190	In the last 10 word experiment, we restricted the position of 422 declaratively subjective clues like (b), (c) and (l) in Table 2, which appear in any position of a sentence, to the same conditions as with the other types of 2,514 declaratively subjective clues." ></td>
	<td class="line x" title="170:190	The fact that the equal position restriction on all declaratively subjective clues slightly improved performance suggests there will be significant improvement in performance from assigning the individual position condition to each declaratively subjective clue." ></td>
	<td class="line x" title="171:190	4.3 Effect of Feature Parameters The third experiment was designed to ascertain the effects of declaratively subjective clues and semantic categories." ></td>
	<td class="line x" title="172:190	The declaratively subjective clues and semantic categories were employed as feature parameters for the SVM learner." ></td>
	<td class="line x" title="173:190	The effect of each particular feature parameter can be seen by using it without the other feature parameter, because the feature parameters are independent of each other." ></td>
	<td class="line x" title="174:190	The experimental results are shown in Table 6." ></td>
	<td class="line x" title="175:190	The first row shows trials using only frequent words and parts of speech as feature parameters." ></td>
	<td class="line x" title="176:190	'Y' in the first and second columns indicates exclusive use of declaratively subjective clues and semantic categories as the feature parameters, respectively." ></td>
	<td class="line x" title="177:190	For instance, we can determine the effect of declaratively subjective clues by comparing the first row with the second row." ></td>
	<td class="line x" title="178:190	The results show the effects of declaratively subjective clues and semantic categories." ></td>
	<td class="line x" title="179:190	The results of the first row show that the method using only frequent words and parts of speech as the feature parameters cannot precisely classify subjective sentences." ></td>
	<td class="line x" title="180:190	Additionally, the last row of the results clearly shows that using both declaratively subjective clues and semantic categories as the feature parameters is the most effective." ></td>
	<td class="line x" title="181:190	The difference between the last row of the results and the other rows cannot be rejected even at the significance level of 5%." ></td>
	<td class="line x" title="182:190	Feature sets Opinion No opinion DS clues Semantic categories Precision Recall Fmeasure Precision Recall Fmeasure Accuracy 71.4% 53.2% 60.9% 87.7% 94.1% 90.8% 85.2% Y 79.9% 64.3% 71.2% 89.6% 95.0% 92.2% 87.8% Y 76.1% 68.9% 72.2% 90.7% 93.3% 92.0% 87.5% Y Y 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% Opinion No opinion Position Precision Recall F-measure Precision Recall F-measure Accuracy All words 76.8% 70.6% 73.5% 91.2% 93.4% 92.3% 88.0% Last 10 words 78.6% 70.8% 74.4% 91.3% 94.0% 92.6% 88.6% Table 5: Results for feature parameters with position information Table 6: Results for effect of feature parameters 45 5 Conclusion and Future Work We proposed a method of extracting sentences classified by an SVM as opinion-expressing that uses feature sets of declaratively subjective clues collected from opinion-expressing sentences in Japanese web pages and semantic categories of words obtained from a Japanese lexicon." ></td>
	<td class="line x" title="183:190	The first experiment showed that our method performed better than baseline methods." ></td>
	<td class="line x" title="184:190	The second experiment suggested that our method performed better when extraction of features was limited to the predicate part of a sentence rather than allowed anywhere in the sentence." ></td>
	<td class="line x" title="185:190	The last experiment showed that using both declaratively subjective clues and semantic categories as feature parameters yielded better results than using either clues or categories exclusively." ></td>
	<td class="line x" title="186:190	Our future work will attempt to develop an open-domain opinion web search engine." ></td>
	<td class="line x" title="187:190	To succeed, we first need to augment the proposed opinion-sentence extraction method by incorporating the query relevancy mechanism." ></td>
	<td class="line x" title="188:190	Accordingly, a user will be able to retrieve opinionexpressing sentences relevant to the query." ></td>
	<td class="line x" title="189:190	Second, we need to classify extracted sentences in terms of emotion, sentiment, requirement, and suggestion so that a user can retrieve relevant opinions on demand." ></td>
	<td class="line x" title="190:190	Finally, we need to summarize the extracted sentences so that the user can quickly learn what the writer wanted to say." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-0308
Towards A Validated Model For Affective Classification Of Texts
Genereux, Michel;Evans, Roger;"></td>
	<td class="line x" title="1:160	Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 5562, Sydney, July 2006." ></td>
	<td class="line x" title="2:160	c2006 Association for Computational Linguistics Towards a validated model for affective classification of texts Michel Genereux and Roger Evans Natural Language Technology Group (NLTG) University of Brighton, United Kingdom {M.Genereux,R.P.Evans}@brighton.ac.uk Abstract In this paper, we present the results of experiments aiming to validate a twodimensional typology of affective states as a suitable basis for affective classi cation of texts." ></td>
	<td class="line x" title="3:160	Using a corpus of English weblog posts, annotated for mood by their authors, we trained support vector machine binary classi ers to distinguish texts on the basis of their af liation with one region of the space." ></td>
	<td class="line x" title="4:160	We then report on experiments which go a step further, using four-class classi ers based on automated scoring of texts for each dimension of the typology." ></td>
	<td class="line x" title="5:160	Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more ne-grained classi cation along these two axes." ></td>
	<td class="line x" title="6:160	1 Introduction We are investigating the subjective use of language in text and the automatic classi cation of texts according to their subjective characteristics, or affect." ></td>
	<td class="line x" title="7:160	Our approach is to view affective states (such as happy, angry) as locations in Osgoods Evaluation-Activation (EA) space (Osgood et aal., 1957), and draws on work in psychology which has a long history of work seeking to construct a typology of such affective states (Scherer, 1984)." ></td>
	<td class="line x" title="9:160	A similar approach has been used more recently to describe emotional states that are expressed in speech (Cowie and Cornelius, 2002; Schrcurrency1oder and Cowie, 2005)." ></td>
	<td class="line x" title="10:160	Our overall aim is to determine the extent to which such a typology can be validated and applied to the task of text classi cation using automatic methods." ></td>
	<td class="line x" title="11:160	In this paper we describe some initial experiments aimed at validating a basic two dimensional classi cation of weblog data, rst with Support Vector Machine (SVM) binary classi ers, then with Pointwise Mutual Information Information Retrieval (PMI-IR)." ></td>
	<td class="line x" title="12:160	The domain of weblog posts is particularly well-suited for this task given its highly subjective nature and the availability of data, including data which has been author-annotated for mood, which is a reasonable approximation of affect." ></td>
	<td class="line x" title="13:160	Recent attempts to classify weblog posts have shown modest, but consistent improvements over a 50% baseline, only slightly worse than human performance (Mishne, 2005)." ></td>
	<td class="line x" title="14:160	One important milestone is the elaboration of a typology of affective states." ></td>
	<td class="line x" title="15:160	To devise such a typology, our starting point is Figure 1, which is based on a model of emotion as a multicomponent process (Scherer, 1984)." ></td>
	<td class="line x" title="16:160	In this model, the distribution of the affective states is the result of analysing similarity judgments by humans for 235 emotion terms1 using cluster-analysis and multidimensional scaling techniques to map out the structure as a twodimensional space." ></td>
	<td class="line x" title="17:160	The positioning of words is not so much controversial as fuzzy; an affective state such as angry to describe facial expression in speech may have a slightly different location than an angry weblog post." ></td>
	<td class="line x" title="18:160	In this model, the well-studied sentiment classi cation is simply a speci c case (left vs. right halves of the space)." ></td>
	<td class="line x" title="19:160	The experiments we describe here seek to go beyond this basic distinction." ></td>
	<td class="line x" title="20:160	They involve an additional dimension of affect, the activity dimension, allowing textual data to be classi ed into four categories corresponding to each of the four quad1Reduced to less than 100 in Figure 1." ></td>
	<td class="line x" title="21:160	55 Figure 1: Typology of affective states based on (Scherer, 1984) rants in the space." ></td>
	<td class="line x" title="22:160	Ultimately, once scores have been promoted to real measures, classi cation can be more precise; for example, a text is not only negative and passive, it is more precisely depressive." ></td>
	<td class="line x" title="23:160	With such a more precise classi cation one might, for example, be able to detect individuals at risk of suicide." ></td>
	<td class="line x" title="24:160	In Experiment 1, we use binary classi ers to investigate how the four quadrants de ned by the typology hold together, the assumption being that if the typology is correct, the classi ers should perform substantially better than a random baseline." ></td>
	<td class="line x" title="25:160	In Experiment 2, we go a step closer towards a more ne-grained classi cation by evaluating the performance of an unsupervised automated technique for scoring texts on both axes." ></td>
	<td class="line x" title="26:160	Both these experiments are preliminary our long term goal is to be able to validate the whole typology in terms of computationally effective classi cation." ></td>
	<td class="line x" title="27:160	2 Corpus We have collected from Livejournal2 a total of 346723 weblogs (mood-annotated by authors) in 2http://www.livejournal.com." ></td>
	<td class="line x" title="28:160	English, from which almost half are annotated with a mood belonging to one of the four quadrants, described as follows: Quadrant1 bellicose, tense, alarmed, envious, hateful, angry, enraged, de ant, annoyed, jealous, indignant, frustrated, distressed, disgusted, suspicious, discontented, bitter, insulted, distrustful, startled, contemptuous and impatient." ></td>
	<td class="line x" title="29:160	Quadrant2 apathetic, disappointed, miserable, dissatis ed, taken aback, worried, languid, feel guilt, ashamed, gloomy, sad, uncomfortable, embarrassed, melancholic, depress, desperate, hesitant, bored, wavering, droopy, tired, insecured, anxious, lonely and doubtful." ></td>
	<td class="line x" title="30:160	Quadrant3 feel well, impressed, pleased, amourous, astonished, glad, content, hopeful, solemn, attentive, longing, relaxed, serious, serene, content, at ease, friendly, satis ed, calm, contemplative, polite, pensive, peaceful, conscientious, empathic, reverent and sleepy." ></td>
	<td class="line x" title="31:160	Quadrant4 happy, ambitious, amused, adventurous, aroused, astonished, triumphant, excited, 56 conceited, self con dent, courageous, feeling superior, enthusiastic, light hearthed, determined, passionate, expectant, interested, joyous and delighted." ></td>
	<td class="line x" title="32:160	In our experiments, we used 15662 from quadrant Q1 (see Figure 1), 54940 from Q2, 49779 from Q3 and 35634 from Q4." ></td>
	<td class="line x" title="33:160	3 Experiment 1: Distinguishing the four Quadrants Our hypothesis is that the classi cation of two disjoint sets of moods should yield a classi cation accuracy signi cantly above a baseline of 50%." ></td>
	<td class="line x" title="34:160	To verify our hypothesis, we conducted a series of experiments using machine learning to classify weblog posts according to their mood, each class corresponding to one particular quadrant." ></td>
	<td class="line x" title="35:160	We used Support Vector Machines (Joachims, 2001) with three basic classic features (unigrams, POS and stems) to classify the posts as belonging to one quadrant or one of the three others." ></td>
	<td class="line x" title="36:160	For each classi cation task, we extracted randomly 1000 testing examples, and trained separately with 2000, 4000, 8000 and 16000 examples." ></td>
	<td class="line x" title="37:160	In each case, examples were divided equally among positive and negative examples3." ></td>
	<td class="line x" title="38:160	The set of features used varied for each of these tasks, they were selected by thresholding each (distinct) training data set, after removing words (unigrams) from the categories poor in affective content (prepositions, determiners, etc.)." ></td>
	<td class="line x" title="39:160	To qualify as a feature, each unigram, POS or stem had to occur at least three times in the training data." ></td>
	<td class="line x" title="40:160	The value of each feature corresponds to its number of occurence in the training examples." ></td>
	<td class="line x" title="41:160	3.1 Results Our hypothesis is that, if the four quadrants depicted in Figure 1 are a suitable arrangement for affective states in the EA space, a classi er should perform signi cantly better than chance (50%)." ></td>
	<td class="line x" title="42:160	Table 1 shows the results for the binary classi cation of the quadrants." ></td>
	<td class="line x" title="43:160	In this table, the rst column identi es the classi cation task in the form P vs N, where P stands for positive examples and N for negative examples." ></td>
	<td class="line x" title="44:160	The Random row shows results for selecting positive and negative examples randomly from all four quadrants." ></td>
	<td class="line x" title="45:160	By 3For instance, 1000 = 500 positives from one QUADRANT + 500 negatives among the other three QUADRANTS." ></td>
	<td class="line x" title="46:160	micro-averaging accuracy for the classi cation of each quadrant vs all others (rows 10 to 13), we obtain at least 60% accuracy for the four binary classi cations of the quadrants4." ></td>
	<td class="line x" title="47:160	The rst six rows show evidence that each quadrant forms a distinctive whole, as the classifer can easily decide between any two of them." ></td>
	<td class="line x" title="48:160	Testing Size of training set 1000 examples 2k 4k 8k 16k Q1 vs Q3 67% 70% 72% 73% Q2 vs Q4 61% 64% 65% 67% Q1 vs Q2 64% 66% 68% 69% Q2 vs Q3 58% 59% 59% 59% Q3 vs Q4 59% 60% 60% 61% Q4 vs Q1 69% 72% 73% 75% Q1+4 vs Q2+3 56% 58% 58% 61% Q3+4 vs Q1+2 62% 65% 67% 66% Random 49% 52% 50% 50% Q1 vs Q2+3+4 67% 72% 72% 73% Q2 vs Q1+3+4 59% 60% 63% 63% Q3 vs Q1+2+4 57% 58% 58% 59% Q4 vs Q1+2+3 60% 63% 65% 65% Micro-accuracy 61% 64% 65% 65% Table 1: Accuracy of binary classi cation 3.2 Analysis of Results We introduce now table 2 that shows two thresholds of signi cance (1% and 5%) for the interpretation of current and coming results." ></td>
	<td class="line x" title="49:160	For example, if we have 1000 trials with each trial having a probability of success of 0.5, the likelihood of getting at least 53.7% of the trials right is only 1%." ></td>
	<td class="line x" title="50:160	This gives us a baseline to see how signi cantly well above chance a classi er performs." ></td>
	<td class="line x" title="51:160	The SVM algorithm has linearly separated the data for each quadrant according to lexical and POS content (the features)." ></td>
	<td class="line x" title="52:160	The most sensible explanation is that the features for each class (quadrant) are semantically related, a piece of information which is relevant for the model (see section 4)." ></td>
	<td class="line x" title="53:160	It is safe to conclude that the results cannot be allocated to chance, that there is something else at work that explains the 4Micro-averaged accuracy is de ned as: summationtext i (tpi + tni)summationtext i (tpi + tni + fpi + fni) where tp stands for true positive, fn for false negative, etc. 57 Trials Prob(Success) 1% 5% 1000 0.50 53.7% 52.6% 750 0.50 54.3% 53.1% 500 0.50 55.2% 53.6% 250 0.50 57.2% 55.2% 1000 0.25 28.2% 27.3% 750 0.25 28.7% 27.6% 500 0.25 29.6% 28.2% 250 0.25 31.6% 29.6% Table 2: Statistical Signi cance accuracies consistently well above a baseline, and this something else is the typology." ></td>
	<td class="line x" title="54:160	These results show that the abstraction offered by the four quadrants in the model seems correct." ></td>
	<td class="line x" title="55:160	This is also supported by the observation that the classi er shows no improvements over the baseline if trained over a random selection of examples in the entire space." ></td>
	<td class="line x" title="56:160	4 Experiment 2: Classification using Semantic Orientation from Association Our next goal is to be able to classify a text according to more than four classes (positive/negative, active/passive), by undertaking multi-category classi cation of texts according to particular regions of the space, (such as angry, sad, etc.)." ></td>
	<td class="line x" title="57:160	In order to do that we need a scoring system for each axis." ></td>
	<td class="line x" title="58:160	In the following experiments we explore the use of such scores and give some insights into how to transform these scores of affect as measures of affect." ></td>
	<td class="line x" title="59:160	Using binary classi ers, we have already established that if we look at the lexical contents of weblog posts tagged according to their mood by their author, these mood classes tend to cluster according to a two-dimensional typology de ned by their semantic orientation: positive or negative (evaluation), active or passive (activity)." ></td>
	<td class="line x" title="60:160	Beyond academic importance, the typology really becomes of practical interest if we can classify the posts using pre-de ned automated scores for both axis." ></td>
	<td class="line x" title="61:160	One strategy of scoring is to extract phrases, including single words, which are good indicators of subjectivity in texts, and score them according to how they relate or associate to one or the other extremity of each axis." ></td>
	<td class="line x" title="62:160	This strategy, called Semantic Orientation (SO) from Association (A) has been used successfully (Turney and Littman, 2003) to classify texts or adjectives of all sorts according to their sentiments (in our typology this corresponds to the evaluation dimension)." ></td>
	<td class="line x" title="63:160	According to these scores, a text or adjective can be said to have, for example, a more or less positive or negative evaluation." ></td>
	<td class="line x" title="64:160	We will use this strategy to go further in the validation of our model of affective states by scoring also the activity dimension; to our knowledge, this is the rst time this strategy is employed to get (text) scores for dimensions other than evaluation." ></td>
	<td class="line x" title="65:160	In SO-A, we score the strength of the association between an indicator from the text and a set of positive or negative words (the paradigms Pwords and Nwords) capturing the very positive/active or negative/passive semantic orientation of the axis poles." ></td>
	<td class="line x" title="66:160	To get the SO-A of a text, we sum over positive scores for indicators positively related to Pwords and negatively related to Nwords and negative scores for indicators positively related to Nwords and negatively related to Pwords." ></td>
	<td class="line x" title="67:160	In mathematical terms, the SO-A of a text is: Textsummationdisplay ind ( Pwordssummationdisplay p A(ind, p)  Nwordssummationdisplay n A(ind, n)) where ind stands for indicator." ></td>
	<td class="line x" title="68:160	Note that the quantity of Pwords must be equal to Nwords." ></td>
	<td class="line x" title="69:160	To compute A, (Kamps et aal., 2004) focus on the use of lexical relations de ned in WordNet5 and de ne a distance measure between two terms which amounts to the length of the shortest path that connects the two terms." ></td>
	<td class="line x" title="71:160	This strategy is interesting because it constrains all values to belong to the [-1,+1] range, but can be applied only to a nite set of indicators and has yet to be tested for the classi cation of texts." ></td>
	<td class="line x" title="72:160	(Turney and Littman, 2003) use Pointwise Mutual Information Information Retrieval (PMI-IR); PMI-IR operates on a wider variety of multi-words indicators, allowing for contextual information to be taken into account, has been tested extensively on different types of texts, and the scoring system can be potentially normalized between [-1,+1], as we will soon see." ></td>
	<td class="line x" title="73:160	PMI (Church and Hanks, 1990) between two phrases is de ned as: log2 prob(ph1 is near ph2)prob(ph 1)  prob(ph2) PMI is positive when two phrases tend to co-occur and negative when they tend to be in a complementary distribution." ></td>
	<td class="line x" title="74:160	PMI-IR refers to the fact 5http://wordnet.princeton.edu/." ></td>
	<td class="line x" title="75:160	58 that, as in Informtion Retrieval (IR), multiple occurrences in the same document count as just one occurrence: according to (Turney and Littman, 2003), this seems to yield a better measure of semantic similarity, providing some resistance to noise." ></td>
	<td class="line x" title="76:160	Computing probabilities using hit counts from IR, this yields to a value for PMI-IR of: logn N  (hits(ph1 NEAR ph2) + 1/N)(hits(ph 1) + 1)  (hits(ph2) + 1) where N is the total number of documents in the corpus." ></td>
	<td class="line x" title="77:160	We are going to use this method for computing A in SO-A, which we call SO-PMI-IR." ></td>
	<td class="line x" title="78:160	The con guration depicted in the remaining of this section follows mostly (Turney and Littman, 2003)." ></td>
	<td class="line x" title="79:160	Smoothing values (1/N and 1) are chosen so that PMI-IR will be zero for words that are not in the corpus, two phrases are considered NEAR if they co-occur within a window of 20 words, and log2 has been replaced by logn, since the natural log is more common in the literature for log-odds ratio and this makes no difference for the algorithm." ></td>
	<td class="line x" title="80:160	Two crucial aspects of the method are the choice of indicators to be extracted from the text to be classi ed, as well as the sets of positive and negative words to be used as paradigms for the evaluation and activity dimensions." ></td>
	<td class="line oc" title="81:160	The ve part-ofspeech (POS) patterns from (Turney, 2002) were used for the extraction of indicators, all involving at least one adjective or adverb." ></td>
	<td class="line x" title="82:160	POS tags were acquired with TreeTagger (Schmid, 1994)6." ></td>
	<td class="line x" title="83:160	Ideally, words used as paradigms should be context insensitive, i.e their semantic orientation is either always positive or negative." ></td>
	<td class="line x" title="84:160	The adjectives good, nice, excellent, positive, fortunate, correct, superior and bad, nasty, poor, negative, unfortunate, wrong, inferior were used as near pure representations of positive and negative evaluation respectively, while fast, alive, noisy, young and slow, dead, quiet, old as near pure representations of active and passive activity (Summers, 1970)." ></td>
	<td class="line x" title="85:160	Departing from (Turney and Littman, 2003), who uses the Alta Vista advanced search with approximately 350 millions web pages, we used the Waterloo corpus7, with approximately 46 millions pages." ></td>
	<td class="line x" title="86:160	To avoid introducing confusing heuristics, we stick to the con guration described above, but (Turney and Littman, 2003) have experimented with different con guation in computing SO-PMIIR." ></td>
	<td class="line x" title="87:160	6(Turney and Littman, 2003) uses (Brill, 1994)." ></td>
	<td class="line x" title="88:160	7http://canola1.uwaterloo.ca/." ></td>
	<td class="line x" title="89:160	4.1 The Typology and SO-PMI-IR We now use the typology with an automated scoring method for semantic orientation." ></td>
	<td class="line x" title="90:160	The results are presented in the form of a Confusion Matrix (CM)." ></td>
	<td class="line x" title="91:160	In this and the following matrices, the topleft cell indicates the overall accuracy8, the POSitive (ACTive) and NEGative (PASsive) columns represent the instances in a predicted class, the P/T column (where present) indicates the average number of patterns per text (blog post), E/P indicates the average evaluation score per pattern and A/P indicates the average activity score per pattern." ></td>
	<td class="line x" title="92:160	Each row represents the instances in an actual class9." ></td>
	<td class="line x" title="93:160	First, it is useful to get a clear idea of how the SO-PMI-IR experimental setup we presented compares with (Turney and Littman, 2003) on a human-annotated set of words according to their evaluation dimension: the General Inquirer (GI, (Stone, 1966)) lexicon is made of 3596 words (1614 positives and 1982 negatives)10." ></td>
	<td class="line x" title="94:160	Table 3 summarizes the results." ></td>
	<td class="line x" title="95:160	(Turney and Littman, (U) 76.4% POS NEG E/P POS(1614) 59.3% 40.7% 1.5 NEG(1982) 9.6% 90.4% -4.3 (T) 82.8% POS NEG E/P POS(1614) 81.2% 18.8% 3.2 NEG(1982) 15.8% 84.2% -3.6 Table 3: CM for the GI: (U)Us and (T)(Turney and Littman, 2003) 2003) reports an accuracy of 82.8% while classifying those words, while our experiment yields an accuracy of 76.4% for the same words." ></td>
	<td class="line x" title="96:160	Their results show that their classi er errs very slightly towards the negative pole (as shown by the accuracies of both predicted classes) and has a very balanced distribution of the word scores (as shown by the almost equal but opposite in signs values of E/Ps)." ></td>
	<td class="line x" title="97:160	This is some evidence that the paradigm words are appropriate as near pure representations of positive and negative evaluation." ></td>
	<td class="line x" title="98:160	By contrast, 8Recall that table 2 gives an interpretation of the statistical signi ance of accuracy, with trials  750 and Prob(success) = 0.5." ></td>
	<td class="line x" title="99:160	9For example, in the comparative evaluation shown in table 3, our classi er classi ed 59.3% of the 1614 positive instances as positive and 40.7% as negative, with an average score of 1.5 per pattern." ></td>
	<td class="line x" title="100:160	10Note that all moods in the typology present in the GI have the same polarity for evaluation in both, which is some evidence in favour of the typology." ></td>
	<td class="line x" title="101:160	59 our classi er appears to be more strongly biased towards the negative pole, probably due to the use of different corpora." ></td>
	<td class="line x" title="102:160	This bias11should be kept in mind in the interpretation of the results to come." ></td>
	<td class="line x" title="103:160	The second experiment focuses on the words from the typology." ></td>
	<td class="line x" title="104:160	Table 4 shows the results." ></td>
	<td class="line x" title="105:160	The 81.1% POS NEG P/T E/P POS(43) 60.5% 39.5% 1 0.4 NEG(47) 0.0% 100.0% 1 -6.4 66.7% ACT PAS P/T A/P ACT(39) 33.3% 66.7% 1 -0.9 PAS(51) 7.8% 92.2% 1 -2.9 Table 4: CM for the Typology affective states value of 1 under P/T re ects the fact that the experiment amounts, in practical terms, to classifying the annotation of the post (a single word)." ></td>
	<td class="line x" title="106:160	For the evaluation dimension, there is another shift towards the negative pole of the axis, which suggests that words in the typology are distributed not exactly as shown on gure 1, but instead appear to have a true location shifted towards the negative pole." ></td>
	<td class="line x" title="107:160	The activity dimension also appear to have a negative (i.e passive) bias." ></td>
	<td class="line x" title="108:160	There are two main possible reasons for that: words in the typology should be shifted towards the passive pole (as in the evaluation case), or the paradigm words for the passive pole are not pure representations of the extremity of the pole 12." ></td>
	<td class="line x" title="109:160	Having established that our classi er has a negative bias for both axes, we now turn to the classi cation of the quadrants per se." ></td>
	<td class="line x" title="110:160	In the next section, we used SO-PMI-IR to classify 1000 randomnly selected blog posts from our corpus, i.e 250 in each of the four quadrants." ></td>
	<td class="line x" title="111:160	Some of these posts were found to have no pattern and were therefore not classi ed, which means that less than 1000 posts were actually classi ed in each experiment." ></td>
	<td class="line x" title="112:160	We also report on the classi cation of an important subcategory of these moods called the Big Six emotions." ></td>
	<td class="line x" title="113:160	11Bias can be introduced by the use of a small corpus, inadequate paradigm words or typology." ></td>
	<td class="line x" title="114:160	In practice, a quick x for neutralizing bias would be to normalize the SO-PMI-IR values by subtracting the average." ></td>
	<td class="line x" title="115:160	This work aims at tuning the model to remove bias introduced by unsound paradigm words or typology." ></td>
	<td class="line x" title="116:160	12At the time of experimenting, we were not aware of an equivalent of the GI to independently verify our paradigm words for activity, but one reviewer pointed out such a resource, see http://www.wjh.harvard.edu/ inquirer/spreadsheet_guide.htm." ></td>
	<td class="line x" title="117:160	4.2 Results Of the 1000 blog posts, there were 938 with at least one pattern." ></td>
	<td class="line x" title="118:160	Table 5 shows the accuracy for the classi cation of these posts." ></td>
	<td class="line x" title="119:160	56.8% POS NEG P/T E/P POS(475) 76.2% 23.8% 10 5.2 NEG(463) 63.1% 36.9% 9 3.5 51.8% ACT PAS P/T A/P ACT(461) 20.6% 79.4% 8 -4.3 PAS(477) 18.0% 82.0% 11 -4.2 Table 5: CM for all Moods An important set of emotions found in the literature (Ekman, 1972) has been termed the Big Six." ></td>
	<td class="line x" title="120:160	These emotions are fear, anger, happiness, sadness, surprise and disgust." ></td>
	<td class="line x" title="121:160	We have used a minimally extended set, adding love and desire (Cowie and Cornelius, 2002), to cover all four quadrants (we called this set the Big Eight)." ></td>
	<td class="line x" title="122:160	Fear, anger and disgust belong to quadrant 1, sadness and surprise (we have taken it to be a synonym of taken aback in the typology) belong to quadrant 2, love and desire (taken to be synonyms of amorous and longing in the typology) belong to quadrant 3 and happy to quadrant 4." ></td>
	<td class="line x" title="123:160	Table 6 shows the results for the classi cation of the blog posts that were tagged with one of these emotions." ></td>
	<td class="line x" title="124:160	This amounts to classifying the posts containing only the Big Eight affective states." ></td>
	<td class="line x" title="125:160	59.0% POS NEG P/T E/P POS(467) 72.4% 27.6% 9 5.1 NEG(351) 58.7% 41.3% 6 2.3 54.9% ACT PAS P/T A/P ACT(357) 23.8% 76.2% 8 -4.4 PAS(461) 21.0% 79.0% 8 -4.6 Table 6: CM for the Big Eight In the remaining two experiments, blog posts have been classifed using a discrete scoring system." ></td>
	<td class="line x" title="126:160	Disregarding the real value of SO, each pattern was scored with a value of +1 for a positive score and -1 for a negative score." ></td>
	<td class="line x" title="127:160	This amounts to counting the number of patterns on each side and has the advantage of providing a normalized value for E/T and A/T between -1 and +1." ></td>
	<td class="line x" title="128:160	Normalized values are the rst step towards a measure of affect, not merely a score, in the sense that it gives an estimate of the strength of affect." ></td>
	<td class="line x" title="129:160	We have not 60 classi ed the posts for which the resulting score was zero, which means that even fewer posts (741) than the previous experiment were actually evaluated." ></td>
	<td class="line x" title="130:160	Table 7 shows the results for all moods and table 8 for the Big Eight." ></td>
	<td class="line x" title="131:160	55.7% POS NEG P/T E/P POS(374) 53.2% 46.8% 11 0.03 NEG(367) 41.7% 58.3% 9 -0.11 53.3% ACT PAS P/T A/P ACT(357) 21.8% 78.2% 8 -0.3 PAS(384) 17.4% 82.6% 12 -0.34 Table 7: CM for all Moods: Discrete scoring 59.8% POS NEG P/T E/P POS(373) 52.3% 47.7% 10 0.01 NEG(354) 32.2% 67.8% 9 -0.2 52.8% ACT PAS P/T A/P ACT(361) 25.8% 74.2% 10 -0.3 PAS(366) 20.5% 79.5% 9 -0.4 Table 8: CM for the Big Eight: Discrete scoring 4.3 Analysis of Results Our concerns about the paradigm words for evaluating the activity dimension are clearly revealed in the classi cation results." ></td>
	<td class="line x" title="132:160	The classi er shows a heavy negative (passive) bias in all experiments." ></td>
	<td class="line x" title="133:160	The overall accuracy for activity is consistently below that for evaluation: three of them are not statistically signi cant at 1% (51.8%, 53.3% and 52.8%) and two at even 5% (51.8% and 52.8%)." ></td>
	<td class="line x" title="134:160	The classi er appears particularly confused in table 5, averaging a score for active posts (-4.3) smaller than for passive posts (-4.2)." ></td>
	<td class="line x" title="135:160	It is not impossible that the moods present in the typology may have to be shifted towards the passive dimension, but further research should look rst at nding better paradigm words for activity." ></td>
	<td class="line x" title="136:160	A good starting point for the calibration of the classi er for activity is the creation of a list of humanannotated words for activity, comparable in size to the GI list, combined with an experiment similar to the one for which results are reported in table 3." ></td>
	<td class="line x" title="137:160	With regards to the evaluation dimension, tables 5 and 6 reveal a positive bias (despite having a classi er which has a built-in negative bias, see section 4.1)." ></td>
	<td class="line x" title="138:160	Possible explanations for this phenomenon include the use of irony by people in negative posts, blogs which are expressed in more positive terms than their annotation would suggest, and failure to detect negative contexts for patterns one example of the latter is provided in table 9." ></td>
	<td class="line x" title="139:160	This phenomena appears to be alleviated Mood: bored (evaluation-) Post: gah!!" ></td>
	<td class="line x" title="140:160	i need new music, any suggestions?" ></td>
	<td class="line x" title="141:160	by the way, GOOD MUSIC." ></td>
	<td class="line x" title="142:160	Patterns: new music [JJ NN] +4.38 GOOD MUSIC [JJ NN] +53.40 Average SO: +57.78 (evaluation+) Table 9: Missclassi ed post by the use of discrete scores (see tables 7 and 8)." ></td>
	<td class="line x" title="143:160	One way of re ning the scoring system is to reduce the effect of scoring antonyms as high as synonyms by not counting co-occurences in the corpus where the word not is in the neighbourhood (Turney, 2001)." ></td>
	<td class="line x" title="144:160	Also, The long-term goal of this research is to be able to classify texts by locating their normalized scores for evaluation and activity between -1 and +1, and we have suggested a simple method of achieving that by averaging over discrete scores." ></td>
	<td class="line x" title="145:160	However, by combining individual results for evaluation and activity for each post13, we can already classify text into one of the four quadrants, and we can expect the average accuracy of this classi cation to be approximately the product of the accuracy for each dimension." ></td>
	<td class="line x" title="146:160	Table 10 shows the results for the classi cation directly into quadrants of the 727 posts already classi ed into halves (E, A) in table 8." ></td>
	<td class="line x" title="147:160	The overall accuracy is 31.1% (expected accuracy is 59.8% * 52.8% = 31.6%)." ></td>
	<td class="line x" title="148:160	There are biases towards Q2 and Q3, but no clear cases of confusion between two or more classes." ></td>
	<td class="line x" title="149:160	31.1% Q1 Q2 Q3 Q4 Q1(180) 21.1% 47.8% 22.2% 8.9% Q2(174) 15.5% 51.1% 25.3% 8.0% Q3(192) 9.9% 42.2% 40.1% 7.8% Q4(181) 9.4% 33.7% 44.8% 12.2% Table 10: CM for Big Eight: Discrete scoring Finally, our experiments show no correlation between the length of a post (in number of patterns) and the accuracy of the classi cation." ></td>
	<td class="line x" title="150:160	13For example, a post with Eand A+ would be classi ed in Q1." ></td>
	<td class="line x" title="151:160	61 5 Conclusion and Future Work In this paper, we have used a machine learning approach to show that there is a relation between the semantic content of texts and the affective state they (wish to) convey, so that a typology of affective states based on semantic association is a good description of the distribution of affect in a twodimensional space." ></td>
	<td class="line x" title="152:160	Using automated methods to score semantic association, we have demonstrated a method to compute semantic orientation on both dimensions, giving some insights into how to go beyond the customary sentiment analysis." ></td>
	<td class="line x" title="153:160	In the classi cation experiments, accuracies were always above a random baseline, although not always statistically signi cant." ></td>
	<td class="line x" title="154:160	To improve the typology and the accuracies of classi ers based on it, a better calibration of the activity axis is the most pressing task." ></td>
	<td class="line x" title="155:160	Our next steps are experiments aiming at re ning the translation of scores to normalized measures, so that individual affects can be distinguished within a single quadrant." ></td>
	<td class="line x" title="156:160	Other interesting avenues are studies investigating how well the typology can be ported to other textual data domains, the inclusion of a neutral tag, and the treatment of texts with multiple affects." ></td>
	<td class="line x" title="157:160	Finally, the domain of weblog posts is attractive because of the easy access to annotated data, but we have found through our experiments that the content is very noisy, annotation is not always consistent among bloggers, and therefore classi cation is dif cult." ></td>
	<td class="line x" title="158:160	We should not underestimate the positive effects that cleaner data, consistent tagging and access to bigger corpora would have on the accuracy of the classi er." ></td>
	<td class="line x" title="159:160	Acknowledgement This work was partially funded by the European Commission through the Network of Excellence EPOCH ( Excellence in Processing Open Cultural Heritage )." ></td>
	<td class="line x" title="160:160	Thanks to Peter Turney for the provision of access to the Waterloo MultiText System." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1613
Automatic Classification Of Citation Function
Teufel, Simone;Siddharthan, Advaith;Tidhar, Dan;"></td>
	<td class="line x" title="1:187	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 103110, Sydney, July 2006." ></td>
	<td class="line x" title="2:187	c2006 Association for Computational Linguistics Automatic classi cation of citation function Simone Teufel Advaith Siddharthan Dan Tidhar Natural Language and Information Processing Group Computer Laboratory Cambridge University, CB3 0FD, UK {Simone.Teufel,Advaith.Siddharthan,Dan.Tidhar}@cl.cam.ac.uk Abstract Citation function is de ned as the authors reason for citing a given paper (e.g. acknowledgement of the use of the cited method)." ></td>
	<td class="line x" title="3:187	The automatic recognition of the rhetorical function of citations in scienti c text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers." ></td>
	<td class="line x" title="4:187	We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features." ></td>
	<td class="line x" title="5:187	We nd, amongst other things, a strong relationship between citation function and sentiment classi cation." ></td>
	<td class="line x" title="6:187	1 Introduction Why do researchers cite a particular paper?" ></td>
	<td class="line x" title="7:187	This is a question that has interested researchers in discourse analysis, sociology of science, and information sciences (library sciences) for decades (Gar eld, 1979; Small, 1982; White, 2004)." ></td>
	<td class="line x" title="8:187	Many annotation schemes for citation motivation have been created over the years, and the question has been studied in detail, even to the level of in-depth interviews with writers about each individual citation (Hodges, 1972)." ></td>
	<td class="line x" title="9:187	Part of this sustained interest in citations can be explained by the fact that bibliometric metrics are commonly used to measure the impact of a researchers work by how often they are cited (Borgman, 1990; Luukkonen, 1992)." ></td>
	<td class="line x" title="10:187	However, researchers from the eld of discourse studies have long criticised purely quantitative citation analysis, pointing out that many citations are done out of politeness, policy or piety (Ziman, 1968), and that criticising citations or citations in passing should not count as much as central citations in a paper, or as those citations where a researchers work is used as the starting point of somebody elses work (Bonzi, 1982)." ></td>
	<td class="line x" title="11:187	A plethora of manual annotation schemes for citation motivation have been invented over the years (Gar eld, 1979; Hodges, 1972; Chubin and Moitra, 1975)." ></td>
	<td class="line x" title="12:187	Other schemes concentrate on citation function (Spiegel-Rcurrency1using, 1977; OConnor, 1982; Weinstock, 1971; Swales, 1990; Small, 1982))." ></td>
	<td class="line x" title="13:187	One of the best-known of these studies (Moravcsik and Murugesan, 1975) divides citations in running text into four dimensions: conceptual or operational use (i.e. , use of theory vs. use of technical method); evolutionary or juxtapositional (i.e. , own work is based on the cited work vs. own work is an alternative to it); organic or perfunctory (i.e. , work is crucially needed for understanding of citing article or just a general acknowledgement); and nally con rmative vs. negational (i.e. , is the correctness of the ndings disputed?)." ></td>
	<td class="line x" title="14:187	They found, for example, that 40% of the citations were perfunctory, which casts further doubt on the citationcounting approach." ></td>
	<td class="line x" title="15:187	Based on such annotation schemes and handanalyzed data, different in uences on citation behaviour can be determined." ></td>
	<td class="line x" title="16:187	Nevertheless, researchers in the eld of citation content analysis do not normally cross-validate their schemes with independent annotation studies with other human annotators, and usually only annotate a small number of citations (in the range of hundreds or thousands)." ></td>
	<td class="line x" title="17:187	Also, automated application of the annotation is not something that is generally considered in the eld, though White (2004) sees the future of discourse-analytic citation analysis in automation." ></td>
	<td class="line x" title="18:187	Apart from raw material for bibliometric studies, citations can also be used for search purposes in document retrieval applications." ></td>
	<td class="line x" title="19:187	In the library world, printed or electronic citation indexes such as ISI (Gar eld, 1979) serve as an orthogonal 103 Nitta and Niwa 94 Resnik 95 Brown et al. 90a Rose et al. 90 Church and Gale 91 Dagan et al. 94 Li and Abe 96 Hindle 93 Hindle 90 Dagan et al 93 Pereira et al. 93 Following Pereira et al, we measure word similarity by the relative entropy or Kulbach-Leibler (KL) distance, between the corresponding conditional distributions." ></td>
	<td class="line x" title="20:187	His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association." ></td>
	<td class="line x" title="21:187	Figure 1: A rhetorical citation map search tool to nd relevant papers, starting from a source paper of interest." ></td>
	<td class="line x" title="22:187	With the increased availability of documents in electronic form in recent years, citation-based search and automatic citation indexing have become highly popular, cf.the successful search tools Google Scholar and CiteSeer (Giles et al. , 1998).1 But not all search needs are ful lled by current citation indexers." ></td>
	<td class="line x" title="24:187	Experienced researchers are often interested in relations between articles (Shum, 1998)." ></td>
	<td class="line x" title="25:187	They want to know if a certain article criticises another and what the criticism is, or if the current work is based on that prior work." ></td>
	<td class="line x" title="26:187	This type of information is hard to come by with current search technology." ></td>
	<td class="line x" title="27:187	Neither the authors abstract, nor raw citation counts help users in assessing the relation between articles." ></td>
	<td class="line x" title="28:187	Fig." ></td>
	<td class="line x" title="29:187	1 shows a hypothetical search tool which displays differences and similarities between a target paper (here: Pereira et al. , 1993) and the papers that it cites and that cite it." ></td>
	<td class="line x" title="30:187	Contrastive links are shown in grey links to rival papers and papers the current paper contrasts itself to." ></td>
	<td class="line x" title="31:187	Continuative links are shown in black links to papers that use the methodology of the current paper." ></td>
	<td class="line x" title="32:187	Fig." ></td>
	<td class="line x" title="33:187	1 also displays the most characteristic textual sentence about each citation." ></td>
	<td class="line x" title="34:187	For instance, we can see which aspect of Hindle (1990) our example paper criticises, and in which way the example papers work was used by Dagan et al.(1994)." ></td>
	<td class="line x" title="36:187	Note that not even the CiteSeer text snippet 1These tools automatically citation-index all scienti c articles reached by a web-crawler, making them available to searchers via authors or keywords in the title, and displaying the citation in context of a text snippet." ></td>
	<td class="line x" title="37:187	can ful l the relation search need: it is always centered around the physical location of the citations, but the context is often not informative enough for the searcher to infer the relation." ></td>
	<td class="line x" title="38:187	In fact, studies from our annotated corpus (Teufel, 1999) show that 69% of the 600 sentences stating contrast with other work and 21% of the 246 sentences stating research continuation with other work do not contain the corresponding citation; the citation is found in preceding sentences (which means that the sentence expressing the contrast or continuation is outside the CiteSeer snippet)." ></td>
	<td class="line x" title="39:187	A more sophisticated, discourse-aware citation indexer which nds these sentences and associates them with the citation would add considerable value to the researchers bibliographic search (Ritchie et al. , 2006b)." ></td>
	<td class="line x" title="40:187	Our annotation scheme for citations is based on empirical work in content citation analysis." ></td>
	<td class="line x" title="41:187	It is designed for information retrieval applications such as improved citation indexing and better bibliometric measures (Teufel et al. , 2006)." ></td>
	<td class="line x" title="42:187	Its 12 categories mark relationships with other works." ></td>
	<td class="line x" title="43:187	Each citation is labelled with exactly one category." ></td>
	<td class="line x" title="44:187	The following top-level four-way distinction applies: Explicit statement of weakness Contrast or comparison with other work (4 categories) Agreement/usage/compatibility with other work (6 categories), and A neutral category." ></td>
	<td class="line x" title="45:187	In this paper, we show that the scheme can be reliably annotated by independent coders." ></td>
	<td class="line x" title="46:187	We also report results of a supervised machine learning experiment which replicates the human annotation." ></td>
	<td class="line x" title="47:187	2 An annotation scheme for citations Our scheme (given in Fig." ></td>
	<td class="line x" title="48:187	2) is adapted from that of Spiegel-Rcurrency1using (1977) after an analysis of a corpus of scienti c articles in computational linguistics." ></td>
	<td class="line x" title="49:187	We avoid sociologically orientated distinctions ( paying homage to pioneers ), as they can be dif cult to operationalise without deep knowledge of the eld and its participants (Swales, 1986)." ></td>
	<td class="line x" title="50:187	Our rede nition of the categories aims at reliably annotation; at the same time, the categories should be informative enough for the document management application sketched in the introduction." ></td>
	<td class="line x" title="51:187	104 Category Description Weak Weakness of cited approach CoCoGM Contrast/Comparison in Goals or Methods(neutral) CoCoAuthors work is stated to be superior to cited work CoCoR0 Contrast/Comparison in Results (neutral) CoCoXY Contrast between 2 cited methods PBas Author uses cited work as basis or starting point PUse Author uses tools/algorithms/data/de nitions PModi Author adapts or modi es tools/algorithms/data PMot This citation is positive about approach used or problem addressed (used to motivate work in current paper) PSim Authors work and cited work are similar PSup Authors work and cited work are compatible/provide support for each other Neut Neutral description of cited work, or not enough textual evidence for above categories, or unlisted citation function Figure 2: Annotation scheme for citation function." ></td>
	<td class="line x" title="52:187	Our categories are as follows: One category (Weak) is reserved for weakness of previous research, if it is addressed by the authors." ></td>
	<td class="line x" title="53:187	The next four categories describe comparisons or contrasts between own and other work." ></td>
	<td class="line x" title="54:187	The difference between them concerns whether the contrast is between methods employed or goals (CoCoGM), or results, and in the case of results, a difference is made between the cited results being worse than the current work (CoCo-), or comparable or better results (CoCoR0)." ></td>
	<td class="line x" title="55:187	As well as considering differences between the current work and other work, we also mark citations if they are explicitly compared and contrasted with other work (i.e. not the work in the current paper)." ></td>
	<td class="line x" title="56:187	This is expressed in category CoCoXY." ></td>
	<td class="line x" title="57:187	While this is not typically annotated in the literature, we expect a potential practical bene t of this category for our application, particularly in searches for differences and rival approaches." ></td>
	<td class="line x" title="58:187	The next set of categories we propose concerns positive sentiment expressed towards a citation, or a statement that the other work is actively used in the current work (which we consider the ultimate praise)." ></td>
	<td class="line x" title="59:187	We mark statements of use of data and methods of the cited work, differentiating unchanged use (PUse) from use with adaptations (PModi)." ></td>
	<td class="line x" title="60:187	Work which is stated as the explicit starting point or intellectual ancestry is marked with our category PBas." ></td>
	<td class="line x" title="61:187	If a claim in the literature is used to strengthen the authors argument, or vice versa, we assign the category PSup." ></td>
	<td class="line x" title="62:187	We also mark similarity of (an aspect of) the approach to the cited work (PSim), and motivation of approach used or problem addressed (PMot)." ></td>
	<td class="line x" title="63:187	Our twelfth category, Neut, bundles truly neutral descriptions of cited work with those cases where the textual evidence for a citation function was not enough to warrant annotation of that category, and all other functions for which our scheme did not provide a speci c category." ></td>
	<td class="line x" title="64:187	Citation function is hard to annotate because it in principle requires interpretation of author intentions (what could the authors intention have been in choosing a certain citation?)." ></td>
	<td class="line x" title="65:187	One of our most fundamental principles is thus to only mark explicitly signalled citation functions." ></td>
	<td class="line x" title="66:187	Our guidelines explicitly state that a general linguistic phrase such as better or used by us must be present; this increases the objectivity of de ning citation function." ></td>
	<td class="line x" title="67:187	Annotators must be able to point to textual evidence for assigning a particular function (and are asked to type the source of this evidence into the annotation tool for each citation)." ></td>
	<td class="line x" title="68:187	Categories are de ned in terms of certain objective types of statements (e.g. , there are 7 cases for PMot, e.g. Citation claims that or gives reasons for why problem Y is hard )." ></td>
	<td class="line x" title="69:187	Annotators can use general text interpretation principles when assigning the categories (such as anaphora resolution and parallel constructions), but are not allowed to use indepth knowledge of the eld or of the authors." ></td>
	<td class="line x" title="70:187	Guidelines (25 pages, 150 rules) describe the categories with examples, provide a decision tree and give decision aids in systematically ambiguous cases." ></td>
	<td class="line x" title="71:187	Nevertheless, subjective judgement of the annotators is still necessary to assign a single tag in an unseen context, because of the many difcult cases for annotation." ></td>
	<td class="line x" title="72:187	Some of these concern the fact that authors do not always state their purpose clearly." ></td>
	<td class="line x" title="73:187	For instance, several earlier studies found that negational citations are rare (Moravcsik and Murugesan, 1975; Spiegel-Rcurrency1using, 1977); MacRoberts and MacRoberts (1984) argue that the reason for this is that they are potentially politically dangerous." ></td>
	<td class="line x" title="74:187	In our data we found ample evidence of the meekness effect." ></td>
	<td class="line x" title="75:187	Other dif culties concern the distinction of the usage of a method from statements of similarity between a method and the own method (i.e. , the choice between categories PSim and PUse)." ></td>
	<td class="line x" title="76:187	This happens in cases where authors do not want to admit (or stress) 105 that they are using somebody elses method." ></td>
	<td class="line x" title="77:187	Another dif cult distinction concerns the judgement of whether the authors continue somebodys research (i.e. , consider their research as intellectual ancestry, i.e. PBas), or whether they simply use the work (PUse)." ></td>
	<td class="line x" title="78:187	The unit of annotation is a) the full citation (as recognised by our automatic citation processor on our corpus), and b) names of authors of cited papers anywhere in running text outside of a formal citation context (i.e. , without date)." ></td>
	<td class="line x" title="79:187	These latter are marked up, slightly unusually in comparison to other citation indexers, because we believe they function as important referents comparable in importance to formal citations.2 In principle, there are many other linguistic expressions by which the authors could refer to other peoples work: pronouns, abbreviations such as Mueller and Sag (1990), henceforth M & S, and names of approaches or theories which are associated with particular authors." ></td>
	<td class="line x" title="80:187	The fact that in these contexts citation function cannot be annotated (because it is not technically feasible to recognise them well enough) sometimes causes problems with context dependencies." ></td>
	<td class="line x" title="81:187	While there are unambiguous example cases where the citation function can be decided on the basis of the sentence alone, this is not always the case." ></td>
	<td class="line x" title="82:187	Most approaches are not criticised in the same sentence where they are also cited: it is more likely that there are several descriptive sentences about a cited approach between its formal citation and the evaluative statement, which is often at the end of the textual segment about this citation." ></td>
	<td class="line x" title="83:187	Nevertheless, the annotator must mark the function on the nearest appropriate annotation unit (citation or author name)." ></td>
	<td class="line x" title="84:187	Our rules decree that context is in most cases constrained to the paragraph boundary." ></td>
	<td class="line x" title="85:187	In rare cases, paper-wide information is required (e.g. , for PMot, we need to know that a praised approach is used by the authors, information which may not be local in the paragraph)." ></td>
	<td class="line x" title="86:187	Annotators are thus asked to skim-read the paper before annotation." ></td>
	<td class="line oc" title="87:187	One possible view on this annotation scheme could consider the rst two sets of categories as negative and the third set of categories positive, in the sense of Pang et al.(2002) and Turney (2002)." ></td>
	<td class="line x" title="89:187	Authors need to make a point (namely, 2Our citation processor can recognise these after parsing the citation list." ></td>
	<td class="line x" title="90:187	that they have contributed something which is better or at least new (Myers, 1992)), and they thus have a stance towards their citations." ></td>
	<td class="line x" title="91:187	But although there is a sentiment aspect to the interpretation of citations, this is not the whole story." ></td>
	<td class="line x" title="92:187	Many of our positive categories are more concerned with different ways in which the cited work is useful to the current work (which aspect of it is used, e.g., just a de nition or the entire solution?), and many of the contrastive statements have no negative connotation at all and simply state a (value-free) difference between approaches." ></td>
	<td class="line x" title="93:187	However, if one looks at the distribution of positive and negative adjectives around citations, it is clear that there is a nontrivial connection between our task and sentiment classi cation." ></td>
	<td class="line x" title="94:187	The data we use comes from our corpus of 360 conference articles in computational linguistics, drawn from the Computation and Language E-Print Archive (http://xxx.lanl.gov/cmp-lg)." ></td>
	<td class="line x" title="95:187	The articles are transformed into XML format; headlines, titles, authors and reference list items are automatically marked up." ></td>
	<td class="line x" title="96:187	Reference lists are parsed using regular patterns, and cited authors names are identi ed." ></td>
	<td class="line x" title="97:187	Our citation parser then nds citations and author names in running text and marks them up." ></td>
	<td class="line x" title="98:187	Ritchie et al.(2006a) report high accuracy for this task (94% of citations recognised, provided the reference list was error-free)." ></td>
	<td class="line x" title="100:187	On average, our papers contain 26.8 citation instances in running text3." ></td>
	<td class="line x" title="101:187	For human annotation, we use our own annotation tool based on XML/XSLT technology, which allows us to use a web browser to interactively assign one of the 12 tags (presented as a pull-down list) to each citation." ></td>
	<td class="line x" title="102:187	We measure inter-annotator agreement between three annotators (the three authors), who independently annotated 26 articles with the scheme (containing a total of 120,000 running words and 548 citations), using the written guidelines." ></td>
	<td class="line x" title="103:187	The guidelines were developed on a different set of articles from the ones used for annotation." ></td>
	<td class="line x" title="104:187	Inter-annotator agreement was Kappa=.72 (n=12;N=548;k=3)4." ></td>
	<td class="line x" title="105:187	This is quite high, considering the number of categories and the dif culties 3As opposed to reference list items, which are fewer." ></td>
	<td class="line x" title="106:187	4Following Carletta (1996), we measure agreement in Kappa, which follows the formula K = P(A)P(E)1P(E) where P(A) is observed, and P(E) expected agreement." ></td>
	<td class="line x" title="107:187	Kappa ranges between -1 and 1." ></td>
	<td class="line x" title="108:187	K=0 means agreement is only as expected by chance." ></td>
	<td class="line x" title="109:187	Generally, Kappas of 0.8 are considered stable, and Kappas of.69 as marginally stable, according to the strictest scheme applied in the eld." ></td>
	<td class="line x" title="110:187	106 (e.g. , non-local dependencies) of the task." ></td>
	<td class="line x" title="111:187	The relative frequency of each category observed in the annotation is listed in Fig." ></td>
	<td class="line x" title="112:187	3." ></td>
	<td class="line x" title="113:187	As expected, the distribution is very skewed, with more than 60% of the citations of category Neut.5 What is interesting is the relatively high frequency of usage categories (PUse, PModi, PBas) with a total of 18.9%." ></td>
	<td class="line x" title="114:187	There is a relatively low frequency of clearly negative citations (Weak, CoCo-, total of 4.1%), whereas the neutral contrastive categories (CoCoR0, CoCoXY, CoCoGM) are slightly more frequent at 7.6%." ></td>
	<td class="line x" title="115:187	This is in concordance with earlier annotation experiments (Moravcsik and Murugesan, 1975; Spiegel-Rcurrency1using, 1977)." ></td>
	<td class="line x" title="116:187	3 Features for automatic recognition of citation function This section summarises the features we use for machine learning citation function." ></td>
	<td class="line x" title="117:187	Some of these features were previously found useful for a different application, namely Argumentative Zoning (Teufel, 1999; Teufel and Moens, 2002), some are speci c to citation classi cation." ></td>
	<td class="line x" title="118:187	3.1 Cue phrases Myers (1992) calls meta-discourse the set of expressions that talk about the act of presenting research in a paper, rather than the research itself (which is called object-level discourse)." ></td>
	<td class="line x" title="119:187	For instance, Swales (1990) names phrases such as to our knowledge, no." ></td>
	<td class="line x" title="120:187	. . or As far as we aware as meta-discourse associated with a gap in the current literature." ></td>
	<td class="line x" title="121:187	Strings such as these have been used in extractive summarisation successfully ever since Paices (1981) work." ></td>
	<td class="line x" title="122:187	We model meta-discourse (cue phrases) and treat it differently from object-level discourse." ></td>
	<td class="line x" title="123:187	There are two different mechanisms: A nite grammar over strings with a placeholder mechanism for POS and for sets of similar words which can be substituted into a string-based cue phrase (Teufel, 1999)." ></td>
	<td class="line x" title="124:187	The grammar corresponds to 1762 cue phrases." ></td>
	<td class="line x" title="125:187	It was developed on 80 papers which are different to the papers used for our experiments here." ></td>
	<td class="line x" title="126:187	The other mechanism is a POS-based recogniser of agents and a recogniser for speci c actions these agents perform." ></td>
	<td class="line x" title="127:187	Two main agent types (the 5Spiegel-Rcurrency1using found that out of 2309 citations she examined, 80% substantiated statements." ></td>
	<td class="line x" title="128:187	authors of the paper, and everybody else) are modelled by 185 patterns." ></td>
	<td class="line x" title="129:187	For instance, in a paragraph describing related work, we expect to nd references to other people in subject position more often than in the section detailing the authors own methods, whereas in the background section, we often nd general subjects such as researchers in computational linguistics or in the literature . For each sentence to be classi ed, its grammatical subject is determined by POS patterns and, if possible, classi ed as one of these agent types." ></td>
	<td class="line x" title="130:187	We also use the observation that in sentences without meta-discourse, one can assume that agenthood has not changed." ></td>
	<td class="line x" title="131:187	20 different action types model the main verbs involved in meta-discourse." ></td>
	<td class="line x" title="132:187	For instance, there is a set of verbs that is often used when the overall scienti c goal of a paper is de ned." ></td>
	<td class="line x" title="133:187	These are the verbs of presentation, such as propose, present, report and suggest ; in the corpus we found other verbs in this function, but with a lower frequency, namely describe, discuss, give, introduce, put forward, show, sketch, state and talk about . There are also specialised verb clusters which co-occur with PBas sentences, e.g., the cluster of continuation of ideas (eg." ></td>
	<td class="line x" title="134:187	adopt, agree with, base, be based on, be derived from, be originated in, be inspired by, borrow, build on,." ></td>
	<td class="line x" title="135:187	On the other hand, the semantics of verbs in Weak sentences is often concerned with failing (of other researchers approaches), and often contain verbs such as abound, aggravate, arise, be cursed, be incapable of, be forced to, be limited to, . . ." ></td>
	<td class="line x" title="136:187	We use 20 manually acquired verb clusters." ></td>
	<td class="line x" title="137:187	Negation is recognised, but too rare to de ne its own clusters: out of the 20 2 = 40 theoretically possible verb clusters, only 27 were observed in our development corpus." ></td>
	<td class="line x" title="138:187	We have recently automated the process of verb object pair acquisition from corpora for two types of cue phrases (Abdalla and Teufel, 2006) and are planning on expanding this work to other cue phrases." ></td>
	<td class="line x" title="139:187	3.2 Cues Identi ed by annotators During the annotator training phase, the annotators were encouraged to type in the metadescription cue phrases that justify their choice of category." ></td>
	<td class="line x" title="140:187	We went through this list by hand and extracted 892 cue phrases (around 75 per category)." ></td>
	<td class="line x" title="141:187	The les these cues came from were not part of the test corpus." ></td>
	<td class="line x" title="142:187	We included 12 features 107 Neut PUse CoCoGM PSim Weak PMot CoCoR0 PBas CoCoXY CoCoPModi PSup 62.7% 15.8% 3.9% 3.8% 3.1% 2.2% 0.8% 1.5% 2.9% 1.0% 1.6% 1.1% Figure 3: Distribution of citation categories Weak CoCoGM CoCoR0 CoCoCoCoXY PBas PUse PModi PMot PSim PSup Neut P .78 .81 .77 .56 .72 .76 .66 .60 .75 .68 .83 .80 R .49 .52 .46 .19 .54 .46 .61 .27 .64 .38 .32 .92 F .60 .64 .57 .28 .62 .58 .63 .37 .69 .48 .47 .86 Percentage Accuracy 0.77 Kappa (n=12; N=2829; k=2) 0.57 Macro-F 0.57 Figure 4: Summary of Citation Analysis results (10-fold cross-validation; IBk algorithm; k=3)." ></td>
	<td class="line x" title="143:187	that recorded the presence of cues that our annotators associated with a particular class." ></td>
	<td class="line x" title="144:187	3.3 Other features There are other features which we use for this task." ></td>
	<td class="line x" title="145:187	We know from Teufel and Moens (2002) that verb tense and voice should be useful for recognizing statements of previous work, future work and work performed in the paper." ></td>
	<td class="line x" title="146:187	We also recognise modality (whether or not a main verb is modi ed by an auxiliary, and which auxiliary it is)." ></td>
	<td class="line x" title="147:187	The overall location of a sentence containing a reference should be relevant." ></td>
	<td class="line x" title="148:187	We observe that more PMot categories appear towards the beginning of the paper, as do Weak citations, whereas comparative results (CoCoR0, CoCoR-) appear towards the end of articles." ></td>
	<td class="line x" title="149:187	More ne-grained location features, such as the location within the paragraph and the section, have also been implemented." ></td>
	<td class="line x" title="150:187	The fact that a citation points to own previous work can be recognised, as we know who the paper authors are." ></td>
	<td class="line x" title="151:187	As we have access to the information in the reference list, we also know the last names of all cited authors (even in the case where an et al. statement in running text obscures the later-occurring authors)." ></td>
	<td class="line x" title="152:187	With self-citations, one might assume that the probability of re-use of material from previous own work should be higher, and the tendency to criticise lower." ></td>
	<td class="line x" title="153:187	4 Results Our evaluation corpus for citation analysis consists of 116 articles (randomly drawn from the part of our corpus which was not used for guideline development or cue phrase acquisition)." ></td>
	<td class="line x" title="154:187	The 116 articles contain 2829 citation instances." ></td>
	<td class="line x" title="155:187	Each citation instance was manually tagged as one Weakness Positive Contrast Neutral P .80 .75 .77 .81 R .49 .65 .52 .90 F .61 .70 .62 .86 Percentage Accuracy 0.79 Kappa (n=12; N=2829; k=2) 0.59 Macro-F 0.68 Figure 5: Summary of results (10-fold crossvalidation; IBk algorithm; k=3): Top level classes." ></td>
	<td class="line x" title="156:187	Weakness Positive Neutral P .77 .75 .85 R .42 .65 .92 F .54 .70 .89 Percentage Accuracy 0.83 Kappa (n=12; N=2829; k=2) 0.58 Macro-F 0.71 Figure 6: Summary of results (10-fold crossvalidation; IBk algorithm; k=3): Sentiment Analysis." ></td>
	<td class="line x" title="157:187	of fWeak, CoCoGM, CoCo-, CoCoR0, CoCoXY, PBas, PUse,PModi,PMot,PSim,PSup,Neutg." ></td>
	<td class="line x" title="158:187	The papers are then further processed (e.g. tokenised and POS-tagged)." ></td>
	<td class="line x" title="159:187	All other features are automatically determined (e.g. self-citations are detected by overlap of citing and cited authors); then, machine learning is applied to the feature vectors." ></td>
	<td class="line x" title="160:187	The 10-fold cross-validation results for citation classi cation are given in Figure 4, comparing the system to one of the annotators." ></td>
	<td class="line x" title="161:187	Results are given in three overall measures: Kappa, percentage accuracy, and Macro-F (following Lewis (1991))." ></td>
	<td class="line x" title="162:187	Macro-F is the mean of the F-measures of all twelve categories." ></td>
	<td class="line x" title="163:187	We use Macro-F and Kappa because we want to measure success particularly on the rare categories, and because Micro-averaging techniques like percentage accuracy tend to overestimate the contribution of frequent categories in 108 heavily skewed distributions like ours6." ></td>
	<td class="line x" title="164:187	In the case of Macro-F, each category is treated as one unit, independent of the number of items contained in it." ></td>
	<td class="line x" title="165:187	Therefore, the classi cation success of the individual items in rare categories is given more importance than classi cation success of frequent category items." ></td>
	<td class="line x" title="166:187	However, one should keep in mind that numerical values in macro-averaging are generally lower (Yang and Liu, 1999), due to fewer training cases for the rare categories." ></td>
	<td class="line x" title="167:187	Kappa has the additional advantage over Macro-F that it lters out random agreement (random use, but following the observed distribution of categories)." ></td>
	<td class="line x" title="168:187	For our task, memory-based learning outperformed other models." ></td>
	<td class="line x" title="169:187	The reported results use the IBk algorithm with k = 3 (we used the Weka machine learning toolkit (Witten and Frank, 2005) for our experiments)." ></td>
	<td class="line x" title="170:187	Fig." ></td>
	<td class="line x" title="171:187	7 provides a few examples from one le in the corpus, along with the gold standard citation class, the machine prediction, and a comment." ></td>
	<td class="line x" title="172:187	Kappa is even higher for the top level distinction." ></td>
	<td class="line x" title="173:187	We collapsed the obvious similar categories (all P categories into one category, and all CoCo categories into another) to give four top level categories (Weak, Positive, Contrast, Neutral; results in Fig." ></td>
	<td class="line x" title="174:187	5)." ></td>
	<td class="line x" title="175:187	Precision for all the categories is above 0.75, and K=0.59." ></td>
	<td class="line x" title="176:187	For contrast, the human agreement for this situation was K=0.76 (n=3,N=548,k=3)." ></td>
	<td class="line x" title="177:187	In a different experiment, we grouped the categories as follows, in an attempt to perform sentiment analysis over the classi cations: Old Categories New Category Weak, CoCoNegative PMot, PUse, PBas, PModi, PSim, PSup Positive CoCoGM, CoCoR0, CoCoXY, Neut Neutral Thus negative contrasts and weaknesses are grouped into Negative, while neutral contrasts are grouped into Neutral." ></td>
	<td class="line x" title="178:187	All positive classes are con ated into Positive." ></td>
	<td class="line x" title="179:187	Results show that this grouping raises results to a smaller degree than the top-level distinction did (to K=.58)." ></td>
	<td class="line x" title="180:187	For contrast, the human agreement for these collapsed categories was K=.75 (n=3,N=548,k=3)." ></td>
	<td class="line x" title="181:187	6This situation has parallels in information retrieval, where precision and recall are used because accuracy overestimates the performance on irrelevant items." ></td>
	<td class="line x" title="182:187	5 Conclusion We have presented a new task: annotation of citation function in scienti c text, a phenomenon which we believe to be closely related to the overall discourse structure of scienti c articles." ></td>
	<td class="line x" title="183:187	Our annotation scheme concentrates on weaknesses of other work, and on similarities and contrast between work and usage of other work." ></td>
	<td class="line x" title="184:187	In this paper, we present machine learning experiments for replicating the human annotation (which is reliable at K=.72)." ></td>
	<td class="line x" title="185:187	The automatic result reached K=.57 (acc=.77) for the full annotation scheme; rising to Kappa=.58 (acc=.83) for a three-way classi cation (Weak, Positive, Neutral)." ></td>
	<td class="line x" title="186:187	We are currently performing an experiment to see if citation processing can increase performance in a large-scale, real-world information retrieval task, by creating a test collection of researchers queries and relevant documents for these (Ritchie et al. , 2006a)." ></td>
	<td class="line x" title="187:187	6 Acknowledgements This work was funded by the EPSRC projects CITRAZ (GR/S27832/01, Rhetorical Citation Maps and Domain-independent Argumentative Zoning ) and SCIBORG (EP/C010035/1, Extracting the Science from Scienti c Publications )." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1639
Get Out The Vote: Determining Support Or Opposition From Congressional Floor-Debate Transcripts
Thomas, Matt;Pang, Bo;Lee, Lillian;"></td>
	<td class="line x" title="1:176	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327335, Sydney, July 2006." ></td>
	<td class="line x" title="2:176	c2006 Association for Computational Linguistics Get out the vote: Determining support or opposition from Congressional floor-debate transcripts Matt Thomas, Bo Pang, and Lillian Lee Department of Computer Science, Cornell University Ithaca, NY 14853-7501 mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu Abstract We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation." ></td>
	<td class="line x" title="3:176	To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another." ></td>
	<td class="line x" title="4:176	We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation." ></td>
	<td class="line x" title="5:176	1 Introduction One ought to recognize that the present political chaos is connected with the decay of language, and that one can probably bring about some improvement by starting at the verbal end." ></td>
	<td class="line x" title="6:176	 Orwell, Politics and the English language We have entered an era where very large amounts of politically oriented text are now available online." ></td>
	<td class="line x" title="7:176	This includes both official documents, such as the full text of laws and the proceedings of legislative bodies, and unofficial documents, such as postings on weblogs (blogs) devoted to politics." ></td>
	<td class="line x" title="8:176	In some sense, the availability of such data is simply a manifestation of a general trend of everybody putting their records on the Internet.1 The 1It is worth pointing out that the United States Library of Congress was an extremely early adopter of Web technology: the THOMAS database (http://thomas.loc.gov) of congresonline accessibility of politically oriented texts in particular, however, is a phenomenon that some have gone so far as to say will have a potentially society-changing effect." ></td>
	<td class="line x" title="9:176	In the United States, for example, governmental bodies are providing and soliciting political documents via the Internet, with lofty goals in mind: electronic rulemaking (eRulemaking) initiatives involving the electronic collection, distribution, synthesis, and analysis of public commentary in the regulatory rulemaking process, may [alter] the citizen-government relationship (Shulman and Schlosberg, 2002)." ></td>
	<td class="line x" title="10:176	Additionally, much media attention has been focused recently on the potential impact that Internet sites may have on politics2, or at least on political journalism3." ></td>
	<td class="line x" title="11:176	Regardless of whether one views such claims as clear-sighted prophecy or mere hype, it is obviously important to help people understand and analyze politically oriented text, given the importance of enabling informed participation in the political process." ></td>
	<td class="line x" title="12:176	Evaluative and persuasive documents, such as a politicians speech regarding a bill or a bloggers commentary on a legislative proposal, form a particularly interesting type of politically oriented text." ></td>
	<td class="line x" title="13:176	People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion, given the dense nature of legislative language and the fact that (U.S)." ></td>
	<td class="line x" title="14:176	bills often reach several hundred pages in length (Smith et al. , 2005)." ></td>
	<td class="line x" title="15:176	Moreover, political opinions are exsional bills and related data was launched in January 1995, when Mosaic was not quite two years old and Altavista did not yet exist." ></td>
	<td class="line x" title="16:176	2E.g., Internet injects sweeping change into U.S. politics, Adam Nagourney, The New York Times, April 2, 2006." ></td>
	<td class="line x" title="17:176	3E.g., The End of News?, Michael Massing, The New York Review of Books, December 1, 2005." ></td>
	<td class="line x" title="18:176	327 plicitly solicited in the eRulemaking scenario." ></td>
	<td class="line x" title="19:176	In the analysis of evaluative language, it is fundamentally necessary to determine whether the author/speaker supports or disapproves of the topic of discussion." ></td>
	<td class="line x" title="20:176	In this paper, we investigate the following specific instantiation of this problem: we seek to determine from the transcripts of U.S. Congressional floor debates whether each speech (continuous single-speaker segment of text) represents support for or opposition to a proposed piece of legislation." ></td>
	<td class="line x" title="21:176	Note that from an experimental point of view, this is a very convenient problem to work with because we can automatically determine ground truth (and thus avoid the need for manual annotation) simply by consulting publicly available voting records." ></td>
	<td class="line x" title="22:176	Task properties Determining whether or not a speaker supports a proposal falls within the realm of sentiment analysis, an extremely active research area devoted to the computational treatment of subjective or opinion-oriented language (early work includes Wiebe and Rapaport (1988), Hearst (1992), Sack (1994), and Wiebe (1994); see Esuli (2006) for an active bibliography)." ></td>
	<td class="line oc" title="23:176	In particular, since we treat each individual speech within a debate as a single document, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al. , 2002; Turney, 2002; Dave et al. , 2003)." ></td>
	<td class="line o" title="24:176	Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently." ></td>
	<td class="line x" title="25:176	A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="26:176	Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions." ></td>
	<td class="line x" title="27:176	For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information." ></td>
	<td class="line x" title="28:176	For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data." ></td>
	<td class="line x" title="29:176	Indeed, in other settings (e.g. , a movie-discussion listserv) one may not be able to determine the participants political leanings, and such information may not lead to significantly improved results even if it were available." ></td>
	<td class="line x" title="30:176	tween two speakers, such as explicit assertions (I second that!) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf.Agrawal et al.(2003))." ></td>
	<td class="line x" title="33:176	Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text." ></td>
	<td class="line x" title="34:176	Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually." ></td>
	<td class="line x" title="35:176	Intuition suggests that this is true of the data with which we experiment, for several reasons." ></td>
	<td class="line x" title="36:176	First, U.S. congressional debates contain very rich language and cover an extremely wide variety of topics, ranging from flag burning to international policy to the federal budget." ></td>
	<td class="line x" title="37:176	Debates are also subject to digressions, some fairly natural and others less so (e.g. , Why are we discussing this bill when the plight of my constituents regarding this other issue is being ignored?) Second, an important characteristic of persuasive language is that speakers may spend more time presenting evidence in support of their positions (or attacking the evidence presented by others) than directly stating their attitudes." ></td>
	<td class="line x" title="38:176	An extreme example will illustrate the problems involved." ></td>
	<td class="line x" title="39:176	Consider a speech that describes the U.S. flag as deeply inspirational, and thus contains only positive language." ></td>
	<td class="line x" title="40:176	If the bill under discussion is a proposed flag-burning ban, then the speech is supportive; but if the bill under discussion is aimed at rescinding an existing flag-burning ban, the speech may represent opposition to the legislation." ></td>
	<td class="line x" title="41:176	Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion." ></td>
	<td class="line x" title="42:176	Qualitative summary of results The above difficulties underscore the importance of enhancing standard classification techniques with new information sources that promise to improve accuracy, such as inter-document relationships between the documents to be labeled." ></td>
	<td class="line x" title="43:176	In this paper, we demonstrate that the incorporation of agreement modeling can provide substantial improvements over the application of support vector machines (SVMs) in isolation, which represents the state of the art in the individual classification of documents." ></td>
	<td class="line x" title="44:176	The enhanced accuracies are obtained via a fairly primitive automatically-acquired agreement detector 328 total train test development speech segments 3857 2740 860 257 debates 53 38 10 5 average number of speech segments per debate 72.8 72.1 86.0 51.4 average number of speakers per debate 32.1 30.9 41.1 22.6 Table 1: Corpus statistics." ></td>
	<td class="line x" title="45:176	and a conceptually simple method for integrating isolated-document and agreement-based information." ></td>
	<td class="line x" title="46:176	We thus view our results as demonstrating the potentially large benefits of exploiting sentiment-related discourse-segment relationships in sentiment-analysis tasks." ></td>
	<td class="line x" title="47:176	2 Corpus This section outlines the main steps of the process by which we created our corpus (download site: www.cs.cornell.edu/home/llee/data/convote.html)." ></td>
	<td class="line x" title="48:176	GovTrack (http://govtrack.us) is an independent website run by Joshua Tauberer that collects publicly available data on the legislative and fundraising activities of U.S. congresspeople." ></td>
	<td class="line x" title="49:176	Due to its extensive cross-referencing and collating of information, it was nominated for a 2006 Webby award." ></td>
	<td class="line x" title="50:176	A crucial characteristic of GovTrack from our point of view is that the information is provided in a very convenient format; for instance, the floor-debate transcripts are broken into separate HTML files according to the subject of the debate, so we can trivially derive long sequences of speeches guaranteed to cover the same topic." ></td>
	<td class="line x" title="51:176	We extracted from GovTrack all available transcripts of U.S. floor debates in the House of Representatives for the year 2005 (3268 pages of transcripts in total), together with voting records for all roll-call votes during that year." ></td>
	<td class="line x" title="52:176	We concentrated on debates regarding controversial bills (ones in which the losing side generated at least 20% of the speeches) because these debates should presumably exhibit more interesting discourse structure." ></td>
	<td class="line x" title="53:176	Each debate consists of a series of speech segments, where each segment is a sequence of uninterrupted utterances by a single speaker." ></td>
	<td class="line x" title="54:176	Since speech segments represent natural discourse units, we treat them as the basic unit to be classified." ></td>
	<td class="line x" title="55:176	Each speech segment was labeled by the vote (yea or nay) cast for the proposed bill by the person who uttered the speech segment." ></td>
	<td class="line x" title="56:176	We automatically discarded those speech segments belonging to a class of formulaic, generally one-sentence utterances focused on the yielding of time on the house floor (for example, Madam Speaker, I am pleased to yield 5 minutes to the gentleman from Massachusetts), as such speech segments are clearly off-topic." ></td>
	<td class="line x" title="57:176	We also removed speech segments containing the term amendment, since we found during initial inspection that these speeches generally reflect a speakers opinion on an amendment, and this opinion may differ from the speakers opinion on the underlying bill under discussion." ></td>
	<td class="line x" title="58:176	We randomly split the data into training, test, and development (parameter-tuning) sets representing roughly 70%, 20%, and 10% of our data, respectively (see Table 1)." ></td>
	<td class="line x" title="59:176	The speech segments remained grouped by debate, with 38 debates assigned to the training set, 10 to the test set, and 5 to the development set; we require that the speech segments from an individual debate all appear in the same set because our goal is to examine classification of speech segments in the context of the surrounding discussion." ></td>
	<td class="line x" title="60:176	3 Method The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation." ></td>
	<td class="line x" title="61:176	As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate." ></td>
	<td class="line x" title="62:176	Our classification framework, directly inspired by Blum and Chawla (2001), integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label." ></td>
	<td class="line x" title="63:176	In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships." ></td>
	<td class="line x" title="64:176	329 3.1 Classification framework Let s1,s2,,sn be the sequence of speech segments within a given debate, and let Y and N stand for the yea and nay class, respectively." ></td>
	<td class="line x" title="65:176	Assume we have a non-negative function ind(s,C) indicating the degree of preference that an individual-document classifier, such as an SVM, has for placing speech-segment s in class C. Also, assume that some pairs of speech segments have weighted links between them, where the non-negative strength (weight) str(lscript) for a link lscript indicates the degree to which it is preferable that the linked speech segments receive the same label." ></td>
	<td class="line x" title="66:176	Then, any class assignment c = c(s1),c(s2),,c(sn) can be assigned a cost summationdisplay s ind(s,c(s))+ summationdisplay s,sprime:c(s)negationslash=c(sprime) summationdisplay lscript betweens,sprime str(lscript), where c(s) is the opposite class from c(s)." ></td>
	<td class="line x" title="67:176	A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes." ></td>
	<td class="line x" title="68:176	As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005), the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs." ></td>
	<td class="line x" title="69:176	In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision." ></td>
	<td class="line x" title="70:176	3.2 Classifying speech segments in isolation In our experiments, we employed the well-known classifier SVMlight to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features.5 Following standard practice in sentiment analysis (Pang et al. , 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors." ></td>
	<td class="line x" title="71:176	The ind value 5SVMlight is available at svmlight.joachims.org." ></td>
	<td class="line x" title="72:176	Default parameters were used, although experimentation with different parameter settings is an important direction for future work (Daelemans and Hoste, 2002; Munson et al. , 2005)." ></td>
	<td class="line x" title="73:176	for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane: ind(s,Y) def=    1 d(s) > 2s;parenleftBig 1+ d(s)2s parenrightBig /2 |d(s)| 2s; 0 d(s) < 2s where s is the standard deviation of d(s) over all speech segments s in the debate in question, and ind(s,N) def= 1 ind(s,Y)." ></td>
	<td class="line x" title="74:176	We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class." ></td>
	<td class="line x" title="75:176	3.3 Relationships between speech segments A wide range of relationships between text segments can be modeled as positive-strength links." ></td>
	<td class="line x" title="76:176	Here we discuss two types of constraints that are considered in this work." ></td>
	<td class="line x" title="77:176	Same-speaker constraints: In Congressional debates and in general social-discourse contexts, a single speaker may make a number of comments regarding a topic." ></td>
	<td class="line x" title="78:176	It is reasonable to expect that in many settings, the participants in a discussion may be convinced to change their opinions midway through a debate." ></td>
	<td class="line x" title="79:176	Hence, in the general case we wish to be able to express soft preferences for all of an authors statements to receive the same label, where the strengths of such constraints could, for instance, vary according to the time elapsed between the statements." ></td>
	<td class="line x" title="80:176	Weighted links are an appropriate means to express such variation." ></td>
	<td class="line x" title="81:176	However, if we assume that most speakers do not change their positions in the course of a discussion, we can conclude that all comments made by the same speaker must receive the same label." ></td>
	<td class="line x" title="82:176	This assumption holds by fiat for the ground-truth labels in our dataset because these labels were derived from the single vote cast by the speaker on the bill being discussed.6 We can implement this assumption via links whose weights are essentially infinite." ></td>
	<td class="line x" title="83:176	Although one can also implement this assumption via concatenation of same-speaker speech segments (see Section 4.3), we view the fact that our graph-based framework incorporates 6We are attempting to determine whether a speech segment represents support or not." ></td>
	<td class="line x" title="84:176	This differs from the problem of determining what the speakers actual opinion is, a problem that, as an anonymous reviewer put it, is complicated by grandstanding, backroom deals, or, more innocently, plain change of mind (I voted for it before I voted against it)." ></td>
	<td class="line x" title="85:176	330 both hard and soft constraints in a principled fashion as an advantage of our approach." ></td>
	<td class="line x" title="86:176	Different-speaker agreements In House discourse, it is common for one speaker to make reference to another in the context of an agreement or disagreement over the topic of discussion." ></td>
	<td class="line x" title="87:176	The systematic identification of instances of agreement can, as we have discussed, be a powerful tool for the development of intelligently selected weights for links between speech segments." ></td>
	<td class="line x" title="88:176	The problem of agreement identification can be decomposed into two sub-problems: identifying references and their targets, and deciding whether each reference represents an instance of agreement." ></td>
	<td class="line x" title="89:176	In our case, the first task is straightforward because we focused solely on by-name references.7 Hence, we will now concentrate on the second, more interesting task." ></td>
	<td class="line x" title="90:176	We approach the problem of classifying references by representing each reference with a wordpresence vector derived from a window of text surrounding the reference.8 In the training set, we classify each reference connecting two speakers with a positive or negative label depending on whether the two voted the same way on the bill under discussion9." ></td>
	<td class="line x" title="91:176	These labels are then used to train an SVM classifier, the output of which is subsequently used to create weights on agreement links in the test set as follows." ></td>
	<td class="line x" title="92:176	Let d(r) denote the distance from the vector representing reference r to the agreement-detector SVMs decision plane, and let r be the standard deviation of d(r) over all references in the debate in question." ></td>
	<td class="line x" title="93:176	We then define the strength agr of the agreement link corresponding to the reference as: agr(r) def=    0 d(r) < agr; d(r)/4r agr  d(r)  4r;  d(r) > 4r. The free parameter  specifies the relative impor7One subtlety is that for the purposes of mining agreement cues (but not for evaluating overall support/oppose classification accuracy), we temporarily re-inserted into our dataset previously filtered speech segments containing the term yield, since the yielding of time on the House floor typically indicates agreement even though the yield statements contain little relevant text on their own." ></td>
	<td class="line x" title="94:176	8We found good development-set performance using the 30 tokens before, 20 tokens after, and the name itself." ></td>
	<td class="line x" title="95:176	9Since we are concerned with references that potentially represent relationships between speech segments, we ignore references for which the target of the reference did not speak in the debate in which the reference was made." ></td>
	<td class="line x" title="96:176	Agreement classifier (referenceagreement?) Devel." ></td>
	<td class="line x" title="97:176	set Test set majority baseline 81.51 80.26 Train: no amdmts; agr = 0 84.25 81.07 Train: with amdmts; agr = 0 86.99 80.10 Table 2: Agreement-classifier accuracy, in percent." ></td>
	<td class="line x" title="98:176	Amdmts=speech segments containing the word amendment." ></td>
	<td class="line x" title="99:176	Recall that boldface indicates results for development-set-optimal settings." ></td>
	<td class="line x" title="100:176	tance of the agr scores." ></td>
	<td class="line x" title="101:176	The threshold agr controls the precision of the agreement links, in that values of agr greater than zero mean that greater confidence is required before an agreement link can be added.10 4 Evaluation This section presents experiments testing the utility of using speech-segment relationships, evaluating against a number of baselines." ></td>
	<td class="line x" title="102:176	All reported results use values for the free parameter  derived via tuning on the development set." ></td>
	<td class="line x" title="103:176	In the tables, boldface indicates the developmentand test-set results for the development-set-optimal parameter settings, as one would make algorithmic choices based on development-set performance." ></td>
	<td class="line x" title="104:176	4.1 Preliminaries: Reference classification Recall that to gather inter-speaker agreement information, the strategy employed in this paper is to classify by-name references to other speakers as to whether they indicate agreement or not." ></td>
	<td class="line x" title="105:176	To train our agreement classifier, we experimented with undoing the deletion of amendmentrelated speech segments in the training set." ></td>
	<td class="line x" title="106:176	Note that such speech segments were never included in the development or test set, since, as discussed in Section 2, their labels are probably noisy; however, including them in the training set allows the classifier to examine more instances even though some of them are labeled incorrectly." ></td>
	<td class="line x" title="107:176	As Table 2 shows, using more, if noisy, data yields better agreement-classification results on the development set, and so we use that policy in all subsequent experiments.11 10Our implementation puts a link between just one arbitrary pair of speech segments among all those uttered by a given pair of apparently agreeing speakers." ></td>
	<td class="line x" title="108:176	The infiniteweight same-speaker links propagate the agreement information to all other such pairs." ></td>
	<td class="line x" title="109:176	11Unfortunately, this policy leads to inferior test-set agree331 Agreement classifier Precision (in percent): Devel." ></td>
	<td class="line x" title="110:176	set Test set agr = 0 86.23 82.55 agr =  89.41 88.47 Table 3: Agreement-classifier precision." ></td>
	<td class="line x" title="111:176	An important observation is that precision may be more important than accuracy in deciding which agreement links to add: false positives with respect to agreement can cause speech segments to be incorrectly assigned the same label, whereas false negatives mean only that agreement-based information about other speech segments is not employed." ></td>
	<td class="line x" title="112:176	As described above, we can raise agreement precision by increasing the threshold agr, which specifies the required confidence for the addition of an agreement link." ></td>
	<td class="line x" title="113:176	Indeed, Table 3 shows that we can improve agreement precision by setting agr to the (positive) mean agreement score  assigned by the SVM agreement-classifier over all references in the given debate12." ></td>
	<td class="line x" title="114:176	However, this comes at the cost of greatly reducing agreement accuracy (development: 64.38%; test: 66.18%) due to lowered recall levels." ></td>
	<td class="line x" title="115:176	Whether or not better speech-segment classification is ultimately achieved is discussed in the next sections." ></td>
	<td class="line x" title="116:176	4.2 Segment-based speech-segment classification Baselines The first two data rows of Table 4 depict baseline performance results." ></td>
	<td class="line x" title="117:176	The #(support)  #(oppos) baseline is meant to explore whether the speech-segment classification task can be reduced to simple lexical checks." ></td>
	<td class="line x" title="118:176	Specifically, this method uses the signed difference between the number of words containing the stem support and the number of words containing the stem oppos (returning the majority class if the difference is 0)." ></td>
	<td class="line x" title="119:176	No better than 62.67% testset accuracy is obtained by either baseline." ></td>
	<td class="line x" title="120:176	Using relationship information Applying an SVM to classify each speech segment in isolation leads to clear improvements over the two baseline methods, as demonstrated in Table 4." ></td>
	<td class="line x" title="121:176	When we impose the constraint that all speech segments uttered by the same speaker receive the same label via same-speaker links, both test-set and ment classification." ></td>
	<td class="line x" title="122:176	Section 4.5 contains further discussion." ></td>
	<td class="line x" title="123:176	12We elected not to explicitly tune the value of agr in order to minimize the number of free parameters to deal with." ></td>
	<td class="line x" title="124:176	Support/oppose classifer (speech segmentyea?) Devel." ></td>
	<td class="line x" title="125:176	set Test set majority baseline 54.09 58.37 #(support)#(oppos) 59.14 62.67 SVM [speech segment] 70.04 66.05 SVM + same-speaker links 79.77 67.21 SVM + same-speaker links + agreement links, agr = 0 89.11 70.81 + agreement links, agr =  87.94 71.16 Table 4: Segment-based speech-segment classification accuracy, in percent." ></td>
	<td class="line x" title="126:176	Support/oppose classifer (speech segmentyea?) Devel." ></td>
	<td class="line x" title="127:176	set Test set SVM [speaker] 71.60 70.00 SVM + agreement links  with agr = 0 88.72 71.28 with agr =  84.44 76.05 Table 5: Speaker-based speech-segment classification accuracy, in percent." ></td>
	<td class="line x" title="128:176	Here, the initial SVM is run on the concatenation of all of a given speakers speech segments, but the results are computed over speech segments (not speakers), so that they can be compared to those in Table 4." ></td>
	<td class="line x" title="129:176	development-set accuracy increase even more, in the latter case quite substantially so." ></td>
	<td class="line x" title="130:176	The last two lines of Table 4 show that the best results are obtained by incorporating agreement information as well." ></td>
	<td class="line x" title="131:176	The highest test-set result, 71.16%, is obtained by using a high-precision threshold to determine which agreement links to add." ></td>
	<td class="line x" title="132:176	While the development-set results would induce us to utilize the standard threshold value of 0, which is sub-optimal on the test set, the agr = 0 agreement-link policy still achieves noticeable improvement over not using agreement links (test set: 70.81% vs. 67.21%)." ></td>
	<td class="line x" title="133:176	4.3 Speaker-based speech-segment classification We use speech segments as the unit of classification because they represent natural discourse units." ></td>
	<td class="line x" title="134:176	As a consequence, we are able to exploit relationships at the speech-segment level." ></td>
	<td class="line x" title="135:176	However, it is interesting to consider whether we really need to consider relationships specifically between speech segments themselves, or whether it suffices to simply consider relationships between the speakers 332 of the speech segments." ></td>
	<td class="line x" title="136:176	In particular, as an alternative to using same-speaker links, we tried a speaker-based approach wherein the way we determine the initial individual-document classification score for each speech segment uttered by a person p in a given debate is to run an SVM on the concatenation of all of ps speech segments within that debate." ></td>
	<td class="line x" title="137:176	(We also ensure that agreement-link information is propagated from speech-segment to speaker pairs.)" ></td>
	<td class="line x" title="138:176	How does the use of same-speaker links compare to the concatenation of each speakers speech segments?" ></td>
	<td class="line x" title="139:176	Tables 4 and 5 show that, not surprisingly, the SVM individual-document classifier works better on the concatenated speech segments than on the speech segments in isolation." ></td>
	<td class="line x" title="140:176	However, the effect on overall classification accuracy is less clear: the development set favors samespeaker links over concatenation, while the test set does not." ></td>
	<td class="line x" title="141:176	But we stress that the most important observation we can make from Table 5 is that once again, the addition of agreement information leads to substantial improvements in accuracy." ></td>
	<td class="line x" title="142:176	4.4 Hard agreement constraints Recall that in in our experiments, we created finite-weight agreement links, so that speech segments appearing in pairs flagged by our (imperfect) agreement detector can potentially receive different labels." ></td>
	<td class="line x" title="143:176	We also experimented with forcing such speech segments to receive the same label, either through infinite-weight agreement links or through a speech-segment concatenation strategy similar to that described in the previous subsection." ></td>
	<td class="line x" title="144:176	Both strategies resulted in clear degradation in performance on both the development and test sets, a finding that validates our encoding of agreement information as soft preferences." ></td>
	<td class="line x" title="145:176	4.5 On the development/test set split We have seen several cases in which the method that performs best on the development set does not yield the best test-set performance." ></td>
	<td class="line x" title="146:176	However, we felt that it would be illegitimate to change the train/development/test sets in a post hoc fashion, that is, after seeing the experimental results." ></td>
	<td class="line x" title="147:176	Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding." ></td>
	<td class="line x" title="148:176	5 Related work Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al. , 2005; Cardie et al. , 2006; Kwon et al. , 2006)." ></td>
	<td class="line x" title="149:176	There has also been work focused upon determining the political leaning (e.g. , liberal vs. conservative) of a document or author, where most previously-proposed methods make no direct use of relationships between the documents to be classified (the unlabeled texts) (Laver et al. , 2003; Efron, 2004; Mullen and Malouf, 2006)." ></td>
	<td class="line x" title="150:176	An exception is Grefenstette et al.(2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site." ></td>
	<td class="line x" title="152:176	Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text (Yang and Callan, 2005; Purpura and Hillard, 2006)." ></td>
	<td class="line x" title="153:176	Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement." ></td>
	<td class="line x" title="154:176	More sophisticated approaches have been proposed (Hillard et al. , 2003), including an extension that, in an interesting reversal of our problem, makes use of sentimentpolarity indicators within speech segments (Galley et al. , 2004)." ></td>
	<td class="line x" title="155:176	Also relevant is work on the general problems of dialog-act tagging (Stolcke et al. , 2000), citation analysis (Lehnert et al. , 1990), and computational rhetorical analysis (Marcu, 2000; Teufel and Moens, 2002)." ></td>
	<td class="line x" title="156:176	We currently do not have an efficient means to encode disagreement information as hard constraints; we plan to investigate incorporating such information in future work." ></td>
	<td class="line x" title="157:176	Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g. , between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations." ></td>
	<td class="line x" title="158:176	Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit 333 inter-document references in the form of hyperlinks (Agrawal et al. , 2003)." ></td>
	<td class="line x" title="159:176	Notable early papers on graph-based semisupervised learning include Blum and Chawla (2001), Bansal et al.(2002), Kondor and Lafferty (2002), and Joachims (2003)." ></td>
	<td class="line x" title="161:176	Zhu (2005) maintains a survey of this area." ></td>
	<td class="line x" title="162:176	Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al. , 2001; Getoor et al. , 2002; Taskar et al. , 2002; Taskar et al. , 2003; Taskar et al. , 2004; McCallum and Wellner, 2004)." ></td>
	<td class="line x" title="163:176	It would be interesting to investigate the application of such methods to our problem." ></td>
	<td class="line x" title="164:176	However, we also believe that our approach has important advantages, including conceptual simplicity and the fact that it is based on an underlying optimization problem that is provably and in practice easy to solve." ></td>
	<td class="line x" title="165:176	6 Conclusion and future work In this study, we focused on very general types of cross-document classification preferences, utilizing constraints based only on speaker identity and on direct textual references between statements." ></td>
	<td class="line x" title="166:176	We showed that the integration of even very limited information regarding inter-document relationships can significantly increase the accuracy of support/opposition classification." ></td>
	<td class="line x" title="167:176	The simple constraints modeled in our study, however, represent just a small portion of the rich network of relationships that connect statements and speakers across the political universe and in the wider realm of opinionated social discourse." ></td>
	<td class="line x" title="168:176	One intriguing possibility is to take advantage of (readily identifiable) information regarding interpersonal relationships, making use of speaker/author affiliations, positions within a social hierarchy, and so on." ></td>
	<td class="line x" title="169:176	Or, we could even attempt to model relationships between topics or concepts, in a kind of extension of collaborative filtering." ></td>
	<td class="line x" title="170:176	For example, perhaps we could infer that two speakers sharing a common opinion on evolutionary biologist Richard Dawkins (a.k.a. Darwins rottweiler) will be likely to agree in a debate centered on Intelligent Design." ></td>
	<td class="line x" title="171:176	While such functionality is well beyond the scope of our current study, we are optimistic that we can develop methods to exploit additional types of relationships in future work." ></td>
	<td class="line x" title="172:176	Acknowledgments We thank Claire Cardie, Jon Kleinberg, Michael Macy, Andrew Myers, and the six anonymous EMNLP referees for valuable discussions and comments." ></td>
	<td class="line x" title="173:176	We also thank Reviewer 1 for generously providing additional post hoc feedback, and the EMNLP chairs Eric Gaussier and Dan Jurafsky for facilitating the process (as well as for allowing authors an extra proceedings page)." ></td>
	<td class="line x" title="174:176	This paper is based upon work supported in part by the National Science Foundation under grant no." ></td>
	<td class="line x" title="175:176	IIS-0329064." ></td>
	<td class="line x" title="176:176	Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1640
Partially Supervised Coreference Resolution For Opinion Summarization Through Structured Rule Learning
Stoyanov, Veselin;Cardie, Claire;"></td>
	<td class="line x" title="1:223	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336344, Sydney, July 2006." ></td>
	<td class="line x" title="2:223	c2006 Association for Computational Linguistics Partially Supervised Coreference Resolution for Opinion Summarization through Structured Rule Learning Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University Ithaca, NY 14850, USA {ves,cardie}@cs.cornell.edu Abstract Combining fine-grained opinion information to produce opinion summaries is important for sentiment analysis applications." ></td>
	<td class="line x" title="3:223	Toward that end, we tackle the problem of source coreference resolution  linking together source mentions that refer to the same entity." ></td>
	<td class="line x" title="4:223	The partially supervised nature of the problem leads us to define and approach it as the novel problem of partially supervised clustering." ></td>
	<td class="line x" title="5:223	We propose and evaluate a new algorithm for the task of source coreference resolution that outperforms competitive baselines." ></td>
	<td class="line x" title="6:223	1 Introduction Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text." ></td>
	<td class="line x" title="7:223	Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g. Coglianese (2004))." ></td>
	<td class="line x" title="8:223	Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al.(2005), Riloff et al.(2005))." ></td>
	<td class="line x" title="11:223	To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information)." ></td>
	<td class="line x" title="12:223	In contrast, our work concerns the summarization of fine-grained information about opinions." ></td>
	<td class="line x" title="13:223	In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al.(2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al.(2004), Choi et al.(2005), Kim and Hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated." ></td>
	<td class="line x" title="17:223	A simple opinion summary is shown in figure 1.1 We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications in any domain where the analysis of opinions is critical." ></td>
	<td class="line x" title="18:223	This paper addresses the problem of opinion summarization by considering the creation of simple opinion summaries like those of figure 1." ></td>
	<td class="line x" title="19:223	We propose source coreference resolution  the task of determining which mentions of opinion sources refer to the same entity  as the primary mechanism for identifying the set of opinions attributed to each real-world source." ></td>
	<td class="line x" title="20:223	For this type of summary, source coreference resolution constitutes an integral step in the process of generating full opinion summaries." ></td>
	<td class="line x" title="21:223	For example, given the opinion expressions of figure 1, their polarity, and the associated opinion sources and targets, the bulk of the resulting summary can be produced by recognizing that source mentions Zacarias Moussaoui, he, my, and Mr. Moussaoui all refer to the same person; and that source mentions Mr. Zerkin and Zerkin refer to the same person.2 1For simplicity, the example summary does not contain any source/target statistics." ></td>
	<td class="line x" title="22:223	2In addition, the summary would require the closely related task of target coreference resolution and a means for aggregating the conflicting opinions from Zerkin toward Moussaoui." ></td>
	<td class="line x" title="23:223	336 At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000))." ></td>
	<td class="line x" title="24:223	We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of a new machine learning setting that we refer to as partially supervised clustering." ></td>
	<td class="line x" title="25:223	In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets)." ></td>
	<td class="line x" title="26:223	As a result, we define the task of partially supervised clustering, the goal of which is to learn a clustering function from a set of partially specified clustering examples (Section 4)." ></td>
	<td class="line x" title="27:223	We are not aware of prior work on the problem of partially supervised clustering and argue that it differs substantially from that of semi-supervised clustering." ></td>
	<td class="line x" title="28:223	We propose an algorithm for partially supervised clustering that extends a rule learner with structure information and is generally applicable to problems that fit the partially supervised clustering definition (Section 5)." ></td>
	<td class="line x" title="29:223	We apply the algorithm to the source coreference resolution task and evaluate its performance on a standard sentiment analysis data set that includes source coreference chains (Section 6)." ></td>
	<td class="line x" title="30:223	We find that our algorithm outperforms highly competitive baselines by a considerable margin  B3 score of 83.2 vs. 81.8 and 67.1 vs. 60.9 F1 score for the identification of positive source coreference links." ></td>
	<td class="line x" title="31:223	2 Related Work Work relevant to our problem can be split into three main areas  sentiment analysis, traditional noun phrase coreference resolution, and supervised and weakly supervised clustering." ></td>
	<td class="line x" title="32:223	Related work in the former two areas is summarized briefly below." ></td>
	<td class="line x" title="33:223	Supervised and weakly supervised clustering approaches are discussed in Section 4." ></td>
	<td class="line x" title="34:223	Sentiment analysis." ></td>
	<td class="line x" title="35:223	Much of the relevant research in sentiment analysis addresses sentiment classification, a text categorization task of extracting opinion at the coarse-grained document level." ></td>
	<td class="line x" title="36:223	The goal in sentiment classification is to assign to [Source Zacarias Moussaoui] [ complained] at length today about [Target his own lawyer], telling a federal court jury that [Target he] was [ more interested in achieving fame than saving Moussaouis life]." ></td>
	<td class="line x" title="37:223	Mr. Moussaoui said he was appearing on the witness stand to tell the truth." ></td>
	<td class="line x" title="38:223	And one part of the truth, [Source he] said, is that [Target sending him to prison for life] would be [ a greater punishment] than being sentenced to death. [ [Target You] have put your interest ahead of [Source my] life], [Source Mr. Moussaoui] told his court-appointed lawyer Gerald T. Zerkin But, [Source Mr. Zerkin] pressed [Target Mr. Moussaoui], was it [ not true] that he told his lawyers earlier not to involve any Muslims in the defense, not to present any evidence that might persuade the jurors to spare his life?" ></td>
	<td class="line x" title="39:223	[Source Zerkin] seemed to be trying to show the jurors that while [Target the defendant] is generally [+ an honest individual], his conduct shows [Target he] is [ not stable mentally], and thus [ undeserving] of [Target the ultimate punishment]." ></td>
	<td class="line x" title="40:223	Moussaoui Zerkin prison for life ultimate punishment    /+ Figure 1: Example text containing opinions (above) and a summary of the opinions (below)." ></td>
	<td class="line x" title="41:223	Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-)." ></td>
	<td class="line x" title="42:223	The underlined phrase will be explained later in the paper." ></td>
	<td class="line oc" title="43:223	a document either positive (thumbs up) or negative (thumbs down) polarity (e.g. Das and Chen (2001), Pang et al.(2002), Turney (2002), Dave et al.(2003))." ></td>
	<td class="line x" title="46:223	Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level." ></td>
	<td class="line x" title="47:223	Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al.(2003), Riloff and Wiebe (2003), Bethard et al.(2004), Wilson et al.(2004), Yu and Hatzivassiloglou (2003), Choi et al.(2005), Kim and Hovy (2005), Wiebe and Riloff (2005))." ></td>
	<td class="line x" title="52:223	Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries." ></td>
	<td class="line x" title="53:223	In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and 337 characterize the relations between opinions and their sources." ></td>
	<td class="line x" title="54:223	Coreference resolution." ></td>
	<td class="line x" title="55:223	Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al.(2003), McCallum and Wellner (2003))." ></td>
	<td class="line x" title="57:223	Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent)." ></td>
	<td class="line x" title="58:223	Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering." ></td>
	<td class="line x" title="59:223	For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent." ></td>
	<td class="line x" title="60:223	Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreference classifier is run, followed by a clustering step to produce the final clusters (coreference chains) of coreferent NPs." ></td>
	<td class="line x" title="61:223	For both training and testing, coreference resolution algorithms rely on feature vectors for pairs of noun phrases that encode linguistic information about the NPs and their local context." ></td>
	<td class="line x" title="62:223	Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002)." ></td>
	<td class="line x" title="63:223	3 Source Coreference Resolution In this section we introduce the problem of source coreference resolution in the context of opinion summarization and argue for the need for novel methods for the task." ></td>
	<td class="line x" title="64:223	The task of source coreference resolution is to decide which mentions of opinion sources refer to the same entity." ></td>
	<td class="line x" title="65:223	Much like traditional coreference resolution, we employ a learning approach; however, our approach differs from traditional coreference resolution in its definition of the learning task." ></td>
	<td class="line x" title="66:223	Motivated by the desire to utilize unlabeled examples (discussed later), we define training as an integrated task in which pairwise NP coreference decisions are learned together with the clustering function as opposed to treating each NP pair as a training example." ></td>
	<td class="line x" title="67:223	Thus, our training phase takes as input a set of documents with manually annotated opinion sources together with coreference annotations for the sources; it outputs a classifier that can produce source coreference chains for previously unseen documents containing marked (manually or automatically) opinion sources." ></td>
	<td class="line x" title="68:223	More specifically, the source coreference resolution training phase proceeds through the following steps: 1." ></td>
	<td class="line x" title="69:223	Source-to-NP mapping: We preprocess each document by running a tokenizer, sentence splitter, POS tagger, parser, and an NP finder." ></td>
	<td class="line x" title="70:223	Subsequently, we augment the set of NPs found by the NP finder with the help of a system for named entity detection." ></td>
	<td class="line x" title="71:223	We then map the sources to the NPs." ></td>
	<td class="line x" title="72:223	Since there is no one-to-one correspondence, we use a set of heuristics to create the mapping." ></td>
	<td class="line x" title="73:223	More details about why heuristics are needed and the process used to map sources to NPs can be found in Stoyanov and Cardie (2006)." ></td>
	<td class="line x" title="74:223	2." ></td>
	<td class="line x" title="75:223	Feature vector creation: We extract a feature vector for every pair of NPs from the preprocessed corpus." ></td>
	<td class="line x" title="76:223	We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution." ></td>
	<td class="line x" title="77:223	3." ></td>
	<td class="line x" title="78:223	Classifier construction: Using the feature vectors from step 2, we construct a training set containing one training example per document." ></td>
	<td class="line x" title="79:223	Each training example consists of the feature vectors for all pairs of NPs in the document, including those that do not map to sources, together with the available coreference information for the source noun phrases (i.e. the noun phrases to which sources are mapped)." ></td>
	<td class="line x" title="80:223	The training instances are provided as input to a learning algorithm (see Section 5), which constructs a classifier that can take the instances associated with a new (previously unseen) document and produce a clustering over all NPs in the document." ></td>
	<td class="line x" title="81:223	The testing phase employs steps 1 and 2 as described above, but replaces step 3 by a straightforward application of the learnt classifier." ></td>
	<td class="line x" title="82:223	Since we are interested in coreference information only for the source NPs, we simply discard the non-source NPs from the resulting clustering." ></td>
	<td class="line x" title="83:223	The approach to source coreference resolution described here would be identical to traditional coreference resolution when provided with training examples containing coreference information for all NPs." ></td>
	<td class="line x" title="84:223	However, opinion corpora in general, and our corpus in particular, contain no coreference information about general NPs." ></td>
	<td class="line x" title="85:223	Nevertheless, after manual sources are mapped to NPs in 338 step 1 above, our approach can rely on the available coreference information for the source NPs." ></td>
	<td class="line x" title="86:223	Due to the high cost of coreference annotation, we desire methods that can work in the presence of only this limited amount of coreference information." ></td>
	<td class="line x" title="87:223	A possible workaround the absence of full NP coreference information is to train a traditional coreference system only on the labeled part of the data (indeed that is one of the baselines against which we compare)." ></td>
	<td class="line x" title="88:223	However, we believe that an effective approach to source coreference resolution has to utilize the unlabeled noun phrases because links between sources might be realized through non-source mentions." ></td>
	<td class="line x" title="89:223	This problem is illustrated in figure 1." ></td>
	<td class="line x" title="90:223	The underlined Moussaoui is coreferent with all of the Moussaoui references marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, the reference would be omitted from the Moussaoui source chain." ></td>
	<td class="line x" title="91:223	Unfortunately, this proper noun phrase might be critical in establishing the coreference of the final source reference he with the other mentions of the source Moussaoui." ></td>
	<td class="line x" title="92:223	As mentioned previously, in order to utilize the unlabeled data, our approach differs from traditional coreference resolution, which uses NP pairs as training instances." ></td>
	<td class="line x" title="93:223	We instead follow the framework of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) and consider each document as a training example." ></td>
	<td class="line x" title="94:223	As in supervised clustering, this framework has the additional advantage that the learning algorithm can consider the clustering algorithm when making decisions about pairwise classification, which could lead to improvements in the classifier." ></td>
	<td class="line x" title="95:223	In the next section we describe our approach to classifier construction for step 3 and compare our problem to traditional weakly supervised clustering, characterizing it as an instance of the novel problem of partially supervised clustering." ></td>
	<td class="line x" title="96:223	4 Partially Supervised Clustering In our desire to perform effective source coreference resolution we arrive at the following learning problem  the learning algorithm is presented with a set of partially specified examples of clusterings and acquires a function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples." ></td>
	<td class="line x" title="97:223	This setting is to be contrasted with semisupervised clustering (or clustering with constraints), which has received much research attention (e.g. Demiriz et al.(1999), Wagstaff and Cardie (2000), Basu (2005), Davidson and Ravi (2005))." ></td>
	<td class="line x" title="99:223	Semi-supervised clustering can be defined as the problem of clustering a set of items in the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeled points." ></td>
	<td class="line x" title="100:223	In contrast to our setting, in the semisupervised case there is no training phase  the algorithm receives all examples (labeled and unlabeled) at the same time together with some distance or cost function and attempts to find a clustering that optimizes a given measure (usually based on the distance or cost function)." ></td>
	<td class="line x" title="101:223	Source coreference resolution might alternatively be approached as a supervised clustering problem." ></td>
	<td class="line x" title="102:223	Traditionally, approaches to supervised clustering have treated the pairwise link decisions as a classification problem." ></td>
	<td class="line x" title="103:223	These approaches first learn a distance metric that optimizes the pairwise decisions; and then follow the pairwise classification with a clustering step." ></td>
	<td class="line x" title="104:223	However, these traditional approaches have no obvious way of utilizing the available unlabeled information." ></td>
	<td class="line x" title="105:223	In contrast, we follow recent approaches to supervised clustering that propose ways to learn the distance measure in the context of the clustering decisions (Li and Roth, 2005; Finley and Joachims, 2005; McCallum and Wellner, 2003)." ></td>
	<td class="line x" title="106:223	This provides two advantages for the problem of source coreference resolution." ></td>
	<td class="line x" title="107:223	First, it allows the algorithm to take advantage of the complexity of the rich structural dependencies introduced by the clustering problem." ></td>
	<td class="line x" title="108:223	Viewed traditionally as a hurdle, the structural complexity of clustering may be beneficial in the partially supervised case." ></td>
	<td class="line x" title="109:223	We believe that provided with a few partially specified clustering examples, an algorithm might be able to generalize from the structural dependencies to infer correctly the whole clustering of the items." ></td>
	<td class="line x" title="110:223	In addition, considering pairwise decisions in the context of the clustering can arguably lead to more accurate classifiers." ></td>
	<td class="line x" title="111:223	Unfortunately, none of the supervised clustering approaches is readily applicable to the partially supervised case." ></td>
	<td class="line x" title="112:223	However, by adapting the formal supervised clustering definition, which we do next, we can develop approaches to partially supervised clustering that take advantage of the un339 labeled portions of the data." ></td>
	<td class="line x" title="113:223	Formal definition." ></td>
	<td class="line x" title="114:223	For partially supervised clustering we extend the formal definition of supervised clustering given by Finley and Joachims (2005)." ></td>
	<td class="line x" title="115:223	In the fully supervised setting, an algorithm is given a set S of n training examples (x1,y1),,(xn,yn)  X  Y, where X is the set of all possible sets of items and Y is the set of all possible clusterings of these sets." ></td>
	<td class="line x" title="116:223	For a training example (x,y), x = {x1,x2,,xk} is a set of k items and y = {y1,y2,,yr} is a clustering of the items in x with each yi  x. Additionally, each item can be in no more than one cluster (i,j.yi yj = ) and in the fully supervised case each item is in at least one cluster (x = uniontextyi)." ></td>
	<td class="line x" title="117:223	The goal of the learning algorithm is to acquire a function h : X  Y that can accurately cluster a (previously unseen) set of items." ></td>
	<td class="line x" title="118:223	In the context of source coreference resolution the training set contains one example for each document." ></td>
	<td class="line x" title="119:223	The items in each training example are the NPs and the clustering over the items is the equivalence relation defined by the coreference information." ></td>
	<td class="line x" title="120:223	For source coreference resolution, however, clustering information is unavailable for the non-source NPs." ></td>
	<td class="line x" title="121:223	Thus, to be able to deal with this unlabeled component of the data we arrive to the setting of partially supervised clustering, in which we relax the condition that each item is in at least one cluster (x = uniontextyi) and replace it with the condition x  uniontextyi." ></td>
	<td class="line x" title="122:223	The items with no linking information (items in x\uniontextyi) constitute the unlabeled (unsupervised) component of the partially supervised clustering." ></td>
	<td class="line x" title="123:223	5 Structured Rule Learner We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002)." ></td>
	<td class="line x" title="124:223	We extend RIPPER so that it can learn rules in the context of singlelink clustering, which both suits our task (i.e. pronouns link to their single antecedent) and has exhibited good performance for coreference resolution (Ng and Cardie, 2002)." ></td>
	<td class="line x" title="125:223	We begin with a brief overview of RIPPER followed by a description of the modifications that we implemented." ></td>
	<td class="line x" title="126:223	For ease of presentation, we assume that we are in the fully supervised case." ></td>
	<td class="line x" title="127:223	We end this section by describing the changes for the partially supervised case." ></td>
	<td class="line x" title="128:223	procedure StRip(TrainData){ GrowData, PruneData = Split(TrainData); //Keep instances from the same document together while(there are positive uncovered instances) { r = growRule(GrowData); r = pruneRule(r, PruneData); DL = relativeDL(Ruleset); if(DL  minDL + d bits) Ruleset.add(r); Mark examples covered by r as +; else exit loop with Ruleset } } procedure growRule(growData){ r = empty rule; for(every unused feature f){ if (f is nominal feature) { for(every possible value v of f) { mark all instances that have values of v for f with +; compute the transitive closure of the positive instances //(including instances marked + from previous rules); compute the infoGain for the future/value combination; } } else //Numeric feature create one bag for each feature value and split the instances into bags; do a forward and a backward pass over the bags keeping a running clustering and compute the information gain for each value; } } add the future/value pair with the best infoGain to r; growData = growData all negative instances; return r; } procedure pruneRule(r, pruneData){ for(all antecedents a in the rule){ apply all antecedents in r up to a to pruneData; compute the transitive closure of the positive instances; compute A(a)  the accuracy of the rule up to antecedent a; } Remove all antecedents after the antecedent for which A(a) is maximum." ></td>
	<td class="line x" title="129:223	} Figure 2: The StRip algorithm." ></td>
	<td class="line x" title="130:223	Additions to RIPPER are shown in bold." ></td>
	<td class="line x" title="131:223	5.1 The RIPPER Algorithm RIPPER (for Repeated Incremental Pruning to Produce Error Reduction) was introduced by Cohen (1995) as an extension of an existing rule induction algorithm." ></td>
	<td class="line x" title="132:223	Cohen (1995) showed that RIPPER produces error rates competitive with C4.5, while exhibiting better running times." ></td>
	<td class="line x" title="133:223	RIPPER consists of two phases  a ruleset is grown and then optimized." ></td>
	<td class="line x" title="134:223	The ruleset creation phase begins by randomly splitting the training data into a rulegrowing set (2/3 of the training data) and a pruning set (the remaining 1/3)." ></td>
	<td class="line x" title="135:223	A rule is then grown on the former set by repeatedly adding the antecedent (the feature value test) with the largest information gain until the accuracy of the rule becomes 1.0 or there are no remaining potential antecedents." ></td>
	<td class="line x" title="136:223	Next the rule is applied to the pruning data and any rulefinal sequence that reduces the accuracy of the rule is removed." ></td>
	<td class="line x" title="137:223	The optimization phase uses the full training 340 set to first grow a replacement rule and a revised rule for each rule in the ruleset." ></td>
	<td class="line x" title="138:223	For each rule, the algorithm then considers the original rule, the replacement rule, and the revised rule, and keeps the rule with the smallest description length in the context of the ruleset." ></td>
	<td class="line x" title="139:223	After all rules are considered, RIPPER attempts to grow residual rules that cover data not already covered by the ruleset." ></td>
	<td class="line x" title="140:223	Finally, RIPPER deletes any rules from the ruleset that reduce the overall minimum description length of the data plus the ruleset." ></td>
	<td class="line x" title="141:223	RIPPER performs two rounds of this optimization phase." ></td>
	<td class="line x" title="142:223	5.2 The StRip Algorithm The property of partially supervised clustering that we want to explore is the structured nature of the decisions." ></td>
	<td class="line x" title="143:223	That is, each decision of whether two items (say a and b) belong to the same cluster has an implication for all items aprime that belong to as cluster and all items bprime that belong to bs cluster." ></td>
	<td class="line x" title="144:223	We target modifications to RIPPER that will allow StRip (for Structured RIPPER) to learn rules that produce good clusterings in the context of single-link clustering." ></td>
	<td class="line x" title="145:223	We extend RIPPER so that every time it makes a decision about a rule, it considers the effect of the rule on the overall clustering of items (as opposed to considering the instances that the rule classifies as positive/negative in isolation)." ></td>
	<td class="line x" title="146:223	More precisely, we precede every computation of rule performance (e.g. information gain or description length) by a transitive closure (i.e. single link clustering) of the data w.r.t. to the pairwise classifications." ></td>
	<td class="line x" title="147:223	Following the transitive closure, all pairs of items that are in the same cluster are considered covered by the rule for performance computation." ></td>
	<td class="line x" title="148:223	The StRip algorithm is given in figure 2, with modifications to the original RIPPER algorithm shown in bold." ></td>
	<td class="line x" title="149:223	Due to space limitations the optimization stage of the algorithm is omitted." ></td>
	<td class="line x" title="150:223	Our modifications to the optimization stage of RIPPER are in the spirit of the rest of the StRip algorithm." ></td>
	<td class="line x" title="151:223	Partially supervised case." ></td>
	<td class="line x" title="152:223	So far we described StRip only for the fully supervised case." ></td>
	<td class="line x" title="153:223	We use a very simple modification to handle the partially supervised setting: we exclude the unlabeled pairs when computing the performance of the rules." ></td>
	<td class="line x" title="154:223	Thus, the unlabeled items do not count as correct or incorrect classifications when acquiring or pruning a rule, although they do participate in the transitive closure." ></td>
	<td class="line x" title="155:223	Links in the unlabeled data are inferred entirely through the indirect links between items in the labeled component that they introduce." ></td>
	<td class="line x" title="156:223	In the example of figure 1, the two problematic unlabeled links are the link between the source mention he and the underlined nonsource NP Mr. Moussaoui and the link between the underlined Mr. Moussaoui to any source mention of Moussaoui." ></td>
	<td class="line x" title="157:223	While StRip will not reward any rule (or rule set) that covers these two links directly, such rules will be rewarded indirectly since they put the source he in the chain for the source Moussaoui." ></td>
	<td class="line x" title="158:223	StRip running time." ></td>
	<td class="line x" title="159:223	StRips running time is generally comparable to that of RIPPER." ></td>
	<td class="line x" title="160:223	We compute transitive closure by using a Union-Find structure, which runs in time O(logn), which for practical purposes can be considered linear (O(n)) 3." ></td>
	<td class="line x" title="161:223	However, when computing the best information gain for a nominal feature, StRip has to make a pass over the data for each value that the feature takes, while RIPPER can split the data into bags and perform the computation in one pass." ></td>
	<td class="line x" title="162:223	6 Evaluation and Results This section describes the source coreference data set, the baselines, our implementation of StRip, and the results of our experiments." ></td>
	<td class="line x" title="163:223	6.1 Data set For evaluation we use the MPQA corpus (Wiebe et al. , 2005).4 The corpus consists of 535 documents from the world press." ></td>
	<td class="line x" title="164:223	All documents in the collection are manually annotated with phraselevel opinion information following the annotation scheme of Wiebe et al.(2005)." ></td>
	<td class="line x" title="166:223	Discussion of the annotation scheme is beyond the scope of this paper; for our purposes it suffices to say that the annotations include the source of each opinion and coreference information for the sources (e.g. source coreference chains)." ></td>
	<td class="line x" title="167:223	The corpus contains no additional noun phrase coreference information." ></td>
	<td class="line x" title="168:223	For our experiments, we randomly split the data set into a training set consisting of 400 documents and a test set consisting of the remaining 135 documents." ></td>
	<td class="line x" title="169:223	We use the same test set for all experi3For the transitive closure, n is the number of items in a document, which is O(k), where k is the number of NP pairs." ></td>
	<td class="line x" title="170:223	Thus, transitive closure is sublinear in the number of training instances." ></td>
	<td class="line x" title="171:223	4The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm." ></td>
	<td class="line x" title="172:223	341 ments, although some learning runs were trained on 200 training documents (see next Subsection)." ></td>
	<td class="line x" title="173:223	The test set contains a total of 4736 source NPs (average of 35.34 source NPs per document) split into 1710 total source NP chains (average of 12.76 chains per document) for an average of 2.77 source NPs per chain." ></td>
	<td class="line x" title="174:223	6.2 Implementation We implemented the StRip algorithm by modifying JRip  the java implementation of RIPPER included in the WEKA toolkit (Witten and Frank, 2000)." ></td>
	<td class="line x" title="175:223	The WEKA implementation follows the original RIPPER specification." ></td>
	<td class="line x" title="176:223	We changed the implementation to incorporate the modifications suggested by the StRip algorithm; we also modified the underlying data representations and data handling techniques for efficiency." ></td>
	<td class="line x" title="177:223	Also due to efficiency considerations, we train StRip only on the 200-document training set." ></td>
	<td class="line x" title="178:223	6.3 Competitive baselines We compare the results of the new method to three fully supervised baseline systems, each of which employs the same traditional coreference resolution approach." ></td>
	<td class="line x" title="179:223	In particular, we use the aforementioned algorithm proposed by Ng and Cardie (2002), which combines a pairwise NP coreference classifier with single-link clustering." ></td>
	<td class="line x" title="180:223	For one baseline, we train the coreference resolution algorithm on the MPQA src corpus  the labeled portion of the MPQA corpus (i.e. NPs from the source coreference chains) with unlabeled instances removed." ></td>
	<td class="line x" title="181:223	The second and third baselines investigate whether the source coreference resolution task can benefit from NP coreference resolution training data from a different domain." ></td>
	<td class="line x" title="182:223	Thus, we train the traditional coreference resolution algorithm on the MUC6 and MUC7 coreference-annotated corpora5 that contain documents similar in style to those in the MPQA corpus (e.g. newspaper articles), but emanate from different domains." ></td>
	<td class="line x" title="183:223	For all baselines we targeted the best possible systems by trying two pairwise NP classifiers (RIPPER and an SVM in the SV Mlight implementation (Joachims, 1998)), many different parameter settings for the classifiers, two different feature sets, two different training set sizes (the 5We train each baseline using both the development set and the test set from the corresponding MUC corpus." ></td>
	<td class="line x" title="184:223	full training set and a smaller training set consisting of half of the documents selected at random), and three different instance selection algorithms6." ></td>
	<td class="line x" title="185:223	This variety of classifier and training data settings was motivated by reported differences in performance of coreference resolution approaches w.r.t. these variations (Ng and Cardie, 2002)." ></td>
	<td class="line x" title="186:223	More details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in Stoyanov and Cardie (2006)." ></td>
	<td class="line x" title="187:223	In the experiments below we report the best performance of each of the two learning algorithms on the MPQA test data." ></td>
	<td class="line x" title="188:223	6.4 Evaluation In addition to the baselines described above, we evaluate StRip both with and without unlabeled data." ></td>
	<td class="line x" title="189:223	That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs." ></td>
	<td class="line x" title="190:223	We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions." ></td>
	<td class="line x" title="191:223	B3 is a measure widely used for evaluating coreference resolution algorithms." ></td>
	<td class="line x" title="192:223	The measure computes the precision and recall for each NP mention in a document, and then averages them to produce combined results for the entire output." ></td>
	<td class="line x" title="193:223	More precisely, given a mention i that has been assigned to chain ci, the precision for mention i is defined as the number of correctly identified mentions in ci divided by the total number of mentions in ci." ></td>
	<td class="line x" title="194:223	Recall for i is defined as the number of correctly identified mentions in ci divided by the number of mentions in the gold standard chain for i. Results are shown in Table 1." ></td>
	<td class="line x" title="195:223	The first six rows of results correspond to the fully supervised baseline systems trained on different corpora  MUC6, MUC7, and MPQA src." ></td>
	<td class="line x" title="196:223	The seventh row of results shows the performance of StRip using only labeled data." ></td>
	<td class="line x" title="197:223	The final row of the table shows the results for partially supervised learning with unlabeled data." ></td>
	<td class="line x" title="198:223	The table lists results from the best performing run for each algorithm." ></td>
	<td class="line x" title="199:223	Performance among the baselines trained on the MUC data is comparable." ></td>
	<td class="line x" title="200:223	However, the two baseline runs trained on the MPQA src corpus (i.e. results rows five and six) show slightly better performance on the B3 metric than the baselines trained 6The goal of the instance selection algorithms is to balance the data, which contains many more negative than positive instances 342 ML Framework Training set Classifier B3 precision recall F1 Fully supervised MUC6 SVM 81.2 72.6 52.5 60.9 RIPPER 80.7 57.4 63.5 60.3 MUC7 SVM 81.7 65.6 55.9 60.4 RIPPER 79.7 71.6 48.5 57.9 MPQA src SVM 81.8 57.5 62.9 60.2 RIPPER 81.8 72.0 52.5 60.6 StRip 82.3 76.5 56.1 64.6 Partially supervised MPQA all StRip 83.2 77.1 59.4 67.1 Table 1: Results for Source Coreference." ></td>
	<td class="line x" title="201:223	MPQA src stands for the MPQA corpus limited to only source NPs, while MPQA full contains the unlabeled NPs." ></td>
	<td class="line x" title="202:223	on the MUC data, which indicates that for our task the similarity of the documents in the training and test sets appears to be more important than the presence of complete supervisory information." ></td>
	<td class="line x" title="203:223	(Improvements over the RIPPER runs trained on the MUC corpora are statistically significant7, while improvements over the SVM runs are not.)" ></td>
	<td class="line x" title="204:223	Table 1 also shows that StRip outperforms the baselines on both performance metrics." ></td>
	<td class="line x" title="205:223	StRips performance is better than the baselines when trained on MPQA src (improvement not statistically significant, p > 0.20) and even better when trained on the full MPQA corpus, which includes the unlabeled NPs (improvement over the baselines and the former StRip run statistically significant)." ></td>
	<td class="line x" title="206:223	These results confirm our hypothesis that StRip improves due to two factors: first, considering pairwise decisions in the context of the clustering function leads to improvements in the classifier; and, second, StRip can take advantage of the unlabeled portion of the data." ></td>
	<td class="line x" title="207:223	StRips performance is all the more impressive considering the strength of the SVM and RIPPER baselines, which which represent the best runs across the 336 different parameter settings tested for SV Mlight and 144 different settings tested for RIPPER." ></td>
	<td class="line x" title="208:223	In contrast, all four of the StRip runs using the full MPQA corpus (we vary the loss ratio for false positive/false negative cost) outperform those baselines." ></td>
	<td class="line x" title="209:223	7 Future Work Source coreference resolution is only one aspect of opinion summarization." ></td>
	<td class="line x" title="210:223	Additionally, an opinion summarization system will need to handle 7Statistical significance is measured using both a 2-tailed paired t-test and the Wilcoxon matched-pairs signed-ranks test (p < 0.05)." ></td>
	<td class="line x" title="211:223	The two tests agreed on all significance judgements, so we will not report them separately." ></td>
	<td class="line x" title="212:223	the closely related task of target coreference resolution in order to cluster targets of opinions8 and combine multiple conflicting opinions from a source to the same targets." ></td>
	<td class="line x" title="213:223	Furthermore, a fully automatic opinion summarizer requires automatic source and opinion extractors." ></td>
	<td class="line x" title="214:223	While we anticipate that target coreference resolution will be subject to error rates similar to those of source coreference resolution, incorporating these imperfect opinions and sources will further impair the performance of the opinion summarizer." ></td>
	<td class="line x" title="215:223	We are not aware of any measure that can be directly used to assess the goodness of opinion summaries, but plan to develop such in future work in conjunction with the development of methods for creating opinion summaries completely automatically." ></td>
	<td class="line x" title="216:223	The evaluation metrics will likely have to depend on the task for which the summaries are used." ></td>
	<td class="line x" title="217:223	A limitation of our approach to partially supervised clustering is that we do not directly optimize for the performance measure (e.g. B3)." ></td>
	<td class="line x" title="218:223	Other efforts in the area of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) have suggested ways to learn distance measures that can optimize directly for a desired performance measure." ></td>
	<td class="line x" title="219:223	We plan to investigate algorithms that can directly optimize for complex measures (such as B3) for the problem of partially supervised clustering." ></td>
	<td class="line x" title="220:223	Unfortunately, a measure as complex as B3 makes extending existing approaches far from trivial due to the difficulty of establishing the connection between individual pairwise decisions (the distance metric) and the score of the clustering algorithm." ></td>
	<td class="line x" title="221:223	Acknowledgements The authors would like to thank Vincent Ng and Art Munson for providing coreference resolution 8We did not tackle the task of target coreference resolution in this paper because the MPQA corpus did not contain target annotations at the time of publication." ></td>
	<td class="line x" title="222:223	343 code, members of the Cornell NLP group (especially Yejin Choi and Art Munson) for many helpful discussions, and the anonymous reviewers for their insightful comments." ></td>
	<td class="line x" title="223:223	This work was supported by the Advanced Research and Development Activity (ARDA), by NSF Grants IIS0535099 and IIS-0208028, by gifts from Google and the Xerox Foundation, and by an NSF Graduate Research Fellowship to the first author." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1641
Sentiment Retrieval Using Generative Models
Eguchi, Koji;Lavrenko, Victor;"></td>
	<td class="line x" title="1:229	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 345354, Sydney, July 2006." ></td>
	<td class="line x" title="2:229	c2006 Association for Computational Linguistics Sentiment Retrieval using Generative Models Koji Eguchi National Institute of Informatics Tokyo 101-8430, Japan eguchi@nii.ac.jp Victor Lavrenko Department of Computer Science University of Massachusetts Amherst, MA 01003, USA lavrenko@cs.umass.edu Abstract Ranking documents or sentences according to both topic and sentiment relevance should serve a critical function in helping users when topics and sentiment polarities of the targeted text are not explicitly given, as is often the case on the web." ></td>
	<td class="line x" title="3:229	In this paper, we propose several sentiment information retrieval models in the framework of probabilistic language models, assuming that a user both inputs query terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner." ></td>
	<td class="line x" title="4:229	We combine sentiment relevance models and topic relevance models with model parameters estimated from training data, considering the topic dependence of the sentiment." ></td>
	<td class="line x" title="5:229	Our experiments prove that our models are effective." ></td>
	<td class="line x" title="6:229	1 Introduction The recent rapid expansion of access to information has significantly increased the demands on retrieval or classification of sentiment information from a large amount of textual data." ></td>
	<td class="line x" title="7:229	The field of sentiment classification has recently received considerable attention, where the polarities of sentiment, such as positive or negative, were identified from unstructured text (Shanahan et al. , 2005)." ></td>
	<td class="line x" title="8:229	A number of studies have investigated sentiment classification at document level, e.g., (Pang et al. , 2002; Dave et al. , 2003), and at sentence level, e.g., (Hu and Liu, 2004; Kim and Hovy, 2004; Nigam and Hurst, 2005); however, the accuracy is still less than desirable." ></td>
	<td class="line x" title="9:229	Therefore, ranking according to the likelihood of containing sentiment information is expected to serve a crucial function in helping users." ></td>
	<td class="line x" title="10:229	We believe that our work is the first attempt at sentiment retrieval that aims at finding sentences containing information with a specific sentiment polarity on a certain topic." ></td>
	<td class="line x" title="11:229	Intuitively, the expression of sentiment in text is dependent on the topic." ></td>
	<td class="line x" title="12:229	For example, a negative view for some voting event may be expressed using flaw, while a negative view for some politician may be expressed using reckless." ></td>
	<td class="line x" title="13:229	Moreover, sentiment polarities are also dependent on topics or domains." ></td>
	<td class="line oc" title="14:229	For example, the adjective unpredictable may have a negative orientation in an automotive review, in a phrase such as unpredictable steering, but it could have a positive orientation in a movie review, in a phrase such as unpredictable plot, as mentioned in (Turney, 2002) in the context of his sentiment word detection." ></td>
	<td class="line x" title="15:229	We propose sentiment retrieval models in the framework of generative language modeling, not only assuming query terms expressing a certain topic, but also assuming that the polarity of sentiment interest is specified by the user in some manner, where the topic dependence of the sentiment is considered." ></td>
	<td class="line x" title="16:229	To the best of our knowledge, there have been no other studies on a retrieval model unifying both topic and sentiment, and further, there have been no other studies on sentiment retrieval." ></td>
	<td class="line x" title="17:229	The sentiment information often appears as local in a document, and therefore focusing on finer levels, i.e., sentence or passage levels rather than document level, is crucial." ></td>
	<td class="line x" title="18:229	We thus experiment on sentiment retrieval at the sentence level in this paper." ></td>
	<td class="line x" title="19:229	The rest of this paper is structured as follows." ></td>
	<td class="line x" title="20:229	Section 2 introduces the work related to this study." ></td>
	<td class="line x" title="21:229	Section 3 describes a generative model of sentiment, which is proposed here as a theoretical framework for our work." ></td>
	<td class="line x" title="22:229	Section 4 describes the task definition and our sentiment retrieval model." ></td>
	<td class="line x" title="23:229	345 Section 5 explains the data we used for our experiments, and gives our experimental results." ></td>
	<td class="line x" title="24:229	Section 6 concludes the paper." ></td>
	<td class="line x" title="25:229	2 Related Work Some efforts for the TREC Novelty Track were related to our work." ></td>
	<td class="line x" title="26:229	Although some of the topics used in the Novelty Track in 2003 and 2004 (Soboroff and Harman, 2003; Soboroff, 2004) were related to opinions, most of the efforts were focused on topic, such as studies using term distribution within each sentence, e.g., (Allan et al. , 2003; Losada, 2005; Murdock and Croft, 2005)." ></td>
	<td class="line x" title="27:229	Amongst the participants in the TREC Novelty Track, only (Kim et al. , 2004) proposed a method specialized to opinion-bearing sentence retrieval, by making use of lists of words with positive or negative polarities." ></td>
	<td class="line x" title="28:229	They aimed to find opinions on a given topic but did not distinguish or did not care about sentiment polarities that should be represented in some sentences (hereafter, opinion retrieval)." ></td>
	<td class="line x" title="29:229	We focus on finding positive views or negative views according to a given topic and sentiment of interest (hereafter, sentiment retrieval)." ></td>
	<td class="line x" title="30:229	Our work is the first work on sentiment retrieval, to the best of our knowledge." ></td>
	<td class="line x" title="31:229	In the context of sentiment classification, some researchers have conducted studies on the topic dependence of sentiment polarities." ></td>
	<td class="line x" title="32:229	(Nasukawa and Yi, 2003) and (Yi et al. , 2003) extracted positive or negative expressions on a given product name using handmade lexicons." ></td>
	<td class="line x" title="33:229	(Engstrom, 2004) studied how the topic dependence influences the accuracy of sentiment classification and attempted to reduce the influence to improve the accuracy." ></td>
	<td class="line x" title="34:229	(Wilson et al. , 2005) investigated how context influences sentiment polarity at the phrase level in a corpus, beginning with a predefined list of words with polarities." ></td>
	<td class="line x" title="35:229	Their focus on the phenomena of topic dependence of sentiment can be shared with our work; however, their work is not directly related to ours, because we focus on a different task, sentiment retrieval, where different approaches are required." ></td>
	<td class="line x" title="36:229	3 A Generative Model of Sentiment In this section we will provide a formal underpinning for our approach to sentiment retrieval." ></td>
	<td class="line x" title="37:229	The approach is based on the generative paradigm: we describe a statistical process that could be viewed, hypothetically, as a source of every statement of interest to our system." ></td>
	<td class="line x" title="38:229	We stress that this generative process is to be treated as purely hypothetical; the process is only intended to reflect those aspects of human discourse that are pertinent to the problem of retrieving affectively appropriate and topicrelevant texts in response to a query posed by our user." ></td>
	<td class="line x" title="39:229	Before giving a formal specification of our model, we will provide a high-level overview of the main ideas." ></td>
	<td class="line x" title="40:229	We are trying to model a collection of natural-language statements, some of which are relevant to a users query." ></td>
	<td class="line x" title="41:229	In our experiments, these statements are individual sentences, but the model can be applied to textual chunks of any length." ></td>
	<td class="line x" title="42:229	We assume that the content of an individual statement can be modeled independently of all other statements in the collection." ></td>
	<td class="line x" title="43:229	Each statement consists of some topicbearing and some sentiment-bearing words." ></td>
	<td class="line x" title="44:229	We assume that the topic-bearing words represent exchangeable samples from some underlying topic language model." ></td>
	<td class="line x" title="45:229	Exchangeability means that the relative order of the words is irrelevant, but the words are not independent of each otherthe idea often stated as a bag-of-words assumption." ></td>
	<td class="line x" title="46:229	Similarly, sentiment-bearing words are viewed as an order-invariant bag, sampled from the underlying sentiment language model." ></td>
	<td class="line x" title="47:229	We will explicitly model dependency between the topic and sentiment language models, and will demonstrate that treating them independently leads to sub-optimal retrieval performance." ></td>
	<td class="line x" title="48:229	When a sentiment polarity value is observed for a given statement, we will treat it as a ternary variable influencing the topic and sentiment language models." ></td>
	<td class="line x" title="49:229	We represent a users query as just another statement, consisting of topic and sentiment parts, subject to all the independence assumptions stated above." ></td>
	<td class="line x" title="50:229	We will use the query to estimate the topic and sentiment language models that are representative of the users interests." ></td>
	<td class="line x" title="51:229	Following (Lavrenko and Croft, 2001), we will use the term relevance models to describe these models, and will use them to rank statements in order of their relevance to the query." ></td>
	<td class="line x" title="52:229	3.1 Definitions We start by providing a set of definitions that will be used in the remainder of this section." ></td>
	<td class="line x" title="53:229	The task of our model is to generate a collection of statements DB BD BMBMBMDB D2." ></td>
	<td class="line x" title="54:229	A statement DB CX is a string of 346 wordsDB CXBD BMBMBMDB CXD2 CX, drawn from a common vocabulary CE." ></td>
	<td class="line x" title="55:229	We introduce a binary variable CQ CXCY BECUCBBNCCCV as an indicator of whether the word in the CYth position of the CXth statement will be a topic word or a sentiment word." ></td>
	<td class="line x" title="56:229	For our purposes, CQ CXCY is either provided by a human annotator (manual annotation), or determined heuristically (automatic annotation)." ></td>
	<td class="line x" title="57:229	The sentiment polarity DC CX for a given statement is a discrete random variable with three outcomes: CUA0BDBNBCBNB7BDCV, representing negative, neutral and positive polarity values, respectively." ></td>
	<td class="line x" title="58:229	As a matter of convenience we will often denote a statement as a triple CUDB D7 CX BNDB D8 CX BNDC CX CV, where DB D7 CX contains the sentiment words and DB D8 CX contains the topic words." ></td>
	<td class="line x" title="59:229	As we mentioned above, the users query is treated as just another statement." ></td>
	<td class="line x" title="60:229	It will be denoted as a triple CUD5D7BND5D8BND5DCCV, corresponding to sentiment words, topic keywords, and the desired polarity value." ></td>
	<td class="line x" title="61:229	We will use D4 to denote a unigram language model, i.e., a function that assigns a number D4B4DAB5BECJBCBNBDCL to every word DA in our vocabulary CE, such that A6 DA D4B4DAB5BPBD." ></td>
	<td class="line x" title="62:229	The set of all possible unigram language models is the probability simplex C1C8." ></td>
	<td class="line x" title="63:229	Similarly, D4 DC will denote a distribution over the three possible polarity values, and C1C8 DC is the corresponding ternary probability simplex." ></td>
	<td class="line x" title="64:229	We define AP BM C1C8A2C1C8A2C1C8 DC AXCJBCBNBDCL to be a measure function that assigns a probability APB4D4 BD BND4 BE BND4 DC B5 to a pair of language models D4 BD and D4 BE together with a polarity model D4 DC . 3.2 Generative model Using the definitions presented above, and assuming that APB4B5 is given, we hypothesize that a new statement DB CX containing words DB CXBD BMBMBMDB CXD1 with sentiment polarity DC CX can be generated according to the following mechanism." ></td>
	<td class="line x" title="65:229	1." ></td>
	<td class="line x" title="66:229	Draw D4 D8 BND4 D7 and D4 DC from APB4A1BNA1BNA1B5." ></td>
	<td class="line x" title="67:229	2." ></td>
	<td class="line x" title="68:229	Sample DC CX from a polarity distribution D4 DC B4A1B5." ></td>
	<td class="line x" title="69:229	3." ></td>
	<td class="line x" title="70:229	For each position CY BP BDBMBMBMD1: (a) if CQ CXCY BPCC: draw DB CXCY from D4 D8 B4A1B5 ; (b) if CQ CXCY BPCB: draw DB CXCY from D4 D7 B4A1B5 . The probability of observing the new statement DB CXBD BMBMBMDB CXD1 under this mechanism is given by: CG D4 D8 BND4 D7 BND4 DC APB4D4 D8 BND4 D7 BND4 DC B5D4 DC B4DC CX B5 D1 CH CYBPBD B4 D4 D8 B4DB CXCY B5 if CQ CXCY BPCC D4 D7 B4DB CXCY B5 otherwise (1) The summation in equation (1) goes over all possible pairs of language models D4 D8 BND4 D7, but we can avoid integration by specifying a mass function APB4B5 that assigns nonzero probabilities to a finite subset of points in C1C8A2C1C8A2C1C8 DC . We accomplish this by using a nonparametric estimate for APB4B5, the details of which are provided below." ></td>
	<td class="line x" title="71:229	3.2.1 A nonparametric generative mass function We use a nonparametric estimate for APB4A1BNA1BNA1B5, which makes our generative model similar to kernel-based density estimators or Parzen-window classifiers (Silverman, 1986)." ></td>
	<td class="line x" title="72:229	The primary difference is that our model operates over discrete events (strings of words), and accordingly the mass function is defined over the space of distributions, rather than directly over the data points." ></td>
	<td class="line x" title="73:229	Our estimate relies on a collection of paired observations BV BP CUDBD8 CX BNDB D7 CX BNDC CX BM CXBPBDBMBMD2CV, which represent statements for which we know which words are topic words B4DB D8 CX B5, and which are sentiment words B4DB D7 CX B5." ></td>
	<td class="line x" title="74:229	Each of these observations corresponds to a unique point D4 D8CX BND4 D7CX BND4 DCCX in the space of paired distributions C1C8A2C1C8A2C1C8 DC, defined by the following coordinates: D4 D8CX B4DAB5 BP AL D8 AZB4DABNDB D8 CX B5BPAZB4DB D8 CX B5 B7 B4BDA0AL D8 B5CR D8DA D4 D7CX B4DAB5 BP AL D7 AZB4DABNDB D7 CX B5BPAZB4DB D7 CX B5 B7 B4BDA0AL D7 B5CR D7DA D4 DCCX B4DCB5 BP AL DC BD DCBPDC CX B7 B4BDA0AL DC B5BM (2) Here, AZB4DABNDB D8 CX B5 represents the number of times the word DA was observed in the topic part of statement CX, the length of which is denoted by AZB4DB D8 CX B5." ></td>
	<td class="line x" title="75:229	CR D8DA stands for the relative frequency of DA in the topic part of the collection." ></td>
	<td class="line x" title="76:229	The same definitions apply to the sentiment parameters AZB4DABNDB D7 CX B5, AZB4DB D7 CX B5 and CR D7DA . The Boolean indicator function BD DD returns one when the predicate DD is true and zero otherwise." ></td>
	<td class="line x" title="77:229	Metaparameters AL D8, AL D7 and AL DC specify the amount of Dirichlet smoothing (Zhai and Lafferty, 2001) applied to the topic, sentiment and polarity estimates respectively; values for these parameters are determined empirically." ></td>
	<td class="line x" title="78:229	We define APB4D4 D8 BND4 D7 BND4 DC B5 to have mass BD D2 when its argument D4 D8 BND4 D7 BND4 DC corresponds to some observation D4 D8CX BND4 D7CX BND4 DCCX, and zero otherwise: APB4D4 D8 BND4 D7 BND4 DC B5 BP BD D2 D2 CG CXBPBD BD D4 D8 BPD4 D8CX A2BD D4 D7 BPD4 D7CX A2BD D4 DC BPD4 DCCX BM (3) Equation (3) maintains empirical dependencies between the topic language model D4 D8 and the sentiment model D4 D7, because we assign nonzero prob347 ability mass only to pairs of models that actually co-occur in our observations." ></td>
	<td class="line x" title="79:229	3.2.2 Limitations of the model Our model represents each statement DB CX as a bag of words, or more formally an order-invariant sequence." ></td>
	<td class="line x" title="80:229	This representation is often confused with word independence, which is a much stronger assumption." ></td>
	<td class="line x" title="81:229	The generative model defined by equation (1) ignores the relative ordering of the words, but it does allow arbitrarily strong unordered dependencies among them." ></td>
	<td class="line x" title="82:229	To illustrate, consider the probability of observing the words unpredictable and plot in the same statement." ></td>
	<td class="line x" title="83:229	Suppose we set AL D8 BNAL D7 BPBD in equation (2), reducing the effects of smoothing." ></td>
	<td class="line x" title="84:229	It should be evident that C8B4unpredictable,plotB5 will be non-zero only when the two words actually co-occur in the training data." ></td>
	<td class="line x" title="85:229	By carefully selecting the smoothing parameters, the model can preserve dependencies between topic and sentiment words, and is quite capable of distinguishing the positive sentiment of unpredictable plot from the negative sentiment of unpredictable steering." ></td>
	<td class="line x" title="86:229	On the other hand, the model does ignore the ordering of the words, so it will not be able to differentiate the negative phrase gone from good to bad from its exact opposite." ></td>
	<td class="line x" title="87:229	Furthermore, our model is not well suited for modeling adjacency effects: the phrase unpredictable plot is treated in the same way as two separate words, unpredictable and plot, co-occurring in the same sentence." ></td>
	<td class="line x" title="88:229	3.3 Using the model for retrieval The generative model presented above can be applied to sentiment retrieval in the following fashion." ></td>
	<td class="line x" title="89:229	We start with a collection of statements BV and a query CUD5 D7 BND5 D8 BND5 DC CV supplied by the user." ></td>
	<td class="line x" title="90:229	We use the machinery outlined in Section 3.2 to estimate the topic and sentiment relevance models corresponding to the users information need, and then determine which statements in our collection most closely correspond to these models of relevance." ></td>
	<td class="line x" title="91:229	The topic relevance model CA D8 and sentiment relevance model CA D7 are estimated as follows." ></td>
	<td class="line x" title="92:229	We assume that our query D5 D7 BND5 D8 BND5 DC is a random sample from a distribution defined by equation (1), and then for each word DA we estimate the likelihood that DA would be observed if we sampled one more topic or sentiment word: CA D8 B4DAB5BP C8B4D5 D7 BND5 D8 DABND5 DC B5 C8B4D5 D7 BND5 D8 BND5 DC B5 BN CA D7 B4DAB5BP C8B4D5 D7 DABND5 D8 BND5 DC B5 C8B4D5 D7 BND5 D8 BND5 DC B5 BM (4) Both the numerator and denominator are computed according to equation (1), with the mass function APB4B5 given by equations (3) and (2)." ></td>
	<td class="line x" title="93:229	We use the notation D5DA to denote appending word DA to the string D5." ></td>
	<td class="line x" title="94:229	Estimation is done over the training corpus, which may or may not include numeric values of sentiment polarity.1 Once we have estimates for the topic and sentiment relevance models, we can rank testing statements DB by their similarity to CA D8 and CA D7 . We rank statements using a variation of cross-entropy, which was proposed by (Zhai, 2002): AB CG DA CA D8 B4DAB5D0D3CVD4 D8 B4DAB5B7B4BDA0ABB5 CG DA CA D7 B4DAB5D0D3CVD4 D7 B4DAB5BM (5) Here the summations extend over all words DA in the vocabulary, CA D8 and CA D7 are given by equation (4), while D4 D8 and D4 D7 are computed according to equation (2)." ></td>
	<td class="line x" title="95:229	A weighting parameter AB allows us to change the balance of topic and sentiment in the final ranking formula; its value is selected empirically." ></td>
	<td class="line x" title="96:229	4 Sentiment Retrieval Task 4.1 Task definition We define two variations of the sentiment retrieval task." ></td>
	<td class="line x" title="97:229	In one, the user supplies us with a numeric value for the desired polarity D5 DC." ></td>
	<td class="line x" title="98:229	In the other, the user supplies a set of seed words D5 D7, reflecting the desired sentiment." ></td>
	<td class="line x" title="99:229	The first task requires us to have polarity observations DC CX in our training data, while the second does not." ></td>
	<td class="line x" title="100:229	Task with training data: Input: (1) a set of topic keywordsD5 D8 and (2) a sentiment specification D5 DC BE CUA0BDBNBDCV." ></td>
	<td class="line x" title="101:229	In this case we assume D5 D7 to be the empty string." ></td>
	<td class="line x" title="102:229	Output: a ranked list of topic-relevant and sentiment-relevant sentences from the test data." ></td>
	<td class="line x" title="103:229	Task with seed words: Input: (1) a set of topic keywordsD5 D8 and (2) a set of sentiment seed words D5 D7 . In this case our model ignores D5 DC and DC CX . 1When the training corpus does not contain numeric polarity values DC CX, we assume APB4D4 D8 BND4 D7 BND4 DC B5BPAPB4D4 D8 BND4 D7 B5 and force D4 DC B4DC CX B5 to be a constant." ></td>
	<td class="line x" title="104:229	348 Output: a ranked list of topic-relevant and sentiment-relevant sentences from the test data." ></td>
	<td class="line x" title="105:229	In the first task, we split our corpus into three parts: (i) the training set, which was used for estimating the relevance models CA D7 and CA D8 ; (ii) the development set, which was used for tuning the model parameters AL D8, AL D7 and AB; and (iii) the testing set, from which we retrieved sentences in response to the query." ></td>
	<td class="line x" title="106:229	In the second task, we split the corpus into two parts: (i) the training set, which was used for tuning the model parameters; and (ii) the testing set, which was used for constructing CA D7 and CA D8 and from which we retrieved sentences in response to queries.2 The testing set was identical in both tasks." ></td>
	<td class="line x" title="107:229	Note that the sentiment relevance model CA D7 can be constructed in a topic-dependent fashion for both tasks." ></td>
	<td class="line x" title="108:229	4.2 Variations of the retrieval model slm: the retrieval model as described in Section 3.3." ></td>
	<td class="line x" title="109:229	lmt: the standard language modeling approach (Ponte and Croft, 1998; Song and Croft, 1999) on the topic keywords D5D8 for the topic part of the text DB D8." ></td>
	<td class="line x" title="110:229	lms: the standard language modeling approach on the sentiment keywords D5 D7 for the sentiment part of the text DB D7." ></td>
	<td class="line x" title="111:229	base: the weighted linear combination of lmt and lms." ></td>
	<td class="line x" title="112:229	rmt: only the topic relevance model was used for ranking using D5 D8 and for DB D8 .3 rms: only the sentiment relevance model was used for ranking using D5 D7 and for DB D7." ></td>
	<td class="line x" title="113:229	rmt-base: the slm model with AB BP BD, ignoring the sentiment relevance model." ></td>
	<td class="line x" title="114:229	rms-base: the slm model with AB BP BC, ignoring the topic relevance model." ></td>
	<td class="line x" title="115:229	2Because the training set was used for tuning the model parameters, no development set was required for this task." ></td>
	<td class="line x" title="116:229	3When we use the automatic annotation that is described in Section 5.2.2, we use the whole text instead of the topic part of the text, for the reasons given in that section." ></td>
	<td class="line x" title="117:229	This treatment is applied to the base, rmt-base, rms-base, rmt-rms, rmt-slm and slm models that are described in this section for using the automatic annotation." ></td>
	<td class="line x" title="118:229	However, we distinguish the lmt and rmt models using the topic part of the text and the lmtf and rmtf models, as baselines, using the whole text, respectively, even in the experiments using the automatic annotation." ></td>
	<td class="line x" title="119:229	rmt-rms: the rmt and rms models are treated independently." ></td>
	<td class="line x" title="120:229	rmt-slm: the rmt and rms-base models are combined." ></td>
	<td class="line x" title="121:229	lmtf: the standard language modeling approach using D5D8 for the nonsplit text, as baseline." ></td>
	<td class="line x" title="122:229	rmtf: the conventional relevance model was used for ranking using D5 D8 for the nonsplit text, as baseline." ></td>
	<td class="line x" title="123:229	lmtsf: the standard language modeling approach using both D5 D8 and D5 D7 for the nonsplit text, for reference." ></td>
	<td class="line x" title="124:229	rmtsf: the conventional relevance model was used for ranking using both D5 D8 and D5 D7 for the nonsplit text, for reference." ></td>
	<td class="line x" title="125:229	Note that the relevance models are constructed using training data for the training-based task, but are constructed using test data for the seed-based task, as mentioned in Section 4.1." ></td>
	<td class="line x" title="126:229	Therefore, the base model is only used for the training data, not for the test data, in the training-based task, while it can be performed for the test data in the case of the seed-based task." ></td>
	<td class="line x" title="127:229	Moreover, the lms, lmtsf and rmtsf models are based on the premise of using seed words to specify sentiments, and so they are only applicable to the seed-based task." ></td>
	<td class="line x" title="128:229	In the models described in this subsection, AL D8 and AL D7 in equation (2) were set to Dirichlet estimates (Zhai and Lafferty, 2001), AZB4DB D8 CX B5BPB4AZB4DB D8 CX B5B7AM D8 B5 and AZB4DB D7 CX B5BPB4AZB4DB D7 CX B5B7AM D7 B5 for the relevance models CA D8 and CA D7, respectively, in equation (4), and were fixed at 0.9 for ranking as in equation (5) for our experiments in Section 5." ></td>
	<td class="line x" title="129:229	Here, AM D8 and AM D7 were selected empirically according to the tasks described in Section 4.1." ></td>
	<td class="line x" title="130:229	The model parameter AB in equation (5) was also selected empirically in the same manner." ></td>
	<td class="line x" title="131:229	The number of ranked documents used in the relevance models CA D8 and CA D7, in equation (4), was selected empirically in the same manner as above; however, we fixed the number of terms used in the relevance models as 1000." ></td>
	<td class="line x" title="132:229	5 Experiments 5.1 Data set and evaluation measure We used the MPQA Opinion Corpus version 1.2 (Wilson et al. , 2005; Wiebe et al. , 2005) to measure the effectiveness of our sentiment re349 trieval models." ></td>
	<td class="line x" title="133:229	We summarize this data set as follows." ></td>
	<td class="line x" title="134:229	AF This corpus contains news articles collected from 187 different foreign and U.S. news sources from June 2001 to May 2002." ></td>
	<td class="line x" title="135:229	The corpus contains 535 documents, a total of 11,114 sentences." ></td>
	<td class="line x" title="136:229	AF The majority of the articles are on 10 different topics, which are labeled at document level, but, in addition to these, a number of additional articles were randomly selected from a larger corpus of 270,000 documents." ></td>
	<td class="line x" title="137:229	AF Each article was manually annotated using an annotation scheme for opinions and other private states at phrase level." ></td>
	<td class="line x" title="138:229	We only used the annotations for sentiments that included some attributes such as polarity and strength." ></td>
	<td class="line x" title="139:229	In this data set, the topic relevance for the 10 topics is known at the document level, but unknown at the sentence level." ></td>
	<td class="line x" title="140:229	We assumed that all the sentences in a relevant document could be considered relevant to the topic.4 This data set was annotated with sentiment polarities at the phrase level, but not explicitly annotated at the sentence level." ></td>
	<td class="line x" title="141:229	Therefore, we provided sentiment polarities at the sentence level to prepare training data and data for evaluation." ></td>
	<td class="line x" title="142:229	We set the sentence-level sentiment polarity equal to the polarity with the highest strength in each sentence.5 Queries were expressed using the title of one of the 10 topics and specified as positive or negative." ></td>
	<td class="line x" title="143:229	Thus, we had 20 types of queries for our experiments." ></td>
	<td class="line x" title="144:229	Because the supposed relevance judgments in this setting are imperfect at sentence level, we used bpref (Buckley and Voorhees, 2004), in both the training and testing phases, as it is known to be tolerant of imperfect judgments." ></td>
	<td class="line x" title="145:229	Bpref uses binary relevance judgments to define the preference relation (i.e. , any relevant document is preferred over any nonrelevant document for a given topic), while other measures, such as mean average precision, depend only on the ranks of the relevant documents." ></td>
	<td class="line x" title="146:229	4This is a strong assumption to make and may not be true in all cases." ></td>
	<td class="line x" title="147:229	A larger, more complete data set is required to perform a more detailed analysis, which is left as future work." ></td>
	<td class="line x" title="148:229	5We disregarded neutral and both if other polarities appeared." ></td>
	<td class="line x" title="149:229	We can also set the sentence-level sentiment polarity according to the presence of polarity in each sentence, but we did not consider this setting here." ></td>
	<td class="line x" title="150:229	5.2 Extracting sentiment expressions 5.2.1 Using manual annotation Because the MPQA corpus was annotated with phrase-level sentiments, we can use these annotations to split a sentence into a topic part DB D8 and a sentiment part DB D7." ></td>
	<td class="line x" title="151:229	The Krovetz stemmer (Krovetz, 1993) was applied to the topic part, the sentiment part and to the query terms6 and, for the retrieval experiments in Sections 5.3 and 5.4, a total of 418 stopwords from a standard stopword list were removed when they appeared." ></td>
	<td class="line x" title="152:229	5.2.2 Using automatic annotation In automatic extraction of sentiment expressions in this study, we detected sentiment-bearing words using lists of words with established polarities." ></td>
	<td class="line x" title="153:229	At this stage, topic dependence was not considered; however, at the stage of sentiment modeling, the topic dependence can be reflected, as described in Sections 3 and 4." ></td>
	<td class="line x" title="154:229	We first prepared a list of words indicating sentiments." ></td>
	<td class="line x" title="155:229	We used Hatzivassiloglou and McKeowns sentiment word list (Hatzivassiloglou and McKeown, 1997), which consists of 657 positive and 679 negative adjectives, and The General Inquirer (Stone et al. , 1966), which contains 1621 positive and 1989 negative words.7 By merging these lists, we obtained 1947 positive and 2348 negative words." ></td>
	<td class="line x" title="156:229	After stemming these words in the same manner as in Section 5.2.1, we were left with 1667 positive and 2129 negative words, which we will use hereafter in this paper." ></td>
	<td class="line x" title="157:229	The sentiment polarities are sometimes sensitive to the structural information, for instance, a negation expression reverses the following sentiment polarity." ></td>
	<td class="line x" title="158:229	To handle negation, every sentiment-bearing word was rewritten with a NEG suffix, such as good NEG, if an odd number of negation expressions was found within the five preceding words in the sentence." ></td>
	<td class="line x" title="159:229	To detect negation expressions, we used a predefined negation expression list." ></td>
	<td class="line x" title="160:229	This negation handling is similar to that used in (Das and Chen, 2001; Pang et al. , 2002)." ></td>
	<td class="line x" title="161:229	We extracted sentiment-bearing expressions using the list of words with established po6We used the topic labels attached to the MPQA corpus as the topic query terms D5 D8 in all the experiments in Sections 5.3 and 5.4." ></td>
	<td class="line x" title="162:229	7We extracted positive and negative words from the General Inquirer basically in the same manner as in (Turney and Littman, 2003); however, we did not exclude any words, unlike (Turney and Littman, 2003), where some seed words were excluded for the evaluation of their work." ></td>
	<td class="line x" title="163:229	350 Table 1: Sample probabilities from the sentiment relevance models Reaction to President Bushs 2002 presidential election Israeli settlements in Topic-independent Topic-independent 2002 State of the Union Address in Zimbabwe Gaza and West Bank w/ manual annot." ></td>
	<td class="line x" title="164:229	w/ automatic annot." ></td>
	<td class="line x" title="165:229	w/ manual annot." ></td>
	<td class="line x" title="166:229	w/ automatic annot." ></td>
	<td class="line x" title="167:229	w/ manual annot." ></td>
	<td class="line x" title="168:229	w/ automatic annot." ></td>
	<td class="line x" title="169:229	w/ manual annot." ></td>
	<td class="line x" title="170:229	w/ automatic annot." ></td>
	<td class="line x" title="171:229	C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB C8B4DBCYC9B5 DB 0.047 demand 0.029 state 0.030 support 0.067 state 0.042 support 0.039 support 0.041 ask 0.097 settle 0.031 expect 0.026 support 0.016 promise 0.034 support 0.033 legitimate 0.033 legitimate 0.036 agreed 0.032 peace 0.031 defend 0.014 lead 0.014 call 0.024 call 0.031 free 0.033 lead 0.036 call 0.025 state 0.031 invite 0.013 call 0.014 excellent 0.019 meet 0.029 congratulate 0.025 free 0.033 aim 0.022 secure 0.031 humane 0.013 minister 0.013 goal 0.017 minister 0.028 fair 0.025 fair 0.028 immediate 0.015 call 0.031 safeguard 0.011 right 0.013 express 0.015 promise 0.023 please 0.018 state 0.025 aware 0.014 conflict 0.031 nutritious 0.010 foreign 0.013 best 0.014 white 0.017 confident 0.017 congratulate 0.024 key 0.013 support 0.031 helpful 0.009 hope 0.012 count 0.013 foreign 0.017 call 0.015 call 0.022 expect 0.012 right 0.016 time 0.009 meet 0.012 cooperate 0.012 success 0.012 hopeful 0.015 meet 0.018 justify 0.011 attack 0.016 say 0.008 interest 0.011 proposal 0.011 defense 0.012 express 0.013 unity 0.018 honoure 0.011 minister 0.091 evil 0.037 state 0.065 evil 0.098 state 0.029 flaw 0.028 flaw 0.018 palestinian 0.100 settle 0.080 axis 0.022 evil 0.049 axis 0.051 evil 0.018 condemn 0.026 critic 0.013 protest 0.031 state 0.045 threat 0.015 right 0.022 critic 0.028 critic 0.015 true 0.023 state 0.012 decide 0.019 peace 0.033 qualify 0.015 prison 0.011 prepare 0.017 call 0.014 critic 0.022 opposition 0.011 peace 0.014 secure NEG 0.030 wrote 0.013 critic 0.010 recognize 0.012 interest 0.012 expect 0.019 reject 0.011 fatten 0.013 critic 0.020 particular 0.010 human 0.010 reckless 0.011 move 0.011 reject 0.017 condemn 0.011 believe 0.012 force 0.020 word 0.008 support 0.010 country 0.011 reject 0.011 s 0.016 legal 0.009 plan 0.012 attack 0.018 harsh 0.008 protest 0.009 upset 0.010 slam 0.011 fair 0.015 move 0.009 fear 0.012 war 0.015 reject 0.008 war 0.009 pick 0.010 right 0.011 free 0.015 democratic 0.009 mistake 0.011 believe 0.015 dangerous 0.008 force 0.009 eyesore 0.010 attack 0.010 angry 0.014 support 0.009 continue 0.011 minister The upper and lower tables correspond to positive and negative sentiments, respectively." ></td>
	<td class="line x" title="172:229	The topic-independent sentiment relevance models (in the left two columns) correspond to rms, and the topic-dependent models (in the rest of the columns) correspond to rms-base, which is used for slm." ></td>
	<td class="line x" title="173:229	larities, considering negation, as described above." ></td>
	<td class="line x" title="174:229	Note that we used the list of words with sentiments to extract sentiment expressions, but we did not use the predefined sentiments to model sentiment relevance." ></td>
	<td class="line x" title="175:229	Some expressions are sometimes used to express a certain topic, such as settlements in Israeli settlements in Gaza and West Bank; but at other times are used to express a certain sentiment, such as the same word in All parties signed courtmediated compromise settlements." ></td>
	<td class="line x" title="176:229	Therefore, we will use whole sentences to model topic relevance, while we will use the automatically extracted sentiment expressions to model sentiment relevance, in Sections 5.3 and 5.4." ></td>
	<td class="line x" title="177:229	5.3 Experiments on training-based task We conducted experiments on the training-based task described in Section 4.1, using either manual annotation as described in Section 5.2.1 or automatic annotation as described in Section 5.2.2." ></td>
	<td class="line x" title="178:229	Table 1 contrasts sample probabilities from topicindependent sentiment relevance models and those from topic-dependent sentiment relevance models." ></td>
	<td class="line x" title="179:229	In the left two columns of this table, two sets of sample probabilities using the topic-independent model are presented." ></td>
	<td class="line x" title="180:229	One was computed from the manual annotation and the other was computed from the automatic annotation." ></td>
	<td class="line x" title="181:229	In the remaining columns, samples using the topic-dependent model are shown according to the three topics: (1) reaction to President Bushs 2002 State of the Union Address, (2) 2002 presidential election in Zimbabwe, and (3) Israeli settlements in Gaza and West Bank." ></td>
	<td class="line x" title="182:229	A number of positive expressions appeared topic dependent, such as promise (stemmed from promising or not) and support for Topic (1), legitimate and congratulate for Topic (2) and justify and secure for Topic (3); while negative expressions appeared topic-dependent, such as critic (stemmed from criticism) and eyesore for Topic (1), flaw and condemn for Topic (2) and mistake and secure NEG (i.e. , secure was negated) for Topic (3)." ></td>
	<td class="line x" title="183:229	Some expressions were unexpectedly generated regardless of the types of annotation, e.g., palestinian for Topic (3); however, we found some characteristics in the results using automatic annotation." ></td>
	<td class="line x" title="184:229	Some expressions on opinions that did not convey sentiments, such as state, frequently appeared regardless of topic." ></td>
	<td class="line x" title="185:229	This sort of expression may effectively function as degrading sentences only conveying facts, but may function harmfully by catching sentences conveying opinions without sentiments in the task of sentiment retrieval." ></td>
	<td class="line x" title="186:229	Some topic expressions, such as settle (stemmed from settlement or not) for Topic (3), were generated, because such words convey positive sentiments in some other contexts and thus they were contained in the list of sentiment-bearing words that we used for automatic annotation." ></td>
	<td class="line x" title="187:229	This will not cause a topic relevance model to drift, because we modeled the topic relevance using whole sentences, as described in Section 5.2.2; however, it may harm the sentiment relevance model to some extent." ></td>
	<td class="line x" title="188:229	351 Table 2: Experimental results of training-based task using manually annotated data 10% 25% 40% Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145) lmt 0.1499 (0.1164) 0.1499 (0.1164) 0.1444 (0.1148) rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691) rmt 0.1712 (0.1619) 0.1712 (0.1619) 0.1922 (0.1705) rmt-base 0.1922 (0.1723) 0.2005 (0.1812) 0.2100* (0.1951) rms 0.0464 (0.0384) 0.0452 (0.0394) 0.0375 (0.0320) rms-base 0.0772 (0.0640) 0.0869 (0.0704) 0.0865 (0.0724) rmt-rms 0.2025 (0.1413) 0.2210 (0.1925) 0.2117 (0.2003) rmt-slm 0.2278* (0.1715) 0.2249 (0.1676) 0.1999 (0.1819) slm 0.2006 (0.1914) 0.2247 (0.1824) 0.2441* (0.2427) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="189:229	We performed retrieval experiments in the steps described in Section 4.1." ></td>
	<td class="line x" title="190:229	For this purpose, we split the data into three parts: (i) DC% as the training data, (ii) B4BHBC A0 DCB5% as the evaluation data, and (iii) BHBC% as the test data." ></td>
	<td class="line x" title="191:229	The test results of training-based task using manually annotated data and automatically annotated data are shown in Tables 2 and 3, respectively." ></td>
	<td class="line x" title="192:229	The scores were computed according to the bpref evaluation measure (Buckley and Voorhees, 2004), as mentioned in Section 5.1." ></td>
	<td class="line x" title="193:229	In addition to the bpref, mean average precision values are presented as AvgP in the tables, for reference.8 In these tables, the top row indicates the percentages of the training data DC." ></td>
	<td class="line x" title="194:229	It turned out that in all our experiments the appropriate fraction of training data was 40%." ></td>
	<td class="line x" title="195:229	In this setting, our slm model worked 76.1% better than the query likelihood model and 32.6% better than the conventional relevance model, when using manual annotation, and both improvements were statistically significant according to the Wilcoxon signed-rank test.9 When using automatic annotation, the slm model worked 67.2% better than the query likelihood model and 25.9% better than the conventional relevance model, where both improvements were statistically significant." ></td>
	<td class="line x" title="196:229	The rmt-base model also worked well with automatic annotation." ></td>
	<td class="line x" title="197:229	5.4 Experiments on seed-based task For experiments on the seed-based task that was described in Section 4.1, we used three groups of 8As mentioned in Section 5.1, the bpref is more appropriate for the evaluation of our experiments than the mean average precision." ></td>
	<td class="line x" title="198:229	9Significance tests involved only 20 queries, which makes it difficult to achieve statistical significance." ></td>
	<td class="line x" title="199:229	Table 3: Experimental results of training-based task using automatically annotated data 10% 25% 40% Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1389 (0.1135) 0.1389 (0.1135) 0.1386 (0.1145) lmt 0.1325 (0.0972) 0.1315 (0.0976) 0.1325 (0.0972) rmtf 0.1811 (0.1706) 0.1887 (0.1770) 0.1841 (0.1691) rmt 0.1490 (0.1418) 0.1762 (0.1584) 0.1695 (0.1485) rmt-base 0.2076* (0.1936) 0.2252* (0.2139) 0.2302* (0.2196) rms 0.0347 (0.0287) 0.0501 (0.0408) 0.0501 (0.0408) rms-base 0.0943 (0.0733) 0.1196 (0.0896) 0.1241 (0.0979) rmt-rms 0.1690 (0.1182) 0.2063 (0.1938) 0.1603 (0.1591) rmt-slm 0.1980 (0.1426) 0.2013 (0.1835) 0.2148 (0.1882) slm 0.2011 (0.1537) 0.2261* (0.1716) 0.2318* (0.1802) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="200:229	seed words: C3BTC5, CCCDCA and C7CABZ." ></td>
	<td class="line x" title="201:229	Each group consists of a positive word set D5 D7 B4B7B5 and a negative word set D5 D7 B4A0B5, as follows: C3BTC5: D5 D7 B4B7B5 BP CUgoodCV, and D5 D7 B4A0B5 BP CUbadCV." ></td>
	<td class="line x" title="202:229	CCCDCA: D5 D7 B4B7B5 BPCUgood, nice, excellent, positive, fortunate, correct, superiorCV, and D5 D7 B4A0B5 BPCUbad, nasty, poor, negative, unfortunate, wrong, inferiorCV." ></td>
	<td class="line x" title="203:229	C7CABZ: D5 D7 B4B7B5 BP CUsupport, demand, promise, want, hopeCV, and D5D7 B4A0B5 BP CUrefuse, accuse, criticism, fear, rejectCV." ></td>
	<td class="line o" title="204:229	C3BTC5 and CCCDCA were used in (Kamps and Marx, 2002) and (Turney and Littman, 2003), respectively." ></td>
	<td class="line x" title="205:229	We constructed C7CABZ considering sentiment-bearing words that may frequently appear in newspaper articles." ></td>
	<td class="line x" title="206:229	We experimented with the seed-based task, making use of each of these seed word groups, in the steps described in Section 4.1." ></td>
	<td class="line x" title="207:229	For this purpose, we split the data into two parts: (i) 50% as the estimation data and (ii) 50% as the test data." ></td>
	<td class="line x" title="208:229	The test results using manually annotated data and automatically annotated data are shown in Tables 4 and 5, respectively, where the scores were computed according to the bpref evaluation measure." ></td>
	<td class="line x" title="209:229	Mean average precision values are also presented as AvgP in the tables, for reference." ></td>
	<td class="line x" title="210:229	When using the manually annotated approach, our slm model worked well, especially with the seed word group C7CABZ, as shown in Table 4." ></td>
	<td class="line x" title="211:229	Using C7CABZ, the slm model worked 61.2% better than the query likelihood model and 15.2% better than the conventional relevance model, where both improvements were statistically significant according to the Wilcoxon signed-rank test." ></td>
	<td class="line x" title="212:229	Even 352 Table 4: Experimental results of seed-based task using manually annotated data ORG TUR KAM Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119) lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062) lmt 0.1501 (0.1171) 0.1501 (0.1171) 0.1501 (0.1171) base 0.1615 (0.1319) 0.1531 (0.1217) 0.1514 (0.1180) rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776) rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754) rmt 0.1974 (0.1826) 0.1974 (0.1826) 0.1974 (0.1826) rmt-base 0.1960 (0.1918) 0.1931 (0.1703) 0.1837 (0.1721) rms 0.0434 (0.0262) 0.0295 (0.0205) 0.0280 (0.0170) rms-base 0.1142 (0.1022) 0.1144 (0.0841) 0.1226 (0.0973) rmt-rms 0.1705 (0.1117) 0.1403 (0.1424) 0.1405 (0.0842) rmt-slm 0.2266* (0.2034) 0.2272* (0.2012) 0.2264* (0.2016) slm 0.2233* (0.2048) 0.2160 (0.1945) 0.2072 (0.1929) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="213:229	using the other seed word groups, the slm model worked 4956% better than the query likelihood model and 612% better than the conventional relevance model; however, the latter improvement was not statistically significant." ></td>
	<td class="line x" title="214:229	The rmt-slm model also worked well with manual annotation." ></td>
	<td class="line x" title="215:229	When using automatic annotation, the slm model worked 4648% better than the query likelihood model and 46% better than the conventional relevance model, as shown in Table 5." ></td>
	<td class="line x" title="216:229	The improvements over the conventional relevance model were statistically significant only when using CCCDCA or C3BTC5; however, the score when using C7CABZ is almost comparable with the others." ></td>
	<td class="line x" title="217:229	6 Conclusion We propose sentiment retrieval models in the framework of probabilistic generative models, not only assuming that a user inputs query terms expressing a certain topic, but also assuming that the user specifies a sentiment polarity of interest either as a sentiment specification D5 DC BE CUA0BDBNBDCV or as a set of sentiment seed words D5 D7." ></td>
	<td class="line x" title="218:229	For this purpose, we combine sentiment relevance models and topic relevance models, considering the topic dependence of the sentiment." ></td>
	<td class="line x" title="219:229	In our experiments, our model worked significantly better than standard language modeling approaches, both when using D5DC and D5D7, and with both manual and automatic annotation of the fragments expressing sentiments in text." ></td>
	<td class="line x" title="220:229	With D5 D7 and automatic annotation, our model still worked significantly better than the standard approaches; however, the perTable 5: Experimental results of seed-based task using automatically annotated data ORG TUR KAM Models Bpref (AvgP) Bpref (AvgP) Bpref (AvgP) lmtf 0.1385 (0.1119) 0.1385 (0.1119) 0.1385 (0.1119) lmtsf 0.1182 (0.1035) 0.1061 (0.0884) 0.1330 (0.1062) lmt 0.1325 (0.0972) 0.1325 (0.0972) 0.1325 (0.0972) basef 0.1550 (0.1369) 0.1451 (0.1188) 0.1416 (0.1142) rmtf 0.1938 (0.1776) 0.1938 (0.1776) 0.1938 (0.1776) rmtsf 0.1884 (0.1775) 0.1661 (0.1412) 0.1927 (0.1754) rmt 0.1757 (0.1578) 0.1757 (0.1578) 0.1757 (0.1578) rmt-base 0.1957 (0.1862) 0.1976 (0.1882) 0.1825 (0.1704) rms 0.0421 (0.0236) 0.0364 (0.0205) 0.0217 (0.0147) rms-base 0.1268 (0.1096) 0.1301 (0.1148) 0.1326 (0.1158) rmt-rms 0.1465 (0.1514) 0.1390 (0.1393) 0.1252 (0.0757) rmt-slm 0.1977 (0.1811) 0.2008 (0.1649) 0.1959 (0.1677) slm 0.2031 (0.1714) 0.2055* (0.1668) 0.2044* (0.1698) * indicates statistically significant improvement over rmtf where D4 BO BCBMBCBH with the twosided Wilcoxon signed-rank test." ></td>
	<td class="line x" title="221:229	formance did not reach that achieved with other settings." ></td>
	<td class="line x" title="222:229	We believe the performance can be improved with larger-scale data." ></td>
	<td class="line x" title="223:229	We experimented to find sentences that were relevant to a given topic and were appropriate to a given sentiment; however, our models can also be applied to textual chunks of any length, such as at document level or passage level." ></td>
	<td class="line x" title="224:229	Our model can be easily extended to opinion retrieval, if the opinion retrieval is defined as retrieving sentences or documents that contain either positive or negative sentiments." ></td>
	<td class="line x" title="225:229	This issue is worth pursuing in future work." ></td>
	<td class="line x" title="226:229	Approaches considering polarity strength or continuous values for the polarity specification, rather than using CUA0BDBNBDCV, can also be considered in future work." ></td>
	<td class="line x" title="227:229	Acknowledgments We thank James Allan, W. Bruce Croft and the anonymous reviewers for valuable discussions and comments." ></td>
	<td class="line x" title="228:229	This work was supported in part by the Overseas Research Scholars Program and the Grant-in-Aid for Scientific Research (#17680011) from the Ministry of Education, Culture, Sports, Science and Technology, Japan, in part by the Telecommunications Advancement Foundation, Japan, in part by the Center for Intelligent Information Retrieval, and in part by the Defense Advanced Research Projects Agency (DARPA), USA under contract number HR0011-06-C-0023." ></td>
	<td class="line x" title="229:229	Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect those of the sponsor." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1642
Fully Automatic Lexicon Expansion For Domain-Oriented Sentiment Analysis
Kanayama, Hiroshi;Nasukawa, Tetsuya;"></td>
	<td class="line x" title="1:252	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 355363, Sydney, July 2006." ></td>
	<td class="line x" title="2:252	c2006 Association for Computational Linguistics Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis Hiroshi Kanayama Tetsuya Nasukawa Tokyo Research Laboratory, IBM Japan, Ltd. 1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan {hkana,nasukawa}@jp.ibm.com Abstract This paper proposes an unsupervised lexicon building method for the detection of polar clauses, which convey positive or negative aspects in a specific domain." ></td>
	<td class="line x" title="3:252	The lexical entries to be acquired are called polar atoms, the minimum human-understandable syntactic structures that specify the polarity of clauses." ></td>
	<td class="line x" title="4:252	As a clue to obtain candidate polar atoms, we use context coherency, the tendency for same polarities to appear successively in contexts." ></td>
	<td class="line x" title="5:252	Using the overall density and precision of coherency in the corpus, the statistical estimation picks up appropriate polar atoms among candidates, without any manual tuning of the threshold values." ></td>
	<td class="line x" title="6:252	The experimental results show that the precision of polarity assignment with the automatically acquired lexicon was 94% on average, and our method is robust for corpora in diverse domains and for the size of the initial lexicon." ></td>
	<td class="line x" title="7:252	1 Introduction Sentiment Analysis (SA) (Nasukawa and Yi, 2003; Yi et al. , 2003) is a task to recognize writers feelings as expressed in positive or negative comments, by analyzing unreadably large numbers of documents." ></td>
	<td class="line x" title="8:252	Extensive syntactic patterns enable us to detect sentiment expressions and to convert them into semantic structures with high precision, as reported by Kanayama et al.(2004)." ></td>
	<td class="line x" title="10:252	From the example Japanese sentence (1) in the digital camera domain, the SA system extracts a sentiment representation as (2), which consists of a predicate and an argument with positive (+) polarity." ></td>
	<td class="line x" title="11:252	(1) Kono kamera-ha subarashii-to omou." ></td>
	<td class="line x" title="12:252	I think this camera is splendid. (2) [+] splendid(camera) SA in general tends to focus on subjectivesentimentexpressions, whichexplicitlydescribe an authors preference as in the above example (1)." ></td>
	<td class="line x" title="13:252	Objective (or factual) expressions such as in the following examples (3) and (4) may be out of scope even though they describe desirable aspects in a specific domain." ></td>
	<td class="line x" title="14:252	However, when customers or corporate users use SA system for their commercial activities, such domain-specific expressions have a more important role, since they convey strong or weak points of the product more directly, and may influence their choice to purchase a specific product, as an example." ></td>
	<td class="line x" title="15:252	(3) Kontorasuto-ga kukkiri-suru." ></td>
	<td class="line x" title="16:252	The contrast is sharp. (4) Atarashii kishu-ha zuumu-mo tsuite-iru." ></td>
	<td class="line x" title="17:252	The new model has a zoom lens, too. This paper addresses the Japanese version of Domain-oriented Sentiment Analysis, which identifies polar clauses conveying goodness and badness in a specific domain, including rather objective expressions." ></td>
	<td class="line x" title="18:252	Building domain-dependent lexicons for many domains is much harder work than preparing domainindependent lexicons and syntactic patterns, because the possible lexical entries are too numerous, and they may differ in each domain." ></td>
	<td class="line x" title="19:252	To solve this problem, we have devised an unsupervised method to acquire domaindependent lexical knowledge where a user has only to collect unannotated domain corpora." ></td>
	<td class="line x" title="20:252	The knowledge to be acquired is a domaindependent set of polar atoms." ></td>
	<td class="line x" title="21:252	A polar atom is a minimum syntactic structure specifying polarity in a predicative expression." ></td>
	<td class="line x" title="22:252	For example, to detect polar clauses in the sentences (3) 355 and (4)1, the following polar atoms (5) and (6) should appear in the lexicon: (5) [+] kukkiri-suru to be sharp (6) [+] tsuku  zuumu-ga to have  zoom lens-NOM The polar atom (5) specified the positive polarity of the verb kukkiri-suru." ></td>
	<td class="line x" title="23:252	This atom can be generally used for this verb regardless of its arguments." ></td>
	<td class="line x" title="24:252	In the polar atom (6), on the other hand, the nominative case of the verb tsuku (have) is limited to a specific noun zuumu (zoom lens), since the verb tsuku does not hold the polarity in itself." ></td>
	<td class="line x" title="25:252	The automatic decision for the scopes of the atoms is one of the major issues." ></td>
	<td class="line x" title="26:252	For lexical learning from unannotated corpora, our method uses context coherency in terms of polarity, an assumption that polar clauses with the same polarity appear successively unless the context is changed with adversative expressions." ></td>
	<td class="line x" title="27:252	Exploiting this tendency, we can collect candidate polar atoms with their tentative polarities as those adjacent to the polar clauses which have been identified by their domain-independent polar atoms in the initial lexicon." ></td>
	<td class="line x" title="28:252	We use both intrasentential and inter-sentential contexts to obtain more candidate polar atoms." ></td>
	<td class="line x" title="29:252	Our assumption is intuitively reasonable, but there are many non-polar (neutral) clauses adjacent to polar clauses." ></td>
	<td class="line x" title="30:252	Errors in sentence delimitation or syntactic parsing also result in false candidate atoms." ></td>
	<td class="line x" title="31:252	Thus, to adopt a candidate polar atom for the new lexicon, some threshold values for the frequencies or ratios are required, but they depend on the type of the corpus, the size of the initial lexicon, etc. Our algorithm is fully automatic in the sense that the criteria for the adoption of polar atoms are set automatically by statistical estimation based on the distributions of coherency: coherent precision and coherent density." ></td>
	<td class="line x" title="32:252	No manual tuning process is required, so the algorithm only needs unannotated domain corpora and the initial lexicon." ></td>
	<td class="line x" title="33:252	Thus our learning method can be used not only by the developers of the system, but also by endusers." ></td>
	<td class="line x" title="34:252	This feature is very helpful for users to 1The English translations are included only for convenience." ></td>
	<td class="line x" title="35:252	analyze documents in new domains." ></td>
	<td class="line x" title="36:252	In the next section, we review related work, and Section 3 describes our runtime SA system." ></td>
	<td class="line x" title="37:252	In Section 4, our assumption for unsupervised learning, context coherency and its key metrics, coherent precision and coherent density are discussed." ></td>
	<td class="line x" title="38:252	Section 5 describes our unsupervised learning method." ></td>
	<td class="line x" title="39:252	Experimental resultsareshowninSection6, andweconclude in Section 7." ></td>
	<td class="line x" title="40:252	2 Related Work Sentiment analysis has been extensively studied in recent years." ></td>
	<td class="line x" title="41:252	The target of SA in this paper is wider than in previous work." ></td>
	<td class="line x" title="42:252	For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions." ></td>
	<td class="line x" title="43:252	In contrast, our system detects factual polar clauses as well as sentiments." ></td>
	<td class="line x" title="44:252	Unsupervised learning for sentiment analysis is also being studied." ></td>
	<td class="line x" title="45:252	For example, Hatzivassiloglou and McKeown (1997) labeled adjectives as positive or negative, relying on semantic orientation." ></td>
	<td class="line oc" title="46:252	Turney (2002) used collocation with excellent or poor to obtain positive and negative clues for document classification." ></td>
	<td class="line x" title="47:252	In this paper, we use contextual information which is wider than for the contexts they used, and address the problem of acquiring lexical entries from the noisy clues." ></td>
	<td class="line x" title="48:252	Inter-sentential contexts as in our approach were used as a clue also for subjectivity analysis (Riloff and Wiebe, 2003; Pang and Lee, 2004), which is two-fold classification into subjective and objective sentences." ></td>
	<td class="line x" title="49:252	Compared to it, this paper solves a more difficult problem: three-fold classification into positive, negative and non-polar expressions using imperfect coherency in terms of sentiment polarity." ></td>
	<td class="line x" title="50:252	Learningmethodsforphrase-levelsentiment analysis closely share an objective of our approach." ></td>
	<td class="line x" title="51:252	Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling." ></td>
	<td class="line x" title="52:252	Their method iteratively assigns a polarity to a phrase, relying on semantic orientation of co-occurring words in specific relations in a sentence, but the scope of semantic orientation is limited to within a sentence." ></td>
	<td class="line x" title="53:252	Wilson et al.(2005) proposed supervised learning, dividing the resources into 356 Document to analyze a45 Sentence Delimitation   Sentences a63Proposition Detection Propositions Clauses a63Polarity Assignment +  Polarities Polar Clauses Modality Patterns Conjunctive Patterns a42 Polar Atoms a45 Figure 1: The flow of the clause-level SA." ></td>
	<td class="line x" title="55:252	prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively." ></td>
	<td class="line x" title="56:252	Wilson et al. prepared prior polarities from existing resources, and learned the context polarities by using prior polarities and annotated corpora." ></td>
	<td class="line x" title="57:252	Therefore the prerequisite data and learned data are opposite from those in our approach." ></td>
	<td class="line x" title="58:252	We took the approach used in this paper because we want to acquire more domain-dependent knowledge, and context polarity is easier to access in Japanese2." ></td>
	<td class="line x" title="59:252	Our approach and their work can complement each other." ></td>
	<td class="line x" title="60:252	3 Methodology of Clause-level SA As Figure 1 illustrates, the flow of our sentiment analysis system involves three steps." ></td>
	<td class="line x" title="61:252	The first step is sentence delimitation: the input document is divided into sentences." ></td>
	<td class="line x" title="62:252	The second step is proposition detection: propositions which can form polar clauses are identifiedineachsentence." ></td>
	<td class="line x" title="63:252	Thethirdstepis polarity assignment: the polarity of each proposition is examined by considering the polar atoms." ></td>
	<td class="line x" title="64:252	This section describes the last two processes, which are based on a deep sentiment analysis method analogous to machine translation (Kanayama et al. , 2004) (hereafter the MT method)." ></td>
	<td class="line x" title="65:252	3.1 Proposition Detection Our basic tactic for clause-level SA is the highprecision detection of polar clauses based on deep syntactic analysis." ></td>
	<td class="line x" title="66:252	Clause-level means that only predicative verbs and adjectives such 2For example, indirect negation such as caused by a subject nobody or a modifier seldom is rare in Japanese." ></td>
	<td class="line x" title="67:252	as in (7) are detected, and adnominal (attributive) usages of verbs and adjectives as in (8) are ignored, because utsukushii (beautiful) in (8) does not convey a positive polarity." ></td>
	<td class="line x" title="68:252	(7) E-ga utsukushii." ></td>
	<td class="line x" title="69:252	The picture is beautiful. (8) Utsukushii hito-ni aitai." ></td>
	<td class="line x" title="70:252	I want to meet a beautiful person. Here we use the notion of a proposition as a clause without modality, led by a predicative verb or a predicative adjective." ></td>
	<td class="line x" title="71:252	The propositions detected from a sentence are subject to the assignment of polarities." ></td>
	<td class="line x" title="72:252	Basically, we detect a proposition only at the head of a syntactic tree3." ></td>
	<td class="line x" title="73:252	However, this limitation reduces the recall of sentiment analysis to a very low level." ></td>
	<td class="line x" title="74:252	In the example (7) above, utsukushii is the head of the tree, while those initial clauses in (9) to (11) below are not." ></td>
	<td class="line x" title="75:252	In order to achieve higher recall while maintaininghighprecision, weapplytwotypes of syntactic patterns, modality patterns and conjunctive patterns4, to the tree structures from the full-parsing." ></td>
	<td class="line x" title="76:252	(9) Sore-ha utsukushii-to omou." ></td>
	<td class="line x" title="77:252	I think it is beautiful. (10) Sore-ha utsukushiku-nai." ></td>
	<td class="line x" title="78:252	It is not beautiful. (11) Sore-ga utsukushii-to yoi." ></td>
	<td class="line x" title="79:252	I hope it is beautiful. Modality patterns match some auxiliary verbs or corresponding sentence-final expressions, to allow for specific kinds of modality and negation." ></td>
	<td class="line x" title="80:252	One of the typical patterns is [ v to omou] (I think v )5, which allows utsukushii in (9) to be a proposition." ></td>
	<td class="line x" title="81:252	Also negation is handled with a modality pattern, such as [ v nai] (not v )." ></td>
	<td class="line x" title="82:252	In this case a neg feature is attached to the proposition to identify utsukushii in (10) as a negated proposition." ></td>
	<td class="line x" title="83:252	On the other hand, no proposition is identified in (11) due to the deliberate absence of a pattern [ v to yoi] (I hope v )." ></td>
	<td class="line x" title="84:252	We used a total of 103 domain-independent modality patterns, most of which are derived from the 3This is same as the rightmost part of the sentence since all Japanese modification is directed left to right." ></td>
	<td class="line x" title="85:252	4These two types of patterns correspond to auxiliary patterns in the MT method, and can be applied independent of domains." ></td>
	<td class="line x" title="86:252	5 v denotes a verb or an adjective." ></td>
	<td class="line x" title="87:252	357 coordinative (roughly and) -te, -shi, -ueni, -dakedenaku, -nominarazu causal (roughly because) -tame, -kara, -node adversative (roughly but) -ga, -kedo, -keredo, monono, -nodaga Table 1: Japanese conjunctions used for conjunctive patterns." ></td>
	<td class="line x" title="88:252	MT method, and some patterns are manually added for this work to achieve higher recall." ></td>
	<td class="line x" title="89:252	Another type of pattern is conjunctive patterns, which allow multiple propositions in a sentence." ></td>
	<td class="line x" title="90:252	We used a total of 22 conjunctive patterns also derived from the MT method, as exemplified in Table 1." ></td>
	<td class="line x" title="91:252	In such cases of coordinative clauses and causal clauses, both clauses can be polar clauses." ></td>
	<td class="line x" title="92:252	On the other hand, no proposition is identified in a conditional clause due to the absence of corresponding conjunctive patterns." ></td>
	<td class="line x" title="93:252	3.2 Polarity Assignment Using Polar Atoms To assign a polarity to each proposition, polar atoms in the lexicon are compared to the proposition." ></td>
	<td class="line x" title="94:252	A polar atom consists of polarity, verb or adjective, and optionally, its arguments." ></td>
	<td class="line x" title="95:252	Example (12) is a simple polar atom, where no argument is specified." ></td>
	<td class="line x" title="96:252	This atom matches any proposition whose head is utsukushii." ></td>
	<td class="line x" title="97:252	Example (13) is a complex polar atom, which assigns a negative polarity to any proposition whose head is the verb kaku and where the accusative case is miryoku." ></td>
	<td class="line x" title="98:252	(12) [+] utsukushii to be beautiful (13) [] kaku  miryoku-wo to lack  attraction-ACC A polarity is assigned if there exists a polar atom for which verb/adjective and the arguments coincide with the proposition, and otherwise no polarity is assigned." ></td>
	<td class="line x" title="99:252	The opposite polarity of the polar atom is assigned to a proposition which has the neg feature." ></td>
	<td class="line x" title="100:252	We used a total of 3,275 polar atoms, most of which are derived from an English sentiment lexicon (Yi et al. , 2003)." ></td>
	<td class="line x" title="101:252	According to the evaluation of the MT method (Kanayama et al. , 2004), highprecision sentiment analysis had been achieved using the polar atoms and patterns, where the splendid light have-zoom small-LCD  satisfied  high-price a73a27 a9 Inter-sentential Context a54 a54 Intra-sentential Context Figure 2: The concept of the intraand intersentential contexts, where the polarities are perfectly coherent." ></td>
	<td class="line x" title="102:252	The symbol  denotes the existence of an adversative conjunction." ></td>
	<td class="line x" title="103:252	system never took positive sentiment for negative and vice versa, and judged positive or negative to neutral expressions in only about 10% cases." ></td>
	<td class="line x" title="104:252	However, the recall is too low, and most of the lexicon is for domain-independent expressions, and thus we need more lexical entries to grasp the positive and negative aspects in a specific domain." ></td>
	<td class="line x" title="105:252	4 Context Coherency This section introduces the intraand intersententialcontextsinwhichweassume context coherency for polarity, and describes some preliminary analysis of the assumption." ></td>
	<td class="line x" title="106:252	4.1 Intra-sentential and Inter-sentential Context The identification of propositions described in Section 3.1 clarifies our viewpoint of the contexts." ></td>
	<td class="line x" title="107:252	Here we consider two types of contexts: intra-sentential context and intersentential context." ></td>
	<td class="line x" title="108:252	Figure 2 illustrates the context coherency in a sample discourse (14), where the polarities are perfectly coherent." ></td>
	<td class="line x" title="109:252	(14) Kono kamera-ha subarashii-to omou." ></td>
	<td class="line x" title="110:252	I think this camera is splendid. Karui-shi, zuumu-mo tsuite-iru." ></td>
	<td class="line x" title="111:252	Its light and has a zoom lens. Ekishou-ga chiisai-kedo, manzoku-da." ></td>
	<td class="line x" title="112:252	Though the LCD is small, Im satisfied. Tada, nedan-ga chotto takai." ></td>
	<td class="line x" title="113:252	But, the price is a little high. The intra-sentential context is the link between propositions in a sentence, which are detected as coordinative or causal clauses." ></td>
	<td class="line x" title="114:252	If there is an adversative conjunction such as -kedo (but) in the third sentence in (14), a flag is attached to the relation, as denoted with  in Figure 2." ></td>
	<td class="line x" title="115:252	Though there are differences in syntactic phenomena, this is sim358 shikashi (however), demo (but), sorenanoni (even though), tadashi (on condition that), dakedo (but), gyakuni (on the contrary), tohaie (although), keredomo (however), ippou (on the other hand) Table 2: Inter-sentential adversative expressions." ></td>
	<td class="line x" title="116:252	Domain Post." ></td>
	<td class="line x" title="117:252	Sent." ></td>
	<td class="line x" title="118:252	Len." ></td>
	<td class="line x" title="119:252	digital cameras 263,934 1,757,917 28.3 movies 163,993 637,054 31.5 mobile phones 155,130 609,072 25.3 cars 159,135 959,831 30.9 Table 3: The corpora from four domains used in this paper." ></td>
	<td class="line x" title="120:252	The Post. and Sent. columns denote the numbers of postings and sentences, respectively." ></td>
	<td class="line x" title="121:252	Len. is the average length of sentences (in Japanese characters)." ></td>
	<td class="line x" title="122:252	ilar to the semantic orientation proposed by Hatzivassiloglou and McKeown (1997)." ></td>
	<td class="line x" title="123:252	The inter-sentential context is the link between propositions in the main clauses of pairs of adjacent sentences in a discourse." ></td>
	<td class="line x" title="124:252	The polarities are assumed to be the same in the inter-sentential context, unless there is an adversative expression as those listed in Table 2." ></td>
	<td class="line x" title="125:252	If no proposition is detected as in a nominal sentence, the context is split." ></td>
	<td class="line x" title="126:252	That is, there is no link between the proposition of the previous sentence and that of the next sentence." ></td>
	<td class="line x" title="127:252	4.2 Preliminary Study on Context Coherency We claim these two types of context can be used for unsupervised learning as clues to assign a tentative polarity to unknown expressions." ></td>
	<td class="line x" title="128:252	To validate our assumption, we conducted preliminary observations using various corpora." ></td>
	<td class="line x" title="129:252	4.2.1 Corpora Throughout this paper we used Japanese corpora from discussion boards in four different domains, whose features are shown in Table 3." ></td>
	<td class="line x" title="130:252	All of the corpora have clues to the boundaries of postings, so they were suitable to identify the discourses." ></td>
	<td class="line x" title="131:252	4.2.2 Coherent Precision How strong is the coherency in the context proposed in Section 4.1?" ></td>
	<td class="line x" title="132:252	Using the polar clauses detected by the SA system with the initial lexicon, we observed the coherent precision of domain d with lexicon L, defined as: cp(d,L) = #(Coherent)#(Coherent)+#(Conflict) (15) where #(Coherent) and #(Conflict) are occurrence counts of the same and opposite polarities observed between two polar clauses as observed in the discourse." ></td>
	<td class="line x" title="133:252	As the two polar clauses, we consider the following types: Window." ></td>
	<td class="line x" title="134:252	A polar clause and the nearest polar clause which is found in the preceding n sentences in the discourse." ></td>
	<td class="line x" title="135:252	Context." ></td>
	<td class="line x" title="136:252	Two polar clauses in the intrasentential and/or inter-sentential context described in Section 4.1." ></td>
	<td class="line x" title="137:252	This is the viewpoint of context in our method." ></td>
	<td class="line x" title="138:252	Table 4 shows the frequencies of coherent pairs, conflicting pairs, and the coherent precision for half of the digital camera domain corpus." ></td>
	<td class="line x" title="139:252	Baseline is the percentage of positive clauses among the polar clauses6." ></td>
	<td class="line x" title="140:252	For the Window method, we tested for n=0, 1, 2, and ." ></td>
	<td class="line x" title="141:252	0 means two propositions within a sentence." ></td>
	<td class="line x" title="142:252	Apparently, the larger the window size, the smaller the cp value." ></td>
	<td class="line x" title="143:252	When the window size is , implying anywhere within a discourse, the ratio is larger than the baseline by only 2.7%, and thus these types of coherency are not reliable even though the number of clues is relatively large." ></td>
	<td class="line x" title="144:252	Context shows the coherency of the two types of context that we considered." ></td>
	<td class="line x" title="145:252	The cp values are much higher than those in the Window methods, because the relationships between adjacent pairs of clauses are handled more appropriately by considering syntactic trees, adversative conjunctions, etc. The cp values for inter-sentential and intra-sentential contexts are almost the same, and thus both contexts can be used to obtain 2.5 times more clues for the intra-sentential context." ></td>
	<td class="line x" title="146:252	In the rest of this paper we will use both contexts." ></td>
	<td class="line x" title="147:252	We also observed the coherent precision for each domain corpus." ></td>
	<td class="line x" title="148:252	The results in the center column of Table 5 indicate the number is slightly different among corpora, but all of them are far from perfect coherency." ></td>
	<td class="line x" title="149:252	6Ifthereisapolarclausewhosepolarityisunknown, the polarity is correctly predicted with at least 57.0% precision by assuming positive." ></td>
	<td class="line x" title="150:252	359 Model Coherent Conflict cp(d,L) Baseline 57.0% Window n = 0 3,428 1,916 64.1% n = 1 11,448 6,865 62.5% n = 2 16,231 10,126 61.6% n =  26,365 17,831 59.7% Context intra." ></td>
	<td class="line x" title="151:252	2,583 996 72.2% inter." ></td>
	<td class="line x" title="152:252	3,987 1,533 72.2% both 6,570 2,529 72.2% Table 4: Coherent precision with various viewpoints of contexts." ></td>
	<td class="line x" title="153:252	Domain cp(d,L) cd(d,L) digital cameras 72.2% 7.23% movies 76.7% 18.71% mobile phones 72.9% 7.31% cars 73.4% 7.36% Table 5: Coherent precision and coherent density for each domain." ></td>
	<td class="line x" title="154:252	4.2.3 Coherent Density Besides the conflicting cases, there are many more cases where a polar clause does not appear in the polar context." ></td>
	<td class="line x" title="155:252	We also observed the coherent density of the domain d with the lexicon L defined as: cd(d,L) = #(Coherent)#(Polar) (16) This indicates the ratio of polar clauses that appear in the coherent context, among all of the polar clauses detected by the system." ></td>
	<td class="line x" title="156:252	The right column of Table 5 shows the coherent density in each domain." ></td>
	<td class="line x" title="157:252	The movie domain has notably higher coherent density than the others." ></td>
	<td class="line x" title="158:252	This indicates the sentiment expressions are more frequently used in the movie domain." ></td>
	<td class="line x" title="159:252	The next section describes the method of our unsupervised learning using this imperfect context coherency." ></td>
	<td class="line x" title="160:252	5 Unsupervised Learning for Acquisition of Polar Atoms Figure 3 shows the flow of our unsupervised learning method." ></td>
	<td class="line x" title="161:252	First, the runtime SA system identifies the polar clauses, and the candidate polar atoms are collected." ></td>
	<td class="line x" title="162:252	Then, each candidate atom is validated using the two metrics in the previous section, cp and cd, which are calculated from all of the polar clauses found in the domain corpus." ></td>
	<td class="line x" title="163:252	Domain Corpus d a45 Initial Lexicon L a42 SA a54 Polar Clauses contexta45 a18a85 Candidate Polar Atoms f(a),p(a),n(a) cd(d,L) cp(d,L) ?test a54 a82a9 a78 a63 ? testa45 a14 a45 a45 New Lexicon Figure 3: The flow of the learning process." ></td>
	<td class="line x" title="164:252	ID Candidate Polar Atom f(a) p(a) n(a) 1* chiisai to be small 3,014 226 227 2 shikkari-suru to be firm 246 54 10 3 chiisai  bodii-ga 11 4 0to be small  body-NOM 4* todoku  mokuyou-ni 2 0 2to be deliveredon Thursday Table 6: Examples of candidate polar atoms and their frequencies." ></td>
	<td class="line x" title="165:252	* denotes that it should not be added to the lexicon." ></td>
	<td class="line x" title="166:252	f(a), p(a), and n(a) denote the frequency of the atom and in positive and negative contexts, respectively." ></td>
	<td class="line x" title="167:252	5.1 Counts of Candidate Polar Atoms From each proposition which does not have a polarity, candidate polar atoms in the form of simple atoms (just a verb or adjective) or complex atoms (a verb or adjective and its rightmost argument consisting of a pair of a noun and a postpositional) are extracted." ></td>
	<td class="line x" title="168:252	For each candidate polar atom a, the total appearances f(a), and the occurrences in positive contexts p(a) and negative contexts n(a) are counted, based on the context of the adjacent clauses (using the method described in Section 4.1)." ></td>
	<td class="line x" title="169:252	If the proposition has the neg feature, the polarity is inverted." ></td>
	<td class="line x" title="170:252	Table 6 shows examples of candidate polar atoms with their frequencies." ></td>
	<td class="line x" title="171:252	5.2 Determination for Adding to Lexicon Among the located candidate polar atoms, how can we distinguish true polar atoms, which should be added to the lexicon, from fake polar atoms, which should be discarded?" ></td>
	<td class="line x" title="172:252	As shown in Section 4, both the coherent precision (72-77%) and the coherent density (7-19%) are so small that we cannot rely on each single appearance of the atom in the polar context." ></td>
	<td class="line x" title="173:252	One possible approach is to set the threshold values for frequency in a polar context, max(p(a),n(a)) and for the ratio of appearances in polar contexts among the to360 tal appearances, max(p(a),n(a))f(a) . However, the optimum threshold values should depend on the corpus and the initial lexicon." ></td>
	<td class="line x" title="174:252	In order to set general criteria, here we assume that a true positive polar atom a should have higher p(a)f(a) than its average i.e. coherent density, cd(d,L+a), and also have higher p(a) p(a)+n(a) than its average i.e. coherent precision, cp(d,L+a) and these criteria should be met with 90% confidence, where L+a is the initial lexicon with a added." ></td>
	<td class="line x" title="175:252	Assuming the binomial distribution, a candidate polar atom is adopted as a positive polar atom7 if both (17) and (18) are satisfied8." ></td>
	<td class="line x" title="176:252	q > cd(d,L), where p(a)summationdisplay k=0 f(a)Ckqk(1q)f(a)k = 0.9 (17) r > cp(d,L) or n(a) = 0, where p(a)summationdisplay k=0 p(a)+n(a)Ckrk(1r)p(a)+n(a)k= 0.9 (18) We can assume cd(d,L+a) similarequal cd(d,L), and cp(d,L+a) similarequal cp(d,L) when L is large." ></td>
	<td class="line x" title="177:252	We compute the confidence interval using approximation with the F-distribution (Blyth, 1986)." ></td>
	<td class="line x" title="178:252	These criteria solve the problems in minimum frequency and scope of the polar atoms simultaneously." ></td>
	<td class="line x" title="179:252	In the example of Table 6, the simple atom chiisai (ID=1) is discarded because it does not meet (18), while the complex atom chiisai  bodii-ga (ID=3) is adopted as a positive atom." ></td>
	<td class="line x" title="180:252	shikkari-suru (ID=2) is adopted as a positive simple atom, even though 10 cases out of 64 were observed in the negative context." ></td>
	<td class="line x" title="181:252	On the other hand, todoku  mokuyou-ni (ID=4) is discarded because it does not meet (17), even though n(a)f(a) = 1.0, i.e. always observed in negative contexts." ></td>
	<td class="line x" title="182:252	6 Evaluation 6.1 Evaluation by Polar Atoms First we propose a method of evaluation of the lexical learning." ></td>
	<td class="line x" title="183:252	7The criteria for the negative atoms are analogous." ></td>
	<td class="line x" title="184:252	8nCr notation is used here for combination (n choose k)." ></td>
	<td class="line x" title="185:252	Annotator B Positive Neutral Negative AnnoPositive 65 11 3 tator Neutral 3 72 0 A Negative 1 4 41 Table 7: Agreement of two annotators judgments of 200 polar atoms." ></td>
	<td class="line x" title="186:252	=0.83." ></td>
	<td class="line x" title="187:252	It is costly to make consistent and large gold standards in multiple domains, especially in identification tasks such as clauselevel SA (cf.classification tasks)." ></td>
	<td class="line x" title="189:252	Therefore we evaluated the learning results by asking human annotators to classify the acquired polar atoms as positive, negative, and neutral, instead of the instances of polar clauses detected with the new lexicon." ></td>
	<td class="line x" title="190:252	This can be done because the polar atoms themselves are informative enough to imply to humans whether the expressions hold positive or negative meanings in the domain." ></td>
	<td class="line x" title="191:252	To justify the reliability of this evaluation method, two annotators9 evaluated 200 randomly selected candidate polar atoms in the digital camera domain." ></td>
	<td class="line x" title="192:252	The agreement results are shown in Table 7." ></td>
	<td class="line x" title="193:252	The manual classification was agreed upon in 89% of the cases and the Kappa value was 0.83, which is high enough to be considered consistent." ></td>
	<td class="line x" title="194:252	Using manual judgment of the polar atoms, we evaluated the performance with the following three metrics." ></td>
	<td class="line x" title="195:252	Type Precision." ></td>
	<td class="line x" title="196:252	The coincidence rate of the polarity between the acquired polar atom and the human evaluators judgments." ></td>
	<td class="line x" title="197:252	It is always false if the evaluators judged it as neutral. Token Precision." ></td>
	<td class="line x" title="198:252	The coincidence rate of the polarity, weighted by its frequency in the corpus." ></td>
	<td class="line x" title="199:252	This metric emulates the precision of the detection of polar clauses with newly acquired poler atoms, in the runtime SA system." ></td>
	<td class="line x" title="200:252	Relative Recall." ></td>
	<td class="line x" title="201:252	The estimated ratio of the number of detected polar clauses with the expanded lexicon to the number of detected polar clauses with the initial lex9For each domain, we asked different annotators who are familiar with the domain." ></td>
	<td class="line x" title="202:252	They are not the authors of this paper." ></td>
	<td class="line x" title="203:252	361 Domain # Type Token RelativePrec." ></td>
	<td class="line x" title="204:252	Prec." ></td>
	<td class="line x" title="205:252	Recall digital cameras 708 65% 96.5% 1.28 movies 462 75% 94.4% 1.19 mobile phones 228 54% 92.1% 1.13 cars 487 68% 91.5% 1.18 Table 8: Evaluation results with our method." ></td>
	<td class="line x" title="206:252	The column # denotes the number of polar atoms acquired in each domain." ></td>
	<td class="line x" title="207:252	icon." ></td>
	<td class="line x" title="208:252	Relative recall will be 1 when no newpolaratomisacquired." ></td>
	<td class="line x" title="209:252	Sincetheprecision was high enough, this metric can be used for approximation of the recall, which is hard to evaluate in extraction tasks such as clause-/phrase-level SA." ></td>
	<td class="line x" title="210:252	6.2 Robustness for Different Conditions 6.2.1 Diversity of Corpora For each of the four domain corpora, the annotators evaluated 100 randomly selected polar atoms which were newly acquired by our method, to measure the precisions." ></td>
	<td class="line x" title="211:252	Relative recall is estimated by comparing the numbers of detected polar clauses from randomly selected 2,000 sentences, with and without the acquired polar atoms." ></td>
	<td class="line x" title="212:252	Table 8 shows the results." ></td>
	<td class="line oc" title="213:252	The token precision is higher than 90% in all of the corpora, including the movie domain, which is considered to be difficult for SA (Turney, 2002)." ></td>
	<td class="line x" title="214:252	This is extremely high precision for this task, because the correctness of both the extraction and polarity assignment was evaluated simultaneously." ></td>
	<td class="line x" title="215:252	The relative recall 1.28 in the digital camera domain means the recall is increased from 43%10 to 55%." ></td>
	<td class="line x" title="216:252	The difference was smaller in other domains, but the domain-dependent polar clauses are much informative than general ones, thus the highprecision detection significantly enhances the system." ></td>
	<td class="line x" title="217:252	To see the effects of our method, we conducted a control experiment which used preset criteria." ></td>
	<td class="line x" title="218:252	To adopt the candidate atom a, the frequency of polarity, max(p(a),n(a)) was required to be 3 or more, and the ratio of polarity, max(p(a),n(a))f(a) was required to be higher than the threshold ." ></td>
	<td class="line x" title="219:252	Varying  from 0.05 to 10The human evaluation result for digital camera domain (Kanayama et al. , 2004)." ></td>
	<td class="line x" title="220:252	a54  a45 Relative recall Token precision 0.5 1 1.0 1.1 1.2 star star  = 0.05 star  = 0.1starstar = 0.3starstarstar star star = 0.8 star stardigital cameras    = 0.05 = 0.1    = 0.3     movies (our method) a14a89 Figure 4: Relative recall vs. token precision with various preset threshold values  for the digital camera and movie domains." ></td>
	<td class="line x" title="221:252	The rightmost star and circle denote the performance of our method." ></td>
	<td class="line x" title="222:252	0.8, we evaluated the token precision and the relative recall in the domains of digital cameras and movies." ></td>
	<td class="line x" title="223:252	Figure 4 shows the results." ></td>
	<td class="line x" title="224:252	The results showed both relative recall and token precision were lower than in our method for every , in both corpora." ></td>
	<td class="line x" title="225:252	The optimum  was 0.3 in the movie domain and 0.1 in the digital camera domain." ></td>
	<td class="line x" title="226:252	Therefore, in this preset approach, a tuning process is necessary for each domain." ></td>
	<td class="line x" title="227:252	Our method does not require this tuning, and thus fully automatic learning was possible." ></td>
	<td class="line x" title="228:252	Unlike the normal precision-recall tradeoff, the token precision in the movie domain got lower when the  is strict." ></td>
	<td class="line x" title="229:252	This is due to the frequent polar atoms which can be acquired at the low ratios of the polarity." ></td>
	<td class="line x" title="230:252	Our method does not discard these important polar atoms." ></td>
	<td class="line x" title="231:252	6.2.2 Size of the Initial Lexicon We also tested the performance while varying the size of the initial lexicon L. We prepared three subsets of the initial lexicon, L0.8, L0.5, and L0.2, removing polar atoms randomly." ></td>
	<td class="line x" title="232:252	These lexicons had 0.8, 0.5, 0.2 times the polar atoms, respectively, compared to L. Table 9 shows the precisions and recalls using these lexicons for the learning process." ></td>
	<td class="line x" title="233:252	Though the cd values vary, the precision was stable, which means that our method was robust even for different sizes of the lexicon." ></td>
	<td class="line x" title="234:252	The smaller the initial lexicon, the higher the relative recall, because the polar atoms which were removed from L were recovered in the learning process." ></td>
	<td class="line x" title="235:252	This result suggests the possibility of 362 lexicon cd Token Prec." ></td>
	<td class="line x" title="236:252	Relative Rec." ></td>
	<td class="line x" title="237:252	L 7.2% 96.5% 1.28 L0.8 6.1% 97.5% 1.41 L0.5 3.9% 94.2% 2.10 L0.2 3.6% 84.8% 3.55 Table 9: Evaluation results for various sizes of the initial lexicon (the digital camera domain)." ></td>
	<td class="line x" title="238:252	the bootstrapping method from a small initial lexicon." ></td>
	<td class="line x" title="239:252	6.3 Qualitative Evaluation As seen in the agreement study, the polar atoms used in our study were intrinsically meaningful to humans." ></td>
	<td class="line x" title="240:252	This is because the atoms are predicate-argument structures derived from predicative clauses, and thus humans could imagine the meaning of a polar atom by generating the corresponding sentence in its predicative form." ></td>
	<td class="line x" title="241:252	In the evaluation process, some interesting results were observed." ></td>
	<td class="line x" title="242:252	For example, a negative atom nai  kerare-ga (to be free from vignetting) was acquired in the digital camera domain." ></td>
	<td class="line x" title="243:252	Even the evaluator who was familiar with digital cameras did not know the term kerare (vignetting), but after looking up the dictionary she labeled it as negative." ></td>
	<td class="line x" title="244:252	Our learning method could pick up such technical terms and labeled them appropriately." ></td>
	<td class="line x" title="245:252	Also, there were discoveries in the error analysis." ></td>
	<td class="line x" title="246:252	An evaluator assigned positive to aru  kamera-ga (to have camera) in the mobile phone domain, but the acquired polar atom had the negative polarity." ></td>
	<td class="line x" title="247:252	This was actually an insight from the recent opinions that many userswantphoneswithoutcamerafunctions11." ></td>
	<td class="line x" title="248:252	7 Conclusion We proposed an unsupervised method to acquire polar atoms for domain-oriented SA, and demonstrated its high performance." ></td>
	<td class="line x" title="249:252	The lexicon can be expanded automatically by using unannotated corpora, and tuning of the threshold values is not required." ></td>
	<td class="line x" title="250:252	Therefore even end-users can use this approach to improve the sentiment analysis." ></td>
	<td class="line x" title="251:252	These features allow them to do on-demand analysis of more narrow domains, such as the domain of digital 11Perhaps because cameras tend to consume battery power and some users dont need them." ></td>
	<td class="line x" title="252:252	cameras of a specific manufacturer, or the domain of mobile phones from the female users point of view." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1650
Automatically Assessing Review Helpfulness
Kim, Soo-Min;Pantel, Patrick;Chklovski, Timothy;Pennacchiotti, Marco;"></td>
	<td class="line x" title="1:189	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423430, Sydney, July 2006." ></td>
	<td class="line x" title="2:189	c2006 Association for Computational Linguistics Automatically Assessing Review Helpfulness Soo-Min Kim , Patrick Pantel , Tim Chklovski , Marco Pennacchiotti   Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 {skim,pantel,timc}@isi.edu  ART Group DISP University of Rome Tor Vergata Viale del Politecnico 1 Rome, Italy pennacchiotti@info.uniroma2.it Abstract User-supplied reviews are widely and increasingly used to enhance ecommerce and other websites." ></td>
	<td class="line x" title="3:189	Because reviews can be numerous and varying in quality, it is important to assess how helpful each review is. While review helpfulness is currently assessed manually, in this paper we consider the task of automatically assessing it." ></td>
	<td class="line x" title="4:189	Experiments using SVM regression on a variety of features over Amazon.com product reviews show promising results, with rank correlations of up to 0.66." ></td>
	<td class="line x" title="5:189	We found that the most useful features include the length of the review, its unigrams, and its product rating." ></td>
	<td class="line x" title="6:189	1 Introduction Unbiased user-supplied reviews are solicited ubiquitously by online retailers like Amazon.com, Overstock.com, Apple.com and Epinions.com, movie sites like imdb.com, traveling sites like citysearch.com, open source software distributors like cpanratings.perl.org, and countless others." ></td>
	<td class="line x" title="7:189	Because reviews can be numerous and varying in quality, it is important to rank them to enhance customer experience." ></td>
	<td class="line x" title="8:189	In contrast with ranking search results, assessing relevance when ranking reviews is of little importance because reviews are directly associated with the relevant product or service." ></td>
	<td class="line x" title="9:189	Instead, a key challenge when ranking reviews is to determine which reviews the customers will find helpful." ></td>
	<td class="line x" title="10:189	Most websites currently rank reviews by their recency or product rating (e.g. , number of stars in Amazon.com reviews)." ></td>
	<td class="line x" title="11:189	Recently, more sophisticated ranking schemes measure reviews by their helpfulness, which is typically estimated by having users manually assess it." ></td>
	<td class="line x" title="12:189	For example, on Amazon.com, an interface allows customers to vote whether a particular review is helpful or not." ></td>
	<td class="line x" title="13:189	Unfortunately, newly written reviews and reviews with few votes cannot be ranked as several assessments are required in order to properly estimate helpfulness." ></td>
	<td class="line x" title="14:189	For example, for all MP3 player products on Amazon.com, 38% of the 20,919 reviews received three or fewer helpfulness votes." ></td>
	<td class="line x" title="15:189	Another problem is that low-traffic items may never gather enough votes." ></td>
	<td class="line x" title="16:189	Among the MP3 player reviews that were authored at least three months ago on Amazon.com, still only 31% had three or fewer helpfulness votes." ></td>
	<td class="line x" title="17:189	It would be useful to assess review helpfulness automatically, as soon as the review is written." ></td>
	<td class="line x" title="18:189	This would accelerate determining a reviews ranking and allow a website to provide rapid feedback to review authors." ></td>
	<td class="line x" title="19:189	In this paper, we investigate the task of automatically predicting review helpfulness using a machine learning approach." ></td>
	<td class="line x" title="20:189	Our main contributions are:  A system for automatically ranking reviews according to helpfulness; using state of the art SVM regression, we empirically evaluate our system on a real world dataset collected from Amazon.com on the task of reconstructing the helpfulness ranking; and  An analysis of different classes of features most important to capture review helpfulness; including structural (e.g. , html tags, punctuation, review length), lexical (e.g. , ngrams), syntactic (e.g. , percentage of verbs and nouns), semantic (e.g. , product feature mentions), and meta-data (e.g. , star rating)." ></td>
	<td class="line x" title="21:189	2 Relevant Work The task of automatically assessing product review helpfulness is related to these broader areas 423 of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification." ></td>
	<td class="line nc" title="22:189	In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here  assessing review helpfulness." ></td>
	<td class="line x" title="23:189	Pang and Lee (2005) have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (discussed in Section 5)." ></td>
	<td class="line x" title="24:189	However, a users overall rating for the product is often already available." ></td>
	<td class="line x" title="25:189	Helpfulness, on the other hand, is valuable to assess because it is not explicitly known in current approaches until many users vote on the helpfulness of a review." ></td>
	<td class="line x" title="26:189	In opinion and sentiment analysis, the focus is on distinguishing between statements of fact vs. opinion, and on detecting the polarity of sentiments being expressed." ></td>
	<td class="line o" title="27:189	Many researchers have worked in various facets of opinion analysis." ></td>
	<td class="line oc" title="28:189	Pang et al.(2002) and Turney (2002) classified sentiment polarity of reviews at the document level." ></td>
	<td class="line x" title="30:189	Wiebe et al.(1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features." ></td>
	<td class="line x" title="32:189	Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process." ></td>
	<td class="line x" title="33:189	Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words." ></td>
	<td class="line o" title="34:189	These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004)." ></td>
	<td class="line x" title="35:189	In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005)." ></td>
	<td class="line x" title="36:189	In this paper, we explore the use of some semantic features for review helpfulness ranking." ></td>
	<td class="line x" title="37:189	Another potential relevant classification task is academic and commercial efforts on detecting email spam messages 1, which aim to capture a much broader notion of helpfulness." ></td>
	<td class="line x" title="38:189	For an SVM-based approach, see (Drucker et al 1999)." ></td>
	<td class="line x" title="39:189	Finally, a related area is work on automatic essay scoring, which seeks to rate the quality of an essay (Attali and Burstein 2006; Burstein et al. 2004)." ></td>
	<td class="line x" title="40:189	The task is important for reducing the human effort required in scoring large numbers 1 See http://www.ceas.cc/, http://spamconference.org/ of student essays regularly written for standard tests such as the GRE." ></td>
	<td class="line x" title="41:189	The exact scoring approaches developed in commercial systems are often not disclosed." ></td>
	<td class="line x" title="42:189	However, more recent work on one of the major systems, e-rater 2.0, has focused on systematizing and simplifying the set of features used (Attali and Burstein 2006)." ></td>
	<td class="line x" title="43:189	Our choice of features to test was partially influenced by the features discussed by Attali and Burstein." ></td>
	<td class="line x" title="44:189	At the same time, due to differences in the tasks, we did not use features aimed at assessing essay structure such as discourse structure analysis features." ></td>
	<td class="line x" title="45:189	Our observations suggest that even helpful reviews vary widely in their discourse structure." ></td>
	<td class="line x" title="46:189	We present the features which we have used below, in Section 3.2." ></td>
	<td class="line x" title="47:189	3 Modeling Review Helpfulness In this section, we formally define the learning task and we investigate several features for assessing review helpfulness." ></td>
	<td class="line x" title="48:189	3.1 Task Definition Formally, given a set of reviews R for a particular product, our task is to rank the reviews according to their helpfulness." ></td>
	<td class="line x" title="49:189	We define a review helpfulness function, h, as: () () () ()rratingrrating rrating Rrh + + + = (1) where rating + (r) is the number of people that will find a review helpful and rating (r) is the number of people that will find the review unhelpful." ></td>
	<td class="line x" title="50:189	For evaluation, we resort to estimates of h from manual review assessments on websites like Amazon.com, as described in Section 4." ></td>
	<td class="line x" title="51:189	3.2 Features One aim of this paper is to investigate how well different classes of features capture the helpfulness of a review." ></td>
	<td class="line x" title="52:189	We experimented with various features organized in five classes: Structural, Lexical, Syntactic, Semantic, and Meta-data." ></td>
	<td class="line x" title="53:189	Below we describe each feature class in turn." ></td>
	<td class="line x" title="54:189	Structural Features Structural features are observations of the document structure and formatting." ></td>
	<td class="line x" title="55:189	Properties such as review length and average sentence length are hypothesized to relate structural complexity to helpfulness." ></td>
	<td class="line x" title="56:189	Also, HTML formatting tags could help in making a review more readable, and consequently more helpful." ></td>
	<td class="line x" title="57:189	We experimented with the following features: 424  Length (LEN): The total number of tokens in a syntactic analysis 2 of the review." ></td>
	<td class="line x" title="58:189	 Sentential (SEN): Observations of the sentences, including the number of sentences, the average sentence length, the percentage of question sentences, and the number of exclamation marks." ></td>
	<td class="line x" title="59:189	 HTML (HTM): Two features for the number of bold tags <b> and line breaks <br>." ></td>
	<td class="line x" title="60:189	Lexical Features Lexical features capture the words observed in the reviews." ></td>
	<td class="line x" title="61:189	We experimented with two sets of features:  Unigram (UGR): The tf-idf statistic of each word occurring in a review." ></td>
	<td class="line x" title="62:189	 Bigram (BGR): The tf-idf statistic of each bigram occurring in a review." ></td>
	<td class="line x" title="63:189	For both unigrams and bigrams, we used lemmatized words from a syntactic analysis of the reviews and computed the tf-idf statistic (Salton and McGill 1983) using the following formula: () N idftf idftf log = where N is the number of tokens in the review." ></td>
	<td class="line x" title="64:189	Syntactic Features Syntactic features aim to capture the linguistic properties of the review." ></td>
	<td class="line x" title="65:189	We grouped them into the following feature set:  Syntax (SYN): Includes the percentage of parsed tokens that are open-class (i.e. , nouns, verbs, adjectives and adverbs), the percentage of tokens that are nouns, the percentage of tokens that are verbs, the percentage of tokens that are verbs conjugated in the first person, and the percentage of tokens that are adjectives or adverbs." ></td>
	<td class="line x" title="66:189	Semantic Features Most online reviews are fairly short; their sparsity suggests that bigram features will not perform well (which is supported by our experiments described in Section 5.3)." ></td>
	<td class="line x" title="67:189	Although semantic features have rarely been effective in many text classification problems (Moschitti and Basili 2004), there is reason here to hypothesize that a specialized vocabulary of important words might help with the sparsity." ></td>
	<td class="line x" title="68:189	We hypothesized 2 Reviews are analyzed using the Minipar dependency parser (Lin 1994)." ></td>
	<td class="line x" title="69:189	that good reviews will often contain: i) references to the features of a product (e.g. , the LCD and resolution of a digital camera), and ii) mentions of sentiment words (i.e. , words that express an opinion such as great screen)." ></td>
	<td class="line x" title="70:189	Below we describe two families of features that capture these semantic observations within the reviews:  Product-Feature (PRF): The features of products that occur in the review, e.g., capacity of MP3 players and zoom of a digital camera." ></td>
	<td class="line x" title="71:189	This feature counts the number of lexical matches that occur in the review for each product feature." ></td>
	<td class="line x" title="72:189	There is no trivial way of obtaining a list of all the features of a product." ></td>
	<td class="line x" title="73:189	In Section 5.1 we describe a method for automatically extracting product features from Pro/Con listings from Epinions.com." ></td>
	<td class="line x" title="74:189	Our assumption is that pro/cons are the features that are important for customers (and hence should be part of a helpful review)." ></td>
	<td class="line x" title="75:189	 General-Inquirer (GIW): Positive and negative sentiment words describing products or product features (e.g. , amazing sound quality and weak zoom)." ></td>
	<td class="line x" title="76:189	The intuition is that reviews that analyze product features are more helpful than those that do not." ></td>
	<td class="line x" title="77:189	We try to capture this analysis by extracting sentiment words using the publicly available list of positive and negative sentiment words from the General Inquirer Dictionaries 3." ></td>
	<td class="line x" title="78:189	Meta-Data Features Unlike the previous four feature classes, metadata features capture observations which are independent of the text (i.e. , unrelated with linguistic features)." ></td>
	<td class="line x" title="79:189	We consider the following feature:  Stars (STR): Most websites require reviewers to include an overall rating for the products that they review (e.g. , star ratings in Amazon.com)." ></td>
	<td class="line x" title="80:189	This feature set includes the rating score (STR1) as well as the absolute value of the difference between the rating score and the average rating score given by all reviewers (STR2)." ></td>
	<td class="line x" title="81:189	We differentiate meta-data features from semantic features since they require external knowledge that may not be available from certain review sites." ></td>
	<td class="line x" title="82:189	Nowadays, however, most sites that collect user reviews also collect some form of product rating (e.g. , Amazon.com, Overstock.com, and Apple.com)." ></td>
	<td class="line x" title="83:189	3 http://www.wjh.harvard.edu/~inquirer/homecat.htm 425 4 Ranking System In this paper, we estimate the helpfulness function in Equation 1 using user ratings extracted from Amazon.com, where rating + (r) is the number of unique users that rated the review r as helpful and rating (r) is the number of unique users that rated r as unhelpful." ></td>
	<td class="line x" title="84:189	Reviews from Amazon.com form a gold standard labeled dataset of {review, h(review)} pairs that can be used to train a supervised machine learning algorithm." ></td>
	<td class="line x" title="85:189	In this paper, we applied an SVM (Vapnik 1995) package on the features extracted from reviews to learn the function h. Two natural options for learning helpfulness according to Equation 1 are SVM Regression and SVM Ranking (Joachims 2002)." ></td>
	<td class="line x" title="86:189	Though learning to rank according to helpfulness requires only SVM Ranking, the helpfulness function provides non-uniform differences between ranks in the training set." ></td>
	<td class="line x" title="87:189	Also, in practice, many products have only one review, which can serve as training data for SVM Regression but not SVM Ranking." ></td>
	<td class="line x" title="88:189	Furthermore, in large sites such as Amazon.com, when new reviews are written it is inefficient to re-rank all previously ranked reviews." ></td>
	<td class="line x" title="89:189	We therefore choose SVM Regression in this paper." ></td>
	<td class="line x" title="90:189	We describe the exact implementation in Section 5.1." ></td>
	<td class="line x" title="91:189	After the SVM is trained, for a given product and its set of reviews R, we rank the reviews of R in decreasing order of h(r), r  R. Table 1 shows four sample reviews for the iPod Photo 20GB product from Amazon.com, their total number of helpful and unhelpful votes, as well as their rank according to the helpfulness score h from both the gold standard from Amazon.com and using the SVM prediction of our best performing system described in Section 5.2." ></td>
	<td class="line x" title="92:189	5 Experimental Results We empirically evaluate our review model and ranking system, described in Section 3 and Section 4, by comparing the performance of various feature combinations on products mined from Amazon.com." ></td>
	<td class="line x" title="93:189	Below, we describe our experimental setup, present our results, and analyze system performance." ></td>
	<td class="line x" title="94:189	5.1 Experimental Setup We describe below the datasets that we extracted from Amazon.com, the implementation of our SVM system, and the method we used for extracting features of reviews." ></td>
	<td class="line x" title="95:189	Extraction and Preprocessing of Datasets We focused our experiments on two products from Amazon.com: MP3 Players and Digital Cameras." ></td>
	<td class="line x" title="96:189	Using Amazon Web Services API, we collected reviews associated with all products in the MP3 Players and Digital Cameras categories." ></td>
	<td class="line x" title="97:189	For MP3 Players, we collected 821 products and 33,016 reviews; for Digital Cameras, we collected 1,104 products and 26,189 reviews." ></td>
	<td class="line x" title="98:189	In most retailer websites like Amazon.com, duplicate reviews, which are quite frequent, skew statistics and can greatly affect a learning algorithm." ></td>
	<td class="line x" title="99:189	Looking for exact string matches between reviews is not a sufficient filter since authors of duplicated reviews often make small changes to the reviews to avoid detection." ></td>
	<td class="line x" title="100:189	We built a simple filter that compares the distribution of word bigrams across each pair of reviews." ></td>
	<td class="line x" title="101:189	A pair is deemed a duplicate if more than 80% of their bigrams match." ></td>
	<td class="line x" title="102:189	Also, whole products can be duplicated." ></td>
	<td class="line x" title="103:189	For different product versions, such as iPods that can come in black or white models, reviews on Amazon.com are duplicated between them." ></td>
	<td class="line x" title="104:189	We filter Table 1." ></td>
	<td class="line x" title="105:189	Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Amazon.com along with their ratings as well as their helpfulness ranks (from both the gold standard from Amazon.com and the SVM prediction of our best performing system described in Section 5.2)." ></td>
	<td class="line x" title="106:189	RANK(h) REVIEW TITLE HELPFUL VOTES UNHELPFUL VOTES GOLD STANDARD SVM PREDICTION iPod Moves to All-color Line-up 215 11 7 1 iPod: It's NOT Music to My Ears 11 13 25 30 The best thing I ever bought 22 32 26 27 VERY disappointing 1 18 40 40 426 out complete products where each of its reviews is detected as a duplicate of another product (i.e. , only one iPod version is retained)." ></td>
	<td class="line x" title="107:189	The filtering of duplicate products and duplicate reviews discarded 85 products and 12,097 reviews for MP3 Players and 38 products and 3,692 reviews for Digital Cameras." ></td>
	<td class="line x" title="108:189	In order to have accurate estimates for the helpfulness function in Equation 1, we filtered out any review that did not receive at least five user ratings (i.e. , reviews where less than five users voted it as helpful or unhelpful are filtered out)." ></td>
	<td class="line x" title="109:189	This filtering was performed before duplicate detection and discarded 45.7% of the MP3 Players reviews and 32.7% of the Digital Cameras reviews." ></td>
	<td class="line x" title="110:189	Table 2 describes statistics for the final datasets after the filtering steps." ></td>
	<td class="line x" title="111:189	10% of products for both datasets were withheld as development corpora and the remaining 90% were randomly sorted into 10 sets for 10-fold cross validation." ></td>
	<td class="line x" title="112:189	SVM Regression For our regression model, we deployed the state of the art SVM regression tool SVM light (Joachims 1999)." ></td>
	<td class="line x" title="113:189	We tested on the development sets various kernels including linear, polynomial (degrees 2, 3, and 4), and radial basis function (RBF)." ></td>
	<td class="line x" title="114:189	The best performing kernel was RBF and we report only these results in this paper (performance was measured using Spearmans correlation coefficient, described in Section 5.2)." ></td>
	<td class="line x" title="115:189	We tuned the RBF kernel parameters C (the penalty parameter) and  (the kernel width hyperparameter) performing full grid search over the 110 combinations of exponentially spaced parameter pairs (C,) following (Hsu et al. 2003)." ></td>
	<td class="line x" title="116:189	Feature Extraction To extract the features described in Section 3.2, we preprocessed each review using the Minipar dependency parser (Lin 1994)." ></td>
	<td class="line x" title="117:189	We used the parser tokenization, sentence breaker, and syntactic categorizations to generate the Length, Sentential, Unigram, Bigram, and Syntax feature sets." ></td>
	<td class="line x" title="118:189	In order to count the occurrences of product features for the Product-Feature set, we developed an automatic way of mining references to product features from Epinions.com." ></td>
	<td class="line x" title="119:189	On this website, user-generated product reviews include explicit lists of pros and cons, describing the best and worst aspects of a product." ></td>
	<td class="line x" title="120:189	For example, for MP3 players, we found the pro belt clip and the con Useless FM tuner." ></td>
	<td class="line x" title="121:189	Our assumption is that the pro/con lists tend to contain references to the product features that are important to customers, and hence their occurrence in a review may correlate with review helpfulness." ></td>
	<td class="line x" title="122:189	We filtered out all single-word entries which were infrequently seen (e.g. , hold, ever)." ></td>
	<td class="line x" title="123:189	After splitting and filtering the pro/con lists, we were left with a total of 9,110 unique features for MP3 Players and 13,991 unique features for Digital Cameras." ></td>
	<td class="line x" title="124:189	The Stars feature set was created directly from the star ratings given by each author of an Amazon.com review." ></td>
	<td class="line x" title="125:189	For each feature measurement f, we applied the following standard transformation: ( )1ln +f and then scaled each feature between [0, 1] as suggested in (Hsu et al. 2003)." ></td>
	<td class="line x" title="126:189	We experimented with various combinations of feature sets." ></td>
	<td class="line x" title="127:189	Our results tables use the abbreviations presented in Section 3.2." ></td>
	<td class="line x" title="128:189	For brevity, we report the combinations which contributed to our best performing system and those that help assess the power of the different feature classes in capturing helpfulness." ></td>
	<td class="line x" title="129:189	5.2 Ranking Performance Evaluating the quality of a particular ranking is difficult since certain ranking intervals can be more important than others (e.g. , top-10 versus bottom-10) We adopt the Spearman correlation coefficient  (Spearman 1904) since it is the most commonly used measure of correlation between two sets of ranked data points 4 . For each fold in our 10-fold cross-validation experiments, we trained our SVM system using 9 folds." ></td>
	<td class="line x" title="130:189	For the remaining test fold, we ranked each products reviews according to the SVM prediction (described in Section 4) and computed the  4 We used the version of Spearmans correlation coefficient that allows for ties in rankings." ></td>
	<td class="line x" title="131:189	See Siegel and Castellan (1988) for more on alternate rank statistics such as Kendalls tau." ></td>
	<td class="line x" title="132:189	Table 2." ></td>
	<td class="line x" title="133:189	Overview of filtered datasets extracted from Amazon.com." ></td>
	<td class="line x" title="134:189	MP3 PLAYERS DIGITAL CAMERAS Total Products 736 1066 Total Reviews 11,374 14,467 Average Reviews/Product 15.4 13.6 Min/MaxReviews/Product 1 / 375 1 / 168 427 correlation between the ranking and the gold standard ranking from the test fold 5 . Although our task definition is to learn review rankings according to helpfulness, as an intermediate step the SVM system learns to predict the absolute helpfulness score for each review." ></td>
	<td class="line x" title="135:189	To test the correlation of this score against the gold standard, we computed the standard Pearson correlation coefficient." ></td>
	<td class="line x" title="136:189	Results show that the highest performing feature combination consisted of the Length, the Unigram, and the Stars feature sets." ></td>
	<td class="line x" title="137:189	Table 3 reports the evaluation results for every combination of these features with 95% confidence bounds." ></td>
	<td class="line x" title="138:189	Of the three features alone, neither was statistically more significant than the others." ></td>
	<td class="line x" title="139:189	Examining each pair combination, only the combination of length with stars outperformed the others." ></td>
	<td class="line x" title="140:189	Surprisingly, adding unigram features to this combination had little effect for the MP3 Players." ></td>
	<td class="line x" title="141:189	Given our list of features defined in Section 3.2, helpfulness of reviews is best captured with a combination of the Length and Stars features." ></td>
	<td class="line x" title="142:189	Training an RBF-kernel SVM regression model does not necessarily make clear the exact relationship between input and output variables." ></td>
	<td class="line x" title="143:189	To investigate this relationship between length and helpfulness, we inspected their Pearson correlation coefficient, which was 0.45." ></td>
	<td class="line x" title="144:189	Users indeed tend to find short reviews less helpful than longer ones: out of the 5,247 reviews for MP3 Players that contained more than 1000 characters, the average gold standard helpfulness score was 82%; the 204 reviews with fewer than 100 characters had on average a score of 23%." ></td>
	<td class="line x" title="145:189	The explicit product rating, such as Stars is also an 5 Recall that the gold standard is extracted directly from user helpfulness votes on Amazon.com (see Section 4)." ></td>
	<td class="line x" title="146:189	indicator of review helpfulness, with a Pearson correlation coefficient of 0.48." ></td>
	<td class="line x" title="147:189	The low Pearson correlations of Table 3 compared to the Spearman correlations suggest that we can learn the ranking without perfectly learning the function itself." ></td>
	<td class="line x" title="148:189	To investigate this, we tested the ability of SVM regression to recover the target helpfulness score, given the score itself as the only feature." ></td>
	<td class="line x" title="149:189	The Spearman correlation for this test was a perfect 1.0." ></td>
	<td class="line x" title="150:189	Interestingly, the Pearson correlation was only 0.798, suggesting that the RBF kernel does learn the helpfulness ranking without learning the function exactly." ></td>
	<td class="line x" title="151:189	5.3 Results Analysis Table 3 shows only the feature combinations of our highest performing system." ></td>
	<td class="line x" title="152:189	In Table 4, we report several other feature combinations to show why we selected certain features and what was the effect of our five feature classes presented in Section 3.2." ></td>
	<td class="line x" title="153:189	In the first block of six feature combinations in Table 4, we show that the unigram features outperform the bigram features, which seem to be suffering from the data sparsity of the short reviews." ></td>
	<td class="line x" title="154:189	Also, unigram features seem to subsume the information carried in our semantic features Product-Feature (PRF) and General-Inquirer (GIW)." ></td>
	<td class="line x" title="155:189	Although both PRF and GIW perform well as standalone features, when combined with unigrams there is little performance difference (for MP3 Players we see a small but insignificant decrease in performance whereas for Digital Cameras we see a small but insignificant improvement)." ></td>
	<td class="line x" title="156:189	Recall that PRF and GIW are simply subsets of review words that are found to be product features or sentiment words." ></td>
	<td class="line x" title="157:189	The learning algorithm seems to discover on its own which Table 3." ></td>
	<td class="line x" title="158:189	Evaluation of the feature combinations that make up our best performing system (in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras according to helpfulness." ></td>
	<td class="line x" title="159:189	MP3 PLAYERS DIGITAL CAMERAS FEATURE COMBINATIONS SPEARMAN  PEARSON  SPEARMAN  PEARSON  LEN 0.575  0.037 0.391  0.038 0.521  0.029 0.357  0.029 UGR 0.593  0.036 0.398  0.038 0.499  0.025 0.328  0.029 STR1 0.589  0.034 0.326  0.038 0.507  0.029 0.266  0.030 UGR+STR1 0.644  0.033 0.436  0.038 0.490  0.032 0.324  0.032 LEN+UGR 0.582  0.036 0.401  0.038 0.553  0.028 0.394  0.029 LEN+STR1 0.652  0.033 0.470  0.038 0.577  0.029 0.423  0.031 LEN+UGR+STR1 0.656  0.033 0.476  0.038 0.595  0.028 0.442  0.031 LEN=Length; UGR=Unigram; STR=Stars  95% confidence bounds are calculated using 10-fold cross-validation." ></td>
	<td class="line x" title="160:189	428 words are most important in a review and does not use additional knowledge about the meaning of the words (at least not the semantics contained in PRF and GIW)." ></td>
	<td class="line x" title="161:189	We tested two different versions of the Stars feature: i) the number of star ratings, STR1; and ii) the difference between the star rating and the average rating of the review, STR2." ></td>
	<td class="line x" title="162:189	The second block of feature combinations in Table 4 shows that neither is significantly better than the other so we chose STR1 for our best performing system." ></td>
	<td class="line x" title="163:189	Our experiments also revealed that our structural features Sentential and HTML, as well as our syntactic features, Syntax, did not show any significant improvement in system performance." ></td>
	<td class="line x" title="164:189	In the last block of feature combinations in Table 4, we report the performance of our best performing features (Length, Unigram, and Stars) along with these other features." ></td>
	<td class="line x" title="165:189	Though none of the features cause a performance deterioration, neither of them significantly improves performance." ></td>
	<td class="line x" title="166:189	5.4 Discussion In this section, we discuss the broader implications and potential impacts of our work, and possible connections with other research directions." ></td>
	<td class="line x" title="167:189	The usefulness of the Stars feature for determining review helpfulness suggests the need for developing automatic methods for assessing product ratings, e.g., (Pang and Lee 2005)." ></td>
	<td class="line x" title="168:189	Our findings focus on predictors of helpfulness of reviews of tangible consumer products (consumer electronics)." ></td>
	<td class="line x" title="169:189	Helpfulness is also solicited and tracked for reviews of many other types of entities: restaurants (citysearch.com), films (imdb.com), reviews of open-source software modules (cpanratings.perl.org), and countless others." ></td>
	<td class="line x" title="170:189	Our findings of the importance of Length, Unigrams, and Stars may provide the basis of comparison for assessing helpfulness of reviews of other entity types." ></td>
	<td class="line x" title="171:189	Our work represents an initial step in assessing helpfulness." ></td>
	<td class="line x" title="172:189	In the future, we plan to investigate other possible indicators of helpfulness such as a reviewers reputation, the use of comparatives (e.g. , more and better than), and references to other products." ></td>
	<td class="line x" title="173:189	Taken further, this work may have interesting connections to work on personalization, social networks, and recommender systems, for instance by identifying the reviews that a particular user would find helpful." ></td>
	<td class="line x" title="174:189	Our work on helpfulness of reviews also has potential applications to work on automatic genTable 4." ></td>
	<td class="line x" title="175:189	Performance evaluation of various feature combinations for ranking reviews of MP3 Players and Digital Cameras on Amazon.com according to helpfulness." ></td>
	<td class="line x" title="176:189	The first six lines suggest that unigrams subsume the semantic features; the next two support the use of the raw counts of product ratings (stars) rather than the distance of this count from the average rating; the final six investigate the importance of auxiliary feature sets." ></td>
	<td class="line x" title="177:189	MP3 PLAYERS DIGITAL CAMERAS FEATURE COMBINATIONS SPEARMAN  PEARSON  SPEARMAN  PEARSON  UGR 0.593  0.036 0.398  0.038 0.499  0.025 0.328  0.029 BGR 0.499  0.040 0.293  0.038 0.434  0.032 0.242  0.029 PRF 0.591 0.037 0.400  0.039 0.527  0.030 0.316  0.028 GIW 0.571  0.036 0.381  0.038 0.524  0.030 0.333  0.028 UGR+PRF 0.570  0.037 0.375  0.038 0.546  0.029 0.348  0.028 UGR+GIW 0.554  0.037 0.358  0.038 0.568  0.031 0.324  0.029 STR1 0.589  0.034 0.326  0.038 0.507  0.029 0.266  0.030 STR2 0.556  0.032 0.303  0.038 0.504  0.027 0.229  0.027 LEN+UGR+STR1 0.656  0.033 0.476  0.038 0.595  0.028 0.442  0.031 LEN+UGR+STR1+SEN 0.653  0.033 0.470  0.038 0.599  0.028 0.448  0.030 LEN+UGR+STR1+HTM 0.640  0.035 0.459  0.039 0.594  0.028 0.442  0.031 LEN+UGR+STR1+SYN 0.645  0.034 0.469  0.039 0.595  0.028 0.447  0.030 LEN+UGR+STR1+SEN+HTM+SYN 0.631  0.035 0.453  0.039 0.600  0.028 0.452  0.030 LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW 0.601  0.035 0.396  0.038 0.604  0.027 0.460  0.030 LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram; SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars  95% confidence bounds are calculated using 10-fold cross-validation." ></td>
	<td class="line x" title="178:189	429 eration of review information, by providing a way to assess helpfulness of automatically generated reviews." ></td>
	<td class="line x" title="179:189	Work on generation of reviews includes review summarization and extraction of useful reviews from blogs and other mixed texts." ></td>
	<td class="line x" title="180:189	6 Conclusions Ranking reviews according to user helpfulness is an important problem for many online sites such as Amazon.com and Ebay.com." ></td>
	<td class="line x" title="181:189	To date, most websites measure helpfulness by having users manually assess how helpful each review is to them." ></td>
	<td class="line x" title="182:189	In this paper, we proposed an algorithm for automatically assessing helpfulness and ranking reviews according to it." ></td>
	<td class="line x" title="183:189	Exploiting the multitude of user-rated reviews on Amazon.com, we trained an SVM regression system to learn a helpfulness function and then applied it to rank unlabeled reviews." ></td>
	<td class="line x" title="184:189	Our best system achieved Spearman correlation coefficient scores of 0.656 and 0.604 against a gold standard for MP3 players and digital cameras." ></td>
	<td class="line x" title="185:189	We also performed a detailed analysis of different features to study the importance of several feature classes in capturing helpfulness." ></td>
	<td class="line x" title="186:189	We found that the most useful features were the length of the review, its unigrams, and its product rating." ></td>
	<td class="line x" title="187:189	Semantic features like mentions of product features and sentiment words seemed to be subsumed by the simple unigram features." ></td>
	<td class="line x" title="188:189	Structural features (other than length) and syntactic features had no significant impact." ></td>
	<td class="line x" title="189:189	It is our hope through this work to shed some light onto what people find helpful in usersupplied reviews and, by automatically ranking them, to ultimately enhance user experience." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1652
Feature Subsumption For Opinion Analysis
Riloff, Ellen;Patwardhan, Siddharth;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:189	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440448, Sydney, July 2006." ></td>
	<td class="line x" title="2:189	c2006 Association for Computational Linguistics Feature Subsumption for Opinion Analysis Ellen Riloff and Siddharth Patwardhan School of Computing University of Utah Salt Lake City, UT 84112 {riloff,sidd}@cs.utah.edu Janyce Wiebe Department of Computer Science University of Pittsburgh Pittsburgh, PA 15260 wiebe@cs.pitt.edu Abstract Lexical features are key to many approaches to sentiment analysis and opinion detection." ></td>
	<td class="line x" title="3:189	A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns." ></td>
	<td class="line x" title="4:189	In this paper, we use a subsumption hierarchy to formally de ne different types of lexical features and their relationship to one another, both in terms of representational coverage and performance." ></td>
	<td class="line x" title="5:189	We use the subsumption hierarchy in two ways: (1) as an analytic tool to automatically identify complex features that outperform simpler features, and (2) to reduce a feature set by removing unnecessary features." ></td>
	<td class="line x" title="6:189	We show that reducing the feature set improves performance on three opinion classi cation tasks, especially when combined with traditional feature selection." ></td>
	<td class="line x" title="7:189	1 Introduction Sentiment analysis and opinion recognition are active research areas that have many potential applications, including review mining, product reputation analysis, multi-document summarization, and multi-perspective question answering." ></td>
	<td class="line x" title="8:189	Lexical features are key to many approaches, and a variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexico-syntactic patterns." ></td>
	<td class="line x" title="9:189	It is common for different features to overlap representationally." ></td>
	<td class="line x" title="10:189	For example, the unigram happy will match all of the texts that the bigram very happy matches." ></td>
	<td class="line x" title="11:189	Since both features represent a positive sentiment and the bigram matches fewer contexts than the unigram, it is probably suf cient just to have the unigram." ></td>
	<td class="line x" title="12:189	However, there are many cases where a feature captures a subtlety or non-compositional meaning that a simpler feature does not." ></td>
	<td class="line x" title="13:189	For example, basket case is a highly opinionated phrase, but the words basket and case individually are not." ></td>
	<td class="line x" title="14:189	An open question in opinion analysis is how often more complex feature representations are needed, and which types of features are most valuable." ></td>
	<td class="line x" title="15:189	Our rst goal is to devise a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators." ></td>
	<td class="line x" title="16:189	These subjective expressions could then be added to a subjectivity lexicon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition." ></td>
	<td class="line x" title="17:189	Many opinion classi ers are created by adopting a kitchen sink approach that throws together a variety of features." ></td>
	<td class="line x" title="18:189	But in many cases adding new types of features does not improve performance." ></td>
	<td class="line x" title="19:189	For example, Pang et al.(2002) found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams." ></td>
	<td class="line x" title="21:189	Our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value." ></td>
	<td class="line x" title="22:189	Our hypothesis is that a reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than a larger feature set that includes the entire kitchen sink of features." ></td>
	<td class="line x" title="23:189	In this paper, we explore the use of a subsumption hierarchy to formally de ne the subsumption relationships between different types of textual features." ></td>
	<td class="line x" title="24:189	We use the subsumption hierarchy in two ways." ></td>
	<td class="line x" title="25:189	First, we use subsumption as an an440 alytic tool to compare features of different complexities and automatically identify complex features that substantially outperform their simpler counterparts." ></td>
	<td class="line x" title="26:189	Second, we use the subsumption hierarchy to reduce a feature set based on representational overlap and on performance." ></td>
	<td class="line x" title="27:189	We conduct experiments with three opinion data sets and show that the reduced feature sets can improve classi cation performance." ></td>
	<td class="line x" title="28:189	2 The Subsumption Hierarchy 2.1 Text Representations We analyze two feature representations that have been used for opinion analysis: Ngrams and Extraction Patterns." ></td>
	<td class="line x" title="29:189	Information extraction (IE) patterns are lexico-syntactic patterns that represent expressions which identify role relationships." ></td>
	<td class="line x" title="30:189	For example, the pattern <subj> ActVP(recommended) extracts the subject of active-voice instances of the verb recommended as the recommender." ></td>
	<td class="line x" title="31:189	The pattern <subj> PassVP(recommended) extracts the subject of passive-voice instances of recommended as the object being recommended." ></td>
	<td class="line x" title="32:189	(Riloff and Wiebe, 2003) explored the idea of using extraction patterns to represent more complex subjective expressions that have noncompositional meanings." ></td>
	<td class="line x" title="33:189	For example, the expression drive (someone) up the wall expresses the feeling of being annoyed, but the meanings of the words drive, up, and wall have no emotional connotations individually." ></td>
	<td class="line x" title="34:189	Furthermore, this expression is not a xed word sequence that can be adequately modeled by Ngrams." ></td>
	<td class="line x" title="35:189	Any noun phrase can appear between the words drive and up, so a exible representation is needed to capture the general pattern drives <NP> up the wall." ></td>
	<td class="line x" title="36:189	This example represents a general phenomenon: many expressions allow intervening noun phrases and/or modifying terms." ></td>
	<td class="line x" title="37:189	For example: stepped on <mods> toes Ex: stepped on the boss toes dealt <np> <mods> blow Ex: dealt the company a decisive blow brought <np> to <mods> knees Ex: brought the man to his knees (Riloff and Wiebe, 2003) also showed that syntactic variations of the same verb phrase can behave very differently." ></td>
	<td class="line x" title="38:189	For example, they found that passive-voice constructions of the verb ask had a 100% correlation with opinion sentences, but active-voice constructions had only a 63% correlation with opinions." ></td>
	<td class="line x" title="39:189	Pattern Type Example Pattern <subj> PassVP <subj> is satis ed <subj> ActVP <subj> complained <subj> ActVP Dobj <subj> dealt blow <subj> ActInfVP <subj> appear to be <subj> PassInfVP <subj> is meant to be <subj> AuxVP Dobj <subj> has position <subj> AuxVP Adj <subj> is happy ActVP <dobj> endorsed <dobj> InfVP <dobj> to condemn <dobj> ActInfVP <dobj> get to know <dobj> PassInfVP <dobj> is meant to be <dobj> Subj AuxVP <dobj> fact is <dobj> NP Prep <np> opinion on <np> ActVP Prep <np> agrees with <np> PassVP Prep <np> is worried about <np> InfVP Prep <np> to resort to <np> <possessive> NP <noun>s speech Figure 1: Extraction Pattern Types Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features." ></td>
	<td class="line x" title="40:189	We used three types of features in our research: unigrams, bigrams, and IE patterns." ></td>
	<td class="line x" title="41:189	The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004)." ></td>
	<td class="line x" title="42:189	AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus." ></td>
	<td class="line x" title="43:189	AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="44:189	Figure 1 shows the 17 types of extraction patterns that AutoSlog generates." ></td>
	<td class="line x" title="45:189	PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to in nitive VPs, and AuxVP refers 1NSP is freely available for use under the GPL from http://search.cpan.org/dist/Text-NSP." ></td>
	<td class="line x" title="46:189	We discarded Ngrams that consisted entirely of stopwords." ></td>
	<td class="line x" title="47:189	We used a list of 281 stopwords." ></td>
	<td class="line x" title="48:189	441 to VPs where the main verb is a form of to be or to have . Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be extracted by the patterns.2 2.2 The Subsumption Hierarchy We created a subsumption hierarchy that de nes the representational scope of different types of features." ></td>
	<td class="line x" title="49:189	We will say that feature A representationally subsumes feature B if the set of text spans that match feature A is a superset of the set of text spans that match feature B. For example, the unigram happy subsumes the bigram very happy because the set of text spans that match happy includes the text spans that match very happy . First, we de ne a hierarchy of valid subsumption relationships, shown in Figure 2." ></td>
	<td class="line x" title="50:189	The 2Gram node, for example, is a child of the 1Gram node because a 1Gram can subsume a 2Gram." ></td>
	<td class="line x" title="51:189	Ngrams may subsume extraction patterns as well." ></td>
	<td class="line x" title="52:189	Every extraction pattern has at least one corresponding 1Gram that will subsume it.3." ></td>
	<td class="line x" title="53:189	For example, the 1Gram recommended subsumes the pattern <subj> ActVP(recommended) because the pattern only matches active-voice instances of recommended . An extraction pattern may also subsume another extraction pattern." ></td>
	<td class="line x" title="54:189	For example, <subj> ActVP(recommended) subsumes <subj> ActVP(recommended) Dobj(movie) . To compare speci c features we need to formally de ne the representation of each type of feature in the hierarchy." ></td>
	<td class="line x" title="55:189	For example, the hierarchy dictates that a 2Gram can subsume the pattern ActInfVP <dobj>, but this should hold only if the words in the bigram correspond to adjacent words in the pattern." ></td>
	<td class="line x" title="56:189	For example, the 2Gram to sh subsumes the pattern ActInfVP(like to sh) <dobj> . But the 2Gram like sh should not subsume it." ></td>
	<td class="line x" title="57:189	Similarly, consider the pattern InfVP(plan) <dobj>, which represents the in nitive to plan . This pattern subsumes the pattern ActInfVP(want to plan) <dobj>, but it should not subsume the pattern ActInfVP(plan to start) . To ensure that different features truly subsume each other representationally, we formally de ne each type of feature based on words, sequential 2However, the items extracted by the patterns are not actually used by our opinion classi ers; only the patterns themselves are matched against the text." ></td>
	<td class="line x" title="58:189	3Because every type of extraction pattern shown in Figure 1 contains at least one word (not including the extracted phrases, which are not used as part of our feature representation)." ></td>
	<td class="line x" title="59:189	dependencies, and syntactic dependencies." ></td>
	<td class="line x" title="60:189	A sequential dependency between words wi and wi+1 means that wi and wi+1 must be adjacent, and that wi must precede wi+1." ></td>
	<td class="line x" title="61:189	Figure 3 shows the formal de nition of a bigram (2Gram) node." ></td>
	<td class="line x" title="62:189	The bigram is de ned as two words with a sequential dependency indicating that they must be adjacent." ></td>
	<td class="line x" title="63:189	Name = 2Gram Constituent[0] = WORD1 Constituent[1] = WORD2 Dependency = Sequential(0, 1) Figure 3: 2Gram De nition A syntactic dependency between words wi and wi+1 means that wi has a speci c syntactic relationship to wi+1, and wi must precede wi+1." ></td>
	<td class="line x" title="64:189	For example, consider the extraction pattern NP Prep <np>, in which the object of the preposition attaches to the NP." ></td>
	<td class="line x" title="65:189	Figure 4 shows the de nition of this extraction pattern in the hierarchy." ></td>
	<td class="line x" title="66:189	The pattern itself contains three components: the NP, the attaching preposition, and the object of the preposition (which is the NP that the pattern extracts)." ></td>
	<td class="line x" title="67:189	The de nition also includes two syntactic dependencies: the rst dependency is between the NP and the preposition (meaning that the preposition syntactically attaches to the NP), while the second dependency is between the preposition and the extraction (meaning that the extracted NP is the syntactic object of the preposition)." ></td>
	<td class="line x" title="68:189	Name = NP Prep <np> Constituent[0] = NP Constituent[1] = PREP Constituent[2] = NP EXTRACTION Dependency = Syntactic(0, 1) Dependency = Syntactic(1, 2) Figure 4: NP Prep <np> Pattern De nition Consequently, the bigram affair with will not subsume the extraction pattern affair with <np> because the bigram requires the noun and preposition to be adjacent but the pattern does not." ></td>
	<td class="line x" title="69:189	For example, the extraction pattern matches the text an affair in his mind with Countess Olenska but the bigram does not." ></td>
	<td class="line x" title="70:189	Conversely, the extraction pattern does not subsume the bigram either because the pattern requires syntactic attachment but the bigram does not." ></td>
	<td class="line x" title="71:189	For example, the bigram matches 442 <subj> ActVP <subj> ActInfVP <subj> ActVP Dobj <subj> PassVP <subj> PassInfVP InfVP <dobj> ActInfVP <dobj> PassInfVP <dobj> 1Gram 2Gram <possessive> NP <subj> AuxVP AdjP <subj> AuxVP Dobj ActVP <dobj> ActVP Prep <np> NP Prep <np> PassVP Prep <np> Subj AuxVP <dobj> 3Gram ActVP Prep:OF <np> InfVP Prep <np> NP Prep:OF <np> PassVP Prep:OF <np> 4Gram InfVP Prep:OF <np> Figure 2: The Subsumption Hierarchy the sentence He ended the affair with a sense of relief, but the extraction pattern does not." ></td>
	<td class="line x" title="72:189	Figure 5 shows the de nition of another extraction pattern, InfVP <dobj>, which includes both syntactic and sequential dependencies." ></td>
	<td class="line x" title="73:189	This pattern would match the text to protest high taxes . The pattern de nition has three components: the in nitive to, a verb, and the direct object of the verb (which is the NP that the pattern extracts)." ></td>
	<td class="line x" title="74:189	The de nition also shows two syntactic dependencies." ></td>
	<td class="line x" title="75:189	The rst dependency indicates that the verb syntactically attaches to the in nitive to . The second dependency indicates that the extracted NP syntactically attaches to the verb (i.e. , it is the direct object of that particular verb)." ></td>
	<td class="line x" title="76:189	The pattern de nition also includes a sequential dependency, which speci es that to must be adjacent to the verb." ></td>
	<td class="line x" title="77:189	Strictly speaking, our parser does not require them to be adjacent." ></td>
	<td class="line x" title="78:189	For example, the parser allows intervening adverbs to split in nitives (e.g. , to strongly protest high taxes ), and this does happen occasionally." ></td>
	<td class="line x" title="79:189	But split innitives are relatively rare, so in the vast majority of cases the in nitive to will be adjacent to the verb." ></td>
	<td class="line x" title="80:189	Consequently, we decided that a bigram (e.g. , to protest ) should representationally subsume this extraction pattern because the syntactic exibility afforded by the pattern is negligible." ></td>
	<td class="line x" title="81:189	The sequential dependency link represents this judgment call that the in nitive to and the verb are adjacent in most cases." ></td>
	<td class="line x" title="82:189	For all of the node de nitions, we used our best judgment to make decisions of this kind." ></td>
	<td class="line x" title="83:189	We tried to represent major distinctions between features, without getting caught up in minor differences that were likely to be negligible in practice." ></td>
	<td class="line x" title="84:189	Name = InfVP <dobj> Constituent[0] = INFINITIVE TO Constituent[1] = VERB Constituent[2] = DOBJ EXTRACTION Dependency = Syntactic(0, 1) Dependency = Syntactic(1, 2) Dependency = Sequential(0, 1) Figure 5: InfVP <dobj> Pattern De nition To use the subsumption hierarchy, we assign each feature to its appropriate node in the hierarchy based on its type." ></td>
	<td class="line x" title="85:189	Then we perform a topdown breadthrst traversal." ></td>
	<td class="line x" title="86:189	Each feature is compared with the features at its ancestor nodes." ></td>
	<td class="line x" title="87:189	If a features words and dependencies are a superset of an ancestors words and dependencies, then it is subsumed by the (more general) ancestor and discarded.4 When the subsumption process is nished, a feature remains in the hierarchy only if 4The words that they have in common must also be in the same relative order." ></td>
	<td class="line x" title="88:189	443 there are no features above it that subsume it." ></td>
	<td class="line x" title="89:189	2.3 Performance-based Subsumption Representational subsumption is concerned with whether one feature is more general than another." ></td>
	<td class="line x" title="90:189	But the purpose of using the subsumption hierarchy is to identify more complex features that outperform simpler ones." ></td>
	<td class="line x" title="91:189	Applying the subsumption hierarchy to features without regard to performance would simply eliminate all features that have a more general counterpart in the feature set." ></td>
	<td class="line x" title="92:189	For example, all bigrams would be discarded if their component unigrams were also present in the hierarchy." ></td>
	<td class="line x" title="93:189	To estimate the quality of a feature, we use Information Gain (IG) because that has been shown to work well as a metric for feature selection (Forman, 2003)." ></td>
	<td class="line x" title="94:189	We will say that feature A behaviorally subsumes feature B if two criteria are met: (1) A representationally subsumes B, and (2) IG(A) IG(B) , where  is a parameter representing an acceptable margin of performance difference." ></td>
	<td class="line x" title="95:189	For example, if =0 then condition (2) means that feature A is just as valuable as feature B because its information gain is the same or higher." ></td>
	<td class="line x" title="96:189	If >0 then feature A is allowed to be a little worse than feature B, but within an acceptable margin." ></td>
	<td class="line x" title="97:189	For example, =.0001 means that As information gain may be up to .0001 lower than Bs information gain, and that is considered to be an acceptable performance difference (i.e. , A is good enough that we are comfortable discarding B in favor of the more general feature A)." ></td>
	<td class="line x" title="98:189	Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features." ></td>
	<td class="line x" title="99:189	Our goal is to identify complex features that are worth adding to a set of unigram features." ></td>
	<td class="line x" title="100:189	3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al. , 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al. , 2005).6 The OP and Polarity data sets involve document-level opinion classi cation, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classi cation." ></td>
	<td class="line x" title="101:189	The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="102:189	Metadata tags assigned by the Wall Street Journal de ne the opinion/non-opinion classes: the class of any document labeled Editorial, Letter to the Editor, Arts & Leisure Review, or Viewpoint by the Wall Street Journal is opinion, and the class of documents in all other categories (such as Business and News) is non-opinion." ></td>
	<td class="line x" title="103:189	This data set is highly skewed, with only 9% of the documents belonging to the opinion class." ></td>
	<td class="line x" title="104:189	Consequently, a trivial (but useless) opinion classi er that labels all documents as nonopinion articles would achieve 91% accuracy." ></td>
	<td class="line x" title="105:189	The Polarity data consists of 700 positive and 700 negative reviews from the Internet Movie Database (IMDb) archive." ></td>
	<td class="line x" title="106:189	The positive and negative classes were derived from author ratings expressed in stars or numerical values." ></td>
	<td class="line x" title="107:189	The MPQA data consists of English language versions of articles from the world press." ></td>
	<td class="line x" title="108:189	It contains 9,732 sentences that have been manually annotated for subjective expressions." ></td>
	<td class="line x" title="109:189	The opinion/non-opinion classes are derived from the lower-level annotations: a sentence is an opinion if it contains a subjective expression of medium or higher intensity; otherwise, it is a non-opinion sentence." ></td>
	<td class="line x" title="110:189	55% of the sentences belong to the opinion class." ></td>
	<td class="line x" title="111:189	4 Using the Subsumption Hierarchy for Analysis In this section, we illustrate how the subsumption hierarchy can be used as an analytic tool to automatically identify features that substantially outperform simpler counterparts." ></td>
	<td class="line x" title="112:189	These features represent specialized usages and expressions that would be good candidates for addition to a subjectivity lexicon." ></td>
	<td class="line x" title="113:189	Figure 6 shows pairs of features, where the rst is more general and the second is more speci c. These feature pairs were identi ed by the subsumption hierarchy as being representationally similar but behaviorally different (so the more speci c feature was retained)." ></td>
	<td class="line x" title="114:189	The IGain column shows the information gain values produced from the training set of one cross-validation fold." ></td>
	<td class="line x" title="115:189	The Class column shows the class that the more speci c feature is correlated with (the more general feature is usually not strongly correlated with either class)." ></td>
	<td class="line x" title="116:189	The top table in Figure 6 contains examples for the opinion/non-opinion classi cation task from 444 Opinion/Non-Opinion Classi cation ID Feature IGain Class Example A1 line .0016 . . ." ></td>
	<td class="line x" title="117:189	issue consists of notes backed by credit line receivables A2 the line .0075 opin lays it on the line; steps across the line B1 nation .0046 . . ." ></td>
	<td class="line x" title="118:189	has 750,000 cable-tv subscribers around the nation B2 a nation .0080 opin Its not that we are spawning a nation of ascetics . . ." ></td>
	<td class="line x" title="119:189	C1 begin .0006 Campeau buyers will begin writing orders C2 begin with .0036 opin To begin with, we should note that in contrast D1 bene ts .0040 . . ." ></td>
	<td class="line x" title="120:189	earlier period included $235,000 in tax bene ts." ></td>
	<td class="line x" title="121:189	DEP NP Prep(bene ts to) .0090 opin . . ." ></td>
	<td class="line x" title="122:189	boon to the rich with no proven bene ts to the economy E1 due .0001 . . ." ></td>
	<td class="line x" title="123:189	an estimated $ 1.23 billion in debt due next spring EEP ActVP Prep(due to) .0038 opin Its all due to the intense scrutiny Positive/Negative Sentiment Classi cation ID Feature IGain Class Example F1 short .0014 to make a long story short F2 nothing short .0039 pos nothing short of spectacular G1 ugly .0008 an ugly monster on a cruise liner G2 and ugly .0054 neg its a disappointment to see something this dumb and ugly H1 disaster .0010 rated pg-13 for disaster related elements HEP AuxVP Dobj(be disaster) .0048 neg . . ." ></td>
	<td class="line x" title="124:189	this is such a confused disaster of a lm I1 work .0002 the next day during the drive to work IEP ActVP(work) .0062 pos the lm will work just as well J1 manages .0003 he still manages to nd time for his wife JEP ActInfVP(manages to keep) .0054 pos this lm manages to keep up a rapid pace Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy." ></td>
	<td class="line x" title="125:189	(1 ) unigram; 2 ) bigram; EP ) extraction pattern) the OP data." ></td>
	<td class="line x" title="126:189	The more speci c features are more strongly correlated with opinion articles." ></td>
	<td class="line x" title="127:189	Surprisingly, simply adding a determiner can dramatically change behavior." ></td>
	<td class="line x" title="128:189	Consider A2." ></td>
	<td class="line x" title="129:189	There are many subjective idioms involving the line (two are shown in the table; others include toe the line and draw the line ), while objective language about credit lines, phone lines, etc. uses the determiner less often." ></td>
	<td class="line x" title="130:189	Similarly, consider B2." ></td>
	<td class="line x" title="131:189	Adding a to nation often corresponds to an abstract reference used when making an argument (e.g. , a nation of ascetics ), whereas other instances of nation are used more literally (e.g. , the 6th largest in the nation )." ></td>
	<td class="line x" title="132:189	21% of feature B1s instances appear in opinion articles, while 70% of feature B2s instances are in opinion articles." ></td>
	<td class="line x" title="133:189	Begin with (C2) captures an adverbial phrase used in argumentation ( To begin with ) but does not match objective usages such as will begin an action." ></td>
	<td class="line x" title="134:189	The word bene ts alone (D1) matches phrases like tax bene ts and employee bene ts that are not opinion expressions, while DEP typically matches positive senses of the word bene ts . Interestingly, the bigram bene ts to is not highly correlated with opinions because it matches in nitive phrases such as tax bene ts to provide and health bene ts to cut . In this case, the extraction pattern NP Prep(bene ts to) is more discriminating than the bigram for opinion classi cation." ></td>
	<td class="line x" title="135:189	The extraction pattern EEP is also highly correlated with opinions, while the unigram due and the bigram due to are not." ></td>
	<td class="line x" title="136:189	The bottom table in Figure 6 shows feature pairs identi ed for their behavioral differences on the Polarity data set, where the task is to distinguish positive reviews from negative reviews." ></td>
	<td class="line x" title="137:189	F2 and G2 are bigrams that behave differently from their component unigrams." ></td>
	<td class="line x" title="138:189	The expression nothing short (of) is typically used to express positive sentiments, while nothing and short by themselves are not." ></td>
	<td class="line x" title="139:189	The word ugly is often used as a descriptive modi er that is not expressing a sentiment per se, while and ugly appears in predicate adjective constructions that are expressing a negative sentiment." ></td>
	<td class="line x" title="140:189	The extraction pattern HEP is more discriminatory than H1 because it distinguishes negative sentiments ( the lm is a disaster!" ></td>
	<td class="line x" title="141:189	) from plot descriptions ( the disaster movie )." ></td>
	<td class="line x" title="142:189	IEP shows that active-voice usages of work are strong positive indicators, while the unigram work appears in a variety of both positive and negative contexts." ></td>
	<td class="line x" title="143:189	Finally, JEP shows that the expression manages to keep is a strong positive indicator, while manages by itelf is much less discriminating." ></td>
	<td class="line x" title="144:189	445 These examples illustrate that the subsumption hierarchy can be a powerful tool to better understand the behaviors of different kinds of features, and to identify speci c features that may be desirable for inclusion in specialized lexical resources." ></td>
	<td class="line x" title="145:189	5 Using the Subsumption Hierarchy to Reduce Feature Sets When creating opinion classi ers, people often throw in a variety of features and trust the machine learning algorithm to gure out how to make the best use of them." ></td>
	<td class="line x" title="146:189	However, we hypothesized that classi ers may perform better if we can proactively eliminate features that are not necesary because they are subsumed by other features." ></td>
	<td class="line x" title="147:189	In this section, we present a series of experiments to explore this hypothesis." ></td>
	<td class="line x" title="148:189	First, we present the results for an SVM classi er trained using different sets of unigram, bigram, and extraction pattern features, both before and after subsumption." ></td>
	<td class="line x" title="149:189	Next, we evaluate a standard feature selection approach as an alternative to subsumption and then show that combining subsumption with standard feature selection produces the best results of all." ></td>
	<td class="line x" title="150:189	5.1 Classi cation Experiments To see whether feature subsumption can improve classi cation performance, we trained an SVM classi er for each of the three opinion data sets." ></td>
	<td class="line x" title="151:189	We used the SVMlight (Joachims, 1998) package with a linear kernel." ></td>
	<td class="line x" title="152:189	For the Polarity and OP data we discarded all features that have frequency < 5, and for the MPQA data we discarded features that have frequency < 2 because this data set is substantially smaller." ></td>
	<td class="line x" title="153:189	All of our experimental results are averages over 3-fold cross-validation." ></td>
	<td class="line x" title="154:189	First, we created 4 baseline classi ers: a 1Gram classi er that uses only the unigram features; a 1+2Gram classi er that uses unigram and bigram features; a 1+EP classi er that uses unigram and extraction pattern features, and a 1+2+EP classier that uses all three types of features." ></td>
	<td class="line x" title="155:189	Next, we created analogous 1+2Gram, 1+EP, and 1+2+EP classi ers but applied the subsumption hierarchy rst to eliminate unnecessary features before training the classi er." ></td>
	<td class="line x" title="156:189	We experimented with three delta values for the subsumption process: =.0005, .001, and .002." ></td>
	<td class="line x" title="157:189	Figures 7, 8, and 9 show the results." ></td>
	<td class="line x" title="158:189	The subsumption process produced small but consistent improvements on all 3 data sets." ></td>
	<td class="line x" title="159:189	For example, Figure 8 shows the results on the OP data, where all of the accuracy values produced after subsumption (the rightmost 3 columns) are higher than the accuracy values produced without subsumption (the Base[line] column)." ></td>
	<td class="line x" title="160:189	For all three data sets, the best overall accuracy (shown in boldface) was always achieved after subsumption." ></td>
	<td class="line x" title="161:189	Features Base =.0005 =.001 =.002 1Gram 79.8 1+2Gram 81.2 81.0 81.3 81.0 1+EP 81.7 81.4 81.4 82.0 1+2+EP 81.7 82.3 82.3 82.7 Figure 7: Accuracies on Polarity Data Features Base =.0005 =.001 =.002 1Gram 97.5 1+2Gram 98.0 98.7 98.6 98.7 1+EP 97.2 97.8 97.9 97.9 1+2+EP 97.8 98.6 98.7 98.7 Figure 8: Accuracies on OP Data Features Base =.0005 =.001 =.002 1Gram 74.8 1+2Gram 74.3 74.9 74.6 74.8 1+EP 74.4 74.6 74.6 74.6 1+2+EP 74.4 74.9 74.7 74.6 Figure 9: Accuracies on MPQA Data We also observed that subsumption had a dramatic effect on the F-measure scores on the OP data, which are shown in Figure 10." ></td>
	<td class="line x" title="162:189	The OP data set is fundamentally different from the other data sets because it is so highly skewed, with 91% of the documents belonging to the non-opinion class." ></td>
	<td class="line x" title="163:189	Without subsumption, the classi er was conservative about assigning documents to the opinion class, achieving F-measure scores in the 82-88 range." ></td>
	<td class="line x" title="164:189	After subsumption, the overall accuracy improved but the F-measure scores increased more dramatically." ></td>
	<td class="line x" title="165:189	These numbers show that the subsumption process produced not only a more accurate classi er, but a more useful classi er that identi es more documents as being opinion articles." ></td>
	<td class="line x" title="166:189	For the MPQA data, we get a very small improvement of 0.1% (74.8% ! 74.9%) using subsumption." ></td>
	<td class="line x" title="167:189	But note that without subsumption the performance actually decreased when bigrams and 446 Features Base =.0005 =.001 =.002 1Gram 84.5 1+2Gram 88.0 92.5 92.0 92.3 1+EP 82.4 86.9 87.4 87.4 1+2+EP 86.7 91.8 92.5 92.3 Figure 10: F-measures on OP Data 97.6 97.8 98 98.2 98.4 98.6 98.8 99 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 11: Feature Selection on OP Data extraction patterns were added!" ></td>
	<td class="line x" title="168:189	The subsumption process counteracted the negative effect of adding the more complex features." ></td>
	<td class="line x" title="169:189	5.2 Feature Selection Experiments We conducted a second series of experiments to determine whether a traditional feature selection approach would produce the same, or better, improvements as subsumption." ></td>
	<td class="line x" title="170:189	For each feature, we computed its information gain (IG) and then selected the N features with the highest scores.7 We experimented with values of N ranging from 1,000 to 10,000 in increments of 1,000." ></td>
	<td class="line x" title="171:189	We hypothesized that applying subsumption before traditional feature selection might also help to identify a more diverse set of high-performing features." ></td>
	<td class="line x" title="172:189	In a parallel set of experiments, we explored this hypothesis by rst applying subsumption to reduce the size of the feature set, and then selecting the best N features using information gain." ></td>
	<td class="line x" title="173:189	Figures 11, 12, and 13 show the results of these experiments for the 1+2+EP classi ers." ></td>
	<td class="line x" title="174:189	Each graph shows four lines." ></td>
	<td class="line x" title="175:189	One line corresponds to the baseline classi er with no subsumption, and another line corresponds to the baseline classi er with subsumption using the best  value for that data set." ></td>
	<td class="line x" title="176:189	Each of these two lines corresponds to 7In the case of ties, we included all features with the same score as the Nth-best as well." ></td>
	<td class="line x" title="177:189	78 78.5 79 79.5 80 80.5 81 81.5 82 82.5 83 83.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.002 Feature Selection Subsumption d=0.002 + Feature Selection Figure 12: Feature Selection on Polarity Data 72 72.5 73 73.5 74 74.5 75 75.5 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy (%) Top N Baseline Subsumption d=0.0005 Feature Selection Subsumption d=0.0005 + Feature Selection Figure 13: Feature Selection on MPQA Data just a single data point (accuracy value), but we drew that value as a line across the graph for the sake of comparison." ></td>
	<td class="line x" title="178:189	The other two lines on the graph correspond to (a) feature selection for different values of N (shown on the x-axis), and (b) subsumption followed by feature selection for different values of N. On all 3 data sets, traditional feature selection performs worse than the baseline in some cases, and it virtually never outperforms the best classier trained after subsumption (but without feature selection)." ></td>
	<td class="line x" title="179:189	Furthermore, the combination of subsumption plus feature selection generally performs best of all, and nearly always outperforms feature selection alone." ></td>
	<td class="line x" title="180:189	For all 3 data sets, our best accuracy results were achieved by performing subsumption prior to feature selection." ></td>
	<td class="line x" title="181:189	The best accuracy results are 99.0% on the OP data, 83.1% on the Polarity data, and 75.4% on the MPQA data." ></td>
	<td class="line x" title="182:189	For the OP data, the improvement over baseline for both accuracy and F-measure are statistically signi cant at the p < 0.05 level (paired t-test)." ></td>
	<td class="line x" title="183:189	For the MPQA data, the improvement over baseline is 447 statistically signi cant at the p < 0.10 level." ></td>
	<td class="line x" title="184:189	6 Related Work Many features and classi cation algorithms have been explored in sentiment analysis and opinion recognition." ></td>
	<td class="line oc" title="185:189	Lexical cues of differing complexities have been used, including single words and Ngrams (e.g. , (Mullen and Collier, 2004; Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al. , 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al. , 2005))." ></td>
	<td class="line n" title="186:189	While many of these studies investigate combinations of features and feature selection, this is the rst work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classi cation." ></td>
	<td class="line x" title="187:189	7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automatic tool to remove unnecessary features to improve opinion classi cation performance." ></td>
	<td class="line x" title="188:189	Experiments with three opinion data sets showed that subsumption can improve classi cation accuracy, especially when combined with feature selection." ></td>
	<td class="line x" title="189:189	Acknowledgments This research was supported by NSF Grants IIS0208798 and IIS-0208985, the ARDA AQUAINT Program, and the Institute for Scienti c Computing Research and the Center for Applied Scienti c Computing within Lawrence Livermore National Laboratory." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-1664
Graph-Based Word Clustering Using A Web Search Engine
Matsuo, Yutaka;Sakaki, Takeshi;Uchiyama, Koki;Ishizuka, Mitsuru;"></td>
	<td class="line x" title="1:246	Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 542550, Sydney, July 2006." ></td>
	<td class="line x" title="2:246	c2006 Association for Computational Linguistics Graph-based Word Clustering using a Web Search Engine Yutaka Matsuo National Institute of Advanced Industrial Science and Technology 1-18-13 Sotokanda, Tokyo 101-0021 y.matsuo@aist.go.jp Takeshi Sakaki University of Tokyo 7-3-1 Hongo Tokyo 113-8656 Koki Uchiyama Hottolink Inc. 2-11-17 Nishi-gotanda Tokyo 141-0031 uchi@hottolink.co.jp Mitsuru Ishizuka University of Tokyo 7-3-1 Hongo Tokyo 113-8656 ishizuka@i.u-tokyo.ac.jp Abstract Word clustering is important for automatic thesaurus construction, text classification, and word sense disambiguation." ></td>
	<td class="line x" title="3:246	Recently, several studies have reported using the web as a corpus." ></td>
	<td class="line x" title="4:246	This paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web counts." ></td>
	<td class="line x" title="5:246	Each pair of words is queried to a search engine, which produces a co-occurrence matrix." ></td>
	<td class="line x" title="6:246	By calculating the similarity of words, a word cooccurrence graph is obtained." ></td>
	<td class="line x" title="7:246	A new kind of graph clustering algorithm called Newman clustering is applied for efficiently identifying word clusters." ></td>
	<td class="line x" title="8:246	Evaluations are made on two sets of word groups derived from a web directory and WordNet." ></td>
	<td class="line x" title="9:246	1 Introduction The web is a good source of linguistic information for several natural language techniques such as question answering, language modeling, and multilingual lexicon acquisition." ></td>
	<td class="line x" title="10:246	Numerous studies have examined the use of the web as a corpus (Kilgarriff, 2003)." ></td>
	<td class="line x" title="11:246	Web-based models perform especially well against the sparse data problem: Statistical techniques perform poorly when the words are rarely used." ></td>
	<td class="line x" title="12:246	For example, F. Keller et al.(2002) use the web to obtain frequencies for unseen bigrams in a given corpus." ></td>
	<td class="line x" title="14:246	They count for adjective-noun, noun-noun, and verb-object bigrams by querying a search engine, and demonstrate that web frequencies (web counts) correlate with frequencies from a carefully edited corpus such as the British National Corpus (BNC)." ></td>
	<td class="line x" title="15:246	Aside from counting bigrams, various tasks are attainable using webbased models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004)." ></td>
	<td class="line x" title="16:246	For some tasks, simple unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a standard large corpus; the web yields better counts than the BNC." ></td>
	<td class="line x" title="17:246	The web is an excellent source of information on new words." ></td>
	<td class="line x" title="18:246	Therefore, automatic thesaurus construction (Curran, 2002) offers great potential for various useful NLP applications." ></td>
	<td class="line x" title="19:246	Several studies have addressed the extraction of hypernyms and hyponyms from the web (Miura et al. , 2004; Cimiano et al. , 2004)." ></td>
	<td class="line x" title="20:246	P. Turney (2001) presents a method to recognize synonyms by obtaining word counts and calculating pointwise mutual information (PMI)." ></td>
	<td class="line x" title="21:246	For further development of automatic thesaurus construction, word clustering is beneficial, e.g. for obtaining synsets." ></td>
	<td class="line x" title="22:246	It also contributes to word sense disambiguation (Li and Abe, 1998) and text classification (Dhillon et al. , 2002) because the dimensionality is reduced efficiently." ></td>
	<td class="line x" title="23:246	This paper presents an unsupervised algorithm for word clustering based on a word similarity measure by web counts." ></td>
	<td class="line x" title="24:246	Given a set of words, the algorithm clusters the words into groups so that the similar words are in the same cluster." ></td>
	<td class="line x" title="25:246	Each pair of words is queried to a search engine, which results in a co-occurrence matrix." ></td>
	<td class="line x" title="26:246	By calculating the similarity of words, a word co-occurrence graph is created." ></td>
	<td class="line x" title="27:246	Then, a new kind of graph clustering algorithm, called Newman clustering, is applied." ></td>
	<td class="line x" title="28:246	Newman clustering emphasizes betweenness of an edge and identifies densely connected subgraphs." ></td>
	<td class="line x" title="29:246	To the best of our knowledge, this is the first attempt to obtain word groups using web counts." ></td>
	<td class="line x" title="30:246	Our contributions are summarized as follows: 542  A new algorithm for word clustering is described." ></td>
	<td class="line x" title="31:246	It has few parameters and thus is easy to implement as a baseline method." ></td>
	<td class="line x" title="32:246	 We evaluate the algorithm on two sets of word groups derived from a web directory and WordNet." ></td>
	<td class="line x" title="33:246	The chi-square measure and Newman clustering are both used in our algorithm, they are revealed to outperform PMI and hierarchical clustering." ></td>
	<td class="line x" title="34:246	We target Japanese words in this paper." ></td>
	<td class="line x" title="35:246	The remainder of this paper is organized as follows: We overview the related studies in the next section." ></td>
	<td class="line x" title="36:246	Our proposed algorithm is described in Section 3." ></td>
	<td class="line x" title="37:246	Sections 4 and 5 explain evaluations and advance discussion." ></td>
	<td class="line x" title="38:246	Finally, we conclude the paper." ></td>
	<td class="line x" title="39:246	2 Related Works A number of studies have explained the use of the web for NLP tasks e.g., creating multilingual translation lexicons (Cheng et al. , 2004), text classification (Huang et al. , 2004), and word sense disambiguation (Turney, 2004)." ></td>
	<td class="line x" title="40:246	M. Baroni and M. Ueyama summarize three approaches to use the web as a corpus (Baroni and Ueyama, 2005): using web counts as frequency estimates, building corpora through search engine queries, and crawling the web for linguistic purposes." ></td>
	<td class="line x" title="41:246	Commercial search engines are optimized for ordinary users." ></td>
	<td class="line x" title="42:246	Therefore, it is desirable to crawl the web and to develop specific search engines for NLP applications (Cafarella and Etzioni, 2005)." ></td>
	<td class="line x" title="43:246	However, considering that great efforts are taken in commercial search engines to maintain quality of crawling and indexing, especially against spammers, it is still important to pursue the possibility of using the current search engines for NLP applications." ></td>
	<td class="line x" title="44:246	P. Turney (Turney, 2001) presents an unsupervised learning algorithm for recognizing synonyms by querying a web search engine." ></td>
	<td class="line x" title="45:246	The task of recognizing synonyms is, given a target word and a set of alternative words, to choose the word that is most similar in meaning to the target word." ></td>
	<td class="line x" title="46:246	The algorithm uses pointwise mutual information (PMI-IR) to measure the similarity of pairs of words." ></td>
	<td class="line x" title="47:246	It is evaluated using 80 synonym test questions from the Test of English as a Foreign Language (TOEFL) and 50 from the English as a Second Language test (ESL)." ></td>
	<td class="line x" title="48:246	The algorithm obtains a score of 74%, contrasted to that of 64% by Latent Semantic Analysis (LSA)." ></td>
	<td class="line x" title="49:246	Terra and Clarke (Terra and Clarke, 2003) provide a comparative investigation of co-occurrence frequency estimation on the performance of synonym tests." ></td>
	<td class="line x" title="50:246	They report that PMI (with a certain window size) performs best on average." ></td>
	<td class="line oc" title="51:246	Also, PMI-IR is useful for calculating semantic orientation and rating reviews (Turney, 2002)." ></td>
	<td class="line x" title="52:246	As described, PMI is one of many measures to calculate the strength of word similarity or word association (Manning and Schutze, 2002)." ></td>
	<td class="line x" title="53:246	An important assumption is that similarity between words is a consequence of word co-occurrence, or that the proximity of words in text is indicative of relationship between them, such as synonymy or antonymy." ></td>
	<td class="line x" title="54:246	A commonly used technique to obtain word groups is distributional clustering (Baker and McCallum, 1998)." ></td>
	<td class="line x" title="55:246	Distributional clustering of words was first proposed by Pereira Tishby & Lee in (Pereira et al. , 1993): They cluster nouns according to their conditional verb distributions." ></td>
	<td class="line x" title="56:246	Graphic representations for word similarity have also been advanced by several researchers." ></td>
	<td class="line x" title="57:246	Kageura et al.(2000) propose automatic thesaurus generation based on a graphic representation." ></td>
	<td class="line x" title="59:246	By applying a minimum edge cut, the corresponding English terms and Japanese terms are identified as a cluster." ></td>
	<td class="line x" title="60:246	Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition." ></td>
	<td class="line x" title="61:246	A graph is produced by linking pairs of words which participate in particular syntactic relationships." ></td>
	<td class="line x" title="62:246	An incremental cluster-building algorithm achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes." ></td>
	<td class="line x" title="63:246	Another study builds a co-occurrence graph of terms and decomposes it to identify relevant terms by duplicating nodes and edges (Tanaka-Ishii and Iwasaki, 1996)." ></td>
	<td class="line x" title="64:246	It focuses on transitivity: if transitivity does not hold between three nodes (e.g. , if edge a-b and b-c exist but edge a-c does not), the nodes should be in separate clusters." ></td>
	<td class="line x" title="65:246	A network of words (or named entities) on the web is investigated also in the context of the Semantic Web (Cimiano et al. , 2004; Bekkerman and McCallum, 2005)." ></td>
	<td class="line x" title="66:246	Especially, a social network of persons is mined from the web using a search engine (Kautz et al. , 1997; Mika, 2005; Matsuo et al. , 2006)." ></td>
	<td class="line x" title="67:246	In these studies, the Jaccard coefficient is often used to measure the co-occurrence of entities." ></td>
	<td class="line x" title="68:246	We compare Jaccard coefficients in our evaluations." ></td>
	<td class="line x" title="69:246	In the research field on complex networks, 543 Table 1: Web counts for each word." ></td>
	<td class="line x" title="70:246	printer print InterLaser ink TV Aquos Sharp 17000000 103000000 215 18900000 69100000 1760000000 2410000 186000000 Table 2: Co-occurrence matrix by web counts." ></td>
	<td class="line x" title="71:246	printer print InterLaser ink TV Aquos Sharp printer  4780000 179 4720000 4530000 201000 990000 print 4780000  183 4800000 8390000 86400 1390000 InterLaser 179 183  116 65 0 0 ink 4720000 4800000 116  10600000 144000 656000 TV 4530000 8390000 65 10600000  1660000 42300000 Aquos 201000 86400 0 144000 1660000  1790000 Sharp 990000 1390000 0 656000 42300000 1790000  structures of various networks are investigated in detail." ></td>
	<td class="line x" title="72:246	For example, Motter (2002) targeted a conceptual network from a thesaurus and demonstrated its small-world structure." ></td>
	<td class="line x" title="73:246	Recently, numerous works have identified communities (or densely-connected subgraphs) from large networks (Newman, 2004; Girvan and Newman, 2002; Palla et al. , 2005) as explained in the next section." ></td>
	<td class="line x" title="74:246	3 Word Clustering using Web Counts 3.1 Co-occurrence by a Search Engine A typical word clustering task is described as follows: given a set of words (nouns), cluster words into groups so that the similar words are in the same cluster 1." ></td>
	<td class="line x" title="75:246	Let us take an example." ></td>
	<td class="line x" title="76:246	Assume a set of words is given:(printer), n(print),(InterLaser), (ink), TV (TV), Aquos (Aquos), and Sharp (Sharp)." ></td>
	<td class="line x" title="77:246	Apparently, the first four words are related to a printer, and the last three words are related to a TV 2." ></td>
	<td class="line x" title="78:246	In this case, we would like to have two word groups: the first four and the last three." ></td>
	<td class="line x" title="79:246	We query a search engine3 to obtain word counts." ></td>
	<td class="line x" title="80:246	Table 1 shows web counts for each word." ></td>
	<td class="line x" title="81:246	Table 2 shows the web counts for pairs of words." ></td>
	<td class="line x" title="82:246	For example, we submit a query printer AND InterLaser to a search engine, and are directed to 179 documents." ></td>
	<td class="line x" title="83:246	Thereby, nC2 queries are necessary to obtain the matrix if we have n words." ></td>
	<td class="line x" title="84:246	We call Table 2 a co-occurrence matrix." ></td>
	<td class="line x" title="85:246	We can calculate the pointwise mutual informa1In this paper, we limit our scope to clustering nouns." ></td>
	<td class="line x" title="86:246	We discuss the extension in Section 4." ></td>
	<td class="line x" title="87:246	2InterLaser is a laser printer made by Epson Corp. Aquos is a liquid crystal TV made by Sharp Corp. 3Google (www.google.co.jp) is used in our study." ></td>
	<td class="line x" title="88:246	tion between word w1 and w2 as PMI(w1,w2) = log2 p(w1,w2)p(w 1)p(w2)." ></td>
	<td class="line x" title="89:246	Probability p(w1) is estimated by fw1/N, where fw1 represents the web count of w1 and N represents the number of documents on the web." ></td>
	<td class="line x" title="90:246	Probability of co-occurrence p(w1,w2) is estimated by fw1,w2/N where fw1,w2 represents the web count of w1 AND w2." ></td>
	<td class="line x" title="91:246	The PMI values are shown in Table 3." ></td>
	<td class="line x" title="92:246	We set N = 1010 according to the number of indexed pages on Google." ></td>
	<td class="line x" title="93:246	Some values are inconsistent with our intuition: Aquos is inferred to have high PMI to TV and Sharp, but also to printer." ></td>
	<td class="line x" title="94:246	None of the words has high PMI with TV." ></td>
	<td class="line x" title="95:246	These are because the range of the word count is broad." ></td>
	<td class="line x" title="96:246	Generally, mutual information tends to provide a large value if either word is much rarer than the other." ></td>
	<td class="line x" title="97:246	Various statistical measures based on cooccurrence analysis have been proposed for estimating term association: the DICE coefficient, Jaccard coefficient, chi-square test, and the loglikelihood ratio (Manning and Schutze, 2002)." ></td>
	<td class="line x" title="98:246	In our algorithm, we use the chi-square (2) value instead of PMI." ></td>
	<td class="line x" title="99:246	The chi-square value is calculated as follows: We denote the number of pages containing both w1 and w2 as a. We also denote b, c, d as follows4." ></td>
	<td class="line x" title="100:246	w2 w2 w1 a b w1 c d Thereby, the expected frequency of (w1, w2) is (a+c)(a+b)/N. Eventually, chi-square is calculated as follows (Manning and Schutze, 2002)." ></td>
	<td class="line x" title="101:246	4Note that N = a + b + c + d. 544 Table 3: A matrix of pointwise mutual information." ></td>
	<td class="line x" title="102:246	printer print InterLaser ink TV Aquos Sharp printer  4.771 8.936 7.199 0.598 5.616 1.647 print 4.771  6.369 4.624 -1.111 1.799 -0.463 InterLaser 8.936 6.369  8.157 0.781 * * ink 7.199 4.624 8.157  1.672 4.983 0.900 TV 0.598 -1.111 0.781 1.672  1.969 0.370 Aquos 5.616 1.799 *." ></td>
	<td class="line x" title="103:246	4.983 1.969  5.319 Sharp 1.647 -0.463 * 0.900 0.370 5.319  * represents that the PMI is not available because the co-occurrence web count is zero, in which case we set ." ></td>
	<td class="line x" title="104:246	Table 4: A matrix of chi-square values." ></td>
	<td class="line x" title="105:246	printer print InterLaser ink TV Aquos Sharp printer  6880482.6 399.2 5689710.7 0.0* 0.0* 0.0* print 6880482.6  277.8 3321184.6 176855.5 0.0* 0.0* InterLaser 399.2 277.8  44.8 0.0* 0.0 0.0 ink 5689710.7 3321184.6 44.8  1419485.5 0.0* 0.0* TV 0.0* 176855.5 0.0* 1419485.5  26803.2 70790877.6 Aquos 0.0* 0.0* 0.0 0.0* 26803.2  729357.7 Sharp 0.0* 0.0* 0.0 0.0* 70790877.6 729357.7  * represents that the observed co-occurrence frequency is below the expected value, in which case we set 0.0." ></td>
	<td class="line x" title="106:246	Figure 1: Examples of Newman clustering." ></td>
	<td class="line x" title="107:246	2(w1,w2) = N (adbc) 2 (a + b)(a + c)(b + d)(c + d) However, N is a huge number on the web and sometimes it is difficult to know exactly." ></td>
	<td class="line x" title="108:246	Therefore we regard the co-occurrence matrix as a contingency table: bprime =  wW;wnegationslash=w2 fw1,w, cprime =  wW;wnegationslash=w1 fw2,w; dprime =  w,wprimeW;w and wprimenegationslash=w1 nor w2 fw,wprime, Nprime =  w,wprimeW fw,wprime, where W represents a given set of words." ></td>
	<td class="line x" title="109:246	Then chi-square (within the word list W) is defined as 2W(w1,w2) = N prime (adprime bprime cprime)2 (a + bprime)(a + cprime)(bprime + dprime)(cprime + dprime)." ></td>
	<td class="line x" title="110:246	We should note that 2W depends on a word set W. It calculates the relative strength of cooccurrences." ></td>
	<td class="line x" title="111:246	Table 4 shows the 2W values." ></td>
	<td class="line x" title="112:246	Aquos has high values only with TV and Sharp as expected." ></td>
	<td class="line x" title="113:246	3.2 Clustering on Co-occurrence Graph Recently, a series of effective graph clustering methods has been advanced." ></td>
	<td class="line x" title="114:246	Pioneering work that specifically emphasizes edge betweenness was done by Girvan and Newman (2002): we call the method as GN algorithm." ></td>
	<td class="line x" title="115:246	Betweenness of an edge is the number of shortest paths between pairs of nodes that run along it." ></td>
	<td class="line x" title="116:246	Figure 1 (i) shows that two communities (in Girvans term), i.e. {a,b,c} and {d,e,f,g}, which are connected by edge c-d." ></td>
	<td class="line x" title="117:246	Edge c-d has high betweenness because numerous shortest paths (e.g. , from a to d, from b to e, ) traverse the edge." ></td>
	<td class="line x" title="118:246	The graph is likely to be separated into densely connected subgraphs if we cut the high betweenness edge." ></td>
	<td class="line x" title="119:246	The GN algorithm is different from the minimum edge cut." ></td>
	<td class="line x" title="120:246	For (i), the results are identical: By cutting edge c-d, which is a minimum edge cut, we can obtain two clusters." ></td>
	<td class="line x" title="121:246	However in case of (ii), there are two candidates for the minimum edge cut, whereas the highest betweenness edge is still only edge c-d." ></td>
	<td class="line x" title="122:246	Girvan et al.(2002) shows that this clustering works well to various networks from biological to social networks." ></td>
	<td class="line x" title="124:246	Numerous studies have been inspired by that work." ></td>
	<td class="line x" title="125:246	One prominent effort is a faster variant of GN algorithm (Newman, 2004), which we call Newman clustering in 545 Figure 2: An illustration of graph-based word clustering." ></td>
	<td class="line x" title="126:246	this paper." ></td>
	<td class="line x" title="127:246	In Newman clustering, instead of explicitly calculating high-betweenness edges (which is computationally demanding), an objective function is defined as follows: Q =  i parenleftBigg eii  parenleftBigg j eij )2) (1) We assume that we have separate clusters, and that eij is the fraction5 of edges in the network that connect nodes in cluster i to those in cluster j. The term eii denotes the fraction of edges within the clusters." ></td>
	<td class="line x" title="128:246	The term j eij represents the expected fraction of edges within the cluster." ></td>
	<td class="line x" title="129:246	If a par5We can calculate eij using the number of edges between cluster i and j divided by the number of all edges." ></td>
	<td class="line x" title="130:246	Figure 3: A word graph for 88 Japanese words." ></td>
	<td class="line x" title="131:246	ticular division gives no more within-community edges than would be expected by random chance, then we would obtain Q = 0." ></td>
	<td class="line x" title="132:246	In practice, values greater than about 0.3 appear to indicate significant group structure (Newman, 2004)." ></td>
	<td class="line x" title="133:246	Newman clustering is agglomerative (although we can intuitively understand that a graph without high betweenness edges is ultimately obtained)." ></td>
	<td class="line x" title="134:246	We repeatedly join clusters together in pairs, choosing at each step the joint that provides the greatest increase in Q. Currently, Newman clustering is one of the most efficient methods for graph-based clustering." ></td>
	<td class="line x" title="135:246	The illustration of our algorithm is shown in Fig." ></td>
	<td class="line x" title="136:246	2." ></td>
	<td class="line x" title="137:246	First, we obtain web counts among a given set of words using a search engine." ></td>
	<td class="line x" title="138:246	Then PMI or the chi-square values are calculated." ></td>
	<td class="line x" title="139:246	If the value is above a certain threshold6, we invent an edge between the two nodes." ></td>
	<td class="line x" title="140:246	Then, we apply graph clustering and finally identify groups of words." ></td>
	<td class="line x" title="141:246	This illustration shows that the chi-square measure yields the correct clusters." ></td>
	<td class="line x" title="142:246	The algorithm is described in Fig." ></td>
	<td class="line x" title="143:246	4." ></td>
	<td class="line x" title="144:246	The parameters are few: a threshold dthre for a graph and, optionally, the number of clusters nc." ></td>
	<td class="line x" title="145:246	This enables easy implementation of the algorithm." ></td>
	<td class="line x" title="146:246	Figure 3 is a small network of 88 Japanese words obtained through 3828 search queries." ></td>
	<td class="line x" title="147:246	We can see that some parts in the graph are densely connected." ></td>
	<td class="line x" title="148:246	4 Experimental Results This section addresses evaluation." ></td>
	<td class="line x" title="149:246	Two sets of word groups are used for the evaluation: one is derived from documents on a web directory; another is from WordNet." ></td>
	<td class="line x" title="150:246	We first evaluate the co6In this example, 4.0 for PMI and 200 for 2." ></td>
	<td class="line x" title="151:246	546 a19 a16 1." ></td>
	<td class="line x" title="152:246	Input A set of words is given." ></td>
	<td class="line x" title="153:246	The number of words is denoted as n. 2." ></td>
	<td class="line x" title="154:246	Obtain frequencies Put a query for each pair of words to a search engine, and obtain a cooccurrence matrix." ></td>
	<td class="line x" title="155:246	Then calculate the chi-square matrix (alternatively a PMI matrix, or a Jaccard matrix.)" ></td>
	<td class="line x" title="156:246	3." ></td>
	<td class="line x" title="157:246	Make a graph Set a node for each word, and an edge to a pair of nodes whose 2 value is above a threshold." ></td>
	<td class="line x" title="158:246	The threshold is determined so that the network density (the number of edges divided by nC2) is dthre." ></td>
	<td class="line x" title="159:246	4." ></td>
	<td class="line x" title="160:246	Apply Newman clustering Initially set each node as a cluster." ></td>
	<td class="line x" title="161:246	Then merge two clusters repeatedly so that Q is maximized." ></td>
	<td class="line x" title="162:246	Terminate if Q does not increase anymore, or when a given number of clusters nc is obtained." ></td>
	<td class="line x" title="163:246	(Alternatively, apply average-link hierarchical clustering.)" ></td>
	<td class="line x" title="164:246	5." ></td>
	<td class="line x" title="165:246	Output Output groups of words." ></td>
	<td class="line x" title="166:246	a18 a17 Figure 4: Our algorithm for word clustering." ></td>
	<td class="line x" title="167:246	occurrence measures, then we evaluate the clustering methods." ></td>
	<td class="line x" title="168:246	4.1 Word Groups from an Open Directory We collected documents from the Japanese Open Directory (dmoz.org/World/Japanese)." ></td>
	<td class="line x" title="169:246	The dmoz japanese category contains about 130,000 documents and more than 10,000 classes." ></td>
	<td class="line x" title="170:246	We chose 9 categories out of the top 12 categories: art, sports, computer, game, society, family, science, and health." ></td>
	<td class="line x" title="171:246	We crawled 1000 documents for each category, i.e., 9000 documents in all." ></td>
	<td class="line x" title="172:246	For each category, a word group is obtained through the procedure in Fig." ></td>
	<td class="line x" title="173:246	5." ></td>
	<td class="line x" title="174:246	We consider that the specific words to a category are relevant to some extent, and that they can therefore be regarded as a word group." ></td>
	<td class="line x" title="175:246	Examples are shown in Table 5." ></td>
	<td class="line x" title="176:246	In all, 90 word sets are obtained and merged." ></td>
	<td class="line x" title="177:246	We call the word set DMOZ-J data." ></td>
	<td class="line x" title="178:246	Our task is, given 90 words, to cluster the words into the correct nine groups." ></td>
	<td class="line x" title="179:246	Here we investigate whether the correct nine words are selected for each word using the co-occurrence measure." ></td>
	<td class="line x" title="180:246	We compare pointwise mutual information (PMI), the Jaccard coefficient (Jaccard), and chi-square (2)." ></td>
	<td class="line x" title="181:246	We chose these methods for comparison because PMI performs best in (Terra and Clarke, 2003)." ></td>
	<td class="line x" title="182:246	The Jaccard coefficient is often used in social network mining from the web." ></td>
	<td class="line x" title="183:246	Table 7 shows the precision of each method." ></td>
	<td class="line x" title="184:246	Experiments are repeated five times." ></td>
	<td class="line x" title="185:246	We keep each method that outputs the a19 a16 1." ></td>
	<td class="line x" title="186:246	For each category, crawl 1000 documents randomlya 2." ></td>
	<td class="line x" title="187:246	Apply the Japanese morphological analysis system ChaSen (Matsumoto et al. , 2000) to the documents." ></td>
	<td class="line x" title="188:246	Calculate the score of each word w in category c similarly to TF-IDF: score(w,c) = fc(w)log(Nall/fall(w)) where fc denotes the document frequency of word w in category c, Nall denotes the number of all documents, and fall(w) denotes the frequency of word w in all documents." ></td>
	<td class="line x" title="189:246	3." ></td>
	<td class="line x" title="190:246	For each category, the top 10 words are selected as the word group." ></td>
	<td class="line x" title="191:246	aWe first get all urls, sort them, and select a sample randomly." ></td>
	<td class="line x" title="192:246	a18 a17 Figure 5: Procedure for obtaining word groups for a category." ></td>
	<td class="line x" title="193:246	Table 7: Precision for DMOZ-J set." ></td>
	<td class="line x" title="194:246	PMI Jaccard 2 Mean 0.415 0.402 0.537 Min 0.396 0.376 0.493 Max 0.447 0.424 0.569 SD 0.020 0.020 0.032 highest nine words for each word, groups of ten words." ></td>
	<td class="line x" title="195:246	Therefore, recall is the same as the precision." ></td>
	<td class="line x" title="196:246	From the table, the chi-square performs best." ></td>
	<td class="line x" title="197:246	PMI is slightly better than the Jaccard coefficient." ></td>
	<td class="line x" title="198:246	4.2 Word Groups from WordNet Next, we make a comparison using WordNet 7." ></td>
	<td class="line x" title="199:246	By extracting 10 words that have the same hypernym (i.e. coordinates), we produce a word group." ></td>
	<td class="line x" title="200:246	Examples are shown in Table 6." ></td>
	<td class="line x" title="201:246	Nine word groups are merged into one, as with DMOZ-J." ></td>
	<td class="line x" title="202:246	The experiments are repeated 10 times." ></td>
	<td class="line x" title="203:246	Table 8 shows the result." ></td>
	<td class="line x" title="204:246	Again, the chi-square performs best among the methods that were compared." ></td>
	<td class="line x" title="205:246	Detailed analyses of the results revealed that word groups such as bacteria and diseases are clustered correctly." ></td>
	<td class="line x" title="206:246	However, word groups such as computers (in which homepage, server and client are included) are not well clustered: these words tend to be polysemic, which causes difficulty." ></td>
	<td class="line x" title="207:246	4.3 Evaluation of Clustering We compare two clustering methods: Newman clustering and average-link agglomerative cluster7We use a partly-translated version of WordNet." ></td>
	<td class="line x" title="208:246	547 Table 5: Examples of word groups from DMOZ-J." ></td>
	<td class="line x" title="209:246	category specific words to a category as a word group (art)h(gallery),^(artwork),6 (theater),(saxophone),yN(verse),(live concert),(guitar),x(performance),(ballet),x 2(personal exhibition)  (recreation) (raising),(poult),(hamster), G(travel diary),q(national park), (brewing), (boat race), (competition),(fishing pond) H(health)(illness), (patient), (myositis),J(surgery),  s(dialysis),(steroid),U *(test), i(medical ward),|j(collagen disease),R(clinic) Table 6: Examples of word groups from WordNet." ></td>
	<td class="line x" title="210:246	hypernym hyponyms as a word group E t(gem)(amethyst),(aquamarine),(diamond),(emerald),(moonstone),(peridot),(ruby),(sapphire), (topaz),(tourmaline) (academic field) J(natural science), :(mathematics), (agronomics),P(architectonics), (geology), g(psychology), C(computer science), J(cognitive science), q(sociology),t(linguistics) (drink) (milk),(alcohol), Z(cooling beverage),x(carbonated beverage), (soda),(cocoa),(fruit juice),(coffee),S(tea), (mineral water) Table 8: Precision of WordNet set." ></td>
	<td class="line x" title="211:246	PMI Jaccard 2 Mean 0.549 0.484 0.584 Min 0.473 0.415 0.498 Max 0.593 0.503 0.656 SD 0.037 0.027 0.048 Table 9: Precision, recall and the F-measure for each clustering." ></td>
	<td class="line x" title="212:246	PMI Jaccard 2 Average precision 0.633 0.603 0.486 -link recall 0.102 0.101 0.100 F-measure 0.179 0.173 0.164 Newman precision 0.751 0.739 0.546 recall 0.103 0.103 0.431 F-measure 0.182 0.181 0.480 ing, which is often used in word clustering." ></td>
	<td class="line x" title="213:246	A word co-occurrence graph is created using PMI, Jaccard, and chi-square measures." ></td>
	<td class="line x" title="214:246	The threshold is determined so that the network density dthre is 0.3." ></td>
	<td class="line x" title="215:246	Then, we apply clustering to obtain nine clusters; nc = 9." ></td>
	<td class="line x" title="216:246	Finally, we compare the resultant clusters with the correct categories." ></td>
	<td class="line x" title="217:246	Clustering results for DMOZ-J sets are shown in Table 9." ></td>
	<td class="line x" title="218:246	Newman clustering produces higher precision and recall." ></td>
	<td class="line x" title="219:246	Especially, the combination of chi-square and Newman is the best in our experiments." ></td>
	<td class="line x" title="220:246	5 Discussion In this paper, the scope of co-occurrence is document-wide." ></td>
	<td class="line x" title="221:246	One reason is that major commercial search engines do not support a type of query w1 NEAR w2." ></td>
	<td class="line x" title="222:246	Another reason is in (Terra and Clarke, 2003) document-wide co-occurrences perform comparable to other Windows-based cooccurrences." ></td>
	<td class="line x" title="223:246	Many types of co-occurrence exist other than noun-noun." ></td>
	<td class="line x" title="224:246	We limit our scope to noun-noun co-occurrences in this paper." ></td>
	<td class="line x" title="225:246	Other types of cooccurrence such as verb-noun can be investigated in future studies." ></td>
	<td class="line x" title="226:246	Also, co-occurrence for the second-order similarity can be sought." ></td>
	<td class="line x" title="227:246	Because web documents are sometimes difficult to analyze, we keep our algorithm as simple as possible." ></td>
	<td class="line x" title="228:246	Analyzing semantic relations and applying distributional clustering is another goal for future work." ></td>
	<td class="line x" title="229:246	A salient weak point of our algorithm is the number of necessary queries allowed to a search engine." ></td>
	<td class="line x" title="230:246	For obtaining a graph of n words, O(n2) queries are required, which discourages us from undertaking large experiments." ></td>
	<td class="line x" title="231:246	However some devices are possible: if we analyze the texts of the top retrieved pages by query w, we can guess what words are likely to co-occur with w. This preprocessing seems promising at least in social network extraction: we can eliminate 85% of queries in the 500 nodes case while retaining more than 90% precision (Asada et al. , 2005)." ></td>
	<td class="line x" title="232:246	In our evaluation, the chi-square measure performed well." ></td>
	<td class="line x" title="233:246	One reason is that the PMI performs worse when a word group contains rare or frequent words, as is generally known for mutual information measure (Manning and Schutze, 2002)." ></td>
	<td class="line x" title="234:246	Another reason is that if we put one word and two words to a search engine, the result might be inconsistent." ></td>
	<td class="line x" title="235:246	In an extreme case, the web count of w1 is below the web count of w1ANDw2." ></td>
	<td class="line x" title="236:246	This 548 phenomenon depends on how a search engine processes AND operator, and results in unstable values for the PMI." ></td>
	<td class="line x" title="237:246	On the other hand, our method by the chi-square uses a co-occurrence matrix as a contingency table." ></td>
	<td class="line x" title="238:246	For that reason, it suffers less from the problem." ></td>
	<td class="line x" title="239:246	Other statistical measures such as the likelihood ratio are also applicable." ></td>
	<td class="line x" title="240:246	6 Conclusion This paper describes a new approach for word clustering using a search engine." ></td>
	<td class="line x" title="241:246	The chi-square measure is used to overcome the broad range of word counts for a given set of words." ></td>
	<td class="line x" title="242:246	We also apply recently-developed Newman clustering, which yields promising results through our evaluations." ></td>
	<td class="line x" title="243:246	Our algorithm has few parameters." ></td>
	<td class="line x" title="244:246	Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004)." ></td>
	<td class="line x" title="245:246	New words are generated day by day on the web." ></td>
	<td class="line x" title="246:246	We believe that to automatically identify new words and obtain word groups potentially enhances many NLP applications." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3301
The Semantics Of A Definiendum Constrains Both The Lexical Semantics And The Lexicosyntactic Patterns In The Definiens
Yu, Hong;Wei, Ying;"></td>
	<td class="line x" title="1:155	Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 18, New York City, June 2006." ></td>
	<td class="line x" title="2:155	c2006 Association for Computational Linguistics The Semantics of a Definiendum Constrains both the Lexical Semantics and the Lexicosyntactic Patterns in the Definiens Hong Yu Ying Wei Department of Health Sciences Department of Biostatistics University of Wisconsin-Milwaukee Columbia University Milwaukee, WI 53201 New York, NY 10032 Hong.Yu@uwm.edu Ying.Wei@columbia.com Abstract Most current definitional question answering systems apply one-size-fits-all lexicosyntactic patterns to identify definitions." ></td>
	<td class="line x" title="3:155	By analyzing a large set of online definitions, this study shows that the semantic types of definienda constrain both lexical semantics and lexicosyntactic patterns of the definientia." ></td>
	<td class="line x" title="4:155	For example, heart has the semantic type [Body Part, Organ, or Organ Component] and its definition (e.g. , heart locates between the lungs) incorporates semantic-typedependent lexicosyntactic patterns (e.g. , TERM locates ) and terms (e.g. , lung has the same semantic type [Body Part, Organ, or Organ Component])." ></td>
	<td class="line x" title="5:155	In contrast, AIDS has a different semantic type [Disease or Syndrome]; its definition (e.g. , An infectious disease caused by human immunodeficiency virus) consists of different lexicosyntactic patterns (e.g. , causes by) and terms (e.g. , infectious disease has the semantic type [Disease or Syndrome])." ></td>
	<td class="line x" title="6:155	The semantic types are defined in the widely used biomedical knowledge resource, the Unified Medical Language System (UMLS)." ></td>
	<td class="line x" title="7:155	1 Introduction Definitional questions (e.g. , What is X?) constitute an important question type and have been a part of the evaluation at the Text Retrieval Conference (TREC) Question Answering Track since 2003." ></td>
	<td class="line x" title="8:155	Most systems apply one-size-fits-all lexicosyntactic patterns to identify definitions (Liang et al. 2001; Blair-Goldensohn et al. 2004; Hildebrandt et al. 2004; Cui et al. 2005)." ></td>
	<td class="line x" title="9:155	For example, the pattern NP, (such as|like|including) query term can be used to identify the definition New research in mice suggests that drugs such as Ritalin quiet hyperactivity (Liang et al. 2001)." ></td>
	<td class="line x" title="10:155	Few existing systems, however, have explored the relations between the semantic type (denoted as SDT) of a definiendum (i.e. , a defined term (DT)) and the semantic types (denoted as SDef) of terms in its definiens (i.e. , definition)." ></td>
	<td class="line x" title="11:155	Additionally, few existing systems have examined whether the lexicosyntactic patterns of definitions correlate with the semantic types of the defined terms." ></td>
	<td class="line x" title="12:155	By analyzing a large set of online definitions, this study shows that 1) SDef correlates with SDT, and 2) SDT constrains the lexicosyntactic patterns of the corresponding definitions." ></td>
	<td class="line x" title="13:155	In the following, we will illustrate our findings with the following four definitions: a. Heart[Body Part, Organ, or Organ Component]: The hollow[Spatial Concept] muscular[Spatial Concept] organ[Body Part, Organ, or Organ Component,Tissue] located[Spatial Concept] behind[Spatial Concept] the sternum[Body Part, Organ, or Organ Component] and between the lungs[Body Part, Organ, or Organ Component]." ></td>
	<td class="line x" title="14:155	b. Kidney[Body Part, Organ, or Organ Component]: The kidneys are a pair of glandular organs[Body Part, Organ, or Organ Component] located[Spatial Concept] in the abdominal_cavities[Body Part, Organ, or Organ Component] of mammals[Mammal] and reptiles[Reptile]." ></td>
	<td class="line x" title="15:155	c. Heart attack[Disease or Syndrome]: also called myocardial_infarction[Disease or Syndrome]; damage[Functional Concept] to the heart_muscle[Tissue] due to insufficient 1 blood supply[Organ or Tissue Function] for an extended[Spatial Concept] time_period[Temporal Concept]." ></td>
	<td class="line x" title="16:155	d. AIDS[Disease or Syndrome]: An infectious_disease[Disease or Syndrome] caused[Functional Concept] by human_immunodeficiency_virus[Virus]." ></td>
	<td class="line x" title="17:155	In the above four definitions, the superscripts in [brackets] are the semantic types (e.g. , [Body Part, Organ, or Organ Component] and [Disease or Syndrome]) of the preceding terms." ></td>
	<td class="line x" title="18:155	A multiword term links words with the underscore _." ></td>
	<td class="line x" title="19:155	For example, heart IS-A [Body Part, Organ, or Organ Component] and heart_muscle IS-A [Tissue]." ></td>
	<td class="line x" title="20:155	The semantic types are defined in the Semantic Network (SN) of the Unified Medical Language System (UMLS), the largest biomedical knowledge resource." ></td>
	<td class="line x" title="21:155	Details of the UMLS and SN will be described in Section 2." ></td>
	<td class="line x" title="22:155	We applied MMTx (Aronson et al. 2004) to automatically map a string to the UMLS semantic types." ></td>
	<td class="line x" title="23:155	MMTx will also be described in Section 2." ></td>
	<td class="line x" title="24:155	Simple analysis of the above four definitions shows that given a defined term (DT) with a semantic type SDT (e.g. , [Body Part, Organ, or Organ Component]), terms that appear in the definition tend to have the same or related semantic types (e.g. , [Body Part, Organ, or Organ Component] and [Spatial Concept])." ></td>
	<td class="line x" title="25:155	Such observations were first reported as Aristotelian definitions (Bodenreider and Burgun 2002) in the limited domain of anatomy." ></td>
	<td class="line x" title="26:155	(Rindflesch and Fiszman 2003) reported that the hyponym related to the definiendum must be in an IS-A relation with the hypernym that is related to the definiens." ></td>
	<td class="line x" title="27:155	However, neither work demonstrated statistical patterns on a large corpus as we report in this study." ></td>
	<td class="line x" title="28:155	Additionally, none of the work explicitly suggested the use of patterns to support question answering." ></td>
	<td class="line x" title="29:155	In addition to statistical correlations among semantic types, the lexicosyntactic patterns of the definitions correlate with SDT." ></td>
	<td class="line x" title="30:155	For example, as shown by sentences a~d, when SDT is [Body Part, Organ, or Organ Component], its lexicosyntactic patterns include located." ></td>
	<td class="line x" title="31:155	In contrast, when SDT is [Disease or Syndrome], the patterns include due to and  caused by." ></td>
	<td class="line x" title="32:155	In this study, we empirically studied statistical correlations between SDT and SDef and between SDT and the lexicosyntactic patterns in the definitions." ></td>
	<td class="line x" title="33:155	Our study is a result of detailed statistical analysis of 36,535 defined terms and their 226,089 online definitions." ></td>
	<td class="line x" title="34:155	We built our semantic constraint model based on the widely used biomedical knowledge resource, the UMLS." ></td>
	<td class="line x" title="35:155	We also adapted a robust information extraction system to generate automatically a large number of lexicosyntactic patterns from definitions." ></td>
	<td class="line x" title="36:155	In the following, we will first describe the UMLS and its semantic types." ></td>
	<td class="line x" title="37:155	We will then describe our data collection and our methods for pattern generation." ></td>
	<td class="line x" title="38:155	2 Unified Medical Language System The Unified Medical Language System (UMLS) is the largest biomedical knowledge source maintained by the National Library of Medicine." ></td>
	<td class="line x" title="39:155	It provides standardized biomedical concept relations and synonyms (Humphreys et al. 1998)." ></td>
	<td class="line x" title="40:155	The UMLS has been widely used in many natural language processing tasks, including information retrieval (Eichmann et al. 1998), extraction (Rindflesch et al. 2000), and text summarization (Elhadad et al. 2004; Fiszman et al. 2004)." ></td>
	<td class="line x" title="41:155	The UMLS includes the Metathesaurus (MT), which contains over one million biomedical concepts and the Semantic Network (SN), which represents a high-level abstraction from the UMLS Metathesaurus." ></td>
	<td class="line x" title="42:155	The SN consists of 134 semantic types with 54 types of semantic relations (e.g. , is-a or part-of) that relate the semantic types to each other." ></td>
	<td class="line x" title="43:155	The UMLS Semantic Network provides broad and general world knowledge that is related to human health." ></td>
	<td class="line x" title="44:155	Each UMLS concept is assigned one or more semantic types." ></td>
	<td class="line x" title="45:155	The National Library of Medicine also makes available MMTx, a programming implementation of MetaMap (Aronson 2001), which maps free text to the UMLS concepts and associated semantic types." ></td>
	<td class="line x" title="46:155	MMTx first parses text into sentences, then chunks the sentences into noun phrases." ></td>
	<td class="line x" title="47:155	Each noun phrase is then mapped to a set of possible UMLS concepts, taking into account spelling and morphological variations; each concept is weighted, with the highest weight representing the most likely mapped concept." ></td>
	<td class="line x" title="48:155	One recent study has evaluated MMTx to have 79% (Yu and Sable 2005) accuracy for mapping a term to the semantic 2 type(s) in a small set of medical questions." ></td>
	<td class="line x" title="49:155	Another study (Lacson and Barzilay 2005) measured MMTx to have a recall of 74.3% for capturing the semantic types in another set of medical texts." ></td>
	<td class="line x" title="50:155	In this study, we applied MMTx to identify the semantic types of terms that appear in their definitions." ></td>
	<td class="line x" title="51:155	For each candidate term, MMTx ranks a list of UMLS concepts with confidence." ></td>
	<td class="line x" title="52:155	In this study, we selected the UMLS concept that was assigned with the highest confidence by MMTx." ></td>
	<td class="line x" title="53:155	The UMLS concepts were then used to obtain the corresponding semantic types." ></td>
	<td class="line x" title="54:155	3 Data Collection We collected a large number of online definitions for the purpose of our study." ></td>
	<td class="line x" title="55:155	Specifically, we applied more than 1 million of the UMLS concepts as candidate definitional terms, and searched for the definitions from the World Wide Web using the Google:Definition service; this resulted in the downloads of a total of 226,089 definitions that corresponded to a total of 36,535 UMLS concepts (or 3.7% of the total of 1 million UMLS concepts)." ></td>
	<td class="line x" title="56:155	We removed from definitions the defined terms; this step is necessary for our statistical studies, which we will explain later in the following sections." ></td>
	<td class="line x" title="57:155	We applied MMTx to obtain the corresponding semantic types." ></td>
	<td class="line x" title="58:155	4 Statistically Correlated Semantic Types We then identified statistically correlated semantic types between SDT and SDef based on bivariate tabular chi-square (Fleiss 1981)." ></td>
	<td class="line x" title="59:155	Specifically, given a semantic type STYi, i=1,2,3,, 134 of any defined term, the observed numbers of definitions that were and were not assigned the STYi are O(Defi) and O(Defi)." ></td>
	<td class="line x" title="60:155	All indicates the total 226,089 definitions." ></td>
	<td class="line x" title="61:155	The observed numbers of definitions in which the semantic type STYi, did and did not appear were O(Alli) and O(Alli)." ></td>
	<td class="line x" title="62:155	134 represents the total number of the UMLS semantic types." ></td>
	<td class="line x" title="63:155	We applied formulas (1) and (2) to calculate expected frequencies and then the chi-square value (the degree of freedom is one)." ></td>
	<td class="line x" title="64:155	A high chi-square value indicates the importance of the semantic type that appears in the definition." ></td>
	<td class="line x" title="65:155	We removed the defined terms from their definitions prior to the semantictype statistical analysis in order to remove the bias introduced by the defined terms (i.e. , defined terms frequently appear in the definitions)." ></td>
	<td class="line x" title="66:155	( )iDefE = N NN iDef *, ( ) iDefE = N NN iDef *, ( )iAllE = N NN iAll *, ( )iAllE = N NN iAll * (1) ( ) = EOE 2 2 (2) To determine whether the chi-square value is large enough for statistical significance, we calculated its p-value." ></td>
	<td class="line x" title="67:155	Typically, 0.05 is the cutoff of significance, i.e. significance is accepted if the corresponding p-value is less than 0.05." ></td>
	<td class="line x" title="68:155	This criterion ensures the chance of false significance (incorrectly detected due to chance) is 0.05 for a single SDT-SDef pair." ></td>
	<td class="line x" title="69:155	However, since there are 134*134 possible SDT-SDef pairs, the chance for obtaining at least one false significance could be very high." ></td>
	<td class="line x" title="70:155	To have a more conservative inference, we employed a Bonferroni-type correction procedure (Hochberg 1988)." ></td>
	<td class="line x" title="71:155	Specifically, let )()2()1( mppp  L be the ordered raw p-values, where m is the total number of SDT-SDef pairs." ></td>
	<td class="line x" title="72:155	A SDef is significantly associated with a SDT if SDefs corresponding p-value )1/()( + imp i  for some i. This correction procedure allows the probability of at-least-onefalse-significance out of the total m pairs is less than alpha (=0.05)." ></td>
	<td class="line x" title="73:155	The number of definitions for each SDT ranges from 4 ([Entity]), 10 ([Event]), 17 ([Vertebrate]) to 8,380 ([Amino Acid, Peptide, or Protein]) and 18,461 ([Organic Chemical]) in our data collection." ></td>
	<td class="line x" title="74:155	As the power of a statistical test relies on the sample size, some correlated semantic types might be undetected when the number of available definitions is small." ></td>
	<td class="line x" title="75:155	It is therefore worthwhile to know what the necessary sample size is in order to have a decent chance of detecting difference statistically." ></td>
	<td class="line x" title="76:155	3 For this task, we assume P0 and P1 are true probabilities that a STY will appear in NDef and NAll." ></td>
	<td class="line x" title="77:155	Based upon that, we calculated the minimal required number of sentences n such that the probability of statistical significance will be larger than or equal to 0.8." ></td>
	<td class="line x" title="78:155	This sample size is determined based on the following two assumptions: 1) the observed frequencies are approximately normally distributed, and 2) we use chi-square significance to test the hypothesis P0 = P1 at significance level 0.05 ( 2 10 PPP += )." ></td>
	<td class="line x" title="79:155	2 10 2 00112.0025.0 )( ))1()1()1(2( PP PPPPzPPzn  ++> (3) 5 Semantic Type Distribution Our null hypothesis is that given any pair of {SDT(X), SDT(Y)}, X  Y, where X and Y represent two different semantic types of the total 134 semantic types, there are no statistical differences in the distributions of the semantic types of the terms that appear in the definitions." ></td>
	<td class="line x" title="80:155	We applied the bivariate tabular chi-square test to measure the semantic type distribution." ></td>
	<td class="line x" title="81:155	Following similar notations to Section 4, we use OXi and OYi for the corresponding frequencies of not being observed in SDef(X) and SDef(Y)." ></td>
	<td class="line x" title="82:155	For each semantic type STY, we calculate the expected frequencies of being observed and not being observed in SDef(X) and SDef(Y), respectively, and their corresponding chi-square value according to formulas (3) and (4): iXE = iYiX NN OON + + )*( iYiXiX, iXE = iYiX iX NN OON + + )(* iYiX, iYE = iYiX NN OON + + )*( iYiXiY, iYE = iYiX iY NN OON + + )(* iYiX (4) ( ) ( )  += iY iY iX iX iYX E OE E OE 2iY2iX2,, (5) where NX and NY are the numbers of sentences in SDef(X) and SDef(Y), respectively, and in both (4) and (5), 134,,2,1=i, and (X, Y)=1,2,, 134 and X  Y. The degree of freedom is 1." ></td>
	<td class="line x" title="83:155	The chi-square value measures whether the occurrences of STYi, are equivalent between SDef(X) and SDef(Y)." ></td>
	<td class="line x" title="84:155	The same multiple testing correction procedure will be used to determine the significance of the chisquare value." ></td>
	<td class="line x" title="85:155	Note that if at least one STYi has been detected to be statistically significant after multiple-testing correction, the distributions of the semantic types are different between SDef(X) and SDef(Y)." ></td>
	<td class="line x" title="86:155	6 Automatically Identifying Semantic-TypeDependent Lexicosyntactic Patterns Most current definitional question answering systems generate lexicosyntactic patterns either manually or semi-automatically." ></td>
	<td class="line x" title="87:155	In this study, we automatically generated large sets of lexicosyntactic patterns from our collection of online definitions." ></td>
	<td class="line x" title="88:155	We applied the information extraction system Autoslog-TS (Riloff and Philips 2004) to automatically generate lexicosyntactic patterns in definitions." ></td>
	<td class="line x" title="89:155	We then identified the statistical correlation between the semantic types of defined terms and their lexicosyntactic patterns in definitions." ></td>
	<td class="line x" title="90:155	AutoSlog-TS is an information extraction system that is built upon AutoSlog (Riloff 1996)." ></td>
	<td class="line x" title="91:155	AutoSlog-TS automatically identifies extraction patterns for noun phrases by learning from two sets of un-annotated texts relevant and non-relevant." ></td>
	<td class="line x" title="92:155	AutoSlog-TS first generates every possible lexicosyntactic pattern to extract every noun phrase in both collections of text and then computes statistics based on how often each pattern appears in the relevant text versus the background and outputs a ranked list of extraction patterns coupled with statistics indicating how strongly each pattern is associated with relevant and non-relevant texts." ></td>
	<td class="line x" title="93:155	We grouped definitions based on the semantic types of the defined terms." ></td>
	<td class="line x" title="94:155	For each semantic type, the relevant text incorporated the definitions, and the non-relevant text incorporated an equal number of sentences that were randomly selected from the MEDLINE collection." ></td>
	<td class="line x" title="95:155	For each semantic type, we applied AutoSlog-TS to its associated relevant and non-relevant sentence collections to generate lexicosyntactic patterns; this resulted in a total of 134 sets of lexicosyntactic patterns that corresponded to different semantic types of defined terms." ></td>
	<td class="line x" title="96:155	Additionally, we identified the common lexicosyntactic patterns across the semantic types and ranked the lexicosyntactic patterns based on their frequencies across semantic types." ></td>
	<td class="line x" title="97:155	4 We also identified statistical correlations between SDT and the lexicosyntactic patterns in definitions based on chi-square statistics that we have described in the previous two sections." ></td>
	<td class="line x" title="98:155	For formula 1~4, we replaced each STY with a lexicosyntactic pattern." ></td>
	<td class="line x" title="99:155	Our null hypothesis is that given any SDT, there are no statistical differences in the distributions of the lexicosyntactic patterns that appear in the definitions." ></td>
	<td class="line x" title="100:155	Figure 1: A list of semantic types of defined terms with the top five statistically correlated semantic types (P<<0.0001) that appear in their definitions." ></td>
	<td class="line x" title="101:155	7 Results Our chi-square statistics show that for any pair of semantic types {SDT(X), SDT(Y)}, X  Y, the distributions of SDef are statistically different at alpha=0.05; the results show that the semantic types of the defined terms correlate to the semantic types in the definitions." ></td>
	<td class="line x" title="102:155	Our results also show that the syntactic patterns are distributed differently among different semantic types of the defined terms (alpha=0.05)." ></td>
	<td class="line x" title="103:155	Our results show that many semantic types that appear in definitions are statistically correlated with the semantic types of the defined terms." ></td>
	<td class="line x" title="104:155	The average number and standard deviation of statistically correlated semantic types is 80.635.4 at P<<0.0001." ></td>
	<td class="line x" title="105:155	Figure 1 shows three SDT ([Body Part, Organ, or Organ Component], [Disease or Syndrome], and [Organization]) with the corresponding top five statistically correlated semantic types that appear in their definitions." ></td>
	<td class="line x" title="106:155	Our results show that in a total of 112 (or 83.6%) cases, SDT appears as one of the top five statistically correlated semantic types in SDef, and that in a total of 94 (or 70.1%) cases, SDT appears at the top in SDef." ></td>
	<td class="line x" title="107:155	Our results indicate that if a definitional term has a semantic type SDT, then the terms in its definition tend to have the same or related semantic types." ></td>
	<td class="line x" title="108:155	We examined the cases in which the semantic types of definitional terms do not appear in the top five semantic types in the definitions." ></td>
	<td class="line x" title="109:155	We found that in all of those cases, the total numbers of definitions that were used for statistical analysis were too small to obtain statistical significance." ></td>
	<td class="line x" title="110:155	For example, when SDT is Entity, the minimum size for a SDef was 4.75, which is larger than the total number of the definitions (i.e. , 4)." ></td>
	<td class="line x" title="111:155	As a result, some actually correlated semantic types might be undetected due to insufficient sample size." ></td>
	<td class="line x" title="112:155	Our results also show that the lexicosyntactic patterns of definitional sentences are SDT-dependent." ></td>
	<td class="line x" title="113:155	Our results show that many lexicosyntactic patterns that appear in definitions are statistically correlated with the semantic types of defined terms." ></td>
	<td class="line x" title="114:155	The average number and standard deviation of statistically correlated lexico-syntactic patterns is 1656.71818.9 at P<<0.0001." ></td>
	<td class="line x" title="115:155	We found that the more definitions an SDT has, the more lexicosyntactic patterns." ></td>
	<td class="line x" title="116:155	Figure 2 shows the top 10 lexicosyntactic patterns (based on chi-square statistics) that were captured by Autoslog-TS with three different SDT; namely, [Disease or Syndrome], [Body Part, Organ, or Organ Component], and [Organization]." ></td>
	<td class="line x" title="117:155	Figure 3 shows the top 10 lexicosyntactic patterns ranked by AutoSlog-TS which incorporated the frequencies of the patterns (Riloff and Philips 2004)." ></td>
	<td class="line x" title="118:155	Figure 4 lists the top 30 common patterns across all different semantic types SDT." ></td>
	<td class="line x" title="119:155	We found that many common lexicosyntactic patterns (e.g. , known as, called, include) have been identified by other research groups through either manual or semi-automatic pattern discovery (Blair-Goldensohn et al. 2004)." ></td>
	<td class="line x" title="120:155	5 Figure 2: The top 10 lexicosyntactic patterns that appear in definitions based on chi-square statistics." ></td>
	<td class="line x" title="121:155	The defined terms have one of the three semantic types [Disease_or_Syndrome], [Body Part, Organ, or Organ Component], and [Organization]." ></td>
	<td class="line x" title="122:155	Figure 3: The top 10 lexicosyntactic patterns ranked by Autoslog-TS." ></td>
	<td class="line x" title="123:155	The defined terms have one of the three semantic types [Disease_or_Syndrome], [Body Part, Organ, or Organ Component], and [Organization]." ></td>
	<td class="line x" title="124:155	Figure 4: The top 30 common lexicosyntactic patterns generated across patterns with different DTS." ></td>
	<td class="line x" title="125:155	8 Discussion The statistical correlations between SDT and SDef may be useful to enhance the performance of a definition-question-answering system by at least two means." ></td>
	<td class="line x" title="126:155	First, the semantic types may be useful for word sense disambiguation." ></td>
	<td class="line x" title="127:155	A simple application is to rank definitional sentences based on the distributions of the semantic types of terms in the definitions to capture the definition of a specific sense." ></td>
	<td class="line x" title="128:155	For example, a biomedical definitional question answering system may exclude the definition of other senses (e.g. , feeling as shown in the sentence The locus of feelings and intuitions; in your heart you know it is true; her story would melt your heart.) if the semantic types that define heart do not include [Body Part, Organ, or Organ Component] of terms other than heart." ></td>
	<td class="line x" title="129:155	Secondly, the semantic-type correlations may be used as features to exclude non-definitional sentences." ></td>
	<td class="line x" title="130:155	For example, a biomedical definitional question answering system may exclude the following non-definitional sentence Heart rate was 6 unaffected by the drug because the semantic types in the sentence do not include [Body Part, Organ, or Organ Component] of terms other than heart." ></td>
	<td class="line x" title="131:155	SDT-dependent lexicosyntactic patterns may enhance both the recall and precision of a definitional question answering system." ></td>
	<td class="line x" title="132:155	First, the large sets of lexicosyntactic patterns we generated automatically may expand the smaller sets of lexicosyntactic patterns that have been reported by the existing question answering systems." ></td>
	<td class="line x" title="133:155	Secondly, SDTdependent lexicosyntactic patterns may be used to capture definitions." ></td>
	<td class="line x" title="134:155	The common lexicosyntactic patterns we identified (in Figure 4) may be useful for a generic definitional question answering system." ></td>
	<td class="line x" title="135:155	For example, a definitional question answering system may implement the most common patterns to detect any generic definitions; specific patterns may be implemented to detect definitions with specific SDT." ></td>
	<td class="line x" title="136:155	One limitation of our work is that the lexicosyntactic patterns generated by Autoslog-TS are within clauses." ></td>
	<td class="line x" title="137:155	This is a disadvantage because 1) lexicosyntactic patterns can extend beyond clauses (Cui et al. 2005) and 2) frequently a definition has multiple lexicosyntactic patterns." ></td>
	<td class="line x" title="138:155	Many of the patterns might not be generalizible." ></td>
	<td class="line x" title="139:155	For example, as shown in Figure 2, some of the top ranked patterns (e.g. , Subj_AuxVp_<dobj>_BE_ARMY>) identified by AutoSlog-TS may be too specific to the text collection." ></td>
	<td class="line x" title="140:155	The pattern-ranking method introduced by AutoSlog-TS takes into consideration the frequency of a pattern and therefore is a better ranking method than the chi-square ranking (shown in Figure 3)." ></td>
	<td class="line x" title="141:155	9 Related Work Systems have used named entities (e.g. , PEOPLE and LOCATION) to assist in information extraction (Agichtein and Gravano 2000) and question answering (Moldovan et al. 2002; Filatova and Prager 2005)." ></td>
	<td class="line x" title="142:155	Semantic constraints were first explored by (Bodenreider and Burgun 2002; Rindflesch and Fiszman 2003) who observed that the principle nouns in definientia are frequently semantically related (e.g. , hyponyms, hypernyms, siblings, and synonyms) to definiena." ></td>
	<td class="line x" title="143:155	Semantic constraints have been introduced to definitional question answering (Prager et al. 2000; Liang et al. 2001)." ></td>
	<td class="line x" title="144:155	For example, an artists work must be completed between his birth and death (Prager et al. 2000); and the hyponyms of defined terms might be incorporated in the definitions (Liang et al. 2001)." ></td>
	<td class="line x" title="145:155	Semantic correlations have been explored in other areas of NLP." ></td>
	<td class="line oc" title="146:155	For example, researchers (Turney 2002; Yu and Hatzivassiloglou 2003) have identified semantic correlation between words and views: positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have a positive semantic orientation and vice versa for negative reviews or sentences with a negative semantic orientation." ></td>
	<td class="line x" title="147:155	10 Conclusions and Future Work This is the first study in definitional question answering that concludes that the semantics of a definiendum constrain both the lexical semantics and the lexicosyntactic patterns in the definition." ></td>
	<td class="line x" title="148:155	Our discoveries may be useful for the building of a biomedical definitional question answering system." ></td>
	<td class="line x" title="149:155	Although our discoveries (i.e. , that the semantic types of the definitional terms determine both the lexicosyntactic patterns and the semantic types in the definitions) were evaluated with the knowledge framework from the biomedical, domain-specific knowledge resource the UMLS, the principles may be generalizable to any type of semantic classification of definitions." ></td>
	<td class="line x" title="150:155	The semantic constraints may enhance both recall and precision of one-size-fitsall question answering systems, which may be evaluated in future work." ></td>
	<td class="line x" title="151:155	As stated in the Discussion session, one disadvantage of this study is that the lexicosyntactic patterns generated by Autoslog-TS are within clauses." ></td>
	<td class="line x" title="152:155	Future work needs to develop pattern-recognition systems that are capable of detecting patterns across clauses." ></td>
	<td class="line x" title="153:155	In addition, future work needs to move beyond lexicosyntactic patterns to extract semanticlexicosyntactic patterns and to evaluate how the semantic-lexicosyntactic patterns can enhance definitional question answering." ></td>
	<td class="line x" title="154:155	7 Acknowledgement: The author thanks Sasha Blair-Goldensohn, Vijay Shanker, and especially the three anonymous reviewers who provide valuable critics and comments." ></td>
	<td class="line x" title="155:155	The concepts Definiendum and Definiens come from one of the reviewers recommendation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3808
Seeing Stars When There Arent Many Stars: Graph-Based Semi-Supervised Learning For Sentiment Categorization
Goldberg, Andrew;Zhu, Xiaojin;"></td>
	<td class="line x" title="1:200	Workshop on TextGraphs, at HLT-NAACL 2006, pages 4552, New York City, June 2006." ></td>
	<td class="line x" title="2:200	c2006 Association for Computational Linguistics Seeing stars when there arent many stars: Graph-based semi-supervised learning for sentiment categorization Andrew B. Goldberg Computer Sciences Department University of Wisconsin-Madison Madison, W.I. 53706 goldberg@cs.wisc.edu Xiaojin Zhu Computer Sciences Department University of Wisconsin-Madison Madison, W.I. 53706 jerryzhu@cs.wisc.edu Abstract We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference." ></td>
	<td class="line x" title="3:200	Given a set of documents (e.g. , movie reviews) and accompanying ratings (e.g. , 4 stars), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text." ></td>
	<td class="line x" title="4:200	In particular, we are interested in the situation where labeled data is scarce." ></td>
	<td class="line x" title="5:200	We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance." ></td>
	<td class="line x" title="6:200	We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task." ></td>
	<td class="line x" title="7:200	We then solve an optimization problem to obtain a smooth rating function over the whole graph." ></td>
	<td class="line x" title="8:200	When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training." ></td>
	<td class="line oc" title="9:200	1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al. , 2005; Turney, 2002; Dave et al. , 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005)." ></td>
	<td class="line x" title="10:200	Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews." ></td>
	<td class="line x" title="11:200	In particular Pang and Lee proposed the rating-inference problem (2005)." ></td>
	<td class="line x" title="12:200	Rating inference is harder than binary positive / negative opinion classification." ></td>
	<td class="line x" title="13:200	The goal is to infer a numerical rating from reviews, for example the number of stars that a critic gave to a movie." ></td>
	<td class="line x" title="14:200	Pang and Lee showed that supervised machine learning techniques (classification and regression) work well for rating inference with large amounts of training data." ></td>
	<td class="line x" title="15:200	However, review documents often do not come with numerical ratings." ></td>
	<td class="line x" title="16:200	We call such documents unlabeled data." ></td>
	<td class="line x" title="17:200	Standard supervised machine learning algorithms cannot learn from unlabeled data." ></td>
	<td class="line x" title="18:200	Assigning labels can be a slow and expensive process because manual inspection and domain expertise are needed." ></td>
	<td class="line x" title="19:200	Often only a small portion of the documents can be labeled within resource constraints, so most documents remain unlabeled." ></td>
	<td class="line x" title="20:200	Supervised learning algorithms trained on small labeled sets suffer in performance." ></td>
	<td class="line x" title="21:200	Can one use the unlabeled reviews to improve rating-inference?" ></td>
	<td class="line x" title="22:200	Pang and Lee (2005) suggested that doing so should be useful." ></td>
	<td class="line x" title="23:200	We demonstrate that the answer is Yes. Our approach is graph-based semi-supervised learning." ></td>
	<td class="line x" title="24:200	Semi-supervised learning is an active research area in machine learning." ></td>
	<td class="line x" title="25:200	It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001)." ></td>
	<td class="line x" title="26:200	This paper contains three contributions:  We present a novel adaptation of graph-based semi-supervised learning (Zhu et al. , 2003) 45 to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005);  We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3;  We show the benefit of semi-supervised learning for rating inference with extensive experimental results in section 4." ></td>
	<td class="line x" title="27:200	2 A Graph for Sentiment Categorization The semi-supervised rating-inference problem is formalized as follows." ></td>
	<td class="line x" title="28:200	There are n review documents x1xn, each represented by some standard feature representation (e.g. , word-presence vectors)." ></td>
	<td class="line x" title="29:200	Without loss of generality, let the first l  n documents be labeled with ratings y1 yl  C. The remaining documents are unlabeled." ></td>
	<td class="line x" title="30:200	In our experiments, the unlabeled documents are also the test documents, a setting known as transduction." ></td>
	<td class="line x" title="31:200	The set of numerical ratings are C = {c1,,cC}, with c1 <  < cC  R. For example, a one-star to four-star movie rating system has C = {0,1,2,3}." ></td>
	<td class="line x" title="32:200	We seek a function f : x mapsto R that gives a continuous rating f(x) to a document x. Classification is done by mapping f(x) to the nearest discrete rating in C. Note this is ordinal classification, which differs from standard multi-class classification in that C is endowed with an order." ></td>
	<td class="line x" title="33:200	In the following we use review and document, rating and label interchangeably." ></td>
	<td class="line x" title="34:200	We make two assumptions: 1." ></td>
	<td class="line x" title="35:200	We are given a similarity measure wij  0 between documents xi and xj." ></td>
	<td class="line x" title="36:200	wij should be computable from features, so that we can measure similarities between any documents, including unlabeled ones." ></td>
	<td class="line x" title="37:200	A large wij implies that the two documents tend to express the same sentiment (i.e. , rating)." ></td>
	<td class="line x" title="38:200	We experiment with positive-sentence percentage (PSP) based similarity which is proposed in (Pang and Lee, 2005), and mutual-information modulated word-vector cosine similarity." ></td>
	<td class="line x" title="39:200	Details can be found in section 4." ></td>
	<td class="line x" title="40:200	2. Optionally, we are given numerical rating predictions yl+1,, yn on the unlabeled documents from a separate learner, for instance epsilon1-insensitive support vector regression (Joachims, 1999; Smola and Scholkopf, 2004) used by (Pang and Lee, 2005)." ></td>
	<td class="line x" title="41:200	This acts as an extra knowledge source for our semisupervised learning framework to improve upon." ></td>
	<td class="line x" title="42:200	We note our framework is general and works without the separate learner, too." ></td>
	<td class="line x" title="43:200	(For this to work in practice, a reliable similarity measure is required.)" ></td>
	<td class="line x" title="44:200	We now describe our graph for the semisupervised rating-inference problem." ></td>
	<td class="line x" title="45:200	We do this piece by piece with reference to Figure 1." ></td>
	<td class="line x" title="46:200	Our undirected graph G = (V,E) has 2n nodes V, and weighted edges E among some of the nodes." ></td>
	<td class="line x" title="47:200	 Each document is a node in the graph (open circles, e.g., xi and xj)." ></td>
	<td class="line x" title="48:200	The true ratings of these nodes f(x) are unobserved." ></td>
	<td class="line x" title="49:200	This is true even for the labeled documents because we allow for noisy labels." ></td>
	<td class="line x" title="50:200	Our goal is to infer f(x) for the unlabeled documents." ></td>
	<td class="line x" title="51:200	 Each labeled document (e.g. , xj) is connected to an observed node (dark circle) whose value is the given rating yj." ></td>
	<td class="line x" title="52:200	The observed node is a dongle (Zhu et al. , 2003) since it connects only to xj." ></td>
	<td class="line x" title="53:200	As we point out later, this serves to pull f(xj) towards yj." ></td>
	<td class="line x" title="54:200	The edge weight between a labeled document and its dongle is a large number M. M represents the influence of yj: if M   then f(xj) = yj becomes a hard constraint." ></td>
	<td class="line x" title="55:200	 Similarly each unlabeled document (e.g. , xi) is also connected to an observed dongle node yi, whose value is the prediction of the separate learner." ></td>
	<td class="line x" title="56:200	Therefore we also require that f(xi) is close to yi." ></td>
	<td class="line x" title="57:200	This is a way to incorporate multiple learners in general." ></td>
	<td class="line x" title="58:200	We set the weight between an unlabeled node and its dongle arbitrarily to 1 (the weights are scale-invariant otherwise)." ></td>
	<td class="line x" title="59:200	As noted earlier, the separate learner is optional: we can remove it and still carry out graph-based semi-supervised learning." ></td>
	<td class="line x" title="60:200	46 yi^ xi xj yj labeledreviews unlabeledreviews 1 a wij b wij neighborsk Mneighborsk Figure 1: The graph for semi-supervised rating inference." ></td>
	<td class="line x" title="61:200	 Each unlabeled document xi is connected to kNNL(i), its k nearest labeled documents." ></td>
	<td class="line x" title="62:200	Distance is measured by the given similarity measure w. We want f(xi) to be consistent with its similar labeled documents." ></td>
	<td class="line x" title="63:200	The weight between xi and xj  kNNL(i) is awij." ></td>
	<td class="line x" title="64:200	 Each unlabeled document is also connected to kprimeNNU(i), its kprime nearest unlabeled documents (excluding itself)." ></td>
	<td class="line x" title="65:200	The weight between xi and xj  kprimeNNU(i) is b  wij." ></td>
	<td class="line x" title="66:200	We also want f(xi) to be consistent with its similar unlabeled neighbors." ></td>
	<td class="line x" title="67:200	We allow potentially different numbers of neighbors (k and kprime), and different weight coefficients (a and b)." ></td>
	<td class="line x" title="68:200	These parameters are set by cross validation in experiments." ></td>
	<td class="line x" title="69:200	The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section." ></td>
	<td class="line x" title="70:200	3 Graph-Based Semi-Supervised Learning With the graph defined, there are several algorithms one can use to carry out semi-supervised learning (Zhu et al. , 2003; Delalleau et al. , 2005; Joachims, 2003; Blum and Chawla, 2001; Belkin et al. , 2005)." ></td>
	<td class="line x" title="71:200	The basic idea is the same and is what we use in this paper." ></td>
	<td class="line x" title="72:200	That is, our rating function f(x) should be smooth with respect to the graph." ></td>
	<td class="line x" title="73:200	f(x) is not smooth if there is an edge with large weight w between nodes xi and xj, and the difference between f(xi) and f(xj) is large." ></td>
	<td class="line x" title="74:200	The (un)smoothness over the particular edge can be defined as wparenleftbigf(xi)  f(xj)parenrightbig2." ></td>
	<td class="line x" title="75:200	Summing over all edges in the graph, we obtain the (un)smoothness L(f) over the whole graph." ></td>
	<td class="line x" title="76:200	We call L(f) the energy or loss, which should be minimized." ></td>
	<td class="line x" title="77:200	Let L = 1l and U = l + 1n be labeled and unlabeled review indices, respectively." ></td>
	<td class="line x" title="78:200	With the graph in Figure 1, the loss L(f) can be written as summationdisplay iL M(f(xi)yi)2 + summationdisplay iU (f(xi) yi)2 + summationdisplay iU summationdisplay jkNNL(i) awij(f(xi)f(xj))2 + summationdisplay iU summationdisplay jkprimeNNU(i) bwij(f(xi)f(xj))2." ></td>
	<td class="line x" title="79:200	(1) A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers." ></td>
	<td class="line x" title="80:200	This is how unlabeled data can participate in learning." ></td>
	<td class="line x" title="81:200	The optimization problem is minf L(f)." ></td>
	<td class="line x" title="82:200	To understand the role of the parameters, we define  = ak + bkprime and  = ba, so that L(f) can be written as summationdisplay iL M(f(xi)yi)2 + summationdisplay iU bracketleftBig (f(xi) yi)2 + k + kprime parenleftBig summationdisplay jkNNL(i) wij(f(xi)f(xj))2 + summationdisplay jkprimeNNU(i) wij(f(xi)f(xj))2 parenrightBigbracketrightBig ." ></td>
	<td class="line x" title="83:200	(2) Thus  controls the relative weight between labeled neighbors and unlabeled neighbors;  is roughly the relative weight given to semi-supervised (nondongle) edges." ></td>
	<td class="line x" title="84:200	We can find the closed-form solution to the optimization problem." ></td>
	<td class="line x" title="85:200	Defining an nn matrix W, Wij =    0, i  L wij, j  kNNL(i) wij, j  kprimeNNU(i)." ></td>
	<td class="line x" title="86:200	(3) Let W = max( W, Wlatticetop) be a symmetrized version of this matrix." ></td>
	<td class="line x" title="87:200	Let D be a diagonal degree matrix with Dii = nsummationdisplay j=1 Wij." ></td>
	<td class="line x" title="88:200	(4) Note that we define a nodes degree to be the sum of its edge weights." ></td>
	<td class="line x" title="89:200	Let  = D W be the combinatorial Laplacian matrix." ></td>
	<td class="line x" title="90:200	Let C be a diagonal dongle 47 weight matrix with Cii = braceleftbigg M, i  L 1, i  U ." ></td>
	<td class="line x" title="91:200	(5) Let f = (f(x1),,f(xn))latticetop and y = (y1,,yl, yl+1,, yn)latticetop." ></td>
	<td class="line x" title="92:200	We can rewrite L(f) as (f y)latticetopC(f y) + k + kprimeflatticetopf." ></td>
	<td class="line x" title="93:200	(6) This is a quadratic function in f. Setting the gradient to zero, L(f)/f = 0, we find the minimum loss function f = parenleftbigg C + k + kprime parenrightbigg1 Cy." ></td>
	<td class="line x" title="94:200	(7) Because C has strictly positive eigenvalues, the inverse is well defined." ></td>
	<td class="line x" title="95:200	All our semi-supervised learning experiments use (7) in what follows." ></td>
	<td class="line x" title="96:200	Before moving on to experiments, we note an interesting connection to the supervised learning method in (Pang and Lee, 2005), which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002)." ></td>
	<td class="line x" title="97:200	Consider a special case of our loss function (1) when b = 0 and M  ." ></td>
	<td class="line x" title="98:200	It is easy to show for labeled nodes j  L, the optimal value is the given label: f(xj) = yj." ></td>
	<td class="line x" title="99:200	Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node i  U: Lb=0,M(f(xi)) = (f(xi) yi)2 + summationdisplay jkNNL(i) awij(f(xi)yj)2." ></td>
	<td class="line x" title="100:200	(8) The above problem is easy to solve." ></td>
	<td class="line x" title="101:200	It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while (Pang and Lee, 2005) used absolute difference." ></td>
	<td class="line x" title="102:200	Indeed in experiments comparing the two (not reported here), their differences are not statistically significant." ></td>
	<td class="line x" title="103:200	From this perspective, our semisupervised learning method is an extension with interacting terms among unlabeled data." ></td>
	<td class="line x" title="104:200	4 Experiments We performed experiments using the movie review documents and accompanying 4-class (C = {0,1,2,3}) labels found in the scale dataset v1.0 available at http://www.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in (Pang and Lee, 2005)." ></td>
	<td class="line x" title="105:200	We chose 4-class instead of 3-class labeling because it is harder." ></td>
	<td class="line x" title="106:200	The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents." ></td>
	<td class="line x" title="107:200	We ran experiments individually for each author." ></td>
	<td class="line x" title="108:200	Each document is represented as a {0,1} word-presence vector, normalized to sum to 1." ></td>
	<td class="line x" title="109:200	We systematically vary labeled set size |L|  {0.9n,800,400,200,100,50,25,12,6} to observe the effect of semi-supervised learning." ></td>
	<td class="line x" title="110:200	|L| = 0.9n is included to match 10-fold cross validation used by (Pang and Lee, 2005)." ></td>
	<td class="line x" title="111:200	For each |L| we run 20 trials where we randomly split the corpus into labeled and test (unlabeled) sets." ></td>
	<td class="line x" title="112:200	We ensure that all four classes are represented in each labeled set." ></td>
	<td class="line x" title="113:200	The same random splits are used for all methods, allowing paired t-tests for statistical significance." ></td>
	<td class="line x" title="114:200	All reported results are average test set accuracy." ></td>
	<td class="line x" title="115:200	We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in (Pang and Lee, 2005)." ></td>
	<td class="line x" title="116:200	4.1 Regression We ran linear epsilon1-insensitive support vector regression using Joachims SVMlight package (1999) with all default parameters." ></td>
	<td class="line x" title="117:200	The continuous prediction on a test document is discretized for classification." ></td>
	<td class="line x" title="118:200	Regression results are reported under the heading reg. Note this method does not use unlabeled data for training." ></td>
	<td class="line x" title="119:200	4.2 Metric labeling We ran Pang and Lees method based on metric labeling, using SVM regression as the initial label preference function." ></td>
	<td class="line x" title="120:200	The method requires an itemsimilarity function, which is equivalent to our similarity measure wij." ></td>
	<td class="line x" title="121:200	Among others, we experimented with PSP-based similarity." ></td>
	<td class="line x" title="122:200	For consistency with (Pang and Lee, 2005), supervised metric labeling results with this measure are reported under reg+PSP. Note this method does not use unlabeled data for training either." ></td>
	<td class="line x" title="123:200	PSPi is defined in (Pang and Lee, 2005) as the percentage of positive sentences in review xi." ></td>
	<td class="line x" title="124:200	The similarity between reviews xi,xj is the cosine angle 48 0 0.2 0.4 0.6 0.8 10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 finegrain rating mean and standard deviation of PSP Positivesentence percentage (PSP) statistics Author (a) Author (b)Author (c) Author (d) Figure 2: PSP for reviews expressing each fine-grain rating." ></td>
	<td class="line x" title="125:200	We identified positive sentences using SVM instead of Nave Bayes, but the trend is qualitatively the same as in (Pang and Lee, 2005)." ></td>
	<td class="line x" title="126:200	between the vectors (PSPi,1PSPi) and (PSPj,1 PSPj)." ></td>
	<td class="line x" title="127:200	Positive sentences are identified using a binary classifier trained on a separate snippet data set located at the same URL as above." ></td>
	<td class="line x" title="128:200	The snippet data set contains 10662 short quotations taken from movie reviews appearing on the rottentomatoes.com Web site." ></td>
	<td class="line x" title="129:200	Each snippet is labeled positive or negative based on the rating of the originating review." ></td>
	<td class="line x" title="130:200	Pang and Lee (2005) trained a Nave Bayes classifier." ></td>
	<td class="line x" title="131:200	They showed that PSP is a (noisy) measure for comparing reviewsreviews with low ratings tend to receive low PSP scores, and those with higher ratings tend to get high PSP scores." ></td>
	<td class="line x" title="132:200	Thus, two reviews with a high PSP-based similarity are expected to have similar ratings." ></td>
	<td class="line x" title="133:200	For our experiments we derived PSP measurements in a similar manner, but using a linear SVM classifier." ></td>
	<td class="line x" title="134:200	We observed the same relationship between PSP and ratings (Figure 2)." ></td>
	<td class="line x" title="135:200	The metric labeling method has parameters (the equivalent of k, in our model)." ></td>
	<td class="line x" title="136:200	Pang and Lee tuned them on a per-author basis using cross validation but did not report the optimal parameters." ></td>
	<td class="line x" title="137:200	We were interested in learning a single set of parameters for use with all authors." ></td>
	<td class="line x" title="138:200	In addition, since we varied labeled set size, it is convenient to tune c = k/|L|, the fraction of labeled reviews used as neighbors, instead of k. We then used the same c, for all authors at all labeled set sizes in experiments involving PSP." ></td>
	<td class="line x" title="139:200	Because c is fixed, k varies directly with |L| (i.e. , when less labeled data is available, our algorithm considers fewer nearby labeled examples)." ></td>
	<td class="line x" title="140:200	In an attempt to reproduce the findings in (Pang and Lee, 2005), we tuned c, with cross validation." ></td>
	<td class="line x" title="141:200	Tuning ranges are c  {0.05,0.1,0.15,0.2,0.25,0.3} and   {0.01,0.1,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,5.0}." ></td>
	<td class="line x" title="142:200	The optimal parameters we found are c = 0.2 and  = 1.5." ></td>
	<td class="line x" title="143:200	(In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.)" ></td>
	<td class="line x" title="144:200	Note that we learned a single set of shared parameters for all authors, whereas (Pang and Lee, 2005) tuned k and  on a per-author basis." ></td>
	<td class="line x" title="145:200	To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific parameters." ></td>
	<td class="line x" title="146:200	Table 1 shows the accuracy obtained over 20 trials with |L| = 0.9n for each author, using SVM regression, reg+PSP using shared c, parameters, and reg+PSP using authorspecific c, parameters (listed in parentheses)." ></td>
	<td class="line x" title="147:200	The best result in each row of the table is highlighted in bold." ></td>
	<td class="line x" title="148:200	We also show in bold any results that cannot be distinguished from the best result using a paired t-test at the 0.05 level." ></td>
	<td class="line x" title="149:200	(Pang and Lee, 2005) found that their metric labeling method, when applied to the 4-class data we are using, was not statistically better than regression, though they observed some improvement for authors (c) and (d)." ></td>
	<td class="line x" title="150:200	Using author-specific parameters, we obtained the same qualitative result, but the improvement for (c) and (d) appears even less significant in our results." ></td>
	<td class="line x" title="151:200	Possible explanations for this difference are the fact that we derived our PSP measurements using an SVM classifier instead of an NB classifier, and that we did not use the same range of parameters for tuning." ></td>
	<td class="line x" title="152:200	The optimal shared parameters produced almost the same results as the optimal author-specific parameters, and were used in subsequent experiments." ></td>
	<td class="line x" title="153:200	4.3 Semi-Supervised Learning We used the same PSP-based similarity measure and the same shared parameters c = 0.2, = 1.5 from our metric labeling experiments to perform graph-based semi-supervised learning." ></td>
	<td class="line x" title="154:200	The results are reported as SSL+PSP. SSL has three 49 reg+PSP reg+PSP Author reg (shared) (specific) (a) 0.592 0.592 0.592 (0.05, 0.01) (b) 0.501 0.498 0.496 (0.05, 3.50) (c) 0.592 0.589 0.593 (0.15, 1.50) (d) 0.496 0.498 0.500 (0.05, 3.00) Table 1: Accuracy using shared (c = 0.2,  = 1.5) vs. author-specific parameters, with |L| = 0.9n." ></td>
	<td class="line x" title="155:200	additional parameters kprime, , and M. Again we tuned kprime, with cross validation." ></td>
	<td class="line x" title="156:200	Tuning ranges are kprime  {2,3,5,10,20} and   {0.001,0.01,0.1,1.0,10.0}." ></td>
	<td class="line x" title="157:200	The optimal parameters are kprime = 5 and  = 1.0." ></td>
	<td class="line x" title="158:200	These were used for all authors and for all labeled set sizes." ></td>
	<td class="line x" title="159:200	Note that unlike k = c|L|, which decreases as the labeled set size decreases, we let kprime remain fixed for all |L|." ></td>
	<td class="line x" title="160:200	We set M arbitrarily to a large number 108 to ensure that the ratings of labeled reviews are respected." ></td>
	<td class="line x" title="161:200	4.4 Alternate Similarity Measures In addition to using PSP as a similarity measure between reviews, we investigated several alternative similarity measures based on the cosine of word vectors." ></td>
	<td class="line x" title="162:200	Among these options were the cosine between the word vectors used to train the SVM regressor, and the cosine between word vectors containing only words with high (top 1000 or top 5000) mutual information values." ></td>
	<td class="line x" title="163:200	The mutual information is computed with respect to the positive and negative classes in the 10662-document snippet data set. Finally, we experimented with using as a similarity measure the cosine between word vectors containing all words, each weighted by its mutual information." ></td>
	<td class="line x" title="164:200	We found this measure to be the best among the options tested in pilot trial runs using the metric labeling algorithm." ></td>
	<td class="line x" title="165:200	Specifically, we scaled the mutual information values such that the maximum value was one." ></td>
	<td class="line x" title="166:200	Then, we used these values as weights for the corresponding words in the word vectors." ></td>
	<td class="line x" title="167:200	For words in the movie review data set that did not appear in the snippet data set, we used a default weight of zero (i.e. , we excluded them." ></td>
	<td class="line x" title="168:200	We experimented with setting the default weight to one, but found this led to inferior performance.)" ></td>
	<td class="line x" title="169:200	We repeated the experiments described in sections 4.2 and 4.3 with the only difference being that we used the mutual-information weighted word vector similarity instead of PSP whenever a similarity measure was required." ></td>
	<td class="line x" title="170:200	We repeated the tuning procedures described in the previous sections." ></td>
	<td class="line x" title="171:200	Using this new similarity measure led to the optimal parameters c = 0.1,  = 1.5, kprime = 5, and  = 10.0." ></td>
	<td class="line x" title="172:200	The results are reported under reg+WV and SSL+WV, respectively." ></td>
	<td class="line x" title="173:200	4.5 Results We tested the five algorithms for all four authors using each of the nine labeled set sizes." ></td>
	<td class="line x" title="174:200	The results are presented in table 2." ></td>
	<td class="line x" title="175:200	Each entry in the table represents the average accuracy across 20 trials for an author, a labeled set size, and an algorithm." ></td>
	<td class="line x" title="176:200	The best result in each row is highlighted in bold." ></td>
	<td class="line x" title="177:200	Any results on the same row that cannot be distinguished from the best result using a paired t-test at the 0.05 level are also bold." ></td>
	<td class="line x" title="178:200	The results indicate that the graph-based semisupervised learning algorithm based on PSP similarity (SSL+PSP) achieved better performance than all other methods in all four author corpora when only 200, 100, 50, 25, or 12 labeled documents were available." ></td>
	<td class="line x" title="179:200	In 19 out of these 20 learning scenarios, the unlabeled set accuracy by the SSL+PSP algorithm was significantly higher than all other methods." ></td>
	<td class="line x" title="180:200	While accuracy generally degraded as we trained on less labeled data, the decrease for the SSL approach was less severe through the mid-range labeled set sizes." ></td>
	<td class="line x" title="181:200	SSL+PSP remains among the best methods with only 6 labeled examples." ></td>
	<td class="line x" title="182:200	Note that the SSL algorithm appears to be quite sensitive to the similarity measure used to form the graph on which it is based." ></td>
	<td class="line x" title="183:200	In the experiments where we used mutual-information weighted word vector similarity (reg+WV and SSL+WV), we notice that reg+WV remained on par with reg+PSP at high labeled set sizes, whereas SSL+WV appears significantly worse in most of these cases." ></td>
	<td class="line x" title="184:200	It is clear that PSP is the more reliable similarity measure." ></td>
	<td class="line x" title="185:200	SSL uses the similarity measure in more ways than the metric labeling approaches (i.e. , SSLs graph is denser), so it is not surprising that SSLs accuracy would suffer more with an inferior similarity measure." ></td>
	<td class="line x" title="186:200	Unfortunately, our SSL approach did not do as well with large labeled set sizes." ></td>
	<td class="line x" title="187:200	We believe this 50 PSP word vector |L| regression reg+PSP SSL+PSP reg+WV SSL+WV Author (a) 1593 0.592 0.592 0.546 0.592 0.544 800 0.553 0.554 0.534 0.553 0.517 400 0.522 0.525 0.526 0.522 0.497 200 0.494 0.498 0.521 0.494 0.472 100 0.463 0.477 0.511 0.462 0.450 50 0.439 0.458 0.499 0.438 0.429 25 0.408 0.421 0.465 0.400 0.404 12 0.401 0.378 0.451 0.335 0.398 6 0.390 0.359 0.422 0.314 0.389 Author (b) 811 0.501 0.498 0.481 0.503 0.473 800 0.501 0.497 0.478 0.503 0.474 400 0.471 0.471 0.465 0.471 0.450 200 0.447 0.449 0.452 0.447 0.429 100 0.415 0.423 0.443 0.415 0.397 50 0.388 0.396 0.434 0.387 0.376 25 0.373 0.380 0.418 0.364 0.367 12 0.354 0.360 0.399 0.313 0.353 6 0.348 0.352 0.380 0.302 0.347 Author (c) 1176 0.592 0.589 0.566 0.594 0.514 800 0.579 0.585 0.559 0.579 0.509 400 0.550 0.556 0.544 0.551 0.491 200 0.513 0.519 0.532 0.513 0.479 100 0.484 0.495 0.521 0.484 0.466 50 0.462 0.476 0.504 0.461 0.456 25 0.459 0.472 0.484 0.439 0.454 12 0.420 0.405 0.477 0.356 0.414 6 0.320 0.382 0.366 0.334 0.322 Author (d) 924 0.496 0.498 0.495 0.499 0.490 800 0.500 0.501 0.495 0.504 0.483 400 0.474 0.478 0.486 0.477 0.463 200 0.459 0.459 0.468 0.459 0.445 100 0.444 0.445 0.460 0.444 0.437 50 0.429 0.431 0.445 0.429 0.428 25 0.411 0.411 0.425 0.400 0.409 12 0.393 0.362 0.405 0.335 0.391 6 0.393 0.357 0.403 0.312 0.393 Table 2: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and methods." ></td>
	<td class="line x" title="188:200	In each row, we list in bold the best result and any results that cannot be distinguished from it with a paired t-test at the 0.05 level." ></td>
	<td class="line x" title="189:200	51 is due to two factors: a) the baseline SVM regressor trained on a large labeled set can achieve fairly high accuracy for this difficult task without considering pairwise relationships between examples; b) PSP similarity is not accurate enough." ></td>
	<td class="line x" title="190:200	Gain in variance reduction achieved by the SSL graph is offset by its bias when labeled data is abundant." ></td>
	<td class="line x" title="191:200	5 Discussion We have demonstrated the benefit of using unlabeled data for rating inference." ></td>
	<td class="line x" title="192:200	There are several directions to improve the work: 1." ></td>
	<td class="line x" title="193:200	We will investigate better document representations and similarity measures based on parsing and other linguistic knowledge, as well as reviews sentiment patterns." ></td>
	<td class="line x" title="194:200	For example, several positive sentences followed by a few concluding negative sentences could indicate an overall negative review, as observed in prior work (Pang and Lee, 2005)." ></td>
	<td class="line x" title="195:200	2." ></td>
	<td class="line x" title="196:200	Our method is transductive: new reviews must be added to the graph before they can be classified." ></td>
	<td class="line x" title="197:200	We will extend it to the inductive learning setting based on (Sindhwani et al. , 2005)." ></td>
	<td class="line x" title="198:200	3." ></td>
	<td class="line x" title="199:200	We plan to experiment with cross-reviewer and cross-domain analysis, such as using a model learned on movie reviews to help classify product reviews." ></td>
	<td class="line x" title="200:200	Acknowledgment We thank Bo Pang, Lillian Lee and anonymous reviewers for helpful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1113
Crystal: Analyzing Predictive Opinions on the Web
Kim, Soo-Min;Hovy, Eduard H.;"></td>
	<td class="line x" title="1:290	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:290	10561064, Prague, June 2007." ></td>
	<td class="line x" title="3:290	c2007 Association for Computational Linguistics Crystal: Analyzing Predictive Opinions on the Web Soo-Min Kim and Eduard Hovy USC Information Sciences Institute 4676 Admiralty Way, Marina del Rey, CA 90292 {skim,hovy}@ISI.EDU Abstract In this paper, we present an election prediction system (Crystal) based on web users opinions posted on an election prediction website." ></td>
	<td class="line x" title="4:290	Given a prediction message, Crystal first identifies which party the message predicts to win and then aggregates prediction analysis results of a large amount of opinions to project the election results." ></td>
	<td class="line x" title="5:290	We collect past election prediction messages from the Web and automatically build a gold standard." ></td>
	<td class="line x" title="6:290	We focus on capturing lexical patterns that people frequently use when they express their predictive opinions about a coming election." ></td>
	<td class="line x" title="7:290	To predict election results, we apply SVM-based supervised learning." ></td>
	<td class="line x" title="8:290	To improve performance, we propose a novel technique which generalizes n-gram feature patterns." ></td>
	<td class="line x" title="9:290	Experimental results show that Crystal significantly outperforms several baselines as well as a non-generalized n-gram approach." ></td>
	<td class="line x" title="10:290	Crystal predicts future elections with 81.68% accuracy." ></td>
	<td class="line x" title="11:290	1 Introduction As a growing number of people use the Web as a medium for expressing their opinions, the Web is becoming a rich source of various opinions in the form of product reviews, travel advice, social issue discussions, consumer complaints, stock market predictions, real estate market predictions, etc. At least two categories of opinions can be identified." ></td>
	<td class="line x" title="12:290	One consists of opinions such as I like/dislike it, and the other consists of opinions like It is likely/unlikely to happen. We call the first category Judgment Opinions and the second (those discussing the future) Predictive Opinions." ></td>
	<td class="line x" title="13:290	Judgment opinions express positive or negative sentiment about a topic such as, for example, reviews about cameras, movies, books, or hotels, and discussions about topics like abortion and war." ></td>
	<td class="line x" title="14:290	In contrast, predictive opinions express a person's opinion about the future of a topic or event such as the housing market, a popular sports match, and national election, based on his or her belief and knowledge." ></td>
	<td class="line x" title="15:290	Due to the different nature of these two categories of opinion, each has different valences." ></td>
	<td class="line x" title="16:290	Judgment opinions have core valences of positive and negative." ></td>
	<td class="line x" title="17:290	For example, liking a product and supporting abortion have the valence positive toward each topic (namely a product and abortion)." ></td>
	<td class="line x" title="18:290	Predictive opinions have the core valence of likely or unlikely predicated on the event." ></td>
	<td class="line x" title="19:290	For example, a sentence Housing prices will go down soon carries the valence of likely for the event of housing prices go down." ></td>
	<td class="line x" title="20:290	The two types of opinions can co-appear." ></td>
	<td class="line x" title="21:290	The sentence I like Democrats but I think they are not likely to win considering the war issue contains both types of opinion: positive valence towards Democrats and unlikely valence towards the event of Democrats wins." ></td>
	<td class="line x" title="22:290	In order to accurately identify and analyze each type of opinion, different approaches are desirable." ></td>
	<td class="line x" title="23:290	Note that our work is different from predictive data mining which models a data mining system using statistical approaches in order to forecast the future or trace a pattern of interest (Rickel and Porter, 1997; Rodionov and Martin, 1996)." ></td>
	<td class="line x" title="24:290	Example domains of predictive data mining include earthquake prediction, air temperature prediction, foreign exchange prediction, and energy price predic1056 tion." ></td>
	<td class="line x" title="25:290	However, predictive data mining is only feasible when a large amount of structured numerical data (e.g. , in a database) is available." ></td>
	<td class="line x" title="26:290	Unlike this research area which analyzes numeric values, our study mines unstructured text using NLP techniques and it can potentially extend the reach of numeric techniques." ></td>
	<td class="line x" title="27:290	Despite the vast amount of predictive opinions and their potential applications such as identification and analysis of people's opinions about the real estate market or a specific country's economic future, studies on predictive opinions have been neglected in Computational Linguistics, where most previous work focuses on judgment opinions (see Section 2)." ></td>
	<td class="line x" title="28:290	In this paper, we concentrate on identifying predictive opinion with its valence." ></td>
	<td class="line x" title="29:290	Among many prediction domains on the Web, we focus on election prediction and introduce Crystal, a system to predict election results using the public's written viewpoints." ></td>
	<td class="line x" title="30:290	To build our system, we collect opinions about past elections posted on an election prediction project website before the election day, and build a corpus 1." ></td>
	<td class="line x" title="31:290	We then use this corpus to train our system for analyzing predictive opinion messages and, using this, to predict the election outcome." ></td>
	<td class="line x" title="32:290	Due to the availability of actual results of the past elections, we can not only evaluate how accurately Crystal analyzes prediction messages (by checking agreement with the gold standard), but also objectively measure the prediction accuracy of our system." ></td>
	<td class="line x" title="33:290	The main contributions of this work are as follows:  an NLP technique for analyzing predictive opinions in the electoral domain;  a method of automatically building a corpus of predictive opinions for a supervised learning approach; and  a feature generalization technique that outperforms all the baselines on the task of identifying a predicted winning party given a predictive opinion." ></td>
	<td class="line x" title="34:290	The rest of this paper is structured as follows." ></td>
	<td class="line x" title="35:290	Section 2 surveys previous work." ></td>
	<td class="line x" title="36:290	Section 3 formally defines our task and describes our data set." ></td>
	<td class="line x" title="37:290	Section 4 describes our system Crystal with proposed feature generalization algorithm." ></td>
	<td class="line x" title="38:290	Section 5 1 The resulting corpus is available at http://www.isi.edu/ ~skim/Download/Data/predictive.htm reports empirical evidence that Crystal outperforms several baseline systems." ></td>
	<td class="line x" title="39:290	Finally, Section 6 concludes with a description of the impact of this work." ></td>
	<td class="line x" title="40:290	2 Related Work This work is closely related to opinion analysis and text classification." ></td>
	<td class="line o" title="41:290	Most research on opinion analysis in computational linguistics has focused on sentiment analysis, subjectivity detection, and review mining." ></td>
	<td class="line oc" title="42:290	Pang et al.(2002) and Turney (2002) classified sentiment polarity of reviews at the document level." ></td>
	<td class="line x" title="44:290	Wiebe et al.(1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features." ></td>
	<td class="line x" title="46:290	Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process." ></td>
	<td class="line x" title="47:290	Wiebe et." ></td>
	<td class="line x" title="48:290	al (2004) and Riloff et." ></td>
	<td class="line x" title="49:290	al (2005) adopted pattern learning with lexical feature generalization for subjective expression detection." ></td>
	<td class="line x" title="50:290	Dave et." ></td>
	<td class="line x" title="51:290	al (2003) and Jindal and Liu (2006) also learned patterns of opinion expression in product reviews." ></td>
	<td class="line x" title="52:290	Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words." ></td>
	<td class="line x" title="53:290	These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004; Popescu et al. , 2005) and news articles (Kim and Hovy, 2004; Wilson et al. , 2005)." ></td>
	<td class="line x" title="54:290	In text classification, systems typically use bagof-words models, mostly with supervised learning algorithms using Naive Bayes or Support Vector Machines (Joachims, 1998) to classify documents into several categories such as sports, art, politics, and religion." ></td>
	<td class="line x" title="55:290	Liu et al.(2004) and Gliozzo et al.(2005) address the difficulty of obtaining training corpora for supervised learning and propose unsupervised learning approaches." ></td>
	<td class="line x" title="58:290	Another recent related classification task focuses on academic and commercial efforts to detect email spam messages." ></td>
	<td class="line x" title="59:290	For an SVM-based approach, see (Drucker et al. , 1999)." ></td>
	<td class="line x" title="60:290	In our study, we explore the use of generalized lexical features for predictive opinion analysis and compare it with the bag-of-words approach." ></td>
	<td class="line x" title="61:290	3 Modeling Prediction In this section, we define the task of analyzing predictive opinions in the electoral domain." ></td>
	<td class="line x" title="62:290	1057 3.1 Task Definition We model predictive opinions in an election as follows: Valence) (Party,inionedictionOpElectionPr = where Party is a political party running for an election (e.g. , Democrats and Republicans) and Valence is the valence of a predictive opinion which can be either likely to win (WIN) or unlikely to win (LOSE)." ></td>
	<td class="line x" title="63:290	Values for Party vary depending on in which year (e.g. , 1996 and 2006) and where an election takes place (e.g. , United States, France, or Japan)." ></td>
	<td class="line x" title="64:290	The unit of a predictive opinion is an unstructured textual document such as an article in a personal blog or a message posted on a news group discussion board about the topic of Which party do you think will win/lose in this election?." ></td>
	<td class="line x" title="65:290	Figure 1 illustrates an overview of our election prediction system Crystal in action." ></td>
	<td class="line x" title="66:290	Given each document posted on blogs or message boards (e.g. , www.election prediction.org) as seen in Figure 1.a, a system can determine a Party that the author of a document thinks to win or lose (Valence), Figure 1.b. For the example document starting with the sentence I think this riding will stay NDP as it has for the past 11 years. in Figure 1.a, our predictive opinion analysis system aims to recognize NDP as Party and WIN as Valence." ></td>
	<td class="line x" title="67:290	After aggregating the predictive opinion analysis results of all documents, we project the election results in Figure 1.c. The following section describes how we obtain our data set and the subsequent sections describe Crystal." ></td>
	<td class="line x" title="68:290	3.2 Automatically Labeled Data We collected messages posted on an election prediction project page, www.electionprediction." ></td>
	<td class="line x" title="69:290	org." ></td>
	<td class="line x" title="70:290	The website contains various election prediction projects (e.g. , provincial election, federal election, and general election) of different countries (e.g. , Canada and United Kingdom) from 1999 to 2006." ></td>
	<td class="line x" title="71:290	For our data set, we downloaded Canadian federal election prediction data for 2004 and 2006." ></td>
	<td class="line x" title="72:290	The Canadian federal electoral system is based on 308 Figure 1." ></td>
	<td class="line x" title="73:290	Our election prediction system." ></td>
	<td class="line x" title="74:290	Public opinions are collected from message boards (a) and our system determines for each the election prediction Party and Valence (b)." ></td>
	<td class="line x" title="75:290	The output of the system is a prediction of the election outcome (c)." ></td>
	<td class="line x" title="76:290	Message text Predicted winning party Riding Year     Message_1457 Party_3 Riding_206 2004 Message_1458 Party_2 Riding_206 2004 Message_1459 Party_2 Riding_189 2006 Message_1460 Party_1 Riding_189 2006 Message_1461 Party_2 Riding_189 2006 Message_1462 Party_1 Riding_46 2006     Table 1." ></td>
	<td class="line x" title="77:290	A snapshot of the processed data Riding name Party Candidate name NDP Noreen Johns Blackstrap Liberal J. Wayne Zimmer PC Lynne Yelich Table 2." ></td>
	<td class="line x" title="78:290	An example of our Party-Candidate listing for a riding (PC: Progressive Conservative) 1058 ridings (electoral districts)." ></td>
	<td class="line x" title="79:290	The website contains 308 separate html files of messages corresponding to the 308 ridings for different years." ></td>
	<td class="line x" title="80:290	In total, we collected 4858 and 4680 messages for the 2004 and 2006 federal elections respectively." ></td>
	<td class="line x" title="81:290	On average, a message consists of 98.8 words." ></td>
	<td class="line x" title="82:290	To train and evaluate our system, we require a gold standard for each message (i.e. , which party does an author of a message predict to win?)." ></td>
	<td class="line x" title="83:290	One option is to hire human annotators to build the gold standard." ></td>
	<td class="line x" title="84:290	Instead, we used an online party logo image file that the author of each message already labeled for the message." ></td>
	<td class="line x" title="85:290	Note that authors only select parties they think will win, which means our gold standard only contains a party with WIN valence of each message." ></td>
	<td class="line x" title="86:290	However, we leverage this information to build a system which is able to determine a party even with LOSE valence." ></td>
	<td class="line x" title="87:290	We describe this idea in detail in Section 4." ></td>
	<td class="line x" title="88:290	Finally, we pre-processed the data by converting the downloaded html source files into a structured format with the following fields: message, party, riding, and year, where message is a text, party is a winning party predicted in the text, riding is one of the 308 ridings, and year is either 2004 or 2006." ></td>
	<td class="line x" title="89:290	Table 1 shows a snapshot of the processed data set that we used for our system training and evaluation." ></td>
	<td class="line x" title="90:290	An additional piece of information consisting of a candidate's name for each party for each riding was also stored in our data set." ></td>
	<td class="line x" title="91:290	With this information, the system can infer opinions about a party based on opinions about candidates who run for the party." ></td>
	<td class="line x" title="92:290	Table 2 shows an example of a riding." ></td>
	<td class="line x" title="93:290	4 Analyzing Predictions In this section we describe Crystal." ></td>
	<td class="line x" title="94:290	One simple approach could be a system (see NGR system in Section 5) trained by a machine learning technique using n-gram features and classifying a message into multiple classes (e.g. , NDP, Liberal, or Progressive)." ></td>
	<td class="line x" title="95:290	However, we develop a more sophisticated algorithm and compare its result with several baselines, including the simple n-gram method 2 . Experimental results in Section 5 show that Crystal outperforms all the baselines." ></td>
	<td class="line x" title="96:290	Our approach consists of three steps: feature generalization, classification using SVMs, and 2 N-gram approach is often unbeatable (and therefore great) in many text classification tasks." ></td>
	<td class="line x" title="97:290	SVM result integration 3 . Crystal generates generalized sentences in the feature generalization step." ></td>
	<td class="line x" title="98:290	Then it classifies each sentence using generalized lexical features in order to determine Valence of Party in a sentence." ></td>
	<td class="line x" title="99:290	Finally, it combines results of sentences to determine Valence and Party of a message." ></td>
	<td class="line x" title="100:290	Note that the classification using SVM is an intermediate step conducting a binary classification (i.e. , WIN or LOSE) for the final multi-class classification in result integration." ></td>
	<td class="line x" title="101:290	The following sections describe each step." ></td>
	<td class="line x" title="102:290	4.1 Feature Generalization In the feature generalization step, we generalize patterns of words used in predictive opinions." ></td>
	<td class="line x" title="103:290	For example, instead of using three different trigrams like Liberals will win, NDP will win, and Conservatives will win, we generalize these to PARTY will win." ></td>
	<td class="line x" title="104:290	The assumption is that the generalized patterns can represent better the relationship among Party, Valence, and words surrounding Party (e.g. , will win) than pure lexical patterns." ></td>
	<td class="line x" title="105:290	For this algorithm, we first substitute a candidate's name (both the first name and the last name) with the political party name that the candidate belongs to (see Table 2)." ></td>
	<td class="line x" title="106:290	We then break each message into sentences 4 . Table 3 outlines the feature generalization algorithm." ></td>
	<td class="line x" title="107:290	Here, our approach is that if a message pre3 feature indicates n-grams in our corpus that we use in the SVM classification step." ></td>
	<td class="line x" title="108:290	4 The sentence breaker that we used is available at http://search.cpan.org/ ~shlomoy/Lingua-EN-sentence 0.25/lib/Lingua/EN/Sentence.pm." ></td>
	<td class="line x" title="109:290	1 for each message M with a party that M predicts to win, P w 2 for each sentence S i in a message M 3 for each party P j in S i 4 valence V j = +1 if P j = P w 5 valence V j = -1 Otherwise 6 Generate S' ij by substituting P j with PARTY 7 and all other parties in S i with OTHER 8 Return (P j, V j, S' ij ) Table 3." ></td>
	<td class="line x" title="110:290	Feature generalization algorithm 1059 dicts a particular party to win, sentences which mention that party in the message also imply that it will win." ></td>
	<td class="line x" title="111:290	Conversely all other parties are assumed to be in sentences that imply they will lose." ></td>
	<td class="line x" title="112:290	As shown in Section 3.2, a message (M) in our corpus has a label of a party (P w ) that the author of M predicts to win." ></td>
	<td class="line x" title="113:290	After breaking sentences in M, we duplicate a sentence by the number of unique parties in the sentence and modify the duplicated sentences by substituting the party names with PARTY and OTHER in order to generalize features." ></td>
	<td class="line x" title="114:290	Consider the following sentence: Dockrill will barely take this riding from Rodger Cuzner which gets re-written as: NDP will barely take this riding from Liberal because Dockrill is an NDP candidate and Rodger Cuzner is a Liberal candidate." ></td>
	<td class="line x" title="115:290	Since the sentence contains two parties (i.e. , NDP and Liberal), the algorithm duplicates the sentence twice, once for each party (see Lines 48 in Table 3) 5 . For NDP, the algorithm determines its Valence as -1 because NDP is not equal to the predicted winning party (i.e. , Liberal) of the message (see Lines 45 in Ta5 In the feature generalization algorithm, we represent WIN and LOSE valence as +1 and -1." ></td>
	<td class="line x" title="116:290	ble 3)." ></td>
	<td class="line x" title="117:290	Then it generates a generalized sentence by substituting NDP with PARTY and Liberal with OTHER (Lines 67)." ></td>
	<td class="line x" title="118:290	It returns (NDP, -1, PARTY will barely take this riding from OTHER)." ></td>
	<td class="line x" title="119:290	For Liberal, on the other hand, the algorithm determines its Valence as +1 since Liberal is the same as the predicted winning party of the message." ></td>
	<td class="line x" title="120:290	After similar generalization, it returns (Liberal, +1, OTHER will barely take this riding from PARTY)." ></td>
	<td class="line x" title="121:290	Note that the final result of the feature generalization algorithm is a set of triplets: (Party, Valence, Generalized Sentence)." ></td>
	<td class="line x" title="122:290	Among a triplet, we use (Valence, Generalized Sentence) to produce feature vectors for a machine learning algorithm (see Section 4.2) and (Party, Valence) to integrate system results of each sentence for the final decision of Party and Valence of a message (see Section 4.3)." ></td>
	<td class="line x" title="123:290	Figure 2 shows an example of the algorithm." ></td>
	<td class="line x" title="124:290	4.2 Classification Using SVMs In this step, we use Support Vector Machines (SVMs) to train our system using the generalized features described in Section 4.1." ></td>
	<td class="line x" title="125:290	After we obtained examples of (Valence, Generalized Sentence) in the feature generalization step, we modeled a subtask of classifying a Generalized Sentence into Valence towards our final goal of determining (Valence, Party) of a message." ></td>
	<td class="line x" title="126:290	This subtask is a binary classification since Valence has only 2 classes: +1 and -1 6 . Given a generalized sentence OTHER will barely take this riding from PARTY in Figure 2, for example, the goal of our system is to learn WIN valence for PARTY." ></td>
	<td class="line x" title="127:290	Features for SVMs are extracted from generalized sentences." ></td>
	<td class="line x" title="128:290	We implemented our SVM learning model using the SVM light package 7 . 4.3 SVM Result Integration In this step, we combine the valence of each sentence predicted by SVMs to determine the final valence and predicted party of a message." ></td>
	<td class="line x" title="129:290	For each party mentioned in a message, we calculate the sum of the party's valences of each sentence and 6 However, the final evaluation of the system and all the baselines is equally performed on the multi-classification results of messages." ></td>
	<td class="line x" title="130:290	7 SVM light is available from http://svmlight.joachims." ></td>
	<td class="line x" title="131:290	org/ Figure 2." ></td>
	<td class="line x" title="132:290	An example of feature generalization of a message 1060 pick a party that has the maximum value." ></td>
	<td class="line x" title="133:290	This integration algorithm can be represented as follows:  = m k k p pValence 0 )(max arg where p is one of parties mentioned in a message, m is the number of sentences that contains party p in a message, and Valence k (p) is the valence of p in the k th sentence that contains p. Given the example in Figure 2, the Liberal party appears twice in sentence S0 and S1 and its total valence score is +2, whereas the NDP party appears once in sentence S1 and its valence sum is -1." ></td>
	<td class="line x" title="134:290	As a result, our algorithm picks liberal as the winning party that the message predicts." ></td>
	<td class="line x" title="135:290	5 Experiments and Results This section reports our experimental results showing empirical evidence that Crystal outperforms several baseline systems." ></td>
	<td class="line x" title="136:290	5.1 Experimental Setup Our corpus consists of 4858 and 4680 messages from 2004 and 2006 Canadian federal election prediction data respectively described in detail in Section 3.2." ></td>
	<td class="line x" title="137:290	We split our pre-processed corpus into 10 folds for cross-validation." ></td>
	<td class="line x" title="138:290	We implemented the following five systems to compare with Crystal 8 .  NGR: In this algorithm, we train the system using SVM with n-gram features without the generalization step described in Section 4.1 9 . The replacement of each candidate's first and last name by his or her party name was still applied." ></td>
	<td class="line x" title="139:290	 FRQ: This system picks the most frequently mentioned party in a message as the predicted winning party." ></td>
	<td class="line x" title="140:290	Party name substitution is also applied." ></td>
	<td class="line x" title="141:290	For example, given a message This riding will go liberal." ></td>
	<td class="line x" title="142:290	Dockrill will barely take this riding from Rodger Cuzner., all candidates' names are replaced by party names (i.e. , This riding will go Liberal." ></td>
	<td class="line x" title="143:290	NDP will barely take this riding from Liberal.)." ></td>
	<td class="line x" title="144:290	After name replacement, the system picks Liberal as an answer because Liberal appears twice whereas NDP appears only once." ></td>
	<td class="line x" title="145:290	Note that, unlike Crystal, this system does not consider the valence of each party (as done in our sentence duplication 8 In our experiments using SVM, we used the linear kernel for all Crystal, NGR, and JDG." ></td>
	<td class="line x" title="146:290	9 This system is exactly like Crystal without the feature generalization and result integration steps." ></td>
	<td class="line x" title="147:290	step of the feature generalization algorithm)." ></td>
	<td class="line x" title="148:290	Instead, it blindly picks the party that appeared most in a message." ></td>
	<td class="line x" title="149:290	 MJR: This system marks all messages with the most dominant predicted party in the entire data set." ></td>
	<td class="line x" title="150:290	In our corpus, Conservatives was the majority party (3480 messages) followed closely by Liberal (3473 messages)." ></td>
	<td class="line x" title="151:290	 INC: This system chooses the incumbent party as the predicted winning party of a message." ></td>
	<td class="line x" title="152:290	(This is a strong baseline since incumbents often win in Canadian politics)." ></td>
	<td class="line x" title="153:290	For example, since the incumbent party of the riding Blackstrap in 2004 was Conservative, all the messages about Blackstrap in 2004 were marked Conservative as their predicted winning party by this system." ></td>
	<td class="line x" title="154:290	 JDG: This system uses judgment opinion words as its features for SVM." ></td>
	<td class="line x" title="155:290	For our list of judgment opinion words, we use General Inquirer which is a publicly available list of 1635 positive and negative sentiment words (e.g. , love, hate, wise, dumb, etc.) 10 . 5.2 Experimental Results We measure the system performance with its accuracy in two different ways: accuracy per message (Acc message ) and accuracy per riding (Acc riding )." ></td>
	<td class="line x" title="156:290	Both accuracies are represented as follows: set test ain messages of # Total labledcorrectly system themessages of # = message Acc set test ain ridings of # Total predictedcorrectly system theridings of # = riding Acc We first report the results with Acc message in Evaluation1 and then report with Acc riding in Evaluation2." ></td>
	<td class="line x" title="157:290	Evaluation1: Table 4 shows accuracies of baselines and Crystal." ></td>
	<td class="line x" title="158:290	We calculated accuracy for each test set in 10-fold data sets and averaged it." ></td>
	<td class="line x" title="159:290	Among the baselines, MJR performed worst (36.48%)." ></td>
	<td class="line x" title="160:290	Both FRQ and INC performed around 50% (54.82% and 53.29% respectively)." ></td>
	<td class="line x" title="161:290	NGR achieved its best score (62.02%) when using unigram, bigram, and trigram features together (uni+bi+tri)." ></td>
	<td class="line x" title="162:290	We also experimented with other feature combinations (see Table 5)." ></td>
	<td class="line x" title="163:290	Our system achieved 73.07% which is 11% higher than NGR and around 20% 10 Available at http://www.wjh.harvard.edu/~inquirer /homecat.htm 1061 higher than FRQ and INC. The best accuracy of our system was also obtained with the combination of unigram, bigram, and trigram features." ></td>
	<td class="line x" title="164:290	The JDG system, which uses positive and negative sentiment word features, had 66.23% accuracy." ></td>
	<td class="line x" title="165:290	This is about 7% lower than Crystal." ></td>
	<td class="line x" title="166:290	Since the lower performance of JDG might be related to the number of features it uses, we also experimented with the reduced number of features of Crystal based on the tfidf scores 11 . With the same number of features (i.e. , 1635), Crystal performed 70.62% which is 4.4% higher than JDG." ></td>
	<td class="line x" title="167:290	An interesting finding was that NGR with 1635 features performed only 54.60% which is significantly 11 The total number of all features of Crystal is 689,642." ></td>
	<td class="line x" title="168:290	lower than both systems." ></td>
	<td class="line x" title="169:290	This indicates that the 1635 pure n-gram features are not as good as the same number of sentiment words carefully chosen from a dictionary but the generalized features of Crystal represent the predictive opinions better than JDG features." ></td>
	<td class="line x" title="170:290	Table 5 illustrates the comparison of NGR (without feature generalization) and Crystal (with feature generalization) in different feature combinations." ></td>
	<td class="line x" title="171:290	uni, bi, tri, and four correspond to unigram, bigram, trigram, and fourgram." ></td>
	<td class="line x" title="172:290	Our proposed technique Crystal performed always better than the pure n-gram system (NGR)." ></td>
	<td class="line x" title="173:290	Both systems performed best (62.02% and 73.07%) with the combination of unigram, bigram, and trigram (uni+bi+tri)." ></td>
	<td class="line x" title="174:290	The second best scores (61.96% and 73.01%) are achieved with the combinations of all grams (uni+bi+tri+four) in both systems." ></td>
	<td class="line x" title="175:290	Using fourgrams alone performed worst since the system overfitted to the training examples." ></td>
	<td class="line x" title="176:290	Table 6 presents several examples of frequent ngram features in both WIN and LOSE classes." ></td>
	<td class="line x" title="177:290	As shown in Table 6, lexical patterns in the WIN class express optimistic sentiments about PARTY (e.g. , PARTY_will_win and go_ PARTY_again) whereas patterns in the LOSE class express pessimistic sentiments (e.g. , PARTY_don't_have) and optimistic ones about OTHER (e.g. , want_OTHER)." ></td>
	<td class="line x" title="178:290	Evaluation2: In this evaluation, we use Acc riding computed as the number of ridings that a system correctly predicted, divided by the total number of ridings." ></td>
	<td class="line x" title="179:290	For each riding R, systems pick a party that obtains the majority prediction votes from messages in R as the winning party of R. For exPatterns in WIN class Patterns in LOSE class PARTY_will_win want_OTHER PARTY_hold PARTY_dont_have PARTY_will_win_this OTHER_and PARTY_win the_PARTY will_go_PARTY OTHER_will_win PARTY_will_take OTHER_is PARTY_will_take_this to_the_OTHER PARTY_is and_OTHER safest_PARTY results_OTHER PARTY_has OTHER_has go_PARTY_again to_OTHER Table 6." ></td>
	<td class="line x" title="180:290	Examples of frequent features in WIN and LOSE classes." ></td>
	<td class="line x" title="181:290	system Acc message (%) Acc riding (%) FRQ 54.82 63.14 MJR 36.48 36.63 INC 53.29 78.03 NGR (uni+bi+tri) 62.02 79.65 JDG 66.23 78.68 Crystal (uni+bi+tri) 73.07 81.68 Table 4." ></td>
	<td class="line x" title="182:290	System performance with accuracy per message (Acc message ) and accuracy per riding (Acc riding ): FRQ, MJR, INC, NGR, JDG, and Crystal." ></td>
	<td class="line x" title="183:290	Acc message (%) Features NGR Crystal uni 60.49 72.03 bi 58.7 71.81 tri 54.04 69.57 four 47.25 67.64 uni + bi 61.54 72.93 uni + tri 61.36 72.20 uni + four 60.70 72.84 bi + tri 58.68 72.26 bi + four 58.54 72.17 uni + bi + tri 62.02 73.07 uni + bi + four 61.75 72.30 uni + tri + four 61.34 72.30 bi + tri + four 58.42 72.62 uni + bi + tri + four 61.96 73.01 Table 5." ></td>
	<td class="line x" title="184:290	System performance with different features: Pure n-gram (NGR) and Generalized n-gram Crystal." ></td>
	<td class="line x" title="185:290	1062 ample, if Crystal identified 9 messages predicting for Conservative Party, 3 messages for NDP, and 1 message for Liberal among 13 messages in the riding Blackstrap, the system will predict that the Conservative Party would win in Blackstrap." ></td>
	<td class="line x" title="186:290	Table 4 shows the system performance with Acc riding . Note that people who write messages on a particular web site are not a random sample for prediction." ></td>
	<td class="line x" title="187:290	So we introduce a measure of confidence (ConfidenceScore) of each system and use the prediction results when the ConfidenceScore is higher than a threshold." ></td>
	<td class="line x" title="188:290	Otherwise, we use a default party (i.e. , the incumbent party) as the winning party." ></td>
	<td class="line x" title="189:290	ConfidenceScore of a riding R is calculated as follows: ConfidenceScore = count message (P first )  count message (P second ) where count message (P x ) is the number of messages that predict a party P x to win, P first is the party that the most number of messages predict to win, and P second is the party that the second most number of messages predict to win." ></td>
	<td class="line x" title="190:290	We used 62 ridings to tune the ConfidenceScore parameter arriving at the value of 4." ></td>
	<td class="line x" title="191:290	As shown in Table 4, the system which just considers the incumbent party (INC) performed fairly well (78.03% accuracy) because incumbents are often re-elected in Canadian elections." ></td>
	<td class="line x" title="192:290	The upper bound of this prediction task is 88.85% accuracy which is the prediction result using numerical values of a prediction survey." ></td>
	<td class="line x" title="193:290	FRQ and MJR performed 63.14% and 36.63% respectively." ></td>
	<td class="line x" title="194:290	Similarly to Evaluation1, JDG which only uses judgment word features performed worse than both Crystal and NGR." ></td>
	<td class="line x" title="195:290	Also, Crystal with our feature generalization algorithm performed better than NGR with nongeneralized n-gram features." ></td>
	<td class="line x" title="196:290	The accuracy of Crystal (81.68%) is comparable to the upper bound 88.85%." ></td>
	<td class="line x" title="197:290	6 Discussion In this section, we discuss possible extensions and improvements of this work." ></td>
	<td class="line x" title="198:290	Our experiment focuses on investigating aspects of predictive opinions by learning lexical patterns and comparing them with judgment opinions." ></td>
	<td class="line x" title="199:290	However, this work can be extended to investigating how those two types of opinions are related to each other and whether lexical features of one (e.g. , judgment opinion) can help identify the other (e.g. , predictive opinion)." ></td>
	<td class="line x" title="200:290	Combining two types of opinion features and testing on each domain can examine this issue." ></td>
	<td class="line x" title="201:290	In our experiment, we used General Inquirer words as judgment opinion indicators for JDG baseline system." ></td>
	<td class="line x" title="202:290	It might be interesting to employ different resources for judgment words such as the polarity lexicon by Wilson et al.(2005) and the recently released SentiWordNet 12 . Our work is an initial step towards analyzing a new type of opinion." ></td>
	<td class="line x" title="204:290	In the future, we plan to incorporate more features such as priors like incumbent party in addition to the lexical features to improve the system performance." ></td>
	<td class="line x" title="205:290	7 Conclusions In this paper, we proposed a framework for working with predictive opinion." ></td>
	<td class="line x" title="206:290	Previously, researchers in opinion analysis mostly focused on judgment opinions which express positive or negative sentiment about a topic, as in product reviews and policy discussions." ></td>
	<td class="line x" title="207:290	Unlike judgment opinions, predictive opinions express a person's opinion about the future of a topic or event such as the housing market, a popular sports match, and election results, based on his or her belief and knowledge." ></td>
	<td class="line x" title="208:290	Among these many kinds of predictive opinions, we focused on election prediction." ></td>
	<td class="line x" title="209:290	We collected past election prediction data from an election prediction project site and automatically built a gold standard." ></td>
	<td class="line x" title="210:290	Using this data, we modeled the election prediction task using a supervised learning approach, SVM." ></td>
	<td class="line x" title="211:290	We proposed a novel technique which generalized n-gram feature patterns." ></td>
	<td class="line x" title="212:290	Experimental results showed that this approach outperforms several baselines as well as a non-generalized n-gram approach." ></td>
	<td class="line x" title="213:290	This is significant because an n-gram model without generalization is often extremely competitive in many text classification tasks." ></td>
	<td class="line x" title="214:290	This work adopts NLP techniques for predictive opinions and it sets the foundation for exploring a whole new subclass of the opinion analysis problems." ></td>
	<td class="line x" title="215:290	Potential applications of this work are systems that analyze various kinds of election predictions by monitoring texts in discussion boards and personal blogs." ></td>
	<td class="line x" title="216:290	In the future, we would like to 12 http://sentiwordnet.isti.cnr.it/ 1063 model predictive opinions in other domains such as the real estate market and the stock market which would require further exploration of system design and data collection." ></td>
	<td class="line x" title="217:290	Reference Engelmore, R. , and Morgan, A. eds." ></td>
	<td class="line x" title="218:290	1986." ></td>
	<td class="line x" title="219:290	Blackboard Systems." ></td>
	<td class="line x" title="220:290	Reading, Mass.: Addison-Wesley." ></td>
	<td class="line x" title="221:290	Dave, K. , Lawrence, S. and Pennock, D. M. 2003." ></td>
	<td class="line x" title="222:290	Mining the peanut gallery: Opinion extraction and semantic classification of product reviews." ></td>
	<td class="line x" title="223:290	Proc." ></td>
	<td class="line x" title="224:290	of World Wide Web Conference 2003 Drucker, H. , Wu, D. and Vapnik, V. 1999." ></td>
	<td class="line x" title="225:290	Support vector machines for spam categorization." ></td>
	<td class="line x" title="226:290	IEEE Trans." ></td>
	<td class="line x" title="227:290	Neural Netw., 10, pp 10481054." ></td>
	<td class="line x" title="229:290	Gliozzo, A. , Strapparava C. and Dagan, I. 2005." ></td>
	<td class="line x" title="230:290	Investigating Unsupervised Learning for Text Categorization Bootstrapping, Proc." ></td>
	<td class="line x" title="231:290	of EMNLP 2005." ></td>
	<td class="line x" title="232:290	Vancouver, B.C., Canada Hu, M. and Liu, B. 2004." ></td>
	<td class="line x" title="233:290	Mining and summarizing customer reviews." ></td>
	<td class="line x" title="234:290	Proc." ></td>
	<td class="line x" title="235:290	Of KDD-2004, Seattle, Washington, USA." ></td>
	<td class="line x" title="236:290	Jindal, N. and Liu, B. 2006." ></td>
	<td class="line x" title="237:290	Mining Comprative Sentences and Relations." ></td>
	<td class="line x" title="238:290	Proc." ></td>
	<td class="line x" title="239:290	of 21st National Conference on Artificial Intellgience (AAAI-2006)." ></td>
	<td class="line x" title="240:290	2006." ></td>
	<td class="line x" title="241:290	Boston, Massachusetts, USA Joachims, T. 1998." ></td>
	<td class="line x" title="242:290	Text categorization with support vector machines: Learning with many relevant features, Proc." ></td>
	<td class="line x" title="243:290	of ECML, p. 137142." ></td>
	<td class="line x" title="244:290	Kim, S-M." ></td>
	<td class="line x" title="245:290	and Hovy, E. 2004." ></td>
	<td class="line x" title="246:290	Determining the Sentiment of Opinions." ></td>
	<td class="line x" title="247:290	Proc." ></td>
	<td class="line x" title="248:290	of COLING 2004." ></td>
	<td class="line x" title="249:290	Liu, B. , Li, X. , Lee, W. S. and Yu, P. S. Text Classification by Labeling Words Proc." ></td>
	<td class="line x" title="250:290	of AAAI-2004, San Jose, USA." ></td>
	<td class="line x" title="251:290	Pang, B, Lee, L. and Vaithyanathan, S. 2002." ></td>
	<td class="line x" title="252:290	Thumbs up?" ></td>
	<td class="line x" title="253:290	Sentiment Classification using Machine Learning Techniques." ></td>
	<td class="line x" title="254:290	Proc." ></td>
	<td class="line x" title="255:290	of EMNLP 2002." ></td>
	<td class="line x" title="256:290	Popescu, A-M." ></td>
	<td class="line x" title="257:290	and Etzioni, O. 2005." ></td>
	<td class="line x" title="258:290	Extracting Product Features and Opinions from Reviews, Proc." ></td>
	<td class="line x" title="259:290	of HLT-EMNLP 2005." ></td>
	<td class="line x" title="260:290	Rickel, J. and Porter, B. 1997." ></td>
	<td class="line x" title="261:290	Automated Modeling of Complex Systems to Answer Prediction Questions, Artificial Intelligence Journal, volume 93, numbers 1-2, pp." ></td>
	<td class="line x" title="262:290	201260 Riloff, E. , Wiebe, J. , and Phillips, W. 2005." ></td>
	<td class="line x" title="263:290	Exploiting Subjectivity Classification to Improve Information Extraction, Proc." ></td>
	<td class="line x" title="264:290	of the 20th National Conference on Artificial Intelligence (AAAI-05) . Riloff, E. , Wiebe, J. and Wilson, T. 2003." ></td>
	<td class="line x" title="265:290	Learning Subjective Nouns Using Extraction Pattern Bootstrapping." ></td>
	<td class="line x" title="266:290	Proc." ></td>
	<td class="line x" title="267:290	of CoNLL 2003." ></td>
	<td class="line x" title="268:290	pp 2532." ></td>
	<td class="line x" title="269:290	Rodionov, S. and Martin, J. H. 1996." ></td>
	<td class="line xc" title="270:290	A Knowledge-Based System for the Diagnosis and Prediction of Short-Term Climatic Changes in the North Atlantic, Journal of Climate, 9(8) Turney, P. 2002." ></td>
	<td class="line x" title="271:290	Thumbs Up or Thumbs Down?" ></td>
	<td class="line x" title="272:290	Semantic Orientation Applied to Unsupervised Classification of Reviews." ></td>
	<td class="line x" title="273:290	Proc." ></td>
	<td class="line x" title="274:290	of ACL 2002, pp 417424." ></td>
	<td class="line x" title="275:290	Wiebe, J. , Bruce, R. and OHara, T. 1999." ></td>
	<td class="line x" title="276:290	Development and use of a gold standard data set for subjectivity classifications." ></td>
	<td class="line x" title="277:290	Proc." ></td>
	<td class="line x" title="278:290	of ACL 1999, pp 246253." ></td>
	<td class="line x" title="279:290	Wiebe, J. , Wilson, TT., Bruce, RR., Bell, M. and Martin, M. Learning Subjective Language." ></td>
	<td class="line x" title="282:290	2004." ></td>
	<td class="line x" title="283:290	Computational Linguistics Wilson, T. , Wiebe, J. and Hoffmann, P. 2005." ></td>
	<td class="line x" title="284:290	Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis." ></td>
	<td class="line x" title="285:290	Proc." ></td>
	<td class="line x" title="286:290	of HLT/EMNLP 2005." ></td>
	<td class="line x" title="287:290	Yu, H. and Hatzivassiloglou, V. 2003." ></td>
	<td class="line x" title="288:290	Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences." ></td>
	<td class="line x" title="289:290	Proc." ></td>
	<td class="line x" title="290:290	of EMNLP 2003 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1114
Extracting Aspect-Evaluation and Aspect-Of Relations in Opinion Mining
Kobayashi, Nozomi;Inui, Kentaro;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:239	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:239	10651074, Prague, June 2007." ></td>
	<td class="line x" title="3:239	c2007 Association for Computational Linguistics Extracting Aspect-Evaluation and Aspect-of Relations in Opinion Mining Nozomi Kobayashi  Kentaro Inui, and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {nozomi-k,inui,matsu}@is.naist.jp Abstract The technology of opinion extraction allows users to retrieve and analyze peoples opinions scattered over Web documents." ></td>
	<td class="line x" title="4:239	We define an opinion unit as a quadruple consisting of the opinion holder, the subject being evaluated, the part or the attribute in which the subject is evaluated, and the value of the evaluation that expresses a positive or negative assessment." ></td>
	<td class="line x" title="5:239	We use this definition as the basis for our opinion extraction task." ></td>
	<td class="line x" title="6:239	We focus on two important subtasks of opinion extraction: (a) extracting aspect-evaluation relations, and (b) extracting aspect-of relations, and we approach each task using methods which combine contextual and statistical clues." ></td>
	<td class="line x" title="7:239	Our experiments on Japanese weblog posts show that the use of contextual clues improve the performance for both tasks." ></td>
	<td class="line x" title="8:239	1 Introduction The explosive increase in Web communication has attracted increasing interest in technologies for automatically mining personal opinions from Web documents such as product reviews and weblogs." ></td>
	<td class="line x" title="9:239	Such technologies would benefit users who seek reviews on certain consumer products of interest." ></td>
	<td class="line x" title="10:239	Previous approaches to the task of mining a largescale document collection of customer opinions (or  Currently, NTT Cyber Space Laboratories, 1-1, Hikarinooka, Yokosuka, Kanagawa, 239-0847 Japan reviews) can be classified into two approaches: Document classification and information extraction." ></td>
	<td class="line x" title="11:239	The former is the task of classifying documents or passages according to their semantic orientation such as positive vs. negative." ></td>
	<td class="line oc" title="12:239	This direction has been forming the mainstream of research on opinion-sensitive text processing (Pang et al. , 2002; Turney, 2002, etc.)." ></td>
	<td class="line x" title="13:239	The latter, on the other hand, focuses on the task of extracting opinions consisting of information about, for example, who feels how about which aspect of what product from unstructured text data." ></td>
	<td class="line x" title="14:239	In this paper, we refer to this information extractionoriented task as opinion extraction." ></td>
	<td class="line x" title="15:239	In contrast to sentiment classification, opinion extraction aims at producing richer information and requires an indepth analysis of opinions, which has only recently been attempted by a growing but still relatively small research community (Yi et al. , 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005, etc.)." ></td>
	<td class="line x" title="16:239	Most previous work on customer opinion extraction assumes the source of information to be customer reviews collected from customer review sites (Popescu and Etzioni, 2005; Hu and Liu, 2004; Liu et al. , 2005)." ></td>
	<td class="line x" title="17:239	In contrast, in this paper, we consider the task of extracting customer opinions from unstructured weblog posts." ></td>
	<td class="line x" title="18:239	Compared with extraction from review articles, extraction from weblogs is more challenging because weblog posts tend to exhibit greater diversity in topics, goals, vocabulary, style, etc. and are much more likely to include descriptions irrelevant to the subject in question." ></td>
	<td class="line x" title="19:239	In this paper, we first describe our task setting of opinion extraction." ></td>
	<td class="line x" title="20:239	We conducted a corpus study and investigated the feasibility of the task def1065 inition by showing the statistics and inter-annotator agreement of our corpus annotation." ></td>
	<td class="line x" title="21:239	Next, we show that the crucial body of the above opinion extraction task can be decomposed into two kinds of relation extraction, i.e. aspect-evaluation relation extraction and aspect-of relation extraction." ></td>
	<td class="line x" title="22:239	For example, the passage I went out for lunch at the Deli and ordered a curry with chicken." ></td>
	<td class="line x" title="23:239	It was pretty good has an aspect-evaluation relation curry with chicken, was good and an aspect-of relation The Deli, curry with the chicken." ></td>
	<td class="line x" title="24:239	The former task can be regarded as a special type of predicate-argument structure analysis or semantic role labeling." ></td>
	<td class="line x" title="25:239	The latter, on the other hand, can be regarded as bridging reference resolution (Clark, 1977), which is the task of identifying relations between definite noun phrases and discourse-new entities implicitly related to some previously mentioned entities." ></td>
	<td class="line x" title="26:239	Most of the previous work on customer opinion extraction, however, does not adopt the state-of-theart techniques in those fields, relying only on simple proximity-based or pattern-based methods." ></td>
	<td class="line x" title="27:239	In this context, this paper empirically shows that incorporating machine learning-based techniques devised for predicate-argument structure analysis and bridging reference resolution improve the performance of both aspect-evaluation and aspect-of relation extraction." ></td>
	<td class="line x" title="28:239	Furthermore, we also show that combining contextual clues with a common co-occurrence statistics-based technique for bridging reference resolution makes a significant improvement on aspectof relation extraction." ></td>
	<td class="line x" title="29:239	2 Opinion extraction: Task design Our present goal is to build a computational model to extract opinions from Web documents in such a form as: Who feels how on which aspects of which subjects." ></td>
	<td class="line x" title="30:239	Given the passage presented in Figure 1, for example, the opinion we want to extract is: the writer feels that the colors of pictures taken with Powershot (product) are beautiful. As suggested by this example, we consider it reasonable to start with an assumption that most evaluative opinions can be structured as a frame composed of the following constituents: Opinion holder The person who is making an evaluation." ></td>
	<td class="line x" title="31:239	An opinion holder is typically the first a0a2a1a4a3a5a0 a6a8a7a10a9a12a11a14a13a16a15a4a17a18a9a4a19a21a20a12a13a23a22a25a24a26a17a4a27a29a28a31a30a32a11a33a20a34a17a21a13a29a22a16a35a36a28a37a27a39a38a4a22a37a40a12a11 a22a12a19a18a17a18a41a31a6a42a13a2a17a31a17a21a43a44a11a12a17a42a45a16a28a47a46a21a48a49a12a13a36a9a42a30a36a28a12a11a44a9a42a11a18a48a50a37a19a51a13a10a20a12a28 a49a34a22a21a45a16a28a31a30a2a22a14a41a42a52a53a17a55a54a17a42a30a32a11a51a22a21a30a32a28a47a11a33a17a56a15a14a28a34a22a18a9a12a13a57a48a35a36a9a18a54a34a28a37a58a31a28a12a50 a27a29a20a18a28a31a50a44a35a59a54a22a14a11a18a20a60a48a11a44a9a31a11a14a28a31a38a18a41a14a61a16a54a11a4a17a60a28a34a22a37a11a33a40a60a13a2a17a62a19a34a30a63a48a46 a11a21a48a50a12a49a18a28a64a13a36a20a42a28a47a15a5a17a55a38a33a40a60a20a34a22a21a11a65a22a51a19a34a30a63a48a46a60a20a34a22a14a50a42a38a18a54a28a31a41 a17a42a46a18a48a50a55a48a17a21a50a47a20a4a17a66a54a38a33a28a42a30a68a67a4a27a29a30a59a48a13a36a28a42a30a63a69 a11a12a9a31a15a32a7a36a28a31a49a12a13a67a34a24a55a17a34a27a68a28a42a30a36a11a33a20a31a17a18a13a57a69 a22a18a11a12a46a18a28a12a49a12a13a67a31a69 a28a18a58a21a22a34a54a9a34a22a37a13a8a48a17a42a50a62a67a37a28a31a22a14a11a14a40a64a13a70a17a64a19a21a30a59a48a46a37a69 a71a33a72a37a73a74a14a73a71a14a74a29a75a14a74a14a73a76a78a77 a17a42a46a21a48a50a42a48a17a18a50a47a20a12a17a55a54a38a14a28a31a30a68a67a4a27a29a30a59a48a13a59a28a31a30a79a69 a11a12a9a42a15a80a7a59a28a31a49a12a13a67a34a24a26a17a4a27a29a28a12a30a36a11a33a20a31a17a21a13a59a69 a22a18a11a12a46a18a28a18a49a12a13a67a21a46a21a48a49a12a13a36a9a31a30a63a28a31a81a18a49a34a17a21a54a17a55a30a32a11a21a69 a28a18a58a34a22a21a54a9a34a22a14a13a8a48a17a55a50a25a67a21a15a14a28a34a22a37a9a12a13a8a48a35a32a9a55a54a69 a71a33a72a37a73a74a14a73a71a37a74a16a75a33a74a14a73a76a83a82 Figure 1: Extraction of opinion units person (the author)." ></td>
	<td class="line x" title="32:239	We say the opinion holder is unspecified if the opinion is mentioned as a rumor." ></td>
	<td class="line x" title="33:239	Subject A named entity (product or company) of a given particular class of interest (e.g. a car model name in the automobile domain)." ></td>
	<td class="line x" title="34:239	Aspect A part, member or related object, or an attribute (of a part) of the subject on which the evaluation is made (engine, size, etc.) Evaluation An evaluative or subjective phrase used to express an evaluation or the opinion holders mental/emotional attitude (good, poor, powerful, stylish, (I) like, (I) am satisfied, etc.) According to this typology, the example in Figure 1 has six constituents, the writer (opinion holder), Powershot (subject), pictures (aspect), colors (aspect), beautiful (evaluation), easy to grip (evaluation), and constitute two units of opinions as presented in the right half of the figure." ></td>
	<td class="line x" title="35:239	We call such a unit an opinion unit." ></td>
	<td class="line x" title="36:239	In this paper, we only consider explicitly mentioned evaluative opinions as our targets of extraction, excluding opinions indirectly expressed through, for example, style or language choice from our scope." ></td>
	<td class="line x" title="37:239	Under this assumption, opinion extraction can be defined as a task of filling a fixed number of slots as above for each of the evaluations expressed in a given text collection." ></td>
	<td class="line x" title="38:239	Two issues then immediately arise." ></td>
	<td class="line x" title="39:239	First, it is necessary to make sure that the definition of the opinion units is clear enough for human annotators to be able to carry out the task with sufficient accuracy." ></td>
	<td class="line x" title="40:239	Second, all the slots might not consist of simple expressions in that the filler of an aspect slot may have a hierarchical structure in itself." ></td>
	<td class="line x" title="41:239	For example, the leather cover of the seats (of a car) refers to a part of a part of a car." ></td>
	<td class="line x" title="42:239	In theory, such a hierarchical chain can be of any length, which 1066 may affect the feasibility of the task." ></td>
	<td class="line x" title="43:239	For tackling these issues, we built a corpus annotated with the above sort of information and investigated the feasibility of the task." ></td>
	<td class="line x" title="44:239	2.1 Corpus study We first collected 116 Japanese weblog posts in the restaurant domain by randomly sampling from a collection of posts classified under the gourmet category on a major blog site: http://blog.livedoor.com/." ></td>
	<td class="line x" title="45:239	We asked two annotators to annotate them independently of each other following the above specification." ></td>
	<td class="line x" title="46:239	The annotators first identified evaluative phrases, and then for each evaluative phrase judged whether it was concerning a particular subject (i.e. a restaurant) in the given domain." ></td>
	<td class="line x" title="47:239	If judged yes, the annotators filled the opinion holder and subject slots obligatorily." ></td>
	<td class="line x" title="48:239	The annotators filled the aspect slot only when its filler appeared in the document and identified the hierarchical relations between aspects if any (e.g. noodle and its volume)." ></td>
	<td class="line x" title="49:239	Note that, if a sentence has two or more evaluations, they have to make one opinion unit for each." ></td>
	<td class="line x" title="50:239	2.1.1 Inter-annotator agreement We investigated the degree of inter-annotator agreement." ></td>
	<td class="line x" title="51:239	In the task of identifying evaluations, one annotator A1 identified 450 evaluations while the other A2 identified 392, and 329 cases of them coincided." ></td>
	<td class="line x" title="52:239	The two annotators did not identify the same number of evaluations, so instead of using kappa statistics, we use the following metric for measuring agreement as Wiebe et al.(2005) do: agr(A1||A2) = # of tags agreed by A1 and A2# of tags annotated by A 1 agr(A1||A2) was 0.73 and agr(A2||A1) was 0.83." ></td>
	<td class="line x" title="54:239	The F1 measure of the agreement between the two was therefore 0.79, which indicate that humans can identify evaluation at a reasonable level." ></td>
	<td class="line x" title="55:239	Next, we investigated the inter-annotator agreement of the aspect-evaluation and subject-evaluation relations." ></td>
	<td class="line x" title="56:239	Annotator A1 identified 328 relations, and A2 identified 346 relations." ></td>
	<td class="line x" title="57:239	295 cases coincided, and agr(A2||A1) was 0.90 and agr(A1||A2) was 0.86 (F1 measure was 0.88)." ></td>
	<td class="line x" title="58:239	This shows that we obtained high consistency." ></td>
	<td class="line x" title="59:239	Finally, for the subject-aspect and aspect-aspect relations, annotator A1 identified 296 relations, while A2 identified 293, 233 cases of which got agreement." ></td>
	<td class="line x" title="60:239	agr(A2||A1) was 0.79 Table 1: Statistics of opinion-annotated corpus (Restaurant, Automobile, cellular phone and video game) Rest Auto Phone Game articles 1,356 564 481 361 sentences 21,666 14,005 11,638 6,448 # of opinion units 4,267 1,519 1,518 775 Asp-Eval 3,692 943 965 521 I Asp-Asp 1,426 280 296 221 Subj-Asp 2,632 877 850 451 II Subj-Eval 575 576 553 243 Subj-Asp-Eval 2,314 736 768 351 Subj-Asp-Asp-Eval 1,065 175 172 127 other 313 32 25 54 Non-writer op." ></td>
	<td class="line x" title="61:239	holder 95 17 22 2 and agr(A1||A2) was 0.80 (F1 measure was 0.79), which show that the human annotators can carry out the task at a reasonable accuracy." ></td>
	<td class="line x" title="62:239	Based on this corpus study, we believe that our definitions of two relations are clear enough for constructing annotated corpus." ></td>
	<td class="line x" title="63:239	2.1.2 Opinion-annotated corpus Based on these results, we collected a larger set of weblog posts in four domains: restaurant, automobile, cellular phone and video game." ></td>
	<td class="line x" title="64:239	We then asked annotator A1 to annotate them in the same annotation scheme as above." ></td>
	<td class="line x" title="65:239	The results are summarized in Table 1." ></td>
	<td class="line x" title="66:239	I in the table shows the number of the identified opinion units and relations, and II shows the number of hierarchical chains of aspects." ></td>
	<td class="line x" title="67:239	For example, Nokia 6800 has a nice color screen is counted as Subj-Asp-Eval since this example includes a subject Nokia 6800, an aspect color screen and an evaluation nice." ></td>
	<td class="line x" title="68:239	Other indicates the number of the case where the length of hierarchical chains of aspects is three or more." ></td>
	<td class="line x" title="69:239	One observation is that, for all the domains, 90 % of all the opinion units have a hierarchical chain of aspects whose length is two or less." ></td>
	<td class="line x" title="70:239	From this, we can conclude that hierarchical chains longer than two are rare, and the problem is not so complicated, though they can be of any length in theory." ></td>
	<td class="line x" title="71:239	The row of Non-writer op(inion) holder at the bottom of Table 1 shows the number of opinion units whose opinion holder is not the writer of the weblog." ></td>
	<td class="line x" title="72:239	This result indicates that when an evaluative expression is found, its opinion holder is highly likely to be the writer of the blogs." ></td>
	<td class="line x" title="73:239	Therefore, we put aside the task of filling the opinion holder slot in this paper." ></td>
	<td class="line x" title="74:239	1067 2.2 Related work on task settings of opinion extraction There are several researches on customer opinion extraction." ></td>
	<td class="line x" title="75:239	Hu and Liu (2004) considered the task of extracting Aspect, Sentence, Semantic-orientation triples in our terminology, where Sentence is the one that includes the Aspect, and Semantic-orientation is either positive or negative." ></td>
	<td class="line x" title="76:239	The notion of Evaluation in our term has also been introduced by previous work (Popescu and Etzioni, 2005; Tateishi et al. , 2004; Suzuki et al. , 2006; Kobayashi et al. , 2005, etc.)." ></td>
	<td class="line x" title="77:239	For example, our previous paper (Kobayashi et al. , 2005) addresses the task of extracting Subject,Aspect,Evaluation." ></td>
	<td class="line x" title="78:239	However, none of those papers reports on such an extensive corpus study as what we report in this paper." ></td>
	<td class="line x" title="79:239	In addition, in this paper, we consider not only aspect-evaluation relations but also hierarchical chains of subject-aspect and aspect-aspect relations, which has never been addressed in previous work." ></td>
	<td class="line x" title="80:239	Open-domain opinion extraction is another trend of research on opinion extraction, which aims to extract a wider range of opinions from such texts as newspaper articles (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Wiebe et al. , 2005; Choi et al. , 2006)." ></td>
	<td class="line x" title="81:239	To the best of our knowledge, one of the most extensive corpus studies in this field has been conducted in the MPQA project (Wiebe et al. , 2005); while their concerns include the types of opinions we consider, they annotate newspaper articles, which presumably exhibit considerably different characteristics from customer-generated texts." ></td>
	<td class="line x" title="82:239	Though we do not discuss the problem of determining semantic orientation, we assume availability of state-of-the-art methods that perform this task (Suzuki et al. , 2006; Takamura et al. , 2006, etc.)." ></td>
	<td class="line x" title="83:239	The problem of determining semantic orientation will be solved by using these techniques, so we focus on the main issue: Extracting opinion units from given texts." ></td>
	<td class="line x" title="84:239	3 Method for opinion extraction Before designing a model for our opinion extraction task, it is important to note that aspect phrases are open-class expressions and tend to be heavily domain-dependent." ></td>
	<td class="line x" title="85:239	In fact, according to our investigation on our opinion-annotated corpus, the number a0 a1a2a0a2a0 a3 a0a2a0a2a0 a3 a1a2a0a2a0 a4a2a0a2a0a2a0 a4a2a1a2a0a2a0 a5a2a0a2a0a2a0 a6 a7 a8 a9a11a10a12 a9a11a13a6a11a14a15a11a6a12 a16a17a19a18 a20a22a21a24a23 a25 a20a24a23 a3 a26a28a27a28a29a31a30a2a32a33 a4a2a26a28a27a28a29a31a30a2a32a33a2a34 a5a2a26a2a27a35a29a31a30a2a32a33a19a34 a36a2a26a2a27a35a29a31a30a2a32a33a19a34 a0 a3 a3 a6 a7 a8 a12 a12 a16 a25 a3 a33a3 a33 Figure 2: The distributions of evaluation and aspect expressions in the four domains of aspect types is nearly 3,200, and only 3% of them appear in two or more domains as shown in Figure 2." ></td>
	<td class="line x" title="86:239	For evaluation expressions, on the other hand, the number of types is much smaller than that of aspect expressions, and 27% of them appear in multiple domains." ></td>
	<td class="line x" title="87:239	This indicates that evaluation expressions are more likely to be used commonly across different domains compared with aspects." ></td>
	<td class="line x" title="88:239	To prove this assumption, we created a dictionary of evaluation expressions from customer reviews of automobiles (230,000 sentences in total) using the semi-automatic method proposed by Kobayashi et al.(2004)." ></td>
	<td class="line x" title="90:239	We expanded the dictionary by hand with external resources including publicly available ordinal thesauri." ></td>
	<td class="line x" title="91:239	As a result, we collected 5,550 entries." ></td>
	<td class="line x" title="92:239	According to our investigation of the coverage by the dictionary, 0.84 (restaurant), 0.88 (cellular phone), 0.91 (automobile), and 0.93 (video game) of the evaluations annotated in our corpus are covered by the dictionary." ></td>
	<td class="line x" title="93:239	From this observation, we consider that it is reasonable to start opinion extraction with the identification of evaluation expressions." ></td>
	<td class="line x" title="94:239	We therefore design the process of extracting Subject, Aspect, Evaluation as follows: 1." ></td>
	<td class="line x" title="95:239	Aspect-evaluation relation extraction: For each of the candidate evaluations that are selected from a given document by dictionary look-up, identify the target of the evaluation." ></td>
	<td class="line x" title="96:239	Here the identified target may be a subject (e.g. IXY (is well-designed)) or an aspect of a subject (e.g. the quality (is amazing))." ></td>
	<td class="line x" title="97:239	Hereafter, we use the term aspect to refer to both an aspect and a subject itself, since the subject can be regarded as the top element in the hierarchical chain of aspects." ></td>
	<td class="line x" title="98:239	2." ></td>
	<td class="line x" title="99:239	Opinion-hood determination: Judge whether or not the obtained pair aspect, evaluation is an expression of an opinion by considering the given context." ></td>
	<td class="line x" title="100:239	If it is judged yes, go to step3; otherwise, return to step 1 with a new candidate 1068 evaluation expression." ></td>
	<td class="line x" title="101:239	3." ></td>
	<td class="line x" title="102:239	Aspect-of relation extraction: If the identified aspect is not a subject, search for its antecedent, i.e. an expression that is a higher aspect or a subject of the current aspect." ></td>
	<td class="line x" title="103:239	Repeat step 3 until reaching a subject or no parent is found." ></td>
	<td class="line x" title="104:239	3.1 Related work on opinion extraction A common approach to the customer opinion extraction task mainly uses simple proximityor patternbased techniques." ></td>
	<td class="line x" title="105:239	For example, Tateishi et al.(2004) implement five syntactic patterns and Popescu et al.(2005) use ten syntactic patterns." ></td>
	<td class="line x" title="108:239	Such an approach is limited in two respects." ></td>
	<td class="line x" title="109:239	First, it assumes the availability of a list of potential aspect expressions as well as evaluation expressions; however creating such a list of aspects for a variety of domains can be prohibitively expensive because of the domain dependency of aspect expressions." ></td>
	<td class="line x" title="110:239	In contrast, our method does not require any aspect lexicon." ></td>
	<td class="line x" title="111:239	Second, their approach lacks the perspective of viewing aspect-evaluation extraction as a specific type of predicate-argument structure analysis, i.e. the task of identifying the arguments of a given predicate in a given text, and fails to benefit from the state-of-the-art techniques of this rapidly growing field." ></td>
	<td class="line x" title="112:239	The syntactic patterns used in their research are analyzed by a dependency parser, however, aspect-evaluation relations appear in diverse syntactic patterns, which cannot be easily captured by a handful of manually devised rules." ></td>
	<td class="line x" title="113:239	An exception is the model reported by Kanayama et al.(2004), which uses a component of an existing MT system to identify the aspect argument of a given evaluation predicate." ></td>
	<td class="line x" title="114:239	However, the MT component they use is not publicly available, and even if it were, it would be difficult to apply it to tasks in hand due of the opaqueness of its mechanism." ></td>
	<td class="line x" title="115:239	Our approach aims to develop a more generally applicable model of aspect-evaluation extraction." ></td>
	<td class="line x" title="116:239	In open-domain opinion extraction, some approaches use syntactic features obtained from parsed input sentences (Choi et al. , 2006; Kim and Hovy, 2006), as is commonly done in semantic role labeling." ></td>
	<td class="line x" title="117:239	Choi et al.(2006) address the task of extracting opinion entities and their relations, and incorporate syntactic features to their relation extraction model." ></td>
	<td class="line x" title="119:239	Kim and Hovy (2006) proposed a method for extracting opinion holders, topics and opinion words, in which they use semantic role labeling as an intermediate step to label opinion holders and topics." ></td>
	<td class="line x" title="120:239	However, these approaches do not address the task of extracting aspect-of relations and make use of syntactic features only for labeling opinion holders and topics." ></td>
	<td class="line x" title="121:239	In contrast, as we describe below, we find the significant overlap between aspectevaluation relation extraction and aspect-of relation extraction and apply the same approach to both tasks, gaining the generality of the model." ></td>
	<td class="line x" title="122:239	Aspect-of relations can be regarded as a sub-type of bridging reference (Clark, 1977), which is a common linguistic phenomenon where the referent of a definite noun phrase refers to a discourse-new entity implicitly related to some previously mentioned entity." ></td>
	<td class="line x" title="123:239	For example, we can see a relation of bridging reference between the door and the room in She entered the room." ></td>
	<td class="line x" title="124:239	The door closed automatically. A common approach is to use cooccurrence statistics between the referring expression (e.g. the door in the above example) and the related entity (the room) (Bunescu, 2003; Poesio et al. , 2004)." ></td>
	<td class="line x" title="125:239	Our approach newly incorporates automatically induced syntactic patterns as contextual clues into such a co-occurrence model, producing significant improvements of accuracy." ></td>
	<td class="line x" title="126:239	3.2 Our approach Now we describe our approach to aspect-evaluation and aspect-of relation extraction." ></td>
	<td class="line x" title="127:239	The key idea is to combine the following two kinds of information using a machine learning technique for both tasks." ></td>
	<td class="line x" title="128:239	Contextual clues: Syntactic patterns such as Aspect-ga VP-te, Evaluation Aspect-NOM VP-CONJ Evaluation which matches such a sentence as sekkyaku-ga kunrens-aretei-te kimochiyoi service-NOM be trained-CONJ feel comfortable (The waiters were well-trained, so I felt comfortable.)" ></td>
	<td class="line x" title="129:239	are considered to be useful for extracting relations between slot fillers when they appear in a single sentence (Here,  indicates a slot filler)." ></td>
	<td class="line x" title="130:239	We employ a supervised learning technique to search for such useful syntactic patterns." ></td>
	<td class="line x" title="131:239	Context-independent statistical clues: Statistics such as aspect-aspect and aspect-evaluation 1069 a0a2a1a3a0a5a4a6 a7a9a8 a10a12a11a2a13a15a14a17a16a18a5a19a21a20 a22a24a23a26a25a27a12a28a15a29a30 a8 a10a26a31a26a14a12a14a17a32a33a14a12a16a34a3a19a21a35 a36a2a37a38a26a39a40a37a37 a41 a14a2a42a43a10a2a43a44a46a45a12a47 a23 a8a17a48a49 a49 a50 a29 a49 a50 a10a12a44a46a51a17a52a11a2a43a51a26a16a53a54a19a21a34a56a55 a57a59a58a9a60a3a61a63a62a3a64a65a62a54a66a5a61a63a62a54a66a65a67a2a68a70a69a72a71a62a56a62 a73a17a74a17a75a48a76 a29 a76a26a77 a41 a11a2a78a59a42a43a51a2a79a40a16a44a2a80 a11a17a47a2a81a2a14a12a10a15a52 a51a2a44 a41 a14 a14a33a82a2a11a2a42a45a26a11a15a52a59a43a44a46a51a51a26a44a41 a14 a83a65a84a56a85a87a86a56a88a90a89 a11 a91 a92a26a93a12a91 a94a54a74a15a75a48a76 a29 a76a26a77a96a95a5a97a54a95a63a98 a29 a7a9a8 a22a24a23a2a25a27a12a28a15a29a30 a8 a23 a8a15a48a49 a49 a50 a29 a49 a50 a36a26a37a38a26a39a40a37a37 a99a5a100a102a101a9a95a46a103a63a104a77a12a105a94 a8 a75 a106a48a76 a30a46a107a108a21a22 a77a17a76a12a49a8a15a48a76 a22a90a23 a50a12a50 a108 a50a109a8a17a76a73 a8 a75 a50a109a110a5a111a15a112 a37 a113a33a37a36a40a114a26a38a115a116 a57a72a117a3a60a118a71a62a3a64a3a71a62a63a119a40a62a54a66a5a69a120a58a46a69a72a121a122a3a66a123a122a65a124a125a121a66a65a64a3a126a40a69a56a69a72a71a62a63a62 a127 a128a129 a127 a130 a130a130 a83a65a84a56a85a87a86a56a88 a52a120a14a83a65a84a63a85a131a86a63a88a51a26a44 Figure 3: Representation of input data co-occurrences are expected to be useful." ></td>
	<td class="line x" title="132:239	We obtain such statistical clues automatically from a large collection of raw documents." ></td>
	<td class="line x" title="133:239	In what follows, we describe our method for aspect-evaluation." ></td>
	<td class="line x" title="134:239	The aspect-of relation extraction is done in an an analogous way." ></td>
	<td class="line x" title="135:239	3.2.1 Supervised learning of contextual clues Let us consider the problem of searching for the aspect of a given evaluation expression t. This problem can be decomposed into binary classification problems of deciding whether each pair of candidate aspect c and target t is in an aspect-evaluation relation or not." ></td>
	<td class="line x" title="136:239	Our goal is to learn a discrimination function for this classification problem." ></td>
	<td class="line x" title="137:239	If such a function is obtained, we can identify the most likely candidate aspect simply by selecting the best scored c-t pair and, if its score is negative for all possible candidates, we conclude that t has no corresponding aspect in the candidate set." ></td>
	<td class="line x" title="138:239	For finding syntactic patterns that extract an aspect c starting with an evaluation t, we first represent all the sentences in the annotated corpus that has both an aspect and its evaluation, as shown in Figure 3." ></td>
	<td class="line x" title="139:239	A sentence is analyzed by a dependency parser, then the dependency tree is converted so as to represent the relation between content words clearly and to attach other information (such as POS labels and other morphological features of content words and the functional words attached to the content words) as shown in the lower part of Figure 3." ></td>
	<td class="line x" title="140:239	Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)s algorithm, packaged as a free software named BACT." ></td>
	<td class="line x" title="141:239	Given a set of training examples represented as ordered trees labeled either positive or negative class, this algorithm learns a list of weighted decision stumps as a discrimination function with the Boosting algorithm." ></td>
	<td class="line x" title="142:239	Each decision stump is associated with tuple s,l,w, where s is a subtree appearing in the training set, l a label, and w a weight of this pattern." ></td>
	<td class="line x" title="143:239	The strength of this algorithm is that it automatically acquires structured features and allows us to analyze the utility of features." ></td>
	<td class="line x" title="144:239	Given a c-t pair in an annotated sentence, tree encoding of this sentence is done as follows: First, we use a dependency parser to obtain a dependency tree as in Figure 3 (a)." ></td>
	<td class="line x" title="145:239	We assume keki (cake) as the candidate aspect c and oishii (delicious) as the target evaluation t. We then find the path between t and c together with their daughter nodes." ></td>
	<td class="line x" title="146:239	For example, the node Darling-no (Darlings) is kept since it is a daughter of c. Then, all the content words are abstracted to either of the class types, evaluation, aspect or node, that is, c is renamed as aspect, t as evaluation and all other content words as node." ></td>
	<td class="line x" title="147:239	Other information of a content word and the information of functional words attaching to the content word are represented as the leaf nodes as shown in Figure 3 (b)." ></td>
	<td class="line x" title="148:239	The features used in our experiments are summarized in Table 2." ></td>
	<td class="line x" title="149:239	We apply the same method to the aspect-of relation extraction by replacing the evaluation label as the second aspect label." ></td>
	<td class="line x" title="150:239	3.3 Context-independent statistical clues We also introduce the following two kinds of statistical clues." ></td>
	<td class="line x" title="151:239	i. Co-occurrences of aspect-evaluation/aspectaspect: Among various ways to estimate the strength of association (e.g. the number of hits returned from a search engine), in our experiments, we extracted aspect-aspect and aspect-evaluation co-occurrences in 1.7 million weblog posts using the patterns aspect ga/wa/mo evaluation (aspect is (subject-marker) evaluation) and aspect A no aspect B ga/wa (aspect B of aspect A is)." ></td>
	<td class="line x" title="152:239	To avoid the data sparseness problem, we use Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999) to estimate conditional probabilities P(Aspect|Evaluation) and P(Aspect A|Aspect B)." ></td>
	<td class="line x" title="153:239	We then incorporate the 1070 information of these probability scores into the learning model described in 3.2 by encoding them as a feature that indicates the relative score rank of each candidate in a given candidate set (see Table 2)." ></td>
	<td class="line x" title="154:239	ii." ></td>
	<td class="line x" title="155:239	Aspect-hood of candidate aspects: Aspect-hood is an index of the degree that measures how plausible a term is used as an aspect within a given domain." ></td>
	<td class="line x" title="156:239	We consider that a phrase directly co-occurred with a subject often is likely to be an aspect of the subject, and extract the expression X which appears in the form Subject no X (X of Subject) and the expression Y which appears in the form X no Y." ></td>
	<td class="line x" title="157:239	We calculate the aspect-hood of the expressions X and Y by the pointwise mutual information." ></td>
	<td class="line x" title="158:239	This score is also used as a features (see Table 2)." ></td>
	<td class="line x" title="159:239	3.4 Intra-/inter-sentential relation extraction Syntactic pattern induction as described in 3.2.1 can apply only when an aspect-evaluation (or aspect-of) relation appears in a single sentence." ></td>
	<td class="line x" title="160:239	We therefore build a separate model for inter-sentential relation extraction, which is carried out after intra-sentential relation extraction." ></td>
	<td class="line x" title="161:239	1) Intra-sentential relation identification: Given a target evaluation (or aspect), select the most likely candidate aspect c within the target sentence with the intra-sentential model described in 3.2.1." ></td>
	<td class="line x" title="162:239	If the score of c is positive, return c; otherwise, go to the inter-sentential relation extraction phase." ></td>
	<td class="line x" title="163:239	2) Inter-sentential relation identification: Search for the most likely candidate aspect in the sentences preceding the target evaluation (or aspect)." ></td>
	<td class="line x" title="164:239	This task can be regarded as a zero-anaphora resolution problem." ></td>
	<td class="line x" title="165:239	For this purpose, we employ the supervised learning model for zero-anaphora resolution proposed by (Iida et al. , 2003)." ></td>
	<td class="line x" title="166:239	3.5 Opinion-hood determination Evaluation phrases do not always extract correct opinion units in a given domain." ></td>
	<td class="line x" title="167:239	Consider an example from the digital camera domain, The weather was good." ></td>
	<td class="line x" title="168:239	so I went to the park to take some pictures." ></td>
	<td class="line x" title="169:239	good expresses the evaluation for the weather, but the weather is not an aspect of digital cameras." ></td>
	<td class="line x" title="170:239	Therefore, the weather, good is not an opinion in the digital camera domain." ></td>
	<td class="line x" title="171:239	We can consider a binary classification task of judging whether the obtained opinion unit is a real opinion or not in a given domain." ></td>
	<td class="line x" title="172:239	In this paper, we conduct a preliminary experiment which uses the opinion-hood determination model learned by Support Vector Machines." ></td>
	<td class="line x" title="173:239	We conduct the model using our opinionannotated corpus." ></td>
	<td class="line x" title="174:239	The positive examples are aspectevaluation pairs annotated in the corpus." ></td>
	<td class="line x" title="175:239	The negative examples are artificially generated as follows: We first identify the expression in the evaluation dictionary that appear in our annotated corpus." ></td>
	<td class="line x" title="176:239	We then apply the above aspect-evaluation extraction method and get the most plausible candidate aspect." ></td>
	<td class="line x" title="177:239	The result is regarded as a negative example if the extracted aspect is not the true aspect." ></td>
	<td class="line x" title="178:239	The features we used in our experiments are summarized in Table 2." ></td>
	<td class="line x" title="179:239	4 Experiments We conducted experiments with our Japanese opinion-annotated corpus to empirically evaluate the performance of our approach." ></td>
	<td class="line x" title="180:239	In these experiments, we separately evaluated the models of aspectevaluation relation extraction, aspect-of relation extraction, and opinion-hood determination." ></td>
	<td class="line x" title="181:239	4.1 Common settings We chose 395 weblog posts in the restaurant domain from our opinion-annotated corpus described in 2.1, and conducted 5-fold cross validation on that dataset." ></td>
	<td class="line x" title="182:239	As preprocessing, we analyzed this corpus using the Japanese morphological analyzer ChaSen1 and the Japanese dependency structure analyzer CaboCha2." ></td>
	<td class="line x" title="183:239	4.2 Models The results are summarized in Tables 3 and 4." ></td>
	<td class="line x" title="184:239	We evaluated the results by recall R and precision P defined as follows R = correctly extracted relationstotal number of relations, P = correctly extracted relationstotal number of relations found by the system." ></td>
	<td class="line x" title="185:239	Note that, in aspect-of relations, we permit A,C to be correct when the data includes the chain of aspect-of relations A,B and B,C." ></td>
	<td class="line x" title="186:239	Therefore, we merged the intraand inter-sentential results as shown in Table 4." ></td>
	<td class="line x" title="187:239	1http://chasen.naist.jp/ 2http://chasen.org/taku/software/cabocha/ 1071 Table 2: Feature list: t denotes a given target (evaluation or aspect) and c a candidate Features for contextual clues  Position of c / t in the sentence (beginning, end, other)  Base phrase distance between c and t (1, 2, 3, 4, other)  Whether c and t has a immediate dependency relation  Whether c precedes t  Whether c appears in a quoted sentence  Part-of-speech of c / t  Suffix of c (-sei, -sa (-ty), etc.)  Character type of c (English, Chinese, Katakana, etc.)  Semantic class of c derived from Nihongo Goi Taikei (Ikehara et al. , 1997)." ></td>
	<td class="line x" title="188:239	Features for statistical clues  Co-occurrence score rank of c (1st, 2nd, 3rd, 4th, other)  Aspect-hood score rank of c (1st, 2nd, 3rd, 4th, other) The Contextual and Contextual+statistics models are our proposed models where the former uses only contextual clues (3.2.1) and the latter uses both contextual and statistical clues." ></td>
	<td class="line x" title="189:239	We prepared two baseline models, one for each of the above tasks." ></td>
	<td class="line x" title="190:239	The Pattern model (in Table 3) simulates the patternbased method proposed by Tateishi at al." ></td>
	<td class="line x" title="191:239	(2004), which uses the following patterns: Aspect caseparticle Evaluation and Evaluation syntactically depends on Aspect." ></td>
	<td class="line x" title="192:239	The Co-occurrence model (in Table 4) simulates the co-occurrence statistics-based model used in bridging reference resolution (Bunescu, 2003): For an aspect expression, we select the nearest candidate that has the highest positive score of the pointwise mutual information regardless of its occurrence (i.e. interor intra-sentential)." ></td>
	<td class="line x" title="193:239	Comparing the Pattern (Cooccurrence) model with the Contextual model shows the effects of the supervised learning with contextual clues, while comparison of the Contextual and Contextual+statistics models shows the joint effect of combining contextual and statistical clues." ></td>
	<td class="line x" title="194:239	4.3 Results and discussions As for the aspect-evaluation relation extraction, concerning the intra-sentential cases, we can see that the models using the contextual clues show nearly 10% improvement in both precision and recall." ></td>
	<td class="line x" title="195:239	This indicates that the machine learning-based method has a great advantage over the pattern-based approach." ></td>
	<td class="line x" title="196:239	Similar results are seen in aspect-of relation extraction." ></td>
	<td class="line x" title="197:239	The models using the contextual clues achieved more than 10% improvement in preTable 3: The results of aspect-evaluation relation intra-sent." ></td>
	<td class="line x" title="198:239	inter-sent." ></td>
	<td class="line x" title="199:239	Patterns P 0.56 (432/774) R 0.53 (432/809) Contextual P 0.70 (504/723) 0.13 (46/360) R 0.62 (504/809) 0.17 (46/274) Contextual P 0.72 (502/694) 0.14 (53/389) +statistics R 0.62 (502/809) 0.19 (53/274) Table 4: The results of aspect-of relation precision recall Co-occurrence 0.27 (175/ 682) 0.17 (175/1048) Contextual 0.44 (458/1047) 0.44 (458/1048) Contextual+statistics 0.45 (474/1047) 0.45 (474/1048) cision and 20% improvement in recall over the cooccurrence statistics-based model." ></td>
	<td class="line x" title="200:239	We can say that contextual clues are also useful in aspect-of relation extraction." ></td>
	<td class="line x" title="201:239	In comparing the Contextual and Contextual+statistics models, on the other hand, we could get only a slight improvement, which indicates that we need to estimate the statistical clues more precisely." ></td>
	<td class="line x" title="202:239	We found that the unsophisticated estimation of the statistical clues was a major source of errors in aspect-of relation extraction, however, this estimation is not so easy since the correct expressions are appeared only once in large data." ></td>
	<td class="line x" title="203:239	We are seeking efficient ways to avoid data sparseness problem (e.g. categorize the aspects)." ></td>
	<td class="line x" title="204:239	In the aspect-evaluation relation extraction, we evaluated the results against the human annotated gold-standard in a strict manner." ></td>
	<td class="line x" title="205:239	However, according to our error analysis, some of the errors can be regarded as correct for some real applications." ></td>
	<td class="line x" title="206:239	In the following example, a relation annotated by the human is aji (taste), koi-me (strong)." ></td>
	<td class="line x" title="207:239	misoshiru-wa aji-ga koi-me miso soup-TOP taste-NOM strong (The taste of the miso soup is strong.)" ></td>
	<td class="line x" title="208:239	However, there is no harm to consider that misoshiru (miso soup), koi-me (strong) is also correct." ></td>
	<td class="line x" title="209:239	If we judge these cases as correct, the Proposed models achieve nearly 0.8 precision and 0.7 recall, and the baseline model also get 7 % improvement (precision 0.63 and recall 0.6)." ></td>
	<td class="line x" title="210:239	Based on this result, we consider that we achieved reasonable performance in intra-sentential aspect-evaluation relation extraction." ></td>
	<td class="line x" title="211:239	As Table 3 shows, inter-sentential relation extraction achieved very poorly." ></td>
	<td class="line x" title="212:239	In the case of inter1072 sentential relations, our model tends to rely heavily on the statistical clues, because syntactic pattern features cannot be used." ></td>
	<td class="line x" title="213:239	However, our current method for estimating co-occurrence distributions is not sophisticated as we discussed above." ></td>
	<td class="line x" title="214:239	We need to seek for more effective use of large scale domain dependent data to obtain better statistics." ></td>
	<td class="line x" title="215:239	We also conducted a preliminary test of the opinion-hood determination model using the features used in aspect-evaluation relation extraction." ></td>
	<td class="line x" title="216:239	As a result, we got 0.5 precision and 0.45 recall." ></td>
	<td class="line x" title="217:239	Opinion-hood determination problem includes two decisions: whether the evaluation candidate is an opinion or not, and whether the opinion is related to the given domain if the evaluation candidate is an opinion." ></td>
	<td class="line x" title="218:239	We plan to use various features known to be effective in the sentence subjectivity recognition task." ></td>
	<td class="line x" title="219:239	This task involves challenging problems." ></td>
	<td class="line x" title="220:239	For example, sentence (1) includes the writers evaluation on shrimps served at a particular restaurant." ></td>
	<td class="line x" title="221:239	In contrast, very similar sentence (2) does not express evaluation since it is a generic description of the writers taste." ></td>
	<td class="line x" title="222:239	(1) watashi-wa konomise-no ebi-ga suki-desu I the restaurant shrimp like (I like shrimps of the restaurant.)" ></td>
	<td class="line x" title="223:239	(2) watashi-wa ebi-ga suki-desu I shrimps like (I like shrimps.)" ></td>
	<td class="line x" title="224:239	Thus we need to conduct further investigation in order to resolve this kind of problems." ></td>
	<td class="line x" title="225:239	4.4 Portability of intra-sentential model We next evaluated effectiveness of the contextual clues learned in the domains to other domains by testing a model trained on the certain domains to other domain." ></td>
	<td class="line x" title="226:239	We selected two new domains, cellular phone and automobile, and annotated 290 weblog posts in each domain." ></td>
	<td class="line x" title="227:239	For the restaurant domain, we randomly selected 290 posts from the previously mentioned our annotated corpus." ></td>
	<td class="line x" title="228:239	We then divide each data set to a training set and a test set so that we had the same amount of training data for each domain." ></td>
	<td class="line x" title="229:239	Then we trained a model on the data for each domain, and applied it to each of the three set of data." ></td>
	<td class="line x" title="230:239	Table 5 shows the results of the experiment." ></td>
	<td class="line x" title="231:239	Compared with the model trained on the same domain, the models trained on different domains exhibited almost comparable performance." ></td>
	<td class="line x" title="232:239	This inTable 5: Comparing intra-sentential models among three domains (upper: aspect-eval, lower: aspect-of) test restaurant cellular phone automobile same P 0.72 (502/694) 0.75 (522/693) 0.76 (562/738) dom." ></td>
	<td class="line x" title="233:239	R 0.62 (502/809) 0.63 (522/833) 0.65 (562/870) other P 0.73 (468/638) 0.72 (517/710) 0.74 (565/768) dom R 0.58 (468/809) 0.62 (517/833) 0.65 (565/870) same P 0.43 (139/321) 0.62 (139/224) 0.66 (185/280) dom." ></td>
	<td class="line x" title="234:239	R 0.59 (139/234) 0.60 (139/230) 0.66 (185/279) other P 0.42 (124/293) 0.53 (138/260) 0.59 (195/329) dom R 0.52 (124/234) 0.60 (138/230) 0.70 (195/279) dicates that the contextual clues learned in other domains are effective in another domain, showing the cross-domain portability of our intra-sentential model." ></td>
	<td class="line x" title="235:239	5 Conclusion In this paper, we described our opinion extraction task, which extract opinion units consisting of four constituents." ></td>
	<td class="line x" title="236:239	We showed the feasibility of the task definition based on our corpus study." ></td>
	<td class="line x" title="237:239	We consider the task as two kinds of relation extraction tasks, aspect-evaluation relation extraction and aspect-of relation extraction, and proposed a machine learning-based method which combines contextual clues and statistical clues." ></td>
	<td class="line x" title="238:239	Our experimental results show that the model using contextual clues improved the performance for both tasks." ></td>
	<td class="line x" title="239:239	We also showed domain portability of the contextual clues." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1115
Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents
Kaji, Nobuhiro;Kitsuregawa, Masaru;"></td>
	<td class="line x" title="1:280	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:280	10751083, Prague, June 2007." ></td>
	<td class="line x" title="3:280	c2007 Association for Computational Linguistics Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents Nobuhiro Kaji and Masaru Kitsuregawa Institute of Industrial Science, University of Tokyo 4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan CUkaji,kitsureCV@tkl.iis.u-tokyo.ac.jp Abstract Recognizing polarity requires a list of polar words and phrases." ></td>
	<td class="line x" title="4:280	For the purpose of building such lexicon automatically, a lot of studies have investigated (semi-) unsupervised method of learning polarity of words and phrases." ></td>
	<td class="line x" title="5:280	In this paper, we explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences." ></td>
	<td class="line x" title="6:280	The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall." ></td>
	<td class="line x" title="7:280	In order to compensate for the low recall, we used massive collection of HTML documents." ></td>
	<td class="line x" title="8:280	Thus, we could prepare enough polar sentence corpus." ></td>
	<td class="line x" title="9:280	1 Introduction Sentiment analysis is a recent attempt to deal with evaluative aspects of text." ></td>
	<td class="line x" title="10:280	In sentiment analysis, one fundamental problem is to recognize whether given text expresses positive or negative evaluation." ></td>
	<td class="line x" title="11:280	Such property of text is called polarity." ></td>
	<td class="line x" title="12:280	Recognizing polarity requires a list of polar words and phrases such as good, bad and high performance etc. For the purpose of building such lexicon automatically, a lot of studies have investigated (semi-) unsupervised approach." ></td>
	<td class="line x" title="13:280	So far, two kinds of approaches have been proposed to this problem." ></td>
	<td class="line x" title="14:280	One is based on a thesaurus." ></td>
	<td class="line x" title="15:280	This method utilizes synonyms or glosses of a thesaurus in order to determine polarity of words (Kamps et al. , 2004; Hu and Liu, 2004; Kim and Hovy, 2004; Esuli and Sebastiani, 2005)." ></td>
	<td class="line x" title="16:280	The second approach exploits raw corpus." ></td>
	<td class="line x" title="17:280	Polarity is decided by using co-occurrence in a corpus." ></td>
	<td class="line x" title="18:280	This is based on a hypothesis that polar phrases conveying the same polarity co-occur with each other." ></td>
	<td class="line oc" title="19:280	Typically, a small set of seed polar phrases are prepared, and new polar phrases are detected based on the strength of co-occurrence with the seeds (Hatzivassiloglous and McKeown, 1997; Turney, 2002; Kanayama and Nasukawa, 2006)." ></td>
	<td class="line x" title="20:280	As for the second approach, it depends on the definition of co-occurrence whether the hypothesis is appropriate or not." ></td>
	<td class="line oc" title="21:280	In Turneys work, the co-occurrence is considered as the appearance in the same window (Turney, 2002)." ></td>
	<td class="line n" title="22:280	Although this idea is simple and feasible, there is a room for improvement." ></td>
	<td class="line x" title="23:280	According to Kanayamas investigation, the hypothesis is appropriate in only 60% of cases if co-occurrence is defined as the appearance in the same window 1." ></td>
	<td class="line x" title="24:280	In Kanayamas method, the co-occurrence is considered as the appearance in intraor inter-sentential context (Kanayama and Nasukawa, 2006)." ></td>
	<td class="line x" title="25:280	They reported that the precision was boosted to 72.2%, but it is still not enough." ></td>
	<td class="line x" title="26:280	Therefore, we think that the above hypothesis is often inappropriate in practice, and this fact is the biggest obstacle to learning lexicon from corpus." ></td>
	<td class="line x" title="27:280	In this paper, we explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the ex1 To be exact, the precision depends on window size and ranges from 59.7 to 64.1%." ></td>
	<td class="line x" title="28:280	See Table 4 in (Kanayama and Nasukawa, 2006) for the detail." ></td>
	<td class="line x" title="29:280	1075 Figure 1: Overview of the proposed method." ></td>
	<td class="line x" title="30:280	kono this software-no software-POST riten-ha advantage-POST hayaku quickly ugoku run koto-desu to-POST The advantage of this software is to run quickly." ></td>
	<td class="line x" title="31:280	Figure 2: Language structure." ></td>
	<td class="line x" title="32:280	tracted polar sentences." ></td>
	<td class="line x" title="33:280	An overview of the proposed method is represented in Figure 1." ></td>
	<td class="line x" title="34:280	First, polar sentences are extracted from HTML documents by using structural clues (step 1)." ></td>
	<td class="line x" title="35:280	The set of polar sentences is called polar sentence corpus." ></td>
	<td class="line x" title="36:280	Next, from the polar sentence corpus, candidates of polar phrases are extracted together with their counts in positive and negative sentences (step 2)." ></td>
	<td class="line x" title="37:280	Finally, polar phrases are selected from the candidates and added to our lexicon (step 3)." ></td>
	<td class="line x" title="38:280	The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall." ></td>
	<td class="line x" title="39:280	As we will see in Section 2.3, the precision was extremely high." ></td>
	<td class="line x" title="40:280	It was around 92% even if ambiguous cases were considered as incorrect." ></td>
	<td class="line x" title="41:280	In order to compensate for the low recall, we used massive collection of HTML documents." ></td>
	<td class="line x" title="42:280	Thus, we could build enough polar sentence corpus." ></td>
	<td class="line x" title="43:280	To be specific, we extracted 500,000 polar sentences from one billion HTML documents." ></td>
	<td class="line x" title="44:280	The contribution of this paper is to empirically show the effectiveness of an approach that makes use of the strength of massive data." ></td>
	<td class="line x" title="45:280	Nowadays, terabyte is not surprisingly large, and larger corpus would be obtained in the future." ></td>
	<td class="line x" title="46:280	Therefore, we think this kind of research direction is important." ></td>
	<td class="line x" title="47:280	2 Extracting Polar Sentences Our method begins by automatically constructing polar sentence corpus with structural clues (step 1)." ></td>
	<td class="line x" title="48:280	The basic idea is exploiting certain language and layout structures as clues to extract polar sentences." ></td>
	<td class="line x" title="49:280	The clues were carefully chosen so that it achieves high precision." ></td>
	<td class="line x" title="50:280	The original idea was represented in our previous paper (Kaji and Kitsuregawa, 2006)." ></td>
	<td class="line x" title="51:280	2.1 Language structure Some polar sentences are described by using characteristic language structures." ></td>
	<td class="line x" title="52:280	Figure 2 illustrates such Japanese polar sentence attached with English translations." ></td>
	<td class="line x" title="53:280	Japanese are written in italics and  denotes that the word is followed by postpositional particles." ></td>
	<td class="line x" title="54:280	For example, software-no means that software is followed by postpositional particle no." ></td>
	<td class="line x" title="55:280	The arrow represents dependency relationship." ></td>
	<td class="line x" title="56:280	Translations are shown below the Japanese sentence." ></td>
	<td class="line x" title="57:280	-POST means postpositional particle." ></td>
	<td class="line x" title="58:280	What characterizes this sentence is the singly underlined phrase." ></td>
	<td class="line x" title="59:280	In this phrase, riten (advantage) is followed by postpositional particle -ha, which is Japanese topic marker." ></td>
	<td class="line x" title="60:280	And hence, we can recognize that something positive is the topic of the sentence." ></td>
	<td class="line x" title="61:280	This kind of linguistic structure can be recognized by lexico-syntactic pattern." ></td>
	<td class="line x" title="62:280	Hereafter, such words like riten (advantage) are called cue words." ></td>
	<td class="line x" title="63:280	1076 In order to handle the language structures, we utilized lexico-syntactic patterns as illustrated below." ></td>
	<td class="line x" title="64:280	riten-ha advantage-POST (polar) koto-desu to-POST A sub-tree that matches (polar) is extracted as polar sentence." ></td>
	<td class="line x" title="65:280	It is obvious whether the polar sentence is positive or negative one." ></td>
	<td class="line x" title="66:280	In case of Figure 2, the doubly underlined part is extracted as polar sentence 2 . Besides riten (advantage), other cue words were also used." ></td>
	<td class="line x" title="67:280	A list of cue words (and phrases) were manually created." ></td>
	<td class="line x" title="68:280	For example, we used pros or good point for positive sentences, and cons, bad point or disadvantage for negative ones." ></td>
	<td class="line x" title="69:280	This list is also used when dealing with layout structures." ></td>
	<td class="line x" title="70:280	2.2 Layout structure Two kinds of layout structures are utilized as clues." ></td>
	<td class="line x" title="71:280	The first clue is the itemization." ></td>
	<td class="line x" title="72:280	In Figure 3, the itemizations have headers and they are cue words (pros and cons)." ></td>
	<td class="line x" title="73:280	Note that we illustrated translations for the sake of readability." ></td>
	<td class="line x" title="74:280	By using the cue words, we can recognize that polar sentences are described in these itemizations." ></td>
	<td class="line x" title="75:280	The other clue is table structure." ></td>
	<td class="line x" title="76:280	In Figure 4, a car review is summarized in the table format." ></td>
	<td class="line x" title="77:280	The left column acts as a header and there are cue words (plus and minus) in that column." ></td>
	<td class="line x" title="78:280	Pros: AF The sound is natural." ></td>
	<td class="line x" title="79:280	AF Music is easy to find." ></td>
	<td class="line x" title="80:280	AF Can enjoy creating my favorite play-lists." ></td>
	<td class="line x" title="81:280	Cons: AF The remote controller does not have an LCD display." ></td>
	<td class="line x" title="82:280	AF The body gets scratched and fingerprinted easily." ></td>
	<td class="line x" title="83:280	AF The battery drains quickly when using the backlight." ></td>
	<td class="line x" title="84:280	Figure 3: Itemization structure." ></td>
	<td class="line x" title="85:280	2 To be exact, the doubly underlined part is polar clause." ></td>
	<td class="line x" title="86:280	However, it is called polar sentence because of the consistency with polar sentences extracted by using layout structures." ></td>
	<td class="line x" title="87:280	Mileage(urban) 7.0km/litter Mileage(highway) 9.0km/litter Plus This is a four door car, but its so cool." ></td>
	<td class="line x" title="88:280	Minus The seat is ragged and the light is dark." ></td>
	<td class="line x" title="89:280	Figure 4: Table structure." ></td>
	<td class="line x" title="90:280	It is easy to extract polar sentences from the itemization." ></td>
	<td class="line x" title="91:280	Such itemizations as illustrated in Figure 3 can be detected by using the list of cue words and HTML tags such as BOh1BQ and BOulBQ etc. Three positive and negative sentences are extracted respectively from Figure 3." ></td>
	<td class="line x" title="92:280	As for table structures, two kinds of tables are considered (Figure 5)." ></td>
	<td class="line x" title="93:280	In the Figure, B7 and A0 represent positive and negative polar sentences, and BV B7 and BV A0 represent cue words." ></td>
	<td class="line x" title="94:280	Type A is a table in which the leftmost column acts as a header." ></td>
	<td class="line x" title="95:280	Figure 4 is categorized into this type." ></td>
	<td class="line x" title="96:280	Type B is a table in which the first row acts as a header." ></td>
	<td class="line x" title="97:280	Type A BV B7 B7 BV A0 A0 Type B BV B7 BV A0 B7 A0 Figure 5: Two types of table structures." ></td>
	<td class="line x" title="98:280	In order to extract polar sentences, first of all, it is necessary to determine the type of the table." ></td>
	<td class="line x" title="99:280	The table is categorized into type A if there are cue words in the leftmost column." ></td>
	<td class="line x" title="100:280	The table is categorized into type B if it is not type A and there are cue words in the first row." ></td>
	<td class="line x" title="101:280	After the type of the table is decided, we can extract polar sentences from the cells that correspond to B7 and A0 in the Figure 5." ></td>
	<td class="line x" title="102:280	2.3 Result of corpus construction The method was applied to one billion HTML documents." ></td>
	<td class="line x" title="103:280	In order to get dependency tree, we used KNP 3 . As the result, 509,471 unique polar sentences were obtained." ></td>
	<td class="line x" title="104:280	220,716 are positive and the others are negative 4 . Table 1 illustrates some translations of the polar sentences." ></td>
	<td class="line x" title="105:280	3 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html 4 The polar sentence corpus is available from http://www.tkl.iis.u-tokyo.ac.jp/kaji/acp/." ></td>
	<td class="line x" title="106:280	1077 Table 1: Examples of polar sentences." ></td>
	<td class="line x" title="107:280	Polarity Polar sentence It becomes easy to compute cost." ></td>
	<td class="line x" title="108:280	positive Its easy and can save time." ></td>
	<td class="line x" title="109:280	The soup is rich and flavorful." ></td>
	<td class="line x" title="110:280	Cannot use mails in HTML format." ></td>
	<td class="line x" title="111:280	negative The lecture is really boring." ></td>
	<td class="line x" title="112:280	There is no impressive music." ></td>
	<td class="line x" title="113:280	In order to investigate the quality of the corpus, two human judges (judge A/B) assessed 500 polar sentences in the corpus." ></td>
	<td class="line x" title="114:280	According to the judge A, the precision was 91.4%." ></td>
	<td class="line x" title="115:280	459 out of 500 polar sentences were regarded as valid ones." ></td>
	<td class="line x" title="116:280	According to the judge B, the precision was 92.0% (460/500)." ></td>
	<td class="line x" title="117:280	The agreement between the two judges was 93.5% (Kappa value was 0.90), and thus we can conclude that the polar sentence corpus has enough quality (Kaji and Kitsuregawa, 2006)." ></td>
	<td class="line x" title="118:280	After error analysis, we found that most of the errors are caused by the lack of context." ></td>
	<td class="line x" title="119:280	The following is a typical example." ></td>
	<td class="line x" title="120:280	There is much information." ></td>
	<td class="line x" title="121:280	This sentence is categorized into positive one in the corpus, and it was regarded as invalid by both judges because the polarity of this sentence is ambiguous without context." ></td>
	<td class="line x" title="122:280	As we described in Section 1, the hypothesis of co-occurrence based method is often inappropriate." ></td>
	<td class="line x" title="123:280	(Kanayama and Nasukawa, 2006) reported that it was appropriate in 72.2% of cases." ></td>
	<td class="line x" title="124:280	On the other hand, by using extremely precise clues, we could build polar sentence corpus that have high precision (around 92%)." ></td>
	<td class="line x" title="125:280	Although the recall of structural clues is low, we could build large corpus by using massive collection of HTML documents." ></td>
	<td class="line x" title="126:280	Of course, we cannot directly compare these two percentages." ></td>
	<td class="line x" title="127:280	We think, however, the high precision of 92% implies the strength of our approach." ></td>
	<td class="line x" title="128:280	3 Acquisition of Polar Phrases The next step is to acquire polar phrases from the polar sentence corpus (step 2 and 3 in Figure 1)." ></td>
	<td class="line x" title="129:280	3.1 Counting candidates From the corpus, candidates of polar phrases are extracted together with their counts (step 2)." ></td>
	<td class="line x" title="130:280	As is often pointed out, adjectives are often used to express evaluative content." ></td>
	<td class="line x" title="131:280	Considering that polarity of isolate adjective is sometimes ambiguous (e.g. high), not only adjectives but also adjective phrases (noun + postpositional particle + adjective) are treated as candidates." ></td>
	<td class="line x" title="132:280	Adjective phrases are extracted by the dependency parser." ></td>
	<td class="line x" title="133:280	To handle negation, an adjective with negation words such as not is annotated by BONEGATIONBQ tag." ></td>
	<td class="line x" title="134:280	For the sake of readability, we simply represent adjective phrases in the form of noun-adjective by omiting postpositional particle, as in the Figure 1." ></td>
	<td class="line x" title="135:280	For each candidate, we count the frequency in positive and negative sentences separately." ></td>
	<td class="line x" title="136:280	Intuitively, we can expect that positive phrases often appear in positive sentences, and vice versa." ></td>
	<td class="line x" title="137:280	However, there are exceptional cases as follows." ></td>
	<td class="line x" title="138:280	Although the price is high, its shape is beautiful." ></td>
	<td class="line x" title="139:280	Although this sentence as a whole expresses positive evaluation and it is positive sentence, negative phrase price is high appears in it." ></td>
	<td class="line x" title="140:280	To handle this, we hypothesized that positive/negative phrases tend to appear in main clause of positive/negative sentences, and we exploited only main clauses to count the frequency." ></td>
	<td class="line x" title="141:280	3.2 Selecting polar phrases For each candidate, we determine numerical value indicating the strength of polarity, which is referred as polarity value." ></td>
	<td class="line x" title="142:280	On the basis of this value, we select polar phrases from the candidates and add them to our lexicon (step 3)." ></td>
	<td class="line x" title="143:280	For each candidate CR, we can create a contingency table as follows." ></td>
	<td class="line x" title="144:280	Table 2: Contingency table D4D3D7 D2CTCV CR CUB4CRBND4D3D7B5 CUB4CRBND2CTCVB5 BMCR CUB4BMCRBND4D3D7B5 CUB4BMCRBND2CTCVB5 CUB4CRBND4D3D7B5 is the frequency of CR in positive sentences." ></td>
	<td class="line x" title="145:280	CUB4BMCRBND4D3D7B5 is that of all candidates but CR." ></td>
	<td class="line x" title="146:280	CUB4CRBND2CTCVB5 and CUB4BMCRBND2CTCVB5 are similarly decided." ></td>
	<td class="line x" title="147:280	From this contingency table, CRs polarity value is determined." ></td>
	<td class="line x" title="148:280	Two ideas are examined for compari1078 son." ></td>
	<td class="line x" title="149:280	One is based on chi-square value and the other is based on Pointwise Mutual Information (PMI)." ></td>
	<td class="line x" title="150:280	Chi-square based polarity value The chi-square value is a statistical measure used to test the null hypothesis that, in our case, the probability of a candidate in positive sentences is equal to the probability in negative sentences." ></td>
	<td class="line x" title="151:280	Given Table 2, the chi-square value is calculated as follows." ></td>
	<td class="line x" title="152:280	AV BE B4CRB5BP CG DCBEB4CRBNBMCRB5 CG DDBEB4D4D3D7BND2CTCVB5 CUCUB4DCBNDDB5 A0 CM CUB4DCBNDDB5CV BE CM CUB4DCBNDDB5 Here, CM CUB4DCBNDDB5 is the expected value of CUB4DCBNDDB5 under the null hypothesis." ></td>
	<td class="line x" title="153:280	Although AV BE B4CRB5B4AL BCB5 indicates the strength of bias toward positive or negative sentences, its direction is not clear." ></td>
	<td class="line x" title="154:280	We determined polarity value so that it is greater than zero if CR appears in positive sentences more frequently than in negative sentences and otherwise it is less than zero." ></td>
	<td class="line x" title="155:280	C8CE AV BEB4CRB5BP AQ AV BE B4CRB5 CXCUC8B4CRCYD2CTCVB5 BOC8B4CRCYD4D3D7B5 A0AV BE B4CRB5 D3D8CWCTD6DBCXD7CT C8B4CRCYD4D3D7B5 is CRs probability in positive sentences, and C8B4CRCYD2CTCVB5 is that in negative sentences." ></td>
	<td class="line x" title="156:280	They are estimated by using Table 2." ></td>
	<td class="line x" title="157:280	C8B4CRCYD4D3D7B5 BP CUB4CRBND4D3D7B5 CUB4CRBND4D3D7B5B7CUB4BMCRBND4D3D7B5 C8B4CRCYD2CTCVB5 BP CUB4CRBND2CTCVB5 CUB4CRBND2CTCVB5B7CUB4BMCRBND2CTCVB5 PMI based polarity value Using PMI, the strength of association between CR and positive sentences (and negative sentences) is defined as follows (Church and Hanks, 1989)." ></td>
	<td class="line x" title="158:280	C8C5C1B4CRBND4D3D7B5 BP D0D3CV BE C8B4CRBND4D3D7B5 C8B4CRB5C8B4D4D3D7B5 C8C5C1B4CRBND2CTCVB5 BP D0D3CV BE C8B4CRBND2CTCVB5 C8B4CRB5C8B4D2CTCVB5 PMI based polarity value is defined as their difference." ></td>
	<td class="line oc" title="159:280	This idea is the same as (Turney, 2002)." ></td>
	<td class="line x" title="160:280	C8CE C8C5C1 B4CRB5 BP C8C5C1B4CRBND4D3D7B5 A0C8C5C1B4CRBND2CTCVB5 BP D0D3CV BE C8B4CRBND4D3D7B5BPC8B4D4D3D7B5 C8B4CRBND2CTCVB5BPC8B4D2CTCVB5 BP D0D3CV BE C8B4CRCYD4D3D7B5 C8B4CRCYD2CTCVB5 C8B4CRCYD4D3D7B5 and C8B4CRCYD2CTCVB5 are estimated in the same way as shown above." ></td>
	<td class="line x" title="161:280	C8CE C8C5C1 B4CRB5 is (log of) the ratio of CRs probability in positive sentences to that in negative sentences." ></td>
	<td class="line x" title="162:280	This formalization follows our intuition." ></td>
	<td class="line x" title="163:280	Similar to C8CE AV BEB4CRB5, C8CE C8C5C1 B4CRB5 is greater than zero if C8B4CRCYD2CTCVB5 BO C8B4CRCYD4D3D7B5, otherwise it is less than zero." ></td>
	<td class="line x" title="164:280	Selecting polar phrases By using polarity value and threshold AIB4BQ BCB5, it is decided whether a candidate CR is polar phrase or not." ></td>
	<td class="line x" title="165:280	If AI BO C8CEB4CRB5, the candidate is regarded as positive phrase." ></td>
	<td class="line x" title="166:280	Similarly, if C8CEB4CRB5 BO A0AI, it is regarded as negative phrase." ></td>
	<td class="line x" title="167:280	Otherwise, it is regarded as neutral." ></td>
	<td class="line x" title="168:280	Only positive and negative phrases are added to our lexicon." ></td>
	<td class="line x" title="169:280	By changing AI, the trade-off between precision and recall can be adjusted." ></td>
	<td class="line x" title="170:280	In order to avoid data sparseness problem, if both CUB4CRBND4D3D7B5 and CUB4CRBND2CTCVB5 are less than three, such candidates were ignored." ></td>
	<td class="line x" title="171:280	4 Related Work As described in Section 1, there have been two approaches to (semi-) unsupervised learning of polarity." ></td>
	<td class="line x" title="172:280	This Section introduces the two approaches and other related work." ></td>
	<td class="line x" title="173:280	4.1 Thesaurus based approach Kamps et al. built lexical network by linking synonyms provided by a thesaurus, and polarity was defined by the distance from seed words (good and bad) in the network (Kamps et al. , 2004)." ></td>
	<td class="line x" title="174:280	This method relies on a hypothesis that synonyms have the same polarity." ></td>
	<td class="line x" title="175:280	Hu and Liu used similar lexical network, but they considered not only synonyms but antonyms (Hu and Liu, 2004)." ></td>
	<td class="line x" title="176:280	Kim and Hovy proposed two probabilistic models to estimate the strength of polarity (Kim and Hovy, 2004)." ></td>
	<td class="line x" title="177:280	In their models, synonyms are used as features." ></td>
	<td class="line x" title="178:280	Esuli et al. utilized glosses of words to determine polarity (Esuli and Sebastiani, 2005; Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="179:280	Compared with our approach, the drawback of using thesaurus is the lack of scalability." ></td>
	<td class="line x" title="180:280	It is difficult to handle such words that are not contained in a thesaurus (e.g. newly-coined words or colloquial words)." ></td>
	<td class="line x" title="181:280	In addition, phrases cannot be handled because the entry of usual thesaurus is not phrase but word." ></td>
	<td class="line x" title="182:280	1079 4.2 Corpus based approach Another approach is based on an idea that polar phrases conveying the same polarity co-occur with each other in corpus." ></td>
	<td class="line pc" title="183:280	(Turney, 2002) is one of the most famous work that discussed learning polarity from corpus." ></td>
	<td class="line o" title="184:280	Turney determined polarity value 5 based on co-occurrence with seed words (excellent and poor)." ></td>
	<td class="line o" title="185:280	The cooccurrence is measured by the number of hits returned by a search engine." ></td>
	<td class="line oc" title="186:280	The polarity value proposed by (Turney, 2002) is as follows." ></td>
	<td class="line x" title="187:280	D0D3CV BE CWCXD8D7B4CRC6BXBTCACTDCCRCTD0D0CTD2D8B5CWCXD8D7B4D4D3D3D6B5 CWCXD8D7B4CRC6BXBTCAD4D3D3D6B5CWCXD8D7B4CTDCCRCTD0D0CTD2D8B5 CWCXD8D7B4D5B5 means the number of hits returned by a search engine when query D5 is issued." ></td>
	<td class="line x" title="188:280	C6BXBTCA means NEAR operator, which enables to retrieve only such documents that contain two queries within ten words." ></td>
	<td class="line x" title="189:280	Hatzivassiloglou and McKeown constructed lexical network and determine polarity of adjectives (Hatzivassiloglous and McKeown, 1997)." ></td>
	<td class="line x" title="190:280	Although this is similar to thesaurus based approach, they built the network from intra-sentential co-occurrence." ></td>
	<td class="line x" title="191:280	Takamura et al. built lexical network from not only such co-occurrence but other resources including thesaurus (Takamura et al. , 2005)." ></td>
	<td class="line x" title="192:280	They used spin model to predict polarity of words." ></td>
	<td class="line x" title="193:280	Popescu and Etzioni applied relaxation labeling to polarity identification (Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="194:280	This method iteratively assigns polarity to words by using various features including intra-sentential cooccurrence and synonyms of a thesaurus." ></td>
	<td class="line x" title="195:280	Kanayama and Nasukawa used both intraand inter-sentential co-occurrence to learn polarity of words and phrases (Kanayama and Nasukawa, 2006)." ></td>
	<td class="line x" title="196:280	Their method covers wider range of cooccurrence than other work such as (Hatzivassiloglous and McKeown, 1997)." ></td>
	<td class="line x" title="197:280	An interesting point of this work is that they discussed building domain oriented lexicon." ></td>
	<td class="line x" title="198:280	This is contrastive to other work including ours that addresses to build domain independent lexicon." ></td>
	<td class="line oc" title="199:280	In summary, the strength of our approach is to exploit extremely precise structural clues, and to use 5 Semantic Orientation in (Turney, 2002)." ></td>
	<td class="line o" title="200:280	massive collection of HTML documents to compensate for the low recall." ></td>
	<td class="line n" title="201:280	Although Turneys method also uses massive collection of HTML documents, his method does not make much of precision compared with our method." ></td>
	<td class="line n" title="202:280	As we will see in Section 5, our experimental result revealed that our method overwhelms Turneys method." ></td>
	<td class="line x" title="203:280	4.3 Other related work In some review sites, pros and cons are stated using such layout that we introduced in Section 2." ></td>
	<td class="line x" title="204:280	Some work examined the importance of such layout (Liu et al. , 2005; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="205:280	However, they regarded layout structures as clues specific to a certain review site." ></td>
	<td class="line x" title="206:280	They did not propose to use layout structure to extract polar sentences from arbitrary HTML documents." ></td>
	<td class="line x" title="207:280	Some studies addressed supervised approach to learning polarity of phrases (Wilson et al. , 2005; Takamura et al. , 2006)." ></td>
	<td class="line x" title="208:280	These are different from ours in a sense that they require manually tagged data." ></td>
	<td class="line x" title="209:280	Kobayashi et al. proposed a framework to reduce the cost of manually building lexicon (Kobayashi et al. , 2004)." ></td>
	<td class="line x" title="210:280	In the experiment, they compared the framework with fully manual method and investigated the effectiveness." ></td>
	<td class="line x" title="211:280	5 Experiment A test set consisting of 405 adjective phrases were created." ></td>
	<td class="line x" title="212:280	From the test set, we extract polar phrases by looking up our lexicon." ></td>
	<td class="line x" title="213:280	The result was evaluated through precision and recall 6 . 5.1 Setting The test set was created in the following manner." ></td>
	<td class="line x" title="214:280	500 adjective phrases were randomly extracted from the Web text." ></td>
	<td class="line x" title="215:280	Note that there is no overlap between our polar sentence corpus and this text." ></td>
	<td class="line x" title="216:280	After removing parsing error and duplicates, 405 unique adjective phrases were obtained." ></td>
	<td class="line x" title="217:280	Each phase was manually annotated with polarity tag (positive, negative and neutral), and we obtained 158 positive phrases, 150 negative phrases and 97 neutral phrases." ></td>
	<td class="line x" title="218:280	In order to check the reliability of annotation, another 6 The lexicon is available from http://www.tkl.iis.utokyo.ac.jp/kaji/polardic/." ></td>
	<td class="line x" title="219:280	1080 Table 3: The experimental result (chi-square)." ></td>
	<td class="line x" title="220:280	AI 0 102030405060 Precision/Recall Positive 76.4/92.4 84.0/86.7 84.1/83.5 86.2/79.1 88.7/74.7 86.7/65.8 86.7/65.8 Negative 68.5/84.0 65.5/63.3 64.3/60.0 62.7/57.3 81.1/51.3 80.0/48.0 80.0/48.0 # of polar words and phrases 9,670 2,056 1,047 698 533 423 335 Table 4: The experimental result (PMI)." ></td>
	<td class="line x" title="221:280	AI 0 0.5 1.0 1.5 2.0 2.5 3.0 Precision/Recall Positive 76.4/92.4 79.6/91.1 86.1/89.9 87.2/86.1 90.9/82.3 92.4/76.6 92.9/65.8 Negative 68.5/84.0 75.8/81.3 82.3/77.3 84.8/74.7 85.8/72.7 86.8/70.0 87.9/62.7 # of polar words and phrases 9,670 9,320 9,039 8,804 8,570 8,398 8,166 Table 5: The effect of data size (PMI, AI=1.0)." ></td>
	<td class="line x" title="222:280	size 1/20 1/15 1/10 1/5 1 Precision/Recall Positive 87.0/63.9 84.6/65.8 85.1/75.9 85.4/84.8 86.1/89.9 Negative 76.9/55.8 86.2/50.0 82.1/58.0 80.3/62.7 82.3/77.3 human judge annotated the same data." ></td>
	<td class="line x" title="223:280	The Kappa value between the two judges was 0.73, and we think the annotation is reliable." ></td>
	<td class="line x" title="224:280	From the test set, we extracted polar phrases by looking up our lexicon." ></td>
	<td class="line x" title="225:280	As for adjectives in the lexicon, partial match is allowed." ></td>
	<td class="line oc" title="226:280	For example, if the lexicon contains an adjective excellent, it matches every adjective phrase that includes excellent such as view-excellent etc. As a baseline, we built lexicon similarly by using polarity value of (Turney, 2002)." ></td>
	<td class="line x" title="227:280	As seed words, we used saikou (best) and saitei (worst)." ></td>
	<td class="line x" title="228:280	Some seeds were tested and these words achieved the best result." ></td>
	<td class="line x" title="229:280	As a search engine, we tested Google and our local engine, which indexes 150 millions Japanese documents." ></td>
	<td class="line xc" title="230:280	Its size is compatible to (Turney and Littman, 2002)." ></td>
	<td class="line x" title="231:280	Since Google does not support NEAR, we used AND." ></td>
	<td class="line x" title="232:280	Our local engine supports NEAR." ></td>
	<td class="line x" title="233:280	5.2 Results and discussion We evaluated the result of polar phrase extraction." ></td>
	<td class="line x" title="234:280	By changing the threshold AI, we investigated recallprecision curve (Figure 6 and 7)." ></td>
	<td class="line x" title="235:280	The detail is represented in Table 3 and 4." ></td>
	<td class="line x" title="236:280	The second/third row represents precision and recall of positive/negative phrases." ></td>
	<td class="line x" title="237:280	The fourth row is the size of the lexicon." ></td>
	<td class="line x" title="238:280	The Figures show that both of the proposed methods outperform the baselines." ></td>
	<td class="line x" title="239:280	The best F-measure was achieved by PMI (AI=1.0)." ></td>
	<td class="line n" title="240:280	Although Turneys method may be improved with minor configurations (e.g. using other seeds etc.), we think this results indicate the feasibility of the proposed method." ></td>
	<td class="line x" title="241:280	AlFigure 6: Recall-precision curve (positive phrases) though the size of lexicon is not surprisingly large, it would be possible to make the lexicon larger by using more HTML documents." ></td>
	<td class="line x" title="242:280	In addition, notice that we focus on only adjectives and adjective phrases." ></td>
	<td class="line x" title="243:280	Comparing the two proposed methods, PMI is always better than chi-square." ></td>
	<td class="line x" title="244:280	Especially, chi-square suffers from low recall, because the size of lexicon is extremely small." ></td>
	<td class="line x" title="245:280	For example, when the threshold is 60, the precision is 80% and the recall is 48% for negative phrases." ></td>
	<td class="line x" title="246:280	On the other hand, PMI would achieve the same precision when recall is around 80% (AI is between 0.5 and 1.0)." ></td>
	<td class="line xc" title="247:280	Turneys method did not work well although they reported 80% accuracy in (Turney and Littman, 2002)." ></td>
	<td class="line x" title="248:280	This is probably because our experimental setting is different." ></td>
	<td class="line x" title="249:280	Turney examined binary classification of positive and negative words, and we discussed extracting positive and negative phrases from the set of positive, negative and neutral phrases." ></td>
	<td class="line x" title="250:280	1081 Figure 7: Recall-precision curve (negative phrases) Error analysis revealed that most of the errors are related to neutral phrases." ></td>
	<td class="line x" title="251:280	For example, PMI (AI=1.0) extracted 48 incorrect polar phrases, and 37 of them were neutral phrases." ></td>
	<td class="line x" title="252:280	We think one reason is that we did not use neutral corpus." ></td>
	<td class="line x" title="253:280	It is one future work to exploit neutral corpus." ></td>
	<td class="line x" title="254:280	The importance of neutral category is also discussed in other literatures (Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="255:280	To further assess our method, we did two additional experiments." ></td>
	<td class="line x" title="256:280	In the first experiment, to investigate the effect of data size, the same experiment was conducted using 1/n (n=1,5,10,15,20) of the entire polar sentence corpus (Table 5)." ></td>
	<td class="line x" title="257:280	PMI (AI=1.0) was also used." ></td>
	<td class="line x" title="258:280	As the size of corpus increases, the performance becomes higher." ></td>
	<td class="line x" title="259:280	Especially, the recall is improved dramatically." ></td>
	<td class="line x" title="260:280	Therefore, the recall would be further improved using more corpus." ></td>
	<td class="line x" title="261:280	In the other experiment, the lexicon was evaluated directly so that we can examine polar words and phrases that are not in the test set." ></td>
	<td class="line x" title="262:280	We think it is difficult to fully assess low frequency words in the previous setting." ></td>
	<td class="line x" title="263:280	Two human judges assessed 200 unique polar words and phrases in the lexicon (PMI, AI=1.0)." ></td>
	<td class="line x" title="264:280	The average precision was 71.3% (Kappa value was 0.66)." ></td>
	<td class="line x" title="265:280	The precision is lower than the result in Table 4." ></td>
	<td class="line x" title="266:280	This result indicates that it is difficult to handle low frequency words." ></td>
	<td class="line x" title="267:280	The Table 6 illustrates examples of polar phrases and their polarity values." ></td>
	<td class="line x" title="268:280	We can see that both phrases and colloquial words such as uncool are appropriately learned." ></td>
	<td class="line x" title="269:280	They are difficult to handle for thesaurus based approach, because such words are not usually in thesaurus." ></td>
	<td class="line x" title="270:280	It is important to discuss how general our frameTable 6: Examples polar phrase C8CE AV BEB4CRB5 C8CEC8C5C1B4CRB5 kenkyoda (modest) 38.3 12.1 exiting (exiting) 13.5 10.4 more-sukunai (leak-small) 9.2 9.8 dasai (uncool) -2.9 -3.3 yakkaida (annoying) -11.9 -3.9 shomo-hayai (consumption-quick) -17.7 -4.4 work is. Although the lexico-syntactic patterns shown in Section 2 are specific to Japanese, we think that the idea of exploiting language structure is applicable to other languages including English." ></td>
	<td class="line x" title="271:280	Roughly speaking, the pattern we exploited can be translated into the advantage/weakness of something is to  in English." ></td>
	<td class="line x" title="272:280	It is worth pointing out that lexico-syntactic patterns have been widely used in English lexical acquisition (Hearst, 1992)." ></td>
	<td class="line x" title="273:280	Obviously, other parts of the proposed method does not depend on Japanese." ></td>
	<td class="line x" title="274:280	6 Conclusion In this paper, we explore to use structural clues that can extract polar sentences from Japanese HTML documents, and build lexicon from the extracted polar sentences." ></td>
	<td class="line x" title="275:280	The key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall." ></td>
	<td class="line x" title="276:280	In order to compensate for the low recall, we used massive collection of HTML documents." ></td>
	<td class="line x" title="277:280	Thus, we could prepare enough polar sentence corpus." ></td>
	<td class="line x" title="278:280	Experimental result demonstrated the feasibility of our approach." ></td>
	<td class="line x" title="279:280	Acknowledgement This work was supported by the Comprehensive Development of e-Society Foundation Software program of the Ministry of Education, Culture, Sports, Science and Technology, Japan." ></td>
	<td class="line x" title="280:280	We would like to thank Assistant Researcher Takayuki Tamura for his development of the Web crawler." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1037
Extracting Semantic Orientations of Phrases from Dictionary
Takamura, Hiroya;Inui, Takashi;Okumura, Manabu;"></td>
	<td class="line x" title="1:300	Proceedings of NAACL HLT 2007, pages 292299, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:300	c2007 Association for Computational Linguistics Extracting Semantic Orientations of Phrases from Dictionary Hiroya Takamura Precision and Intelligence Laboratory Tokyo Institute of Technology takamura@pi.titech.ac.jp Takashi Inui Integrated Research Institute Tokyo Institute of Technology inui@iri.titech.ac.jp Manabu Okumura Precision and Intelligence Laboratory Tokyo Institute of Technology oku@pi.titech.ac.jp Abstract We propose a method for extracting semantic orientations of phrases (pairs of an adjective and a noun): positive, negative, or neutral." ></td>
	<td class="line x" title="3:300	Given an adjective, the semantic orientation classification of phrases can be reduced to the classification of words." ></td>
	<td class="line x" title="4:300	We construct a lexical network by connecting similar/related words." ></td>
	<td class="line x" title="5:300	In the network, each node has one of the three orientation values and the neighboring nodes tend to have the same value." ></td>
	<td class="line x" title="6:300	We adopt the Potts model for the probability model of the lexical network." ></td>
	<td class="line x" title="7:300	For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs." ></td>
	<td class="line x" title="8:300	Unlike existing methods for phrase classification, the proposed method can classify phrases consisting of unseen words." ></td>
	<td class="line x" title="9:300	We also propose to use unlabeled data for a seed set of probability computation." ></td>
	<td class="line x" title="10:300	Empirical evaluation shows the effectiveness of the proposed method." ></td>
	<td class="line x" title="11:300	1 Introduction Technology for affect analysis of texts has recently gained attention in both academic and industrial areas." ></td>
	<td class="line x" title="12:300	It can be applied to, for example, a survey of new products or a questionnaire analysis." ></td>
	<td class="line x" title="13:300	Automatic sentiment analysis enables a fast and comprehensive investigation." ></td>
	<td class="line x" title="14:300	The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: positive or negative (desirable or undesirable)." ></td>
	<td class="line x" title="15:300	For example, the word beautiful is positive, while the word dirty is negative." ></td>
	<td class="line x" title="16:300	Many researchers have developed several methods for this purpose and obtained good results." ></td>
	<td class="line x" title="17:300	One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions, such as high+risk and light+laptop-computer." ></td>
	<td class="line x" title="18:300	Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the orientations of phrases as basic units for sentiment analysis." ></td>
	<td class="line x" title="19:300	We believe that we can use the obtained basic orientations of phrases for affect analysis of higher linguistic units such as sentences and documents." ></td>
	<td class="line x" title="20:300	A computational model for the semantic orientations of phrases has been proposed by Takamura et al.(2006)." ></td>
	<td class="line x" title="22:300	However, their method cannot deal with the words that did not appear in the training data." ></td>
	<td class="line x" title="23:300	The purpose of this paper is to propose a method for extracting semantic orientations of phrases, which is applicable also to expressions consisting of unseen words." ></td>
	<td class="line x" title="24:300	In our method, we regard this task as the noun classification problem for each adjective; the nouns that become respectively positive (negative, or neutral) when combined with a given adjective are distinguished from the other nouns." ></td>
	<td class="line x" title="25:300	We create a lexical network with words being nodes, by connecting two words if one of the two appears in the gloss of the other." ></td>
	<td class="line x" title="26:300	In the network, each node has one of the three orientation values and the neighboring nodes expectedly tend to have the same value." ></td>
	<td class="line x" title="27:300	For 292 example, the gloss of cost is a sacrifice, loss, or penalty and these words (cost, sacrifice, loss, and penalty) have the same orientation." ></td>
	<td class="line x" title="28:300	To capture this tendency of the network, we adopt the Potts model for the probability distribution of the lexical network." ></td>
	<td class="line x" title="29:300	For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs." ></td>
	<td class="line x" title="30:300	Information from seed words is diffused to unseen nouns on the network." ></td>
	<td class="line x" title="31:300	We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation." ></td>
	<td class="line x" title="32:300	Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model." ></td>
	<td class="line x" title="33:300	2 Related Work The semantic orientation classification of words has been pursued by several researchers." ></td>
	<td class="line x" title="34:300	Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al. , 2001; Kamps et al. , 2004; Takamura et al. , 2005; Esuli and Sebastiani, 2005)." ></td>
	<td class="line oc" title="35:300	Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification." ></td>
	<td class="line x" title="36:300	In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g. , phrase NEAR good) is used to determine the orientation." ></td>
	<td class="line x" title="37:300	Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences." ></td>
	<td class="line o" title="38:300	Their method is similar to Turneys in the sense that cooccurrence with seed words is used." ></td>
	<td class="line x" title="39:300	In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created." ></td>
	<td class="line o" title="40:300	The four methods above are based on context information." ></td>
	<td class="line x" title="41:300	In contrast, our method exploits the internal structure of the semantic orientations of phrases." ></td>
	<td class="line x" title="42:300	Wilson et al.(2005) worked on phrase-level semantic orientations." ></td>
	<td class="line x" title="44:300	They introduced a polarity shifter." ></td>
	<td class="line x" title="45:300	They manually created the list of polarity shifters." ></td>
	<td class="line x" title="46:300	Inui (2004) also proposed a similar idea." ></td>
	<td class="line x" title="47:300	Takamura et al.(2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs." ></td>
	<td class="line x" title="49:300	Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes." ></td>
	<td class="line x" title="50:300	The words that are similar in terms of semantic orientations, such as risk and mortality (i.e. , the positive orientation emerges when they are low), make a cluster in their model, which can be an automated version of Inuis or Wilson et al.s idea above." ></td>
	<td class="line x" title="51:300	However, their method cannot do anything for the words that did not appear in the labeled training data." ></td>
	<td class="line x" title="52:300	In this paper, we call their method the latent variable method (LVM)." ></td>
	<td class="line x" title="53:300	3 Potts Model If a variable can have more than two values and there is no ordering relation between the values, the network comprised of such variables is called Potts model (Wu, 1982)." ></td>
	<td class="line x" title="54:300	In this section, we explain the simplified mathematical model of Potts model, which is used for our task in Section 4." ></td>
	<td class="line x" title="55:300	The Potts system has been used as a mathematical model in several applications such as image restoration (Tanaka and Morita, 1996) and rumor transmission (Liu et al. , 2001)." ></td>
	<td class="line x" title="56:300	3.1 Introduction to the Potts Model Suppose a network consisting of nodes and weighted edges is given." ></td>
	<td class="line x" title="57:300	States of nodes are represented by c. The weight between i and j is represented by wij." ></td>
	<td class="line x" title="58:300	Let H(c) denote an energy function, which indicates a state of the whole network: H(c) = summationdisplay ij wij(ci,cj)+summationdisplay iL (ci,ai), (1) where  is a constant called the inverse-temperature, L is the set of the indices for the observed variables, ai is the state of each observed variable indexed by i, and  is a positive constant representing a weight on labeled data." ></td>
	<td class="line x" title="59:300	Function  returns 1 if two arguments are equal to each other, 0 otherwise." ></td>
	<td class="line x" title="60:300	The state is penalized if ci (i  L) is different from ai." ></td>
	<td class="line x" title="61:300	Using H(c), the probability distribution of the network is represented as P(c) = expH(c)}/Z, where Z is a normalization factor." ></td>
	<td class="line x" title="62:300	However, it is computationally difficult to exactly estimate the state of this network." ></td>
	<td class="line x" title="63:300	We resort to a 293 mean-field approximation method that is described by Nishimori (2001)." ></td>
	<td class="line x" title="64:300	In the method, P(c) is replaced by factorized function (c) = producttexti i(ci)." ></td>
	<td class="line x" title="65:300	Then we can obtain the function with the smallest value of the variational free energy: F(c) = summationdisplay c P(c)H(c) summationdisplay c P(c)logP(c) =  summationdisplay i summationdisplay ci i(ci)(ci,ai)  summationdisplay ij summationdisplay ci,cj i(ci)j(cj)wij(ci,cj)  summationdisplay i summationdisplay ci i(ci)logi(ci)." ></td>
	<td class="line x" title="66:300	(2) By minimizing F(c) under the condition that i,summationtext ci i(ci) = 1, we obtain the following fixed point equation for i  L: i(c) = exp((c,ai) +  summationtext j wijj(c))summationtext n exp((n,ai) +  summationtext j wijj(n))." ></td>
	<td class="line x" title="67:300	(3) The fixed point equation for i / L can be obtained by removing (c,ai) from above." ></td>
	<td class="line x" title="68:300	This fixed point equation is solved by an iterative computation." ></td>
	<td class="line x" title="69:300	In the actual implementation, we represent i with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996)." ></td>
	<td class="line x" title="70:300	Details on the Potts model and its computation can be found in the literature (Nishimori, 2001)." ></td>
	<td class="line x" title="71:300	After the computation, we obtain the functionproducttext i i(ci)." ></td>
	<td class="line x" title="72:300	When the number of classes is 2, the Potts model in this formulation is equivalent to the meanfield Ising model (Nishimori, 2001)." ></td>
	<td class="line x" title="73:300	3.2 Relation to Other Models This Potts model with the mean-field approximation has relation to several other models." ></td>
	<td class="line x" title="74:300	As is often discussed (Mackay, 2003), the minimization of the variational free energy (Equation (2)) is equivalent to the obtaining the factorized model that is most similar to the maximum likelihood model in terms of the Kullback-Leibler divergence." ></td>
	<td class="line x" title="75:300	The second term of Equation (2) is the entropy of the factorized function." ></td>
	<td class="line x" title="76:300	Hence the optimization problem to be solved here is a kind of the maximum entropy model with a penalty term, which corresponds to the first term of Equation (2)." ></td>
	<td class="line x" title="77:300	We can find a similarity also to the PageRank algorithm (Brin and Page, 1998), which has been applied also to natural language processing tasks (Mihalcea, 2004; Mihalcea, 2005)." ></td>
	<td class="line x" title="78:300	In the PageRank algorithm, the pagerank score ri is updated as ri = (1d) + d summationdisplay j wijrj, (4) where d is a constant (0  d  1)." ></td>
	<td class="line x" title="79:300	This update equation consists of the first term corresponding to random jump from an arbitrary node and the second term corresponding to the random walk from the neighboring node." ></td>
	<td class="line x" title="80:300	Let us derive the first order Taylor expansion of Equation (3)." ></td>
	<td class="line x" title="81:300	We use the equation for i / L and denote the denominator by Z, for simplicity." ></td>
	<td class="line x" title="82:300	Since expx  1+x, we obtain i(c) = exp( summationtext j wijj(c)) Z  1 +  summationtext j wijj(c) Z = 1Z  + Z  summationdisplay j wijj(c)." ></td>
	<td class="line x" title="83:300	(5) Equation (5) clearly has a quite similar form as Equation (4)." ></td>
	<td class="line x" title="84:300	Thus, the PageRank algorithm can be regarded as an approximation of our model." ></td>
	<td class="line x" title="85:300	Let us clarify the difference between the two algorithms." ></td>
	<td class="line x" title="86:300	The PageRank is designed for two-class classification, while the Potts model can be used for an arbitrary number of classes." ></td>
	<td class="line x" title="87:300	In this sense, the PageRank is an approximated Ising model." ></td>
	<td class="line x" title="88:300	The PageRank is applicable to asymmetric graphs, while the theory used in this paper is based on symmetric graphs." ></td>
	<td class="line x" title="89:300	4 Potts Model for Phrasal Semantic Orientations In this section, we explain our classification method, which is applicable also to the pairs consisting of an adjective and an unseen noun." ></td>
	<td class="line x" title="90:300	4.1 Construction of Lexical Networks We construct a lexical network, which Takamura et al.(2005) call the gloss network, by linking two words if one word appears in the gloss of the other word." ></td>
	<td class="line x" title="92:300	Each link belongs to one of two groups: 294 the same-orientation links SL and the differentorientation links DL." ></td>
	<td class="line x" title="93:300	If a negation word (e.g. , nai, for Japanese) follows a word in the gloss of the other word, the link is a different-orientation link." ></td>
	<td class="line x" title="94:300	Otherwise the links is a same-orientation link1." ></td>
	<td class="line x" title="95:300	We next set weights W = (wij) to links : wij =    1 d(i)d(j) (lij  SL)  1d(i)d(j) (lij  DL) 0 otherwise, (6) where lij denotes the link between word i and word j, and d(i) denotes the degree of word i, which means the number of words linked with word i. Two words without connections are regarded as being connected by a link of weight 0." ></td>
	<td class="line x" title="96:300	4.2 Classification of Phrases Takamura et al.(2005) used the Ising model to extract semantic orientations of words (not phrases)." ></td>
	<td class="line x" title="98:300	We extend their idea and use the Potts model to extract semantic orientations of phrasal expressions." ></td>
	<td class="line x" title="99:300	Given an adjective, the decision remaining to be made in classification of phrasal expressions concerns nouns." ></td>
	<td class="line x" title="100:300	We therefore estimate the state of the nodes on the lexical network for each adjective." ></td>
	<td class="line x" title="101:300	The nouns paring with the given adjective in the training data are regarded as seed words, which we call seen words, while the words that did not appear in the training data are referred to as unseen words." ></td>
	<td class="line x" title="102:300	We use the mean-field method to estimate the state of the system." ></td>
	<td class="line x" title="103:300	If the probability i(c) of a variable being positive (negative, neutral) is the highest of the three classes, then the word corresponding to the variable is classified as a positive (negative, neutral) word." ></td>
	<td class="line x" title="104:300	We explain the reason why we use the Potts model instead of the Ising model." ></td>
	<td class="line x" title="105:300	While only two classes (i.e. , positive and negative) can be modeled by the Ising model, three classes (i.e. , positive, negative and neutral) can be modelled by the Potts model." ></td>
	<td class="line x" title="106:300	For the semantic orientations of words, all the words are sorted in the order of the average orientation value, equivalently the probability of the word being positive." ></td>
	<td class="line x" title="107:300	Therefore, even if the neutral class is 1For English data, a negation should precede a word, in order for the corresponding link to be a different-orientation link." ></td>
	<td class="line x" title="108:300	not explicitly incorporated, we can manually determine two thresholds that define respectively the positive/neutral and negative/neutral boundaries." ></td>
	<td class="line x" title="109:300	For the semantic orientations of phrasal expressions, however, it is impractical to manually determine the thresholds for each of the numerous adjectives." ></td>
	<td class="line x" title="110:300	Therefore, we have to incorporate the neutral class using the Potts model." ></td>
	<td class="line x" title="111:300	For some adjectives, the semantic orientation is constant regardless of the nouns." ></td>
	<td class="line x" title="112:300	We need not use the Potts model for those unambiguous adjectives." ></td>
	<td class="line x" title="113:300	We thus propose the following two-step classification procedure for a given noun-adjective pair < n,a >." ></td>
	<td class="line x" title="114:300	1." ></td>
	<td class="line x" title="115:300	if the semantic orientation of all the instances with a in L is c, then classify < n,a > into c. 2." ></td>
	<td class="line x" title="116:300	otherwise, use the Potts model." ></td>
	<td class="line x" title="117:300	We can also construct a probability model for each noun to deal with unseen adjectives." ></td>
	<td class="line x" title="118:300	However, we focus on the unseen nouns in this paper, because our dataset has many more nouns than adjectives." ></td>
	<td class="line x" title="119:300	4.3 Hyper-parameter Prediction The performance of the proposed method largely depends on the value of hyper-parameter ." ></td>
	<td class="line x" title="120:300	In order to make the method more practical, we propose a criterion for determining its value." ></td>
	<td class="line x" title="121:300	Takamura et al.(2005) proposed two kinds of criteria." ></td>
	<td class="line x" title="123:300	One of the two criteria is an approximated leave-one-out error rate and can be used only when a large labeled dataset is available." ></td>
	<td class="line x" title="124:300	The other is a notion from statistical physics, that is, magnetization: m = summationdisplay i xi/N." ></td>
	<td class="line x" title="125:300	(7) At a high temperature, variables are randomly oriented (paramagnetic phase, m  0)." ></td>
	<td class="line x" title="126:300	At a low temperature, most of the variables have the same direction (ferromagnetic phase, m negationslash= 0)." ></td>
	<td class="line x" title="127:300	It is known that at some intermediate temperature, ferromagnetic phase suddenly changes to paramagnetic phase." ></td>
	<td class="line x" title="128:300	This phenomenon is called phase transition." ></td>
	<td class="line x" title="129:300	Slightly before the phase transition, variables are locally polarized; strongly connected nodes have the same polarity, but not in a global way." ></td>
	<td class="line x" title="130:300	Intuitively, the state of the lexical network is locally polarized." ></td>
	<td class="line x" title="131:300	295 Therefore, they calculate values of m with several different values of  and select the value just before the phase transition." ></td>
	<td class="line x" title="132:300	Since we cannot expect a large labeled dataset to be available for each adjective, we use not the approximated leave-one-out error rate, but the magnetization-like criterion." ></td>
	<td class="line x" title="133:300	However, the magnetization above is defined for the Ising model." ></td>
	<td class="line x" title="134:300	We therefore consider that the phase transition has occurred, if a certain class c begins to be favored all over the system." ></td>
	<td class="line x" title="135:300	In practice, when the maximum of the spatial averages of the approximated probabilities maxcsummationtexti i(c)/N exceeds a threshold during increasing , we consider that the phase transition has occurred." ></td>
	<td class="line x" title="136:300	We select the value of  slightly before the phase transition." ></td>
	<td class="line x" title="137:300	4.4 Enlarging Seed Word Set We usually have only a few seed words for a given adjective." ></td>
	<td class="line x" title="138:300	Enlarging the set of seed words will increase the classification performance." ></td>
	<td class="line x" title="139:300	Therefore, we automatically classify unlabeled pairs by means of an existing method and use the classified instances as seeds." ></td>
	<td class="line x" title="140:300	As an existing classifier, we use LVM." ></td>
	<td class="line x" title="141:300	Their model can classify instances that consist of a seen noun and a seen adjective, but are unseen as a pair." ></td>
	<td class="line x" title="142:300	Although we could classify and use all the nouns that appeared in the training data (with an adjective which is different from the given one), we do not adopt such an alternative, because it will incorporate even non-collocating pairs such as green+idea into seeds, resulting in possible degradation of classification performance." ></td>
	<td class="line x" title="143:300	Therefore, we sample unseen pairs consisting of a seen noun and a seen adjective from a corpus, classify the pairs with the latent variable model, and add them to the seed set." ></td>
	<td class="line x" title="144:300	The enlarged seed set consists of pairs used in newspaper articles and does not include non-collocating pairs." ></td>
	<td class="line x" title="145:300	5 Experiments 5.1 Dataset We extracted pairs of a noun (subject) and an adjective (predicate), from Mainichi newspaper articles (1995) written in Japanese, and annotated the pairs with semantic orientation tags : positive, neutral or negative." ></td>
	<td class="line x" title="146:300	We thus obtained the labeled dataset consisting of 12066 pair instances (7416 different pairs)." ></td>
	<td class="line x" title="147:300	The dataset contains 4459 negative instances, 4252 neutral instances, and 3355 positive instances." ></td>
	<td class="line x" title="148:300	The number of distinct nouns is 4770 and the number of distinct adjectives is 384." ></td>
	<td class="line x" title="149:300	To check the interannotator agreement between two annotators, we calculated  statistics, which was 0.6402." ></td>
	<td class="line x" title="150:300	This value is allowable, but not quite high." ></td>
	<td class="line x" title="151:300	However, positivenegative disagreement is observed for only 0.7% of the data." ></td>
	<td class="line x" title="152:300	In other words, this statistics means that the task of extracting neutral examples, which has hardly been explored, is intrinsically difficult." ></td>
	<td class="line x" title="153:300	We should note that the judgment in annotation depends on which perspective the annotator takes; high+salary is positive from employees perspective, but negative from employers perspective." ></td>
	<td class="line x" title="154:300	The annotators are supposed to take a perspective subjectively." ></td>
	<td class="line x" title="155:300	Our attempt is to imitate annotators decision." ></td>
	<td class="line x" title="156:300	To construct a classifier that matches the decision of the average person, we also have to address how to create an average corpus." ></td>
	<td class="line x" title="157:300	We do not pursue this issue because it is out of the scope of the paper." ></td>
	<td class="line x" title="158:300	As unlabeled data, we extracted approximately 65,000 pairs for each iteration of the 10-fold crossvalidation, from the same news source." ></td>
	<td class="line x" title="159:300	The average number of seed nouns for each ambiguous adjective was respectively 104 in the labeled seed set and 264 in the labeled+unlabeled seed set." ></td>
	<td class="line x" title="160:300	Please note that these figures are counted for only ambiguous adjectives." ></td>
	<td class="line x" title="161:300	Usually ambiguous adjectives are more frequent than unambiguous adjectives." ></td>
	<td class="line x" title="162:300	5.2 Experimental Settings We employ 10-fold cross-validation to obtain the averaged classification accuracy." ></td>
	<td class="line x" title="163:300	We split the data such that there is no overlapping pair (i.e. , any pair in the training data does not appear in the test data)." ></td>
	<td class="line x" title="164:300	Hyperparameter  was set to 1000, which is very large since we regard the labels in the seed set is reliable." ></td>
	<td class="line x" title="165:300	For the seed words added by the classifier, lower  can be better." ></td>
	<td class="line x" title="166:300	Determining a good value for  is regarded as future work." ></td>
	<td class="line x" title="167:300	Hyperparameter  is automatically selected from 2Although Kanayama and Nasukawa (2006) that  for their dataset similar to ours was 0.83, this value cannot be directly compared with our value because their dataset includes both individual words and pairs of words." ></td>
	<td class="line x" title="168:300	296 {0.1, 0.2, , 2.5} for each adjective and each fold of the cross-validation using the prediction method described in Section 4.3." ></td>
	<td class="line x" title="169:300	5.3 Results The results of the classification experiments are summarized in Table 1." ></td>
	<td class="line x" title="170:300	The proposed method succeeded in classifying, with approximately 65% in accuracy, those phrases consisting of an ambiguous adjective and an unseen noun, which could not be classified with existing computational models such as LVM." ></td>
	<td class="line x" title="171:300	Incorporation of unlabeled data improves accuracy by 15.5 points for pairs consisting of a seen noun and an ambiguous adjective, and by 3.5 points for pairs consisting of an unseen noun and an ambiguous adjective, approximately." ></td>
	<td class="line x" title="172:300	The reason why the former obtained high increase is that pairs with an ambiguous adjective3 are usually frequent and likely to be found in the added unlabeled dataset." ></td>
	<td class="line x" title="173:300	If we regard this classification task as binary classification problems where we are to classify instances into one class or not, we obtain three accuracies: 90.76% for positive, 81.75% for neutral, and 86.85% for negative." ></td>
	<td class="line x" title="174:300	This results suggests the identification of neutral instances is relatively difficult." ></td>
	<td class="line x" title="175:300	Next we compare the proposed method with LVM." ></td>
	<td class="line x" title="176:300	The latent variable method is applicable only to instance pairs consisting of an adjective and a seen noun." ></td>
	<td class="line x" title="177:300	Therefore, we computed the accuracy for 6586 instances using the latent variable method and obtained 80.76 %." ></td>
	<td class="line x" title="178:300	The corresponding accuracy by our method was 80.93%." ></td>
	<td class="line x" title="179:300	This comparison shows that our method is better than or at least comparable to the latent variable method." ></td>
	<td class="line x" title="180:300	However, we have to note that this accuracy of the proposed method was computed using the unlabeled data classified by the latent variable method." ></td>
	<td class="line x" title="181:300	5.4 Discussion There are still 3320 (=12066-8746) word pairs which could not be classified, because there are no entries for those words in the dictionary." ></td>
	<td class="line x" title="182:300	However, the main cause of this problem is word segmenta3Seen nouns are observed in both the training and the test datasets because they are frequent." ></td>
	<td class="line x" title="183:300	Ambiguous adjectives are often-used adjectives such as large, small, high, and low." ></td>
	<td class="line x" title="184:300	tion, since many compound nouns and exceedinglysubdivided morphemes are not in dictionaries." ></td>
	<td class="line x" title="185:300	An appropriate mapping from the words found in corpus to entries of a dictionary will solve this problem." ></td>
	<td class="line x" title="186:300	We found a number of proper nouns, many of which are not in the dictionary." ></td>
	<td class="line x" title="187:300	By estimating a class of a proper noun and finding the words that matches the class in the dictionary, we can predict the semantic orientations of the proper noun based on the orientations of the found words." ></td>
	<td class="line x" title="188:300	In order to see the overall tendency of errors, we calculated the confusion matrices both for pairs of an ambiguous adjective and a seen noun, and for pairs of an ambiguous adjective and an unseen noun (Table 2)." ></td>
	<td class="line x" title="189:300	The proposed method works quite well for positive/negative classification, though it finds still some difficulty in correctly classifying neutral instances even after enhanced with the unlabeled data." ></td>
	<td class="line x" title="190:300	In order to qualitatively evaluate the method, we list several word pairs below." ></td>
	<td class="line x" title="191:300	These word pairs are classified by the Potts model with the labeled+unlabeled seed set." ></td>
	<td class="line x" title="192:300	All nouns are unseen; they did not appear in the original training dataset." ></td>
	<td class="line x" title="193:300	Please note again that the actual data is Japanese." ></td>
	<td class="line x" title="194:300	positive instances noun adjective cost low basic price low loss little intelligence high educational background high contagion not-happening version new cafe many salary high commission low negative instances noun adjective damage heavy chance little terrorist many trouble many variation little capacity small salary low disaster many disappointment big knowledge little For example, although both salary and commission are kinds of money, our method captures 297 Table 1: Classification accuracies (%) for various seed sets and test datasets." ></td>
	<td class="line x" title="195:300	Labeled seed set corresponds to the set of manually labeled pairs." ></td>
	<td class="line x" title="196:300	Labeled+unlabeled seed set corresponds to the union of labeled seed set and the set of pairs labeled by LVM." ></td>
	<td class="line x" title="197:300	Seen nouns for test are the nouns that appeared in the training data, while unseen nouns are the nouns that did not appear in the training dataset." ></td>
	<td class="line x" title="198:300	Please note that seen pairs are excluded from the test data." ></td>
	<td class="line x" title="199:300	Unambiguous adjectives corresponds to the pairs with an adjective which has a unique orientation in the original training dataset, while ambiguous adjectives corresponds to the pairs with an adjective which has more than one orientation in the original training dataset." ></td>
	<td class="line x" title="200:300	seed\test seen nouns unseen nouns total labeled 68.24 73.70 69.59 (4494/6586) (1592/2160) (6086/8746) unambiguous ambiguous unambiguous ambiguous 98.15 61.65 94.85 61.85 (1166/1188) (3328/5398) (736/776) (856/1384) labeled+unlabeled 80.93 75.88 79.68 (5330/6586) (1639/2160) (6969/8746) unambiguous ambiguous unambiguous ambiguous 98.15 77.14 94.85 65.25 (1166/1188) (4164/5398) (736/776) (903/1384) Table 2: Confusion matrices of classification result with labeled+unlabeled seed set Potts model seen nouns unseen nouns positive neutral negative sum positive neutral negative sum positive 964 254 60 1278 126 84 30 240 Gold standard neutral 198 1656 286 2140 60 427 104 591 negative 39 397 1544 1980 46 157 350 553 sum 1201 2307 1890 5398 232 668 484 1384 the difference between them; high salary is positive, while low (cheap) commission is also positive." ></td>
	<td class="line x" title="201:300	6 Conclusion We proposed a method for extracting semantic orientations of phrases (pairs of an adjective and a noun)." ></td>
	<td class="line x" title="202:300	For each adjective, we constructed a Potts system, which is actually a lexical network extracted from glosses in a dictionary." ></td>
	<td class="line x" title="203:300	We empirically showed that the proposed method works well in terms of classification accuracy." ></td>
	<td class="line x" title="204:300	Future work includes the following:  We assumed that each word has a semantic orientation." ></td>
	<td class="line x" title="205:300	However, word senses and subjectivity have strong interaction (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="206:300	 The value of  must be properly set, because lower  can be better for the seed words added by the classifier,  To address word-segmentation problem discussed in Section 5.3, we can utilize the fact that the heads of compound nouns often inherit the property determining the semantic orientation when combined with an adjective." ></td>
	<td class="line x" title="207:300	 The semantic orientations of pairs consisting of a proper noun will be estimated from the named entity classes of the proper nouns such as person name and organization." ></td>
	<td class="line x" title="208:300	298 References Faye Baron and Graeme Hirst." ></td>
	<td class="line x" title="209:300	2004." ></td>
	<td class="line x" title="210:300	Collocations as cues to semantic orientation." ></td>
	<td class="line x" title="211:300	In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications." ></td>
	<td class="line x" title="212:300	Sergey Brin and Lawrence Page." ></td>
	<td class="line x" title="213:300	1998." ></td>
	<td class="line x" title="214:300	The anatomy of a large-scale hypertextual Web search engine." ></td>
	<td class="line x" title="215:300	Computer Networks and ISDN Systems, 30(17):107117." ></td>
	<td class="line x" title="216:300	Andrea Esuli and Fabrizio Sebastiani." ></td>
	<td class="line x" title="217:300	2005." ></td>
	<td class="line x" title="218:300	Determining the semantic orientation of terms through gloss analysis." ></td>
	<td class="line x" title="219:300	In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (CIKM05), pages 617624." ></td>
	<td class="line x" title="220:300	Vasileios Hatzivassiloglou and Kathleen R. McKeown." ></td>
	<td class="line x" title="221:300	1997." ></td>
	<td class="line x" title="222:300	Predicting the semantic orientation of adjectives." ></td>
	<td class="line x" title="223:300	In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 174181." ></td>
	<td class="line x" title="224:300	Takashi Inui." ></td>
	<td class="line x" title="225:300	2004." ></td>
	<td class="line x" title="226:300	Acquiring Causal Knowledge from Text Using Connective Markers." ></td>
	<td class="line x" title="227:300	Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science and Technology." ></td>
	<td class="line x" title="228:300	Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke." ></td>
	<td class="line x" title="229:300	2004." ></td>
	<td class="line x" title="230:300	Using wordnet to measure semantic orientation of adjectives." ></td>
	<td class="line x" title="231:300	In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04), volume IV, pages 11151118." ></td>
	<td class="line x" title="232:300	Hiroshi Kanayama and Tetsuya Nasukawa." ></td>
	<td class="line x" title="233:300	2006." ></td>
	<td class="line x" title="234:300	Fully automatic lexicon expansion for domain-oriented sentiment analysis." ></td>
	<td class="line x" title="235:300	In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP06), pages 355363." ></td>
	<td class="line x" title="236:300	Nozomi Kobayashi, Takashi Inui, and Kentaro Inui." ></td>
	<td class="line x" title="237:300	2001." ></td>
	<td class="line x" title="238:300	Dictionary-based acquisition of the lexical knowledge for p/n analysis (in Japanese)." ></td>
	<td class="line x" title="239:300	In Proceedings of Japanese Society for Artificial Intelligence, SLUD-33, pages 4550." ></td>
	<td class="line x" title="240:300	Zhongzhu Liu, Jun Luo, and Chenggang Shao." ></td>
	<td class="line x" title="241:300	2001." ></td>
	<td class="line x" title="242:300	Potts model for exaggeration of a simple rumor transmitted by recreant rumormongers." ></td>
	<td class="line x" title="243:300	Physical Review E, 64:046134,1046134,9." ></td>
	<td class="line x" title="244:300	David J. C. Mackay." ></td>
	<td class="line x" title="245:300	2003." ></td>
	<td class="line x" title="246:300	Information Theory, Inference and Learning Algorithms." ></td>
	<td class="line x" title="247:300	Cambridge University Press." ></td>
	<td class="line x" title="248:300	Mainichi." ></td>
	<td class="line x" title="249:300	1995." ></td>
	<td class="line x" title="250:300	Mainichi Shimbun CD-ROM version." ></td>
	<td class="line x" title="251:300	Rada Mihalcea." ></td>
	<td class="line x" title="252:300	2004." ></td>
	<td class="line x" title="253:300	Graph-based ranking algorithms for sentence extraction, applied to text summarization." ></td>
	<td class="line x" title="254:300	In The Companion Volume to the Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, (ACL04), pages 170173." ></td>
	<td class="line x" title="255:300	Rada Mihalcea." ></td>
	<td class="line x" title="256:300	2005." ></td>
	<td class="line x" title="257:300	Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling." ></td>
	<td class="line x" title="258:300	In Proceedings of the Joint Conference on Human Language Technology / Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 411418." ></td>
	<td class="line x" title="259:300	Hidetoshi Nishimori." ></td>
	<td class="line x" title="260:300	2001." ></td>
	<td class="line x" title="261:300	Statistical Physics of Spin Glasses and Information Processing." ></td>
	<td class="line x" title="262:300	Oxford University Press." ></td>
	<td class="line x" title="263:300	Frank Z. Smadja." ></td>
	<td class="line x" title="264:300	1993." ></td>
	<td class="line x" title="265:300	Retrieving collocations from text: Xtract." ></td>
	<td class="line x" title="266:300	Computational Linguistics, 19(1):143177." ></td>
	<td class="line x" title="267:300	Hiroya Takamura, Takashi Inui, and Manabu Okumura." ></td>
	<td class="line x" title="268:300	2005." ></td>
	<td class="line x" title="269:300	Extracting semantic orientations of words using spin model." ></td>
	<td class="line x" title="270:300	In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL05), pages 133140." ></td>
	<td class="line x" title="271:300	Hiroya Takamura, Takashi Inui, and Manabu Okumura." ></td>
	<td class="line x" title="272:300	2006." ></td>
	<td class="line x" title="273:300	Latent variable models for semantic orientations of phrases." ></td>
	<td class="line x" title="274:300	In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL06)." ></td>
	<td class="line x" title="275:300	Kazuyuki Tanaka and Tohru Morita." ></td>
	<td class="line x" title="276:300	1996." ></td>
	<td class="line x" title="277:300	Application of cluster variation method to image restoration problem." ></td>
	<td class="line x" title="278:300	In Theory and Applications of the Cluster Variation and Path Probability Methods, pages 353373." ></td>
	<td class="line x" title="279:300	Plenum Press, New York." ></td>
	<td class="line x" title="280:300	Peter D. Turney and Michael L. Littman." ></td>
	<td class="line x" title="281:300	2003." ></td>
	<td class="line x" title="282:300	Measuring praise and criticism: Inference of semantic orientation from association." ></td>
	<td class="line x" title="283:300	ACM Transactions on Information Systems, 21(4):315346." ></td>
	<td class="line x" title="284:300	Peter D. Turney." ></td>
	<td class="line x" title="285:300	2002." ></td>
	<td class="line x" title="286:300	Thumbs up or thumbs down?" ></td>
	<td class="line x" title="287:300	semantic orientation applied to unsupervised classification of reviews." ></td>
	<td class="line x" title="288:300	In Proceedings 40th Annual Meeting of the Association for Computational Linguistics (ACL02), pages 417424." ></td>
	<td class="line x" title="289:300	Janyce M. Wiebe and Rada Mihalcea." ></td>
	<td class="line x" title="290:300	2006." ></td>
	<td class="line x" title="291:300	Word sense and subjectivity." ></td>
	<td class="line x" title="292:300	In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL06), pages 10651072." ></td>
	<td class="line x" title="293:300	Theresa Wilson, Janyce Wiebe, and Paul Hoffmann." ></td>
	<td class="line x" title="294:300	2005." ></td>
	<td class="line x" title="295:300	Recognizing contextual polarity in phrase-level sentiment analysis." ></td>
	<td class="line x" title="296:300	In Proceedings of joint conference on Human Language Technology / Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP05), pages 347354." ></td>
	<td class="line x" title="297:300	Fa-Yueh Wu." ></td>
	<td class="line x" title="298:300	1982." ></td>
	<td class="line x" title="299:300	The potts model." ></td>
	<td class="line x" title="300:300	Reviews of Modern Physics, 54(1):235268 ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1038
Multiple Aspect Ranking Using the Good Grief Algorithm
Snyder, Benjamin;Barzilay, Regina;"></td>
	<td class="line x" title="1:212	Proceedings of NAACL HLT 2007, pages 300307, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:212	c2007 Association for Computational Linguistics Multiple Aspect Ranking using the Good Grief Algorithm Benjamin Snyder and Regina Barzilay Computer Science and Arti cial Intelligence Laboratory Massachusetts Institute of Technology {bsnyder,regina}@csail.mit.edu Abstract We address the problem of analyzing multiple related opinions in a text." ></td>
	<td class="line x" title="3:212	For instance, in a restaurant review such opinions may include food, ambience and service." ></td>
	<td class="line x" title="4:212	We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect." ></td>
	<td class="line x" title="5:212	We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks." ></td>
	<td class="line x" title="6:212	This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast." ></td>
	<td class="line x" title="7:212	We prove that our agreementbased joint model is more expressive than individual ranking models." ></td>
	<td class="line x" title="8:212	Our empirical results further con rm the strength of the model: the algorithm provides signi cant improvement over both individual rankers and a state-of-the-art joint ranking model." ></td>
	<td class="line oc" title="9:212	1 Introduction Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line n" title="10:212	However, multiple opinions on related matters are often intertwined throughout a text." ></td>
	<td class="line x" title="11:212	For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant." ></td>
	<td class="line x" title="12:212	Rather than lumping these aspects into a single score, we would like to capture each aspect of the writers opinion separately, thereby providing a more ne-grained view of opinions in the review." ></td>
	<td class="line x" title="13:212	To this end, we aim to predict a set of numeric ranks that re ects the users satisfaction for each aspect." ></td>
	<td class="line x" title="14:212	In the example above, we would assign a numeric rank from 1-5 for each of: food quality, service, and ambience." ></td>
	<td class="line x" title="15:212	A straightforward approach to this task would be to rank1 the text independently for each aspect, using standard ranking techniques such as regression or classi cation." ></td>
	<td class="line x" title="16:212	However, this approach fails to exploit meaningful dependencies between users judgments across different aspects." ></td>
	<td class="line x" title="17:212	Knowledge of these dependencies can be crucial in predicting accurate ranks, as a users opinions on one aspect can in uence his or her opinions on others." ></td>
	<td class="line x" title="18:212	The algorithm presented in this paper models the dependencies between different labels via the agreement relation." ></td>
	<td class="line x" title="19:212	The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction." ></td>
	<td class="line x" title="20:212	Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction." ></td>
	<td class="line x" title="21:212	The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 1In this paper, ranking refers to the task of assigning an integer from 1 tok to each instance." ></td>
	<td class="line x" title="22:212	This task is sometimes referred to as ordinal regression (Crammer and Singer, 2001) and rating prediction (Pang and Lee, 2005)." ></td>
	<td class="line x" title="23:212	300 equal." ></td>
	<td class="line x" title="24:212	The Good Grief decoding algorithm predicts a set of ranks one for each aspect which maximally satisfy the preferences of the individual rankers and the agreement model." ></td>
	<td class="line x" title="25:212	For example, if the agreement model predicts consensus but the individual rankers select ranks h5,5,4i, then the decoder decides whether to trust the the third ranker, or alter its prediction and outputh5,5,5ito be consistent with the agreement prediction." ></td>
	<td class="line x" title="26:212	To obtain a model well-suited for this decoding, we also develop a joint training method that conjoins the training of multiple aspect models." ></td>
	<td class="line x" title="27:212	We demonstrate that the agreement-based joint model is more expressive than individual ranking models." ></td>
	<td class="line x" title="28:212	That is, every training set that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model." ></td>
	<td class="line x" title="29:212	In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference." ></td>
	<td class="line x" title="30:212	Our experimental results further con rm the strength of the Good Grief model." ></td>
	<td class="line x" title="31:212	Our model signi cantly outperforms individual ranking models as well as a stateof-the-art joint ranking model." ></td>
	<td class="line oc" title="32:212	2 Related Work Sentiment Classi cation Traditionally, categorization of opinion texts has been cast as a binary classication task (Pang et al. , 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al. , 2003)." ></td>
	<td class="line x" title="33:212	More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale." ></td>
	<td class="line x" title="34:212	While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text." ></td>
	<td class="line x" title="35:212	Our work generalizes this setting to the problem of analyzing multiple opinions or multiple aspects of an opinion." ></td>
	<td class="line x" title="36:212	Since multiple opinions in a single text are related, it is insuf cient to treat them as separate single-aspect ranking tasks." ></td>
	<td class="line x" title="37:212	This motivates our exploration of a new method for joint multiple aspect ranking." ></td>
	<td class="line x" title="38:212	Ranking The ranking, or ordinal regression, problem has been extensivly studied in the Machine Learning and Information Retrieval communities." ></td>
	<td class="line x" title="39:212	In this section we focus on two online ranking methods which form the basis of our approach." ></td>
	<td class="line x" title="40:212	The rst is a model proposed by Crammer and Singer (2001)." ></td>
	<td class="line x" title="41:212	The task is to predict a rank y 2f1,,kg for every input x 2 Rn." ></td>
	<td class="line x" title="42:212	Their model stores a weight vector w 2 Rn and a vector of increasing boundaries b0 = 1 b1 bk1 bk = 1 which divide the real line into k segments, one for each possible rank." ></td>
	<td class="line x" title="43:212	The model rst scores each input with the weight vector: score(x) = w x. Finally, the model locates score(x) on the real line and returns the appropriate rank as indicated by the boundaries." ></td>
	<td class="line x" title="44:212	Formally, the model returns the rank r such that br1 score(x) < br." ></td>
	<td class="line x" title="45:212	The model is trained with the Perceptron Ranking algorithm (or PRank algorithm ), which reacts to incorrect predictions on the training set by updating the weight and boundary vectors." ></td>
	<td class="line x" title="46:212	The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database." ></td>
	<td class="line x" title="47:212	An extension of this model is provided by Basilico and Hofmann (2004) in the context of collaborative ltering." ></td>
	<td class="line x" title="48:212	Instead of training a separate model for each user, Basilico and Hofmann train a joint ranking model which shares a set of boundaries across all users." ></td>
	<td class="line x" title="49:212	In addition to these shared boundaries, userspeci c weight vectors are stored." ></td>
	<td class="line x" title="50:212	To compute the score for input x and user i, the weight vectors for all users are employed: scorei(x) = w[i] x + summationdisplay j sim(i,j)(w[j] x) (1) where 0 sim(i,j) 1 is the cosine similarity between users i and j, computed on the entire training set." ></td>
	<td class="line x" title="51:212	Once the score has been computed, the prediction rule follows that of the PRanking model." ></td>
	<td class="line x" title="52:212	The model is trained using the PRank algorithm, with the exception of the new de nition for the scoring function.2 While this model shares information between the different ranking problems, it fails to explicitly model relations between the rank predictions." ></td>
	<td class="line x" title="53:212	In contrast, our algorithm uses an agreement model to learn such relations and inform joint predictions." ></td>
	<td class="line x" title="54:212	2In the notation of Basilico and Hofmann (2004), this definition of scorei(x) corresponds to the kernel K = (KidU + KcoU )KatX . 301 3 The Algorithm The goal of our algorithm is to nd a rank assignment that is consistent with predictions of individual rankers and the agreement model." ></td>
	<td class="line x" title="55:212	To this end, we develop the Good Grief decoding procedure that minimizes the dissatisfaction (grief ) of individual components with a joint prediction." ></td>
	<td class="line x" title="56:212	In this section, we formally de ne the grief of each component, and a mechanism for its minimization." ></td>
	<td class="line x" title="57:212	We then describe our method for joint training of individual rankers that takes into account the Good Grief decoding procedure." ></td>
	<td class="line x" title="58:212	3.1 Problem Formulation In an m-aspect ranking problem, we are given a training sequence of instance-label pairs (x1,y1),,(xt,yt), Each instance xt is a feature vector in Rn and the label yt is a vector of m ranks in Ym, where Y = f1, ,kg is the set of possible ranks." ></td>
	<td class="line x" title="59:212	The ith component of yt is the rank for the ith aspect, and will be denoted by y[i]t. The goal is to learn a mapping from instances to rank sets, H : X !Ym, which minimizes the distance between predicted ranks and true ranks." ></td>
	<td class="line x" title="60:212	3.2 The Model Our m-aspect ranking model containsm+1 components: (hw[1],b[1]i,,hw[m],b[m]i,a)." ></td>
	<td class="line x" title="61:212	The rst m components are individual ranking models, one for each aspect, and the nal component is the agreement model." ></td>
	<td class="line x" title="62:212	For each aspect i2 1m, w[i] 2Rn is a vector of weights on the input features, and b[i] 2Rk1 is a vector of boundaries which divide the real line into k intervals, corresponding to the k possible ranks." ></td>
	<td class="line x" title="63:212	The default prediction of the aspect ranking model simply uses the ranking rule of the PRank algorithm." ></td>
	<td class="line x" title="64:212	This rule predicts the rank r such that b[i]r1 scorei(x) < b[i]r.3 The value scorei(x) can be de ned simply as the dot product w[i] x, or it can take into account the weight vectors for other aspects weighted by a measure of interaspect similarity." ></td>
	<td class="line x" title="65:212	We adopt the de nition given in equation 1, replacing the user-speci c weight vectors with our aspect-speci c weight vectors." ></td>
	<td class="line x" title="66:212	3More precisely (taking into account the possibility of ties): y[i] = minr{1, ,k}{r : scorei(x)b[i]r < 0} The agreement model is a vector of weights a 2 Rn." ></td>
	<td class="line x" title="67:212	A value of a x > 0 predicts that the ranks of all m aspects are equal, and a value of a x 0 indicates disagreement." ></td>
	<td class="line x" title="68:212	The absolute value ja xj indicates the con dence in the agreement prediction." ></td>
	<td class="line x" title="69:212	The goal of the decoding procedure is to predict a joint rank for the m aspects which satis es the individual ranking models as well as the agreement model." ></td>
	<td class="line x" title="70:212	For a given input x, the individual model for aspect i predicts a default rank y[i] based on its feature weight and boundary vectorshw[i],b[i]i. In addition, the agreement model makes a prediction regarding rank consensus based on a x. However, the default aspect predictions y[1] y[m] may not accord with the agreement model." ></td>
	<td class="line x" title="71:212	For example, if a x > 0, but y[i]6= y[j] for some i,j21m, then the agreement model predicts complete consensus, whereas the individual aspect models do not." ></td>
	<td class="line x" title="72:212	We therefore adopt a joint prediction criterion which simultaneously takes into account all model components individual aspect models as well as the agreement model." ></td>
	<td class="line x" title="73:212	For each possible prediction r = (r[1],,r[m]) this criterion assesses the level of grief associated with the ith-aspect ranking model, gi(x,r[i])." ></td>
	<td class="line x" title="74:212	Similarly, we compute the grief of the agreement model with the joint prediction, ga(x,r) (bothgi andga are de ned formally below)." ></td>
	<td class="line x" title="75:212	The decoder then predicts the m ranks which minimize the overall grief: H(x) = arg min rYm bracketleftBigg ga(x,r) + msummationdisplay i=1 gi(x,r[i]) bracketrightBigg (2) If the default rank predictions for the aspect models, y = (y[1],, y[m]), are in accord with the agreement model (both indicating consensus or both indicating contrast), then the grief of all model components will be zero, and we simply output y. On the other hand, if y indicates disagreement but the agreement model predicts consensus, then we have the option of predicting y and bearing the grief of the agreement model." ></td>
	<td class="line x" title="76:212	Alternatively, we can predict some consensus yprime (i.e. with yprime[i] = yprime[j],8i,j) and bear the grief of the component ranking models." ></td>
	<td class="line x" title="77:212	The decoder H chooses the option with lowest overall grief.4 4This decoding criterion assumes that the griefs of the com302 Now we formally de ne the measures of grief used in this criterion." ></td>
	<td class="line x" title="78:212	Aspect Model Grief We de ne the grief of theithaspect ranking model with respect to a rank r to be the smallest magnitude correction term which places the inputs score into therth segment of the real line: gi(x,r) = minjcj s.t. b[i]r1 scorei(x) +c<b[i]r Agreement Model Grief Similarly, we de ne the grief of the agreement model with respect to a joint rank r = (r[1],,r[m]) as the smallest correction needed to bring the agreement score into accord with the agreement relation between the individual ranks r[1],,r[m]: ga(x,r) = minjcj s.t. a x +c> 0^8i,j21m : r[i] = r[j] _ a x +c 0^9i,j21m : r[i]6= r[j] 3.3 Training Ranking models Pseudo-code for Good Grief training is shown in Figure 1." ></td>
	<td class="line x" title="79:212	This training algorithm is based on PRanking (Crammer and Singer, 2001), an online perceptron algorithm." ></td>
	<td class="line x" title="80:212	The training is performed by iteratively ranking each training input x and updating the model." ></td>
	<td class="line x" title="81:212	If the predicted rank y is equal to the true rank y, the weight and boundaries vectors remain unchanged." ></td>
	<td class="line x" title="82:212	On the other hand, if y6= y, then the weights and boundaries are updated to improve the prediction for x (step 4.c in Figure 1)." ></td>
	<td class="line x" title="83:212	See (Crammer and Singer, 2001) for explanation and analysis of this update rule." ></td>
	<td class="line x" title="84:212	Our algorithm departs from PRanking by conjoining the updates for the m ranking models." ></td>
	<td class="line x" title="85:212	We achieve this by using Good Grief decoding at each step throughout training." ></td>
	<td class="line x" title="86:212	Our decoder H(x) (from equation 2) uses all the aspect component models ponent models are comparable." ></td>
	<td class="line x" title="87:212	In practice, we take an uncalibrated agreement model aprime and reweight it with a tuning parameter: a = aprime." ></td>
	<td class="line x" title="88:212	The value of  is estimated using the development set." ></td>
	<td class="line x" title="89:212	We assume that the griefs of the ranking models are comparable since they are jointly trained." ></td>
	<td class="line x" title="90:212	as well as the (previously trained) agreement model to determine the predicted rank for each aspect." ></td>
	<td class="line x" title="91:212	In concrete terms, for every training instance x, we predict the ranks of all aspects simultaneously (step 2 in Figure 1)." ></td>
	<td class="line x" title="92:212	Then, for each aspect we make a separate update based on this joint prediction (step 4 in Figure 1), instead of using the individual models predictions." ></td>
	<td class="line x" title="93:212	Agreement model The agreement model a is assumed to have been previously trained on the same training data." ></td>
	<td class="line x" title="94:212	An instance is labeled with a positive label if all the ranks associated with this instance are equal." ></td>
	<td class="line x" title="95:212	The rest of the instances are labeled as negative." ></td>
	<td class="line x" title="96:212	This model can use any standard training algorithm for binary classi cation such as Perceptron or SVM optimization." ></td>
	<td class="line x" title="97:212	3.4 Feature Representation Ranking Models Following previous work on sentiment classi cation (Pang et al. , 2002), we represent each review as a vector of lexical features." ></td>
	<td class="line x" title="98:212	More speci cally, we extract all unigrams and bigrams, discarding those that appear fewer than three times." ></td>
	<td class="line x" title="99:212	This process yields about 30,000 features." ></td>
	<td class="line x" title="100:212	Agreement Model The agreement model also operates over lexicalized features." ></td>
	<td class="line x" title="101:212	The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi (2002)." ></td>
	<td class="line x" title="102:212	In addition to unigrams and bigrams, we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review." ></td>
	<td class="line x" title="103:212	For example, the presence of delicious and dirty indicate high contrast, whereas the pair expensive and slow indicate low contrast." ></td>
	<td class="line x" title="104:212	The contrastive distance for a pair of words is computed by considering the difference in relative weight assigned to the words in individually trained PRanking models." ></td>
	<td class="line x" title="105:212	4 Analysis In this section, we prove that our model is able to perfectly rank a strict superset of the training corpora perfectly rankable by m ranking models individually." ></td>
	<td class="line x" title="106:212	We rst show that if the independent ranking models can individually rank a training set perfectly, then our model can do so as well." ></td>
	<td class="line x" title="107:212	Next, we show that our model is more expressive by providing 303 Input : (x1,y1),,(xT,yT), Agreement model a, Decoder de ntion H(x) (from equation 2)." ></td>
	<td class="line x" title="108:212	Initialize : Set w[i]1 = 0, b[i]11,,b[i]1k1 = 0, b[i]1k =1,8i21m." ></td>
	<td class="line x" title="109:212	Loop : For t = 1,2,,T : 1." ></td>
	<td class="line x" title="110:212	Get a new instance xt2Rn." ></td>
	<td class="line x" title="111:212	2." ></td>
	<td class="line x" title="112:212	Predict yt = H(x; wt,bt,a) (Equation 2)." ></td>
	<td class="line x" title="113:212	3." ></td>
	<td class="line x" title="114:212	Get a new label yt." ></td>
	<td class="line x" title="115:212	4." ></td>
	<td class="line x" title="116:212	For aspect i = 1,,m: If y[i]t6= y[i]t update model (otherwise set w[i]t+1 = w[i]t, b[i]t+1r = b[i]tr,8r): 4.a For r = 1,,k 1 : If y[i]t r then y[i]tr = 1 else y[i]tr = 1." ></td>
	<td class="line x" title="117:212	4.b For r = 1,,k 1 : If (y[i]t r)y[i]tr 0 then [i]tr = y[i]tr else [i]tr = 0." ></td>
	<td class="line x" title="118:212	4.c Update w[i]t+1 w[i]t + (summationtextr[i]tr)xt." ></td>
	<td class="line x" title="119:212	For r = 1,,k 1 update : b[i]t+1r b[i]tr [i]tr." ></td>
	<td class="line x" title="120:212	Output : H(x; wT+1,bT+1,a)." ></td>
	<td class="line x" title="121:212	Figure 1: Good Grief Training." ></td>
	<td class="line x" title="122:212	The algorithm is based on PRanking training algorithm." ></td>
	<td class="line x" title="123:212	Our algorithm differs in the joint computation of all aspect predictions yt based on the Good Grief Criterion (step 2) and the calculation of updates for each aspect based on the joint prediction (step 4)." ></td>
	<td class="line x" title="124:212	a simple illustrative example of a training set which can only be perfectly ranked with the inclusion of an agreement model." ></td>
	<td class="line x" title="125:212	First we introduce some notation." ></td>
	<td class="line x" title="126:212	For each training instance (xt,yt), each aspect i 2 1m, and each rank r 2 1k, de ne an auxiliary variable y[i]tr with y[i]tr = 1 if y[i]t r and y[i]tr = 1 if y[i]t > r. In words, y[i]tr indicates whether the true rank y[i]t is to the right or left of a potential rank r. Now suppose that a training set (x1,y1),,(xT,yT) is perfectly rankable for each aspect independently." ></td>
	<td class="line x" title="127:212	That is, for each aspect i 2 1m, there exists some ideal model v[i] = (w[i],b[i]) such that the signed distance from the prediction to the rth boundary: w[i] xt b[i]r has the same sign as the auxiliary variable y[i]tr." ></td>
	<td class="line x" title="128:212	In other words, the minimum margin over all training instances and ranks,  = minr,tf(w[i] xt b[i]r)y[i]trg, is no less than zero." ></td>
	<td class="line x" title="129:212	Now for the tth training instance, de ne an agreement auxiliary variable at, where at = 1 when all aspects agree in rank and at = 1 when at least two aspects disagree in rank." ></td>
	<td class="line x" title="130:212	First consider the case where the agreement model a perfectly classi es all training instances: (a xt)at > 0,8t." ></td>
	<td class="line x" title="131:212	It is clear that Good Grief decoding with the ideal joint model (hw[1],b[1]i,,hw[m],b[m]i,a) will produce the same output as the component ranking models run separately (since the grief will always be zero for the default rank predictions)." ></td>
	<td class="line x" title="132:212	Now consider the case where the training data is not linearly separable with regard to agreement classi cation." ></td>
	<td class="line x" title="133:212	De ne the margin of the worst case error to be = maxtfj(a xt)j: (a xt)at < 0g." ></td>
	<td class="line x" title="134:212	If <, then again Good Grief decoding will always produce the default results (since the grief of the agreement model will be at most in cases of error, whereas the grief of the ranking models for any deviation from their default predictions will be at least )." ></td>
	<td class="line x" title="135:212	On the other hand, if  , then the agreement model errors could potentially disrupt the perfect ranking." ></td>
	<td class="line x" title="136:212	However, we need only rescale w := w( +epsilon1) and b := b( +epsilon1) to ensure that the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error." ></td>
	<td class="line x" title="137:212	Thus whenever independent ranking models can perfectly rank a training set, a joint ranking model with Good Grief decoding can do so as well." ></td>
	<td class="line x" title="138:212	Now we give a simple example of a training set which can only be perfectly ranked with the addition of an agreement model." ></td>
	<td class="line x" title="139:212	Consider a training set of four instances with two rank aspects: 304 hx1,y1i=h(1,0,1), (2,1)i hx2,y2i=h(1,0,0), (2,2)i hx3,y3i=h(0,1,1), (1,2)i hx4,y4i=h(0,1,0), (1,1)i We can interpret these inputs as feature vectors corresponding to the presence of good, bad, and but not in the following four sentences: The food was good, but not the ambience." ></td>
	<td class="line x" title="140:212	The food was good, and so was the ambience." ></td>
	<td class="line x" title="141:212	The food was bad, but not the ambience." ></td>
	<td class="line x" title="142:212	The food was bad, and so was the ambience." ></td>
	<td class="line x" title="143:212	We can further interpret the rst rank aspect as the quality of food, and the second as the quality of the ambience, both on a scale of 1-2." ></td>
	<td class="line x" title="144:212	A simple ranking model which only considers the words good and bad perfectly ranks the food aspect." ></td>
	<td class="line x" title="145:212	However, it is easy to see that no single model perfectly ranks the ambience aspect." ></td>
	<td class="line x" title="146:212	Consider any model hw,b = (b)i. Note that w x1 < b and w x2 b together imply that w3 < 0, whereas w x3 b and w x4 < b together imply that w3 > 0." ></td>
	<td class="line x" title="147:212	Thus independent ranking models cannot perfectly rank this corpus." ></td>
	<td class="line x" title="148:212	The addition of an agreement model, however, can easily yield a perfect ranking." ></td>
	<td class="line x" title="149:212	With a = (0,0, 5) (which predicts contrast with the presence of the words but not ) and a ranking model for the ambience aspect such as w = (1, 1,0),b = (0), the Good Grief decoder will produce a perfect rank." ></td>
	<td class="line x" title="150:212	5 Experimental Set-Up We evaluate our multi-aspect ranking algorithm on a corpus5 of restaurant reviews available on the website http://www.we8there.com." ></td>
	<td class="line x" title="151:212	Reviews from this website have been previously used in other sentiment analysis tasks (Higashinaka et al. , 2006)." ></td>
	<td class="line x" title="152:212	Each review is accompanied by a set of ve ranks, each on a scale of 1-5, covering food, ambience, service, value, and overall experience." ></td>
	<td class="line x" title="153:212	These ranks are provided by consumers who wrote original reviews." ></td>
	<td class="line x" title="154:212	Our corpus does not contain incomplete data points since all the reviews available on this website contain both a review text and the values for all the ve aspects." ></td>
	<td class="line x" title="155:212	Training and Testing Division Our corpus con5Data and code used in this paper are available at http://people.csail.mit.edu/bsnyder/naacl07 tains 4,488 reviews, averaging 115 words." ></td>
	<td class="line x" title="156:212	We randomly select 3,488 reviews for training, 500 for development and 500 for testing." ></td>
	<td class="line x" title="157:212	Parameter Tuning We used the development set to determine optimal numbers of training iterations for our model and for the baseline models." ></td>
	<td class="line x" title="158:212	Also, given an initial uncalibrated agreement model aprime, we de ne our agreement model to be a = aprime for an appropriate scaling factor ." ></td>
	<td class="line x" title="159:212	We tune the value of  on the development set." ></td>
	<td class="line x" title="160:212	Corpus Statistics Our training corpus contains 528 among 55 = 3025 possible rank sets." ></td>
	<td class="line x" title="161:212	The most frequent rank set h5,5,5,5,5i accounts for 30.5% of the training set." ></td>
	<td class="line x" title="162:212	However, no other rank set comprises more than 5% of the data." ></td>
	<td class="line x" title="163:212	To cover 90% of occurrences in the training set, 227 rank sets are required." ></td>
	<td class="line x" title="164:212	Therefore, treating a rank tuple as a single label is not a viable option for this task." ></td>
	<td class="line x" title="165:212	We also nd that reviews with full agreement across rank aspects are quite common in our corpus, accounting for 38% of the training data." ></td>
	<td class="line x" title="166:212	Thus an agreementbased approach is natural and relevant." ></td>
	<td class="line x" title="167:212	A rank of 5 is the most common rank for all aspects and thus a prediction of all 5s gives a MAJORITY baseline and a natural indication of task dif culty." ></td>
	<td class="line x" title="168:212	Evaluation Measures We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer, 2001; Basilico and Hofmann, 2004)." ></td>
	<td class="line x" title="169:212	Ranking loss measures the average distance between the true rank and the predicted rank." ></td>
	<td class="line x" title="170:212	Formally, given N test instances (x1,y1),,(xN,yN) of an m-aspect ranking problem and the corresponding predictions y1,,yN, ranking loss is de ned assummationtext t,i |y[i]ty[i]t| mN . Lower values of this measure cor-respond to a better performance of the algorithm." ></td>
	<td class="line x" title="171:212	6 Results Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN+DECODE along with various baselines, including the simple MAJORITY baseline mentioned in section 5." ></td>
	<td class="line x" title="172:212	The rst competitive baseline, PRANK, learns a separate ranker for each aspect using the PRank algorithm." ></td>
	<td class="line x" title="173:212	The second competitive baseline, SIM, shares the weight vectors across aspects using a similarity measure (Basilico and Hofmann, 2004)." ></td>
	<td class="line x" title="174:212	305 Food Service Value Atmosphere Experience Total MAJORITY 0.848 1.056 1.030 1.044 1.028 1.001 PRANK 0.606 0.676 0.700 0.776 0.618 0.675 SIM 0.562 0.648 0.706 0.798 0.600 0.663 GG DECODE 0.544 0.648 0.704 0.798 0.584 0.656 GG TRAIN+DECODE 0.534 0.622 0.644 0.774 0.584 0.632 GG ORACLE 0.510 0.578 0.674 0.694 0.518 0.595 Table 1: Ranking loss on the test set for variants of Good Grief and various baselines." ></td>
	<td class="line x" title="175:212	Figure 2: Rank loss for our algorithm and baselines as a function of training round." ></td>
	<td class="line x" title="176:212	Both of these methods are described in detail in Section 2." ></td>
	<td class="line x" title="177:212	In addition, we consider two variants of our algorithm: GG DECODE employs the PRank training algorithm to independently train all component ranking models and only applies Good Grief decoding at test time." ></td>
	<td class="line x" title="178:212	GG ORACLE uses Good Grief training and decoding but in both cases is given perfect knowledge of whether or not the true ranks all agree (instead of using the trained agreement model)." ></td>
	<td class="line x" title="179:212	Our model achieves a rank error of 0.632, compared to 0.675 for PRANK and 0.663 for SIM." ></td>
	<td class="line x" title="180:212	Both of these differences are statistically signi cant at p< 0.002 by a Fisher Sign Test." ></td>
	<td class="line x" title="181:212	The gain in performance is observed across all ve aspects." ></td>
	<td class="line x" title="182:212	Our model also yields signi cant improvement (p< 0.05) over the decoding-only variant GG DECODE, con rming the importance of joint training." ></td>
	<td class="line x" title="183:212	As shown in Figure 2, our model demonstrates consistent improvement over the baselines across all the training rounds." ></td>
	<td class="line x" title="184:212	Model Analysis We separately analyze our perConsensus Non-consensus PRANK 0.414 0.864 GG TRAIN+DECODE 0.324 0.854 GG ORACLE 0.281 0.830 Table 2: Ranking loss for our model and PRANK computed separately on cases of actual consensus and actual disagreement." ></td>
	<td class="line x" title="185:212	formance on the 210 test instances where all the target ranks agree and the remaining 290 instances where there is some contrast." ></td>
	<td class="line x" title="186:212	As Table 2 shows, we outperform the PRANK baseline in both cases." ></td>
	<td class="line x" title="187:212	However on the consensus instances we achieve a relative reduction in error of 21.8% compared to only a 1.1% reduction for the other set." ></td>
	<td class="line x" title="188:212	In cases of consensus, the agreement model can guide the ranking models by reducing the decision space to ve rank sets." ></td>
	<td class="line x" title="189:212	In cases of disagreement, however, our model does not provide suf cient constraints as the vast majority of ranking sets remain viable." ></td>
	<td class="line x" title="190:212	This explains the performance of GG ORACLE, the variant of our algorithm with perfect knowledge of agreement/disagreement facts." ></td>
	<td class="line x" title="191:212	As shown in Table 1, GG ORACLE yields substantial improvement over our algorithm, but most of this gain comes from consensus instances (see Table 2)." ></td>
	<td class="line x" title="192:212	We also examine the impact of the agreement model accuracy on our algorithm." ></td>
	<td class="line x" title="193:212	The agreement model, when considered on its own, achieves classi cation accuracy of 67% on the test set, compared to a majority baseline of 58%." ></td>
	<td class="line x" title="194:212	However, those instances with high con denceja xjexhibit substantially higher classi cation accuracy." ></td>
	<td class="line x" title="195:212	Figure 3 shows the performance of the agreement model as a function of the con dence value." ></td>
	<td class="line x" title="196:212	The 10% of the data with highest con dence values can be classi ed by 306 Figure 3: Accuracy of the agreement model on subsets of test instances with highest con denceja xj." ></td>
	<td class="line x" title="197:212	the agreement model with 90% accuracy, and the third of the data with highest con dence can be classi ed at 80% accuracy." ></td>
	<td class="line x" title="198:212	This property explains why the agreement model helps in joint ranking even though its overall accuracy may seem low." ></td>
	<td class="line x" title="199:212	Under the Good Grief criterion, the agreement models prediction will only be enforced when its grief outweighs that of the ranking models." ></td>
	<td class="line x" title="200:212	Thus in cases where the prediction con dence (ja xj) is relatively low,6 the agreement model will essentially be ignored." ></td>
	<td class="line x" title="201:212	7 Conclusion and Future Work We considered the problem of analyzing multiple related aspects of user reviews." ></td>
	<td class="line x" title="202:212	The algorithm presented jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks." ></td>
	<td class="line x" title="203:212	The strength of our algorithm lies in its ability to guide the prediction of individual rankers using rhetorical relations between aspects such as agreement and contrast." ></td>
	<td class="line x" title="204:212	Our method yields signi cant empirical improvements over individual rankers as well as a state-of-the-art joint ranking model." ></td>
	<td class="line x" title="205:212	Our current model employs a single rhetorical relation agreement vs. contrast to model dependencies between different opinions." ></td>
	<td class="line x" title="206:212	As our analy6What counts as relatively low will depend on both the value of the tuning parameter  and the con dence of the component ranking models for a particular input x. sis shows, this relation does not provide suf cient constraints for non-consensus instances." ></td>
	<td class="line x" title="207:212	An avenue for future research is to consider the impact of additional rhetorical relations between aspects." ></td>
	<td class="line x" title="208:212	We also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms." ></td>
	<td class="line x" title="209:212	Acknowledgments The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168 and grant IIS0415865) and the Microsoft Research Faculty Fellowship." ></td>
	<td class="line x" title="210:212	Thanks to Michael Collins, Pawan Deshpande, Jacob Eisenstein, Igor Malioutov, Luke Zettlemoyer, and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="211:212	Thanks also to Vasumathi Raman for programming assistance." ></td>
	<td class="line x" title="212:212	Any opinions, ndings, and conclusions or recommendations expressed above are those of the authors and do not necessarily re ect the views of the NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1039
Extracting Appraisal Expressions
Bloom, Kenneth;Garg, Navendu;Argamon, Shlomo;"></td>
	<td class="line x" title="1:46	Proceedings of NAACL HLT 2007, pages 308315, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:46	c2007 Association for Computational Linguistics Extracting Appraisal Expressions Kenneth Bloom and Navendu Garg and Shlomo Argamon Computer Science Department Illinois Institute of Technology 10 W. 31st St. Chicago, IL 60616 {kbloom1,gargnav,argamon}@iit.edu Abstract Sentiment analysis seeks to characterize opinionated or evaluative aspects of natural language text." ></td>
	<td class="line x" title="3:46	We suggest here that appraisal expression extraction should be viewed as a fundamental task in sentiment analysis." ></td>
	<td class="line x" title="4:46	An appraisal expression is a textual unit expressing an evaluative stance towards some target." ></td>
	<td class="line x" title="5:46	The task is to find and characterize the evaluative attributes of such elements." ></td>
	<td class="line x" title="6:46	This paper describes a system for effectively extracting and disambiguating adjectival appraisal expressions in English outputting a generic representation in terms of their evaluative function in the text." ></td>
	<td class="line x" title="7:46	Data mining on appraisal expressions gives meaningful and non-obvious insights." ></td>
	<td class="line x" title="8:46	1 Introduction Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years." ></td>
	<td class="line oc" title="9:46	Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al. , 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004); classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004); identifying or classifying appraisal targets (Nigam and Hurst, 2004); identifying the source of an opinion in a text (Choi et al. , 2005), whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al. , 2005; Popescu and Etzioni, 2005)." ></td>
	<td class="line nc" title="10:46	Much of this work has utilized the fundamental concept of semantic orientation, (Turney, 2002); however, sentiment analysis still lacks a unified field theory." ></td>
	<td class="line x" title="11:46	We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of some object." ></td>
	<td class="line x" title="12:46	An appraisal expression hasthreemaincomponents: an attitude (whichtakes an evaluative stance about an object), a target (the object of the stance), and a source (the person taking the stance) which may be implied." ></td>
	<td class="line x" title="13:46	The idea of appraisal extraction is a generalization of problem formulations developed in earlier works." ></td>
	<td class="line x" title="14:46	Mullen and Colliers (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions." ></td>
	<td class="line x" title="15:46	In previous work (Whitelaw et al. , 2005), we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases." ></td>
	<td class="line x" title="16:46	The use of multiple text classifiers by Wiebe and colleagues (Wilson et al. , 2005; Wiebe et al. , 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions." ></td>
	<td class="line x" title="17:46	Nigam and Hursts (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzionis (2005) opinion mining technique also fits well into our framework." ></td>
	<td class="line x" title="18:46	In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense disambiguation." ></td>
	<td class="line x" title="19:46	Each ex308 tracted appraisal expression is represented as a set of feature values in terms of its evaluative function in thetext." ></td>
	<td class="line x" title="20:46	Wehaveappliedthissystemtotwodomains of texts: product reviews, and movie reviews." ></td>
	<td class="line x" title="21:46	Manual evaluation of the extraction shows our system to work well, as well as giving some directions for improvement." ></td>
	<td class="line x" title="22:46	We also show how straightforward data mining can give users very useful information about public opinion." ></td>
	<td class="line x" title="23:46	2 Appraisal Expressions We define an appraisal expression to be an elementary linguistic unit that conveys an attitude of some kind towards some target." ></td>
	<td class="line x" title="24:46	An appraisal expression is defined to comprise a source, an attitude, and a target, each represented by various attributes." ></td>
	<td class="line x" title="25:46	For example, in I found the movie quite monotonous, the speaker (the Source) expresses a negative Attitude (quite monotonous) towards the movie (the Target)." ></td>
	<td class="line x" title="26:46	Note that attitudes come in different types; for example, monotonous describes an inherent quality of the Target, while loathed would describe the emotional reaction of the Source." ></td>
	<td class="line x" title="27:46	Attitude may be expressed through nouns, verbs, adjectives and metaphors." ></td>
	<td class="line x" title="28:46	Extracting all of this information accurately for all of these types of appraisal expressions is a very difficult problem." ></td>
	<td class="line x" title="29:46	We therefore restrict ourselves for now to adjectival appraisal expressions that are each contained in a single sentence." ></td>
	<td class="line x" title="30:46	Additionally, we focus here only on extracting and analyzing the attitude and the target, but not the source." ></td>
	<td class="line x" title="31:46	Even with these restrictions, we obtain interesting results (Sec." ></td>
	<td class="line x" title="32:46	7)." ></td>
	<td class="line x" title="33:46	2.1 Appraisal attributes Our method is grounded in Appraisal Theory, developed by Martin and White (2005), which analyzes the way opinion is expressed." ></td>
	<td class="line x" title="34:46	Following Martin and White, we define: Attitude type is type of appraisal being expressedone of affect, appreciation, or judgment (Figure 1)." ></td>
	<td class="line x" title="35:46	Affect refers to an emotional state (e.g. , happy, angry), and is the most explicitly subjective type of appraisal." ></td>
	<td class="line x" title="36:46	The other two types express evaluation of external entities, differentiating between intrinsic appreciation of object properties (e.g. , slender, ugly) and social judgment (e.g. , heroic, idiotic)." ></td>
	<td class="line x" title="37:46	Orientation is whether the attitude is positive Attitude Type Appreciation Composition Balance: consistent, discordant, Complexity: elaborate, convoluted,  Reaction Impact: amazing, compelling, dull,  Quality: beautiful, elegant, hideous,  Valuation: innovative, profound, inferior,  Affect: happy, joyful, furious,  Judgment Social Esteem Capacity: clever, competent, immature,  Tenacity: brave, hard-working, foolhardy,  Normality: famous, lucky, obscure,  Social Sanction Propriety: generous, virtuous, corrupt,  Veracity: honest, sincere, sneaky,  Figure 1: The Attitude Type taxonomy, with examples of adjectives from the lexicon." ></td>
	<td class="line x" title="38:46	(good) or negative (bad)." ></td>
	<td class="line x" title="39:46	Force describes the intensity of the appraisal." ></td>
	<td class="line x" title="40:46	Force is largely expressed via modifiers such as very (increased force), or slightly (decreased force), but may also be expressed lexically, for example greatest vs. great vs. good." ></td>
	<td class="line x" title="41:46	Polarity of an appraisal is marked if it is scoped in a polarity marker (such as not), or unmarked otherwise." ></td>
	<td class="line x" title="42:46	Other attributes of appraisal are affected by negation; e.g., not good also has the opposite orientation from good." ></td>
	<td class="line x" title="43:46	Target type is a domain-dependent semantic type for the target." ></td>
	<td class="line x" title="44:46	This attribute takes on values fromadomain-dependenttaxonomy,representing important (and easily extractable) distinctions between targets in the domain." ></td>
	<td class="line x" title="45:46	2.2 Target taxonomies Two domain-dependent target type taxonomies are shown in Figure 2." ></td>
	<td class="line x" title="46:46	In both, the primary distinction is between a direct naming of a kind of Thing or a deictic/pronominal reference (e.g. , those or it), since the system does not currently rely on coreference resolution." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-2048
Modifying SO-PMI for Japanese Weblog Opinion Mining by Using a Balancing Factor and Detecting Neutral Expressions
Wang, Guangwei;Araki, Kenji;"></td>
	<td class="line x" title="1:58	Proceedings of NAACL HLT 2007, Companion Volume, pages 189192, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:58	c2007 Association for Computational Linguistics Modifying SO-PMI for Japanese Weblog Opinion Mining by Using a Balancing Factor and Detecting Neutral Expressions Guangwei Wang Graduate School of Information Science and Technology Hokkaido University Sapporo, Japan 060-0814 wgw@media.eng.hokudai.ac.jp Kenji Araki Graduate School of Information Science and Technology Hokkaido University Sapporo, Japan 060-0814 araki@media.eng.hokudai.ac.jp Abstract We propose a variation of the SO-PMI algorithm for Japanese, for use in Weblog Opinion Mining." ></td>
	<td class="line p" title="3:58	SO-PMI is an unsupervised approach proposed by Turney that has been shown to work well for English." ></td>
	<td class="line o" title="4:58	We first used the SO-PMI algorithm on Japanese in a way very similar to Turneys original idea." ></td>
	<td class="line x" title="5:58	The result of this trial leaned heavily toward positive opinions." ></td>
	<td class="line x" title="6:58	We then expanded the reference words to be sets of words, tried to introduce a balancing factor and to detect neutral expressions." ></td>
	<td class="line x" title="7:58	After these modifications, we achieved a wellbalanced result: both positive and negative accuracy exceeded 70%." ></td>
	<td class="line o" title="8:58	This shows that our proposed approach not only adapted the SO-PMI for Japanese, but also modified it to analyze Japanese opinions more effectively." ></td>
	<td class="line x" title="9:58	1 Introduction Recently, more and more websites add information in the form of personal opinions to the Web, e.g. customer reviews of products, forums, discussion groups, and blogs." ></td>
	<td class="line x" title="10:58	Here, we use the term Weblog for these sites." ></td>
	<td class="line x" title="11:58	This type of information is often useful." ></td>
	<td class="line x" title="12:58	However, we have to deal with an enormous amount of unstructured and/or semi-structured data." ></td>
	<td class="line x" title="13:58	These data are subjective, in free format and mostly textual, thus using them is difficult and time consuming." ></td>
	<td class="line x" title="14:58	Therefore, how to mine the Weblog opinions automatically more effectively has attracted more and more attention (Gamon, 2005; Popescu, 2005; Chaovalit, 2005)." ></td>
	<td class="line oc" title="15:58	Turney (2002) has presented an unsupervised opinion classification algorithm called SO-PMI (Semantic Orientation Using Pointwise Mutual Information)." ></td>
	<td class="line o" title="16:58	The main use of SO-PMI is to estimate the semantic orientation (i.e. positive or negative) of a phrase by measuring the hits returned from a search engine of pairs of words or phrases, based on the mutual information theory." ></td>
	<td class="line p" title="17:58	This approach has previously been successfully used on English." ></td>
	<td class="line x" title="18:58	The average accuracy was 74% when evaluated on 410 reviews from Epinions1." ></td>
	<td class="line n" title="19:58	However, according to our preliminary experiment, directly translating Turneys original idea into Japanese gave a very slanted result, with a positive accuracy of 95% and a negative accuracy of only 8%." ></td>
	<td class="line x" title="20:58	We found that the balance between the positive and negative sides is influenced greatly by the page hits of reference words/sets, since a search engine is used." ></td>
	<td class="line x" title="21:58	Therefore, we introduced a balancing factor according for the difference in occurrence between positive and negative words." ></td>
	<td class="line x" title="22:58	And then we added several threshold rules to detect neutral expressions." ></td>
	<td class="line x" title="23:58	The proposed approach is evaluated on 200 positive and 200 negative Japanese opinion sentences and yielded a well-balanced result." ></td>
	<td class="line x" title="24:58	In the remainder of this paper, we review the SOPMI Algorithm in Section 2, then adapt the SO-PMI for Japanese and present the modifications in Section 3." ></td>
	<td class="line x" title="25:58	In section 4, we evaluate and discuss the experimental results." ></td>
	<td class="line x" title="26:58	Section 5 gives concluding remarks." ></td>
	<td class="line xc" title="27:58	2 Details of the SO-PMI Algorithm The SO-PMI algorithm (Turney, 2002) is used to estimate the semantic orientation (SO) of a phrase by 1http://www.epinions.com 189 References Peter D. Turney." ></td>
	<td class="line x" title="28:58	2002." ></td>
	<td class="line x" title="29:58	Thumbs up or thumbs down?" ></td>
	<td class="line x" title="30:58	Semantic orientation applied to unsupervised classification of reviews." ></td>
	<td class="line x" title="31:58	Proceedings 40th Annual Meeting of the ACL, pp." ></td>
	<td class="line x" title="32:58	417-424." ></td>
	<td class="line x" title="33:58	Popescu, Ana-Maria, and Oren Etzioni." ></td>
	<td class="line x" title="34:58	2005." ></td>
	<td class="line x" title="35:58	Extracting Product Features and Opinions from Reviews." ></td>
	<td class="line x" title="36:58	Proceedings of HLT-EMNLP." ></td>
	<td class="line x" title="37:58	Michael Gamon, Anthony Aue, Simon Corston-Oliver and Eric K. Ringger." ></td>
	<td class="line x" title="38:58	2005." ></td>
	<td class="line x" title="39:58	Pulse: Mining Customer Opinions from Free Text." ></td>
	<td class="line x" title="40:58	Proceedings of the 2005 Conference on Intelligent Data Analysis (IDA), pp.121-132." ></td>
	<td class="line x" title="41:58	Pimwadee Chaovalit and Lina Zhou." ></td>
	<td class="line x" title="42:58	2005." ></td>
	<td class="line x" title="43:58	Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches." ></td>
	<td class="line x" title="44:58	Proceedings of the 38th Annual HICSS." ></td>
	<td class="line x" title="45:58	Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji Tateishi and Toshikazu Fukushima." ></td>
	<td class="line x" title="46:58	2003." ></td>
	<td class="line x" title="47:58	Collecting evaluative expressions by a text mining technique." ></td>
	<td class="line x" title="48:58	IPSJ SIG NOTE, Vol.154, No.12, In Japanese." ></td>
	<td class="line x" title="49:58	Taku Kudoh and Yuji Matsumoto." ></td>
	<td class="line x" title="50:58	2002." ></td>
	<td class="line x" title="51:58	Applying Cascaded Chunking to Japanese Dependency Structure Analysis." ></td>
	<td class="line x" title="52:58	Information Processing Society of Japan (IPSJ)Academic Journals, Vol 43, No 6, pp." ></td>
	<td class="line x" title="53:58	1834-1842, In Japanese." ></td>
	<td class="line x" title="54:58	Guangwei Wang and Kenji Araki." ></td>
	<td class="line x" title="55:58	2006." ></td>
	<td class="line x" title="56:58	A Decision Support System Using Text Mining Technology." ></td>
	<td class="line x" title="57:58	IEICE SIG Notes WI2-2006-6, pp." ></td>
	<td class="line x" title="58:58	55-56." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-4010
OMS-J: An Opinion Mining System for Japanese Weblog Reviews Using a Combination of Supervised and Unsupervised Approaches
Wang, Guangwei;Araki, Kenji;"></td>
	<td class="line x" title="1:47	NAACL HLT Demonstration Program, pages 1920, Rochester, New York, USA, April 2007." ></td>
	<td class="line x" title="2:47	c2007 Association for Computational Linguistics OMS-J: An Opinion Mining System for Japanese Weblog Reviews Using a Combination of Supervised and Unsupervised Approaches Guangwei Wang Graduate School of Information Science and Technology Hokkaido University Sapporo, Japan 060-0814 wgw@media.eng.hokudai.ac.jp Kenji Araki Graduate School of Information Science and Technology Hokkaido University Sapporo, Japan 060-0814 araki@media.eng.hokudai.ac.jp Abstract We introduce a simple opinion mining system for analyzing Japanese Weblog reviews called OMS-J." ></td>
	<td class="line x" title="3:47	OMS-J is designed to provide an intuitive visual GUI of opinion mining graphs for a comparison of different products of the same type to help a user make a quick purchase decision." ></td>
	<td class="line x" title="4:47	We first use an opinion mining method using a combination of supervised (a Naive Bayes Classifier) and unsupervised (an improved SO-PMI: Semantic Orientation Using Pointwise Mutual Information) learning." ></td>
	<td class="line x" title="5:47	1 Introduction Nowadays, there are numerous Web sites containing personal opinions, e.g. customer reviews of products, forums, discussion groups, and blogs." ></td>
	<td class="line x" title="6:47	Here, we use the term Weblog for these sites." ></td>
	<td class="line x" title="7:47	How to extract and analyze these opinions automatically, i.e. Opinion Mining, has seen increasing attention in recent years." ></td>
	<td class="line x" title="8:47	This paper presents a simple opinion mining system (OMS-J) for analyzing Japanese Weblog reviews automatically." ></td>
	<td class="line x" title="9:47	The novelty of OMS-J is twofold: First, it provides a GUI using intuitive visual mining graphs aimed at inexperienced users who want to check opinions on the Weblog before purchasing something." ></td>
	<td class="line x" title="10:47	These graphs can help the user to make a quick decision on which product is suitable." ></td>
	<td class="line x" title="11:47	Secondly, this system combines a supervised and an unsupervised approach to perform opinion mining." ></td>
	<td class="line oc" title="12:47	In related work (Chaovalit, 2005; Turney, 2002), both supervised and unsupervised approaches have been shown to have their pros and cons." ></td>
	<td class="line x" title="13:47	Based on the merits of these approaches and the characteristics of Japanese (Kobayashi, 2003), we proposed an opinion mining method using a Naive Bayes Classifier (supervised approach) and an improved SO-PMI method (unsupervised approach) to perform different parts of the classification task (Wang, 2006)." ></td>
	<td class="line x" title="14:47	OMS-J implements Weblog opinion mining by the steps shown in Figure 1." ></td>
	<td class="line x" title="15:47	In the next section, we describe the proposed system in detail." ></td>
	<td class="line x" title="16:47	1." ></td>
	<td class="line x" title="17:47	Information Search 2." ></td>
	<td class="line x" title="18:47	Weblog Content Extraction 3." ></td>
	<td class="line x" title="19:47	OpinionMining DB 4." ></td>
	<td class="line x" title="20:47	Mining GraphsGUIUser Search Engine (Google), KeywordLynx (Text Browser) Cabocha (Structure Analyzer)Template content extraction Feature ClassificationSupervised Approach (Nave Bayes)P/N Classification Unsupervised Approach (SO-PMI) Figure 1: System Flow 2 Proposed System 2.1 Information Search The first step is information search." ></td>
	<td class="line x" title="21:47	We used the Google search engine1 to get all the information on one product category or one specific product in the Japanese weblog on the Internet." ></td>
	<td class="line x" title="22:47	The search keyword is the product category name or the product name." ></td>
	<td class="line x" title="23:47	The URL range of the search is restricted by the URL type (e.g. blog, bbs, review)." ></td>
	<td class="line x" title="24:47	2.2 Weblog Content Extraction The Content Extraction step first analyzes the Weblog content using a dependency structure analyzer for Japanese, Cabocha2." ></td>
	<td class="line x" title="25:47	Based on the syntactic characteristics of Japanese reviews and the results 1http://www.google.co.jp/ 2http://www.chasen.org/taku/software/cabocha/ 19 of related work (Kobayashi, 2003; Taku, 2002), we designed the following templates to extract opinion phrases: < noun + auxiliary word + adj/ verb / noun >< adj+ noun / undefined / verb >< noun + verb > < noun + symbol + adj/ verb / noun >Except the above < adj> 2.3 Opinion Mining Opinion mining methods can usually be divided into two types: supervised and unsupervised approaches." ></td>
	<td class="line x" title="26:47	Supervised approaches are likely to provide more accurate classification results but need a training corpus." ></td>
	<td class="line x" title="27:47	Unsupervised approaches on the other hand require no training data but tend to produce weaker results." ></td>
	<td class="line x" title="28:47	We propose a combined opinion mining method by performing feature classification and P/N classification (Wang, 2006)." ></td>
	<td class="line x" title="29:47	The purpose of these classifications is to know what the opinion expresses about a certain products features." ></td>
	<td class="line x" title="30:47	Feature means a products attribute, i.e. price, design, function or battery feature." ></td>
	<td class="line x" title="31:47	Based on our previous study, it is easy to create a feature corpus." ></td>
	<td class="line x" title="32:47	Therefore feature classification is performed by a supervised approach, a Naive Bayes Classifier." ></td>
	<td class="line x" title="33:47	P/N classification classifies reputation expressions into positive or negative meaning using an unsupervised approach, SO-PMI." ></td>
	<td class="line x" title="34:47	The SOPMI approach measures the similarity of pairs of words or phrases based on the mutual information theory, in our case the closeness of an opinion and words for good or bad." ></td>
	<td class="line x" title="35:47	No human effort is required when mining a new product or category." ></td>
	<td class="line x" title="36:47	Only inputting the name of the product or category is needed." ></td>
	<td class="line x" title="37:47	It does however require quite a lot of processing time, since the SOPMI approach using a search engine is very time consuming." ></td>
	<td class="line x" title="38:47	Adding new features requires manual work, since a small hand labeled training corpus is used." ></td>
	<td class="line x" title="39:47	Similar categories of products, for instance cameras and mp3 players, use the same features though, so this is not done very often." ></td>
	<td class="line x" title="40:47	2.4 Mining Graphs GUI Finally, OMS-J provides a GUI with mining graphs showing the opinion mining data in the database, as shown in Figure 2." ></td>
	<td class="line x" title="41:47	These graphs show the distribution of positive and negative opinions of each feature type such as design, and for each product." ></td>
	<td class="line x" title="42:47	The distribution of positive opinions among the different product choices are shown in a pie chart, as is the same for negative opinions." ></td>
	<td class="line x" title="43:47	This GUI can also show graphs for a single products mining results, showing the positive/negative opinion distribution of each feature." ></td>
	<td class="line x" title="44:47	Figure 2: OMS-Js GUI Screenshot for One Product Category 3 Demonstration During the demonstration, we will show that OMSJ is an intuitive opinion mining system that can help people to make a quick decision on purchasing some product." ></td>
	<td class="line x" title="45:47	OMS-Js trial version has been developed and tested with three kinds of products: Electronic Dictionaries, MP3 Players and Notebook PCs." ></td>
	<td class="line x" title="46:47	The experiment results were positive." ></td>
	<td class="line x" title="47:47	We will show how the system works when a user wants to buy a good MP3 player or wants to get a feel for the general opinions on a specific Notebook PC etc." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1053
Opinion Mining using Econometrics: A Case Study on Reputation Systems
Ghose, Anindya;Ipeirotis, Panagiotis;Sundararajan, Arun;"></td>
	<td class="line x" title="1:201	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 416423, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:201	c2007 Association for Computational Linguistics Opinion Mining Using Econometrics: A Case Study on Reputation Systems Anindya Ghose Panagiotis G. Ipeirotis Department of Information, Operations, and Management Sciences Leonard N. Stern School of Business, New York University {aghose,panos,arun}@stern.nyu.edu Arun Sundararajan Abstract Deriving the polarity and strength of opinions is an important research topic, attracting significant attention over the last few years." ></td>
	<td class="line x" title="3:201	In this work, to measure the strength and polarity of an opinion, we consider the economic context in which the opinion is evaluated, instead of using human annotators or linguistic resources." ></td>
	<td class="line x" title="4:201	We rely on the fact that text in on-line systems influences the behavior of humans and this effect can be observed using some easy-to-measure economic variables, such as revenues or product prices." ></td>
	<td class="line x" title="5:201	By reversing the logic, we infer the semantic orientation and strength of an opinion by tracing the changes in the associated economic variable." ></td>
	<td class="line x" title="6:201	In effect, we use econometrics to identify the economic value of text and assign a dollar value to each opinion phrase, measuring sentiment effectively and without the need for manual labeling." ></td>
	<td class="line x" title="7:201	We argue that by interpreting opinions using econometrics, we have the first objective, quantifiable, and contextsensitive evaluation of opinions." ></td>
	<td class="line x" title="8:201	We make the discussion concrete by presenting results on the reputation system of Amazon.com." ></td>
	<td class="line x" title="9:201	We show that user feedback affects the pricing power of merchants and by measuring their pricing power we can infer the polarity and strength of the underlying feedback postings." ></td>
	<td class="line x" title="10:201	1 Introduction A significant number of websites today allow users to post articles where they express opinions about products, firms, people, and so on." ></td>
	<td class="line x" title="11:201	For example, users on Amazom.com post reviews about products they bought and users on eBay.com post feedback describing their experiences with sellers." ></td>
	<td class="line x" title="12:201	The goal of opinion mining systems is to identify such pieces of the text that express opinions (Breck et al. , 2007; Konig and Brill, 2006) and then measure the polarity and strength of the expressed opinions." ></td>
	<td class="line x" title="13:201	While intuitively the task seems straightforward, there are multiple challenges involved." ></td>
	<td class="line x" title="14:201	 What makes an opinion positive or negative?" ></td>
	<td class="line x" title="15:201	Is there an objective measure for this task?" ></td>
	<td class="line x" title="16:201	 How can we rank opinions according to their strength?" ></td>
	<td class="line x" title="17:201	Can we define an objective measure for ranking opinions?" ></td>
	<td class="line x" title="18:201	 How does the context change the polarity and strength of an opinion and how can we take the context into consideration?" ></td>
	<td class="line oc" title="19:201	To evaluate the polarity and strength of opinions, most of the existing approaches rely either on training from human-annotated data (Hatzivassiloglou and McKeown, 1997), or use linguistic resources (Hu and Liu, 2004; Kim and Hovy, 2004) like WordNet, or rely on co-occurrence statistics (Turney, 2002) between words that are unambiguously positive (e.g. , excellent) and unambiguously negative (e.g. , horrible)." ></td>
	<td class="line x" title="20:201	Finally, other approaches rely on reviews with numeric ratings from websites (Pang and Lee, 2002; Dave et al. , 2003; Pang and Lee, 2004; Cui et al. , 2006) and train (semi-)supervised learning algorithms to classify reviews as positive or negative, or in more fine-grained scales (Pang and Lee, 2005; Wilson et al. , 2006)." ></td>
	<td class="line x" title="21:201	Implicitly, the supervised learning techniques assume that numeric ratings fully encapsulate the sentiment of the review.416 In this paper, we take a different approach and instead consider the economic context in which an opinion is evaluated." ></td>
	<td class="line x" title="22:201	We observe that the text in on-line systems influence the behavior of the readers." ></td>
	<td class="line x" title="23:201	This effect can be measured by observing some easy-tomeasure economic variable, such as product prices." ></td>
	<td class="line x" title="24:201	For instance, online merchants on eBay with positive feedback can sell products for higher prices than competitors with negative evaluations." ></td>
	<td class="line x" title="25:201	Therefore, each of these (positive or negative) evaluations has a (positive or negative) effect on the prices that the merchant can charge." ></td>
	<td class="line x" title="26:201	For example, everything else being equal, a seller with speedy delivery may be able to charge $10 more than a seller with slow delivery." ></td>
	<td class="line x" title="27:201	Using this information, we can conclude that speedy is better than slow when applied to delivery and their difference is $10." ></td>
	<td class="line x" title="28:201	Thus, we can infer the semantic orientation and the strength of an evaluation from the changes in the observed economic variable." ></td>
	<td class="line x" title="29:201	Following this idea, we use techniques from econometrics to identify the economic value of text and assign a dollar value to each text snippet, measuring sentiment strength and polarity effectively and without the need for labeling or any other resource." ></td>
	<td class="line x" title="30:201	We argue that by interpreting opinions within an econometric framework, we have the first objective and context-sensitive evaluation of opinions." ></td>
	<td class="line x" title="31:201	For example, consider the comment good packaging, posted by a buyer to evaluate a merchant." ></td>
	<td class="line x" title="32:201	This comment would have been considered unambiguously positive by the existing opinion mining systems." ></td>
	<td class="line x" title="33:201	We observed, though, that within electronic markets, such as eBay, a posting that contains the words good packaging has actually negative effect on the power of a merchant to charge higher prices." ></td>
	<td class="line x" title="34:201	This surprising effect reflects the nature of the comments in online marketplaces: buyers tend to use superlatives and highly enthusiastic language to praise a good merchant, and a lukewarm good packaging is interpreted as negative." ></td>
	<td class="line x" title="35:201	By introducing the econometric interpretation of opinions we can effortlessly capture such challenging scenarios, something that is impossible to achieve with the existing approaches." ></td>
	<td class="line x" title="36:201	We focus our paper on reputation systems in electronic markets and we examine the effect of opinions on the pricing power of merchants in the marketplace of Amazon.com." ></td>
	<td class="line x" title="37:201	(We discuss more applications in Section 7)." ></td>
	<td class="line x" title="38:201	We demonstrate the value of our technique using a dataset with 9,500 transactions that took place over 180 days." ></td>
	<td class="line x" title="39:201	We show that textual feedback affects the power of merchants to charge higher prices than the competition, for the same product, and still make a sale." ></td>
	<td class="line x" title="40:201	We then reverse the logic and determine the contribution of each comment in the pricing power of a merchant." ></td>
	<td class="line x" title="41:201	Thus, we discover the polarity and strength of each evaluation without the need for human annotation or any other form of linguistic resource." ></td>
	<td class="line x" title="42:201	The structure of the rest of the paper is as follows." ></td>
	<td class="line x" title="43:201	Section 2 gives the basic background on reputation systems." ></td>
	<td class="line x" title="44:201	Section 3 describes our methodology for constructing the data set that we use in our experiments." ></td>
	<td class="line x" title="45:201	Section 4 shows how we combine established techniques from econometrics with text mining techniques to identify the strength and polarity of the posted feedback evaluations." ></td>
	<td class="line x" title="46:201	Section 5 presents the experimental evaluations of our techniques." ></td>
	<td class="line x" title="47:201	Finally, Section 6 discusses related work and Section 7 discusses further applications and concludes the paper." ></td>
	<td class="line x" title="48:201	2 Reputation Systems and Price Premiums When buyers purchase products in an electronic market, they assess and pay not only for the product they wish to purchase but for a set of fulfillment characteristics as well, e.g., packaging, delivery, and the extent to which the product description matches the actual product." ></td>
	<td class="line x" title="49:201	Electronic markets rely on reputation systems to ensure the quality of these characteristics for each merchant, and the importance of such systems is widely recognized in the literature (Resnick et al. , 2000; Dellarocas, 2003)." ></td>
	<td class="line x" title="50:201	Typically, merchants reputation in electronic markets is encoded by a reputation profile that includes: (a) the number of past transactions for the merchant, (b) a summary of numeric ratings from buyers who have completed transactions with the seller, and (c) a chronological list of textual feedback provided by these buyers." ></td>
	<td class="line x" title="51:201	Studies of online reputation, thus far, base a merchants reputation on the numeric rating that characterizes the seller (e.g. , average number of stars and number of completed transactions) (Melnik and Alm, 2002)." ></td>
	<td class="line x" title="52:201	The general conclusion of these studies show that merchants with higher (numeric) reputation can charge higher prices than the competition, for the same products, and still manage to make a sale." ></td>
	<td class="line x" title="53:201	This price premium that the merchants can command over the competition is a measure of their reputation." ></td>
	<td class="line x" title="54:201	Definition 2.1 Consider a set of merchants s1,,sn selling a product for prices p1,,pn." ></td>
	<td class="line x" title="55:201	If si makes417 Figure 1: A set of merchants on Amazon.com selling an identical product for different prices the sale for price pi, then si commands a price premium equal to pi  pj over sj and a relative price premium equal to pipjpi." ></td>
	<td class="line x" title="56:201	Hence, a transaction that involves n competing merchants generates n 1 price premiums.1 The average price premium for the transaction is summationtext jnegationslash=i(pipj) n1 and the average relative price premium is summationtext jnegationslash=i(pipj) pi(n1) . a50 Example 2.1 Consider the case in Figure 1 where three merchants sell the same product for $631.95, $632.26, and $637.05, respectively." ></td>
	<td class="line x" title="57:201	If GameHog sells the product, then the price premium against XP Passport is $4.79 (= $637.05$632.26) and against the merchant BuyPCsoft is $5.10." ></td>
	<td class="line x" title="58:201	The relative price premium is 0.75% and 0.8%, respectively." ></td>
	<td class="line x" title="59:201	Similarly, the average price premium for this transaction is $4.95 and the average relative price premium 0.78%." ></td>
	<td class="line x" title="60:201	a50 Different sellers in these markets derive their reputation from different characteristics: some sellers have a reputation for fast delivery, while some others have a reputation of having the lowest price among their peers." ></td>
	<td class="line x" title="61:201	Similarly, while some sellers are praised for their packaging in the feedback, others get good comments for selling high-quality goods but are criticized for being rather slow with shipping." ></td>
	<td class="line x" title="62:201	Even though previous studies have established the positive correlation between higher (numeric) reputation and higher price premiums, they ignored completely the role of the textual feedback and, in turn, the multi-dimensional nature of reputation in electronic markets." ></td>
	<td class="line x" title="63:201	We show that the textual feedback adds significant additional value to the numerical scores, and affects the pricing power of the merchants." ></td>
	<td class="line x" title="64:201	1As an alternative definition we can ignore the negative price premiums." ></td>
	<td class="line x" title="65:201	The experimental results are similar for both versions." ></td>
	<td class="line x" title="66:201	3 Data We compiled a data set using software resellers from publicly available information on software product listings at Amazon.com." ></td>
	<td class="line x" title="67:201	Our data set includes 280 individual software titles." ></td>
	<td class="line x" title="68:201	The sellers reputation matters when selling identical goods, and the price variation observed can be attributed primarily to variation in the merchants reputation." ></td>
	<td class="line x" title="69:201	We collected the data using Amazon Web Services over a period of 180 days, between October 2004 and March 2005." ></td>
	<td class="line x" title="70:201	We describe below the two categories of data that we collected." ></td>
	<td class="line x" title="71:201	Transaction Data: The first part of our data set contains details of the transactions that took place on the marketplace of Amazon.com for each of the software titles." ></td>
	<td class="line x" title="72:201	The Amazon Web Services associates a unique transaction ID for each unique product listed by a seller." ></td>
	<td class="line x" title="73:201	This transaction ID enables us to distinguish between multiple or successive listings of identical products sold by the same merchant." ></td>
	<td class="line x" title="74:201	Keeping with the methodology in prior research (Ghose et al. , 2006), we crawl the Amazons XML listings every 8 hours and when a transaction ID associated with a particular listing is removed, we infer that the listed product was successfully sold in the prior 8 hour window.2 For each transaction that takes place, we keep the price at which the product was sold and the merchants reputation at the time of the transaction (more on this later)." ></td>
	<td class="line x" title="75:201	Additionally, for each of the competing listings for identical products, we keep the listed price along with the competitors reputation." ></td>
	<td class="line x" title="76:201	Using the collected data, we compute the price premium variables for each transaction3 using Definition 2.1." ></td>
	<td class="line x" title="77:201	Overall, our data set contains 1,078 merchants, 9,484 unique transactions and 107,922 price premiums (recall that each transaction generates multiple price premiums)." ></td>
	<td class="line x" title="78:201	Reputation Data: The second part of our data set contains the reputation history of each merchant that had a (monitored) product for sale during our 180-day window." ></td>
	<td class="line x" title="79:201	Each of these merchants has a feedback profile, which consists of numerical scores and text-based feedback, posted by buyers." ></td>
	<td class="line x" title="80:201	We had an average of 4,932 postings per merchant." ></td>
	<td class="line x" title="81:201	The numerical ratings 2Amazon indicates that their seller listings remain on the site indefinitely until they are sold and sellers can change the price of the product without altering the transaction ID. 3Ideally, we would also include the tax and shipping cost charged by each merchant in the computation of the price premiums." ></td>
	<td class="line x" title="82:201	Unfortunately, we could not capture these costs using our methodology." ></td>
	<td class="line x" title="83:201	Assuming that the fees for shipping and tax are independent of the merchants reputation, our analysis is not affected.418 are provided on a scale of one to five stars." ></td>
	<td class="line x" title="84:201	These ratings are averaged to provide an overall score to the seller." ></td>
	<td class="line x" title="85:201	Note that we collect all feedback (both numerical and textual) associated with a seller over the entire lifetime of the seller and we reconstruct each sellers exact feedback profile at the time of each transaction." ></td>
	<td class="line x" title="86:201	4 Econometrics-based Opinion Mining In this section, we describe how we combine econometric techniques with NLP techniques to derive the semantic orientation and strength of the feedback evaluations." ></td>
	<td class="line x" title="87:201	Section 4.1 describes how we structure the textual feedback and Section 4.2 shows how we use econometrics to estimate the polarity and strength of the evaluations." ></td>
	<td class="line x" title="88:201	4.1 Retrieving the Dimensions of Reputation We characterize a merchant using a vector of reputation dimensions X = (X1,X2,,Xn), representing its ability on each of n dimensions." ></td>
	<td class="line x" title="89:201	We assume that each of these n dimensions is expressed by a noun, noun phrase, verb, or a verb phrase chosen from the set of all feedback postings, and that a merchant is evaluated on these n dimensions." ></td>
	<td class="line x" title="90:201	For example, dimension 1 might be shipping, dimension 2 might be packaging and so on." ></td>
	<td class="line x" title="91:201	In our model, each of these dimensions is assigned a numerical score." ></td>
	<td class="line x" title="92:201	Of course, when posting textual feedback, buyers do not assign explicit numeric scores to any dimension." ></td>
	<td class="line x" title="93:201	Rather, they use modifiers (typically adjectives or adverbs) to evaluate the seller along each of these dimensions (we describe how we assign numeric scores to each modifier in Section 4.2)." ></td>
	<td class="line x" title="94:201	Once we have identified the set of all dimensions, we can then parse each of the feedback postings, associate a modifier with each dimension, and represent a feedback posting as an n-dimensional vector  of modifiers." ></td>
	<td class="line x" title="95:201	Example 4.1 Suppose dimension 1 is delivery, dimension 2 is packaging, and dimension 3 is service. The feedback posting I was impressed by the speedy delivery!" ></td>
	<td class="line x" title="96:201	Great service! is then encoded as 1 = [speedy,NULL,great], while the posting The item arrived in awful packaging, and the delivery was slow is encoded as 2 = [slow,awful,NULL]." ></td>
	<td class="line x" title="97:201	a50 Let M = {NULL,1,,M} be the set of modifiers and consider a seller si with p postings in its reputation profile." ></td>
	<td class="line x" title="98:201	We denote with ijk M the modifier that appears in the j-th posting and is used to assess the k-th reputation dimension." ></td>
	<td class="line x" title="99:201	We then structure the merchants feedback as an np matrix M(si) whose rows are the p encoded vectors of modifiers associated with the seller." ></td>
	<td class="line x" title="100:201	We construct M(si) as follows: 1." ></td>
	<td class="line x" title="101:201	Retrieve the postings associated with a merchant." ></td>
	<td class="line x" title="102:201	2." ></td>
	<td class="line x" title="103:201	Parse the postings to identify the dimensions across which the buyer evaluates a seller, keeping4 the nouns, noun phrases, verbs, and verbal phrases as reputation characteristics.5." ></td>
	<td class="line x" title="104:201	3. Retrieve adjectives and adverbs that refer to6 dimensions (Step 2) and construct the  vectors." ></td>
	<td class="line x" title="105:201	We have implemented this algorithm on the feedback postings of each of our sellers." ></td>
	<td class="line x" title="106:201	Our analysis yields 151 unique dimensions, and a total of 142 modifiers (note that the same modifier can be used to evaluate multiple dimensions)." ></td>
	<td class="line x" title="107:201	4.2 Scoring the Dimensions of Reputation As discussed above, the textual feedback profile of merchant si is encoded as a np matrix M(si); the elements of this matrix belong to the set of modifiers M. In our case, we are interested in computing the score a(,d,j) that a modifier   M assigns to the dimension d, when it appears in the j-th posting." ></td>
	<td class="line x" title="108:201	Since buyers tend to read only the first few pages of text-based feedback, we weight higher the influence of recent text postings." ></td>
	<td class="line x" title="109:201	We model this by assuming that K is the number of postings that appear on each page (K = 25 on Amazon.com), and that c is the probability of clicking on the Next link and moving the next page of evaluations.7 This assigns a posting-specific weight rj = cfloorleft jKfloorright/summationtextpq=1 cfloorleft qKfloorright for the jth posting, where j is the rank of the posting, K is the number of postings per page, and p is the total number of postings for the given seller." ></td>
	<td class="line x" title="110:201	Then, we set a(,d,j) = rj a(,d) where a(,d) is the global score that modifier  assigns to dimension d. Finally, since each reputation dimension has potentially a different weight, we use a weight vector w to 4We eliminate all dimensions appearing in the profiles of less than 50 (out of 1078) merchants, since we cannot extract statistically meaningful results for such sparse dimensions 5The technique as described in this paper, considers words like shipping and  delivery as separate dimensions, although they refer to the same real-life dimension." ></td>
	<td class="line x" title="111:201	We can use Latent Dirichlet Allocation (Blei et al. , 2003) to reduce the number of dimensions, but this is outside the scope of this paper." ></td>
	<td class="line x" title="112:201	6To associate the adjectives and adverbs with the correct dimensions, we use the Collins HeadFinder capability of the Stanford NLP Parser." ></td>
	<td class="line x" title="113:201	7We report only results for c = 0.5." ></td>
	<td class="line x" title="114:201	We conducted experiments other values of c as well and the results are similar.419 weight the contribution of each reputation dimension to the overall reputation score (si) of seller si: (si) = rT A(M(si))w (1) where rT = [r1,r2,rp] is the vector of the postingspecific weights and A(M(i)) is a matrix that contains as element the score a(j,dk) where M(si) contains the modifier j in the column of the dimension dk." ></td>
	<td class="line x" title="115:201	If we model the buyers preferences as independently distributed along each dimension and each modifier score a(,dk) also as an independent random variable, then the random variable (si) is a sum of random variables." ></td>
	<td class="line x" title="116:201	Specifically, we have: (si) = Msummationdisplay j=1 nsummationdisplay k=1 (wk a(j,dk))R(j,dk) (2) where R(j,dk) is equal to the sum of the ri weights across all postings in which the modifier j modifies dimension dk." ></td>
	<td class="line x" title="117:201	We can easily compute the R(j,dk) values by simply counting appearances and weighting each appearance using the definition of ri." ></td>
	<td class="line x" title="118:201	The question is, of course, how to estimate the values of wk  a(j,dk), which determine the polarity and intensity of the modifier j modifying the dimension dk." ></td>
	<td class="line x" title="119:201	For this, we observe that the appearance of such modifier-dimension opinion phrases has an effect on the price premiums that a merchant can charge." ></td>
	<td class="line x" title="120:201	Hence, there is a correlation between the reputation scores () of the merchants and the price premiums observed for each transaction." ></td>
	<td class="line x" title="121:201	To discover the level of association, we use regression." ></td>
	<td class="line x" title="122:201	Since we are dealing with panel data, we estimate ordinary-leastsquares (OLS) regression with fixed effects (Greene, 2002), where the dependent variable is the price premium variable, and the independent variables are the reputation scores () of the merchants, together with a few other control variables." ></td>
	<td class="line x" title="123:201	Generally, we estimate models of the form: PricePremiumij = summationdisplay c Xcij +fij +epsilon1ij+ t1 (merchant)ij +t2 (competitor)ij (3) where PricePremiumij is one of the variations of price premium as given in Definition 2.1 for a seller si and product j, c, t1, and t2 are the regressor coefficients, Xc are the control variables, () are the text reputation scores (see Equation 1), fij denotes the fixed effects and epsilon1 is the error term." ></td>
	<td class="line x" title="124:201	In Section 5, we give the details about the control variables and the regression settings." ></td>
	<td class="line x" title="125:201	Interestingly, if we expand the () variables according to Equation 2, we can run the regression using the modifier-dimension pairs as independent variables, whose values are equal to the R(j,dk) values." ></td>
	<td class="line x" title="126:201	After running the regression, the coefficients assigned to each modifier-dimension pair correspond to the value wk  a(j,dk) for each modifier-dimension pair." ></td>
	<td class="line x" title="127:201	Therefore, we can easily estimate in economic terms the value of a particular modifier when used to evaluate a particular dimension." ></td>
	<td class="line x" title="128:201	5 Experimental Evaluation In this section, we first present the experimental settings (Section 5.1), and then we describe the results of our experimental evaluation (Section 5.2)." ></td>
	<td class="line x" title="129:201	5.1 Regression Settings In Equation 3 we presented the general form of the regression for estimating the scores a(j,dk)." ></td>
	<td class="line x" title="130:201	Since we want to eliminate the effect of any other factors that may influence the price premiums, we also use a set of control variables." ></td>
	<td class="line x" title="131:201	After all the control factors are taken into consideration, the modifier scores reflect the additional value of the text opinions." ></td>
	<td class="line x" title="132:201	Specifically, we used as control variables the products price on Amazon, the average star rating of the merchant, the number of merchants past transactions, and the number of sellers for the product." ></td>
	<td class="line x" title="133:201	First, we ran OLS regressions with product-seller fixed effects controlling for unobserved heterogeneity across sellers and products." ></td>
	<td class="line x" title="134:201	These fixed effects control for average product quality and differences in seller characteristics." ></td>
	<td class="line x" title="135:201	We run multiple variations of our model, using different versions of the price premium variable as listed in Definition 2.1." ></td>
	<td class="line x" title="136:201	We also tested variations where we include as independent variable not the individual reputation scores but the difference (merchant)(competitor)." ></td>
	<td class="line x" title="137:201	All regressions yielded qualitatively similar results, so due to space restrictions we only report results for the regressions that include all the control variables and all the text variables; we report results using the price premium as the dependent variable." ></td>
	<td class="line x" title="138:201	Our regressions in this setting contain 107,922 observations, and a total of 547 independent variables." ></td>
	<td class="line x" title="139:201	5.2 Experimental Results Recall of Extraction: The first step of our experimental evaluation is to examine whether the opinion extraction technique of Section 4.1 indeed captures all the reputation characteristics expressed in the feed-420 Dimension Human Recall Computer Recall Product Condition 0.76 0.76 Price 0.91 0.61 Package 0.96 0.66 Overall Experience 0.65 0.55 Delivery Speed 0.96 0.92 Item Description 0.22 0.43 Product Satisfaction 0.68 0.58 Problem Response 0.30 0.37 Customer Service 0.57 0.50 Average 0.66 0.60 Table 1: The recall of our technique compared to the recall of the human annotators back (recall) and whether the dimensions that we capture are accurate (precision)." ></td>
	<td class="line x" title="140:201	To examine the recall question, we used two human annotators." ></td>
	<td class="line x" title="141:201	The annotators read a random sample of 1,000 feedback postings, and identified the reputation dimensions mentioned in the text." ></td>
	<td class="line x" title="142:201	Then, they examined the extracted modifierdimension pairs for each posting and marked whether the modifier-dimension pairs captured the identified real reputation dimensions mentioned in the posting and which pairs were spurious, non-opinion phrases." ></td>
	<td class="line x" title="143:201	Both annotators identified nine reputation dimensions (see Table 1)." ></td>
	<td class="line x" title="144:201	Since the annotators did not agree in all annotations, we computed the average human recall hRecd = agreeddalld for each dimension d, where agreedd is the number of postings for which both annotators identified the reputation dimension d, and alld is the number of postings in which at least one annotator identified the dimension d. Based on the annotations, we computed the recall of our algorithm against each annotator." ></td>
	<td class="line x" title="145:201	We report the average recall for each dimension, together with the human recall in Table 1." ></td>
	<td class="line x" title="146:201	The recall of our technique is only slightly inferior to the performance of humans, indicating that the technique of Section 4.1 extracts the majority of the posted evaluations.8 Interestingly, precision is not an issue in our setting." ></td>
	<td class="line x" title="147:201	In our framework, if an particular modifier-dimension pair is just noise, then it is almost impossible to have a statistically significant correlation with the price premiums." ></td>
	<td class="line x" title="148:201	The noisy opinion phrases are statistically guaranteed to be filtered out by the regression." ></td>
	<td class="line x" title="149:201	Estimating Polarity and Strength: In Table 2, 8In the case of Item Description, where the computer recall was higher than the human recall, our technique identified almost all the phrases of one annotator, but the other annotator had a more liberal interpretation of Item Description dimension and annotated significantly more postings with the dimension Item Description than the other annotator, thus decreasing the human recall." ></td>
	<td class="line x" title="150:201	we present the modifier-dimension pairs (positive and negative) that had the strongest dollar value and were statistically significant across all regressions." ></td>
	<td class="line x" title="151:201	(Due to space issues, we cannot list the values for all pairs)." ></td>
	<td class="line x" title="152:201	These values reflect changes in the merchantss pricing power after taking their average numerical score and level of experience into account, and also highlight the additional the value contained in textbased reputation." ></td>
	<td class="line x" title="153:201	The examples that we list here illustrate that our technique generates a natural ranking of the opinion phrases, inferring the strength of each modifier within the context in which this opinion is evaluated." ></td>
	<td class="line x" title="154:201	This holds true even for misspelled evaluations that would break existing techniques based on annotation or on resources like WordNet." ></td>
	<td class="line x" title="155:201	Furthermore, these values reflect the context in which the opinion is evaluated." ></td>
	<td class="line x" title="156:201	For example, the pair good packaging has a dollar value of -$0.58." ></td>
	<td class="line x" title="157:201	Even though this seems counterintuitive, it actually reflects the nature of an online marketplace where most of the positive evaluations contain superlatives, and a mere good is actually interpreted by the buyers as a lukewarm, slightly negative evaluation." ></td>
	<td class="line x" title="158:201	Existing techniques cannot capture such phenomena." ></td>
	<td class="line x" title="159:201	Price Premiums vs. Ratings: One of the natural comparisons is to examine whether we could reach similar results by just using the average star rating associated with each feedback posting to infer the score of each opinion phrase." ></td>
	<td class="line x" title="160:201	The underlying assumption behind using the ratings is that the review is perfectly summarized by the star rating, and hence the text plays mainly an explanatory role and carries no extra information, given the star rating." ></td>
	<td class="line x" title="161:201	For this, we examined the R2 fit of the regression, with and without the use of the text variables." ></td>
	<td class="line x" title="162:201	Without the use of text variables, the R2 was 0.35, while when using only the text-based regressors, the R2 fit increased to 0.63." ></td>
	<td class="line x" title="163:201	This result clearly indicates that the actual text contains significantly more information than the ratings." ></td>
	<td class="line x" title="164:201	We also experimented with predicting which merchant will make a sale, if they simultaneously sell the same product, based on their listed prices and on their numeric and text reputation." ></td>
	<td class="line x" title="165:201	Our C4.5 classifier (Quinlan, 1992) takes a pair of merchants and decides which of the two will make a sale." ></td>
	<td class="line x" title="166:201	We used as training set the transactions that took place in the first four months and as test set the transactions in the last two months of our data set." ></td>
	<td class="line x" title="167:201	Table 3 summarizes the results for different sets of features used." ></td>
	<td class="line x" title="168:201	The 55%421 Modifier Dimension Dollar Value [wonderful experience] $5.86 [outstanding seller] $5.76 [excellant service] $5.27 [lightning delivery] $4.84 [highly recommended] $4.15 [best seller] $3.80 [perfectly packaged] $3.74 [excellent condition] $3.53 [excellent purchase] $3.22 [excellent seller] $2.70 [excellent communication] $2.38 [perfect item] $1.92 [terrific condition] $1.87 [top quality] $1.67 [awesome service] $1.05 [A+++ seller] $1.03 [great merchant] $0.93 [friendly service] $0.81 [easy service] $0.78 [never received] -$7.56 [defective product] -$6.82 [horible experience] -$6.79 [never sent] -$6.69 [never recieved] -$5.29 [bad experience] -$5.26 [cancelled order] -$5.01 [never responded] -$4.87 [wrong product] -$4.39 [not as advertised] -$3.93 [poor packaging] -$2.92 [late shipping] -$2.89 [wrong item] -$2.50 [not yet received] -$2.35 [still waiting] -$2.25 [wrong address] -$1.54 [never buy] -$1.48 Table 2: The highest scoring opinion phrases, as determined by the product wk a(j,dk)." ></td>
	<td class="line x" title="169:201	accuracy when using only prices as features indicates that customers rarely choose a product based solely on price." ></td>
	<td class="line x" title="170:201	Rather, as indicated by the 74% accuracy, they also consider the reputation of the merchants." ></td>
	<td class="line x" title="171:201	However, the real value of the postings relies on the text and not on the numeric ratings: the accuracy is 87%89% when using the textual reputation variables." ></td>
	<td class="line x" title="172:201	In fact, text subsumes the numeric variables but not vice versa, as indicated by the results in Table 3." ></td>
	<td class="line x" title="173:201	6 Related Work To the best of our knowledge, our work is the first to use economics for measuring the effect of opinions and deriving their polarity and strength in an econometric manner." ></td>
	<td class="line x" title="174:201	A few papers in the past tried to combine text analysis with economics (Das and Chen, 2006; Lewitt and Syverson, 2005), but the text analysis was limited to token counting and did not use Features Accuracy on Test Set Price 55% Price + Numeric Reputation 74% Price + Numeric Reputation 89% + Text Reputation Price + Text Reputation 87% Table 3: Predicting the merchant who makes the sale." ></td>
	<td class="line x" title="175:201	any NLP techniques." ></td>
	<td class="line x" title="176:201	The technique of Section 4.1 is based on existing research in sentiment analysis." ></td>
	<td class="line x" title="177:201	For instance, (Hatzivassiloglou and McKeown, 1997; Nigam and Hurst, 2004) use annotated data to create a supervised learning technique to identify the semantic orientation of adjectives." ></td>
	<td class="line oc" title="178:201	We follow the approach by Turney (2002), who note that the semantic orientation of an adjective depends on the noun that it modifies and suggest using adjective-noun or adverb-verb pairs to extract semantic orientation." ></td>
	<td class="line o" title="179:201	However, we do not rely on linguistic resources (Kamps and Marx, 2002) or on search engines (Turney and Littman, 2003) to determine the semantic orientation, but rather rely on econometrics for this task." ></td>
	<td class="line x" title="180:201	Hu and Liu (2004), whose study is the closest to our work, use WordNet to compute the semantic orientation of product evaluations and try to summarize user reviews by extracting the positive and negative evaluations of the different product features." ></td>
	<td class="line x" title="181:201	Similarly, Snyder and Barzilay (2007) decompose an opinion across several dimensions and capture the sentiment across each dimension." ></td>
	<td class="line x" title="182:201	Other work in this area includes (Lee, 2004; Popescu and Etzioni, 2005) which uses text mining in the context product reviews, but none uses the economic context to evaluate the opinions." ></td>
	<td class="line x" title="183:201	7 Conclusion and Further Applications We demonstrated the value of using econometrics for extracting a quantitative interpretation of opinions." ></td>
	<td class="line x" title="184:201	Our technique, additionally, takes into consideration the context within which these opinions are evaluated." ></td>
	<td class="line x" title="185:201	Our experimental results show that our techniques can capture the pragmatic meaning of the expressed opinions using simple economic variables as a form of training data." ></td>
	<td class="line x" title="186:201	The source code with our implementation together with the data set used in this paper are available from http://economining.stern.nyu.edu." ></td>
	<td class="line x" title="187:201	There are many other applications beyond reputation systems." ></td>
	<td class="line x" title="188:201	For example, using sales rank data from Amazon.com, we can examine the effect of product reviews on product sales and detect the weight that422 customers put on different product features; furthermore, we can discover how customer evaluations on individual product features affect product sales and extract the pragmatic meaning of these evaluations." ></td>
	<td class="line x" title="189:201	Another application is the analysis of the effect of news stories on stock prices: we can examine what news topics are important for the stock market and see how the views of different opinion holders and the wording that they use can cause the market to move up or down." ></td>
	<td class="line x" title="190:201	In a slightly different twist, we can analyze news stories and blogs in conjunction with results from prediction markets and extract the pragmatic effect of news and blogs on elections or other political events." ></td>
	<td class="line x" title="191:201	Another research direction is to examine the effect of summarizing product descriptions on product sales: short descriptions reduce the cognitive load of consumers but increase their uncertainty about the underlying product characteristics; a longer description has the opposite effect." ></td>
	<td class="line x" title="192:201	The optimum description length is the one that balances both effects and maximizes product sales." ></td>
	<td class="line x" title="193:201	Similar approaches can improve the state of art in both economics and computational linguistics." ></td>
	<td class="line x" title="194:201	In economics and in social sciences in general, most researchers handle textual data manually or with simplistic token counting techniques; in the worst case they ignore text data altogether." ></td>
	<td class="line x" title="195:201	In computational linguistics, researchers often rely on human annotators to generate training data, a laborious and errorprone task." ></td>
	<td class="line x" title="196:201	We believe that cross-fertilization of ideas between the fields of computational linguistics and econometrics can be beneficial for both fields." ></td>
	<td class="line x" title="197:201	Acknowledgments The authors would like to thank Elena Filatova for the useful discussions and the pointers to related literature." ></td>
	<td class="line x" title="198:201	We also thank Sanjeev Dewan, Alok Gupta, Bin Gu, and seminar participants at Carnegie Mellon University, Columbia University, Microsoft Research, New York University, Polytechnic University, and University of Florida for their comments and feedback." ></td>
	<td class="line x" title="199:201	We thank Rhong Zheng for assistance in data collection." ></td>
	<td class="line x" title="200:201	This work was partially supported by a Microsoft Live Labs Search Award, a Microsoft Virtual Earth Award, and by NSF grants IIS-0643847 and IIS-0643846." ></td>
	<td class="line x" title="201:201	Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the Microsoft Corporation or of the National Science Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1055
Structured Models for Fine-to-Coarse Sentiment Analysis
McDonald, Ryan;Hannan, Kerry;Neylon, Tyler;Wells, Mike;Reynar, Jeffrey C.;"></td>
	<td class="line x" title="1:215	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432439, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:215	c2007 Association for Computational Linguistics Structured Models for Fine-to-Coarse Sentiment Analysis Ryan McDonald Kerry Hannan Tyler Neylon Mike Wells Jeff Reynar Google, Inc. 76 Ninth Avenue New York, NY 10011 Contact email: ryanmcd@google.com Abstract In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="3:215	Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions." ></td>
	<td class="line x" title="4:215	The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another." ></td>
	<td class="line x" title="5:215	Experiments show that this method can significantly reduce classification error relative to models trained in isolation." ></td>
	<td class="line x" title="6:215	1 Introduction Extractingsentimentfromtextisachallengingproblem with applications throughout Natural Language Processing and Information Retrieval." ></td>
	<td class="line oc" title="7:215	Previous workonsentimentanalysishascoveredawiderange of tasks, including polarity classification (Pang et al. , 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al. , 2005; Choi et al. , 2006)." ></td>
	<td class="line oc" title="8:215	Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al. , 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al. , 2005), as well as the speaker level in debates (Thomas et al. , 2006)." ></td>
	<td class="line x" title="9:215	The ability to classify sentiment on multiple levels is importantsincedifferentapplicationshavedifferentneeds." ></td>
	<td class="line x" title="10:215	For example, a summarization system for product reviews might require polarity classification at the sentence or phrase level; a question answering system would most likely require the sentiment of paragraphs; and a system that determines which articles from an online news source are editorial in nature would require a document level analysis." ></td>
	<td class="line x" title="11:215	This work focuses on models that jointly classify sentimentonmultiplelevelsofgranularity." ></td>
	<td class="line x" title="12:215	Consider the following example, This is the first Mp3 player that I have used I thought it sounded great  After only a few weeks, it started having trouble with the earphone connection  I wont be buying another." ></td>
	<td class="line x" title="13:215	Mp3 player review from Amazon.com Thisexcerptexpressesanoverallnegativeopinionof the product being reviewed." ></td>
	<td class="line x" title="14:215	However, not all parts of the review are negative." ></td>
	<td class="line x" title="15:215	The first sentence merely provides some context on the reviewers experience with such devices and the second sentence indicates that, at least in one regard, the product performed well." ></td>
	<td class="line x" title="16:215	We call the problem of identifying the sentiment of the document and of all its subcomponents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis." ></td>
	<td class="line x" title="17:215	The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity." ></td>
	<td class="line x" title="18:215	There are, however, obvious advantages to building a single model that classifies each level in tandem." ></td>
	<td class="line x" title="19:215	Consider the sentence, My 11 year old daughter has also been using it and it is a lot harder than it looks." ></td>
	<td class="line x" title="20:215	Inisolation, thissentenceappearstoconveynegative sentiment." ></td>
	<td class="line x" title="21:215	However, it is part of a favorable review 432 for a piece of fitness equipment, where hard essentially means good workout." ></td>
	<td class="line x" title="22:215	In this domain, hards sentiment can only be determined in context (i.e. , hard toassembleversusa hard workout)." ></td>
	<td class="line x" title="23:215	Iftheclassifierknewtheoverallsentimentofadocument, then disambiguating such cases would be easier." ></td>
	<td class="line x" title="24:215	Conversely, document level analysis can benefit from finer level classification by taking advantage of common discourse cues, such as the last sentence being a reliable indicator for overall sentiment in reviews." ></td>
	<td class="line x" title="25:215	Furthermore, during training, the model will not need to modify its parameters to explain phenomena like the typically positive word great appearing in a negative text (as is the case above)." ></td>
	<td class="line x" title="26:215	The model can also avoid overfitting to features derived from neutral or objective sentences." ></td>
	<td class="line x" title="27:215	In fact, it has already been established that sentence level classification can improve document level analysis (Pang and Lee, 2004)." ></td>
	<td class="line x" title="28:215	This line of reasoning suggests that a cascaded approach would also be insufficient." ></td>
	<td class="line x" title="29:215	Valuable information is passed in both directions, which means any model of fine-to-coarse analysis should account for this." ></td>
	<td class="line x" title="30:215	In Section 2 we describe a simple structured model that jointly learns and infers sentiment on different levels of granularity." ></td>
	<td class="line x" title="31:215	In particular, we reduce the problem of joint sentence and document level analysis to a sequential classification problem using constrained Viterbi inference." ></td>
	<td class="line x" title="32:215	Extensions to the model that move beyond just two-levels of analysis are also presented." ></td>
	<td class="line x" title="33:215	In Section 3 an empirical evaluation of the model is given that shows significant gains in accuracy over both single level classifiers and cascaded systems." ></td>
	<td class="line x" title="34:215	1.1 Related Work The models in this work fall into the broad class of globalstructuredmodels, whicharetypicallytrained with structured learning algorithms." ></td>
	<td class="line x" title="35:215	Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followedbydiscriminativelearningapproachessuch as conditional random fields (CRFs) (Lafferty et al. , 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al. , 2003; Tsochantaridis et al. , 2004; McDonald et al. , 2005; Daume III et al. , 2006)." ></td>
	<td class="line x" title="36:215	These algorithms are usually applied to sequential labeling or chunking, but have also been applied to parsing (Taskar et al. , 2004; McDonald et al. , 2005), machine translation (Liang et al. , 2006) and summarization (Daume III et al. , 2006)." ></td>
	<td class="line x" title="37:215	Structured models have previously been used for sentiment analysis." ></td>
	<td class="line x" title="38:215	Choi et al.(2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions." ></td>
	<td class="line x" title="40:215	Mao and Lebanon (2006) used a sequential CRF regression model to measure polarity on the sentence level in order to determine the sentiment flow of authors in reviews." ></td>
	<td class="line x" title="41:215	Here we show that fine-to-coarse models of sentiment can often be reduced to the sequential case." ></td>
	<td class="line x" title="42:215	Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004)." ></td>
	<td class="line x" title="43:215	In that work an initial model classified each sentence as being subjective or objective using a global mincut inference algorithm that considered local labeling consistencies." ></td>
	<td class="line x" title="44:215	The top subjective sentences are then input into a standard document level polarity classifier with improved results." ></td>
	<td class="line x" title="45:215	The current work differs from that in Pang and Lee through the use of a single joint structured model for both sentence and document level analysis." ></td>
	<td class="line x" title="46:215	Many problems in natural language processing can be improved by learning and/or predicting multiple outputs jointly." ></td>
	<td class="line x" title="47:215	This includes parsing and relation extraction (Miller et al. , 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al. , 2004)." ></td>
	<td class="line x" title="48:215	One interesting work on sentiment analysis isthatofPopescuandEtzioni(2005)whichattempts to classify the sentiment of phrases with respect to possible product features." ></td>
	<td class="line x" title="49:215	To do this an iterative algorithm is used that attempts to globally maximize the classification of all phrases while satisfying local consistency constraints." ></td>
	<td class="line x" title="50:215	2 Structured Model In this section we present a structured model for fine-to-coarse sentiment analysis." ></td>
	<td class="line x" title="51:215	We start by examining the simple case with two-levels of granularity  the sentence and document  and show that the problem can be reduced to sequential classification with constrained inference." ></td>
	<td class="line x" title="52:215	We then discuss the feature space and give an algorithm for learning the parameters based on large-margin structured learning." ></td>
	<td class="line x" title="53:215	433 Extensions to the model are also examined." ></td>
	<td class="line x" title="54:215	2.1 A Sentence-Document Model Let Y(d) be a discrete set of sentiment labels at the document level and Y(s) be a discrete set of sentiment labels at the sentence level." ></td>
	<td class="line x" title="55:215	As input a system is given a document containing sentences s = s1,,sn and must produce sentiment labels for the document, yd  Y(d), and each individual sentence, ys = ys1,,ysn, where ysi  Y(s)  1  i  n. Define y = (yd,ys) = (yd,ys1,,ysn) as the joint labeling of the document and sentences." ></td>
	<td class="line x" title="56:215	For instance, in Pang and Lee (2004), yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective." ></td>
	<td class="line x" title="57:215	The models presented here are compatible with arbitrary sets of discrete output labels." ></td>
	<td class="line x" title="58:215	Figure 1 presents a model for jointly classifying the sentiment of both the sentences and the document." ></td>
	<td class="line x" title="59:215	In this undirected graphical model, the label of each sentence is dependent on the labels of its neighbouring sentences plus the label of the document." ></td>
	<td class="line x" title="60:215	The label of the document is dependent on the label of every sentence." ></td>
	<td class="line x" title="61:215	Note that the edges between the input (each sentence) and the output labels are not solid, indicating that they are given as input and are not being modeled." ></td>
	<td class="line x" title="62:215	The fact that the sentiment of sentences is dependent not only on the local sentiment of other sentences, but also the global document sentiment  and vice versa  allows the model to directly capture the importance of classification decisions across levels in fine-tocoarse sentiment analysis." ></td>
	<td class="line x" title="63:215	The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a min-cut algorithm." ></td>
	<td class="line x" title="64:215	However, jointly modeling the document label and allowing for non-binary labelscomplicatesmin-cut stylesolutionsasinference becomes intractable." ></td>
	<td class="line x" title="65:215	Learning and inference in undirected graphical models is a well studied problem in machine learning and NLP." ></td>
	<td class="line x" title="66:215	For example, CRFs define the probability over the labels conditioned on the input using the property that the joint probability distribution over the labels factors over clique potentials in undirected graphical models (Lafferty et al. , 2001)." ></td>
	<td class="line x" title="67:215	Figure 1: Sentence and document level model." ></td>
	<td class="line x" title="68:215	In this work we will use structured linear classifiers (Collins, 2002)." ></td>
	<td class="line x" title="69:215	We denote the score of a labeling y for an input s as score(y,s) and define this score as the sum of scores over each clique, score(y,s) = score((yd,ys),s) = score((yd,ys1,,ysn),s) = nsummationdisplay i=2 score(yd,ysi1,ysi,s) where each clique score is a linear combination of features and their weights, score(yd,ysi1,ysi,s) = wf(yd,ysi1,ysi,s) (1) and f is a high dimensional feature representation of the clique and w a corresponding weight vector." ></td>
	<td class="line x" title="70:215	Note that s is included in each score since it is given as input and can always be conditioned on." ></td>
	<td class="line x" title="71:215	Ingeneral, inferenceinundirectedgraphicalmodels is intractable." ></td>
	<td class="line x" title="72:215	However, for the common case of sequences(a.k.a.linear-chain models)theViterbialgorithm can be used (Rabiner, 1989; Lafferty et al. , 2001)." ></td>
	<td class="line x" title="73:215	Fortunately there is a simple technique that reduces inference in the above model to sequence classification with a constrained version of Viterbi." ></td>
	<td class="line x" title="74:215	2.1.1 Inference as Sequential Labeling The inference problem is to find the highest scoring labeling y for an input s, i.e., argmax y score(y,s) If the document label yd is fixed, then inference in the model from Figure 1 reduces to the sequential case." ></td>
	<td class="line x" title="75:215	This is because the search space is only over the sentence labels ysi, whose graphical structure forms a chain." ></td>
	<td class="line x" title="76:215	Thus the problem of finding the 434 Input: s = s1,,sn 1." ></td>
	<td class="line x" title="77:215	y = null 2." ></td>
	<td class="line x" title="78:215	for each yd  Y(d) 3." ></td>
	<td class="line x" title="79:215	ys = argmaxys score((yd,ys),s) 4." ></td>
	<td class="line x" title="80:215	yprime = (yd,ys) 5." ></td>
	<td class="line x" title="81:215	if score(yprime,s) > score(y,s) or y = null 6." ></td>
	<td class="line x" title="82:215	y = yprime 7." ></td>
	<td class="line x" title="83:215	return y Figure 2: Inference algorithm for model in Figure 1." ></td>
	<td class="line x" title="84:215	The argmax in line 3 can be solved using Viterbis algorithm since yd is fixed." ></td>
	<td class="line x" title="85:215	highest scoring sentiment labels for all sentences, given a particular document label yd, can be solved efficiently using Viterbis algorithm." ></td>
	<td class="line x" title="86:215	The general inference problem can then be solved by iterating over each possible yd, finding ys maximizing score((yd,ys),s) and keeping the single best y = (yd,ys)." ></td>
	<td class="line x" title="87:215	This algorithm is outlined in Figure 2 and has a runtime of O(|Y(d)||Y(s)|2n), due to running Viterbi |Y(d)| times over a label space of size |Y(s)|." ></td>
	<td class="line x" title="88:215	The algorithm can be extended to produce exact k-best lists." ></td>
	<td class="line x" title="89:215	This is achieved by using k-best Viterbi techniques to return the k-best global labelings for each document label in line 3." ></td>
	<td class="line x" title="90:215	Merging these sets will produce the final k-best list." ></td>
	<td class="line x" title="91:215	It is possible to view the inference algorithm in Figure 2 as a constrained Viterbi search since it is equivalent to flattening the model in Figure 1 to a sequential model with sentence labels from the set Y(s)  Y(d)." ></td>
	<td class="line x" title="92:215	The resulting Viterbi search would then need to be constrained to ensure consistent solutions, i.e., the label assignments agree on the document label over all sentences." ></td>
	<td class="line x" title="93:215	If viewed this way, it is also possible to run a constrained forwardbackward algorithm and learn the parameters for CRFs as well." ></td>
	<td class="line x" title="94:215	2.1.2 Feature Space In this section we define the feature representation for each clique, f(yd,ysi1,ysi,s)." ></td>
	<td class="line x" title="95:215	Assume that each sentence si is represented by a set of binary predicates P(si)." ></td>
	<td class="line x" title="96:215	This set can contain any predicate over the input s, but for the present purposes it will include all the unigram, bigram and trigrams in the sentence si conjoined with their part-of-speech (obtained from an automatic classifier)." ></td>
	<td class="line x" title="97:215	Back-offs of each predicate are also included where one or more word is discarded." ></td>
	<td class="line x" title="98:215	For instance, if P(si) contains the predicate a:DT great:JJ product:NN, then it would also have the predicates a:DT great:JJ *:NN, a:DT *:JJ product:NN, *:DT great:JJ product:NN, a:DT *:JJ *:NN, etc. Each predicate, p, is then conjoined with the label information to construct a binary feature." ></td>
	<td class="line x" title="99:215	For example, if the sentence label set is Y(s) = {subj,obj} and the document set is Y(d) = {pos,neg}, then the system might contain the following feature, f(j)(yd,ysi1,ysi,s) =     1 if p  P(si) and ysi1 = obj and ysi = subj and yd = neg 0 otherwise Where f(j) is the jth dimension of the feature space." ></td>
	<td class="line x" title="100:215	For each feature, a set of back-off features are included that only consider the document label yd, the current sentence label ysi, the current sentence and document label ysi and yd, and the current and previous sentence labels ysi and ysi1." ></td>
	<td class="line x" title="101:215	Note that through these back-off features the joint models feature set will subsume the feature set of any individual level model." ></td>
	<td class="line x" title="102:215	Only features observed in the training data were considered." ></td>
	<td class="line x" title="103:215	Depending on the data set, the dimension of the feature vector f ranged from 350K to 500K." ></td>
	<td class="line x" title="104:215	Though the feature vectors can be sparse, the feature weights will be learned using large-margin techniques that are well known to be robust to large and sparse feature representations." ></td>
	<td class="line x" title="105:215	2.1.3 Training the Model Let Y = Y(d)  Y(s)n be the set of all valid sentence-document labelings for an input s. The weights, w, are set using the MIRA learning algorithm, which is an inference based online largemargin learning technique (Crammer and Singer, 2003; McDonald et al. , 2005)." ></td>
	<td class="line x" title="106:215	An advantage of this algorithm is that it relies only on inference to learn the weight vector (see Section 2.1.1)." ></td>
	<td class="line x" title="107:215	MIRA has been shown to provide state-of-the-art accuracy for many language processing tasks including parsing, chunking and entity extraction (McDonald, 2006)." ></td>
	<td class="line x" title="108:215	The basic algorithm is outlined in Figure 3." ></td>
	<td class="line x" title="109:215	The algorithm works by considering a single training instance during each iteration." ></td>
	<td class="line x" title="110:215	The weight vector w is updated in line 4 through a quadratic programming problem." ></td>
	<td class="line x" title="111:215	This update modifies the weight vector so 435 Training data: T = {(yt,st)}Tt=1 1." ></td>
	<td class="line x" title="112:215	w(0) = 0; i = 0 2." ></td>
	<td class="line x" title="113:215	for n : 1N 3." ></td>
	<td class="line x" title="114:215	for t : 1T 4." ></td>
	<td class="line x" title="115:215	w(i+1) = argminw*  w*w(i)   s.t. score(yt,st) score(yprime,s)  L(yt,yprime) relative to w* yprime  C  Y, where |C| = k 5." ></td>
	<td class="line x" title="116:215	i = i + 1 6." ></td>
	<td class="line x" title="117:215	return w(NT) Figure 3: MIRA learning algorithm." ></td>
	<td class="line x" title="118:215	that the score of the correct labeling is larger than the score of every labeling in a constraint set C with a margin proportional to the loss." ></td>
	<td class="line x" title="119:215	The constraint set C can be chosen arbitrarily, but it is usually taken to be the k labelings that have the highest score under the old weight vector w(i) (McDonald et al. , 2005)." ></td>
	<td class="line x" title="120:215	In this manner, the learning algorithm can update its parameters relative to those labelings closest to the decisionboundary." ></td>
	<td class="line x" title="121:215	Ofalltheweightvectorsthatsatisfy these constraints, MIRA chooses the one that is as close as possible to the previous weight vector in order to retain information about previous updates." ></td>
	<td class="line x" title="122:215	The loss function L(y,yprime) is a positive real valued function and is equal to zero when y = yprime." ></td>
	<td class="line x" title="123:215	This function is task specific and is usually the hamming loss for sequence classification problems (Taskar et al. , 2003)." ></td>
	<td class="line x" title="124:215	Experiments with different loss functions forthe jointsentence-document modelon adevelopment data set indicated that the hamming loss over sentence labels multiplied by the 0-1 loss over document labels worked best." ></td>
	<td class="line x" title="125:215	An important modification that was made to the learning algorithm deals with how the k constraints arechosenfortheoptimization." ></td>
	<td class="line x" title="126:215	Typicallytheseconstraints are the k highest scoring labelings under the current weight vector." ></td>
	<td class="line x" title="127:215	However, early experiments showed that the model quickly learned to discard any labeling with an incorrect document label for the instances in the training set." ></td>
	<td class="line x" title="128:215	As a result, the constraints were dominated by labelings that only differed over sentence labels." ></td>
	<td class="line x" title="129:215	This did not allow the algorithm adequate opportunity to set parameters relative to incorrect document labeling decisions." ></td>
	<td class="line x" title="130:215	To combat this, k was divided by the number of document labels, to get a new value kprime." ></td>
	<td class="line x" title="131:215	For each document label, the kprime highest scoring labelings were Figure 4: An extension to the model from Figure 1 incorporating paragraph level analysis." ></td>
	<td class="line x" title="132:215	extracted." ></td>
	<td class="line x" title="133:215	Each of these sets were then combined to produce the final constraint set." ></td>
	<td class="line x" title="134:215	This allowed constraints to be equally distributed amongst different document labels." ></td>
	<td class="line x" title="135:215	Based on performance on the development data set the number of training iterations was set to N = 5 and the number of constraints to k = 10." ></td>
	<td class="line x" title="136:215	Weight averaging was also employed (Collins, 2002), which helped improve performance." ></td>
	<td class="line x" title="137:215	2.2 Beyond Two-Level Models To this point, we have focused solely on a model for two-level fine-to-coarse sentiment analysis not only for simplicity, but because the experiments in Section 3 deal exclusively with this scenario." ></td>
	<td class="line x" title="138:215	In this section, we briefly discuss possible extensions for more complex situations." ></td>
	<td class="line x" title="139:215	For example, longer documents might benefit from an analysis on the paragraph level as well as the sentence and document levels." ></td>
	<td class="line x" title="140:215	One possible model for this case is given in Figure 4, which essentially inserts an additional layer between the sentence and document level from the original model." ></td>
	<td class="line x" title="141:215	Sentence level analysis is dependent on neighbouring sentences as well as the paragraph level analysis, and the paragraph analysis is dependent on each of the sentences within it, the neighbouring paragraphs, and the document level analysis." ></td>
	<td class="line x" title="142:215	This can be extended to an arbitrary level of fine-to-coarse sentiment analysis by simply inserting new layers in this fashion to create more complex hierarchical models." ></td>
	<td class="line x" title="143:215	The advantage of using hierarchical models of this form is that they are nested, which keeps inference tractable." ></td>
	<td class="line x" title="144:215	Observe that each pair of adjacent levels in the model is equivalent to the original model from Figure 1." ></td>
	<td class="line x" title="145:215	As a result, the scores of the every label at each node in the graph can be calculated with a straight-forward bottom-up dynamic programming algorithm." ></td>
	<td class="line x" title="146:215	Details are omitted 436 Sentence Stats Document Stats Pos Neg Neu Tot Pos Neg Tot Car 472 443 264 1179 98 80 178 Fit 568 635 371 1574 92 97 189 Mp3 485 464 214 1163 98 89 187 Tot 1525 1542 849 3916 288 266 554 Table 1: Data statistics for corpus." ></td>
	<td class="line x" title="147:215	Pos = positive polarity, Neg = negative polarity, Neu = no polarity." ></td>
	<td class="line x" title="148:215	for space reasons." ></td>
	<td class="line x" title="149:215	Other models are possible where dependencies occur across non-neighbouring levels, e.g., by inserting edges between the sentence level nodes and the document level node." ></td>
	<td class="line x" title="150:215	In the general case, inference is exponential in the size of each clique." ></td>
	<td class="line x" title="151:215	Both the models in Figure 1 and Figure 4 have maximum clique sizes of three." ></td>
	<td class="line x" title="152:215	3 Experiments 3.1 Data To test the model we compiled a corpus of 600 online product reviews from three domains: car seats forchildren, fitnessequipment, andMp3players." ></td>
	<td class="line x" title="153:215	Of the original 600 reviews that were gathered, we discarded duplicate reviews, reviews with insufficient text, and spam." ></td>
	<td class="line x" title="154:215	All reviews were labeled by onlinecustomersashavingapositiveornegativepolarity on the document level, i.e., Y(d) = {pos,neg}." ></td>
	<td class="line x" title="155:215	Each review was then split into sentences and every sentence annotated by a single annotator as either being positive, negative or neutral, i.e., Y(s) = {pos,neg,neu}." ></td>
	<td class="line x" title="156:215	Data statistics for the corpus are given in Table 1." ></td>
	<td class="line x" title="157:215	All sentences were annotated based on their context within the document." ></td>
	<td class="line x" title="158:215	Sentences were annotated as neutral if they conveyed no sentiment or had indeterminate sentiment from their context." ></td>
	<td class="line x" title="159:215	Many neutral sentences pertain to the circumstances under which the product was purchased." ></td>
	<td class="line x" title="160:215	A common class of sentences were those containing product features." ></td>
	<td class="line x" title="161:215	These sentences were annotated as having positive or negative polarity if the context supported it." ></td>
	<td class="line x" title="162:215	This could include punctuation such as exclamation points, smiley/frowny faces, question marks, etc. The supporting evidence could also come from another sentence, e.g., I love it." ></td>
	<td class="line x" title="163:215	It has 64Mb of memory and comes with a set of earphones." ></td>
	<td class="line x" title="164:215	3.2 Results Three baseline systems were created,  Document-Classifier is a classifier that learns to predict the document label only." ></td>
	<td class="line x" title="165:215	 Sentence-Classifier is a classifier that learns to predict sentence labels in isolation of one another, i.e., without consideration for either the document or neighbouring sentences sentiment." ></td>
	<td class="line x" title="166:215	 Sentence-Structured is another sentence classifier, but this classifier uses a sequential chain model to learn and classify sentences." ></td>
	<td class="line x" title="167:215	The thirdbaselineisessentiallythemodelfromFigure1withoutthetopleveldocumentnode." ></td>
	<td class="line x" title="168:215	This baselinewillhelptogagetheempiricalgainsof the different components of the joint structured model on sentence level classification." ></td>
	<td class="line x" title="169:215	The model described in Section 2 will be called Joint-Structured." ></td>
	<td class="line x" title="170:215	All models use the same basic predicate space: unigram, bigram, trigram conjoined with part-of-speech, plus back-offs of these (see Section 2.1.2 for more)." ></td>
	<td class="line x" title="171:215	However, due to the structure of the model and its label space, the feature space of each might be different, e.g., the document classifier will only conjoin predicates with the document label to create the feature set." ></td>
	<td class="line x" title="172:215	All models are trained using the MIRA learning algorithm." ></td>
	<td class="line x" title="173:215	Results for each model are given in the first four rows of Table 2." ></td>
	<td class="line x" title="174:215	These results were gathered using 10-fold cross validation with one fold for development and the other nine folds for evaluation." ></td>
	<td class="line x" title="175:215	This table shows that classifying sentences in isolation from one another is inferior to accounting for a more global context." ></td>
	<td class="line x" title="176:215	A significant increase in performance can be obtained when labeling decisions between sentences are modeled (Sentence-Structured)." ></td>
	<td class="line x" title="177:215	More interestingly, even further gains can be had when document level decisions are modeled (JointStructured)." ></td>
	<td class="line x" title="178:215	In many cases, these improvements are highly statistically significant." ></td>
	<td class="line x" title="179:215	On the document level, performance can also be improved by incorporating sentence level decisions  though these improvements are not consistent." ></td>
	<td class="line x" title="180:215	This inconsistency may be a result of the model overfitting on the small set of training data." ></td>
	<td class="line x" title="181:215	We 437 suspect this because the document level error rate on the Mp3 training set converges to zero much more rapidly for the Joint-Structured model than the Document-Classifier." ></td>
	<td class="line x" title="182:215	This suggests that the JointStructured model might be relying too much on the sentence level sentiment features  in order to minimize its error rate  instead of distributing the weights across all features more evenly." ></td>
	<td class="line x" title="183:215	One interesting application of sentence level sentiment analysis is summarizing product reviews on retail websites like Amazon.com or review aggregators like Yelp.com." ></td>
	<td class="line x" title="184:215	In this setting the correct polarity of a document is often known, but we wish to label sentiment on the sentence or phrase level to aid in generating a cohesive and informative summary." ></td>
	<td class="line x" title="185:215	The joint model can be used to classify sentences in this setting by constraining inference to the known fixed document label for a review." ></td>
	<td class="line x" title="186:215	If this is done, then sentiment accuracy on the sentence level increases substantially from 62.6% to 70.3%." ></td>
	<td class="line x" title="187:215	Finally we should note that experiments using CRFs to train the structured models and logistic regression to train the local models yielded similar results to those in Table 2." ></td>
	<td class="line x" title="188:215	3.2.1 Cascaded Models Another approach to fine-to-coarse sentiment analysis is to use a cascaded system." ></td>
	<td class="line x" title="189:215	In such a system, a sentence level classifier might first be run on the data, and then the results input into a document level classifier  or vice-versa.1 Two cascaded systems were built." ></td>
	<td class="line x" title="190:215	The first uses the SentenceStructured classifier to classify all the sentences from a review, then passes this information to the document classifier as input." ></td>
	<td class="line x" title="191:215	In particular, for every predicate in the original document classifier, an additional predicate that specifies the polarity of the sentence in which this predicate occurred was created." ></td>
	<td class="line x" title="192:215	The second cascaded system uses the document classifier to determine the global polarity, then passes this information as input into the SentenceStructured model, constructing predicates in a similar manner." ></td>
	<td class="line x" title="193:215	The results for these two systems can be seen in the last two rows of Table 2." ></td>
	<td class="line x" title="194:215	In both cases there 1Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004)." ></td>
	<td class="line x" title="195:215	is a slight improvement in performance suggesting that an iterative approach might be beneficial." ></td>
	<td class="line x" title="196:215	That is, a system could start by classifying documents, use the document information to classify sentences, use the sentence information to classify documents, and repeat until convergence." ></td>
	<td class="line x" title="197:215	However, experiments showedthatthisdidnotimproveaccuracyoverasingle iteration and often hurt performance." ></td>
	<td class="line x" title="198:215	Improvements from the cascaded models are far less consistent than those given from the joint structure model." ></td>
	<td class="line x" title="199:215	This is because decisions in the cascaded system are passed to the next layer as the gold standard at test time, which results in errors from the first classifier propagating to errors in the second." ></td>
	<td class="line x" title="200:215	This could be improved by passing a lattice of possibilities from the first classifier to the second withcorrespondingconfidences." ></td>
	<td class="line x" title="201:215	However, solutions such as these are really just approximations of the joint structured model that was presented here." ></td>
	<td class="line x" title="202:215	4 Future Work One important extension to this work is to augment the models for partially labeled data." ></td>
	<td class="line x" title="203:215	It is realistic to imagine a training set where many examples do not have every level of sentiment annotated." ></td>
	<td class="line x" title="204:215	For example, there are thousands of online product reviews with labeled document sentiment, but a much smaller amount where sentences are also labeled." ></td>
	<td class="line x" title="205:215	Work on learning with hidden variables can be used for both CRFs (Quattoni et al. , 2004) and for inference based learning algorithms like those used in this work (Liang et al. , 2006)." ></td>
	<td class="line x" title="206:215	Another area of future work is to empirically investigate the use of these models on longer documents that require more levels of sentiment analysis than product reviews." ></td>
	<td class="line x" title="207:215	In particular, the relative position of a phrase to a contrastive discourse connective or a cue phrase like in conclusion or to summarize may lead to improved performance since higher level classifications can learn to weigh information passed from these lower level components more heavily." ></td>
	<td class="line x" title="208:215	5 Discussion In this paper we have investigated the use of a global structured model that learns to predict sentiment on different levels of granularity for a text." ></td>
	<td class="line x" title="209:215	We de438 Sentence Accuracy Document Accuracy Car Fit Mp3 Total Car Fit Mp3 Total Document-Classifier 72.8 80.1 87.2 80.3 Sentence-Classifier 54.8 56.8 49.4 53.1 Sentence-Structured 60.5 61.4 55.7 58.8 Joint-Structured 63.5 65.2 60.1 62.6 81.5 81.9 85.0 82.8 Cascaded Sentence  Document 60.5 61.4 55.7 58.8 75.9 80.7 86.1 81.1 Cascaded Document  Sentence 59.7 61.0 58.3 59.5 72.8 80.1 87.2 80.3 Table 2: Fine-to-coarse sentiment accuracy." ></td>
	<td class="line x" title="210:215	Significance calculated using McNemars test between top two performing systems." ></td>
	<td class="line x" title="211:215	Statistically significant p < 0.05." ></td>
	<td class="line x" title="212:215	Statistically significant p < 0.005." ></td>
	<td class="line x" title="213:215	scribed a simple model for sentence-document analysis and showed that inference in it is tractable." ></td>
	<td class="line x" title="214:215	Experiments show that this model obtains higher accuracy than classifiers trained in isolation as well as cascaded systems that pass information from one leveltoanotherattesttime." ></td>
	<td class="line x" title="215:215	Furthermore, extensions to the sentence-document model were discussed and it was argued that a nested hierarchical structure would be beneficial since it would allow for efficient inference algorithms." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1056
Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification
Blitzer, John;Dredze, Mark;Pereira, Fernando C. N.;"></td>
	<td class="line x" title="1:215	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440447, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:215	c2007 Association for Computational Linguistics Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification John Blitzer Mark Dredze Department of Computer and Information Science University of Pennsylvania {blitzer|mdredze|pereria@cis.upenn.edu} Fernando Pereira Abstract Automatic sentiment classification has been extensively studied and applied in recent years." ></td>
	<td class="line x" title="3:215	However, sentiment is expressed differently in different domains, and annotating corporaforeverypossibledomainofinterest is impractical." ></td>
	<td class="line x" title="4:215	We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="5:215	First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline." ></td>
	<td class="line x" title="6:215	Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another." ></td>
	<td class="line x" title="7:215	This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains." ></td>
	<td class="line oc" title="8:215	1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al. , 2002; Turney, 2002; Goldberg and Zhu, 2004)." ></td>
	<td class="line x" title="9:215	While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al. , 2006)." ></td>
	<td class="line x" title="10:215	Research results have been deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs." ></td>
	<td class="line x" title="11:215	With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter." ></td>
	<td class="line x" title="12:215	Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time." ></td>
	<td class="line x" title="13:215	We envision a scenario in which developers annotate corpora for a small number of domains, train classifiers on those corpora, and then apply them to other similar corpora." ></td>
	<td class="line x" title="14:215	However, this approach raises two important questions." ></td>
	<td class="line x" title="15:215	First, it is well known that trained classifiers lose accuracy when the test data distribution is significantly differentfromthetrainingdatadistribution 1." ></td>
	<td class="line x" title="16:215	Second, itis not clear which notion of domain similarity should be used to select domains to annotate that would be good proxies for many other domains." ></td>
	<td class="line x" title="17:215	We propose solutions to these two questions and evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs, electronics, and kitchen appliances2." ></td>
	<td class="line x" title="18:215	First, we show how to extend the recently proposed structural cor1For surveys of recent research on domain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning (http://gameairesearch.uta." ></td>
	<td class="line x" title="19:215	edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution (http://ida. first.fraunhofer.de/projects/different06/) 2The dataset will be made available by the authors at publication time." ></td>
	<td class="line x" title="20:215	440 respondence learning (SCL) domain adaptation algorithm (Blitzer et al. , 2006) for use in sentiment classification." ></td>
	<td class="line x" title="21:215	A key step in SCL is the selection of pivot features thatare usedtolink thesourceandtarget domains." ></td>
	<td class="line x" title="22:215	We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels." ></td>
	<td class="line x" title="23:215	For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains." ></td>
	<td class="line x" title="24:215	In our second extensionweshowhowtocorrectmisalignmentsusing a very small number of labeled instances." ></td>
	<td class="line x" title="25:215	Second, we evaluate the A-distance (Ben-David et al. , 2006) between domains as measure of the loss due to adaptation from one to the other." ></td>
	<td class="line x" title="26:215	The Adistancecanbemeasuredfromunlabeleddata, andit was designed to take into account only divergences which affect classification accuracy." ></td>
	<td class="line x" title="27:215	We show that it correlates well with adaptation loss, indicating that we can use the A-distance to select a subset of domains to label as sources." ></td>
	<td class="line x" title="28:215	In the next section we briefly review SCL and introduce our new pivot selection method." ></td>
	<td class="line x" title="29:215	Section 3 describes datasets and experimental method." ></td>
	<td class="line x" title="30:215	Section 4 gives results for SCL and the mutual information method for selecting pivot features." ></td>
	<td class="line x" title="31:215	Section 5 shows how to correct feature misalignments using a small amount of labeled target domain data." ></td>
	<td class="line x" title="32:215	Section 6 motivates the A-distance and shows that it correlates well with adaptability." ></td>
	<td class="line x" title="33:215	We discuss related work in Section 7 and conclude in Section 8." ></td>
	<td class="line x" title="34:215	2 Structural Correspondence Learning Before reviewing SCL, we give a brief illustrative example." ></td>
	<td class="line x" title="35:215	Suppose that we are adapting from reviews of computers to reviews of cell phones." ></td>
	<td class="line x" title="36:215	While many of the features of a good cell phone review are the same as a computer review  the words excellent and awful for example  many words are totally new, like reception." ></td>
	<td class="line x" title="37:215	At the same time, many features which were useful for computers, such as dual-core are no longer useful for cell phones." ></td>
	<td class="line x" title="38:215	Our key intuition is that even when good-quality reception and fast dual-core are completely distinct for each domain, if they both have high correlation with excellent and low correlation with awful on unlabeled data, then we can tentatively align them." ></td>
	<td class="line x" title="39:215	After learning a classifier for computer reviews, when we see a cell-phone feature like goodquality reception, we know it should behave in a roughly similar manner to fast dual-core." ></td>
	<td class="line x" title="40:215	2.1 Algorithm Overview Given labeled data from a source domain and unlabeled data from both source and target domains, SCLfirstchoosesasetofmpivotfeatureswhichoccur frequently in both domains." ></td>
	<td class="line x" title="41:215	Then, it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains (Ando and Zhang, 2005; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="42:215	The lscriptth pivot predictor is characterized by its weight vector wlscript; positive entries in that weight vector mean that a non-pivot feature (like fast dualcore) is highly correlated with the corresponding pivot (like excellent)." ></td>
	<td class="line x" title="43:215	The pivot predictor column weight vectors can be arranged into a matrix W = [wlscript]nlscript=1." ></td>
	<td class="line x" title="44:215	Let   Rkd be the top k left singular vectors of W (here d indicatesthetotalnumberoffeatures)." ></td>
	<td class="line x" title="45:215	Thesevectorsare the principal predictors for our weight space." ></td>
	<td class="line x" title="46:215	If we chose our pivot features well, then we expect these principal predictors to discriminate among positive and negative words in both domains." ></td>
	<td class="line x" title="47:215	At training and test time, suppose we observe a feature vector x. We apply the projection x to obtain k new real-valued features." ></td>
	<td class="line x" title="48:215	Now we learn a predictor for the augmented instance x,x." ></td>
	<td class="line x" title="49:215	If  contains meaningful correspondences, then the predictor which uses  will perform well in both source and target domains." ></td>
	<td class="line x" title="50:215	2.2 Selecting Pivots with Mutual Information The efficacy of SCL depends on the choice of pivot features." ></td>
	<td class="line x" title="51:215	For the part of speech tagging problem studied by Blitzer et al.(2006), frequently-occurring words in both domains were good choices, since they often correspond to function words such as prepositions and determiners, which are good indicators of parts of speech." ></td>
	<td class="line x" title="53:215	This is not the case for sentiment classification, however." ></td>
	<td class="line x" title="54:215	Therefore, we require that pivot features also be good predictors of the source label." ></td>
	<td class="line x" title="55:215	Among those features, we then choose the ones with highest mutual information to the source label." ></td>
	<td class="line x" title="56:215	Table 1 shows the set-symmetric 441 SCL, not SCL-MI SCL-MI, not SCL book one <num> so all a must a wonderful loved it very about they like weak dont waste awful good when highly recommended and easy Table 1: Top pivots selected by SCL, but not SCLMI (left) and vice-versa (right) differencesbetweenthetwomethodsforpivotselectionwhenadaptingaclassifierfrombookstokitchen appliances." ></td>
	<td class="line x" title="57:215	Wereferthroughouttherest ofthiswork to our method for selecting pivots as SCL-MI." ></td>
	<td class="line x" title="58:215	3 Dataset and Baseline We constructed a new dataset for sentiment domain adaptation by selecting Amazon product reviews for fourdifferentproducttypes: books, DVDs, electronics and kitchen appliances." ></td>
	<td class="line x" title="59:215	Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text." ></td>
	<td class="line x" title="60:215	Reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous." ></td>
	<td class="line x" title="61:215	After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset (Pang et al. , 2002)." ></td>
	<td class="line x" title="62:215	In addition to the labeled data, we included between 3685 (DVDs) and 5945 (kitchen)instancesofunlabeleddata." ></td>
	<td class="line x" title="63:215	Thesizeofthe unlabeled data was limited primarily by the number of reviews we could crawl and download from the Amazon website." ></td>
	<td class="line x" title="64:215	Since we were able to obtain labels for all of the reviews, we also ensured that they were balanced between positive and negative examples, as well." ></td>
	<td class="line x" title="65:215	While the polarity dataset is a popular choice in the literature, we were unable to use it for our task." ></td>
	<td class="line x" title="66:215	Our method requires many unlabeled reviews and despite a large number of IMDB reviews available online, the extensive curation requirements made preparing a large amount of data difficult 3." ></td>
	<td class="line x" title="67:215	For classification, we use linear predictors on unigram and bigram features, trained to minimize the Huber loss with stochastic gradient descent (Zhang, 3For a description of the construction of the polarity dataset, see http://www.cs.cornell.edu/people/ pabo/movie-review-data/." ></td>
	<td class="line x" title="68:215	2004)." ></td>
	<td class="line x" title="69:215	On the polarity dataset, this model matches the results reported by Pang et al.(2002)." ></td>
	<td class="line x" title="71:215	When we reportresultswithSCLandSCL-MI,werequirethat pivots occur in more than five documents in each domain." ></td>
	<td class="line x" title="72:215	Wesetk,thenumberofsingularvectorsofthe weight matrix, to 50." ></td>
	<td class="line x" title="73:215	4 Experiments with SCL and SCL-MI Each labeled dataset was split into a training set of 1600 instances and a test set of 400 instances." ></td>
	<td class="line x" title="74:215	All the experiments use a classifier trained on the training set of one domain and tested on the test set of a possibly different domain." ></td>
	<td class="line x" title="75:215	The baseline is a linear classifier trained without adaptation, while the gold standard is an in-domain classifier trained on the same domain as it is tested." ></td>
	<td class="line x" title="76:215	Figure 1 gives accuracies for all pairs of domain adaptation." ></td>
	<td class="line x" title="77:215	The domains are ordered clockwise from the top left: books, DVDs, electronics, and kitchen." ></td>
	<td class="line x" title="78:215	For each set of bars, the first letter is the source domain and the second letter is the target domain." ></td>
	<td class="line x" title="79:215	The thick horizontal bars are the accuracies of the in-domain classifiers for these domains." ></td>
	<td class="line x" title="80:215	Thus the first set of bars shows that the baseline achieves 72.8% accuracy adapting from DVDs to books." ></td>
	<td class="line x" title="81:215	SCL-MI achieves 79.7% and the in-domain gold standard is 80.4%." ></td>
	<td class="line x" title="82:215	We say that the adaptation loss for the baseline model is 7.6% and the adaptationlossfortheSCL-MImodelis0.7%." ></td>
	<td class="line x" title="83:215	Therelative reduction in error due to adaptation of SCL-MI for this test is 90.8%." ></td>
	<td class="line x" title="84:215	We can observe from these results that there is a rough grouping of our domains." ></td>
	<td class="line x" title="85:215	Books and DVDs are similar, as are kitchen appliances and electronics, but the two groups are different from one another." ></td>
	<td class="line x" title="86:215	Adapting classifiers from books to DVDs, for instance, is easier than adapting them from books to kitchen appliances." ></td>
	<td class="line x" title="87:215	We note that when transferring from kitchen to electronics, SCL-MI actually outperforms the in-domain classifier." ></td>
	<td class="line x" title="88:215	This is possiblesincetheunlabeleddatamaycontaininformation that the in-domain classifier does not have access to." ></td>
	<td class="line x" title="89:215	At the beginning of Section 2 we gave examples of how features can change behavior across domains." ></td>
	<td class="line x" title="90:215	The first type of behavior is when predictive features from the source domain are not predictive or do not appear in the target domain." ></td>
	<td class="line x" title="91:215	The second is 442 65 70 75 80 85 90 D->BE->BK->BB->DE->DK->D baselineSCLSCL-MIbooks 72.8 76.8 79.7 70.7 75.475.4 70.9 66.1 68.6 80.4 82.4 77.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9 dvd 65 70 75 80 85 90 B->ED->EK->EB->KD->KE->K electronics kitchen 70.8 77.5 75.9 73.0 74.1 74.1 82.7 83.7 86.8 84.4 87.7 74.5 78.778.9 74.0 79.4 81.484.0 84.4 85.9 Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI." ></td>
	<td class="line x" title="92:215	Thick black lines are the accuracies of in-domain classifiers." ></td>
	<td class="line x" title="93:215	domain\polarity negative positive books plot <num> pages predictable reader grisham engaging reading this page <num> must read fascinating kitchen the plastic poorly designed excellent product espresso leaking awkward to defective are perfect years now a breeze Table 2: Correspondences discovered by SCL for books and kitchen appliances." ></td>
	<td class="line x" title="94:215	The top row shows features that only appear in books and the bottom features that only appear in kitchen appliances." ></td>
	<td class="line x" title="95:215	The left and right columns show negative and positive features in correspondence, respectively." ></td>
	<td class="line x" title="96:215	when predictive features from the target domain do not appear in the source domain." ></td>
	<td class="line x" title="97:215	To show how SCL deals with those domain mismatches, we look at the adaptation from book reviews to reviews of kitchen appliances." ></td>
	<td class="line x" title="98:215	We selected the top 1000 most informative features in both domains." ></td>
	<td class="line x" title="99:215	In both cases, between 85 and 90% of the informative features from one domain were not among the most informative of the other domain4." ></td>
	<td class="line x" title="100:215	SCL addresses both of these issues simultaneously by aligning features from the two domains." ></td>
	<td class="line x" title="101:215	4There is a third type, features which are positive in one domain but negative in another, but they appear very infrequently in our datasets." ></td>
	<td class="line x" title="102:215	Table 2 illustrates one row of the projection matrix for adapting from books to kitchen appliances; the features on each row appear only in the corresponding domain." ></td>
	<td class="line x" title="103:215	A supervised classifier trained on book reviews cannot assign weight to the kitchen features in the second row of table 2." ></td>
	<td class="line x" title="104:215	In contrast, SCL assigns weight to these features indirectly through the projection matrix." ></td>
	<td class="line x" title="105:215	When we observe the feature predictable with a negative book review, we update parameters corresponding to the entire projection, including the kitchen-specific features poorly designed and awkward to." ></td>
	<td class="line x" title="106:215	While some rows of the projection matrix  are 443 useful for classification, SCL can also misalign features." ></td>
	<td class="line x" title="107:215	This causes problems when a projection is discriminative in the source domain but not in the target." ></td>
	<td class="line x" title="108:215	This is the case for adapting from kitchen appliances to books." ></td>
	<td class="line x" title="109:215	Since the book domain is quite broad, many projections in books model topic distinctions such as between religious and political books." ></td>
	<td class="line x" title="110:215	These projections, which are uninformative as to the target label, are put into correspondence with the fewer discriminating projections in the much narrower kitchen domain." ></td>
	<td class="line x" title="111:215	When we adapt from kitchen to books, we assign weight to these uninformative projections, degrading target classification accuracy." ></td>
	<td class="line x" title="112:215	5 Correcting Misalignments We now show how to use a small amount of target domain labeled data to learn to ignore misaligned projections from SCL-MI." ></td>
	<td class="line x" title="113:215	Using the notation of AndoandZhang(2005),wecanwritethesupervised training objective of SCL on the source domain as minw,v summationdisplay i Lparenleftbigwprimexi +vprimexi,yiparenrightbig+ ||w||2 + ||v||2, where y is the label." ></td>
	<td class="line x" title="114:215	The weight vector w  Rd weighs the original features, while v  Rk weighs the projected features." ></td>
	<td class="line x" title="115:215	Ando and Zhang (2005) and Blitzeretal.(2006)suggest = 104, = 0,which we have used in our results so far." ></td>
	<td class="line x" title="116:215	Suppose now that we have trained source model weight vectors ws and vs. A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller." ></td>
	<td class="line x" title="117:215	We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al. , 2004; Blitzer et al. , 2006)." ></td>
	<td class="line x" title="118:215	Then we solve minw,vsummationtextj L(wprimexj +vprimexj,yj) + ||w||2 +||vvs||2." ></td>
	<td class="line x" title="119:215	Sincewedontwanttodeviatesignificantlyfromthe source parameters, we set  =  = 101." ></td>
	<td class="line x" title="120:215	Figure 2 shows the corrected SCL-MI model using 50 target domain labeled instances." ></td>
	<td class="line x" title="121:215	We chose this number since we believe it to be a reasonable amount for a single engineer to label with minimal effort." ></td>
	<td class="line x" title="122:215	For reasons of space, for each target domain dom \ model base base scl scl-mi scl-mi +targ +targ books 8.9 9.0 7.4 5.8 4.4 dvd 8.9 8.9 7.8 6.1 5.3 electron 8.3 8.5 6.0 5.5 4.8 kitchen 10.2 9.9 7.0 5.6 5.1 average 9.1 9.1 7.1 5.8 4.9 Table 3: For each domain, we show the loss due to transfer for each method, averaged over all domains." ></td>
	<td class="line x" title="123:215	The bottom row shows the average loss over all runs." ></td>
	<td class="line x" title="124:215	we show adaptation from only the two domains on which SCL-MI performed the worst relative to the supervised baseline." ></td>
	<td class="line x" title="125:215	For example, the book domain shows only results from electronics and kitchen, but not DVDs." ></td>
	<td class="line x" title="126:215	As a baseline, we used the label of the sourcedomainclassifierasafeatureinthetarget, but did not use any SCL features." ></td>
	<td class="line x" title="127:215	We note that the baseline is very close to just using the source domain classifier, because with only 50 target domain instances we do not have enough data to relearn all of the parameters inw." ></td>
	<td class="line x" title="128:215	As we can see, though, relearning the 50 parameters in v is quite helpful." ></td>
	<td class="line x" title="129:215	The corrected model always improves over the baseline for every possible transfer, including those not shown in the figure." ></td>
	<td class="line x" title="130:215	The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation." ></td>
	<td class="line x" title="131:215	In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation." ></td>
	<td class="line x" title="132:215	The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only." ></td>
	<td class="line x" title="133:215	For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline." ></td>
	<td class="line x" title="134:215	Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000." ></td>
	<td class="line x" title="135:215	Table 3 summarizes the results of sections 4 and 5." ></td>
	<td class="line x" title="136:215	Structural correspondence learning reduces the error due to transfer by 21%." ></td>
	<td class="line x" title="137:215	Choosing pivots by mutual information allows us to further reduce the error to 36%." ></td>
	<td class="line x" title="138:215	Finally, by adding 50 instances of target domain data and using this to correct the misaligned projections, we achieve an average relative 444 65 70 75 80 85 90 E->BK->BB->DK->DB->ED->EB->KE->K base+50-targSCL-MI+50-targ books kitchen 70.9 76.0 70.7 76.8 78.5 72.7 80.4 87.7 76.6 70.8 76.6 73.0 77.9 74.3 80.7 84.3 dvd electronics 82.4 84.4 73.2 85.9 Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances." ></td>
	<td class="line x" title="139:215	reduction in error of 46%." ></td>
	<td class="line x" title="140:215	6 Measuring Adaptability Sections 2-5 focused on how to adapt to a target domain when you had a labeled source dataset." ></td>
	<td class="line x" title="141:215	We now take a step back to look at the problem of selecting source domain data to label." ></td>
	<td class="line x" title="142:215	We study a setting where an engineer knows roughly her domains of interest but does not have any labeled data yet." ></td>
	<td class="line x" title="143:215	In that case, she can ask the question Which sources should I label to obtain the best performance over all my domains? On our product domains, for example, if we are interested in classifying reviews of kitchen appliances, we know from sections 4-5 that it would be foolish to label reviews of books or DVDs rather than electronics." ></td>
	<td class="line x" title="144:215	Here we show how to select source domains using only unlabeled data and the SCL representation." ></td>
	<td class="line x" title="145:215	6.1 The A-distance We propose to measure domain adaptability by using the divergence of two domains after the SCL projection." ></td>
	<td class="line x" title="146:215	We can characterize domains by their induced distributions on instance space: the more different the domains, the more divergent the distributions." ></td>
	<td class="line x" title="147:215	Here we make use of the A-distance (BenDavid et al. , 2006)." ></td>
	<td class="line x" title="148:215	The key intuition behind the A-distance is that while two domains can differ in arbitrary ways, we are only interested in the differences that affect classification accuracy." ></td>
	<td class="line x" title="149:215	Let A be the family of subsets of Rk corresponding to characteristic functions of linear classifiers (sets on which a linear classifier returns positive value)." ></td>
	<td class="line x" title="150:215	ThentheAdistancebetweentwoprobability distributions is dA(D,Dprime) = 2 sup AA |PrD [A]  PrDprime [A]| . That is, we find the subset in A on which the distributions differ the most in the L1 sense." ></td>
	<td class="line x" title="151:215	Ben-David et al.(2006) show that computing the A-distance for a finite sample is exactly the problem of minimizing the empirical risk of a classifier that discriminatesbetweeninstancesdrawnfromD andinstances drawn from Dprime." ></td>
	<td class="line x" title="153:215	This is convenient for us, since it allows us to use classification machinery to compute the A-distance." ></td>
	<td class="line x" title="154:215	6.2 Unlabeled Adaptability Measurements We follow Ben-David et al.(2006) and use the Huber loss as a proxy for the A-distance." ></td>
	<td class="line x" title="156:215	Our procedure is as follows: Given two domains, we compute the SCL representation." ></td>
	<td class="line x" title="157:215	Then we create a data set where each instance x is labeled with the identity of the domain from which it came and train a linear classifier." ></td>
	<td class="line x" title="158:215	For each pair of domains we compute the empirical average per-instance Huber loss, subtract it from 1, and multiply the result by 100." ></td>
	<td class="line x" title="159:215	We refer to this quantity as the proxy A-distance." ></td>
	<td class="line x" title="160:215	When it is 100, the two domains are completely distinct." ></td>
	<td class="line x" title="161:215	When it is 0, the two domains are indistinguishable using a linear classifier." ></td>
	<td class="line x" title="162:215	Figure 3 is a correlation plot between the proxy A-distance and the adaptation error." ></td>
	<td class="line x" title="163:215	Suppose we wantedtolabeltwodomainsoutofthefourinsucha 445 0 2 4 6 8 10 12 14 6065707580859095100 Proxy A-distance Adaptation Loss EK BD DE DK BE, BK Figure 3: The proxy A-distance between each domain pair plotted against the average adaptation loss of as measured by our baseline system." ></td>
	<td class="line x" title="164:215	Each pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen." ></td>
	<td class="line x" title="165:215	way asto minimizeour erroron all thedomains." ></td>
	<td class="line x" title="166:215	Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be abletoadequatelycoverelectronicsorkitchenappliances." ></td>
	<td class="line x" title="167:215	Similarly we would also choose one domain fromeitherelectronicsorkitchenappliances, butnot both." ></td>
	<td class="line x" title="168:215	7 Related Work Sentiment classification has advanced considerably since the work of Pang et al.(2002), which we use as our baseline." ></td>
	<td class="line x" title="170:215	Thomas et al.(2006) use discourse structurepresentincongressionalrecordstoperform more accurate sentiment classification." ></td>
	<td class="line x" title="172:215	Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem." ></td>
	<td class="line x" title="173:215	In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features." ></td>
	<td class="line x" title="174:215	Thus webelievethatouradaptationmethodscouldbealso applied to those more refined models." ></td>
	<td class="line x" title="175:215	While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation." ></td>
	<td class="line oc" title="176:215	The work most similar in spirit to ours that of Turney (2002)." ></td>
	<td class="line o" title="177:215	He used the difference in mutual information with two human-selected features (the words excellent and poor) to score features in a completely unsupervised manner." ></td>
	<td class="line o" title="178:215	Then he classified documents according to various functions of these mutual information scores." ></td>
	<td class="line x" title="179:215	We stress that our method improves a supervised baseline." ></td>
	<td class="line nc" title="180:215	While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset." ></td>
	<td class="line x" title="181:215	We also note the work of Aue and Gamon (2005), who performed a number of empirical tests on domain adaptation of sentiment classifiers." ></td>
	<td class="line x" title="182:215	Most of these tests were unsuccessful." ></td>
	<td class="line x" title="183:215	We briefly note their results on combining a number of source domains." ></td>
	<td class="line x" title="184:215	They observed that source domains closer to the target helped more." ></td>
	<td class="line x" title="185:215	In preliminary experiments we confirmed these results." ></td>
	<td class="line x" title="186:215	Adding more labeled data always helps, but diversifying training data does not." ></td>
	<td class="line x" title="187:215	When classifying kitchen appliances, for any fixed amount of labeled data, it is always better to draw from electronics as a source than use some combination of all three other domains." ></td>
	<td class="line x" title="188:215	Domain adaptation alone is a generally wellstudied area, and we cannot possibly hope to cover all of it here." ></td>
	<td class="line x" title="189:215	As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al. , 2006)." ></td>
	<td class="line x" title="190:215	We also note that while Florian et al.(2004) and Blitzer et al.(2006) observe that including the label of a source classifier asa featureon smallamounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data." ></td>
	<td class="line x" title="193:215	We believe the most important reason for this is that they explore structured prediction problems, where labels of surrounding words from the source classifier may be very informative, even if the current label is not." ></td>
	<td class="line x" title="194:215	In contrast our simple binary predictionproblemdoesnotexhibitsuchbehavior." ></td>
	<td class="line x" title="195:215	This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation." ></td>
	<td class="line x" title="196:215	Finally we note that while Blitzer et al.(2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al.(2004)." ></td>
	<td class="line x" title="199:215	By only adapting the SCLrelated part of the weight vector v, we are able to make better use of our small amount of unlabeled data than these previous techniques." ></td>
	<td class="line x" title="200:215	446 8 Conclusion Sentiment classification has seen a great deal of attention." ></td>
	<td class="line x" title="201:215	Its application to many different domains of discourse makes it an ideal candidate for domain adaptation." ></td>
	<td class="line x" title="202:215	This work addressed two important questions of domain adaptation." ></td>
	<td class="line x" title="203:215	First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al.(2006)." ></td>
	<td class="line x" title="205:215	We chose pivot features using not only common frequency among domains but also mutual information with the source labels." ></td>
	<td class="line x" title="206:215	We also showed how to correct structural correspondence misalignments by using a small amount of labeled target domain data." ></td>
	<td class="line x" title="207:215	Second, we provided a method for selecting those source domains most likely to adapt well to given target domains." ></td>
	<td class="line x" title="208:215	The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation." ></td>
	<td class="line x" title="209:215	Thus we can use the Adistance to select source domains to label which will give low target domain error." ></td>
	<td class="line x" title="210:215	In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking." ></td>
	<td class="line x" title="211:215	We are also actively searching for a larger and morevariedsetofdomainsonwhichtotestourtechniques." ></td>
	<td class="line x" title="212:215	Acknowledgements We thank Nikhil Dinesh for helpful advice throughout the course of this work." ></td>
	<td class="line x" title="213:215	This material is based upon work partially supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="214:215	NBCHD03001." ></td>
	<td class="line x" title="215:215	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National BusinessCenter (DOI-NBC)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1123
Learning Multilingual Subjective Language via Cross-Lingual Projections
Mihalcea, Rada;Banea, Carmen;Wiebe, Janyce M.;"></td>
	<td class="line x" title="1:193	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976983, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:193	c2007 Association for Computational Linguistics Learning Multilingual Subjective Language via Cross-Lingual Projections Rada Mihalcea and Carmen Banea Department of Computer Science University of North Texas rada@cs.unt.edu, carmenb@unt.edu Janyce Wiebe Department of Computer Science University of Pittsburgh wiebe@cs.pitt.edu Abstract This paper explores methods for generating subjectivity analysis resources in a new language by leveraging on the tools and resources available in English." ></td>
	<td class="line x" title="3:193	Given a bridge between English and the selected target language (e.g. , a bilingual dictionary or a parallel corpus), the methods can be used to rapidly create tools for subjectivity analysis in the new language." ></td>
	<td class="line x" title="4:193	1 Introduction There is growing interest in the automatic extraction of opinions, emotions, and sentiments in text (subjectivity), to provide tools and support for various natural language processing applications." ></td>
	<td class="line x" title="5:193	Most of the research to date has focused on English, which is mainly explained by the availability of resources for subjectivity analysis, such as lexicons and manually labeled corpora." ></td>
	<td class="line x" title="6:193	In this paper, we investigate methods to automatically generate resources for subjectivity analysis for a new target language by leveraging on the resources and tools available for English, which in many cases took years of work to complete." ></td>
	<td class="line x" title="7:193	Specifically, through experiments with cross-lingual projection of subjectivity, we seek answers to the following questions." ></td>
	<td class="line x" title="8:193	First, can we derive a subjectivity lexicon for a new language using an existing English subjectivity lexicon and a bilingual dictionary?" ></td>
	<td class="line x" title="9:193	Second, can we derive subjectivity-annotated corpora in a new language using existing subjectivity analysis tools for English and a parallel corpus?" ></td>
	<td class="line x" title="10:193	Finally, third, can we build tools for subjectivity analysis for a new target language by relying on these automatically generated resources?" ></td>
	<td class="line x" title="11:193	We focus our experiments on Romanian, selected as a representative of the large number of languages that have only limited text processing resources developed to date." ></td>
	<td class="line x" title="12:193	Note that, although we work with Romanian, the methods described are applicable to any other language, as in these experiments we (purposely) do not use any language-specific knowledge of the target language." ></td>
	<td class="line x" title="13:193	Given a bridge between English and the selected target language (e.g. , a bilingual dictionary or a parallel corpus), the methods can be applied to other languages as well." ></td>
	<td class="line x" title="14:193	After providing motivations, we present two approaches to developing sentence-level subjectivity classifiers for a new target language." ></td>
	<td class="line x" title="15:193	The first uses a subjectivity lexicon translated from an English one." ></td>
	<td class="line x" title="16:193	The second uses an English subjectivity classifier and a parallel corpus to create target-language training data for developing a statistical classifier." ></td>
	<td class="line oc" title="17:193	2 Motivation Automatic subjectivity analysis methods have been used in a wide variety of text processing applications, such as tracking sentiment timelines in online forums and news (Lloyd et al. , 2005; Balog et al. , 2006), review classification (Turney, 2002; Pang et al. , 2002), mining opinions from product reviews (Hu and Liu, 2004), automatic expressive text-to-speech synthesis (Alm et al. , 2005), text semantic analysis (Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2006), and question answering (Yu and Hatzivassiloglou, 2003)." ></td>
	<td class="line x" title="18:193	976 While much recent work in subjectivity analysis focuses on sentiment (a type of subjectivity, namely positive and negative emotions, evaluations, and judgments), we opt to focus on recognizing subjectivity in general, for two reasons." ></td>
	<td class="line x" title="19:193	First, even when sentiment is the desired focus, researchers in sentiment analysis have shown that a two-stage approach is often beneficial, in which subjective instances are distinguished from objective ones, and then the subjective instances are further classified according to polarity (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al. , 2005; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="20:193	In fact, the problem of distinguishing subjective versus objective instances has often proved to be more difficult than subsequent polarity classification, so improvements in subjectivity classification promise to positively impact sentiment classification." ></td>
	<td class="line x" title="21:193	This is reported in studies of manual annotation of phrases (Takamura et al. , 2006), recognizing contextual polarity of expressions (Wilson et al. , 2005), and sentiment tagging of words and word senses (Andreevskaia and Bergler, 2006; Esuli and Sebastiani, 2006)." ></td>
	<td class="line x" title="22:193	Second, an NLP application may seek a wide range of types of subjectivity attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line x" title="23:193	For instance, the opinion tracking system Lydia (Lloyd et al. , 2005) gives separate ratings for subjectivity and sentiment." ></td>
	<td class="line x" title="24:193	These can be detected with subjectivity analysis but not by a method focused only on sentiment." ></td>
	<td class="line x" title="25:193	There is world-wide interest in text analysis applications." ></td>
	<td class="line x" title="26:193	While work on subjectivity analysis in other languages is growing (e.g. , Japanese data are used in (Takamura et al. , 2006; Kanayama and Nasukawa, 2006), Chinese data are used in (Hu et al. , 2005), and German data are used in (Kim and Hovy, 2006)), much of the work in subjectivity analysis has been applied to English data." ></td>
	<td class="line x" title="27:193	Creating corpora and lexical resources for a new language is very time consuming." ></td>
	<td class="line x" title="28:193	In general, we would like to leverage resources already developed for one language to more rapidly create subjectivity analysis tools for a new one." ></td>
	<td class="line x" title="29:193	This motivates our exploration and use of cross-lingual lexicon translations and annotation projections." ></td>
	<td class="line x" title="30:193	Most if not all work on subjectivity analysis has been carried out in a monolingual framework." ></td>
	<td class="line x" title="31:193	We are not aware of multi-lingual work in subjectivity analysis such as that proposed here, in which subjectivity analysis resources developed for one language are used to support developing resources in another." ></td>
	<td class="line x" title="32:193	3 A Lexicon-Based Approach Many subjectivity and sentiment analysis tools rely on manually or semi-automatically constructed lexicons (Yu and Hatzivassiloglou, 2003; Riloff and Wiebe, 2003; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="33:193	Given the success of such techniques, the first approach we take to generating a target-language subjectivity classifier is to create a subjectivity lexicon by translating an existing source language lexicon, and then build a classifier that relies on the resulting lexicon." ></td>
	<td class="line x" title="34:193	Below, we describe the translation process and discuss the results of an annotation study to assess the quality of the translated lexicon." ></td>
	<td class="line x" title="35:193	We then describe and evaluate a lexicon-based target-language classifier." ></td>
	<td class="line x" title="36:193	3.1 Translating a Subjectivity Lexicon The subjectivity lexicon we use is from OpinionFinder (Wiebe and Riloff, 2005), an English subjectivity analysis system which, among other things, classifies sentences as subjective or objective." ></td>
	<td class="line x" title="37:193	The lexicon was compiled from manually developed resources augmented with entries learned from corpora." ></td>
	<td class="line x" title="38:193	It contains 6,856 unique entries, out of which 990 are multi-word expressions." ></td>
	<td class="line x" title="39:193	The entries in the lexicon have been labeled for part of speech, and for reliability  those that appear most often in subjective contexts are strong clues of subjectivity, while those that appear less often, but still more often than expected by chance, are labeled weak." ></td>
	<td class="line x" title="40:193	To perform the translation, we use two bilingual dictionaries." ></td>
	<td class="line x" title="41:193	The first is an authoritative EnglishRomanian dictionary, consisting of 41,500 entries,1 which we use as the main translation resource for the lexicon translation." ></td>
	<td class="line x" title="42:193	The second dictionary, drawn from the Universal Dictionary download site (UDP, 2007) consists of 4,500 entries written largely by Web volunteer contributors, and thus is not error free." ></td>
	<td class="line x" title="43:193	We use this dictionary only for those entries that do not appear in the main dictionary." ></td>
	<td class="line x" title="44:193	1Unique English entries, each with multiple Romanian translations." ></td>
	<td class="line x" title="45:193	977 There were several challenges encountered in the translation process." ></td>
	<td class="line x" title="46:193	First, although the English subjectivity lexicon contains inflected words, we must use the lemmatized form in order to be able to translate the entries using the bilingual dictionary." ></td>
	<td class="line x" title="47:193	However, words may lose their subjective meaning once lemmatized." ></td>
	<td class="line x" title="48:193	For instance, the inflected form of memories becomes memory." ></td>
	<td class="line x" title="49:193	Once translated into Romanian (as memorie), its main meaning is objective, referring to the power of retaining information as in Iron supplements may improve a womans memory." ></td>
	<td class="line x" title="50:193	Second, neither the lexicon nor the bilingual dictionary provides information on the sense of the individual entries, and therefore the translation has to rely on the most probable sense in the target language." ></td>
	<td class="line x" title="51:193	Fortunately, the bilingual dictionary lists the translations in reverse order of their usage frequencies." ></td>
	<td class="line x" title="52:193	Nonetheless, the ambiguity of the words and the translations still seems to represent an important source of error." ></td>
	<td class="line x" title="53:193	Moreover, the lexicon sometimes includes identical entries expressed through different parts of speech, e.g., grudge has two separate entries, for its noun and verb roles, respectively." ></td>
	<td class="line x" title="54:193	On the other hand, the bilingual dictionary does not make this distinction, and therefore we have again to rely on the most frequent heuristic captured by the translation order in the bilingual dictionary." ></td>
	<td class="line x" title="55:193	Finally, the lexicon includes a significant number (990) of multi-word expressions that pose translation difficulties, sometimes because their meaning is idiomatic, and sometimes because the multi-word expression is not listed in the bilingual dictionary and the translation of the entire phrase is difficult to reconstruct from the translations of the individual words." ></td>
	<td class="line x" title="56:193	To address this problem, when a translation is not found in the dictionary, we create one using a word-by-word approach." ></td>
	<td class="line x" title="57:193	These translations are then validated by enforcing that they occur at least three times on the Web, using counts collected from the AltaVista search engine." ></td>
	<td class="line x" title="58:193	The multi-word expressions that are not validated in this process are discarded, reducing the number of expressions from an initial set of 990 to a final set of 264." ></td>
	<td class="line x" title="59:193	The final subjectivity lexicon in Romanian contains 4,983 entries." ></td>
	<td class="line x" title="60:193	Table 1 shows examples of entries in the Romanian lexicon, together with their corresponding original English form." ></td>
	<td class="line x" title="61:193	The table Romanian English attributes nfrumuseta beautifying strong, verb notabil notable weak, adj plin de regret full of regrets strong, adj sclav slaves weak, noun Table 1: Examples of entries in the Romanian subjectivity lexicon also shows the reliability of the expression (weak or strong) and the part of speech  attributes that are provided in the English subjectivity lexicon." ></td>
	<td class="line x" title="62:193	Manual Evaluation." ></td>
	<td class="line x" title="63:193	We want to assess the quality of the translated lexicon, and compare it to the quality of the original English lexicon." ></td>
	<td class="line x" title="64:193	The English subjectivity lexicon was evaluated in (Wiebe and Riloff, 2005) against a corpus of English-language news articles manually annotated for subjectivity (the MPQA corpus (Wiebe et al. , 2005))." ></td>
	<td class="line x" title="65:193	According to this evaluation, 85% of the instances of the clues marked as strong and 71.5% of the clues marked as weak are in subjective sentences in the MPQA corpus." ></td>
	<td class="line x" title="66:193	Since there is no comparable Romanian corpus, an alternate way to judge the subjectivity of a Romanian lexicon entry is needed." ></td>
	<td class="line x" title="67:193	Two native speakers of Romanian annotated the subjectivity of 150 randomly selected entries." ></td>
	<td class="line x" title="68:193	Each annotator independently read approximately 100 examples of each drawn from the Web, including a large number from news sources." ></td>
	<td class="line x" title="69:193	The subjectivity of a word was consequently judged in the contexts where it most frequently appears, accounting for its most frequent meanings on the Web." ></td>
	<td class="line x" title="70:193	The tagset used for the annotations consists of S(ubjective), O(bjective), and B(oth)." ></td>
	<td class="line x" title="71:193	A W(rong) label is also used to indicate a wrong translation." ></td>
	<td class="line x" title="72:193	Table 2 shows the contingency table for the two annotators judgments on this data." ></td>
	<td class="line x" title="73:193	S O B W Total S 53 6 9 0 68 O 1 27 1 0 29 B 5 3 18 0 26 W 0 0 0 27 27 Total 59 36 28 27 150 Table 2: Agreement on 150 entries in the Romanian lexicon Without counting the wrong translations, the agreement is measured at 0.80, with a Kappa  = 978 0.70, which indicates consistent agreement." ></td>
	<td class="line x" title="74:193	After the disagreements were reconciled through discussions, the final set of 123 correctly translated entries does include 49.6% (61) subjective entries, but fully 23.6% (29) were found in the study to have primarily objective uses (the other 26.8% are mixed)." ></td>
	<td class="line x" title="75:193	Thus, this study suggests that the Romanian subjectivity clues derived through translation are less reliable than the original set of English clues." ></td>
	<td class="line x" title="76:193	In several cases, the subjectivity is lost in the translation, mainly due to word ambiguity in either the source or target language, or both." ></td>
	<td class="line x" title="77:193	For instance, the word fragile correctly translates into Romanian as fragil, yet this word is frequently used to refer to breakable objects, and it loses its subjective meaning of delicate." ></td>
	<td class="line x" title="78:193	Other words, such as one-sided, completely lose subjectivity once translated, as it becomes in Romanian cu o singura latura, meaning with only one side (as of objects)." ></td>
	<td class="line x" title="79:193	Interestingly, the reliability of clues in the English lexicon seems to help preserve subjectivity." ></td>
	<td class="line x" title="80:193	Out of the 77 entries marked as strong, 11 were judged to be objective in Romanian (14.3%), compared to 14 objective Romanian entries obtained from the 36 weak English clues (39.0%)." ></td>
	<td class="line x" title="81:193	3.2 Rule-based Subjectivity Classifier Using a Subjectivity Lexicon Starting with the Romanian lexicon, we developed a lexical classifier similar to the one introduced by (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="82:193	At the core of this method is a high-precision subjectivity and objectivity classifier that can label large amounts of raw text using only a subjectivity lexicon." ></td>
	<td class="line x" title="83:193	Their method is further improved with a bootstrapping process that learns extraction patterns." ></td>
	<td class="line x" title="84:193	In our experiments, however, we apply only the rule-based classification step, since the extraction step cannot be implemented without tools for syntactic parsing and information extraction not available in Romanian." ></td>
	<td class="line x" title="85:193	The classifier relies on three main heuristics to label subjective and objective sentences: (1) if two or more strong subjective expressions occur in the same sentence, the sentence is labeled Subjective; (2) if no strong subjective expressions occur in a sentence, and at most two weak subjective expressions occur in the previous, current, and next sentence combined, then the sentence is labeled Objective; (3) otherwise, if none of the previous rules apply, the sentence is labeled Unknown." ></td>
	<td class="line x" title="86:193	The quality of the classifier was evaluated on a Romanian gold-standard corpus annotated for subjectivity." ></td>
	<td class="line x" title="87:193	Two native Romanian speakers (Ro1 and Ro2) manually annotated the subjectivity of the sentences of five randomly selected documents (504 sentences) from the Romanian side of an EnglishRomanian parallel corpus, according to the annotation scheme in (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="88:193	Agreement between annotators was measured, and then their differences were adjudicated." ></td>
	<td class="line x" title="89:193	The baseline on this data set is 54.16%, which can be obtained by assigning a default Subjective label to all sentences." ></td>
	<td class="line x" title="90:193	(More information about the corpus and annotations are given in Section 4 below, where agreement between English and Romanian aligned sentences is also assessed.)" ></td>
	<td class="line x" title="91:193	As mentioned earlier, due to the lexicon projection process that is performed via a bilingual dictionary, the entries in our Romanian subjectivity lexicon are in a lemmatized form." ></td>
	<td class="line x" title="92:193	Consequently, we also lemmatize the gold-standard corpus, to allow for the identification of matches with the lexicon." ></td>
	<td class="line x" title="93:193	For this purpose, we use the Romanian lemmatizer developed by Ion and Tufis (Ion, 2007), which has an estimated accuracy of 98%.2 Table 3 shows the results of the rule-based classifier." ></td>
	<td class="line x" title="94:193	We show the precision, recall, and F-measure independently measured for the subjective, objective, and all sentences." ></td>
	<td class="line x" title="95:193	We also evaluated a variation of the rule-based classifier that labels a sentence as objective if there are at most three weak expressions in the previous, current, and next sentence combined, which raises the recall of the objective classifier." ></td>
	<td class="line x" title="96:193	Our attempts to increase the recall of the subjective classifier all resulted in significant loss in precision, and thus we kept the original heuristic." ></td>
	<td class="line x" title="97:193	In its original English implementation, this system was proposed as being high-precision but low coverage." ></td>
	<td class="line x" title="98:193	Evaluated on the MPQA corpus, it has subjective precision of 90.4, subjective recall of 34.2, objective precision of 82.4, and objective recall of 30.7; overall, precision is 86.7 and recall is 32.6 (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="99:193	We see a similar behavior on Romanian for subjective sentences." ></td>
	<td class="line x" title="100:193	The subjective precision is good, albeit at the cost of low 2Dan Tufis, personal communication." ></td>
	<td class="line x" title="101:193	979 Measure Subjective Objective All subj = at least two strong; obj = at most two weak Precision 80.00 56.50 62.59 Recall 20.51 48.91 33.53 F-measure 32.64 52.52 43.66 subj = at least two strong; obj = at most three weak Precision 80.00 56.85 61.94 Recall 20.51 61.03 39.08 F-measure 32.64 58.86 47.93 Table 3: Evaluation of the rule-based classifier recall, and thus the classifier could be used to harvest subjective sentences from unlabeled Romanian data (e.g. , for a subsequent bootstrapping process)." ></td>
	<td class="line x" title="102:193	The system is not very effective for objective classification, however." ></td>
	<td class="line x" title="103:193	Recall that the objective classifier relies on the weak subjectivity clues, for which the transfer of subjectivity in the translation process was particularly low." ></td>
	<td class="line x" title="104:193	4 A Corpus-Based Approach Given the low number of subjective entries found in the automatically generated lexicon and the subsequent low recall of the lexical classifier, we decided to also explore a second, corpus-based approach." ></td>
	<td class="line x" title="105:193	This approach builds a subjectivity-annotated corpus for the target language through projection, and then trains a statistical classifier on the resulting corpus (numerous statistical classifiers have been trained for subjectivity or sentiment classification, e.g., (Pang et al. , 2002; Yu and Hatzivassiloglou, 2003))." ></td>
	<td class="line x" title="106:193	The hypothesis is that we can eliminate some of the ambiguities (and consequent loss of subjectivity) observed during the lexicon translation by accounting for the context of the ambiguous words, which is possible in a corpus-based approach." ></td>
	<td class="line x" title="107:193	Additionally, we also hope to improve the recall of the classifier, by addressing those cases not covered by the lexicon-based approach." ></td>
	<td class="line x" title="108:193	In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al. , 1993) and their manual translations into Romanian.3 The corpus consists of roughly 11,000 sentences, with approximately 250,000 tokens on each side." ></td>
	<td class="line x" title="109:193	It is a balanced corpus covering a number of topics in sports, politics, fashion, education, and others." ></td>
	<td class="line x" title="110:193	3The translation was carried out by a Romanian native speaker, student in a department of Foreign Languages and Translations in Romania." ></td>
	<td class="line x" title="111:193	Below, we begin with a manual annotation study to assess the quality of annotation and preservation of subjectivity in translation." ></td>
	<td class="line x" title="112:193	We then describe the automatic construction of a target-language training set, and evaluate a classifier trained on that data." ></td>
	<td class="line x" title="113:193	Annotation Study." ></td>
	<td class="line x" title="114:193	We start by performing an agreement study meant to determine the extent to which subjectivity is preserved by the cross-lingual projections." ></td>
	<td class="line x" title="115:193	In the study, three annotators  one native English speaker (En) and two native Romanian speakers (Ro1 and Ro2)  first trained on 3 randomly selected documents (331 sentences)." ></td>
	<td class="line x" title="116:193	They then independently annotated the subjectivity of the sentences of two randomly selected documents from the parallel corpus, accounting for 173 aligned sentence pairs." ></td>
	<td class="line x" title="117:193	The annotators had access exclusively to the version of the sentences in their language, to avoid any bias that could be introduced by seeing the translation in the other language." ></td>
	<td class="line x" title="118:193	Note that the Romanian annotations (after all differences between the Romanian annotators were adjudicated) of all 331 + 173 sentences make up the gold standard corpus used in the experiments reported in Sections 3.2 and 4.1." ></td>
	<td class="line x" title="119:193	Before presenting the results of the annotation study, we give some examples." ></td>
	<td class="line x" title="120:193	The following are English subjective sentences and their Romanian translations (the subjective elements are shown in bold)." ></td>
	<td class="line x" title="121:193	[en] The desire to give Broglio as many starts as possible." ></td>
	<td class="line x" title="122:193	[ro] Dorinta de a-i da lui Broglio cat mai multe starturi posibile." ></td>
	<td class="line x" title="123:193	[en] Suppose he did lie beside Lenin, would it be permanent ? [ro] Sa presupunem ca ar fi asezat alaturi de Lenin, oare va fi pentru totdeauna?" ></td>
	<td class="line x" title="124:193	The following are examples of objective parallel sentences." ></td>
	<td class="line x" title="125:193	[en]The Pirates have a 9-6 record this year and the Redbirds are 7-9." ></td>
	<td class="line x" title="126:193	[ro] Piratii au un palmares de 9 la 6 anul acesta si Pasarile Rosii au 7 la 9." ></td>
	<td class="line x" title="127:193	[en] One of the obstacles to the easy control of a 2-year old child is a lack of verbal communication." ></td>
	<td class="line x" title="128:193	[ro] Unul dintre obstacolele n controlarea unui copil de 2 ani este lipsa comunicarii verbale." ></td>
	<td class="line x" title="129:193	980 The annotators were trained using the MPQA annotation guidelines (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="130:193	The tagset consists of S(ubjective), O(bjective) and U(ncertain)." ></td>
	<td class="line x" title="131:193	For the U tags, a class was also given; OU means, for instance, that the annotator is uncertain but she is leaning toward O. Table 4 shows the pairwise agreement figures and the Kappa () calculated for the three annotators." ></td>
	<td class="line x" title="132:193	The table also shows the agreement when the borderline uncertain cases are removed." ></td>
	<td class="line x" title="133:193	all sentences Uncertain removed pair agree  agree  (%) removed Ro1 & Ro2 0.83 0.67 0.89 0.77 23 En & Ro1 0.77 0.54 0.86 0.73 26 En & Ro2 0.78 0.55 0.91 0.82 20 Table 4: Agreement on the data set of 173 sentences." ></td>
	<td class="line x" title="134:193	Annotations performed by three annotators: one native English speaker (En) and two native Romanian speakers (Ro1 and Ro2) When all the sentences are included, the agreement between the two Romanian annotators is measured at 0.83 ( = 0.67)." ></td>
	<td class="line x" title="135:193	If we remove the borderline cases where at least one annotators tag is Uncertain, the agreement rises to 0.89 with  = 0.77." ></td>
	<td class="line x" title="136:193	These figures are somewhat lower than the agreement observed during previous subjectivity annotation studies conducted on English (Wiebe et al. , 2005) (the annotators were more extensively trained in those studies), but they nonetheless indicate consistent agreement." ></td>
	<td class="line x" title="137:193	Interestingly, when the agreement is conducted cross-lingually between an English and a Romanian annotator, the agreement figures, although somewhat lower, are comparable." ></td>
	<td class="line x" title="138:193	In fact, once the Uncertain tags are removed, the monolingual and cross-lingual agreement and  values become almost equal, which suggests that in most cases the sentence-level subjectivity is preserved." ></td>
	<td class="line x" title="139:193	The disagreements were reconciled first between the labels assigned by the two Romanian annotators, followed by a reconciliation between the resulting Romanian gold-standard labels and the labels assigned by the English annotator." ></td>
	<td class="line x" title="140:193	In most cases, the disagreement across the two languages was found to be due to a difference of opinion about the sentence subjectivity, similar to the differences encountered in monolingual annotations." ></td>
	<td class="line x" title="141:193	However, there are cases where the differences are due to the subjectivity being lost in the translation." ></td>
	<td class="line x" title="142:193	Sometimes, this is due to several possible interpretations for the translated sentence." ></td>
	<td class="line x" title="143:193	For instance, the following sentence: [en] They honored the battling Billikens last night." ></td>
	<td class="line x" title="144:193	[ro] Ei i-au celebrat pe Billikens seara trecuta. is marked as Subjective in English (in context, the English annotator interpreted honored as referring to praises of the Billikens)." ></td>
	<td class="line x" title="145:193	However, the Romanian translation of honored is celebrat which, while correct as a translation, has the more frequent interpretation of having a party." ></td>
	<td class="line x" title="146:193	The two Romanian annotators chose this interpretation, which correspondingly lead them to mark the sentence as Objective." ></td>
	<td class="line x" title="147:193	In other cases, in particular when the subjectivity is due to figures of speech such as irony, the translation sometimes misses the ironic aspects." ></td>
	<td class="line x" title="148:193	For instance, the translation of egghead was not perceived as ironic by the Romanian annotators, and consequently the following sentence labeled Subjective in English is annotated as Objective in Romanian." ></td>
	<td class="line x" title="149:193	[en] I have lived for many years in a Connecticut commuting town with a high percentage of [] business executives of egghead tastes." ></td>
	<td class="line x" title="150:193	[ro] Am trait multi ani ntr-un oras din apropiere de Connecticut ce avea o mare proportie de [] oameni de afaceri cu gusturi intelectuale." ></td>
	<td class="line x" title="151:193	4.1 Translating a Subjectivity-Annotated Corpus and Creating a Machine Learning Subjectivity Classifier To further validate the corpus-based projection of subjectivity, we developed a subjectivity classifier trained on Romanian subjectivity-annotated corpora obtained via cross-lingual projections." ></td>
	<td class="line x" title="152:193	Ideally, one would generate an annotated Romanian corpus by translating English documents manually annotated for subjectivity such as the MPQA corpus." ></td>
	<td class="line x" title="153:193	Unfortunately, the manual translation of this corpus would be prohibitively expensive, both timewise and financially." ></td>
	<td class="line x" title="154:193	The other alternative  automatic machine translation  has not yet reached a level that would enable the generation of a highquality translated corpus." ></td>
	<td class="line x" title="155:193	We therefore decided to use a different approach where we automatically annotate the English side of an existing EnglishRomanian corpus, and subsequently project the annotations onto the Romanian side of the parallel cor981 Precision Recall F-measure high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 5: Precision, recall, and F-measure for the two OpinionFinder classifiers, as measured on the MPQA corpus." ></td>
	<td class="line x" title="156:193	pus across the sentence-level alignments available in the corpus." ></td>
	<td class="line x" title="157:193	For the automatic subjectivity annotations, we generated two sets of the English-side annotations, one using the high-precision classifier and one using the high-coverage classifier available in the OpinionFinder tool." ></td>
	<td class="line x" title="158:193	The high-precision classifier in OpinionFinder uses the clues of the subjectivity lexicon to harvest subjective and objective sentences from a large amount of unannotated text; this data is then used to automatically identify a set of extraction patterns, which are then used iteratively to identify a larger set of subjective and objective sentences." ></td>
	<td class="line x" title="159:193	In addition, in OpinionFinder, the high-precision classifier is used to produce an English labeled data set for training, which is used to generate its Naive Bayes high-coverage subjectivity classifier." ></td>
	<td class="line x" title="160:193	Table 5 shows the performance of the two classifiers on the MPQA corpus as reported in (Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="161:193	Note that 55% of the sentences in the MPQA corpus are subjective  which represents the baseline for this data set." ></td>
	<td class="line x" title="162:193	The two OpinionFinder classifiers are used to label the training corpus." ></td>
	<td class="line x" title="163:193	After removing the 504 test sentences, we are left with 10,628 sentences that are automatically annotated for subjectivity." ></td>
	<td class="line x" title="164:193	Table 6 shows the number of subjective and objective sentences obtained with each classifier." ></td>
	<td class="line x" title="165:193	Classifier Subjective Objective All high-precision 1,629 2,334 3,963 high-coverage 5,050 5,578 10,628 Table 6: Subjective and objective training sentences automatically annotated with OpinionFinder." ></td>
	<td class="line x" title="166:193	Next, the OpinionFinder annotations are projected onto the Romanian training sentences, which are then used to develop a probabilistic classifier for the automatic labeling of subjectivity in Romanian sentences." ></td>
	<td class="line x" title="167:193	Similar to, e.g., (Pang et al. , 2002), we use a Naive Bayes algorithm trained on word features cooccurring with the subjective and the objective classifications." ></td>
	<td class="line x" title="168:193	We assume word independence, and we use a 0.3 cut-off for feature selection." ></td>
	<td class="line x" title="169:193	While recent work has also considered more complex syntactic features, we are not able to generate such features for Romanian as they require tools currently not available for this language." ></td>
	<td class="line x" title="170:193	We create two classifiers, one trained on each data set." ></td>
	<td class="line x" title="171:193	The quality of the classifiers is evaluated on the 504-sentence Romanian gold-standard corpus described above." ></td>
	<td class="line x" title="172:193	Recall that the baseline on this data set is 54.16%, the percentage of sentences in the corpus that are subjective." ></td>
	<td class="line x" title="173:193	Table 7 shows the results." ></td>
	<td class="line x" title="174:193	Subjective Objective All projection source: OF high-precision classifier Precision 65.02 69.62 64.48 Recall 82.41 47.61 64.48 F-measure 72.68 56.54 64.68 projection source: OF high-coverage classifier Precision 66.66 70.17 67.85 Recall 81.31 52.17 67.85 F-measure 72.68 56.54 67.85 Table 7: Evaluation of the machine learning classifier using training data obtained via projections from data automatically labeled by OpinionFinder (OF)." ></td>
	<td class="line x" title="175:193	Our best classifier has an F-measure of 67.85, and is obtained by training on projections from the high-coverage OpinionFinder annotations." ></td>
	<td class="line x" title="176:193	Although smaller than the 74.70 F-measure obtained by the English high-coverage classifier (see Table 5), the result appears remarkable given that no language-specific Romanian information was used." ></td>
	<td class="line x" title="177:193	The overall results obtained with the machine learning approach are considerably higher than those obtained from the rule-based classifier (except for the precision of the subjective sentences)." ></td>
	<td class="line x" title="178:193	This is most likely due to the lexicon translation process, which as mentioned in the agreement study in Section 3.1, leads to ambiguity and loss of subjectivity." ></td>
	<td class="line x" title="179:193	Instead, the corpus-based translations seem to better account for the ambiguity of the words, and the subjectivity is generally preserved in the sentence translations." ></td>
	<td class="line x" title="180:193	5 Conclusions In this paper, we described two approaches to generating resources for subjectivity annotations for a new 982 language, by leveraging on resources and tools available for English." ></td>
	<td class="line x" title="181:193	The first approach builds a target language subjectivity lexicon by translating an existing English lexicon using a bilingual dictionary." ></td>
	<td class="line x" title="182:193	The second generates a subjectivity-annotated corpus in a target language by projecting annotations from an automatically annotated English corpus." ></td>
	<td class="line x" title="183:193	These resources were validated in two ways." ></td>
	<td class="line x" title="184:193	First, we carried out annotation studies measuring the extent to which subjectivity is preserved across languages in each of the two resources." ></td>
	<td class="line x" title="185:193	These studies show that only a relatively small fraction of the entries in the lexicon preserve their subjectivity in the translation, mainly due to the ambiguity in both the source and the target languages." ></td>
	<td class="line x" title="186:193	This is consistent with observations made in previous work that subjectivity is a property associated not with words, but with word meanings (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="187:193	In contrast, the sentence-level subjectivity was found to be more reliably preserved across languages, with cross-lingual inter-annotator agreements comparable to the monolingual ones." ></td>
	<td class="line x" title="188:193	Second, we validated the two automatically generated subjectivity resources by using them to build a tool for subjectivity analysis in the target language." ></td>
	<td class="line x" title="189:193	Specifically, we developed two classifiers: a rulebased classifier that relies on the subjectivity lexicon described in Section 3.1, and a machine learning classifier trained on the subjectivity-annotated corpus described in Section 4.1." ></td>
	<td class="line x" title="190:193	While the highest precision for the subjective classification is obtained with the rule-based classifier, the overall best result of 67.85 F-measure is due to the machine learning approach." ></td>
	<td class="line x" title="191:193	This result is consistent with the annotation studies, showing that the corpus projections preserve subjectivity more reliably than the lexicon translations." ></td>
	<td class="line x" title="192:193	Finally, neither one of the classifiers relies on language-specific information, but rather on knowledge obtained through projections from English." ></td>
	<td class="line x" title="193:193	A similar method can therefore be used to derive tools for subjectivity analysis in other languages." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1124
Sentiment Polarity Identification in Financial News: A Cohesion-based Approach
Devitt, Ann;Ahmad, Khurshid;"></td>
	<td class="line x" title="1:173	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 984991, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:173	c2007 Association for Computational Linguistics Sentiment Polarity Identification in Financial News: A Cohesion-based Approach Ann Devitt School of Computer Science & Statistics, Trinity College Dublin, Ireland Ann.Devitt@cs.tcd.ie Khurshid Ahmad School of Computer Science & Statistics, Trinity College Dublin, Ireland Khurshid.Ahmad@cs.tcd.ie Abstract Text is not unadulterated fact." ></td>
	<td class="line x" title="3:173	A text can make you laugh or cry but can it also make you short sell your stocks in company A and buy up options in company B?" ></td>
	<td class="line x" title="4:173	Research in the domain of finance strongly suggests that it can." ></td>
	<td class="line x" title="5:173	Studies have shown that both the informational and affective aspects of news text affect the markets in profound ways, impacting on volumes of trades, stock prices, volatility and even future firm earnings." ></td>
	<td class="line x" title="6:173	This paper aims to explore a computable metric of positive or negative polarity in financial news text which is consistent with human judgments and can be used in a quantitative analysis of news sentiment impact on financial markets." ></td>
	<td class="line x" title="7:173	Results from a preliminary evaluation are presented and discussed." ></td>
	<td class="line x" title="8:173	1 Introduction Research in sentiment analysis has emerged to address the research questions: what is affect in text?" ></td>
	<td class="line x" title="9:173	what features of text serve to convey it?" ></td>
	<td class="line x" title="10:173	how can these features be detected and measured automatically." ></td>
	<td class="line x" title="11:173	Sentence and phrase level sentiment analysis involves a systematic examination of texts, such as blogs, reviews and news reports, for positive, negative or neutral emotions (Wilson et al. , 2005; Grefenstette et al. , 2004)." ></td>
	<td class="line x" title="12:173	The term sentiment analysis is used rather differently in financial economics where it refers to the derivation of market confidence indicators from proxies such as stock prices and trading volumes." ></td>
	<td class="line x" title="13:173	There is a tradition going back to the Nobel SverigesRiksbank Laureates Herbert Simon (1978 Prize) and Daniel Kahneman (2002 Prize), that shows that investors and traders in such markets can behave irrationally and that this bounded rationality is inspired by what the traders and investors hear from others about the conditions that may or may not prevail in the markets." ></td>
	<td class="line x" title="14:173	Robert Engle (2003 Prize) has given a mathematical description of the asymmetric and affective impact of news on prices: positive news is typically related to large changes in prices but only for a short time; conversely the effect of negative news on prices and volumes of trading is longer lasting." ></td>
	<td class="line x" title="15:173	The emergent domain of sociology of finance examines financial markets as social constructs and how communications, such as e-mails and news reports, may be loaded with sentiment which could distort market trading (MacKenzie, 2003)." ></td>
	<td class="line x" title="16:173	It would appear that news affects the markets in profound ways, impacting on volumes of trade, stock returns, volatility of prices and even future firm earnings." ></td>
	<td class="line x" title="17:173	In the domain of news impact analysis in finance, in recent years the focus has expanded from informational to affective content of text in an effort to explain the relationship between text and the markets." ></td>
	<td class="line x" title="18:173	All text, be it news, blogs, accounting reports or poetry, has a non-factual dimension conveying opinion, invoking emotion, providing a nuanced perspective of the factual content of the text." ></td>
	<td class="line x" title="19:173	With the increase of computational power and lexical and corpus resources it seems computationally feasible to detect some of the affective content of text automatically." ></td>
	<td class="line x" title="20:173	The motivation for the work reported here is to identify a metric for sentiment po984 larity which reliably replicates human evaluations and which is readily derivable from free text." ></td>
	<td class="line x" title="21:173	This research is being carried out in the context of a study of the impact of news and its attendant biases on financial markets, formalizing earlier multi-lingual, corpus-based empirical work that analysed change in sentiment and volume of news in large financial news corpora (Ahmad et al. , 2006)." ></td>
	<td class="line x" title="22:173	A systematic analysis of the impact of news bias or polarity on market variables requires a numeric value for sentiment intensity, as well as a binary tag for sentiment polarity, to identify trends in the sentiment indicator as well as turning points." ></td>
	<td class="line x" title="23:173	In this approach, the contribution to an overall sentiment polarity and intensity metric of individual lexical items which are affective by definition is determined by their connectivity and position within a representation of the text as a whole based on the principles of lexical cohesion." ></td>
	<td class="line x" title="24:173	The contribution of each element is therefore not purely additive but rather is mitigated by its relevance and position relative to other elements." ></td>
	<td class="line x" title="25:173	Section 2 sets out related work in the sentiment analysis domain both in computational linguistics and in finance where these techniques have been applied with some success." ></td>
	<td class="line x" title="26:173	Section 3 outlines the cohesion-based algorithm for sentiment polarity detection, the resources used and the benefits of using the graph-based text representation approach." ></td>
	<td class="line x" title="27:173	This approach was evaluated relative to a small corpus of gold standard sentiment judgments." ></td>
	<td class="line x" title="28:173	The derivation of the gold standard and details of the evaluation are outlined in section 4." ></td>
	<td class="line x" title="29:173	The results are presented and discussed in section 5 and section 6 concludes with a look at future challenges for this research." ></td>
	<td class="line x" title="30:173	2 Related Work 2.1 Cognitive Theories of Emotion In order to understand how emotion can be realised in text, we must first have a notion of what emotion is and how people experience it." ></td>
	<td class="line x" title="31:173	Current cognitive theories of what constitutes emotion are divided between two primary approaches: categorical and dimensional." ></td>
	<td class="line x" title="32:173	The Darwinian categorical approach posits a finite set of basic emotions which are experienced universally across cultures, (e.g. anger, fear, sadness, surprise (Ekman and Friesen, 1971))." ></td>
	<td class="line x" title="33:173	The second approach delineates emotions according to multiple dimensions rather than into discrete categories." ></td>
	<td class="line x" title="34:173	The two primary dimensions in this account are a goodbad axis, the dimension of valence or evaluation, and a strong-weak axis, the dimension of activation or intensity (Osgood et al. , 1957)." ></td>
	<td class="line x" title="35:173	The work reported here aims to conflate the evaluation and activation dimensions into one metric with the size of the value indicating strength of activation and its sign, polarity on the evaluation axis." ></td>
	<td class="line x" title="36:173	2.2 Sentiment Analysis Sentiment analysis in computational linguistics has focused on examining what textual features (lexical, syntactic, punctuation, etc) contribute to affective content of text and how these features can be detected automatically to derive a sentiment metric for a word, sentence or whole text." ></td>
	<td class="line x" title="37:173	Wiebe and colleagues have largely focused on identifying subjectivity in texts, i.e. identifying those texts which are affectively neutral and those which are not." ></td>
	<td class="line x" title="38:173	This work has been grounded in a strong human evaluative component." ></td>
	<td class="line x" title="39:173	The subjectivity identification research has moved from initial work using syntactic class, punctuation and sentence position features for subjectivity classifiers to later work using more lexical features like gradation of adjectives or word frequency (Wiebe et al. , 1999; Wiebe et al. , 2005)." ></td>
	<td class="line oc" title="40:173	Others, such as Turney (2002), Pang and Vaithyanathan (2002), have examined the positive or negative polarity, rather than presence or absence, of affective content in text." ></td>
	<td class="line x" title="41:173	Kim and Hovy (2004), among others, have combined the two tasks, identifying subjective text and detecting its sentiment polarity." ></td>
	<td class="line x" title="42:173	The indicators of affective content have been drawn from lexical sources, corpora and the world wide web and combined in a variety of ways, including factor analysis and machine learning techniques, to determine when a text contains affective content and what is the polarity of that content." ></td>
	<td class="line x" title="43:173	2.3 Sentiment and News Impact Analysis Niederhoffer (1971), academic and hedge fund manager, analysed 20 years of New York Times headlines classified into 19 semantic categories and on a good-bad rating scale to evaluate how the markets reacted to good and bed news: he found that markets do react to news with a tendency to overreact to bad news." ></td>
	<td class="line x" title="44:173	Somewhat prophetically, he suggests 985 that news should be analysed by computers to introduce more objectivity in the analysis." ></td>
	<td class="line x" title="45:173	Engle and Ng (1993) proposed the news impact curve as a model for how news impacts on volatility in the market with bad news introducing more volatility than good news." ></td>
	<td class="line x" title="46:173	They used the market variable, stock returns, as a proxy for news, an unexpected drop in returns for bad news and an unexpected rise for good news." ></td>
	<td class="line x" title="47:173	Indeed, much early work used such market variables or readily quantifiable aspects of news as a proxy for the news itself: e.g. news arrival, type, provenance and volumes (Cutler et al. , 1989; Mitchell and Mulherin, 1994)." ></td>
	<td class="line x" title="48:173	More recent studies have proceeded in a spirit of computer-aided objectivity which entails determining linguistic features to be used to automatically categorise text into positive or negative news." ></td>
	<td class="line x" title="49:173	Davis et al (2006) investigate the effects of optimistic or pessimistic language used in financial press releases on future firm performance." ></td>
	<td class="line x" title="50:173	They conclude that a) readers form expectations regarding the habitual bias of writers and b) react more strongly to reports which violate these expectations, strongly suggesting that readers, and by extension the markets, form expectations about and react to not only content but also affective aspects of text." ></td>
	<td class="line x" title="51:173	Tetlock (2007) also investigates how a pessimism factor, automatically generated from news text through term classification and principal components analysis, may forecast market activity, in particular stock returns." ></td>
	<td class="line x" title="52:173	He finds that high negativity in news predicts lower returns up to 4 weeks around story release." ></td>
	<td class="line x" title="53:173	The studies establish a relationship between affective bias in text and market activity that market players and regulators may have to address." ></td>
	<td class="line x" title="54:173	3 Approach 3.1 Cohesion-based Text Representation The approach employed here builds on a cohesionbased text representation algorithm used in a news story comparison application described in (Devitt, 2004)." ></td>
	<td class="line x" title="55:173	The algorithm builds a graph representation of text from part-of-speech tagged text without disambiguation using WordNet (Fellbaum, 1998) as a real world knowledge source to reduce information loss in the transition from text to text-based structure." ></td>
	<td class="line x" title="56:173	The representation is designed within the theoretical framework of lexical cohesion (Halliday and Hasan, 1976)." ></td>
	<td class="line x" title="57:173	Aspects of the cohesive structure of a text are captured in a graph representation which combines information derived from the text and WordNet semantic content." ></td>
	<td class="line x" title="58:173	The graph structure is composed of nodes representing concepts in or derived from the text connected by relations between these concepts in WordNet, such as antonymy or hypernymy, or derived from the text, such as adjacency in the text." ></td>
	<td class="line x" title="59:173	In addition, the approach provides the facility to manipulate or control how the WordNet semantic content information is interpreted through the use of topological features of the knowledge base." ></td>
	<td class="line x" title="60:173	In order to evaluate the relative contribution of WordNet concepts to the information content of a text as a whole, a node specificity metric was derived based on an empirical analysis of the distribution of topological features of WordNet such as inheritance, hierarchy depth, clustering coefficients and node degree and how these features map onto human judgments of concept specificity or informativity." ></td>
	<td class="line x" title="61:173	This metric addresses the issue of the uneven population of most knowledge bases so that the local idiosyncratic characteristics of WordNet can be mitigated by some of its global features." ></td>
	<td class="line x" title="62:173	3.2 Sentiment Polarity Overlay By exploiting existing lexical resources for sentiment analysis, an explicit affective dimension can be overlaid on this basic text model." ></td>
	<td class="line x" title="63:173	Our approach to polarity measurement, like others, relies on a lexicon of tagged positive and negative sentiment terms which are used to quantify positive/negative sentiment." ></td>
	<td class="line x" title="64:173	In this first iteration of the work, SentiWN (Esuli and Sebastiani, 2006) was used as it provides a readily interpretable positive and negative polarity value for a set of affective terms which conflates Osgoods (1957) evaluative and activation dimensions." ></td>
	<td class="line x" title="65:173	Furthermore, it is based on WordNet 2.0 and can therefore be integrated into the existing text representation algorithm, where some nodes in the cohesion graph carry a SentiWN sentiment value and others do not." ></td>
	<td class="line x" title="66:173	The contribution of individual polarity nodes to the polarity metric of the text as a whole is then determined with respect to the textual information and WN semantic and topological features encoded in the underlying graph representation of the text." ></td>
	<td class="line x" title="67:173	Three polarity metrics were implemented to evaluate the effectiveness of exploiting different 986 aspects of the cohesion-based graph structure." ></td>
	<td class="line x" title="68:173	Basic Cohesion Metric is based solely on frequency of sentiment-bearing nodes in or derived from the source text, i.e. the sum of polarity values for all nodes in the graph." ></td>
	<td class="line x" title="69:173	Relation Type Metric modifies the basic metric with respect to the types of WordNet relations in the text-derived graph." ></td>
	<td class="line x" title="70:173	For each node in the graph, its sentiment value is the product of its polarity value and a relation weight for each relation this node enters into in the graph structure." ></td>
	<td class="line x" title="71:173	Unlike most lexical chaining algorithms, not all WordNet relations are treated as equal." ></td>
	<td class="line x" title="72:173	In this sentiment overlay, the relations which are deemed most relevant are those that potentially denote a relation of the affective dimension, like antonymy, and those which constitute key organising principles of the database, such as hypernymy." ></td>
	<td class="line x" title="73:173	Potentially affect-effecting relations have the strongest weighting while more amorphous relations, such as also see, have the lowest." ></td>
	<td class="line x" title="74:173	Node Specificity Metric modifies the basic metric with respect to a measure of node specificity calculated on the basis of topographical features of WordNet." ></td>
	<td class="line x" title="75:173	The intuition behind this measure is that highly specific nodes or concepts may carry more informational and, by extension, affective content than less specific ones." ></td>
	<td class="line x" title="76:173	We have noted the difficulty of using a knowledge base whose internal structure is not homogeneous and whose idiosyncrasies are not quantified." ></td>
	<td class="line x" title="77:173	The specificity measure aims to factor out population sparseness or density in WordNet by evaluating the contribution of each node relative to its depth in the hierarchy, its connectivity (branchingFactor) and its siblings: Spc = (depth+ln(siblings)ln(branchingFactor))NormalizingFactor (1) The three metrics are further specialised according to the following two boolean flags: InText: the metric is calculated based on 1) only those nodes representing terms in the source text, or 2) all nodes in the graph representation derived from the text." ></td>
	<td class="line x" title="78:173	In this way, the metrics can be calculated using information derived from the graph representation, such as node specificity, without potentially noisy contributions from nodes not in the source text but related to them, via relations such as hypernymy." ></td>
	<td class="line x" title="79:173	Modifiers: the metric is calculated using all open class parts of speech or modifiers alone." ></td>
	<td class="line x" title="80:173	On a cursory inspection of SentiWN, it seems that modifiers have more reliable values than nouns or verbs." ></td>
	<td class="line x" title="81:173	This option was included to test for possible adverse effects of the lexicon." ></td>
	<td class="line x" title="82:173	In total for each metric there are four outcomes combining inText true/false and modifiers true/false." ></td>
	<td class="line x" title="83:173	4 Evaluation The goal of this research is to examine the relationship between financial markets and financial news, in particular the polarity of financial news." ></td>
	<td class="line x" title="84:173	The domain of finance provides data and methods for solid quantitative analysis of the impact of sentiment polarity in news." ></td>
	<td class="line x" title="85:173	However, in order to engage with this long tradition of analysis of the instruments and related variables of the financial markets, the quantitative measure of polarity must be not only easy to compute, it must be consistent with human judgments of polarity in this domain." ></td>
	<td class="line x" title="86:173	This evaluation is a first step on the path to establishing reliability for a sentiment measure of news." ></td>
	<td class="line x" title="87:173	Unfortunately, the focus on news, as opposed to other text types, has its difficulties." ></td>
	<td class="line oc" title="88:173	Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments, such as sentences (Wilson et al. , 2005), or on longer documents with an explicit polarity orientation like movie or product reviews (Turney, 2002)." ></td>
	<td class="line x" title="89:173	Not all news items may express overt sentiment." ></td>
	<td class="line x" title="90:173	Therefore, in order to test our hypothesis, we selected a news topic which was considered a priori to have emotive content." ></td>
	<td class="line x" title="91:173	4.1 Corpus Markets react strongest to information about firms to which they have an emotional attachment (MacGregor et al. , 2000)." ></td>
	<td class="line x" title="92:173	Furthermore, takeovers and mergers are usually seen as highly emotive contexts." ></td>
	<td class="line x" title="93:173	To combine these two emotion-enhancing factors, a corpus of news texts was compiled on the topic of the aggressive takeover bid of a low-cost airline (Ryanair) for the Irish flag-carrier airline (Aer Lingus)." ></td>
	<td class="line x" title="94:173	Both airlines have a strong (positive and negative) emotional attachment for many in Ireland." ></td>
	<td class="line x" title="95:173	Furthermore, both airlines are highly visible within the country and have vocal supporters and detractors in the public arena." ></td>
	<td class="line x" title="96:173	The corpus is drawn from the 987 national media and international news wire sources and spans 4 months in 2006 from the flotation of the flag carrier on the stock exchange in September 2006, through the surprise take-over bid announcement by Ryanair, to the withdrawal of the bid by Ryanair in December 2006.1 4.2 Gold Standard A set of 30 texts selected from the corpus was annotated by 3 people on a 7-point scale from very positive to very negative." ></td>
	<td class="line x" title="97:173	Given that a takeover bid has two players, the respondents were asked also to rate the semantic orientation of the texts with respect to the two players, Ryanair and Aer Lingus." ></td>
	<td class="line x" title="98:173	Respondents were all native English speakers, 2 female and 1 male." ></td>
	<td class="line x" title="99:173	To ensure emotional engagement in the task, they were first asked to rate their personal attitude to the two airlines." ></td>
	<td class="line x" title="100:173	The ratings in all three cases were on the extreme ends of the 7 point scale, with very positive attitudes towards the flag carrier and very negative attitudes towards the low-cost airline." ></td>
	<td class="line x" title="101:173	Respondent attitudes may impact on their text evaluations but, given the high agreement of attitudes in this study, this impact should at least be consistent across the individuals in the study." ></td>
	<td class="line x" title="102:173	A larger study should control explicitly for this variable." ></td>
	<td class="line x" title="103:173	As the respondents gave ratings on a ranked scale, inter-respondent reliability was determined using Krippendorfs alpha, a modification of the Kappa coefficient for ordinal data (Krippendorff, 1980)." ></td>
	<td class="line x" title="104:173	On the general ranking scale, there was little agreement (kappa = 0.1685), corroborating feedback from respondents on the difficulty of providing a general rating for text polarity distinct from a rating with respect to one of the two companies." ></td>
	<td class="line x" title="105:173	However, there was an acceptable degree of agreement (Grove et al. , 1981) on the Ryanair and Aer Lingus polarity ratings, kappa = 0.5795 and kappa = 0.5589 respectively." ></td>
	<td class="line x" title="106:173	Results report correlations with these ratings which are consistent and, from the financial market perspective, potentially more interesting.2 1A correlation analysis of human sentiment ratings with Ryanair and Aer Lingus stock prices for the last quarter of 2006 was conducted." ></td>
	<td class="line x" title="107:173	The findings suggest that stock prices were correlated with ratings with respect to Aer Lingus, suggesting that, during this takeover period, investors may have been influenced by sentiment expressed in news towards Aer Lingus." ></td>
	<td class="line x" title="108:173	However, the timeseries is too short to ensure statistical significance." ></td>
	<td class="line x" title="109:173	2Results in this paper are reported with respect to the 4.3 Performance Metrics The performance of the polarity algorithm was evaluated relative to a corpus of human-annotated news texts, focusing on two separate dimensions of polarity: 1." ></td>
	<td class="line x" title="110:173	Polarity direction: the task of assigning a binary positive/negative value to a text 2." ></td>
	<td class="line x" title="111:173	Polarity intensity: the task of assigning a value to indicate the strength of the negative/positive polarity in a text." ></td>
	<td class="line x" title="112:173	Performance on the former is reported using standard recall and precision metrics." ></td>
	<td class="line x" title="113:173	The latter is reported as a correlation with average human ratings." ></td>
	<td class="line x" title="114:173	4.4 Baseline For the metrics in section 3, the baseline for comparison sums the SentiWN polarity rating for only those lexical items present in the text, not exploiting any aspect of the graph representation of the text." ></td>
	<td class="line x" title="115:173	This baseline corresponds to the Basic Cohesion Metric, with inText = true (only lexical items in the text) and modifiers = false (all parts of speech)." ></td>
	<td class="line x" title="116:173	5 Results and Discussion 5.1 Binary Polarity Assignment The baseline results for positive ratings, negative ratings and overall accuracy for the task of assigning a polarity tag are reported in table 1." ></td>
	<td class="line x" title="117:173	The results show Type Precision Recall FScore Positive 0.381 0.7273 0.5 Negative 0.667 0.3158 0.4286 Overall 0.4667 0.4667 0.4667 Table 1: Baseline results that the baseline tends towards the positive end of the rating spectrum, with high recall for positive ratings but low precision." ></td>
	<td class="line x" title="118:173	Conversely, negative ratings have high precision but low recall." ></td>
	<td class="line x" title="119:173	Figures 1 to 3 illustrate the performance for positive, negative and overall ratings of all metricinTextModifier combinations, enumerated in table 2, relative to this baseline, the horizontal." ></td>
	<td class="line x" title="120:173	Those metrics which surpass this line are deemed to outperform the baseline." ></td>
	<td class="line x" title="121:173	Ryanair ratings as they had the highest inter-rater agreement." ></td>
	<td class="line x" title="122:173	988 1 Cohesion 5 Relation 9 NodeSpec 2 CohesionTxt 6 RelationTxt 10 NodeSpecTxt 3 CohesionMod 7 RelationMod 11 NodeSpecMod 4 CohesionTxtMod 8 RelationTxtMod 12 NodeSpecTxtMod Table 2: Metric types in Figures 1-3 Figure 1: F Score for Positive Ratings All metrics have a bias towards positive ratings with attendant high positive recall values and improved f-score for positive polarity assignments." ></td>
	<td class="line x" title="123:173	The Basic Cohesion Metric marginally outperforms the baseline overall indicating that exploiting the graph structure gives some added benefit." ></td>
	<td class="line x" title="124:173	For the Relations and Specificity metrics, system performance greatly improves on the baseline for the modifiers = true options, whereas, when all parts of speech are included (modifier = false), performance drops significantly." ></td>
	<td class="line x" title="125:173	This sensitivity to inclusion of all word classes could suggest that modifiers are better indicators of text polarity than other word classes or that the metrics used are not appropriate to non-modifier parts of speech." ></td>
	<td class="line x" title="126:173	The former hypothesis is not supported by the literature while the latter is not supported by prior successful application of these metrics in a text comparison task." ></td>
	<td class="line x" title="127:173	In order to investigate the source of this sensitivity, we intend to examine the distribution of relation types and node specificity values for sentiment-bearing terms to determine how best to tailor these metrics to the sentiment identification task." ></td>
	<td class="line x" title="128:173	A further hypothesis is that the basic polarity values for non-modifiers are less reliable than for adjectives and adverbs." ></td>
	<td class="line x" title="129:173	On a cursory inspection of polarity values of nouns and adjectives in SentiWN, it would appear that adjectives are somewhat more reliably labelled than nouns." ></td>
	<td class="line x" title="130:173	For example, crime and Figure 2: F Score for Negative Ratings some of its hyponyms are labelled as neutral (e.g. forgery) or even positive (e.g. assault) whereas criminal is labelled as negative." ></td>
	<td class="line x" title="131:173	This illustrates a key weakness in a lexical approach such as this: overreliance on lexical resources." ></td>
	<td class="line x" title="132:173	No lexical resource is infallible." ></td>
	<td class="line x" title="133:173	It is therefore vital to spread the associated risk by using more than one knowledge source, e.g. multiple sentiment lexica or using corpus data." ></td>
	<td class="line x" title="134:173	Figure 3: F Score for All Ratings 5.2 Polarity Intensity Values The results on the polarity intensity task parallel the results on polarity tag assignment." ></td>
	<td class="line x" title="135:173	Table 3 sets out the correlation coefficients for the metrics with respect to the average human rating." ></td>
	<td class="line x" title="136:173	Again, the best performers are the relation type and node specificity metrics using only modifiers, significant to the 0.05 level." ></td>
	<td class="line x" title="137:173	Yet the correlation coefficients overall are not very high." ></td>
	<td class="line x" title="138:173	This would suggest that perhaps the relationship between the human ranking scale and the automatic one is not strictly linear." ></td>
	<td class="line x" title="139:173	Although the human ratings map approximately onto the automati989 cally derived scale, there does not seem to be a clear one to one mapping." ></td>
	<td class="line x" title="140:173	The section that follows discuss this and some of the other issues which this evaluation process has brought to light." ></td>
	<td class="line x" title="141:173	Metric inText Modifier Correlation Basic Cohesion No No 0.47** Yes No 0.42* No Yes 0.47** Yes Yes 0.47** Relation Type No No -0.1** Yes No -0.13* No Yes 0.5** Yes Yes 0.38* Node Specificity No No 0.00 Yes No -0.03 No Yes 0.48** Yes Yes 0.38* Table 3: Correlation Coefficients for human ratings." ></td>
	<td class="line x" title="142:173	Significant at the 0.01 level." ></td>
	<td class="line x" title="143:173	Significant at the 0.05 level." ></td>
	<td class="line x" title="144:173	5.3 Issues The Rating Scale and Thresholding Overall the algorithm tends towards the positive end of the spectrum in direct contrast to human raters with 55-70% of all ratings being negative." ></td>
	<td class="line x" title="145:173	Furthermore, the correlation of human to algorithm ratings is significant but not strongly directional." ></td>
	<td class="line x" title="146:173	It would appear that there are more positive lexical items in text, hence the algorithms positive bias." ></td>
	<td class="line x" title="147:173	Yet much of this positivity is not having a strong impact on readers, hence the negative bias observed in these evaluators." ></td>
	<td class="line x" title="148:173	This raises questions about the scale of human polarity judgments: are people more sensitive to negativity in text?" ></td>
	<td class="line x" title="149:173	is there a positive baseline in text that people find unremarkable and ignore?" ></td>
	<td class="line x" title="150:173	To investigate this issue, we will conduct a comparative corpus analysis of the distribution of positive and negative lexical items in text and their perceived strengths in text." ></td>
	<td class="line x" title="151:173	The results of this analysis should help to locate sentiment turning points or thresholds and establish an elastic sentiment scale which allows for baseline but disregarded positivity in text." ></td>
	<td class="line x" title="152:173	The Impact of the Lexicon The algorithm described here is lexicon-based, fully reliant on available lexical resources." ></td>
	<td class="line x" title="153:173	However, we have noted that an over-reliance on lexica has its disadvantages, as any hand-coded or corpus-derived lexicon will have some degree of error or inconsistency." ></td>
	<td class="line x" title="154:173	In order to address this issue, it is necessary to spread the risk associated with a single lexical resource by drawing on multiple sources, as in (Kim and Hovy, 2005)." ></td>
	<td class="line x" title="155:173	The SentiWN lexicon used in this implementation is derived from a seed word set supplemented WordNet relations and as such it has not been psychologically validated." ></td>
	<td class="line x" title="156:173	For this reason, it has good coverage but some inconsistency." ></td>
	<td class="line x" title="157:173	Whissels Dictionary of Affect (1989) on the other hand is based entirely on human ratings of terms." ></td>
	<td class="line x" title="158:173	Its coverage may be narrower but accuracy might be more reliable." ></td>
	<td class="line x" title="159:173	This dictionary also has the advantage of separating out Osgoods (1957) evaluative and activation dimensions as well as an imaging rating for each term to allow a multi-dimensional analysis of affective content." ></td>
	<td class="line x" title="160:173	The WN Affect lexicon (Valitutti et al. , 2004) again provides somewhat different rating types where terms are classified in terms of denoting or evoking different physical or mental affective reactions." ></td>
	<td class="line x" title="161:173	Together, these resources could offer not only more accurate base polarity values but also more nuanced metrics that may better correspond to human notions of affect in text." ></td>
	<td class="line x" title="162:173	The Gold Standard Sentiment rating evaluation is not a straight-forward task." ></td>
	<td class="line x" title="163:173	Wiebe et al (2005) note many of the difficulties associated human sentiment ratings of text." ></td>
	<td class="line x" title="164:173	As noted above, it can be even more difficult when evaluating news where the text is intended to appear impartial." ></td>
	<td class="line x" title="165:173	The attitude of the evaluator can be all important: their attitude to the individuals or organisations in the text, their professional viewpoint as a market player or an ordinary punter, their attitude to uncertainty and risk which can be a key factor in the world of finance." ></td>
	<td class="line x" title="166:173	In order to address these issues for the domain of news impact in financial markets, the expertise of market professionals must be elicited to determine what they look for in text and what viewpoint they adopt when reading financial news." ></td>
	<td class="line x" title="167:173	In econometric analysis, stock price or trading volume data constitute an alternative gold standard, representing a proxy for human reaction to news." ></td>
	<td class="line x" title="168:173	For economic significance, the data must span a time period of several years and compilation of a text and stock 990 price corpus for a large scale analysis is underway." ></td>
	<td class="line x" title="169:173	6 Conclusions and Future Work This paper presents a lexical cohesion based metric of sentiment intensity and polarity in text and an evaluation of this metric relative to human judgments of polarity in financial news." ></td>
	<td class="line x" title="170:173	We are conducting further research on how best to capture a psychologically plausible measure of affective content of text by exploiting available resources and a broader evaluation of the measure relative to human judgments and existing metrics." ></td>
	<td class="line x" title="171:173	This research is expected to contribute to sentiment analysis in finance." ></td>
	<td class="line x" title="172:173	Given a reliable metric of sentiment in text, what is the impact of changes in this value on market variables?" ></td>
	<td class="line x" title="173:173	This involves a sociolinguistic dimension to determine what publications or texts best characterise or are most read and have the greatest influence in this domain and the economic dimension of correlation with economic indicators." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-3007
Kinds of Features for Chinese Opinionated Information Retrieval
Zagibalov, Taras;"></td>
	<td class="line x" title="1:134	Proceedings of the ACL 2007 Student Research Workshop, pages 3742, Prague, June 2007." ></td>
	<td class="line x" title="2:134	c2007 Association for Computational Linguistics Kinds of Features for Chinese Opinionated Information Retrieval Taras Zagibalov Department of Informatics University of Sussex United Kingdom T.Zagibalov@sussex.ac.uk Abstract This paper presents the results of experiments in which we tested different kinds of features for retrieval of Chinese opinionated texts." ></td>
	<td class="line x" title="3:134	We assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR, but with some distinct features." ></td>
	<td class="line x" title="4:134	The experiments showed that the best results were obtained from the combination of character-based processing, dictionary look up (maximum matching) and a negation check." ></td>
	<td class="line x" title="5:134	1 Introduction The extraction of opinionated information has recently become an important research topic." ></td>
	<td class="line x" title="6:134	Business and governmental institutions often need to have information about how their products or actions are perceived by people." ></td>
	<td class="line x" title="7:134	Individuals may be interested in other peoples opinions on various topics ranging from political events to consumer products." ></td>
	<td class="line x" title="8:134	At the same time globalization has made the whole world smaller, and a notion of the world as a global village does not surprise people nowadays." ></td>
	<td class="line x" title="9:134	In this context we assume information in Chinese to be of particular interest as the Chinese world (the mainland China, Taiwan, Hong Kong, Singapore and numerous Chinese communities all over the world) is getting more and more influential over the world economy and politics." ></td>
	<td class="line x" title="10:134	We therefore believe that a system capable of providing access to opinionated information in other languages (especially in Chinese) might be of great use for individuals as well as for institutions involved in international trade or international relations." ></td>
	<td class="line x" title="11:134	The sentiment classification experiments presented in this paper were done in the context of Opinionated Information Retrieval which is planned to be a module in a Cross-Language Opinion Extraction system (CLOE)." ></td>
	<td class="line x" title="12:134	The main goal of this system is to provide access to opinionated information on any topic ad-hoc in a language different to the language of a query." ></td>
	<td class="line x" title="13:134	To implement the idea the CLOE system which is the context for the experiments described in the paper will consist of four main modules: 1." ></td>
	<td class="line x" title="14:134	Query translation 2." ></td>
	<td class="line x" title="15:134	Opinionated Information Retrieval 3." ></td>
	<td class="line x" title="16:134	Opinionated Information Extraction 4." ></td>
	<td class="line x" title="17:134	Results presentation The OIR module will process complex queries consisting of a word sequence indicating a topic and sentiment information." ></td>
	<td class="line x" title="18:134	An example of such a query is: Asus laptop + OPINIONS, another, more detailed query, might be Asus laptop + POSITIVE OPINIONS." ></td>
	<td class="line x" title="19:134	Another possible approach to the architecture of the CLOE system would be to implement the processing as a pipeline consisting, first, of using IR to retrieve certain articles relevant to the topic followed by second stage of classifying them according to sentiment polarity." ></td>
	<td class="line x" title="20:134	But such an approach probably would be too inefficient, as the search will produce a lot of irrelevant results (containing no opinionated information)." ></td>
	<td class="line x" title="21:134	37 2 Chinese NLP and Feature Selection Problem One of the central problems in Chinese NLP is what the basic unit1 of processing should be." ></td>
	<td class="line x" title="22:134	The problem is caused by a distinctive feature of the Chinese language absence of explicit word boundaries, while it is widely assumed that a word is of extreme importance for any NLP task." ></td>
	<td class="line x" title="23:134	This problem is also crucial for the present study as the basic unit definition affects the kinds of features to be used." ></td>
	<td class="line x" title="24:134	In this study we use a mixed approached, based both on words (tokens consisting of more than one character) and characters as basic units." ></td>
	<td class="line x" title="25:134	It is also important to note, that we use notion of words in the sense of Vocabulary Word as it was stated by Li (2000)." ></td>
	<td class="line x" title="26:134	This means that we use only tokens that are listed in a dictionary, and do not look for all words (including grammar words)." ></td>
	<td class="line x" title="27:134	3 Related Work Processing of subjective texts and opinions has received a lot of interest recently." ></td>
	<td class="line oc" title="28:134	Most of the authors traditionally use a classification-based approach for sentiment extraction and sentiment polarity detection (for example, Pang et al.(2002), Turney (2002), Kim and Hovy (2004) and others), however, the research described in this paper uses the information retrieval (IR) paradigm which has also been used by some researchers." ></td>
	<td class="line x" title="30:134	Several sentiment information retrieval models were proposed in the framework of probabilistic language models by Eguchi and Lavrenko (2006)." ></td>
	<td class="line x" title="31:134	The setting for the study was a situation when a users query specifies not only terms expressing a certain topic and also specifies a sentiment polarity of interest in some manner, which makes this research very similar to the present one." ></td>
	<td class="line x" title="32:134	However, we use sentiment scores (not probabilistic language models) for sentiment retrieval (see Section 4.1)." ></td>
	<td class="line x" title="33:134	Dave et al.(Dave et al. , 2003) described a tool for sifting through and synthesizing product reviews, automating the sort of work done by aggregation sites or clipping services." ></td>
	<td class="line x" title="35:134	The authors of this paper used probability scores of arbitrary-length substrings that provide optimal classification." ></td>
	<td class="line x" title="36:134	Unlike this approach 1In the context of this study terms feature and basic unit are used interchangeably." ></td>
	<td class="line x" title="37:134	we use a combination of sentiment weights of characters and words (see Section 4)." ></td>
	<td class="line x" title="38:134	Recently several works on sentiment extraction from Chinese texts were published." ></td>
	<td class="line x" title="39:134	In a paper by Ku et al.(2006a) a dictionary-based approach was used in the context of sentiment extraction and summarization." ></td>
	<td class="line x" title="41:134	The same authors describe a corpus of opinionated texts in another paper (2006b)." ></td>
	<td class="line x" title="42:134	This paper also defines the annotations for opinionated materials." ></td>
	<td class="line x" title="43:134	Although we use the same dictionary in our research, we do not use only word-based approach to sentiment detection, but we also use scores for characters obtained by processing the dictionary as a training corpus (see Section 4)." ></td>
	<td class="line x" title="44:134	4 Experiments In this paper we present the results of sentiment classification experiments in which we tested different kinds of features for retrieval of Chinese opinionated information." ></td>
	<td class="line x" title="45:134	As stated earlier (see Section 1), we assume that the task of retrieval of opinionated texts (OIR) can be regarded as a subtask of general IR with a query consisting of two parts: (1) words indicating topic and (2) a semantic class indicating sentiment (OPINIONS)." ></td>
	<td class="line x" title="46:134	The latter part of the query cannot be specified in terms that can be instantly used in the process of retrieval." ></td>
	<td class="line x" title="47:134	The sentiment part of the query can be further detailed into subcategories such as POSITIVE OPINIONS, NEGATIVE OPINIONS, NEUTRAL OPINIONS each of which can be split according to sentiment intensity (HIGHLY POSITIVE OPINIONS, SLIGHTLY NEGATIVE OPINIONS etc.)." ></td>
	<td class="line x" title="48:134	But whatever level of categorisation we use, the query is still too abstract and cannot be used in practice." ></td>
	<td class="line x" title="49:134	It therefore needs to be put into words and most probably expanded." ></td>
	<td class="line x" title="50:134	The texts should also be indexed with appropriate sentiment tags which in the context of sentiment processing implies classification of the texts according to presence / absence of a sentiment and, if the texts are opinionated, according to their sentiment polarity." ></td>
	<td class="line x" title="51:134	To test the proposed approach we designed two experiments." ></td>
	<td class="line x" title="52:134	The purpose of the first experiment was to find the most effective kind of features for sentiment polar38 ity discrimination (detection) which can be used for OIR 2." ></td>
	<td class="line x" title="53:134	Nie et al.(2000) found that for Chinese IR the most effective kinds of features were a combination of dictionary look up (longest-match algorithm) together with unigrams (single characters)." ></td>
	<td class="line x" title="55:134	The approach was tested in the first experiment." ></td>
	<td class="line x" title="56:134	The second experiment was designed to test the found set of features for text classification (indexing) for an OIR query of the first level (finds opinionated information) and for an OIR query of the second level (finds opinionated information with sentiment direction detection), thus the classifier should 1) detect opinionated texts and 2) classify the found items either as positive or as negative." ></td>
	<td class="line x" title="57:134	As training corpus for the second experiment we use the NTU sentiment dictionary (NTUSD) (by Ku et al.(2006a))3 as well as a list of sentiment scores of Chinese characters obtained from processing of the same dictionary." ></td>
	<td class="line x" title="59:134	Dictionary look up used the longest-match algorithm." ></td>
	<td class="line x" title="60:134	The dictionary has 2809 items in the positive part and 8273 items in the negative." ></td>
	<td class="line x" title="61:134	The same dictionary was also used as a corpus for calculating the sentiment scores of Chinese characters." ></td>
	<td class="line x" title="62:134	The use of the dictionary as a training corpus for obtaining the sentiment scores of characters is justified by two reasons: 1) it is domain-independent and 2) it contains only relevant (sentiment-related) information." ></td>
	<td class="line x" title="63:134	The above mentioned parts of the dictionary used as the corpus comprised 24308 characters in the negative part and 7898 characters in the positive part." ></td>
	<td class="line x" title="64:134	4.1 Experiment 1 A corpus of E-Bay4 customers reviews of products and services was used as a test corpus." ></td>
	<td class="line x" title="65:134	The total number of reviews is 128, of which 37 are negative (average length 64 characters) and 91 are positive (average length 18 characters), all of the reviews were tagged as positive or negative by the 2For simplicity we used only binary polarity in both experiments: positive or negative." ></td>
	<td class="line x" title="66:134	Thus terms sentiment polarity and sentiment direction are used interchangeably in this paper." ></td>
	<td class="line x" title="67:134	3Ku et al.(2006a) automatically generated the dictionary by enlarging an initial manually created seed vocabulary by consulting two thesauri, including tong2yi4ci2ci2lin2 and the Academia Sinica Bilingual Ontological Wordnet 3." ></td>
	<td class="line x" title="69:134	4http://www.ebay.com.cn/ reviewers5." ></td>
	<td class="line x" title="70:134	We computed two scores for each item (a review): one for positive sentiment, another for negative sentiment." ></td>
	<td class="line x" title="71:134	The decision about an items sentiment polarity was made every time by finding the biggest score of the two." ></td>
	<td class="line x" title="72:134	For every phrase (a chunk of characters between punctuation marks) a score was calculated as: Scphrase =summationdisplay(Scdictionary) +summationdisplay(Sccharacter) where Scdictionary is a dictionary based score calculated using following formula: Scdictionary = LdL s 100 where Ld length of a dictionary item, Ls length of a phrase." ></td>
	<td class="line x" title="73:134	The constant value 100 is used to weight the score, obtained by a series of preliminary tests as a value that most significantly improved the accuracy." ></td>
	<td class="line x" title="74:134	The sentiment scores for characters were obtained by the formula: Sci = Fi/F(i+j) where Sci is the sentiment score for a character for a given class i, Fi the characters relative frequency in a class i, F(i+j) the characters relative frequency in both classes i and j taken as one unit." ></td>
	<td class="line x" title="75:134	The relative frequency of character c is calculated as Fc = summationtextN csummationtext N(1n) where summationtextNc is a number of the characters occurrences in the corpus, andsummationtextN(1n) is the number of all characters in the same corpus." ></td>
	<td class="line x" title="76:134	Preliminary tests showed that inverting all the characters for which Sci  1 improves accuracy." ></td>
	<td class="line x" title="77:134	The inverting is calculated as follows: Scinverted = Sci 1 We compute scores rather than probabilities since we are combining information from two distinct sources (characters and words)." ></td>
	<td class="line x" title="78:134	5The corpus is available at http://www.informatics.sussex.ac.uk/users/tz21/corpSmall.zip." ></td>
	<td class="line x" title="79:134	39 In addition to the features specified (characters and dictionary items) we also used a simple negation check." ></td>
	<td class="line x" title="80:134	The system checked two most widely used negations in Chinese: bu and mei." ></td>
	<td class="line x" title="81:134	Every phrase was compared with the following pattern: negation+ 0-2 characters+ phrase." ></td>
	<td class="line x" title="82:134	The scores of all the unigrams in the phrase that matched the pattern were multiplied by -1." ></td>
	<td class="line x" title="83:134	Finally, the score was calculated for an item as the sum of the phrases scores modified by the negation check: Scitem =summationdisplay(Scphrase NegCheck) For sentiment polarity detection the item scores for each of the two polarities were compared to each other: the polarity with bigger score was assigned to the item." ></td>
	<td class="line x" title="84:134	SentimentPolarity = argmax(Sci|Scj) where Sci is an item score for one polarity and Scj is an item score for the other." ></td>
	<td class="line x" title="85:134	The main evaluation measure was accuracy of sentiment identification, expressed in percent." ></td>
	<td class="line x" title="86:134	4.1.1 Results of Experiment 1 To find out which kinds of features perform best for sentiment polarity detection the system was run several times with different settings." ></td>
	<td class="line x" title="87:134	Running without character scores (with dictionary longest-match only) gave the following results: almost 64% of positive and near 65% for negative reviews were detected correctly, which is 64% accuracy for the whole corpus (note that a baseline classifier tagging all items as positive achieves an accuracy of 71.1%)." ></td>
	<td class="line x" title="88:134	Characters with sentiment scores alone performed much better on negative reviews (84% accuracy) rather than on positive (65%), but overall performance was still better: 70%." ></td>
	<td class="line x" title="89:134	Both methods combined gave a significant increase on positive reviews (73%) and no improvement on negative (84%), giving 77% overall." ></td>
	<td class="line x" title="90:134	The last run was with the dictionary look up, the characters and the negation check." ></td>
	<td class="line x" title="91:134	The results were: 77% for positive and 89% for negative, 80% corpus-wide (see Table 1)." ></td>
	<td class="line x" title="92:134	Judging from the results it is possible to suggest that both the word-based dictionary look up method Method Positive Negative All Dictionary 63.7 64.8 64.0 Characters 64.8 83.7 70.3 Characters+Dictionary 73.6 83.7 76.5 Chars+Dictionary+negation 76.9 89.1 80.4 Table 1: Results of Experiment 1 (accuracy in percent)." ></td>
	<td class="line x" title="93:134	and character-based method contributed to the final result." ></td>
	<td class="line x" title="94:134	It also corresponds to the results obtained by Nie et al.(2000) for Chinese information retrieval, where the same combination of features (characters and words) also performed best." ></td>
	<td class="line x" title="96:134	The negation check increased the performance by 3% overall, up to 80%." ></td>
	<td class="line x" title="97:134	Although the performance gain is not very high, the computational cost of this feature is very low." ></td>
	<td class="line x" title="98:134	As we used a non-balanced corpus (71% of the reviews are positive), it is quite difficult to compare the results with the results obtained by other authors." ></td>
	<td class="line x" title="99:134	But the proposed classifier outperformed some standart classifiers on the same data set: a Naive Bayes (multinomial) classifier gained only 49.6 % of accuracy (63 items tagged correctly) while a Support vector machine classifier got 64.5 % of accuracy (82 items).6 4.2 Experiment 2 The second experiment included two parts: determining whether texts are opinionated which is a precondition for the processing of the OPINION part of the query; and tagging found texts with relevant sentiment for processing a more detailed form of this query POSITIVE/NEGATIVE OPINION." ></td>
	<td class="line x" title="100:134	For this experiment we used the features that showed the best performance as described in section 4.1: the dictionary items and the characters with the sentiment scores." ></td>
	<td class="line x" title="101:134	The test corpus for this experiment consisted of 282 items, where every item is a paragraph." ></td>
	<td class="line x" title="102:134	We used paragraphs as basic items in this experiment because of two reasons: 1." ></td>
	<td class="line x" title="103:134	opinionated texts (reviews) are usually quite short (in our corpus all of them are one paragraph), while texts of other genres are usually much longer; and 2." ></td>
	<td class="line x" title="104:134	for IR tasks it is more usual to retrieve units longer then a sentence." ></td>
	<td class="line x" title="105:134	6We used WEKA 3.4.10 (http://www.cs.waikato.ac.nz/ ml/weka ) 40 The test corpus has following structure: 128 items are opinionated, of which 91 are positive and 37 are negative (all the items are the reviews used in the first experiment, see 4.1)." ></td>
	<td class="line x" title="106:134	154 items are not opinionated, of which 97 are paragraphs taken from a scientific book on Chinese linguistics and 57 items are from articles taken form a Chinese on-line encyclopedia Baidu Baike7." ></td>
	<td class="line x" title="107:134	For the first task we used the following technique: every item was assigned a score (a sum of the characters scores and dictionary scores described in 4.1)." ></td>
	<td class="line x" title="108:134	The score was divided by the number of characters in the item to obtain the average score: averScitem = ScitemL item where Scitem is the item score, and Litem is the length of an item (number of characters in it)." ></td>
	<td class="line x" title="109:134	A positive and a negative average score is computed for each item." ></td>
	<td class="line x" title="110:134	4.2.1 Results of Experiment 2 To determine whether an item is opinionated (for OPINION query), the maximum of the two scores was compared to a threshold value." ></td>
	<td class="line x" title="111:134	The best performance was achieved with the threshold value of 1.6 more than 85% of accuracy8 (see Table 2)." ></td>
	<td class="line x" title="112:134	Next task (NEGATIVE/POSITIVE OPINIONS) was processed by comparing the negative and positive scores for each found item (see Table 2)." ></td>
	<td class="line x" title="113:134	Query Recall Precision F-measure OPINION 71.8 85.1 77.9 POS/NEG OPINION 64.0 75.9 69.4 Table 2: Results of Experiment 2 (in percent)." ></td>
	<td class="line x" title="114:134	Although the unopinionated texts are very different from the opinionated ones in terms of genre and topic, the standard classifiers (Naive Bayes (multinomial) and SVM) failed to identify any nonopinionated texts." ></td>
	<td class="line x" title="115:134	The most probable explanation for this is that there were no items tagged unopinionated in the training corpus (the sentiment dictionary) and there were only words and phrases with predominant sentiment meaning rather then topicrelated." ></td>
	<td class="line x" title="116:134	7http://baike.baidu.com/ 8A random choice could have approximately 55% of accuracy if tagged all items as negative." ></td>
	<td class="line x" title="117:134	It is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by Pang and Lee (2004) and Eriksson (2006)." ></td>
	<td class="line x" title="118:134	The accuracy of the sentiment detection of opinionated texts (excluding erroneously detected unopinionated texts) in Experiment 2 has increased by 13% for positive reviews and by 6% for negative reviews (see Table 3)." ></td>
	<td class="line x" title="119:134	Query Positive Negative Experiment 1 76.9 89.1 Experiment 2 89.9 95.6 Table 3: Accuracy of sentiment polarity detection of opinionated texts (in percent)." ></td>
	<td class="line x" title="120:134	5 Conclusion and Future Work These preliminary experiments showed that using single characters and dictionary items modified by the negation check can produce reasonable results: about 78% F-measure for sentiment detection (see 4.1.1) and almost 70% F-measure for sentiment polarity identification (see 4.2.1) in the context of domain-independent opinionated information retrieval." ></td>
	<td class="line x" title="121:134	However, since the test corpus is very small the results obtained need further validation on bigger corpora." ></td>
	<td class="line x" title="122:134	The use of the dictionary as a training corpus helped to avoid domain-dependency, however, using a dictionary as a training corpus makes it impossible to obtain grammar information by means of analysis of punctuation marks and grammar word frequencies." ></td>
	<td class="line x" title="123:134	More intensive use of context information could improve the accuracy." ></td>
	<td class="line x" title="124:134	The dictionary-based processing may benefit from the use of word relations information: some words have sentiment information only when used with others." ></td>
	<td class="line x" title="125:134	For example, a noun dongxi (a thing) does not seem to have any sentiment information on its own, although it is tagged as negative in the dictionary." ></td>
	<td class="line x" title="126:134	Some manual filtering of the dictionary may improve the output." ></td>
	<td class="line x" title="127:134	It might also be promising to test the influence on performance of the different classes of words in the dictionary, for example, to use only adjectives or adjectives and nouns together (excluding adverbials)." ></td>
	<td class="line x" title="128:134	Another technique to be tested is computing the 41 positive and negative scores for the characters used only in one class, but absent in another." ></td>
	<td class="line x" title="129:134	In the current system, characters are assigned only one score (for the class they are present in)." ></td>
	<td class="line x" title="130:134	It might improve accuracy if such characters have an appropriate negative score for the other class." ></td>
	<td class="line x" title="131:134	Finally, the average sentiment score may be used for sentiment scaling." ></td>
	<td class="line x" title="132:134	For example, if in our experiments items with a score less than 1.6 were considered not to be opinionated, then ones with score more than 1.6 can be put on a scale where higher scores are interpreted as evidence for higher sentiment intensity (the highest score was 52)." ></td>
	<td class="line x" title="133:134	The scaling approach could help to avoid the problem of assigning documents to more than one sentiment category as the approach uses a continuous scale rather than a predefined number of rigid classes." ></td>
	<td class="line x" title="134:134	The scale (or the scores directly) may be used as a means of indexing for a search engine comprising OIR functionality." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-1515
Annotating Expressions of Appraisal in English
Read, Jonathon;Hope, David;Carroll, John A.;"></td>
	<td class="line x" title="1:159	Proceedings of the Linguistic Annotation Workshop, pages 93100, Prague, June 2007." ></td>
	<td class="line x" title="2:159	c2007 Association for Computational Linguistics Annotating Expressions of Appraisal in English Jonathon Read, David Hope and John Carroll Department of Informatics University of Sussex United Kingdom {j.l.read,drh21,j.a.carroll}@sussex.ac.uk Abstract The Appraisal framework is a theory of the languageofevaluation, developedwithinthe tradition of systemic functional linguistics." ></td>
	<td class="line x" title="3:159	The framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people." ></td>
	<td class="line x" title="4:159	Accurate automatic recognition of these types of language can inform an analysis of document sentiment." ></td>
	<td class="line x" title="5:159	This paper describes the preparation of test data for algorithms for automatic Appraisal analysis." ></td>
	<td class="line x" title="6:159	The difficulty of the task is assessed by way of an inter-annotator agreement study, based on measures analogous to those used in the MUC-7 evaluation." ></td>
	<td class="line x" title="7:159	1 Introduction The Appraisal framework (Martin and White, 2005) describes a taxonomy of the language employed in communicating evaluation, explaining how users of Englishconveyattitude(emotion, judgementofpeople and appreciation of objects), engagement (assessment of the evaluations of other people) and how writers may modify the strength of their attitude/engagement." ></td>
	<td class="line x" title="8:159	Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al. , 2002) andsubjectivityanalysis(Wiebeetal., 2004), butassessing the usefulness of analysis algorithms leveragingtheAppraisalframeworkwillrequiretestdata." ></td>
	<td class="line x" title="10:159	At present there are no machine-readable Appraisal-annotated texts publicly available." ></td>
	<td class="line x" title="11:159	Realworld instances of Appraisal in use are limited to example extracts that demonstrate the theory, coming from a wide variety of genres as disparate as news reporting (White, 2002; Martin, 2004) and poetry (Martin and White, 2005)." ></td>
	<td class="line x" title="12:159	These examples, while useful in demonstrating the various aspects of Appraisal, can only be employed in a qualitative analysis and would bring about inconsistencies if analysed collectively  one can expect the writing style to depend upon the genre, resulting in significantly different syntactic constructions and lexical choices." ></td>
	<td class="line x" title="13:159	We therefore need to examine Appraisal across documents in the same genre and investigate patterns within that particular register." ></td>
	<td class="line x" title="14:159	This paper discusses the methodology of an Appraisal annotation study and an analysis of the inter-annotator agreement exhibited by two human judges." ></td>
	<td class="line x" title="15:159	The output of this study has the additional benefit of bringing a set of machine-readable annotations of Appraisal into the public domain for further research." ></td>
	<td class="line x" title="16:159	This paper is structured as follows." ></td>
	<td class="line x" title="17:159	The next section offers an overview of the Appraisal framework." ></td>
	<td class="line x" title="18:159	Section 3 discusses the methodology adopted for the annotation study." ></td>
	<td class="line x" title="19:159	Section 4 discusses the measures employed to assess inter-annotator agreement and reports the results of these measures." ></td>
	<td class="line x" title="20:159	Section 5 offers an analysis of cases of systematic disagreement." ></td>
	<td class="line x" title="21:159	Other computational work utilising the Appraisal framework is reviewed in Section 6." ></td>
	<td class="line x" title="22:159	Section 7 summarises the paper and outlines future work." ></td>
	<td class="line x" title="23:159	2 The linguistic framework of Appraisal The Appraisal framework (Martin and White, 2005) is a development of work in Systemic Functional 93 appraisal attitude engagement graduation affect judgement appreciation inclination happiness security satisfaction esteem sanction normality capacity tenacity veracity propriety reaction composition valuation impact quality balance complexity contract expand disclaim proclaim deny counter pronounce endorse concur affirm concede entertain attribute acknowledge distance force focus quantification intensification number mass extent proximity (space) proximity (time) distribution (space) distribution (time) degree vigour Figure 1: The Appraisal framework." ></td>
	<td class="line x" title="24:159	Linguistics (Halliday, 1994) and is concerned with interpersonal meaning in textthe negotiation of social relationships by communicating emotion, judgement and appreciation." ></td>
	<td class="line x" title="25:159	The taxonomy described by the Appraisal framework is depicted in Figure 1." ></td>
	<td class="line x" title="26:159	Appraisal consists of three subsystems that operate in parallel: attitude looks at how one expresses private state (Quirk et al. , 1985) (ones emotion and opinions); engagement considers the positioning of oneself with respect to the opinions of others and graduation investigates how the use of language functions to amplify or diminish the attitude and engagement conveyed by a text." ></td>
	<td class="line x" title="27:159	2.1 Attitude: emotion, ethics and aesthetics TheAttitudesub-systemdescribesthreeareasofprivate state: emotion, ethics and aesthetics." ></td>
	<td class="line x" title="28:159	An attitude is further qualified by its polarity (positive or negative)." ></td>
	<td class="line x" title="29:159	Affect identifies feelingsauthors emotions as represented by their text." ></td>
	<td class="line x" title="30:159	Judgement deals with authors attitude towards the behaviour of people; how authors applaud or reproach the actions of others." ></td>
	<td class="line x" title="31:159	Appreciation considers the evaluation of thingsboth man-made and natural phenomena." ></td>
	<td class="line x" title="32:159	2.2 Engagement: appraisals of appraisals Through engagement, Martin and White (2005) deal with the linguistic constructions by which authors construe their point of view and the resources used to adopt stances towards the opinions of other people." ></td>
	<td class="line x" title="33:159	The theory of engagement follows Stubbs (1996) in that it assumes that all utterances convey point of view and Bakhtin (1981) in supposing that all utterances occur in a miscellany of other utterances on the same motif, and that they carry both implicit and explicit responses to one another." ></td>
	<td class="line x" title="34:159	In other words, all text is inherently dialogistic as it encodesauthorsreactionstotheirexperiences(includingpreviousinteractionwithotherwriters)." ></td>
	<td class="line x" title="35:159	Engagementcanbebothretrospective(thatis,anauthorwill acknowledge and agree or disagree with the stances of others who have previously appraised a subject), andprospective(onemayanticipatetheresponsesof an intended audience and include counter-responses in the original text)." ></td>
	<td class="line x" title="36:159	2.3 Graduation: strength of evaluations Martin and White (2005) consider the resources by which writers alter the strength of their evaluation as a system of graduation." ></td>
	<td class="line x" title="37:159	Graduation is a general property of both attitude and engagement." ></td>
	<td class="line x" title="38:159	In attitude it enables authors to convey greater or lesser degrees of positivity or negativity, while graduation of engagements scales authors conviction in their utterance." ></td>
	<td class="line x" title="39:159	Graduation is divided into two subsystems." ></td>
	<td class="line x" title="40:159	Force alters appraisal propositions in terms of its inten94 sity, quantity or temporality, or by means of spatial metaphor." ></td>
	<td class="line x" title="41:159	Focus considers the resolution of semantic categories, for example: They play real jazz." ></td>
	<td class="line x" title="42:159	They play jazz, sort of." ></td>
	<td class="line x" title="43:159	In real terms a musician either plays jazz or they do not, but these examples demonstrate how authors blur the lines of semantic sets and how binary relationships can be turned into scalar ones." ></td>
	<td class="line x" title="44:159	3 Annotation methodology The corpus used in this study consists of unedited book reviews." ></td>
	<td class="line x" title="45:159	Book reviews are good candidates for this study as, while they are likely to contain similar language by virtue of being from the same genre of writing, we can also expect examples of Appraisals many classes (for example, the emotion attributed to the characters in reviews of novels, judgements of authors competence and character, appreciation of the qualities of books and engagement with the propositions put forth by the authors under review)." ></td>
	<td class="line x" title="46:159	The articles were taken from the web sites of four British newspapers (The Guardian, The Independent, The Telegraph and The Times) on two different dates31 July 2006 and 11 September 2006." ></td>
	<td class="line x" title="47:159	Each review is attributed to a unique author." ></td>
	<td class="line x" title="48:159	The corpus is comprised of 38 documents, containing a total of 36,997 tokens in 1,245 sentences." ></td>
	<td class="line x" title="49:159	Two human annotators, d and j, participated in this study, assigning tags independently." ></td>
	<td class="line x" title="50:159	The annotators were well-versed in the Appraisal framework, having studied the latest literature." ></td>
	<td class="line x" title="51:159	The judges were asked to annotate appraisal-bearing terms with the appraisal type presumed to be intended by the author of the text." ></td>
	<td class="line x" title="52:159	They were asked to highlight each example of appraisal and specify the type of attitude, engagement or graduation present." ></td>
	<td class="line x" title="53:159	They also assigned a polarity (positive or negative) to attitudinal items and a scaling (up or down) to graduating items, employing a custom-developed software tool to annotate the documents." ></td>
	<td class="line x" title="54:159	Four alternative annotation strategies were considered." ></td>
	<td class="line x" title="55:159	Oneapproachistoallowonlyasingletoken per annotation." ></td>
	<td class="line x" title="56:159	However, this is too simplistic for an Appraisal annotation studya unit of Appraisal is frequently larger than a single token." ></td>
	<td class="line x" title="57:159	Consider the following examples: (1) The design was deceptivelyVERACITY simple COMPLEXITY." ></td>
	<td class="line x" title="58:159	() (2) The design was deceptively simpleCOMPLEXITY." ></td>
	<td class="line x" title="59:159	Example 1 demonstrates that a single-token approach is inappropriate as it ascribes a judgement of someones honesty, whereas Example 2 indicates the correct analysisthe sentence is an appreciation of the simplicity of the design." ></td>
	<td class="line x" title="60:159	This example shows how it is necessary to annotate larger units of appraisal-bearing language." ></td>
	<td class="line x" title="61:159	Including more tokens, however, increases the complexity of the annotation task, and reduces the likelihood of agreement between the judges, as the annotated tokens of one judge may be a subset of, or overlap with, those of another." ></td>
	<td class="line x" title="62:159	We therefore experimented with tagging entire sentences in order to constrain the annotators range of choices." ></td>
	<td class="line x" title="63:159	This resultedinitsownproblemsasthereisoftenmorethan one appraisal in a sentence, for example: (3) The design was deceptively simpleCOMPLEXITY and belied his ingenuityCAPACITY." ></td>
	<td class="line x" title="64:159	An alternative approach is to permit annotators to tag an arbitrary number of contiguous tokens." ></td>
	<td class="line x" title="65:159	Arbitrary-length tagging is disadvantageous as the judges will frequently tag units of differing length, but this can be compensated for by relaxing the rules for agreementfor example, by allowing intersecting annotations to match successfully (Wiebe et al. , 2005)." ></td>
	<td class="line x" title="66:159	Bruce and Wiebe (1999) employ another approach, creating units from every non-compound sentence and each conjunct of every compound sentence." ></td>
	<td class="line x" title="67:159	This side-steps the problem of ambiguity in appraisalunitlength,butwillstillfailtocaptureboth appraisals demonstrated in the second conjunct of Example 4." ></td>
	<td class="line x" title="68:159	(4) The design was deceptively simpleCOMPLEXITY and belied his remarkableNORMALITY ingenuityCAPACITY." ></td>
	<td class="line x" title="69:159	Ultimately in this study, we permitted judges to annotate any number of tokens in order to allow for multiple Appraisal units of differing sizes within sentences." ></td>
	<td class="line x" title="70:159	Annotation was carried out over two rounds, punctuated by an intermediary analysis of 95 d j d j d j Inclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59 Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63 Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63 Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14 Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55 Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39 Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56 Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72 Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45 Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29 Quality 2.55 3.40 Acknowledge 2.42 3.33 Table 1: The distribution of the Appraisal types selected by each annotator (%)." ></td>
	<td class="line x" title="71:159	d j Documents 115.74 77.21 Sentences 3.65 2.43 Words 0.12 0.08 Table 2: The density of annotations relative to the number of documents, sentences and words." ></td>
	<td class="line x" title="72:159	agreement and disagreement between the two annotators." ></td>
	<td class="line x" title="73:159	The judges discussed examples of the most common types of disagreement in an attempt to acquireacommonunderstandingforthesecondround, but annotations from the first round were left unaltered." ></td>
	<td class="line x" title="74:159	Following the methodology described above, d made 3,176 annotations whilst j made 2,886 annotations." ></td>
	<td class="line x" title="75:159	The distribution of the Appraisal types ascribed is shown in Table 1, while Table 2 details the density of annotations in documents, sentences and words." ></td>
	<td class="line x" title="76:159	4 Measuring inter-annotator agreement The study of inter-annotator agreement begins by considering the level of agreement exhibited by the annotators in deciding which tokens are representative of Appraisal, irrespective of the type." ></td>
	<td class="line x" title="77:159	As discussed, this is problematic as judges are liable to choose different length token spans when marking up what is essentially the same appraisal, as demonstrated by Example 5." ></td>
	<td class="line x" title="78:159	(5) [d] It is tempting to point to the bombs in London and elsewhere, to the hideous messQUALITY in Iraq, to recent victories of the Islamists, to theviolent and polarised rhetoricPROPRIETY and answer yes." ></td>
	<td class="line x" title="79:159	[j] It is tempting to point to the bombs in London and elsewhere, to the hideousQUALITY messBALANCE in Iraq, to recent victories of Islamists, to the violentPROPRIETY and polarised PROPRIETY rhetoric and answer yes." ></td>
	<td class="line x" title="80:159	Wiebe et al.(2005), who faced this problem when annotating expressions of opinion under their own framework, acceptthatitisnecessarytoconsiderthe validity of all judges interpretations and therefore consider intersecting annotations (such as hideous and hideous mess) to be matches." ></td>
	<td class="line x" title="82:159	The same relaxation of constraints is employed in this study." ></td>
	<td class="line x" title="83:159	Tasks with a known number of annotative units can be analysed with measures of agreement such as Cohens  Coefficient (1960), but the judges freedom in this task prohibits meaningful application of thismeasure." ></td>
	<td class="line x" title="84:159	Forexample,considerhowwordsense annotatorsareobligedtochoosefromalimitedfixed set of senses for each token, whereas judges annotating Appraisal are free to select one of thirty-two classes for any contiguous substring of any length within each document; there are 16parenleftbign2  nparenrightbig possible choices in a document of n tokens (approximately 6.5  108 possibilities in this corpus)." ></td>
	<td class="line x" title="85:159	A wide range of evaluation metrics have been employed by the Message Understanding Conferences (MUCs)." ></td>
	<td class="line x" title="86:159	The MUC-7 tasks included extraction of named entities, equivalence classes, attributes, facts and events (Chinchor, 1998)." ></td>
	<td class="line x" title="87:159	The participating systems were evaluated using a variety of related measures, defined in Table 3." ></td>
	<td class="line x" title="88:159	These tasks are similar to Appraisal annotation in that the units are formed of an arbitrary number of contiguous tokens." ></td>
	<td class="line x" title="89:159	In this study the agreement exhibited by an annotator a is evaluated as a pair-wise comparison against the other annotator b. Annotator b provides 96 COR Number correct INC Number incorrect MIS Number missing SPU Number spurious POS Number possible = COR + INC + MIS ACT Number actual = COR + INC + SPU FSC F-score = (2  REC  PRE) /(REC + PRE) REC Precision = COR/POS PRE Recall = COR/ACT SUB Substitution = INC/(COR + INC) ERR Error per response = (INC + SPU + MIS) /(COR + INC + SPU + MIS) UND Under-generation = MIS/POS OVG Over-generation = SPU/ACT Table 3: MUC-7 score definitions (Chinchor 1998)." ></td>
	<td class="line x" title="90:159	FSC REC PRE ERR UND OVG d 0.682 0.706 0.660 0.482 0.294 0.340 j 0.715 0.667 0.770 0.444 0.333 0.230 x 0.698 0.686 0.711 0.462 0.312 0.274 Table 4: MUC-7 test scores, evaluating the agreement in text anchors selected by the annotators." ></td>
	<td class="line x" title="91:159	x denotes the average value, calculated using the harmonic mean." ></td>
	<td class="line x" title="92:159	a presumed gold standard for the purposes of evaluating agreement." ></td>
	<td class="line x" title="93:159	Note, however, that in this case it does not necessarily follow that REC(a w.r.t. b) = PRE(b w.r.t. a)." ></td>
	<td class="line x" title="94:159	Consider that a may tend to make one-word annotations whilst b prefers to annotate phrases; the set of as annotations will contain multiple matches for some of the phrases annotated by b (refer to Example 5, for instance)." ></td>
	<td class="line x" title="95:159	The number correct will differ for each annotator in the pair under evaluation." ></td>
	<td class="line x" title="96:159	Table 4 lists the values for the MUC-7 measures applied to the text spans selected by the annotators." ></td>
	<td class="line x" title="97:159	Annotator d is inclined to identify text as Appraisal more frequently than annotator j. This results in higher recall for d, but with lower precision." ></td>
	<td class="line x" title="98:159	Naturally, the opposite observation can be made about annotator j. Both annotators exhibit a high error rate at 48.2% and 44.4% for d and j respectively." ></td>
	<td class="line x" title="99:159	The substitution rate is not listed as there are no classes to substitute when considering only text anchor agreement." ></td>
	<td class="line x" title="100:159	The second round of annotation achieved slightly higher agreement (the mean F-score increased by 0.033)." ></td>
	<td class="line x" title="101:159	FSC REC PRE SUB ERR 0 0.698 0.686 0.711 0.000 0.462 1 0.635 0.624 0.647 0.090 0.511 2 0.528 0.518 0.538 0.244 0.594 3 0.448 0.441 0.457 0.357 0.655 4 0.396 0.388 0.403 0.433 0.696 5 0.395 0.388 0.403 0.433 0.696 Table 5: Harmonic means of the MUC-7 test scores evaluating the agreement in text anchors and Appraisal classes selected by the annotators, at each level of hierarchical abstraction." ></td>
	<td class="line x" title="102:159	Having considered the annotators agreement with respect to text anchors, we go on to analyse the agreement exhibited by the annotators with respect to the types of Appraisal assigned to the text anchors." ></td>
	<td class="line x" title="103:159	The Appraisal framework is a hierarchical systema tree with leaves corresponding to the annotation types chosen by the judges." ></td>
	<td class="line x" title="104:159	When investigating agreement in Appraisal type, the following measures include not just the leaf nodes but also their parent types, collapsing the nodes into increasingly abstract representations." ></td>
	<td class="line x" title="105:159	For example happiness is a kind of affect, which is a kind of attitude, which is a kind of appraisal." ></td>
	<td class="line x" title="106:159	These relationships are depicted in full in Figure 2." ></td>
	<td class="line x" title="107:159	Note that in the following measurements of inter-annotator agreement leaf nodes are included in subsequent levels (for example, focus is a leaf node at level 2, but is also considered to be a member of levels 3, 4 and 5)." ></td>
	<td class="line x" title="108:159	Table 5 shows the harmonic means of the MUC7 measures of the annotators agreement at each of the levels depicted in Figure 2." ></td>
	<td class="line x" title="109:159	As one might expect, the agreement steadily drops as the classes become more concreteclasses become more specific and more numerous so the complexity of the task increases." ></td>
	<td class="line x" title="110:159	Table 5 also lists the average rate of substitutions as the annotation tasks complexity increases, showing that the annotators were able to fairly easily distinguish between instances of the three subsystems of Appraisal (Attitude, Engagement and Graduation) as the substitution rate at level 1 is low (only 9%)." ></td>
	<td class="line x" title="111:159	Asthenumberofpossibleclassesincreasesannotators are more likely to confuse appraisal types, with disagreement occurring on approximately 44% of annotations at level 5." ></td>
	<td class="line x" title="112:159	The second round of annotations resulted in slightly improved agreement at 97 Level 0:.698 Level 1: .635 Level 2: .528 Level 3: .448 Level 4: .396 Level 5: .395 appraisal attitude: .701 engagement: .507 graduation: .479 affect: .519 judgement: .586 appreciation: .567 contract: .502 expand: .445 force: .420 focus: .287 inclination: .249 happiness: .448 security: .335 satisfaction: .374 esteem: .489 sanction: .575 reaction: .510 composition: .432 valuation: .299 disclaim: .555 proclaim: .336 entertain: .459 attribute: .427 quantification: .233 intensification: .513 normality: .289 capacity: .431 tenacity: .395 veracity: .519 propriety: .540 impact: .462 quality: .336 balance: .300 complexity: .314 deny: .451 counter: .603 pronounce: .195 endorse: .331 concur: .297 acknowledge: .390 distance: .415 number: .191 mass: .104 extent: .242 degree: .510 vigour: .117 affirm: .325 concede: .000 proximity (space): .000 proximity (time): .000 distribution (space): .110 distribution (time): .352 Figure 2: The Appraisal framework with hierarchical levels highlighted." ></td>
	<td class="line x" title="113:159	Appraisal classes and levels are accompanied by the harmonic mean of the F-scores of the annotators for that class/level." ></td>
	<td class="line x" title="114:159	eachlevelofabstraction(themeanF-scoreincreased by 0.051 at the most abstract level)." ></td>
	<td class="line x" title="115:159	Of course, some Appraisal classes are easier to identify than others." ></td>
	<td class="line x" title="116:159	Figure 2 summarises the agreement for each node in the Appraisal hierarchy with the harmonic mean of the F-scores of the annotators for each class." ></td>
	<td class="line x" title="117:159	Typically, the attitude annotations are easiest to identify, whereas the other subsystems of engagementandgraduationtendtobemoredifficult." ></td>
	<td class="line x" title="118:159	The Proximity children of Extent exhibited no agreement whatsoever." ></td>
	<td class="line x" title="119:159	This seems to have arisen from the differences in the judges interpretations of proximity." ></td>
	<td class="line x" title="120:159	In the case of Proximity (Space), for example, one judge annotated words that function to modify the spatial distance of other concepts (e.g. near), whereastheotherselectedwordsplacingconcepts at a specific location (e.g. homegrown, local)." ></td>
	<td class="line x" title="121:159	This confusion between modifying words and spe98 cific locations also accounts for the low agreement in the Distribution (Space) type." ></td>
	<td class="line x" title="122:159	The measures show that it is also difficult to achieve a consensus on what qualifies as engagements of the Pronounce type." ></td>
	<td class="line x" title="123:159	Both annotators select expressions that assert the irrefutability of a proposition (e.g. certainly or in fact or it has to be said)." ></td>
	<td class="line x" title="124:159	Judge d, however, tends to perceive pronouncement as occurring wherever the author makes an assertion (e.g. this is or there will be)." ></td>
	<td class="line x" title="125:159	Judge j seems to require that the assertion carry a degree of emphasis to include a term in the Pronounce class." ></td>
	<td class="line x" title="126:159	The low agreement of the Mass graduations can also be explained in this way, as both d and j select strong expressions relating to size (e.g. massive or scant)." ></td>
	<td class="line x" title="127:159	Annotator j found additional but weaker terms like largely or slightly." ></td>
	<td class="line x" title="128:159	The Pronounce and Mass classes provide typical examples of the disagreement exhibited by the annotators." ></td>
	<td class="line x" title="129:159	It is not that the judges have wildly differentunderstandingsofthesystem, butrathertheydisagree in the bounds of a classone annotator may require a greater degree of strength of a term to warrant its inclusion in a class." ></td>
	<td class="line x" title="130:159	Contingency tables (not depicted due to space constraints) reveal some interesting tendencies for confusion between the two annotators." ></td>
	<td class="line x" title="131:159	Approximately 33% of ds annotations of Proximity (Space) were ascribed as Capacity by j. The high percentage is due to the rarity of annotations of Proximity (Space), but the confusion comes from differing units of Appraisal, as shown in Example 6." ></td>
	<td class="line x" title="132:159	(6) [d] But at key points in this story, one gets the feeling that the essential factors are operating just outsidePROXIMITY (SPACE) Jamess field of visionCAPACITY." ></td>
	<td class="line x" title="133:159	[j] But at key points in this story, one gets the feeling that the essential factors are operating just outside Jamess field of visionCAPACITY." ></td>
	<td class="line x" title="134:159	Another interesting case of frequent confusion is the pair of Satisfaction and Propriety." ></td>
	<td class="line x" title="135:159	Though not closely related in the Attitude subsystem, j chooses Propriety for 21% of ds annotations of Satisfaction." ></td>
	<td class="line x" title="136:159	The confusion is typified by Example 7, where it is apparent that there is disagreement in terms of who is being appraised." ></td>
	<td class="line x" title="137:159	(7) [d] Like him, Vermeer  or so he chose to believe  was an artist neglectedSATISFACTION and wrongedSATISFACTION by critics and who had died an almost unknown." ></td>
	<td class="line x" title="138:159	[j] Like him, Vermeer  or so he chose to believe wasanartistneglected and wrongedPROPRIETY by critics and who had died an almost unknown." ></td>
	<td class="line x" title="139:159	Annotator d believes that the author is communicating the artists dissatisfaction with the way he is treated by critics, whereas j believes that the critics are being reproached for their treatment of the artist." ></td>
	<td class="line x" title="140:159	This highlights a problem with the coding scheme, which simplifies the task by assuming only one type of Appraisal is conveyed by each unit." ></td>
	<td class="line x" title="141:159	5 Related work Taboada and Grieve (2004) initiated computational experimentation with the Appraisal framework, assigning adjectives into one of the three broad attitudeclasses." ></td>
	<td class="line oc" title="142:159	TheauthorsapplySO-PMI-IR(Turney, 2002) to extract and determine the polarity of adjectives." ></td>
	<td class="line o" title="143:159	They then use a variant of SO-PMI-IR to determine a potential value for affect, judgement and appreciation, calculating the mutual information between the adjective and three pronoun-copular pairs: I was (affect); he was (judgement) and it was (appreciation)." ></td>
	<td class="line x" title="144:159	While the pairs seem compelling markers of the respective attitude types, they incorrectly assume that appraisals of affect are limited to the first person whilst judgements are made only of the third person." ></td>
	<td class="line x" title="145:159	We can expect a high degree of overlap between the sets of documents retrieved by queries formed using these pairs (e.g. I was a happy X; he was a happy X; It was a happy X)." ></td>
	<td class="line x" title="146:159	Whitelaw et al.(2005) use the Appraisal framework to specify frames of sentiment." ></td>
	<td class="line x" title="148:159	These Appraisal Groups are derived from aspects of Attitude and Graduation: Attitude: affect | judgement | appreciation Orientation positive | negative Force: low | neutral | high Focus: low | neutral | high Polarity: marked | unmarked Their process begins with a semi-automatically constructed lexicon of these Appraisal groups, built using example terms from Martin and White (2005) as seeds into WordNet synsets." ></td>
	<td class="line x" title="149:159	The frames supplement bag of words-based machine learning techniques for 99 sentiment analysis and they achieve minor improvements over unigram features." ></td>
	<td class="line x" title="150:159	6 Summary This paper has discussed the methodology of an exercise annotating book reviews according to the Appraisal framework, a functional linguistic theory of evaluation in English." ></td>
	<td class="line x" title="151:159	The agreement exhibited by two human judges was measured by analogy with theevaluationemployedfortheMUC-7sharedtasks (Chinchor, 1998)." ></td>
	<td class="line x" title="152:159	The agreement varied greatly depending on the level of abstraction in the Appraisal hierarchy (a mean F-score of 0.698 at the most abstract level through to 0.395 at the most concrete level)." ></td>
	<td class="line x" title="153:159	The agreement also depended on the type being annotatedthere was more agreement evident for types of attitude compared to types of engagement or graduation." ></td>
	<td class="line x" title="154:159	The exercise is the first step in an ongoing study of approaches for the automatic analysis of expressions of Appraisal." ></td>
	<td class="line x" title="155:159	The primary output of this work is a corpus of book reviews independently annotated with Appraisal types by two coders." ></td>
	<td class="line x" title="156:159	Agreement was in general low, but if one assumes that the intersection of both sets of annotations contains reliable examples, this leaves 2,223 usable annotations." ></td>
	<td class="line x" title="157:159	Future work will employ these annotations to evaluate algorithms for the analysis of Appraisal, and investigate the usefulness of the Appraisal framework when in the computational analysis of document sentiment and subjectivity." ></td>
	<td class="line x" title="158:159	Acknowledgments We would like to thank Bill Keller for advice when designing the annotation methodology." ></td>
	<td class="line x" title="159:159	The work of the first author is supported by a UK EPSRC studentship." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2064
SICS: Valence annotation based on seeds in word space
Sahlgren, Magnus;Karlgren, Jussi;Eriksson, Gunnar;"></td>
	<td class="line x" title="1:72	Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 296299, Prague, June 2007." ></td>
	<td class="line x" title="2:72	c2007 Association for Computational Linguistics SICS: Valence annotation based on seeds in word space Magnus Sahlgren SICS Box 1263 SE-164 29 Kista Sweden mange@sics.se Jussi Karlgren SICS Box 1263 SE-164 29 Kista Sweden jussi@sics.se Gunnar Eriksson SICS Box 1263 SE-164 29 Kista Sweden guer@sics.se Abstract This paper reports on a experiment to identify the emotional loading (the valence) of news headlines." ></td>
	<td class="line x" title="3:72	The experiment reported is based on a resource-thrifty approach for valence annotation based on a word-space model and a set of seed words." ></td>
	<td class="line x" title="4:72	The model was trained on newsprint, and valence was computed using proximity to one of two manually defined points in a highdimensional word space  one representing positive valence, the other representing negative valence." ></td>
	<td class="line x" title="5:72	By projecting each headline into this space, choosing as valence the similarity score to the point that was closer to the headline, the experiment provided results with high recall of negative or positive headlines." ></td>
	<td class="line x" title="6:72	These results show that working without a high-coverage lexicon is a viable approach to content analysis of textual data." ></td>
	<td class="line x" title="7:72	1 The Semeval task This a report of an experiment proposed as the Affective Text task of the 4th international Workshop on Semantic Evaluation (SemEval) to determine whether news headlines are loaded with preeminently positive or negative emotion or valence." ></td>
	<td class="line x" title="8:72	An example of a test headline can be: DISCOVERED BOYS BRING SHOCK, JOY 2 Working without a lexicon Our approach takes as its starting point the observation that lexical resources always are noisy, out of date, and most often suffer simultaneously from being both too specific and too general." ></td>
	<td class="line x" title="9:72	For our experiments, our only lexical resource consists of a list of eight positive words and eight negative words, as shown below in Table 1." ></td>
	<td class="line x" title="10:72	We use a medium-sized corpus of general newsprint to build a general word space, and use our minimal lexical resource to orient ourselves in it." ></td>
	<td class="line x" title="11:72	3 Word space A word space is a high-dimensional vector space built from distributional statistics (Schutze, 1993; Sahlgren, 2006), in which each word in the vocabulary is represented as a context vector a0a1 of occurrence frequencies: a0a1a3a2a5a4a7a6a9a8a11a10a13a12a11a14a11a14a11a14a15a12a16a8a18a17a20a19 where a8 is the frequency of word a21 in some context a22." ></td>
	<td class="line x" title="12:72	The point of this representation is that semantic similarity between words can be computed using vector similarity measures." ></td>
	<td class="line x" title="13:72	Thus, the similarity in meaning between the words a23a25a24 and a23a27a26 can be quantified by computing the similarity between their respective context vectors: sima28a29a23a30a24 a12 a23a27a26a32a31a34a33 sima28 a0 a1 a24 a12 a0 a1 a26a32a31 . The semantics of such a word space are determined by the data from which the occurrence information has been collected." ></td>
	<td class="line x" title="14:72	Since the data set in the SemEval Affective Text task consists of news headlines, a relevant word space should be produced from topically and stylistically similar texts, such as newswire documents." ></td>
	<td class="line x" title="15:72	For this reason, we trained our model on a corpus of English-language newsprint which is available for experimentation for participants in the Cross Language Evaluation Fo296 rum (CLEF).1 The corpus consists of some 100 000 newswire documents from Los Angeles Times for the year 1994." ></td>
	<td class="line x" title="16:72	We presume any similarly sized collection of newsprint would produce similar results." ></td>
	<td class="line x" title="17:72	We lemmatized the data using tools from Connexor,2 and removed stop words, leaving some 28 million words with a vocabulary of approximately 300 000 words." ></td>
	<td class="line x" title="18:72	Since the data for the affective task only consisted of news headlines, we treated each headline in the LA times corpus as a separate document, thus doubling the number of documents in the data." ></td>
	<td class="line x" title="19:72	For harvesting occurrence information, we used documents as contexts and standard tfidf-weighting of frequencies, resulting in a 220 220-dimensional word space." ></td>
	<td class="line x" title="20:72	No dimensionality reduction was used." ></td>
	<td class="line x" title="21:72	4 Seeds In order to construct valence vectors, we used a set of manually selected seed words (8 positive and 8 negative words), shown in Table 1." ></td>
	<td class="line x" title="22:72	These words were chosen (subjectively) to represent typical expression of positive or negative attitude in news texts." ></td>
	<td class="line x" title="23:72	The size of the seed set was determined by a number of initial experiments on the development data, where we varied the size of the seed sets from these 8 words to some 700 words in each set (using the WordNet Affect hierarchy (Strapparava and Valitutti, 2004))." ></td>
	<td class="line oc" title="24:72	As comparison, Turney and Littman (2003) used seed sets consisting of 7 words in their word valence annotation experiments, while Turney (2002) used minimal seed sets consisting of only one positive and one negative word (excellent and poor) in his experiments on review classification." ></td>
	<td class="line n" title="25:72	Such minimal seed sets of antonym pairs are not possible to use in the present experiment because they are often nearest neighbors to each other in the word space." ></td>
	<td class="line n" title="26:72	Also, it is difficult to find such clear paradigm words for the newswire domain." ></td>
	<td class="line x" title="27:72	The seed words were used to postulate one positive and one negative point (i.e. vector) in the word space by simply taking the centroid of the seed word points: a0a1a1a0 a4a3a2 a0a1a5a4a7a6a8a0 where a9 is one of the seed sets, and a23 is a word in this set." ></td>
	<td class="line x" title="28:72	1http://www.clef-campaign.org/ 2http://www.conexor.fi/ Positive Negative positive negative good bad win defeat success disaster peace war happy sad healthy sick safe dangerous Table 1: The seed words used to create valence vectors." ></td>
	<td class="line x" title="29:72	5 Syntagmatic vs paradigmatic relations Our hypothesis is that words carrying most of the valence in news headlines in the experimental test set are syntagmatically rather than paradigmatically related to the kind of very general words used in our seed set.3 As an example, consider test headline 501: TWO HUSSEIN ALLIES ARE HANGED, IRAQI OFFICIAL SAYS." ></td>
	<td class="line x" title="30:72	It seems reasonable to believe that this headline should be annotated with a negative valence, and that the desicive word in this case is hanged. Obviously, hanged has no paradigmatic neighbors (e.g. synonyms, antonyms or other nyms) among the seed words." ></td>
	<td class="line x" title="31:72	However, it is likely that hanged will co-occur with (and therefore have a syntagmatic relation to) general negative terms such as dangerous and maybe war. In fact, in this example headline, the most negatively associated words are probably Hussein and Iraqi, which often co-occur with general negative terms such as war and dangerous in newswire text." ></td>
	<td class="line x" title="32:72	To produce a word space that contains predominantely syntagmatic relations, we built the distributional relations using entire documents as contexts (i.e. each dimension in the word space corresponds to a document in the data)." ></td>
	<td class="line x" title="33:72	If we would have used words as contexts instead, we would have ended up with a paradigmatic word space.4 3Syntagmatic relations hold between co-occurring words, while paradigmatic relations hold between words that do not co-occur, but that occur with the same other words." ></td>
	<td class="line x" title="34:72	4See Sahlgren (2006) for an explanation of how the choice of contexts determines the semantic content of the word space." ></td>
	<td class="line x" title="35:72	297 6 Compositionality and semantic relations The relations between words in headlines were modeled using the most simple operation conceivable: we simply add all words context vectors to a compound headline vector and use that as the representation of the headline: a0a1a1a0 a4 a2 a0a1a8a4 a6a2a0 where a3 is a test headline, and a23 is a word in this headline." ></td>
	<td class="line x" title="36:72	This is obviously a daring, if not foolhardy, approach to modelling syntactic structure, compositional semantics, and all types of intra-sentential semantic dependencies." ></td>
	<td class="line x" title="37:72	It can fairly be expected to be improved upon through an appropriate finer-grained analysis of word presence, adjacency and syntactic relationships." ></td>
	<td class="line x" title="38:72	However, this approach is similar to that taken by most search engines in use today, is a useful first baseline, and as can be seen from our results below, does deliver acceptable results." ></td>
	<td class="line x" title="39:72	7 Valence annotation To perform the valence annotation, we first lemmatized the headlines and removed stop words and words with frequency above 10 000 in the LA times corpus." ></td>
	<td class="line x" title="40:72	For each headline, we then summed  as discussed above  the context vectors of the remaining words, thus producing a 220 220dimensional vector for each headline." ></td>
	<td class="line x" title="41:72	This vector was then compared to each of the postulated valence vectors by computing the cosine of the angles between the vectors." ></td>
	<td class="line x" title="42:72	We thus have for each headline two cosines, one between the headline and the positive vector and one between the headline and the negative vector." ></td>
	<td class="line x" title="43:72	The valence vector with highest cosine score (and thus the smallest spatial angle) was chosen to annotate the headline." ></td>
	<td class="line x" title="44:72	For the negative valence vector we assigned a negative valence value, and for the positive vector a positive value." ></td>
	<td class="line x" title="45:72	In 11 cases, a value of a4a6a5a8a7a9a5 was ascribed, either because all headline words were removed by frequency and stop word filtering, or because none of the remaining words occurred in our newsprint corpus." ></td>
	<td class="line x" title="46:72	Our method thus only delivers a binary valence decision  either positive or negative valence." ></td>
	<td class="line x" title="47:72	Granted, we could have assigned a neutral valence to very low cosine scores, but as any threshold for deciding on a neutral score would be completely arbitrary, we decided to only give fully positive or negative scores to the test headlines." ></td>
	<td class="line x" title="48:72	Also, since our aim was to provide a high-recall result, we did not wish to leave any headline with an equivocal score." ></td>
	<td class="line x" title="49:72	We scaled the scores to fit the requirements of the coarse-grained evaluation: for each headline with a non-zero score, we multiplied the value with a10a11a5a2a5 and boosted each value with a12a13a5 .5 By this scaling operation we guaranteed a positive or a negative score for each headline (apart from the 11 exceptions, in effect unanalyzed by our algorithm, as mentioned above)." ></td>
	<td class="line x" title="50:72	8 Results The results from the fine-grained and coarse-grained evaluations are shown in Table 2." ></td>
	<td class="line x" title="51:72	They show, much as we anticipated, that the coarse-grained evaluation was appropriate for our purposes." ></td>
	<td class="line x" title="52:72	Fine-grained Coarse-grained Accuracy Precision Recall 20.68 29.00 28.41 60.17 Table 2: The results of the valence annotation." ></td>
	<td class="line x" title="53:72	8.1 Correlation coefficients, normality assumptions, and validity of results The fine-grained evaluation as given by the organisers and as shown in Table 2 was computed using Pearsons product-moment coefficient." ></td>
	<td class="line x" title="54:72	Pearsons correlation coefficient is a parametric statistic and assumes normal distribution of the data it is testing for correlation." ></td>
	<td class="line x" title="55:72	While we have no idea of neither the other contributions score distribution, nor that of the given test set, we certainly do know that our data are not normally distributed." ></td>
	<td class="line x" title="56:72	We would much prefer to evaluate our results using a non-parametric correlation test, such as Spearmans a14, and suggest that the all results would be rescored using some non-parametric method instead  this would reduce the risk of inadvertent false positives stemming from divergence from the normal distribution rather than divergence from the test set." ></td>
	<td class="line x" title="57:72	5The coarse-grained evaluation collapsed values in the ranges a15a17a16a19a18a21a20a22a20a24a23a21a16a26a25a27a20a27a28 as negative, a15a17a16a26a25a22a20a29a23a30a25a22a20a27a28 as neutral, and a15a25a22a20a29a23a21a18a21a20a31a20a32a28 as positive." ></td>
	<td class="line x" title="58:72	298 8.2 Use cases Evaluation of abstract features such as emotional valence can be done within a system oriented framework such as the one used in this experiment." ></td>
	<td class="line x" title="59:72	Alternatively, one could evaluate the results using a parametrized use case scenario." ></td>
	<td class="line x" title="60:72	A simple example might be to aim for either high recall or high precision, rather than using an average which folds in both scenarios into one numeric score  easy to compare between systems but dubious in its relevance to any imaginable real life task." ></td>
	<td class="line x" title="61:72	There are metrics, as formal as the simple recallprecision-framework in traditional adhoc retrieval, that could be adapted for this purpose (Jarvelin and Kekalainen, 2002, e.g.)." ></td>
	<td class="line x" title="62:72	9 Related research Our approach to valence annotation is similar to the second method described by Turney and Littman (2003)." ></td>
	<td class="line x" title="63:72	In short, their method uses singular value decomposition to produce a reduceddimensional word space, in which word valence is computed by subtracting the cosine between the word and a set of negative seed words from the cosine between the word and a set of positive seed words." ></td>
	<td class="line x" title="64:72	The difference between our approach and theirs is that our approach does not require any computationally expensive matrix decomposition, as we do not see any reason to restructure our word space." ></td>
	<td class="line x" title="65:72	Turney and Littman (2003) hypothesize that singular value decomposition is beneficial for the results in valency annotation because it infers paradigmatic relations between words in the reduced space." ></td>
	<td class="line x" title="66:72	However, as we argued in Section 5, we believe that the headline valency annotation task calls for syntagmatic rather than paradigmatic relations." ></td>
	<td class="line x" title="67:72	Furthermore, we fail to see the motivation for using singular value decomposition, since if paradigmatic relations are what is needed, then why not simply use words as dimensions of the word space?" ></td>
	<td class="line x" title="68:72	10 Concluding remarks Our results show that a resource-poor but data-rich method can deliver sensible results." ></td>
	<td class="line x" title="69:72	This is in keeping with our overall approach, which aims for as little pre-computed resources as possible." ></td>
	<td class="line x" title="70:72	At almost every juncture in our processing we made risky and simplistic assumptions  using simple frequencies of word occurrence as a semantic model; using a small seed set of positive and negative terms as a target; postulating one semantic locus each for positive and negative emotion; modelling syntactic and semantic relations between terms by vector addition  and yet we find that the semantic structure of distributional statistics yields a signal good enough for distinguishing positive from negative headlines with a non-random accuracy." ></td>
	<td class="line x" title="71:72	Despite its simplicity, out method produces very good recall (60.17) in the coarse-grained evaluation (the median recall for all systems is 29.59)." ></td>
	<td class="line x" title="72:72	This speaks to the power of distributional semantics and gives promise of improvement if some of the choice points during the process are returned to: some decisions can well benefit from being made on principled and informed grounds rather than searching under the street lamp, as it were." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-2072
UA-ZBSA: A Headline Emotion Classification through Web Information
Kozareva, Zornitsa;Navarro, Borja;Vazquez, Sonia;Montoyo, AndrMontoyo;s, null;"></td>
	<td class="line x" title="1:86	Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 334??37, Prague, June 2007." ></td>
	<td class="line x" title="2:86	c2007 Association for Computational Linguistics UA-ZBSA: A Headline Emotion Classification through Web Information Zornitsa Kozareva, Borja Navarro, Sonia Vazquez, Andres Montoyo DLSI, University of Alicante Carretera de San Vicente S/N Alicante, Spain 03080 zkozareva,borja,svazquez,montoyo@dlsi.ua.es Abstract This paper presents a headline emotion classification approach based on frequency and co-occurrence information collected from the World Wide Web." ></td>
	<td class="line x" title="3:86	The content words of a headline (nouns, verbs, adverbs and adjectives) are extracted in order to form different bag of word pairs with the joy, disgust, fear, anger, sadness and surprise emotions." ></td>
	<td class="line x" title="4:86	For each pair, we compute the Mutual Information Score which is obtained from the web occurrences of an emotion and the content words." ></td>
	<td class="line x" title="5:86	Our approach is based on the hypothesis that group of words which co-occur together across many documents with a given emotion are highly probable to express the same emotion." ></td>
	<td class="line x" title="6:86	1 Introduction The subjective analysis of a text is becoming important for many Natural Language Processing (NLP) applications such as Question Answering, Information Extraction, Text Categorization among others (Shanahan et al. , 2006)." ></td>
	<td class="line x" title="7:86	The resolution of this problem can lead to a complete, realistic and coherent analysis of the natural language, therefore major attention is drawn to the opinion, sentiment and emotion analysis, and to the identification of beliefs, thoughts, feelings and judgments (Quirk et al. , 1985), (Wilson and Wiebe, 2005)." ></td>
	<td class="line x" title="8:86	The aim of the Affective Text task is to classify a set of news headlines into six types of emotions: ?anger??" ></td>
	<td class="line x" title="9:86	?disgust??" ></td>
	<td class="line x" title="10:86	?fear??" ></td>
	<td class="line x" title="11:86	?joy??" ></td>
	<td class="line x" title="12:86	?sadness??" ></td>
	<td class="line x" title="13:86	and ?surprise??" ></td>
	<td class="line x" title="14:86	In order to be able to conduct such multi-category analysis, we believe that first we need a comprehensive theory of what a human emotion is, and then we need to understand how the emotion is expressed and transmitted within the natural language." ></td>
	<td class="line x" title="15:86	These aspects rise the need of syntactic, semantic, textual and pragmatic analysis of a text (Polanyi and Zaenen, 2006)." ></td>
	<td class="line x" title="16:86	However, some of the major drawbacks in this field are related to the manual or automatic acquisition of subjective expressions, as well as to the lack of resources in terms of coverage." ></td>
	<td class="line x" title="17:86	For this reason, our current emotion classification approach is based on frequency and co-occurrence bag of word counts collected from the World Wide Web." ></td>
	<td class="line x" title="18:86	Our hypothesis is that words which tend to cooccur across many documents with a given emotion are highly probable to express this emotion." ></td>
	<td class="line x" title="19:86	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="20:86	In Section 2 we review some of the related work, in Section 3 we describe our web-based emotion classification approach for which we show a walk-through example in Section 4." ></td>
	<td class="line x" title="21:86	A discussion of the obtained results can be found in Section 5 and finally we conclude in Section 6." ></td>
	<td class="line oc" title="22:86	2 Related work Our approach for emotion classification is based on the idea of (Hatzivassiloglou and McKeown, 1997) and is similar to those of (Turney, 2002) and (Turney and Littman, 2003)." ></td>
	<td class="line x" title="23:86	According to Hatzivassiloglou and McKeown (1997), adjectives with the same polarity tended to appear together." ></td>
	<td class="line x" title="24:86	For example the negative adjectives ?corrupt and brutal??co334 occur very often." ></td>
	<td class="line oc" title="25:86	The idea of tracing polarity through adjective cooccurrence is adopted by Turney (2002) for the binary (positive and negative) classification of text reviews." ></td>
	<td class="line o" title="26:86	They take two adjectives, for instance ?excellent??and ?poor??in a way that the first adjective expresses positive meaning, meanwhile the second one expresses negative." ></td>
	<td class="line o" title="27:86	Then, they extract all adjectives from the review text and combine them with ?excellent??and ?poor??" ></td>
	<td class="line x" title="28:86	The co-occurrences of these words are searched on the web, and then the Mutual Information score for the two groups of adjectives is measured." ></td>
	<td class="line x" title="29:86	When the adjective of the review appear more often with ?excellent??" ></td>
	<td class="line x" title="30:86	then the review is classified as positive, and when the adjectives appear more often with ?poor??" ></td>
	<td class="line x" title="31:86	then the review is classified as negative." ></td>
	<td class="line oc" title="32:86	Following Hatzivassiloglou and McKeown (1997) and Turney (2002), we decided to observe how often the words from the headline co-occur with each one of the six emotions." ></td>
	<td class="line o" title="33:86	This study helped us deduce information according to which ?birthday??appears more often with ?joy??" ></td>
	<td class="line x" title="34:86	while ?war??appears more often with ?fear??" ></td>
	<td class="line oc" title="35:86	Some of the differences between our approach and those of Turney (2002) are mentioned below: ??objectives: Turney (2002) aims at binary text classification, while our objective is six class classification of one-liner headlines." ></td>
	<td class="line o" title="36:86	Moreover, we have to provide a score between 0 and 100 indicating the presence of an emotion, and not simply to identify what the emotion in the text is. Apart from the difficulty introduced by the multi-category classification, we have to deal with a small number of content words while Turney works with large list of adjectives." ></td>
	<td class="line oc" title="37:86	??word class: Turney (2002) measures polarity using only adjectives, however in our approach we consider the noun, the verb, the adverb and the adjective content words." ></td>
	<td class="line x" title="38:86	The motivation of our study comes from (Polanyi and Zaenen, 2006), according to which each content word can express sentiment and emotion." ></td>
	<td class="line x" title="39:86	In addition to this issue we saw that most of the headlines contain only nouns and verbs, because they express objectivity." ></td>
	<td class="line oc" title="40:86	??search engines: Turney (2002) uses the Altavista web browser, while we consider and combine the frequency information acquired from three web search engines." ></td>
	<td class="line oc" title="41:86	??word proximity: For the web searches, Turney (2002) uses the NEAR operator and considers only those documents that contain the adjectives within a specific proximity." ></td>
	<td class="line x" title="42:86	In our approach, as far as the majority of the query words appear in the documents, the frequency count is considered." ></td>
	<td class="line oc" title="43:86	??queries: The queries of Turney (2002) are made up of a pair of adjectives, and in our approach the query contains the content words of the headline and an emotion." ></td>
	<td class="line x" title="44:86	There are other emotion classification approaches that use the web as a source of information." ></td>
	<td class="line x" title="45:86	For instance, (Taboada et al. , 2006) extracted from the web co-occurrences of adverbs, adjectives, nouns and verbs." ></td>
	<td class="line x" title="46:86	Gamon and Aue (2005) were looking for adjectives that did not co-occur at sentence level." ></td>
	<td class="line x" title="47:86	(Baroni and Vegnaduzzo, 2004) and (Grefenstette et al. , 2004) gathered subjective adjectives from the web calculating the Mutual Information score." ></td>
	<td class="line x" title="48:86	Other important works on sentiment analysis are those of (Wilson et al. , 2005) and (Wiebe et al. , 2005; Wilson and Wiebe, 2005), who used linguistic information such as syntax and negations to determine polarity." ></td>
	<td class="line x" title="49:86	Kim and Hovy (2006) integrated verb information from FrameNet and incorporated it into semantic role labeling." ></td>
	<td class="line x" title="50:86	3 Web co-occurrences In order to determine the emotions of a headline, we measure the Pointwise Mutual Information (MI) of ei and cwj as MI(ei,cwj) = log2 hits(ei,cwj)hits(ei)hits(cwj), where ei ??" ></td>
	<td class="line x" title="51:86	{anger,disgust,fear,joy,sadness,surprise} and cwj are the content words of the headline j. For each headline, we have six MI scores which indicate the presence of the emotion." ></td>
	<td class="line x" title="52:86	MI is used in our experiments because it provides information about the independence of an emotion and a bag of words." ></td>
	<td class="line x" title="53:86	To collect the frequency and co-occurrence counts of the headline words, we need large and massive 335 data repositories." ></td>
	<td class="line x" title="54:86	To surmount the data sparsity problem, we used as corpus the World Wide Web which is constantly growing and daily updated." ></td>
	<td class="line x" title="55:86	Our statistical information is collected from three web search engines: MyWay1, AlltheWeb2 and Yahoo3." ></td>
	<td class="line x" title="56:86	It is interesting to note that the emotion distribution provided by each one of the search engines for the same headline has different scores." ></td>
	<td class="line x" title="57:86	For this reason, we decided to compute an intermediate MI score as aMI = summationtextn s=1 MI(ei,cwj)s. In the trail data, besides the MI score of an emotion and all headline content words, we have calculated the MI for an emotion and each one of the content words." ></td>
	<td class="line x" title="58:86	This allowed us to determine the most sentiment oriented word in the headline and then we use this predominant emotion to weight the association sentiment score for the whole text." ></td>
	<td class="line x" title="59:86	Unfortunately, we could not provide results for the test data set, due to the high number of emotion-content word pairs and the increment in processing time and returned responses of the search engines." ></td>
	<td class="line x" title="60:86	4 Example for Emotion Classification As a walk through example, we use the Mortar assault leaves at least 18 dead headline which is taken from the trial data." ></td>
	<td class="line x" title="61:86	The first step in our emotion classification approach consists in the determination of the part-of-speech tags for the one-liner." ></td>
	<td class="line x" title="62:86	The noncontent words are stripped away, and the rest of the words are taken for web queries." ></td>
	<td class="line x" title="63:86	To calculate the MI score of a headline, we query the three search engines combining ?mortar, assault, leave, dead??with the anger, joy, disgust, fear, sadness and surprise emotions." ></td>
	<td class="line x" title="64:86	The obtained results are normalized in a range from 0 to 100 and are shown in Table 1." ></td>
	<td class="line x" title="65:86	MyWay AllWeb Yahoo Av." ></td>
	<td class="line x" title="66:86	G.Stand." ></td>
	<td class="line x" title="67:86	anger 19 22 24 22 22 disgust 5 6 7 6 2 fear 44 50 53 49 60 joy 15 19 20 18 0 sadness 28 36 36 33 64 surprise 4 5 6 5 0 Table 1: Performance of the web-based emotion classification for a trail data headline 1www.myway.com 2www.alltheweb.com 3www.yahoo.com As can be seen from the table, the three search engines provide different sentiment distribution for the same headline, therefore in our final experiment we decided to calculate intermediate MI." ></td>
	<td class="line x" title="68:86	Comparing our results to those of the gold standard, we can say that our approach detects significantly well the fear, sadness and angry emotions." ></td>
	<td class="line x" title="69:86	5 Results and Discussion Table 2 shows the obtained results for the affective test data." ></td>
	<td class="line x" title="70:86	The low performance of our approach is explainable by the minimal knowledge we have used." ></td>
	<td class="line x" title="71:86	An interesting conclusion deduced from the trail and test emotion data is that the system detects better the negative feelings such as anger, disgust, fear and sadness, in comparison to the positive emotions such as joy and surprise." ></td>
	<td class="line x" title="72:86	This makes us believe that according to the web most of the word-emotion combinations we queried are related to the expression of negative emotions." ></td>
	<td class="line x" title="73:86	UA-ZBSA Fine-grained Coarse-grained Pearson Acc." ></td>
	<td class="line x" title="74:86	P. R. Anger 23.20 86.40 12.74 21.66 Disgust 16.21 97.30 0.00 0.00 Fear 23.15 75.30 16.23 26.27 Joy 2.35 81.80 40.00 2.22 Sadness 12.28 88.90 25.00 0.91 Surprise 7.75 84.60 13.70 16.56 Table 2: Performance of the web-based emotion classification for the whole test data set In the test run, we could not apply the emotionword weighting, however we believe that it has a significant impact over the final performance." ></td>
	<td class="line x" title="75:86	Presently, we were looking for the distribution of all content words and the emotions, but in the future we would like to transform all words into adjectives and then conduct web queries." ></td>
	<td class="line x" title="76:86	Furthermore, we would like to combine the results from the web emotion classification with the polarity information given by SentiWordNet4." ></td>
	<td class="line x" title="77:86	Apriory we want to disambiguate the headline content words and to determine the polarities of the words and their corresponding senses." ></td>
	<td class="line x" title="78:86	For instance, the adjective ?new??has eleven senses, where new#a#3 and new#a#5 express negativism, new#a#4 and new#a#9 positivism and the rest of the senses are objective." ></td>
	<td class="line x" title="79:86	4http://sentiwordnet.isti.cnr.it/ 336 So far we did not consider the impact of valence shifter (Polanyi and Zaenen, 2006) and we were unable to detect that a negative adverb or adjective transforms the emotion from positive into negative and vice versa." ></td>
	<td class="line x" title="80:86	We are also interested in studying how to conduct queries not as a bag of words but bind by syntactic relations (Wilson et al. , 2005)." ></td>
	<td class="line x" title="81:86	6 Conclusion Emotion classification is a challenging and difficult task in Natural Language Processing." ></td>
	<td class="line x" title="82:86	For our first attempt to detect the amount of angry, fear, sadness, surprise, disgust and joy emotions, we have presented a simple web co-occurrence approach." ></td>
	<td class="line x" title="83:86	We have combined the frequency count information of three search engines and we have measured the Mutual Information score between a bag of content words and emotion." ></td>
	<td class="line x" title="84:86	According to the yielded results, the presented approach can determine whether one sentiment is predominant or not, and most of the correct sentiment assignments correspond to the negative emotions." ></td>
	<td class="line x" title="85:86	However, we need to improve the approach in many aspects and to incorporate more knowledge-rich resources, as well as to tune the 0-100 emotion scale." ></td>
	<td class="line x" title="86:86	Acknowledgements This research has been funded by QALLME number FP6 IST-033860 and TEX-MESS number TIN200615265-C06-01." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1024
Looking for Trouble
De Saeger, Stijn;Torisawa, Kentaro;Kazama, Jun'ichi;"></td>
	<td class="line x" title="1:189	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 185192 Manchester, August 2008 Looking for Trouble Stijn De Saeger Kentaro Torisawa Language Infrastructure Group National Institute of Information and Communications Technology {stijn,torisawa}@nict.go.jp Junichi Kazama School of Information Science Japan Advanced Institute of Science and Technology kazama@jaist.ac.jp Abstract This paper presents a method for mining potential troubles or obstacles related to the use of a given object." ></td>
	<td class="line x" title="2:189	Some example instances of this relation aremedicine, side effect and amusement park, height restriction." ></td>
	<td class="line x" title="3:189	Our acquisition method consists of three steps." ></td>
	<td class="line x" title="4:189	First, we use an unsupervised method to collect training samples from Web documents." ></td>
	<td class="line x" title="5:189	Second, a set of expressions generally referring to troubles is acquired by a supervised learning method." ></td>
	<td class="line x" title="6:189	Finally, the acquired troubles are associated with objects so that each of the resulting pairs consists of an object and a trouble or obstacle in using that object." ></td>
	<td class="line x" title="7:189	To show the effectiveness of our method we conducted experiments using a large collection of Japanese Web documents for acquisition." ></td>
	<td class="line x" title="8:189	Experimental results show an 85.5% precision for the top 10,000 acquired troubles, and a 74% precision for the top 10% of over 60,000 acquired object-trouble pairs." ></td>
	<td class="line x" title="9:189	1 Introduction The Stanford Encyclopedia of Philosophy defines an artifact as an object that has been intentionally made or produced for a certain purpose." ></td>
	<td class="line x" title="10:189	Because of this purpose-orientedness, most human actions relating to an object or artifact fall into two broad categories  actions relating to its intended use (e.g. reading a book), and the preparations necessary therefore (like buying the book)." ></td>
	<td class="line x" title="11:189	Information concerning potential obstacles, harmful effects or troubles that interfere with this intended use is therefore highly relevant to the user." ></td>
	<td class="line x" title="12:189	c2008." ></td>
	<td class="line x" title="13:189	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="14:189	Some rights reserved." ></td>
	<td class="line x" title="15:189	While some such troubles are self-evident, others represent a genuine obstacle whose existence was thusfar unknown to the user." ></td>
	<td class="line x" title="16:189	For example, in early 2008 a food poisoning case caused a big media stir in Japan when dozens of people fell ill after eating Chinese-imported frozen food products containing residual traces of toxic pesticides." ></td>
	<td class="line x" title="17:189	While supposedly the presence of toxic chemicals in imported frozen foods had already been established on several occasions before, until the recent incidents public awareness of these facts remained low." ></td>
	<td class="line x" title="18:189	In retrospect, a publicly available system suggesting residualagrichemicalsasapotentialdangerwith the consumption of frozen foods based on information mined from a large collection of Web documents might have led to earlier detection of this crisis." ></td>
	<td class="line x" title="19:189	From the viewpoint of manufacturers as well, regularly monitoring the Internet for product names and associated troubles may allow them to find out about perceived flaws in their products sooner and avoid large scale recalls and damage to their brand." ></td>
	<td class="line x" title="20:189	For a less dramatic example, searching for Tokyo Disneyland on the Internet typically yields many commercial sites offering travel deals, but little or no information about potential obstacles such as height restrictions (constraints on who can enjoy a given attraction1) and traffic jams (a necessary preparation for enjoying a theme park is actually getting there in time)." ></td>
	<td class="line x" title="21:189	Ofter users have no way of finding out about this until they actually go there." ></td>
	<td class="line x" title="22:189	These examples demonstrate the importance of a highly accurate automatic method for acquiring what we will call object-trouble relations  pairs eo,et in which the thing referred to by et constitutes an (actual or potential) trouble, obstacle or risk in the context of use of an object eo." ></td>
	<td class="line x" title="23:189	1 For example, one has to be over 3 ft. tall to get on the Splash Mountain." ></td>
	<td class="line x" title="24:189	185 Large scale acquisition of this type of contextual knowledge has not been thoroughly studied so far." ></td>
	<td class="line x" title="25:189	In this paper, we propose a method for automatically acquiring Japanese noun phrases referring to troubles, (henceforth referred to as trouble expressions), and associating them with expressions denoting artifacts, objects or facilities." ></td>
	<td class="line x" title="26:189	Our acquisition method consists of three steps." ></td>
	<td class="line x" title="27:189	As a first step, we use an unsupervised method for efficiently collecting training data from a Web corpus." ></td>
	<td class="line x" title="28:189	Then, asetofexpressionsdenotingtroublesis acquired by a supervised learning method  Support Vector Machines (Vapnik, 1998)  trained on this data." ></td>
	<td class="line x" title="29:189	Finally, the acquired trouble expressions are paired with noun phrases referring to objects, using a combination of pairwise mutual information and a verb-noun dependency filter based on statistics in a Web corpus." ></td>
	<td class="line x" title="30:189	A broad focus on noun-verb dependencies  and in particular the distinction between dependency relations with negated versus non-negated verbs  is the main characteristic of our method." ></td>
	<td class="line x" title="31:189	While this distinction did not prove useful for improving the supervised classifiers performance in step 2, it forms the basis underlying the unsupervised method for training sample selection in the first step, and the final filtering mechanism in the third step." ></td>
	<td class="line x" title="32:189	The rest of this paper is organized as follows." ></td>
	<td class="line x" title="33:189	Section 2 points out related work." ></td>
	<td class="line x" title="34:189	Section 3 examines the notion of trouble expressions and their evidences." ></td>
	<td class="line x" title="35:189	Section 4 describes our method, whose experimental results are discussed in Section 5." ></td>
	<td class="line x" title="36:189	2 Related Work Our goal of automatically acquiring object-trouble pairs from Web documents is perhaps best viewed as a problem of semantic relation extraction." ></td>
	<td class="line x" title="37:189	Recently the Automatic Content Extraction (ACE) program (Doddington et al., 2004) is a wellknown benchmark task concerned with the automatic recognition of semantic relations from unstructured text." ></td>
	<td class="line x" title="38:189	Typical target relations include Reaction and Production (Pantel and Pennacchiootti, 2006), person-affiliation and organization-location (Zelenko et al., 2002), part-whole (Berland and Charniak, 1999; Girju et al., 2006) and temporal precedence relations between events (Chklovski and Pantel, 2004; Torisawa, 2006)." ></td>
	<td class="line x" title="39:189	Ourcurrenttaskofacquiringobjecttrouble relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation  they crucially depend on additional knowledge about whether and how a given objects use might be hampered by a specific trouble." ></td>
	<td class="line oc" title="40:189	Another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (Turney, 2002; Takamura et al., 2006; Kaji and Kitsuregawa, 2006)." ></td>
	<td class="line x" title="41:189	Clearly troubles should be associated with a negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation." ></td>
	<td class="line x" title="42:189	While recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (Kobayashi et al., 2007), these attributes usually concern intrinsic properties of the objects, such as a digital cameras colors  they do not extend to sentiment-related factors external to the object like traffic jams for theme parks." ></td>
	<td class="line x" title="43:189	The acquisition method proposed in this work addresses both these matters." ></td>
	<td class="line x" title="44:189	Finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word trouble." ></td>
	<td class="line x" title="45:189	Although we used the classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992; Imasumi, 2001; Ando et al., 2003) to reflect this intuition, our experiments show we were unable to attain satisfactory performance using lexico-syntactic patterns alone." ></td>
	<td class="line x" title="46:189	Thus, we also use verb-noun dependencies as evidence in learning (Pantel and Ravichandran, 2004; Shinzato and Torisawa, 2004)." ></td>
	<td class="line x" title="47:189	We treat the evidences uniformly as elements in a feature vector given to a supervised learning method, which allowed us to extract a considerably larger number of trouble expressions than could be acquired by sparse lexicosyntactic patterns alone, while still keeping decent precision." ></td>
	<td class="line x" title="48:189	What kind of hyponymy relations can be acquired by noun-verb dependencies is still an open question in NLP." ></td>
	<td class="line x" title="49:189	In this work we show that at least trouble expressions can successfully be acquired based on noun-verb dependency information alone." ></td>
	<td class="line x" title="50:189	3 Trouble Expressions and Features for Their Acquisition In section 1 we have characterized trouble expressions as a kind of trouble that occurs in the specific context of using some object, in other words: 186 1." ></td>
	<td class="line x" title="51:189	hyponym ni nita hypernym (hyponym similar to hypernym) 2." ></td>
	<td class="line x" title="52:189	hyponym to yobareru hypernym (hypernym called hyponym) 3." ></td>
	<td class="line x" title="53:189	hyponym igai no hypernym (hypernym other than hyponym) 4." ></td>
	<td class="line x" title="54:189	hyponym no youna hypernym (hypernym like hyponym) 5." ></td>
	<td class="line x" title="55:189	hyponym to iu hypernym (hypernym called hyponym) 6." ></td>
	<td class="line x" title="56:189	hyponym nado(no|,) hypernym (hypernym such as hyponym) Table 1: Japanese lexico-syntactic patterns for hyponymy relations ashyponymsoftrouble." ></td>
	<td class="line x" title="57:189	Henceonesourceofevidence for acquisition are hyponymy relations with trouble or its synonyms." ></td>
	<td class="line x" title="58:189	Another characterization of trouble expressions is to think of them as obstacles in a broad sense: things that prevent certain actions from being undertaken properly." ></td>
	<td class="line x" title="59:189	In this sense traffic jams and sickness are troubles since they prevent people from going places and doing things." ></td>
	<td class="line x" title="60:189	This assumption underlies a second important class of evidences for learning." ></td>
	<td class="line x" title="61:189	More precisely, the evidence used for learning is classifiedintothreecategories: (i)lexico-syntactic patterns for hyponymy relations, (ii) dependency relations between expressions and negated verbs, and (iii) dependency relations between expressions and non-negated verbs." ></td>
	<td class="line x" title="62:189	The first two categories are assumed to contain positive evidence of trouble expressions, while we assumed the third to function mostly as negative evidence." ></td>
	<td class="line x" title="63:189	Our experiments show that (i) turns out to be less useful than expected, while the combination of (ii) and (iii) alone already gave quite reasonable precision in acquiring trouble expressions." ></td>
	<td class="line x" title="64:189	Each category of evidence is described further below." ></td>
	<td class="line x" title="65:189	3.1 Lexico-syntactic patterns for hyponymy Since trouble expressions are hyponyms of trouble, one obvious way of acquiring trouble expressions is to use classical lexico-syntactic patterns for hyponymy acquisition (Hearst, 1992)." ></td>
	<td class="line x" title="66:189	Table 1 lists some of the patterns proposed in studies on hyponymy acquisition for Japanese (Ando et al., 2003; Imasumi, 2001) that are utilized in this work." ></td>
	<td class="line x" title="67:189	In actual acquisition, we instantiated the hypernym positions in the patterns by Japanese translations of trouble and its synonyms, namely toraburu (troubles), sainan (accidents), saigai (disasters) and shougai (obstacles or handicaps), and used the instantiated patterns as evidence." ></td>
	<td class="line x" title="68:189	Hereafter, we call these patterns LSPHs (Lexico-Syntactic Patterns for Hyponymy)." ></td>
	<td class="line x" title="69:189	3.2 Dependency relations with Verbs We expect expressions that frequently refer to troubles to have a distinct dependency profile, by which we mean a specific set of dependency relations with verbs (i.e. occurrences in specific argument positions)." ></td>
	<td class="line x" title="70:189	If T is a trouble expression, then given a sufficiently large corpus one would expect to find a reasonable number of instantiations of patterns like the following:  T kept X from doing Y .  X didnt enjoy Y because of T. Similarly, X enjoyed T would present negative evidence for T being a trouble expression." ></td>
	<td class="line x" title="71:189	Rather than single out a set of particular dependency relations suspected to be indicative of trouble expressions, we let a supervised classifier learn an appropriate weight for each feature in a large vector of dependency relations." ></td>
	<td class="line x" title="72:189	Two classes of dependency relations proved to be especially beneficial in determining trouble candidates in an unsupervisedmanner,sowediscusstheminmoredetail below." ></td>
	<td class="line x" title="73:189	Dependency relations with negated verbs Following our characterization of troubles as things that prevent specific actions from taking place, we expectagooddealof troubleexpressionstoappear in patterns like the following." ></td>
	<td class="line x" title="74:189	 X cannot go to Y because of T.  X did not enjoy Y because of T. The important points in the above are (i) the negated verbs and (ii) the mention of T as the reason for not verb-ing." ></td>
	<td class="line x" title="75:189	The following are Japanese translations of the above patterns." ></td>
	<td class="line x" title="76:189	Here P denotes postpositions (Japanese case markers), V stands for verbs and the phrase because of is translated as the postposition de." ></td>
	<td class="line x" title="77:189	 T de Y ni ikenai.P P V (cannot go)  T de X ga tanoshikunakatta.P P V (did not enjoy) 187 We refer to the following dependency relations between expressions marked with the postposition de and negated verbs in these patterns as DNVs (Dependencies to Negated Verbs)." ></td>
	<td class="line x" title="78:189	T de  negated verb (1) We allow any verb to be the negated verb, expecting that inappropriate verbs will be less weighted by machine learning techniques." ></td>
	<td class="line x" title="79:189	For instance, the dependency relations to negated verbs with an originally negative orientation such as suffer and die will not work as positive examples for trouble expressions." ></td>
	<td class="line x" title="80:189	Unfortunately, these patterns still present only weak evidence for trouble expressions." ></td>
	<td class="line x" title="81:189	The precision of the trouble expressions collected using DNV patterns is extremely low  around 6.5%." ></td>
	<td class="line x" title="82:189	This is due to the postposition des ambiguity  besidesbecauseofrelationsitalsofunctionsasa marker for location, time and instrument relations, among others." ></td>
	<td class="line x" title="83:189	As a result, non-trouble expressions such as by car (instrument) and in Tokyo (location) are marked by the postposition de as well." ></td>
	<td class="line x" title="84:189	We consider a second class of dependency relations, acting mostly as a counter to the noisy expressions introduced by the ambiguity of the postposition de." ></td>
	<td class="line x" title="85:189	Dependency relations with non-negated verbs The final type of evidence is formulated as the following dependency relation." ></td>
	<td class="line x" title="86:189	T de  non-negated verb We call this type of relation DAVs (Dependencies to Affirmative Verbs)." ></td>
	<td class="line x" title="87:189	The use of these patterns is motivated by the intuition that noisy expressions found with DNVs, such as expressions about locations or instruments, will also frequently appear with non-negated verbs." ></td>
	<td class="line x" title="88:189	That is, if you observe cannot go to Y (by / because of) X and X is not a trouble expression, then you can expect to find can go to Y (by / because of) X as well." ></td>
	<td class="line x" title="89:189	Our initial expectation was that the DNV and DAV evidences observed with the postposition de alone would contain sufficient information to obtain an accurate classifier, but this expectation was not borne out by our early experiments." ></td>
	<td class="line x" title="90:189	As it turns out, using dependency relations to verbs in all argument positions as features to the SVM resulted roughly in a 1015% increase in precision." ></td>
	<td class="line x" title="91:189	Therefore in our final experiments we let the DNV and DAV evidence consist of dependencies with four additional postpositions (ha,ga,woandni), which are used to indicate topicalization, subject, object and indirect object." ></td>
	<td class="line x" title="92:189	We found that the SVM was quite successful in learning a dependency profile for trouble expressions based on this information." ></td>
	<td class="line x" title="93:189	Nonetheless, the DNV/DAV patterns proved to be useful besides as evidence for supervised learning,forinstanceingatheringsufficienttroublecandidates and sample selection when preparing training data 2." ></td>
	<td class="line x" title="94:189	4 Method As mentioned, our method for finding troubles in using some objects consists of three steps, described in more detail below." ></td>
	<td class="line x" title="95:189	Step 1 Gather training data with a sufficient amount of positive samples using an unsupervised method to reduce the workload of manual annotation." ></td>
	<td class="line x" title="96:189	Step 2 Collect expressions commonly perceived as troubles by using the evidences described in the previous section." ></td>
	<td class="line x" title="97:189	Step 3 Identify pairs of trouble expressions and objects such that the trouble expressions represent an obstacle in using the objects." ></td>
	<td class="line x" title="98:189	4.1 Step 1: Gathering Training Data We considered noun phrases observed with the LSPHandDNVevidencesascandidatetroubleexpressions." ></td>
	<td class="line x" title="99:189	However, we still found only 7% of the samples observed with these evidences to be real troubles." ></td>
	<td class="line x" title="100:189	Because of the diversity of our evidences (dependencies with verbs) we need a reasonable amount of positive samples in order to obtain an accurate classifier." ></td>
	<td class="line x" title="101:189	Without some sample selection scheme, we would have to manually annotate about 8000 samples in order to obtain only 560 positive samples in the training data." ></td>
	<td class="line x" title="102:189	For this reason we used the following scoring function as an unsupervised method for sample selection." ></td>
	<td class="line x" title="103:189	Score(e) = fLSPH(e) + fDNV(e)f LSPH(e) + fDNV(e) + fDAV(e) (2) Here, fLSPH(e), fDNV(e) and fDAV(e) are the frequencies that expression e appears with the respective evidences." ></td>
	<td class="line x" title="104:189	Intuitively, this function gives 2 We will discuss yet another use of the DNV evidence in step 2 of our acquisition method." ></td>
	<td class="line x" title="105:189	188 a large score to expressions that occur frequently with the positive evidences for trouble expressions (LSPHs and DNVs), or those that appear rarely with the negative evidences (DAVs)." ></td>
	<td class="line x" title="106:189	In preparing training data we ranked all candidates according to the above score, and annotated N elements from the top and bottom of the ranking as training data." ></td>
	<td class="line x" title="107:189	In our experiments, the top elements included a reasonable number of positive samples (25.8%) while there were almost none in the worst elements." ></td>
	<td class="line x" title="108:189	4.2 Step 2: Finding Trouble Expressions In this step our aim is to acquire expressions often associated with troubles." ></td>
	<td class="line x" title="109:189	We use a supervised classifier, namely Support Vector Machines (SVMs) (Vapnik, 1998) for distinguishing troubles from non-troubles, based on the evidences described above." ></td>
	<td class="line x" title="110:189	Each dimension of the feature vector presented to the SVM corresponds to the observation of a particular evidence (i.e., these are binary features)." ></td>
	<td class="line x" title="111:189	We tried using frequencies instead of binary feature values but could not find any significant improvement in performance." ></td>
	<td class="line x" title="112:189	After learning we sort the candidate trouble expressions according to their distance to the hyperplane learned by the SVM, and consider the top N expressions in the sorted list as true trouble expressions." ></td>
	<td class="line x" title="113:189	4.3 Step 2: Identifying Object-Trouble Pairs In this third stage we rank possible combinations of objects and trouble expressions acquired in the previous step according to their degree of association and apply a filter using negated verbs to the top pairs in the ranking." ></td>
	<td class="line x" title="114:189	The final output of our method is the top N pairs that survived the filtering." ></td>
	<td class="line x" title="115:189	We describe each step below." ></td>
	<td class="line x" title="116:189	Generating Object-Trouble Pairs To generate and rank object-trouble pairs we use a variant of pairwise mutual information that scores an objecttrouble pair eo,et based on the observed frequency of the following pattern." ></td>
	<td class="line x" title="117:189	eo no et P (3) The postposition no is a genitive case marker, and the whole pattern can be translated as et of / in eo." ></td>
	<td class="line x" title="118:189	We assume that appearance of expression et in this pattern refers to a trouble in using the object eo." ></td>
	<td class="line x" title="119:189	More precisely, we generate all possible combinations of trouble expression and objects and rank them according to the following score." ></td>
	<td class="line x" title="120:189	I(eo,et) = f(eo no et)f(e o)f(et) (4) where f(e) denotes an expression es frequency." ></td>
	<td class="line x" title="121:189	This score is large when the pattern eo no et is observed more frequently than can be expected fromeo andetsindividualfrequencies." ></td>
	<td class="line x" title="122:189	Frequency data for all noun phrases was precomputed for the whole Web corpus." ></td>
	<td class="line x" title="123:189	Filtering Object-Trouble Pairs The filtering in the second step is based on the following assumption." ></td>
	<td class="line x" title="124:189	Assumption If a trouble expression et refers to a trouble in using an object eo , there is a verb v such that v frequently co-occurs with eo and v has the following dependency relation with et." ></td>
	<td class="line x" title="125:189	et de  negated v The intuition behind this assumption can be explained as follows." ></td>
	<td class="line x" title="126:189	First, if eo denotes an object or artifact then its frequently co-occurring verbs are likely to be related to a use of eo." ></td>
	<td class="line x" title="127:189	Second, if et is a trouble in using eo, there is some action associated witheo thatet prevents or hinders, implying thatet should be observed with its negation." ></td>
	<td class="line x" title="128:189	For instance, if traffic jam is a trouble in using an amusement park, then we can expect the following pattern to appear also in a corpus." ></td>
	<td class="line x" title="129:189	 juutai de yuuenchi ni ikenai.traffic jam P theme park P V (cannot go) cannot go to a theme park because of a traffic jam The verb to go co-occurs often with the noun theme park and the above pattern contains the dependencyrelationtrafficjamdecannotgo." ></td>
	<td class="line x" title="130:189	Substituting v in the hypothesis for to go, the assumption becomes valid." ></td>
	<td class="line x" title="131:189	Because of data sparseness the above pattern may not actually appear in the corpus, but even so the dependency relation traffic jam de  cannot go may be observed with other facilities, and thus making the assumption hold anyway." ></td>
	<td class="line x" title="132:189	As a final filtering procedure, we gathered K verbs most frequently co-occurring with each object and checked if the trouble expression in the pair has dependency relations with the K verbs in 189 negated form and the postposition de." ></td>
	<td class="line x" title="133:189	If none of the K verbs has such a dependency with the trouble expression, the pair is discarded." ></td>
	<td class="line x" title="134:189	Otherwise, it is produced as the final output of our method." ></td>
	<td class="line x" title="135:189	5 Experimental Results 5.1 Finding Trouble Expressions We extracted noun phrases observed in LSPH, DNV and DAV patterns from 6109 sentences in about 108 crawled Japanese Web documents, and used the LSPH and DNV data 3 as candidate trouble expressions." ></td>
	<td class="line x" title="136:189	After restricting the noun phrases to those observed more than 10 times in the evidences, we had 136,212 noun phrases." ></td>
	<td class="line x" title="137:189	We denote thissetasD." ></td>
	<td class="line x" title="138:189	Extracting200randomsamplesfrom D we found the ratio of troubles to non-troubles was around 7% and thus expected to find about 10,000 real trouble expressions in D. 4 Using the sample selection method described in Section 4.2 we prepared 6,500 annotated samples taken from D as training data." ></td>
	<td class="line x" title="139:189	The top 3,500 samples included 912 positive samples and the worst 3,000 had just 9 positives, thereby confirming the effectiveness of the scoring function for selecting a reasonable amount of positive samples." ></td>
	<td class="line x" title="140:189	Our final training data thus contained 14% positives." ></td>
	<td class="line x" title="141:189	For the feature vectors we included dependencies with all verbs occurring more than 30 times in our Web corpus." ></td>
	<td class="line x" title="142:189	Besides the LSPH, DNV and DAV evidences discussed previously, we also included 10 additional binary features indicating for each of the five postpositions whether the expression was observed with DNV or DAV evidence at all, and found that including this information improved performance." ></td>
	<td class="line x" title="143:189	We trained a classifier with a polynomial kernel of degree 1 on these evidences using the software TinySVM5, and evaluated the results obtained by the supervised acquisition method by asking three human raters whether a randomly selected sample expression denotes a kind of trouble in general situations." ></td>
	<td class="line x" title="144:189	More specifically, we asked whether the expression is a kind of toraburu (trouble), sainan (accident), saigai (disaster) or shougai (obstacle or handicap).6 For various 3 We restricted noun phrases from the DNV data to those found with the postposition de, as these are most likely to refer to troubles." ></td>
	<td class="line x" title="145:189	4 Thus, in the experiments we evaluated the top 10,000 samples output by our method." ></td>
	<td class="line x" title="146:189	5 http://chasen.org/taku/software/TinySVM/ 6 Actually one of the raters is a co-author of this paper, but  0  20  40  60  80  100  0  20  40  60  80  100 Precision (%) Number of Samples (%) randomLSPH Scorefull w/o LSPHw/o DAV w/o DNVw/o sum DAV/DNV Figure 1: Performance of trouble expression acquisition (all 3 raters) combinations of evidences (described below), we presented200randomlysampledexpressionsfrom the top 10,000 expressions ranked according to the distance to the hyperplane learned by the SVM." ></td>
	<td class="line x" title="147:189	Samples of all the compared methods are merged and shuffled before evaluation." ></td>
	<td class="line x" title="148:189	The kappa statistic for assessing the inter-rater agreement was 0.78, indicating substantial agreement according to Landis and Koch, 1977.7 We made no effort to remove samples used in training from the experiment, and found that the samples scored by the raters (1281 in total, after removal of duplicates) contained 67 training samples." ></td>
	<td class="line x" title="149:189	The 200 samples from the full classifier contained 12 of these." ></td>
	<td class="line x" title="150:189	Fig." ></td>
	<td class="line x" title="151:189	1 shows the precision of the acquired trouble expressions compared to the samples labeled as troubles by all three raters." ></td>
	<td class="line x" title="152:189	We sorted the samples according to their distance to the SVM hyperplane and plotted the precision of the top N samples." ></td>
	<td class="line x" title="153:189	The best overall precision (85.5%) was obtained by a classifier trained on the full combination of evidences (labeled full in Fig." ></td>
	<td class="line x" title="154:189	1), maintaining over 90% precision for the top 70% of the 200 samples." ></td>
	<td class="line x" title="155:189	The remaining results show the relative contributions of the evidences." ></td>
	<td class="line x" title="156:189	They were obtained by retraining the full classifier with a particular set of evidences removed, respectively LSPH evidences (labeled w/o LSPH), DNV evidences (w/o DNV), DAV evidences (w/o DAV) and the 10 features indicating the observation of he had no knowledge of the experimental setting nor had seen the acquired data prior to the experiment." ></td>
	<td class="line x" title="157:189	7 This kappa value was calculated over the sum total of samples presented to the raters for scoring (duplicates removed)." ></td>
	<td class="line x" title="158:189	190  20  30  40  50  60  70  80  90  100  0  20  40  60  80  100 Precision (%) Number of Samples (%) randomtop 10% MI top 50% MItop 10% proposed top 50% proposed Figure 2: Performance of object-trouble pair acquisition (3 raters) DAV/DNV evidence per postposition (w/o sum DAV/DNV)." ></td>
	<td class="line x" title="159:189	As Fig." ></td>
	<td class="line x" title="160:189	1 shows, leaving out DNV and even LSPH evidences did not affect performance as much as we expected, while leaving out the DAV dependencies gave more than 20% worse results." ></td>
	<td class="line x" title="161:189	Of further interest is the importance of the binary features for DAV/DNV presence per postposition (w/o sum DAV/DNV)." ></td>
	<td class="line x" title="162:189	The absence of these 10 binary features accounts for a 10% precision loss compared to the full feature set (75%)." ></td>
	<td class="line x" title="163:189	We also compared it with a baseline method using only lexico-syntactic patterns." ></td>
	<td class="line x" title="164:189	We extracted 100randomnounphrasesfromtheLSPHevidence in D for evaluation (LSPH in Fig." ></td>
	<td class="line x" title="165:189	1)." ></td>
	<td class="line x" title="166:189	The precision for this method was 31%, confirming that lexico-syntactic patterns for hyponymy constitute fairly weak evidence for predicting trouble expressions when used alone." ></td>
	<td class="line x" title="167:189	Score shows the precision of the top 100 samples output by our Score function from section 4." ></td>
	<td class="line x" title="168:189	Finally, random (drawn as a straight line) denotes 100 random samples from D and roughly corresponds to our estimate of 7% true positives." ></td>
	<td class="line x" title="169:189	5.2 Identifying Object-Trouble Pairs Forthesecondstep,weassumedthetop10,000expressions obtained by our best-scoring supervised learning method (full in the previous experiments) to be trouble expressions, and proceeded to combine them with terms denoting artifacts or facilities." ></td>
	<td class="line x" title="170:189	We randomly picked 2,500 words that appeared as direct objects of the verbs kau (to buy),tsukau(touse),tsukuru(tomake), taberu (to eat) and tanoshimu (to enjoy) rank /raters object trouble expressions 1/3 kousoku douro sakeyoi unten (highway) (drunk driving) 7/3 kouseibushitsu ranyou (antibiotics) (abuse) 8/3 suidousui suiatsu teika (tap water) (drop in water pressure) 21/3 nouyaku zanryuubushitsu (agrichemicals) (residue) 98/2 kikai gohandan (machine) (judgement error) 136/3 zaisan souzoku funsou (estate) (succession dispute) Figure 3: Examples of acquired object-trouble pairs more than 500 times in our Web corpus, assuming that this would yield a representative set of noun phrases denoting objects or artifacts.8 Combining this set of objects with the acquired trouble expressions gave a list of 61,873 object-trouble pairs (all pairs eo,et with at least one occurrence of the pattern eo no et)." ></td>
	<td class="line x" title="171:189	Of this list, 58,570 pairs survivedtheDNVfilteringstepandformthefinaloutput of our method." ></td>
	<td class="line x" title="172:189	For the DNV filtering, we used the top 30 verbs most frequently co-occurring with the object." ></td>
	<td class="line x" title="173:189	We again evaluated the resulting object-trouble pairsbyaskingthreehumanraterswhetherthepresented pairs consist of an object and an expression referring to an actual or potential trouble in using the object." ></td>
	<td class="line x" title="174:189	The kappa statistic was 0.60, indicating moderate inter-rater agreement." ></td>
	<td class="line x" title="175:189	Fig." ></td>
	<td class="line x" title="176:189	2 shows the precision of the acquired pairs when comparing with what are considered true object-trouble relations by all three raters." ></td>
	<td class="line x" title="177:189	Some examples of the pairs obtained by our method are listed in table 3 along with their ranking and the number of raters who judged the pair to be correct." ></td>
	<td class="line x" title="178:189	The precision for our proposed method when considering the top 10% of pairs ranked by the I score and filtered by the method described in section 4.3 is 71.5% (top 10% proposed in Fig." ></td>
	<td class="line x" title="179:189	2), which is actually worse than the results obtained without the final DNV filtering (top 10% MI, 74%)." ></td>
	<td class="line x" title="180:189	For the first half of all samples however, we do observe some performance increase by the filtering, though both methods appear to converge in the second half of the graph." ></td>
	<td class="line x" title="181:189	This tendency is mirrored closely when considering the results for the top 50% of all pairs (respectively top 50% proposed and top 50% MI in Fig." ></td>
	<td class="line x" title="182:189	2)." ></td>
	<td class="line x" title="183:189	The 15% decrease in precision compared to top 10% results 8 We manually removed pronouns from this set." ></td>
	<td class="line x" title="184:189	191 indicates that performance drops gradually when moving to the lower ranked pairs." ></td>
	<td class="line x" title="185:189	6 Concluding Remarks and Future Work We have presented an automatic method for finding potential troubles in using objects, mainly artifacts and facilities." ></td>
	<td class="line x" title="186:189	Our method acquired 10,000 troubleexpressionswith85.5%precision,andover 6000 pairs of objects and trouble expressions with 74% precision." ></td>
	<td class="line x" title="187:189	Currently, we are developing an Internet search engine frontend that issues warnings about potential troubles related to search keywords." ></td>
	<td class="line x" title="188:189	Although we were able to acquire object-trouble pairs with reasonableprecision,weplantomakealarge-scale highly precise list of troubles by manually checking the output of our method." ></td>
	<td class="line x" title="189:189	We expect such a list to lead to even more acurate object-trouble pair acquisition." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1031
Mining Opinions in Comparative Sentences
Ganapathibhotla, Murthy;Liu, Bing;"></td>
	<td class="line x" title="1:290	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 241248 Manchester, August 2008 Mining Opinions in Comparative Sentences Murthy Ganapathibhotla Department of Computer Science University of Illinois at Chicago 851 South Morgan Street Chicago, IL 60607-7053 sganapat@cs.uic.edu Bing Liu Department of Computer Science University of Illinois at Chicago 851 South Morgan Street Chicago, IL 60607-7053 liub@cs.uic.edu   Abstract This paper studies sentiment analysis from the user-generated content on the Web." ></td>
	<td class="line x" title="2:290	In particular, it focuses on mining opinions from comparative sentences, i.e., to determine which entities in a comparison are preferred by its author." ></td>
	<td class="line x" title="3:290	A typical comparative sentence compares two or more entities." ></td>
	<td class="line x" title="4:290	For example, the sentence, the picture quality of Camera X is better than that of Camera Y, compares two entities Camera X and Camera Y with regard to their picture quality." ></td>
	<td class="line x" title="5:290	Clearly, Camera X is the preferred entity." ></td>
	<td class="line x" title="6:290	Existing research has studied the problem of extracting some key elements in a comparative sentence." ></td>
	<td class="line x" title="7:290	However, there is still no study of mining opinions from comparative sentences, i.e., identifying preferred entities of the author." ></td>
	<td class="line x" title="8:290	This paper studies this problem, and proposes a technique to solve the problem." ></td>
	<td class="line x" title="9:290	Our experiments using comparative sentences from product reviews and forum posts show that the approach is effective." ></td>
	<td class="line x" title="10:290	1 Introduction In the past few years, there was a growing interest in mining opinions in the user-generated content (UGC) on the Web, e.g., customer reviews, forum posts, and blogs." ></td>
	<td class="line oc" title="11:290	One major focus is sentiment classification and opinion mining (e.g., Pang et al 2002; Turney 2002; Hu and Liu 2004; Wilson et al 2004; Kim and Hovy 2004; Popescu and Etzioni 2005)   2008." ></td>
	<td class="line x" title="12:290	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="13:290	Some rights reserved." ></td>
	<td class="line o" title="14:290	However, these studies mainly center on direct opinions or sentiments expressed on entities." ></td>
	<td class="line x" title="15:290	Little study has been done on comparisons, which represent another type of opinion-bearing text." ></td>
	<td class="line x" title="16:290	Comparisons are related to but are also quite different from direct opinions." ></td>
	<td class="line x" title="17:290	For example, a typical direct opinion sentence is the picture quality of Camera X is great, while a typical comparative sentence is the picture quality of Camera X is better than that of Camera Y. We can see that comparisons use different language constructs from direct opinions." ></td>
	<td class="line x" title="18:290	A comparison typically expresses a comparative opinion on two or more entities with regard to their shared features or attributes, e.g., picture quality." ></td>
	<td class="line x" title="19:290	Although direct opinions are most common in UGC, comparisons are also widely used (about 10% of the sentences), especially in forum discussions where users often ask questions such as X vs. Y (X and Y are competing products)." ></td>
	<td class="line x" title="20:290	Discussions are then centered on comparisons." ></td>
	<td class="line x" title="21:290	Jindal and Liu (2006) proposed a technique to identify comparative sentences from reviews and forum posts, and to extract entities, comparative words, and entity features that are being compared." ></td>
	<td class="line x" title="22:290	For example, in the sentence, Camera X has longer battery life than Camera Y, the technique extracts Camera X and Camera Y as entities, and longer as the comparative word and battery life as the attribute of the cameras being compared." ></td>
	<td class="line x" title="23:290	However, the technique does not find which entity is preferred by the author." ></td>
	<td class="line x" title="24:290	For this example, clearly Camera Y is the preferred camera with respect to the battery life of the cameras." ></td>
	<td class="line x" title="25:290	This paper aims to solve this problem, which is useful in many applications because the preferred entity is the key piece of information in a comparative opinion." ></td>
	<td class="line x" title="26:290	For example, a potential customer clearly wants to buy the product that is better or preferred." ></td>
	<td class="line x" title="27:290	In this work, we treat a sentence as the basic 241 information unit." ></td>
	<td class="line x" title="28:290	Our objective is thus to identify the preferred entity in each comparative sentence." ></td>
	<td class="line x" title="29:290	A useful observation about comparative sentences is that in each such sentence there is usually a comparative word (e.g., better, worse and er word) or a superlative word (e.g., best, worst and est word)." ></td>
	<td class="line x" title="30:290	The entities being compared often appear on the two sides of the comparative word." ></td>
	<td class="line x" title="31:290	A superlative sentence may only have one entity, e.g., Camera X is the best." ></td>
	<td class="line x" title="32:290	For simplicity, we use comparative words (sentences) to mean both comparative words (sentences) and superlative words (sentences)." ></td>
	<td class="line x" title="33:290	Clearly, the preferred entity in a comparative sentence is mainly determined by the comparative word in the sentence." ></td>
	<td class="line x" title="34:290	Some comparative words explicitly indicate user preferences, e.g., better, worse, and best." ></td>
	<td class="line x" title="35:290	We call such words opinionated comparative words." ></td>
	<td class="line x" title="36:290	For example, in the sentence, the picture quality of Camera X is better than that of Camera Y, Camera X is preferred due to the opinionated comparative word better." ></td>
	<td class="line x" title="37:290	However, many comparative words are not opinionated, or their opinion orientations (i.e., positive or negative) depend on the context and/or the application domain." ></td>
	<td class="line x" title="38:290	For instance, the word longer is not opinionated as it is normally used to express that the length of some feature of an entity is greater than the length of the same feature of another entity." ></td>
	<td class="line x" title="39:290	However, in a particular context, it can express a desired (or positive) or undesired (or negative) state." ></td>
	<td class="line x" title="40:290	For example, in the sentence, the battery life of Camera X is longer than Camera Y, longer clearly expresses a desired state for battery life (although this is an objective sentence with no explicit opinion)." ></td>
	<td class="line x" title="41:290	Camera X is thus preferred with regard to battery life of the cameras." ></td>
	<td class="line x" title="42:290	The opinion in this sentence is called an implicit opinion." ></td>
	<td class="line x" title="43:290	We also say that longer is positive in this context." ></td>
	<td class="line x" title="44:290	We know this because of our existing domain knowledge." ></td>
	<td class="line x" title="45:290	However, longer may also be used to express an undesirable state in a different context, e.g., Program Xs execution time is longer than Program Y." ></td>
	<td class="line x" title="46:290	longer is clearly negative here." ></td>
	<td class="line x" title="47:290	Program Y is thus preferred." ></td>
	<td class="line x" title="48:290	We call comparative words such as longer and smaller context-dependent opinion comparatives." ></td>
	<td class="line x" title="49:290	Sentences with opinionated words (e.g., better, and worse) are usually easy to handle." ></td>
	<td class="line x" title="50:290	Then the key to solve our problem is to identify the opinion orientations (positive or negative) of context-dependent comparative words." ></td>
	<td class="line x" title="51:290	To this end, two questions need to be answered: (1) what is a context and (2) how to use the context to help determine the opinion orientation of a comparative word?" ></td>
	<td class="line x" title="52:290	The simple answer to question (1) is the whole sentence." ></td>
	<td class="line x" title="53:290	However, a whole sentence as context is too complex because it may contain too much irrelevant information, which can confuse the system." ></td>
	<td class="line x" title="54:290	Intuitively, we want to use the smallest context that can determine the orientation of the comparative word." ></td>
	<td class="line x" title="55:290	Obviously, the comparative word itself must be involved." ></td>
	<td class="line x" title="56:290	We thus conjecture that the context should consist of the entity feature being compared and the comparative word." ></td>
	<td class="line x" title="57:290	Our experimental results show that this context definition works quite well." ></td>
	<td class="line x" title="58:290	To answer the second question, we need external information or knowledge because there is no way that a computer program can solve the problem by analyzing the sentence itself." ></td>
	<td class="line x" title="59:290	In this paper, we propose to use the external information in customer reviews on the Web to help solve the problem." ></td>
	<td class="line x" title="60:290	There are a large number of such reviews on almost any product or service." ></td>
	<td class="line x" title="61:290	These reviews can be readily downloaded from many sites." ></td>
	<td class="line x" title="62:290	In our work, we use reviews from epinions.com." ></td>
	<td class="line x" title="63:290	Each review in epinions.com has separate Pros and Cons (which is also the case in most other review sites)." ></td>
	<td class="line x" title="64:290	Thus, positive and negative opinions are known as they are separated by reviewers." ></td>
	<td class="line x" title="65:290	However, they cannot be used directly because Pros and Cons seldom contain comparative words." ></td>
	<td class="line x" title="66:290	We need to deal with this problem." ></td>
	<td class="line x" title="67:290	Essentially, the proposed method computes whether the comparative word and the feature are more associated in Pros or in Cons." ></td>
	<td class="line x" title="68:290	If they are more associated in Pros (or Cons) than Cons (or Pros), then the comparative word is likely to be positive (or negative) for the feature." ></td>
	<td class="line x" title="69:290	A new association measure is also proposed to suit our purpose." ></td>
	<td class="line x" title="70:290	Our experiment results show that it can achieve high precision and recall." ></td>
	<td class="line x" title="71:290	2 Related Work Sentiment analysis has been studied by many researchers recently." ></td>
	<td class="line x" title="72:290	Two main directions are sentiment classification at the document and sentence levels, and feature-based opinion mining." ></td>
	<td class="line oc" title="73:290	Sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or negative (Pang et al 2002; Turney 2002)." ></td>
	<td class="line x" title="74:290	Sentiment classification at the sentence-level has also been studied (e.g., Riloff and Wiebe 2003; Kim and Hovy 2004; Wilson et al 2004; Gamon et al 242 2005; Stoyanov and Cardie 2006)." ></td>
	<td class="line o" title="75:290	These works are different from ours as we study comparatives." ></td>
	<td class="line x" title="76:290	The works in (Hu and Liu 2004; Liu et al 2005; Popescu and Etzioni 2005; Mei et al 2007) perform opinion mining at the feature level." ></td>
	<td class="line x" title="77:290	The task involves (1) extracting entity features (e.g., picture quality and battery life in a camera review) and (2) finding orientations (positive, negative or neutral) of opinions expressed on the features by reviewers." ></td>
	<td class="line x" title="78:290	Again, our work is different because we deal with comparisons." ></td>
	<td class="line x" title="79:290	Discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words (Hatzivassiloglou and McKeown 1997; Kanayama and Nasukawa 2006)." ></td>
	<td class="line x" title="80:290	Both works use conjunction rules to find such words from large domain corpora." ></td>
	<td class="line x" title="81:290	One conjunction rule states that when two opinion words are linked by and, their opinions are the same." ></td>
	<td class="line x" title="82:290	Our method is different in three aspects." ></td>
	<td class="line x" title="83:290	First, we argue that finding domain opinion words is problematic because in the same domain the same word may indicate different opinions depending on what features it is applied to." ></td>
	<td class="line x" title="84:290	For example, in the camera domain, long is positive in the battery life is very long but negative in it takes a long time to focus." ></td>
	<td class="line x" title="85:290	Thus, we should consider both the feature and the opinion word rather than only the opinion word." ></td>
	<td class="line x" title="86:290	Second, we focus on studying opinionated comparative words." ></td>
	<td class="line x" title="87:290	Third, our technique is quite different as we utilize readily available external opinion sources." ></td>
	<td class="line x" title="88:290	As discussed in the introduction, a closely related work to ours is (Jindal and Liu 2006)." ></td>
	<td class="line x" title="89:290	However, it does not find which entities are preferred by authors." ></td>
	<td class="line x" title="90:290	Bos and Nissim (2006) proposes a method to extract some useful items from superlative sentences." ></td>
	<td class="line x" title="91:290	Fiszman et al (2007) studied the problem of identifying which entity has more of certain features in comparative sentences." ></td>
	<td class="line x" title="92:290	It does not find which entity is preferred." ></td>
	<td class="line x" title="93:290	3 Problem Statement Definition (entity and feature): An entity is the name of a person, a product, a company, a location, etc, under comparison in a comparative sentence." ></td>
	<td class="line x" title="94:290	A feature is a part or attribute of the entity that is being compared." ></td>
	<td class="line x" title="95:290	For example, in the sentence, Camera Xs battery life is longer than that of Camera Y, Camera X and Camera Y are entities and battery life is the camera feature." ></td>
	<td class="line x" title="96:290	Types of Comparatives 1)  Non-equal gradable: Relations of the type greater or less than that express a total ordering of some entities with regard to their shared features." ></td>
	<td class="line x" title="97:290	For example, the sentence, Camera Xs battery life is longer than that of Camera Y, orders Camera X and Camera Y based on their shared feature battery life." ></td>
	<td class="line x" title="98:290	2)  Equative: Relations of the type equal to that state two objects as equal with respect to some features, e.g., Camera X and Camera Y are about the same size." ></td>
	<td class="line x" title="99:290	3)  Superlative: Relations of the type greater or less than all others that rank one object over all others, Camera Xs battery life is the longest." ></td>
	<td class="line x" title="100:290	4)  Non-gradable: Sentences which compare features of two or more entities, but do not explicitly grade them, e.g., Camera X and Camera Y have different features The first three types are called gradable comparatives." ></td>
	<td class="line x" title="101:290	This paper focuses on the first and the third types as they express ordering relationships of entities." ></td>
	<td class="line x" title="102:290	Equative and non-gradable sentences usually do not express preferences." ></td>
	<td class="line x" title="103:290	Definition (comparative relation): A comparative relation is the following: <ComparativeWord, Features, EntityS1, EntityS2, Type> ComparativeWord is the keyword used to express a comparative relation in the sentence." ></td>
	<td class="line x" title="104:290	Features is a set of features being compared." ></td>
	<td class="line x" title="105:290	EntityS1 and EntityS2 are sets of entities being compared." ></td>
	<td class="line x" title="106:290	Entities in EntityS1 appear on the left of the comparative word and entities in EntityS2 appear on the right." ></td>
	<td class="line x" title="107:290	Type is non-equal gradable, equative or superlative." ></td>
	<td class="line x" title="108:290	Let us see an example." ></td>
	<td class="line x" title="109:290	For the sentence Camera X has longer battery life than Camera Y, the extracted relation is: <longer, {battery life}, {Camera X}, {Camera Y}, non-equal gradable>." ></td>
	<td class="line x" title="110:290	We assume that the work in (Jindal and Liu 2006) has extracted the above relation from a comparative sentence." ></td>
	<td class="line x" title="111:290	In this work, we aim to identify the preferred entity of the author, which is not studied in (Jindal and Liu 2006)." ></td>
	<td class="line x" title="112:290	Our objective: Given the extracted comparative relation from a comparative sentence, we want to identify whether the entities in EntityS1 or in EntityS2 are preferred by the author." ></td>
	<td class="line x" title="113:290	4 Proposed Technique We now present the proposed technique." ></td>
	<td class="line x" title="114:290	As discussed above, the primary determining factors of the preferred entity in a comparative sentence are 243 the feature being compared and the comparative word, which we conjecture, form the context for opinions (or preferred entities)." ></td>
	<td class="line x" title="115:290	We develop our ideas from here." ></td>
	<td class="line x" title="116:290	4.1 Comparatives and superlatives In English, comparatives and superlatives are special forms of adjectives and adverbs." ></td>
	<td class="line x" title="117:290	In general, comparatives are formed by adding the suffix -er and superlatives are formed by adding the suffix est to the base adjectives and adverbs." ></td>
	<td class="line x" title="118:290	We call this type of comparatives and superlatives Type 1 comparatives and superlatives." ></td>
	<td class="line x" title="119:290	For simplicity, we will use Type 1 comparatives to represent both from now on." ></td>
	<td class="line x" title="120:290	Adjectives and adverbs with two syllables or more and not ending in y do not form comparatives or superlatives by adding er or est." ></td>
	<td class="line x" title="121:290	Instead, more, most, less and least are used before such words, e.g., more beautiful." ></td>
	<td class="line x" title="122:290	We call this type of comparatives and superlatives Type 2 comparatives and Type 2 superlatives." ></td>
	<td class="line x" title="123:290	These two types are called regular comparatives and superlatives respectively." ></td>
	<td class="line x" title="124:290	In English, there are also some irregular comparatives and superlatives, which do not follow the above rules, i.e., more, most, less, least, better, best, worse, worst, further/farther and furthest/farthest." ></td>
	<td class="line x" title="125:290	They behave similarly to Type 1 comparatives and superlatives and thus are grouped under Type 1." ></td>
	<td class="line x" title="126:290	Apart from these comparatives and superlatives, there are non-standard words that express gradable comparisons, e.g., prefer, and superior." ></td>
	<td class="line x" title="127:290	For example, the sentence, in term of battery life, Camera X is superior to Camera Y, says that Camera X is preferred." ></td>
	<td class="line x" title="128:290	We obtained a list of 27 such words from (Jindal and Liu 2006) (which used more words, but most of them are not used to express gradable comparisons)." ></td>
	<td class="line x" title="129:290	Since these words behave similarly to Type 1 comparatives, they are thus grouped under Type 1." ></td>
	<td class="line x" title="130:290	Further analysis also shows that we can group comparatives into two categories according to whether they express increased or decreased values: Increasing comparatives: Such a comparative expresses an increased value of a quantity, e.g., more, and longer." ></td>
	<td class="line x" title="131:290	Decreasing comparatives: Such a comparative expresses a decreased value of a quantity, e.g., less, and fewer." ></td>
	<td class="line x" title="132:290	As we will see later, this categorization is very useful in identifying the preferred entity." ></td>
	<td class="line x" title="133:290	Since comparatives originate from adjectives and adverbs, they may carry positive or negative sentiments/opinions." ></td>
	<td class="line x" title="134:290	Along this dimension, we can divide them into two categories." ></td>
	<td class="line x" title="135:290	1." ></td>
	<td class="line x" title="136:290	Opinionated comparatives: For Type 1 comparatives, this category contains words such as 'better', 'worse', etc, which has explicit opinions." ></td>
	<td class="line x" title="137:290	In sentences involving such words, it is normally easy to determine which entity is the preferred one of the sentence author." ></td>
	<td class="line x" title="138:290	In the case of Type 2 comparatives, formed by adding more, less, most, and least before adjectives or adverbs, the opinion (or preferred entity) is determined by both words." ></td>
	<td class="line x" title="139:290	The following rules apply: increasing comparative Negative     Negative Opinion increasing comparative Positive      Positive Opinion decreasing comparative Negative    Positive Opinion decreasing comparative Positive     Negative Opinion  The first rule says that the combination of an increasing comparative word (e.g., more) and a negative opinion adjective/adverb (e.g., awful) implies a negative Type 2 comparative." ></td>
	<td class="line x" title="140:290	The other rules are similar." ></td>
	<td class="line x" title="141:290	These rules are intuitive and will not be discussed further." ></td>
	<td class="line x" title="142:290	2." ></td>
	<td class="line x" title="143:290	Comparatives with context-dependent opinions: These comparatives are used to compare gradable quantities of entities." ></td>
	<td class="line x" title="144:290	In the case of Type 1 comparatives, such words include higher, lower, etc. Although they do not explicitly describe the opinion of the author, they often carry implicit sentiments or preferences based on contexts." ></td>
	<td class="line x" title="145:290	For example, in Car X has higher mileage per gallon than Car Y, it is hard to know whether higher is positive or negative without domain knowledge." ></td>
	<td class="line x" title="146:290	It is only when the two words, higher and mileage, are combined we know that higher is desirable for mileage from our domain knowledge." ></td>
	<td class="line x" title="147:290	In the case of Type 2 comparatives, the situation is similar." ></td>
	<td class="line x" title="148:290	However, the comparative word (more, most, less or least), the adjective/adverb and the feature are all important in determining the opinion or the preference." ></td>
	<td class="line x" title="149:290	If we know whether the comparative word is increasing or decreasing (which is easy since there are only four such words), then the opinion can be determined by applying the four rules above in (1)." ></td>
	<td class="line x" title="150:290	For this work, we used the opinion word list from (Hu and Liu 2004), which was compiled using a bootstrapping approach based on WordNet." ></td>
	<td class="line x" title="151:290	For opinionated comparatives, due to the observation below we simply convert the opinion 244 adjectives/adverbs to their comparative forms, which is done automatically based on grammar (comparative formation) rules described above and WordNet." ></td>
	<td class="line x" title="152:290	Observation: If a word is positive (or negative), then its comparative or superlative form is also positive (or negative), e.g., good, better and best." ></td>
	<td class="line x" title="153:290	After the conversion, these words are manually categorized into increasing and decreasing comparatives." ></td>
	<td class="line x" title="154:290	Although this consumes some time, it is only a one-time effort." ></td>
	<td class="line x" title="155:290	4.2 Contexts To deal with comparatives with context dependent opinions, we need contexts." ></td>
	<td class="line x" title="156:290	It is conjectured that the comparative and the feature in the sentence form the context." ></td>
	<td class="line x" title="157:290	This works very well." ></td>
	<td class="line x" title="158:290	For a Type 2 comparative, we only need the feature and the adjective/adverb to form a context." ></td>
	<td class="line x" title="159:290	For example, in the sentence, Program X runs more quickly than Program Y, the context is the pair, (run, quickly), where run is a verb feature." ></td>
	<td class="line x" title="160:290	If we find out that (run, quickly) is positive based on some external information, we can conclude that Program X is preferred using one of the four rules above since more is an increasing comparative." ></td>
	<td class="line x" title="161:290	We will use such contexts to find opinion orientations of comparatives with regard to some features from the external information, i.e., Pros and Cons in online reviews." ></td>
	<td class="line x" title="162:290	4.3 Pros and Cons in Reviews Figure 1 shows a popular review format." ></td>
	<td class="line x" title="163:290	The reviewer first describes Pros and Cons briefly, and then writes a full review." ></td>
	<td class="line x" title="164:290	Pros and Cons are used in our work for two main reasons." ></td>
	<td class="line x" title="165:290	First, the brief information in Pros and Cons contains the essential information related to opinions." ></td>
	<td class="line x" title="166:290	Each phrase or sentence segment usually contains an entity feature and an opinion word." ></td>
	<td class="line x" title="167:290	Second, depending on whether it is in Pros or in Cons, the user opinion on the product feature is clear." ></td>
	<td class="line x" title="168:290	To use the Pros and Cons phrases, we separate them use punctuations and words, i.e., ,, ., and, and but." ></td>
	<td class="line x" title="169:290	Pros in Figure 1 can be separated into 5 phrases or segments, great photos  <photo> easy to use    <use> good manual  <manual> many options <option> takes videos <video> We can see that each segment describes an entity feature on which the reviewer has expressed an opinion." ></td>
	<td class="line x" title="170:290	The entity feature for each segment is listed within <>." ></td>
	<td class="line x" title="171:290	4.4 Identifying Preferred Entities: The Algorithm Since we use Pros and Cons as the external information source to help determine whether the combination of a comparative and an entity feature is positive or negative, we need to find comparative and entity features words in Pros and Cons." ></td>
	<td class="line x" title="172:290	However, in Pros and Cons, comparatives are seldom used (entity features are always there)." ></td>
	<td class="line x" title="173:290	Thus we need to first convert comparatives to their base forms." ></td>
	<td class="line x" title="174:290	This can be done automatically using WordNet and grammar rules described in Section 4.1." ></td>
	<td class="line x" title="175:290	We will not discuss the process here as it is fairly straightforward." ></td>
	<td class="line x" title="176:290	We now put everything together to identify the preferred entity in a comparative sentence." ></td>
	<td class="line x" title="177:290	For easy reference, we denote the comparative word as C and the feature being compared as F. After obtaining the base forms of C, we work on two main cases for the two types of comparatives: Case 1." ></td>
	<td class="line x" title="178:290	Type 1 Comparative or Superlative: There are four sub-cases." ></td>
	<td class="line x" title="179:290	1.A. C is opinionated: If the comparative or superlative C has a positive orientation (e.g., better), EntityS1 (which appears before C in the sentence) is temporarily assigned as the preferred entity." ></td>
	<td class="line x" title="180:290	Otherwise, EntityS2 is assigned as the preferred entity." ></td>
	<td class="line x" title="181:290	The reason for the temporary assignment is that the sentence may contain negations, e.g., not, which is discussed below." ></td>
	<td class="line x" title="182:290	1.B. C is not opinionated but F is opinionated: An example is, Car X generates more noise than Car Y, which has the feature F noise, a negative noun." ></td>
	<td class="line x" title="183:290	If the orientation of F is positive and C is an increasing comparative word, we assign EntityS1 as the preferred entity." ></td>
	<td class="line x" title="184:290	Otherwise, we assign EntityS2 as the preferred entity." ></td>
	<td class="line x" title="185:290	The possibilities are listed as four rules below, which are derived from the 4 rules earlier: increasing C + Positive   EntityS1 preferred decreasing C + Positive   EntityS2 preferred  Figure 1: An example review 245 increasing C + Negative   EntityS2 preferred decreasing C + Negative   EntityS1 preferred Positive and Negative stand for the orientation of feature F being positive and negative respectively." ></td>
	<td class="line x" title="186:290	1.C. Both C and F are not opinionated: In this case, we need external information to identify the preferred entity." ></td>
	<td class="line x" title="187:290	We use phrases in Pros and Cons from reviews." ></td>
	<td class="line x" title="188:290	In this case, we look for the feature F and comparative word C, (i.e., the context) in the list of phrases in Pros and Cons." ></td>
	<td class="line x" title="189:290	In order to find whether the combination of C and F indicates a positive or negative opinion, we compute their associations in Pros and in Cons." ></td>
	<td class="line x" title="190:290	If they are more associated in Pros than in Cons, we conclude that the combination indicates a positive sentiment, and otherwise a negative sentiment." ></td>
	<td class="line x" title="191:290	The result decides the preferred entity." ></td>
	<td class="line nc" title="192:290	Point-wise mutual information (PMI) is commonly used for computing the association of two terms (e.g., Turney 2002), which is defined as: nullnullnull null null,null null nullnullnull nullnullnullnull,nullnull nullnull null null null nullnullnullnullnull . However, we argue that PMI is not a suitable measure for our purpose." ></td>
	<td class="line x" title="193:290	The reason is that PMI is symmetric in the sense that PMI(F, C) is the same as PMI(C, F)." ></td>
	<td class="line x" title="194:290	However, in our case, the feature F and comparative word C association is not symmetric because although a feature is usually modified by a particular adjective word, the adjective word can modify many other features." ></td>
	<td class="line x" title="195:290	For example, long can be used in long lag, but it can also be used in long battery life, long execution time and many others." ></td>
	<td class="line x" title="196:290	Thus, this association is asymmetric." ></td>
	<td class="line x" title="197:290	We are more interested in the conditional probability of C (including its synonyms) given F, which is essentially the confidence measure in traditional data mining." ></td>
	<td class="line x" title="198:290	However, confidence does not handle well the situation where C occurs frequently but F appears rarely." ></td>
	<td class="line x" title="199:290	In such cases a high conditional probability Pr(C|F) may just represent some pure chance, and consequently the resulting association may be spurious." ></td>
	<td class="line x" title="200:290	We propose the following measure, which we call one-side association (OSA), and it works quite well: nullnullnull null null,null null nullnullnull nullnullnullnull,nullnullnullnullnullnull|nullnull nullnull null null null nullnullnullnullnull  The difference between OSA and PMI is the conditional probability Pr(C|F) used in OSA, which biases the mutual association of F and C to one side." ></td>
	<td class="line x" title="201:290	Given the comparative word C and the feature F, we first compute an OSA value for positive, denoted by OSA P (F, C), and then compute an OSA value for negative, denoted by OSA N (F, C)." ></td>
	<td class="line x" title="202:290	The decision rule is simply the following: If OSA P (F, C)  OSA N (F, C)  0 then EntityS1 is preferred Otherwise,  EntityS2 is preferred Computing OSA P (F, C): We need to compute Pr P (F, C), for which we need to count the number of times that comparative word C and the feature F co-occur." ></td>
	<td class="line x" title="203:290	Instead of using C alone, we also use its base forms and synonyms and antonyms." ></td>
	<td class="line x" title="204:290	Similarly, for F, we also use its synonyms." ></td>
	<td class="line x" title="205:290	If C (or a synonym of C) and F (or a synonym) co-occur in a Pros phrase, we count 1." ></td>
	<td class="line x" title="206:290	If an antonym of C and F (or a synonym) co-occur in a Cons phrase, we also count 1." ></td>
	<td class="line x" title="207:290	Thus, although we only evaluate for positive, we actually use both Pros and Cons." ></td>
	<td class="line x" title="208:290	This is important because it allows us to find more occurrences to produce more reliable results." ></td>
	<td class="line x" title="209:290	Synonyms and antonyms are obtained from WordNet." ></td>
	<td class="line x" title="210:290	Currently, synonyms and antonyms are only found for single word features." ></td>
	<td class="line x" title="211:290	We then count the number of occurrences of the comparative word C and the feature F separately in both Pros and Cons to compute Pr P (F) and Pr P (C)." ></td>
	<td class="line x" title="212:290	In counting the number of occurrences of C, we consider both its synonyms in Pros and antonyms in Cons." ></td>
	<td class="line x" title="213:290	In counting the number of occurrences of F, we consider its synonyms in both Pros and Cons." ></td>
	<td class="line x" title="214:290	Computing OSA N (F, C): To compute Pr N (F, C), we use a similar strategy as for computing Pr P (F, C)." ></td>
	<td class="line x" title="215:290	In this case, we start with Cons." ></td>
	<td class="line x" title="216:290	1.D. C is a feature indicator: An example sentence is Camera X is smaller than Camera Y, where smaller is the feature indicator for feature size." ></td>
	<td class="line x" title="217:290	In this case, we simply count the number of times (denoted by n+) that C appears in Pros and the number of times (denoted by n-) that C appears in Cons." ></td>
	<td class="line x" title="218:290	If n+  n-, we temporarily assign EntityS1 as the preferred entity." ></td>
	<td class="line x" title="219:290	Otherwise, we assign EntityS2 as the preferred entity." ></td>
	<td class="line x" title="220:290	Note that in some sentences, the entity features do not appear explicitly in the sentences but are implied." ></td>
	<td class="line x" title="221:290	The words that imply the features are called feature indicators." ></td>
	<td class="line x" title="222:290	246 Case 2: Type 2 Comparative or Superlative: There are two sub-cases: 2.A. Adjective/adverb in the comparison is opinionated: In this case, the feature F is not important." ></td>
	<td class="line x" title="223:290	An example sentence is: Car X has more beautiful interior than Car Y, more is an increasing comparative, and beautiful is the adjective with a positive orientation (the feature F is interior)." ></td>
	<td class="line x" title="224:290	Car X is clearly preferred in this case." ></td>
	<td class="line x" title="225:290	Another example is: Car X is more beautiful than Car Y." ></td>
	<td class="line x" title="226:290	In this case, beautiful is a feature indicator for the feature appearance." ></td>
	<td class="line x" title="227:290	Again, Car X is preferred." ></td>
	<td class="line x" title="228:290	This sub-case can be handled similarly as case 1.B. 2.B. adjective/adverb in the comparison is not opinionated: If the adjective/adverb in comparison is a feature indicator, we can use the method in 1.D. Otherwise, we form a context using the feature and adjective/adverb, and apply the method in 1.C. We then combine the result with the comparative word before the adjective/adverb to decide based on the rules in 1.B. Negations: The steps above temporarily determine which entity is the preferred entity." ></td>
	<td class="line x" title="229:290	However, a comparative sentence may contain a negation word or phrase (we have compiled 26 of them), e.g., Camera Xs battery life is not longer than that of Camera Y. Without considering not, Camera X is preferred." ></td>
	<td class="line x" title="230:290	After considering not, we assign the preferred entity to Camera Y." ></td>
	<td class="line x" title="231:290	This decision may be problematic because not longer does not mean shorter (thus it can also be seen to have no preference)." ></td>
	<td class="line x" title="232:290	5 Evaluation A system, called PCS (Preferred entities in Comparative Sentences), has been implemented based the proposed method." ></td>
	<td class="line x" title="233:290	Since there is no existing system that can perform the task, we could not compare with an existing approach." ></td>
	<td class="line x" title="234:290	Below, we first describe the evaluation datasets and then present the results." ></td>
	<td class="line x" title="235:290	5.1 Evaluation Datasets Our comparative sentence dataset consists of two subsets." ></td>
	<td class="line x" title="236:290	The first subset is from (Jindal and Liu 2006), which are product review and forum discussion sentences on digital cameras, DVD players, MP3 players, Intel vs AMD, Coke vs Pepsi, and Microsoft vs Google." ></td>
	<td class="line x" title="237:290	The original dataset used in (Jindal and Liu 2006) also contains many non-gradable comparative sentences, which are not used here as most such sentences do not express any preferences." ></td>
	<td class="line x" title="238:290	To make the data more diverse, we collected more forum discussion data about mobile phones from http://www.howardforums.com/, and reviews from amazon.com and cnet.com on products such as laptops, cameras and mobile phones." ></td>
	<td class="line x" title="239:290	Table 1 gives the number of sentences from these two sources." ></td>
	<td class="line x" title="240:290	Although we only have 837 comparative sentences, they were collected from thousands of sentences in reviews and forums." ></td>
	<td class="line x" title="241:290	About 10% of the sentences from them are comparative sentences." ></td>
	<td class="line x" title="242:290	Skewed Distribution: An interesting observation about comparative sentences is that a large proportion (based on our data) of them (84%) has EntityS1 as the preferred entity." ></td>
	<td class="line x" title="243:290	This means that when people make comparisons, they tend to put the preferred entities first." ></td>
	<td class="line x" title="244:290	Pros and Cons corpus: The Pros and Cons corpus was crawled from reviews of epinions.com." ></td>
	<td class="line x" title="245:290	It has 15162 Pros and 15162 Cons extracted from 15162 reviews of three types of products, i.e., digital cameras (8479), and printers (5778), and Strollers (905)." ></td>
	<td class="line x" title="246:290	Table 1." ></td>
	<td class="line x" title="247:290	Sentences from different sources Data Sources No." ></td>
	<td class="line x" title="248:290	of Comparative Sentences (Jindal and Liu 2006) 418 Reviews and forum posts 419 Total 837 5.2 Results The results on the whole dataset are given in Table 2." ></td>
	<td class="line x" title="249:290	Note that 84% of the sentences have EntityS1 as the preferred entity." ></td>
	<td class="line x" title="250:290	If a system does nothing but simply announces that EntityS1 is preferred, we will have the accuracy of 84%." ></td>
	<td class="line x" title="251:290	However, PCS using the OSA measure achieves the accuracy of 94.4%, which is much better than the baseline of taking the majority." ></td>
	<td class="line x" title="252:290	Since in skewed datasets accuracy does not reflect the prediction well, we will mainly use precision (Prec.), recall (Rec.)" ></td>
	<td class="line x" title="253:290	and F-score (F) in evaluation." ></td>
	<td class="line x" title="254:290	For the case that EntityS1 is preferred, the algorithm does extremely well." ></td>
	<td class="line x" title="255:290	For the case that EntityS2 is preferred, the algorithm also does well although not as well as for the EntityS1 case." ></td>
	<td class="line x" title="256:290	Based on our observation, we found that in such cases, the sentences are usually more complex." ></td>
	<td class="line x" title="257:290	Next, we compare with the case that the system does not use Pros and Cons (then OSA or PMI is not needed) (row 2)." ></td>
	<td class="line x" title="258:290	When a sentence requires context dependency handling, the system simply takes the majority as the default, i.e., 247 assigning EntityS1 as the preferred entity." ></td>
	<td class="line x" title="259:290	From the results in Table 2, we can see that F-scores are all worse." ></td>
	<td class="line x" title="260:290	In the case that EntityS1 is the preferred entity, taking defaults is not so bad, which is not surprising because of the skewed data distribution." ></td>
	<td class="line x" title="261:290	Even in this case, the precision improvement of PCS(OSA) is statistically significant at the 95% confidence level." ></td>
	<td class="line x" title="262:290	The recall is slight less but their difference is not statistically significant." ></td>
	<td class="line x" title="263:290	When EntityS2 is the preferred entity, its F-score (row 2) is much worse, which shows that our technique is effective." ></td>
	<td class="line x" title="264:290	The recall improvement of PCS (OSA) is dramatic (statistically significant at the 95% confidence level)." ></td>
	<td class="line x" title="265:290	The two precisions are not statistically different." ></td>
	<td class="line x" title="266:290	For OSA vs. PMI, see below." ></td>
	<td class="line x" title="267:290	Table 2: Preferred entity identification: whole data   EntityS1 Preferred EntityS2 Preferred Prec." ></td>
	<td class="line x" title="268:290	Rec." ></td>
	<td class="line x" title="269:290	F Prec." ></td>
	<td class="line x" title="270:290	Rec." ></td>
	<td class="line x" title="271:290	F PCS (OSA) 0.967 0.966 0.966 0.822 0.828 0.825 PCS: No Pros & Cons 0.925 0.980 0.952 0.848 0.582 0.690 PCS (PMI) 0.967 0.961 0.964 0.804 0.828 0.816 Now let us look at only the 187 sentences that need context dependency handling." ></td>
	<td class="line x" title="272:290	The data is still skewed." ></td>
	<td class="line x" title="273:290	72.2% of the sentences have EntityS1 as the preferred entities." ></td>
	<td class="line x" title="274:290	Table 3 shows the results of PCS with and without using Pros and Cons." ></td>
	<td class="line x" title="275:290	The results of PCS without Pros and Cons (OSA or PMI is not needed) are based on assigning EntityS1 as preferred for every sentence (taking the majority)." ></td>
	<td class="line x" title="276:290	Again, we can see that using external Pros and Cons (PCS(OSA)) helps dramatically." ></td>
	<td class="line x" title="277:290	Not surprisingly, the improvements are statistically significant except the recall when EntityS1 is preferred." ></td>
	<td class="line x" title="278:290	Table 3: Preferred entity identification with 187 sentences that need context dependency handling   EntityS1 Preferred EntityS2 Preferred Prec." ></td>
	<td class="line x" title="279:290	Rec." ></td>
	<td class="line x" title="280:290	F Prec." ></td>
	<td class="line x" title="281:290	Rec." ></td>
	<td class="line x" title="282:290	F PCS (OSA) 0.896 0.877 0.886 0.696 0.736 0.716 PCS: No Pros & Cons 0.722 1.000 0.839 0.000 0.000 0.000 PCS (PMI) 0.894 0.855 0.874 0.661 0.736 0.696 OSA vs. PMI: Comparing PCS(OSA) with PCS (PMI) (Table 3), OSA is better in F-score when EntityS1 is preferred by 1.2%, and better in Fscore when EntityS2 is preferred by 2%." ></td>
	<td class="line x" title="283:290	Although OSAs improvements over PMI are not large, we believe that in principle OSA is a more suitable measure." ></td>
	<td class="line x" title="284:290	Comparing with PMI when the whole dataset is used (Table 2), OSAs gains are less because the number of sentences requiring context dependency handling is small (22%)." ></td>
	<td class="line x" title="285:290	6 Conclusions This paper studied sentiments expressed in comparative sentences." ></td>
	<td class="line x" title="286:290	To our knowledge, no work has been reported on this topic." ></td>
	<td class="line x" title="287:290	This paper proposed an effective method to solve the problem, which also deals with context based sentiments by exploiting external information available on the Web." ></td>
	<td class="line x" title="288:290	To use the external information, we needed a measure of association of the comparative word and the entity feature." ></td>
	<td class="line x" title="289:290	A new measure, called one-side association (OSA), was then proposed." ></td>
	<td class="line x" title="290:290	Experimental results show that the technique produces accurate results." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1052
Textual Demand Analysis: Detection of Users% Wants and Needs from Opinions
Kanayama, Hiroshi;Nasukawa, Tetsuya;"></td>
	<td class="line x" title="1:193	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 409416 Manchester, August 2008 Textual Demand Analysis: Detection of Users Wants and Needs from Opinions Hiroshi Kanayama Tetsuya Nasukawa Tokyo Research Laboratory, IBM Japan, Ltd. 1623-14 Shimotsuruma, Yamato-shi, Kanagawa-ken, 242-8502 Japan {hkana,nasukawa}@jp.ibm.com Abstract This paper tackles textual demand analysis, the task of capturing what people want or need, rather than identifying what they like or dislike, on which much conventional work has focused." ></td>
	<td class="line x" title="2:193	It exploits syntactic patterns as clues to detect previously unknown demands, and requires domaindependent knowledge to get high recall." ></td>
	<td class="line x" title="3:193	To build such patterns we created an unsupervised pattern induction method relying on the hypothesis that there are commonly desired aspects throughout a domain corpus." ></td>
	<td class="line x" title="4:193	Experimental results show that the proposed method detects twice to four times as many demand expressions in Japanese discussion forums compared to a baseline method." ></td>
	<td class="line x" title="5:193	1 Introduction Increasingly we can access many opinions towards products, services, or companies through electronic documents including questionnaires, call logs, and other consumer-generated media (CGM) such as Internet discussion forums and blogs." ></td>
	<td class="line x" title="6:193	It is very important for companies to get insights from their customers opinions by analyzing such documents in large numbers." ></td>
	<td class="line x" title="7:193	The most popular way to utilize such data has involved sentiment analysis (SA) (Nasukawa and Yi, 2003; Yi et al., 2003), which is the task of recognizing the writers feelings as expressed in positive or negative comments." ></td>
	<td class="line x" title="8:193	Typically, a SA system focuses on expressions to identify the strong c2008." ></td>
	<td class="line x" title="9:193	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:193	Some rights reserved." ></td>
	<td class="line x" title="11:193	or weak points of the subjects as in (1) or in the writers evaluations as in (2)." ></td>
	<td class="line x" title="12:193	(1) I think the pictures are beautiful." ></td>
	<td class="line x" title="13:193	(2) I dont like this camera very much." ></td>
	<td class="line x" title="14:193	Here we call them polar expressions because they convey positive or negative polarities." ></td>
	<td class="line x" title="15:193	By counting the polar expressions related to products or services, one can quantitatively compare the goodness of competing services, find the drawbacks of specific products, and so on." ></td>
	<td class="line x" title="16:193	In addition to polar expressions, there are other types of expressions that provide valuable information, especially for the supplier side rather than the consumer side." ></td>
	<td class="line x" title="17:193	Examples (3) and (4) express the demands of the writers." ></td>
	<td class="line x" title="18:193	(3) Id be happy if it is equipped with a crisp LCD." ></td>
	<td class="line x" title="19:193	(4) Im waiting for a single-lens reflex less than 30,000 yen to come on the market." ></td>
	<td class="line x" title="20:193	We call such expressions demand expressions, and the underlined phrases demand targets. While sentiment analysis reveals evaluations of existing products or services, the task proposed here, textual demand analysis 1 , gives more direct suggestions to companies: things they should do to attract customers." ></td>
	<td class="line x" title="21:193	For example, by investigating demand targets, companies can add new functions to products on the market or plan new services to satisfy customers." ></td>
	<td class="line x" title="22:193	These activities should lead to positive evaluations in the future." ></td>
	<td class="line x" title="23:193	Interestingly, demand expressions may be noise in sentiment analysis, because the demand expressions do not actually convey positive or neg1 Note that textual demand analysis is different from the demand analysis in the field of marketing or software engineering." ></td>
	<td class="line x" title="24:193	409 Consumers Company Opinions a63 Textual Demand Analysis a63     Demands Outliner Syntactic Patterns Pattern Extraction Frequent Demand Instances a45 a27 a63a63 a63 Pattern Induction Figure 1: A demand analysis system and the flow of the pattern induction method." ></td>
	<td class="line x" title="25:193	ative evaluations of existing products or services, even though these demand expressions often contain positive or negative words, as in Example (3) which contains the positive expressions happy and crisp LCD." ></td>
	<td class="line x" title="26:193	The detection of novel demand targets requires deep syntactic information because such demand targets themselves can not be predefined." ></td>
	<td class="line x" title="27:193	For example, to regard the underlined parts of (3) and (4) as demand targets, the non-underlined parts of these sentences have to be properly interpreted as triggers." ></td>
	<td class="line x" title="28:193	This is a major difference from sentiment analysis where the polar expressions can be defined in the lexicon." ></td>
	<td class="line x" title="29:193	The left parts of Figure 1 illustrate the concepts of a system that visualizes the users demands described in the input opinion data, where the main analysis component processes the documents and extracts the demand targets." ></td>
	<td class="line x" title="30:193	The output of the system is created by a demand outliner, which the company uses to grasp the trends of consumers demands." ></td>
	<td class="line x" title="31:193	The syntactic patterns that can be used as clues to demand expressions depend on the topic domain or the writing style." ></td>
	<td class="line x" title="32:193	To organize this linguistic knowledge we propose an unsupervised induction method for syntactic patterns." ></td>
	<td class="line x" title="33:193	The right part of Figure 1 shows the flow of pattern induction." ></td>
	<td class="line x" title="34:193	In the next section, we review related work, and Section 3 defines our task more formally." ></td>
	<td class="line x" title="35:193	In Section 4 we describe a naive approach to the task and Section 5 shows a form of unsupervised pattern induction used to cover more demand expressions." ></td>
	<td class="line x" title="36:193	Section 6 gives the experimental results and we conclude in Section 7." ></td>
	<td class="line x" title="37:193	2 Related Work Sentiment analysis (SA) and related topics have been extensively studied in recent years." ></td>
	<td class="line x" title="38:193	The textual demand analysis proposed in this paper shares some properties with phrase-level SA, the detection of sentiments and evaluations expressed in phrases, rather than document-level SA, the classification of documents in terms of goodness of reputation." ></td>
	<td class="line x" title="39:193	Yi et al.(2003) pointed out that the multiple sentiment aspects in a document should be extracted, and Nasukawa and Yi (2003) clarified the need for deep syntactic analysis for the phrase-level SA." ></td>
	<td class="line oc" title="41:193	The acquisition of clues is a key technology in these research efforts, as seen in learning methods for document-level SA (Hatzivassiloglou and McKeown, 1997; Turney, 2002) and for phraselevel SA (Wilson et al., 2005; Kanayama and Nasukawa, 2006)." ></td>
	<td class="line x" title="42:193	As well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (Wiebe and Mihalcea, 2006), comparative sentences (Jindal and Liu, 2006), or predictive expressions (Kim and Hovy, 2007)." ></td>
	<td class="line x" title="43:193	However, the extraction of the contents of writers demands which this paper handles is less studied while this type of information is very valuable for commercial applications." ></td>
	<td class="line x" title="44:193	For the tasks of information extraction and relation extraction, bootstrapping approaches have been proven successful (Yangarber, 2003; Pantel and Pennacchiotti, 2006)." ></td>
	<td class="line x" title="45:193	The pattern induction method in this paper exploits their ideas, but their application to the demand detection is not trivial, because some instances of demands are previously unknown and do not appear frequently, so they have to be abstracted effectively." ></td>
	<td class="line x" title="46:193	The work by Inui et al.(2003) handles semantics of a type similar to ours." ></td>
	<td class="line x" title="48:193	They aimed to detect the requests in the responses to open-ended questionnaires, seeking direct requests such as   ([I] would like you to ) and other forms which can be paraphrased as direct requests." ></td>
	<td class="line x" title="49:193	They classified sentences into requests or non-requests, where their source documents were responses to questionnaires, and where more than 60% of the utterances could be regarded as requests of some sort." ></td>
	<td class="line x" title="50:193	In contrast, our method detects the content of the demands in the form of noun phrases, and handles more general target documents including 410 CGM documents that contain much more diverse utterances." ></td>
	<td class="line x" title="51:193	3 Task Definition As shown in Section 1, our goal is to create a system to enumerate in an easily understandable way the demand targets in the input text." ></td>
	<td class="line x" title="52:193	This section describes the definition of a demand target and its representation format." ></td>
	<td class="line x" title="53:193	3.1 Demand targets Demands or requests written in opinion texts can be represented by verb phrases as in e.g. I want to V. and I want you to V., or noun phrases as in I want N. 2 In this paper we focus on the last type, i.e. noun phrases which represent desired objects, because they are easier to aggregate and grasp than verb phrases." ></td>
	<td class="line x" title="54:193	Another reason is that some demands represented with a verb phrase only describe the objects that are desired." ></td>
	<td class="line x" title="55:193	For example, I want to buy N and I want you to provide N can be simply interpreted as meaning that N is what the writer wants." ></td>
	<td class="line x" title="56:193	We call such a noun phrase a demand target, and these are the outputs of our system." ></td>
	<td class="line x" title="57:193	For applications, the demand targets to be detected by the system depend on the type of input documents." ></td>
	<td class="line x" title="58:193	For example, from a consumers forum on digital cameras, the underlined parts in Examples (3) and (4) from Section 1 apparently describe the writers demands, so they are valuable information for such users of demand analysis such as the makers of digital camera." ></td>
	<td class="line x" title="59:193	However, the request in Example (5) does not express the authors demands about any digital camera, but rather it is written for other participants in the forum." ></td>
	<td class="line x" title="60:193	This type should be excluded from the output." ></td>
	<td class="line x" title="61:193	(5) Please give me a good advice." ></td>
	<td class="line x" title="62:193	In contrast, when the responses to a questionnaire about the activities of an organization are processed, statements such as Example (5) should be regarded as a demand target, since the writer wrote it as a request to the sponsor of the questionnaire and the advice is indeed a thing that can be provided by the sponsoring organization." ></td>
	<td class="line x" title="63:193	3.2 Representation of demand targets A demand target tends to be expressed by a noun phrase with modifiers, as seen in Examples (3) and 2 V and N indicate a verb phrase and a noun phrase, respectively." ></td>
	<td class="line x" title="64:193	7 happy  if !" ></td>
	<td class="line x" title="65:193	equip   -NOM 2J LCD  db crisp  standpoint  F I  want H sell # -ACC -) reflex  T!" ></td>
	<td class="line x" title="66:193	can buy  with 30,0003 -yen 15 single-lens Figure 2: Syntactic trees for the sentences (6) and (7)." ></td>
	<td class="line x" title="67:193	* indicates the headword of the demand targets." ></td>
	<td class="line x" title="68:193	(4), rather than by a single noun." ></td>
	<td class="line x" title="69:193	The headwords of such phrases (e.g. LCD in (3) and reflex in (4)) represent the main categories of the demanded objects, but they are not distinctive enough to recognize as knowledge of the authors demands." ></td>
	<td class="line x" title="70:193	Therefore the key task of this research was to find ways to markup the headword of a noun phrase that represents the content of a demand in the syntactic parse tree." ></td>
	<td class="line x" title="71:193	Examples (6) and (7) are the original Japanese sentences corresponding to Examples (3) and (4)." ></td>
	<td class="line x" title="72:193	(6)Fdb2J !7 Id be happy if it is equipped with a crisp LCD. (7)Z3T!15-)#H [Im] waiting for a single-lens reflex less than 30,000 yen to come on the market. Figure 2 represents the parse trees corresponding to sentences (6) and (7), where the demand targets are identified by the mark *." ></td>
	<td class="line x" title="73:193	This simple representation is advantageous for both the collection of and the deeper investigation of the demand targets." ></td>
	<td class="line x" title="74:193	One can easily grasp the content of a demand if the application shows the whole surface structure of the subtree under * in the tree, e.g. the underlined parts of Examples (6) and (7)." ></td>
	<td class="line x" title="75:193	At the same time the tree structure supports the further analysis of the trends of the demands by picking up the headwords or modifiers prominent in the subtrees that were detected as demand targets." ></td>
	<td class="line x" title="76:193	4 Baseline Method of Textual Demand Analysis This section describes an algorithm to extract demand targets with high precision and describes a preliminary experiment to measure the performance." ></td>
	<td class="line x" title="77:193	411 N    -ACC ] want (a) V  that E think (b) V  though V (c) Figure 3: (a) is a demand pattern which indicates that the noun in N  is detected as a demand target." ></td>
	<td class="line x" title="78:193	(b) and (c) are auxiliary patterns, where V indicates the node matches any verb." ></td>
	<td class="line x" title="79:193	4.1 Syntactic patterns and top-down matching A major purpose of textual demand analysis is to discover novel demands embedded in the text, thus the triggers of their detection should not be a predefined set of demand targets but should be their surrounding syntactic information." ></td>
	<td class="line x" title="80:193	We use two types of syntactic patterns shown in Figure 3." ></td>
	<td class="line x" title="81:193	Those patterns are compared with the syntactic tree as the parsing result of the input sentence." ></td>
	<td class="line x" title="82:193	The pattern (a) in Figure 3 is a demand pattern, which is used to search for demand targets." ></td>
	<td class="line x" title="83:193	The node with the  mark indicates the corresponding node will be the headword of a demand target." ></td>
	<td class="line x" title="84:193	Hence we write the pattern (a) as N  - -]  for simplicity." ></td>
	<td class="line x" title="85:193	The patterns are applied in a top-down manner, that is, initially the top node of the input tree is examined to see whether or not the node and its combination of children nodes match with one of the patterns in the pattern repository." ></td>
	<td class="line x" title="86:193	This method supports higher precision in the detection than the surface pattern matching." ></td>
	<td class="line x" title="87:193	For example, the expression WaV ]K  (There is no one who wants low quality goods) should not be misunderstood to express a demand." ></td>
	<td class="line x" title="88:193	The patterns (b) and (c) in Figure 3 are auxiliary patterns." ></td>
	<td class="line x" title="89:193	These are used to apply the demand patterns to nodes other than the root of the syntactic tree." ></td>
	<td class="line x" title="90:193	For example, by applying the patterns (b) and (c), the pattern (a) can then be applied to the expressions N ]E (I think that I want N) and N ]" ></td>
	<td class="line x" title="91:193		 ; O  (Though I want N, I dont have enough money), respectively, even though N   -] doesnt appear at the top of the trees." ></td>
	<td class="line x" title="92:193	In other words, the auxiliary patterns contribute to generate variations of the demand patterns." ></td>
	<td class="line x" title="93:193	In addition, simple rules can be applied to filter out certain meaningless outputs." ></td>
	<td class="line x" title="94:193	When a noun phrase that matched to the  part of the demand Table 1: The result on the small gold standard with DP 1 . PM signifies surface pattern matching, TM signifies tree matching." ></td>
	<td class="line x" title="95:193	+AP means that auxiliary patterns are used." ></td>
	<td class="line x" title="96:193	Method Precision Recall PM 39% (14/36) 25% (14/56) TM 92% (11/12) 20% (11/56) TM+AP 94% (17/18) 30% (17/56) pattern was a pronoun or very common noun (e.g. camera in the camera domain) without any modifier, it is not output as a demand target." ></td>
	<td class="line x" title="97:193	4.2 Preliminary experiment We conducted a preliminary experiment to assess the feasibility of our approach." ></td>
	<td class="line x" title="98:193	We prepared a small gold-standard dataset which consists of 1,152 sentences from a discussion forum on digital cameras, for which two human annotators attached marks to the demand targets." ></td>
	<td class="line x" title="99:193	There were 56 demand targets that at least one of the annotators detected, and the sentencelevel agreement value 3 was  =0.73, which is regarded as a good level of agreement." ></td>
	<td class="line x" title="100:193	There was no sentence in which the two annotators attached marks to different nouns." ></td>
	<td class="line x" title="101:193	First, we made a minimum set of demand patterns DP 1 , which contained only one basic pattern N  - -] 4  (I want N  )." ></td>
	<td class="line x" title="102:193	To see the effect of the top-down matching and the auxiliary patterns described in Section 4.1, demand targets in the gold-standard corpus were automatically detected using three methods: pattern matching with surface strings like  ] (PM), tree matching without the auxiliary patterns (TM), and tree matching with the auxiliary patterns 5 (TM+AP)." ></td>
	<td class="line x" title="103:193	Table 1 shows the results." ></td>
	<td class="line x" title="104:193	The top-down matching on the syntactic tree resulted in much higher precision than the surface pattern matching, and the auxiliary patterns improved the recall." ></td>
	<td class="line x" title="105:193	The only misdetection in the tree matching method was due to an error in the sentence segmentation." ></td>
	<td class="line x" title="106:193	However, all of them show low recall values, 3 The agreement on whether or not the sentence has a demand target." ></td>
	<td class="line x" title="107:193	4 Apparent character variations like ] and , and alternative forms of particles were aggregated in the parsing process." ></td>
	<td class="line x" title="108:193	5 A total of 95 auxiliary patterns which Kanayama et al.(2004) used for the sentiment analysis." ></td>
	<td class="line x" title="110:193	412 Table 2: The list of augmented demand patterns DP q . N  - -](I want N  ), N  -#-Y(I hope N  ), N  -#-	6-!(Please [give] N  ), N  -#-6 (I wish N  ), N  -#--4
(Please do N  ), N  -#-^(I ask [you] N  ), N  - -!- (N  should be), N  -#--P(Please do N  ) Table 3: The result with the minimum set of demand patterns DP 1 and the larger set DP q . Patterns Precision Recall DP 1 94% (17/18) 30% (17/56) DP q 78% (18/22) 32% (18/56) since only one demand pattern was used." ></td>
	<td class="line x" title="111:193	To make the recall higher, we created another set of demand patterns DP q listed in Table 2, which are generally used as clues for the request expressions in the analysis of responses to open-ended questionnaires." ></td>
	<td class="line x" title="112:193	They are derived from the previous work on request classification (Yamamoto et al., 2007)." ></td>
	<td class="line x" title="113:193	The result in Table 3 shows that the patterns newly added in DP q do not perform well." ></td>
	<td class="line x" title="114:193	This is because these patterns appear in responses to questionnaires but are not suitable for the writing styles used in discussion forums, as mentioned in Section 3.1." ></td>
	<td class="line x" title="115:193	Therefore we used the TM+AP method with the minimum pattern set DP 1 as the baseline in this paper rather than the pattern set DP q . 5 Automatic Pattern Induction The preliminary experiment in Section 4.2 showed that high precision can be obtained by the topdown matching method, and at the same time, revealed the difficulty in building demand patterns to achieve high recall." ></td>
	<td class="line x" title="116:193	To overcome this problem, we devised an automatic pattern induction algorithm." ></td>
	<td class="line x" title="117:193	5.1 Frequent fragments of demand targets Here we make an assumption that there are commonly desired aspects or things throughout a domain corpus." ></td>
	<td class="line x" title="118:193	Based on this assumption, we extract the syntactic fragments which appear relatively frequently as the elements of demand targets from the training corpus in a specific domain." ></td>
	<td class="line x" title="119:193	First we obtain demand targets from the domain corpus, in this case from the discussion forum on digital cameras, by using the baseline method with -) reflex  T!" ></td>
	<td class="line x" title="120:193	can buy  with 30,0003 -yen 15 single-lens -)-) T!15 -) 15 -) T!" ></td>
	<td class="line x" title="121:193	 -) T!" ></td>
	<td class="line x" title="122:193	N T!15 N T!" ></td>
	<td class="line x" title="123:193	 N T!" ></td>
	<td class="line x" title="124:193	N 15 Figure 4: Sample extraction of demand instances from a demand target detected by the system." ></td>
	<td class="line x" title="125:193	N denotes the wildcard for any nouns." ></td>
	<td class="line x" title="126:193	the pattern set DP 1 . Next, demand instances are extracted from each demand target." ></td>
	<td class="line x" title="127:193	A demand instance is a one-to-three-node subtree of a demand target, and shares the root node with the demand target." ></td>
	<td class="line x" title="128:193	The root node that is modified by one or more nodes can be replaced with a wildcard." ></td>
	<td class="line x" title="129:193	Figure 4 shows a sample extraction of demand instances from a demand target Z3T!1 5-) (single-lens reflex less than 30,000 yen), where nine demand instances are extracted, and four of them have a wildcard at the root node." ></td>
	<td class="line x" title="130:193	The demand instances which appear more than  f times in the corpus are selected as frequent demand instances (FDIs), and each FDI is assigned the following reliability value r i : r i = freq DT (i) freq(i) (8) where freq(i) denotes the frequency of the instance subtree in the whole corpus and freq DT (i) means the is frequency in the demand targets." ></td>
	<td class="line x" title="131:193	The notion of an instances reliability is inspired by the method of relation extraction (Pantel and Pennacchiotti, 2006), but our usage is different from theirs." ></td>
	<td class="line x" title="132:193	Here we use the reliability value only for the relative comparison among demand instances, so normalization of the values is not needed." ></td>
	<td class="line x" title="133:193	The intrinsic difference from the instance of relation extraction is that the demand instances are not the final outputs of the extraction, but are just triggers for new demand patterns." ></td>
	<td class="line x" title="134:193	When  f was set to 5, a total of 42 FDIs which had reliabilities above 0.01 were picked from 150,000 postings in the discussion forum on digital cameras." ></td>
	<td class="line x" title="135:193	Table 4 shows examples of FDIs." ></td>
	<td class="line x" title="136:193	5.2 Frequent patterns as the clue The FDIs with high reliability correspond to aspects which are likely to be demanded, therefore the syntactic structures which often govern such 413 Table 4: Examples of frequent demand instances (FDIs) with r i > 0.01 in the digital camera domain." ></td>
	<td class="line x" title="137:193	- denotes the split of nodes." ></td>
	<td class="line x" title="138:193	'!-(&$+(digital camera which can) --/'(mm lens),><9(newer model), a-(good thing),[!--/'(sharp lens), db-N (beautiful N),*%.-N (macro N),  ] want H sell # -ACC $+, camera !" ></td>
	<td class="line x" title="139:193	can I
 small   -ACC LGclose-up Nearly Figure 5: An example of extraction of a candidate demand pattern." ></td>
	<td class="line x" title="140:193	From the sentence I
 LG !$+,#NH] (I want a small camera with the close-up function sold earlier), the CDP N  -#-H-] (the oval) is extracted, triggered by the FDI  !-$+, (the square)." ></td>
	<td class="line x" title="141:193	FDIs are expected to be clues for detecting additional demands." ></td>
	<td class="line x" title="142:193	Following this expectation, the candidate demand patterns (hence CDPs) are extracted." ></td>
	<td class="line x" title="143:193	A CDP is a subtree that connects the head of an FDI and the root of the syntactic tree, or auxiliary patterns which cover the root of the tree." ></td>
	<td class="line x" title="144:193	A CDP forms a node sequence without a branch, and corresponds to a sentence-final expression in Japanese that usually conveys modality information." ></td>
	<td class="line x" title="145:193	Figure 5 illustrates the extraction of a CDP from a syntactic tree triggered by an FDI." ></td>
	<td class="line x" title="146:193	For each CDP extracted in this way, the reliability is determined by Equation (9): r p = summationdisplay iFDI freq(i,p)  r i freq(p) (9) where freq(i,p) denotes the frequency of the collocation of the instance i and the pattern p, and freq(p) is the frequency of p in the entire corpus." ></td>
	<td class="line x" title="147:193	These ratios are summed up over all of the FDIs, weighted by the reliability of the instance." ></td>
	<td class="line x" title="148:193	Also r p is used only for the relative comparison among CDPs, so it is not normalized to be in the range [0,1]." ></td>
	<td class="line x" title="149:193	Table 5 shows the CDPs which appeared 10 times or more and their reliability values." ></td>
	<td class="line x" title="150:193	Some Table 5: The extracted candidate demand patterns (CDPs) sorted by their reliability." ></td>
	<td class="line x" title="151:193	Candidate Demand Pattern Reliability N  - -'-a1.70 10 2 (be good if it includes N  ) N  -#-T-4
(please buy N  ) 1.48 10 2 N  - -!-a1.12 10 2 (be good if it includes N  ) N  - -!-X_4.81 10 3 (be convenient if it includes N  ) N  -#-T	-E-!3.32 10 3 ([Im] going to buy N  ) N  - -'-X_3.00 10 3 (be convenient if it includes N  ) N  -#-H-]1.88 10 3 ([I] want N  to be sold) N  - -8Y-(N  is longed for) 1.34 10 3 N  -#-M!([I] recommend N  ) 1.06 10 3 N  --AS-#-=R-!8.92 10 4 ([Im] thinking about buying N  ) N  - -WO!(N  is lacking) 3.53 10 4 . . ." ></td>
	<td class="line x" title="152:193	N  -#-D(to use N  ) 5.28 10 5 N  -#-AS!(to purchase N  ) 4.31 10 5 N  --C!(to take [pictures] with N  ) 6.06 10 6 . . ." ></td>
	<td class="line x" title="153:193	of them apparently reflect the writing style of the discussion forum and the digital camera domain." ></td>
	<td class="line x" title="154:193	The effect of these patterns will be verified in Section 6." ></td>
	<td class="line x" title="155:193	6 Evaluation We conducted experiments to assess the contributions of the candidate demand patterns acquired in Section 5." ></td>
	<td class="line x" title="156:193	6.1 Experimental setup In Section 4.2 we created a gold-standard dataset with human annotations, however, the number of annotation is not enough to fairly compare the several methods due to the sparseness of the demand targets in the original corpus, and it would be very laborious to prepare a larger annotated dataset." ></td>
	<td class="line x" title="157:193	Therefore we used an unannotated corpus for the evaluation in this section." ></td>
	<td class="line x" title="158:193	A total of 50,000 postings in the digital camera forum 6 were processed by each method, and 100 demand targets were randomly selected from the system output for each trial and a human evaluator decided for each demand target whether or not it referred to any demanded object related to the domain." ></td>
	<td class="line x" title="159:193	6 They are separate from the 150,000 postings used for the training." ></td>
	<td class="line x" title="160:193	414 Table 6: The evaluations when CDPs with reliability more than  were used." ></td>
	<td class="line x" title="161:193	 Precision Recall F1  93% 30% 0.45 10 2 91% 31% 0.46 10 3 87% 37% 0.52 10 4 68% 59% 0.63 10 5 33% 57% 0.42 In this way, the precision can be computed directly, and the recall can be estimated as follows: Rec(T) similarequal Num(T)Prec(T)Rec(B) Num(B)Prec(B) (10) where Rec(), Prec(), and Num() denote the recall, the precision, and the number of demand targets detected by the system in the entire test corpus, respectively, and T and B denote the tested method and the baseline method, respectively." ></td>
	<td class="line x" title="162:193	Prec(B) is assumed to be 30% as observed in the preliminary experiment." ></td>
	<td class="line x" title="163:193	6.2 Effect of new demand patterns The CDFs for the digital camera domain that were acquired with the method in Section 5 are tested by varying the threshold ." ></td>
	<td class="line x" title="164:193	The CDFs which have reliability value more than  were added to the demand pattern set." ></td>
	<td class="line x" title="165:193	Table 6 shows the results." ></td>
	<td class="line x" title="166:193	The baseline method was without any newly acquired demand patterns (i.e.  = ), thus it is the same condition as the DP 1 in the preliminary experiment in Section 4.2." ></td>
	<td class="line x" title="167:193	When  was set to 10 3 , the recall increased drastically with little harm to the precision." ></td>
	<td class="line x" title="168:193	The value of  =10 5 did not work well because the precision was very low and the increase of the recall was limited." ></td>
	<td class="line x" title="169:193	The value  =10 4 performed best in terms of the F1 value." ></td>
	<td class="line x" title="170:193	We observed the demand targets derived from the new demand patterns." ></td>
	<td class="line x" title="171:193	In most cases desirable functions and features of the digital cameras were successfully obtained from conditional positive expressions such as N  - -!-X_ (be convenient if it includes N  )." ></td>
	<td class="line x" title="172:193	Also, preferred machines were clarified by the expression N  -#T-4
 (please buy N  ) which is in a postings to recommend something to other users." ></td>
	<td class="line x" title="173:193	On the other hand, the expression N  - -WO !(N  is lacking) tend to result in noisy demand targets." ></td>
	<td class="line x" title="174:193	Table 7: The extracted CDPs and their reliability for the domain of companys questionnaire." ></td>
	<td class="line x" title="175:193	Candidate Demand Pattern Reliability N  -#-Y([I] hope N  ) 3.10 10 2 N  -#-6-1.37 10 2 ([I] want to ask for N  ) N  -#-6([I] wish N  ) 4.92 10 3 N  - -Y-'!(N  is hoped for) 1.45 10 3 N  -#-Q:-]1.04 10 3 ([I] want N  to be provided) N  - -U\--@!6.72 10 4 ([I] think N  is necessary) . . ." ></td>
	<td class="line x" title="176:193	N  - -WO-!(N  is lacking) 1.02 10 4 N  - -0(N  is bad) 7.22 10 5 . . ." ></td>
	<td class="line x" title="177:193	We also tried the iterative acquisition using the newly acquired patterns, but the useful patterns were rarely acquired in the second step." ></td>
	<td class="line x" title="178:193	This is because FDIs cannot be definitive triggers, and the first seed pattern N  - -] (I want N  ) was a prominently reliable pattern compared with the other demand patterns." ></td>
	<td class="line x" title="179:193	6.3 Applicability to other demand targets The pattern induction method was expected to have advantage that domain-dependent patterns can be acquired, and indeed some of the patterns were specific for the digital camera domain as shown in Table 5." ></td>
	<td class="line x" title="180:193	To see the applicability of our algorithm to other domains or other types of text, the pattern induction was tested on another corpus." ></td>
	<td class="line x" title="181:193	The responses to a questionnaire about collaboration process in a company were used as the corpus." ></td>
	<td class="line x" title="182:193	Unlike the discussion forum on digital cameras, the writing style of direct request such as Please provide N   was observed frequently, and the demand targets are much more dense in the corpus." ></td>
	<td class="line x" title="183:193	Table 7 shows the CDPs acquired in this domain, and Table 8 shows the evaluation where 25,000 and 5,000 sentences were used for the training and the test, respectively." ></td>
	<td class="line x" title="184:193	As a result, higher precision was achieved in this domain than in the digital camera domain, because the demands are stated more explicitly in the responses to the questionnaires." ></td>
	<td class="line x" title="185:193	Unlike in the digital camera domain, the pattern N  - -WO- !(N  is lacking) worked well because in many cases of this domain what are lacking equal to what are needed." ></td>
	<td class="line x" title="186:193	For example, ?`acB  WO! (effective discussion is lack415 Table 8: The evaluations for the company questionnaire domain." ></td>
	<td class="line x" title="187:193	The initial recall was estimated as 15%.DC10 4 means that the CDPs for the digital camera domain ( =10 4 ) were used." ></td>
	<td class="line x" title="188:193	 Precision Recall F1  98% 15% 0.26 10 2 96% 24% 0.39 10 3 92% 30% 0.45 10 4 85% 71% 0.77 10 5 41% 73% 0.53 DC10 4 72% 40% 0.51 ing) implies that the effective discussion is a demand." ></td>
	<td class="line x" title="189:193	When the demand patterns acquired in the digital camera domain (DC10 4 ) were used, the increase of the recall was limited." ></td>
	<td class="line x" title="190:193	These results support the value of the unsupervised pattern induction method which works for any domain when only a raw domain corpus is provided." ></td>
	<td class="line x" title="191:193	7 Conclusion We formalized the task textual demand analysis and proposed a pattern induction method to increase the coverage of the automatic detection of demand targets." ></td>
	<td class="line x" title="192:193	The pattern induction proposed here allows for the discovery of novel demands that can be represented by various forms of noun phrases, though they were triggered by frequently appeared syntactic fragments." ></td>
	<td class="line x" title="193:193	Beyond sentiment analysis, textual demand analysis provides valuable knowledge for industries, clarifying not only the favorable aspects in the current products, but also the essential features in the future." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1103
Topic Identification for Fine-Grained Opinion Analysis
Stoyanov, Veselin;Cardie, Claire;"></td>
	<td class="line x" title="1:219	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 817824 Manchester, August 2008 Topic Identification for Fine-Grained Opinion Analysis Veselin Stoyanov and Claire Cardie Department of Computer Science Cornell University {stoyanov,cardie}@cs.cornell.edu Abstract Within the area of general-purpose finegrained subjectivity analysis, opinion topic identification has, to date, received little attention due to both the difficulty of the task and the lack of appropriately annotated resources." ></td>
	<td class="line x" title="2:219	In this paper, we provide an operational definition of opinion topic and present an algorithm for opinion topic identification that, following our new definition, treats the task as a problem in topic coreference resolution." ></td>
	<td class="line x" title="3:219	We develop a methodology for the manual annotation of opinion topics and use it to annotate topic information for a portion of an existing general-purpose opinion corpus." ></td>
	<td class="line x" title="4:219	In experiments using the corpus, our topic identification approach statistically significantly outperforms several non-trivial baselines according to three evaluation measures." ></td>
	<td class="line x" title="5:219	1 Introduction Subjectivity analysis is concerned with extracting information about attitudes, beliefs, emotions, opinions, evaluations, sentiment and other private states expressed in texts." ></td>
	<td class="line oc" title="6:219	In contrast to the problem of identifying subjectivity or sentiment at the document level (e.g. Pang et al.(2002), Turney (2002)), we are interested in fine-grained subjectivity analysis, which is concerned with subjectivity at the phrase or clause level." ></td>
	<td class="line x" title="8:219	We expect fine-grained subjectivity analysis to be useful for question-answering, summarization, information extraction and search engine support for queries of c2008." ></td>
	<td class="line x" title="9:219	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:219	Some rights reserved." ></td>
	<td class="line x" title="11:219	the form How/what does entity X feel/think about topic Y?, for which document-level opinion analysis methods can be problematic." ></td>
	<td class="line x" title="12:219	Fine-grained subjectivity analyses typically identify SUBJECTIVE EXPRESSIONS in context, characterize their POLARITY (e.g. positive, neutral or negative) and INTENSITY (e.g. weak, medium, strong, extreme), and identify the associated SOURCE, or OPINION HOLDER, as well as the TOPIC, or TARGET, of the opinion." ></td>
	<td class="line x" title="13:219	While substantial progress has been made in automating some of these tasks, opinion topic identification has received by far the least attention due to both the difficulty of the task and the lack of appropriately annotated resources.1 This paper addresses the problem of topic identification for fine-grained opinion analysis of general text.2 We begin by providing a new, operational definition of opinion topic in which the topic of an opinion depends on the context in which its associated opinion expression occurs." ></td>
	<td class="line x" title="14:219	We also present a novel method for general-purpose opiniontopicidentificationthat,followingournewdefinition, treats the problem as an exercise in topic coreference resolution." ></td>
	<td class="line x" title="15:219	We evaluate the approach using the existing MPQA corpus (Wiebe et al., 2005), which we extend with manual annotations that encode topic information (and refer to hereafter as the MPQATOPIC corpus)." ></td>
	<td class="line x" title="16:219	Inter-annotator agreement results for the manual annotations are reasonably strong across a number of metrics and the results of experiments that evaluate our topic identification method in the contextoffine-grainedopinionanalysisarepromising: 1Section 3 on related work provides additional discussion." ></td>
	<td class="line x" title="17:219	2The identification of products and their components and attributes from product reviews is a related, but quite different task from that addressed here." ></td>
	<td class="line x" title="18:219	Section 3 briefly discusses, and provides references, to the most relevant research in that area." ></td>
	<td class="line x" title="19:219	817 using either automatically or manually identified topic spans, we achieve topic coreference scores that statistically significantly outperform two topic segmentation baselines across three coreference resolutionevaluationmeasures(B3, andCEAF)." ></td>
	<td class="line x" title="20:219	For the B3 metric, for example, the best baseline achieves a topic coreference score on the MPQATOPIC corpus of 0.55 while our topic coreference algorithm scores 0.57 and 0.71 using automatically, and manually, identified topic spans, respectively." ></td>
	<td class="line x" title="21:219	In the remainder of the paper, we define opinion topics (Section 2), present related work (Section 3), and motivate and describe the key idea of topic coreference that underlies our methodology for both the manual and automatic annotation of opinion topics (Section 4)." ></td>
	<td class="line x" title="22:219	Creation of the MPQATOPIC corpus is described in Section 5 andourtopicidentificationalgorithm, inSection6." ></td>
	<td class="line x" title="23:219	The evaluation methodology and results are presented in Sections 7 and 8, respectively." ></td>
	<td class="line x" title="24:219	2 Definitions and Examples Consider the following opinion sentences: (1)[OH John] adores [TARGET+TOPICSPAN Marseille] and visits it often." ></td>
	<td class="line x" title="25:219	(2)[OH Al] thinks that [TARGETSPAN [TOPICSPAN?" ></td>
	<td class="line x" title="26:219	the government] should [TOPICSPAN?" ></td>
	<td class="line x" title="27:219	tax gas] more in order to [TOPICSPAN?" ></td>
	<td class="line x" title="28:219	curb [TOPICSPAN?" ></td>
	<td class="line x" title="29:219	CO2 emissions]]]." ></td>
	<td class="line x" title="30:219	Afine-grainedsubjectivityanalysisshouldidentify: the OPINION EXPRESSION3 as adores in Example 1 and thinks in Example 2; the POLARITY as positive in Example 1 and neutral in Example 2; the INTENSITY asmediumandlow,respectively; and the OPINION HOLDER (OH) as John and Al, respectively." ></td>
	<td class="line x" title="31:219	To be able to discuss the opinion TOPIC in each example, we begin with three definitions:  Topic." ></td>
	<td class="line x" title="32:219	The TOPIC of a fine-grained opinion is thereal-worldobject,eventorabstractentitythatis the subject of the opinion as intended by the opinion holder." ></td>
	<td class="line x" title="33:219	 Topicspan." ></td>
	<td class="line x" title="34:219	The TOPIC SPAN associated with an OPINION EXPRESSION is the closest, minimal span of text that mentions the topic." ></td>
	<td class="line x" title="35:219	 Target span." ></td>
	<td class="line x" title="36:219	In contrast, we use TARGET SPAN to denote the span of text that covers the syntactic 3For simplicity, we will use the term opinion throughout the paper to cover all types of private states expressed in subjective language." ></td>
	<td class="line x" title="37:219	surface form comprising the contents of the opinion." ></td>
	<td class="line x" title="38:219	In Example 1, for instance, Marseille is both the TOPIC SPAN and the TARGET SPAN associated with thecityofMarseille,whichisthe TOPIC oftheopinion." ></td>
	<td class="line x" title="39:219	In Example 2, the TARGET SPAN consists of the text that comprises the complement of the subjective verb thinks." ></td>
	<td class="line x" title="40:219	Example 2 illustrates why opinion topic identification is difficult: within the single target span of the opinion, there are multiple potential topics, each identified with its own topic span." ></td>
	<td class="line x" title="41:219	Without more context, however, it is impossible to know which phrase indicates the intended topic." ></td>
	<td class="line x" title="42:219	If followed by sentence 3, however, (3)Although he doesnt like government-imposed taxes, he thinks that a fuel tax is the only effective solution." ></td>
	<td class="line x" title="43:219	the topic of Als opinion in 2 is much clearer  it is likely to be fuel tax, denoted via the TOPIC SPAN tax gas or tax." ></td>
	<td class="line x" title="44:219	3 Related Work As previously mentioned, there has been much recent progress in extracting fine-grained subjectivity information from general text." ></td>
	<td class="line x" title="45:219	Previous efforts have focused on the extraction of opinion expressions in context (e.g. Bethard et al.(2004), Breck et al.(2007)), the assignment of polarity to these expressions (e.g. Wilson et al.(2005), Kim and Hovy (2006)), source extraction (e.g. Bethard et al.(2004), Choi et al.(2005)), and identification of the source-expresses-opinion relation (e.g. Choi et al.(2006)), i.e. linking sources to the opinions that they express." ></td>
	<td class="line x" title="52:219	Not surprisingly, progress has been driven by the creation of language resources." ></td>
	<td class="line x" title="53:219	In this regard, Wiebe et al.s (2005) opinion annotation scheme for subjective expressions was used to create the MPQA corpus, which consists of 535 documents manuallyannotatedforphrase-levelexpressionsof opinions, their sources, polarities, and intensities." ></td>
	<td class="line x" title="54:219	Although other opinion corpora exist (e.g. Bethard et al.(2004), Voorhees and Buckland (2003), the product review corpora of Liu4), we are not aware of any corpus that rivals the scale and depth of the MPQA corpus." ></td>
	<td class="line x" title="56:219	In the related area of opinion extraction from product reviews, several research efforts have focused on the extraction of the topic of the opinion (e.g. Kobayashi et al.(2004), Yi et al.(2003), 4http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html 818 Popescu and Etzioni (2005), Hu and Liu (2004))." ></td>
	<td class="line x" title="59:219	For this specialized text genre, it has been sufficient to limit the notion of topic to mentions of product names and components and their attributes." ></td>
	<td class="line x" title="60:219	Thus, topic extraction has been effectively substituted with a lexicon look-up and techniques have focused on how to learn or acquire an appropriate lexicon for the task." ></td>
	<td class="line x" title="61:219	While the techniques have been very successful for this genre of text, they have not been applied outside the product reviews domain." ></td>
	<td class="line x" title="62:219	Further, there are analyses (Wiebe et al., 2005) and experiments (Wilson et al., 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts." ></td>
	<td class="line x" title="63:219	Outside the product review domain, there has been little effort devoted to opinion topic annotation." ></td>
	<td class="line x" title="64:219	The MPQA corpus, for example, was originally intended to include topic annotations, but the task was abandoned after confirming that it was very difficult (Wiebe, 2005; Wilson, 2005), although target span annotation is currently underway." ></td>
	<td class="line x" title="65:219	Whileuseful,targetspansalonewillbeinsufficient for many applications: they neither contain information indicating which opinions are about the same topic, nor provide a concise textual representation of the topics." ></td>
	<td class="line x" title="66:219	Due to the lack of appropriately annotated corpora, the problem of opinion topic extraction has been largely unexplored in NLP." ></td>
	<td class="line x" title="67:219	A notable exception is the work of Kim and Hovy (2006)." ></td>
	<td class="line x" title="68:219	They propose a model that extracts opinion topics for subjective expressions signaled by verbs and adjectives." ></td>
	<td class="line x" title="69:219	Their model relies on semantic frames and extracts as the topic the syntactic constituent at a specific argument position for the given verb or adjective." ></td>
	<td class="line x" title="70:219	In other words, Kim and Hovy extract what we refer to as the target spans, and do so for a subset of the opinion-bearing words in the text." ></td>
	<td class="line x" title="71:219	Although on many occasions target spans coincide with opinion topics (as in Example 1), we have observed that on many other occasions this is not the case (as in Example 2)." ></td>
	<td class="line x" title="72:219	Furthermore, hampered by the lack of resources with manually annotated targets, Kim and Hovy could provide only a limited evaluation." ></td>
	<td class="line x" title="73:219	As we have defined it, opinion topic identification bears some resemblance to topic segmentation, the goal of which is to partition a text into a linear sequence of topically coherent segments." ></td>
	<td class="line x" title="74:219	Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006))." ></td>
	<td class="line x" title="75:219	Opinion topic identification differs from topic segmentationinthatopiniontopicsarenotnecessarilyspatially coherent  there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic." ></td>
	<td class="line x" title="76:219	Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation." ></td>
	<td class="line x" title="77:219	Other work has successfully adopted the use of clustering to discover entity relations by identifying entities that appear in the same sentence and clustering the intervening context (e.g. Hasegawa etal.(2004), RosenfeldandFeldman(2007))." ></td>
	<td class="line x" title="78:219	This work,however, considersnamedentitiesandheads of proper noun phrases rather than topic spans, and the relations learned are those commonly held between NPs (e.g. senator-of-state, city-of-state, chairman-of-organization) rather than a more general coreference relation." ></td>
	<td class="line x" title="79:219	4 A Coreference Approach to Topic Identification Given our initial definition of opinion topics (Section 2), the next task is to determine which computational approaches might be employed for automatic opinion topic identification." ></td>
	<td class="line x" title="80:219	We begin this exercise by considering some of the problematic characteristics of opinion topics." ></td>
	<td class="line x" title="81:219	Multiple potential topics." ></td>
	<td class="line x" title="82:219	As noted earlier via Example 2, a serious problem in opinion topic identification is the mention of multiple potential topics within the target span of the opinion." ></td>
	<td class="line x" title="83:219	Although an issue for all opinions, this problem is typically more pronounced in opinions that do not carry sentiment (as in Example 2)." ></td>
	<td class="line x" title="84:219	Our current definition of opinion topic requires the NLP system (or a human annotator) to decide which of the entities described in the target span, if any, refers to the intended topic." ></td>
	<td class="line x" title="85:219	This decision can be aided by the following change to our definition of opinion topic, which introduces the idea of a contextdependent information focus: the TOPIC of an opinion is the real-world entity that is the subject of the opinion as intended by the opinion holder based 819 on the discourse context." ></td>
	<td class="line x" title="86:219	With this modified definition in hand, and given Example 3 as the succeeding context for Example 2, we argue that the intended subject, and hence the TOPIC, of Als opinion in 2 can be quickly identifiedasthe FUEL TAX, whichisdenotedbythe TOPIC SPANS tax gas in 2 and fuel tax in 3." ></td>
	<td class="line x" title="87:219	Opinion topics not always explicitly mentioned." ></td>
	<td class="line x" title="88:219	In stark contrast to the above, on many occasions the topic is not mentioned explicitly at all within the target span, as in the following example: (5)[OH John] identified the violation of Palestinian human rights as one of the main factors." ></td>
	<td class="line x" title="89:219	TOPIC: ISRAELIPALESTINIAN CONFLICT We have further observed that the opinion topic is often not mentioned within the same paragraph and, on a few occasions, not even within the same document as the opinion expression." ></td>
	<td class="line x" title="90:219	4.1 Our Solution: Topic Coreference With the above examples and problems in mind, we hypothesize that the notion of topic coreference will facilitate both the manual and automatic identification of opinion topics: We say that two opinions are topic-coreferent if they share the same opinion topic." ></td>
	<td class="line x" title="91:219	In particular, we conjecture that judging whether or not two opinions are topic-coreferent is easier than specifying the topic of each opinion (due to the problems described above)." ></td>
	<td class="line x" title="92:219	5 Constructing the MPQATOPIC Corpus Relyingonthenotionoftopiccoreference,wenext introduceanewmethodologyforthemanualannotation of opinion topics in text: 1." ></td>
	<td class="line x" title="93:219	The annotator begins with a corpus of documents that has been annotated w.r.t. OPINION EXPRESSIONS." ></td>
	<td class="line x" title="94:219	With each opinion expression, the corpus provides POLARITY and OPINION HOLDER information." ></td>
	<td class="line x" title="95:219	(We use the aforementioned MPQA corpus.)" ></td>
	<td class="line x" title="96:219	2." ></td>
	<td class="line x" title="97:219	The annotator maintains a list of the opinion expressions that remain to be annotated (initially, all opinion expressions in the document) as well as a list of the current groupings (i.e. clusters) of opinion expressions that have been identified as topic-coreferent (initially this list is empty)." ></td>
	<td class="line x" title="98:219	3." ></td>
	<td class="line x" title="99:219	For each opinion expression, in turn, the annotator decides whether the opinion is on the same topic as the opinions in one of the existing clusters or should start a new cluster, and inserts the opinion in the appropriate cluster." ></td>
	<td class="line x" title="100:219	4." ></td>
	<td class="line x" title="101:219	The annotator labels each cluster with a string that describestheopiniontopicthatcoversallopinionsinthecluster." ></td>
	<td class="line x" title="102:219	5." ></td>
	<td class="line x" title="103:219	The annotator marks the TOPIC SPAN of each opinion." ></td>
	<td class="line x" title="104:219	(This can be done at any point in the process.)" ></td>
	<td class="line x" title="105:219	The manual annotation procedure is described in a set of instructions available at http://www.cs.cornell.edu/ves." ></td>
	<td class="line x" title="106:219	In addition, we created a GUI that facilitates the annotation procedure." ></td>
	<td class="line x" title="107:219	With the help of these resources, one person annotated opinion topics for a randomly selected set of 150 of the 535 documents in the MPQA corpus." ></td>
	<td class="line x" title="108:219	In addition, 20 of the 150 documents were selected at random and annotated by a second annotator for the purposes of an inter-annotator agreementstudy, theresultsofwhicharepresented in Section 8.1." ></td>
	<td class="line x" title="109:219	The MPQATOPIC and the procedure by which it was created are described in more detail in (Stoyanov and Cardie, 2008)." ></td>
	<td class="line x" title="110:219	6 The Topic Coreference Algorithm As mentioned in Section 4, our computational approach to opinion topic identification is based on topic coreference: For each document (1) find the clusters of coreferent opinions, and (2) label the clusters with the name of the topic." ></td>
	<td class="line x" title="111:219	In this paper we focus only on the first task, topic coreference resolution  the most critical step for topic identification." ></td>
	<td class="line x" title="112:219	We conjecture that the second step can be performed through frequency analysis of the terms in each of the clusters and leave it for future work." ></td>
	<td class="line x" title="113:219	Topic coreference resolution resembles another well-known problem in NLP  noun phrase (NP) coreference resolution." ></td>
	<td class="line x" title="114:219	Therefore, we adapt a standard machine learning-based approach to NP coreference resolution (Soon et al., 2001; Ng and Cardie, 2002)forourpurposes." ></td>
	<td class="line x" title="115:219	Ouradaptationhas threesteps: (i)identifythetopicspans; (ii)perform pairwise classification of the associated opinions as to whether or not they are topic-coreferent; and, (iii) cluster the opinions according to the results of (ii)." ></td>
	<td class="line x" title="116:219	Each step is discussed in more detail below." ></td>
	<td class="line x" title="117:219	6.1 Identifying Topic Spans Decisions about topic coreference should depend on the text spans that express the topic." ></td>
	<td class="line x" title="118:219	Ideally, we would be able to recover the topic span of each opinion and use its content for the topic coreference decision." ></td>
	<td class="line x" title="119:219	However, the topic span depends on the topic itself, so it is unrealistic that topic spans can be recovered with simple methods." ></td>
	<td class="line x" title="120:219	Nevertheless, in this initial work, we investigate two sim820 ple methods for automatic topic span identification and compare them to two manual approaches:  Sentence." ></td>
	<td class="line x" title="121:219	Assume that the topic span is the whole sentence containing the opinion." ></td>
	<td class="line x" title="122:219	 Automatic." ></td>
	<td class="line x" title="123:219	A rule-based method for identifying the topic span (developed using MPQA documents that are not part of MPQATOPIC)." ></td>
	<td class="line x" title="124:219	Rules depend on the syntactic constituent type of the opinion expression and rely on syntactic parsing and grammatical role labeling." ></td>
	<td class="line x" title="125:219	 Manual." ></td>
	<td class="line x" title="126:219	Use the topic span marked by the human annotator." ></td>
	<td class="line x" title="127:219	We included this method to provide an upper bound on performance of the topic span extractor." ></td>
	<td class="line x" title="128:219	 Modified Manual." ></td>
	<td class="line x" title="129:219	Meant to be a more realisticuseofthemanualtopicspanannotations, this method returns the manually identified topic span only when it is within the sentence of the opinion expression." ></td>
	<td class="line x" title="130:219	When this span is outside the sentence boundary, this method returns the opinion sentence." ></td>
	<td class="line x" title="131:219	Of the 4976 opinions annotated across the 150 documents of MPQATOPIC, the topic spans associated with 4293 were within the same sentence as the opinion; 3653 were within the span extracted by our topic span extractor." ></td>
	<td class="line x" title="132:219	Additionally, the topic spans of 173 opinions were outside of the paragraph containing the opinion." ></td>
	<td class="line x" title="133:219	6.2 Pairwise Topic Coreference Classification The heart of our method is a pairwise topic coreference classifier." ></td>
	<td class="line x" title="134:219	Given a pair of opinions (and their associated polarity and opinion holder information), the goal of the classifier is to determine whether the opinions are topic-coreferent." ></td>
	<td class="line x" title="135:219	We use the manually annotated data to automatically learn the pairwise classifier." ></td>
	<td class="line x" title="136:219	Given a training document, we construct a training example for every pair of opinions in the document (each pair is represented as a feature vector)." ></td>
	<td class="line x" title="137:219	The pair is labeled as a positiveexampleifthetwoopinionsbelongtothesame topic cluster, and a negative example otherwise." ></td>
	<td class="line x" title="138:219	Pairwise coreference classification relies critically on the expressiveness of the features used to describe the opinion pair." ></td>
	<td class="line x" title="139:219	We use three categories of features: positional, lexico-semantic and opinion-based features." ></td>
	<td class="line x" title="140:219	Positional features These features are intended to exploit the fact that opinions that are close to each other are more likely to be on the same topic." ></td>
	<td class="line x" title="141:219	We use six positional features:  Same Sentence/Paragraph5 True if the two opinions are in the same sentence/paragraph." ></td>
	<td class="line x" title="142:219	 Consecutive Sentences/Paragraphs True if the two opinions are in consecutive sentences/paragraphs." ></td>
	<td class="line x" title="143:219	 Number of Sentences/Paragraphs The number of sentences/paragraphs that separate the two opinions." ></td>
	<td class="line x" title="144:219	TOPIC SPAN-based lexico-semantic features The features in this group rely on the topic spans and are recomputed w.r.t. each of the four topic span methods." ></td>
	<td class="line x" title="145:219	The intuition behind this group of features is that topic-coreferent opinions are likely to exhibit lexical and semantic similarity within the topic span." ></td>
	<td class="line x" title="146:219	 tf.idf The cosine similarity of the tf.idf weightedvectorsofthetermscontainedinthe two spans." ></td>
	<td class="line x" title="147:219	 Word overlap True if the two topic spans contain any contain words in common." ></td>
	<td class="line x" title="148:219	 NP coref True if the two spans contain NPs that are determined to be coreferent by a simple rule-based coreference system." ></td>
	<td class="line x" title="149:219	 NE overlap True if the two topic spans contain named entities that can be considered aliases of each other." ></td>
	<td class="line x" title="150:219	Opinion features The features in this group depend on the attributes of the opinion." ></td>
	<td class="line x" title="151:219	In the current work, we obtain these features directly from the manual annotations of the MPQATOPIC corpus, butthey mightalso beobtained fromautomatically identified opinion information using the methods referenced in Section 3." ></td>
	<td class="line x" title="152:219	 Source Match True if the two opinions have the same opinion holder." ></td>
	<td class="line x" title="153:219	 PolarityMatchTrue if the two opinions have the same polarity." ></td>
	<td class="line x" title="154:219	5Weusesentence/paragraphtodescribetwofeaturesone based on the sentence and one on the paragraph." ></td>
	<td class="line x" title="155:219	821  Source-PolarityMatchFalseifthetwoopinions have the same opinion holder but conflicting polarities (since it is unlikely that a source will have two opinions with conflicting polarities on the same topic)." ></td>
	<td class="line x" title="156:219	Weemploythreeclassifiersforpairwisecoreference classification  an averaged perceptron (Freund and Schapire, 1998), SVMlight (Joachims, 1998) and a rule-learner  RIPPER (Cohen, 1995)." ></td>
	<td class="line x" title="157:219	However, we report results only for the averaged perceptron, which exhibited the best performance." ></td>
	<td class="line x" title="158:219	6.3 Clustering Pairwise classification provides an estimate of the likelihood that two opinions are topic-coreferent." ></td>
	<td class="line x" title="159:219	To form the topic clusters, we follow the pairwise classification with a clustering step." ></td>
	<td class="line x" title="160:219	We selected a simple clustering algorithm  single-link clustering, which has shown good performance for NP coreference." ></td>
	<td class="line x" title="161:219	Given a threshold, single-link clustering proceeds by assigning pairs of opinions with a topic-coreference score above the threshold to the sametopicclusterandthenperformstransitiveclosure of the clusters.6 7 Evaluation Methodology For training and evaluation we use the 150document MPQATOPIC corpus." ></td>
	<td class="line x" title="162:219	All machine learning methods were tested via 10-fold cross validation." ></td>
	<td class="line x" title="163:219	In each round of cross validation, we use eight of the data partitions for training and one for parameter estimation (we varied the threshold for theclusteringalgorithm), andtestontheremaining partition." ></td>
	<td class="line x" title="164:219	We report results for the three evaluation measures of Section 7 using the four topic span extraction methods introduced in Section 6." ></td>
	<td class="line x" title="165:219	The threshold is tuned separately for each evaluation measure." ></td>
	<td class="line x" title="166:219	As noted earlier, all runs obtain opinion information from the MPQATOPIC corpus (i.e. this work does not incorporate automatic opinion extraction)." ></td>
	<td class="line x" title="167:219	7.1 Topic Coreference Baselines We compare our topic coreference system to four baselines." ></td>
	<td class="line x" title="168:219	The first two are the default baselines:  one topic  assigns all opinions to the same cluster." ></td>
	<td class="line x" title="169:219	6Experiments using best-first and last-first clustering approaches provided similar or worse results." ></td>
	<td class="line x" title="170:219	 one opinion per cluster  assigns each opinion to its own cluster." ></td>
	<td class="line x" title="171:219	Theothertwobaselinesattempttoperformtopic segmentation (discussed in Section 3) and assign all opinions within the same segment to the same opinion topic:  same paragraph  simple topic segmentation by splitting documents into segments at paragraph boundaries." ></td>
	<td class="line x" title="172:219	 Choi 2000  Chois (2000) state-of-the-art approach to finding segment boundaries." ></td>
	<td class="line x" title="173:219	We use the freely available C99 software described in Choi (2000), varying a parameter that allows us to control the average number of sentences per segment and reporting the best result on the test data." ></td>
	<td class="line x" title="174:219	7.2 Evaluation Metrics Because there is disagreement among researchers w.r.t. the proper evaluation measure for NP coreference resolution, we use three generally accepted metrics7 to evaluate our topic coreference system." ></td>
	<td class="line x" title="175:219	B-CUBED." ></td>
	<td class="line x" title="176:219	B-CUBED (B3) is a commonly used NP coreference metric (Bagga and Baldwin, 1998)." ></td>
	<td class="line x" title="177:219	It calculates precision and recall for each item (in our case, each opinion) based on the number of correctly identified coreference links, and then computes the average of the item scores in each document." ></td>
	<td class="line x" title="178:219	Precision/recall for an item i is computed as the proportion of items in the intersectionoftheresponse(system-generated)andkey (gold standard) clusters containingidivided by the number of items in the response/key cluster." ></td>
	<td class="line x" title="179:219	CEAF." ></td>
	<td class="line x" title="180:219	As a representative of another group of coreference measures that rely on mapping response clusters to key clusters, we selected Luos (2005) CEAF score (short for Constrained EntityAlignment F-Measure)." ></td>
	<td class="line x" title="181:219	Similar to the ACE (2005) score, CEAF operates by computing an optimal mapping of response clusters to key clusters and assessing the goodness of the match of each of the mapped clusters." ></td>
	<td class="line x" title="182:219	Krippendorffs ." ></td>
	<td class="line x" title="183:219	Finally, we use Passonneaus (2004) generalization of Krippendorffs (1980)   a standard metric employed for inter-annotator 7The MUC scoring algorithm (Vilain et al., 1995) was omitted because it led to an unjustifiably high MUC F-score (.920) for the ONE TOPIC baseline." ></td>
	<td class="line x" title="184:219	822 B3  CEAF All opinions .6424 .5476 .6904 Sentiment opinions .7180 .7285 .7967 Strong opinions .7374 .7669 .8217 Table 1: Inter-annotator agreement results." ></td>
	<td class="line x" title="185:219	reliability studies." ></td>
	<td class="line x" title="186:219	Krippendorffs  is based on a probabilistic interpretation of the agreement of coders as compared to agreement by chance." ></td>
	<td class="line x" title="187:219	While Passonneaus innovation makes it possible to apply Krippendorffs  to coreference clusters, theprobabilisticinterpretationofthestatisticisunfortunately lost." ></td>
	<td class="line x" title="188:219	8 Results 8.1 Inter-annotator Agreement As mentioned previously, out of the 150 annotated documents, 20 were annotated by two annotators for the purpose of studying the agreement between coders." ></td>
	<td class="line x" title="189:219	Inter-annotator agreement results are shown in Table 1." ></td>
	<td class="line x" title="190:219	We compute agreement for three subsets of opinions: all available opinions, only the sentiment-bearing opinions and the subset of sentiment-bearing opinions judged to have polarity of medium or higher." ></td>
	<td class="line x" title="191:219	The results support our conjecture that topics of sentiment-bearing opinions are much easier to identify: inter-annotator agreement for opinions with non-neutral polarity (SENTIMENT OPINIONS) improves by a large margin for all measures." ></td>
	<td class="line x" title="192:219	As in other work in subjectivity annotation, we find that strong sentiment-bearing opinions are easier to annotate than sentiment-bearing opinions in general." ></td>
	<td class="line x" title="193:219	Generally, the  score aims to probabilistically capture the agreement of annotation data and separate it from chance agreement." ></td>
	<td class="line x" title="194:219	It is generally accepted that an  score of .667 indicates reliable agreement." ></td>
	<td class="line x" title="195:219	The score that we observed for the overallagreementwasanof.547,whichisbelow the generally accepted level, while  for the two subsetsofsentiment-bearingopinionsisabove.72." ></td>
	<td class="line x" title="196:219	However, as discussed above, due to the way that it is adapted to the problem of coreference resolution, the  score loses its probabilistic interpretation." ></td>
	<td class="line x" title="197:219	For example, the  score requires that a pairwise distance function between clusters is specified." ></td>
	<td class="line x" title="198:219	We used one sensible choice for such a function (we measured the distance between clusters A and B as dist(A,B) = (2|AB|)/(|A|+|B|)), B3  CEAF One topic .3739 -.1017 .2976 One opinion per cluster .2941 .2238 .2741 Same paragraph .5542 .3123 .5090 Choi .5399 .3734 .5370 Sentence .5749 .4032 .5393 Rule-based .5730 .4056 .5420 Modified manual .6416 .5134 .6124 Manual .7097 .6585 .6184 Table 2: Results for the topic coreference algorithms." ></td>
	<td class="line x" title="199:219	but other sensible choices for the distance lead to much higher scores." ></td>
	<td class="line x" title="200:219	Furthermore, we observed that the behavior of the  score can be rather erratic  small changes in one of the clusterings can lead to big differences in the score." ></td>
	<td class="line x" title="201:219	Perhaps a better indicator of the reliability of the coreference annotation is a comparison with the baselines, shown in the top half of Table 2." ></td>
	<td class="line x" title="202:219	All baselines score significantly lower than the inter-annotator agreement scores." ></td>
	<td class="line x" title="203:219	With one exception, the inter-annotator agreement scores are also higher than those for the learning-based approach (results shown in the lower half of Table 2), as would typically be expected." ></td>
	<td class="line x" title="204:219	The exception is the classifier that uses the manual topic spans, but as we argued earlier these spans carry significant information about the decision of the annotator." ></td>
	<td class="line x" title="205:219	8.2 Baselines Results for the four baselines are shown in the first four rows of Table 2." ></td>
	<td class="line x" title="206:219	As expected, the two baselinesperformingtopicsegmentationshowsubstantially better scores than the two default baselines." ></td>
	<td class="line x" title="207:219	8.3 Learning methods Results for the learning-based approaches are shown in the bottom half of Table 2." ></td>
	<td class="line x" title="208:219	First, we see that each of the learning-based methods outperforms the baselines." ></td>
	<td class="line x" title="209:219	This is the case even when sentences are employed as a coarse substitute for the true topic span." ></td>
	<td class="line x" title="210:219	A Wilcoxon Signed-Rank test shows that differences from the baselines for the learning-based runs are statistically significant for the B3 and  measures (p < 0.01); for CEAF, using sentences as topic spans for the learning algorithm outperforms the SAME PARAGRAPH baseline (p < 0.05), but the results are inconclusive when 823 compared with the system of CHOI." ></td>
	<td class="line x" title="211:219	In addition, relying on manual topic span information (MANUAL and MODIFIED MANUAL) allows the learning-based approach to perform significantly better than the two runs that use automatically identified spans (p < 0.01, for all three measures)." ></td>
	<td class="line x" title="212:219	The improvement in the scores hints at the importance of improving automatic topic span extraction, which will be a focus of our future work." ></td>
	<td class="line x" title="213:219	9 Conclusions Wepresentedanew, operationaldefinitionofopinion topics in the context of fine-grained subjectivity analysis." ></td>
	<td class="line x" title="214:219	Based on this definition, we introduced an approach to opinion topic identification that relies on the identification of topiccoreferent opinions." ></td>
	<td class="line x" title="215:219	We further employed the opinion topic definition for the manual annotation of opinion topics to create the MPQATOPIC corpus." ></td>
	<td class="line x" title="216:219	Inter-annotator agreement results show that opinion topic annotation can be performed reliably." ></td>
	<td class="line x" title="217:219	Finally, we proposed an automatic approach for identifying topic-coreferent opinions, which significantly outperforms all baselines across three coreference evaluation metrics." ></td>
	<td class="line x" title="218:219	Acknowledgments The authors of this paper would like to thank Janyce Wiebe and Theresa Wilson for many insightful discussions." ></td>
	<td class="line x" title="219:219	This work was supported in part by National Science Foundation Grants BCS0624277 and IIS-0535099 and by DHS Grant N0014-07-1-0152." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1111
Emotion Classification Using Massive Examples Extracted from the Web
Tokhisa, Ryoko;Inui, Kentaro;Matsumoto, Yuji;"></td>
	<td class="line x" title="1:185	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 881888 Manchester, August 2008 Emotion Classification Using Massive Examples Extracted from the Web Ryoko TOKUHISA   Toyota Central R&D Labs., INC. Nagakute Aichi JAPAN tokuhisa@mosk.tytlabs.co.jp Kentaro INUI   Nara Institute of Science and Technology Ikoma Nara JAPAN {ryoko-t,inui,matsu}@is.naist.jp Yuji MATSUMOTO  Abstract In this paper, we propose a data-oriented method for inferring the emotion of a speaker conversing with a dialog system from the semantic content of an utterance." ></td>
	<td class="line x" title="2:185	We first fully automatically obtain a huge collection of emotion-provoking event instances from the Web." ></td>
	<td class="line x" title="3:185	With Japanese chosen as a target language, about 1.3 million emotion provoking event instances are extracted using an emotion lexicon and lexical patterns." ></td>
	<td class="line x" title="4:185	We then decompose the emotion classification task into two sub-steps: sentiment polarity classification (coarsegrained emotion classification), and emotion classification (fine-grained emotion classification)." ></td>
	<td class="line x" title="5:185	For each subtask, the collection of emotion-proviking event instances is used as labelled examples to train a classifier." ></td>
	<td class="line x" title="6:185	The results of our experiments indicate that our method significantly outperforms the baseline method." ></td>
	<td class="line x" title="7:185	We also find that compared with the singlestep model, which applies the emotion classifier directly to inputs, our two-step model significantly reduces sentiment polarity errors, which are considered fatal errors in real dialog applications." ></td>
	<td class="line x" title="8:185	1 Introduction Previous research into human-computer interaction has mostly focused on task-oriented dialogs, where the goal is considered to be to achieve a c2008." ></td>
	<td class="line x" title="9:185	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="10:185	Some rights reserved." ></td>
	<td class="line x" title="11:185	given task as precisely and efficiently as possible by exchanging information required for the task through dialog (Allen et al., 1994, etc.)." ></td>
	<td class="line x" title="12:185	More recent research (Foster, 2007; Tokuhisa and Terashima, 2006, etc.), on the other hand, has been providing evidence for the importance of the affective or emotional aspect in a wider range of dialogic contexts, which has been largely neglected in the context of task-oriented dialogs." ></td>
	<td class="line x" title="13:185	A dialog system may be expected to serve, for example, as an active listening 1 partner of an elderly user living alone who sometimes wishes to have a chat." ></td>
	<td class="line x" title="14:185	In such a context, the dialog system is expected to understand the users emotions and sympathize with the user." ></td>
	<td class="line x" title="15:185	For example, given an utterence I traveled far to get to the shop, but it was closed from the user, if the system could infer the users emotion behind it, it would know that it would be appropriate to say Thats too bad or Thats really disappointing." ></td>
	<td class="line x" title="16:185	It can be easily imagined that such affective behaviors of a dialog system would be beneficial not only for active listening but also for a wide variety of dialog purposes including even task-oriented dialogs." ></td>
	<td class="line x" title="17:185	To be capable of generating sympathetic responses, a dialog system needs a computational model that can infer the users emotion behind his/her utterence." ></td>
	<td class="line x" title="18:185	There have been a range of studies for building a model for classifying a users emotions based on acoustic-prosodic features and facial expressions (Pantic and Rothkrantz, 2004, etc.)." ></td>
	<td class="line x" title="19:185	Such methods are, however, severely limited in that they tend to work well only when the user expresses his/her emotions by exaggerated 1 Active listening is a specific communication skill, based on the work of psychologist Carl Rogers, which involves giving free and undivided attention to the speaker (Robertson, 2005)." ></td>
	<td class="line x" title="20:185	881 X54X68X65X20X72X65X73X74X61X75X72X61X6EX74X20X77X61X73X20 X20X20X20X76X65X72X79X20X66X61X72X20X62X75X74X20X69X74X20X77X61X73X20 X20X20X20X63X6CX6FX73X65X64 X49X6EX70X75X74 X45X6DX6FX74X69X6FX6EX20 X63X6CX61X73X73X69X66X69X63X61X74X69X6FX6EX20 X75X73X69X6EX67X20X45X50X20X43X6FX72X70X75X73X20 X28X73X65X63X74X69X6FX6EX20X33X2EX34X29 X3CX64X69X73X61X70X70X6FX69X6EX74X6DX65X6EX74X3E X4FX75X74X70X75X74 X49X20X77X61X73X20X64X69X73X61X70X70X6FX69X6EX74X65X64X20X62X65X63X61X75X73X65X20X74X68X65X20X73X68X6FX70X20X77X61X73X20 X63X6CX6FX73X65X64X20X61X6EX64X20X49 X64X20X74X72X61X76X65X6CX65X64X20X61X20X6CX6FX6EX67X20X77X61X79X20X74X6FX20X67X65X74X20X74X68X65X72X65X20 X49X20X77X61X73X20X64X69X73X61X70X70X6FX69X6EX74X65X64X20X74X68X61X74X20X69X74X20X73X74X61X72X74X65X64X20X72X61X69X6EX69X6EX67X20 X49X20X61X6DX20X68X61X70X70X79X20X73X69X6EX63X65X20X74X68X65X20X62X6FX6FX6BX20X73X74X6FX72X65X20X77X61X73X20X6FX70X65X6EX20 X77X68X65X6EX20X49X20X67X6FX74X20X62X61X63X6BX20X68X6FX6DX65X20 X57X65X62X20X54X65X78X74 X53X65X6EX74X69X6DX65X6EX74X20 X70X6FX6CX61X72X69X74X79X20 X63X6CX61X73X73X69X66X69X63X61X74X69X6FX6EX20 X28X73X65X63X74X69X6FX6EX20X33X2EX33X29 X45X6DX6FX74X69X6FX6EX2DX70X72X6FX76X6FX6BX69X6EX67X20X65X76X65X6EX74X20X63X6FX72X70X75X73X20X28X45X50X20X63X6FX72X70X75X73X29 X74X68X65X20X73X68X6FX70X20X77X61X73X20X63X6CX6FX73X65X64X20X61X6EX64X20X49 X64X20 X74X72X61X76X65X6CX65X64X20X61X20X6CX6FX6EX67X20X77X61X79X20X74X6FX20X67X65X74X20X74X68X65X72X65X20 X69X74X20X73X74X61X72X74X65X64X20X72X61X69X6EX69X6EX67X20 X20 X49X20X61X6DX20X61X6CX6FX6EX65X20X6FX6EX20X43X68X72X69X73X74X6DX61X73X20 X74X68X65X20X62X6FX6FX6BX20X73X74X6FX72X65X20X77X61X73X20X6FX70X65X6EX20X77X68X65X6EX20X49X20 X67X6FX74X20X62X61X63X6BX20X68X6FX6DX65 X45X6DX6FX74X69X6FX6EX2DX70X72X6FX76X6FX6BX69X6EX67X20X65X76X65X6EX74X20X20X20X20X20X20X20X20X20X20X20X20X45X6DX6FX74X69X6FX6EX28X70X6FX6CX61X72X69X74X79X29 X64X69X73X61X70X70X6FX69X6EX74X6DX65X6EX74X20 X28X6EX65X67X61X74X69X76X65X29 X68X61X70X70X69X6EX65X73X73X20X28X70X6FX73X69X74X69X76X65X29X20 X45X6DX6FX74X69X6FX6EX20X63X6CX61X73X73X69X66X69X63X61X74X69X6FX6E X42X75X69X6CX64X69X6EX67X20 X65X6DX6FX74X69X6FX6EX2DX70X72X6FX76X6FX6BX69X6EX67X20 X65X76X65X6EX74X20X63X6FX72X70X75X73X20 X28X73X65X63X74X69X6FX6EX20X33X2EX32X29 X4FX46X46X4CX49X4EX45 X4FX4EX4CX49X4EX45 X6CX6FX6EX65X6CX69X6EX65X73X73X20X28X6EX65X67X61X74X69X76X65X29X20 X64X69X73X61X70X70X6FX69X6EX74X6DX65X6EX74X20 X28X6EX65X67X61X74X69X76X65X29 Figure 1: Overview of our approach to emotion classification prosodic or facial expressions." ></td>
	<td class="line x" title="21:185	Furthermore, what is required in generating sympathetic responses is the identification of the users emotion in a finer grain-size." ></td>
	<td class="line x" title="22:185	For example, in contrast to the above example of disappointing, one may expect the response to My pet parrot died yesterday should be Thats really sad, wheras the response to I may have forgotten to lock my house should be Youre worried about that." ></td>
	<td class="line x" title="23:185	In this paper, we address the above issue of emotion classification in the context of humancomputer dialog, and demonstrate that massive examples of emotion-provoking events can be extracted from the Web with a reasonable accuracy and those examples can be used to build a semantic content-based model for fine-grained emotion classification." ></td>
	<td class="line x" title="24:185	2 Related Work Recently, several studies have reported about dialog systems that are capable of classifying emotions in a human-computer dialog (Batliner et al., 2004; Ang et al., 2002; Litman and Forbes-Riley, 2004; Rotaru et al., 2005)." ></td>
	<td class="line x" title="25:185	ITSPOKE is a tutoring dialog system, that can recognize the users emotion using acoustic-prosodic features and lexical features." ></td>
	<td class="line x" title="26:185	However, the emotion classes are limited to Uncertain and Non-Uncertain because the purpose of ITSPOKE is to recognize the users problem or discomfort in a tutoring dialog." ></td>
	<td class="line x" title="27:185	Our goal, on the other hand, is to classify the users emotions into more fine-grained emotion classes." ></td>
	<td class="line x" title="28:185	In a more general research context, while quite a few studies have been presented about opinion mining and sentiment analysis (Liu, 2006), research into fine-grained emotion classification has emerged only recently." ></td>
	<td class="line x" title="29:185	There are two approaches commonly used in emotion classification: a rulebased approach and a statistical approach." ></td>
	<td class="line x" title="30:185	Masum et al.(2007) and Chaumartin (2007) propose a rule-based approach to emotion classification." ></td>
	<td class="line x" title="32:185	Chaumartin has developed a linguistic rulebased system, which classifies the emotions engendered by news headlines using the WordNet, SentiWordNet, and WordNet-Affect lexical resources." ></td>
	<td class="line x" title="33:185	The system detects the sentiment polarity for each word in a news headline based on linguistic resources, and then attempts emotion classification by using rules based on its knowledge of sentence structures." ></td>
	<td class="line x" title="34:185	The recall of this system is low, however, because of the limited coverage of the lexical resources." ></td>
	<td class="line oc" title="35:185	Regarding the statistical approach, Kozareva et al.(2007) apply the theory of (Hatzivassiloglou and McKeown, 1997) and (Turney, 2002) to emotion classification and propose a method based on the co-occurrence distribution over content words and six emotion words (e.g. joy, fear)." ></td>
	<td class="line x" title="37:185	For example, birthday appears more often with joy, while war appears more often with fear." ></td>
	<td class="line x" title="38:185	However, the accuracy achieved by their method is not practical in applications assumed in this paper." ></td>
	<td class="line x" title="39:185	As we demonstrate in Section 4, our method significantly outperforms Kozarevas method." ></td>
	<td class="line x" title="40:185	3 Emotion Classification 3.1 The basic idea We consider the task of emotion classification as a classification problem where a given input sentence (a users utterance) is to be classified either into such 10 emotion classes as given later in Table1orasneutral if no emotion is involved in the input." ></td>
	<td class="line x" title="41:185	Since it is a classification problem, the task should be approached straightforwardly in a vari882 Table 1: Distribution of the emotion expressions and examples Sentiment 10 Emotion Emotion lexicon (349 Japanese emotion words) Polarity Classes Total Examples happiness 90.`M(happy)|*(joyful)|*(glad)|(glad) Positive pleasantness 7`M(pleasant)|`(enjoy)|`(can enjoy) relief 5	(relief)|lq(relief) fear 22M(fear)| M(fear)|`M(frightening) sadness 21 u`M(sad)|i`M(sad)| u`(feel sad) disappointment 15UlT(lose heart)|UlX(drop ones head) Negative unpleasantness 109O(disgust)|OU(dislike)|OM(dislike) loneliness 15	`M(lonely)|`M(lonely)||`M(lonely) anxiety 17 (anxiety)|	 (anxiety)|>UT(worry) anger 48qh`M(angry)| qm(get angry)|q (angry) ety of machine learning-based methods if a sufficient number of labelled examples were available." ></td>
	<td class="line x" title="42:185	Our basic idea is to learn what emotion is typically provoked in what situation, from massive examples that can be collected from the Web." ></td>
	<td class="line x" title="43:185	The development of this approach and its subsequent implementation has forced us to consider the following two issues." ></td>
	<td class="line x" title="44:185	First, we have to consider the quantity and accuracy of emotion-provoking examples to be collected." ></td>
	<td class="line x" title="45:185	The process we use to collect emotionprovoking examples is illustrated in the upper half of Figure 1." ></td>
	<td class="line x" title="46:185	For example, from the sentence I was disappointed because the shop was closed and Id I traveled a long way to get there, pulled from the Web, we learn that the clause the shop was closed and Id traveled a long way to get there is an example of an event that provokes disappointment." ></td>
	<td class="line x" title="47:185	In this paper, we refer to such an example as an emotion-provoking event and a collection of eventprovoking events as an emotion-provoking event corpus (an EP corpus)." ></td>
	<td class="line x" title="48:185	Details are described in Section 3.2." ></td>
	<td class="line x" title="49:185	Second, assuming that an EP corpus can be obtained, the next issue is how to use it for our emotion classification task." ></td>
	<td class="line x" title="50:185	We propose a method whereby an input utterance (sentence) is classified in two steps, sentiment polarity classification followed by fine-grained emotion classification as shown in the lower half of Figure 1." ></td>
	<td class="line x" title="51:185	Details are given in Sections 3.3 and 3.4." ></td>
	<td class="line x" title="52:185	3.2 Building an EP corpus We used ten emotions happiness, pleasantness, relief, fear, sadness, disappointment, unpleasantness, loneliness, anxiety, anger in our emotion classification experiment." ></td>
	<td class="line x" title="53:185	First, we built a handcrafted lexicon of emotion words classified into the ten emotions." ></td>
	<td class="line x" title="54:185	From the Japanese Evaluation Expression Dictionary created by Kobayashi et al.(2005), we identified 349 emotion words based X73X75X62X6FX72X64X69X6EX61X74X65X20X63X6CX61X75X73X65 X65X6DX6FX74X69X6FX6EX20X70X72X6FX76X6FX6BX69X6EX67X20X65X76X65X6EX74 X63X6FX6EX6EX65X63X74X69X76X65 X65X6DX6FX74X69X6FX6EX20X6CX65X78X69X63X6FX6E Figure 2: An example of a lexico-syntactic pattern Table 2: Number of emotion-provoking events 10 Emotions EP event 10 Emotions EP event happiness 387,275 disappoint106,284 ment pleasantness 209,682 unpleasantness 396,002 relief 46,228 loneliness 26,493 fear 49,516 anxiety 45,018 sadness 31,369 anger 8,478 on the definition of emotion words proposed by Teramura (1982)." ></td>
	<td class="line x" title="56:185	The distribution is shown in Table 1 with major examples." ></td>
	<td class="line x" title="57:185	We then went on to find sentences in the Web corpus that possibly contain emotion-provoking events." ></td>
	<td class="line x" title="58:185	A subordinate clause was extracted as an emotion-provoking event instance if (a) it was subordinated to a matrix clause headed by an emotion word and (b) the relation between the subordinate and matrix clauses is marked by one of the following eight connectives:wp,T,h, o,wx,wU,\qx,\qU. An example is given in Figure 2." ></td>
	<td class="line x" title="59:185	In the sentence  U	Z`h wxUlTi(I was disappointed that it suddenly started raining), the subordinate clause   U	Z`h(it suddenly started raining) modifies UlTi(I was disappointed) with the connective wx(that)." ></td>
	<td class="line x" title="60:185	In this case, therefore, the event mention  U	Z`h(it suddenly started raining) is learned as an event instance that provokesdisappointment." ></td>
	<td class="line x" title="61:185	Applying the emotion lexicon and the lexical patterns to the Japanese Web corpus (Kawahara and Kurohashi, 2006), which contains 500 million sentences, we were able to collect about 1.3 million events as causes of emotion." ></td>
	<td class="line x" title="62:185	The distribution is shown in Table 2." ></td>
	<td class="line x" title="63:185	Tables 3 and 4 show the results of our evalua883 Table 4: Examples from in the EP corpus EP-Corpus Result of evaluation Emotion-provoking Event Emotion word 10 Emotions (P/N) Polarity Emotion VjUqM(A flower died quickly)i(diappointed) disappointment(N) Correct Correct 'UM(There are a lot of enemies)^V(lose interest) unpleasantness(N) Correct Context-dep." ></td>
	<td class="line x" title="64:185	j[JUM(There is a lot of Chinese cabbage) .`M(happy) happiness(P) Context-dep." ></td>
	<td class="line x" title="65:185	Context-dep." ></td>
	<td class="line x" title="66:185	UhM(I would like to drink orange juice) G!i(terrible) unpleasantness(N) Error Error Table 3: Correctness of samples from the EP corpus Polarity Emotion Correct 1140 (57.0%) 988 (49.4%) Context-dep." ></td>
	<td class="line x" title="67:185	678 (33.9%) 489 (24.5%) Error 182 (9.1%) 523 (26.2%) tion for the resultant EP corpus." ></td>
	<td class="line x" title="68:185	One annotator, who was not the developer of the EP corpus, evaluated 2000 randomly chosen events." ></td>
	<td class="line x" title="69:185	The Polarity column in Table 3 shows the results of evaluating whether the sentiment polarity of each event is correctly labelled, whereas theEmotion column shows the correctness at the level of the 10 emotion classes." ></td>
	<td class="line x" title="70:185	The correctness of each example was evaluated as exemplified in Table 4." ></td>
	<td class="line x" title="71:185	Correct indicates a correct example, Contex-dep." ></td>
	<td class="line x" title="72:185	indicates a context-dependent example, and Error is an error example." ></td>
	<td class="line x" title="73:185	For example, in the case of There are a lot of enemies in Table 4, the Polarity is Correct because it represents a negative emotion." ></td>
	<td class="line x" title="74:185	However, its emotion classunpleasantness is judged Context-dep." ></td>
	<td class="line x" title="75:185	As Table 3 shows, the Sentiment Polarity is correct in 57.0% of cases and partially correct (Correct + Context-dep.)" ></td>
	<td class="line x" title="76:185	in 90.9% of cases." ></td>
	<td class="line x" title="77:185	On the other hand, the Emotion is correct in only 49.4% of cases and partially correct in 73.9% of cases." ></td>
	<td class="line x" title="78:185	These figures may not seem very impressive." ></td>
	<td class="line x" title="79:185	As far as its impact on the emotion classification accuracy is concerned, however, the use of our EP corpus, which requires no supervision, makes remarkable improvements upon Kozareva et al.(2007)s unsupervised method as we show later." ></td>
	<td class="line x" title="81:185	3.3 Sentiment Polarity Classification Given the large collection of emotion-labelled examples, it may seem straightforward to develop a trainable model for emotion classification." ></td>
	<td class="line x" title="82:185	Before moving on to emotion classification, however, it should be noted that a users input utterance may not involve any emotion." ></td>
	<td class="line x" title="83:185	For example, if the user gives an utterance I have a lunch at the school cafeteria every day, it is not appropriate for the system to make any sympathetic response." ></td>
	<td class="line x" title="84:185	In such a case, the users input should be classified as neutral." ></td>
	<td class="line x" title="85:185	The classification between emotion-involved and neutral is not necessarily a simple problem, however, because we have not found yet any practical method for collecting training examples of the classneutral." ></td>
	<td class="line x" title="86:185	We cannot rely on the analogy to the pattern-based method we have adopted to collect emotion-provoking events  there seems no reliable lexico-syntactic pattern for extracting neutral examples." ></td>
	<td class="line x" title="87:185	Alternatively, if the majority of the sentences on the Web were neutral, one would simply use a set of randomly sampled sentences as labelled data for neutral." ></td>
	<td class="line x" title="88:185	This strategy, however, does not work because neutral sentences are not the majority in real Web texts." ></td>
	<td class="line x" title="89:185	As an attempt, we collected 1000 sentences randomly from the Web and investigated their distribution of sentiment polarity." ></td>
	<td class="line x" title="90:185	The results, shown in Table 5, revealed that the ratio of neutral events was unexpectedly low." ></td>
	<td class="line x" title="91:185	These results indicate the difficulty of collecting neutral events from Web documents." ></td>
	<td class="line x" title="92:185	Taking this problem into account, we adopt a two-step approach, where we first classify a given input into three sentiment polarity classes, either positive, negative or neutral, and then classify only those judged positive or negative into our 10 finegrained emotion classes." ></td>
	<td class="line x" title="93:185	In the first step, i.e. sentiment polarity classification, we use only the positive and negative examples stored in the EP corpus and assume sentence to be neutral if the output of the classification model is near the decision boundary." ></td>
	<td class="line x" title="94:185	There are additional advantages in this approach." ></td>
	<td class="line x" title="95:185	First, it is generally known that performing fine-grained classification after coarse classification often provides good results particularly when the number of the classes is large." ></td>
	<td class="line x" title="96:185	Second, in the context of dialog, a misunderstanding the users emotion at the sentiment polarity level would be a disaster." ></td>
	<td class="line x" title="97:185	Imagine that the system says You must be happy when the user in fact feels sad." ></td>
	<td class="line x" title="98:185	As we show in Section 4.2, such fatal errors can be reduced by taking the two-step approach." ></td>
	<td class="line x" title="99:185	884 Table 5: Distribution of the Sentiment polarity of sentences randomly sampled from the Web Sentiment Polarity Number Ratio positive 650 65.0% negative 153 15.3% neutral 117 11.7% Context-dep." ></td>
	<td class="line oc" title="100:185	80 8.0% Positive child education Positive cost Negative SUBJECT increase Figure 3: An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al., 2002) and dependency structure (Kudo and Matsumoto, 2004)." ></td>
	<td class="line x" title="101:185	Our sentiment polarity classification model is trained with SVMs (Vapnik, 1995), and the features are {1-gram, 2-gram, 3gram} of words and the sentiment polarity of the words themselves." ></td>
	<td class="line x" title="102:185	Figure 3 illustrates how the sentence ww rU Q(The cost of educating my child increases) is encoded to a feature vector." ></td>
	<td class="line x" title="103:185	Here we assume the sentiment polarity of the (child) and (education) are positive, while the r(cost) is negative." ></td>
	<td class="line x" title="104:185	These polarity values are represented in parallel with the corresponding words, as shown in Figure 3." ></td>
	<td class="line x" title="105:185	By expanding {1-gram, 2-gram, 3-gram} in this lattice representation, the following list of features are extracted:(child), Positive,(child)w(of), Positive-w(of),(child)-w(of)-(education), etc The polarity value of each word is defined in our sentiment polarity dictionary, which includes 1880 positive words and 2490 negative words." ></td>
	<td class="line x" title="106:185	To create this dictionary, one annotator identified positive and negative words from the 50 thousand most frequent words sampled from the Web." ></td>
	<td class="line x" title="107:185	Table 6 shows some examples." ></td>
	<td class="line x" title="108:185	3.4 Emotion Classification For fine-grained emotion classification, we propose a k-nearest-neighbor approach (kNN) using the EP corpus." ></td>
	<td class="line x" title="109:185	Given an input utterance, the kNN model retrieves k-most similar labelled examples from the EP corpus." ></td>
	<td class="line x" title="110:185	Given the input The restaurant was very far but it was closed as Figure 1, for example, the kNN model finds similar labelled examples, say, labelled example {the shop was closed and Id traveled far to get there} in the EP corpus." ></td>
	<td class="line x" title="111:185	Table 6: Examples of positive and negative words P(child)|Fs(summer vacation)| qm(useful)| Rb(succeed) N r(cost)|`M(difficult)| `M(difficult)|b(failure) Ranking of similar events rank event emotion similarity 1." ></td>
	<td class="line x" title="112:185	2. 2." ></td>
	<td class="line x" title="113:185	4. 5." ></td>
	<td class="line x" title="114:185	{event1} <disappointment> 1." ></td>
	<td class="line x" title="115:185	2. 3." ></td>
	<td class="line x" title="116:185	{event2} <unpleasantness> {event3} <loneliness>          0.70 {event4} <loneliness>          0.67 0.75 0.70 {event5} <loneliness>          0.63 Ranking of emotion rank emotion score <loneliness> <unpleasantness> <disappointment> 2.0 0.75 0.70 voting Figure 4: Emotion Classification by kNN (k=5) For the similarity measure, we use cosine similarity between bag-of-words vectors; sim(I,EP)= IEP |I||EP| for input sentence I and an emotionprovoking event EPin the EP corpus." ></td>
	<td class="line x" title="117:185	The score of each class is given by the sum of its similarity scores." ></td>
	<td class="line x" title="118:185	An example is presented in Figure 4." ></td>
	<td class="line x" title="119:185	The emotion of the most similar event is disappointment, that of the second-most similar event is unpleasantness tied with loneliness." ></td>
	<td class="line x" title="120:185	After calculating the sum for each emotion, the system outputs loneliness as the emotion for the input I because the score for loneliness is the highest." ></td>
	<td class="line x" title="121:185	4 Experiments 4.1 Sentiment polarity classification We conducted experiments on sentiment polarity classification using the following two test sets: TestSet1: The first test set was a set of utterances which 6 subject speakers produced interacting with our prototype dialog system." ></td>
	<td class="line x" title="122:185	This data include 31 positive utterances, 34 negative utterances, and 25 neutral utterances." ></td>
	<td class="line x" title="123:185	TestSet2: For the second test set, we used the 1140 samples that were judged Correct with respect to sentiment polarity in Table 3." ></td>
	<td class="line x" title="124:185	491 samples (43.1%) were positive and 649 (56.9%) were negative." ></td>
	<td class="line x" title="125:185	We then added 501 neutral sentences newly sampled from the Web." ></td>
	<td class="line x" title="126:185	These samples are disjoint from the EP corpus used for training classifiers." ></td>
	<td class="line x" title="127:185	For each test set, we tested our sentiment polarity classifier in both the two-class (positive/negative) setting, where only positive or negative test samples were used, and the three-class (positive/negative/neutral) setting." ></td>
	<td class="line x" title="128:185	The performance was evaluated in F-measure." ></td>
	<td class="line x" title="129:185	885 Table 7: F-values of sentiment polarity classification (positive/negative) TestSet1 TestSet2 Pos Neg Pos Neg Word 0.839 0.853 0.794 0.842 Word + Polarity 0.833 0.857 0.793 0.841 Table 8: F-values of sentiment polarity classification (positive/negative/neutral) TestSet1 TestSet2 Pos Neg Pos Neg Word 0.743 0.758 0.610 0.742 Word + Polarity 0.758 0.769 0.610 0.742 Table 7 shows the results for the two-class setting, whereas Table 8 shows the results for the three-class." ></td>
	<td class="line x" title="130:185	Word denotes the model trained with only word n-gram features, whereas Word+Polarity denotes the model trained with n-gram features extracted from a word-polarity lattice (see Figure 3)." ></td>
	<td class="line x" title="131:185	The results shown in Table 7 indicate that both the Word and Word+Polarity models are capable of separating positive samples from negative ones at a high level of accuracy." ></td>
	<td class="line x" title="132:185	This is an important finding, given the degree of the correctness of our EP corpus." ></td>
	<td class="line x" title="133:185	As we have shown in Table 3, only 57% of samples in our EP corpus are exactly correct in terms of sentiment polarity." ></td>
	<td class="line x" title="134:185	The figures in Table 7 indicate that context-dependent samples are also useful for training a classifier." ></td>
	<td class="line x" title="135:185	Table 7 also indicates that no significant difference is found between the Word and Word+Polarity models." ></td>
	<td class="line x" title="136:185	In fact, we also examined another model which used dependency-structure information as well; however, no significant gain was achieved." ></td>
	<td class="line x" title="137:185	From these results, we speculate that, as far as the two-class sentiment polarity problem is concerned, word n-gram features might be sufficient if a very large set of labelled data are available." ></td>
	<td class="line x" title="138:185	On the other hand, Table 8 indicates that the three-class problem is much harder than the twoclass problem." ></td>
	<td class="line x" title="139:185	Specifically, positive sentences tend to be classified as neutral." ></td>
	<td class="line x" title="140:185	This method has to be improved in future models." ></td>
	<td class="line x" title="141:185	4.2 Emotion classification For fine-grained emotion classification, we used the following three test sets: TestSet1 (2p, best): Two annotators were asked to annotate each positive or negative sentence in TestSet1 with one of the 10 emotion classess." ></td>
	<td class="line x" title="142:185	The annotators chose only one emotion class even if they thought several emotions would fit a sentence." ></td>
	<td class="line x" title="143:185	Some examples are shown in Table 9." ></td>
	<td class="line x" title="144:185	The inter-annotator agreement is =0.76 in the kappa statistic (Cohen, 1960)." ></td>
	<td class="line x" title="145:185	For sentences annotated with two different labels (i.e. in the cases where the two annotators disagreed with), both labels were considered correct in the experiments  a models answer was considered correct if it was identical with either of the two labels." ></td>
	<td class="line x" title="146:185	TestSet1 (1p, acceptable): One of the above two annotators was asked to annotate each positive or negative sentence in TestSet1 with all the emotions involved in it." ></td>
	<td class="line x" title="147:185	The number of emotions for a positive sentence was 1.48 on average, and 2.47 for negative sentences." ></td>
	<td class="line x" title="148:185	Table 10 lists some examples." ></td>
	<td class="line x" title="149:185	In the experiments, a models answer was considered correct if it was identical with one of the labelled classes." ></td>
	<td class="line x" title="150:185	TestSet2: For TestSet2, we used the results of our judgments on the correctness for estimating the quality of the EP corpus described in Section 3.2." ></td>
	<td class="line x" title="151:185	In the experiments, the following two models were compared: Baseline: The baseline model simulates the method proposed by (Kozareva et al., 2007)." ></td>
	<td class="line x" title="152:185	Given an input sentence, their model first estimates the pointwise mutual information (PMI) between each content word cw j included in the sentence and emotion expression e  {anger, disgust, fear, joy, sudness, surprise} by PMI(e,cw)=log hits(e,cw) hits(e)hits(cw) , where hits(x) is a hit count of word(s) x on a Web search engine." ></td>
	<td class="line x" title="153:185	The model then calculates the score of each emotion class E i by summing the PMI scores between each content word cw j in the input and emotion expression e i corresponding to that emotion class: score(E i )= summationtext j PMI(e i ,cw j )." ></td>
	<td class="line x" title="154:185	Finally, the model chooses the best scored emotion class as an output." ></td>
	<td class="line x" title="155:185	For our experiments, we selected the following 10 emotion expressions: .`M(happy),`M(pleased),	(relieved),M(affraid),u`M(sad), (disappointed),O(hate),	`M(lonely), (anxious),qh`M(angry) For hit counts, we used the Google search engine." ></td>
	<td class="line x" title="156:185	886 Table 9: Examples of TestSet1 (2p, best) Annotator A Annotator B tlh(I got a Christmas present) happiness happiness awHt!|tX(Im going to go to my friends house ) pleasantness pleasantness V_tlh U	Z`h(It rained suddenly when I went to see the cherry blossoms) sadness sadness 	L:pqrTsM(My car cant move because of the traffic jam) unpleasantness anger Table 10: Examples of TestSet1 (1p, acceptable) Annotator A tlh(I got a Christmas present) happiness awHt!|tX(Im going to go to my friends house ) pleasantness, happiness V_tlh U	Z`h(It rained suddenly when I went to see the cherry blossoms) anger, sad, unpleasantness, disappointment 	L:pqrTsM(My car cant move because of the traffic jam) unpleasantness, anger :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| :::::| ::::| :::::| :::::| :::::| :::::| :::::| :::::| : :| : : :| : : :| : : :| : : :| : : :| : : :| : : :| : : :| : : :| : : : :| : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :w : : : : : : : : : : : : : : : : : : : : : : : :w : : : : : : : : :w : :w : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : Figure 5: Results of emotion classification k-NN: We tested the 1-NN, 3-NN and 10-NN models." ></td>
	<td class="line x" title="157:185	In each model, we examined a single-step emotion classification and twostep emotion classification." ></td>
	<td class="line x" title="158:185	In the former method, the kNN model retrieves k-most similar examples from the all of the EP corpus." ></td>
	<td class="line x" title="159:185	In the latter method, when the sentiment polarity of the input utterance has obtained by the sentiment polarity classifier, the kNN model retrieves similar examples from only the examples whose sentiment polarity are the same as the input utterance in the EP corpus." ></td>
	<td class="line x" title="160:185	The results are shown in Figure 5." ></td>
	<td class="line x" title="161:185	Emotion Classification denotes the single-step models, whereas Sentiment Polarity + Emotion Classification denotes the two-step models." ></td>
	<td class="line x" title="162:185	An important observation from Figure 5 is that our models remarkably outperformed the baseline." ></td>
	<td class="line x" title="163:185	Apparently, an important motivation behind Kozareva et al.(2007)s method is that it does not require any manual supervion." ></td>
	<td class="line x" title="165:185	However, our models, which rely on emotion-provoking event instances, are also totally unsupervised  no supervision is required to collect emotion-provoking event instances." ></td>
	<td class="line x" title="166:185	Given this commonality between the two methods, the superiority of our method in accuracy can be considered as a crucial advantage." ></td>
	<td class="line x" title="167:185	Regarding the issue of single-step vs. two-step, Figure 5 indicates that the two-step models tended to outperform the single-step models for all the test set." ></td>
	<td class="line x" title="168:185	A paired t-test for TestSet2, however, did not reach significance 2 . So we next examined this issue in further detail." ></td>
	<td class="line x" title="169:185	As argued in Section 3.3, in the context of human-computer dialog, a misunderstanding of the users emotion at the level of sentiment polarity would lead to a serious problem, which we call a fatal error." ></td>
	<td class="line x" title="170:185	On the other hand, misclassifying a case ofhappiness as, for example,pleasantness may well be tolerable." ></td>
	<td class="line x" title="171:185	Table 11 shows the ratio of fatal errors for each model." ></td>
	<td class="line x" title="172:185	For TestSet2, the single-step 10-NN model made fatal errors in 30% of cases, while the two-step 10-NN model in only 17%." ></td>
	<td class="line x" title="173:185	This improvement is statistically significant (p<0.01)." ></td>
	<td class="line x" title="174:185	5 Conclusion In this paper, we have addressed the issue of emotion classification assuming its potential applications to be human-computer dialog system including active-listening dialog." ></td>
	<td class="line x" title="175:185	We first automatically collected a huge collection, as many as 1.3M, of emotion-provoking event instances from the Web." ></td>
	<td class="line x" title="176:185	We then decomposed the emotion classification task into two sub-steps: sentiment polarity classification and emotion classification." ></td>
	<td class="line x" title="177:185	In sentiment polarity classification, we used the EP-corpus as training data." ></td>
	<td class="line x" title="178:185	The results of the polarity classification experiment showed that word n-gram features alone are more or less sufficient to classify positive and negative sentences when a very large amount of training data is available." ></td>
	<td class="line x" title="179:185	In the emotion classification experiments, on the other hand, 2 The data size of TestSet1 was not sufficient for statistical significance test 887 Table 11: Fatal error rate in emotion classification experiments Baseline Emotion Classification Sentiment Polarity 1-NN 3-NN 10-NN + Emotion Classification TestSet1 49.2% 29.2% 26.2% 24.6% 15.4% TestSet2 41.5% 37.6% 32.8% 30.0% 17.0% we adopted the k-nearest-neighbor (kNN) method." ></td>
	<td class="line x" title="180:185	The results of the experiments showed that our method significantly outperformed the baseline method." ></td>
	<td class="line x" title="181:185	The results also showed that our twostep emotion classification was effective for finegrained emotion classification." ></td>
	<td class="line x" title="182:185	Specifically, fatal errors were significantly reduced with sentiment polarity classification before fine-grained emotion classification." ></td>
	<td class="line x" title="183:185	For future work, we first need to examine other machine learning methods to see their advantages and disadvantages in our task." ></td>
	<td class="line x" title="184:185	We also need an extensive improvement in identifying neutral sentences." ></td>
	<td class="line x" title="185:185	Finally, we are planning to apply our model to the active-listening dialog system that our group has been developing and investigate its effects on the users behavior." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1135
Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text
Zagibalov, Taras;Carroll, John A.;"></td>
	<td class="line x" title="1:153	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 10731080 Manchester, August 2008 Automatic Seed Word Selection for Unsupervised Sentiment Classification of Chinese Text Taras Zagibalov    John Carroll University of Sussex Department of Informatics Brighton  BN1 9QH, UK {T.Zagibalov,J.A.Carroll}@sussex.ac.uk Abstract We describe and evaluate a new method of automatic seed word selection for unsupervised sentiment classification of product reviews in Chinese." ></td>
	<td class="line x" title="2:153	The whole method is unsupervised and does not require any annotated training data; it only requires information about commonly occurring negations and adverbials." ></td>
	<td class="line x" title="3:153	Unsupervised techniques are promising for this task since they avoid problems of domain-dependency typically associated with supervised methods." ></td>
	<td class="line x" title="4:153	The results obtained are close to those of supervised classifiers and sometimes better, up to an F1 of 92%." ></td>
	<td class="line x" title="5:153	1 Introduction Automatic classification of document sentiment (and more generally extraction of opinion from text) has recently attracted a lot of interest." ></td>
	<td class="line x" title="6:153	One of the main reasons for this is the importance of such  information  to  companies,  other organizations, and individuals." ></td>
	<td class="line x" title="7:153	Applications include marketing research tools that help a company see market or media reaction towards their brands, products or services, or search engines that help potential purchasers make an informed choice of a product they want to buy." ></td>
	<td class="line x" title="8:153	Sentiment classification research has drawn on and contributed to research in text classification, unsupervised machine learning, and crossdomain adaptation." ></td>
	<td class="line x" title="9:153	This paper presents a new, automatic approach to automatic seed word selection as part of sentiment classification of product reviews written in Chinese, which addresses the problem of do  2008." ></td>
	<td class="line x" title="10:153	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="11:153	Some rights reserved." ></td>
	<td class="line x" title="12:153	main-dependency of sentiment classification that has been observed in previous work." ></td>
	<td class="line x" title="13:153	It may also facilitate building sentiment classification systems in other languages since the approach assumes a very small amount of linguistic knowledge: the only language-specific information required is a basic description of the most frequent negated adverbial constructions in the language." ></td>
	<td class="line x" title="14:153	The paper is structured as follows." ></td>
	<td class="line x" title="15:153	Section 2 surveys related work in sentiment classification, unsupervised machine learning and Chinese language processing." ></td>
	<td class="line x" title="16:153	Section 3 motivates our approach, which is described in detail in Section 4." ></td>
	<td class="line x" title="17:153	The data used for experiments and baselines, as well as the results of experiments are covered in Section 5." ></td>
	<td class="line x" title="18:153	Section 6 discusses the lessons learned and proposes directions for future work." ></td>
	<td class="line x" title="19:153	2 Related Work 2.1 Sentiment Classification Most work on sentiment classification has used approaches based on supervised machine learning." ></td>
	<td class="line x" title="20:153	For example, Pang et al.(2002) collected movie reviews that had been annotated with respect to sentiment by the authors of the reviews, and used this data to train supervised classifiers." ></td>
	<td class="line x" title="22:153	A number of studies have investigated the impact on classification accuracy of different factors, including choice of feature set, machine learning algorithm, and pre-selection of the segments of text to be classified." ></td>
	<td class="line x" title="23:153	For example, Dave et al.(2003) experiment with the use of linguistic, statistical and n-gram features and measures for feature selection and weighting." ></td>
	<td class="line x" title="25:153	Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts." ></td>
	<td class="line x" title="26:153	Yu and Hatzivassiloglou (2003) use semanticallyoriented words for identification of polarity at the sentence level." ></td>
	<td class="line x" title="27:153	Most of this work assumes binary classification (positive and negative), some1073 times with the addition of a neutral class (in terms of polarity, representing lack of sentiment)." ></td>
	<td class="line x" title="28:153	While supervised systems generally achieve reasonably high accuracy, they do so only on test data that is similar to the training data." ></td>
	<td class="line x" title="29:153	To move to another domain one would have to collect annotated data in the new domain and retrain the classifier." ></td>
	<td class="line x" title="30:153	Engstrm (2004) reports decreased accuracy in cross-domain classification since sentiment in different domains is often expressed in different ways." ></td>
	<td class="line x" title="31:153	However, it is impossible in practice to have annotated data for all possible domains of interest." ></td>
	<td class="line x" title="32:153	Aue and Gamon (2005) attempt to solve the problem of the absence of large amounts of labeled data by customizing sentiment classifiers to new domains using training data from other domains." ></td>
	<td class="line x" title="33:153	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers using structural correspondence learning." ></td>
	<td class="line x" title="35:153	Read (2005) also observed significant differences between the accuracy of classification of reviews in the same domain but published in different time periods." ></td>
	<td class="line x" title="36:153	Recently, there has been a shift of interest towards more fine-grained approaches to processing of sentiment, in which opinion is extracted at the sentence level, sometimes including information about different features of a product that are commented on and/or the opinion holder (Hu and Liu, 2004; Ku et al., 2006)." ></td>
	<td class="line x" title="37:153	But even in such approaches, McDonald et al.(2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text." ></td>
	<td class="line x" title="39:153	2.2 Unsupervised Approach One way of tackling the problem of domain dependency could be to use an approach that does not rely on annotated data." ></td>
	<td class="line oc" title="40:153	Turney (2002) describes a method of sentiment classification using two human-selected seed words (the words poor and excellent) in conjunction with a very large text corpus; the semantic orientation of phrases is computed as their association with the seed words (as measured by pointwise mutual information)." ></td>
	<td class="line o" title="41:153	The sentiment of a document is calculated as the average semantic orientation of all such phrases." ></td>
	<td class="line x" title="42:153	Yarowsky (1995) describes a 'semi-unsupervised' approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations." ></td>
	<td class="line x" title="43:153	These annotations are used to start an iterative process of learning information about the contexts in which senses of words appear, in each iteration labeling senses of previously unlabeled word tokens using information from the previous iteration." ></td>
	<td class="line x" title="44:153	2.3 Chinese Language Processing A major issue in processing Chinese text is the fact that words are not delimited in the written language." ></td>
	<td class="line x" title="45:153	In many cases, NLP researchers working with Chinese use an initial segmentation module that is intended to break a text into words." ></td>
	<td class="line x" title="46:153	Although this can facilitate the use of subsequent computational techniques, there is no a clear definition of what a 'word' is in the modern Chinese language, so the use of such segmenters is of dubious theoretical status; indeed, good results have been reported from systems which do not assume such pre-processing (Foo and Li, 2004; Xu et al., 2004)." ></td>
	<td class="line x" title="47:153	2.4 Seed Word Selection We are not aware of any sentiment analysis system that uses unsupervised seed word selection." ></td>
	<td class="line x" title="48:153	However, Pang et al.(2002) showed that it is difficult to get good coverage of a target domain from manually selected words, and even simple corpus frequency counts may produce a better list of features for supervised classification: human-created lists resulted in 64% accuracy on a movie review corpus, while a list of frequent words scored 69%." ></td>
	<td class="line x" title="50:153	Pang et al. also observed that some words without any significant emotional orientation were quite good features: for example, the word still turned out to be a good indicator of positive reviews as it was often used in sentences such as Still, though, it was worth seeing''." ></td>
	<td class="line x" title="51:153	3 Our Approach Our main goal is to overcome the problem of domain-dependency in sentiment classification." ></td>
	<td class="line x" title="52:153	Unsupervised approaches seem promising in this regard, since they do not require annotated training data, just access to sufficient raw text in each domain." ></td>
	<td class="line x" title="53:153	We base our approach on a previously described, 'almost-unsupervised' system that starts with only a single, human-selected seed  (good) and uses an iterative method to extract a training sub-corpus (Zagibalov & Carroll, 2008)." ></td>
	<td class="line x" title="54:153	The approach does not use a word segmentation module; in this paper we use the term 'lexical item' to denote any sequence of Chinese characters that is treated by the system as a unit, whatever it is linguistically  a morpheme, a word or a phrase." ></td>
	<td class="line x" title="55:153	1074 Our initial aim was to investigate ways of improving the classifier by automatically finding a better seed, because Zagibalov & Carroll indicate that in different domains they could, by manual trial and error, find a seed other than  (good) which produced better results." ></td>
	<td class="line x" title="56:153	To find such a seed automatically, we make two assumptions: 1." ></td>
	<td class="line x" title="57:153	Attitude is often expressed through the negation of vocabulary items with the opposite meaning; for example in Chinese it is more common to say not good than bad (Tan, 2002)." ></td>
	<td class="line x" title="58:153	Zagibalov & Carroll's system uses this observation to find negative lexical items while nevertheless starting only from a positive seed." ></td>
	<td class="line x" title="59:153	This leads us to believe that it is possible to find candidate seeds themselves by looking for sequences of characters which are used with negation." ></td>
	<td class="line x" title="60:153	2." ></td>
	<td class="line x" title="61:153	The polarity of a candidate seed needs to be determined." ></td>
	<td class="line x" title="62:153	To do this we assume we can use the lexical item   (good) as a gold standard for positive lexical items and compare the pattern of contexts a candidate seed occurs in to the pattern exhibited by the gold standard." ></td>
	<td class="line x" title="63:153	Looking at product review corpora, we observed that good is always more often used without negation in positive texts, while in negative texts it is more often used with negation (e.g. not good)." ></td>
	<td class="line x" title="64:153	Also, good occurs more often in positive texts than negative, and more frequently without negation than with it." ></td>
	<td class="line x" title="65:153	We use the latter observation as the basis for identifying seed lexical items, finding those which occur with negation but more frequently occur without it." ></td>
	<td class="line x" title="66:153	As well as detecting negation1 we also use adverbials2 to avoid hypothesizing non-contentful seeds: the characters following the sequence of a negation and an adverbial are in general contentful units, as opposed to parts of words, function words, etc. In what follows we refer to such constructions as negated adverbial constructions." ></td>
	<td class="line x" title="67:153	1We use only six frequently occurring negations:  (bu),   (buhui),  (meiyou),  (baituo),  (mianqu), and  (bimian)." ></td>
	<td class="line x" title="68:153	We are trying to be as language-independent as possible so we take a simplistic approach to detecting negation." ></td>
	<td class="line x" title="69:153	2We use five frequently occurring adverbials:  (hen),  (feichang),  (tai),  (zui), and  (bijiao)." ></td>
	<td class="line x" title="70:153	Similarly to negation, we deliberately take a simplistic approach." ></td>
	<td class="line x" title="71:153	4 Method We use a similar sentiment classifier and iterative retraining technique to the almost-unsupervised system of Zagibalov & Carroll (2008), summarized below in Sections 4.2 and 4.3." ></td>
	<td class="line x" title="72:153	The main new contributions of this paper are techniques for automatically finding the seeds from raw text in a particular domain (Section 4.1), and for detecting when the process should stop (Section 4.4)." ></td>
	<td class="line x" title="73:153	This new system therefore differs from that of Zagibalov & Carroll (2008) in being completely unsupervised and not depending on arbitrary iteration limits." ></td>
	<td class="line x" title="74:153	(The evaluation also differs since we focus in this paper on the effects of domain on sentiment classification accuracy)." ></td>
	<td class="line x" title="75:153	4.1 Seed Lexical Item Identification The first step is to identify suitable positive seeds for the given corpus." ></td>
	<td class="line x" title="76:153	The intuition behind the way this is done is outlined above in Section 3." ></td>
	<td class="line x" title="77:153	The algorithm is as follows: 1." ></td>
	<td class="line x" title="78:153	find all sequences of characters between non-character symbols (i.e. punctuation marks, digits and so on) that contain negation and an adverbial, split the sequence at the negation, and store the character sequence that follows the negated adverbial construction; 2." ></td>
	<td class="line x" title="79:153	count the number of occurrences of each distinct sequence that follows a negated adverbial construction (X); 3." ></td>
	<td class="line x" title="80:153	count the number of occurrences of each distinct sequence without the construction (Y); 4." ></td>
	<td class="line x" title="81:153	find all sequences with Y  X > 0." ></td>
	<td class="line x" title="82:153	4.2 Sentiment Classification This approach to Chinese language processing does not use pre-segmentation (in the sense discussed in Section 2.3) or grammatical analysis: the basic unit of processing is the 'lexical item', each of which is a sequence of one or more Chinese characters excluding punctuation marks (so a lexical item may actually form part of a word, a whole word or a sequence of words), and 'zones', each of which is a sequence of characters delimited by punctuation marks." ></td>
	<td class="line x" title="83:153	Each zone is classified as either positive or negative based whether positive or negative vocabulary items predominate." ></td>
	<td class="line x" title="84:153	As there are two parts of the vocabulary (positive and negative), we correspondingly calculate two scores (Si , 1075 where i is either positive or negative) using Equation (1), where Ld is the length in characters of a matching lexical item (raised to the power of two to increase the significance of longer items which capture more context), Lphrase is the length of the current zone in characters, Sd is the current sentiment score of the matching lexical item (initially 1.0), and Nd is a negation check coefficient." ></td>
	<td class="line x" title="85:153	Si= Ld 2 LphraseSd Nd                    (1) The negation check is a regular expression which determines if the lexical item is preceded by a negation within its enclosing zone." ></td>
	<td class="line x" title="86:153	If a negation is found then Nd is set to 1." ></td>
	<td class="line x" title="87:153	The sentiment score of a zone is the sum of sentiment of all the items found in it." ></td>
	<td class="line x" title="88:153	To determine the sentiment orientation of the whole document, the classifier computes the difference between the number of positive and negative zones." ></td>
	<td class="line x" title="89:153	If the result is greater than zero the document is classified as positive, and vice versa." ></td>
	<td class="line x" title="90:153	4.3 Iterative Retraining Iterative retraining is used to enlarge the initial seed vocabulary into a comprehensive vocabulary list of sentiment-bearing lexical items." ></td>
	<td class="line x" title="91:153	In each iteration, the current version of the classifier is run on the input corpus to classify each document, resulting in a training subcorpus of positive and a negative documents." ></td>
	<td class="line x" title="92:153	The subcorpus is used to adjust the scores of existing positive and negative vocabulary items and to find new items to be included in the vocabulary." ></td>
	<td class="line x" title="93:153	Each lexical item that occurs at least twice in the corpus is a candidate for inclusion in the vocabulary list." ></td>
	<td class="line x" title="94:153	After candidate items are found, the system calculates their relative frequencies in both the positive and negative parts of the current training subcorpus." ></td>
	<td class="line x" title="95:153	The system also checks for negation while counting occurrences: if a lexical item is preceded by a negation, its count is reduced by one." ></td>
	<td class="line x" title="96:153	For all candidate items we compare their relative frequencies in the positive and negative documents in the subcorpus using Equation (2)." ></td>
	<td class="line x" title="97:153	difference= F pFnF pFn/2         (2) If difference < 1, then the frequencies are similar and the item does not have enough distinguishing power, so it is not included in the vocabulary." ></td>
	<td class="line x" title="98:153	Otherwise the sentiment score of the item is (re-)calculated  according to Equation (3) for positive items, and analogously for negative items." ></td>
	<td class="line x" title="99:153	F pFn         (3) Finally, the adjusted vocabulary list with the new scores is ready for the next iteration3." ></td>
	<td class="line x" title="100:153	4.4 Iteration Control To maximize the number of productive iterations while avoiding unnecessary processing and arbitrary iteration limits, iterative retraining is stopped when there is no change to the classification of any document over the previous two iterations." ></td>
	<td class="line x" title="101:153	5 Experiments 5.1 Data As our approach is unsupervised, we do not use an annotated training corpus, but run our iterative procedure on the raw data extracted from an annotated test corpus, and evaluate the final accuracy of the system with respect to the annotations in that corpus." ></td>
	<td class="line x" title="102:153	Our test corpus is derived from product reviews harvested from the website IT1684." ></td>
	<td class="line x" title="103:153	All the reviews were tagged by their authors as either positive or negative overall." ></td>
	<td class="line x" title="104:153	Most reviews consist of two or three distinct parts: positive opinions, negative opinions, and comments ('other')  although some reviews have only one part." ></td>
	<td class="line x" title="105:153	We removed duplicate reviews automatically using approximate matching, giving a corpus of 29531 reviews of which 23122 are positive (78%) and 6409 are negative (22%)." ></td>
	<td class="line x" title="106:153	The total number of different products in the corpus is 10631, the number of product categories is 255, and most of the reviewed products are either software products or consumer electronics." ></td>
	<td class="line x" title="107:153	Unfortunately, it appears that some users misuse the sentiment 3An alternative approach might be to use point-wise mutual information instead of relative frequencies of newly found features in a subcorpus produced in the previous iteration." ></td>
	<td class="line x" title="108:153	However, in preliminary experiments, SO-PMI did not produce good corpora from the first iteration." ></td>
	<td class="line x" title="109:153	Also, it is not clear how to manage subsequent iterations since PMI would have to be calculated between thousands of new vocabulary items and every newly found sequence of characters, which would be computationally intractable." ></td>
	<td class="line x" title="110:153	4http://product.it168.com 1076 tagging facility on the website so quite a lot of reviews have incorrect tags." ></td>
	<td class="line x" title="111:153	However, the parts of the reviews are much more reliably identified as being positive or negative so we used these as the items of the test corpus." ></td>
	<td class="line x" title="112:153	In the experiments described below we use 10 subcorpora containing a total of 7982 reviews, distributed between product types as shown in Table 1." ></td>
	<td class="line x" title="113:153	Corpus/product type Reviews Monitors 683 Mobile phones 2317 Digital cameras 1705 MP3 players 779 Computer parts (CD-drives, motherboards) 308 Video cameras and lenses 361 Networking (routers, network cards) 350 Office equipment (copiers, multifunction devices, scanners) 611 Printers (laser, inkjet) 569 Computer peripherals (mice, keyboards, speakers) 457 Table 1." ></td>
	<td class="line x" title="114:153	Product types and sizes of the test corpora." ></td>
	<td class="line x" title="115:153	We constructed five of the corpora by combining smaller ones of 100250 reviews each (as indicated in parentheses in Table 1) in order to have reasonable amounts of data." ></td>
	<td class="line x" title="116:153	Each corpus has equal numbers of positive and negative reviews so we can derive upper bounds from the corpora (Section 5.2) by applying supervised classifiers." ></td>
	<td class="line x" title="117:153	We balance the corpora since (at least on this data) these classifiers perform less well with skewed class distributions5." ></td>
	<td class="line x" title="118:153	5.2 Baseline and Upper Bound Since the corpora are balanced with respect to sentiment orientation the nave (unsupervised) baseline is 50%." ></td>
	<td class="line x" title="119:153	We also produced an upper bound using Naive Bayes multinomial (NBm) and Support Vector Machine (SVM)6 classifiers with the NTU Sentiment Dictionary (Ku et al., 2006) vocabulary items as the feature set." ></td>
	<td class="line x" title="120:153	The dictionary contains 2809 items in the 'positive' part and 8273 items in the 'negative'." ></td>
	<td class="line x" title="121:153	We ran 5We have made this corpus publicly available at http:// www.informatics.sussex.ac.uk/users/tz21/coling08.zip 6We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/ml/ weka ) both classifiers in 10-fold stratified cross-validation mode, resulting in the accuracies shown in Table 2." ></td>
	<td class="line x" title="122:153	The macroaveraged accuracies across all 10 corpora are 82.78% (NBm) and 80.89% (SVM)." ></td>
	<td class="line x" title="123:153	Corpus Nbm (%) SVM (%) Monitors 86.21 83.87 Mobile phones 86.52 84.49 Digital cameras 82.27 82.04 MP3 players 82.64 79.43 Computer parts 81.10 79.47 Video cameras and lenses 83.05 84.16 Networking 77.65 75.35 Office equipment 82.13 80.00 Printers 81.33 79.57 Computer peripherals 84.86 80.48 Table 2." ></td>
	<td class="line x" title="124:153	Upper bound accuracies." ></td>
	<td class="line x" title="125:153	We also tried adding the negations and adverbials specified in Section 3 to the feature set, and this resulted in slightly improved accuracies, of 83.90% (Nbm) and 82.49% (SVM)." ></td>
	<td class="line x" title="126:153	An alternative approach would have been to automatically segment the reviews and then derive a feature set of a manageable size by setting a threshold on word frequencies; however the extra processing means that this is a less valid upper bound." ></td>
	<td class="line oc" title="127:153	Another possible comparison could be with a version of Turney's (2002) sentiment classification method applied to Chinese." ></td>
	<td class="line n" title="128:153	However, the results would not be comparable since Turney's method would require the additional use of very large text corpus and the manual selection of positive and negative seed words." ></td>
	<td class="line x" title="129:153	5.3 Experiment 1 To be able to compare to the accuracy of the almost-unsupervised approach of Zagibalov & Carroll (2008), we ran our system using the seed  (good) for each corpus." ></td>
	<td class="line x" title="130:153	The results are shown in Table 3." ></td>
	<td class="line x" title="131:153	We compute precision, recall and F1 measure rather than just accuracy, since our classifier can omit some reviews whereas the supervised classifiers attempt to classify all reviews." ></td>
	<td class="line x" title="132:153	The macroaveraged F1 measure is 80.55, which beats the nave baseline by over 30 percentage points, and approaches the two upper bounds." ></td>
	<td class="line x" title="133:153	1077 Corpus Iter P R F1 Monitors 12 86.62 86.24 86.43 Mobile phones 11 90.15 89.68 89.91 Digital cameras 13 81.33 80.23 80.78 MP3 players 13 86.10 85.10 85.60 Computer parts 10 69.10 67.53 68.31 Video cameras and lenses 10 82.81 81.44 82.12 Networking 11 69.28 68.29 68.78 Office equipment 12 81.83 80.36 81.09 Printers 12 81.04 79.61 80.32 Computer peripherals 10 82.20 81.84 82.02 Macroaverage 81.05 80.03 80.54 Table 3." ></td>
	<td class="line x" title="134:153	Results with the single, manually chosen seed  (good) for each corpus." ></td>
	<td class="line x" title="135:153	5.4 Experiment 2 We then ran our full system, including the seed identifier." ></td>
	<td class="line x" title="136:153	Appendix A shows that for most of the corpora the algorithm found different (highly domain-salient) seeds." ></td>
	<td class="line x" title="137:153	Table 4 shows the results achieved." ></td>
	<td class="line x" title="138:153	Corpus Iter P R F1 Monitors 11 85.57 85.07 85.32 Mobile phones 10 92.63 92.19 92.41 Digital cameras 13 84.92 83.58 84.24 MP3 players 13 88.69 87.55 88.11 Computer parts 12 77.78 77.27 77.52 Video cameras and lenses 11 83.62 81.99 82.8 Networking 13 72.83 72.00 72.41 Office equipment 10 82.42 81.34 81.88 Printers 12 81.04 79.61 80.32 Computer peripherals 10 82.24 82.06 82.15 Macroaverage 83.17 82.27 82.72 Table 4." ></td>
	<td class="line x" title="139:153	Results with the seeds automatically identified for each corpus." ></td>
	<td class="line x" title="140:153	Across all 10 subcorpora, the improvement using automatically identified seed words compared with just using the seed good is significant (paired t-test, P<0.0001), and the F1 measure lies between the two upper bounds." ></td>
	<td class="line x" title="141:153	6 Conclusions and Future Work The unsupervised approach to seed words selection for sentiment classification presented in this paper produces results which in most cases are close to the results of supervised classifiers and to the previous almost-unsupervised approach: eight out of ten results showed improvement over the human selected seed word and three results outperformed the supervised approach, while three other results were less than 1% inferior to the supervised ones." ></td>
	<td class="line x" title="142:153	How does it happen that the chosen seed is usually (in our dataset  always) positive?" ></td>
	<td class="line x" title="143:153	We think that this happens due to the socially accepted norm of behaviour: as a rule one needs to be friendly to communicate with others." ></td>
	<td class="line x" title="144:153	This in turn defines linguistic means of expressing ideas  they will be at least slightly positive overall." ></td>
	<td class="line x" title="145:153	The higher prevalence of positive reviews has been observed previously: for example, in our corpus before we balanced it almost 80% of reviews were positive; Pang et al.(2002) constructed their move review corpus from an original dataset of 1301 positive and 752 negative reviews (63% positive)." ></td>
	<td class="line x" title="147:153	Ghose et al.(2007) quote typical examples of highly positive language used in the online marketplace." ></td>
	<td class="line x" title="149:153	We can make a preliminary conclusion that a relatively high frequency of positive words is determined by the usage of language that reflects the social behaviour of people." ></td>
	<td class="line x" title="150:153	In future work we intend to explore these issues of positivity of language use." ></td>
	<td class="line x" title="151:153	We will also apply our approach to other genres containing some quantity of evaluative language (for example newspaper articles), and see if it works equally well for languages other than Chinese." ></td>
	<td class="line x" title="152:153	It is also likely we can use a smaller set of negation words and adverbials to produce the seed lists." ></td>
	<td class="line x" title="153:153	Acknowledgements The first author is supported by the Ford Foundation International Fellowships Program." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1058
Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis
Wan, Xiaojun;"></td>
	<td class="line x" title="1:180	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 553561, Honolulu, October 2008." ></td>
	<td class="line x" title="2:180	c2008 Association for Computational Linguistics Using Bilingual Knowledge and Ensemble Techniques for Unsupervised Chinese Sentiment Analysis  Xiaojun Wan Institute of Compute Science and Technology Peking University Beijing 100871, China wanxiaojun@icst.pku.edu.cn  Abstract It is a challenging task to identify sentiment polarity of Chinese reviews because the resources for Chinese sentiment analysis are limited." ></td>
	<td class="line x" title="3:180	Instead of leveraging only monolingual Chinese knowledge, this study proposes a novel approach to leverage reliable English resources to improve Chinese sentiment analysis." ></td>
	<td class="line x" title="4:180	Rather than simply projecting English resources onto Chinese resources, our approach first translates Chinese reviews into English reviews by machine translation services, and then identifies the sentiment polarity of English reviews by directly leveraging English resources." ></td>
	<td class="line x" title="5:180	Furthermore, our approach performs sentiment analysis for both Chinese reviews and English reviews, and then uses ensemble methods to combine the individual analysis results." ></td>
	<td class="line x" title="6:180	Experimental results on a dataset of 886 Chinese product reviews demonstrate the effectiveness of the proposed approach." ></td>
	<td class="line x" title="7:180	The individual analysis of the translated English reviews outperforms the individual analysis of the original Chinese reviews, and the combination of the individual analysis results further improves the performance." ></td>
	<td class="line x" title="8:180	1 Introduction In recent years, sentiment analysis (including subjective/objective analysis, polarity identification, opinion extraction, etc.) has drawn much attention in the NLP field." ></td>
	<td class="line x" title="9:180	In this study, the objective of sentiment analysis is to annotate a given text for polarity orientation (positive/negative)." ></td>
	<td class="line x" title="10:180	Polarity orientation identification has many useful applications, including opinion summarization (Ku et al., 2006) and sentiment retrieval (Eguchi and Lavrenko, 2006)." ></td>
	<td class="line x" title="11:180	To date, most of the research focuses on English and a variety of reliable English resources for sentiment analysis are available, including polarity lexicon, contextual valence shifters, etc. However, the resources for other languages are limited." ></td>
	<td class="line x" title="12:180	In particular, few reliable resources are available for Chinese sentiment analysis 1  and it is not a trivial task to manually label reliable Chinese sentiment resources." ></td>
	<td class="line x" title="13:180	Instead of using only the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources." ></td>
	<td class="line x" title="14:180	Generally speaking, there are two unsupervised scenarios for borrowing English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al.(2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated." ></td>
	<td class="line x" title="16:180	In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources." ></td>
	<td class="line x" title="17:180	Furthermore, ensemble methods are employed to combine the individual analysis results in each language (i.e. Chinese and English) in order to obtain improved results." ></td>
	<td class="line x" title="18:180	Given machine translation services between the selected target language and English, the proposed approach can be applied to any other languages as well." ></td>
	<td class="line x" title="19:180	Experiments have been performed on a dataset of 886 Chinese product reviews." ></td>
	<td class="line x" title="20:180	Two commercial  1  This study focuses on Simplified Chinese." ></td>
	<td class="line x" title="21:180	553 machine translation services (i.e. Google Translate and Yahoo Babel Fish) and a baseline dictionarybased system are used for translating Chinese reviews into English reviews." ></td>
	<td class="line x" title="22:180	Experimental results show that the analysis of English reviews translated by the commercial translation services outperforms the analysis of original Chinese reviews." ></td>
	<td class="line x" title="23:180	Moreover, the analysis performance can be further improved by combining the individual analysis results in different languages." ></td>
	<td class="line x" title="24:180	The results also demonstrate that our proposed approach is more effective than the approach that leverages generated Chinese resources." ></td>
	<td class="line x" title="25:180	The rest of this paper is organized as follows: Section 2 introduces related work." ></td>
	<td class="line x" title="26:180	The proposed approach is described in detail in Section 3." ></td>
	<td class="line x" title="27:180	Section 4 shows the experimental results." ></td>
	<td class="line x" title="28:180	Lastly we conclude this paper in Section 5." ></td>
	<td class="line x" title="29:180	2 Related Work Polarity identification can be performed on word level, sentence level or document level." ></td>
	<td class="line x" title="30:180	Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy." ></td>
	<td class="line x" title="31:180	2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy." ></td>
	<td class="line x" title="32:180	2004) Word-level or sentence-level sentiment analysis is not the focus of this paper." ></td>
	<td class="line x" title="33:180	Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised." ></td>
	<td class="line x" title="34:180	Unsupervised methods involve deriving a sentiment metric for text without training corpus." ></td>
	<td class="line oc" title="35:180	Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method." ></td>
	<td class="line x" title="36:180	Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words." ></td>
	<td class="line x" title="37:180	Hiroshi et al.(2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents." ></td>
	<td class="line x" title="39:180	Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers." ></td>
	<td class="line x" title="40:180	Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text." ></td>
	<td class="line x" title="41:180	Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier." ></td>
	<td class="line x" title="42:180	Since the work of Pang et al.(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005)." ></td>
	<td class="line x" title="44:180	Most recently, McDonald et al.(2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="46:180	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="48:180	Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting." ></td>
	<td class="line x" title="49:180	Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007)." ></td>
	<td class="line x" title="50:180	Such work represents heuristic extensions of the unsupervised or supervised methods for English sentiment analysis." ></td>
	<td class="line x" title="51:180	To date, the most closely related work is Mihalcea et al.(2007), which explores cross-lingual projections to generate subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English." ></td>
	<td class="line x" title="53:180	They have investigated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections." ></td>
	<td class="line x" title="54:180	In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments." ></td>
	<td class="line x" title="55:180	Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni." ></td>
	<td class="line x" title="56:180	2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="57:180	3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources 554 for sentiment analysis are limited or unreliable." ></td>
	<td class="line x" title="58:180	This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge." ></td>
	<td class="line x" title="59:180	The framework of our approach is illustrated in Figure 1." ></td>
	<td class="line x" title="60:180	A Chinese review is translated into the corresponding English review using machine translation services, and then the Chinese review and the English review are analyzed based on Chinese resources and English resources, respectively." ></td>
	<td class="line x" title="61:180	The analysis results are then combined to obtain more accurate results under the assumption that the individual sentiment analysis can complement each other." ></td>
	<td class="line x" title="62:180	Note that in the framework, different machine translation services can be used to obtain different English reviews, and the analysis of English reviews translated by a specific machine translation service is conducted separately." ></td>
	<td class="line x" title="63:180	For simplicity, we consider the English reviews translated by different machine translation services as reviews in different languages, despite the fact that in essence, they are still in English." ></td>
	<td class="line x" title="64:180	Figure 1." ></td>
	<td class="line x" title="65:180	Framework of our approach Formally, give a review rev 0  in the target language (i.e. Chinese), the corresponding review rev i in the ith language is obtained by using a translation function:  rev i =f  i Trans (rev 0 ) where 1ip and p is the total number of machine translation services." ></td>
	<td class="line x" title="66:180	For each review rev k  in the kth language (0kp), we employ the semantic oriented approach to assign a semantic orientation value f  k SO (rev k ) to the review, and the polarity orientation of the review can be simply predicated based on the value by using a threshold." ></td>
	<td class="line x" title="67:180	Given a set of semantic orientation values F SO ={f  k SO (rev k ) | 0kp}, the ensemble methods aim to derive a new semantic orientation value )( 0 revf Ensemble SO based on the values in F SO , which can be used to better classify the review as positive or negative." ></td>
	<td class="line x" title="68:180	The steps of review translation, individual semantic orientation value computation and ensemble combination are described in details in the next sections, respectively." ></td>
	<td class="line x" title="69:180	3.2 Review Translation Translation of a Chinese review into an English review is the first step of the proposed approach." ></td>
	<td class="line x" title="70:180	Manual translation is time-consuming and laborintensive, and it is not feasible to manually translate a large amount of Chinese product reviews in real applications." ></td>
	<td class="line x" title="71:180	Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory." ></td>
	<td class="line x" title="72:180	A few commercial machine translation services can be publicly accessed." ></td>
	<td class="line x" title="73:180	In this study, the following two commercial machine translation services and one baseline system are used to translate Chinese reviews into English reviews." ></td>
	<td class="line x" title="74:180	Google Translate 2  (GoogleTrans): Google Translate is one of the state-of-the-art commercial machine translation systems used today." ></td>
	<td class="line x" title="75:180	Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages." ></td>
	<td class="line x" title="76:180	Yahoo Babel Fish 3  (YahooTrans): Different from Google Translate, Yaho Babel Fish uses SYSTRANs rule-based translation engine." ></td>
	<td class="line x" title="77:180	SYSTRAN was one of the earliest developers of machine translation software." ></td>
	<td class="line x" title="78:180	SYSTRAN applies complex sets of specific rules defined by linguists to analyze and then transfer the grammatical structure of the source language into the target language." ></td>
	<td class="line x" title="79:180	Baseline Translate (DictTrans): We simply develop a translation method based only on one-toone term translation in a large Chinese-to-English  2  http://translate.google.com/translate_t 3  http://babelfish.yahoo.com/translate_txt Chinese review Chinese Resource English review Machine translation Chinese sentiment analysis Ensemble English sentiment analysis English Resource Polarity Value Polarity Value Pos\Neg 555 dictionary." ></td>
	<td class="line x" title="80:180	Each term in a Chinese review is translated by the first corresponding term in the Chinese-to-English dictionary, without any other processing steps." ></td>
	<td class="line x" title="81:180	In this study, we use the LDC_CE_DIC2.0 4  constructed by LDC as the dictionary for translation, which contains 128366 Chinese terms and their corresponding English terms." ></td>
	<td class="line x" title="82:180	The Chinese-to-English translation performances of the two commercial systems are deemed much better than the weak baseline system." ></td>
	<td class="line x" title="83:180	Google Translate has achieved very good results on the Chinese-to-English translation tracks of NIST open machine translation test (MT) 5  and it ranks the first on most tracks." ></td>
	<td class="line x" title="84:180	In the Chinese-to-English task of MT2005, the BLEU-4 score of Google Translate is 0.3531, and the BLEU-4 score of SYSTRAN is 0.1471." ></td>
	<td class="line x" title="85:180	We can deduce that Google Translate is better than Yahoo Babel Fish, without considering the recent improvements of the two systems." ></td>
	<td class="line x" title="86:180	Here are two running example of Chinese reviews and the translated English reviews (HumanTrans refers to human translation): Positive Example: , HumanTrans: Many advantages and very good shape." ></td>
	<td class="line x" title="87:180	GoogleTrans: Many advantages, the shape is also very good." ></td>
	<td class="line x" title="88:180	YahooTrans: Merit very many, the contour very is also good." ></td>
	<td class="line x" title="89:180	DictTrans: merit very many figure also very good Negative example:  HumanTrans: The memory is too small to support IR." ></td>
	<td class="line x" title="90:180	GoogleTrans: Memory is too small not to support IR." ></td>
	<td class="line x" title="91:180	YahooTrans:The memory too is small does not support infrared." ></td>
	<td class="line x" title="92:180	DictTrans: memory highest small negative not to be in favor of ir." ></td>
	<td class="line x" title="93:180	3.3 Individual Semantic Orientation Value Computation For any specific language, we employ the semantic orientated approach (Kennedy and Inkpen, 2006) to compute the semantic orientation value of a review." ></td>
	<td class="line x" title="94:180	The unsupervised approach is quite  straightforward and it makes use of the following sentiment lexicons: positive Lexicon (Positive_Dic) including terms expressing positive polarity, Negative Lexicon (Negative_Dic) including terms expressing negative polarity, Negation  4  http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 5  http://www.nist.gov/speech/tests/mt/ Lexicon (Negation_Dic) including terms that are used to reverse the semantic polarity of a particular term, and Intensifier Lexicon (Intensifier_Dic) including terms that are used to change the degree to which a term is positive or negative." ></td>
	<td class="line x" title="95:180	In this study, we conduct our experiments within two languages, and we collect and use the following popular and available Chinese and English sentiment lexicons 6 , without any further filtering and labeling: 1) Chinese lexicons Positive_Dic cn : 3730 Chinese positive terms were collected from the Chinese Vocabulary for Sentiment Analysis (VSA) 7  released by HOWNET." ></td>
	<td class="line x" title="96:180	Negative_Dic cn : 3116 Chinese negative terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET." ></td>
	<td class="line x" title="97:180	Negation_Dic cn : 13 negation terms were collected from related papers." ></td>
	<td class="line x" title="98:180	Intensifier_Dic cn : 148 intensifier terms were collected from Chinese Vocabulary for Sentiment Analysis (VSA) released by HOWNET." ></td>
	<td class="line x" title="99:180	2) English lexicons Positive_Dic en : 2718 English positive terms were collected from the feature file subjclueslen1HLTEMNLP05.tff 8  containing the subjectivity clues used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="100:180	The clues in this file were collected from a number of sources." ></td>
	<td class="line x" title="101:180	Some were culled from manually developed resources, e.g. general inquirer 9  (Stone et al., 1966)." ></td>
	<td class="line x" title="102:180	Others were identified automatically using both annotated and unannotated data." ></td>
	<td class="line x" title="103:180	A majority of the clues were collected as part of work reported in Riloff and Wiebe (2003)." ></td>
	<td class="line x" title="104:180	Negative_Dic en : 4910 English negative terms were collected from the same file described above." ></td>
	<td class="line x" title="105:180	Negation_Dic en : 88 negation terms were collected from the feature file valenceshifters.tff used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="106:180	Intensifier_Dic en : 244 intensifier terms were collected from the feature file intensifiers2.tff used in the work (Wilson et al., 2005a; Wilson et al., 2005b)." ></td>
	<td class="line x" title="107:180	6  In this study, we focus on using a few popular resources in both Chinese and English for comparative study, instead of trying to collect and use all available resources." ></td>
	<td class="line x" title="108:180	7  http://www.keenage.com/html/e_index.html 8  http://www.cs.pitt.edu/mpqa/ 9  http://www.wjh.harvard.edu/~inquirer/homecat.htm 556 The semantic orientation value f  k SO (rev k ) for rev k  is computed by summing the polarity values of all words in the review, making use of both the word polarity defined in the positive and negative lexicons and the contextual valence shifters defined in the negation and intensifier lexicons." ></td>
	<td class="line x" title="109:180	The algorithm is illustrated in Figure 2." ></td>
	<td class="line x" title="110:180	Input: a review rev k  in the kth language." ></td>
	<td class="line x" title="111:180	Four lexicons in the kth language: Positive_Dic k , Negative_Dic k , Negation_Dic k , Intensifier_Dic k , which are either Chinese or English lexicons; Output: Polarity Value f  k SO (rev k ); Algorithm Compute_SO: 1." ></td>
	<td class="line x" title="112:180	Tokenize review rev k  into sentence set S and each sentence sS  is tokenized into word set W s ; 2." ></td>
	<td class="line x" title="113:180	For any word w in a sentence sS, compute its SO value SO(w) as follows: 1) if wPositive_Dic k , SO(w)=PosValue; 2) If wNegative_Dic k , SO(w)=NegValue; 3) Otherwise, SO(w)=0; 4) Within the window of q words previous to w, if there is a term wNegation_Dic k , SO(w)= SO(w); 5) Within the window of q words previous to w, if there is a term wIntensifier_Dic k , SO(w) = SO(w); 3." ></td>
	<td class="line x" title="114:180	  = SsWw kk SO s wSOrevf )()( ; Figure 2." ></td>
	<td class="line x" title="115:180	The algorithm for semantic orientation value computation In the above algorithm, PosValue and NegValue are the polarity values for positive words and negative words respectively." ></td>
	<td class="line x" title="116:180	We empirically set PosValue=1 and NegValue= 2 because negative words usually contribute more to the overall semantic orientation of the review than positive words, according to our empirical analysis." ></td>
	<td class="line x" title="117:180	 >1 aims to intensify the polarity value and we simply set  =2." ></td>
	<td class="line x" title="118:180	q is the parameter controlling the window size within which the negation terms and intensifier terms have influence on the polarity words and here q is set to 2 words." ></td>
	<td class="line x" title="119:180	Note that the above parameters are tuned only for Chinese sentiment analysis, and they are used for sentiment analysis in the English language without further tuning." ></td>
	<td class="line x" title="120:180	The tokenization of Chinese reviews involves Chinese word segmentation." ></td>
	<td class="line x" title="121:180	Usually, if the semantic orientation value of a review is less than 0, the review is labeled as negative, otherwise, the review is labeled as positive." ></td>
	<td class="line x" title="122:180	3.4 Ensemble Combination After obtaining the set of semantic orientation values F SO ={f  k SO (rev k ) | 0kp} by using the semantic oriented approach, where p is the number of English translations for each Chinese review, we exploit the following ensemble methods for deriving a new semantic orientation value )( 0 revf Ensemble SO : 1) Average It is the most intuitive combination method and the new value is the average of the values in F SO : 1 )( )( 00 + =  = p revf revf p k kk SO Ensemble SO  Note that after the new value of a review is obtained, the polarity tag of the review is assigned in the same way as described in Section 3.3." ></td>
	<td class="line x" title="123:180	2) Weighted Average This combination method improves the average combination method by associating each individual value with a weight, indicating the relative confidence in the value." ></td>
	<td class="line x" title="124:180	 = = p k kk SOk Ensemble SO revfrevf 0 0 )()(   where  k [0, 1] is the weight associated with f  k SO (rev k )." ></td>
	<td class="line x" title="125:180	The weights can be set in the following two ways: Weighting Scheme1: The weight of f  k SO (rev k ) is set to the accuracy of the individual analysis in the kth language." ></td>
	<td class="line x" title="126:180	Weighting Scheme2: The weight of f  k SO (rev k ) is set to be the maximal correlation coefficient between the analysis results in the kth language and the analysis results in any other language." ></td>
	<td class="line x" title="127:180	The underlying idea is that if the analysis results in one language are highly consistent with the analysis results in another language, the results are deemed to be more reliable." ></td>
	<td class="line x" title="128:180	Given two lists of semantic values for all reviews, we use the Pearsons correlation coefficient to measure the correlation between them." ></td>
	<td class="line x" title="129:180	The weight associated with function f  k SO (rev k ) is then defined as the maximal Pearsons correlation coefficient between the reviews values in the kth language and the reviews values in any other language." ></td>
	<td class="line x" title="130:180	3) Max 557 The new value is the maximum value in F SO : { }pkrevfrevf kk SO Ensemble SO = 0|)(max)( 0  4) Min The new value is the minimum value in F SO : { }pkrevfrevf kk SO Ensemble SO = 0|)(min)( 0  5) Average Max&Min The new value is the average of the maximum value and the minimum value in F SO : {}{} 2 0|)(min0|)(max )( 0 pkrevfpkrevf revf kk SO kk SOEnsemble SO + =  6) Majority Voting This combination method relies on the final polarity tags, instead of the semantic orientation values." ></td>
	<td class="line x" title="131:180	A review can obtain p+1 polarity tags based on the individual analysis results in the p+1 languages." ></td>
	<td class="line x" title="132:180	The polarity tag receiving more votes is chosen as the final polarity tag of the review." ></td>
	<td class="line x" title="133:180	4 Empirical Evaluation 4.1 Dataset and Evaluation Metrics In order to assess the performance of the proposed approach, we collected 1000 product reviews from a popular Chinese IT product web site-IT168 10 . The reviews were posted by users and they focused on such products as mp3 players, mobile phones, digital camera and laptop computers." ></td>
	<td class="line x" title="134:180	Users usually selected for each review an icon indicating postive or negative." ></td>
	<td class="line x" title="135:180	The reviews were first categorized into positive and negative classes according to the associated icon." ></td>
	<td class="line x" title="136:180	The polarity labels for the reviews were then checked by subjects." ></td>
	<td class="line x" title="137:180	Finally, the dataset contained 886 product reviews with accurate polarity labels." ></td>
	<td class="line x" title="138:180	All the 886 reviews were used as test set." ></td>
	<td class="line x" title="139:180	We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the MacroF measure and accuracy metric to measure the overall performance of the system." ></td>
	<td class="line x" title="140:180	The metrics are defined the same as in general text categorization." ></td>
	<td class="line x" title="141:180	4.2 Individual Analysis Results In this section, we investigate the following individual sentiment analysis results in each specified language: CN: This method uses only Chinese lexicons to analyze Chinese reviews;  10  http://www.it168.com GoogleEN: This method uses only English lexicons to analyze English reviews translated by GoogleTrans; YahooEN: This method uses only English lexicons to analyze English reviews translated by YahooTrans; DictEN: This method uses only English lexicons to analyze English reviews translated by DictTrans; In addition to the above methods for using English resources, the lexicon-based method investigated in Mihalcea et al.(2007) can also use English resources by directly projecting English lexicons into Chinese lexicons." ></td>
	<td class="line x" title="143:180	We use a large English-to-Chinese dictionary LDC_EC_DIC2.0 11  with 110834 entries for projecting English lexicons into Chinese lexicons via one-to-one translation." ></td>
	<td class="line x" title="144:180	Based on the generated Chinese lexicons, two other individual methods are investigated in the experiments: CN2: This method uses only the generated Chinese Resources to analyze Chinese reviews." ></td>
	<td class="line x" title="145:180	CN3: This method combines the original Chinese lexicons and the generated Chinese lexicons and uses the extended lexicons to analyze Chinese reviews." ></td>
	<td class="line x" title="146:180	Table 1 provides the performance values of all the above individual methods." ></td>
	<td class="line x" title="147:180	Seen from the table, the performances of GoogleEN and YahooEN are much better than the baseline CN method, and even the DictEN performs as well as CN." ></td>
	<td class="line x" title="148:180	The results demonstrate that the use of English resources for sentiment analysis of translated English reviews is an effective way for Chinese sentiment analysis." ></td>
	<td class="line x" title="149:180	We can also see that the English sentiment analysis performance relies positively on the translation performance, and GoogleEN performs the best while DictEN performs the worst, which is consistent with the fact the GoogleTrans is deemed the best of the three machine translation systems, while DictTrans is the weakest one." ></td>
	<td class="line x" title="150:180	Furthermore, the CN method outperforms the CN2 and CN3 methods, and the CN2 method performs the worst, which shows that the generated Chinese lexicons do not give any contributions to the performance of Chinese sentiment analysis." ></td>
	<td class="line x" title="151:180	We explain the results by the fact that the term-based one-to-one translation is inaccurate and the generated Chinese lexicons are not reliable." ></td>
	<td class="line x" title="152:180	Overall, the  11  http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 558 approach through cross-lingual lexicon translation does not work well for Chinese sentiment analysis in our experiments." ></td>
	<td class="line x" title="153:180	4.3 Ensemble Results In this section, we first use the simple average ensemble method to combine different individual analysis results." ></td>
	<td class="line x" title="154:180	Table 2 provides the performance values of the average ensemble results based on different individual methods." ></td>
	<td class="line x" title="155:180	Seen from Tables 1 and 2, almost all of the average ensembles outperforms the baseline CN method and the corresponding individual methods, which shows that each individual methods have their own evidences for sentiment analysis, and thus fusing the evidences together can improve performance." ></td>
	<td class="line x" title="156:180	For the methods of CN+GoogleEN, CN+YahooEN and CN+DictEN, we can see the ensemble performance is not positively relying on the translation performance: CN+YahooEN performs better than CN+GoogleEN, and even CN+DictEN performs as well as CN+GoogleEN." ></td>
	<td class="line x" title="157:180	The results show that the individual methods in the ensembles can complement each other, and even the combination of two weak individual methods can achieve good performance." ></td>
	<td class="line x" title="158:180	However, the DictEN method is not effective when the ensemble methods have already included GoogleEN and YahooEN." ></td>
	<td class="line x" title="159:180	Overall, the performances of the ensemble methods rely on the performances of the most effective constituent individual methods: the methods including both GoogleEN and YahooEN perform much better than other methods, and CN+GoogleEN+YahooEN performs the best out of all the methods." ></td>
	<td class="line x" title="160:180	We further show the results of four typical average ensembles by varying the combination weights." ></td>
	<td class="line x" title="161:180	The combination weights are respectively specified as  CN+(1- )GoogleEN,  CN+(1 )YahooEN,  CN+(1- )DictEN,  1 CN+ 2 GoogleEN+(1- 1 - 2 )YahooEN." ></td>
	<td class="line x" title="162:180	The results over the MacroF metric are shown in Figures 3 and 4 respectively." ></td>
	<td class="line x" title="163:180	We can see from the figures that GoogleEN and YahooEN are dominant factors in the ensemble methods." ></td>
	<td class="line x" title="164:180	We then investigate to use other ensemble methods introduced in Section 3.4 to combine the CN, GoogleEN and YahooEN methods." ></td>
	<td class="line x" title="165:180	Table 3 gives the comparison results." ></td>
	<td class="line x" title="166:180	The methods of Weighted Average1 and Weighted Average2 are two weighted average ensembles using the two weighing schemes, respectively." ></td>
	<td class="line x" title="167:180	We can see that all the ensemble methods outperform the constituent individual method, while the two weighted average ensembles perform the best." ></td>
	<td class="line x" title="168:180	The results further demonstrate the good effectiveness of the ensemble combination of individual analysis results for Chinese sentiment analysis." ></td>
	<td class="line x" title="169:180	Positive Negative Total Individual Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy CN 0.681 0.929 0.786 0.882 0.549 0.677 0.732 0.743 CN2 0.615 0.772 0.684 0.678 0.499 0.575 0.630 0.638 CN3 0.702 0.836 0.763 0.788 0.632 0.702 0.732 0.736 GoogleEN 0.764 0.914 0.832 0.888 0.708 0.787 0.810 0.813 YahooEN 0.763 0.871 0.814 0.844 0.720 0.777 0.795 0.797 DictEN 0.738 0.761 0.749 0.743 0.720 0.731 0.740 0.740 Table 1." ></td>
	<td class="line x" title="170:180	Individual analysis results Positive Negative Total Average Ensemble Precision Recall F-measure Precision Recall F-measure MacroF Accuracy GoogleEN+YahooEN 0.820 0.900 0.858 0.885 0.795 0.838 0.848 0.848 GoogleEN+YahooEN +DictEN 0.841 0.845 0.843 0.838 0.834 0.836 0.840 0.840 CN+GoogleEN 0.754 0.949 0.840 0.928 0.678 0.784 0.812 0.816 CN+YahooEN 0.784 0.925 0.848 0.904 0.736 0.811 0.830 0.832 CN+DictEN 0.790 0.867 0.827 0.847 0.761 0.801 0.814 0.815 CN+GoogleEN +YahooEN 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 CN+GoogleEN+ YahooEN+DictEN 0.831 0.891 0.860 0.878 0.811 0.843 0.852 0.852 Table 2." ></td>
	<td class="line x" title="171:180	Average combination results 559 0.72 0.74 0.76 0.78 0.8 0.82 0.84 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  Mac r oF CN+GoogleEN CN+YahooEN CN+DictEN  0 0.3 0.6 0.9 0 0.2 0.4 0.6 0.8 1 0.65 0.7 0.75 0.8 0.85 0.9 MacroF  1  2 0.85-0.9 0.8-0.85 0.75-0.8 0.7-0.75 0.65-0.7  Figure 3." ></td>
	<td class="line x" title="172:180	Ensemble performance vs. weight   for  CN+(1- )GoogleEN/YahooEN/DictEN Figure 4." ></td>
	<td class="line x" title="173:180	Ensemble performance vs. weights  1 and  2  for  1 CN+ 2 GoogleEN+(1- 1 - 2 ) YahooEN  Positive Negative Total Ensemble Method Precision Recall F-measure Precision Recall F-measure MacroF Accuracy Average 0.813 0.927 0.866 0.911 0.779 0.840 0.853 0.854 Weighted Average1 0.825 0.922 0.871 0.908 0.798 0.849 0.860 0.861 Weighted Average2 0.822 0.922 0.869 0.908 0.793 0.847 0.858 0.859 Max 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 Min 0.901 0.787 0.840 0.805 0.910 0.854 0.847 0.848 Average Max&Min 0.793 0.936 0.859 0.918 0.747 0.824 0.841 0.843 Majority Voting 0.765 0.940 0.844 0.919 0.701 0.795 0.820 0.823 Table 3." ></td>
	<td class="line x" title="174:180	Ensemble results for CN & GoogleEN & YahooEN 5 Conclusion and Future Work This paper proposes a novel approach to use English sentiment resources for Chinese sentiment analysis by employing machine translation and ensemble techniques." ></td>
	<td class="line x" title="175:180	Chinese reviews are translated into English reviews and the analysis results of both Chinese reviews and English reviews are combined to improve the overall accuracy." ></td>
	<td class="line x" title="176:180	Experimental results demonstrate the encouraging performance of the proposed approach." ></td>
	<td class="line x" title="177:180	In future work, more additional English resources will be used to further improve the results." ></td>
	<td class="line x" title="178:180	We will also apply the idea to supervised Chinese sentiment analysis." ></td>
	<td class="line x" title="179:180	Acknowledgments This work was supported by the National Science Foundation of China (No.60703064), the Research Fund for the Doctoral Program of Higher Education of China (No.20070001059) and the National High Technology Research and Development Program of China (No.2008AA01Z421)." ></td>
	<td class="line x" title="180:180	We also thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1040
Unsupervised Classification of Sentiment and Objectivity in Chinese Text
Zagibalov, Taras;Carroll, John A.;"></td>
	<td class="line x" title="1:190	UnsupervisedClassificationofSentimentandObjectivity inChineseText TarasZagibalov JohnCarroll UniversityofSussex DepartmentofInformatics BrightonBN19QH,UK {T.Zagibalov,J.A.Carroll}@sussex.ac.uk Abstract We address the problem of sentiment and objectivity classification of product reviews in Chinese." ></td>
	<td class="line x" title="2:190	Our approach is distinctive in that it treats both positive / negative sentiment and subjectivity / objectivity not as distinct classes but rather as a continuum; we arguethatthis is desirablefrom the perspective of would-be customers who read the reviews." ></td>
	<td class="line x" title="3:190	We use novel unsupervised techniques, including a one-word 'seed' vocabulary and iterative retraining for sentiment processing, and a criterion of 'sentiment density' for determining the extent to which a document is opinionated." ></td>
	<td class="line x" title="4:190	The classifier achieves up to 87% F-measureforsentimentpolaritydetection." ></td>
	<td class="line oc" title="5:190	1 Introduction Automatic classification of sentiment has been a focus of a number of recent research efforts (e.g.(Turney, 2002; Pang et al., 2002; Dave at al., 2003)." ></td>
	<td class="line p" title="7:190	An important potential application of such work is in business intelligence: brands and company image are valuable property,so organizations want to know how they are viewed by the media (what the 'spin' is on news stories, and editorials), business analysts (as expressed in stock market reports), customers (for example on product review sites) and their own employees." ></td>
	<td class="line p" title="8:190	Another important application is to help people find out others' views about products they have purchased(e.g. consumer electronics), services and entertainment (e.g. movies), stocks and shares (from investor bulletin boards), and so on." ></td>
	<td class="line x" title="9:190	In the work reported in this paper we focus onproduct reviews, with the intended usersoftheprocessingbeingwould-becustomers." ></td>
	<td class="line x" title="10:190	Our approach is based on the insight that positive and negative sentiments are extreme points in a continuum of sentiment, and that intermediate points in this continuum are of potential interest." ></td>
	<td class="line x" title="11:190	For instance, in one scenario, someone might want to get an idea of the types of things people are saying about a particular product through reading a sample of reviews covering the spectrum from highly positive, through balanced, to highly negative." ></td>
	<td class="line x" title="12:190	(Wecall a review balanced if it is an opinionated text with an undecided or weak sentiment direction)." ></td>
	<td class="line x" title="13:190	In another scenario, a would-be customer might only be interested in reading balanced reviews, since they often present more reasoned arguments with fewer unsupported claims." ></td>
	<td class="line x" title="14:190	Such a person might therefore want to avoid reviews such asExample(1)writtenbyaChinesepurchaserof amobilephone(ourEnglishgloss)." ></td>
	<td class="line x" title="15:190	(1) q" ></td>
	<td class="line x" title="16:190	H HZ l " ></td>
	<td class="line x" title="17:190	Y9" ></td>
	<td class="line x" title="18:190	ml ^ y " ></td>
	<td class="line x" title="19:190	C`" ></td>
	<td class="line x" title="20:190	vrTQ" ></td>
	<td class="line x" title="21:190	 ^1lT9 ." ></td>
	<td class="line x" title="22:190	 Q1" ></td>
	<td class="line x" title="23:190	  P " ></td>
	<td class="line x" title="24:190	 " ></td>
	<td class="line x" title="25:190	q V Thesoftwareisbad,somesentSMSareneverreceivedbytheaddressee;compatibility isalsobad,onsomemobilephonesthereceivedmessagesareinascrambledencoding!Andsometimesthephone'dies'!Photos arehorrible!Itdoesn'thaveacyclicorpro304 grammablealarm-clock,youhavetosetit everytime,howcumbersome!Thebackcoverdoesnotfit!Theoriginalsoftwarehas manyholes!" ></td>
	<td class="line x" title="26:190	In a third scenario, someone might decide they would like only to read opinionated, weakly negative reviews such as Example (2), since these often contain good argumentation while still identifying themostsalientbadaspectsofaproduct." ></td>
	<td class="line x" title="27:190	(2) 0Q w" ></td>
	<td class="line x" title="28:190	A 1 ,#[/?l" ></td>
	<td class="line x" title="29:190	9 .1 .2" ></td>
	<td class="line x" title="30:190	 1{ .29zP" ></td>
	<td class="line x" title="31:190	HH
" ></td>
	<td class="line x" title="32:190	 'HW" ></td>
	<td class="line x" title="33:190	'0W+" ></td>
	<td class="line x" title="34:190	 q" ></td>
	<td class="line x" title="35:190	W" ></td>
	<td class="line x" title="36:190	9L" ></td>
	<td class="line x" title="37:190	 ^'L^9" ></td>
	<td class="line x" title="38:190	h." ></td>
	<td class="line x" title="39:190	Theresponsetimeofthismobileisvery long,MMSshouldbelessthan30kbonlyto bedownloaded,alsoitdoesn'tsupportMP3 ringtones,(while)thebuilt-intunesarenot good,andfromtimetotimeit'dies',but whenIwasbuyingitIreallylikedit:very original,verynicelymatchingredandwhite colours,ithasitsindividuality,alsoit'snot expensive,butwhenuseditalwayscauses trouble,makesone'sheadache The review contains both positive and negative sentiment coveringdifferentaspects oftheproduct, and the fact that it contains a balance of views means that it is likely to be useful for a would-be customer." ></td>
	<td class="line x" title="40:190	Moving beyond review classification, more advanced tasks such as automatic summarization of reviews (e.g. Feiguina & LaPalme, 2007) might also benefit from techniques which could distinguish more shades of sentiment than justabinarypositive/negativedistinction." ></td>
	<td class="line x" title="41:190	A second dimension, orthogonal to positive / negative, is opinionated / unopinionated (or equivalently subjective / objective)." ></td>
	<td class="line x" title="42:190	When shopping for a product, one might be interested in the physical characteristics of the product or what features the product has, rather than opinions about how well these features work or about how well the product as a whole functions." ></td>
	<td class="line x" title="43:190	Thus, if one is looking for a review that contains more factual information than opinion, one might be interested in reviews like Example(3)." ></td>
	<td class="line x" title="44:190	(3) 9$p" ></td>
	<td class="line x" title="45:190	L" ></td>
	<td class="line x" title="46:190	 7" ></td>
	<td class="line x" title="47:190	1" ></td>
	<td class="line x" title="48:190	9 " ></td>
	<td class="line x" title="49:190	 H" ></td>
	<td class="line x" title="50:190	 " ></td>
	<td class="line x" title="51:190	 '" ></td>
	<td class="line x" title="52:190	|AU" ></td>
	<td class="line x" title="53:190	HWy " ></td>
	<td class="line x" title="54:190	8'1 
" ></td>
	<td class="line x" title="55:190	V" ></td>
	<td class="line x" title="56:190	:Y'b (My)overallfeelingaboutthismobileisnot bad,itfeatures:5alarm-clocksthatswitch thephoneon(off),phonebookfor800items (500people),lunarandsolarcalendars, fastswitchingbetweentimeanddatemodes, WAPnetworking,organizer,notebookand soon." ></td>
	<td class="line x" title="57:190	This review is mostly neutral (unopinionated), but contains information that could be useful to a would-be customer which might not be in a product specification document, e.g. fast switching between different operating modes." ></td>
	<td class="line x" title="58:190	Similarly,wouldbe customers might be interested in retrieving completely unopinionated documents such as technical descriptions and usermanuals.Again, aswith sentiment classification, we argue that opinionated and unopinionated texts are not easily distinguishable separate sets, but form a continuum." ></td>
	<td class="line x" title="59:190	In this continuum, intermediate points are of interest as wellastheextremes." ></td>
	<td class="line x" title="60:190	A major obstacle for automatic classification of sentiment and objectivity is lack of training data, which limits the applicability of approaches based on supervised machine learning." ></td>
	<td class="line x" title="61:190	With the rapid growth in textual data and the emergence of new domains of knowledge it is virtually impossible to maintain corpora of tagged data that cover all  or even most  areas of interest." ></td>
	<td class="line x" title="62:190	The cost of manual taggingalsoaddstotheproblem.Reusingthesame corpus for training classifiers for new domains is also not effective: several studies report decreased accuracy in cross-domain classification (Engstrm, 2004; Aue & Gamon, 2005) a similar problem has also been observed in classification of documents createdoverdifferenttimeperiods(Read,2005)." ></td>
	<td class="line x" title="63:190	Inthispaperwedescribeanunsupervised classification technique which is able to build its own sentiment vocabulary starting from a very small seed vocabulary, using iterative retraining to enlarge the vocabulary.In order to avoid problems of domain dependence, the vocabulary is built using textfrom the samesourceas thetextwhich isto be classified." ></td>
	<td class="line x" title="64:190	In this paper we work with Chinese, but using a very small seed vocabulary may mean that this approach would in principle need very little linguistic adjustment to be applied to a different 305 language." ></td>
	<td class="line x" title="65:190	Written Chinese has some specific features, one of which is the absence of explicitly markedwordboundaries,whichmakesword-based processing problematic." ></td>
	<td class="line x" title="66:190	In keeping with our unsupervised, knowledge-poor approach, we do not use any preliminary word segmentation tools or higher levelgrammaticalanalysis." ></td>
	<td class="line x" title="67:190	The paper is structured as follows." ></td>
	<td class="line x" title="68:190	Section 2 reviews related work in sentiment classification and more generally in unsupervised training of classifiers." ></td>
	<td class="line x" title="69:190	Section 3 describes our datasets, and Section 4 the techniques we use for unsupervised classification and iterative retraining." ></td>
	<td class="line x" title="70:190	Sections 5 and 6 describe a number of experiments into how well the approacheswork,andSection7concludes." ></td>
	<td class="line oc" title="71:190	2 RelatedWork 2.1 Sentiment Classification Most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment (Turney, 2002; Pang et al., 2002; Dave at al., 2003)." ></td>
	<td class="line x" title="72:190	However, Pang & Lee (2005) describe an approach closer to ours in which they determine an author's evaluation with respect to a multi-point scale, similar to the 'five-star' sentiment scale widely used on review sites." ></td>
	<td class="line x" title="73:190	However, authors of reviews are inconsistent in assigning fine-grained ratings and quite often star systems are not consistent between critics." ></td>
	<td class="line x" title="74:190	This makes their approach very author-dependent." ></td>
	<td class="line x" title="75:190	The main differences are that Pang and Lee use discrete classes (although more than two), not a continuum as in our approach, and use supervisedmachine learningrather than unsupervised techniques." ></td>
	<td class="line x" title="76:190	A similar approach was adopted by Hagedorn et al.(2007), applied to news stories: they defined five classes encoding sentiment intensity and trained their classifier on a manually tagged training corpus." ></td>
	<td class="line x" title="78:190	They note that world knowledge is necessary for accurate classificationinsuchopen-endeddomains." ></td>
	<td class="line x" title="79:190	There has also been previous work on determining whether a given text is factual or expresses opinion (Yu& Hatzivassiloglu, 2003; Pang & Lee, 2004); again this work uses a binary distinction, and supervised rather than unsupervised approaches." ></td>
	<td class="line x" title="80:190	Recent work on classification of terms with respect to opinion (Esuli & Sebastiani, 2006) uses a three-category system to characterize the opinionrelated properties of word meanings, assigning numerical scores to Positive, Negative and Objective categories." ></td>
	<td class="line x" title="81:190	The visualization of these scores somewhat resembles our graphs in Section 5, although we use two orthogonal scales rather than three categories; we are also concerned with classification ofdocumentsratherthanterms." ></td>
	<td class="line x" title="82:190	2.2 Unsupervised Classification Abney (2002) compares two major kinds of unsupervised approachto classification (co-training and the Yarowskyalgorithm)." ></td>
	<td class="line x" title="83:190	As we do not use multiple classifiers our approach is quite far from cotraining." ></td>
	<td class="line oc" title="84:190	But it is close to the paradigm described by Yarowsky (1995) and Turney (2002) as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples." ></td>
	<td class="line x" title="85:190	But our approach does not use point-wise mutual information." ></td>
	<td class="line x" title="86:190	Instead we use relative frequencies of newly found features in a training subcorpus produced by the previous iteration of the classifier." ></td>
	<td class="line x" title="87:190	Wealso use the smallest possible seed vocabulary, containing just a single word; however there are no restrictions regarding the maximum number of items in the seed vocabulary." ></td>
	<td class="line x" title="88:190	3 Data 3.1 Seed Vocabulary Our approach starts out with a seed vocabulary consisting of a single word, z (good)." ></td>
	<td class="line x" title="89:190	This word is tagged as a positive vocabulary item; initially there are no negative items." ></td>
	<td class="line x" title="90:190	The choice of word was arbitrary, and other words with strongly positive or negative meaning would also be plausible seeds." ></td>
	<td class="line x" title="91:190	Indeed, z might not be the best possible seed, as it is relatively ambiguous: in some contexts it means to like or acts as the adverbial very, and is often used as part of other words (although usually contributing a positive meaning)." ></td>
	<td class="line x" title="92:190	But since it is one of the most frequent units in the Chinese language, it is likely to occur in a relatively large number of reviews, which is important for the rapidgrowthofthevocabularylist." ></td>
	<td class="line x" title="93:190	3.2 TestCorpus Our test corpus is derived from product reviews harvested from the website IT1681." ></td>
	<td class="line x" title="94:190	All the reviews were tagged by their authors as either positive or negative overall." ></td>
	<td class="line x" title="95:190	Most reviews consist of two or three distinct parts: positive opinions, negative opinions, and comments ('other')  although some 1http://product.it168.com 306 reviews have only one part." ></td>
	<td class="line x" title="96:190	Weremoved duplicate reviews automatically using approximate matching, giving a corpus of 29531 reviews of which 23122 are positive (78%) and 6409 are negative (22%)." ></td>
	<td class="line x" title="97:190	The total number of different products in the corpus is 10631, the number of product categories is 255, and most of the reviewed products are either software products or consumer electronics." ></td>
	<td class="line x" title="98:190	Unfortunately, it appears that some users misused the sentiment tagging facility on the website so quite a lot of reviews have incorrect tags." ></td>
	<td class="line x" title="99:190	However, the parts of the reviews are much more reliably identified as being positive or negative so we usedtheseastheitemsofthetestcorpus.Intheexperiments described in this paper we used 2317 reviews of mobile phones of which 1158 are negative and 1159 are positive." ></td>
	<td class="line x" title="100:190	Thus random choice would have approximately 50% accuracy if all itemsweretaggedeitherasnegativeorpositive2." ></td>
	<td class="line x" title="101:190	4 Method 4.1 Sentiment Classification As discussed in Section 1, we do not carry out any word segmentation or grammatical processing of input documents." ></td>
	<td class="line x" title="102:190	We use a very broad notion of words (or phrases) in the Chinese language." ></td>
	<td class="line x" title="103:190	The basic units of processing are 'lexical items', each of which is a sequence of one or more Chinese characters excluding punctuation marks (which may actually form part of a word, a whole word or a sequence of words), and `zones', each of which is a sequence of characters delimited by punctuation marks." ></td>
	<td class="line x" title="104:190	Each zone is classified as either positive or negative based whether positive or negative vocabulary items predominate." ></td>
	<td class="line x" title="105:190	In more detail, a simple maximum match algorithm is used to find all lexical items (character sequences) in the zone that are in the vocabulary list." ></td>
	<td class="line x" title="106:190	As there are two parts of the vocabulary (positiveand negative), wecorrespondinglycalculatetwoscoresusingEquation(1)3, Si= LdL phrase Sd Nd (1) where Ld is the length in characters of a matching lexical item, Lphrase is the length of the current zone 2Thiscorpusispubliclyavailableathttp://www.informatics." ></td>
	<td class="line x" title="107:190	sussex.ac.uk/users/tz21/it168test.zip 3Inthefirstiteration,whenwehaveonlyoneiteminthevocabulary,negativezonesarefoundbymeansofthenegation check(sonot+good=negativeitem)." ></td>
	<td class="line x" title="108:190	in characters, Sd is the current sentiment score of the matching lexical item (initially1.0), and Nd is a negation check coefficient.Thenegation check is a regular expression which determines if the lexical item is preceded by a negation within its enclosing zone." ></td>
	<td class="line x" title="109:190	If a negation is found then Nd is set to 1." ></td>
	<td class="line x" title="110:190	The check looks for six frequently occurring negations:(bu),  (buhui),  (meiyou),  (baituo),  (mianqu),and E  (bimian)." ></td>
	<td class="line x" title="111:190	The sentiment score of a zone is the sum of sentiment scores of all the items found in it." ></td>
	<td class="line x" title="112:190	In fact there are twocompeting sentiment scores for every zone: one positive (the sum of all scores of items found in the positive part of the vocabulary list) and one negative (the sum of the scores for the items in the negative part)." ></td>
	<td class="line x" title="113:190	The sentiment direction of a zone is determined from the maximum of the absolutevaluesofthetwocompetingscoresforthe zone." ></td>
	<td class="line x" title="114:190	This procedure is applied to all zones in a document, classifying each zone as positive, negative, or neither (in cases where there are no positive or negative vocabulary items in the zone)." ></td>
	<td class="line x" title="115:190	To determine the sentiment direction of the whole document, the classifier computes the difference between the number of positive and negative zones." ></td>
	<td class="line x" title="116:190	If the result is greater than zero the document is classifiedas positive,andviceversa.Iftheresultis zero the document is balanced or neutral for sentiment." ></td>
	<td class="line x" title="117:190	4.2 IterativeRetraining The task of iterative retraining is to enlargethe initial seed vocabulary (consisting of a single wordas discussed in Section 3.1) into a comprehensive vocabulary list of sentiment-bearing lexical items." ></td>
	<td class="line x" title="118:190	In each iteration, the current version of the classifier isrunontheproductreviewcorpustoclassifyeach document, resulting in a training subcorpus of positive and a negative documents." ></td>
	<td class="line x" title="119:190	The subcorpus is used to adjust the scores of existing positive and negative vocabulary items and to find new itemsto beincludedinthevocabulary." ></td>
	<td class="line x" title="120:190	Eachlexicalitemthatoccursatleasttwiceinthe corpus is a candidate for inclusion in the vocabulary list." ></td>
	<td class="line x" title="121:190	After candidate items are found, the system calculates their relative frequencies in both the positive and negative parts of the current training subcorpus." ></td>
	<td class="line x" title="122:190	The system also checks for negation whilecountingoccurrences:ifalexicalitemispreceded by a negation, its count is reduced by one." ></td>
	<td class="line x" title="123:190	This results in negative counts (and thus negative relativefrequenciesandscores)forthoseitemsthat 307 are usually used with negation; for example,     (thequalityisfartoobad)isinthe positive part of the vocabulary with a score of 1.70." ></td>
	<td class="line x" title="124:190	This meansthattheitemwasfoundinreviewsclassified by the system as positive but it was preceded by a negation." ></td>
	<td class="line x" title="125:190	If during classification this item is found in a document it will reduce the positive score for that document (as it is in the positive part of the vocabulary), unlessthe item is preceded bya negation." ></td>
	<td class="line x" title="126:190	In this situation the score will be reversed (multiplied by 1), and the positive score will be increasedseeEquation(1)above." ></td>
	<td class="line x" title="127:190	Forallcandidateitemswecomparetheirrelative frequencies in the positive and negative documents inthesubcorpususingEquation(2)." ></td>
	<td class="line x" title="128:190	difference= FpFnF pFn/2 (2) If difference < 1, then the frequencies are similar and the item does not have enough distinguishing power,so it is not included in the vocabulary.Otherwise the the sentiment score of the item is (re-) calculated  according to Equation (3) for positive items,andanalogouslyfornegativeitems." ></td>
	<td class="line x" title="129:190	Fp FpFn (3) Finally, the adjusted vocabulary list with the new scoresisreadyforthenextiteration." ></td>
	<td class="line x" title="130:190	4.3 ObjectivityClassification Given a sentiment classification for each zone in a document, we compute sentiment density as the proportion of opinionated zones with respect to the total number of zones in the document." ></td>
	<td class="line x" title="131:190	Sentiment densitymeasurestheproportionofopinionatedtext in a document, and thus the degree to which the documentasawholeisopinionated." ></td>
	<td class="line x" title="132:190	It should be noted that neither sentiment score nor sentiment density are absolute values, but are relative and only valid for comparing one document with other." ></td>
	<td class="line x" title="133:190	Thus, a sentiment density of 0.5 does not mean that the review is half opinionated, half not." ></td>
	<td class="line x" title="134:190	It means that the review is less opinionatedthanareviewwithdensity0.9." ></td>
	<td class="line x" title="135:190	5 Experiments We ran the system on the product review corpus (Section 3.2) for 20iterations." ></td>
	<td class="line x" title="136:190	Theresults for binary sentiment classification are shown in Table 1." ></td>
	<td class="line x" title="137:190	Wesee increasing F-measure up to iteration 18, after which both precision and recall start to descrease; we therefore use the version of the classifier as it stood after iteration 184." ></td>
	<td class="line x" title="138:190	These figures are only indicative of the classification accuracy of the system." ></td>
	<td class="line x" title="139:190	Accuracy might be lower for unseen text, although since our approach is unsupervised we could in principle perform further retraining iterations on any sample of new text to tune the vocabularylisttoit." ></td>
	<td class="line x" title="140:190	We also computed a (strong) baseline, using as the vocabulary list the NTU Sentiment Dictionary (Ku et al., 2006)5 which is intended to contain only sentiment-related words and phrases." ></td>
	<td class="line x" title="141:190	We assigned each positive and negative vocabulary item a score of 1 or 1 respectively." ></td>
	<td class="line x" title="142:190	This setup achieved 87.77 precision and 77.09 recall on the product review corpus." ></td>
	<td class="line x" title="143:190	InSection1wearguedthatsentimentandobjectivityshouldbothbeconsideredascontinuums,not Table1.Resultsforbinarysentimentclassificationduringiterativeretraining." ></td>
	<td class="line x" title="144:190	4Thesizeofthesentimentvocabularyafteriteration18was 22530(13462positiveand9068negative)." ></td>
	<td class="line x" title="145:190	5Kuetal.automaticallygeneratedthedictionarybyenlarging aninitialmanuallycreatedseedvocabularybyconsultingtwo thesauri,includingtong2yi4ci2ci2lin2andthe AcademiaSinicaBilingualOntologicalWordNet3." ></td>
	<td class="line x" title="146:190	Iteration Precision Recall F-measure 1 77.62 28.43 41.62 2 76.15 73.81 74.96 3 81.15 80.07 80.61 4 83.54 82.79 83.16 5 84.66 83.78 84.22 6 85.51 84.77 85.14 7 86.59 85.76 86.17 8 86.78 86.11 86.44 9 87.15 86.32 86.74 10 87.01 86.37 86.69 11 86.9 86.15 86.53 12 87.05 86.41 86.73 13 86.87 86.19 86.53 14 87.35 86.67 87.01 15 87.13 86.45 86.79 16 87.14 86.5 86.82 17 86.8 86.24 86.52 18 87.57 86.89 87.22 19 87.23 86.67 86.95 20 87.18 86.54 86.86 308 binary distinctions." ></td>
	<td class="line x" title="147:190	Section 4.1 describes how our approach compares the number of positive and negativezonesforadocumentandtreatsthedifference as a measure of the 'positivity' or 'negativity' of a review.The document in Example(2), with 12 zones, is assigned a score of 1 (the least negative score possible): the review contains some positive sentiment but the overall sentiment direction of the review is negative." ></td>
	<td class="line x" title="148:190	In contrast, Example (1) is identified as a highly negative review,as would be expected,withascoreof8,from11zones." ></td>
	<td class="line x" title="149:190	Similarly, with regard to objectivity, the sentiment density of the text in Example (3) is 0.53, which reflects its more factual character compared to Example (1), which has a score of 0.91." ></td>
	<td class="line x" title="150:190	Wecan represent sentiment and objectivity on the followingscales: Negative Balanced Positive Unopinionated Neutral Opinionated The scales are orthogonal, so we can combine themintoasinglecoordinatesystem: Opinionated Negative Positive We would expect most product reviews to be placedtowardsthetopofthethecoordinatesystem (i.e.opinionated),andstretchfromlefttoright." ></td>
	<td class="line x" title="151:190	Figure1plotstheresultsofsentimentandobjectivityclassificationofthetestcorpusinthistwodimensional coordinate system, where X represents sentiment (with scores scaled with respect to the number of zones so that 100 is the most negative possible and +100 the most positive), and Y represents sentiment density (0 being unopinionatedand 1beinghighlyopinionated)." ></td>
	<td class="line x" title="152:190	Most of the reviewsare located in the upper part of the coordinate system, indicating that they have been classified as opinionated, with either positive or negative sentiment direction." ></td>
	<td class="line x" title="153:190	Looking at the overall shape of the plot, more opinionated documents tend to have more explicit sentiment direction, while less opinionated texts stay closer to the balanced/neutralregion(aroundX=0)." ></td>
	<td class="line x" title="154:190	Figure1.Reviewsclassifiedaccordingto sentiment(Xaxis)anddegreeof opinionation(Yaxis)." ></td>
	<td class="line x" title="155:190	6 Discussion As can be seen in Figure 1, the classifier managed to map the reviews onto the coordinate system." ></td>
	<td class="line x" title="156:190	However, there are very few points in the neutral region, that is, on the same X = 0 line as balanced but with low sentiment density." ></td>
	<td class="line x" title="157:190	By inspection, we know that there are neutral reviews in our data set." ></td>
	<td class="line x" title="158:190	Wetherefore conducted a further experiment to investigate what the problem might be." ></td>
	<td class="line x" title="159:190	We took Wikipedia6 articles written in Chinese on mobile telephony and related issues, as well as several articles about the technology,the market and the history of mobile telecommunications, and split them into small parts (about a paragraph long, to make their size close to the size of the reviews) resulting in a corpus of 115documents, which we assume to be mostly unopinionated." ></td>
	<td class="line x" title="160:190	Weprocessed these documents with the trained classifier and found that they were mapped almost exactly where balanced documentsshouldbe(seeFigure2)." ></td>
	<td class="line x" title="161:190	Most of these documents have weak sentiment direction (X = 5 to +10), but are classified as relatively opinionated (Y > 0.5)." ></td>
	<td class="line x" title="162:190	The former is to be expected, whereas the latter is not." ></td>
	<td class="line x" title="163:190	When investigatingthepossiblereasonsforthisbehaviorwenoticed that the classifier found not only feature descriptions (like mz nice touch) or expressions which describe attitude ( (one) like(s)), butalsoproductfeatures(forexample,  MMS or jTV) to be opinionated." ></td>
	<td class="line x" title="164:190	This is because the presence of some advanced features such as MMS inmobilephonesisoftenregardedasapositiveby 6www.wikipedia.org -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 309 Figure2.Classificationofasampleofarticles fromWikipedia." ></td>
	<td class="line x" title="165:190	Figure3.Classificationofasampleofarticles fromWikipedia,usingtheNTUSentiment Dictionaryasthevocabularylist." ></td>
	<td class="line x" title="166:190	authors of reviews." ></td>
	<td class="line x" title="167:190	In addition, the classifier found words that were used in reviews to describe situations connected with a product and its features: for example,  (service)wasoftenusedindescriptionsofquiteunpleasantsituationswhenauserhad toturntoamanufacturer'spost-salesserviceforrepairorreplacementofa malfunctioningphone,and  (user) was often used to describe what one candowithsomeadvancedfeatures.Thustheclassifier was able to capture some product-specific as well as market-specific sentiment markers, however, it was not able to distinguish the context these generally objective words were used in." ></td>
	<td class="line x" title="168:190	This resulted in relatively high sentiment density of neutral texts which contained these words but used in othertypesofcontext." ></td>
	<td class="line x" title="169:190	To verify this hypothesis we applied the same processing to our corpus derived from Wikipedia articles, but using as the vocabulary list the NTU Sentiment Dictionary." ></td>
	<td class="line x" title="170:190	The results (Figure 3) show that most of the neutral texts are now mapped to the lower part of the opinionation scale (Y < 0.5), as expected." ></td>
	<td class="line x" title="171:190	Therefore, to successfully distinguish between balanced reviewsand neutral documents a classifier should be able to detect when product features are used as sentiment markers and when theyarenot." ></td>
	<td class="line x" title="172:190	7 Conclusionsand FutureWork Wehave described an approach to classification of documents with respect to sentiment polarity and objectivity, representing both as a continuum, and mapping classified documents onto a coordinate system that also represents the difference between balanced and neutral text." ></td>
	<td class="line x" title="173:190	We have presented a novel, unsupervised, iterative retraining procedure for deriving the classifier, starting from the most minimal size seed vocabulary, in conjunction with a simple negation check." ></td>
	<td class="line x" title="174:190	Wehave verified that the approach produces reasonable results." ></td>
	<td class="line x" title="175:190	The approach is extremely minimal in terms of language processing technology, giving it good possibilities for porting to different genres, domains and languages." ></td>
	<td class="line x" title="176:190	We also found that the accuracy of the method depends a lot on the seed word chosen." ></td>
	<td class="line x" title="177:190	If the word has a relatively low frequency or does not have a definite sentiment-related meaning, the results may be very poor.For example,an antonymous wordto (good) in Chinese is(bad), but the latter is notafrequentword:theChineseprefertosay (not good)." ></td>
	<td class="line x" title="178:190	When this word was used as the seed word, accuracy was little more than 15%." ></td>
	<td class="line x" title="179:190	Although the first iteration produced high precision (82%), the size of the extracted subcorpus was only 24 items, resulting in the system being unable to produce a good classifier for the following iterations." ></td>
	<td class="line x" title="180:190	Every new iteration produced an even poorer result as each new extracted corpus was of lower accuracy." ></td>
	<td class="line x" title="181:190	On the other hand, it seems that a seed list consisting of several low-frequency one-character words can compensate each other and produce better results by capturing a larger part of the corpus (thus increasing recall)." ></td>
	<td class="line x" title="182:190	Nevertheless a single word may also produce results even better than those for multiword seed lists." ></td>
	<td class="line x" title="183:190	For example, the two-character wordM((comfortable) as seed reached 91% -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -40 -30 -20 -10 0 10 20 30 40 50 60 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 310 accuracy with 90% recall." ></td>
	<td class="line x" title="184:190	We can conclude that our method relies on the quality of the seed word." ></td>
	<td class="line x" title="185:190	Wetherefore need to investigate ways of choosing 'lucky'seedsandavoiding'unlucky'ones." ></td>
	<td class="line x" title="186:190	Future work should also focus on improving classification accuracy: adding a little languagespecific knowledge to be able to detect some word boundariesshouldhelp;wealsoplantoexperiment with more sophisticated methods of sentiment score calculation." ></td>
	<td class="line x" title="187:190	In addition, the notion of 'zone' needs refining and language-specific adjustments (for example, a 'reversed comma' should not be considered to be a zone boundary marker, since thispunctuationmarkisgenerallyusedfortheenumerationofrelatedobjects)." ></td>
	<td class="line x" title="188:190	More experiments are also necessary to determine how the approach works across domains, and further investigation into methods for distinguishingbetweenbalancedandneutraltext." ></td>
	<td class="line x" title="189:190	Finally, we need to produce a new corpus that would enable us to evaluate the performance of a pre-trained version of the classifier that did not have any prior access to the documents it was classifying: we need the reviews to be tagged not in a binary way as they are now, but in a way that reflects the two continuums we use (sentiment and objectivity)." ></td>
	<td class="line x" title="190:190	Acknowledgements The first author is supported by the Ford FoundationInternationalFellowshipsProgram." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1034
When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging
Andreevskaia, Alina;Bergler, Sabine;"></td>
	<td class="line x" title="1:177	Proceedings of ACL-08: HLT, pages 290298, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:177	c2008 Association for Computational Linguistics When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging Alina Andreevskaia Concordia University Montreal, Quebec andreev@cs.concordia.ca Sabine Bergler Concordia University Montreal, Canada bergler@cs.concordia.ca Abstract This study presents a novel approach to the problem of system portability across different domains: a sentiment annotation system that integrates a corpus-based classifier trained on a small set of annotated in-domain data and a lexicon-based system trained on WordNet." ></td>
	<td class="line x" title="3:177	The paper explores the challenges of system portability across domains and text genres (movie reviews, news, blogs, and product reviews), highlights the factors affecting system performance on out-of-domain and smallset in-domain data, and presents a new system consisting of the ensemble of two classifiers with precision-based vote weighting, that provides significant gains in accuracy and recall over the corpus-based classifier and the lexicon-based system taken individually." ></td>
	<td class="line x" title="4:177	1 Introduction One of the emerging directions in NLP is the development of machine learning methods that perform well not only on the domain on which they were trained, but also on other domains, for which training data is not available or is not sufficient to ensure adequate machine learning." ></td>
	<td class="line x" title="5:177	Many applications require reliable processing of heterogeneous corpora, such as the World Wide Web, where the diversity of genres and domains present in the Internet limits the feasibility of in-domain training." ></td>
	<td class="line x" title="6:177	In this paper, sentiment annotation is defined as the assignment of positive, negative or neutral sentiment values to texts, sentences, and other linguistic units." ></td>
	<td class="line x" title="7:177	Recent experiments assessing system portability across different domains, conducted by Aue and Gamon (2005), demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains." ></td>
	<td class="line x" title="8:177	A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data, unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples (Aue and Gamon, 2005; Bai et al., 2005; Drezde et al., 2007; Tan et al., 2007)." ></td>
	<td class="line x" title="9:177	In this paper, we present a novel approach to the problem of system portability across different domains by developing a sentiment annotation system that integrates a corpus-based classifier with a lexicon-based system trained on WordNet." ></td>
	<td class="line x" title="10:177	By adopting this approach, we sought to develop a system that relies on both general and domainspecific knowledge, as humans do when analyzing a text." ></td>
	<td class="line x" title="11:177	The information contained in lexicographical sources, such as WordNet, reflects a lay persons general knowledge about the world, while domainspecific knowledge can be acquired through classifier training on a small set of in-domain data." ></td>
	<td class="line x" title="12:177	The first part of this paper reviews the extant literature on domain adaptation in sentiment analysis and highlights promising directions for research." ></td>
	<td class="line x" title="13:177	The second part establishes a baseline for system evaluation by drawing comparisons of system performance across four different domains/genres movie reviews, news, blogs, and product reviews." ></td>
	<td class="line x" title="14:177	The final, third part of the paper presents our system, composed of an ensemble of two classifiers  one trained on WordNet glosses and synsets and the other trained on a small in-domain training set." ></td>
	<td class="line x" title="15:177	290 2 Domain Adaptation in Sentiment Research Most text-level sentiment classifiers use standard machine learning techniques to learn and select features from labeled corpora." ></td>
	<td class="line x" title="16:177	Such approaches work well in situations where large labeled corpora are available for training and validation (e.g., movie reviews), but they do not perform well when training data is scarce or when it comes from a different domain (Aue and Gamon, 2005; Read, 2005), topic (Read, 2005) or time period (Read, 2005)." ></td>
	<td class="line x" title="17:177	There are two alternatives to supervised machine learning that can be used to get around this problem: on the one hand, general lists of sentiment clues/features can be acquired from domain-independent sources such as dictionaries or the Internet, on the other hand, unsupervised and weakly-supervised approaches can be used to take advantage of a small number of annotated in-domain examples and/or of unlabelled indomain data." ></td>
	<td class="line x" title="18:177	The first approach, using general word lists automatically acquired from the Internet or from dictionaries, outperforms corpus-based classifiers when such classifiers use out-of-domain training data or when the training corpus is not sufficiently large to accumulate the necessary feature frequency information." ></td>
	<td class="line x" title="19:177	But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews (Pang et al., 2002)." ></td>
	<td class="line x" title="20:177	On other domains, such as product reviews, the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches (Gamon and Aue, 2005)." ></td>
	<td class="line x" title="21:177	The recognition of major performance deficiencies of supervised machine learning methods with insufficient or out-of-domain training brought about an increased interest in unsupervised and weaklysupervised approaches to feature learning." ></td>
	<td class="line x" title="22:177	For instance, Aue and Gamon (2005) proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data." ></td>
	<td class="line x" title="23:177	This system performed well even when compared to systems trained on a large set of in-domain examples: on feedback messages from a web survey on knowledge bases, Aue and Gamon report 73.86% accuracy using unlabelled data compared to 77.34% for in-domain and 72.39% for the best out-of-domain training on a large training set." ></td>
	<td class="line x" title="24:177	Drezde et al.(2007) applied structural correspondence learning (Drezde et al., 2007) to the task of domain adaptation for sentiment classification of product reviews." ></td>
	<td class="line x" title="26:177	They showed that, depending on the domain, a small number (e.g., 50) of labeled examples allows to adapt the model learned on another corpus to a new domain." ></td>
	<td class="line x" title="27:177	However, they note that the success of such adaptation and the number of necessary in-domain examples depends on the similarity between the original domain and the new one." ></td>
	<td class="line x" title="28:177	Similarly, Tan et al.(2007) suggested to combine out-of-domain labeled examples with unlabelled ones from the target domain in order to solve the domain-transfer problem." ></td>
	<td class="line x" title="30:177	They applied an outof-domain-trained SVM classifier to label examples from the target domain and then retrained the classifier using these new examples." ></td>
	<td class="line x" title="31:177	In order to maximize the utility of the examples from the target domain, these examples were selected using Similarity Ranking and Relative Similarity Ranking algorithms (Tan et al., 2007)." ></td>
	<td class="line x" title="32:177	Depending on the similarity between domains, this method brought up to 15% gain compared to the baseline SVM." ></td>
	<td class="line x" title="33:177	Overall, the development of semi-supervised approaches to sentiment tagging is a promising direction of the research in this area but so far, based on reported results, the performance of such methods is inferior to the supervised approaches with indomain training and to the methods that use general word lists." ></td>
	<td class="line x" title="34:177	It also strongly depends on the similarity between the domains as has been shown by (Drezde et al., 2007; Tan et al., 2007)." ></td>
	<td class="line x" title="35:177	3 Factors Affecting System Performance The comparison of system performance across different domains involves a number of factors that can significantly affect system performance  from training set size to level of analysis (sentence or entire document), document domain/genre and many other factors." ></td>
	<td class="line x" title="36:177	In this section we present a series of experiments conducted to assess the effects of different external factors (i.e., factors unrelated to the merits of the system itself) on system performance in order to establish the baseline for performance comparisons across different domains/genres." ></td>
	<td class="line oc" title="37:177	291 3.1 Level of Analysis Research on sentiment annotation is usually conducted at the text (Aue and Gamon, 2005; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006; Turney, 2002; Turney and Littman, 2003) or at the sentence levels (Gamon and Aue, 2005; Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006)." ></td>
	<td class="line x" title="38:177	It should be noted that each of these levels presents different challenges for sentiment annotation." ></td>
	<td class="line oc" title="39:177	For example, it has been observed that texts often contain multiple opinions on different topics (Turney, 2002; Wiebe et al., 2001), which makes assignment of the overall sentiment to the whole document problematic." ></td>
	<td class="line x" title="40:177	On the other hand, each individual sentence contains a limited number of sentiment clues, which often negatively affects the accuracy and recall if that single sentiment clue encountered in the sentence was not learned by the system." ></td>
	<td class="line x" title="41:177	Since the comparison of sentiment annotation system performance on texts and on sentences has not been attempted to date, we also sought to close this gap in the literature by conducting the first set of our comparative experiments on data sets of 2,002 movie review texts and 10,662 movie review snippets (5331 with positive and 5331 with negative sentiment) provided by Bo Pang (http://www.cs.cornell.edu/People/pabo/moviereview-data/)." ></td>
	<td class="line x" title="42:177	3.2 Domain Effects The second set of our experiments explores system performance on different domains at sentence level." ></td>
	<td class="line x" title="43:177	For this we used four different data sets of sentences annotated with sentiment tags:  A set of movie review snippets (further: movie) from (Pang and Lee, 2005)." ></td>
	<td class="line x" title="44:177	This dataset of 10,662 snippets was collected automatically from www.rottentomatoes.com website." ></td>
	<td class="line x" title="45:177	All sentences in reviews marked rotten were considered negative and snippets from fresh reviews were deemed positive." ></td>
	<td class="line x" title="46:177	In order to make the results obtained on this dataset comparable to other domains, a randomly selected subset of 1066 snippets was used in the experiments." ></td>
	<td class="line x" title="47:177	 A balanced corpus of 800 manually annotated sentences extracted from 83 newspaper texts (further, news)." ></td>
	<td class="line x" title="48:177	The full set of sentences was annotated by one judge." ></td>
	<td class="line x" title="49:177	200 sentences from this corpus (100 positive and 100 negative) were also randomly selected from the corpus for an inter-annotator agreement study and were manually annotated by two independent annotators." ></td>
	<td class="line x" title="50:177	The pairwise agreement between annotators was calculated as the percent of same tags divided by the number of sentences with this tag in the gold standard." ></td>
	<td class="line x" title="51:177	The pair-wise agreement between the three annotators ranged from 92.5 to 95.9% (=0.74 and 0.75 respectively) on positive vs. negative tags." ></td>
	<td class="line x" title="52:177	 A set of sentences taken from personal weblogs (further, blogs) posted on LiveJournal (http://www.livejournal.com) and on http://www.cyberjournalist.com." ></td>
	<td class="line x" title="53:177	This corpus is composed of 800 sentences (400 sentences with positive and 400 sentences with negative sentiment)." ></td>
	<td class="line x" title="54:177	In order to establish the interannotator agreement, two independent judges were asked to annotate 200 sentences from this corpus." ></td>
	<td class="line x" title="55:177	The agreement between the two annotators on positive vs. negative tags reached 99% (=0.97)." ></td>
	<td class="line x" title="56:177	 A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu (Hu and Liu, 2004) (http://www.cs.uic.edu/ liub/FBS/FBS.html)." ></td>
	<td class="line x" title="57:177	The data set sizes are summarized in Table 1." ></td>
	<td class="line x" title="58:177	Movies News Blogs PR Text level 2002 texts n/a n/a n/a Sentence level 10662 800 800 1200 snippets sent." ></td>
	<td class="line x" title="59:177	sent." ></td>
	<td class="line x" title="60:177	sent." ></td>
	<td class="line x" title="61:177	Table 1: Datasets 3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004)." ></td>
	<td class="line x" title="62:177	These methods perform particularly well when a large volume of labeled data from the same domain as the 292 test set is available for training (Aue and Gamon, 2005)." ></td>
	<td class="line x" title="63:177	For this reason, most of the research on sentiment tagging using statistical classifiers was limited to product and movie reviews, where review authors usually indicate their sentiment in a form of a standardized score that accompanies the texts of their reviews." ></td>
	<td class="line x" title="64:177	The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level." ></td>
	<td class="line x" title="65:177	To our knowledge, the only work that describes the application of statistical classifiers (SVM) to sentence-level sentiment classification is (Gamon and Aue, 2005)1." ></td>
	<td class="line x" title="66:177	The average performance of the system on ternary classification (positive, negative, and neutral) was between 0.50 and 0.52 for both average precision and recall." ></td>
	<td class="line x" title="67:177	The results reported by (Riloff et al., 2006) for binary classification of sentences in a related domain of subjectivity tagging (i.e., the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus (Riloff et al., 2006)." ></td>
	<td class="line x" title="68:177	In order to explore the performance of different approaches in sentiment annotation at the text and sentence levels, we used a basic Nave Bayes classifier." ></td>
	<td class="line x" title="69:177	It has been shown that both Nave Bayes and SVMs perform with similar accuracy on different sentiment tagging tasks (Pang and Lee, 2004)." ></td>
	<td class="line x" title="70:177	These observations were confirmed with our own experiments with SVMs and Nave Bayes (Table 3)." ></td>
	<td class="line x" title="71:177	We used the Weka package (http://www.cs.waikato.ac.nz/ml/weka/) with default settings." ></td>
	<td class="line x" title="72:177	In the sections that follow, we describe a set of comparative experiments with SVMs and Nave Bayes classifiers (1) on texts and sentences and (2) on four different domains (movie reviews, news, blogs, and product reviews)." ></td>
	<td class="line x" title="73:177	System runs with unigrams, bigrams, and trigrams as features and with different training set sizes are presented." ></td>
	<td class="line x" title="74:177	1Recently, a similar task has been addressed by the Affective Text Task at SemEval-1 where even shorter units  headlines  were classified into positive, negative and neutral categories using a variety of techniques (Strapparava and Mihalcea, 2007)." ></td>
	<td class="line x" title="75:177	4 Experiments 4.1 System Performance on Texts vs. Sentences The experiments comparing in-domain trained system performance on texts vs. sentences were conducted on 2,002 movie review texts and on 10,662 movie review snippets." ></td>
	<td class="line x" title="76:177	The results with 10-fold cross-validation are reported in Table 22." ></td>
	<td class="line x" title="77:177	Trained on Texts Trained on Sent." ></td>
	<td class="line x" title="78:177	Tested on Tested on Tested on Tested on Texts Sent." ></td>
	<td class="line x" title="79:177	Texts Sent." ></td>
	<td class="line x" title="80:177	1gram 81.1 69.0 66.8 77.4 2gram 83.7 68.6 71.2 73.9 3gram 82.5 64.1 70.0 65.4 Table 2: Accuracy of Nave Bayes on movie reviews." ></td>
	<td class="line x" title="81:177	Consistent with findings in the literature (Cui et al., 2006; Dave et al., 2003; Gamon and Aue, 2005), on the large corpus of movie review texts, the indomain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams." ></td>
	<td class="line x" title="82:177	But the trigrams fared slightly worse than bigrams." ></td>
	<td class="line x" title="83:177	On sentences, however, we have observed an inverse pattern: unigrams performed better than bigrams and trigrams." ></td>
	<td class="line x" title="84:177	These results highlight a special property of sentence-level annotation: greater sensitivity to sparseness of the model: On texts, classifier error on one particular sentiment marker is often compensated by a number of correctly identified other sentiment clues." ></td>
	<td class="line x" title="85:177	Since sentences usually contain a much smaller number of sentiment clues than texts, sentence-level annotation more readily yields errors when a single sentiment clue is incorrectly identified or missed by the system." ></td>
	<td class="line x" title="86:177	Due to lower frequency of higher-order n-grams (as opposed to unigrams), higher-order ngram language models are more sparse, which increases the probability of missing a particular sentiment marker in a sentence (Table 33)." ></td>
	<td class="line x" title="87:177	Very large 2All results are statistically significant at = 0.01 with two exceptions: the difference between trigrams and bigrams for the system trained and tested on texts is statistically significant at alpha=0.1 and for the system trained on sentences and tested on texts is not statistically significant at  = 0.01." ></td>
	<td class="line x" title="88:177	3The results for movie reviews are lower than those reported in Table 2 since the dataset is 10 times smaller, which results in less accurate classification." ></td>
	<td class="line x" title="89:177	The statistical significance of the 293 training sets are required to overcome this higher ngram sparseness in sentence-level annotation." ></td>
	<td class="line x" title="90:177	Dataset Movie News Blogs PRs Dataset size 1066 800 800 1200 unigrams SVM 68.5 61.5 63.85 76.9 NB 60.2 59.5 60.5 74.25 nb features 5410 4544 3615 2832 bigrams SVM 59.9 63.2 61.5 75.9 NB 57.0 58.4 59.5 67.8 nb features 16286 14633 15182 12951 trigrams SVM 54.3 55.4 52.7 64.4 NB 53.3 57.0 56.0 69.7 nb features 20837 18738 19847 19132 Table 3: Accuracy of unigram, bigram and trigram models across domains." ></td>
	<td class="line x" title="91:177	4.2 System Performance on Different Domains In the second set of experiments we sought to compare system results on sentences using in-domain and out-of-domain training." ></td>
	<td class="line x" title="92:177	Table 4 shows that indomain training, as expected, consistently yields superior accuracy than out-of-domain training across all four datasets: movie reviews (Movies), news, blogs, and product reviews (PRs)." ></td>
	<td class="line x" title="93:177	The numbers for in-domain trained runs are highlighted in bold." ></td>
	<td class="line x" title="94:177	Test Data Training Data Movies News Blogs PRs Movies 68.5 55.2 53.2 60.7 News 55.0 61.5 56.25 57.4 Blogs 53.7 49.9 63.85 58.8 PRs 55.8 55.9 56.25 76.9 Table 4: Accuracy of SVM with unigram model results depends on the genre and size of the n-gram: on product reviews, all results are statistically significant at  = 0.025 level; on movie reviews, the difference between Nave Bayes and SVM is statistically significant at  = 0.01 but the significance diminishes as the size of the n-gram increases; on news, only bi-grams produce a statistically significant ( = 0.01) difference between the two machine learning methods, while on blogs the difference between SVMs and Nave Bayes is most pronounced when unigrams are used ( = 0.025)." ></td>
	<td class="line x" title="95:177	It is interesting to note that on sentences, regardless of the domain used in system training and regardless of the domain used in system testing, unigrams tend to perform better than higher-order ngrams." ></td>
	<td class="line x" title="96:177	This observation suggests that, given the constraints on the size of the available training sets, unigram-based systems may be better suited for sentence-level sentiment annotation." ></td>
	<td class="line x" title="97:177	5 Lexicon-Based Approach The search for a base-learner that can produce greatest synergies with a classifier trained on small-set in-domain data has turned our attention to lexiconbased systems." ></td>
	<td class="line x" title="98:177	Since the benefits from combining classifiers that always make similar decisions is minimal, the two (or more) base-learners should complement each other (Alpaydin, 2004)." ></td>
	<td class="line x" title="99:177	Since a system based on a fairly different learning approach is more likely to produce a different decision under a given set of circumstances, the diversity of approaches integrated in the ensemble of classifiers was expected to have a beneficial effect on the overall system performance." ></td>
	<td class="line x" title="100:177	A lexicon-based approach capitalizes on the fact that dictionaries, such as WordNet (Fellbaum, 1998), contain a comprehensive and domainindependent set of sentiment clues that exist in general English." ></td>
	<td class="line x" title="101:177	A system trained on such general data, therefore, should be less sensitive to domain changes." ></td>
	<td class="line x" title="102:177	This robustness, however is expected to come at some cost, since some domain-specific sentiment clues may not be covered in the dictionary." ></td>
	<td class="line x" title="103:177	Our hypothesis was, therefore, that a lexiconbased system will perform worse than an in-domain trained classifier but possibly better than a classifier trained on out-of domain data." ></td>
	<td class="line x" title="104:177	One of the limitations of general lexicons and dictionaries, such as WordNet (Fellbaum, 1998), as training sets for sentiment tagging systems is that they contain only definitions of individual words and, hence, only unigrams could be effectively learned from dictionary entries." ></td>
	<td class="line x" title="105:177	Since the structure of WordNet glosses is fairly different from that of other types of corpora, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and then learned additional unigrams 294 from WordNet synsets and glosses with up to 88% accuracy, when evaluated against General Inquirer (Stone et al., 1966) (GI) on the intersection of our automatically acquired list with GI." ></td>
	<td class="line x" title="106:177	In order to expand the list coverage for our experiments at the text and sentence levels, we then augmented the list by adding to it all the words annotated with Positiv or Negativ tags in GI, that were not picked up by the system." ></td>
	<td class="line x" title="107:177	The resulting list of features contained 11,000 unigrams with the degree of membership in the category of positive or negative sentiment assigned to each of them." ></td>
	<td class="line x" title="108:177	In order to assign the membership score to each word, we did 58 system runs on unique nonintersecting seed lists drawn from manually annotated list of positive and negative adjectives from (Hatzivassiloglou and McKeown, 1997)." ></td>
	<td class="line x" title="109:177	The 58 runs were then collapsed into a single set of 7,813 unique words." ></td>
	<td class="line x" title="110:177	For each word we computed a score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive." ></td>
	<td class="line x" title="111:177	The resulting measure, termed Net Overlap Score (NOS), reflected the number of ties linking a given word with other sentimentladen words in WordNet, and hence, could be used as a measure of the words centrality in the fuzzy category of sentiment." ></td>
	<td class="line x" title="112:177	The NOSs were then normalized into the interval from -1 to +1 using a sigmoid fuzzy membership function (Zadeh, 1975)4." ></td>
	<td class="line x" title="113:177	Only words with fuzzy membership degree not equal to zero were retained in the list." ></td>
	<td class="line x" title="114:177	The resulting list contained 10,809 sentiment-bearing words of different parts of speech." ></td>
	<td class="line x" title="115:177	The sentiment determination at the sentence and text level was then done by summing up the scores of all identified positive unigrams (NOS>0) and all negative unigrams (NOS<0) (Andreevskaia and Bergler, 2006)." ></td>
	<td class="line x" title="116:177	5.1 Establishing a Baseline for the Lexicon-Based System (LBS) The baseline performance of the Lexicon-Based System (LBS) described above is presented in Table 5, along with the performance results of the indomainand out-of-domain-trained SVM classifier." ></td>
	<td class="line x" title="117:177	Table 5 confirms the predicted pattern: the LBS performs with lower accuracy than in-domain4With coefficients: =1, =15." ></td>
	<td class="line x" title="118:177	Movies News Blogs PRs LBS 57.5 62.3 63.3 59.3 SVM in-dom." ></td>
	<td class="line x" title="119:177	68.5 61.5 63.85 76.9 SVM out-of-dom." ></td>
	<td class="line x" title="120:177	55.8 55.9 56.25 60.7 Table 5: System accuracy on best runs on sentences trained corpus-based classifiers, and with similar or better accuracy than the corpus-based classifiers trained on out-of-domain data." ></td>
	<td class="line x" title="121:177	Thus, the lexiconbased approach is characterized by a bounded but stable performance when the system is ported across domains." ></td>
	<td class="line x" title="122:177	These performance characteristics of corpus-based and lexicon-based approaches prompt further investigation into the possibility to combine the portability of dictionary-trained systems with the accuracy of in-domain trained systems." ></td>
	<td class="line x" title="123:177	6 Integrating the Corpus-based and Dictionary-based Approaches The strategy of integration of two or more systems in a single ensemble of classifiers has been actively used on different tasks within NLP." ></td>
	<td class="line x" title="124:177	In sentiment tagging and related areas, Aue and Gamon (2005) demonstrated that combining classifiers can be a valuable tool in domain adaptation for sentiment analysis." ></td>
	<td class="line x" title="125:177	In the ensemble of classifiers, they used a combination of nine SVM-based classifiers deployed to learn unigrams, bigrams, and trigrams on three different domains, while the fourth domain was used as an evaluation set." ></td>
	<td class="line x" title="126:177	Using then an SVM meta-classifier trained on a small number of target domain examples to combine the nine base classifiers, they obtained a statistically significant improvement on out-of-domain texts from book reviews, knowledge-base feedback, and product support services survey data." ></td>
	<td class="line x" title="127:177	No improvement occurred on movie reviews." ></td>
	<td class="line x" title="128:177	Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative." ></td>
	<td class="line x" title="129:177	Das and Chen (2004) used five classifiers to determine market sentiment on Yahoo!" ></td>
	<td class="line x" title="130:177	postings." ></td>
	<td class="line x" title="131:177	Simple majority vote was applied to make decisions within 295 the ensemble of classifiers and achieved accuracy of 62% on ternary in-domain classification." ></td>
	<td class="line x" title="132:177	In this study we describe a system that attempts to combine the portability of a dictionary-trained system (LBS) with the accuracy of an in-domain trained corpus-based system (CBS)." ></td>
	<td class="line x" title="133:177	The selection of these two classifiers for this system, thus, was theorybased." ></td>
	<td class="line x" title="134:177	The section that follows describes the classifier integration and presents the performance results of the system consisting of an ensemble CBS and LBS classifier and a precision-based vote weighting procedure." ></td>
	<td class="line x" title="135:177	6.1 The Classifier Integration Procedure and System Evaluation The comparative analysis of the corpus-based and lexicon-based systems described above revealed that the errors produced by CBS and LBS were to a great extent complementary (i.e., where one classifier makes an error, the other tends to give the correct answer)." ></td>
	<td class="line x" title="136:177	This provided further justification to the integration of corpus-based and lexicon-based approaches in a single system." ></td>
	<td class="line x" title="137:177	Table 6 below illustrates the complementarity of the performance CBS and LBS classifiers on the positive and negative categories." ></td>
	<td class="line x" title="138:177	In this experiment, the corpus-based classifier was trained on 400 annotated product review sentences5." ></td>
	<td class="line x" title="139:177	The two systems were then evaluated on a test set of another 400 product review sentences." ></td>
	<td class="line x" title="140:177	The results reported in Table 6 are statistically significant at  = 0.01." ></td>
	<td class="line x" title="141:177	CBS LBS Precision positives 89.3% 69.3% Precision negatives 55.5% 81.5% Pos/Neg Precision 58.0% 72.1% Table 6: Base-learners precision and recall on product reviews on test data." ></td>
	<td class="line x" title="142:177	Table 6 shows that the corpus-based system has a very good precision on those sentences that it classifies as positive but makes a lot of errors on those sentences that it deems negative." ></td>
	<td class="line x" title="143:177	At the same time, the lexicon-based system has low precision on positives 5The small training set explains relatively low overall performance of the CBS system." ></td>
	<td class="line x" title="144:177	and high precision on negatives6." ></td>
	<td class="line x" title="145:177	Such complementary distribution of errors produced by the two systems was observed on different data sets from different domains, which suggests that the observed distribution pattern reflects the properties of each of the classifiers, rather than the specifics of the domain/genre." ></td>
	<td class="line x" title="146:177	In order to take advantage of the observed complementarity of the two systems, the following procedure was used." ></td>
	<td class="line x" title="147:177	First, a small set of in-domain data was used to train the CBS system." ></td>
	<td class="line x" title="148:177	Then both CBS and LBS systems were run separately on the same training set, and for each classifier, the precision measures were calculated separately for those sentences that the classifier considered positive and those it considered negative." ></td>
	<td class="line x" title="149:177	The chance-level performance (50%) was then subtracted from the precision figures to ensure that the final weights reflect by how much the classifiers precision exceeds the chance level." ></td>
	<td class="line x" title="150:177	The resulting chance-adjusted precision numbers of the two classifiers were then normalized, so that the weights of CBS and LBS classifiers sum up to 100% on positive and to 100% on negative sentences." ></td>
	<td class="line x" title="151:177	These weights were then used to adjust the contribution of each classifier to the decision of the ensemble system." ></td>
	<td class="line x" title="152:177	The choice of the weight applied to the classifier decision, thus, varied depending on whether the classifier scored a given sentence as positive or as negative." ></td>
	<td class="line x" title="153:177	The resulting system was then tested on a separate test set of sentences7." ></td>
	<td class="line x" title="154:177	The small-set training and evaluation experiments with the system were performed on different domains using 3-fold validation." ></td>
	<td class="line x" title="155:177	The experiments conducted with the Ensemble system were designed to explore system performance under conditions of limited availability of annotated data for classifier training." ></td>
	<td class="line x" title="156:177	For this reason, the numbers reported for the corpus-based classifier do not reflect the full potential of machine learning approaches when sufficient in-domain training data is available." ></td>
	<td class="line x" title="157:177	Table 7 presents the results of these experiments by domain/genre." ></td>
	<td class="line x" title="158:177	The results 6These results are consistent with an observation in (Kennedy and Inkpen, 2006), where a lexicon-based system performed with a better precision on negative than on positive texts." ></td>
	<td class="line x" title="159:177	7The size of the test set varied in different experiments due to the availability of annotated data for a particular domain." ></td>
	<td class="line x" title="160:177	296 are statistically significant at  = 0.01, except the runs on movie reviews where the difference between the LBS and Ensemble classifiers was significant at  = 0.05." ></td>
	<td class="line x" title="161:177	LBS CBS Ensemble News Acc 67.8 53.2 73.3 F 0.82 0.71 0.85 Movies Acc 54.5 53.5 62.1 F 0.73 0.72 0.77 Blogs Acc 61.2 51.1 70.9 F 0.78 0.69 0.83 PRs Acc 59.5 58.9 78.0 F 0.77 0.75 0.88 Average Acc 60.7 54.2 71.1 F 0.77 0.72 0.83 Table 7: Performance of the ensemble classifier Table 7 shows that the combination of two classifiers into an ensemble using the weighting technique described above leads to consistent improvement in system performance across all domains/genres." ></td>
	<td class="line x" title="162:177	In the ensemble system, the average gain in accuracy across the four domains was 16.9% relative to CBS and 10.3% relative to LBS." ></td>
	<td class="line x" title="163:177	Moreover, the gain in accuracy and precision was not offset by decreases in recall: the net gain in recall was 7.4% relative to CBS and 13.5% vs. LBS." ></td>
	<td class="line x" title="164:177	The ensemble system on average reached 99.1% recall." ></td>
	<td class="line x" title="165:177	The F-measure has increased from 0.77 and 0.72 for LBS and CBS classifiers respectively to 0.83 for the whole ensemble system." ></td>
	<td class="line x" title="166:177	7 Discussion The development of domain-independent sentiment determination systems poses a substantial challenge for researchers in NLP and artificial intelligence." ></td>
	<td class="line x" title="167:177	The results presented in this study suggest that the integration of two fairly different classifier learning approaches in a single ensemble of classifiers can yield substantial gains in system performance on all measures." ></td>
	<td class="line x" title="168:177	The most substantial gains occurred in recall, accuracy, and F-measure." ></td>
	<td class="line x" title="169:177	This study permits to highlight a set of factors that enable substantial performance gains with the ensemble of classifiers approach." ></td>
	<td class="line x" title="170:177	Such gains are most likely when (1) the errors made by the classifiers are complementary, i.e., where one classifier makes an error, the other tends to give the correct answer, (2) the classifier errors are not fully random and occur more often in a certain segment (or category) of classifier results, and (3) there is a way for a system to identify that low-precision segment and reduce the weights of that classifiers results on that segment accordingly." ></td>
	<td class="line x" title="171:177	The two classifiers used in this study  corpus-based and lexicon-based  provided an interesting illustration of potential performance gains associated with these three conditions." ></td>
	<td class="line x" title="172:177	The use of precision of classifier results on the positives and negatives proved to be an effective technique for classifier vote weighting within the ensemble." ></td>
	<td class="line x" title="173:177	8 Conclusion This study contributes to the research on sentiment tagging, domain adaptation, and the development of ensembles of classifiers (1) by proposing a novel approach for sentiment determination at sentence level and delineating the conditions under which greatest synergies among combined classifiers can be achieved, (2) by describing a precision-based technique for assigning differential weights to classifier results on different categories identified by the classifier (i.e., categories of positive vs. negative sentences), and (3) by proposing a new method for sentiment annotation in situations where the annotated in-domain data is scarce and insufficient to ensure adequate performance of the corpus-based classifier, which still remains the preferred choice when large volumes of annotated data are available for system training." ></td>
	<td class="line x" title="174:177	Among the most promising directions for future research in the direction laid out in this paper is the deployment of more advanced classifiers and feature selection techniques that can further enhance the performance of the ensemble of classifiers." ></td>
	<td class="line x" title="175:177	The precision-based vote weighting technique may prove to be effective also in situations, where more than two classifiers are integrated into a single system." ></td>
	<td class="line x" title="176:177	We expect that these more advanced ensemble-ofclassifiers systems would inherit the benefits of multiple complementary approaches to sentiment annotation and will be able to achieve better and more stable accuracy on in-domain, as well as on out-ofdomain data." ></td>
	<td class="line x" title="177:177	297" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1036
A Joint Model of Text and Aspect Ratings for Sentiment Summarization
Titov, Ivan;McDonald, Ryan;"></td>
	<td class="line x" title="1:223	Proceedings of ACL-08: HLT, pages 308316, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:223	c2008 Association for Computational Linguistics A Joint Model of Text and Aspect Ratings for Sentiment Summarization Ivan Titov Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 titov@uiuc.edu Ryan McDonald Google Inc. 76 Ninth Avenue New York, NY 10011 ryanmcd@google.com Abstract Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects." ></td>
	<td class="line x" title="3:223	We propose a statistical model which is able to discover corresponding topics in text and extract textual evidence from reviews supporting each of these aspect ratings  a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a)." ></td>
	<td class="line x" title="4:223	Our model achieves high accuracy, without any explicitly labeled data except the user provided opinion ratings." ></td>
	<td class="line x" title="5:223	The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with correlated signals." ></td>
	<td class="line x" title="6:223	1 Introduction Usergeneratedcontentrepresentsauniquesourceof information in which user interface tools have facilitated the creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums." ></td>
	<td class="line x" title="7:223	Many previous studies on user generated content have attempted to predict these labels automatically from the associated text." ></td>
	<td class="line x" title="8:223	However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications." ></td>
	<td class="line x" title="9:223	In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Nikos Fine Dining Food 4/5 Best fish in the city, Excellent appetizers Decor 3/5 Cozy with an old world feel, Too dark Service 1/5 Our waitress was rude, Awful service Value 5/5 Good Greek food for the $, Great price! Figure 1: An example aspect-based summary." ></td>
	<td class="line x" title="10:223	Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence." ></td>
	<td class="line x" title="11:223	For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5." ></td>
	<td class="line x" title="12:223	Standard aspect-based summarization consists of two problems." ></td>
	<td class="line x" title="13:223	The first is aspect identification and mention extraction." ></td>
	<td class="line x" title="14:223	Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions that are associated with each." ></td>
	<td class="line x" title="15:223	Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service." ></td>
	<td class="line x" title="16:223	Similarly, extracted text can range from a single word to phrases and sentences." ></td>
	<td class="line x" title="17:223	The second problem is sentiment classification." ></td>
	<td class="line x" title="18:223	Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating." ></td>
	<td class="line oc" title="19:223	Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007)." ></td>
	<td class="line x" title="20:223	Other studies use the term feature (Hu and Liu, 2004b)." ></td>
	<td class="line x" title="21:223	308 Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great." ></td>
	<td class="line x" title="22:223	On top of that our service was excellent and the price was right." ></td>
	<td class="line x" title="23:223	Cant wait to go back!" ></td>
	<td class="line x" title="24:223	Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary." ></td>
	<td class="line x" title="25:223	My soup was cold and expensive plus it felt like they hadnt painted since 1980." ></td>
	<td class="line x" title="26:223	Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost." ></td>
	<td class="line x" title="27:223	Wait staff was friendly." ></td>
	<td class="line x" title="28:223	Lots of fun decorations." ></td>
	<td class="line x" title="29:223	 Food The chicken was great, My soup wascold, The food is only mediocre Decor it felt like they hadnt painted since1980, Lots of fun decorations Service service was excellent,Wait staff was friendly Value the price was right, My soup was coldand expensive, well worth the cost Figure 2: Extraction problem: Produce aspect mentions from a corpus of aspect rated reviews." ></td>
	<td class="line x" title="30:223	provide ratings for each aspect making automated means unnecessary.2 Aspect identification has also been thoroughly studied (Hu and Liu, 2004b; Gamon et al., 2005; Titov and McDonald, 2008), but again, ontologies and users often provide this information negating the need for automation." ></td>
	<td class="line x" title="31:223	Though it may be reasonable to expect a user to provide a rating for each aspect, it is unlikely that a user will annotate every sentence and phrase in a review as being relevant to some aspect." ></td>
	<td class="line x" title="32:223	Thus, it can be argued that the most pressing challenge in an aspect-based summarization system is to extract all relevant mentions for each aspect, as illustrated in figure 2." ></td>
	<td class="line x" title="33:223	When labeled data exists, this problem can be solved effectively using a wide variety of methods available for text classification and information extraction (Manning and Schutze, 1999)." ></td>
	<td class="line x" title="34:223	However, labeled data is often hard to come by, especially when one considers all possible domains of products and services." ></td>
	<td class="line x" title="35:223	Instead, we propose an unsupervised model that leverages aspect ratings that frequently accompany an online review." ></td>
	<td class="line x" title="36:223	In order to construct such model, we make two assumptions." ></td>
	<td class="line x" title="37:223	First, ratable aspects normally represent coherent topics which can be potentially discovered from co-occurrence information in the text." ></td>
	<td class="line x" title="38:223	Second, we hypothesize that the most predictive features of an aspect rating are features derived from the text segments discussing the corresponding aspect." ></td>
	<td class="line x" title="39:223	Motivated by these observations, we construct ajointstatisticalmodeloftextandsentimentratings." ></td>
	<td class="line x" title="40:223	The model is at heart a topic model in that it assigns words to a set of induced topics, each of which may represent one particular aspect." ></td>
	<td class="line x" title="41:223	The model is extended through a set of maximum entropy classifiers, one per each rated aspect, that are used to pre2E.g., http://zagat.com and http://tripadvisor.com." ></td>
	<td class="line x" title="42:223	dict the sentiment rating towards each of the aspects." ></td>
	<td class="line x" title="43:223	However, only the words assigned to an aspects corresponding topic are used in predicting the rating for that aspect." ></td>
	<td class="line x" title="44:223	As a result, the model enforces that words assigned to an aspects topic are predictive of the associated rating." ></td>
	<td class="line x" title="45:223	Our approach is more general than the particular statistical model we consider in this paper." ></td>
	<td class="line x" title="46:223	For example, other topic models can be used as a part of our model and the proposed class of modelscan beemployed inothertasksbeyond sentiment summarization, e.g., segmentation of blogs on the basis of topic labels provided by users, or topic discovery on the basis of tags given by users on social bookmarking sites.3 The rest of the paper is structured as follows." ></td>
	<td class="line x" title="47:223	Section 2 begins with a discussion of the joint textsentiment model approach." ></td>
	<td class="line x" title="48:223	In Section 3 we provide both a qualitative and quantitative evaluation of the proposed method." ></td>
	<td class="line x" title="49:223	We conclude in Section 4 with an examination of related work." ></td>
	<td class="line x" title="50:223	2 The Model In this section we describe a new statistical model called the Multi-Aspect Sentiment model (MAS), whichconsistsoftwoparts." ></td>
	<td class="line x" title="51:223	Thefirstpartisbasedon Multi-Grain Latent Dirichlet Allocation (Titov and McDonald, 2008), whichhasbeenpreviouslyshown to build topics that are representative of ratable aspects." ></td>
	<td class="line x" title="52:223	The second part is a set of sentiment predictors per aspect that are designed to force specific topics in the model to be directly correlated with a particular aspect." ></td>
	<td class="line x" title="53:223	2.1 Multi-Grain LDA The Multi-Grain Latent Dirichlet Allocation model (MG-LDA) is an extension of Latent Dirichlet Allocation (LDA) (Blei et al., 2003)." ></td>
	<td class="line x" title="54:223	As was demon3See e.g. del.ico.us (http://del.ico.us)." ></td>
	<td class="line x" title="55:223	309 strated in Titov and McDonald (2008), the topics produced by LDA do not correspond to ratable aspects of entities." ></td>
	<td class="line x" title="56:223	In particular, these models tend to build topics that globally classify terms into product instances (e.g., Creative Labs Mp3 players versus iPods, or New York versus Paris Hotels)." ></td>
	<td class="line x" title="57:223	To combat this, MG-LDA models two distinct types of topics: global topics and local topics." ></td>
	<td class="line x" title="58:223	As in LDA, the distribution of global topics is fixed for a document (a user review)." ></td>
	<td class="line x" title="59:223	However, the distribution of local topics is allowed to vary across the document." ></td>
	<td class="line x" title="60:223	A word in the document is sampled either from the mixture of global topics or from the mixture of local topics specific to the local context of the word." ></td>
	<td class="line x" title="61:223	It was demonstrated in Titov and McDonald (2008) that ratable aspects will be captured by local topics and global topics will capture properties of reviewed items." ></td>
	<td class="line x" title="62:223	For example, consider an extract from a review of a London hotel: public transport in London is straightforward, the tube station is about an 8 minute walk or you can get a bus for 1.50." ></td>
	<td class="line x" title="63:223	It can be viewed as a mixture of topic London shared bytheentirereview(words: London, tube, ), and the ratable aspect location, specific for the local context of the sentence (words: transport, walk, bus)." ></td>
	<td class="line x" title="64:223	Local topics are reused between very different types of items, whereas global topics correspond only to particular types of items." ></td>
	<td class="line x" title="65:223	In MG-LDA a document is represented as a set of sliding windows, each covering T adjacent sentences within a document.4 Each windowvin documentdhas an associated distribution over local topicslocd,v and a distribution defining preference for local topics versus global topics pid,v. A word can be sampled using any window covering its sentence s, where the window is chosen according to a categorical distributiond,s. Importantly, the fact that windows overlap permits the model to exploit a larger co-occurrence domain." ></td>
	<td class="line x" title="66:223	These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007)." ></td>
	<td class="line x" title="67:223	Introduction of a symmetrical Dirichlet priorDir() for the distributiond,s can control the smoothness of transitions." ></td>
	<td class="line x" title="68:223	4Ourparticularimplementationisoversentences,butsliding windows in theory can be over any sized fragment of text." ></td>
	<td class="line x" title="69:223	(a) (b) Figure 3: (a) MG-LDA model." ></td>
	<td class="line x" title="70:223	(b) An extension of MGLDA to obtain MAS." ></td>
	<td class="line x" title="71:223	The formal definition of the model with Kgl global and Kloc local topics is as follows: First, draw Kgl word distributions for global topics glz from a Dirichlet prior Dir(gl) and Kloc word distributions for local topics locz from Dir(loc)." ></td>
	<td class="line x" title="72:223	Then, for each documentd:  Choose a distribution of global topicsgld Dir(gl)." ></td>
	<td class="line x" title="73:223	 For each sentence s choose a distribution over sliding windowsd,s(v) Dir()." ></td>
	<td class="line x" title="74:223	 For each sliding windowv  chooselocd,v Dir(loc),  choosepid,v Beta(mix)." ></td>
	<td class="line x" title="75:223	 For each wordiin sentencesof documentd  choose windowvd,i d,s,  chooserd,i pid,vd,i,  ifrd,i = glchoose global topiczd,i gld ,  ifrd,i=locchoose local topiczd,ilocd,vd,i,  choose wordwd,i from the word distributionrd,izd,i. Beta(mix)is a prior Beta distribution for choosing between local and global topics." ></td>
	<td class="line x" title="76:223	In Figure 3a the corresponding graphical model is presented." ></td>
	<td class="line x" title="77:223	2.2 Multi-Aspect Sentiment Model MG-LDA constructs a set of topics that ideally correspond to ratable aspects of an entity (often in a many-to-one relationship of topics to aspects)." ></td>
	<td class="line x" title="78:223	A major shortcoming of this model  and all other unsupervised models  is that this correspondence is not explicit, i.e., how does one say that topic X is really about aspect Y?" ></td>
	<td class="line x" title="79:223	However, we can observe that numeric aspect ratings are often included in our data by users who left the reviews." ></td>
	<td class="line x" title="80:223	We then make the assumption that the text of the review discussing an aspect is predictive of its rating." ></td>
	<td class="line x" title="81:223	Thus, if we model the prediction of aspect ratings jointly with the construction of explicitly associated topics, then such a 310 model should benefit from both higher quality topics and a direct assignment from topics to aspects." ></td>
	<td class="line x" title="82:223	This is the basic idea behind the Multi-Aspect Sentiment model (MAS)." ></td>
	<td class="line x" title="83:223	In its simplest form, MAS introduces a classifier for each aspect, which is used to predict its rating." ></td>
	<td class="line x" title="84:223	Each classifier is explicitly associated to a single topic in the model and only words assigned to that topic can participate in the prediction of the sentiment rating for the aspect." ></td>
	<td class="line x" title="85:223	However, it has been observed that ratings for different aspects can be correlated (Snyder and Barzilay, 2007), e.g., very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms, but also is very predictive of low ratings for the aspectsserviceanddining." ></td>
	<td class="line x" title="86:223	Thiscomplicatesdiscovery of the corresponding topics, as in many reviews the most predictive features for an aspect rating might correspond to another aspect." ></td>
	<td class="line x" title="87:223	Another problem with this overly simplistic model is the presence of opinions about an item in general without referring to any particular aspect." ></td>
	<td class="line x" title="88:223	For example, this product is the worst I have ever purchased is a good predictor of low ratings for every aspect." ></td>
	<td class="line x" title="89:223	In such cases, non-aspect background words will appear to be the mostpredictive." ></td>
	<td class="line x" title="90:223	Therefore,theuseoftheaspectsentiment classifiers based only on the words assigned to the corresponding topics is problematic." ></td>
	<td class="line x" title="91:223	Such a model will not be able to discover coherent topics associated with each aspect, because in many cases the most predictive fragments for each aspect rating will not be the ones where this aspect is discussed." ></td>
	<td class="line x" title="92:223	Ourproposalistoestimatethedistributionofpossible values of an aspect rating on the basis of the overall sentiment rating and to use the words assigned to the corresponding topic to compute corrections for this aspect." ></td>
	<td class="line x" title="93:223	An aspect rating is typically correlated to the overall sentiment rating5 and the fragments discussing this particular aspect will help to correct the overall sentiment in the appropriate direction." ></td>
	<td class="line x" title="94:223	For example, if a review of a hotel is generally positive, but it includes a sentence the neighborhood is somewhat seedy then this sentence is predictive of rating for an aspect location being below other ratings." ></td>
	<td class="line x" title="95:223	This rectifies the aforementioned 5In the dataset used in our experiments all three aspect ratings are equivalent for 5,250 reviews out of 10,000." ></td>
	<td class="line x" title="96:223	problems." ></td>
	<td class="line x" title="97:223	First, aspect sentiment ratings can often be regarded as conditionally independent given the overall rating, therefore the model will not be forced to include in an aspect topic any words from other aspect topics." ></td>
	<td class="line x" title="98:223	Secondly, the fragments discussing overall opinion will influence the aspect rating only through the overall sentiment rating." ></td>
	<td class="line x" title="99:223	The overall sentiment is almost always present in the real data along with the aspect ratings, but it can be coarsely discretized and we preferred to use a latent overall sentiment." ></td>
	<td class="line x" title="100:223	The MAS model is presented in Figure 3b." ></td>
	<td class="line x" title="101:223	Note that for simplicity we decided to omit in the figure the components of the MG-LDA model other than variables r, z and w, though they are present in the statistical model." ></td>
	<td class="line x" title="102:223	MAS also allows for extra unassociated local topics in order to capture aspects not explicitly rated by the user." ></td>
	<td class="line x" title="103:223	As in MG-LDA, MAS has global topics which are expected to capture topics correspondingtoparticulartypesofitems,suchLondon hotelsorseaside resortsforthehoteldomain." ></td>
	<td class="line x" title="104:223	In figure 3b we shaded the aspect ratingsya, assuming that every aspect rating is present in the data (though in practice they might be available only for some reviews)." ></td>
	<td class="line x" title="105:223	In this model the distribution of the overall sentiment rating yov is based on all the n-gram featuresofareviewtext." ></td>
	<td class="line x" title="106:223	Thenthedistributionofya, for every rated aspecta, can be computed from the distribution of yov and from any n-gram feature where at least one word in the n-gram is assigned to the associated aspect topic (r = loc, z = a)." ></td>
	<td class="line x" title="107:223	Instead of having a latent variable yov,6 we use a similar model which does not have an explicit notion ofyov." ></td>
	<td class="line x" title="108:223	The distribution of a sentiment ratingya for each rated aspectais computed from two scores." ></td>
	<td class="line x" title="109:223	The first score is computed on the basis of all the ngrams, but using a common set of weights independent of the aspecta." ></td>
	<td class="line x" title="110:223	Another score is computed only using n-grams associated with the related topic, but an aspect-specific set of weights is used in this computation." ></td>
	<td class="line x" title="111:223	More formally, we consider the log-linear distribution: P(ya = y|w,r,z)exp(bay+ summationdisplay fw Jf,y+paf,r,zJaf,y), (1) where w, r, z are vectors of all the words in a docu6Preliminary experiments suggested that this is also a feasible approach, but somewhat more computationally expensive." ></td>
	<td class="line x" title="112:223	311 ment, assignments of context (global or local) and topics for all the words in the document, respectively." ></td>
	<td class="line x" title="113:223	bay is the bias term which regulates the prior distribution P(ya = y), f iterates through all the n-grams, Jy,f and Jay,f are common weights and aspect-specific weights for n-gram feature f. paf,r,z is equal to a fraction of words in n-gram feature f assigned to the aspect topic (r = loc,z = a)." ></td>
	<td class="line x" title="114:223	2.3 Inference in MAS Exact inference in the MAS model is intractable." ></td>
	<td class="line x" title="115:223	Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in (Griffiths and Steyvers, 2004)." ></td>
	<td class="line x" title="116:223	Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984)." ></td>
	<td class="line x" title="117:223	It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed." ></td>
	<td class="line x" title="118:223	In Gibbs sampling, variables are sequentially sampled from their distributions conditioned on all other variables in the model." ></td>
	<td class="line x" title="119:223	Such a chain of model states converges to a sample from the joint distribution." ></td>
	<td class="line x" title="120:223	A naive application of this technique to LDA would imply that both assignments of topics to words z and distributions  and  should be sampled." ></td>
	<td class="line x" title="121:223	However, (Griffiths and Steyvers, 2004) demonstrated that anefficient collapsedGibbssamplercan be constructed, where only assignments z need to be sampled, whereas the dependency on distributionsand can be integrated out analytically." ></td>
	<td class="line x" title="122:223	In the case of MAS we also use maximum aposteriori estimates of the sentiment predictor parameters bay, Jy,f and Jay,f. The MAP estimates for parameters bay, Jy,f and Jay,f are obtained by using stochastic gradient ascent." ></td>
	<td class="line x" title="123:223	The direction of the gradient is computed simultaneously with running a chain by generating several assignments at each step and averaging over the corresponding gradient estimates." ></td>
	<td class="line x" title="124:223	For details on computing gradients for loglinear graphical models with Gibbs sampling we refer the reader to (Neal, 1992)." ></td>
	<td class="line x" title="125:223	Space constraints do not allow us to present either the derivation or a detailed description of the sampling algorithm." ></td>
	<td class="line x" title="126:223	However, note that the conditional distribution used in sampling decomposes into two parts: P(vd,i = v,rd,i = r,zd,i = z|v,r,z,w,y)  d,iv,r,z d,ir,z, (2) where v, r and z are vectors of assignments of sliding windows, context (global or local) and topics for all the words in the collection except for the considered word at positioniin documentd;yis the vector of sentiment ratings." ></td>
	<td class="line x" title="127:223	The first factor d,iv,r,z is responsibleformodelingco-occurrencesonthewindowanddocumentlevelandcoherenceofthetopics." ></td>
	<td class="line x" title="128:223	This factor is proportional to the conditional distribution used in the Gibbs sampler of the MG-LDA model (Titov and McDonald, 2008)." ></td>
	<td class="line x" title="129:223	The last factor quantifies the influence of the assignment of the word (d,i) on the probability of the sentiment ratings." ></td>
	<td class="line x" title="130:223	It appears only if ratings are known (observable) and equals: d,ir,z = productdisplay a P(yda|w,r,rd,i = r,z,zd,i = z) P(yda|w,r,z,rd,i = gl) , wheretheprobabilitydistributioniscomputedasdefined in expression (1), yda is the rating for the ath aspect of reviewd." ></td>
	<td class="line x" title="131:223	3 Experiments In this section we present qualitative and quantitative experiments." ></td>
	<td class="line x" title="132:223	For the qualitative analysis we show that topics inferred by the MAS model correspond directly to the associated aspects." ></td>
	<td class="line x" title="133:223	For the quantitative analysis we show that the MAS model induces a distribution over the rated aspects which can be used to accurately predict whether a text fragment is relevant to an aspect or not." ></td>
	<td class="line x" title="134:223	3.1 Qualitative Evaluation To perform qualitative experiments we used a set of reviews of hotels taken from TripAdvisor.com7 that contained 10,000 reviews (109,024 sentences, 2,145,313 words in total)." ></td>
	<td class="line x" title="135:223	Every review was rated with at least three aspects: service, location and rooms." ></td>
	<td class="line x" title="136:223	Each rating is an integer from 1 to 5." ></td>
	<td class="line x" title="137:223	The dataset was tokenized and sentence split automatically." ></td>
	<td class="line x" title="138:223	7(c) 2005-06, TripAdvisor, LLC All rights reserved 312 rated aspect top words service staff friendly helpful service desk concierge excellent extremely hotel great reception english pleasant help location hotel walk location station metro walking away right minutes close bus city located just easy restaurants local rooms room bathroom shower bed tv small water clean comfortable towels bath nice large pillows space beds tub topics breakfast free coffee internet morning access buffet day wine nice lobby complimentary included good fruit $ night parking rate price paid day euros got cost pay hotel worth euro expensive car extra deal booked room noise night street air did door floor rooms open noisy window windows hear outside problem quiet sleep global moscow st russian petersburg nevsky russia palace hermitage kremlin prospect river prospekt kempinski topics paris tower french eiffel dame notre rue st louvre rer champs opera elysee george parisian du pantheon cafes Table 1: Top words from MAS for hotel reviews." ></td>
	<td class="line x" title="139:223	Krooms top words 2 rooms clean hotel room small nice comfortable modern good quite large lobby old decor spacious decorated bathroom size room noise night street did air rooms door open noisy window floor hear windows problem outside quiet sleep bit light 3 room clean bed comfortable rooms bathroom small beds nice large size tv spacious good double big space huge king room floor view rooms suite got views given quiet building small balcony upgraded nice high booked asked overlooking room bathroom shower air water did like hot small towels door old window toilet conditioning open bath dirty wall tub 4 room clean rooms comfortable bed small beds nice bathroom size large modern spacious good double big quiet decorated check arrived time day airport early room luggage took late morning got long flight ready minutes did taxi bags went room noise night street did air rooms noisy open door hear windows window outside quiet sleep problem floor conditioning bathroom room shower tv bed small water towels bath tub large nice toilet clean space toiletries flat wall sink screen Table 2: Top words for aspect rooms with different number of topicsKrooms." ></td>
	<td class="line x" title="140:223	We ran the sampling chain for 700 iterations to produce a sample." ></td>
	<td class="line x" title="141:223	Distributions of words in each topic were estimated as the proportion of words assignedtoeachtopic, takingintoaccounttopicmodel priors gl and loc." ></td>
	<td class="line x" title="142:223	The sliding windows were chosen to cover 3 sentences for all the experiments." ></td>
	<td class="line x" title="143:223	All the priors were chosen to be equal to 0.1." ></td>
	<td class="line x" title="144:223	We used 15 local topics and 30 global topics." ></td>
	<td class="line x" title="145:223	In the model, the first three local topics were associated to the rating classifiers for each aspects." ></td>
	<td class="line x" title="146:223	As a result, we would expect these topics to correspond to the service, location, and rooms aspects respectively." ></td>
	<td class="line x" title="147:223	Unigram and bigram features were used in the sentiment predictors in the MAS model." ></td>
	<td class="line x" title="148:223	Before applying the topic models we removed punctuation and also removed stop words using the standard list of stop words,8 however, all the words and punctuation were used in the sentiment predictors." ></td>
	<td class="line x" title="149:223	It does not take many chain iterations to discover initial topics." ></td>
	<td class="line x" title="150:223	This happens considerably faster than the appropriate weights of the sentiment predictor being learned." ></td>
	<td class="line x" title="151:223	This poses a problem, because, in the beginning, the sentiment predictors are not accurate enough to force the model to discover appropriate topics associated with each of the rated aspects." ></td>
	<td class="line x" title="152:223	And assoonastopicareformed, aspectsentimentpredictors cannot affect them anymore because they do not 8http://www.dcs.gla.ac.uk/idom/ir resources/linguistic utils/ stop words have access to the true words associated with their aspects." ></td>
	<td class="line x" title="153:223	To combat this problem we first train the sentiment classifiers by assuming thatpaf,r,z is equal for all the local topics, which effectively ignores the topic model." ></td>
	<td class="line x" title="154:223	Then we use the estimated parameters within the topic model.9 Secondly, we modify the sampling algorithm." ></td>
	<td class="line x" title="155:223	The conditional probability used in sampling, expression (2), is proportional to the product of two factors." ></td>
	<td class="line x" title="156:223	The first factor, d,iv,r,z, expresses a preference for topics likely from the co-occurrence information, whereas the second one, d,ir,z, favors the choice of topics which are predictive of the observable sentiment ratings." ></td>
	<td class="line x" title="157:223	We used (d,ir,z)1+0.95tq in the sampling distribution instead of d,ir,z, where t is the iteration number." ></td>
	<td class="line x" title="158:223	q was chosen to be 4, though the quality of the topics seemed to be indistinguishable with any q between 3 and 10." ></td>
	<td class="line x" title="159:223	This can be thought of as having 1 + 0.95tq ratings instead of a single vector assigned to each review, i.e., focusing the model on prediction of the ratings rather than finding the topic labels which are good at explaining co-occurrences of words." ></td>
	<td class="line x" title="160:223	These heuristics influence sampling only during the first iterations of the chain." ></td>
	<td class="line x" title="161:223	Top words for some of discovered local topics, in9Initial experiments suggested that instead of doing this pre-training we could start with very large priors loc and mix, and then reduce them through the course of training." ></td>
	<td class="line x" title="162:223	However, this is significantly more computationally expensive." ></td>
	<td class="line x" title="163:223	313  0  10  20  30  40  50  60  70  80  90  100  0  10  20  30  40  50  60  70  80  90  100 Recall Precision topic modelmaxent classifier topic model maxent classifier  0  10  20  30  40  50  60  70  80  90  100  0  10  20  30  40  50  60  70  80  90  100 Recall Precision maxent classifier 1 topic2 topics 3 topics4 topics  0  10  20  30  40  50  60  70  80  90  100  0  10  20  30  40  50  60  70  80  90  100 Recall Precision (a) (b) (c) Figure 4: (a) Aspect service." ></td>
	<td class="line x" title="164:223	(b) Aspect location." ></td>
	<td class="line x" title="165:223	(c) Aspect rooms." ></td>
	<td class="line x" title="166:223	cludingthefirst3topicsassociatedwiththeratedaspects, and also top words for some of global topics are presented in Table 1." ></td>
	<td class="line x" title="167:223	We can see that the model discovered as its first three topics the correct associated aspects: service, location, and rooms." ></td>
	<td class="line x" title="168:223	Other local topics, as for the MG-LDA model, correspond to otheraspectsdiscussedinreviews(breakfast, prices, noise), and as it was previously shown in Titov and McDonald (2008), aspects for global topics correspond to the types of reviewed items (hotels in Russia, Paris hotels) or background words." ></td>
	<td class="line x" title="169:223	Notice though, that the 3rd local topic induced for the rating rooms is slightly narrow." ></td>
	<td class="line x" title="170:223	This can be explained by the fact that the aspect rooms is a central aspect of hotel reviews." ></td>
	<td class="line x" title="171:223	A very significant fraction of text in every review can be thought of as a part of the aspect rooms." ></td>
	<td class="line x" title="172:223	These portions of reviews discuss different coherent sub-aspects related to the aspect rooms, e.g., the previously discovered topic noise." ></td>
	<td class="line x" title="173:223	Therefore, it is natural to associate several topics to such central aspects." ></td>
	<td class="line x" title="174:223	To test this we varied the number of topics associated with the sentiment predictor for the aspect rooms." ></td>
	<td class="line x" title="175:223	Top words for resulting topics are presented in Table 2." ></td>
	<td class="line x" title="176:223	It can be observed that the topic model discovered appropriate topics while the number of topics was below 4." ></td>
	<td class="line x" title="177:223	With 4 topics a semantically unrelated topic (check-in/arrival) is induced." ></td>
	<td class="line x" title="178:223	Manual selection of the number of topics is undesirable, but this problem can be potentially tackled with Dirichlet Process priors or a topic split criterion based on the accuracy of the sentiment predictor in the MAS model." ></td>
	<td class="line x" title="179:223	We found that both service and location did not benefit by the assignment of additional topics to their sentiment rating models." ></td>
	<td class="line x" title="180:223	The experimental results suggest that the MAS model is reliable in the discovery of topics corresponding to the rated aspects." ></td>
	<td class="line x" title="181:223	In the next section we will show that the induced topics can be used to accurately extract fragments for each aspect." ></td>
	<td class="line x" title="182:223	3.2 Sentence Labeling A primary advantage of MAS over unsupervised models, such as MG-LDA or clustering, is that topics are linked to a rated aspect, i.e., we know exactly which topics model which aspects." ></td>
	<td class="line x" title="183:223	As a result, these topics can be directly used to extract textual mentions that are relevant for an aspect." ></td>
	<td class="line x" title="184:223	To test this, we hand labeled 779 random sentences from the dataset considered in the previous set of experiments." ></td>
	<td class="line x" title="185:223	The sentences were labeled with one or more aspects." ></td>
	<td class="line x" title="186:223	Among them, 164, 176 and 263 sentences were labeled as related to aspects service, location and rooms, respectively." ></td>
	<td class="line x" title="187:223	The remaining sentences were not relevant to any of the rated aspects." ></td>
	<td class="line x" title="188:223	We compared two models." ></td>
	<td class="line x" title="189:223	The first model uses the first three topics of MAS to extract relevant mentionsbasedontheprobabilityofthattopic/aspectbeingpresentinthesentence." ></td>
	<td class="line x" title="190:223	Toobtaintheseprobabilities we used estimators based on the proportion of words in the sentence assigned to an aspects topic and normalized within local topics." ></td>
	<td class="line x" title="191:223	To improve the reliability of the estimator we produced 100 samples for each document while keeping assignments ofthetopicstoallotherwordsinthecollectionfixed." ></td>
	<td class="line x" title="192:223	The probability estimates were then obtained by averaging over these samples." ></td>
	<td class="line x" title="193:223	We did not perform any model selection on the basis of the hand-labeled data, and tested only a single model of each type." ></td>
	<td class="line x" title="194:223	314 For the second model we trained a maximum entropy classifier, one per each aspect, using 10-fold cross validation and unigram/bigram features." ></td>
	<td class="line x" title="195:223	Note that this is a supervised system and as such represents an upper-bound in performance one might expect when comparing an unsupervised model such as MAS." ></td>
	<td class="line x" title="196:223	We chose this comparison to demonstrate that our model can find relevant text mentions with high accuracy relative to a supervised model." ></td>
	<td class="line x" title="197:223	It is difficult to compare our model to other unsupervised systems such as MG-LDA or LDA." ></td>
	<td class="line x" title="198:223	Again, this is because those systems have no mechanism for directly correlating topics or clusters to corresponding aspects, highlighting the benefit of MAS." ></td>
	<td class="line x" title="199:223	The resulting precision-recall curves for the aspects service, location and rooms are presented in Figure 4." ></td>
	<td class="line x" title="200:223	In Figure 4c, we varied the number of topics associated with the aspect rooms.10 The average precision we obtained (the standard measure proportional to the area under the curve) is 75.8%, 85.5% for aspects service and location, respectively." ></td>
	<td class="line x" title="201:223	For the aspect rooms these scores are equal to 75.0%, 74.5%, 87.6%, 79.8% with 14 topics per aspect, respectively." ></td>
	<td class="line x" title="202:223	The logistic regression models achieve 80.8%, 94.0% and 88.3% for the aspects service, location and rooms." ></td>
	<td class="line x" title="203:223	We can observe that the topic model, which does not use any explicitly aspect-labeled text, achieves accuracies lower than, but comparable to a supervised model." ></td>
	<td class="line x" title="204:223	4 Related Work There is a growing body of work on summarizing sentiment by extracting and aggregating sentiment over ratable aspects and providing corresponding textual evidence." ></td>
	<td class="line x" title="205:223	Text excerpts are usually extracted through string matching (Hu and Liu, 2004a; Popescu and Etzioni, 2005), sentence clustering (Gamon et al., 2005), or through topic models (Mei et al., 2007; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="206:223	String extraction methods are limited to fine-grained aspects whereasclusteringandtopicmodelapproachesmust resort to ad-hoc means of labeling clusters or topics." ></td>
	<td class="line x" title="207:223	However, this is the first work we are aware of that uses a pre-defined set of aspects plus an associated signal to learn a mapping from text to an aspect for 10To improve readability we smoothed the curve for the aspect rooms." ></td>
	<td class="line x" title="208:223	the purpose of extraction." ></td>
	<td class="line x" title="209:223	A closely related model to ours is that of Mei et al.(2007) which performs joint topic and sentiment modeling of collections." ></td>
	<td class="line x" title="211:223	Our model differs from theirs in many respects: Mei et al. only model sentiment predictions for the entire document and not on the aspect level; They treat sentiment predictions as unobserved variables, whereas we treat them as observed signals that help to guide the creation of topics; They model co-occurrences solely on the documentlevel, whereasourmodelisbasedonMG-LDA and models both local and global contexts." ></td>
	<td class="line x" title="212:223	Recently, Blei and McAuliffe (2008) proposed an approach for joint sentiment and topic modeling that can be viewed as a supervised LDA (sLDA) model that tries to infer topics appropriate for use in a givenclassificationorregressionproblem." ></td>
	<td class="line x" title="213:223	MASand sLDA are similar in that both use sentiment predictions as an observed signal that is predicted by the model." ></td>
	<td class="line x" title="214:223	However, Blei et al. do not consider multiaspect ranking or look at co-occurrences beyond the document level, both of which are central to our model." ></td>
	<td class="line x" title="215:223	Parallel to this study Branavan et al.(2008) also showed that joint models of text and user annotations benefit extractive summarization." ></td>
	<td class="line x" title="217:223	In particular, they used signals from pros-cons lists whereas our models use aspect rating signals." ></td>
	<td class="line x" title="218:223	5 Conclusions In this paper we presented a joint model of text and aspect ratings for extracting text to be displayed in sentimentsummaries." ></td>
	<td class="line x" title="219:223	Themodelusesaspectratings todiscoverthecorrespondingtopicsandcanthusextract fragments of text discussing these aspects without the need of annotated data." ></td>
	<td class="line x" title="220:223	We demonstrated that the model indeed discovers corresponding coherent topics and achieves accuracy in sentence labeling comparable to a standard supervised model." ></td>
	<td class="line x" title="221:223	The primary area of future work is to incorporate the model into an end-to-end sentiment summarization system in order to evaluate it at that level." ></td>
	<td class="line x" title="222:223	Acknowledgments This work benefited from discussions with Sasha Blair-Goldensohn and Fernando Pereira." ></td>
	<td class="line x" title="223:223	315" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1017
Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm
Liu, Jingjing;Seneff, Stephanie;"></td>
	<td class="line x" title="1:229	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161169, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:229	c 2009 ACL and AFNLP Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm   Jingjing Liu, Stephanie Seneff MIT Computer Science & Artificial Intelligence Laboratory 32 Vassar Street, Cambridge, MA 02139 {jingl, seneff}@csail.mit.edu    Abstract  This paper presents a parse-and-paraphrase paradigm to assess the degrees of sentiment for product reviews." ></td>
	<td class="line x" title="3:229	Sentiment identification has been well studied; however, most previous work provides binary polarities only (positive and negative), and the polarity of sentiment is simply reversed when a negation is detected." ></td>
	<td class="line x" title="4:229	The extraction of lexical features such as unigram/bigram also complicates the sentiment classification task, as linguistic structure such as implicit long-distance dependency is often disregarded." ></td>
	<td class="line x" title="5:229	In this paper, we propose an approach to extracting adverb-adjective-noun phrases based on clause structure obtained by parsing sentences into a hierarchical representation." ></td>
	<td class="line x" title="6:229	We also propose a robust general solution for modeling the contribution of adverbials and negation to the score for degree of sentiment." ></td>
	<td class="line x" title="7:229	In an application involving extracting aspect-based pros and cons from restaurant reviews, we obtained a 45% relative improvement in recall through the use of parsing methods, while also improving precision." ></td>
	<td class="line x" title="8:229	1 Introduction Online product reviews have provided an extensive collection of free-style texts as well as product ratings prepared by general users, which in return provide grassroots contributions to users interested in a particular product or service as assistance." ></td>
	<td class="line x" title="9:229	Yet, valuable as they are, free-style reviews contain much noisy data and are tedious to read through in order to reach an overall conclusion." ></td>
	<td class="line x" title="10:229	Thus, we conducted this study to automatically process and evaluate product reviews in order to generate both numerical evaluation and textual summaries of users opinions, with the ultimate goal of adding value to real systems such as a restaurant-guide dialogue system." ></td>
	<td class="line oc" title="11:229	Sentiment summarization has been well studied in the past decade (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004a, 2004b; Carenini et al., 2006; Liu et al., 2007)." ></td>
	<td class="line x" title="12:229	The polarity of users sentiments in each segment of review texts is extracted, and the polarities of individual sentiments are aggregated among all the sentences/segments of texts to give a numerical scaling on sentiment orientation." ></td>
	<td class="line x" title="13:229	Most of the work done for sentiment analysis so far has employed shallow parsing features such as part-of-speech tagging." ></td>
	<td class="line x" title="14:229	Frequent adjectives and nouns/noun phrases are extracted as opinion words and representative product features." ></td>
	<td class="line x" title="15:229	However, the linguistic structure of the sentence is usually not taken into consideration." ></td>
	<td class="line x" title="16:229	High level linguistic features, if well utilized and accurately extracted, can provide much insight into the semantic meaning of user opinions and contribute to the task of sentiment identification." ></td>
	<td class="line x" title="17:229	Furthermore, in addition to adjectives and nouns, adverbials and negation also play an important role in determining the degree of the orientation level." ></td>
	<td class="line x" title="18:229	For example, very good and good certainly express different degrees of positive sentiment." ></td>
	<td class="line x" title="19:229	Also, in previous studies, when negative expressions are identified, the polarity of sentiment in the associated segment of text is simply reversed." ></td>
	<td class="line x" title="20:229	However, semantic expressions are quite different from the absolute opposite values in mathematics." ></td>
	<td class="line x" title="21:229	For example, not bad does not express the opposite meaning of bad, which would be highly positive." ></td>
	<td class="line x" title="22:229	Simply reversing the polarity of sentiment on the appearance of negations may result in inaccurate interpretation of sentiment expressions." ></td>
	<td class="line x" title="23:229	Thus, a system which attempts to quantify sentiment while ignoring adverbials is missing a significant component of the sentiment score, especially if the adverbial is a negative word." ></td>
	<td class="line x" title="24:229	161 Another challenging aspect of negation is proper scoping of the negative reference over the right constituent, which we argue, can be handled quite well with careful linguistic analysis." ></td>
	<td class="line x" title="25:229	Take the sentence I dont think the place is very clean as example." ></td>
	<td class="line x" title="26:229	A linguistic approach associating long-distance elements with semantic relations can identify that the negation not scopes over the complement clause, thus extracting not very clean instead of very clean." ></td>
	<td class="line x" title="27:229	Our goal in modeling adverbials is to investigate whether a simple linear correction model can capture the polarity contribution of all adverbials." ></td>
	<td class="line x" title="28:229	Furthermore, is it also appropriate to adjust for multiple adverbs, including negation, via a linear additive model?" ></td>
	<td class="line x" title="29:229	I.e., can not very good be modeled as not(very(good))?" ></td>
	<td class="line x" title="30:229	The fact that not very good seems to be less negative than not good suggests that such an algorithm might work well." ></td>
	<td class="line x" title="31:229	From these derivations we have developed a model which treats negations in the exact same way as modifying adverbs, via an accumulative linear offset model." ></td>
	<td class="line x" title="32:229	This yields a very generic and straightforward solution to modeling the strength of sentiment expression." ></td>
	<td class="line x" title="33:229	In this paper we utilize a parse-and-paraphrase paradigm to identify semantically related phrases in review texts, taking quantifiers (e.g., modifying adverbs) and qualifiers (e.g., negations) into special consideration." ></td>
	<td class="line x" title="34:229	The approach makes use of a lexicalized probabilistic syntactic grammar to identify and extract sets of adverb-adjectivenoun phrases that match review-related patterns." ></td>
	<td class="line x" title="35:229	Such patterns are constructed based on wellformed linguistic structure; thus, relevant phrases can be extracted reliably." ></td>
	<td class="line x" title="36:229	We also propose a cumulative linear offset model to calculate the degree of sentiment for joint adjectives and quantifiers/qualifiers." ></td>
	<td class="line x" title="37:229	The proposed sentiment prediction model takes modifying adverbs and negations as universal scales on strength of sentiment, and conducts cumulative calculation on the degree of sentiment for the associated adjective." ></td>
	<td class="line x" title="38:229	With this model, we can provide not only qualitative textual summarization such as good food and bad service, but also a numerical scoring of sentiment, i.e., how good the food is and how bad the service is. 2 Related Work There have been many studies on sentiment classification and opinion summarization (Pang and Lee, 2004, 2005; Gamon et al., 2005; Popescu and Etzioni, 2005; Liu et al., 2005; Zhuang et al., 2006; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="39:229	Specifically, aspect rating as an interesting topic has also been widely studied (Titov and McDonald, 2008a; Snyder and Barzilay, 2007; Goldberg and Zhu, 2006)." ></td>
	<td class="line x" title="40:229	Recently, Baccianella et." ></td>
	<td class="line x" title="41:229	al." ></td>
	<td class="line x" title="42:229	(2009) conducted a study on multi-facet rating of product reviews with special emphasis on how to generate vectorial representations of the text by means of POS tagging, sentiment analysis, and feature selection for ordinal regression learning." ></td>
	<td class="line x" title="43:229	Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors." ></td>
	<td class="line x" title="44:229	Branavan et al.(2008) proposed a method for leveraging unstructured annotations in product reviews to infer semantic document properties, by clustering user annotations into semantic properties and tying the induced clusters to hidden topics in the text." ></td>
	<td class="line x" title="46:229	3 System Overview Our review summarization task is to extract sets of descriptor-topic pairs (e.g., excellent service) from a set of reviews (e.g., for a particular restaurant), and to cluster the extracted phrases into representative aspects on a set of dimensions (e.g., food, service and atmosphere)." ></td>
	<td class="line x" title="47:229	Driven by this motivation, we propose a three-stage system that automatically processes reviews." ></td>
	<td class="line x" title="48:229	A block diagram is given in Figure 1." ></td>
	<td class="line x" title="49:229	Figure 1." ></td>
	<td class="line x" title="50:229	Framework of review processing." ></td>
	<td class="line x" title="51:229	The first stage is sentence-level data filtering." ></td>
	<td class="line x" title="52:229	Review data published by general users is often in free-style, and a large fraction of the data is either ill-formed or not relevant to the task." ></td>
	<td class="line x" title="53:229	We classify these as out of domain sentences." ></td>
	<td class="line x" title="54:229	To filter out such noisy data, we collect unigram statistics on all the relevant words in the corpus, and select high frequency adjectives and nouns." ></td>
	<td class="line x" title="55:229	Any sentence that contains none of the highfrequency nouns or adjectives is rejected from further analysis." ></td>
	<td class="line x" title="56:229	The remaining in-domain sentences are subjected to the second stage, parse 162 analysis and semantic understanding, for topic extraction." ></td>
	<td class="line x" title="57:229	From the parsable sentences we extract descriptor-topic phrase patterns based on a carefully-designed generation grammar." ></td>
	<td class="line x" title="58:229	We then apply LM (language model) based topic clustering to group the extracted phrases into representative aspects." ></td>
	<td class="line x" title="59:229	The third stage scores the degree of sentiment for adjectives, as well as the strength of sentiment for modifying adverbs and negations, which further refine the degree of sentiment of the associated adjectives." ></td>
	<td class="line x" title="60:229	We then run a linear additive model to assign a combined sentiment score for each extracted phrase." ></td>
	<td class="line x" title="61:229	The rest of the paper is structured as follows: In Section 4, we explain the linguistic analysis." ></td>
	<td class="line x" title="62:229	In Section 5, we describe the cumulative model for assessing the degree of sentiment." ></td>
	<td class="line x" title="63:229	Section 6 provides a systematic evaluation, conducted on real data in the restaurant review domain harvested from the Web." ></td>
	<td class="line x" title="64:229	Section 7 provides a discussion analyzing the results." ></td>
	<td class="line x" title="65:229	Section 8 summarizes the paper as well as pointing to future work." ></td>
	<td class="line x" title="66:229	4 Linguistic Analysis 4.1 Parse-and-Paraphrase Our linguistic analysis is based on a parse-andparaphrase paradigm." ></td>
	<td class="line x" title="67:229	Instead of the flat structure of a surface string, the parser provides a hierarchical representation, which we call a linguistic frame (Xu et al., 2008)." ></td>
	<td class="line x" title="68:229	It preserves linguistic structure by encoding different layers of semantic dependencies." ></td>
	<td class="line x" title="69:229	The grammar captures syntactic structure through a set of carefully constructed context free grammar rules, and employs a feature-passing mechanism to enforce long distance constraints." ></td>
	<td class="line x" title="70:229	The grammar is lexicalized, and uses a statistical model to rank order competing hypotheses." ></td>
	<td class="line x" title="71:229	It knows explicitly about 9,000 words, with all unknown words being interpreted as nouns." ></td>
	<td class="line x" title="72:229	The grammar probability model was trained automatically on the corpus of review sentences." ></td>
	<td class="line x" title="73:229	To produce the phrases, a set of generation rules is carefully constructed to only extract sets of related adverbs, adjectives and nouns." ></td>
	<td class="line x" title="74:229	The adjective-noun relationships are captured from the following linguistic patterns: (1) all adjectives attached directly to a noun in a noun phrase, (2) adjectives embedded in a relative clause modifying a noun, and (3) adjectives related to nouns in a subject-predicate relationship in a clause." ></td>
	<td class="line x" title="75:229	These patterns are compatible, i.e., if a clause contains both a modifying adjective and a predicate adjective related to the same noun, two adjective-noun pairs are generated by different patterns." ></td>
	<td class="line x" title="76:229	As in, The efficient waitress was nonetheless very courteous. It is a parse-andparaphrase-like paradigm: the paraphrase tries to preserve the original words intact, while reordering them and/or duplicating them into multiple NP units." ></td>
	<td class="line x" title="77:229	Since they are based on syntactic structure, the generation rules can also be applied in any other domain involving opinion mining." ></td>
	<td class="line x" title="78:229	An example linguistic frame is shown in Figure 2, which encodes the sentence The caesar with salmon or chicken is really quite good. In this example, for the adjective good, the nearby noun chicken would be associated with it if only proximity is considered." ></td>
	<td class="line x" title="79:229	From the linguistic frame, however, we can easily associate caesar with good by extracting the head of the topic sub-frame and the head of the predicate subframe, which are encoded in the same layer (root layer) of the linguistic frame." ></td>
	<td class="line x" title="80:229	Also, we can tell from the predicate sub-frame that there is an adverb quite modifying the head word good." ></td>
	<td class="line x" title="81:229	The linguistic frame also encodes an adverb really in the upstairs layer." ></td>
	<td class="line x" title="82:229	A well-constructed generation grammar can create customized adverb-adjective-noun phrases such as quite good caesar or really quite good caesar." ></td>
	<td class="line x" title="83:229	{c cstatement   :topic {q caesar              :quantifier 'def'              :pred {p with :topic {q salmon                                          :pred {p conjunction                                            :or {q chicken  }}}}   :adv 'really'    :pred {p adj_complement             :pred {p adjective                     :adv 'quite'                 :pred {p quality :topic 'good' }}}} Figure 2." ></td>
	<td class="line x" title="84:229	Linguistic frame for The caesar with salmon or chicken is really quite good. Interpreting negation in English is not straightforward, and it is often impossible to do correctly without a deep linguistic analysis." ></td>
	<td class="line x" title="85:229	Xuehui Wu (2005) wrote: The scope of negation is a complex linguistic phenomenon." ></td>
	<td class="line x" title="86:229	It is easy to perceive but hard to be defined from a syntactic point of view." ></td>
	<td class="line x" title="87:229	Misunderstanding or ambiguity may occur when the negative scope is not understood clearly and correctly. The majority rule for negation is that it scopes over the remainder of its containing clause, and this works well for most cases." ></td>
	<td class="line x" title="88:229	For example, Figure 3 shows 163 the linguistic frame for the sentence Their menu was a good one that didnt try to do too much. {c cstatement :topic {q menu   :poss 'their' } }    :complement {q pronoun   :name one              :adj_clause {c cstatement                            :conjn 'that'                            :negate 'not'                            :pred {p try :to_clause  {p do                                            :topic {q object                                            :adv 'too'                                            :quant 'much' }}}}    :pred {p adjective                 :pred {p quality :topic 'good' }}} Figure 3." ></td>
	<td class="line x" title="89:229	Linguistic frame for Their menu was a good one that didnt try to do too much. Traditional approaches which do not consider the linguistic structure would treat the appearance of not as a negation and simply reverse the sentiment of the sentence to negative polarity, which is wrong as the sentence actually expresses positive opinion for the topic menu." ></td>
	<td class="line x" title="90:229	In our approach, the negation not is identified as under the sub-frame of the complement clause, instead of in the same or higher layer of the adjective sub-frame; thus it is considered as unrelated to the adjective good." ></td>
	<td class="line x" title="91:229	In this way we can successfully predict the scope of the reference of the negation over the correct constituent of a sentence and create proper association between negation and its modified words." ></td>
	<td class="line x" title="92:229	4.2 LM-based Topic Clustering To categorize the extracted phrases into representative aspects, we automatically group the identified topics into a set of clusters based on LM probabilities." ></td>
	<td class="line x" title="93:229	The LM-based algorithm assumes that topics which are semantically related have high probability of co-occurring with similar descriptive words." ></td>
	<td class="line x" title="94:229	For example, delicious might co-occur frequently with both pizza and dessert." ></td>
	<td class="line x" title="95:229	By examining the distribution of bigram probability of these topics with corresponding descriptive words, we can group pizza and dessert into the same cluster of food." ></td>
	<td class="line x" title="96:229	We select a small set of the most common topics, i.e., topics with the highest frequency counts, and put them into an initial set I. Then, for each candidate topic g1872g1855 outside set I, we calculate its probability given each topic g1872g1861 within the initial set I, given by:        g1842g4666g1872g3030| g1872g3036g4667g3404 g1842g4666g1872g3030|g1853g4667g1842g4666g1853|g1872g3036g4667g3028g1488g3002                       g3404  g3017g4666g3028,g3047g3278g4667g3017g4666g3028g4667 g3017g4666g3028,g3047g3284g4667g3017g4666g3047 g3284g4667g3028g1488g3002                     g3404 g2869g3017g4666g3047 g3284g4667  g2869g3017g4666g3028g4667g1842g4666g1853,g1872g3030g4667g1842g4666g1853,g1872g3036g4667g3028g1488g3002        (1) where A represents the set of all the adjectives in the corpus." ></td>
	<td class="line x" title="97:229	For each candidate topic g1872g1855 , we choose the cluster of the initial topic g1872g1861  with which it has the highest probability score." ></td>
	<td class="line x" title="98:229	There are also cases where a meaningful adjective occurs in the absence of an associated topic, e.g., It is quite expensive. We call such cases the widow-adjective case." ></td>
	<td class="line x" title="99:229	Without hardcoded ontology matching, it is difficult to identify expensive as a price-related expression." ></td>
	<td class="line x" title="100:229	To discover such cases and associate them with related topics, we propose a surrogate topic matching approach based on bigram probability." ></td>
	<td class="line x" title="101:229	As aforementioned, the linguistic frame organizes all adjectives into separate clauses." ></td>
	<td class="line x" title="102:229	Thus, we create a surrogate topic category in the linguistic frames for widow-adjective cases, which makes it easy to detect descriptors that are affiliated with uninformative topics like the pronoun it." ></td>
	<td class="line x" title="103:229	We then have it generate phrases such as expensive surrogate_topic and use bigram probability statistics to automatically map each sufficiently strongly associated adjective to its most common topic among our major classes, e.g., mapping expensive with its surrogate topic price." ></td>
	<td class="line x" title="104:229	Therefore, we can generate sets of additional phrases in which the topic is hallucinated from the widow-adjective." ></td>
	<td class="line x" title="105:229	5 Assessment of Sentiment Strength 5.1 Problem Formulation Given the sets of adverb-adjective-noun phrases extracted by linguistic analysis, our goal is to assign a score for the degree of sentiment to each phrase and calculate an average rating for each aspect." ></td>
	<td class="line x" title="106:229	An example summary is given in Table 1." ></td>
	<td class="line x" title="107:229	Table 1." ></td>
	<td class="line x" title="108:229	Example of review summary." ></td>
	<td class="line x" title="109:229	Aspect Extracted phrases Rating Atmosphere very nice ambiance, outdoor patio 4.8 Food not bad meal, quite authentic food 4.1 Place not great place, very smoky restaurant 2.8 Price so high bill, high cost, not cheap price 2.2 To calculate the numerical degree of sentiment, there are three major problems to solve: 1) how to associate numerical scores with textual sentiment; 2) whether to calculate sentiment scores for adjectives and adverbs jointly or separately; 3) 164 whether to treat negations as special cases or in the same way as modifying adverbs." ></td>
	<td class="line x" title="110:229	There have been studies on building sentiment lexicons to define the strength of sentiment of words." ></td>
	<td class="line x" title="111:229	Esuli and Sebastiani (2006) constructed a lexical resource, SentiWordNet, a WordNet-like lexicon emphasizing sentiment orientation of words and providing numerical scores of how objective, positive and negative these words are." ></td>
	<td class="line x" title="112:229	However, lexicon-based methods can be tedious and inefficient and may not be accurate due to the complex cross-relations in dictionaries like WordNet." ></td>
	<td class="line x" title="113:229	Instead, our primary approach to sentiment scoring is to make use of collective data such as user ratings." ></td>
	<td class="line x" title="114:229	In product reviews collected from online forums, the format of a review entry often consists of three parts: pros/cons, free-style text and user rating." ></td>
	<td class="line x" title="115:229	We assume that user rating is normally consistent with the tone of the review text published by the same user." ></td>
	<td class="line x" title="116:229	By associating user ratings with each phrase extracted from review texts, we can easily associate numerical scores with textual sentiment." ></td>
	<td class="line x" title="117:229	A simple strategy of rating assignment is to take each extracted adverb-adjective pair as a composite unit." ></td>
	<td class="line x" title="118:229	However, this method is likely to lead to a large number of rare combinations, thus suffering from sparse data problems." ></td>
	<td class="line x" title="119:229	Therefore, an interesting question to ask is whether it is feasible to assign to each adverb a perturbation score, which adjusts the score of the associated adjective up or down by a fixed scalar value." ></td>
	<td class="line x" title="120:229	This approach thus hypothesizes that very expensive is as much worse than expensive as very romantic is better than romantic." ></td>
	<td class="line x" title="121:229	This allows us to pool all instances of a given adverb regardless of which adjective it is associated with, in order to compute the absolute value of the perturbation score for that adverb." ></td>
	<td class="line x" title="122:229	Therefore, we consider adverbs and adjectives separately when calculating the sentiment score, treating each modifying adverb as a universal quantifier which consistently scales up/down the strength of sentiment for the adjectives it modifies." ></td>
	<td class="line x" title="123:229	Furthermore, instead of treating negation as a special case, the universal model works for all adverbials." ></td>
	<td class="line x" title="124:229	The model hypothesizes that not bad is as much better than bad as not good is worse than good, i.e., negations push positive/negative adjectives to the other side of sentiment polarity by a universal scale." ></td>
	<td class="line x" title="125:229	This again, allows us to pool all instances of a given negation and compute the absolute value of the perturbation score for that negation, in the same way as dealing with modifying adverbs." ></td>
	<td class="line x" title="126:229	5.2 Linear Additive Model For each adjective, we average all its ratings given by: g1845g1855g1867g1870g1857g4666g1853g1856g1862g4667g3404   g3263g3289g3293 g3284   g3045g3284g3284g1488g3265   g3263g3289g3293 g3284 g3293g3284             (2) where g1842 represents the set of appearances of adjective g1853g1856g1862, g1870g3036 represents the associated user rating in each appearance of g1853g1856g1862, g1840 represents the number of entities (e.g., restaurants) in the entire data set, and g1866g3045g3284 represents the number of entities with rating g1870g3036." ></td>
	<td class="line x" title="127:229	The score is averaged over all the appearances, weighted by the frequency count of each category of rating to remove bias towards any category." ></td>
	<td class="line x" title="128:229	As for adverbs, using a slightly modified version of equation (2), we can get a rating table for all adverb-adjective pairs." ></td>
	<td class="line x" title="129:229	For each adverb adv, we get a list of all its possible combinations with adjectives." ></td>
	<td class="line x" title="130:229	Then, for each adj in the list, we calculate the distance between the rating of adv-adj and the rating of the adj alone." ></td>
	<td class="line x" title="131:229	We then aggregate the distances among all the pairs of adv-adj and adj in the list, weighted by the frequency count of each adv-adj pair: g1845g1855g1867g1870g1857g4666g1853g1856g1874g4667g3404  g3030g3042g3048g3041g3047g4666g3028g3031g3049,g3028g3031g3037g3295g4667 g3030g3042g3048g3041g3047g3435g3028g3031g3049,g3028g3031g3037 g3285g3439g3285g1488g3250g3047g1488g3002  g1842g1867g1864g4666g1853g1856g1862g3047g4667g4666g1870g4666g1853g1856g1874,g1853g1856g1862g3047g4667g3398g1870g4666g1853g1856g1862g3047g4667g4667          (3) where g1855g1867g1873g1866g1872g4666g1853g1856g1874, g1853g1856g1862g3047g4667 represents the count of the combination g1853g1856g1874g3398g1853g1856g1862g3047, g1827 represents the set of adjectives that co-occur with g1853g1856g1874 , g1870g4666g1853g1856g1874,g1853g1856g1862g3047g4667 represents the sentiment rating of the combination g1853g1856g1874g3398g1853g1856g1862g3047 , and g1870g4666g1853g1856g1862g3047g4667 represents the sentiment rating of the adjective g1853g1856g1862g3047 alone." ></td>
	<td class="line x" title="132:229	g1842g1867g1864g4666g1853g1856g1862g3047g4667 represents the polarity of g1853g1856g1862g3047, assigned as 1 if g1853g1856g1862g3047 is positive, and -1 if negative." ></td>
	<td class="line x" title="133:229	Specifically, negations are well handled by the same scoring strategy, treated exactly the same as modifying adverbs, except that they get such strong negative scores that the sentiment of the associated adjectives is pushed to the other side of the polarity scale." ></td>
	<td class="line x" title="134:229	After obtaining the strength rating for adverbs and the sentiment rating for adjectives, the next step is to assign the strength of sentiment to each phrase (negation-adverb-adjective-noun) extracted by linguistic analysis, as given by: g1845g1855g1867g1870g1857g4672g1866g1857g1859g3435g1853g1856g1874g4666g1853g1856g1862g4667g3439g4673g3404g1870g4666g1853g1856g1862g4667g3397 g1842g1867g1864g4666g1853g1856g1862g4667g1870g4666g1853g1856g1874g4667g3397g1842g1867g1864g4666g1853g1856g1862g4667g1870g4666g1866g1857g1859g4667       (4) 165 where g1870g4666g1853g1856g1862g4667 represents the rating of adjective g1853g1856g1862, g1870g4666g1853g1856g1874g4667 represents the rating of adverb g1853g1856g1874, and g1870g4666g1866g1857g1859g4667 represents the rating of negation g1866g1857g1859." ></td>
	<td class="line x" title="135:229	g1842g1867g1864g4666g1853g1856g1862g4667 represents the polarity of g1853g1856g1862, assigned as 1 if g1853g1856g1862 is positive, and -1 if negative." ></td>
	<td class="line x" title="136:229	Thus, if g1853g1856g1862 is positive, we assign a combined rating g1870g4666g1853g1856g1862g4667g3397g1870g4666g1853g1856g1874g4667 to this phrase." ></td>
	<td class="line x" title="137:229	If it is negative, we assign g1870g4666g1853g1856g1862g4667g3398g1870g4666g1853g1856g1874g4667." ></td>
	<td class="line x" title="138:229	Specifically, if it is a negation case, we further assign a linear offset g1870g4666g1866g1857g1859g4667 if g1853g1856g1862 is positive or g3398g1870g4666g1866g1857g1859g4667 if g1853g1856g1862 is negative." ></td>
	<td class="line x" title="139:229	For example, given the ratings <good: 4.5>, <bad: 1.5>, <very: 0.5> and <not: -3.0>, we would assign 5.0 to very good (score(very(good))=4.5+0.5), 1.0 to very bad (score(very(bad))=1.5-0.5), and 2.0 to not very good (score(not(very(good)))= 4.5+0.53.0)." ></td>
	<td class="line x" title="140:229	The corresponding sequence of different degrees of sentiment is: very good: 5.0 > good: 4.5 > not very good: 2.0 > bad: 1.5 > very bad: 1.0." ></td>
	<td class="line x" title="141:229	6 Experiments In this section we present a systematic evaluation of the proposed approaches conducted on real data." ></td>
	<td class="line x" title="142:229	We crawled a data collection of 137,569 reviews on 24,043 restaurants in 9 cities in the U.S. from an online restaurant evaluation website1." ></td>
	<td class="line x" title="143:229	Most of the reviews have both pros/cons and free-style text." ></td>
	<td class="line x" title="144:229	For the purpose of evaluation, we take those reviews containing pros/cons as the experimental set, which is 72.7% (99,147 reviews) of the original set." ></td>
	<td class="line x" title="145:229	6.1 Topic Extraction Based on the experimental set, we first filtered out-of-domain sentences based on frequency count, leaving a set of 857,466 in-domain sentences (67.5%)." ></td>
	<td class="line x" title="146:229	This set was then subjected to parse analysis; 78.6% of them are parsable." ></td>
	<td class="line x" title="147:229	Given the parsing results in the format of linguistic frame, we used a set of language generation rules to extract relevant adverb-adjectivenoun phrases." ></td>
	<td class="line x" title="148:229	We then selected the most frequent 6 topics that represented appropriate dimensions for the restaurant domain (place, food, service, price, atmosphere and portion) as the initial set, and clustered the extracted topic mentions into different aspect categories by creating a set of topic mappings with the LMbased clustering method." ></td>
	<td class="line x" title="149:229	Phrases not belonging to any category are filtered out." ></td>
	<td class="line x" title="150:229	1  http://www.citysearch.com To evaluate the performance of the proposed approach (LING) to topic extraction, we compare it with a baseline method similar to (Hu and Liu, 2004a, 2004b; Liu et al., 2005)." ></td>
	<td class="line x" title="151:229	We performed part-of-speech tagging on both parsable and unparsable sentences, extracted each pair of noun and adjective that has the smallest proximity, and filtered out those with low frequency counts." ></td>
	<td class="line x" title="152:229	Adverbs and negation words that are adjacent to the identified adjectives were also extracted along with the adjective-noun pairs." ></td>
	<td class="line x" title="153:229	We call this the neighbor baseline (NB)." ></td>
	<td class="line x" title="154:229	The proposed method is unable to make use of the non-parsable sentences, which make up over 20% of the data." ></td>
	<td class="line x" title="155:229	Hence, it seems plausible to utilize a back-off mechanism for these sentences via a combined system (COMB) incorporating NB only for the sentences that fail to parse." ></td>
	<td class="line x" title="156:229	In considering how to construct the ground truth set of pros and cons for particular aspects, our goal was to minimize error as much as possible without requiring exorbitant amounts of manual labeling." ></td>
	<td class="line x" title="157:229	We also wanted to assure that the methods were equally fair to both systems (LING and NB)." ></td>
	<td class="line x" title="158:229	To these ends, we decided to pool together all of the topic mappings and surrogate topic hallucinations obtained automatically from both systems, and then to manually edit the resulting list to eliminate any that were deemed unreasonable." ></td>
	<td class="line x" title="159:229	We then applied these edited mappings in an automatic procedure to the adjective-noun pairs in the user-provided pros and cons of all the restaurant reviews." ></td>
	<td class="line x" title="160:229	The resulting aspect-categorized phrase lists are taken as the ground truth." ></td>
	<td class="line x" title="161:229	Each system then used its own (unedited) set of mappings in processing the associated review texts." ></td>
	<td class="line x" title="162:229	We also needed an algorithm to decide on a particular set of reviews for consideration, again, with the goal of omitting bias towards either of the two systems." ></td>
	<td class="line x" title="163:229	We decided to retain as the evaluation set all reviews which obtained at least one topic extraction from both systems." ></td>
	<td class="line x" title="164:229	Thus the two systems processed exactly the same data with exactly the same definitions of ground truth." ></td>
	<td class="line x" title="165:229	Performance was evaluated on this set of 62,588 reviews in terms of recall (percentage of topics in the ground truth that are also identified from the review body) and precision (percentage of extracted topics that are also in the ground truth)." ></td>
	<td class="line x" title="166:229	These measures are computed separately for each review, and then averaged over all reviews." ></td>
	<td class="line x" title="167:229	As shown in Table 2, without clustering, the LING approach gets 4.6% higher recall than the 166 NB baseline." ></td>
	<td class="line x" title="168:229	And the recall from the COMB approach is 3.9% higher than that from the LING approach and 8.5% higher than that from the NB baseline." ></td>
	<td class="line x" title="169:229	With topic clustering, the COMB approach also gets the highest recall, with a 4.9% and 17.5% increase from the LING approach and the NB baseline respectively." ></td>
	<td class="line x" title="170:229	The precision is quite close among the different approaches, around 60%." ></td>
	<td class="line x" title="171:229	Table 2 also shows that the topic clustering approach increases the recall by 4.8% for the NB baseline, 12.8% for the LING approach, and 13.8% for the COMB approach." ></td>
	<td class="line x" title="172:229	Table 2." ></td>
	<td class="line x" title="173:229	Experimental results of topic extraction by the NB baseline, the proposed LING approach and a combined system (COMB)." ></td>
	<td class="line x" title="174:229	No Clustering NB LING COMB Recall 39.6% 44.2% 48.1% Precision 60.2% 60.0% 59.8%  With Clustering NB LING COMB Recall 44.4% 57.0% 61.9% Precision 56.8% 61.1% 60.8% 6.2 Sentiment Scoring To score the degree of sentiment for each extracted phrase, we built a table of sentiment score (<adjective: score>) for adjectives and a table of strength score (<adverb: score>) for adverbs." ></td>
	<td class="line x" title="175:229	The pros/cons often contain short and wellstructured phrases, and have better parsing quality than the long and complex sentences in freestyle texts; pros/cons also have clear sentiment orientations." ></td>
	<td class="line x" title="176:229	Thus, we use pros/cons to score the sentiment of adjectives, which requires strong polarity association." ></td>
	<td class="line x" title="177:229	To obtain reliable ratings, we associate the adjectives in the pros of review entries which have a user rating 4 or 5, and associate the adjectives in the cons of review entries with user ratings 1 or 2 (the scale of user rating is 1 to 5)." ></td>
	<td class="line x" title="178:229	Reviews with rating 3 are on the boundary of sentiment, so we associate both sides with the overall rating." ></td>
	<td class="line x" title="179:229	On the other hand, the frequencies of adverbs in free-style texts are much higher than those in pros/cons, as pros/cons mostly contain adjective-noun patterns." ></td>
	<td class="line x" title="180:229	Thus, we use free-style texts instead of pros/cons to score the strength of adverbs." ></td>
	<td class="line x" title="181:229	Partial results of the sentiment scoring are shown in Tables 3 and 4." ></td>
	<td class="line x" title="182:229	As shown in Table 3, the polarity of sentiment as well as the degree of polarity of an adjective can be distinguished by its score." ></td>
	<td class="line x" title="183:229	The higher the sentiment score is, the more positive the adjective is. Table 3." ></td>
	<td class="line x" title="184:229	Sentiment scoring for selected adjectives." ></td>
	<td class="line x" title="185:229	Adjective Rating Adjective Rating Excellent  5.0 Awesome  4.8 Easy  4.1 Great  4.4 Good  3.9 Limited  3.4 Inattentive  2.75 Overpriced  2.3 Rude  1.69 Horrible  1.3 Table 4 gives the scores of strength for most common adverbs." ></td>
	<td class="line x" title="186:229	The higher the strength score is, the more the adverb scales up/down the degree of sentiment of the adjective it modifies." ></td>
	<td class="line x" title="187:229	While not gets a strong negative score, some adverbs such as a little (-0.65) and a bit (0.83) also get negative scores, indicating slightly less sentiment for the associated adjectives." ></td>
	<td class="line x" title="188:229	Table 4." ></td>
	<td class="line x" title="189:229	Strength scoring for selected adverbs." ></td>
	<td class="line x" title="190:229	Adverb Rating Adverb Rating Super  0.58 Fairly  0.13 Extremely  0.54 Pretty 0.07 Incredibly  0.49 A little  -0.65 Very 0.44 A bit -0.83 Really  0.39 Not -3.10 To evaluate the performance of sentiment scoring, we randomly selected a subset of 1,000 adjective-noun phrases and asked two annotators to independently rate the sentiment of each phrase on a scale of 1 to 5." ></td>
	<td class="line x" title="191:229	We compared the sentiment scoring between our system and the annotations in a measurement of mean distance: g1856g1861g1871g1872g1853g1866g1855g1857 g3404 g2869|g3020| |g1870g3036g3043g3043g3106g3020 g3398g1870g3028g3043|      (5) where g1845  represents the set of phrases, g1868 represents each phrase in the set g1845, g1870g3036g3043 represents the rating on phrase g1868 from our sentiment scoring system, and g1870g3028g3043 represents the annotated rating on phrase g1868." ></td>
	<td class="line x" title="192:229	As shown in Table 5, the obtained mean distance between the scoring from our approach and that from each annotation set is 0.46 and 0.43 respectively, based on the absolute rating scale from 1 to 5." ></td>
	<td class="line x" title="193:229	This shows that the scoring of sentiment from our system is quite close to human annotation." ></td>
	<td class="line x" title="194:229	The kappa agreement between the two annotation sets is 0.68, indicating high consistency between the annotators." ></td>
	<td class="line x" title="195:229	The reliability of these results gives us sufficient confidence to make use of the scores of sentiments for summarization." ></td>
	<td class="line x" title="196:229	To examine the prediction of sentiment polarity, for each annotation set, we pooled the phrases with rating 4/5 into positive, rating 1/2 into negative, and rating 3 into neutral." ></td>
	<td class="line x" title="197:229	Then we rounded up the sentiment scores from our system to integers and pooled the scores into three polar167 ity sets (positive, negative and neutral) in the same way." ></td>
	<td class="line x" title="198:229	As shown in Table 5, the obtained kappa agreement between the result from our system and that from each annotation set is 0.55 and 0.60 respectively." ></td>
	<td class="line x" title="199:229	This shows reasonably high agreement on the polarity of sentiment between our system and human evaluation." ></td>
	<td class="line x" title="200:229	Table 5." ></td>
	<td class="line x" title="201:229	Comparison of sentiment scoring between the proposed approach and two annotation sets." ></td>
	<td class="line x" title="202:229	Annotation 1 Annotation 2 Mean distance 0.46 0.43 Kappa agreement 0.55 0.60 Table 6." ></td>
	<td class="line x" title="203:229	Experimental results of topic extraction based on sentiment polarity matching." ></td>
	<td class="line x" title="204:229	No Clustering NB LING COMB Recall 34.5% 38.9% 42.2% Precision 53.8% 54.0% 53.3%  With Clustering NB LING COMB Recall 37.4% 49.7% 54.1% Precision 48.5% 52.9% 51.4% To evaluate the combination of topic extraction and sentiment identification, we repeated the topic extraction experiments presented in Table 2, but this time requiring as well a correct polarity assignment to obtain a match with the pros/cons ground truth." ></td>
	<td class="line x" title="205:229	As shown in Table 6, the COMB approach gets the highest recall both with and without topic clustering, and the recall from the LING approach is higher than that from the NB baseline in both cases as well, indicating the superiority of the proposed approach." ></td>
	<td class="line x" title="206:229	The precision is stable among the different approaches, consistent with the case without the consideration of sentiment polarity." ></td>
	<td class="line x" title="207:229	7 Discussion It is surprising that the parse-and-paraphrase method performs so well, despite the fact that it utilizes less than 80% of the data (parsable set)." ></td>
	<td class="line x" title="208:229	In this section, we will discuss two experiments that were done to tease apart the contributions of different variables." ></td>
	<td class="line x" title="209:229	In both experiments, we compared the change in relative improvement in recall between NB and LING, relative to the values in Table 6, in the with-clustering condition." ></td>
	<td class="line x" title="210:229	In the table, LING obtains a score of 49.7% for recall, which is a 33% relative increase from the score for NB (37.4%)." ></td>
	<td class="line x" title="211:229	Three distinct factors could play a role in the improvement: the widowadjective topic hallucinations, the topic mapping for clustering, and the extracted phrases themselves." ></td>
	<td class="line x" title="212:229	An experiment involving omitting topic hallucinations from widow adjectives determined that these account for 12% of the relative increase." ></td>
	<td class="line x" title="213:229	To evaluate the contribution of clustering, we replaced the mapping tables used by both systems with the edited one used by the ground truth computation." ></td>
	<td class="line x" title="214:229	Thus, both systems made use of the same mapping table, removing this variable from consideration." ></td>
	<td class="line x" title="215:229	This improved the performance of both systems (NB and LING), but resulted in a decrease of LINGs relative improvement by 17%." ></td>
	<td class="line x" title="216:229	This implies that LINGs mapping table is superior." ></td>
	<td class="line x" title="217:229	Since both systems use the same sentiment scores for adjectives and adverbs, the remainder of the difference (71%) must be due simply to higher quality extracted phrases." ></td>
	<td class="line x" title="218:229	We suspected that over-generated phrases (the 40% of phrases that find no mappings in the pros/cons) might not really be a problem." ></td>
	<td class="line x" title="219:229	To test this hypothesis, we selected 100 reviews for their high density of extracted phrases, and manually evaluated all the over-generated phrases." ></td>
	<td class="line x" title="220:229	We found that over 80% were well formed, correct, and informative." ></td>
	<td class="line x" title="221:229	Therefore, a lower precision here does not necessarily mean poor performance, but instead shows that the pros/cons provided by users are often incomplete." ></td>
	<td class="line x" title="222:229	By extracting summaries from review texts we can recover additional valuable information." ></td>
	<td class="line x" title="223:229	8 Conclusions & Future Work This paper presents a parse-and-paraphrase approach to assessing the degree of sentiment for product reviews." ></td>
	<td class="line x" title="224:229	A general purpose context free grammar is employed to parse review sentences, and semantic understanding methods are developed to extract representative negation-adverbadjective-noun phrases based on well-defined semantic rules." ></td>
	<td class="line x" title="225:229	A language modeling-based method is proposed to cluster topics into respective categories." ></td>
	<td class="line x" title="226:229	We also introduced in this paper a cumulative linear offset model for supporting the assessment of the strength of sentiment in adjectives and quantifiers/qualifiers (including negations) on a numerical scale." ></td>
	<td class="line x" title="227:229	We demonstrated that the parse-and-paraphrase method can perform substantially better than a neighbor baseline on topic extraction from reviews even with less data." ></td>
	<td class="line x" title="228:229	The future work focuses in two directions: (1) building a relational database from the summaries and ratings and using it to enhance users experiences in a multimodal spoken dialogue system; and (2) applying our techniques to other domains to demonstrate generality." ></td>
	<td class="line x" title="229:229	168" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1019
Sentiment Analysis of Conditional Sentences
Narayanan, Ramanathan;Liu, Bing;Choudhary, Alok;"></td>
	<td class="line x" title="1:344	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 180189, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:344	c 2009 ACL and AFNLP Sentiment Analysis of Conditional Sentences   Ramanathan Narayanan Dept. of EECS Northwestern University ramanathan.an@gmail.com Bing Liu *  Dept. of Computer Science Univ. of Illinois at Chicago liub@cs.uic.edu Alok Choudhary Dept. of EECS Northwestern University alokchoudhary01@gmail.com     Abstract This paper studies sentiment analysis of conditional sentences." ></td>
	<td class="line x" title="3:344	The aim is to determine whether opinions expressed on different topics in a conditional sentence are positive, negative or neutral." ></td>
	<td class="line x" title="4:344	Conditional sentences are one of the commonly used language constructs in text." ></td>
	<td class="line x" title="5:344	In a typical document, there are around 8% of such sentences." ></td>
	<td class="line x" title="6:344	Due to the condition clause, sentiments expressed in a conditional sentence can be hard to determine." ></td>
	<td class="line x" title="7:344	For example, in the sentence, if your Nokia phone is not good, buy this great Samsung phone, the author is positive about Samsung phone but does not express an opinion on Nokia phone (although the owner of the Nokia phone may be negative about it)." ></td>
	<td class="line x" title="8:344	However, if the sentence does not have if, the first clause is clearly negative." ></td>
	<td class="line x" title="9:344	Although if commonly signifies a conditional sentence, there are many other words and constructs that can express conditions." ></td>
	<td class="line x" title="10:344	This paper first presents a linguistic analysis of such sentences, and then builds some supervised learning models to determine if sentiments expressed on different topics in a conditional sentence are positive, negative or neutral." ></td>
	<td class="line x" title="11:344	Experimental results on conditional sentences from 5 diverse domains are given to demonstrate the effectiveness of the proposed approach." ></td>
	<td class="line x" title="12:344	1 Introduction Sentiment analysis (also called opinion mining) has been an active research area in recent years." ></td>
	<td class="line oc" title="13:344	There are many research directions, e.g., sentiment classification (classifying an opinion document as positive or negative) (e.g., Pang, Lee and Vaithyanathan, 2002; Turney, 2002), subjectivity classification (determining whether a sentence is subjective or objective, and its associated opinion) (Wiebe and Wilson, 2002; Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and Hovy, 2004; Riloff and Wiebe, 2005), feature/topic-based sentiment analysis (assigning positive or negative sentiments to topics or product features) (Hu and Liu 2004; Popescu and Etzioni, 2005; Carenini et al., 2005; Ku et al., 2006; Kobayashi, Inui and Matsumoto, 2007; Titov and McDonald." ></td>
	<td class="line x" title="14:344	2008)." ></td>
	<td class="line x" title="15:344	Formal definitions of different aspects of the sentiment analysis problem and discussions of major research directions and algorithms can be found in (Liu, 2006; Liu, 2009)." ></td>
	<td class="line x" title="16:344	A comprehensive survey of the field can be found in (Pang and Lee, 2008)." ></td>
	<td class="line x" title="17:344	Our work is in the area of topic/feature-based sentiment analysis or opinion mining (Hu and Liu, 2004)." ></td>
	<td class="line x" title="18:344	The existing research focuses on solving the general problem." ></td>
	<td class="line x" title="19:344	However, we argue that it is unlikely to have a one-technique-fit-all solution because different types of sentences express sentiments/opinions in different ways." ></td>
	<td class="line x" title="20:344	A divide-and-conquer approach is needed, e.g., focused studies on different types of sentences." ></td>
	<td class="line x" title="21:344	This paper focuses on one type of sentences, i.e., conditional sentences, which have some unique characteristics that make it hard to determine the orientation of sentiments on topics/features in such sentences." ></td>
	<td class="line x" title="22:344	By sentiment orientation, we mean positive, negative or neutral opinions." ></td>
	<td class="line x" title="23:344	By topic, we mean the target on which an opinion has been expressed." ></td>
	<td class="line x" title="24:344	In the product domain, a topic is usually a product feature (i.e., a component or attribute)." ></td>
	<td class="line x" title="25:344	For example, in the sentence, I do not like the sound quality, but love the design of this MP3 player, the product features (topics) are sound quality and design of the MP3 player as opinions have been expressed on them." ></td>
	<td class="line x" title="26:344	The sentiment is positive on design but negative on sound quality." ></td>
	<td class="line x" title="27:344	Conditional sentences are sentences that describe implications or hypothetical situations and their consequences." ></td>
	<td class="line x" title="28:344	In the English language, a variety of conditional connectives can be used to form these sentences." ></td>
	<td class="line x" title="29:344	A conditional sentence contains two clauses: the condition clause and *  This work was done when Bing Liu was on sabbatical leave at Northwestern University." ></td>
	<td class="line x" title="30:344	180 the consequent clause, that are dependent on each other." ></td>
	<td class="line x" title="31:344	Their relationship has significant implications on whether the sentence describes an opinion." ></td>
	<td class="line x" title="32:344	One simple observation is that sentiment words (also known as opinion words) (e.g., great, beautiful, bad) alone cannot distinguish an opinion sentence from a non-opinion one." ></td>
	<td class="line x" title="33:344	A conditional sentence may contain many sentiment words or phrases, but express no opinion." ></td>
	<td class="line x" title="34:344	Example 1: If someone makes a beautiful and reliable car, I will buy it expresses no sentiment towards any particular car, although beautiful and reliable are positive sentiment words." ></td>
	<td class="line x" title="35:344	This, however, does not mean that a conditional sentence cannot express opinions/sentiments." ></td>
	<td class="line x" title="36:344	Example 2: If your Nokia phone is not good, buy this great Samsung phone is positive about the Samsung phone but does not express an opinion on the Nokia phone (although the owner of the Nokia phone may be negative about it)." ></td>
	<td class="line x" title="37:344	Clearly, if the sentence does not have if, the first clause is negative." ></td>
	<td class="line x" title="38:344	Hence, a method for determining sentiments in normal sentences will not work for conditional sentences." ></td>
	<td class="line x" title="39:344	The examples below further illustrate the point." ></td>
	<td class="line x" title="40:344	In many cases, both the condition and consequent together determine the opinion." ></td>
	<td class="line x" title="41:344	Example 3: If you are looking for a phone with good voice quality, dont buy this Nokia phone is negative about the voice quality of the Nokia phone, although there is a positive sentiment word good in the conditional clause modifying voice quality." ></td>
	<td class="line x" title="42:344	However, in the following example, the opinion is just the opposite." ></td>
	<td class="line x" title="43:344	Example 4: If you want a phone with good voice quality, buy this Nokia phone is positive about the voice quality of the Nokia phone." ></td>
	<td class="line x" title="44:344	As we can see, sentiment analysis of conditional sentences is a challenging problem." ></td>
	<td class="line x" title="45:344	One may ask whether there is a large percentage of conditional sentences to warrant a focused study." ></td>
	<td class="line x" title="46:344	Indeed, there is a fairly large proportion of such sentences in evaluative text." ></td>
	<td class="line x" title="47:344	They can have a major impact on the sentiment analysis accuracy." ></td>
	<td class="line x" title="48:344	Table 1 shows the percentage of conditional sentences (sentences containing the words if, unless, assuming, etc) and also the total number of sentences from which we computed the percentage in several user-forums." ></td>
	<td class="line x" title="49:344	The figures definitely suggest that there is considerable benefit to be gained by developing techniques that can analyze conditional sentences." ></td>
	<td class="line x" title="50:344	To the best of our knowledge, there is no focused study on conditional sentences." ></td>
	<td class="line x" title="51:344	This paper makes such an attempt." ></td>
	<td class="line x" title="52:344	Specifically, we determine whether a conditional sentence (which is also called a conditional in the linguistic literature) expresses positive, negative or neutral opinions on some topics/features." ></td>
	<td class="line x" title="53:344	Since our focus is on studying how conditions and consequents affect sentiments, we assume that topics are given, which are product attributes since our data sets are user comments on different products." ></td>
	<td class="line x" title="54:344	Our study is conducted from two perspectives." ></td>
	<td class="line x" title="55:344	We start with the linguistic angle to gain a good understanding of existing work on different types of conditionals." ></td>
	<td class="line x" title="56:344	As conditionals can be expressed with other words or phrases than if, we will study how they behave compared to if." ></td>
	<td class="line x" title="57:344	We will also show that the distribution of these conditionals based on our data sets." ></td>
	<td class="line x" title="58:344	With the linguistic knowledge, we perform a computational study using machine learning." ></td>
	<td class="line x" title="59:344	A set of features for learning is designed to capture the essential determining information." ></td>
	<td class="line x" title="60:344	Note that the features here are data attributes used in learning rather than product attributes or features." ></td>
	<td class="line x" title="61:344	Three classification strategies are designed to study how to best perform the classification task due to the complex situation of two clauses and their interactions in conditional sentences." ></td>
	<td class="line x" title="62:344	These three classification strategies are clause-based, consequent-based and whole-sentence-based." ></td>
	<td class="line x" title="63:344	Clause-based classification classifies each clause separately and then combines their results." ></td>
	<td class="line x" title="64:344	Consequent-based classification only uses consequents for classification as it is observed that in conditional sentences, it is often the consequents that decide the opinion." ></td>
	<td class="line x" title="65:344	Whole-sentence-based classification treats the entire sentence as a whole in classification." ></td>
	<td class="line x" title="66:344	Experimental results on conditional sentences from diverse domains demonstrate the effectiveness of these classification models." ></td>
	<td class="line x" title="67:344	The results indicate that the wholesentence-based classifier performs the best." ></td>
	<td class="line x" title="68:344	Since this paper only studies conditional sentences, a natural question is whether the proposed technique can be easily integrated into an overall sentiment analysis or opinion mining system." ></td>
	<td class="line x" title="69:344	The answer is yes because a large proportion of conditional sentences can be detected using conditional connectives." ></td>
	<td class="line x" title="70:344	Keyword search is Table 1: Percent of conditional sentences Source % of cond." ></td>
	<td class="line x" title="71:344	(total #." ></td>
	<td class="line x" title="72:344	of sent.)" ></td>
	<td class="line x" title="73:344	Cellphone 8.6 (47711) Automobile 5.0 (8113) LCD TV 9.92 (258078) Audio Systems 8.1 (5702) Medicine 8.29 (160259) 181 thus sufficient to identify such sentences for special handling using the proposed approach." ></td>
	<td class="line x" title="74:344	There are, however, some subtle conditionals which do not use normal conditional connectives and will need an additional module to identify them, but such sentences are very rare as Table 2 indicates." ></td>
	<td class="line x" title="75:344	2 The Problem Statement The paper follows the feature-based sentiment analysis model in (Hu and Liu 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="76:344	We are particularly interested in sentiments on products and services, which are called objects or entities." ></td>
	<td class="line x" title="77:344	Each object is described by its parts and attributes, which are collectively called features in (Hu and Liu, 2004; Liu, 2006)." ></td>
	<td class="line x" title="78:344	For example, in the sentence, If this camera has great picture quality, I will buy it, picture quality is a feature of the camera." ></td>
	<td class="line x" title="79:344	For formal definitions of objects and features, please refer to (Liu, 2006; Liu, 2009)." ></td>
	<td class="line x" title="80:344	In this paper, we use the term topic to mean feature as the feature here can confuse with the feature used in machine learning." ></td>
	<td class="line x" title="81:344	The term topic has also been used by some researchers (e.g., Kim and Hovy, 2004; Stoyanov and Cardie, 2008)." ></td>
	<td class="line x" title="82:344	Our objective is to predict the sentiment orientation (positive, negative or neutral) on each topic that has been commented on in a sentence." ></td>
	<td class="line x" title="83:344	The problem of automatically identifying features or topics being spoken about in a sentence has been studied in (Hu and Liu, 2004; Popescu and Etzioni, 2005; Stoyanov and Cardie, 2008)." ></td>
	<td class="line x" title="84:344	In this work, we do not attempt to identify such topics automatically." ></td>
	<td class="line x" title="85:344	Instead, we assume that they are given because our objective is to study how the interaction of the condition and consequent clauses affects sentiments." ></td>
	<td class="line x" title="86:344	For this purpose, we manually identify all the topics." ></td>
	<td class="line x" title="87:344	3 Conditional Sentences This section presents the linguistic perspective of conditional sentences." ></td>
	<td class="line x" title="88:344	3.1 Conditional Connectives A large majority of conditional sentences are introduced by the subordinating conjunction If." ></td>
	<td class="line x" title="89:344	However, there are also many other conditional connectives, e.g., even if, unless, in case, assuming/supposing, as long as, etc. Table 2 shows the distribution of conditional sentences with various connectives in our data." ></td>
	<td class="line x" title="90:344	Detailed linguistic discussions of them are beyond the scope of this paper." ></td>
	<td class="line x" title="91:344	Interested readers, please refer to (Declerck and Reed, 2001)." ></td>
	<td class="line x" title="92:344	Below, we briefly discuss some important ones and their interpretations." ></td>
	<td class="line x" title="93:344	If: This is the most commonly used conditional connective." ></td>
	<td class="line x" title="94:344	In addition to its own usage, it can also be used to replace other conditional connectives, except some semantically richer connectives (Declerck and Reed, 2001)." ></td>
	<td class="line x" title="95:344	Most (but not all) conditional sentences can be logically expressed in the form If P then Q, where P is the condition clause and Q is the consequent clause." ></td>
	<td class="line x" title="96:344	For practical purposes, we can automatically segment the condition and consequent clauses using simple rules generated by observing grammatical and linguistic patterns." ></td>
	<td class="line x" title="97:344	Unless: Most conditional sentences containing unless can be replaced with equivalent sentences with an if and a not." ></td>
	<td class="line x" title="98:344	For example, the sentence Unless you need clarity, buy the cheaper model can be expressed with If you dont need clarity, buy the cheaper model." ></td>
	<td class="line x" title="99:344	Even if: Linguistic theories claim that even if is a special case of a conditional which may not always imply an if-then relationship (Gauker 2005)." ></td>
	<td class="line x" title="100:344	However, in our datasets, we have observed that the usage of even if almost always translates into a conditional." ></td>
	<td class="line x" title="101:344	Replacing even if by if will yield a sentence that is semantically similar enough for the purpose of sentiment analysis." ></td>
	<td class="line x" title="102:344	Only if, provided/providing that, on condition that: Conditionals involving these phrases typically express a necessary condition, e.g., I will buy this camera only if they can reduce the price." ></td>
	<td class="line x" title="103:344	In such sentences, only usually does not affect whether the sentence is opinionated or not." ></td>
	<td class="line x" title="104:344	In case: Conditional sentences containing in case usually describe a precaution (I will close the window in case it rains), prevention (I wore sunglasses in case I was recognized), or a relevance conditional (In case you need a car, you can rent one)." ></td>
	<td class="line x" title="105:344	Identifying the conditional and consequent clauses is not straightforward in many cases." ></td>
	<td class="line x" title="106:344	Further, in these instances, replacing in case with if may not convey the intended meaning of the conditional." ></td>
	<td class="line x" title="107:344	We have ignored these cases in Table 2: Percentage of sentences with some main conditional connectives Conditional Connective % of sentences If 6.42 Unless 0.32 Even if 0.17 Until 0.10 As (so) long as 0.09 Assuming/supposing 0.04 In case 0.04 Only if 0.03 182 our analysis as we believe that they need a separate study, and also such sentences are rare." ></td>
	<td class="line x" title="108:344	As (so) long as: Sentences with these connectives behave similarly to if and can usually be replaced with if." ></td>
	<td class="line x" title="109:344	Assuming/Supposing: These are a category of conditionals that behave quite differently." ></td>
	<td class="line x" title="110:344	The participles supposing and assuming create conditional sentences where the conditional clause and the consequent clause can be syntactically independent." ></td>
	<td class="line x" title="111:344	It is quite difficult to distinguish those conditional sentences which contain an explicit consequent clause and fit within our analysis framework." ></td>
	<td class="line x" title="112:344	In our data, most of such sentences have no consequent, thus representing assumptions rather than opinions." ></td>
	<td class="line x" title="113:344	We omit these sentences in our study (they are also rare)." ></td>
	<td class="line x" title="114:344	3.2 Types of Conditionals There are extensive studies of conditional sentences (also known as conditionals) in linguistics." ></td>
	<td class="line x" title="115:344	Various theories have led to a number of classification systems." ></td>
	<td class="line x" title="116:344	Popular types of conditionals include actualization conditionals, inferential conditionals, implicative conditionals, etc (Declerck and Reed, 2001)." ></td>
	<td class="line x" title="117:344	However, these classifications are mainly based on semantic meanings which are difficult to recognize by a computer program." ></td>
	<td class="line x" title="118:344	To build classification models, we instead exploit canonical tense patterns of conditionals, which are often used in pedagogic grammar books." ></td>
	<td class="line x" title="119:344	They are defined based on tense and are associated with general meanings." ></td>
	<td class="line x" title="120:344	However, as described in (Declerck and Reed, 2001), their meanings are much more complex and numerous than their associated general meanings." ></td>
	<td class="line x" title="121:344	However, the advantage of this classification is that different types can be detected easily because they depend on tense which can be produced by a part-of-speech tagger." ></td>
	<td class="line x" title="122:344	As we will see in Section 5, canonical tense patterns help sentiment classification significantly." ></td>
	<td class="line x" title="123:344	Below, we introduce the four canonical tense patterns." ></td>
	<td class="line x" title="124:344	Zero Conditional:  This conditional form is used to describe universal statements like facts, rules and certainties." ></td>
	<td class="line x" title="125:344	In a zero conditional, both the condition and consequent clauses are in the simple present tense." ></td>
	<td class="line x" title="126:344	An example of such sentences is: If you heat water, it boils." ></td>
	<td class="line x" title="127:344	First Conditional: Conditional sentences of this type are also called potential or indicative conditionals." ></td>
	<td class="line x" title="128:344	They are used to express a hypothetical situation that is probably true, but the truth of which is unverified." ></td>
	<td class="line x" title="129:344	In the first conditional, the condition is in the simple present tense, and the consequent can be either in past tense or present tense, usually with a modal auxiliary verb preceding the main verb, e.g., If the acceleration is good, I will buy it." ></td>
	<td class="line x" title="130:344	Second Conditional: This is usually used to describe less probable situations, for stating preferences and imaginary events." ></td>
	<td class="line x" title="131:344	The condition clause of a second conditional sentence is in the past subjunctive (past tense), and the consequent clause contains a conditional verb modifier (like would, should, might), in addition to the main verb, e.g., If the cell phone was robust, I would consider buying it." ></td>
	<td class="line x" title="132:344	Third conditional: This is usually used to describe contrary-to-fact (impossible) past events." ></td>
	<td class="line x" title="133:344	The past perfect tense is used in the condition clause, and the consequent clause is in the present perfect tense, e.g., If I had bought the a767, I would have hated it." ></td>
	<td class="line x" title="134:344	Based on the above definitions, we have developed approximate part-of-speech (POS) tags 1  for the condition and the consequent of each pattern (Table 3), which do not cover all sentences, but overall they cover a majority of the sentences." ></td>
	<td class="line x" title="135:344	For those not covered cases, the problem is mainly due to incomplete sentences and wrong grammars, which are typical for informal writings in forum postings and blogs." ></td>
	<td class="line x" title="136:344	For example, the sentence, Great car if you need powerful acceleration, does not fall into any category, but it actually means It is a great car if you need powerful acceleration, which is a zero conditional." ></td>
	<td class="line x" title="137:344	To handle such sentences, we designed a set of rules to assign them some default types:  If condition contains VB/VBP/VBZ  0 conditional  If consequent contains VB/VBP/VBS  0 conditional  If condition contains VBG  1 st  conditional  If condition contains VBD  2 nd  conditional  If conditional contains VBN  3 rd  conditional." ></td>
	<td class="line x" title="138:344	1  The list of Part-Of-Speech (POS) tags can be found at: http://www.ling.upenn.edu/courses/Fall_2003/ling001/ penn_treebank_pos.html Table 3: Tenses for identifying conditional types Type Linguistic Rule Condition POS tags Consequent POS tags 0 If + simple present  simple present VB/VBP/VBZ VB/VBP/ VBZ 1 If + simple present  will + bare infinitive VB/VBP/VBZ /VBG MD + VB 2 If + past tense  would + infinitive VBD MD + VB 3 If + past perfect  present perfect VBD+VBN MD + VBD 183 By using these rules, we can increase the sentence coverage from 73% to 95%." ></td>
	<td class="line x" title="139:344	4 Sentiment Analysis of Conditionals We now describe our computational study." ></td>
	<td class="line x" title="140:344	We take a machine learning approach to predict sentiment orientations." ></td>
	<td class="line x" title="141:344	Below, we first describe features used and then classification strategies." ></td>
	<td class="line x" title="142:344	4.1 Feature construction I.  Sentiment words/phrases and their locations: Sentiment words are words used to express positive or negative opinions, which are instrumental for sentiment classification for obvious reasons." ></td>
	<td class="line x" title="143:344	We obtained a list of over 6500 sentiment words gathered from various sources." ></td>
	<td class="line x" title="144:344	The bulk of it is from http://www.cs.pitt.edu/mpqa." ></td>
	<td class="line x" title="145:344	We also added some of our own." ></td>
	<td class="line x" title="146:344	Our list is mainly from the work in (Hu and Liu, 2004; Ding, Liu and Yu, 2008)." ></td>
	<td class="line x" title="147:344	In addition to words, there are phrases that describe opinions." ></td>
	<td class="line x" title="148:344	We have identified a set of such phrases." ></td>
	<td class="line x" title="149:344	Although obtaining these phrases was time-consuming, it was only a one-time effort." ></td>
	<td class="line x" title="150:344	We will make this list available as a community resource." ></td>
	<td class="line x" title="151:344	It is possible that there is a better automated method for finding such phrases, such as the methods in (Kanayama and Nasukawa, 2006; Breck, Choi and Cardie, 2007)." ></td>
	<td class="line x" title="152:344	However, automatically generating sentiment phrases has not been the focus of this work as our objective is to study how the two clauses interact to determine opinions given the sentiment words and phrases are known." ></td>
	<td class="line x" title="153:344	Our list of phrases is by no means complete and we will continue to expand it in the future." ></td>
	<td class="line x" title="154:344	For each sentence, we also identify whether it contains sentiment words/phrases in its condition or consequent clause." ></td>
	<td class="line x" title="155:344	It was observed that the presence of a sentiment word/phrase in the consequent clause has more effect on the sentiment of a sentence." ></td>
	<td class="line x" title="156:344	II." ></td>
	<td class="line x" title="157:344	POS tags of sentiment words: Sentiment words may be used in several contexts, not all of which may correspond to an opinion." ></td>
	<td class="line x" title="158:344	For example, I trust Motorola and He has a trust fund both contain the word trust." ></td>
	<td class="line x" title="159:344	But only the former contains an opinion." ></td>
	<td class="line x" title="160:344	In such cases, the POS tags can provide useful information." ></td>
	<td class="line x" title="161:344	III." ></td>
	<td class="line x" title="162:344	Words indicating no opinion: Similar to how sentiment words are related to opinions, there are also a number of words which imply the opposite." ></td>
	<td class="line x" title="163:344	Words like wondering, thinking, debating are used when the user is posing a question or expressing doubts." ></td>
	<td class="line x" title="164:344	Thus such phrases usually do not contribute an opinion, especially if they are in the vicinity of the if connective." ></td>
	<td class="line x" title="165:344	We search a window of 3 words on either side of if to determine if there is any such word." ></td>
	<td class="line x" title="166:344	We have compiled a list of these words as well and use it in our experiments." ></td>
	<td class="line x" title="167:344	IV." ></td>
	<td class="line x" title="168:344	Tense patterns: These are the canonical tense patterns in Section 3.2." ></td>
	<td class="line x" title="169:344	They are used to generate a set of features." ></td>
	<td class="line x" title="170:344	We identify the first verb in both the condition and consequent clauses by searching for the relevant POS tags in Table 3." ></td>
	<td class="line x" title="171:344	We also search for the words preceding the main verb to find modal auxiliary verbs, which are also used as features." ></td>
	<td class="line x" title="172:344	V. Special characters: The presence or absence of ? and !." ></td>
	<td class="line x" title="173:344	VI." ></td>
	<td class="line x" title="174:344	Conditional connectives: The conditional connective used in the sentence (if, even if, unless, only if, etc) is also taken as a feature." ></td>
	<td class="line x" title="175:344	VII." ></td>
	<td class="line x" title="176:344	Length of condition and consequent clauses: Using simple linguistic and punctuation rules, we automatically segment a sentence into condition and consequent clauses." ></td>
	<td class="line x" title="177:344	The numbers of words in the condition and consequent clauses are then used as features." ></td>
	<td class="line x" title="178:344	We observed that when the condition clause is short, it usually has no impact on whether the sentence expresses an opinion." ></td>
	<td class="line x" title="179:344	VIII." ></td>
	<td class="line x" title="180:344	Negation words: The use of negation words like not, dont, never, etc, often alter the sentiment orientation of a sentence." ></td>
	<td class="line x" title="181:344	For example, the addition of not before a sentiment word can change the orientation of a sentence from positive to negative." ></td>
	<td class="line x" title="182:344	We consider a window of 3-6 words before an opinion word, and search for these kinds of words." ></td>
	<td class="line x" title="183:344	The following two features are singled out for easy reference later." ></td>
	<td class="line x" title="184:344	They are only used in one classification strategy." ></td>
	<td class="line x" title="185:344	The first feature is an indicator, and the second feature has a parameter (which will be evaluated separately)." ></td>
	<td class="line x" title="186:344	(1)." ></td>
	<td class="line x" title="187:344	Topic location: This feature indicates whether the topic is in the conditional clause or the consequent clause." ></td>
	<td class="line x" title="188:344	(2)." ></td>
	<td class="line x" title="189:344	Opinion weight: This feature considers only sentiment words in the vicinity of the topic, since they are more likely to influence the opinion on the topic." ></td>
	<td class="line x" title="190:344	A window size is used to control what we mean by vicinity." ></td>
	<td class="line x" title="191:344	The following formula is used to assign a weight to each sentiment word, which is inversely proportional to the distance (D op ) of the sentiment word to the topic mention." ></td>
	<td class="line x" title="192:344	Sentiment 184 value is +1 for a positive word and -1 for a negative word." ></td>
	<td class="line x" title="193:344	Sentwords are the set of known sentiment words and phrases." ></td>
	<td class="line x" title="194:344	}{, 1 sentwordsop D weight op op   =   4.2 Classification Strategies Since we are interested in topic-based sentiment analysis, how to perform classification becomes an interesting issue." ></td>
	<td class="line x" title="195:344	Due to the two clauses, it may not be sufficient to classify the whole sentence as positive or negative as in the same sentence, some topics may be positive and some may be negative." ></td>
	<td class="line x" title="196:344	We propose three strategies." ></td>
	<td class="line x" title="197:344	Clause-based classification: Since there are two clauses in a conditional sentence, in this case we build two classifiers, one for the condition and one for the consequent." ></td>
	<td class="line x" title="198:344	Condition classifier: This method classifies the condition clause as expressing positive, negative or neutral opinion." ></td>
	<td class="line x" title="199:344	Training data: Each training sentence is represented as a feature vector." ></td>
	<td class="line x" title="200:344	Its class is positive, negative or neutral depending on whether the conditional clause is positive, negative or neutral while considering both clauses." ></td>
	<td class="line x" title="201:344	Testing: For each test sentence, the resulting classifier predicts the opinion of the condition clause." ></td>
	<td class="line x" title="202:344	Topic class prediction: To predict the opinion on a topic, if the topic is in the condition clause, it takes the predicted class of the clause." ></td>
	<td class="line x" title="203:344	Consequent classifier: This classifier classifies the consequent clause as expressing positive, negative or neutral opinion." ></td>
	<td class="line x" title="204:344	Training data: Each training sentence is represented as a feature vector." ></td>
	<td class="line x" title="205:344	Its class is positive, negative or neutral depending on whether the consequent clause is positive, negative or neutral while considering both clauses." ></td>
	<td class="line x" title="206:344	Testing: For each test sentence, the resulting classifier predicts the opinion of the consequent clause." ></td>
	<td class="line x" title="207:344	Topic class prediction: To predict the opinion on a topic, if the topic is in the consequent clause, it takes the predicted class of the clause." ></td>
	<td class="line x" title="208:344	The combination of these two classifiers is called the clause-based classifier." ></td>
	<td class="line x" title="209:344	It works as follows: If a topic is in the conditional clause, the condition classifier is used, and if a topic is in the consequent clause, the consequent classifier is used." ></td>
	<td class="line x" title="210:344	Consequent-based classification: It is observed that in most cases, the condition clause contains no opinion whereas the consequent clause reflects the sentiment of the entire sentence." ></td>
	<td class="line x" title="211:344	Thus, this method uses (in a different way) only the above consequent classifier." ></td>
	<td class="line x" title="212:344	If it classifies the consequent of a testing conditional sentence as positive, all the topics in the whole sentence are assigned the positive orientation, and likewise for negative and neutral." ></td>
	<td class="line x" title="213:344	Whole-sentence-based classification: In this case, a single classifier is built to predict the opinion on each topic in a sentence." ></td>
	<td class="line x" title="214:344	Training data: In addition to the normal features, the two features (1) and (2) in Section 4.1 are used for this classifier." ></td>
	<td class="line x" title="215:344	If a sentence contains multiple topics, multiple training instances of the same sentence are created in the training data." ></td>
	<td class="line x" title="216:344	Each instance represents one specific topic." ></td>
	<td class="line x" title="217:344	The class of the instance depends on whether the opinion on the topic is positive, negative or neutral." ></td>
	<td class="line x" title="218:344	Testing: For each topic in each test sentence, the resulting classifier predicts its opinion." ></td>
	<td class="line x" title="219:344	Topic class prediction: This is not needed as the prediction has been done in testing." ></td>
	<td class="line x" title="220:344	5 Results and Discussions 5.1 Data sets Our data consists of conditional sentences from 5 different user forums: Cellphone, Automobile, LCD TV, Audio systems and Medicine." ></td>
	<td class="line x" title="221:344	We obtained user postings from these forums and extracted the conditional sentences." ></td>
	<td class="line x" title="222:344	We then manually annotated 1378 sentences from this corpus." ></td>
	<td class="line x" title="223:344	We also annotated the conditional and consequent clauses and identified the topics (or product features) being commented upon, and their sentiment orientations." ></td>
	<td class="line x" title="224:344	In our annotation, we observed that sentences with no sentiment words or phrases almost never express opinions, i.e., only around 3% of them express opinions." ></td>
	<td class="line x" title="225:344	There are around 26% sentences containing no sentiment words or phrases in our data." ></td>
	<td class="line x" title="226:344	To make the problem challenging, we restrict our attention to only those sentences that contain at least one sentiment word or phrase." ></td>
	<td class="line x" title="227:344	We have annotated topics from around 900 such sentences." ></td>
	<td class="line x" title="228:344	Table 4 shows the class distributions of this data." ></td>
	<td class="line x" title="229:344	At the clause level (topics are not considered), we observe that conditional clauses contain few opinions." ></td>
	<td class="line x" title="230:344	At the topic-level, 43.5% of the topics have positive opinions, 26.4% of the topics have negative opinions, and the rest have no opinions." ></td>
	<td class="line x" title="231:344	185 Table 4: Distribution of classes For the annotation of data, we assume that topics are known." ></td>
	<td class="line x" title="232:344	One student annotated the topics first." ></td>
	<td class="line x" title="233:344	Then two students annotated the sentiments on the topics." ></td>
	<td class="line x" title="234:344	If a student found that a topic annotation is wrong, he will let us know." ></td>
	<td class="line x" title="235:344	Some mistakes and missing topics were found but there were mainly due to oversights rather than disagreements." ></td>
	<td class="line x" title="236:344	The agreement on sentiment annotations were computed using the Kappa score." ></td>
	<td class="line x" title="237:344	We achieved the Kappa score of 0.63, which indicates strong agreements." ></td>
	<td class="line x" title="238:344	The conflicting cases were then solved through discussion to reach consensus." ></td>
	<td class="line x" title="239:344	We did not find anything that the annotators absolutely disagree with each other." ></td>
	<td class="line x" title="240:344	5.2 Experimental results We now present the results for different combinations of features and classification strategies." ></td>
	<td class="line x" title="241:344	For model building, we used Support Vector Machines (SVM), and the LIBSVM implementation (Chang and Lin, 2001) with a Gaussian kernel, which produces the best results." ></td>
	<td class="line x" title="242:344	All the results are obtained via 10-fold cross validation." ></td>
	<td class="line x" title="243:344	Two-class classification: We first discuss the results for a simpler version of the problem that involves only sentences with positive or negative orientations on some topics (at least one of the clauses must have a positive/negative opinion on a topic)." ></td>
	<td class="line x" title="244:344	Neutral sentences are not used (~28% of the total)." ></td>
	<td class="line x" title="245:344	The results of all three classifiers are given in Table 5." ></td>
	<td class="line x" title="246:344	The feature sets have been described in Section 4.1." ></td>
	<td class="line x" title="247:344	For all the experiments below, features (1) and (2) are only used by the whole-sentence-based classifier, but not used by the other two classifiers for obvious reasons." ></td>
	<td class="line x" title="248:344	{I+II}: This setting uses sentiment words and phrases, their positions and POS tags as features (we used Brills POS tagger)." ></td>
	<td class="line x" title="249:344	This can be seen as the baseline." ></td>
	<td class="line x" title="250:344	We observe that both the consequent-based and whole-sentence-based classifiers perform dramatically better than the clause-based classifier." ></td>
	<td class="line x" title="251:344	The consequent-based classifier and the whole-sentence-based classifier perform similarly (with the latter being slightly better)." ></td>
	<td class="line x" title="252:344	The precision, recall, and F-score are computed as the average of the two classes." ></td>
	<td class="line x" title="253:344	{I+II+III}: In this setting, the list of special non-sentiment related words is added to the feature set." ></td>
	<td class="line x" title="254:344	All three classifiers improve slightly." ></td>
	<td class="line x" title="255:344	{I+II+III+IV}: This setting includes all the canonical tense based features." ></td>
	<td class="line x" title="256:344	We see marked improvements for the consequent-based and wholesentence-based classifiers both in term of accuracy and F-score, which are statistically significant compared to those of {I+II+III} at the 95% confidence level based on paired t-test." ></td>
	<td class="line x" title="257:344	All: When all the features are used, the results of all the classifiers improve further." ></td>
	<td class="line x" title="258:344	Two main observations worth mentioning: 1." ></td>
	<td class="line x" title="259:344	Both the consequent-based and wholesentence-based classifiers outperform the clause-based classifier dramatically." ></td>
	<td class="line x" title="260:344	This confirms our observation that the consequent usually plays the key role in determining the sentiment of the sentence." ></td>
	<td class="line x" title="261:344	This is further reinforced by the fact that the consequent-based classifier actually performs similarly to the whole-sentence-based classifier." ></td>
	<td class="line x" title="262:344	The condition clause seems to give no help." ></td>
	<td class="line x" title="263:344	2." ></td>
	<td class="line x" title="264:344	The second observation is that the linguistic knowledge of canonical tense patterns helps significantly." ></td>
	<td class="line x" title="265:344	This shows that the linguistic knowledge is very useful." ></td>
	<td class="line x" title="266:344	We also noticed that many misclassifications are caused by grammatical errors, use of slang phrases and improper punctuations, which are typical of postings on the Web." ></td>
	<td class="line x" title="267:344	Due to language irregularities (e.g., wrong grammar, missing punctuations, sarcasm, exclamations), the POS tagger makes many mistakes as well causing some errors in the tense based features." ></td>
	<td class="line x" title="268:344	Three-class classification: We now move to the more difficult and realistic case of three classes: positive, negative and neutral (no-opinion)." ></td>
	<td class="line x" title="269:344	Table 6 shows the results." ></td>
	<td class="line x" title="270:344	The trend is similar except that the whole-sentence-based classifier now performs markedly better than the consequentbased classifier." ></td>
	<td class="line x" title="271:344	We believe that this is because the neutral class needs information from both the condition and consequent clauses." ></td>
	<td class="line x" title="272:344	This is evident from the fact that there is little or no improvement after {I+II} for the consequent-based classifier." ></td>
	<td class="line x" title="273:344	We also observe that the accuracies and Fscores for the three-class classification are lower than those for the two-class classification." ></td>
	<td class="line x" title="274:344	This is understandable due to the difficulty of determining whether a sentence has opinion or not." ></td>
	<td class="line x" title="275:344	Again, statistical test shows that the canonical tensebased features help significantly." ></td>
	<td class="line x" title="276:344	As mentioned in Section 4.1, the wholesentence-based classifier only considers those sentiment words in the vicinity of the topic under  Positive Negative Neutral Condition 6.9% 6.7% 86.4% Consequent 49.3% 16.5% 34% Topic-level 43.5% 26.4% 29.9% 186 investigation." ></td>
	<td class="line x" title="277:344	For this, we search a window of n words on either side of the topic mention." ></td>
	<td class="line x" title="278:344	To study the effect of varying n, we performed an experiment with various values of the window size and measured the overall accuracy for each case." ></td>
	<td class="line x" title="279:344	Table 7 shows how the accuracy changes as we increase the window size." ></td>
	<td class="line x" title="280:344	We found that a window size of 6-10 yielded good accuracies." ></td>
	<td class="line x" title="281:344	This is because lower values of n lead to loss of information regarding sentiment words as some sentiment words could be far from the topic." ></td>
	<td class="line x" title="282:344	We finally used 8, which gave the best results." ></td>
	<td class="line x" title="283:344	We also investigated ways of using the negation word in the sentence to correctly predict the sentiment." ></td>
	<td class="line x" title="284:344	One method is to use the negation word as a feature, as described in Section 4.1." ></td>
	<td class="line x" title="285:344	Another technique is to reverse the orientation of the prediction for those sentences which contain negation words." ></td>
	<td class="line x" title="286:344	We found that the former technique yielded better results." ></td>
	<td class="line x" title="287:344	The results reported so far are based on the former approach." ></td>
	<td class="line x" title="288:344	6 Related Work There are several research directions in sentiment analysis (or opinion mining)." ></td>
	<td class="line oc" title="289:344	One of the main directions is sentiment classification, which classifies the whole opinion document (e.g., a product review) as positive or negative (e.g., Pang et al, 2002; Turney, 2002; Dave et al, 2003; Ng et al. 2006; McDonald et al, 2007)." ></td>
	<td class="line o" title="290:344	It is clearly different from our work as we are interested in conditional sentences." ></td>
	<td class="line x" title="291:344	Another important direction is classifying sentences as subjective or objective, and classifying subjective sentences or clauses as positive or negative (Wiebe et al, 1999; Wiebe and Wilson, 2002, Yu and Hatzivassiloglou, 2003; Wilson et al, 2004; Kim and Hovy, 2004; Riloff and Wiebe, 2005; Gamon et al 2005; McDonald et al, 2007)." ></td>
	<td class="line x" title="292:344	Although these works deal with sentences, they aim to solve the general problem." ></td>
	<td class="line x" title="293:344	This paper argues that there is unlikely a onetechnique-fit-all solution, and advocates dealing with specific types of sentences differently by exploiting their unique characteristics." ></td>
	<td class="line x" title="294:344	Conditional sentences are the focus of this paper." ></td>
	<td class="line x" title="295:344	To the best of our knowledge, there is no focused study on them." ></td>
	<td class="line x" title="296:344	Several researchers also studied feature/topicbased sentiment analysis (e.g., Hu and Liu, 2004; Popescu and Etzioni, 2005; Ku et al, 2006; Carenini et al, 2006; Mei et al, 2007; Ding, Liu and Yu, 2008; Titov and R. McDonald, 2008; Stoyanov and Cardie, 2008; Lu and Zhai, 2008)." ></td>
	<td class="line x" title="297:344	Their objective is to extract topics or product features in sentences and determine whether the sentiments expressed on them are positive or negative." ></td>
	<td class="line x" title="298:344	Again, no focused study has been made to handle conditional sentences." ></td>
	<td class="line x" title="299:344	Effectively handling of conditional sentences can help their effort significantly." ></td>
	<td class="line x" title="300:344	Table 5: Two-class classification  positive and negative  Clause-based classifier Consequent-based classifier Whole-sentence-based classifier Acc." ></td>
	<td class="line x" title="301:344	Prec." ></td>
	<td class="line x" title="302:344	Rec." ></td>
	<td class="line x" title="303:344	F Acc." ></td>
	<td class="line x" title="304:344	Prec." ></td>
	<td class="line x" title="305:344	Rec." ></td>
	<td class="line x" title="306:344	F Acc." ></td>
	<td class="line x" title="307:344	Prec." ></td>
	<td class="line x" title="308:344	Rec." ></td>
	<td class="line x" title="309:344	F I+II (senti." ></td>
	<td class="line x" title="310:344	words+POS) 39.9 42.8 34.0 37.9 69.1 72.9 67.1 69.8 68.9 73.7 68.13 70.8 I+II+III (+ non-senti." ></td>
	<td class="line x" title="311:344	words)  41.5 44.9 37.1 40.6 69.3 73.9 66.3   69.9 69.2 73.7 63.5 71.0 I+II+III+IV (+ tenses) 42.7 45.2 38.5 41.6 72.7 76.4 72.0 74.1   71.1 77.9 72.2 74.9 All 43.2 46.1 38.9 42.2 73.3 77.0 72.7 74.8 72.3 77.8 73.6 75.6 Table 6: Three-class classification  positive, negative and neutral (no opinion)  Clause-based classifier Consequent-based classifier Whole-sentence-based classifier Acc." ></td>
	<td class="line x" title="312:344	Prec." ></td>
	<td class="line x" title="313:344	Rec." ></td>
	<td class="line x" title="314:344	F Acc." ></td>
	<td class="line x" title="315:344	Prec." ></td>
	<td class="line x" title="316:344	Rec." ></td>
	<td class="line x" title="317:344	F Acc." ></td>
	<td class="line x" title="318:344	Prec." ></td>
	<td class="line x" title="319:344	Rec." ></td>
	<td class="line x" title="320:344	F I+II (senti." ></td>
	<td class="line x" title="321:344	words+POS) 45.2 41.3 35.1 37.9 54.6 57.7 52.9 55.2 59.1 58.1 56.4 57.2 I+II+III (+ non-senti." ></td>
	<td class="line x" title="322:344	words)  46.9 42.8 37.8 40.1 55.3 60.0 51.3 55.3 61.4 60.1 60.8 60.4 I+II+III+IV (+ tenses) 50.3 48.7 40.9 44.5 57.3 64.0 50.0 56.1 64.6 63.3 63.9 63.6 All 53.3 49.8 44.1 46.8 58.7 64.5 50.1 56.4 67.8 66.9 65.1 66.0 Table 7: Accuracy of the whole-sentence-based classifier with varying window sizes (n) Window size 1 2 3 4 5 6 7 8 9 10 Accuracy 66.1 62.6 64.1 64.8 65.3 65.7 66.3 67.3 66.9 66.8 187 In this work, we used many sentiment words and phrases." ></td>
	<td class="line x" title="323:344	These words and phrases are usually compiled using different approaches (Hatzivassiloglou and McKeown, 1997; Kaji and Kitsuregawa, 2006; Kanayama and Nasukawa, 2006; Esuli and Sebastiani, 2006; Breck et al, 2007; Ding, Liu and Yu." ></td>
	<td class="line x" title="324:344	2008; Qiu et al, 2009)." ></td>
	<td class="line x" title="325:344	There are several existing lists produced by researchers." ></td>
	<td class="line x" title="326:344	We used the one from the MPQA corpus (http://www.cs.pitt.edu/mpqa) with added phrases of our own from (Ding, Liu and Yu." ></td>
	<td class="line x" title="327:344	2008)." ></td>
	<td class="line x" title="328:344	In our work, we also assume that the topics are known." ></td>
	<td class="line x" title="329:344	(Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi, Inui and Matsumoto, 2007; Stoyanov and Cardie, 2008) have studied topic/feature extraction." ></td>
	<td class="line x" title="330:344	One existing focused study is on comparative and superlative sentences (Jindal and Liu, 2006; Bos and Nissim, 2006; Fiszman et al, 2007; Ganapathibhotla and Liu, 2008)." ></td>
	<td class="line x" title="331:344	Their work identifies comparative sentences, extracts comparative relations in the sentences and analyzes comparative opinions (Ganapathibhotla and Liu, 2008)." ></td>
	<td class="line x" title="332:344	An example comparative sentence is Honda looks better than Toyota." ></td>
	<td class="line x" title="333:344	As we can see, comparative sentences are entirely different from conditional sentences." ></td>
	<td class="line x" title="334:344	Thus, their methods cannot be directly applied to conditional sentences." ></td>
	<td class="line x" title="335:344	7 Conclusion To perform sentiment analysis accurately, we argue that a divide-and-conquer approach is needed, i.e., focused study on each type of sentences." ></td>
	<td class="line x" title="336:344	It is unlikely that there is a one-size-fit-all solution." ></td>
	<td class="line x" title="337:344	This paper studied one type, i.e., conditional sentences, which have some unique characteristics that need special handling." ></td>
	<td class="line x" title="338:344	Our study was carried out from both the linguistic and computational perspectives." ></td>
	<td class="line x" title="339:344	In the linguistic study, we focused on canonical tense patterns, which have been showed useful in classification." ></td>
	<td class="line x" title="340:344	In the computational study, we built SVM models to automatically predict whether opinions on topics are positive, negative or neutral." ></td>
	<td class="line x" title="341:344	Experimental results have shown the effectiveness of the models." ></td>
	<td class="line x" title="342:344	In our future work, we will further improve the classification accuracy and study related problems, e.g., identifying topics/features." ></td>
	<td class="line x" title="343:344	Although there are some special conditional sentences that do not use easily recognizable conditional connectives and identifying them are useful, such sentences are very rare and spending time and effort on them may not be cost-effective at the moment." ></td>
	<td class="line x" title="344:344	Acknowledgements This work was supported in part by DOE SCIDAC-2: Scientific Data Management Center for Enabling Technologies (CET) grant DE-FC0207ER25808, DOE FASTOS award number DEFG02-08ER25848, NSF HECURA CCF0621443, NSF SDCI OCI-0724599, and NSF ST-HEC CCF-0444405." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1020
Subjectivity Word Sense Disambiguation
Akkaya, Cem;Wiebe, Janyce M.;Mihalcea, Rada;"></td>
	<td class="line x" title="1:249	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 190199, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:249	c 2009 ACL and AFNLP Subjectivity Word Sense Disambiguation Cem Akkaya and Janyce Wiebe University of Pittsburgh {cem,wiebe}@cs.pitt.edu Rada Mihalcea University of North Texas rada@cs.unt.edu Abstract This paper investigates a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses." ></td>
	<td class="line x" title="3:249	We provide empirical evidence that SWSD is more feasible than full word sense disambiguation, and that it can be exploited to improve the performance of contextual subjectivity and sentiment analysis systems." ></td>
	<td class="line x" title="4:249	1 Introduction The automatic extraction of opinions, emotions, and sentiments in text (subjectivity analysis) to support applications such as product review mining, summarization, question answering, and information extraction is an active area of research in NLP." ></td>
	<td class="line x" title="5:249	Many approaches to opinion, sentiment, and subjectivity analysis rely on lexicons of words that may be used to express subjectivity." ></td>
	<td class="line x" title="6:249	Examples of such words are the following (in bold): (1) He is a disease to every team he has gone to." ></td>
	<td class="line x" title="7:249	Converting to SMF is a headache." ></td>
	<td class="line x" title="8:249	The concert left me cold." ></td>
	<td class="line x" title="9:249	That guy is such a pain." ></td>
	<td class="line x" title="10:249	Knowing the meaning (and thus subjectivity) of these words would help a system recognize the negative sentiments in these sentences." ></td>
	<td class="line x" title="11:249	Most subjectivity lexicons are compiled as lists of keywords, rather than word meanings (senses)." ></td>
	<td class="line x" title="12:249	However, many keywords have both subjective and objective senses." ></td>
	<td class="line x" title="13:249	False hits  subjectivity clues used with objective senses  are a significant source of error in subjectivity and sentiment analysis." ></td>
	<td class="line x" title="14:249	For example, even though the following sentence contains all of the negative keywords above, it is nevertheless objective, as they are all false hits: (2) Early symptoms of the disease include severe headaches, red eyes, fevers and cold chills, body pain, and vomiting." ></td>
	<td class="line x" title="15:249	To tackle this source of error, we define a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses." ></td>
	<td class="line x" title="16:249	We hypothesize that SWSD is more feasible than full word sense disambiguation, because it is more coarse grained  often, the exact sense need not be pinpointed." ></td>
	<td class="line x" title="17:249	We also hypothesize that SWSD can be exploited to improve the performance of contextual subjectivity analysis systems via sense-aware classification." ></td>
	<td class="line x" title="18:249	The paper consists of two parts." ></td>
	<td class="line x" title="19:249	In the first part, we build and evaluate a targeted supervised SWSD system that aims to disambiguate members of a subjectivity lexicon." ></td>
	<td class="line x" title="20:249	It labels clue instances as having a subjective sense or an objective sense in context." ></td>
	<td class="line x" title="21:249	The system relies on common machine learning features for word sense disambiguation (WSD)." ></td>
	<td class="line x" title="22:249	The performance is substantially above both baseline and the performance of full WSD on the same data, suggesting that the task is feasible, and that subjectivity provides a natural coarsegrained grouping of senses." ></td>
	<td class="line x" title="23:249	The second part demonstrates the promise of SWSD for contextual subjectivity analysis." ></td>
	<td class="line x" title="24:249	First, we show that subjectivity sense ambiguity is highly prevalent in the MPQA opinion-annotated corpus (Wiebe et al., 2005; Wilson, 2008), thus establishing the potential benefit of performing SWSD." ></td>
	<td class="line x" title="25:249	Then, we exploit SWSD to improve performance on several subjectivity analysis tasks, from subjective/objective sentence-level classification to positive/negative/neutral expressionlevel classification." ></td>
	<td class="line x" title="26:249	To our knowledge, this is the 190 first attempt to explicitly use sense-level subjectivity tags in contextual subjectivity and sentiment analysis." ></td>
	<td class="line x" title="27:249	2 Background We adopt the definitions of subjective and objective from (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson, 2008)." ></td>
	<td class="line x" title="28:249	Subjective expressions are words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs." ></td>
	<td class="line x" title="29:249	A general covering term for such states is private state (Quirk et al., 1985), an internal state that cannot be directly observed or verified by others." ></td>
	<td class="line x" title="30:249	(Wiebe and Mihalcea, 2006) give the following examples: (3) His alarm grew." ></td>
	<td class="line x" title="31:249	He absorbed the information quickly." ></td>
	<td class="line x" title="32:249	UCC/Disciples leaders roundly condemned the Iranian Presidents verbal assault on Israel." ></td>
	<td class="line x" title="33:249	Whats the catch?" ></td>
	<td class="line x" title="34:249	Polarity (also called semantic orientation) is also important to NLP applications." ></td>
	<td class="line x" title="35:249	In review mining, for example, we want to know whether an opinion about a product is positive or negative." ></td>
	<td class="line x" title="36:249	Nonetheless, as argued by (Wiebe and Mihalcea, 2006; Su and Markert, 2008), there are also motivations for a separate subjective/objective (S/O) classification." ></td>
	<td class="line x" title="37:249	First, expressions may be subjective but not have any particular polarity." ></td>
	<td class="line x" title="38:249	An example given by (Wilson et al., 2005a) is Jerome says the hospital feels no different than a hospital in the states." ></td>
	<td class="line x" title="39:249	An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line x" title="40:249	Second, benefits for sentiment analysis can be realized by decomposing the problem into S/O (or neutral versus polar) and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005a; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="41:249	We will see further evidence of this in Section 4.2.3 in this paper." ></td>
	<td class="line x" title="42:249	The contextual subjectivity analysis experiments in Section 4 include both S/O and polarity classifications." ></td>
	<td class="line x" title="43:249	The data used in those experiments is from the MPQA Corpus (Wiebe et al., 2005; Wilson, 2008),1 which consists of texts from the world press annotated for subjective expressions." ></td>
	<td class="line x" title="44:249	1Available at http://www.cs.pitt.edu/mpqa In the MPQA Corpus, subjective expressions of varying lengths are marked, from single words to long phrases." ></td>
	<td class="line x" title="45:249	In addition, other properties are annotated, including polarity." ></td>
	<td class="line x" title="46:249	For SWSD, we need the notions of subjective and objective senses of words in a dictionary." ></td>
	<td class="line x" title="47:249	We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows." ></td>
	<td class="line x" title="48:249	Classifying a sense as S means that, when the sense is used in a text or conversation, one expects it to express subjectivity, and also that the phrase or sentence containing it expresses subjectivity." ></td>
	<td class="line x" title="49:249	As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective." ></td>
	<td class="line x" title="50:249	Thus, objective senses are defined as follows: Classifying a sense as O means that, when the sense is used in a text or conversation, one does not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else." ></td>
	<td class="line x" title="51:249	Finally, classifying a sense as B means it covers both subjective and objective usages." ></td>
	<td class="line x" title="52:249	The following subjective examples are given in (Wiebe and Mihalcea, 2006): His alarm grew." ></td>
	<td class="line x" title="53:249	alarm, dismay, consternation  (fear resulting from the awareness of danger) => fear, fearfulness, fright  (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)) Whats the catch?" ></td>
	<td class="line x" title="54:249	catch  (a hidden drawback; it sounds good but whats the catch?) => drawback  (the quality of being a hindrance; he pointed out all the drawbacks to my plan) They give the following objective examples: The alarm went off." ></td>
	<td class="line x" title="55:249	alarm, warning device, alarm system  (a device that signals the occurrence of some undesirable event) => device  (an instrumentality invented for a particular purpose; the device is small enough to wear on your wrist; a device intended to conserve water) He sold his catch at the market." ></td>
	<td class="line x" title="56:249	catch, haul  (the quantity that was caught; the catch was only 10 fish) => indefinite quantity  (an estimated quantity) Wiebe and Mihalcea performed an agreement study and report that good agreement (=0.74) can be achieved between human annotators labeling the subjectivity of senses." ></td>
	<td class="line x" title="57:249	For a similar task, (Su and Markert, 2008) also report good agreement (=0.79)." ></td>
	<td class="line x" title="58:249	191 3 Subjectivity Word Sense Disambiguation 3.1 Task Definition and Method We now turn to SWSD, and our method for performing it." ></td>
	<td class="line x" title="59:249	Note that SWSD is midway between pure dictionary classification and pure contextual interpretation." ></td>
	<td class="line x" title="60:249	For SWSD, the context of the word is considered in order to perform the task, but the subjectivity is determined solely by the dictionary." ></td>
	<td class="line x" title="61:249	In contrast, full contextual interpretation can deviate from a senses subjectivity label in the dictionary." ></td>
	<td class="line x" title="62:249	As noted above, words used with objective senses may appear in subjective expressions." ></td>
	<td class="line x" title="63:249	For example, an SWSD system would label the following examples of alarm as S, O and O, respectively." ></td>
	<td class="line x" title="64:249	On the other hand, a sentence-level subjectivity classifier would label the sentences as S, S, and O, respectively." ></td>
	<td class="line x" title="65:249	(4) His alarm grew." ></td>
	<td class="line x" title="66:249	Will someone shut that darn alarm off?" ></td>
	<td class="line x" title="67:249	The alarm went off." ></td>
	<td class="line x" title="68:249	We use a supervised approach to SWSD." ></td>
	<td class="line x" title="69:249	We train a different classifier for each lexicon entry for which we have training data." ></td>
	<td class="line x" title="70:249	Thus, our approach is like targeted WSD (in contrast to allwords WSD), with two labels: S and O. We borrow machine learning features which have been successfully used in WSD." ></td>
	<td class="line x" title="71:249	Specifically, given an ambiguous target word, we use the following features from (Mihalcea, 2002): CW : the target word itself CP : POS of the target word CF : surrounding context of 3 words and their POS HNP : the head of the noun phrase to which the target word belongs NB : the first noun before the target word VB : the first verb before the target word NA : the first noun after the target word VA : the first verb after the target word SK : at most 10 context words occurring at least 5 times; determined for each sense 3.2 Lexicon and Data Our target words are members of a subjectivity lexicon, because, since they are in such a lexicon, we know they have subjective usages." ></td>
	<td class="line x" title="72:249	Specifically, we use the lexicon of (Wilson et al., 2005b; Wilson, 2008).2 The entries have been divided into 2Available at http://www.cs.pitt.edu/mpqa those that are strongly subjective (strongsubj) and those that are weakly subjective (weaksubj), reflecting their reliability as subjectivity clues." ></td>
	<td class="line x" title="73:249	The sources of the entries in the lexicon are identified in (Wilson, 2008)." ></td>
	<td class="line x" title="74:249	In the second part of this paper, we evaluate systems against the MPQA corpus." ></td>
	<td class="line x" title="75:249	Wilson also uses this corpus for her evaluations." ></td>
	<td class="line x" title="76:249	To enable this, entries were added to the lexicon independently from the MPQA corpus (that is, none of the entries were derived using the MPQA corpus)." ></td>
	<td class="line x" title="77:249	The training and test data for SWSD consists of word instances in a corpus labeled as S or O, indicating whether they are used with a subjective or objective sense." ></td>
	<td class="line x" title="78:249	Because we do not have data labeled with the S/O coarse-grained senses and we did not want to undertake the annotation effort at this stage, we created an annotated corpus by combining two types of sense annotations: (1) labels of senses within a dictionary as S or O (i.e., subjectivity sense labels), and (2) sense tags of word instances in a corpus (i.e., sense-tagged data)." ></td>
	<td class="line x" title="79:249	The subjectivity sense labels are used to collapse the sense labels in the sense-tagged data into the two new senses, S and O. Our sense-tagged data are the lexical sample corpora (training and test data) from SENSEVAL1 (Kilgarriff and Palmer, 2000), SENSEVAL2 (Preiss and Yarowsky, 2001), and SENSEVAL3 (Mihalcea and Edmonds, 2004)." ></td>
	<td class="line x" title="80:249	We selected all of the SENSEVAL words that are also in the subjectivity lexicon, and labeled their dictionary senses as S, O, or B according to the annotation scheme described above in Section 2." ></td>
	<td class="line x" title="81:249	We did this subjectivity sense labeling according to the sense inventory of the underlying corpus (Hector for SENSEVAL1; WordNet1.7 for SENSEVAL2; and WordNet1.7.1 for SENSEVAL3)." ></td>
	<td class="line x" title="82:249	Among the words, we found that 11 are not ambiguous either they have only S or only O senses (in the corresponding sense inventory), or the senses of their instances in the SENSEVAL data are all S or all O. So as not to inflate our results, we removed those 11 from the data, leaving 39 words." ></td>
	<td class="line x" title="83:249	In addition, we excluded the senses labeled B (a total of 10 senses)." ></td>
	<td class="line x" title="84:249	This leaves a total of 372 senses: 9 words (64 senses) from SENSEVAL1, 18 words (201 senses) from SENSEVAL2, and 12 words (107 senses) from SENSEVAL3." ></td>
	<td class="line x" title="85:249	192 Base Acc SP SR SF OP OR OF IB EB(%) All 79.9 88.3 89.3 89.1 89.2 87.1 87.4 87.2 8.4 41.8 S1 57.9 80.7 81.1 78.3 79.7 80.2 82.9 81.5 22.8 54.2 S2 81.1 87.3 86.5 85.2 85.8 87.9 89.0 88.4 6.2 32.8 S3 95.0 96.4 96.5 99.0 97.7 96.3 87.8 91.8 1.4 28.0 Table 1: Overall SWSD results (micro averages)." ></td>
	<td class="line x" title="86:249	Base is majority-class baseline; Acc is accuracy; SP, SR, and SF are subjective precision, recall and F-measure; similarly for OP, OR, and OF." ></td>
	<td class="line x" title="87:249	IB is absolute improvement in Acc over Base; EB is percent error reduction in Acc." ></td>
	<td class="line x" title="88:249	3.3 SWSD Experiments In this section, we evaluate our SWSD system, and compare its performance to an WSD system on the same data." ></td>
	<td class="line x" title="89:249	Note that, although generally in the SENSEVAL datasets, training and test data are provided separately, a few target words from SENSEVAL1 do not have both training and testing data." ></td>
	<td class="line x" title="90:249	Thus, we opted to combine the training and test data into one dataset, and then perform 10-fold cross validation experiments." ></td>
	<td class="line x" title="91:249	For our classifier, we use the SVM classifier from the Weka package (Witten and Frank., 2005) with its default settings." ></td>
	<td class="line x" title="92:249	We were interested in how well the system would perform on more and less ambiguous words." ></td>
	<td class="line x" title="93:249	Thus, we split the words into three subsets according to their majority-class baselines, and report separate results: S1 (9 words), S2 (18 words), and S3 (12 words) have majority-class baselines in the intervals [50%,70%) , [70%,90%), and [90%,100%), respectively." ></td>
	<td class="line x" title="94:249	Table 1 contains the results, giving the overall results (micro averages), as well as results for the subsets S1, S2, and S3." ></td>
	<td class="line x" title="95:249	The improvement for SWSD over baseline is especially high for the less skewed set, S1." ></td>
	<td class="line x" title="96:249	This is very encouraging because these words are the more ambiguous words, and thus are the ones that most need SWSD (assuming the SENSEVAL priors are similar to the priors in the corpus)." ></td>
	<td class="line x" title="97:249	The average error reduction over baseline for S1 words is 54.2%." ></td>
	<td class="line x" title="98:249	Even for the more skewed sets S2 and S3, reductions are 32.8% and 28.0%, respectively, with an overall reduction of 41.8%." ></td>
	<td class="line x" title="99:249	To compare SWSD with WSD, we re-ran the 10-fold cross validation experiments, but this time using the original sense labels, rather than S and O. The (micro-averaged) accuracy is 67.9%, much lower than the overall accuracy for SWSD (88.3%)." ></td>
	<td class="line x" title="100:249	The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones, since the system is able to learn to distinguish between them with high accuracy." ></td>
	<td class="line x" title="101:249	There is also potential for improvement by using a richer feature set, including subjectivity features." ></td>
	<td class="line x" title="102:249	4 Opinion Analysis with Subjectivity Word Sense Disambiguation In this section, we explore the promise of SWSD for contextual subjectivity analysis." ></td>
	<td class="line x" title="103:249	First, we provide evidence that a subjectivity lexicon can have substantial coverage of the subjective expressions in a corpus, yet still be responsible for significant subjectivity sense ambiguity in that corpus." ></td>
	<td class="line x" title="104:249	Then, we exploit SWSD in several contextual opinion analysis systems, comparing the performance of sense-aware and non-sense-aware versions." ></td>
	<td class="line x" title="105:249	They are all variations of components of the OpinionFinder opinion recognition system.3 4.1 Coverage and Ambiguity of Lexicon Entries in the MPQA Corpus In this section, we consider the distribution of lexicon entries in the MPQA corpus." ></td>
	<td class="line x" title="106:249	The lexicon covers a substantial subset of the subjective expressions in the corpus: 67.1% of the subjective expressions contain one or more lexicon entries." ></td>
	<td class="line x" title="107:249	On the other hand, fully 42.9% of the instances of the lexicon entries in the MPQA corpus are not in subjective expressions." ></td>
	<td class="line x" title="108:249	An instance that is not in a subjective expression is, by definition, being used with an objective sense." ></td>
	<td class="line x" title="109:249	Thus, these instances are false hits of subjectivity clues." ></td>
	<td class="line x" title="110:249	As mentioned above, the entries in the lexicon have been pre-classified as either more (strongsubj) or less (weaksubj) reliable." ></td>
	<td class="line x" title="111:249	We see this difference reflected in their degree of ambiguity  53% of the 3Available at http://www.cs.pitt.edu/opin 193 weaksubj instances are false hits, while only 22% of the strongsubj instances are." ></td>
	<td class="line x" title="112:249	The high coverage of the lexicon demonstrates its potential usefulness for opinion analysis systems, while its degree of ambiguity, in the form of false hits in a subjectivity annotated corpus, shows the potential benefit to opinion analysis of performing SWSD." ></td>
	<td class="line x" title="113:249	As mentioned above, our experiments involve only lexicon entries that are covered by the SENSEVAL data, as we did not perform manual sense tagging for this work." ></td>
	<td class="line x" title="114:249	We have hope to expand the systems coverage in the future, as more wordsense tagged data is produced (e.g., ONTONOTES (Hovy et al., 2006))." ></td>
	<td class="line x" title="115:249	We also have evidence that a moderate amount of manual annotation would be worth the effort." ></td>
	<td class="line x" title="116:249	For example, let us order the lexicon entries from highest to lowest by frequency in the MPQA corpus." ></td>
	<td class="line x" title="117:249	The top 20 are responsible for 25% of all false hits in the corpus; the top 40 are responsible for 34%; and the top 80 are responsible for 44%." ></td>
	<td class="line x" title="118:249	If the SWSD system could be trained for these words, the potential impact on reducing false hits could be substantial, especially considering the good performance of the SWSD system on the more ambiguous words." ></td>
	<td class="line x" title="119:249	Note that we do not want to simply discard these clues." ></td>
	<td class="line x" title="120:249	The top 20 cover 9.4% of all subjective expressions; the top 40 cover 15.4%; and the top 80 cover 29.5%." ></td>
	<td class="line x" title="121:249	Note that SWSD only needs the data annotated with the coarse-grained binary labels, which should be less time consuming to produce than full word sense tags." ></td>
	<td class="line x" title="122:249	4.2 Contextual Classification We found in Section 3.3 that SWSD is a feasible task and then in Section 4.1 that there is a great deal of subjectivity sense ambiguity in a standard subjectivity-annotated corpus (MPQA)." ></td>
	<td class="line x" title="123:249	We now turn to exploiting the results of SWSD to automatically recognize subjectivity and sentiment in the MPQA corpus." ></td>
	<td class="line x" title="124:249	A motivation for using the MPQA data is that many types of classifiers have been evaluated on it, and we can directly test the effect of SWSD on these classifiers." ></td>
	<td class="line x" title="125:249	Note that, for the SWSD experiments, the number of words does not limit the amount of data, as SENSEVAL provides data for each word." ></td>
	<td class="line x" title="126:249	However, the only parts of the MPQA corpus for which SWSD could affect performance is the subset containing instances of the words in the SWSD systems coverage." ></td>
	<td class="line x" title="127:249	Thus, for the classifiers in this section, the data used is the SenMPQA dataset, which consists of the sentences in the MPQA Corpus that contain at least one instance of the 39 keywords." ></td>
	<td class="line x" title="128:249	There are 689 such sentences (containing, in total, 723 instances of the 39 keywords)." ></td>
	<td class="line x" title="129:249	Even though this dataset is smaller than the one used above, it gives us enough data to draw conclusions according to McNemars test for statistical significance." ></td>
	<td class="line x" title="130:249	4.2.1 Rule-based Classifier We first apply SWSD to the rule-based classifier from (Riloff and Wiebe, 2003)." ></td>
	<td class="line x" title="131:249	The classifier, which is a sentence-level S/O classifier, has low subjective and objective recall but high subjective and objective precision." ></td>
	<td class="line x" title="132:249	It is useful for creating training data for subsequent processing by applying it to large amounts of unannotated data." ></td>
	<td class="line x" title="133:249	The classifier is a good candidate for directly measuring the effects of SWSD on contextual subjectivity analysis, because it classifies sentences only by looking for the presence of subjectivity keywords." ></td>
	<td class="line x" title="134:249	Performance will improve if false hits can be ignored." ></td>
	<td class="line x" title="135:249	The classifier labels a sentence as S if it contains two or more strongsubj clues." ></td>
	<td class="line x" title="136:249	On the other hand, it considers three conditions to classify a sentence as O: there are no strongsubj clues in the current sentence, there are together at most one strongsubj clue in the previous and next sentence, and there are together at most 2 weaksubj clues in the current, previous, and next sentence." ></td>
	<td class="line x" title="137:249	A sentence that is not labeled S or O is labeled unknown." ></td>
	<td class="line x" title="138:249	The rule-based classifier is made sense aware by making it blind to the target word instances labeled O by the SWSD system, as these represent false hits of subjectivity keywords." ></td>
	<td class="line x" title="139:249	We compare this sense-aware method (SE), with the original classifier (ORB), in order to see if SWSD would improve performance." ></td>
	<td class="line x" title="140:249	We also built another modified rule-based classifier RE to demonstrate the effect of randomly ignoring subjectivity keywords." ></td>
	<td class="line x" title="141:249	RE ignores a keyword instance randomly with a probability of 0.429, the expected value of false hits in the MPQA corpus." ></td>
	<td class="line x" title="142:249	The results are listed in Table 2." ></td>
	<td class="line x" title="143:249	The rule-based classifier looks for the presence of the keywords to find subjective sentences and for the absence of the keywords to find objective sentences." ></td>
	<td class="line x" title="144:249	It is obvious that a variant working on 194 Acc OP OR OF SP SR SF ORB 27.0 50.0 4.1 7.6 92.7 36.0 51.8 SE 28.3 62.1 9.3 16.1 92.7 35.8 51.6 RE 27.6 48.4 7.7 13.3 92.6 35.4 51.2 Table 2: Effect of SWSD on the rule-based classifiers." ></td>
	<td class="line x" title="145:249	fewer keyword instances than ORB will always have the same or higher objective recall and the same or lower subjective recall than ORB." ></td>
	<td class="line x" title="146:249	That is the case for both SE and RE." ></td>
	<td class="line x" title="147:249	The real benefit we see is in objective precision, which is substantially higher for SE than ORB." ></td>
	<td class="line x" title="148:249	For our experiments, OP gives a better idea of the impact of SWSD, because most of the keyword instances SWSD disambiguates are weaksubj clues, and weaksubj keywords figure more prominently in objective classification." ></td>
	<td class="line x" title="149:249	On the other hand, RE has both lower OP and SP than ORB." ></td>
	<td class="line x" title="150:249	Note that accuracy for all three systems is low, because all unknown predictions are counted as incorrect." ></td>
	<td class="line x" title="151:249	These findings suggest that SWSD performs well on disambiguating keyword instances in the MPQA corpus,4 and demonstrates a positive impact of SWSD on sentence-level subjectivity classification." ></td>
	<td class="line x" title="152:249	4.2.2 Subjective/Objective Classifier We now move to more fine-grained expressionlevel subjectivity classification." ></td>
	<td class="line x" title="153:249	Since sentences often contain multiple subjective expressions, expression-level classification is more informative than sentence-level classification." ></td>
	<td class="line x" title="154:249	The classifier in this section is an implementation of the neutral/polar supervised classifier of (Wilson et al., 2005a) (using the same features), except that the classes are S/O rather than neutral/polar." ></td>
	<td class="line x" title="155:249	These classifiers label instances of lexicon entries." ></td>
	<td class="line x" title="156:249	The gold standard is defined on the MPQA Corpus as follows: If an instance is in a subjective expression, it is contextually S. If the instance is in an objective expression, it is contextually O. We evaluate the system on the 723 clue instances in the SenMPQA dataset." ></td>
	<td class="line x" title="157:249	We incorporate SWSD information into the contextual subjectivity classifier in a straightforward fashion: outputs are modified according to simple, intuitive rules." ></td>
	<td class="line x" title="158:249	4which we cannot evaluate directly, as the MPQA corpus is not sense tagged." ></td>
	<td class="line x" title="159:249	Our strategy is defined by the relation between sense subjectivity and contextual subjectivity and involves two rules, R1 and R2." ></td>
	<td class="line x" title="160:249	We know that a keyword instance used with a S sense must be in a subjective expression." ></td>
	<td class="line x" title="161:249	R1 is to simply trust SWSD: If the contextual classifier labels an instance as O, but SWSD determines that it has an S sense, then R1 flips the contextual classifiers label to S. Things are not as simple in the case of O senses, since they may appear in both subjective and objective expressions." ></td>
	<td class="line x" title="162:249	We will state R2, and then explain it: If the contextual classifier labels an instance as S, but (1) SWSD determines that it has an O sense, (2) the contextual classifiers confidence is low, and (3) there is no other subjective keyword in the same expression, then R2 flips the contextual classifiers label to O. First, consider confidence: though a keyword with an O sense may appear in either subjective or objective expressions, it is more likely to appear in an objective expression." ></td>
	<td class="line x" title="163:249	We assume that this is reflected to some extent in the contextual classifiers confidence." ></td>
	<td class="line x" title="164:249	Second, if a keyword with an O sense appears in a subjective expression, then the subjectivity is not due to that keyword but rather due to something else." ></td>
	<td class="line x" title="165:249	Thus, the presence of another lexicon entry explains away the presence of the O sense in the subjective expression, and we do not want SWSD to overrule the contextual classifier." ></td>
	<td class="line x" title="166:249	Only when the contextual classifier isnt certain and only when there isnt another keyword does R2 flip the label to O. Our definition of low confidence is in terms of the label weights assigned by BoosTexter (Schapire and Singer, 2000), which is the underlying machine learning algorithm of the classifier." ></td>
	<td class="line x" title="167:249	We use the difference between the largest label weight and the second largest label weight as a measure of confidence, as suggested in the BoosTexter documentation." ></td>
	<td class="line x" title="168:249	The threshold we use is 0.0008.5 We apply the contextual classifier and the SWSD system to the data, and compare the performance of the original system (OS/O) and three sense-aware variants: one using only R1, one us5As will be noted below, we experimented with three thresholds for the classifier in Section 4.2.3, with no significant difference in accuracy." ></td>
	<td class="line x" title="169:249	Here, we simply adopt 0.0008, without further experimentation." ></td>
	<td class="line x" title="170:249	In addition, we did not experiment with other conditions than those incorporated in the two rules in this section and the two rules in Section 4.2.3 below." ></td>
	<td class="line x" title="171:249	195 Acc OP OR OF SP SR SF OS/O 75.4 68.0 62.9 65.4 79.2 82.7 80.9 R1 77.7 75.5 58.8 66.1 78.6 88.8 83.4 R2 79.0 67.3 83.9 74.7 89.0 76.1 82.0 R1R2 81.3 72.5 79.8 75.9 87.4 82.2 84.8 Table 3: Effect of SWSD on the subjective/objective classifier ing only R2, and one using both (R1R2)." ></td>
	<td class="line x" title="172:249	The results are in Table 3." ></td>
	<td class="line x" title="173:249	The R1 variant shows an improvement of 2.3 points in accuracy (a 9.4% error reduction)." ></td>
	<td class="line x" title="174:249	The R2 variant shows an improvement of 3.6 points in accuracy (a 14.6% error reduction)." ></td>
	<td class="line x" title="175:249	Applying both rules (R1R2) gives an improvement of 5.9 percentage points in accuracy (a 24% error reduction)." ></td>
	<td class="line x" title="176:249	In our case, a paired t-test is not appropriate to measure statistical significance, as we are not doing multiple runs." ></td>
	<td class="line x" title="177:249	Thus, we apply McNemars test, which is a non-parametric method for algorithms that can be executed only once, meaning training once and testing once (Dietterich, 1998)." ></td>
	<td class="line x" title="178:249	For R1, the improvement in accuracy is statistically significant at the p < .05 level." ></td>
	<td class="line x" title="179:249	For R2 and R1R2, the improvement in accuracy is statistically significant at the p < .01 level." ></td>
	<td class="line x" title="180:249	Moreover, in all cases, we see improvement in both objective and subjective F-measure." ></td>
	<td class="line x" title="181:249	4.2.3 Contextual Polarity Classifier We now apply SWSD to contextual polarity classification (positive/negative/neutral), in the hope that avoiding false hits of subjectivity keywords will also lead to performance improvement in contextual sentiment analysis." ></td>
	<td class="line x" title="182:249	We use an implementation of the classifier of (Wilson et al., 2005a)." ></td>
	<td class="line x" title="183:249	This classifier labels instances of lexicon entries." ></td>
	<td class="line x" title="184:249	The gold standard is defined on the MPQA Corpus as follows: If an instance is in a positive subjective expression, it is contextually positive (Ps); if in a negative subjective expression, it is contextually negative (Ng); and if it is in an objective expression or a neutral subjective expression, then it is contextually N(eutral)." ></td>
	<td class="line x" title="185:249	As above, we evaluate the system on the keyword instances in the SenMPQA dataset." ></td>
	<td class="line x" title="186:249	Wilson et al. use a two step approach." ></td>
	<td class="line x" title="187:249	The first step classifies keyword instances as being in a polar (positive or negative) or a neutral context." ></td>
	<td class="line x" title="188:249	The first step is performed by the neutral/polar classifier mentioned above in Section 4.2.2." ></td>
	<td class="line x" title="189:249	The second step decides the contextual polarity (positive or negative) of the instances classified as polar in the first step, and is performed by a separate classifier." ></td>
	<td class="line x" title="190:249	To make a sense-aware version of the system, we use rules to change some of the answers of the neutral/polar classifier." ></td>
	<td class="line x" title="191:249	Unfortunately, we cannot simply trust SWSD when it labels a keyword as an S sense, because an S sense might be in a N(eutral) expression (since there are neutral subjective expressions)." ></td>
	<td class="line x" title="192:249	But, an S sense is more likely to appear in a P(olar) expression." ></td>
	<td class="line x" title="193:249	Thus, we consider confidence (rule R3): If the contextual classifier labels an instance as N, but SWSD determines it has an S sense and the contextual classifiers confidence is low,6 then R3 flips the contextual classifiers label to P. Rule R4 is analogous to R2 in the previous section: If the contextual classifier labels an instance as P, but (1) SWSD determines that it has an O sense, (2) the contextual classifiers confidence is low, and (3) there is no other subjective keyword in the same expression, then R2 flips the contextual classifiers label to N. We compare the performance of the original neutral/polar classifier (ON/P) and sense-aware variants using R3 and R4." ></td>
	<td class="line x" title="194:249	The results are in Table 4." ></td>
	<td class="line x" title="195:249	This time, the table does not include a combined method, because only R4 improves performance." ></td>
	<td class="line x" title="196:249	This is consistent with the finding in (Wilson et al., 2005a) that most errors are caused by subjectivity keywords with non-neutral prior polarity appearing in phrases with neutral contextual polarity." ></td>
	<td class="line x" title="197:249	R4 targets these cases." ></td>
	<td class="line x" title="198:249	It is promising to see that SWSD provides enough information to fix some of them." ></td>
	<td class="line x" title="199:249	There is a 2.6 point improvement in accuracy (a 12.4% error reduction)." ></td>
	<td class="line x" title="200:249	The improvement in accuracy is statistically significant at the p < .01 level with McNemars test." ></td>
	<td class="line x" title="201:249	The improvement in accuracy is accompanied by improvements in both neutral and polar F-measure." ></td>
	<td class="line x" title="202:249	We wanted to see if the improvements in the 6As in the previous section, low confidence is defined in terms of the difference between the largest label weight and the second largest label weight assigned by BoosTexter." ></td>
	<td class="line x" title="203:249	We tried three thresholds, 0.0007, 0.0008, and 0.0009, resulting in only a slight difference in accuracy: 0.0007 and 0.0009 both give 81.5 accuracy compared to 81.6 accuracy for 0.0008." ></td>
	<td class="line x" title="204:249	We report results using 0.0008, though the accuracy using the other thresholds is statistically significantly better than the accuracy of the original classifier at the same level." ></td>
	<td class="line x" title="205:249	196 Acc NP NR NF NgP NgR NgF PsP PsR PsF OPs/Ng/N 77.6 80.9 94.6 87.2 60.4 29.4 39.5 52.2 32.4 40.0 R4 80.6 81.2 98.7 89.1 82.1 29.4 43.2 68.6 32.4 44.0 Table 5: Effect of SWSD on the contextual polarity classifier Acc NP NR NF PP PR PF ON/P 79.0 81.5 92.5 86.7 65.8 40.7 50.3 R3 70.0 83.7 73.8 78.4 44.4 59.3 50.8 R4 81.6 81.7 96.8 88.6 81.1 38.6 52.3 Table 4: Effect of SWSD on the neutral/polar classifier first step of Wilson et als system can be propagated to their second step, yielding an overall improvement in positive /negative/neutral (Ps/Ng/N) classification." ></td>
	<td class="line x" title="206:249	The sense-aware variant of the overall two-part system is the same as the original except that we apply R4 to the output of the first step (flipping some of the neutral/polar classifiers P labels to N)." ></td>
	<td class="line x" title="207:249	Thus, since the second step in Wilson et al.s classifier processes only those instances labeled P in the first step, in the sense-aware system, fewer instances are passed from the first to the second step." ></td>
	<td class="line x" title="208:249	Table 5 reports results for the original system (OPs/Ng/N) and the sense-aware variant (R4)." ></td>
	<td class="line x" title="209:249	These results are for the entire SenMPQA dataset, not just those labeled P in the first step." ></td>
	<td class="line x" title="210:249	The accuracy improves 3 percentage points (a 13.4% error reduction)." ></td>
	<td class="line x" title="211:249	The improvement in accuracy is statistically significant at the p < .01 level with McNemars test." ></td>
	<td class="line x" title="212:249	We see the real benefit when we look at the precision of the positive and negative classes." ></td>
	<td class="line x" title="213:249	Negative precision goes from 60.4 to 82.1 and positive precision goes from 52.2 to 68.6, with no loss in recall." ></td>
	<td class="line x" title="214:249	This is evidence that the SWSD system is doing a good job of removing some false hits of subjectivity clues that harm the original version of the system." ></td>
	<td class="line x" title="215:249	5 Comparisons to Previous Work Several researchers exploit lexical resources for contextual subjectivity and sentiment analysis." ></td>
	<td class="line x" title="216:249	These systems typically look for the presence of subjective or sentiment-bearing words in the text." ></td>
	<td class="line oc" title="217:249	They may rely only on this information (e.g., (Turney, 2002; Whitelaw et al., 2005; Riloff and Wiebe, 2003)), or they may combine it with additional information as well (e.g., (Yu and Hatzivassiloglou, 2003; Kim and Hovy, 2004; Bloom et al., 2007; Wilson et al., 2005a))." ></td>
	<td class="line x" title="218:249	We apply SWSD to some of those systems to show the effect of SWSD on contextual subjectivity and sentiment analysis." ></td>
	<td class="line x" title="219:249	Another set of related work is on subjectivity and polarity labeling of word senses (e.g.(Esuli and Sebastiani, 2006; Andreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Su and Markert, 2008))." ></td>
	<td class="line x" title="221:249	They label senses of words in a dictionary." ></td>
	<td class="line x" title="222:249	In comparison, we label senses of word instances in a corpus." ></td>
	<td class="line x" title="223:249	Moreover, our work extends findings in (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008)." ></td>
	<td class="line x" title="224:249	(Wiebe and Mihalcea, 2006) demonstrates that subjectivity is a property that can be associated with word senses." ></td>
	<td class="line x" title="225:249	We show that it is a natural grouping of word senses and that it provides a principled way for clustering senses." ></td>
	<td class="line x" title="226:249	They also demonstrate that subjectivity helps with WSD." ></td>
	<td class="line x" title="227:249	We show that a coarse-grained WSD variant (SWSD) helps with subjectivity and sentiment analysis." ></td>
	<td class="line x" title="228:249	Both (Wiebe and Mihalcea, 2006) and (Su and Markert, 2008) show that even reliable subjectivity clues have objective senses." ></td>
	<td class="line x" title="229:249	We demonstrate that this ambiguity is also prevalent in a corpus." ></td>
	<td class="line x" title="230:249	Several researchers (e.g., (Palmer et al., 2004; Navigli, 2006; Snow et al., 2007; Hovy et al., 2006)) work on reducing the granularity of sense inventories for WSD." ></td>
	<td class="line x" title="231:249	They aim for a more coarsegrained sense inventory to overcome performance shortcomings related to fine-grained sense distinctions." ></td>
	<td class="line x" title="232:249	Our work is similar in the sense that we reduce all senses of a word to two senses (S/O)." ></td>
	<td class="line x" title="233:249	The difference is the criterion driving the grouping." ></td>
	<td class="line x" title="234:249	Related work concentrates on syntactic and semantic similarity between senses to group them." ></td>
	<td class="line x" title="235:249	In contrast, our grouping is driven by subjectivity with a specific application area in mind, namely subjectivity and sentiment analysis." ></td>
	<td class="line x" title="236:249	6 Conclusions and Future Work We introduced the task of subjectivity word sense disambiguation (SWSD), and evaluated a supervised method inspired by research in WSD." ></td>
	<td class="line x" title="237:249	The 197 system achieves high accuracy, especially on highly ambiguous words, and substantially outperforms WSD on the same data." ></td>
	<td class="line x" title="238:249	The positive results provide evidence that SWSD is a feasible variant of WSD, and that the S/O sense groupings are natural ones." ></td>
	<td class="line x" title="239:249	We also explored the promise of SWSD for contextual subjectivity analysis." ></td>
	<td class="line x" title="240:249	We showed that a subjectivity lexicon can have substantial coverage of the subjective expressions in the corpus, yet still be responsible for significant sense ambiguity." ></td>
	<td class="line x" title="241:249	This demonstrates the potential benefit to opinion analysis of performing SWSD." ></td>
	<td class="line x" title="242:249	We then exploit SWSD in several contextual opinion analysis systems, including positive/negative/neutral sentiment classification." ></td>
	<td class="line x" title="243:249	Improvements in performance were realized for all of the systems." ></td>
	<td class="line x" title="244:249	We plan several future directions which promise to further increase the impact of SWSD on subjectivity and sentiment analysis." ></td>
	<td class="line x" title="245:249	We will manually annotate a moderate number of strategically chosen words, namely frequent ones which are highly ambiguous." ></td>
	<td class="line x" title="246:249	In addition, we will add features to the SWSD system reflecting the subjectivity of the surrounding context." ></td>
	<td class="line x" title="247:249	Finally, there are more sophisticated strategies to explore for improving subjectivity and sentiment analysis via SWSD than the simple, intuitive rules we began with in this paper." ></td>
	<td class="line x" title="248:249	Acknowledgments This material is based in part upon work supported by National Science Foundation awards #0840632 and #0840608." ></td>
	<td class="line x" title="249:249	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1061
Topic-wise, Sentiment-wise, or Otherwise? Identifying the Hidden Dimension for Unsupervised Text Classification
Dasgupta, Sajib;Ng, Vincent;"></td>
	<td class="line x" title="1:238	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 580589, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:238	c 2009 ACL and AFNLP Topic-wise, Sentiment-wise, or Otherwise?" ></td>
	<td class="line x" title="3:238	Identifying the Hidden Dimension for Unsupervised Text Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract While traditional work on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the authors mood, gender, age, or sentiment." ></td>
	<td class="line x" title="4:238	Without knowing the users intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires." ></td>
	<td class="line x" title="5:238	To address this problem, we propose a novel way of incorporating user feedback into a clustering algorithm, which allows a user to easily specify the dimension along which she wants the data points to be clustered via inspecting only a small number of words." ></td>
	<td class="line x" title="6:238	This distinguishes our method from existing ones, which typically require a large amount of effort on the part of humans in the form of document annotation or interactive construction of the feature space." ></td>
	<td class="line x" title="7:238	We demonstrate the viability of our method on several challenging sentiment datasets." ></td>
	<td class="line x" title="8:238	1 Introduction Text clustering is one of the most important applications in Natural Language Processing (NLP)." ></td>
	<td class="line x" title="9:238	A common approach to this problem consists of (1) computing the similarity between each pair of documents, each of which is typically represented as a bag of words; and (2) using an unsupervised clustering algorithm to partition the documents." ></td>
	<td class="line x" title="10:238	The majority of existing work on text clustering has focused on topic-based clustering, where high accuracies can be achieved even for datasets with a large number of classes (e.g., 20 Newsgroups)." ></td>
	<td class="line x" title="11:238	On the other hand, there has been relatively little work on sentiment-based clustering and the related task of unsupervised polarity classification, where the goal is to cluster (or classify) a set of documents (e.g., reviews) according to the polarity (e.g., thumbs up or thumbs down) expressed by the author in an unsupervised manner." ></td>
	<td class="line x" title="12:238	Despite the large amount of recent work on sentiment analysis and opinion mining, much of it has focused on supervised methods (e.g., Pang et al.(2002), Kim and Hovy (2004), Mullen and Collier (2004))." ></td>
	<td class="line x" title="14:238	One weakness of these existing supervised polarity classification systems is that they are typically domainand language-specific." ></td>
	<td class="line x" title="15:238	Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier." ></td>
	<td class="line x" title="16:238	Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages." ></td>
	<td class="line x" title="17:238	However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is similar enough to the target domain (Blitzer et al., 2007)." ></td>
	<td class="line x" title="18:238	When the target domain or language fails to meet this requirement, sentiment-based clustering or unsupervised polarity classification become appealing alternatives." ></td>
	<td class="line x" title="19:238	Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community." ></td>
	<td class="line pc" title="20:238	Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification." ></td>
	<td class="line n" title="21:238	However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict the polarity of a review heuristically." ></td>
	<td class="line x" title="22:238	Despite its practical significance, sentimentbased clustering is a challenging task." ></td>
	<td class="line x" title="23:238	To illustrate its difficulty, consider the task of clustering a set of movie reviews." ></td>
	<td class="line x" title="24:238	Since each review may contain a description of the plot and the authors 580 sentiment, a clustering algorithm may cluster reviews along either the plot dimension or the sentiment dimension; and without knowing the users intention, they will be clustered along the most prominent dimension." ></td>
	<td class="line x" title="25:238	Assuming the usual bagof-words representation, the most prominent dimension will more likely be plot, as it is not uncommon for a review to be devoted almost exclusively to the plot, with the author briefly expressing her sentiment only at the end of the review." ></td>
	<td class="line x" title="26:238	Even if the reviews contain mostly subjective material, the most prominent dimension may still not be sentiment, due to the fact that many reviews are sentimentally ambiguous." ></td>
	<td class="line x" title="27:238	Specifically, a reviewer may have negative opinions on the actors but at the same time talk enthusiastically about how much she enjoyed the plot." ></td>
	<td class="line x" title="28:238	The presence of both positive and negative sentiment-bearing words in these reviews renders the sentiment dimension hidden (i.e., less prominent) as far as clustering is concerned." ></td>
	<td class="line x" title="29:238	Therefore, there is no guarantee that the clustering algorithm will automatically produce a sentiment-based clustering of the reviews." ></td>
	<td class="line x" title="30:238	Hence, it is important for a user to provide feedback on the clustering process to ensure that the reviews are clustered along the sentiment dimension, possibly in an interactive manner." ></td>
	<td class="line x" title="31:238	One way to do this would be to ask the user to annotate a small number of reviews with polarity information, possibly through an active learning procedure to minimize human intervention (Dredze and Crammer, 2008)." ></td>
	<td class="line x" title="32:238	Another way would be to have the user explicitly identify the relevant features (in our case, the sentiment-bearing words) at the beginning of the clustering process (Liu et al., 2004), or incrementally construct the set of relevant features in an interactive fashion (Bekkerman et al., 2007; Raghavan and Allan, 2007; Roth and Small, 2009)." ></td>
	<td class="line x" title="33:238	In addition, the user may supply constraints on which pairs of documents must or must not appear in the same cluster (Wagstaff et al., 2001), or simply tell the algorithm whether two clusters should be merged or split during the clustering process (Balcan and Blum, 2008)." ></td>
	<td class="line x" title="34:238	It is worth noting that many of these feedback mechanisms were developed by machine learning researchers for general clustering tasks and not for sentiment-based clustering." ></td>
	<td class="line x" title="35:238	Our goal in this paper is to propose a novel mechanism allowing a user to cluster a set of documents along the desired dimension, which may be a hidden dimension, with very limited user feedback." ></td>
	<td class="line x" title="36:238	In comparison to the aforementioned feedback mechanisms, ours is arguably much simpler: we only require that the user select a dimension by examining a small number of features for each dimension, as opposed to having the user generate the feature space in an interactive manner or identify clusters that need to be merged or split." ></td>
	<td class="line x" title="37:238	In particular, identifying clusters for merging or splitting in Balcan and Blums algorithm may not be as easy as it appears: for each MERGE or SPLIT decision the user makes, she has to sample a large number of documents from the cluster(s), read through the documents, and base her decision on the extent to which the documents are (dis)similar to each other." ></td>
	<td class="line x" title="38:238	Perhaps more importantly, our human experiments involving five users indicate that all of them can easily identify the sentiment dimension based on the features, thus providing suggestive evidence that our method is viable." ></td>
	<td class="line x" title="39:238	In sum, our contributions in this paper are threefold." ></td>
	<td class="line x" title="40:238	First, we propose a novel feedback mechanism for clustering allowing a user to easily specify the dimension along which she wants data points to be clustered and apply the mechanism to the challenging, yet under-investigated problem of sentiment-based clustering." ></td>
	<td class="line x" title="41:238	Second, spectral learning, which is the core of our method, has not been applied extensively to NLP problems, and we hope that our work can increase the awareness of this powerful machine learning technique in the NLP community." ></td>
	<td class="line x" title="42:238	Finally, we demonstrate the viability of our method not only by evaluating its performance on sentiment datasets, but also via a set of human experiments, which is typically absent in papers that involve algorithms for incorporating user feedback." ></td>
	<td class="line x" title="43:238	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="44:238	Section 2 presents the basics of spectral clustering, which will facilitate the discussion of our feedback mechanism in Section 3." ></td>
	<td class="line x" title="45:238	We describe our human experiments and evaluation results on several sentiment datasets in Section 4, and present our conclusions in Section 5." ></td>
	<td class="line x" title="46:238	2 Spectral Clustering When given a clustering task, an important question to ask is: which clustering algorithm should we use?" ></td>
	<td class="line x" title="47:238	A popular choice is k-means." ></td>
	<td class="line x" title="48:238	Nevertheless, it is well-known that k-means has the major drawback of not being able to separate data points 581 that are not linearly separable in the given feature space (e.g., see Dhillon et al.(2004) and Cai et al.(2005))." ></td>
	<td class="line x" title="51:238	Spectral clustering algorithms were developed in response to this problem with k-means." ></td>
	<td class="line x" title="52:238	The central idea behind spectral clustering is to (1) construct a low-dimensional space from the original (typically high-dimensional) space while retaining as much information about the original space as possible, and (2) cluster the data points in this low-dimensional space." ></td>
	<td class="line x" title="53:238	The rest of this section provides the details of spectral clustering." ></td>
	<td class="line x" title="54:238	2.1 Algorithm Although there are several well-known spectral clustering algorithms in the literature (e.g., Weiss (1999), Shi and Malik (2000), Kannan et al.(2004)), we adopt the one proposed by Ng et al.(2002), as it is arguably the most widely-used." ></td>
	<td class="line x" title="57:238	The algorithm takes as input a similarity matrix S created by applying a user-defined similarity function to each pair of data points." ></td>
	<td class="line x" title="58:238	Below are the main steps of the algorithm: 1." ></td>
	<td class="line x" title="59:238	Create the diagonal matrix D whose (i,i)th entry is the sum of the i-th row of S, and then construct the Laplacian matrix L = D1/2SD1/2." ></td>
	<td class="line x" title="60:238	2. Find the eigenvalues and eigenvectors of L. 3." ></td>
	<td class="line x" title="61:238	Create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues.1 4." ></td>
	<td class="line x" title="62:238	Each data point is now rank-reduced to a point in the m-dimensional space." ></td>
	<td class="line x" title="63:238	Normalize each point to unit length (while retaining the sign of each value)." ></td>
	<td class="line x" title="64:238	5." ></td>
	<td class="line x" title="65:238	Cluster the resulting data points using kmeans." ></td>
	<td class="line x" title="66:238	In essence, each dimension in the reduced space is defined by exactly one eigenvector." ></td>
	<td class="line x" title="67:238	The reason why eigenvectors with large eigenvalues are used is that they capture the largest variance in the data." ></td>
	<td class="line x" title="68:238	As a result, each of them can be thought of as revealing an important dimension of the data." ></td>
	<td class="line x" title="69:238	2.2 Clustering with Eigenvectors As Ng et al.(2002) point out, different authors still disagree on which eigenvectors to use, and how to derive clusters from them." ></td>
	<td class="line x" title="71:238	There are two common methods for deriving clusters using the eigenvectors." ></td>
	<td class="line x" title="72:238	These methods will serve as our baselines in our evaluation." ></td>
	<td class="line x" title="73:238	1For brevity, we will refer to the eigenvector with the n-th largest eigenvalue simply as the n-th eigenvector." ></td>
	<td class="line x" title="74:238	Method 1: Using the second eigenvector only The first method is to use only the second eigenvector, e2, to partition the points." ></td>
	<td class="line x" title="75:238	Besides revealing one of the most important dimensions of the data, this eigenvector induces an intuitively ideal partition of the data  the partition induced by the minimum normalized cut of the similarity graph2, where the nodes are the data points and the edge weights are the pairwise similarity values of the points (Shi and Malik, 2000)." ></td>
	<td class="line x" title="76:238	Clustering in a onedimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points." ></td>
	<td class="line x" title="77:238	However, we follow Ng et al.(2002) and cluster using 2-means in this one-dimensional space." ></td>
	<td class="line x" title="79:238	Method 2: Using m eigenvectors Recall from Section 2.1 that after eigendecomposing the Laplacian matrix, each data point is represented by m co-ordinates." ></td>
	<td class="line x" title="80:238	In the second method, we simply use 2-means to cluster the data points in this m-dimensional space, effectively exploiting all of the m eigenvectors." ></td>
	<td class="line x" title="81:238	3 Our Approach As mentioned before, sentiment-based clustering is challenging, in part due to the fact that the reviews can be clustered along more than one dimension." ></td>
	<td class="line x" title="82:238	In this section, we propose and incorporate a user feedback mechanism into a spectral clustering algorithm, which makes it easy for a user to specify the dimension along which she wants to cluster the data points." ></td>
	<td class="line x" title="83:238	Recall that our method first applies spectral clustering to reveal the most important dimensions of the data, and then lets the user select the desired dimension." ></td>
	<td class="line x" title="84:238	To motivate the importance of user feedback, it helps to understand why the two baseline clustering algorithms described in Section 2.2, which are also based on spectral methods but do not rely on user feedback, may not always yield a sentiment-based clustering." ></td>
	<td class="line x" title="85:238	To begin with, consider the first method, where only the second eigenvector is used to induce the partition." ></td>
	<td class="line x" title="86:238	Recall that the second eigenvector reveals the most prominent dimension of the data." ></td>
	<td class="line x" title="87:238	Hence, if sentiment is not the most prominent dimension (which can happen if the non-sentiment-bearing 2Using the normalized cut (as opposed to the usual cut) ensures that the size of the two clusters are relatively balanced, avoiding trivial cuts where one cluster is empty and the other is full." ></td>
	<td class="line x" title="88:238	See Shi and Malik (2000) for details." ></td>
	<td class="line x" title="89:238	582 words outnumber the sentiment-bearing words in the bag-of-words representation of a review), then the resulting clustering of the reviews may not be sentiment-oriented." ></td>
	<td class="line x" title="90:238	A similar line of reasoning can be used to explain why the second baseline clustering algorithm, which clusters based on all of the eigenvectors in the low-dimensional space, may not always work well." ></td>
	<td class="line x" title="91:238	Since each eigenvector corresponds to a different dimension (and, in particular, some of them correspond to non-sentiment dimensions), using all of them to represent a review may hamper the accurate computation of the similarity of two reviews as far as clustering along the sentiment dimension is concerned." ></td>
	<td class="line x" title="92:238	In the rest of this section, we discuss the major steps of our user-feedback mechanism in detail." ></td>
	<td class="line x" title="93:238	Step 1: Identify the important dimensions To identify the important dimensions of the given reviews, we take the top eigenvectors computed from the eigen-decomposition of the Laplacian matrix, which is in turn formed from the input similarity matrix." ></td>
	<td class="line x" title="94:238	We compute the similarity between two reviews by taking the dot product of their feature vectors (see Section 4.1 for details on feature vector generation)." ></td>
	<td class="line x" title="95:238	Following Ng et al., we set the diagonal entries of the similarity matrix to 0." ></td>
	<td class="line x" title="96:238	Step 2: Identify the relevant features Given the eigen-decomposition from Step 1, we first obtain the second through the fifth eigenvectors3, which as mentioned above, correspond to the most important dimensions of the data." ></td>
	<td class="line x" title="97:238	Then, we ask the user to select one of the four dimensions defined by these eigenvectors according to their relevance to sentiment." ></td>
	<td class="line x" title="98:238	One way to do this is to (1) induce one partition of the reviews from each of the four eigenvectors, using a procedure identical to Method 1 in Section 2.2, and (2) have the user inspect the four partitions and decide which corresponds most closely to a sentimentbased clustering." ></td>
	<td class="line x" title="99:238	The main drawback associated with this kind of user feedback is that the user may have to read a large number of reviews in order to make a decision." ></td>
	<td class="line x" title="100:238	Hence, to reduce human effort, we employ an alternative procedure: we (1) identify the most informative features for characterizing each partition, and (2) have the user inspect just the features rather than the reviews." ></td>
	<td class="line x" title="101:238	While traditional feature selection techniques such as log-likelihood ratio and information 3The first eigenvector is not used because it is a constant vector, meaning that it cannot be used to partition the data." ></td>
	<td class="line x" title="102:238	gain can be applied to identify these informative features (see Yang and Pedersen (1997) for an overview), we employ a more sophisticated feature-ranking method that we call maximum margin feature ranking (MMFR)." ></td>
	<td class="line x" title="103:238	Recall that a maximum margin classifier (e.g., a support vector machine) separates data points from two classes while maximizing the margin of separation." ></td>
	<td class="line x" title="104:238	Specifically, a maximum margin hyperplane is defined by w  x  b = 0, where x is a feature vector representing an arbitrary data point, and w (a weight vector) and b (a scalar) are parameters that are learned by solving the following constrained optimization problem: argmin 12bardblwbardbl2 + C summationdisplay i i subject to ci(wxi b)  1i, 1  i  n, where ci  {+1,1} is the class of the i-th training point xi, i is the degree of misclassification of xi, and C is a regularization parameter that balances training error and model complexity." ></td>
	<td class="line x" title="105:238	We use w to identify the most informative features for a partition." ></td>
	<td class="line x" title="106:238	Note that a feature with a large positive weight is strongly indicative of the positive class, whereas a feature with a large negative weight is strongly indicative of the negative class." ></td>
	<td class="line x" title="107:238	In other words, the most informative features are those with large absolute weight values." ></td>
	<td class="line x" title="108:238	We exploit this observation and identify the most informative features for a partition by (1) training an SVM classifier4 on the partition, where data points in the same cluster belong to the same class; (2) sorting the features according to the SVMlearned feature weights; and (3) generating two ranked lists of informative features using the top and bottom 100 features, respectively." ></td>
	<td class="line x" title="109:238	Given the ranked lists generated for each of the four partitions, the user will select one of the partitions/dimensions as most relevant to sentiment by inspecting as many features in the ranked lists as needed." ></td>
	<td class="line x" title="110:238	After picking the most relevant dimension, the user will label one of the two feature lists associated with this dimension as POSITIVE and the other as NEGATIVE." ></td>
	<td class="line x" title="111:238	Since each feature list represents one of the clusters, the cluster associated with the positive list is labeled POSITIVE and 4All the SVM classifiers in this paper are trained using the SVMlight package (Joachims, 1999), with the learning parameters set to their default values." ></td>
	<td class="line x" title="112:238	583 the cluster associated with the negative list is labeled NEGATIVE." ></td>
	<td class="line x" title="113:238	In comparison to existing user feedback mechanisms for assisting a clustering algorithm, ours requires comparatively little human intervention: we only require that the user select a dimension by examining a small number of features, as opposed to having the user construct the feature space or identify clusters that need to be merged or split as is required with other methods." ></td>
	<td class="line x" title="114:238	Step 3: Identify the unambiguous reviews There is a caveat, however." ></td>
	<td class="line x" title="115:238	As mentioned in the introduction, many reviews contain both positive and negative sentiment-bearing words." ></td>
	<td class="line x" title="116:238	These ambiguous reviews are more likely to be clustered incorrectly than their unambiguous counterparts." ></td>
	<td class="line x" title="117:238	Now, since the ranked lists of features are derived from the partition, the presence of these ambiguous reviews can adversely affect the identification of informative features using MMFR." ></td>
	<td class="line x" title="118:238	As a result, we remove the ambiguous reviews before deriving informative features from a partition." ></td>
	<td class="line x" title="119:238	We employ a simple method for identifying unambiguous reviews." ></td>
	<td class="line x" title="120:238	In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity." ></td>
	<td class="line x" title="121:238	Ambiguous data points receive the orthogonal projections from both the positive and negative data points, and hence they have near zero values in the pivot eigenvectors." ></td>
	<td class="line x" title="122:238	We exploit this important information." ></td>
	<td class="line x" title="123:238	The basic idea is that the data points with near zero values in the eigenvectors are more ambiguous than those with large absolute values." ></td>
	<td class="line x" title="124:238	As a result, we posit 250 reviews from each cluster whose corresponding values in the eigenvector are farthest away from zero as unambiguous, and induce the ranked list of features only from the resulting 500 unambiguous reviews.5 Step 4: Cluster along the selected dimension Finally, we employ the 2-means algorithm to cluster all the reviews along the dimension (i.e., the eigenvector) selected by the user, regardless of whether a review is ambiguous or not." ></td>
	<td class="line x" title="125:238	5Note that 500 is a somewhat arbitrary choice." ></td>
	<td class="line x" title="126:238	Underlying this choice is our assumption that a fraction of the reviews is unambiguous." ></td>
	<td class="line x" title="127:238	As we will see in the evaluation section, these 500 reviews can be classified with a high accuracy; consequently, the features induced from the resulting clusters are also of high quality." ></td>
	<td class="line x" title="128:238	Additional experiments reveal that the list of top-ranking features does not change significantly when induced from a smaller number of unambiguous reviews." ></td>
	<td class="line x" title="129:238	4 Evaluation 4.1 Experimental Setup Datasets." ></td>
	<td class="line x" title="130:238	We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al., 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al., 2007)." ></td>
	<td class="line x" title="131:238	Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives)." ></td>
	<td class="line x" title="132:238	To illustrate the difference between topic-based clustering and sentiment-based clustering, we will also show topic-based clustering results on POL, a dataset created by taking all the documents from two sections of 20 Newsgroups, namely, sci.crypt and talks.politics." ></td>
	<td class="line x" title="133:238	To preprocess a document, we first tokenize and downcase it, and then represent it as a vector of unigrams, using frequency as presence." ></td>
	<td class="line x" title="134:238	In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in only a single review." ></td>
	<td class="line x" title="135:238	Following the common practice in the information retrieval community, we also exclude words with high document frequency, many of which are stopwords or domainspecific general-purpose words (e.g., movies in the movie domain)." ></td>
	<td class="line x" title="136:238	A preliminary examination of our evaluation datasets reveals that these words typically comprise 12% of a vocabulary." ></td>
	<td class="line x" title="137:238	The decision of exactly how many terms to remove from each dataset is subjective: a large corpus typically requires more removals than a small corpus." ></td>
	<td class="line x" title="138:238	To be consistent, we simply sort the vocabulary by document frequency and remove the top 1.5%." ></td>
	<td class="line x" title="139:238	Evaluation metrics." ></td>
	<td class="line x" title="140:238	We employ two evaluation metrics." ></td>
	<td class="line x" title="141:238	First, we report results in terms of the accuracy achieved on the 2000 labeled reviews for each dataset." ></td>
	<td class="line x" title="142:238	Second, following Kamvar et al.(2003), we evaluate the clusters produced by our approach against the gold-standard clusters using the Adjusted Rand Index (ARI)." ></td>
	<td class="line x" title="144:238	ARI ranges from 1 to 1; better clusterings have higher ARI values." ></td>
	<td class="line x" title="145:238	4.2 Baseline Systems Clustering using the second eigenvector only." ></td>
	<td class="line x" title="146:238	As our first baseline, we adopt Shi and Maliks approach and cluster the reviews using only the second eigenvector, e2, as described in Section 2.2." ></td>
	<td class="line x" title="147:238	Results on POL and the five sentiment datasets are 584 Accuracy Adjusted Rand Index System Variation POL MOV KIT BOO DVD ELE POL MOV KIT BOO DVD ELE Baseline: 2nd eigenvector 93.7 70.9 69.7 58.9 55.3 50.8 0.76 0.17 0.15 0.03 0.01 0.01 Baseline: m eigenvectors 95.9 59.3 63.2 60.1 62.5 63.8 0.84 0.03 0.07 0.04 0.06 0.08 Our approach 93.7 70.9 69.7 69.5 70.8 65.8 0.76 0.17 0.15 0.15 0.17 0.10 Table 1: Results in terms of accuracy and Adjusted Rand Index for the six datasets." ></td>
	<td class="line x" title="148:238	shown in row 1 of Table 1.6 As we can see, this baseline achieves an accuracy of 90% on POL, but a much lower accuracy (of 5070%) on the sentiment datasets." ></td>
	<td class="line x" title="149:238	The same performance trend can be observed with ARI." ></td>
	<td class="line x" title="150:238	These results provide support for the claim that sentiment-based clustering is more difficult than topic-based clustering." ></td>
	<td class="line x" title="151:238	In addition, it is worth noting that the baseline achieves much lower accuracies and ARI values on BOO, DVD, and ELE than on the remaining two sentiment datasets." ></td>
	<td class="line x" title="152:238	Since e2 captures the most prominent dimension, these results suggest that sentiment dimension is not the most prominent dimension in these three datasets." ></td>
	<td class="line x" title="153:238	In fact, this is intuitively plausible." ></td>
	<td class="line x" title="154:238	For instance, in the book domain, positive book reviews typically contain a short description of the content, with the reviewer only briefly expressing her sentiment somewhere in the review." ></td>
	<td class="line x" title="155:238	Similarly for the electronics domain: electronic product reviews are typically aspect-oriented, with the reviewer talking about the pros and cons of each aspect of the product (e.g., battery, durability)." ></td>
	<td class="line x" title="156:238	Since the reviews are likely to contain both positive and negative sentiment-bearing words, the sentiment-based clustering is unlikely to be captured by e2." ></td>
	<td class="line x" title="157:238	Clustering using top five eigenvectors." ></td>
	<td class="line x" title="158:238	As our second baseline, we represent each data point using the top five eigenvectors (i.e., e1 through e5), and cluster them using 2-means in this 5dimensional space, as described in Section 2.2." ></td>
	<td class="line x" title="159:238	Hence, this can be thought of as an ensemble approach, where the clustering decision is collectively made by the five eigenvectors." ></td>
	<td class="line x" title="160:238	Results are shown in row 2 of Table 1." ></td>
	<td class="line x" title="161:238	In comparison to the first baseline, we see improvements in accuracy and ARI for the three datasets on which the first baseline performs poorly (i.e., BOO, DVD, and ELE), with the most drastic improvement observed on ELE." ></td>
	<td class="line x" title="162:238	On the other hand, performance on the remaining two senti6Owing to the randomness in the choice of seeds for 2means, these and all other experimental results involving 2means are averaged over ten independent runs." ></td>
	<td class="line x" title="163:238	ment datasets deteriorates." ></td>
	<td class="line x" title="164:238	These results can be attributed to the fact that for BOO, DVD, and ELE, e2 does not capture the sentiment dimension, but since some other eigenvector in the ensemble does, we see improvements." ></td>
	<td class="line x" title="165:238	On the other hand, e2 has already captured the sentiment dimension in MOV and KIT; as a result, employing additional dimensions, which may not be sentiment-related, may only introduce noise into the computation of the similarities between the reviews." ></td>
	<td class="line x" title="166:238	4.3 Our Approach Human experiments." ></td>
	<td class="line x" title="167:238	Unlike the two baselines, our approach requires users to specify which of the four dimensions (defined by the second through fifth eigenvectors) are most closely related to sentiment by inspecting a set of features derived from the unambiguous reviews for each dimension using MMFR." ></td>
	<td class="line x" title="168:238	To better understand how easy it is for a human to select the desired dimension given the features, we performed the experiment independently with five humans (all of whom are computer science graduate students not affiliated with this research) and computed the agreement rate." ></td>
	<td class="line x" title="169:238	More specifically, for each dataset, we showed each human judge the top 100 features for each cluster according to MMFR (see Tables 46 for a snippet)." ></td>
	<td class="line x" title="170:238	In addition, we informed them of the intended dimension: for example, for POL, the judge was told that the intended clustering is Politics vs. Science." ></td>
	<td class="line x" title="171:238	Also, if she determined that more than one dimension was relevant to the intended clustering, she was instructed to rank these dimensions in terms of their degree of relevance, where the most relevant one would appear first in the list." ></td>
	<td class="line x" title="172:238	The dimensions (expressed in terms of the IDs of the eigenvectors) selected by each of the five judges for each dataset are shown in Table 2." ></td>
	<td class="line x" title="173:238	The agreement rate (shown in the last row of the table) was computed based on only the highestranked dimension selected by each judge." ></td>
	<td class="line x" title="174:238	As we can see, perfect agreement is achieved for four of the five sentiment datasets, and for the remaining two datasets, near-perfect agreement is achieved." ></td>
	<td class="line x" title="175:238	585 Judge POL MOV KIT BOO DVD ELE 1 2,3,4 2 2 4 3 3 2 2,4 2 2 4 3 3 3 4 2,4 4 4 3 3 4 2,3 2 2 4 3 3,4 5 2 2 2 4 3 3 Agr 80% 100% 80% 100% 100% 100% Table 2: Human agreement rate." ></td>
	<td class="line x" title="176:238	POL MOV KIT BOO DVD ELE Acc 99.8 87.0 87.6 86.2 87.4 77.6 Table 3: Accuracies on unambiguous documents." ></td>
	<td class="line x" title="177:238	These results together with the fact that it took 5 6 minutes to identify the relevant dimension, indicate that asking a human to determine the intended dimension based on solely the informative features is a viable task." ></td>
	<td class="line x" title="178:238	Clustering results." ></td>
	<td class="line x" title="179:238	Next, we cluster all 2000 documents for each dataset using the dimension selected by the majority of the human judges." ></td>
	<td class="line x" title="180:238	The clustering results are shown in row 3 of Table 1." ></td>
	<td class="line x" title="181:238	In comparison to the better baseline for each dataset, we see that our approach performs substantially better on BOO, DVD and ELE, at almost the same level on MOV and KIT, but slightly worse on POL." ></td>
	<td class="line x" title="182:238	Note that the improvements observed for BOO, DVD and ELE can be attributed to the failure of e2 to capture the sentiment dimension." ></td>
	<td class="line x" title="183:238	Perhaps most importantly, by exploiting human feedback, our approach has achieved more stable performance across the datasets than the baselines, with accuracies ranging from 65.8% to 93.7% and ARI ranging from 0.10 to 0.76." ></td>
	<td class="line x" title="184:238	Role of unambiguous documents." ></td>
	<td class="line x" title="185:238	Recall that the features with the largest MMFR were computed from the unambiguous documents only." ></td>
	<td class="line x" title="186:238	To get an intuitive understanding of the role of unambiguous documents in our approach, we show in Table 3 the accuracy when the unambiguous documents in each dataset were clustered using the eigenvector selected by the majority of the judges." ></td>
	<td class="line x" title="187:238	As we can see, the accuracy of each dataset is higher than the corresponding accuracy shown in row 3 of Table 1." ></td>
	<td class="line x" title="188:238	In fact, an accuracy of more than 85% was achieved on all but one dataset." ></td>
	<td class="line x" title="189:238	This suggests that our method of identifying unambiguous documents is useful." ></td>
	<td class="line x" title="190:238	Note that it is crucial to be able to achieve a high accuracy on the unambiguous documents: if clustering accuracy is low, the features induced from the clusters may not be an accurate representation of the corresponding dimension, and the human judge may have a difficult time identifying the intended dimension." ></td>
	<td class="line x" title="191:238	In fact, some human judges reported difficulty in identifying the correct dimension for the ELE dataset, and this can be attributed in part to the low accuracy achieved on the unambiguous documents." ></td>
	<td class="line x" title="192:238	Features as summary." ></td>
	<td class="line x" title="193:238	Recall that the method we proposed represents each dimension with a small number of features and asks a user to select the desired dimension by inspecting the corresponding feature lists." ></td>
	<td class="line x" title="194:238	In other words, each feature list serves as a summary of its corresponding dimension, and inspecting the features induced for each dimension can give us insights into the different dimensions of a dataset." ></td>
	<td class="line x" title="195:238	Hence, if a user is not sure how she wants the data points to be clustered (due to lack of knowledge of the data, for instance), our automatically induced features may serve as an overview of the different dimensions of the data." ></td>
	<td class="line x" title="196:238	To better understand whether these features can indeed provide a user with additional useful information about a dataset, we show in Tables 46 the top ten features induced for each cluster and each dimension for the six datasets." ></td>
	<td class="line x" title="197:238	As an example, consider the MOV dataset." ></td>
	<td class="line x" title="198:238	Inspecting the induced features, we can determine that it has a sentiment dimension (e2), as well as a humor vs. thriller dimension (e4)." ></td>
	<td class="line x" title="199:238	In other words, if we cluster along e2, we get a sentiment-based clustering; and if we cluster along e4, we obtain a genre-based (humor vs. thriller) clustering." ></td>
	<td class="line x" title="200:238	User feedback vs. labeled data." ></td>
	<td class="line x" title="201:238	Recall that our two baselines are unsupervised, whereas our approach can be characterized as semi-supervised, as it relies on user feedback to select the intended dimension." ></td>
	<td class="line x" title="202:238	Hence, it should not be surprising to see that the average clustering performance of our approach is better than that of the baselines." ></td>
	<td class="line x" title="203:238	To do a fairer comparison, we conduct another experiment in which we compare our approach against a semi-supervised sentiment classification system, which uses transductive SVM as the underlying semi-supervised learner." ></td>
	<td class="line x" title="204:238	More specifically, the goal of this experiment is to determine how many labeled documents are needed in order for the transductive learner to achieve the same level of performance as our approach." ></td>
	<td class="line x" title="205:238	To answer this question, we first give the transductive learner access to the 2000 documents for each dataset as 586 POL MOV e2 e3 e4 e5 e2 e3 e4 e5 C1 C1 C1 C1 C1 C1 C1 C1 serder beyer serbs escrow relationship production jokes starts armenian arabs palestinians serial son earth kids person turkey andi muslims algorithm tale sequences live saw armenians research wrong chips husband aliens animation feeling muslims israelis department ensure perfect war disney lives sdpa tim bosnia care drama crew animated told argic uci live strong focus alien laughs happen davidian ab matter police strong planet production am dbd@ura z@virginia freedom omissions beautiful horror voice felt troops holocaust politics excepted nature evil hilarious happened C2 C2 C2 C2 C2 C2 C2 C2 sternlight escrow standard internet worst sex thriller comic wouldn sternlight sternlight uucp stupid romantic killer sequences pgp algorithm des uk waste school murder michael crypto access escrow net bunch relationship crime supporting algorithm net employer quote wasn friends police career isn des net ac video jokes car production likely privacy york co worse laughs dead peter access uk jake didn boring sexual killed style idea systems code ai guess cute starts latest cryptograph pgp algorithm mit anyway mother violence entertaining Table 4: Top ten features induced for each dimension for the POL and MOV domains." ></td>
	<td class="line x" title="206:238	The shaded columns correspond to the dimensions selected by the human judges." ></td>
	<td class="line x" title="207:238	e2, , e5 are the top eigenvectors; C1 and C2 are the clusters." ></td>
	<td class="line x" title="208:238	BOO ELE e2 e3 e4 e5 e2 e3 e4 e5 C1 C1 C1 C1 C1 C1 C1 C1 history series loved must mouse music easy amazon must man highly wonderful cable really used cable modern history easy old cables ipod card card important character enjoyed feel case too fine recommend text death children away red little using dvd reference between again children monster headphones problems camera excellent war although year picture hard fine fast provides seems excellent someone kit excellent drive far business political understand man overall need computer printer both american three made paid fit install picture C2 C2 C2 C2 C2 C2 C2 C2 plot buy money boring working worked money phone didn bought bad series never problem worth off thought information nothing history before never amazon worked boring easy waste pages phone item over power got money buy information days amazon return battery character recipes anything between headset working years unit couldn pictures doesn highly money support much set ll look already page months months headphones phones ending waste instead excellent return returned sony range fan copy seems couldn second another received little Table 5: Top ten features induced for each dimension for the BOO and ELE domains." ></td>
	<td class="line x" title="209:238	The shaded columns correspond to the dimensions selected by the human judges." ></td>
	<td class="line x" title="210:238	e2, , e5 are the top eigenvectors; C1 and C2 are the clusters." ></td>
	<td class="line x" title="211:238	unlabeled data." ></td>
	<td class="line x" title="212:238	Next, we randomly sample 50 unlabeled documents and assign them the true label." ></td>
	<td class="line x" title="213:238	We then re-train the classifier and compute its accuracy on the 2000 documents." ></td>
	<td class="line x" title="214:238	We keep adding more labeled data (50 in each iteration) until it reaches the accuracy achieved by our system." ></td>
	<td class="line x" title="215:238	Results of this experiment are shown in Table 7." ></td>
	<td class="line x" title="216:238	Owing in the randomness involved in the selection of unlabeled documents, these results are averaged over ten independent runs." ></td>
	<td class="line x" title="217:238	As we can see, our 587 KIT DVD e2 e3 e4 e5 e2 e3 e4 e5 C1 C1 C1 C1 C1 C1 C1 C1 love works really pan worth music video money clean water nice oven bought collection music quality nice clean works cooking series excellent found video size work too made money wonderful feel worth set ice quality pans season must bought found kitchen makes small better fan loved workout version easily thing sturdy heat collection perfect daughter picture sturdy need little cook music highly recommend waste recommend keep think using tv makes our special price best item clean thought special disappointed sound C2 C2 C2 C2 C2 C2 C2 C2 months price ve love young worst series saw still item years coffee between money cast watched back set love too actors thought fan loved never ordered never recommend men boring stars enjoy worked amazon clean makes cast nothing original whole money gift months over seems minutes comedy got did got over size job waste actors family amazon quality pan little beautiful saw worth series return received been maker around pretty classic season machine knives pans cup director reviews action liked Table 6: Top ten features induced for each dimension for the KIT and DVD domains." ></td>
	<td class="line x" title="218:238	The shaded columns correspond to the dimensions selected by the human judges." ></td>
	<td class="line x" title="219:238	e2, , e5 are the top eigenvectors; C1 and C2 are the clusters." ></td>
	<td class="line x" title="220:238	POL MOV KIT BOO DVD ELE # labels 400 150 200 350 350 200 Table 7: Transductive SVM results." ></td>
	<td class="line x" title="221:238	user feedback is equivalent to the effort of handannotating 275 documents per dataset on average." ></td>
	<td class="line x" title="222:238	Multiple relevant dimensions." ></td>
	<td class="line x" title="223:238	As seen from Table 2, some human judges selected more than one dimension for some datasets (e.g., 2,3,4 for POL; 2,4 for MOV; and 3,4 for ELE)." ></td>
	<td class="line x" title="224:238	However, we never took into account these extra dimensions in our previous experiments." ></td>
	<td class="line x" title="225:238	To better understand whether these extra dimensions can help improve accuracy and ARI, we conduct another experiment in which we apply 2-means to cluster the documents in a space that is defined by all of the selected dimensions." ></td>
	<td class="line x" title="226:238	The final accuracy turns out to be 95.9%, 70.9%, and 67.5% for POL, MOV, and ELE respectively, which is considerably better than using only the optimal dimension and suggests that the extra dimensions contain useful information." ></td>
	<td class="line x" title="227:238	5 Conclusions Unsupervised clustering algorithms typically group objects along the most prominent dimension, in part owing to their objective of simultaneously maximizing inter-cluster similarity and intra-cluster dissimilarity." ></td>
	<td class="line x" title="228:238	Hence, if the users intended clustering dimension is not the most prominent dimension, these unsupervised clustering algorithms will fail miserably." ></td>
	<td class="line x" title="229:238	To address this problem, we proposed to integrate a novel user feedback mechanism into a spectral clustering algorithm, which allows us to mine the intended, possibly hidden, dimension of the data and produce the desired clustering." ></td>
	<td class="line x" title="230:238	This mechanism differs from competing methods in that it requires very limited feedback: to select the intended dimension, the user only needs to inspect a small number of features." ></td>
	<td class="line x" title="231:238	We demonstrated its viability via a set of human and automatic experiments with unsupervised sentiment classification, obtaining promising results." ></td>
	<td class="line x" title="232:238	In future work, we plan to explore several extensions to our proposed method." ></td>
	<td class="line x" title="233:238	First, we plan to use our user-feedback method in combination with existing methods (e.g., Bekkerman et al.(2007)) for improving its performance." ></td>
	<td class="line x" title="235:238	For instance, instead of having the user construct a relevant feature space from scratch, she can simply extend the set of informative features identified for the user-selected dimension." ></td>
	<td class="line x" title="236:238	Second, since none of the steps in our method is specifically designed for sentiment classification, we plan to apply it to other non-topic-based text classification tasks." ></td>
	<td class="line x" title="237:238	588 Acknowledgments We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper." ></td>
	<td class="line x" title="238:238	This work was supported in part by NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1063
Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus
Mohammad, Saif;Dunne, Cody;Dorr, Bonnie Jean;"></td>
	<td class="line x" title="1:211	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599608, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:211	c 2009 ACL and AFNLP Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus Saif Mohammad, Cody Dunnediamondmath, and Bonnie Dorr Laboratory for Computational Linguistics and Information Processing Human-Computer Interaction Labdiamondmath Institute for Advanced Computer Studies Department of Computer Science, University of Maryland." ></td>
	<td class="line x" title="3:211	Human Language Technology Center of Excellence {saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.edu Abstract Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words." ></td>
	<td class="line x" title="4:211	A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora." ></td>
	<td class="line x" title="5:211	Most of these methods use WordNet." ></td>
	<td class="line x" title="6:211	In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes." ></td>
	<td class="line x" title="7:211	Further, the lexicon has properties that support the Polyanna Hypothesis." ></td>
	<td class="line x" title="8:211	Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet)." ></td>
	<td class="line x" title="9:211	In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other." ></td>
	<td class="line x" title="10:211	Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above." ></td>
	<td class="line x" title="11:211	1 Introduction Sentiment analysis involves determining the opinions and private states (beliefs, emotions, speculations, and so on) of the speaker (Wiebe, 1994)." ></td>
	<td class="line x" title="12:211	It has received significant attention in recent years due to increasing online opinion content and applications in tasks such as automatic product recommendation systems (Tatemura, 2000; Terveen et al., 1997), question answering (Somasundaran et al., 2007; Lita et al., 2005), and summarizing multiple view points (Seki et al., 2004) and opinions (Mohammad et al., 2008a)." ></td>
	<td class="line x" title="13:211	A crucial sub-problem is to determine whether positive or negative sentiment is expressed." ></td>
	<td class="line oc" title="14:211	Automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation (Turney, 2002; Wilson et al., 2005; Pang and Lee, 2008)." ></td>
	<td class="line x" title="15:211	A word is said to have a positive semantic orientation (SO) (or polarity) if it is often used to convey favorable sentiment or evaluation of the topic under discussion." ></td>
	<td class="line x" title="16:211	Some example words that have positive semantic orientation are excellent, happy, honest, and so on." ></td>
	<td class="line x" title="17:211	Similarly, a word is said to have negative semantic orientation if it is often used to convey unfavorable sentiment or evaluation of the target." ></td>
	<td class="line x" title="18:211	Examples include poor, sad, and dishonest." ></td>
	<td class="line x" title="19:211	Certain semantic orientation lexicons have been manually compiled for Englishthe most notable being the General Inquirer (GI) (Stone et al., 1966).1 However, the GI lexicon has orientation labels for only about 3,600 entries." ></td>
	<td class="line x" title="20:211	The Pittsburgh subjectivity lexicon (PSL) (Wilson et al., 2005), which draws from the General Inquirer and other sources, also has semantic orientation labels, but only for about 8,000 words." ></td>
	<td class="line x" title="21:211	Automatic approaches to creating a semantic orientation lexicon and, more generally, approaches for word-level sentiment annotation can be grouped into two kinds: (1) those that rely on manually created lexical resourcesmost of which use WordNet (Strapparava and Valitutti, 2004; Hu and Liu, 2004; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2006; An1http://www.wjh.harvard.edu/ inquirer 599 dreevskaia and Bergler, 2006; Kanayama and Nasukawa, 2006); and (2) those that rely on text corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004)." ></td>
	<td class="line x" title="22:211	Many of these lexicons, such as SentiWordNet (SWN) (Esuli and Sebastiani, 2006) were created using supervised classifiers and significant manual annotation, whereas others such as the Turney and Littman lexicon (TLL) (2003) were created from very large corpora (more than 100 billion words)." ></td>
	<td class="line x" title="23:211	In contrast, we propose a computationally inexpensive method to compile a high-coverage semantic orientation lexicon without the use of any text corpora or manually annotated semantic orientation labels." ></td>
	<td class="line x" title="24:211	Both of these resources may be used, if available, to further improve results." ></td>
	<td class="line x" title="25:211	The lexicon has about twenty times the number of entries in the GI lexicon, and it includes entries for both individual words and common multiword expressions." ></td>
	<td class="line x" title="26:211	The method makes use of a Roget-like thesaurus and a handful of antonymgenerating affix patterns." ></td>
	<td class="line x" title="27:211	Whereas thesauri have long been used to estimate semantic distance (Jarmasz and Szpakowicz, 2003; Mohammad and Hirst, 2006), the closest thesaurus-based work on sentiment analysis is by Aman and Szpakowicz (2007) on detecting emotions such as happiness, sadness, and anger." ></td>
	<td class="line x" title="28:211	We evaluated our thesaurusbased algorithm both intrinsically and extrinsically and show that the semantic orientation lexicon it generates has significantly more correct entries than the state-of-the-art high-coverage lexicon SentiWordNet, and that it has a significantly higher coverage than the General Inquirer and TurneyLittman lexicons." ></td>
	<td class="line x" title="29:211	In Section 2 we examine related work." ></td>
	<td class="line x" title="30:211	Section 3 presents our algorithm for creating semantic orientation lexicons." ></td>
	<td class="line x" title="31:211	We describe intrinsic and extrinsic evaluation experiments in Section 4, followed by a discussion of the results in Section 5." ></td>
	<td class="line x" title="32:211	Additionally, in Section 6 we show preliminary visualizations of how our algorithm forms chains of positive and negative thesaurus categories." ></td>
	<td class="line x" title="33:211	Good visualizations are not only effective in presenting information to the user, but also help us better understand our algorithm." ></td>
	<td class="line x" title="34:211	Section 7 has our conclusions." ></td>
	<td class="line x" title="35:211	2 Related Work Pang and Lee (2008) provide an excellent survey of the literature on sentiment analysis." ></td>
	<td class="line x" title="36:211	Here we briefly describe the work closest to ours." ></td>
	<td class="line x" title="37:211	Hatzivassiloglou and McKeown (1997) proposed a supervised algorithm to determine the semantic orientation of adjectives." ></td>
	<td class="line x" title="38:211	They first generate a graph that has adjectives as nodes." ></td>
	<td class="line x" title="39:211	An edge between two nodes indicates either that the two adjectives have the same or opposite semantic orientation." ></td>
	<td class="line x" title="40:211	A clustering algorithm partitions the graph into two subgraphs such that the nodes in a subgraph have the same semantic orientation." ></td>
	<td class="line x" title="41:211	The subgraph with adjectives that occur more often in text is marked positive and the other negative." ></td>
	<td class="line x" title="42:211	They used a 21 million word corpus and evaluated their algorithm on a labeled set of 1336 adjectives (657 positive and 679 negative)." ></td>
	<td class="line x" title="43:211	Our approach does not require manually annotated semantic orientation entries to train on and is much simpler." ></td>
	<td class="line x" title="44:211	Esuli and Sebastiani (2006) used a supervised algorithm to attach semantic orientation scores to WordNet glosses." ></td>
	<td class="line x" title="45:211	They train a set of ternary classifiers using different training data and learning methods." ></td>
	<td class="line x" title="46:211	The set of semantic orientation scores of all WordNet synsets is released by the name SentiWordNet.2 An evaluation of SentiWordNet by comparing orientation scores of about 1,000 WordNet glosses to scores assigned by human annotators is presented in Esuli (2008)." ></td>
	<td class="line x" title="47:211	Our approach uses a Roget-like thesaurus, and it does not use any supervised classifiers." ></td>
	<td class="line x" title="48:211	Turney and Littman (2003) proposed a minimally supervised algorithm to calculate the semantic orientation of a word by determining if its tendency to co-occur with a small set of positive words is greater than its tendency to co-occur with a small set of negative words." ></td>
	<td class="line x" title="49:211	They show that their approach performs better when it has a large amount of text at its disposal." ></td>
	<td class="line x" title="50:211	They use text from 350 million web-pages (more than 100 billion words)." ></td>
	<td class="line x" title="51:211	Our approach does not make use of any text corpora, although co-occurrence statistics could be used to further improve the lexicon." ></td>
	<td class="line x" title="52:211	Furthermore, our lexicon has entries for commonly used multi-word expressions as well." ></td>
	<td class="line x" title="53:211	Mohammad et al.(2008b) developed a method to determine the degree of antonymy (contrast) between two words using the Macquarie The2http://sentiwordnet.isti.cnr.it/ 600 saurus (Bernard, 1986), co-occurrence statistics, and a small set of antonym-generating affix patterns such as XdisX." ></td>
	<td class="line x" title="55:211	Often, one member of a pair of contrasting terms is positive and one member is negative." ></td>
	<td class="line x" title="56:211	In this paper, we describe how a subset of those affix patterns can be used in combination with a thesaurus and the edicts of marking theory to create a large lexicon of words and phrases marked with their semantic orientation." ></td>
	<td class="line x" title="57:211	3 Generating the Semantic Orientation Lexicon Our algorithm to generate a semantic orientation lexicon has two steps: (1) identify a seed set of positive and negative words; (2) use a Roget-like thesaurus to mark the words synonymous with the positive seeds positive and words synonymous with the negative seeds negative." ></td>
	<td class="line x" title="58:211	The two steps are described in the subsections below." ></td>
	<td class="line x" title="59:211	Our implementation of the algorithm used the Macquarie Thesaurus (Bernard, 1986)." ></td>
	<td class="line x" title="60:211	It has about 100,000 unique words and phrases." ></td>
	<td class="line x" title="61:211	3.1 Seed words 3.1.1 Automatically identifying seed words It is known from marking theory that overtly marked words, such as dishonest, unhappy, and impure, tend to have negative semantic orientation, whereas their unmarked counterparts, honest, happy, and pure, tend to have positive semantic orientation (Lehrer, 1974; Battistella, 1990)." ></td>
	<td class="line x" title="62:211	Exceptions such as biasedunbiased and partial impartial do exist, and in some contexts even a predominantly negative marked word may be positive." ></td>
	<td class="line x" title="63:211	For example irreverent is negative in most contexts, but positive in the sentence below: Millions of fans follow Moulders irreverent quest for truth." ></td>
	<td class="line x" title="64:211	However, as we will show through experiments, the exceptions are far outnumbered by those that abide by the predictions of marking theory." ></td>
	<td class="line x" title="65:211	We used a set of 11 antonym-generating affix patterns to generate overtly marked words and their unmarked counterparts (Table 1)." ></td>
	<td class="line x" title="66:211	Similar antonyms-generating affix patterns exist for many languages (Lyons, 1977)." ></td>
	<td class="line x" title="67:211	The 11 chosen affix patterns generated 2,692 pairs of marked and unmarked valid English words that are listed in the Macquarie Thesaurus." ></td>
	<td class="line x" title="68:211	The marked words Affix pattern # word w1 w2 pairs example word pair X disX 382 honestdishonest X imX 196 possibleimpossible X inX 691 consistentinconsistent X malX 28 adroitmaladroit X misX 146 fortunemisfortune X nonX 73 sensenonsense X unX 844 happyunhappy X Xless 208 gutgutless lX illX 25 legalillegal rX irX 48 responsibleirresponsible Xless Xful 51 harmlessharmful Total 2692 Table 1: Eleven affix patterns used to generate the seed set of marked and unmarked words." ></td>
	<td class="line x" title="69:211	Here X stands for any sequence of letters common to both words w1 and w2." ></td>
	<td class="line x" title="70:211	are deemed negative and the unmarked ones positive, and these form our seed set of positive and negative words." ></td>
	<td class="line x" title="71:211	We will refer to this set of orientation-marked words as the affix seeds lexicon (ASL)." ></td>
	<td class="line x" title="72:211	Note that some words may have multiple marked counterparts, for example, trust trustless and trustmistrust." ></td>
	<td class="line x" title="73:211	Thus, ASL has more negative words (2,652) than positive ones (2,379)." ></td>
	<td class="line x" title="74:211	Also, the XlessXful pattern generates word pairs that are both overtly marked; words generated from Xless are deemed negative and words generated from Xful are deemed positive." ></td>
	<td class="line x" title="75:211	It should be noted that the affix patterns used here are a subset of those used in Mohammad et al.(2008b) to generate antonym pairs." ></td>
	<td class="line x" title="77:211	The affix patterns ignored are those that are not expected to generate pairs of words with opposite semantic orientation." ></td>
	<td class="line x" title="78:211	For instance, the pattern imX-exX generates word pairs such as importexport and implicitexplicit that are antonymous, but do not have opposite semantic orientations." ></td>
	<td class="line x" title="79:211	3.1.2 Using manually annotated seed words Since manual semantic orientation labels exist for some English words (the GI lexicon), we investigated their usefulness in further improving the coverage and correctness of the entries in our lexicon." ></td>
	<td class="line x" title="80:211	We used the GI words as seeds in the same way as the words generated from the affix patterns were used (Section 3.1.1)." ></td>
	<td class="line x" title="81:211	3.2 Generalizing from the seeds A published thesaurus such as the Rogets or Macquarie has about 1,000 categories, each consisting of on average 120 words and commonly used 601 Mode of SO lexicon creation Resources used # entries # positives # negatives ASL automatic 11 affix rules 5,031 2,379 (47.3%) 2,652 (52.7%) GI manual human SO annotation 3,605 1,617 (44.9%) 1,988 (55.1%) GI-subset manual human SO annotation 2,761 1,262 (45.7%) 1,499 (54.3%) MSOL(ASL) automatic thesaurus, 11 affix rules 51,157 34,152 (66.8%) 17,005 (33.2%) MSOL(GI) automatic GI, thesaurus 69,971 25,995 (37.2%) 43,976 (62.8%) MSOL(ASL and GI) automatic GI, thesaurus, 11 affix rules 76,400 30,458 (39.9%) 45,942 (60.1%) PSL mostly manual GI, other sources 6,450 2,298 (35.6%) 4,485 (64.4%) SWN automatic human SO annotation, 56,200 47,806 (85.1%) 8,394 (14.9%) WordNet, ternary classifiers TLL automatic 100 billion word corpus, 3,596 1,625 (45.2%) 1,971 (54.8%) minimal human SO annotation Table 2: Key details of semantic orientation (SO) lexicons." ></td>
	<td class="line x" title="82:211	ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN = SentiWordNet, TLL = TurneyLittman lexicon." ></td>
	<td class="line x" title="83:211	multi-word expressions." ></td>
	<td class="line x" title="84:211	Terms within a category tend to be closely related, and they are further grouped into sets of near-synonymous words and phrases called paragraphs." ></td>
	<td class="line x" title="85:211	There are about 10,000 paragraphs in most Roget-like thesauri." ></td>
	<td class="line x" title="86:211	Every thesaurus paragraph is examined to determine if it has a seed word (by looking up the seed lexicon described in Section 3.1)." ></td>
	<td class="line x" title="87:211	If a thesaurus paragraph has more positive seed words than negative seed words, then all the words (and multiword expressions) in that paragraph are marked as positive." ></td>
	<td class="line x" title="88:211	Otherwise, all its words are marked negative." ></td>
	<td class="line x" title="89:211	Note that this method assigns semantic orientation labels to wordthesaurus paragraph pairs." ></td>
	<td class="line x" title="90:211	Thesaurus paragraphs can be thought of as word senses." ></td>
	<td class="line x" title="91:211	A word with multiple meanings is listed in multiple thesaurus paragraphs, and so will be assigned semantic orientation labels for each of these paragraphs." ></td>
	<td class="line x" title="92:211	Thus, the method assigns a semantic orientation to a wordsense combination similar to the SentiWordNet approach and differing from the General Inquirer and TurneyLittman lexicons." ></td>
	<td class="line x" title="93:211	However, in most natural language tasks, the intended sense of the target word is not explicitly marked." ></td>
	<td class="line x" title="94:211	So we generate a word-based lexicon by asking the different senses of a word to vote." ></td>
	<td class="line x" title="95:211	If a word is listed in multiple thesaurus paragraphs, then the semantic orientation label most common to them is chosen as the words label." ></td>
	<td class="line x" title="96:211	We will refer to this set of wordsemantic orientation pairs as the Macquarie Semantic Orientation Lexicon (MSOL)." ></td>
	<td class="line x" title="97:211	A set created from only the affix seeds will be called MSOL(ASL), a set created from only the GI seeds will be called MSOL(GI), and the set created using both affix seeds and GI seeds will be called MSOL(ASL and GI).3 We generated a similar word-based lexicon for SentiWordNet (SWN) by choosing the semantic orientation label most common to the synsets pertaining to a target word." ></td>
	<td class="line x" title="98:211	Table 2 summarizes the details of all the lexicons." ></td>
	<td class="line x" title="99:211	MSOL(ASL and GI) has a much larger percentage of negatives than MSOL(ASL) because GI has a much larger percentage of negative words." ></td>
	<td class="line x" title="100:211	These negative seeds generate many more negative entries in MSOL(ASL and GI)." ></td>
	<td class="line x" title="101:211	Of the 51,157 entries in MSOL(ASL), 47,514 are singleword entries and 3,643 are entries for multi-word expressions." ></td>
	<td class="line x" title="102:211	Of the 69,971 entries in MSOL(GI), 45,197 are single-word entries and 24,774 are entries for common multi-word expressions." ></td>
	<td class="line x" title="103:211	Of the 76,400 entries in MSOL(ASL and GI), 51,208 are single-word entries and 25,192 are entries for common multi-word expressions." ></td>
	<td class="line x" title="104:211	In our evaluation, we used only the single-word entries to maintain a level playing field with other lexicons." ></td>
	<td class="line x" title="105:211	4 Evaluation We evaluated the semantic orientation lexicons both intrinsically (by comparing their entries to the General Inquirer) and extrinsically (by using them in a phrase polarity annotation task)." ></td>
	<td class="line x" title="106:211	4.1 Intrinsic: Comparison with GI Similar to how Turney and Littman (2003) evaluated their lexicon (TLL), we determine if the semantic orientation labels in the automatically generated lexicons match the semantic orientation la3MSOL is publicly available at: www.umiacs.umd.edu/ saif/WebPages/ResearchInterests.html." ></td>
	<td class="line x" title="107:211	602 Lexicon All Positives Negatives MSOL(ASL) 74.3 84.2 65.9 SWN 60.1 86.5 37.9 TLL 83.3 83.8 82.8 Table 3: The percentage of GI-subset entries (all, only the positives, only the negatives) that match those of the automatically generated lexicons." ></td>
	<td class="line x" title="108:211	bels of words in GI." ></td>
	<td class="line x" title="109:211	GI, MSOL(ASL), SWN, and TLL all have 2,761 words in common." ></td>
	<td class="line x" title="110:211	We will call the corresponding 2,761 GI entries the GIsubset." ></td>
	<td class="line x" title="111:211	Table 3 shows the percentage of GI-subset entries that match those of the three automaticallygenerated lexicons (MSOL(ASL), SWN, and TLL)." ></td>
	<td class="line x" title="112:211	(The differences in percentages shown in the table are all statistically significantp < 0.001.)" ></td>
	<td class="line x" title="113:211	We do not show results for MSOL(GI), MSOL(ASL and GI), and the Pittsburgh subjectivity lexicon (PSL) because these lexicons were created using GI entries." ></td>
	<td class="line x" title="114:211	TLL most closely matches the GI-subset, and MSOL matches the GI-subset more closely than SWN with the GI-subset." ></td>
	<td class="line x" title="115:211	However, the goal of this work is to produce a highcoverage semantic orientation lexicon and so we additionally evaluate the lexicons on the extrinsic task described below." ></td>
	<td class="line x" title="116:211	4.2 Extrinsic: Identifying phrase polarity The MPQA corpus contains news articles manually annotated for opinions and private states.4 Notably, it also has polarity annotations (positive/negative) at the phrase-level." ></td>
	<td class="line x" title="117:211	We conducted an extrinsic evaluation of the manually-generated and automatically-generated lexicons by using them to determine the polarity of phrases in the MPQA version 1.1 collection of positive and negative phrases (1,726 positive and 4,485 negative)." ></td>
	<td class="line x" title="118:211	We used a simple algorithm to determine the polarity of a phrase: (1) If any of the words in the target phrase is listed in the lexicon as having negative semantic orientation, then the phrase is marked negative." ></td>
	<td class="line x" title="119:211	(2) If none of the words in the phrase is negative and if there is at least one positive word in the phrase, then it is marked positive." ></td>
	<td class="line x" title="120:211	(3) In all other instances, the classifier refrains from assigning a tag." ></td>
	<td class="line x" title="121:211	Indeed better accuracies in phrase semantic orientation annotation can be obtained by using supervised classifiers and more sophisticated context features (Choi and Cardie, 4http://www.cs.pitt.edu/mpqa 2008)." ></td>
	<td class="line x" title="122:211	However, our goal here is only to use this task as a testbed for evaluating different semantic orientation lexicons, and so we use the method described above to avoid other factors from influencing the results." ></td>
	<td class="line x" title="123:211	Table 4 shows the performance of the algorithm when using different lexicons." ></td>
	<td class="line x" title="124:211	The performance when using lexicons that additionally make use of GI entriesMSOL(GI), MSOL(ASL and GI), PSL, and a combined GI-SWN lexiconis shown lower down in the table." ></td>
	<td class="line x" title="125:211	GISWN has entries from both GI and SWN." ></td>
	<td class="line x" title="126:211	(For entries with opposing labels, the GI label was chosen since GI entries were created manually.)" ></td>
	<td class="line x" title="127:211	Observe that the best F-scores are obtained when using MSOL (in both categoriesindividual lexicons and combinations with GI)." ></td>
	<td class="line x" title="128:211	The values are significantly better than those attained by others (p < 0.001)." ></td>
	<td class="line x" title="129:211	5 Discussion The extrinsic evaluation shows that our thesaurusand affix-based lexicon is significantly more accurate than SentiWordNet." ></td>
	<td class="line x" title="130:211	Moreover, it has a much larger coverage than the GI and Pitt lexicons." ></td>
	<td class="line x" title="131:211	Observe also that the affix seeds set, by itself, attains only a modest precision and a low recall." ></td>
	<td class="line x" title="132:211	This is expected because it is generated by largely automatic means." ></td>
	<td class="line x" title="133:211	However, the significantly higher MSOL performance suggests that the generalization step (described in Section 3.2) helps improve both precision and recall." ></td>
	<td class="line x" title="134:211	Precision is improved because multiple seed words vote to decide the semantic orientation of a thesaurus paragraph." ></td>
	<td class="line x" title="135:211	Recall improves simply because non-seed words in a paragraph are assigned the semantic orientation that is most prevalent among the seeds in the paragraph." ></td>
	<td class="line x" title="136:211	5.1 Support for the Polyanna Hypothesis Boucher and Osgoods (1969) Polyanna Hypothesis states that people have a preference for using positive words and expressions as opposed to using negative words and expressions." ></td>
	<td class="line x" title="137:211	Studies have shown that indeed speakers across languages use positive words much more frequently than negative words (Kelly, 2000)." ></td>
	<td class="line x" title="138:211	The distribution of positive and negative words in MSOL(ASL) further supports the Polyanna Hypothesis as it shows that even if we start with an equal number of positive and negative seed words, the expansion of the positive set through the thesaurus is much more pro603 All phrases Only positives Only negatives SO lexicon P R F P R F P R F Individual lexicons ASL 0.451 0.165 0.242 0.451 0.165 0.242 0.192 0.063 0.095 GI 0.797 0.323 0.459 0.871 0.417 0.564 0.763 0.288 0.419 MSOL(ASL) 0.623 0.474 0.539 0.631 0.525 0.573 0.623 0.458 0.528 SWN 0.541 0.408 0.465 0.745 0.624 0.679 0.452 0.328 0.380 TLL 0.769 0.298 0.430 0.761 0.352 0.482 0.775 0.279 0.411 Automatic lexicons + GI information MSOL(GI) 0.713 0.540 0.615 0.572 0.470 0.516 0.777 0.571 0.658 MSOL(ASL and GI) 0.710 0.546 0.617 0.577 0.481 0.525 0.771 0.574 0.658 PSL 0.823 0.422 0.558 0.860 0.487 0.622 0.810 0.399 0.535 GI-SWN 0.650 0.494 0.561 0.740 0.623 0.677 0.612 0.448 0.517 Table 4: Performance in phrase polarity tagging." ></td>
	<td class="line x" title="139:211	P = precision, R = recall, F = balanced F-score." ></td>
	<td class="line x" title="140:211	The best F-scores in each category are marked in bold." ></td>
	<td class="line x" title="141:211	nounced than the expansion of the negative set." ></td>
	<td class="line x" title="142:211	(About 66.8% of MSOL(ASL) words are positive, whereas only 33.2% are negative.)" ></td>
	<td class="line x" title="143:211	This suggests that there are many more near-synonyms of positive words than near-synonyms of negative ones, and so there are many more forms for expressing positive sentiments than forms for expressing negative sentiment." ></td>
	<td class="line x" title="144:211	5.2 Limitations Some of the errors in MSOL were due to nonantonymous instantiations of the affix patterns." ></td>
	<td class="line x" title="145:211	For example, immigrate is not antonymous to migrate." ></td>
	<td class="line x" title="146:211	Other errors occur because occasionally the words in the same thesaurus paragraph have differing semantic orientations." ></td>
	<td class="line x" title="147:211	For example, one paragraph has the words slender and slim (which, many will agree, are positive) as well as the words wiry and lanky (which many will deem negative)." ></td>
	<td class="line x" title="148:211	Both these kinds of errors can be mitigated using a complementary source of information, such as cooccurrence with other known positive and negative words (the TurneyLittman method)." ></td>
	<td class="line x" title="149:211	5.3 Future work Theoretically, a much larger TurneyLittman lexicon can be created even though it may be computationally intensive when working with 100 billion words." ></td>
	<td class="line x" title="150:211	However, MSOL and TLL are created from different sources of informationMSOL from overtly marked words and a thesaurus, and TLL from co-occurrence information." ></td>
	<td class="line x" title="151:211	Therefore, a combination of the two approaches is expected to produce an even more accurate semantic orientation lexicon, even with a modest-sized corpus at its disposal." ></td>
	<td class="line x" title="152:211	This is especially attractive for low resource languages." ></td>
	<td class="line x" title="153:211	We are also developing methods to leverage the information in an English thesaurus to create semantic orientation lexicons for a low-resource language through the use of a bilingual lexicon and a translation disambiguation algorithm." ></td>
	<td class="line x" title="154:211	6 Visualizing the semantic orientation of thesaurus categories In recent years, there have been substantial developments in the field of information visualization, and it is becoming increasingly clear that good visualizations can not only convey information quickly, but are also an important tool for gaining insight into an algorithm, detecting systematic errors, and understanding the task." ></td>
	<td class="line x" title="155:211	In this section, we present some preliminary visualizations that are helping us understand our approach beyond the evaluations described above." ></td>
	<td class="line x" title="156:211	As discussed in Section 3.1.1, the affix seeds set connects the thesaurus words with opposite semantic orientation." ></td>
	<td class="line x" title="157:211	Usually these pairs of words occur in different thesaurus categories, but this is not necessary." ></td>
	<td class="line x" title="158:211	We can think of these connections as relationships of contrast in meaning and semantic orientation, not just between the two words but also between the two categories." ></td>
	<td class="line x" title="159:211	To better aid our understanding of the automatically determined category relationships we visualized this network using the Fruchterman-Reingold forcedirected graph layout algorithm (Fruchterman and Reingold, 1991) and the NodeXL network analysis tool (Smith et al., 2009) 5." ></td>
	<td class="line x" title="160:211	Our dataset consists of 812 categories from the Macquarie Thesaurus and 27,155 antonym edges between them." ></td>
	<td class="line x" title="161:211	There can be an edge from a cat5Available from http://www.codeplex.com/NodeXL 604 Figure 1: After removing edges with low weight we can see the structure the network backbone." ></td>
	<td class="line x" title="162:211	Isolate category pairs are drawn in a ring around the main connected component and singletons are staggered in the corners." ></td>
	<td class="line x" title="163:211	Each node is colored by its semantic orientation (red for negative, blue for positive) and edges are colored by their weight, from red to blue." ></td>
	<td class="line x" title="164:211	Node shape also codes semantic orientation, with triangles positive and circles negative." ></td>
	<td class="line x" title="165:211	Size codes the magnitude the semantic orientation, with the largest nodes representing the extremes." ></td>
	<td class="line x" title="166:211	Node labels are shown for nodes in isolates and those in the top 20 for betweenness centrality." ></td>
	<td class="line x" title="167:211	egory to itself called a self-edge, indicating that a word and its antonym (with opposite semantic orientation) both exist in the same category." ></td>
	<td class="line x" title="168:211	There can be multiple edges between two categories indicating that one or more words in one category have one or more antonyms in the other category." ></td>
	<td class="line x" title="169:211	These multiple edges between category pairs were merged together resulting in 14,597 weighted meta-edges." ></td>
	<td class="line x" title="170:211	For example, if there are n edges between a category pair they were replaced by a single meta-edge of weight n. The network is too dense and interconnected for force-directed placement to generate a useful publication-size drawing of the entire network." ></td>
	<td class="line x" title="171:211	By removing edges that had a weight less than 6, we can visualize a smaller and more understandable 540 edge network of the core categories and any new isolates created." ></td>
	<td class="line x" title="172:211	Additionally, we show only edges between categories with opposite semantic orientations (Figure 1)." ></td>
	<td class="line x" title="173:211	Observe that there are three groups of nodes: those in the core connected component, the small isolates in the ring surrounding it, and the connectionless singletons arranged in the corners." ></td>
	<td class="line x" title="174:211	Each node c (thesaurus category) is colored on a red to blue continuous scale according to its semantic orientation SO, which is computed purely from its graph structure (in-degree ID and outdegree OD): SO(c) = OD(c)ID(c)OD(c) + ID(c) (1) Blue nodes represent categories with many positive words; we will call them positive cate605 gories (p)." ></td>
	<td class="line x" title="175:211	Red nodes are categories with many negative words; we will call them negative categories (n)." ></td>
	<td class="line x" title="176:211	Shades of purple in between are categories that have words with both positive and negative semantic orientation (mixed categories)." ></td>
	<td class="line x" title="177:211	Similarly, edges are colored according to their weight from red (small weight) to blue (large weight)." ></td>
	<td class="line x" title="178:211	We also use shape coding for semantic orientation, with triangles being positive and circles negative, and the size of the node depicts the magnitude of the semantic orientation." ></td>
	<td class="line x" title="179:211	For example, the pair HEARING(p)DEAFNESS(n) in the top left of Figure 1 represent the two size extremes: HEARING has a semantic orientation of 1 and DEAFNESS has a score of -1." ></td>
	<td class="line x" title="180:211	The mixed categories with near 0 semantic orientation such as LIKELIHOOD with a score of .07 are the smallest." ></td>
	<td class="line x" title="181:211	Nodes are labeled by the thesaurus-provided head wordsa word or phrase that best represents the coarse meaning of the category." ></td>
	<td class="line x" title="182:211	For readability, we have restricted the labels to nodes in the isolates and the top 20 nodes in the core connected component that have the highest betweenness centrality, which means they occur on more shortest paths between other nodes in the network (i.e., they are the bridges or gatekeepers)." ></td>
	<td class="line x" title="183:211	From the ring of isolates we can see how many antonymous categories, and their semantic orientations, are correctly recognized." ></td>
	<td class="line x" title="184:211	For example, ASSERTION(p)DENIAL(n), HEARING(p)DEAFNESS(n), GRATEFULNESS(p) UNGRATEFULNESS(n), and so on." ></td>
	<td class="line x" title="185:211	Some codings may seem less intuitive, such as those in the core, but much of this is the effect of abstracting away the low weight edges, which may have more clearly identified the relationships." ></td>
	<td class="line x" title="186:211	An alternative approach to removing edges with low weight is to filter categories in the network based on graph-theoretic metrics like betweenness centrality, closeness centrality, and eigenvector centrality." ></td>
	<td class="line x" title="187:211	We discussed betweenness centrality before." ></td>
	<td class="line x" title="188:211	The closeness centrality of a node is the average distance along the shortest path between that node and all other nodes reachable from it." ></td>
	<td class="line x" title="189:211	Eigenvector centrality is another measure of node importance, assigning node score based on the idea that connections to high-scoring nodes are more important than those to low-scoring ones." ></td>
	<td class="line x" title="190:211	We removed nodes with less than 0.1 betweenness centrality, less than 0.04 eigenvector centrality, and above 2.1 closeness centrality, leaving the key 56 nodes." ></td>
	<td class="line x" title="191:211	They have 497 edges between them, of which we show only those between categories with opposite semantic orientations (Figure 2)." ></td>
	<td class="line x" title="192:211	Node and edge color, size, and shape coding is as before." ></td>
	<td class="line x" title="193:211	Observe that most of these categories have a strongly evaluative nature." ></td>
	<td class="line x" title="194:211	Also, as our algorithm makes connections using overt negative markers, it makes sense that the central categories in our network have negative orientation (negative categories have many words with overt markings)." ></td>
	<td class="line x" title="195:211	It is interesting, though, how some positive and mixed categories reside in the core too." ></td>
	<td class="line x" title="196:211	Further inspection revealed that these categories have a large number of words within them." ></td>
	<td class="line x" title="197:211	For example, it may be less intuitive as to why the category of MUSIC is listed in the core, but this is because it has about 1,200 words in it (on average, each category has about 120 words), and because many of these words, such as harmonious(p), melodious(n), and lament(n) are evaluative in nature." ></td>
	<td class="line x" title="198:211	7 Conclusion We created a high-coverage semantic orientation lexicon using only affix rules and a Roget-like thesaurus." ></td>
	<td class="line x" title="199:211	The method does not require terms with manually annotated semantic orientation labels, though we show that if available they can be used to further improve both the correctness of its entries and its coverage." ></td>
	<td class="line x" title="200:211	The lexicon has about twenty times as many entries as in the General Inquirer and the TurneyLittman lexicons, and includes entries for both individual words and common multi-word expressions." ></td>
	<td class="line x" title="201:211	Experiments show that it has significantly more correct entries than SentiWordNet." ></td>
	<td class="line x" title="202:211	The approach is complementary to that of Turney and Littman (2003) and a combination of this approach with co-occurrence statistics (even if drawn from a modest sized corpus) is expected to yield an even better lexicon." ></td>
	<td class="line x" title="203:211	Visualization of the thesaurus categories as per the semantic orientations assigned to them by our algorithm reveals that affix patterns produce a strongly connected graph and that indeed there are many long chains of positive and negative categories." ></td>
	<td class="line x" title="204:211	Furthermore, the key categories of this graph (the ones with high centrality and closeness) are strongly evaluative in nature, and most of them tend to have negative semantic orientation." ></td>
	<td class="line x" title="205:211	606 Figure 2: After filtering out nodes based on graph-theoretic metrics, the core of the network becomes visible." ></td>
	<td class="line x" title="206:211	The visualization is colored as in Figure 1, and we can see how the core is dominated by categories with negative semantic orientation (red)." ></td>
	<td class="line x" title="207:211	Shape, size, and color coding is as before." ></td>
	<td class="line x" title="208:211	Acknowledgments We thank Douglas W. Oard, Ben Schneiderman, Judith Klavans and the anonymous reviewers for their valuable feedback." ></td>
	<td class="line x" title="209:211	This work was supported, in part, by the National Science Foundation under Grant No." ></td>
	<td class="line x" title="210:211	IIS-0705832, in part, by the Human Language Technology Center of Excellence, and in part, by Microsoft Research for the NodeXL project." ></td>
	<td class="line x" title="211:211	Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsor." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1004
Contextual Phrase-Level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-Grams
Agarwal, Apoorv;Biadsy, Fadi;Mckeown, Kathleen;"></td>
	<td class="line x" title="1:247	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 2432, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:247	c2009 Association for Computational Linguistics Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring and Syntactic N-grams Apoorv Agarwal Department of Computer Science Columbia University New York, USA aa2644@columbia.edu Fadi Biadsy Department of Computer Science Columbia University New York, USA fadi@cs.columbia.edu Kathleen R. Mckeown Department of Computer Science Columbia University New York, USA kathy@cs.columbia.edu Abstract We present a classifier to predict contextual polarity of subjective phrases in a sentence." ></td>
	<td class="line x" title="3:247	Our approach features lexical scoring derived from the Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score the vast majority of words in our input avoiding the need for manual labeling." ></td>
	<td class="line x" title="4:247	We augment lexical scoring with n-gram analysis to capture the effect of context." ></td>
	<td class="line x" title="5:247	We combine DAL scores with syntactic constituents and then extract ngrams of constituents from all sentences." ></td>
	<td class="line x" title="6:247	We also use the polarity of all syntactic constituents within the sentence as features." ></td>
	<td class="line x" title="7:247	Our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams." ></td>
	<td class="line x" title="8:247	1 Introduction Sentiment analysis is a much-researched area that deals with identification of positive, negative and neutral opinions in text." ></td>
	<td class="line x" title="9:247	The task has evolved from document level analysis to sentence and phrasal level analysis." ></td>
	<td class="line x" title="10:247	Whereas the former is suitable for classifying news (e.g., editorials vs. reports) into positive and negative, the latter is essential for question-answering and recommendation systems." ></td>
	<td class="line x" title="11:247	A recommendation system, for example, must be able to recommend restaurants (or movies, books, etc.) based on a variety of features such as food, service or ambience." ></td>
	<td class="line x" title="12:247	Any single review sentence may contain both positive and negative opinions, evaluating different features of a restaurant." ></td>
	<td class="line x" title="13:247	Consider the following sentence (1) where the writer expresses opposing sentiments towards food and service of a restaurant." ></td>
	<td class="line x" title="14:247	In tasks such as this, therefore, it is important that sentiment analysis be done at the phrase level." ></td>
	<td class="line x" title="15:247	(1) The Taj has great food but I found their service to be lacking." ></td>
	<td class="line x" title="16:247	Subjective phrases in a sentence are carriers of sentiments in which an experiencer expresses an attitude, often towards a target." ></td>
	<td class="line x" title="17:247	These subjective phrases may express neutral or polar attitudes depending on the context of the sentence in which they appear." ></td>
	<td class="line x" title="18:247	Context is mainly determined by content and structure of the sentence." ></td>
	<td class="line x" title="19:247	For example, in the following sentence (2), the underlined subjective phrase seems to be negative, but in the larger context of the sentence, it is positive.1 (2) The robber entered the store but his efforts were crushed when the police arrived on time." ></td>
	<td class="line x" title="20:247	Our task is to predict contextual polarity of subjective phrases in a sentence." ></td>
	<td class="line x" title="21:247	A traditional approach to this problem is to use a prior polarity lexicon of words to first set priors on target phrases and then make use of the syntactic and semantic information in and around the sentence to make the final prediction." ></td>
	<td class="line x" title="22:247	As in earlier approaches, we also use a lexicon to set priors, but we explore new uses of a Dictionary of Affect in Language (DAL) (Whissel, 1989) extended using WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="23:247	We augment this approach with n-gram analysis to capture the effect of context." ></td>
	<td class="line x" title="24:247	We present a system for classification of neutral versus positive versus negative and positive versus negative polarity (as is also done by (Wilson et al., 2005))." ></td>
	<td class="line x" title="25:247	Our approach is novel in the use of following features:  Lexical scores derived from DAL and extended through WordNet: The Dictionary of Affect has been widely used to aid in interpretation of emotion in speech (Hirschberg 1We assign polarity to phrases based on Wiebe (Wiebe et al., 2005); the polarity of all examples shown here is drawn from annnotations in the MPQA corpus." ></td>
	<td class="line x" title="26:247	Clearly the assignment of polarity chosen in this corpus depends on general cultural norms." ></td>
	<td class="line x" title="27:247	24 et al., 2005)." ></td>
	<td class="line x" title="28:247	It contains numeric scores assigned along axes of pleasantness, activeness and concreteness." ></td>
	<td class="line x" title="29:247	We introduce a method for setting numerical priors on words using these three axes, which we refer to as a scoring scheme throughout the paper." ></td>
	<td class="line x" title="30:247	This scheme has high coverage of the phrases for classification and requires no manual intervention when tagging words with prior polarities." ></td>
	<td class="line x" title="31:247	 N-gram Analysis: exploiting automatically derived polarity of syntactic constituents We compute polarity for each syntactic constituent in the input phrase using lexical affect scores for its words and extract n-grams over these constituents." ></td>
	<td class="line x" title="32:247	N-grams of syntactic constituents tagged with polarity provide patterns that improve prediction of polarity for the subjective phrase." ></td>
	<td class="line x" title="33:247	 Polarity of Surrounding Constituents: We use the computed polarity of syntactic constituents surrounding the phrase we want to classify." ></td>
	<td class="line x" title="34:247	These features help to capture the effect of context on the polarity of the subjective phrase." ></td>
	<td class="line x" title="35:247	We show that classification of subjective phrases using our approach yields better accuracy than two baselines, a majority class baseline and a more difficult baseline of lexical n-gram features." ></td>
	<td class="line x" title="36:247	We also provide an analysis of how the different component DAL scores contribute to our results through the introduction of a norm that combines the component scores, separating polar words that are less subjective (e.g., Christmas , murder) from neutral words that are more subjective (e.g., most, lack)." ></td>
	<td class="line x" title="37:247	Section 2 presents an overview of previous work, focusing on phrasal level sentiment analysis." ></td>
	<td class="line x" title="38:247	Section 3 describes the corpus and the gold standard we used for our experiments." ></td>
	<td class="line x" title="39:247	In section 4, we give a brief description of DAL, discussing its utility and previous uses for emotion and for sentiment analysis." ></td>
	<td class="line x" title="40:247	Section 5 presents, in detail, our polarity classification framework." ></td>
	<td class="line x" title="41:247	Here we describe our scoring scheme and the features we extract from sentences for classification tasks." ></td>
	<td class="line x" title="42:247	Experimental set-up and results are presented in Section 6." ></td>
	<td class="line x" title="43:247	We conclude with Section 7 where we also look at future directions for this research." ></td>
	<td class="line oc" title="44:247	2 Literature Survey The task of sentiment analysis has evolved from document level analysis (e.g., (Turney., 2002); (Pang and Lee, 2004)) to sentence level analysis (e.g., (Hu and Liu., 2004); (Kim and Hovy., 2004); (Yu and Hatzivassiloglou, 2003))." ></td>
	<td class="line o" title="45:247	These researchers first set priors on words using a prior polarity lexicon." ></td>
	<td class="line x" title="46:247	When classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment." ></td>
	<td class="line x" title="47:247	Research on contextual phrasal level sentiment analysis was pioneered by Nasukawa and Yi (2003), who used manually developed patterns to identify sentiment." ></td>
	<td class="line x" title="48:247	Their approach had high precision, but low recall." ></td>
	<td class="line x" title="49:247	Wilson et al., (2005) also explore contextual phrasal level sentiment analysis, using a machine learning approach that is closer to the one we present." ></td>
	<td class="line x" title="50:247	Both of these researchers also follow the traditional approach and first set priors on words using a prior polarity lexicon." ></td>
	<td class="line x" title="51:247	Wilson et al.(2005) use a lexicon of over 8000 subjectivity clues, gathered from three sources ((Riloff and Wiebe, 2003); (Hatzivassiloglou and McKeown, 1997) and The General Inquirer2)." ></td>
	<td class="line x" title="53:247	Words that were not tagged as positive or negative were manually labeled." ></td>
	<td class="line x" title="54:247	Yi et al.(2003) acquired words from GI, DAL and WordNet." ></td>
	<td class="line x" title="56:247	From DAL, only words whose pleasantness score is one standard deviation away from the mean were used." ></td>
	<td class="line x" title="57:247	Nasukawa as well as other researchers (Kamps and Marx, 2002)) also manually tag words with prior polarities." ></td>
	<td class="line x" title="58:247	All of these researchers use categorical tags for prior lexical polarity; in contrast, we use quantitative scores, making it possible to use them in computation of scores for the full phrase." ></td>
	<td class="line x" title="59:247	While Wilson et al.(2005) aim at phrasal level analysis, their system actually only gives each clue instance its own label [p. 350]." ></td>
	<td class="line x" title="61:247	Their gold standard is also at the clue level and assigns a value based on the clues appearance in different expressions (e.g., if a clue appears in a mixture of negative and neutral expressions, its class is negative)." ></td>
	<td class="line x" title="62:247	They note that they do not determine subjective expression boundaries and for this reason, they classify at the word level." ></td>
	<td class="line x" title="63:247	This approach is quite different from ours, as we compute the polarity of the full phrase." ></td>
	<td class="line x" title="64:247	The average length of the subjective phrases in the corpus was 2.7 words, with a standard deviation of 2.3." ></td>
	<td class="line x" title="65:247	Like Wilson et al. 2http://www.wjh.harvard.edu/ inquirer 25 (2005) we do not attempt to determine the boundary of subjective expressions; we use the labeled boundaries in the corpus." ></td>
	<td class="line x" title="66:247	3 Corpus We used the Multi-Perspective QuestionAnswering (MPQA version 1.2) Opinion corpus (Wiebe et al., 2005) for our experiments." ></td>
	<td class="line x" title="67:247	We extracted a total of 17,243 subjective phrases annotated for contextual polarity from the corpus of 535 documents (11,114 sentences)." ></td>
	<td class="line x" title="68:247	These subjective phrases are either direct subjective or expressive subjective." ></td>
	<td class="line x" title="69:247	Direct subjective expressions are explicit mentions of a private state (Quirk et al., 1985) and are much easier to classify." ></td>
	<td class="line x" title="70:247	Expressive subjective phrases are indirect or implicit mentions of private states and therefore are harder to classify." ></td>
	<td class="line x" title="71:247	Approximately one third of the phrases we extracted were direct subjective with non-neutral expressive intensity whereas the rest of the phrases were expressive subjective." ></td>
	<td class="line x" title="72:247	In terms of polarity, there were 2779 positive, 6471 negative and 7993 neutral expressions." ></td>
	<td class="line x" title="73:247	Our Gold Standard is the manual annotation tag given to phrases in the corpus." ></td>
	<td class="line x" title="74:247	4 DAL DAL is an English language dictionary built to measure emotional meaning of texts." ></td>
	<td class="line x" title="75:247	The samples employed to build the dictionary were gathered from different sources such as interviews, adolescents descriptions of their emotions and university students essays." ></td>
	<td class="line x" title="76:247	Thus, the 8742 word dictionary is broad and avoids bias from any one particular source." ></td>
	<td class="line x" title="77:247	Each word is given three kinds of scores (pleasantness  also called evaluation, ee, activeness, aa and imagery, ii) on a scale of 1 (low) to 3 (high)." ></td>
	<td class="line x" title="78:247	Pleasantness is a measure of polarity." ></td>
	<td class="line x" title="79:247	For example, in Table 1, affection is given a pleasantness score of 2.77 which is closer to 3.0 and is thus a highly positive word." ></td>
	<td class="line x" title="80:247	Likewise, activeness is a measure of the activation or arousal level of a word, which is apparent from the activeness scores of slug and energetic in the table." ></td>
	<td class="line x" title="81:247	The third score, imagery, is a measure of the ease with which a word forms a mental picture." ></td>
	<td class="line x" title="82:247	For example, affect cannot be imagined easily and therefore has a score closer to 1, as opposed to flower which is a very concrete and therefore has an imagery score of 3." ></td>
	<td class="line x" title="83:247	A notable feature of the dictionary is that it has different scores for various inflectional forms of a word ( affect and affection) and thus, morphological parsing, and the possibility of resulting errors, is avoided." ></td>
	<td class="line x" title="84:247	Moreover, Cowie et al., (2001) showed that the three scores are uncorrelated; this implies that each of the three scores provide complementary information." ></td>
	<td class="line x" title="85:247	Word ee aa ii Affect 1.75 1.85 1.60 Affection 2.77 2.25 2.00 Slug 1.00 1.18 2.40 Energetic 2.25 3.00 3.00 Flower 2.75 1.07 3.00 Table 1: DAL scores for words The dictionary has previously been used for detecting deceptive speech (Hirschberg et al., 2005) and recognizing emotion in speech (Athanaselis et al., 2006)." ></td>
	<td class="line x" title="86:247	5 The Polarity Classification Framework In this section, we present our polarity classification framework." ></td>
	<td class="line x" title="87:247	The system takes a sentence marked with a subjective phrase and identifies the most likely contextual polarity of this phrase." ></td>
	<td class="line x" title="88:247	We use a logistic regression classifier, implemented in Weka, to perform two types of classification: Three way (positive, negative, vs. neutral) and binary (positive vs. negative)." ></td>
	<td class="line x" title="89:247	The features we use for classification can be broadly divided into three categories: I. Prior polarity features computed from DAL and augmented using WordNet (Section 5.1)." ></td>
	<td class="line x" title="90:247	II." ></td>
	<td class="line x" title="91:247	lexical features including POS and word n-gram features (Section 5.3), and III." ></td>
	<td class="line x" title="92:247	the combination of DAL scores and syntactic features to allow both n-gram analysis and polarity features of neighbors (Section 5.4)." ></td>
	<td class="line x" title="93:247	5.1 Scoring based on DAL and WordNet DAL is used to assign three prior polarity scores to each word in a sentence." ></td>
	<td class="line x" title="94:247	If a word is found in DAL, scores of pleasantness (ee), activeness (aa), and imagery (ii) are assigned to it." ></td>
	<td class="line x" title="95:247	Otherwise, a list of the words synonyms and antonyms is created using WordNet." ></td>
	<td class="line x" title="96:247	This list is sequentially traversed until a match is found in DAL or the list ends, in which case no scores are assigned." ></td>
	<td class="line x" title="97:247	For example, astounded, a word absent in DAL, was scored by using its synonym amazed." ></td>
	<td class="line x" title="98:247	Similarly, in-humane was scored using the reverse polarity of 26 its antonym humane, present in DAL." ></td>
	<td class="line x" title="99:247	These scores are Z-Normalized using the mean and standard deviation measures given in the dictionarys manual (Whissel, 1989)." ></td>
	<td class="line x" title="100:247	It should be noted that in our current implementation all function words are given zero scores since they typically do not demonstrate any polarity." ></td>
	<td class="line x" title="101:247	The next step is to boost these normalized scores depending on how far they lie from the mean." ></td>
	<td class="line x" title="102:247	The reason for doing this is to be able to differentiate between phrases like fairly decent advice and excellent advice." ></td>
	<td class="line x" title="103:247	Without boosting, the pleasantness scores of both phrases are almost the same." ></td>
	<td class="line x" title="104:247	To boost the score, we multiply it by the number of standard deviations it lies from the mean." ></td>
	<td class="line x" title="105:247	After the assignment of scores to individual words, we handle local negations in a sentence by using a simple finite state machine with two states: RETAIN and INVERT." ></td>
	<td class="line x" title="106:247	In the INVERT state, the sign of the pleasantness score of the current word is inverted, while in the RETAIN state the sign of the score stays the same." ></td>
	<td class="line x" title="107:247	Initially, the first word in a given sentence is fed to the RETAIN state." ></td>
	<td class="line x" title="108:247	When a negation (e.g., not, no, never, cannot, didnt) is encountered, the state changes to the INVERT state." ></td>
	<td class="line x" title="109:247	While in the INVERT state, if but is encountered, it switches back to the RETAIN state." ></td>
	<td class="line x" title="110:247	In this machine we also take care of not only which serves as an intensifier rather than negation (Wilson et al., 2005)." ></td>
	<td class="line x" title="111:247	To handle phrases like no better than evil and could not be clearer, we also switch states from INVERT to RETAIN when a comparative degree adjective is found after not." ></td>
	<td class="line x" title="112:247	For example, the words in phrase in Table (2) are given positive pleasantness scores labeled with positive prior polarity." ></td>
	<td class="line x" title="113:247	Phrase has no greater desire POS VBZ DT JJR NN (ee) 0 0 3.37 0.68 State RETAIN INVERT RETAIN RETAIN Table 2: Example of scoring scheme using DAL We observed that roughly 74% of the content words in the corpus were directly found in DAL." ></td>
	<td class="line x" title="114:247	Synonyms of around 22% of the words in the corpus were found to exist in DAL." ></td>
	<td class="line x" title="115:247	Antonyms of only 1% of the words in the corpus were found in DAL." ></td>
	<td class="line x" title="116:247	Our system failed to find prior semantic orientations of roughly 3% of the total words in the corpus." ></td>
	<td class="line x" title="117:247	These were rarely occurring words like apartheid, apocalyptic and ulterior." ></td>
	<td class="line x" title="118:247	We assigned zero scores for these words." ></td>
	<td class="line x" title="119:247	In our system, we assign three DAL scores, using the above scheme, for the subjective phrase in a given sentence." ></td>
	<td class="line x" title="120:247	The features are (1) ee, the mean of the pleasantness scores of the words in the phrase, (2) aa, the mean of the activeness scores of the words in the phrase, and similarly (3) ii, the mean of the imagery scores." ></td>
	<td class="line x" title="121:247	5.2 Norm We gave each phrase another score, which we call the norm, that is a combination of the three scores from DAL." ></td>
	<td class="line x" title="122:247	Cowie et al.(2001) suggest a mechanism of mapping emotional states to a 2-D continuous space using an Activation-Evaluation space (AE) representation." ></td>
	<td class="line x" title="124:247	This representation makes use of the pleasantness and activeness scores from DAL and divides the space into four quadrants: delightful, angry, serene, and depressed." ></td>
	<td class="line x" title="125:247	Whissel (2008), observes that tragedies, which are easily imaginable in general, have higher imagery scores than comedies." ></td>
	<td class="line x" title="126:247	Drawing on these approaches and our intuition that neutral expressions tend to be more subjective, we define the norm in the following equation (1)." ></td>
	<td class="line x" title="127:247	norm= ee2 +aa2 ii (1) Words of interest to us may fall into the following four broad categories: 1." ></td>
	<td class="line x" title="128:247	High AE score and high imagery: These are words that are highly polar and less subjective (e.g., angel and lively)." ></td>
	<td class="line x" title="129:247	2." ></td>
	<td class="line x" title="130:247	Low AE score and low imagery: These are highly subjective neutral words (e.g., generally and ordinary)." ></td>
	<td class="line x" title="131:247	3." ></td>
	<td class="line x" title="132:247	High AE score and low imagery: These are words that are both highly polar and subjective (e.g., succeed and good)." ></td>
	<td class="line x" title="133:247	4." ></td>
	<td class="line x" title="134:247	Low AE score and high imagery: These are words that are neutral and easily imaginable (e.g., car and door)." ></td>
	<td class="line x" title="135:247	It is important to differentiate between these categories of words, because highly subjective words may change orientation depending on context; less subjective words tend to retain their prior orientation." ></td>
	<td class="line x" title="136:247	For instance, in the example sentence from Wilson et al.(2005)., the underlined phrase 27 seems negative, but in the context it is positive." ></td>
	<td class="line x" title="137:247	Since a subjective word like succeed depends on what one succeeds in, it may change its polarity accordingly." ></td>
	<td class="line x" title="138:247	In contrast, less subjective words, like angel, do not depend on the context in which they are used; they evoke the same connotation as their prior polarity." ></td>
	<td class="line x" title="139:247	(3) They havent succeeded and will never succeed in breaking the will of this valiant people." ></td>
	<td class="line x" title="140:247	As another example, AE space scores of goodies and good turn out to be the same." ></td>
	<td class="line x" title="141:247	What differentiates one from the another is the imagery score, which is higher for the former." ></td>
	<td class="line x" title="142:247	Therefore, value of the norm is lower for goodies than for good." ></td>
	<td class="line x" title="143:247	Unsurprisingly, this feature always appears in the top 10 features when the classification task contains neutral expressions as one of the classes." ></td>
	<td class="line x" title="144:247	5.3 Lexical Features We extract two types of lexical features, part of speech (POS) tags and n-gram word features." ></td>
	<td class="line x" title="145:247	We count the number of occurrences of each POS in the subjective phrase and represent each POS as an integer in our feature vector.3 For each subjective phrase, we also extract a subset of unigram, bigrams, and trigrams of words (selected automatically, see Section 6)." ></td>
	<td class="line x" title="146:247	We represent each n-gram feature as a binary feature." ></td>
	<td class="line x" title="147:247	These types of features were used to approximate standard n-gram language modeling (LM)." ></td>
	<td class="line x" title="148:247	In fact, we did experiment with a standard trigram LM, but found that it did not improve performance." ></td>
	<td class="line x" title="149:247	In particular, we trained two LMs, one on the polar subjective phrases and another on the neutral subjective phrases." ></td>
	<td class="line x" title="150:247	Given a sentence, we computed two perplexities of the two LMs on the subjective phrase in the sentence and added them as features in our feature vectors." ></td>
	<td class="line x" title="151:247	This procedure provided us with significant improvement over a chance baseline but did not outperform our current system." ></td>
	<td class="line x" title="152:247	We speculate that this was caused by the split of training data into two parts, one for training the LMs and another for training the classifier." ></td>
	<td class="line x" title="153:247	The resulting small quantity of training data may be the reason for bad performance." ></td>
	<td class="line x" title="154:247	Therefore, we decided to back off to only binary n-gram features as part of our feature vector." ></td>
	<td class="line x" title="155:247	3We use the Stanford Tagger to assign parts of speech tags to sentences." ></td>
	<td class="line x" title="156:247	(Toutanova and Manning, 2000) 5.4 Syntactic Features In this section, we show how we can combine the DAL scores with syntactic constituents." ></td>
	<td class="line x" title="157:247	This process involves two steps." ></td>
	<td class="line x" title="158:247	First, we chunk each sentence to its syntactic constituents (NP, VP, PP, JJP, and Other) using a CRF Chunker.4 If the marked-up subjective phrase does not contain complete chunks (i.e., it partially overlaps with other chunks), we expand the subjective phrase to include the chunks that it overlaps with." ></td>
	<td class="line x" title="159:247	We term this expanded phrase as the target phrase, see Figure 1." ></td>
	<td class="line x" title="160:247	Second, each chunk in a sentence is then assigned a 2-D AE space score as defined by Cowie et al., (2001) by adding the individual AE space scores of all the words in the chunk and then normalizing it by the number of words." ></td>
	<td class="line x" title="161:247	At this point, we are only concerned with the polarity of the chunk (i.e., whether it is positive or negative or neutral) and imagery will not help in this task; the AE space score is determined from pleasantness and activeness alone." ></td>
	<td class="line x" title="162:247	A threshold, determined empirically by analyzing the distributions of positive (pos), negative (neg) and neutral (neu) expressions, is used to define ranges for these classes of expressions." ></td>
	<td class="line x" title="163:247	This enables us to assign each chunk a prior semantic polarity." ></td>
	<td class="line x" title="164:247	Having the semantic orientation (positive, negative, neutral) and phrasal tags, the sentence is then converted to a sequence of encodings [PhrasalTag]polarity." ></td>
	<td class="line x" title="165:247	We mark each phrase that we want to classify as a target to differentiate it from the other chunks and attach its encoding." ></td>
	<td class="line x" title="166:247	As mentioned, if the target phrase partially overlaps with chunks, it is simply expanded to subsume the chunks." ></td>
	<td class="line x" title="167:247	This encoding is illustrated in Figure 1." ></td>
	<td class="line x" title="168:247	After these two steps, we extract a set of features that are used in classifying the target phrase." ></td>
	<td class="line x" title="169:247	These include n-grams of chunks from the all sentences, minimum and maximum pleasantness scores from the chunks in the target phrase itself, and the syntactic categories that occur in the context of the target phrase." ></td>
	<td class="line x" title="170:247	In the remainder of this section, we describe how these features are extracted." ></td>
	<td class="line x" title="171:247	We extract unigrams, bigrams and trigrams of chunks from all the sentences." ></td>
	<td class="line x" title="172:247	For example, we may extract a bigram from Figure 1 of [VP]neu followed by [PP]targetneg . Similar to the lexical 4Xuan-Hieu Phan, CRFChunker: CRF English Phrase Chunker, http://crfchunker.sourceforge.net/, 2006." ></td>
	<td class="line x" title="173:247	28 !''# !'# $%&($ !'#$ %& !'#$ %& !''# $%& !'#$%&()%*+,-./% !'#$%&()*+,+ -%." ></td>
	<td class="line x" title="174:247	&$%,+-%.#-'%)&#,()$ %* (/+,&0(%12%-'+%# +3&0(&4%,+/#&5% ! !" ></td>
	<td class="line x" title="175:247	# Figure 1: Converting a sentence with a subjective phrase to a sequence of chunks with their types and polarities n-grams, for the sentence containing the target phrase, we add binary values in our feature vector such that the value is 1 if the sentence contains that chunk n-gram." ></td>
	<td class="line x" title="176:247	We also include two features related to the target phrase." ></td>
	<td class="line x" title="177:247	The target phrase often consists of many chunks." ></td>
	<td class="line x" title="178:247	To detect if a chunk of the target phrase is highly polar, minimum and maximum pleasantness scores over all the chunks in the target phrase are noted." ></td>
	<td class="line x" title="179:247	In addition, we add features which attempt to capture contextual information using the prior semantic polarity assigned to each chunk both within the target phrase itself and within the context of the target phrase." ></td>
	<td class="line x" title="180:247	In cases where the target phrase is in the beginning of the sentence or at the end, we simply assign zero scores." ></td>
	<td class="line x" title="181:247	Then we compute the frequency of each syntactic type (i.e., NP, VP, PP, JJP) and polarity (i.e., positive, negative, neutral) to the left of the target, to the right of the target and for the target." ></td>
	<td class="line x" title="182:247	This additional set of contextual features yields 36 features in total: three polarities:{positive, negative, neutral}* three contexts: {left, target, right} * four chunk syntactic types: {NP, VP, PP, JJP}." ></td>
	<td class="line x" title="183:247	The full set of features captures different types of information." ></td>
	<td class="line x" title="184:247	N-grams look for certain patterns that may be specific to either polar or neutral sentiments." ></td>
	<td class="line x" title="185:247	Minimum and maximum scores capture information about the target phrase standalone." ></td>
	<td class="line x" title="186:247	The last set of features incorporate information about the neighbors of the target phrase." ></td>
	<td class="line x" title="187:247	We performed feature selection on this full set of n-gram related features and thus, a small subset of these n-gram related features, selected automatically (see section 6) were used in the experiments." ></td>
	<td class="line x" title="188:247	6 Experiments and Results Subjective phrases from the MPQA corpus were used in 10-fold cross-validation experiments." ></td>
	<td class="line x" title="189:247	The MPQA corpus includes gold standard tags for each Feature Types Accuracy Pos.* Neg.* Neu.* Chance baseline 33.33% N-gram baseline 59.05% 0.602 0.578 0.592 DAL scores only 59.66% 0.635 0.635 0.539 + POS 60.55% 0.621 0.542 0.655 + Chunks 64.72% 0.681 0.665 0.596 + N-gram (all) 67.51% 0.703 0.688 0.632 All (unbalanced) 70.76% 0.582 0.716 0.739 Table 3: Results of 3 way classification (Positive, Negative, and Neutral)." ></td>
	<td class="line x" title="190:247	In the unbalanced case, majority class baseline is 46.3% (*F-Measure)." ></td>
	<td class="line x" title="191:247	Feature Types Accuracy Pos.* Neg.* Chance baseline 50% N-gram baseline 73.21% 0.736 0.728 DAL scores only 77.02% 0.763 0.728 + POS 79.02% 0.788 0.792 + Chunks 80.72% 0.807 0.807 + N-gram (all) 82.32% 0.802 0.823 All (unbalanced) 84.08% 0.716 0.889 Table 4: Positive vs. Negative classification results." ></td>
	<td class="line x" title="192:247	Baseline is the majority class." ></td>
	<td class="line x" title="193:247	In the unbalanced case, majority class baseline is 69.74%." ></td>
	<td class="line x" title="194:247	(* F-Measure) phrase." ></td>
	<td class="line x" title="195:247	A logistic classifier was used for two polarity classification tasks, positive versus negative versus neutral and positive versus negative." ></td>
	<td class="line x" title="196:247	We report accuracy, and F-measure for both balanced and unbalanced data." ></td>
	<td class="line x" title="197:247	6.1 Positive versus Negative versus Neutral Table 3 shows results for a 3-way classifier." ></td>
	<td class="line x" title="198:247	For the balanced data-set, each class has 2799 instances and hence the chance baseline is 33%." ></td>
	<td class="line x" title="199:247	For the unbalanced data-set, there are 2799 instances of positive, 6471 instances of negative and 7993 instances of neutral phrases and thus the baseline is about 46%." ></td>
	<td class="line x" title="200:247	Results show that the accuracy increases as more features are added." ></td>
	<td class="line x" title="201:247	It may be seen from the table that prior polarity scores do not do well alone, but when used in conjunction with other features they play an important role in achieving an accuracy much higher than both baselines (chance and lexical n-grams)." ></td>
	<td class="line x" title="202:247	To re29 Figure 2: (a) An example sentence with three annotated subjective phrases in the same sentence." ></td>
	<td class="line x" title="203:247	(b) Part of the sentence with the target phrase (B) and their chunks with prior polarities." ></td>
	<td class="line x" title="204:247	confirm if prior polarity scores add value, we experimented by using all features except the prior polarity scores and noticed a drop in accuracy by about 4%." ></td>
	<td class="line x" title="205:247	This was found to be true for the other classification task as well." ></td>
	<td class="line x" title="206:247	The table shows that parts of speech and lexical n-grams are good features." ></td>
	<td class="line x" title="207:247	A significant improvement in accuracy (over 4%, p-value = 4.2e-15) is observed when chunk features (i.e., n-grams of constituents and polarity of neighboring constituents) are used in conjunction with prior polarity scores and part of speech features.5 This improvement may be explained by the following observation." ></td>
	<td class="line x" title="208:247	The bigram [Other]targetneu [NP]neu was selected as a top feature by the Chi-square feature selector." ></td>
	<td class="line x" title="209:247	So were unigrams, [Other]targetneu and [Other]targetneg . We thus learned n-gram patterns that are characteristic of neutral expressions (the just mentioned bigram and the first of the unigrams) as well as a pattern found mostly in negative expressions (the latter unigram)." ></td>
	<td class="line x" title="210:247	It was surprising to find another top chunk feature, the bigram [Other]targetneu [NP]neg (i.e., a neutral chunk of syntactic type Other preceding a negative noun phrase), present in neutral expressions six times more than in polar expressions." ></td>
	<td class="line x" title="211:247	An instance where these chunk features could have been responsible for the correct prediction of a target phrase is shown in Figure 2." ></td>
	<td class="line x" title="212:247	Figure 2(a) shows an example sentence from the MPQA corpus, which has three annotated subjective phrases." ></td>
	<td class="line x" title="213:247	The manually labeled polarity of phrases (A) and (C) is negative and that of (B) is neutral." ></td>
	<td class="line x" title="214:247	Figure 2(b) shows the 5We use the binomial test procedure to test statistical significance throughout the paper." ></td>
	<td class="line x" title="215:247	relevant chunk bigram which is used to predict the contextual polarity of the target phrase (B)." ></td>
	<td class="line x" title="216:247	It was interesting to see that the top 10 features consisted of all categories (i.e., prior DAL scores, lexical n-grams and POS, and syntactic) of features." ></td>
	<td class="line x" title="217:247	In this and the other experiment, pleasantness, activation and the norm were among the top 5 features." ></td>
	<td class="line x" title="218:247	We ran a significance test to show the importance of the norm feature in our classification task and observed that it exerted a significant increase in accuracy (2.26%, p-value = 1.45e-5)." ></td>
	<td class="line x" title="219:247	6.2 Positive versus Negative Table 4 shows results for positive versus negative classification." ></td>
	<td class="line x" title="220:247	We show results for both balanced and unbalanced data-sets." ></td>
	<td class="line x" title="221:247	For balanced, there are 2779 instances of each class." ></td>
	<td class="line x" title="222:247	For the unbalanced data-set, there are 2779 instances of positive and 6471 instances of neutral, thus our chance baseline is around 70%." ></td>
	<td class="line x" title="223:247	As in the earlier classification, accuracy and F-measure increase as we add features." ></td>
	<td class="line x" title="224:247	While the increase of adding the chunk features, for example, is not as great as in the previous classification, it is nonetheless significant (p-value = 0.0018) in this classification task." ></td>
	<td class="line x" title="225:247	The smaller increase lends support to our hypothesis that polar expressions tend to be less subjective and thus are less likely to be affected by contextual polarity." ></td>
	<td class="line x" title="226:247	Another thing that supports our hypothesis that neutral expressions are more subjective is the fact that the rank of imagery (ii), dropped significantly in this classification task as compared to the previous classification task." ></td>
	<td class="line x" title="227:247	This implies that imagery has a much lesser role to play when we are dealing with non-neutral expressions." ></td>
	<td class="line x" title="228:247	30 7 Conclusion and Future Work We present new features (DAL scores, norm scores computed using DAL, n-gram over chunks with polarity) for phrasal level sentiment analysis." ></td>
	<td class="line x" title="229:247	They work well and help in achieving high accuracy in a three-way classification of positive, negative and neutral expressions." ></td>
	<td class="line x" title="230:247	We do not require any manual intervention during feature selection, and thus our system is fully automated." ></td>
	<td class="line x" title="231:247	We also introduced a 3-D representation that maps different classes to spatial coordinates." ></td>
	<td class="line x" title="232:247	It may seem to be a limitation of our system that it requires accurate expression boundaries." ></td>
	<td class="line x" title="233:247	However, this is not true for the following two reasons: first, Wiebe et al., (2005) declare that while marking the span of subjective expressions and hand annotating the MPQA corpus, the annotators were not trained to mark accurate expression boundaries." ></td>
	<td class="line x" title="234:247	The only constraint was that the subjective expression should be within the mark-ups for all annotators." ></td>
	<td class="line x" title="235:247	Second, we expanded the marked subjective phrase to subsume neighboring phrases at the time of chunking." ></td>
	<td class="line x" title="236:247	A limitation of our scoring scheme is that it does not handle polysemy, since words in DAL are not provided with their parts of speech." ></td>
	<td class="line x" title="237:247	Statistics show, however, that most words occurred with primarily one part of speech only." ></td>
	<td class="line x" title="238:247	For example, will occurred as modal 1272 times in the corpus, whereas it appeared 34 times as a noun." ></td>
	<td class="line x" title="239:247	The case is similar for like and just, which mostly occur as a preposition and an adverb, respectively." ></td>
	<td class="line x" title="240:247	Also, in our state machine, we havent accounted for the impact of connectives such as but or although; we propose drawing on work in argumentative orientation to do so ((Anscombre and Ducrot, 1983); (Elhadad and McKeown, 1990))." ></td>
	<td class="line x" title="241:247	For future work, it would be interesting to do subjectivity and intensity classification using the same scheme and features." ></td>
	<td class="line x" title="242:247	Particularly, for the task of subjectivity analysis, we speculate that the imagery score might be useful for tagging chunks with subjective and objective instead of positive, negative, and neutral." ></td>
	<td class="line x" title="243:247	Acknowledgments This work was supported by the National Science Foundation under the KDD program." ></td>
	<td class="line x" title="244:247	Any opinions, ndings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reect the views of the National Science Foundation." ></td>
	<td class="line x" title="245:247	score." ></td>
	<td class="line x" title="246:247	We would like to thank Julia Hirschberg for useful discussion." ></td>
	<td class="line x" title="247:247	We would also like to acknowledge Narayanan Venkiteswaran for implementing parts of the system and Amal El Masri, Ashleigh White and Oliver Elliot for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1001
Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
Su, Fangzhong;Markert, Katja;"></td>
	<td class="line x" title="1:199	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:199	c 2009 Association for Computational Linguistics Subjectivity Recognition on Word Senses via Semi-supervised Mincuts Fangzhong Su School of Computing University of Leeds fzsu@comp.leeds.ac.uk Katja Markert School of Computing University of Leeds markert@comp.leeds.ac.uk Abstract We supplement WordNet entries with information on the subjectivity of its word senses." ></td>
	<td class="line x" title="3:199	Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data." ></td>
	<td class="line x" title="4:199	The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short." ></td>
	<td class="line x" title="5:199	We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure." ></td>
	<td class="line x" title="6:199	The experimental results show that it outperformssupervisedminimumcutaswellasstandard supervised, non-graph classification, reducing the error rate by 40%." ></td>
	<td class="line x" title="7:199	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data." ></td>
	<td class="line x" title="8:199	1 Introduction There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005)." ></td>
	<td class="line x" title="9:199	Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed." ></td>
	<td class="line x" title="10:199	Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005)." ></td>
	<td class="line x" title="11:199	Thus, the word positive in the sentence This deal is a positive development for our company. gives a strong indication that the sentence contains a favourable opinion." ></td>
	<td class="line x" title="12:199	However, such word-based indicators can be misleading for two reasons." ></td>
	<td class="line x" title="13:199	First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004)." ></td>
	<td class="line x" title="14:199	Second, different word senses of a single word can actually be of different subjectivity or polarity." ></td>
	<td class="line x" title="15:199	A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositivehaving a positive electric charge;protons are positive (objective) (2) plus, positiveinvolving advantage or good; a plus (or positive) factor (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet." ></td>
	<td class="line x" title="16:199	This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous." ></td>
	<td class="line x" title="17:199	Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006)." ></td>
	<td class="line x" title="18:199	Moreover, Andreevskaia and Bergler (2006) show that the performance of automaticannotationofsubjectivityatthewordlevelcan be hurt by the presence of subjectivity-ambiguous words in the training sets they use." ></td>
	<td class="line x" title="19:199	1All examples in this paper are from WordNet 2.0." ></td>
	<td class="line x" title="20:199	1 We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Ouralgorithmoutperformssupervisedminimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%." ></td>
	<td class="line x" title="21:199	In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data." ></td>
	<td class="line x" title="22:199	Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets." ></td>
	<td class="line x" title="23:199	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="24:199	Section 2 discusses previous work." ></td>
	<td class="line x" title="25:199	Section 3 describes our proposed semi-supervised minimum cut framework in detail." ></td>
	<td class="line x" title="26:199	Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5." ></td>
	<td class="line o" title="27:199	2 Related Work There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level." ></td>
	<td class="line x" title="28:199	An up-to-date overview is given in Pang and Lee (2008)." ></td>
	<td class="line x" title="29:199	Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do." ></td>
	<td class="line x" title="30:199	We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level." ></td>
	<td class="line x" title="31:199	At the word level Takamura et al.(2005) use a semi-supervised spin model for word polarity determination, where the graph 2It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign." ></td>
	<td class="line x" title="33:199	However, in Su and Markert (2008a) as well as Wiebe and Mihalcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability." ></td>
	<td class="line x" title="34:199	is constructed using a variety of information such as gloss co-occurrences and WordNet links." ></td>
	<td class="line x" title="35:199	Apart from using a different graph-based model from ours, theyassumethatsubjectivityrecognitionhasalready been achieved prior to polarity recognition and test against word lists containing subjective words only." ></td>
	<td class="line x" title="36:199	However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance." ></td>
	<td class="line x" title="37:199	In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al.(2005)." ></td>
	<td class="line x" title="39:199	Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification." ></td>
	<td class="line x" title="40:199	Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet." ></td>
	<td class="line x" title="41:199	However, there is no evaluation as to the accuracy of their approach." ></td>
	<td class="line x" title="42:199	They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative)." ></td>
	<td class="line x" title="43:199	Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations." ></td>
	<td class="line x" title="44:199	Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification." ></td>
	<td class="line x" title="45:199	Wiebe and Mihalcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005)." ></td>
	<td class="line x" title="46:199	Oneofthedisadvantagesoftheiralgorithmis thatitisrestrictedtosensesthathavedistributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification." ></td>
	<td class="line x" title="47:199	Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNets relation structure." ></td>
	<td class="line x" title="48:199	3 Semi-Supervised Mincuts 3.1 Minimum Cuts: The Main Idea Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items 2 should be grouped in the same cut." ></td>
	<td class="line x" title="49:199	All items in the training/test data are seen as vertices in a graph with undirected weighted edges between them specifying how strong the similarity/association between two vertices is. We use minimum s-t cuts: the graph contains two particular vertices s (source, corresponds to subjective) and t (sink, corresponds to objective) and each vertex u is connected to s and t via a weighted edge that can express how likely u is to be classified as s or t in isolation." ></td>
	<td class="line x" title="50:199	Binary classification of the vertices is equivalent to splitting the graph into two disconnected subsets of all vertices, S and T with s  S and t  T. This corresponds to removing a set of edges from the graph." ></td>
	<td class="line x" title="51:199	As similar items should be in the same part of the split, the best split is one which removes edges with low weights." ></td>
	<td class="line x" title="52:199	In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u,v) expresses the weight of an edge between two vertices." ></td>
	<td class="line x" title="53:199	W(S,T) = summationdisplay uS,vT w(u,v) Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002)." ></td>
	<td class="line x" title="54:199	3.2 Why might Semi-supervised Minimum Cuts Work?" ></td>
	<td class="line x" title="55:199	We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons." ></td>
	<td class="line x" title="56:199	First, our problem satisfies two major conditions necessary for using minimum cuts." ></td>
	<td class="line x" title="57:199	It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components." ></td>
	<td class="line x" title="58:199	Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data." ></td>
	<td class="line x" title="59:199	Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objective without taking similaritytoothersensesintoaccount, forexampleviaconsidering only the sense gloss." ></td>
	<td class="line x" title="60:199	In contrast, the edges between two senses can incorporate the WordNet relation hierarchy, which is a good source of similarity for our problem as many WordNet relations are subjectivity-preserving, i.e. if two senses are connected via such a relation they are likely to be both subjective or both objective.3 An example here is the antonym relation, where two antonyms such as goodmorally admirable and evil, wickedmorally bad or wrong are both subjective." ></td>
	<td class="line x" title="61:199	Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001)." ></td>
	<td class="line x" title="62:199	This is essential as the existing labeled datasets for our problem are small." ></td>
	<td class="line x" title="63:199	In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations." ></td>
	<td class="line x" title="64:199	Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework." ></td>
	<td class="line x" title="65:199	Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components." ></td>
	<td class="line x" title="66:199	More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories)." ></td>
	<td class="line x" title="67:199	3.3 Formulation of Semi-supervised Mincuts The formulation of our semi-supervised Mincut for sense subjectivity classification involves the following steps, which we later describe in more detail." ></td>
	<td class="line x" title="68:199	1." ></td>
	<td class="line x" title="69:199	We define two vertices s (source) and t (sink), which correspond to the subjective and objective category, respectively." ></td>
	<td class="line x" title="70:199	Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices." ></td>
	<td class="line x" title="71:199	Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge." ></td>
	<td class="line x" title="72:199	The latter guarantees that the graph is connected." ></td>
	<td class="line x" title="73:199	2." ></td>
	<td class="line x" title="74:199	For the test and unlabeled examples, we see the edges to the classification vertices as the probability of them being subjective/objective disregarding other example vertices." ></td>
	<td class="line x" title="75:199	We use a supervised classifier to set these edge weights." ></td>
	<td class="line x" title="76:199	Forthelabeledtrainingexamples, theyareconnected by edges with a high constant weight to the classification vertices that they belong to." ></td>
	<td class="line x" title="77:199	3." ></td>
	<td class="line x" title="78:199	WordNet relations are used to construct the edges between two example vertices." ></td>
	<td class="line x" title="79:199	Such 3See Kamps et al.(2004) for an early indication of such properties for some WordNet relations." ></td>
	<td class="line x" title="81:199	3 edges can exist between any pair of example vertices, for example between two unlabeled examples." ></td>
	<td class="line x" title="82:199	4." ></td>
	<td class="line x" title="83:199	After graph construction we then employ a maximum-flow algorithm to find the minimum s-t cuts of the graph." ></td>
	<td class="line x" title="84:199	The cut in which the sourcevertexsliesisclassified assubjective, andthecutinwhichthesinkvertextliesisobjective." ></td>
	<td class="line x" title="85:199	We now describe the above steps in more detail." ></td>
	<td class="line x" title="86:199	Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A)." ></td>
	<td class="line x" title="87:199	Thus a basic principle is that the selected unlabeled senses should be related to the training/test data by WordNet relations." ></td>
	<td class="line x" title="88:199	WethereforesimplyscaneachsenseinA, and collect all senses related to it via one of the WordNet relations in Table 1." ></td>
	<td class="line x" title="89:199	All such senses that are not in A are collected in the unlabeled data set." ></td>
	<td class="line x" title="90:199	Weighting of edges to the classification vertices: The edge weight to s and t represents how likely it is that an example vertex is initially put in the cut in which s (subjective) or t (objective) lies." ></td>
	<td class="line x" title="91:199	For unlabeled and test vertices, we use a supervised classifier (SVM4) with the labeled data as training data to assign the edge weights." ></td>
	<td class="line x" title="92:199	The SVM is also used as a baseline and its features are described in Section 4.3." ></td>
	<td class="line x" title="93:199	As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeledvertexanditscorrespondingclassificationvertex, and a low weight of 0.01 to the edge to the other classification vertex." ></td>
	<td class="line x" title="94:199	Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge." ></td>
	<td class="line x" title="95:199	Not all WordNet relations we use are subjectivitypreserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective." ></td>
	<td class="line x" title="96:199	However, we aim for high graph connectivity and we can assign different weights to different relations 4We employ LIBSVM, available at http://www.csie." ></td>
	<td class="line x" title="97:199	ntu.edu.tw/cjlin/libsvm/." ></td>
	<td class="line x" title="98:199	Linear kernel and probability estimates are used in this work." ></td>
	<td class="line x" title="99:199	to reflect the degree to which they are subjectivitypreserving." ></td>
	<td class="line x" title="100:199	Therefore, we experiment with two methods of weight assignment." ></td>
	<td class="line x" title="101:199	Method 1 (NoSL) assigns the same constant weight of 1.0 to all WordNet relations." ></td>
	<td class="line x" title="102:199	Method 2 (SL) reflects different degrees of preserving subjectivity." ></td>
	<td class="line x" title="103:199	To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008)." ></td>
	<td class="line x" title="104:199	This method uses a list of subjective words (SL)5 to classify each WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective." ></td>
	<td class="line x" title="105:199	We then count how often two senses related via a given relation have the same or a different subjectivity label." ></td>
	<td class="line x" title="106:199	The weight is computed by #same/(#same+#different)." ></td>
	<td class="line x" title="107:199	Results are listed in Table 1." ></td>
	<td class="line x" title="108:199	Table 1: Relation weights (Method 2) Method #Same #Different Weight Antonym 2,808 309 0.90 Similar-to 6,887 1,614 0.81 Derived-from 4,630 947 0.83 Direct-Hypernym 71,915 8,600 0.89 Direct-Hyponym 71,915 8,600 0.89 Attribute 350 109 0.76 Also-see 1,037 337 0.75 Extended-Antonym 6,917 1,651 0.81 Domain 4,387 892 0.83 Domain-member 4,387 892 0.83 Example graph: An example graph is shown in Figure 1." ></td>
	<td class="line x" title="109:199	The three example vertices correspond to the senses religiousextremely scrupulous and conscientious, scrupuloushaving scruples; arising from a sense of right and wrong; principled; and flicker, spark, glinta momentary flash of light respectively." ></td>
	<td class="line x" title="110:199	The vertex scrupulous is unlabeled data derived from the vertex religious(a test item) by the relation similar-to." ></td>
	<td class="line x" title="111:199	4 Experiments and Evaluation 4.1 Datasets We conduct the experiments on two different gold standard datasets." ></td>
	<td class="line x" title="112:199	One is the Micro-WNOp corpus, 5Available at http://www.cs.pitt.edu/mpqa 4 scrupulous religious subjective objective flicker 0.24 0.76 0.83 0.17 0.16 0.84 0.81similar-to Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6." ></td>
	<td class="line x" title="113:199	It includes 298 words with 703 objective and 358 subjective WordNet senses." ></td>
	<td class="line x" title="114:199	The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses." ></td>
	<td class="line x" title="115:199	As the Micro-WNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4." ></td>
	<td class="line x" title="116:199	In Section 4.5, we shortly discuss results on Wiebe&Mihalceas dataset." ></td>
	<td class="line x" title="117:199	4.2 Baseline and Evaluation We compare to a baseline that assigns the most frequent category objective to all senses, which achievesanaccuracyof66.3%and72.0%onMicroWNOp and Wiebe&Mihalceas dataset respectively." ></td>
	<td class="line x" title="118:199	We use the McNemar test at the significance level of 5% for significance statements." ></td>
	<td class="line x" title="119:199	All evaluations are carried out by 10-fold cross-validation." ></td>
	<td class="line x" title="120:199	4.3 Standard Supervised Learning We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable 6Available at http://www.comp.leeds.ac.uk/ markert/data." ></td>
	<td class="line x" title="121:199	This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008)." ></td>
	<td class="line x" title="122:199	7Available at http://www.cs.pitt.edu/wiebe/ pubs/papers/goldstandard.total.acl06." ></td>
	<td class="line x" title="123:199	baseline.8 Three different feature types are used." ></td>
	<td class="line x" title="124:199	LexicalFeatures(L):a bag-of-words representation of the sense glosses with stop word filtering." ></td>
	<td class="line x" title="125:199	RelationFeatures (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense hastosensesinthesubjectiveorobjectivepartofthe training set, respectively." ></td>
	<td class="line x" title="126:199	This provides a non-graph summary of subjectivity-preserving links." ></td>
	<td class="line x" title="127:199	Second, we manually collected a small set (denoted by SubjSet) of seven subjective verb and noun senses which are close to the root in WordNets hypernym tree." ></td>
	<td class="line x" title="128:199	A typical example element of SubjSet is psychological feature a feature of the mental life of a living organism, which indicates subjectivity for its hyponyms such as hope  the general feeling that some desire will be fulfilled." ></td>
	<td class="line x" title="129:199	A binary feature describes whether a noun/verb sense is a hyponym of an element of SubjSet." ></td>
	<td class="line x" title="130:199	Monosemous Feature (M): for each sense, we scan if a monosemous word is part of its synset." ></td>
	<td class="line x" title="131:199	If so, we further check if the monosemous word is collected in the subjective word list (SL)." ></td>
	<td class="line x" title="132:199	The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective." ></td>
	<td class="line x" title="133:199	For example, the sense uncompromising, inflexiblenot making concessions is subjective, as uncompromising is a monosemous word and also in SL." ></td>
	<td class="line x" title="134:199	We experiment with different combinations of features and the results are listed in Table 2, prefixed by SVM." ></td>
	<td class="line x" title="135:199	All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Markert (2008)." ></td>
	<td class="line x" title="136:199	However, improvements by adding more features remain small." ></td>
	<td class="line x" title="137:199	In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Lesks similarity measure from Pedersens WordNet similarity package9." ></td>
	<td class="line x" title="138:199	We use Lesk as it is one of the few measures applicable across all parts-of-speech." ></td>
	<td class="line x" title="139:199	8This SVM is also used to provide the edge weights to the classification vertices in the Mincut approach." ></td>
	<td class="line x" title="140:199	9Availableathttp://www.d.umn.edu/tpederse/ similarity.html." ></td>
	<td class="line x" title="141:199	5 Table 2: Results of SVM and Mincuts with different settings of feature Method Subjective Objective Accuracy Precision Recall F-score Precision Recall F-score Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3% S&M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9% Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4% SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4% L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7% L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5% SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1% LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2% LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2% SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5% LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7% LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6% SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3% LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6% LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4% 1 L, R and M correspond to the lexical, relation and monosemous features respectively." ></td>
	<td class="line x" title="142:199	2 SVM-LcorrespondstousinglexicalfeaturesonlyfortheSVMclassifier." ></td>
	<td class="line x" title="143:199	Likewise,SVMLRM corresponds to using a combination for lexical, relation, and monosemous features for the SVM classifier." ></td>
	<td class="line x" title="144:199	3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations." ></td>
	<td class="line x" title="145:199	Likewise, LM-NoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list)." ></td>
	<td class="line x" title="146:199	4.4 Semi-supervised Graph Mincuts Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in Micro-WNOp as unlabeled data." ></td>
	<td class="line x" title="147:199	We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL)." ></td>
	<td class="line x" title="148:199	The results are also summarized in Table 2." ></td>
	<td class="line x" title="149:199	Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights." ></td>
	<td class="line x" title="150:199	We can also see that we achieve good results without using any other knowledge sources (setting LR-NoSL)." ></td>
	<td class="line x" title="151:199	The example in Figure 1 explains why semisupervised Mincuts outperforms the supervised approach." ></td>
	<td class="line x" title="152:199	The vertex religious is initially assigned the subjective/objective probabilities 0.24/0.76 by theSVMclassifier, leadingtoawrongclassification." ></td>
	<td class="line x" title="153:199	However, inourgraph-basedMincutframework, the vertex religious might link to other vertices (for example, it links to the vertex scrupulous in the unlabeled data by the relation similar-to)." ></td>
	<td class="line x" title="154:199	The mincut algorithm will put vertices religious and scrupulousinthesamecut(subjectivecategory)as this results in the least cost 0.93 (ignoring the cost of assigning the unrelated sense of flicker)." ></td>
	<td class="line x" title="155:199	In other words, the edges between the vertices are likely to correct some initially wrong classification and pull the vertices into the right cuts." ></td>
	<td class="line x" title="156:199	In the following we will analyze the best minimum cut algorithm LRM-SL in more detail." ></td>
	<td class="line x" title="157:199	We measure its accuracy for each part-of-speech in the Micro-WNOp dataset." ></td>
	<td class="line x" title="158:199	The number of noun, adjective, adverb and verb senses in Micro-WNOp is 484, 265, 31 and 281, respectively." ></td>
	<td class="line x" title="159:199	The result is listed in Table 3." ></td>
	<td class="line x" title="160:199	The significantly better performance of semi-supervised mincuts holds across all parts-ofspeech but the small set of adverbs, where there is no significant difference between the baseline, SVM and the Mincut algorithm." ></td>
	<td class="line x" title="161:199	6 Table 3: Accuracy for Different Part-Of-Speech Method Noun Adjective Adverb Verb Baseline 76.9% 61.1% 77.4% 72.6% SVM 81.4% 63.4% 83.9% 75.1% Mincut 88.6% 78.9% 77.4% 84.0% We will now investigate how LRM-SL performs with different sizes of labeled and unlabeled data." ></td>
	<td class="line x" title="162:199	All learning curves are generated via averaging 10 learning curves from 10-fold cross-validation." ></td>
	<td class="line x" title="163:199	Performance with different sizes of labeled data: we randomly generate subsets of labeled data A1, A2 An, and guarantee that A1  A2  An." ></td>
	<td class="line x" title="164:199	Results for the best SVM (LRM) and the best minimum cut (LRM-SL) are listed in Table 4, and the corresponding learning curve is shown in Figure 2." ></td>
	<td class="line x" title="165:199	As can be seen, the semi-supervised Mincuts is consistently better than SVM." ></td>
	<td class="line x" title="166:199	Moreover, the semisupervised Mincut with only 200 labeled data items performs even better than SVM with 954 training items (78.9% vs 75.3%), showing that our semisupervised framework allows for a training data reduction of more than 80%." ></td>
	<td class="line x" title="167:199	Table 4: Accuracy with different sizes of labeled data # labeled data SVM Mincuts 100 69.1% 72.2% 200 72.6% 78.9% 400 74.4% 82.7% 600 75.5% 83.7% 800 76.0% 84.1% 900 75.6% 84.8% 954 (all) 75.3% 84.6% Performance with different sizes of unlabeled data: We propose two different settings." ></td>
	<td class="line x" title="168:199	Option1: Use a subset of the ten relations to generate the unlabeled data (and edges between example vertices)." ></td>
	<td class="line x" title="169:199	For example, we first use {antonym, similar-to} only to obtain a unlabeled dataset U1, then use a larger subset of the relations like {antonym, similar-to, direct-hyponym, directhypernym} to generate another unlabeled dataset U2, and so forth." ></td>
	<td class="line x" title="170:199	Obviously, Ui is a subset of Ui+1." ></td>
	<td class="line x" title="171:199	Option2: Use all the ten relations to generate the unlabeled data U. We then randomly select subsets of U, such as subset U1, U2 and U3, and guarantee that U1  U2  U3  U.  68  71  74  77  80  83  86  89  100  200  300  400  500  600  700  800  900  1000 Accuracy(%) Size of Labeled Data Mincuts SVM Figure 2: Learning curve with different sizes of labeled data The results are listed in Table 5 and Table 6 respectively." ></td>
	<td class="line x" title="172:199	The corresponding learning curves are shown in Figure 3." ></td>
	<td class="line x" title="173:199	We see that performance improves with the increase of unlabeled data." ></td>
	<td class="line x" title="174:199	In addition, the curves seem to converge when the size of unlabeled data is larger than 3,000." ></td>
	<td class="line x" title="175:199	From the results in Tabel 5 one can also see that hyponymy is the relation accounting for the largest increase." ></td>
	<td class="line x" title="176:199	Table 6: Accuracy with different sizes of unlabeled data (random selection) # unlabeled data Accuracy 0 75.9% 200 76.5% 500 78.6% 1000 80.2% 2000 82.8% 3000 84.0% 3220 84.6% Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%)." ></td>
	<td class="line x" title="177:199	The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small Micro-WNOp dataset." ></td>
	<td class="line x" title="178:199	In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1)." ></td>
	<td class="line x" title="179:199	4.5 Comparison to Prior Approaches In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro7 Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Relation # unlabeled data Accuracy {} 0 75.3% {similar-to} 418 79.1% {similar-to, antonym} 514 79.5% {similar-to, antonym, direct-hypernym, directhyponym} 2,721 84.4% {similar-to, antonym, direct-hypernym, directhyponym, also-see, extended-antonym} 3,004 84.4% {similar-to, antonym, direct-hypernym, directhyponym, also-see, extended-antonym, derived-from, attribute, domain, domain-member} 3,220 84.6%  75  77  79  81  83  85  87  89  0  500  1000  1500  2000  2500  3000  3500 Accuracy(%) Size of Unlabeled Data Option1Option2 Figure3: Learningcurvewithdifferentsizesofunlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2)." ></td>
	<td class="line x" title="180:199	Our best result from Mincuts is significantly better at 84.6% (see LRM-SL in Table 2)." ></td>
	<td class="line x" title="181:199	For comparison to Wiebe and Mihalcea (2006), we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description)." ></td>
	<td class="line x" title="182:199	Wiebe and Mihalcea (2006) report their results in precision andrecallcurvesforsubjectivesenses,suchasaprecision of about 55% at a recall of 50% for subjective senses." ></td>
	<td class="line x" title="183:199	Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve." ></td>
	<td class="line x" title="184:199	We run our best Mincut LRM-SL algorithm with two different settings on Wiebe." ></td>
	<td class="line x" title="185:199	Using MicroWNOp as training set and Wiebe as test set, we achieveanaccuracyof83.2%,whichissimilartothe results on the Micro-WNOp dataset." ></td>
	<td class="line x" title="186:199	At the recall of 50% we achieve a precision of 83.6% (in comparisontotheirprecisionof55%atthesamerecall)." ></td>
	<td class="line x" title="187:199	Our F-score is 0.63 (vs. 0.52)." ></td>
	<td class="line x" title="188:199	Tocheckwhetherthehighperformanceisjustdue to our larger training set, we also conduct 10-fold cross-validation on Wiebe." ></td>
	<td class="line x" title="189:199	The accuracy achieved is 81.1% and the F-score 0.56 (vs. 0.52), suggesting that our algorithm performs better." ></td>
	<td class="line x" title="190:199	Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2)." ></td>
	<td class="line x" title="191:199	However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus." ></td>
	<td class="line x" title="192:199	5 Conclusion and Future Work We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses." ></td>
	<td class="line x" title="193:199	The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut." ></td>
	<td class="line x" title="194:199	Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%)." ></td>
	<td class="line x" title="195:199	To achieve the results of standard supervised approaches with our model, we need less than20%oftheirtrainingdata." ></td>
	<td class="line x" title="196:199	Inaddition, wecompare our algorithm to previous state-of-the-art approaches, showing that our model performs better on the same datasets." ></td>
	<td class="line x" title="197:199	Future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures." ></td>
	<td class="line x" title="198:199	We will also explore other semisupervised algorithms." ></td>
	<td class="line x" title="199:199	8" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1002
Integrating Knowledge for Subjectivity Sense Labeling
Gyamfi, Yaw;Wiebe, Janyce M.;Mihalcea, Rada;Akkaya, Cem;"></td>
	<td class="line x" title="1:226	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1018, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:226	c 2009 Association for Computational Linguistics Integrating Knowledge for Subjectivity Sense Labeling Yaw Gyamfi and Janyce Wiebe University of Pittsburgh {anti,wiebe}@cs.pitt.edu Rada Mihalcea University of North Texas rada@cs.unt.edu Cem Akkaya University of Pittsburgh cem@cs.pitt.edu Abstract This paper introduces an integrative approach to automatic word sense subjectivity annotation." ></td>
	<td class="line x" title="3:226	We use features that exploit the hierarchical structure and domain information in lexical resources such as WordNet, as well as other types of features that measure the similarity of glosses and the overlap among sets of semantically related words." ></td>
	<td class="line x" title="4:226	Integrated in a machine learning framework, the entire set of features is found to give better results than any individual type of feature." ></td>
	<td class="line x" title="5:226	1 Introduction Automatic extraction of opinions, emotions, and sentiments in text (subjectivity analysis) to support applications such as product review mining, summarization, question answering, and information extraction is an active area of research in NLP." ></td>
	<td class="line x" title="6:226	Many approaches to opinion, sentiment, and subjectivity analysis rely on lexicons of words that may be used to express subjectivity." ></td>
	<td class="line x" title="7:226	However, words may have both subjective and objective senses, which is a source of ambiguity in subjectivity and sentiment analysis." ></td>
	<td class="line x" title="8:226	We show that even words judged in previous work to be reliable clues of subjectivity have significant degrees of subjectivity sense ambiguity." ></td>
	<td class="line x" title="9:226	To address this ambiguity, we present a method for automatically assigning subjectivity labels to word senses in a taxonomy, which uses new features and integrates more diverse types of knowledge than in previous work." ></td>
	<td class="line x" title="10:226	We focus on nouns, which are challenging and have received less attention in automatic subjectivity and sentiment analysis." ></td>
	<td class="line x" title="11:226	A common approach to building lexicons for subjectivity analysis is to begin with a small set of seeds which are prototypically subjective (or positive/negative, in sentiment analysis), and then follow semantic links in WordNet-like resources." ></td>
	<td class="line x" title="12:226	By far, the emphasis has been on horizontal relations, such as synonymy and antonymy." ></td>
	<td class="line x" title="13:226	Exploiting vertical links opens the door to taking into account the information content of ancestor concepts of senses with known and unknown subjectivity." ></td>
	<td class="line x" title="14:226	We develop novel features that measure the similarity of a target word sense with a seed set of senses known to be subjective, where the similarity between two concepts is determined by the extent to which they share information, measured by the information content associated with their least common subsumer (LCS)." ></td>
	<td class="line x" title="15:226	Further, particularizing the LCS features to domain greatly reduces calculation while still maintaining effective features." ></td>
	<td class="line x" title="16:226	We find that our new features do lead to significant improvements over methods proposed in previous work, and that the combination of all features gives significantly better performance than any single type of feature alone." ></td>
	<td class="line x" title="17:226	We also ask, given that there are many approaches to finding subjective words, if it would make sense for wordand sense-level approaches to work in tandem, or should we best view them as competing approaches?" ></td>
	<td class="line x" title="18:226	We give evidence suggesting that first identifying subjective words and then disambiguating their senses would be an effective approach to subjectivity sense labeling." ></td>
	<td class="line x" title="19:226	10 There are several motivations for assigning subjectivity labels to senses." ></td>
	<td class="line x" title="20:226	First, (Wiebe and Mihalcea, 2006) provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation." ></td>
	<td class="line x" title="21:226	Similarly, given subjectivity sense labels, word-sense disambiguation may potentially help contextual subjectivity analysis." ></td>
	<td class="line x" title="22:226	In addition, as lexical resources such as WordNet are developed further, subjectivity labels would provide principled criteria for refining word senses, as well as for clustering similar meanings to create more coursegrained sense inventories." ></td>
	<td class="line x" title="23:226	For many opinion mining applications, polarity (positive, negative) is also important." ></td>
	<td class="line x" title="24:226	The overall framework we envision is a layered approach: classifying instances as objective or subjective, and further classifying the subjective instances by polarity." ></td>
	<td class="line x" title="25:226	Decomposing the problem into subproblems has been found to be effective for opinion mining." ></td>
	<td class="line x" title="26:226	This paper addresses the first of these subproblems." ></td>
	<td class="line x" title="27:226	2 Background We adopt the definitions of subjective and objective from Wiebe and Mihalcea (2006) (hereafter WM)." ></td>
	<td class="line x" title="28:226	Subjective expressions are words and phrases being used to express opinions, emotions, speculations, etc. WM give the following examples: His alarm grew." ></td>
	<td class="line x" title="29:226	He absorbed the information quickly." ></td>
	<td class="line x" title="30:226	UCC/Disciples leaders roundly condemned the Iranian Presidents verbal assault on Israel." ></td>
	<td class="line x" title="31:226	Whats the catch?" ></td>
	<td class="line x" title="32:226	Polarity (also called semantic orientation) is also important to NLP applications in sentiment analysis and opinion extraction." ></td>
	<td class="line x" title="33:226	In review mining, for example, we want to know whether an opinion about a product is positive or negative." ></td>
	<td class="line x" title="34:226	Even so, we believe there are strong motivations for a separate subjective/objective (S/O) classification as well." ></td>
	<td class="line x" title="35:226	First, expressions may be subjective but not have any particular polarity." ></td>
	<td class="line x" title="36:226	An example given by (Wilson et al., 2005) is Jerome says the hospital feels no different than a hospital in the states." ></td>
	<td class="line x" title="37:226	An NLP application system may want to find a wide range of private states attributed to a person, such as their motivations, thoughts, and speculations, in addition to their positive and negative sentiments." ></td>
	<td class="line x" title="38:226	Second, distinguishing S and O instances has often proven more difficult than subsequent polarity classification." ></td>
	<td class="line x" title="39:226	Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al., 2006), sentiment classification of phrases (Wilson et al., 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a)." ></td>
	<td class="line x" title="40:226	Thus, effective methods for S/O classification promise to improve performance for sentiment classification." ></td>
	<td class="line x" title="41:226	In fact, researchers in sentiment analysis have realized benefits by decomposing the problem into S/O and polarity classification (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Wilson et al., 2005; Kim and Hovy, 2006)." ></td>
	<td class="line x" title="42:226	One reason is that different features may be relevant for the two subproblems." ></td>
	<td class="line x" title="43:226	For example, negation features are more important for polarity classification than for subjectivity classification." ></td>
	<td class="line x" title="44:226	Note that some of our features require vertical links that are present in WordNet for nouns and verbs but not for other parts of speech." ></td>
	<td class="line x" title="45:226	Thus we address nouns (leaving verbs to future work)." ></td>
	<td class="line x" title="46:226	There are other motivations for focusing on nouns." ></td>
	<td class="line x" title="47:226	Relatively little work in subjectivity and sentiment analysis has focused on subjective nouns." ></td>
	<td class="line x" title="48:226	Also, a study (Bruce and Wiebe, 1999) showed that, of the major parts of speech, nouns are the most ambiguous with respect to the subjectivity of their instances." ></td>
	<td class="line x" title="49:226	Turning to word senses, we adopt the definitions from WM." ></td>
	<td class="line x" title="50:226	First, subjective: Classifying a sense as S means that, when the sense is used in a text or conversation, we expect it to express subjectivity; we also expect the phrase or sentence containing it to be subjective [WM, pp." ></td>
	<td class="line x" title="51:226	2-3]. In WM, it is noted that sentences containing objective senses may not be objective, as in the sentence Will someone shut that darn alarm off?" ></td>
	<td class="line x" title="52:226	Thus, objective senses are defined as follows: Classifying a sense as O means that, when the sense is used in a text or conversation, we do not expect it to express subjectivity and, if the phrase or sentence containing it is subjective, the subjectivity is due to something else [WM, p 3]. The following subjective examples are given in 11 WM: His alarm grew." ></td>
	<td class="line x" title="53:226	alarm, dismay, consternation  (fear resulting from the awareness of danger) => fear, fearfulness, fright  (an emotion experienced in anticipation of some specific pain or danger (usually accompanied by a desire to flee or fight)) Whats the catch?" ></td>
	<td class="line x" title="54:226	catch  (a hidden drawback; it sounds good but whats the catch?) => drawback  (the quality of being a hindrance; he pointed out all the drawbacks to my plan) The following objective examples are given in WM: The alarm went off." ></td>
	<td class="line x" title="55:226	alarm, warning device, alarm system  (a device that signals the occurrence of some undesirable event) =>device  (an instrumentality invented for a particular purpose; the device is small enough to wear on your wrist; a device intended to conserve water) He sold his catch at the market." ></td>
	<td class="line x" title="56:226	catch, haul  (the quantity that was caught; the catch was only 10 fish) => indefinite quantity  (an estimated quantity) WM performed an agreement study and report that good agreement (=0.74) can be achieved between human annotators labeling the subjectivity of senses." ></td>
	<td class="line x" title="57:226	For a similar task, (Su and Markert, 2008) also report good agreement." ></td>
	<td class="line oc" title="58:226	3 Related Work Many methods have been developed for automatically identifying subjective (opinion, sentiment, attitude, affect-bearing, etc.) words, e.g., (Turney, 2002; Riloff and Wiebe, 2003; Kim and Hovy, 2004; Taboada et al., 2006; Takamura et al., 2006)." ></td>
	<td class="line x" title="59:226	Five groups have worked on subjectivity sense labeling." ></td>
	<td class="line x" title="60:226	WM and Su and Markert (2008) (hereafter SM) assign S/O labels to senses, while Esuli and Sebastiani (hereafter ES) (2006a; 2007), Andreevskaia and Bergler (hereafter AB) (2006b; 2006a), and (Valitutti et al., 2004) assign polarity labels." ></td>
	<td class="line x" title="61:226	WM, SM, and ES have evaluated their systems against manually annotated word-sense data." ></td>
	<td class="line x" title="62:226	WMs annotations are described above; SMs are similar." ></td>
	<td class="line x" title="63:226	In the scheme ES use (Cerini et al., 2007), senses are assigned three scores, for positivity, negativity, and neutrality." ></td>
	<td class="line x" title="64:226	There is no unambiguous mapping between the labels of WM/SM and ES, first because WM/SM use distinct classes and ES use numerical ratings, and second because WM/SM distinguish between objective senses on the one hand and neutral subjective senses on the other, while those are both neutral in the scheme used by ES." ></td>
	<td class="line x" title="65:226	WM use an unsupervised corpus-based approach, in which subjectivity labels are assigned to word senses based on a set of distributionally similar words in a corpus annotated with subjective expressions." ></td>
	<td class="line x" title="66:226	SM explore methods that use existing resources that do not require manually annotated data; they also implement a supervised system for comparison, which we will call SMsup." ></td>
	<td class="line x" title="67:226	The other three groups start with positive and negative seed sets and expand them by adding synonyms and antonyms, and traversing horizontal links in WordNet." ></td>
	<td class="line x" title="68:226	AB, ES, and SMsup additionally use information contained in glosses; AB also use hyponyms; SMsup also uses relation and POS features." ></td>
	<td class="line x" title="69:226	AB perform multiple runs of their system to assign fuzzy categories to senses." ></td>
	<td class="line x" title="70:226	ES use a semi-supervised, multiple-classifier learning approach." ></td>
	<td class="line x" title="71:226	In a later paper, (Esuli and Sebastiani, 2007), ES again use information in glosses, applying a random walk ranking algorithm to a graph in which synsets are linked if a member of the first synset appears in the gloss of the second." ></td>
	<td class="line x" title="72:226	Like ES and SMsup, we use machine learning, but with more diverse sources of knowledge." ></td>
	<td class="line x" title="73:226	Further, several of our features are novel for the task." ></td>
	<td class="line x" title="74:226	The LCS features (Section 6.1) detect subjectivity by measuring the similarity of a candidate word sense with a seed set." ></td>
	<td class="line x" title="75:226	WM also use a similarity measure, but as a way to filter the output of a measure of distributional similarity (selecting words for a given word sense), not as we do to cumulatively calculate the subjectivity of a word sense." ></td>
	<td class="line x" title="76:226	Another novel aspect of our similarity features is that they are particularized to domain, which greatly reduces calculation." ></td>
	<td class="line x" title="77:226	The domain subjectivity LCS features (Section 6.2) are also novel for our task." ></td>
	<td class="line x" title="78:226	So is augmenting seed sets with monosemous words, for greater coverage without requiring human intervention or sacrificing quality." ></td>
	<td class="line x" title="79:226	Note that none of our features as we specifically define them has been used in previous work; combining them together, our approach outperforms previous approaches." ></td>
	<td class="line x" title="80:226	12 4 Lexicon and Annotations We use the subjectivity lexicon of (Wiebe and Riloff, 2005)1 both to create a subjective seed set and to create the experimental data sets." ></td>
	<td class="line x" title="81:226	The lexicon is a list of words and phrases that have subjective uses, though only word entries are used in this paper (i.e., we do not address phrases at this point)." ></td>
	<td class="line x" title="82:226	Some entries are from manually developed resources, including the General Inquirer, while others were derived from corpora using automatic methods." ></td>
	<td class="line x" title="83:226	Through manual review and empirical testing on data, (Wiebe and Riloff, 2005) divided the clues into strong (strongsubj) and weak (weaksubj) subjectivity clues." ></td>
	<td class="line x" title="84:226	Strongsubj clues have subjective meanings with high probability, and weaksubj clues have subjective meanings with lower probability." ></td>
	<td class="line x" title="85:226	To support our experiments, we annotated the senses2 of polysemous nouns selected from the lexicon, using WMs annotation scheme described in Section 2." ></td>
	<td class="line x" title="86:226	Due to time constraints, only some of the data was labeled through consensus labeling by two annotators; the rest was labeled by one annotator." ></td>
	<td class="line x" title="87:226	Overall, 2875 senses for 882 words were annotated." ></td>
	<td class="line x" title="88:226	Even though all are senses of words from the subjectivity lexicon, only 1383 (48%) of the senses are subjective." ></td>
	<td class="line x" title="89:226	The words labeled strongsubj are in fact less ambiguous than those labeled weaksubj in our analysis, thus supporting the reliability classifications in the lexicon." ></td>
	<td class="line x" title="90:226	55% (1038/1924) of the senses of strongsubj words are subjective, while only 36% (345/951) of the senses of weaksubj words are subjective." ></td>
	<td class="line x" title="91:226	For the analysis in Section 7.3, we form subsets of the data annotated here to test performance of our method on different data compositions." ></td>
	<td class="line x" title="92:226	5 Seed Sets Both subjective and objective seed sets are used to define the features described below." ></td>
	<td class="line x" title="93:226	For seeds, a large number is desirable for greater coverage, although high quality is also important." ></td>
	<td class="line x" title="94:226	We begin to build our subjective seed set by adding the monosemous strongsubj nouns of the subjectivity lexicon (there are 397 of these)." ></td>
	<td class="line x" title="95:226	Since they are monosemous, they pose no problem of sense ambiguity." ></td>
	<td class="line x" title="96:226	We 1Available at http://www.cs.pitt.edu/mpqa 2In WordNet 2.0 then expand the set with their hyponyms, as they were found useful in previous work by AB (2006b; 2006a)." ></td>
	<td class="line x" title="97:226	This yields a subjective seed set of 645 senses." ></td>
	<td class="line x" title="98:226	After removing the word senses that belong to the same synset, so that only one word sense per synset is left, we ended up with 603 senses." ></td>
	<td class="line x" title="99:226	To create the objective seed set, two annotators manually annotated 800 random senses from WordNet, and selected for the objective seed set the ones they both agreed are clearly objective." ></td>
	<td class="line x" title="100:226	This creates an objective seed set of 727." ></td>
	<td class="line x" title="101:226	Again we removed multiple senses from the same synset leaving us with 722." ></td>
	<td class="line x" title="102:226	The other 73 senses they annotated are added to the mixed data set described below." ></td>
	<td class="line x" title="103:226	As this sampling shows, WordNet nouns are highly skewed toward objective senses, so finding an objective seed set is not difficult." ></td>
	<td class="line x" title="104:226	6 Features 6.1 Sense Subjectivity LCS Feature This feature measures the similarity of a target sense with members of the subjective seed set." ></td>
	<td class="line x" title="105:226	Here, similarity between two senses is determined by the extent to which they share information, measured by using the information content associated with their least common subsumer." ></td>
	<td class="line x" title="106:226	For an intuition behind this feature, consider this example." ></td>
	<td class="line x" title="107:226	In WordNet, the hypernym of the strong criticism sense of attack is criticism." ></td>
	<td class="line x" title="108:226	Several other negative subjective senses are descendants of criticism, including the relevant senses of fire, thrust, and rebuke." ></td>
	<td class="line x" title="109:226	Going up one more level, the hypernym of criticism is the expression of disapproval meaning of disapproval, which has several additional negative subjective descendants, such as the expression of opposition and disapproval sense of discouragement." ></td>
	<td class="line x" title="110:226	Our hypothesis is that the cases where subjectivity is preserved in the hypernym structure, or where hypernyms do lead from subjective senses to others, are the ones that have the highest least common subsumer score with the seed set of known subjective senses." ></td>
	<td class="line x" title="111:226	We calculate similarity using the informationcontent based measure proposed in (Resnik, 1995), as implemented in the WordNet::Similarity package (using the default option in which LCS values are computed over the SemCor corpus).3 Given a 3http://search.cpan.org/dist/WordNet-Similarity/ 13 taxonomy such as WordNet, the information content associated with a concept is determined as the likelihood of encountering that concept, defined as log(p(C)), where p(C) is the probability of seeing concept C in a corpus." ></td>
	<td class="line x" title="112:226	The similarity between two concepts is then defined in terms of information content as: LCSs(C1,C2) = max[log(p(C))], where C is the concept that subsumes both C1 and C2 and has the highest information content (i.e., it is the least common subsumer (LCS))." ></td>
	<td class="line x" title="113:226	For this feature, a score is assigned to a target sense based on its semantic similarity to the members of a seed set; in particular, the maximum such similarity is used." ></td>
	<td class="line x" title="114:226	For a target sense t and a seed set S, we could have used the following score: Score(t,S) = maxsS LCSs(t,s) However, several researchers have noted that subjectivity may be domain specific." ></td>
	<td class="line x" title="115:226	A version of WordNet exists, WordNet Domains (Gliozzo et al., 2005), which associates each synset with one of the domains in the Dewey Decimal library classification." ></td>
	<td class="line x" title="116:226	After sorting our subjective seed set into different domains, we observed that over 80% of the subjective seed senses are concentrated in six domains (the rest are distributed among 35 domains)." ></td>
	<td class="line x" title="117:226	Thus, we decided to particularize the semantic similarity feature to domain, such that only the subset of the seed set in the same domain as the target sense is used to compute the feature." ></td>
	<td class="line x" title="118:226	This involves much less calculation, as LCS values are calculated only with respect to a subset of the seed set." ></td>
	<td class="line x" title="119:226	We hypothesized that this would still be an effective feature, while being more efficient to calculate." ></td>
	<td class="line x" title="120:226	This will be important when this method is applied to large resources such as the entire WordNet." ></td>
	<td class="line x" title="121:226	Thus, for seed set S and target sense t which is in domain D, the feature is defined as the following score: SenseLCSscore(t,D,S) = max dDS LCSs(t,d) The seed set is a parameter, so we could have defined a feature reflecting similarity to the objective seed set as well." ></td>
	<td class="line x" title="122:226	Since WordNet is already highly skewed toward objective noun senses, any naive classifier need only guess the majority class for high accuracy for the objective senses." ></td>
	<td class="line x" title="123:226	We included only a subjective feature to put more emphasis on the subjective senses." ></td>
	<td class="line x" title="124:226	In the future, features could be defined with respect to objectivity, as well as polarity and other properties of subjectivity." ></td>
	<td class="line x" title="125:226	6.2 Domain Subjectivity LCS Score We also include a feature reflecting the subjectivity of the domain of the target sense." ></td>
	<td class="line x" title="126:226	Domains are assigned scores as follows." ></td>
	<td class="line x" title="127:226	For domain D and seed set S: DomainLCSscore(D,S) = avedDSMemLCSscore(d,D,S) where: MemLCSscore(d,D,S) = max diDS,dinegationslash=d LCSs(d,di) The value of this feature for a sense is the score assigned to that senses domain." ></td>
	<td class="line x" title="128:226	6.3 Common Related Senses This feature is based on the intersection between the set of senses related (via WordNet relations) to the target sense and the set of senses related to members of a seed set." ></td>
	<td class="line x" title="129:226	First, for the target sense and each member of the seed set, a set of related senses is formed consisting of its synonyms, antonyms and direct hypernyms as defined by WordNet." ></td>
	<td class="line x" title="130:226	For a sense s, R(s) is s together with its related senses." ></td>
	<td class="line x" title="131:226	Then, given a target sense t and a seed set S we compute an average percentage overlap as follows: RelOverlap(t,S) = summationtext siS |R(t)R(si)| max(|R(t)|,|R(si)|) |S| The value of a feature is its score." ></td>
	<td class="line x" title="132:226	Two features are included in the experiments below, one for each of the subjective and objective seed sets." ></td>
	<td class="line x" title="133:226	6.4 Gloss-based features These features are Lesk-style features (Lesk, 1986) that exploit overlaps between glosses of target and seed senses." ></td>
	<td class="line x" title="134:226	We include two types in our work." ></td>
	<td class="line x" title="135:226	6.4.1 Average Percentage Gloss Overlap Features For a sense s, gloss(s) is the set of stems in the gloss of s (excluding stop words)." ></td>
	<td class="line x" title="136:226	Then, given a tar14 get sense t and a seed set S, we compute an average percentage overlap as follows: GlOverlap(t,S) = summationtext siS |gloss(t)rR(si)gloss(r)| max(|gloss(t)|,|rR(si)gloss(r)|) |S| As above, R(s) is considered for each seed sense s, but now only the target sense t is considered, not R(t)." ></td>
	<td class="line x" title="137:226	We did this because we hypothesized that the gloss can provide sufficient context for a given target sense, so that the addition of related words is not necessary." ></td>
	<td class="line x" title="138:226	We include two features, one for each of the subjective and objective seed sets." ></td>
	<td class="line x" title="139:226	6.4.2 Vector Gloss Overlap Features For this feature we also consider overlaps of stems in glosses (excluding stop words)." ></td>
	<td class="line x" title="140:226	The overlaps considered are between the gloss of the target sense t and the glosses of R(s) for all s in a seed set (for convenience, we will refer to these as seedRelationSets)." ></td>
	<td class="line x" title="141:226	A vector of stems is created, one for each stem (excluding stop words) that appears in a gloss of a member of seedRelationSets." ></td>
	<td class="line x" title="142:226	If a stem in the gloss of the target sense appears in this vector, then the vector entry for that stem is the total count of that stem in the glosses of the target sense and all members of seedRelationSets." ></td>
	<td class="line x" title="143:226	A feature is created for each vector entry whose value is the count at that position." ></td>
	<td class="line x" title="144:226	Thus, these features consider counts of individual stems, rather than average proportions of overlaps, as for the previous type of gloss feature." ></td>
	<td class="line x" title="145:226	Two vectors of features are used, one where the seed set is the subjective seed set, and one where it is the objective seed set." ></td>
	<td class="line x" title="146:226	6.5 Summary In summary, we use the following features (here, SS is the subjective seed set and OS is the objective one)." ></td>
	<td class="line x" title="147:226	1." ></td>
	<td class="line x" title="148:226	SenseLCSscore(t,D,SS) 2." ></td>
	<td class="line x" title="149:226	DomainLCSscore(D,SS) 3." ></td>
	<td class="line x" title="150:226	RelOverlap(t,SS) 4." ></td>
	<td class="line x" title="151:226	RelOverlap(t,OS) 5." ></td>
	<td class="line x" title="152:226	GlOverlap(t,SS) 6." ></td>
	<td class="line x" title="153:226	GlOverlap(t,OS) Features Acc P R F All 77.3 72.8 74.3 73.5 Standalone Ablation Results All 77.3 72.8 74.3 73.5 LCS 68.2 69.3 44.2 54.0 Gloss vector 74.3 71.2 68.5 69.8 Overlaps 69.4 75.8 40.6 52.9 Leave-One-Out Ablation Results All 77.3 72.8 74.3 73.5 LCS 75.2 70.9 70.6 70.7 Gloss vector 75.0 74.4 61.8 67.5 Overlaps 74.8 71.9 73.8 72.8 Table 1: Results for the mixed corpus (2354 senses, 57.82% O)) 7." ></td>
	<td class="line x" title="154:226	Vector of gloss words (SS) 8." ></td>
	<td class="line x" title="155:226	Vector of gloss words (OS) 7 Experiments We perform 10-fold cross validation experiments on several data sets, using SVM light (Joachims, 1999)4 under its default settings." ></td>
	<td class="line x" title="156:226	Based on our random sampling of WordNet, it appears that WordNet nouns are highly skewed toward objective senses." ></td>
	<td class="line x" title="157:226	(Esuli and Sebastiani, 2007) argue that random sampling from WordNet would yield a corpus mostly consisting of objective (neutral) senses, which would be pretty useless as a benchmark for testing derived lexical resources for opinion mining [p. 428]. So, they use a mixture of subjective and objective senses in their data set." ></td>
	<td class="line x" title="158:226	To create a mixed corpus for our task, we annotated a second random sample from WordNet (which is as skewed as the previously mentioned one)." ></td>
	<td class="line x" title="159:226	We added together all of the senses of words in the lexicon which we annotated, the leftover senses from the selection of objective seed senses, and this new sample." ></td>
	<td class="line x" title="160:226	We removed duplicates, multiple senses from the same synset, and any senses belonging to the same synset in either of the seed sets." ></td>
	<td class="line x" title="161:226	This resulted in a corpus of 2354 senses, 993 (42.18%) of which are subjective and 1361 (57.82%) of which are objective." ></td>
	<td class="line x" title="162:226	The results with all of our features on this mixed corpus are given in Row 1 of Table 1." ></td>
	<td class="line x" title="163:226	In Table 1, the 4http://svmlight.joachims.org/ 15 first column identifies the features, which in this case is all of them." ></td>
	<td class="line x" title="164:226	The next three columns show overall accuracy, and precision and recall for finding subjective senses." ></td>
	<td class="line x" title="165:226	The baseline accuracy for the mixed data set (guessing the more frequent class, which is objective) is 57.82%." ></td>
	<td class="line x" title="166:226	As the table shows, the accuracy is substantially above baseline.5 7.1 Analysis and Discussion In this section, we seek to gain insights by performing ablation studies, evaluating our method on different data compositions, and comparing our results to previous results." ></td>
	<td class="line x" title="167:226	7.2 Ablation Studies Since there are several features, we divided them into sets for the ablation studies." ></td>
	<td class="line x" title="168:226	The vector-ofgloss-words features are the most similar to ones used in previous work." ></td>
	<td class="line x" title="169:226	Thus, we opted to treat them as one ablation group (Gloss vector)." ></td>
	<td class="line x" title="170:226	The Overlaps group includes the RelOverlap(t,SS), RelOverlap(t,OS), GlOverlap(t,SS), and GlOverlap(t,OS) features." ></td>
	<td class="line x" title="171:226	Finally, the LCS group includes the SenseLCSscore and the DomainLCSscore features." ></td>
	<td class="line x" title="172:226	There are two types of ablation studies." ></td>
	<td class="line x" title="173:226	In the first, one group of features at a time is included." ></td>
	<td class="line x" title="174:226	Those results are in the middle section of Table 1." ></td>
	<td class="line x" title="175:226	Thus, for example, the row labeled LCS in this section is for an experiment using only the LCS features." ></td>
	<td class="line x" title="176:226	In comparison to performance when all features are used, F-measure for the Overlaps and LCS ablations is significantly different at the p < .01 level, and, for the Gloss Vector ablation, it is significantly different at the p = .052 level (one-tailed t-test)." ></td>
	<td class="line x" title="177:226	Thus, all of the features together have better performance than any single type of feature alone." ></td>
	<td class="line x" title="178:226	In the second type of ablation study, we use all the features minus one group of features at a time." ></td>
	<td class="line x" title="179:226	The results are in the bottom section of Table 1." ></td>
	<td class="line x" title="180:226	Thus, for example, the row labeled LCS in this section is for an experiment using all but the LCS features." ></td>
	<td class="line x" title="181:226	F-measures for LCS and Gloss vector are significantly different at the p = .056 and p = .014 levels, respectively." ></td>
	<td class="line x" title="182:226	However, F-measure for the Overlaps ablation is not significantly different (p = .39)." ></td>
	<td class="line x" title="183:226	5Note that, because the majority class is O, baseline recall (and thus F-measure) is 0." ></td>
	<td class="line x" title="184:226	Data (#senses) Acc P R F mixed (2354 57.8% O) 77.3 72.8 74.3 73.5 strong+weak (1132) 77.7 76.8 78.9 77.8 weaksubj (566) 71.3 70.3 71.1 70.7 strongsubj (566) 78.6 78.8 78.6 78.7 Table 2: Results for different data sets (all are 50% S, unless otherwise notes) These results provide evidence that LCS and Gloss vector are better together than either of them alone." ></td>
	<td class="line x" title="185:226	7.3 Results on Different Data Sets Several methods have been developed for identifying subjective words." ></td>
	<td class="line x" title="186:226	Perhaps an effective strategy would be to begin with a word-level subjectivity lexicon, and then perform subjectivity sense labeling to sort the subjective from objective senses of those words." ></td>
	<td class="line x" title="187:226	We also wondered about the relative effectiveness of our method on strongsubj versus weaksubj clues." ></td>
	<td class="line x" title="188:226	To answer these questions, we apply the full model (again in 10-fold cross validation experiments) to data sets composed of senses of polysemous words in the subjectivity lexicon." ></td>
	<td class="line x" title="189:226	To support comparison, all of the data sets in this section have a 50%-50% objective/subjective distribution.6 The results are presented in Table 2." ></td>
	<td class="line x" title="190:226	For comparison, the first row repeats the results for the mixed corpus from Table 1." ></td>
	<td class="line x" title="191:226	The second row shows results for a corpus of senses of a mixture of strongsubj and weaksubj words." ></td>
	<td class="line x" title="192:226	The corpus was created by selecting a mixture of strongsubj and weaksubj words, extracting their senses and the S/O labels applied to them in Section 4, and then randomly removing senses of the more frequent class until the distribution is uniform." ></td>
	<td class="line x" title="193:226	We see that the results on this corpus are better than on the mixed data set, even though the baseline accuracy is lower and the corpus is smaller." ></td>
	<td class="line x" title="194:226	This supports the idea that an effective strategy would be to first identify opinion-bearing words, and then apply our method to those words to sort out their subjective and objective senses." ></td>
	<td class="line x" title="195:226	The third row shows results for a weaksubj subset 6As with the mixed data set, we removed from these data sets multiple senses from the same synset and any senses in the same synset in either of the seed sets." ></td>
	<td class="line x" title="196:226	16 Method P R F Our method 56.8 66.0 61.1 WM, 60% recall 44.0 66.0 52.8 SentiWordNet mapping 60.0 17.3 26.8 Table 3: Results for WM Corpus (212 senses, 76% O) Method A P R F Our Method 81.3% 60.3% 63.3% 61.8% SM CV* 82.4% 70.8% 41.1% 52.0% SM SL* 78.3% 53.0% 57.4% 54.9% Table 4: Results for SM Corpus (484 senses, 76.9% O) of the strong+weak corpus and the fourth shows results for a strongsubj subset that is of the same size." ></td>
	<td class="line x" title="197:226	As expected, the results for the weaksubj senses are lower while those for the strongsubj senses are higher, as weaksubj clues are more ambiguous." ></td>
	<td class="line x" title="198:226	7.4 Comparisons with Previous Work WM and SM address the same task as we do." ></td>
	<td class="line x" title="199:226	To compare our results to theirs, we apply our full model (in 10-fold cross validation experiments) to their data sets.7 Table 3 has the WM data set results." ></td>
	<td class="line x" title="200:226	WM rank their senses and present their results in the form of precision recall curves." ></td>
	<td class="line x" title="201:226	The second row of Table 3 shows their results at the recall level achieved by our method (66%)." ></td>
	<td class="line x" title="202:226	Their precision at that level is substantially below ours." ></td>
	<td class="line x" title="203:226	Turning to ES, to create S/O annotations, we applied the following heuristic mapping (which is also used by SM for the purpose of comparison): any sense for which the sum of positive and negative scores is greater than or equal to 0.5 is S, otherwise it is O. We then evaluate the mapped tags against the gold standard of WM." ></td>
	<td class="line x" title="204:226	The results are in Row 3 of Table 3." ></td>
	<td class="line x" title="205:226	Note that this mapping is not fair to SentiWordNet, as the tasks are quite different, and we do not believe any conclusions can be drawn." ></td>
	<td class="line x" title="206:226	We include the results to eliminate the possibility that their method is as good ours on our task, despite the differences between the tasks." ></td>
	<td class="line x" title="207:226	Table 4 has the results for the noun subset of SMs 7The WM data set is available at http://www.cs.pitt.edu/www.cs.pitt.edu/wiebe." ></td>
	<td class="line x" title="208:226	ES applied their method in (2006b) to WordNet, and made the results available as SentiWordNet at http://sentiwordnet.isti.cnr.it/." ></td>
	<td class="line x" title="209:226	data set, which is the data set used by ES, reannotated by SM." ></td>
	<td class="line x" title="210:226	CV* is their supervised system and SL* is their best non-supervised one." ></td>
	<td class="line x" title="211:226	Our method has higher F-measure than the others.8 Note that the focus of SMs work is not supervised machine learning." ></td>
	<td class="line x" title="212:226	8 Conclusions In this paper, we introduced an integrative approach to automatic subjectivity word sense labeling which combines features exploiting the hierarchical structure and domain information of WordNet, as well as similarity of glosses and overlap among sets of semantically related words." ></td>
	<td class="line x" title="213:226	There are several contributions." ></td>
	<td class="line x" title="214:226	First, we learn several things." ></td>
	<td class="line x" title="215:226	We found (in Section 4) that even reliable lists of subjective (opinion-bearing) words have many objective senses." ></td>
	<td class="line x" title="216:226	We asked if wordand sense-level approaches could be used effectively in tandem, and found (in Section 7.3) that an effective strategy is to first identify opinion-bearing words, and then apply our method to sort out their subjective and objective senses." ></td>
	<td class="line x" title="217:226	We also found (in Section 7.2) that the entire set of features gives better results than any individual type of feature alone." ></td>
	<td class="line x" title="218:226	Second, several of the features are novel for our task, including those exploiting the hierarchical structure of a lexical resource, domain information, and relations to seed sets expanded with monosemous senses." ></td>
	<td class="line x" title="219:226	Finally, the combination of our particular features is effective." ></td>
	<td class="line x" title="220:226	For example, on senses of words from a subjectivity lexicon, accuracies range from 20 to 29 percentage points above baseline." ></td>
	<td class="line x" title="221:226	Further, our combination of features outperforms previous approaches." ></td>
	<td class="line x" title="222:226	Acknowledgments This work was supported in part by National Science Foundation awards #0840632 and #0840608." ></td>
	<td class="line x" title="223:226	The authors are grateful to Fangzhong Su and Katja Markert for making their data set available, and to the three paper reviewers for their helpful suggestions." ></td>
	<td class="line x" title="224:226	8We performed the same type of evaluation as in SMs paper." ></td>
	<td class="line x" title="225:226	That is, we assign a subjectivity label to one word sense for each synset, which is the same as applying a subjectivity label to a synset as a whole as done by SM." ></td>
	<td class="line x" title="226:226	17" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1055
An Iterative Reinforcement Approach for Fine-Grained Opinion Mining
Du, Weifu;Tan, Songbo;"></td>
	<td class="line x" title="1:133	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 486493, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:133	c 2009 Association for Computational Linguistics An Iterative Reinforcement Approach for Fine-Grained Opinion Mining  Weifu Du Haerbin Institute of Technology Haerbin, China duweifu@software.ict.ac.cn Songbo Tan Institute of Computing Technology Beijing, China tansongbo@software.ict.ac.cn  Abstract With the in-depth study of sentiment analysis research, finer-grained opinion mining, which aims to detect opinions on different review features as opposed to the whole review level, has been receiving more and more attention in the sentiment analysis research community recently." ></td>
	<td class="line x" title="3:133	Most of existing approaches rely mainly on the template extraction to identify the explicit relatedness between product feature and opinion terms, which is insufficient to detect the implicit review features and mine the hidden sentiment association in reviews, which satisfies (1) the review features are not appear explicit in the review sentences; (2) it can be deduced by the opinion words in its context." ></td>
	<td class="line x" title="4:133	From an information theoretic point of view, this paper proposed an iterative reinforcement framework based on the improved information bottleneck algorithm to address such problem." ></td>
	<td class="line x" title="5:133	More specifically, the approach clusters product features and opinion words simultaneously and iteratively by fusing both their semantic information and co-occurrence information." ></td>
	<td class="line x" title="6:133	The experimental results demonstrate that our approach outperforms the template extraction based approaches." ></td>
	<td class="line x" title="7:133	1 Introduction In the Web2.0 era, the Internet turns from a static information media into a platform for dynamic information exchanging, on which people can express their views and show their selfhood." ></td>
	<td class="line x" title="8:133	More and more people are willing to record their feelings (blog), give voice to public affairs (news review), express their likes and dislikes on products (product review), and so on." ></td>
	<td class="line x" title="9:133	In the face of the volume of sentimental information available on the Internet continues to increase, there is growing interest in helping people better find, filter, and manage these resources." ></td>
	<td class="line x" title="10:133	Automatic opinion mining (Turney et al., 2003; Ku et al., 2006; Devitt et al., 2007) can play an important role in a wide variety of more flexible and dynamic information management tasks." ></td>
	<td class="line x" title="11:133	For example, with the help of sentiment analysis system, in the field of public administration, the administrators can receive the feedbacks on one policy in a timelier manner; in the field of business, manufacturers can perform more targeted updates on products to improve the consumer experience." ></td>
	<td class="line oc" title="12:133	The research of opinion mining began in 1997, the early research results mainly focused on the polarity of opinion words (Hatzivassiloglou et al., 1997) and treated the text-level opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text (Turney et al., 2003; Pang et al., 2002; Zagibalov et al., 2008;)." ></td>
	<td class="line x" title="13:133	With the in-depth study of opinion mining, researchers committed their efforts for more accurate results: the research of sentiment summarization (Philip et al., 2004; Hu et al., KDD 2004), domain transfer problem of the sentiment analysis (Kanayama et al., 2006; Tan et al., 2007; Blitzer et al., 2007; Tan et al., 2008; Andreevskaia et al., 2008; Tan et al., 2009) and finegrained opinion mining (Hatzivassiloglou et al., 2000; Takamura et al., 2007; Bloom et al., 2007; Wang et al., 2008; Titov et al., 2008) are the main branches of the research of opinion mining." ></td>
	<td class="line x" title="14:133	In this paper, we focus on the fine-grained (feature-level) opinion mining." ></td>
	<td class="line x" title="15:133	For many applications (e.g. the task of public affairs review analysis and the products review analysis), simply judging the sentiment orientation of a review unit is not sufficient." ></td>
	<td class="line x" title="16:133	Researchers (Kushal, 2003; Hu et al., KDD 2004; Hu et al., AAAI 2004; Popescu et al., 2005) began to work on finer-grained opinion mining which predicts the sentiment orientation related to different review features." ></td>
	<td class="line x" title="17:133	The task is known as feature-level opinion mining." ></td>
	<td class="line x" title="18:133	486 In feature-level opinion mining, most of the existing researches associate product features and opinion words by their explicit co-occurrence." ></td>
	<td class="line x" title="19:133	Template extraction based method (Popescu et al., 2005) and association rule mining based method (Hu et al., AAAI 2004) are the representative ones." ></td>
	<td class="line x" title="20:133	These approaches did good jobs for identifying the review features that appear explicitly in reviews, however, real reviews from customers are usually complicated." ></td>
	<td class="line x" title="21:133	In some cases, the review features are implicit in the review sentences, but can be deduced by the opinion words in its context." ></td>
	<td class="line x" title="22:133	The detection of such hidden sentiment association is a big challenge in feature-level opinion mining on Chinese reviews due to the nature of Chinese language (Qi et al., 2008)." ></td>
	<td class="line x" title="23:133	Obviously, neither the template extraction based method nor the association rule mining based method is effective for such cases." ></td>
	<td class="line x" title="24:133	Moreover, in some cases, even if the review features appear explicitly in the review sentences, the co-occurrence information between review features and opinion words is too quantitatively sparse to be utilized." ></td>
	<td class="line x" title="25:133	So we consider whether it is a more sensible way to construct or cluster review feature groups and opinion words groups to mine the implicit or hidden sentiment association in the reviews." ></td>
	<td class="line x" title="26:133	The general approach will cluster the two types of objects separately, which neglects the highly interrelationship." ></td>
	<td class="line x" title="27:133	To address this problem, in this paper, we propose an iterative reinforcement framework, under which we cluster product features and opinion words simultaneously and iteratively by fusing both their semantic information and sentiment link information." ></td>
	<td class="line x" title="28:133	We take improved information bottleneck algorithm (Tishby, 1999) as the kernel of the proposed framework." ></td>
	<td class="line x" title="29:133	The information bottleneck approach was presented by Tishby (1999)." ></td>
	<td class="line x" title="30:133	The basic idea of the approach is that it treats the clustering problems from the information compressing point of view, and takes this problem as a case of much more fundamental problem: what are the features of the variable X that are relevant for the prediction of another, relevance, variable Y?" ></td>
	<td class="line x" title="31:133	Based on the information theory, the problem can be formulated as: find a compressed representation of the variable X, denoted C, such that the mutual information between C and Y is as high as possible, under a constraint on the mutual information between X and C. For our case, take the hotel reviews as example, X is one type of objects of review features (e.g. facilities, service, surrounding environment, etc) or opinion words (e.g. perfect, circumspect, quiet, etc), and Y is another one." ></td>
	<td class="line x" title="32:133	Given some review features (or opinion words) gained from review corpus, we want to assemble them into categories, conserving the information about opinion words (or review features) as high as possible." ></td>
	<td class="line x" title="33:133	The information bottleneck algorithm has some benefits, mainly including (1) it treats the trade-off of precision versus complexity of clustering model through the rate distortion theory, which is a subfield of information theory; (2) it defines the distance or similarity in a well-defined way based on the mutual information." ></td>
	<td class="line x" title="34:133	The efficiency of information bottleneck algorithm (Slonim and Tishby, 2000) motivates us to take it as the kernel of our framework." ></td>
	<td class="line x" title="35:133	As far as we know, this approach has not been employed in opinion mining yet." ></td>
	<td class="line x" title="36:133	In traditional information bottleneck approach, the distance between two data objects is measured by the Jensen-Shannon divergence (Lin, 1991), which aims to measure the divergence between two probability distributions." ></td>
	<td class="line x" title="37:133	We alter this measure to integrate more semantic information, which will be illustrated in detail in the following sections, and the experimental result shows the effectiveness of the alteration." ></td>
	<td class="line x" title="38:133	It would be worthwhile to highlight several aspects of our work here: null We propose an iterative reinforcement framework, and under this framework, review feature words and opinion words are organized into categories in a simultaneous and iterative manner." ></td>
	<td class="line x" title="39:133	null In the process of clustering, the semantic information and the co-occurrence information are integrated." ></td>
	<td class="line x" title="40:133	null The experimental results on real Chinese web reviews demonstrate that proposed method outperforms the template extraction based algorithm." ></td>
	<td class="line x" title="41:133	2 Proposed Algorithm 2.1 The Problem In product reviews, opinion words are used to express opinion, sentiment or attitude of reviewers." ></td>
	<td class="line x" title="42:133	Although some review units may express general opinions toward a product, most review units are 487 regarding to specific features of the product." ></td>
	<td class="line x" title="43:133	A product is always reviewed under a certain feature set F. Suppose we have got a lexical list O which includes all the opinion expressions and their sentiment polarities." ></td>
	<td class="line x" title="44:133	For the feature-level opinion mining, identifying the sentiment association between F and O is essential." ></td>
	<td class="line x" title="45:133	The key points in the whole process are as follows: null get opinion word set O (with polarity labels) null get product feature set F null identify relationships between F and O The focus of the paper is on the latter two steps, especially for the case of hidden sentiment association that the review features are implicit in the review sentences, but can be deduced by the opinion words in its context." ></td>
	<td class="line x" title="46:133	In contrast to existing explicit adjacency approaches, the proposed approach detects the sentiment association between F and O based on review feature categories and opinion word groups gained from the review corpus." ></td>
	<td class="line x" title="47:133	To this end, we first consider two sets of association objects: the set of product feature words F = {f 1 ,f 2 ,,f m } and the set of opinion words O = {o 1 ,o 2 ,o n }." ></td>
	<td class="line x" title="48:133	A weighted bipartite graph from F and O can be built, denoted by G = {F, O, R}." ></td>
	<td class="line x" title="49:133	Here R = [r ij ] is the m*n link weight matrix containing all the pair-wise weights between set F and O. The weight can be calculated with different weighting schemes, in this paper, we set r ij  by the co-appearance frequency of f i  and o j  in clause level." ></td>
	<td class="line x" title="50:133	We take F and O as two random variables, and the question of constructing or clustering the object groups can be defined as finding compressed representation of each variable that reserves the information about another variable as high as possible." ></td>
	<td class="line x" title="51:133	Take F as an example, we want to find its compression, denoted as C, such that the mutual information between C and O is as high as possible, under a constraint on the mutual information between F and C. We propose an iterative reinforcement framework to deal with the tasks." ></td>
	<td class="line x" title="52:133	An improved information bottleneck algorithm is employed in this framework, which will be illustrated in detail in the following sections." ></td>
	<td class="line x" title="53:133	2.2 Information Bottleneck Algorithm The information bottleneck method (IB) was presented by Tishby et al.(1999)." ></td>
	<td class="line x" title="55:133	According to Shannons information theory (Cover and Thomas, 1991), for the two random variables X, Y, the mutual information I(X;Y) between the random variables X, Y is given by the symmetric functional: , (|) (;) ()(|)log () xXyY py x IXY pxpyx py  =          (1) and the mutual information between them measures the relative entropy between their joint distribution p(x, y) and the product of respective marginal distributions p(x)p(y), which is the only consistent statistical measure of the information that variable X contains about variable Y (and vice versa)." ></td>
	<td class="line x" title="56:133	Roughly speaking, some of the mutual information will be lost in the process of compression, e.g.(,) ( ,)I CY I XY  (C is a compressed representation of X)." ></td>
	<td class="line x" title="58:133	This representation is defined through a (possibly stochastic) mapping between each value x X to each representative value cC . Formally, this mapping can be characterized by a conditional distribution p(c|x), inducing a soft partitioning of X values, Specifically, each value of X _is associated with all the codebook elements (C values), with some normalized probability." ></td>
	<td class="line x" title="59:133	_ The IB method is based on the following simple idea." ></td>
	<td class="line x" title="60:133	Given the empirical joint distribution of two variables, one variable is compressed so that the mutual information about the other variable is preserved as much as possible." ></td>
	<td class="line x" title="61:133	The method can be considered as finding a minimal sufficient partition or efficient relevant coding of one variable with respect to the other one." ></td>
	<td class="line x" title="62:133	This problem can be solved by introducing a Lagrange multiplier  , and then minimizing the functional: [(|)] (, ) (,)Lpc x ICX ICY=                        (2) This solution is given in terms of the three distributions that characterize every cluster cC , the prior probability for this cluster, p(c), its membership probabilities p(c|x), and its distribution over the relevance variable p(y|c)." ></td>
	<td class="line x" title="63:133	In general, the membership probabilities, p(c|x) is soft, i.e. every x X can be assigned to every cC in some (normalized) probability." ></td>
	<td class="line x" title="64:133	The information bottleneck principle determines the distortion measure between the points x and c to be the [] (|) (|)| (|) (|)log (|) KL y py x D pyx pyc pyx pyc =  , the Kullback-Leibler divergence (Cover and Thomas, 1991) between the conditional distributions p(y|x) 488 and p(y|c)." ></td>
	<td class="line x" title="65:133	Specifically, the formal optimal solution is given by the following equations which must be solved together." ></td>
	<td class="line x" title="66:133	() (|) exp( [( |)| ( |)]) (,) 1 (|) (|)()(|) () () ( | ) () KL x x pc pc x D py x py c Zx pyc pc xpxpy x pc pc pc x px    =    =    =      (3) Where (,)Z x is a normalization factor, and the single positive (Lagrange) parameter determines the softness of the classification." ></td>
	<td class="line x" title="67:133	Intuitively, in this procedure the information contained in X about Y squeezed through a compact bottleneck of clusters C, that is forced to represent the relevant part in X w.r.t to Y. An important special case is the hard clustering case where C is a deterministic function of X. That is, p(c|x) can only take values of zero or one, This restriction, which corresponds to the limit  in Eqs 3 meaning every x X  is assigned to exactly one cluster cC  with a probability of one and to all the others with a probability of zero." ></td>
	<td class="line x" title="68:133	This yields a natural distance measure between distributions which can be easily implemented in an agglomerative hierarchical clustering procedure (Slonim and Tishby, 1999)." ></td>
	<td class="line x" title="69:133	1, (|) 0, 1 (|) (,) () () () xc xc if x c pc x otherwise pyc pxy pc pc px    =      =    =                                    (4) The algorithm starts with a trivial partitioning into |X| singleton clusters, where each cluster contains exactly one element of X. At each step we merge two components of the current partition into a single new component in a way that locally minimizes the loss of mutual information about the categories." ></td>
	<td class="line x" title="70:133	Every merger, * (, ) ij cc c , is formally defined by the following equation: * * ** * 1, (|) 0, () () (| ) (| ) (| ) () () () () () ij j i ij ij xcorxc pc x otherwise pc pc p yc pyc pyc pc pc pc pc pc   =       =+    =+         (5) The decrease in the mutual information I(C, Y) due to this merger is defined by (, ) ( ,) ( ,) i j before after I cc IC Y IC Y               (6) When (,) before I CY and (,) after I CYare the information values before and after the merger, respectively." ></td>
	<td class="line x" title="71:133	After a little algebra, one can see ( ) (, ) () ( ) (| ),(| ) ij i j JS i j I cc pc pc D pyc pyc +   (7) Where the functional D JS   is the Jensen-Shannon divergence (Lin, 1991), defined as ^^ ,||| JS i j i KL i j KL j D pp D p p D p p   =+        (8) where in our case {}{ } {} ** ^ ,(|),(|) () () ,, () () (| ) (| ) ij i j j i ij iij j pp pyc pyc pc pc pc pc ppyc pyc              =+                       (9) By introducing the information optimization criterion the resulting similarity measure directly emerges from the analysis." ></td>
	<td class="line x" title="72:133	The algorithm is now very simple." ></td>
	<td class="line x" title="73:133	At each step we perform the best possible merger, i.e. merge the clusters {, } ij cc which minimize (, ) ij I cc . 2.3 Improved Information Bottleneck Algorithm for Semantic Information In traditional information bottleneck approach, the distance between two data objects is measured by the difference of information values before and after the merger, which is measured by the JensenShannon divergence." ></td>
	<td class="line x" title="74:133	This divergence aims to measure the divergence between two probability distributions." ></td>
	<td class="line x" title="75:133	For our case, the divergence is based on the co-occurrence information between the two variables F and O. While the co-occurrence in corpus is usually quantitatively sparse; additionally, Statistics based 489 on word-occurrence loses semantic related information." ></td>
	<td class="line x" title="76:133	To avoid such reversed effects, in the proposed framework we combine the co-occurrence information and semantic information as the final distance between the two types of objects." ></td>
	<td class="line x" title="77:133	(, ) (, ) (1 )(, ) {}{} i j semantic i j ij ij ij DX X D X X IX X where X F X F X O X O   = +       (10) In equation 10, the distance between two data objects X i  and X j  is denoted as a linear combination of semantic distance and information value difference." ></td>
	<td class="line x" title="78:133	The parameter   reflects the contribution of different distances to the final distance." ></td>
	<td class="line x" title="79:133	Input: Joint probability distribution p(f,o) Output: A partition of F into m clusters, m {1,,|F|}, and a partition of O into n clusters  n{1,,|O|} 1." ></td>
	<td class="line x" title="80:133	t0 2." ></td>
	<td class="line x" title="81:133	Repeat a. Construct CF t F t  b. i, j=1,,|CF t |, i<j, calculate          (,)(1 )(,) tttt ij semantic i j i j dDcfcf Icfcf+ c. for m|CF t |-1 to 1 1) find the indices {i, j}, for which d ij t  is minimized 2) merge {cf i t , cf j t }into cf * t  3) update CF t {CF t  -{cf i t , cf j t }}U {cf * t } 4) update d ij t  costs w.r.t cf * t  d. Construct CO t O t  e. i, j=1,,|CO t |, i<j,calculate      (, )(1 )(, ) tttt ij semantic i j i j dDcoco Icoco+ f. for n|CO t |-1 to 1 1) find the indices {i, j}, for which d ij t  is minimized 2) merge {co i t ,co j t }into co * t  3) update CO t  {CO t  -{co i t ,co j t }}U {co * t } 4) update d ij t  costs w.r.t co * t  g. tt+1 3." ></td>
	<td class="line x" title="82:133	until (CF t  = CF t-1  and CO t  =CO t-1 ) Figure 1: Pseudo-code of semantic information bottleneck in iterative reinforcement framework  The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006)." ></td>
	<td class="line x" title="83:133	In this paper, we use the Chinese lexicon HowNet 1 . The basic idea of the iterative reinforcement principle is to propagate the clustered results between different type data objects by updating their inter-relationship spaces." ></td>
	<td class="line x" title="84:133	The clustering process can begin from an arbitrary type of data object." ></td>
	<td class="line x" title="85:133	The clustering results of one data object type update the interrelationship thus reinforce the data object categorization of another type." ></td>
	<td class="line x" title="86:133	The process is iterative until clustering results of both object types converge." ></td>
	<td class="line x" title="87:133	Suppose we begin the clustering process from data objects in set F, and then the steps can be expressed as Figure 1." ></td>
	<td class="line x" title="88:133	After the iteration, we can get the strongest n links between product feature categories and opinion word groups." ></td>
	<td class="line x" title="89:133	That constitutes our set of sentiment association." ></td>
	<td class="line x" title="90:133	3 Experimental Setup In this section we describe our experiments and the data used in these experiments." ></td>
	<td class="line x" title="91:133	3.1 Data Our experiments take hotel reviews (in Chinese) as example." ></td>
	<td class="line x" title="92:133	The corpus used in the experiments is composed by 4000 editor reviews on hotel, including 857,692 Chinese characters." ></td>
	<td class="line x" title="93:133	They are extracted from www.ctrip.com." ></td>
	<td class="line x" title="94:133	Each review contains a users rating represented by stars, the number of the star denotes the users satisfaction." ></td>
	<td class="line x" title="95:133	The detailed information is illustrated in Table 1,  Table 1: The detail information of corpus Users rating Number 1 star 555 2 star 1375 3 star 70 4 star 2000  Then we use ICTCLAS 2 , a Chinese word segmentation software to extract candidate review features and opinion words." ></td>
	<td class="line x" title="96:133	Usually, adjectives are normally used to express opinions in reviews." ></td>
	<td class="line x" title="97:133	Therefore, most of the existing researches take adjectives as opinion words." ></td>
	<td class="line x" title="98:133	In the research of Hu et al.(2004), they proposed that  1  http://www.keenage.com/ 2  www.searchforum.org.cn 490 other components of a sentence are unlikely to be product features except for nouns and noun phrases." ></td>
	<td class="line x" title="100:133	Some researchers (Fujii and Ishikawa, 2006) targeted nouns, noun phrases and verb phrases." ></td>
	<td class="line x" title="101:133	The adding of verb phrases caused the identification of more possible product features, while brought lots of noises." ></td>
	<td class="line x" title="102:133	So in this paper, we follow the points of Hus, extracting nouns and noun phrases as candidate product feature words." ></td>
	<td class="line x" title="103:133	Take the whole set of nouns and noun phrases as candidate features will bring some noise." ></td>
	<td class="line x" title="104:133	In order to reduce such adverse effects, we use the function of Named Entity Recognition (NER) in ICTCLAS to filter out named entities, including: person, location, organization." ></td>
	<td class="line x" title="105:133	Since the NEs have small probability of being product features, we prune the candidate nouns or noun phrases which have the above NE taggers." ></td>
	<td class="line x" title="106:133	Table 2: The number of candidate review features and opinion words in our corpus Extracted Instance Total NonRepeated Candidate review feature 86,623 15,249 Opinion word 26,721 1,231  By pruning candidate product feature words, we get the set of product feature words F. And the set of opinion words O is composed by all the adjectives in reviews." ></td>
	<td class="line x" title="107:133	The number of candidate product feature words and opinion words extracted from the corpus are shown as Table 2: 3.2 Experimental Procedure We evaluate our approach from two perspectives: 1) Effectiveness of product feature category construction by mutual reinforcement based clustering; 2) Precision of sentiment association between product feature categories and opinion word groups; 4 Experimental Results and Discussion 4.1 Evaluation of Review Feature Category Construction To calculate agreement between the review feature category construction results and the correct labels, we make use of the Rand index (Rand, 1971)." ></td>
	<td class="line x" title="108:133	This allows for a measure of agreement between two partitions, P 1  and P 2 , of the same data set D. Each partition is viewed as a collection of n*(n-1)/2 pair wise decisions, where n is the size of D. For each pair of points d i  and d j  in D, P i  either assigns them to the same cluster or to different clusters." ></td>
	<td class="line x" title="109:133	Let a be the number of decisions where d i  is in the same cluster as d j  in P 1  and in P 2 . Let b be the number of decisions where the two instances are placed in different clusters in both partitions." ></td>
	<td class="line x" title="110:133	Total agreement can then be calculated using 12 (, ) (1)/2 ab Rand P P nn + =                              (11) In our case, the parts of product feature words in the pre-constructed evaluation set are used to represent the data set D; a and b represent the partition agreements between the pairs of any two words in the parts and in the clustering results respectively." ></td>
	<td class="line x" title="111:133	In equation 10, the parameter  reflects the respective contribution of semantic information and co-occurrence information to the final distance." ></td>
	<td class="line x" title="112:133	When 0 = or 1 = , the co-occurrence information or the semantic information will be utilized alone." ></td>
	<td class="line x" title="113:133	In order to get the optimal combination of the two type of distance, we adjust the parameter  from 0 to 1(stepped by 0.2), and the accuracy of feature category construction with different  are shown in Figure 2:   Figure 2: The accuracy of review feature category construction with the variation of the parameter   From this figure, we can find that the semantic information ( =1) contributes much more to the accuracy of review feature category construction than the co-occurrence information (  =0), and when  =0, the approach is equivalent to the traditional information bottleneck approach." ></td>
	<td class="line x" title="114:133	We consider this is due partly to the sparseness of the cor491 pus, by enlarging the scale of the corpus or using the search engine (e.g. google etc), we can get more accurate results." ></td>
	<td class="line x" title="115:133	Additionally, by a sensible adjust on the parameter  (in this experiment, we set   as 0.6), we can get higher accuracy than the two baselines ( =0 and  =1), which indicates the necessity and effectiveness of the integration of semantic information and co-occurrence information in the proposed approach." ></td>
	<td class="line x" title="116:133	4.2 Evaluation of Sentiment Association We use precision to evaluate the performance of sentiment association." ></td>
	<td class="line x" title="117:133	An evaluation set is constructed manually first, in which there are not only the categories that every review feature word belong to, but also the relationship between each category and opinion word." ></td>
	<td class="line x" title="118:133	Then we define precision as: number of correctly associated pairs Precision number of detected pairs =                                                                           (12) A comparative result is got by the means of template-extraction based approach on the same test set." ></td>
	<td class="line x" title="119:133	By the usage of regular expression, the nouns (phrase) and gerund (phrase) are extracted as the review features, and the nearest adjectives are extracted as the related opinion words." ></td>
	<td class="line x" title="120:133	Because the modifiers of adjectives in reviews also contain rich sentiment information and express the view of customs, we extract adjectives and their modifiers simultaneously, and take them as the opinion words." ></td>
	<td class="line x" title="121:133	Table 3: Performance comparison in sentiment association Approach Pairs Precision Template extraction 27,683 65.89% Proposed approach 141,826 78.90%  Table 3 shows the advantage of our approach over the extraction by explicit adjacency." ></td>
	<td class="line x" title="122:133	Using the same product feature categorization, our sentiment association approach get a more accurate pair set than the direct extraction based on explicit adjacency." ></td>
	<td class="line x" title="123:133	The precision we obtained by the iterative reinforcement approach is 78.90%, almost 13 points higher than the adjacency approach." ></td>
	<td class="line x" title="124:133	This indicates that there are a large number of hidden sentiment associations in the real custom reviews, which underlines the importance and value of our work." ></td>
	<td class="line x" title="125:133	5 Conclusions and Further Work In this paper, we propose a novel iterative reinforcement framework based on improved information bottleneck approach to deal with the featurelevel product opinion-mining problem." ></td>
	<td class="line x" title="126:133	We alter traditional information bottleneck method by integration with semantic information, and the experimental result demonstrates the effectiveness of the alteration." ></td>
	<td class="line x" title="127:133	The main contribution of our work mainly including: null We propose an iterative reinforcement information bottleneck framework, and in this framework, review feature words and opinion words are organized into categories in a simultaneous and iterative manner." ></td>
	<td class="line x" title="128:133	null In the process of clustering, the semantic information and the co-occurrence information are integrated." ></td>
	<td class="line x" title="129:133	null The experimental results based on real Chinese web reviews demonstrate that our method outperforms the template extraction based algorithm." ></td>
	<td class="line x" title="130:133	Although our methods for candidate product feature extraction and filtering (see in 3.1) can partly identify real product features, it may lose some data and remain some noises." ></td>
	<td class="line x" title="131:133	Well conduct deeper research in this area in future work." ></td>
	<td class="line x" title="132:133	Additionally, we plan to exploit more information, such as background knowledge, to improve the performance." ></td>
	<td class="line x" title="133:133	6 Acknowledgments This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another project, i.e., 2004CB318109." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1056
For a few dollars less: Identifying review pages sans human labels
Barbosa, Luciano;Kumar, Ravi;Pang, Bo;Tomkins, Andrew;"></td>
	<td class="line x" title="1:259	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 494502, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:259	c 2009 Association for Computational Linguistics For a few dollars less: Identifying review pages sans human labels Luciano Barbosa Dept. of Computer Science University of Utah Salt Lake City, UT 84112, USA." ></td>
	<td class="line x" title="3:259	lbarbosa@cs.utah.edu Ravi Kumar Bo Pang Andrew Tomkins Yahoo!" ></td>
	<td class="line x" title="4:259	Research 701 First Ave Sunnyvale, CA 94089, USA." ></td>
	<td class="line x" title="5:259	{ravikumar,bopang,atomkins}@yahoo-inc.com Abstract We address the problem of large-scale automatic detection of online reviews without using any human labels." ></td>
	<td class="line x" title="6:259	We propose an efficient method that combines two basic ideas: Building a classifier from a large number of noisy examples and using the structure of the website to enhance the performance of this classifier." ></td>
	<td class="line x" title="7:259	Experiments suggest that our method is competitive against supervised learning methods that mandate expensive human effort." ></td>
	<td class="line x" title="8:259	1 Introduction Shoppers are migrating to the web and online reviews are playing a critical role in affecting their shopping decisions, online and offline." ></td>
	<td class="line x" title="9:259	According to two surveys published by comScore (2007) and Horrigan (2008), 81% of web users have done online research on a product at least once." ></td>
	<td class="line x" title="10:259	Among readers of online reviews, more than 70% reported that the reviews had a significant influence on their purchases." ></td>
	<td class="line x" title="11:259	Realizingthiseconomicpotential, search engines have been scrambling to cater to such user needs in innovative ways." ></td>
	<td class="line x" title="12:259	For example, in response to a product-related query, a search engine might wanttosurfaceonlyreviewpages, perhapsviaafilter by option, to the user." ></td>
	<td class="line x" title="13:259	More ambitiously, they might want to dissect the reviews, segregate them into novice and expert judgments, distill sentiments, and present an aggregated wisdom of the crowds opinion to the user." ></td>
	<td class="line x" title="14:259	Identifying review pages is the indispensable enabler to fulfill any such ambition; nonetheless, this problem does not seem to have been addressed at web scale before." ></td>
	<td class="line x" title="15:259	Detecting review webpages in a few, review-only websites is an easy, manually-doable task." ></td>
	<td class="line x" title="16:259	A large fraction of the interesting review content, however, is present on pages outside such websites." ></td>
	<td class="line x" title="17:259	This is where the task becomes challenging." ></td>
	<td class="line x" title="18:259	Review pages might constitute a minority and can be buried in a multitude of ways among non-review pages  for instance, the movie review pages in nytimes." ></td>
	<td class="line x" title="19:259	com, which are scattered among all news articles, or the product review pages in amazon.com, which areaccessiblefromtheproductdescriptionpage." ></td>
	<td class="line x" title="20:259	An automatic and scalable method to identify reviews is thus a practical necessity for the next-generation search engines." ></td>
	<td class="line x" title="21:259	The problem is actually more general than detecting reviews: it applies to detecting any horizontal category such as buying guides, forums, discussion boards, FAQs, etc. Given the nature of these problems, it is tempting to use supervised classification." ></td>
	<td class="line x" title="22:259	A formidable barrier is the labeling task itself since human labels need time and money." ></td>
	<td class="line x" title="23:259	On the other hand, it is easier to generate an enormous number of lowquality labeled examples through purely automatic methods." ></td>
	<td class="line x" title="24:259	This prompts the question: Can we do review detection by focusing just on the textual content of a large number of automatically obtained but low-quality labeled examples, perhaps also utilizing the site structure specific to each website?" ></td>
	<td class="line x" title="25:259	And how will it compare to the best supervised classification method?" ></td>
	<td class="line x" title="26:259	We address these questions in this paper." ></td>
	<td class="line x" title="27:259	Main contributions." ></td>
	<td class="line x" title="28:259	We propose the first end-toend method that can operate at web scale to efficiently detect review pages." ></td>
	<td class="line x" title="29:259	Our method is based on using simple URL-based clues to automatically 494 partition a large collection of webpages into two noisy classes: One that consists mostly of review webpages and another that consists of a mixture of some review but predominantly non-review webpages (more details in Section 4.2)." ></td>
	<td class="line x" title="30:259	We analyze the use of a naive Bayes classifier in this noisy setting and present a simple algorithm for review page classification." ></td>
	<td class="line x" title="31:259	We further enhance the performanceofthisclassifierbyincorporatinginformationaboutthestructureofthewebsitethatismanifested through the URLs of the webpages." ></td>
	<td class="line x" title="32:259	We do this by partitioning the website into clusters of webpages, where the clustering delicately balances the information in the site-unaware labels provided by the classifier in the previous step and the site structure encoded in the URL tokens; a decision tree is used to accomplish this." ></td>
	<td class="line x" title="33:259	Our classification method for noisily-labeled examples and the use of sitespecific cues to improve upon a site-independent classifier are general techniques that may be applicable in other large-scale web analyses." ></td>
	<td class="line x" title="34:259	Experiments on 2000 hand-labeled webpages from 40 websites of varying sizes show that besides being computationally efficient, our human-labelfree method not only outperforms those based on off-the-shelf subjectivity detection but also remains competitive against the state-of-the-art supervised text classification that relies on editorial labels." ></td>
	<td class="line x" title="35:259	2 Related work The related work falls into roughly four categories: Documentand sentence-level subjectivity detection, sentiment analysis in the context of reviews, learning from noisy labeled examples, and exploiting site structure for classification." ></td>
	<td class="line x" title="36:259	Giventhesubjectivenatureofreviews, documentlevel subjectivity classification is closely related to our work." ></td>
	<td class="line x" title="37:259	There have been a number of approaches proposed to address document-level subjectivity in news articles, weblogs, etc.(Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004; Finn and Kushmerick, 2006; Ni et al., 2007; Stepinski and Mittal, 2007)." ></td>
	<td class="line x" title="39:259	Ng et al.(2006) experiment with review identificationforknowndomainsusingdatasetswith clean labels (e.g., movie reviews vs. movie-related non-reviews), a setting different from that of ours." ></td>
	<td class="line x" title="41:259	Pang and Lee (2008b) present a method on reranking documents that are web search results for a specific query (containing the word review) based onthesubjective/objectivedistinction." ></td>
	<td class="line x" title="42:259	Giventhenature of the query, they implicitly detect reviews from unknown sources." ></td>
	<td class="line x" title="43:259	But their re-ranking algorithm only applies to webpages known to be (roughly) related to the same narrow subject." ></td>
	<td class="line x" title="44:259	Since the webpages in our datasets cover not only a diverse range of websites but also a diverse range of topics, their approach does not apply." ></td>
	<td class="line x" title="45:259	To the best of our knowledge, there has been no work on identifying review pages at the scale and diversity we consider." ></td>
	<td class="line x" title="46:259	Subjectivity classification of within-document items, such as terms, has been an active line of research (Wiebe et al.(2004) present a survey)." ></td>
	<td class="line x" title="48:259	Identifying subjective sentences in a document via offthe-shelf packages is an alternative way of detecting reviews without (additional) human annotations." ></td>
	<td class="line x" title="49:259	In particular, the OpinionFinder system (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005) is a state-ofthe-art knowledge-rich sentiment-analysis system." ></td>
	<td class="line x" title="50:259	We will use it as one of our baselines and compare its performance with our methods." ></td>
	<td class="line n" title="51:259	There has been a great deal of previous work in sentiment analysis that worked with reviews, but they were typically restricted to using reviews extracted from one or two well-known sources, bypassing automatic review detection." ></td>
	<td class="line oc" title="52:259	Examples of such early work include (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Hu and Liu, 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="53:259	See Pang and Lee (2008a) for a more comprehensive survey." ></td>
	<td class="line x" title="54:259	Building a collection of diverse review webpages, not limited to one or two hosts, can better facilitate such research." ></td>
	<td class="line x" title="55:259	Learning from noisy examples has been studied for a long time in the learning theory community (Angluin and Laird, 1988)." ></td>
	<td class="line x" title="56:259	Learning naive Bayes classifiers from noisy data (either features or labels or both) was studied by Yang et al.(2003)." ></td>
	<td class="line x" title="58:259	Their focus, however, is to reconstruct the underlying conditional probability distributions from the observed noisydataset." ></td>
	<td class="line x" title="59:259	We, ontheotherhand, relyonthevolume of labels to drown the noise." ></td>
	<td class="line x" title="60:259	Along this spirit, Snowetal.(2008)showthatobtainingmultiplelowquality labels (through Mechanical Turk) can approach high-quality editorial labels." ></td>
	<td class="line x" title="61:259	Unlike in their setting, we do not have multiple low-quality labels for the same URL." ></td>
	<td class="line x" title="62:259	The extensive body of work in 495 semi-supervised learning or learning from one class is also somewhat relevant to our work." ></td>
	<td class="line x" title="63:259	A major difference is that they tend to work with small amount of clean, labeled data." ></td>
	<td class="line x" title="64:259	In addition, many semisupervised/transductive learning algorithms are not efficient for web-scale data." ></td>
	<td class="line x" title="65:259	Using site structure for web analysis tasks has been addressed in a variety of contexts." ></td>
	<td class="line x" title="66:259	For example, Kening et al.(2005) exploit the structure of a website to improve classification." ></td>
	<td class="line x" title="68:259	On a related note, co-training has also been used to utilize inter-page link information in addition to intra-page textual content: Blum and Mitchell (1998) use anchor texts pointing to a webpage as the alternative view of the page in the context of webpage classification." ></td>
	<td class="line x" title="69:259	Their algorithm is largely site-unaware in that it does not explicitly exploit site structures." ></td>
	<td class="line x" title="70:259	Utilizing site structures also has remote connections to wrapper induction, and there is extensive literature on this topic." ></td>
	<td class="line x" title="71:259	Unfortunately, the methods in all of these work require human labeling, which is precisely what our work is trying to circumvent." ></td>
	<td class="line x" title="72:259	3 Methodology In this section we describe our basic methodology for identifying review pages." ></td>
	<td class="line x" title="73:259	Our method consists of two main steps." ></td>
	<td class="line x" title="74:259	The first is to use a large amount of noisy training examples to learn a basic classifier for review webpages; we adapt a simple naive Bayes classifier for this purpose." ></td>
	<td class="line x" title="75:259	The second is to improve the performance of this basic classifier by exploiting the website structure; we use a decision tree for this." ></td>
	<td class="line x" title="76:259	Let P be the set of all webpages." ></td>
	<td class="line x" title="77:259	Let C+ denote the positive class, i.e., the set of all review pages and let C denote the negative class, i.e., the set of all non-review pages." ></td>
	<td class="line x" title="78:259	Each webpage p is exactly in one of C+ or C, and is labeled +1 or 1 respectively." ></td>
	<td class="line x" title="79:259	3.1 Learning from large amounts of noisy data Previous work using supervised or semi-supervised learning approaches for sentiment analysis assumes relatively high-quality labels that are produced either via human annotation or automatically generated through highly accurate rules (e.g., assigning positive or negative label to a review according to automatically extracted star ratings)." ></td>
	<td class="line x" title="80:259	Weexamineadifferentscenariowherewecanautomatically generate large amount of relatively lowquality labels." ></td>
	<td class="line x" title="81:259	Section 4.2 describes the process in more detail, but briefly, in a collection of pages crawled from sites that are very likely to host reviews, those with the word review in their URLs are very likely to contain reviews (the noisy positive set tildewideC+) and the rest of the pages on those sites are less likely to contain reviews (the more noisy negative set tildewideC)." ></td>
	<td class="line x" title="82:259	More formally, for a webpage p, suppose Pr[p  C+ | p  tildewideC+] =  and Pr[p  C+ | p  tildewideC] = , where 1 >  greaternotequal  > 0." ></td>
	<td class="line x" title="83:259	Canwestilllearnsomethingusefulfrom tildewideC+ and tildewideC despite the labels being highly noisy?" ></td>
	<td class="line x" title="84:259	The following analysis is based on a naive Bayes classifier." ></td>
	<td class="line x" title="85:259	We chose naive Bayes classifier since the learning phase can easily be parallelized." ></td>
	<td class="line x" title="86:259	Given a webpage (or a document) p represented as a bag of features {fi}, we wish to assign a class argmaxc{C+,C} Pr[c | p] to this webpage." ></td>
	<td class="line x" title="87:259	Naive Bayes classifiers assume fis to be conditionally independent and we have Pr[p | c] = producttextPr[fi | c]." ></td>
	<td class="line x" title="88:259	Let ri = Pr[fi | C+]/Pr[fi | C] denote the contribution of each feature towards classification, and rc = Pr[C+]/Pr[C] denote the ratio of class priors." ></td>
	<td class="line x" title="89:259	First note that log Pr[C+|p]Pr[C|p] = log parenleftBigPr[C +] Pr[C]  Pr[p|C+] Pr[p|C] parenrightBig = log parenleftBigPr[C +] Pr[C]  producttextr i parenrightBig = logrc +summationtextlogri." ></td>
	<td class="line x" title="90:259	A webpage p receives label +1 iff Pr[C+ | p] > Pr[C | p], and by above, if and only if summationtextlogri > logrc." ></td>
	<td class="line x" title="91:259	When we do not have a reasonable estimate of Pr[C+] and Pr[C], as in our setting, the best we can do is to assume rc = 1." ></td>
	<td class="line x" title="92:259	In this case, p receives label +1 if and only ifsummationtextlogri > 0." ></td>
	<td class="line x" title="93:259	Thus, a feature fi with logri > 0 has a positive contribution towards p being labeled +1; call fi to be a positive feature." ></td>
	<td class="line x" title="94:259	Typically we use relative-frequency estimation of Pr[c] and Pr[fi | c] for c  {C+,C}." ></td>
	<td class="line x" title="95:259	Now, how does the estimation from a dataset with noisy labels compare with the estimation from a dataset with clean labels?" ></td>
	<td class="line x" title="96:259	To examine this, we calculate the following: Pr[fi | tildewideC+] = Pr[fi | C+] + (1)Pr[fi | C], Pr[fi | tildewideC] =  Pr[fi | C+] + (1)Pr[fi | C]." ></td>
	<td class="line x" title="97:259	Let ri = Pr[fi|eC+]Pr[f i|eC] = ri+(1)ri+(1)." ></td>
	<td class="line x" title="98:259	Clearly ri is monotonic but not linear in ri." ></td>
	<td class="line x" title="99:259	Furthermore, it is bounded: 496 (1)/(1)  ri  /." ></td>
	<td class="line x" title="100:259	However, ri > 1  ri + (1) > ri + (1)  ()ri > ()  ri > 1,where the last step used  > ." ></td>
	<td class="line x" title="101:259	Thus, the sign of log ri is the same as that of logri, i.e., a feature contributing positively tosummationtextlogri will continue to contribute positively tosummationtextlog ri (although its magnitude is distorted) and vice versa." ></td>
	<td class="line x" title="102:259	Theaboveanalysismotivatesanalternativemodel to naive Bayes." ></td>
	<td class="line x" title="103:259	Instead of each feature fi placing a weighted vote log ri in the final decision, we trust only the sign of log ri, and let each feature fi place a vote for the class C+ (respectively, C) if log ri > 0 (respectively, log ri < 0)." ></td>
	<td class="line x" title="104:259	Intuitively, this model just compares the number of positive features and thenumberofnegativefeatures,ignoringthemagnitude (since it is distorted anyway)." ></td>
	<td class="line x" title="105:259	This is precisely our algorithm: For a given threshold , the final label nbu(p) of a webpage p is given by nbu(p) = sgn(summationtextsgn(log ri)), where sgn is the sign function." ></td>
	<td class="line x" title="106:259	For comparison purposes, we also indicate the weighted version: nbw(p) = sgn(summationtextlog ri )." ></td>
	<td class="line x" title="107:259	If  = 0, we omit  and use nb to denote a generic label assigned by any of the above algorithms." ></td>
	<td class="line x" title="108:259	Note that even though our discussions were for two-class and in particular, review classification, they are equally applicable to a wide range of classification tasks in large-scale web-content analysis." ></td>
	<td class="line x" title="109:259	Our analysis of learning from automatically generated noisy examples is thus of independent interest." ></td>
	<td class="line x" title="110:259	3.2 Utilizing site structure Can the structure of a website be exploited to improve the classification of webpages given by nb()?" ></td>
	<td class="line x" title="111:259	While not all websites are well-organized, quite a number of them exhibit certain structure that makes it possible to identify large subsites that contain only review pages." ></td>
	<td class="line x" title="112:259	Typically but not always this structure is manifested through the tokens in the URL corresponding to the webpage." ></td>
	<td class="line x" title="113:259	For instance, the pattern http://www.zagat.com/verticals/ PropertyDetails.aspx?VID=a&R=b, where a,b are numbers, is indicative of all webpages in zagat.com that are reviews of restaurants." ></td>
	<td class="line x" title="114:259	In fact, we can think of this as a generalization of having the keyword review in the URL." ></td>
	<td class="line x" title="115:259	Now, suppose we have an initial labeling nb(p)  {1} for each webpage p produced by a classifier (as in the previous section, or one that is trained on a small set of human annotated pages), can we further improve the labeling using the pattern in the URL structure?" ></td>
	<td class="line x" title="116:259	It is not immediate how to best use the URL structure to identify the review subsites." ></td>
	<td class="line x" title="117:259	First, URLs contain irrelevant information (e.g., the token verticals in the above example), thus clustering by simple cosine similarity may not discover the review subsites." ></td>
	<td class="line x" title="118:259	Second, the subsite may not correspond to a subtree in the URL hierarchy, i.e., it is not reasonable to expect all the review URLs to share a common prefix." ></td>
	<td class="line x" title="119:259	Third, the URLs contain a mixture of path components (e.g., www.zagat.com/verticals/ PropertyDetails.aspx) and key-value pairs (e.g., VID=a and R=b) and hence each token (regardless of its position) in the URL could play a role in determining the review subsite." ></td>
	<td class="line x" title="120:259	Furthermore, conjunction of presence/absence of certain tokens in the URL may best correspond to subsite membership." ></td>
	<td class="line x" title="121:259	In light of these, we represent each URL (and hence the corresponding webpage) by a bag {gi} of tokens obtained from the URL." ></td>
	<td class="line x" title="122:259	We perform a crude form of feature selection by dropping tokens that are either ubiquitous (occurring in more than 99% of URLs) or infrequent (occurring in fewer than 1% of URLs) in a website; neither yields useful information." ></td>
	<td class="line x" title="123:259	Our overall approach will be to use gis to partition P into clusters {Ci} of webpages such that each cluster Ci is predominantly labeled as either review or non-review by nb()." ></td>
	<td class="line x" title="124:259	This automatically yields a new label cls(p) for each page p, which is the majority label of the cluster of p: cls(p) = sgn parenleftBigsummationtext qC(p) nb(q) parenrightBig , where C(p) is the cluster of p. To this end, we use a decision tree classifier to build the clusters." ></td>
	<td class="line x" title="125:259	This classifier will use the features {gi} and the target labels nb()." ></td>
	<td class="line x" title="126:259	The classifier is trained on all the webpages in the website and in the obtained decision tree, each leaf, consisting of pages with the same set of feature values leading down the path, corresponds to a cluster of webpages." ></td>
	<td class="line x" title="127:259	Note that the clusters delicately balance the information in the siteunaware labels nb() and the site structure encoded 497 in the URLs (given by gis)." ></td>
	<td class="line x" title="128:259	Thus the label cls(p) can be thought of as a smoothed version of nb(p)." ></td>
	<td class="line x" title="129:259	Eventhoughwecanexpectmostclusterstobehomogeneous (i.e., pure reviews or non-reviews), the above method can produce clusters that are inherently heterogeneous." ></td>
	<td class="line x" title="130:259	This can happen if the website URLs are organized such that many subsites contain both review and non-review webpages." ></td>
	<td class="line x" title="131:259	To take this into account, we propose the following hybrid approach that interpolates between the unsmoothed labels given by nb() and the smoothed labels given by cls()." ></td>
	<td class="line x" title="132:259	For a cluster Ci, the discrepancy disc(Ci) = summationtextpCi[cls(p) negationslash= nb(p)]; this quantity measures the number of disagreements between the majority label cls(p) and the original label nb(p) for each page p in the cluster." ></td>
	<td class="line x" title="133:259	The decision tree guarantees disc(Ci)  |Ci|/2." ></td>
	<td class="line x" title="134:259	We call a cluster Ci to be -homogeneous if disc(Ci)  |Ci|, where   [0,1/2]." ></td>
	<td class="line x" title="135:259	For a fixed , the hybrid label of a webpage p is given by hyb(p) = braceleftbigg cls(p) if C(p) is -homogeneous, nb(p) otherwise." ></td>
	<td class="line x" title="136:259	Note that hyb1/2(p) = cls(p) and hyb0(p) = nb(p)." ></td>
	<td class="line x" title="137:259	Note that in the above discussions, any clustering method that can incorporate the site-unaware labels nb() and the site-specific tokens in gis could have been used; off-the-shelf decision tree was merely a specific way to realize this." ></td>
	<td class="line x" title="138:259	4 Data It is crucial for this study to create a dataset that is representative of a diverse range of websites that host reviews over different topics in different styles." ></td>
	<td class="line x" title="139:259	We are not aware of any extensive index of online review websites and we do not want to restrict our study to a few well-known review aggregation websites (such as yelp.com or zagat.com) since this will not represent the less popular and more specialized ones." ></td>
	<td class="line x" title="140:259	Instead, we utilized user-generated tags for webpages, available on social bookmarking websites such as del.icio.us." ></td>
	<td class="line x" title="141:259	We obtained (a sample of) a snapshot of URLtag pairs from del.icio.us." ></td>
	<td class="line x" title="142:259	We took the top one thousand sites with review* tags; these websites hopefully represent a broad coverage." ></td>
	<td class="line x" title="143:259	We were able to crawl over nine hundred of these sites and the resulting collection of webpages served as the basis of the experiments in this paper." ></td>
	<td class="line x" title="144:259	We refer to these websites (or the webpages from these sites, when it is clear from the context) as Sall." ></td>
	<td class="line x" title="145:259	4.1 Gold-standard test set When the websites are as diverse as represented in Sall, there is no perfect automatic way to generate the ground truth labels." ></td>
	<td class="line x" title="146:259	Thus we sampled a number of pages for human labeling as follows." ></td>
	<td class="line x" title="147:259	First, we set aside 40 sites as the test sites (S40)." ></td>
	<td class="line x" title="148:259	In order to represent different types of websites (to the best we can), we sampled the 40 sites so that S40 covers different size ranges, since large-scale websites and small-scale websites are often quite different in style, topic, and content." ></td>
	<td class="line x" title="149:259	We uniformly sampled 10 sites from each of the four size categories (roughly, sites with 1005K, 5K25K, 25K 100K, and 100K+ webpages)1." ></td>
	<td class="line x" title="150:259	Indeed, S40 (as did Sall) covered a wide range of topics (e.g., games, books, restaurants, movies, music, and electronics) and styles (e.g., dedicated review sites, product sites thatincludeuserreviews,newspaperswithmoviereview sections, religious sites hosting book reviews, and non-English review sites)." ></td>
	<td class="line x" title="151:259	Wethensampled50pagestobelabeledfromeach site in S40." ></td>
	<td class="line x" title="152:259	Since there are some fairly large sites that have only a small number of review pages, a uniform sampling may yield no review webpages from those sites." ></td>
	<td class="line x" title="153:259	To reflect the natural distribution on a website and to represent pages from both classes, the webpages were sampled in the following way." ></td>
	<td class="line x" title="154:259	For each website in S40, 25 pages were uniformly sampled (representing the natural distribution) and 25 pages were sampled from among equivalence classes based on URLs so that pages from each major URL pattern were represented." ></td>
	<td class="line x" title="155:259	Here, each webpage in the site is represented by a URL signature containing the most frequent tokens that occur in the URLs in that site and all pages with the same signature form an equivalence class." ></td>
	<td class="line x" title="156:259	For our purposes, a webpage is considered a review if it contains significant amount of textual information expressing subjective opinions on or personal experiences with a given product / service." ></td>
	<td class="line x" title="157:259	When in doubt, the guiding principle is whether 1As we do not want to waste human annotation on sites with no reviews at all, a quick pre-screening process eliminated candidate sites that did not seem to host any reviews." ></td>
	<td class="line x" title="158:259	498 a page can be a satisfactory result page for users searching for reviews." ></td>
	<td class="line x" title="159:259	More specifically, the human annotation labeled each webpage, after thoroughly examining the content, with one of the following seven intuitive labels: single (contains exactly one review), multiple (concatenation of more than one review), no (clearly not a review page), empty (looks like a page that could contain reviews but had none), login (a valid user login needed to look at the content), hub (a pointer to one or more review pages), and ambiguous (border-line case, e.g., a webpagewithaonelinereview)." ></td>
	<td class="line x" title="160:259	Thefirsttwolabels were treated as +1 (i.e., reviews) and the last five labels were treated as 1 (i.e., non-reviews)." ></td>
	<td class="line x" title="161:259	Out of the 2000 pages, we obtained 578 pages labeled +1 and the 1422 pages labeled 1." ></td>
	<td class="line x" title="162:259	On a pilot study using two human judges, we obtained 78% inter-judge agreement for the seven labels and 92% inter-judge agreement if we collapse the labels to 1." ></td>
	<td class="line x" title="163:259	Percentages of reviews in our samples from different sites range from 14.6% to 93.9%." ></td>
	<td class="line x" title="164:259	Preprocessing for text-based analysis." ></td>
	<td class="line x" title="165:259	We processed the crawled webpages using lynx to extract the text content." ></td>
	<td class="line x" title="166:259	To discard templated content, which is an annoying issue in large-scale web processing, and HTML artifacts, we used the following preprocessing." ></td>
	<td class="line x" title="167:259	First, the HTML tags <p>, <br>, </tr>, and </td> were interpreted as paragraph breaks, the . inside a paragraph was interpreted as a sentence break, and whitespace was used to tokenize words in a sentence." ></td>
	<td class="line x" title="168:259	A sentence is considered good if it has at least seven alphabetic words and a paragraph is considered good if it has at least two good sentences." ></td>
	<td class="line x" title="169:259	After extracting the text using lynx, only the good paragraphs were retained." ></td>
	<td class="line x" title="170:259	This effectively removes most of the templated content (e.g., navigational phrases) and retains most of the natural language texts." ></td>
	<td class="line x" title="171:259	Because of this preprocessing, 485 pages out of 2000 turned out to be empty and these were discarded (human labels on 97% of these empty pages were 1)." ></td>
	<td class="line x" title="172:259	4.2 Dataset with noisy labels As discussed in Section 3.1, our goal is to obtain a large noisy set of positive and negative labeled examples." ></td>
	<td class="line x" title="173:259	We obtained these labels for the webpages in the training sites, Srest, which is essentially Sall \ S40." ></td>
	<td class="line x" title="174:259	First, the URLs in Srest were tokenized using a unigram model based on an English dictionary; this is so that strings such as reviewoftheday are properly interpreted." ></td>
	<td class="line x" title="175:259	tildewideC+: To be labeled +1, the path-component of the URL of the webpage has to contain the token review." ></td>
	<td class="line x" title="176:259	Our assumption is that such pages are highly likely to be review pages." ></td>
	<td class="line x" title="177:259	On a uniform sample of 100 such pages in Sall, 90% were found to be genuine reviews." ></td>
	<td class="line x" title="178:259	Thus, we obtained a collection of webpages with slightly noisy positive labels." ></td>
	<td class="line x" title="179:259	tildewideC: The rest of the pages in Srest were labeled 1." ></td>
	<td class="line x" title="180:259	Clearly this is a noisy negative set since not all pages containing reviews have review as part of their URLs (recall the example from zagat.com); thus many pages in tildewideC can still be reviews." ></td>
	<td class="line x" title="181:259	While the negative labels in Srest are more noisy than the positive labels, we believe most of the nonreview pages are in tildewideC, and as most websites contain a significant number of non-review pages, the percentage of reviews in tildewideC is smaller than that in tildewideC+ (the assumption  greaternotequal  in Section 3.1)." ></td>
	<td class="line x" title="182:259	We collected all the paragraphs (as defined earlier) from both tildewideC+ and tildewideC separately." ></td>
	<td class="line x" title="183:259	We eliminated duplicate paragraphs (this further mitigates the templates issue, especially for sites generated by content-management software), and trained a unigram language model as in Section 3.1." ></td>
	<td class="line x" title="184:259	5 Evaluations The evaluations were conducted on the 1515 labeled (non-empty) pages in S40 described in Section 4.1." ></td>
	<td class="line x" title="185:259	We report the accuracy (acc.)" ></td>
	<td class="line x" title="186:259	as well as precision (prec.), recall (rec.), and f-measure (fmeas.)" ></td>
	<td class="line x" title="187:259	for C+." ></td>
	<td class="line x" title="188:259	Trivial baselines." ></td>
	<td class="line x" title="189:259	Out of the 1515 labeled pages, 565 were labeled +1 and 950 were labeled 1." ></td>
	<td class="line x" title="190:259	Table 1 summarizes the performance of baselines that always predict one of the classes and a baseline that randomly select a class according to the class distribution S40." ></td>
	<td class="line x" title="191:259	As we can see, the best accuracy is .63, the best f-measure is .54, and they cannot be achieved by the same baseline." ></td>
	<td class="line x" title="192:259	Before presentacc." ></td>
	<td class="line x" title="193:259	prec." ></td>
	<td class="line x" title="194:259	rec." ></td>
	<td class="line x" title="195:259	fmeas." ></td>
	<td class="line x" title="196:259	always C .63 0 always C+ .37 .37 1 .54 random .53 .37 .37 .37 Table 1: Trivial baseline performances." ></td>
	<td class="line x" title="197:259	499 ing the main results of our methods, we introduce a much stronger baseline that utilizes a knowledgerich subjectivity detection package." ></td>
	<td class="line x" title="198:259	5.1 Using subjectivity detectors This baseline is motivated by the fact that reviews oftencontainextensivesubjectivecontent." ></td>
	<td class="line x" title="199:259	Thereare many existing techniques that detect subjectivity in text." ></td>
	<td class="line x" title="200:259	OpinionFinder (http://www.cs.pitt." ></td>
	<td class="line x" title="201:259	edu/mpqa/opinionfinderrelease/) is a well-known system that processes documents and automatically identifies subjective sentences in them." ></td>
	<td class="line x" title="202:259	OpinionFinder uses two subjective sentence classifiers (Riloff and Wiebe, 2003; Wiebe and Riloff, 2005)." ></td>
	<td class="line x" title="203:259	The first (denoted opfA) focuses on yielding the highest accuracy; the second (denoted opfB) optimizes precision at the expense of recall." ></td>
	<td class="line x" title="204:259	The methods underlying OpinionFinder incorporate extensive tools from linguistics (including, speech activity verbs, psychological verbs, FrameNet verbs andadjectiveswithframeexperiencer,amongothers) and machine learning." ></td>
	<td class="line x" title="205:259	In terms of performance, previous work has shown that OpinionFinder is a challenging system to improve upon for review retrieval (Pang and Lee, 2008b)." ></td>
	<td class="line x" title="206:259	Computationally, OpinionFinderisveryexpensiveandhenceunattractive for large-scale webpage analysis (running OpinionFinder on 1515 pages took about five hours)." ></td>
	<td class="line x" title="207:259	Therefore, we also propose a light-weight subjectivity detection mechanism called lwd, which counts the number of opinion words in each sentence in the text." ></td>
	<td class="line x" title="208:259	The opinion words (5403 of them) were obtained from an existing subjectivity lexicon (http: //www.cs.pitt.edu/mpqa)." ></td>
	<td class="line x" title="209:259	We ran both opfA and opfB on the tokenized text (running them on raw HTML produced worse results)." ></td>
	<td class="line x" title="210:259	Each sentence in the text was labeled subjective or objective." ></td>
	<td class="line x" title="211:259	We experimented with two ways to label a document using sentence-level subjectivity labels." ></td>
	<td class="line x" title="212:259	We labeled a document +1 if it contained at least k subjective sentences (denoted as opfstar(k), where k > 0 is the absolute threshold), or at least f fraction of its sentences were labeled subjective (denoted as opfstar(f), where f  (0,1] is the relative threshold)." ></td>
	<td class="line x" title="213:259	We conducted exhaustive parameter search with both opfA and opfB." ></td>
	<td class="line x" title="214:259	For instance, the performances of opfA as a function of the thresholds, both absolute and relative, is shown in Figure 1." ></td>
	<td class="line x" title="215:259	Table 2 summarizes the best performances of opfstar(k) (first two rows) and opfstar(f) (next two rows), in terms of accuracy and f-measure (boldfaced)." ></td>
	<td class="line x" title="216:259	Similarly, for lwd, we labeled a document +1 if at least k sentences have at least lscript opinion words (denoted lwd(k,lscript).)" ></td>
	<td class="line x" title="217:259	Table 2 once again shows the best performing parameters for both accuracy and f-measure for lwd." ></td>
	<td class="line x" title="218:259	Our results indicate that a simple method such as lwd can come very close to a sophisticated system such as opfstar." ></td>
	<td class="line x" title="219:259	acc." ></td>
	<td class="line x" title="220:259	prec." ></td>
	<td class="line x" title="221:259	rec." ></td>
	<td class="line x" title="222:259	fmeas." ></td>
	<td class="line x" title="223:259	opfA(2) .704 .597 .634 .615 opfB(2) .659 .526 .857 .652 opfA(.17) .652 .529 .614 .568 opfB(.36) .636 .523 .797 .632 lwd(1,4) .716 .631 .572 .600 lwd(1,1) .666 .538 .740 .623 Table 2: Best performances of opfstar and lwd methods." ></td>
	<td class="line x" title="224:259	Figure 1: Performance of opfA as a function of thresholds: Absolute and relative." ></td>
	<td class="line x" title="225:259	5.2 Main results As stated earlier, we do not have any prior knowledge about the value of  and hence have to work with  = 0." ></td>
	<td class="line x" title="226:259	To investigate the implications of this assumption, we study the performance of nbu and nbw as a function of ." ></td>
	<td class="line x" title="227:259	The accuracy and fmeasures are plotted in Figure 2." ></td>
	<td class="line x" title="228:259	There are three 500 acc." ></td>
	<td class="line x" title="229:259	prec." ></td>
	<td class="line x" title="230:259	rec." ></td>
	<td class="line x" title="231:259	fmeas." ></td>
	<td class="line x" title="232:259	nbu .753 .652 .726 .687 cls .756 .696 .616 .654 hyb1/3 .777 .712 .674 .693 Table 3: Performance of our methods." ></td>
	<td class="line x" title="233:259	conclusions that can be drawn from this study: (i) The peak values of accuracy and f-measure are comparable for both nbu and nbw, (ii) at  = 0, nbu is much better than nbw, in terms of both accuracy and f-measure, and(iii)thebestperformanceof nbu occurs at   0." ></td>
	<td class="line x" title="234:259	Given the difficulty of obtaining  if one were to use nbw, the above conclusions validate our intuition and the algorithm in Section 3.1." ></td>
	<td class="line x" title="235:259	Figure 2: Performance as threshold changes: Comparing nbu (marked as (u)) with nbw (marked as (w))." ></td>
	<td class="line x" title="236:259	Table 3 shows the performance of the site-specific method outlined in Section 3.2." ></td>
	<td class="line x" title="237:259	The clusters were generated using the unpruned J48 decision tree in Weka (www.cs.waikato.ac.nz/ml/ weka)." ></td>
	<td class="line x" title="238:259	In our experiments, we set  = 1/3 as a natural choice for the hybrid method." ></td>
	<td class="line x" title="239:259	As we see the performance of nbu is about 7% better than the best performance using a subjectivity-based method (in terms of accuracy)." ></td>
	<td class="line x" title="240:259	The performance of the smoothed labels (decision tree-based clustering) is comparable to that of nbu." ></td>
	<td class="line x" title="241:259	However, the hybrid method hyb1/3 yields an additional 3% relative improvement over nbu." ></td>
	<td class="line x" title="242:259	Paired t-test over the accuracies for these 40 sites shows both hyb1/3 and nbu to be statistically significantly better than the opfstar with best accuracy (with p < 0.05, p < 0.005, respectively), and hyb1/3 to be statistically significantly better than nbu (with p < 0.05)." ></td>
	<td class="line x" title="243:259	5.3 Cross-validation on S40 While the main focus of our paper is to study how to detect reviews without human labels, we present cross validation results on S40 as a comparison point." ></td>
	<td class="line x" title="244:259	The goal of this experiment is to get a sense of the best possible accuracy and f-measure numbers using labeled data and the state-of-theart method for text classification, namely, SVMs." ></td>
	<td class="line x" title="245:259	In other words, the performance numbers obtained through SVMs and cross-validation can be thought of as realistic upper bounds on the performance of content-based review detection." ></td>
	<td class="line x" title="246:259	We used SVMlight (svmlight.joachims.org) for this purpose." ></td>
	<td class="line x" title="247:259	The cross-validation experiment was conducted as follows." ></td>
	<td class="line x" title="248:259	We split the data by site to simulate the more realistic setting where pages in the test set do not necessarily come from a known site." ></td>
	<td class="line x" title="249:259	Each fold consisted of one site from each size category; thus, 36 of the 40 sites in S40 were used for training and the remainder for testing." ></td>
	<td class="line x" title="250:259	Over ten folds, the averageperformancewas: accuracy.795, precision.759, recall .658, and f-measure .705." ></td>
	<td class="line x" title="251:259	Thus our methods in Section 3 come reasonably close to the upper bound given by SVMs and human-labeled data." ></td>
	<td class="line x" title="252:259	In fact, while the supervised SVMs statistically significantly outperform nbu, they are statistically indistinguishable from hyb1/3 via paired t-test over site-level accuracies." ></td>
	<td class="line x" title="253:259	6 Conclusions In this paper we proposed an automatic method to perform efficient and large-scale detection of reviews." ></td>
	<td class="line x" title="254:259	Our method is based on two principles: Building a classifier from a large number of noisy labeled examples and using the site structure to improve the performance of this classifier." ></td>
	<td class="line x" title="255:259	Extensive experiments suggest that our method is competitive against supervised learning methods that depend on expensive human labels." ></td>
	<td class="line x" title="256:259	There are several interesting avenues for future research, including improving the current method for exploiting the site structure." ></td>
	<td class="line x" title="257:259	On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al., 2007)." ></td>
	<td class="line x" title="258:259	Given the diverse range of topics present in our dataset, addressing topic-dependency is also an interesting future research direction." ></td>
	<td class="line x" title="259:259	501" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-2046
Improving SCL Model for Sentiment-Transfer Learning
Tan, Songbo;Cheng, Xue-Qi;"></td>
	<td class="line x" title="1:84	Proceedings of NAACL HLT 2009: Short Papers, pages 181184, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:84	c 2009 Association for Computational Linguistics Improving SCL Model for Sentiment-Transfer Learning  Songbo Tan Institute of Computing Technology Beijing, China tansongbo@software.ict.ac.cn Xueqi Cheng Institute of Computing Technology Beijing, China cxq@ict.ac.cn  ABSTRACT In recent years, Structural Correspondence Learning (SCL) is becoming one of the most promising techniques for sentiment-transfer learning." ></td>
	<td class="line x" title="3:84	However, SCL model treats each feature as well as each instance by an equivalent-weight strategy." ></td>
	<td class="line x" title="4:84	To address the two issues effectively, we proposed a weighted SCL model (W-SCL), which weights the features as well as the instances." ></td>
	<td class="line x" title="5:84	More specifically, W-SCL assigns a smaller weight to high-frequency domain-specific (HFDS) features and assigns a larger weight to instances with the same label as the involved pivot feature." ></td>
	<td class="line x" title="6:84	The experimental results indicate that proposed W-SCL model could overcome the adverse influence of HFDS features, and leverage knowledge from labels of instances and pivot features." ></td>
	<td class="line oc" title="7:84	1   Introduction In the community of sentiment analysis (Turney 2002; Pang et al., 2002; Tang et al., 2009), transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work, because sentiment expression often behaves with strong domain-specific nature." ></td>
	<td class="line x" title="8:84	Up to this time, many researchers have proposed techniques to address this problem, such as classifiers adaptation, generalizable features detection and so on (DaumeIII et al., 2006; Jiang et al., 2007; Tan et al., 2007; Tan et al., 2008; Tan et al., 2009)." ></td>
	<td class="line x" title="9:84	Among these techniques, SCL (Structural Correspondence Learning) (Blitzer et al., 2006) is regarded as a promising method to tackle transfer-learning problem." ></td>
	<td class="line x" title="10:84	The main idea behind SCL model is to identify correspondences among features from different domains by modeling their correlations with pivot features (or generalizable features)." ></td>
	<td class="line x" title="11:84	Pivot features behave similarly in both domains." ></td>
	<td class="line x" title="12:84	If non-pivot features from different domains are correlated with many of the same pivot features, then we assume them to be corresponded with each other, and treat them similarly when training a sentiment classifier." ></td>
	<td class="line x" title="13:84	However, SCL model treats each feature as well as each instance by an equivalent-weight strategy." ></td>
	<td class="line x" title="14:84	From the perspective of feature, this strategy fails to overcome the adverse influence of highfrequency domain-specific (HFDS) features." ></td>
	<td class="line x" title="15:84	For example, the words stock or market occurs frequently in most of stock reviews, so these nonsentiment features tend to have a strong correspondence with pivot features." ></td>
	<td class="line x" title="16:84	As a result, the representative ability of the other sentiment features will inevitably be weakened to some degree." ></td>
	<td class="line x" title="17:84	To address this issue, we proposed Frequently Exclusively-occurring Entropy (FEE) to pick out HFDS features, and proposed a feature-weighted SCL model (FW-SCL) to adjust the influence of HFDS features in building correspondence." ></td>
	<td class="line x" title="18:84	The main idea of FW-SCL is to assign a smaller weight to HFDS features so that the adverse influence of HFDS features can be decreased." ></td>
	<td class="line x" title="19:84	From the other perspective, the equivalentweight strategy of SCL model ignores the labels (positive or negative) of labeled instances." ></td>
	<td class="line x" title="20:84	Obviously, this is not a good idea." ></td>
	<td class="line x" title="21:84	In fact, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than that built on negative instances; and vice versa." ></td>
	<td class="line x" title="22:84	Consequently, utilization of labels of instances and pivot features can decrease the adverse influence of some co-occurrences, such as co-occurrences involved with positive pivot features and negative instances, or involved with negative pivot features and positive instances." ></td>
	<td class="line x" title="23:84	In order to take into account the labels of labeled instances, we proposed an instanceweighted SCL model (IW-SCL), which assigns a larger weight to instances with the same label as the involved pivot feature." ></td>
	<td class="line x" title="24:84	In this time, we obtain a combined model: feature-weighted and instanceweighted SCL model (FWIW-SCL)." ></td>
	<td class="line x" title="25:84	For the sake 181 of convenience, we simplify FWIW-SCL as W-SCL in the rest of this paper." ></td>
	<td class="line x" title="26:84	2   Structural Correspondence Learning In the section, we provide the detailed procedures for SCL model." ></td>
	<td class="line x" title="27:84	First we need to pick out pivot features." ></td>
	<td class="line x" title="28:84	Pivot features occur frequently in both the source and the target domain." ></td>
	<td class="line x" title="29:84	In the community of sentiment analysis, generalizable sentiment words are good candidates for pivot features, such as good and excellent." ></td>
	<td class="line x" title="30:84	In the rest of this paper, we use K to stand for the number of pivot features." ></td>
	<td class="line x" title="31:84	Second, we need to compute the pivot predictors (or mapping vectors) using selected pivot features." ></td>
	<td class="line x" title="32:84	The pivot predictors are the key job, because they directly decide the performance of SCL." ></td>
	<td class="line x" title="33:84	For each pivot feature k, we use a loss function L k , () 2 1)( wxwxpL i i T ikk +=         (1) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i , otherwise xif xp ik ik 0 1 1 )( >     = , where the weight vector w encodes the correspondence of the non-pivot features with the pivot feature k (Blitzer et al., 2006)." ></td>
	<td class="line x" title="34:84	Finally we use the augmented space [x T ,  x T W] T  to train the classifier on the source labeled data and predict the examples on the target domain, where W=[w 1 ,w 2 , , w K ]." ></td>
	<td class="line x" title="35:84	3   Feature-Weighted SCL Model 3.1 Measure to pick out HFDS features In order to pick out HFDS features, we proposed Frequently Exclusively-occurring Entropy (FEE)." ></td>
	<td class="line x" title="36:84	Our measure includes two criteria: occur in one domain as frequently as possible, while occur on another domain as rarely as possible." ></td>
	<td class="line x" title="37:84	To satisfy this requirement, we proposed the following formula: () ()         += )(),(min )(),(max log)(),(maxlog wPwP wPwP wPwPf no no now (2) where P o (w) and P n (w) indicate the probability of word w in the source domain and the target domain respectively:  () ()  + + = 2 )( )( o o o N wN wP                     (3) ( ) ()  + + = 2 )( )( n n n N wN wP                     (4) where N o (w) and N n (w) is the number of examples with word w in the source domain and the target domain respectively; N o  and N n  is the number of examples in the source domain and the target domain respectively." ></td>
	<td class="line x" title="38:84	In order to overcome overflow, we set  =0.0001 in our experiment reported in section 5." ></td>
	<td class="line x" title="39:84	To better understand this measure, lets take a simple example (see Table 1)." ></td>
	<td class="line x" title="40:84	Given a source dataset with 1000 documents and a target dataset with 1000 documents, 12 candidate features, and a task to pick out 2 HFDS features." ></td>
	<td class="line x" title="41:84	According to our understanding, the best choice is to pick out w 4  and w 8 .  According to formula (2), fortunately, we successfully pick out w 4 , and w 8 . This simple example validates the effectiveness of proposed FEE formula." ></td>
	<td class="line x" title="42:84	Table 1: A simple example for FEE FEE Words N o (w) N n (w) Score Rank w 1  100 100 -2.3025 6 w 2  100 90 -2.1971 4 w 3  100 45 -1.5040 3 w 4  100 4 0.9163 1 w 5  50 50 -2.9956 8 w 6  50 45 -2.8903 7 w 7  50 23 -2.2192 5 w 8  50 2 0.2231 2 w 9  4 4 -5.5214 11 w 10 4 3 -5.2337 10 w 11 4 2 -4.8283 9 w 12 1 1 -6.9077 12 3.2 Feature-Weighted SCL model To adjust the influence of HFDS features in building correspondence, we proposed featureweighted SCL model (FW-SCL), ( ) 2 1)( wxwxpL i il l llikk  +=      (5) where the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i ; otherwise xif xp ik ik 0 1 1 )( >     = , and  l  is the parameter to control the weight of the HFDS feature l, 182  otherwise Zlif HFDS l     =  1    where Z HFDS  indicates the HFDS feature set and  is located in the range [0,1]." ></td>
	<td class="line x" title="43:84	When  =0, it indicates that no HFDS features are used to build the correspondence vectors; while  =1 indicates that all features are equally used to build the correspondence vectors, that is to say, proposed FW-SCL algorithm is simplified as traditional SCL algorithm." ></td>
	<td class="line x" title="44:84	Consequently, proposed FW-SCL algorithm could be regarded as a generalized version of traditional SCL algorithm." ></td>
	<td class="line x" title="45:84	4 Instance-Weighted SCL Model The traditional SCL model does not take into account the labels (positive or negative) of instances on the source domain and pivot features." ></td>
	<td class="line x" title="46:84	Although the labels of pivot features are not given at first, it is very easy to obtain these labels because the number of pivot features is typically very small." ></td>
	<td class="line x" title="47:84	Obviously, positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than the correlations built on negative instances; and vice versa." ></td>
	<td class="line x" title="48:84	As a result, the ideal choice is to assign a larger weight to the instances with the same label as the involved pivot feature, while assign a smaller weight to the instances with the different label as the involved pivot feature." ></td>
	<td class="line x" title="49:84	This strategy can make correlations more reliable." ></td>
	<td class="line x" title="50:84	This is the key idea of instance-weighted SCL model (IWSCL)." ></td>
	<td class="line x" title="51:84	Combining the idea of feature-weighted SCL model (FW-SCL), we obtain the featureweighted and instance-weighted SCL model (FWIW-SCL), ()()() () ()()()   ++= 1),(11 1),( 2 jl l lljkj il l llikik xwxpxk wxwxpxkL   (6) where   is the instance weight and the function p k (x i ) indicates whether the pivot feature k occurs in the instance x i ; otherwise xif xp ik ik 0 1 1 )( >     = and  l  is the parameter to control the weight of the HFDS feature l,  otherwise Zlif HFDS l     =  1   , where Z HFDS  indicates the HFDS feature set and  is located in the range [0,1]." ></td>
	<td class="line x" title="52:84	In equation (6), the function  (z,y) indicates whether the two variables z and y have the same non-zero value, () otherwise 0y and zzif z,y =    =  0 1  ; and the function  (z) is a hinge function, whose variables are either pivot features or instances, labelnegativez has aif unknown labelpositivez has aif z   1 0 1 )(       = . For the sake of convenience, we simplify FWIW-SCL as W-SCL." ></td>
	<td class="line x" title="53:84	5   Experimental Results 5.1 Datasets We collected three Chinese domain-specific datasets: Education Reviews (Edu, from http://blog.sohu.com/learning/), Stock Reviews (Sto, from http://blog.sohu.com/stock/) and Computer Reviews (Comp, from http://detail.zol.com.cn/)." ></td>
	<td class="line x" title="54:84	All of these datasets are annotated by three linguists." ></td>
	<td class="line x" title="55:84	We use ICTCLAS (a Chinese text POS tool, http://ictclas.org/) to parse Chinese words." ></td>
	<td class="line x" title="56:84	The dataset Edu includes 1,012 negative reviews and 254 positive reviews." ></td>
	<td class="line x" title="57:84	The average size of reviews is about 600 words." ></td>
	<td class="line x" title="58:84	The dataset Sto consists of 683 negative reviews and 364 positive reviews." ></td>
	<td class="line x" title="59:84	The average length of reviews is about 460 terms." ></td>
	<td class="line x" title="60:84	The dataset Comp contains 390 negative reviews and 544 positive reviews." ></td>
	<td class="line x" title="61:84	The average length of reviews is about 120 words." ></td>
	<td class="line x" title="62:84	5.2 Comparison Methods In our experiments, we run one supervised baseline, i.e., Nave Bayes (NB), which only uses one source-domain labeled data as training data." ></td>
	<td class="line x" title="63:84	For transfer-learning baseline, we implement traditional SCL model (T-SCL) (Blitzer et al., 2006)." ></td>
	<td class="line x" title="64:84	Like TSVM, it makes use of the sourcedomain labeled data as well as the target-domain unlabeled data." ></td>
	<td class="line x" title="65:84	5.3 Does proposed method work?" ></td>
	<td class="line x" title="66:84	To conduct our experiments, we use sourcedomain data as unlabeled set or labeled training set, and use target-domain data as unlabeled set or testing set." ></td>
	<td class="line x" title="67:84	Note that we use 100 manualannotated pivot features for T-SCL, FW-SCL and W-SCL in the following experiments." ></td>
	<td class="line x" title="68:84	We select 183 pivot features use three criteria: a) is a sentiment word; b) occurs frequently in both domains; c) has similar occurring probability." ></td>
	<td class="line x" title="69:84	For T-SCL, FWSCL and W-SCL, we use prototype classifier (Sebastiani, 2002) to train the final model." ></td>
	<td class="line x" title="70:84	Table 2 shows the results of experiments comparing proposed method with supervised learning, transductive learning and T-SCL." ></td>
	<td class="line x" title="71:84	For FW-SCL, the Z HFDS  is set to 200 and   is set to 0.1; For W-SCL, the Z HFDS  is set to 200,   is set to 0.1, and   is set to 0.9." ></td>
	<td class="line x" title="72:84	As expected, proposed method FW-SCL does indeed provide much better performance than supervised baselines, TSVM and T-SCL model." ></td>
	<td class="line x" title="73:84	For example, the average accuracy of FW-SCL beats supervised baselines by about 12 percents, beats TSVM by about 11 percents and beats TSCL by about 10 percents." ></td>
	<td class="line x" title="74:84	This result indicates that proposed FW-SCL model could overcome the shortcomings of HFDS features in building correspondence vectors." ></td>
	<td class="line x" title="75:84	More surprisingly, instance-weighting strategy can further boost the performance of FW-SCL by about 4 percents." ></td>
	<td class="line x" title="76:84	This result indicates that the labels of instances and pivot features are very useful in building the correlation vectors." ></td>
	<td class="line x" title="77:84	This result also verifies our analysis in section 4: positive pivot features tend to occur in positive instances, so the correlations built on positive instances are more reliable than the correlations built on negative instances, and vice versa." ></td>
	<td class="line x" title="78:84	Table 2: Accuracy of different methods  NB T-SCL FW-SCL W-SCL Edu->Sto 0.6704 0.7965 0.7917 0.8108 Edu->Comp 0.5085 0.8019 0.8993 0.9025 Sto->Edu 0.6824 0.7712 0.9072 0.9368 Sto->Comp 0.5053 0.8126 0.8126 0.8693 Comp->Sto 0.6580 0.6523 0.7010 0.7717 Comp->Edu 0.6114 0.5976 0.9112 0.9408 Average 0.6060 0.7387 0.8372 0.8720 Although SCL is a method designed for transfer learning, but it cannot provide better performance than TSVM." ></td>
	<td class="line x" title="79:84	This result verifies the analysis in section 3: a small amount of HFDS features occupy a large amount of weight in classification model, but hardly carry corresponding sentiment." ></td>
	<td class="line x" title="80:84	In another word, very few top-frequency words degrade the representative ability of SCL model for sentiment classification." ></td>
	<td class="line x" title="81:84	6 Conclusion Remarks In this paper, we proposed a weighted SCL model (W-SCL) for domain adaptation in the context of sentiment analysis." ></td>
	<td class="line x" title="82:84	On six domaintransfer tasks, W-SCL consistently produces much better performance than the supervised, semisupervised and transfer-learning baselines." ></td>
	<td class="line x" title="83:84	As a result, we can say that proposed W-SCL model offers a better choice for sentiment-analysis applications that require high-precision classification but hardly have any labeled training data." ></td>
	<td class="line x" title="84:84	7 Acknowledgments This work was mainly supported by two funds, i.e., 0704021000 and 60803085, and one another project, i.e., 2004CB318109." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-3013
Towards Building a Competitive Opinion Summarization System: Challenges and Keys
Lloret, Elena;Balahur-Dobrescu, Alexandra;Palomar, Manuel;Montoyo, Andrs;"></td>
	<td class="line x" title="1:119	Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7277, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:119	c 2009 Association for Computational Linguistics Towards Building a Competitive Opinion Summarization System: Challenges and Keys   Elena Lloret*, Alexandra Balahur, Manuel Palomar and Andrs Montoyo Department of Software and Computing Systems University of Alicante Apartado de Correos 99, E-03080, Alicante, Spain {elloret, abalahur, mpalomar, montoyo}@dlsi.ua.es    Abstract This paper presents an overview of our participation in the TAC 2008 Opinion Pilot Summarization task, as well as the proposed and evaluated post-competition improvements." ></td>
	<td class="line x" title="3:119	We first describe our opinion summarization system and the results obtained." ></td>
	<td class="line x" title="4:119	Further on, we identify the systems weak points and suggest several improvements, focused both on information content, as well as linguistic and readability aspects." ></td>
	<td class="line x" title="5:119	We obtain encouraging results, especially as far as Fmeasure is concerned, outperforming the competition results by approximately 80%." ></td>
	<td class="line x" title="6:119	1 Introduction The Opinion Summarization Pilot (OSP) task within the TAC 2008 competition consisted in generating summaries from answers to opinion questions retrieved from blogs (the Blog061 collection)." ></td>
	<td class="line x" title="7:119	The questions were organized around 25 targets  persons, events, organizations etc. Additionally, a set of text snippets that contained the answers to the questions were provided by the organizers, their use being optional." ></td>
	<td class="line x" title="8:119	An example of target, question and provided snippet is given in Figure 1." ></td>
	<td class="line x" title="9:119	Figure 1." ></td>
	<td class="line x" title="10:119	Examples of target, question and snippet   *Elena Lloret is funded by the FPI program (BES-200716268) from the Spanish Ministry of Science and Innovation, under the project TEXT-MESS (TIN-2006-15265) 1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html The techniques employed by the participants were mainly based on the already existing summarization systems." ></td>
	<td class="line x" title="11:119	While most participants added new features (sentiment, pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008);  LIPN (Bossard et al., 2008);  IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity  DLSIUAES (Balahur et al., 2008) or on separating information rich clauses italica (Cruz et al., 2008)." ></td>
	<td class="line oc" title="12:119	In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003)." ></td>
	<td class="line x" title="13:119	Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006)." ></td>
	<td class="line x" title="14:119	Finally, fine grained, feature-based opinion summarization is defined in (Hu and Liu, 2004)." ></td>
	<td class="line x" title="15:119	2 Opinion Summarization System In order to tackle the OSP task, we considered the use of two different methods for opinion mining and summarization, differing mainly with respect to the use of the optional text snippets provided." ></td>
	<td class="line x" title="16:119	Our first approach (the Snippet-driven Approach) Target : George Clooney Question: Why do people like George Clooney?" ></td>
	<td class="line x" title="17:119	Snippet 1: 1050 BLOG06-20060125-0150025581509 he is a great actor 72 used these snippets, whereas the second one (Blogdriven Approach) found the answers directly in the corresponding blogs." ></td>
	<td class="line x" title="18:119	A general overview of the systems architecture is shown in Figure 2, where three main parts can be distinguished: the question processing stage, the snippets processing stage (only carried out for the first approach), and the final summary generation module." ></td>
	<td class="line x" title="19:119	Next, the main steps involved in each process will be explained in more detail." ></td>
	<td class="line x" title="20:119	Figure 2." ></td>
	<td class="line x" title="21:119	System architecture  The first step was to determine the polarity of each question, extract the keywords from each of them and finally, build some patterns of reformulation." ></td>
	<td class="line x" title="22:119	The latter were defined in order to give the final summary an abstract nature, rather than a simple joining of sentences." ></td>
	<td class="line x" title="23:119	The polarity of the question was determined using a set of created patterns, whose goal was to extract for further classification the nouns, verbs, adverbs or adjectives indicating some kind of polarity (positive or negative)." ></td>
	<td class="line x" title="24:119	These extracted words, together with their determiners, were classified using the emotions lists in WordNet Affect (Strapparava and Valitutti, 2005), jointly with the emotions lists of attitudes, triggers resource (Balahur and Montoyo, 2008 [1]), four created lists of attitudes, expressing criticism, support, admiration and rejection and two categories for value (good and bad), taking for the opinion mining systems in (Balahur and Montoyo, 2008 [2])." ></td>
	<td class="line x" title="25:119	Moreover, the focus of each question was automatically extracted using the Freeling2 Named Entity Recognizer module." ></td>
	<td class="line x" title="26:119	This information was used to determine whether or not all the questions within the same topic had the same focus, as well as be able to decide later on which text snippet belonged to which question." ></td>
	<td class="line x" title="27:119	Regarding the given text snippets, we also computed their polarity and their focus." ></td>
	<td class="line x" title="28:119	The  2 http://garraf.epsevg.upc.es/freeling/ polarity was calculated as a vector similarity between the snippets and vectors constructed from the list of sentences contained in the ISEAR corpus (Scherer and Wallbot, 1997), WordNet Affect emotion lists of anger, sadness, disgust and joy and the emotion triggers resource, using Pedersen's Text Similarity Package.3 Concerning the blogs, our opinion mining and summarization system is focused only on plain text; therefore, as pre processing stage, we removed all unnecessary tags and irrelevant information, such as links, images etc. Further on, we split the remaining text into individual sentences." ></td>
	<td class="line x" title="29:119	A matching between blogs' sentences and text snippets was performed so that a preliminary set of potential meaningful sentences was recorded for further processing." ></td>
	<td class="line x" title="30:119	To achieve this, snippets not literally contained in the blogs were tokenized and stemmed using Porter's Stemmer,4 and stop words were removed in order to find the most similar possible sentence associated with it." ></td>
	<td class="line x" title="31:119	Subsequently, by means of the same Pedersen Text Similarity Package as for computing the snippets' polarity, we computed the similarity between the given snippets and this created set of potential sentences." ></td>
	<td class="line x" title="32:119	We extracted the complete blog sentences to which each snippet was related." ></td>
	<td class="line x" title="33:119	Further on, we extracted the focus for each blog phrase sentence as well." ></td>
	<td class="line x" title="34:119	Then, we filtered redundant sentences using a nave similarity based approach." ></td>
	<td class="line x" title="35:119	Once we obtained the possible answers, we used Minipar5 to filter out incomplete sentences." ></td>
	<td class="line x" title="36:119	Having computed the polarity for the questions and snippets, and set out the final set of sentences to produce the summary, we bound each sentence to its corresponding question, and we grouped all sentences which were related to the same question together, so that we could generate the language for this group, according to the patterns of reformulation previously mentioned." ></td>
	<td class="line x" title="37:119	Finally, the speech style was changed to an impersonal one, in order to avoid directly expressed opinion sentences." ></td>
	<td class="line x" title="38:119	A POS-tagger tool (TreeTagger6) was used to identify third person verbs and change them to a neutral style." ></td>
	<td class="line x" title="39:119	A set of rules to identify  3http://www.d.umn.edu/~tpederse/text-similarity.html 4http://tartarus.org/~martin/PorterStemmer/ 5http://www.cs.ualberta.ca/~lindek/minipar.htm 6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/ 73 pronouns was created, and they were also changed to the more general pronoun they and its corresponding forms, to avoid personal opinions." ></td>
	<td class="line x" title="40:119	3 Evaluation Table 1 shows the final results obtained by our approaches in the TAC 2008 Opinion Pilot (the rank among the 36 participating systems is shown in brackets for each evaluation measure)." ></td>
	<td class="line x" title="41:119	Both of our approaches were totally automatic, and the only difference between them was the use of the given snippets in the first one (A1) and not in the second (A2)." ></td>
	<td class="line x" title="42:119	The column numbers stand for the following average scores: summarizerID (1); pyramid F-score (Beta=1) (2), grammaticality (3); non-redundancy (4); structure/coherence (including focus and referential clarity) (5); overall fluency/readability (6); overall responsiveness (7)." ></td>
	<td class="line x" title="43:119	1 2 3 4 5 6 7 A1 0.357 (7) 4.727 (8) 5.364 (28) 3.409 (4) 3.636 (16) 5.045 (5) A2 0.155 (23) 3.545 (36) 4.364 (36) 3.091 (13) 2.636 (36) 2.227 (28) Table 1." ></td>
	<td class="line x" title="44:119	Evaluation results  As it can be noticed from Table 1, our system performed well regarding F-measure, the first run being classified 7th among the 36 evaluated." ></td>
	<td class="line x" title="45:119	As far as the structure and coherence are concerned, the results were also good, placing the first approach in the fourth." ></td>
	<td class="line x" title="46:119	Also worth mentioning is the good performance obtained regarding the overall responsiveness, where A1 ranked 5th." ></td>
	<td class="line x" title="47:119	Generally speaking, the results for A1 showed well-balanced among all the criteria evaluated, except for non redundancy and grammaticality." ></td>
	<td class="line x" title="48:119	For the second approach, results were not as good, due to the difficulty in selecting the appropriate opinion blog sentence by only taking into account the keywords of the question." ></td>
	<td class="line x" title="49:119	4 Post-competition tests, experiments and improvements When an exhaustive examination of the nuggets used for evaluating the summaries was done, we found some problems that are worth mentioning." ></td>
	<td class="line x" title="50:119	a) Some nuggets with high score did not exist in the snippet list (e.g. When buying from CARMAX, got a better than blue book trade-in on old car (0.9))." ></td>
	<td class="line x" title="51:119	b) Some nuggets for the same target express the same idea, despite their not being identical (e.g. NAFTA needs to be renegotiated to protect Canadian sovereignty and Green Party: Renegotiate NAFTA to protect Canadian Sovereignty)." ></td>
	<td class="line x" title="52:119	c) The meaning of one nugget can be deduced from another's (e.g. reasonably healthy food and sandwiches are healthy)." ></td>
	<td class="line x" title="53:119	d) Some nuggets are not very clear in meaning (e.g. hot, fun)." ></td>
	<td class="line x" title="54:119	e) A snippet can be covered by several nuggets (e.g. both nuggets it is an honest book and it is a great book correspond to the same snippet It was such a great bookhonest and hard to read (content not language difficulty))." ></td>
	<td class="line x" title="55:119	On the other hand, regarding the use of the optional snippets, the main problem to address is to remove redundancy, because many of them are repeated for the same target, and we have to determine which snippet represents better the idea for the final summary, in order to avoid noisy irrelevant information." ></td>
	<td class="line x" title="56:119	4.1 Measuring the Performance of a Generic Summarization System Several participants in the TAC 2008 edition performed the OSP task by using generic summarization systems." ></td>
	<td class="line x" title="57:119	Most were adjusted by integrating an opinion classifier module so that the task could be fulfilled, but some were not (Bossard et al., 2008), (Hendrickx and Bosma, 2008)." ></td>
	<td class="line x" title="58:119	This fact made us realize that a generic summarizer could be used to achieve this task." ></td>
	<td class="line x" title="59:119	We wanted to analyze the effects of such a kind of summarizer to produce opinion summaries." ></td>
	<td class="line x" title="60:119	We followed the approach described in (Lloret et al., 2008)." ></td>
	<td class="line x" title="61:119	The main idea employed is to score sentences of a document with regard to the word frequency count (WF), which can be combined with a Textual Entailment (TE) module." ></td>
	<td class="line x" title="62:119	Although the first approach suggested for opinion summarization obtained much better results in the evaluation than the second one (see Section 3.1), we decided to run the generic system over both approaches, with and without applying TE, to 74 provide a more extent analysis and conclusions." ></td>
	<td class="line x" title="63:119	After preprocessing the blogs and having all the possible candidate sentences grouped together, we considered these as the input for the generic summarizer." ></td>
	<td class="line x" title="64:119	The goal of these experiments was to determine whether the techniques used for a generic summarizer would have a positive influence in selecting the main relevant information to become part of the final summary." ></td>
	<td class="line x" title="65:119	4.2 Results and Discussion We re-evaluated the summaries generated by the generic system following the nuggets list provided by the TAC 2008 organization, and counting manually the number of nuggets that were covered in the summaries." ></td>
	<td class="line x" title="66:119	This was a tedious task, but it could not be automatically performed because of the fact that many of the provided nuggets were not found in the original blog collection." ></td>
	<td class="line x" title="67:119	After the manual matching of nuggets and sentences, we computed the average Recall, Precision and Fmeasure (Beta =1) in the same way as in the TAC 2008 was done, according to the number and weight of the nuggets that were also covered in the summary." ></td>
	<td class="line x" title="68:119	Each nugget had a weight ranging from 0 to 1 reflecting its importance, and it was counted only once, even though the information was repeated within the summary." ></td>
	<td class="line x" title="69:119	The average for each value was calculated taking into account the results for all the summaries in each approach." ></td>
	<td class="line x" title="70:119	Unfortunately, we could not measure criteria such as readability or coherence as they were manually evaluated by human experts." ></td>
	<td class="line x" title="71:119	Table 2 points out the results for all the approaches reported." ></td>
	<td class="line x" title="72:119	We have also considered the results derived from our participation in the TAC 2008 conference (OpSum-1 and OpSum-2), in order to analyze whether they have been improved or not." ></td>
	<td class="line x" title="73:119	From these results it can be stated that the TE module in conjunction with the WF counts, have been very appropriate in selecting the most important information of a document." ></td>
	<td class="line x" title="74:119	Although it can be thought that applying TE can remove some meaningful sentences which contained important information, results show the opposite." ></td>
	<td class="line x" title="75:119	It benefits the Precision value, because a shorter summary contains greater ratio of relevant information." ></td>
	<td class="line x" title="76:119	On the other hand, taking into consideration the Fmeasure value only, it can be seen that the approach combining TE and WF, for the sentences in the first approach, has beaten significantly the best F-measure result among the participants of TAC 2008 (please see Table 3), increasing its performance by 20% (with respect to WF only), and improving by approximately 80% with respect to our first approach submitted to TAC 2008." ></td>
	<td class="line x" title="77:119	However, a simple generic summarization system like the one we have used here is not enough to produce opinion oriented summaries, since semantic coherence given by the grouping of positive and negative opinions is not taken into account." ></td>
	<td class="line x" title="78:119	Therefore, the opinion classification stage must be added in the same manner as used in the competition." ></td>
	<td class="line x" title="79:119	SYSTEM RECALL PRECISION F-MEASURE OpSum-1 0.592 0.272 0.357 OpSum-2 0.251 0.141 0.155 WF-1 0.705 0.392 0.486 TE+WF -1  0.684 0.630  0.639 WF -2 0.322 0.234  0.241 TE+WF-2 0.292 0.282 0.262 Table 2." ></td>
	<td class="line x" title="80:119	Comparison of the results 4.3 Improving the quality of summaries In the evaluation performed by the TAC organization, a manual quality evaluation was also carried out." ></td>
	<td class="line x" title="81:119	In this evaluation the important aspects were grammaticality, non-redundancy, structure and coherence, readability, and overall responsiveness." ></td>
	<td class="line x" title="82:119	Although our participating systems obtained good F-measure values, in other scores, especially in grammaticality and non-redundancy, the results achieved were very low." ></td>
	<td class="line x" title="83:119	Focusing all our efforts in improving the first approach, OpSum-1, non-redundancy and grammaticality verification had to be performed." ></td>
	<td class="line x" title="84:119	In this approach, we wanted to test how much of the redundant information would be possible to remove by using a Textual Entailment system similar to (Iftene and Balahur-Dobrescu, 2007), without it affecting the quality of the remaining data." ></td>
	<td class="line x" title="85:119	As input for the TE system, we considered the snippets retrieved from the original blog posts." ></td>
	<td class="line x" title="86:119	We applied the entailment verification on each of the possible pairs, taking in turn all snippets as Text and Hypothesis with all other snippets as Hypothesis and Text, respectively." ></td>
	<td class="line x" title="87:119	Thus, as output, we obtained the list of snippets from which we eliminated those that 75 are entailed by any of the other snippets." ></td>
	<td class="line x" title="88:119	We further eliminated those snippets which had a high entailment score with any of the remaining snippets." ></td>
	<td class="line x" title="89:119	SYSTEM F-MEASURE Best system  0.534 Second best system 0.490 OpSum-1 + TE  0.530 OpSum-1 0.357 Table 3." ></td>
	<td class="line x" title="90:119	F-measure results after improving the system  Table 3 shows that applying TE before generating the final summary leads to very good results increasing the F-measure by 48.50% with respect to the original first approach." ></td>
	<td class="line x" title="91:119	Moreover, it can be seen form Table 3 that our improved approach would have ranked in the second place among all the participants, regarding F-measure." ></td>
	<td class="line x" title="92:119	The main problem with this approach is the long processing time." ></td>
	<td class="line x" title="93:119	We can apply Textual Entailment in the manner described within the generic summarization system presented, successively testing the relation as Snippet1 entails Snippet2?, Snippet1+Snippet2 entails Snippet3?" ></td>
	<td class="line x" title="94:119	and so on." ></td>
	<td class="line x" title="95:119	The problem then becomes the fact that this approach is random, since different snippets come from different sources, so there is no order among them." ></td>
	<td class="line x" title="96:119	Further on, we have seen that many problems arise from the fact that extracting information from blogs introduces a lot of noise." ></td>
	<td class="line x" title="97:119	In many cases, we had examples such as: At 4:00 PM John said Starbucks coffee tastes great John said Starbucks coffee tastes great, always get one when reading New York Times." ></td>
	<td class="line x" title="98:119	To the final summary, the important information that should be added is Starbucks coffee tastes great." ></td>
	<td class="line x" title="99:119	Our TE system contains a rule specifying that the existence or not of a Named Entity in the hypothesis and its not being mentioned in the text leads to the decision of NO entailment." ></td>
	<td class="line x" title="100:119	For the example given, both snippets are maintained, although they contain the same data." ></td>
	<td class="line x" title="101:119	Another issue to be addressed is the extra information contained in final summaries that is not scored as nugget." ></td>
	<td class="line x" title="102:119	As we have seen from our data, much of this information is also valid and correctly answers the questions." ></td>
	<td class="line x" title="103:119	Therefore, what methods can be employed to give more weight to some and penalize others automatically?" ></td>
	<td class="line x" title="104:119	Regarding the grammaticality criteria, once we had a summary generated we used the module Language Tool7 as a post-processing step." ></td>
	<td class="line x" title="105:119	The errors that we needed correcting included the number matching between nouns and determiners as well as among subject and predicate, upper case for sentence start, repeated words or punctuation marks and lack of punctuation marks." ></td>
	<td class="line x" title="106:119	The rules present in the module and that we switched off, due to the fact that they produced more errors, were those concerning the limit in the number of consecutive nouns and the need for an article before a noun (since it always seemed to want to correct Vista for the Vista a.o.)." ></td>
	<td class="line x" title="107:119	We evaluated by observing the mistakes that the texts contained, and counting the number of remaining or introduced errors in the output." ></td>
	<td class="line x" title="108:119	The results obtained can be seen in Table 4." ></td>
	<td class="line x" title="109:119	Problem Rightly corrected  Wrongly corrected Match S-P 90% 10% Noun-det 75% 25% Upper case 80% 20% Repeated words 100% 0% Repeated . 80% 20% Spelling mistakes 60% 40% Unpaired /() 100% 0% Table 4." ></td>
	<td class="line x" title="110:119	Grammaticality analysis  The greatest problem encountered was the fact that bigrams are not detected and agreement is not made in cases in which the noun does not appear exactly after the determiner." ></td>
	<td class="line x" title="111:119	All in all, using this module, the grammaticality of our texts was greatly improved." ></td>
	<td class="line x" title="112:119	5 Conclusions and future work The Opinion Pilot in the TAC 2008 competition was a difficult task, involving the development of systems including components for QA, IR, polarity classification and summarization." ></td>
	<td class="line x" title="113:119	Our contribution presented in this paper resides in proposing an opinion mining and summarization method using different approaches and resources, evaluating each of them in turn." ></td>
	<td class="line x" title="114:119	We have shown that using a generic summarization system, we obtain 80% improvement over the results obtained in the competition, with coherence being maintained by using the same polarity classification mechanisms." ></td>
	<td class="line x" title="115:119	7http://community.languagetool.org/ 76 Using redundancy removal with TE, as opposed to our initial polarity strength based sentence filtering improved the system performance by almost 50%." ></td>
	<td class="line x" title="116:119	Finally, we showed that grammaticality can be checked and improved using an independent solution given by Language Tool." ></td>
	<td class="line x" title="117:119	Further work includes the improvement of the polarity classification component by using machine learning over annotated corpora and other techniques, such as anaphora resolution." ></td>
	<td class="line x" title="118:119	As we could see, the well functioning of this component ensures logic, structure and coherence to the produced summaries." ></td>
	<td class="line x" title="119:119	Moreover, we plan to study the manner in which opinion sentences of blogs/bloggers can be coherently combined." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1027
Co-Training for Cross-Lingual Sentiment Classification
Wan, Xiaojun;"></td>
	<td class="line x" title="1:195	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 235243, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:195	c2009 ACL and AFNLP Co-Training for Cross-Lingual Sentiment Classification  Xiaojun Wan Institute of Compute Science and Technology & Key Laboratory of Computational Linguistics, MOE Peking University, Beijing 100871, China wanxiaojun@icst.pku.edu.cn   Abstract The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification." ></td>
	<td class="line x" title="3:195	However, there are many freely available English sentiment corpora on the Web." ></td>
	<td class="line x" title="4:195	This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data." ></td>
	<td class="line x" title="5:195	Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem." ></td>
	<td class="line x" title="6:195	We propose a cotraining approach to making use of unlabeled Chinese data." ></td>
	<td class="line x" title="7:195	Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers." ></td>
	<td class="line x" title="8:195	1 Introduction Sentiment classification is the task of identifying the sentiment polarity of a given text." ></td>
	<td class="line x" title="9:195	The sentiment polarity is usually positive or negative and the text genre is usually product review." ></td>
	<td class="line x" title="10:195	In recent years, sentiment classification has drawn much attention in the NLP field and it has many useful applications, such as opinion mining and summarization (Liu et al., 2005; Ku et al., 2006; Titov and McDonald, 2008)." ></td>
	<td class="line x" title="11:195	To date, a variety of corpus-based methods have been developed for sentiment classification." ></td>
	<td class="line x" title="12:195	The methods usually rely heavily on an annotated corpus for training the sentiment classifier." ></td>
	<td class="line x" title="13:195	The sentiment corpora are considered as the most valuable resources for the sentiment classification task." ></td>
	<td class="line x" title="14:195	However, such resources in different languages are very imbalanced." ></td>
	<td class="line x" title="15:195	Because most previous work focuses on English sentiment classification, many annotated corpora for English sentiment classification are freely available on the Web." ></td>
	<td class="line x" title="16:195	However, the annotated corpora for Chinese sentiment classification are scarce and it is not a trivial task to manually label reliable Chinese sentiment corpora." ></td>
	<td class="line x" title="17:195	The challenge before us is how to leverage rich English corpora for Chinese sentiment classification." ></td>
	<td class="line x" title="18:195	In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources." ></td>
	<td class="line x" title="19:195	Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages." ></td>
	<td class="line x" title="20:195	Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g. SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language." ></td>
	<td class="line x" title="21:195	In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews." ></td>
	<td class="line x" title="22:195	Unlabeled Chinese reviews can be fully leveraged in the proposed approach." ></td>
	<td class="line x" title="23:195	First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews." ></td>
	<td class="line x" title="24:195	Then, we can view the classification problem in two independent views: Chinese view with only Chinese features and English view with only English features." ></td>
	<td class="line x" title="25:195	We then use the co-training approach to making full use of the two redundant views of features." ></td>
	<td class="line x" title="26:195	The SVM classifier is adopted as the basic classifier in the proposed approach." ></td>
	<td class="line x" title="27:195	Experimental results show that the proposed approach can outperform the baseline inductive classifiers and the more advanced transductive classifiers." ></td>
	<td class="line x" title="28:195	The rest of this paper is organized as follows: Section 2 introduces related work." ></td>
	<td class="line x" title="29:195	The proposed 235 co-training approach is described in detail in Section 3." ></td>
	<td class="line x" title="30:195	Section 4 shows the experimental results." ></td>
	<td class="line x" title="31:195	Lastly we conclude this paper in Section 5." ></td>
	<td class="line x" title="32:195	2 Related Work 2.1 Sentiment Classification Sentiment classification can be performed on words, sentences or documents." ></td>
	<td class="line x" title="33:195	In this paper we focus on document sentiment classification." ></td>
	<td class="line x" title="34:195	The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based." ></td>
	<td class="line x" title="35:195	Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons." ></td>
	<td class="line oc" title="36:195	Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method." ></td>
	<td class="line x" title="37:195	Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words." ></td>
	<td class="line x" title="38:195	Hiroshi et al.(2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents." ></td>
	<td class="line x" title="40:195	Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers." ></td>
	<td class="line x" title="41:195	Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text." ></td>
	<td class="line x" title="42:195	Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier." ></td>
	<td class="line x" title="43:195	Since the work of Pang et al.(2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005)." ></td>
	<td class="line x" title="45:195	Most recently, McDonald et al.(2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity." ></td>
	<td class="line x" title="47:195	Blitzer et al.(2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products." ></td>
	<td class="line x" title="49:195	Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting." ></td>
	<td class="line x" title="50:195	Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification." ></td>
	<td class="line x" title="51:195	To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages." ></td>
	<td class="line x" title="52:195	Standard Nave Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language." ></td>
	<td class="line x" title="53:195	Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods." ></td>
	<td class="line x" title="54:195	In this study, we focus on improving the corpus-based method for crosslingual sentiment classification of Chinese product reviews by developing novel approaches." ></td>
	<td class="line x" title="55:195	2.2 Cross-Domain Text Classification Cross-domain text classification can be considered as a more general task than cross-lingual sentiment classification." ></td>
	<td class="line x" title="56:195	In the problem of crossdomain text classification, the labeled and unlabeled data come from different domains, and their underlying distributions are often different from each other, which violates the basic assumption of traditional classification learning." ></td>
	<td class="line x" title="57:195	To date, many semi-supervised learning algorithms have been developed for addressing the cross-domain text classification problem by transferring knowledge across domains, including Transductive SVM (Joachims, 1999), EM(Nigam et al., 2000), EM-based Nave Bayes classifier (Dai et al., 2007a), Topic-bridged PLSA (Xue et al., 2008), Co-Clustering based classification (Dai et al., 2007b), two-stage approach (Jiang and Zhai, 2007)." ></td>
	<td class="line x" title="58:195	DaumIII and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model." ></td>
	<td class="line x" title="59:195	In particular, several previous studies focus on the problem of cross-lingual text classification, which can be considered as a special case of general cross-domain text classification." ></td>
	<td class="line x" title="60:195	Bel et al.(2003) present practical and cost-effective solutions." ></td>
	<td class="line x" title="62:195	A few novel models have been proposed to address the problem, e.g. the EM-based algorithm (Rigutini et al., 2005), the information bottleneck approach (Ling et al., 2008), the multilingual domain models (Gliozzo and Strapparava, 2005), etc. To the best of our knowledge, cotraining has not yet been investigated for crossdomain or cross-lingual text classification." ></td>
	<td class="line x" title="63:195	236 3 The Co-Training Approach 3.1 Overview The purpose of our approach is to make use of the annotated English corpus for sentiment polarity identification of Chinese reviews in a supervised framework, without using any Chinese resources." ></td>
	<td class="line x" title="64:195	Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chinese reviews into English reviews." ></td>
	<td class="line x" title="65:195	Lastly, we use the classifier to classify the translated English reviews." ></td>
	<td class="line x" title="66:195	2) We first translate the labeled English reviews into Chinese reviews, and then learn a classifier based on the translated Chinese reviews with labels." ></td>
	<td class="line x" title="67:195	Lastly, we use the classifier to classify the unlabeled Chinese reviews." ></td>
	<td class="line x" title="68:195	The above two methods have been used in (Banea et al., 2008) for Romanian subjectivity analysis, but the experimental results are not very promising." ></td>
	<td class="line x" title="69:195	As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different." ></td>
	<td class="line x" title="70:195	In order to address the above problem, we propose to use the co-training approach to make use of some amounts of unlabeled Chinese reviews to improve the classification accuracy." ></td>
	<td class="line x" title="71:195	The co-training approach can make full use of both the English features and the Chinese features in a unified framework." ></td>
	<td class="line x" title="72:195	The framework of the proposed approach is illustrated in Figure 1." ></td>
	<td class="line x" title="73:195	The framework consists of a training phase and a classification phase." ></td>
	<td class="line x" title="74:195	In the training phase, the input is the labeled English reviews and some amounts of unlabeled Chinese reviews 1 . The labeled English reviews are translated into labeled Chinese reviews, and the unlabeled Chinese reviews are translated into unlabeled English reviews, by using machine translation services." ></td>
	<td class="line x" title="75:195	Therefore, each review is associated with an English version and a Chinese version." ></td>
	<td class="line x" title="76:195	The English features and the Chinese features for each review are considered two independent and redundant views of the review." ></td>
	<td class="line x" title="77:195	The co-training algorithm is then applied to learn two classifiers  1  The unlabeled Chinese reviews used for co-training do not include the unlabeled Chinese reviews for testing, i.e., the Chinese reviews for testing are blind to the training phase." ></td>
	<td class="line x" title="78:195	and finally the two classifiers are combined into a single sentiment classifier." ></td>
	<td class="line x" title="79:195	In the classification phase, each unlabeled Chinese review for testing is first translated into English review, and then the learned classifier is applied to classify the review into either positive or negative." ></td>
	<td class="line x" title="80:195	The steps of review translation and the cotraining algorithm are described in details in the next sections, respectively." ></td>
	<td class="line x" title="81:195	Figure 1." ></td>
	<td class="line x" title="82:195	Framework of the proposed approach 3.2 Review Translation In order to overcome the language gap, we must translate one language into another language." ></td>
	<td class="line x" title="83:195	Fortunately, machine translation techniques have been well developed in the NLP field, though the translation performance is far from satisfactory." ></td>
	<td class="line x" title="84:195	A few commercial machine translation services can be publicly accessed, e.g. Google Translate 2 , Yahoo Babel Fish 3  and Windows Live Translate 4 .  2  http://translate.google.com/translate_t 3  http://babelfish.yahoo.com/translate_txt 4  http://www.windowslivetranslator.com/ Unlabeled Chinese Reviews Labeled English Reviews Machine Translation (CN-EN) Co-Training Machine Translation (EN-CN) Labeled Chinese Reviews Unlabeled English Reviews Pos\Neg Chinese View English View Test Chinese Review Sentiment Classifier Machine Translation (CN-EN) Test English Review Training Phase Classification Phase 237 In this study, we adopt Google Translate for both English-to-Chinese Translation and Chinese-toEnglish Translation, because it is one of the state-of-the-art commercial machine translation systems used today." ></td>
	<td class="line x" title="85:195	Google Translate applies statistical learning techniques to build a translation model based on both monolingual text in the target language and aligned text consisting of examples of human translations between the languages." ></td>
	<td class="line x" title="86:195	3.3 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way." ></td>
	<td class="line x" title="87:195	One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed." ></td>
	<td class="line x" title="88:195	Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001)." ></td>
	<td class="line x" title="89:195	In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features." ></td>
	<td class="line x" title="90:195	Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise." ></td>
	<td class="line x" title="91:195	The co-training algorithm is illustrated in Figure 2." ></td>
	<td class="line x" title="92:195	In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration." ></td>
	<td class="line x" title="93:195	The intuition of the co-training algorithm is that if one classifier can confidently predict the class of an example, which is very similar to some of labeled ones, it can provide one more training example for the other classifier." ></td>
	<td class="line x" title="94:195	But, of course, if this example happens to be easy to be classified by the first classifier, it does not mean that this example will be easy to be classified by the second classifier, so the second classifier will get useful information to improve itself and vice versa (Kiritchenko and Matwin, 2001)." ></td>
	<td class="line x" title="95:195	In the co-training algorithm, a basic classification algorithm is required to construct C en  and C cn . Typical text classifiers include Support Vector Machine (SVM), Nave Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), etc. In this study, we adopt the widely-used SVM classifier (Joachims, 2002)." ></td>
	<td class="line x" title="96:195	Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets." ></td>
	<td class="line x" title="97:195	The English or Chinese features used in this study include both unigrams and bigrams 5  and the feature weight is simply set to term frequency 6 . Feature selection methods (e.g. Document Frequency (DF), Information Gain (IG), and Mutual Information (MI)) can be used for dimension reduction." ></td>
	<td class="line x" title="98:195	But we use all the features in the experiments for comparative analysis, because there is no significant performance improvement after applying the feature selection techniques in our empirical study." ></td>
	<td class="line x" title="99:195	The output value of the SVM classifier for a review indicates the confidence level of the reviews classification." ></td>
	<td class="line x" title="100:195	Usually, the sentiment polarity of a review is indicated by the sign of the prediction value." ></td>
	<td class="line x" title="101:195	Given: F en  and F cn  are redundantly sufficient sets of features, where F en  represents the English features, F cn  represents the Chinese features; L is a set of labeled training reviews; U is a set of unlabeled reviews; Loop for I iterations: 1." ></td>
	<td class="line x" title="102:195	Learn the first classifier C en  from L based on F en ; 2." ></td>
	<td class="line x" title="103:195	Use C en  to label reviews from U based on F en ; 3." ></td>
	<td class="line x" title="104:195	Choose p positive and n negative the most confidently predicted reviews E en  from U; 4." ></td>
	<td class="line x" title="105:195	Learn the second classifier C cn  from L based on F cn ; 5." ></td>
	<td class="line x" title="106:195	Use C cn  to label reviews from U based on F cn ; 6." ></td>
	<td class="line x" title="107:195	Choose p positive and n negative the most confidently predicted reviews E cn  from U; 7." ></td>
	<td class="line x" title="108:195	Removes reviews E en E cn  from U 7 ; 8." ></td>
	<td class="line x" title="109:195	Add reviews E en E cn  with the corresponding labels to L; Figure 2." ></td>
	<td class="line x" title="110:195	The co-training algorithm In the training phase, the co-training algorithm learns two separate classifiers: C en  and C cn .  5  For Chinese text, a unigram refers to a Chinese word and a bigram refers to two adjacent Chinese words." ></td>
	<td class="line x" title="111:195	6  Term frequency performs better than TFIDF by our empirical analysis." ></td>
	<td class="line x" title="112:195	7  Note that the examples with conflicting labels are not included in E en E cn In other words, if an example is in both E en  and E cn , but the labels for the example is conflicting, the example will be excluded from E en E cn." ></td>
	<td class="line x" title="113:195	238 Therefore, in the classification phase, we can obtain two prediction values for a test review." ></td>
	<td class="line x" title="114:195	We normalize the prediction values into [-1, 1] by dividing the maximum absolute value." ></td>
	<td class="line x" title="115:195	Finally, the average of the normalized values is used as the overall prediction value of the review." ></td>
	<td class="line x" title="116:195	4 Empirical Evaluation 4.1 Evaluation Setup 4.1.1 Data set The following three datasets were collected and used in the experiments: Test Set (Labeled Chinese Reviews): In order to assess the performance of the proposed approach, we collected and labeled 886 product reviews (451 positive reviews + 435 negative reviews) from a popular Chinese IT product web site-IT168 8 . The reviews focused on such products as mp3 players, mobile phones, digital camera and laptop computers." ></td>
	<td class="line x" title="117:195	Training Set (Labeled English Reviews): There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007) 9 , because the corpus was large-scale and it was within similar domains as the test set." ></td>
	<td class="line x" title="118:195	The dataset consisted of 8000 Amazon product reviews (4000 positive reviews + 4000 negative reviews) for four different product types: books, DVDs, electronics and kitchen appliances." ></td>
	<td class="line x" title="119:195	Unlabeled Set (Unlabeled Chinese Reviews): We downloaded additional 1000 Chinese product reviews from IT168 and used the reviews as the unlabeled set." ></td>
	<td class="line x" title="120:195	Therefore, the unlabeled set and the test set were in the same domain and had similar underlying feature distributions." ></td>
	<td class="line x" title="121:195	Each Chinese review was translated into English review, and each English review was translated into Chinese review." ></td>
	<td class="line x" title="122:195	Therefore, each review has two independent views: English view and Chinese view." ></td>
	<td class="line x" title="123:195	A review is represented by both its English view and its Chinese view." ></td>
	<td class="line x" title="124:195	Note that the training set and the unlabeled set are used in the training phase, while the test set is blind to the training phase." ></td>
	<td class="line x" title="125:195	4.1.2 Evaluation Metric We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the  8  http://www.it168.com 9  http://www.cis.upenn.edu/~mdredze/datasets/sentiment/ accuracy metric to measure the overall performance of the system." ></td>
	<td class="line x" title="126:195	The metrics are defined the same as in general text categorization." ></td>
	<td class="line x" title="127:195	4.1.3 Baseline Methods In the experiments, the proposed co-training approach (CoTrain) is compared with the following baseline methods: SVM(CN): This method applies the inductive SVM with only Chinese features for sentiment classification in the Chinese view." ></td>
	<td class="line x" title="128:195	Only Englishto-Chinese translation is needed." ></td>
	<td class="line x" title="129:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="130:195	SVM(EN): This method applies the inductive SVM with only English features for sentiment classification in the English view." ></td>
	<td class="line x" title="131:195	Only Chineseto-English translation is needed." ></td>
	<td class="line x" title="132:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="133:195	SVM(ENCN1): This method applies the inductive SVM with both English and Chinese features for sentiment classification in the two views." ></td>
	<td class="line x" title="134:195	Both English-to-Chinese and Chinese-toEnglish translations are required." ></td>
	<td class="line x" title="135:195	And the unlabeled set is not used." ></td>
	<td class="line x" title="136:195	SVM(ENCN2): This method combines the results of SVM(EN) and SVM(CN) by averaging the prediction values in the same way with the co-training approach." ></td>
	<td class="line x" title="137:195	TSVM(CN): This method applies the transductive SVM with only Chinese features for sentiment classification in the Chinese view." ></td>
	<td class="line x" title="138:195	Only English-to-Chinese translation is needed." ></td>
	<td class="line x" title="139:195	And the unlabeled set is used." ></td>
	<td class="line x" title="140:195	TSVM(EN): This method applies the transductive SVM with only English features for sentiment classification in the English view." ></td>
	<td class="line x" title="141:195	Only Chinese-to-English translation is needed." ></td>
	<td class="line x" title="142:195	And the unlabeled set is used." ></td>
	<td class="line x" title="143:195	TSVM(ENCN1): This method applies the transductive SVM with both English and Chinese features for sentiment classification in the two views." ></td>
	<td class="line x" title="144:195	Both English-to-Chinese and Chinese-toEnglish translations are required." ></td>
	<td class="line x" title="145:195	And the unlabeled set is used." ></td>
	<td class="line x" title="146:195	TSVM(ENCN2): This method combines the results of TSVM(EN) and TSVM(CN) by averaging the prediction values." ></td>
	<td class="line x" title="147:195	Note that the first four methods are straightforward methods used in previous work, while the latter four methods are strong baselines because the transductive SVM has been widely used for improving the classification accuracy by leveraging additional unlabeled examples." ></td>
	<td class="line x" title="148:195	239 4.2 Evaluation Results 4.2.1 Method Comparison In the experiments, we first compare the proposed co-training approach (I=40 and p=n=5) with the eight baseline methods." ></td>
	<td class="line x" title="149:195	The three parameters in the co-training approach are empirically set by considering the total number (i.e. 1000) of the unlabeled Chinese reviews." ></td>
	<td class="line x" title="150:195	In our empirical study, the proposed approach can perform well with a wide range of parameter values, which will be shown later." ></td>
	<td class="line x" title="151:195	Table 1 shows the comparison results." ></td>
	<td class="line x" title="152:195	Seen from the table, the proposed co-training approach outperforms all eight baseline methods over all metrics." ></td>
	<td class="line x" title="153:195	Among the eight baselines, the best one is TSVM(ENCN2), which combines the results of two transductive SVM classifiers." ></td>
	<td class="line x" title="154:195	Actually, TSVM(ENCN2) is similar to CoTrain because CoTrain also combines the results of two classifiers in the same way." ></td>
	<td class="line x" title="155:195	However, the co-training approach can train two more effective classifiers, and the accuracy values of the component English and Chinese classifiers are 0.775 and 0.790, respectively, which are higher than the corresponding TSVM classifiers." ></td>
	<td class="line x" title="156:195	Overall, the use of transductive learning and the combination of English and Chinese views are beneficial to the final classification accuracy, and the cotraining approach is more suitable for making use of the unlabeled Chinese reviews than the transductive SVM." ></td>
	<td class="line x" title="157:195	4.2.2 Influences of Iteration Number (I) Figure 3 shows the accuracy curve of the cotraining approach (Combined Classifier) with different numbers of iterations." ></td>
	<td class="line x" title="158:195	The iteration number I is varied from 1 to 80." ></td>
	<td class="line x" title="159:195	When I is set to 1, the co-training approach is degenerated into SVM(ENCN2)." ></td>
	<td class="line x" title="160:195	The accuracy curves of the component English and Chinese classifiers learned in the co-training approach are also shown in the figure." ></td>
	<td class="line x" title="161:195	We can see that the proposed co-training approach can outperform the best baselineTSVM(ENCN2) after 20 iterations." ></td>
	<td class="line x" title="162:195	After a large number of iterations, the performance of the cotraining approach decreases because noisy training examples may be selected from the remaining unlabeled set." ></td>
	<td class="line x" title="163:195	Finally, the performance of the approach does not change any more, because the algorithm runs out of all possible examples in the unlabeled set." ></td>
	<td class="line x" title="164:195	Fortunately, the proposed approach performs well with a wide range of iteration numbers." ></td>
	<td class="line x" title="165:195	We can also see that the two component classifier has similar trends with the cotraining approach." ></td>
	<td class="line x" title="166:195	It is encouraging that the component Chinese classifier alone can perform better than the best baseline when the iteration number is set between 40 and 70." ></td>
	<td class="line x" title="167:195	4.2.3 Influences of Growth Size (p, n) Figure 4 shows how the growth size at each iteration (p positive and n negative confident examples) influences the accuracy of the proposed co-training approach." ></td>
	<td class="line x" title="168:195	In the above experiments, we set p=n, which is considered as a balanced growth." ></td>
	<td class="line x" title="169:195	When p differs very much from n, the growth is considered as an imbalanced growth." ></td>
	<td class="line x" title="170:195	Balanced growth of (2, 2), (5, 5), (10, 10) and (15, 15) examples and imbalanced growth of (1, 5), (5, 1) examples are compared in the figure." ></td>
	<td class="line x" title="171:195	We can see that the performance of the cotraining approach with the balanced growth can be improved after a few iterations." ></td>
	<td class="line x" title="172:195	And the performance of the co-training approach with large p and n will more quickly become unchanged, because the approach runs out of the limited examples in the unlabeled set more quickly." ></td>
	<td class="line x" title="173:195	However, the performance of the co-training approaches with the two imbalanced growths is always going down quite rapidly, because the labeled unbalanced examples hurt the performance badly at each iteration." ></td>
	<td class="line x" title="174:195	Positive Negative Total Method Precision Recall F-measure Precision Recall F-measure Accuracy SVM(CN) 0.733 0.865 0.793 0.828 0.674 0.743 0.771 SVM(EN) 0.717 0.803 0.757 0.766 0.671 0.716 0.738 SVM(ENCN1) 0.744 0.820 0.781 0.792 0.708 0.748 0.765 SVM(ENCN2) 0.746 0.847 0.793 0.816 0.701 0.754 0.775 TSVM(CN) 0.724 0.878 0.794 0.838 0.653 0.734 0.767 TSVM(EN) 0.732 0.860 0.791 0.823 0.674 0.741 0.769 TSVM(ENCN1) 0.743 0.878 0.805 0.844 0.685 0.756 0.783 TSVM(ENCN2) 0.744 0.896 0.813 0.863 0.680 0.761 0.790 CoTrain (I=40; p=n=5) 0.768 0.905 0.831 0.879 0.717 0.790 0.813 Table 1." ></td>
	<td class="line x" title="175:195	Comparison results 240 0.72 0.73 0.74 0.75 0.76 0.77 0.78 0.79 0.8 0.81 0.82 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Iteration Number (I ) Acc u ra c y English Classifier(CoTrain) Chinese Classifier(CoTrain) Combined Classifier(CoTrain) TSVM(ENCN2)  Figure 3." ></td>
	<td class="line x" title="176:195	Accuracy vs. number of iterations for co-training (p=n=5) 0.5 0.55 0.6 0.65 0.7 0.75 0.8 1 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 Iteration Number (I ) Ac cura c y (p=2,n=2) (p=5,n=5) (p=10,n=10) (p=15,n=15) (p=1,n=5) (p=5,n=1)  Figure 4." ></td>
	<td class="line x" title="177:195	Accuracy vs. different (p, n) for co-training 0.76 0.77 0.78 0.79 0.8 0.81 0.82 25% 50% 75% 100% Feature size Acc u r a c y TSVM(ENCN1) TSVM(ENCN2) CoTrain (I=40; p=n=5)  Figure 5." ></td>
	<td class="line x" title="178:195	Influences of feature size  241 4.2.4 Influences of Feature Selection In the above experiments, all features (unigram + bigram) are used." ></td>
	<td class="line x" title="179:195	As mentioned earlier, feature selection techniques are widely used for dimension reduction." ></td>
	<td class="line x" title="180:195	In this section, we further conduct experiments to investigate the influences of feature selection techniques on the classification results." ></td>
	<td class="line x" title="181:195	We use the simple but effective document frequency (DF) for feature selection." ></td>
	<td class="line x" title="182:195	Figures 6 show the comparison results of different feature sizes for the co-training approach and two strong baselines." ></td>
	<td class="line x" title="183:195	The feature size is measured as the proportion of the selected features against the total features (i.e. 100%)." ></td>
	<td class="line x" title="184:195	We can see from the figure that the feature selection technique has very slight influences on the classification accuracy of the methods." ></td>
	<td class="line x" title="185:195	It can be seen that the co-training approach can always outperform the two baselines with different feature sizes." ></td>
	<td class="line x" title="186:195	The results further demonstrate the effectiveness and robustness of the proposed cotraining approach." ></td>
	<td class="line x" title="187:195	5 Conclusion and Future Work In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification." ></td>
	<td class="line x" title="188:195	The experimental results show the effectiveness of the proposed approach." ></td>
	<td class="line x" title="189:195	In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification." ></td>
	<td class="line x" title="190:195	The approach has the effect of smoothing the learning curves." ></td>
	<td class="line x" title="191:195	During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations." ></td>
	<td class="line x" title="192:195	2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service." ></td>
	<td class="line x" title="193:195	We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text." ></td>
	<td class="line x" title="194:195	Acknowledgments This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006)." ></td>
	<td class="line x" title="195:195	We also thank the anonymous reviewers for their useful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1028
A Non-negative Matrix Tri-factorization Approach to Sentiment Classification with Lexical Prior Knowledge
Li, Tao;Zhang, Yi;Sindhwani, Vikas;"></td>
	<td class="line x" title="1:191	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 244252, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:191	c2009 ACL and AFNLP A Non-negative Matrix Tri-factorization Approach to Sentiment Classification with Lexical Prior Knowledge Tao Li Yi Zhang School of Computer Science Florida International University {taoli,yzhan004}@cs.fiu.edu Vikas Sindhwani Mathematical Sciences IBM T.J. Watson Research Center vsindhw@us.ibm.com Abstract Sentiment classi cation refers to the task of automatically identifying whether a given piece of text expresses positive or negative opinion towards a subject at hand." ></td>
	<td class="line x" title="3:191	The proliferation of user-generated web content such as blogs, discussion forums and online review sites has made it possible to perform large-scale mining of public opinion." ></td>
	<td class="line x" title="4:191	Sentiment modeling is thus becoming a critical component of market intelligence and social media technologies that aim to tap into the collective wisdom of crowds." ></td>
	<td class="line x" title="5:191	In this paper, we consider the problem of learning high-quality sentiment models with minimal manual supervision." ></td>
	<td class="line x" title="6:191	We propose a novel approach to learn from lexical prior knowledge in the form of domain-independent sentimentladen terms, in conjunction with domaindependent unlabeled data and a few labeled documents." ></td>
	<td class="line x" title="7:191	Our model is based on a constrained non-negative tri-factorization of the term-document matrix which can be implemented using simple update rules." ></td>
	<td class="line x" title="8:191	Extensive experimental studies demonstrate the effectiveness of our approach on a variety of real-world sentiment prediction tasks." ></td>
	<td class="line x" title="9:191	1 Introduction Web 2.0 platforms such as blogs, discussion forums and other such social media have now given a public voice to every consumer." ></td>
	<td class="line x" title="10:191	Recent surveys have estimated that a massive number of internet users turn to such forums to collect recommendations for products and services, guiding their own choices and decisions by the opinions that other consumers have publically expressed." ></td>
	<td class="line x" title="11:191	Gleaning insights by monitoring and analyzing large amounts of such user-generated data is thus becoming a key competitive differentiator for many companies." ></td>
	<td class="line x" title="12:191	While tracking brand perceptions in traditional media is hardly a new challenge, handling the unprecedented scale of unstructured user-generated web content requires new methodologies." ></td>
	<td class="line x" title="13:191	These methodologies are likely to be rooted in natural language processing and machine learning techniques." ></td>
	<td class="line x" title="14:191	Automatically classifying the sentiment expressed in a blog around selected topics of interest is a canonical machine learning task in this discussion." ></td>
	<td class="line x" title="15:191	A standard approach would be to manually label documents with their sentiment orientation and then apply off-the-shelf text classi cation techniques." ></td>
	<td class="line x" title="16:191	However, sentiment is often conveyed with subtle linguistic mechanisms such as the use of sarcasm and highly domain-speci c contextual cues." ></td>
	<td class="line x" title="17:191	This makes manual annotation of sentiment time consuming and error-prone, presenting a bottleneck in learning high quality models." ></td>
	<td class="line x" title="18:191	Moreover, products and services of current focus, and associated community of bloggers with their idiosyncratic expressions, may rapidly evolve over time causing models to potentially lose performance and become stale." ></td>
	<td class="line x" title="19:191	This motivates the problem of learning robust sentiment models from minimal supervision." ></td>
	<td class="line x" title="20:191	In their seminal work, (Pang et al., 2002) demonstrated that supervised learning signi cantly outperformed a competing body of work where hand-crafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms." ></td>
	<td class="line x" title="21:191	As observed by (Ng et al., 2006), most semi-automated dictionary-based approaches yield unsatisfactory lexicons, with either high coverage and low precision or vice versa." ></td>
	<td class="line x" title="22:191	However, the treatment of such dictionaries as forms of prior knowledge that can be incorporated in machine learning models is a relatively less explored topic; even lesser so in conjunction with semi-supervised models that attempt to utilize un244 labeled data." ></td>
	<td class="line x" title="23:191	This is the focus of the current paper." ></td>
	<td class="line x" title="24:191	Our models are based on a constrained nonnegative tri-factorization of the term-document matrix, which can be implemented using simple update rules." ></td>
	<td class="line x" title="25:191	Treated as a set of labeled features, the sentiment lexicon is incorporated as one set of constraints that enforce domain-independent prior knowledge." ></td>
	<td class="line x" title="26:191	A second set of constraints introduce domain-speci c supervision via a few document labels." ></td>
	<td class="line x" title="27:191	Together these constraints enable learning from partial supervision along both dimensions of the term-document matrix, in what may be viewed more broadly as a framework for incorporating dual-supervision in matrix factorization models." ></td>
	<td class="line x" title="28:191	We provide empirical comparisons with several competing methodologies on four, very different domains  blogs discussing enterprise software products, political blogs discussing US presidential candidates, amazon.com product reviews and IMDB movie reviews." ></td>
	<td class="line x" title="29:191	Results demonstrate the effectiveness and generality of our approach." ></td>
	<td class="line x" title="30:191	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="31:191	We begin by discussing related work in Section 2." ></td>
	<td class="line x" title="32:191	Section 3 gives a quick background on Nonnegative Matrix Tri-factorization models." ></td>
	<td class="line x" title="33:191	In Section 4, we present a constrained model and computational algorithm for incorporating lexical knowledge in sentiment analysis." ></td>
	<td class="line x" title="34:191	In Section 5, we enhance this model by introducing document labels as additional constraints." ></td>
	<td class="line x" title="35:191	Section 6 presents an empirical study on four datasets." ></td>
	<td class="line x" title="36:191	Finally, Section 7 concludes this paper." ></td>
	<td class="line x" title="37:191	2 Related Work We point the reader to a recent book (Pang and Lee, 2008) for an in-depth survey of literature on sentiment analysis." ></td>
	<td class="line x" title="38:191	In this section, we briskly cover related work to position our contributions appropriately in the sentiment analysis and machine learning literature." ></td>
	<td class="line oc" title="39:191	Methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domain-dependent lexicons (Das and Chen, 2001) to semi-automated approaches (Hu and Liu, 2004; Zhuang et al., 2006; Kim and Hovy, 2004), and even an almost fully automated approach (Turney, 2002)." ></td>
	<td class="line x" title="40:191	Most semi-automated approaches have met with limited success (Ng et al., 2006) and supervised learning models have tended to outperform dictionary-based classi cation schemes (Pang et al., 2002)." ></td>
	<td class="line x" title="41:191	A two-tier scheme (Pang and Lee, 2004) where sentences are  rst classi ed as subjective versus objective, and then applying the sentiment classi er on only the subjective sentences further improves performance." ></td>
	<td class="line x" title="42:191	Results in these papers also suggest that using more sophisticated linguistic models, incorporating parts-of-speech and n-gram language models, do not improve over the simple unigram bag-of-words representation." ></td>
	<td class="line x" title="43:191	In keeping with these  ndings, we also adopt a unigram text model." ></td>
	<td class="line x" title="44:191	A subjectivity classi cation phase before our models are applied may further improve the results reported in this paper, but our focus is on driving the polarity prediction stage with minimal manual effort." ></td>
	<td class="line x" title="45:191	In this regard, our model brings two interrelated but distinct themes from machine learning to bear on this problem: semi-supervised learning and learning from labeled features." ></td>
	<td class="line x" title="46:191	The goal of the former theme is to learn from few labeled examples by making use of unlabeled data, while the goal of the latter theme is to utilize weak prior knowledge about term-class af nities (e.g., the term  awful indicates negative sentiment and therefore may be considered as a negatively labeled feature)." ></td>
	<td class="line x" title="47:191	Empirical results in this paper demonstrate that simultaneously attempting both these goals in a single model leads to improvements over models that focus on a single goal." ></td>
	<td class="line x" title="48:191	(Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features." ></td>
	<td class="line x" title="49:191	Most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning: (Schapire et al., 2002) propose one such framework for boosting logistic regression; (Wu and Srihari, 2004) build a modi ed SVM and (Liu et al., 2004) use a combination of clustering and EM based methods to instantiate similar frameworks." ></td>
	<td class="line x" title="50:191	By contrast, we incorporate lexical knowledge directly as constraints on our matrix factorization model." ></td>
	<td class="line x" title="51:191	In recent work, Druck et al.(Druck et al., 2008) constrain the predictions of a multinomial logistic regression model on unlabeled instances in a Generalized Expectation formulation for learning from labeled features." ></td>
	<td class="line x" title="53:191	Unlike their approach which uses only unlabeled instances, our method uses both labeled and unlabeled documents in conjunction with labeled and 245 unlabeled words." ></td>
	<td class="line x" title="54:191	The matrix tri-factorization models explored in this paper are closely related to the models proposed recently in (Li et al., 2008; Sindhwani et al., 2008)." ></td>
	<td class="line x" title="55:191	Though, their techniques for proving algorithm convergence and correctness can be readily adapted for our models, (Li et al., 2008) do not incorporate dual supervision as we do." ></td>
	<td class="line x" title="56:191	On the other hand, while (Sindhwani et al., 2008) do incorporate dual supervision in a non-linear kernelbased setting, they do not enforce non-negativity or orthogonality  aspects of matrix factorization models that have shown bene ts in prior empirical studies, see e.g., (Ding et al., 2006)." ></td>
	<td class="line x" title="57:191	We also note the very recent work of (Sindhwani and Melville, 2008) which proposes a dualsupervision model for semi-supervised sentiment analysis." ></td>
	<td class="line x" title="58:191	In this model, bipartite graph regularization is used to diffuse label information along both sides of the term-document matrix." ></td>
	<td class="line x" title="59:191	Conceptually, their model implements a co-clustering assumption closely related to Singular Value Decomposition (see also (Dhillon, 2001; Zha et al., 2001) for more on this perspective) while our model is based on Non-negative Matrix Factorization." ></td>
	<td class="line x" title="60:191	In another recent paper (Sandler et al., 2008), standard regularization models are constrained using graphs of word co-occurences." ></td>
	<td class="line x" title="61:191	These are very recently proposed competing methodologies, and we have not been able to address empirical comparisons with them in this paper." ></td>
	<td class="line x" title="62:191	Finally, recent efforts have also looked at transfer learning mechanisms for sentiment analysis, e.g., see (Blitzer et al., 2007)." ></td>
	<td class="line x" title="63:191	While our focus is on single-domain learning in this paper, we note that cross-domain variants of our model can also be orthogonally developed." ></td>
	<td class="line x" title="64:191	3 Background 3.1 Basic Matrix Factorization Model Our proposed models are based on non-negative matrix Tri-factorization (Ding et al., 2006)." ></td>
	<td class="line x" title="65:191	In these models, an m n term-document matrix X is approximated by three factors that specify soft membership of terms and documents in one of kclasses: X FSGT ." ></td>
	<td class="line x" title="66:191	(1) where F is an m k non-negative matrix representing knowledge in the word space, i.e., i-th row of F represents the posterior probability of word i belonging to the k classes, G is an n k nonnegative matrix representing knowledge in document space, i.e., the i-th row of G represents the posterior probability of document i belonging to the k classes, and S is an k k nonnegative matrix providing a condensed view of X. The matrix factorization model is similar to the probabilistic latent semantic indexing (PLSI) model (Hofmann, 1999)." ></td>
	<td class="line x" title="67:191	In PLSI, X is treated as the joint distribution between words and documents by the scaling X ! flX = X/i j Xi j thus i j flXi j = 1)." ></td>
	<td class="line x" title="68:191	flX is factorized as flX WSDT , k Wik = 1, k D jk = 1, k Skk = 1." ></td>
	<td class="line x" title="69:191	(2) where X is the m n word-document semantic matrix, X = WSD, W is the word classconditional probability, and D is the document class-conditional probability and S is the class probability distribution." ></td>
	<td class="line x" title="70:191	PLSI provides a simultaneous solution for the word and document class conditional distribution." ></td>
	<td class="line x" title="71:191	Our model provides simultaneous solution for clustering the rows and the columns of X. To avoid ambiguity, the orthogonality conditions FT F = I, GT G = I." ></td>
	<td class="line x" title="72:191	(3) can be imposed to enforce each row of F and G to possess only one nonzero entry." ></td>
	<td class="line x" title="73:191	Approximating the term-document matrix with a tri-factorization while imposing non-negativity and orthogonality constraints gives a principled framework for simultaneously clustering the rows (words) and columns (documents) of X. In the context of coclustering, these models return excellent empirical performance, see e.g., (Ding et al., 2006)." ></td>
	<td class="line x" title="74:191	Our goal now is to bias these models with constraints incorporating (a) labels of features (coming from a domain-independent sentiment lexicon), and (b) labels of documents for the purposes of domainspeci c adaptation." ></td>
	<td class="line x" title="75:191	These enhancements are addressed in Sections 4 and 5 respectively." ></td>
	<td class="line x" title="76:191	4 Incorporating Lexical Knowledge We used a sentiment lexicon generated by the IBM India Research Labs that was developed for other text mining applications (Ramakrishnan et al., 2003)." ></td>
	<td class="line x" title="77:191	It contains 2,968 words that have been human-labeled as expressing positive or negative sentiment." ></td>
	<td class="line x" title="78:191	In total, there are 1,267 positive (e.g.  great ) and 1,701 negative (e.g.,  bad ) unique 246 terms after stemming." ></td>
	<td class="line x" title="79:191	We eliminated terms that were ambiguous and dependent on context, such as  dear and   ne . It should be noted, that this list was constructed without a speci c domain in mind; which is further motivation for using training examples and unlabeled data to learn domain speci c connotations." ></td>
	<td class="line x" title="80:191	Lexical knowledge in the form of the polarity of terms in this lexicon can be introduced in the matrix factorization model." ></td>
	<td class="line x" title="81:191	By partially specifying term polarities via F, the lexicon in uences the sentiment predictions G over documents." ></td>
	<td class="line x" title="82:191	4.1 Representing Knowledge in Word Space Let F0 represent prior knowledge about sentimentladen words in the lexicon, i.e., if word i is a positive word (F0)i1 = 1 while if it is negative (F0)i2 = 1." ></td>
	<td class="line x" title="83:191	Note that one may also use soft sentiment polarities though our experiments are conducted with hard assignments." ></td>
	<td class="line x" title="84:191	This information is incorporated in the tri-factorization model via a squared loss term, min F;G;S kX FSGTk2 + Trbracketleftbig(F F0)TC1(F F0)bracketrightbig (4) where the notation Tr(A) means trace of the matrix A. Here,  > 0 is a parameter which determines the extent to which we enforce F F0, C1 is a m m diagonal matrix whose entry (C1)ii = 1 if the category of the i-th word is known (i.e., speci ed by the i-th row of F0) and (C1)ii = 0 otherwise." ></td>
	<td class="line x" title="85:191	The squared loss terms ensure that the solution for F in the otherwise unsupervised learning problem be close to the prior knowledge F0." ></td>
	<td class="line x" title="86:191	Note that if C1 = I, then we know the class orientation of all the words and thus have a full speci cation of F0, Eq.(4) is then reduced to min F;G;S kX FSGTk2 + kF F0k2 (5) The above model is generic and it allows certain  exibility." ></td>
	<td class="line x" title="87:191	For example, in some cases, our prior knowledge on F0 is not very accurate and we use smaller  so that the  nal results are not dependent on F0 very much, i.e., the results are mostly unsupervised learning results." ></td>
	<td class="line x" title="88:191	In addition, the introduction of C1 allows us to incorporate partial knowledge on word polarity information." ></td>
	<td class="line x" title="89:191	4.2 Computational Algorithm The optimization problem in Eq.( 4) can be solved using the following update rules G jk G jk (X T FS)jk (GGT XT FS)jk , (6) Sik  Sik (F T XG)ik (FT FSGT G)ik ." ></td>
	<td class="line x" title="90:191	(7) Fik Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik ." ></td>
	<td class="line x" title="91:191	(8) The algorithm consists of an iterative procedure using the above three rules until convergence." ></td>
	<td class="line x" title="92:191	We call this approach Matrix Factorization with Lexical Knowledge (MFLK) and outline the precise steps in the table below." ></td>
	<td class="line x" title="93:191	Algorithm 1 Matrix Factorization with Lexical Knowledge (MFLK) begin 1." ></td>
	<td class="line x" title="94:191	Initialization: Initialize F = F0 G to K-means clustering results, S = (FT F) 1FT XG(GT G) 1." ></td>
	<td class="line x" title="95:191	2. Iteration: Update G:  xing F,S, updating G Update F:  xing S,G, updating F Update S:  xing F,G, updating S end 4.3 Algorithm Correctness and Convergence Updating F,G,S using the rules above leads to an asymptotic convergence to a local minima." ></td>
	<td class="line x" title="96:191	This can be proved using arguments similar to (Ding et al., 2006)." ></td>
	<td class="line x" title="97:191	We outline the proof of correctness for updating F since the squared loss term that involves F is a new component in our models." ></td>
	<td class="line x" title="98:191	Theorem 1 The above iterative algorithm converges." ></td>
	<td class="line x" title="99:191	Theorem 2 At convergence, the solution satisfies the Karuch, Kuhn, Tucker optimality condition, i.e., the algorithm converges correctly to a local optima." ></td>
	<td class="line x" title="100:191	Theorem 1 can be proved using the standard auxiliary function approach used in (Lee and Seung, 2001)." ></td>
	<td class="line x" title="101:191	Proof of Theorem 2." ></td>
	<td class="line x" title="102:191	Following the theory of constrained optimization (Nocedal and Wright, 1999), 247 we minimize the following function L(F) =jjX FSGTjj2 +Trbracketleftbig(F F0)TC1(F F0)bracketrightbig Note that the gradient of L is, L F = 2XGS T + 2FSGT GST + 2C1(F F0)." ></td>
	<td class="line x" title="103:191	(9) The KKT complementarity condition for the nonnegativity of Fik gives [ 2XGST + FSGT GST + 2C1(F F0)]ikFik = 0." ></td>
	<td class="line x" title="104:191	(10) This is the  xed point relation that local minima for F must satisfy." ></td>
	<td class="line x" title="105:191	Given an initial guess of F, the successive update of F using Eq.(8) will converge to a local minima." ></td>
	<td class="line x" title="106:191	At convergence, we have Fik = Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik . which is equivalent to the KKT condition of Eq.(10)." ></td>
	<td class="line x" title="107:191	The correctness of updating rules for G in Eq.(6) and S in Eq.(7) have been proved in (Ding et al., 2006)." ></td>
	<td class="line x" title="108:191	u Note that we do not enforce exact orthogonality in our updating rules since this often implies softer class assignments." ></td>
	<td class="line x" title="109:191	5 Semi-Supervised Learning With Lexical Knowledge So far our models have made no demands on human effort, other than unsupervised collection of the term-document matrix and a one-time effort in compiling a domain-independent sentiment lexicon." ></td>
	<td class="line x" title="110:191	We now assume that a few documents are manually labeled for the purposes of capturing some domain-speci c connotations leading to a more domain-adapted model." ></td>
	<td class="line x" title="111:191	The partial labels on documents can be described using G0 where (G0)i1 = 1 if the document expresses positive sentiment, and (G0)i2 = 1 for negative sentiment." ></td>
	<td class="line x" title="112:191	As with F0, one can also use soft sentiment labeling for documents, though our experiments are conducted with hard assignments." ></td>
	<td class="line x" title="113:191	Therefore, the semi-supervised learning with lexical knowledge can be described as min F;G;S kX FSGTk2 + Trbracketleftbig(F F0)TC1(F F0)bracketrightbig+ Trbracketleftbig(G G0)TC2(G G0)bracketrightbig Where  > 0, > 0 are parameters which determine the extent to which we enforce F  F0 and G G0 respectively, C1 and C2 are diagonal matrices indicating the entries of F0 and G0 that correspond to labeled entities." ></td>
	<td class="line x" title="114:191	The squared loss terms ensure that the solution for F,G, in the otherwise unsupervised learning problem, be close to the prior knowledge F0 and G0." ></td>
	<td class="line x" title="115:191	5.1 Computational Algorithm The optimization problem in Eq.( 4) can be solved using the following update rules G jk G jk (X T FS + C2G0)jk (GGT XT FS + GGTC2G0)jk (11) Sik  Sik (F T XG)ik (FT FSGT G)ik ." ></td>
	<td class="line x" title="116:191	(12) Fik Fik (XGS T + C1F0)ik (FFT XGST + C1F)ik ." ></td>
	<td class="line x" title="117:191	(13) Thus the algorithm for semi-supervised learning with lexical knowledge based on our matrix factorization framework, referred as SSMFLK, consists of an iterative procedure using the above three rules until convergence." ></td>
	<td class="line x" title="118:191	The correctness and convergence of the algorithm can also be proved using similar arguments as what we outlined earlier for MFLK in Section 4.3." ></td>
	<td class="line x" title="119:191	A quick word about computational complexity." ></td>
	<td class="line x" title="120:191	The term-document matrix is typically very sparse with z nm non-zero entries while k is typically also much smaller than n,m. By using sparse matrix multiplications and avoiding dense intermediate matrices, the updates can be very ef ciently and easily implemented." ></td>
	<td class="line x" title="121:191	In particular, updating F,S,G each takes O(k2(m + n) + kz) time per iteration which scales linearly with the dimensions and density of the data matrix." ></td>
	<td class="line x" title="122:191	Empirically, the number of iterations before practical convergence is usually very small (less than 100)." ></td>
	<td class="line x" title="123:191	Thus, computationally our approach scales to large datasets even though our experiments are run on relatively small-sized datasets." ></td>
	<td class="line x" title="124:191	6 Experiments 6.1 Datasets Description Four different datasets are used in our experiments." ></td>
	<td class="line x" title="125:191	Movies Reviews: This is a popular dataset in sentiment analysis literature (Pang et al., 2002)." ></td>
	<td class="line x" title="126:191	It consists of 1000 positive and 1000 negative movie reviews drawn from the IMDB archive of the rec.arts.movies.reviews newsgroups." ></td>
	<td class="line x" title="127:191	248 Lotus blogs: The data set is targeted at detecting sentiment around enterprise software, specifically pertaining to the IBM Lotus brand (Sindhwani and Melville, 2008)." ></td>
	<td class="line x" title="128:191	An unlabeled set of blog posts was created by randomly sampling 2000 posts from a universe of 14,258 blogs that discuss issues relevant to Lotus software." ></td>
	<td class="line x" title="129:191	In addition to this unlabeled set, 145 posts were chosen for manual labeling." ></td>
	<td class="line x" title="130:191	These posts came from 14 individual blogs, 4 of which are actively posting negative content on the brand, with the rest tending to write more positive or neutral posts." ></td>
	<td class="line x" title="131:191	The data was collected by downloading the latest posts from each bloggers RSS feeds, or accessing the blogs archives." ></td>
	<td class="line x" title="132:191	Manual labeling resulted in 34 positive and 111 negative examples." ></td>
	<td class="line x" title="133:191	Political candidate blogs: For our second blog domain, we used data gathered from 16,742 political blogs, which contain over 500,000 posts." ></td>
	<td class="line x" title="134:191	As with the Lotus dataset, an unlabeled set was created by randomly sampling 2000 posts." ></td>
	<td class="line x" title="135:191	107 posts were chosen for labeling." ></td>
	<td class="line x" title="136:191	A post was labeled as having positive or negative sentiment about a speci c candidate (Barack Obama or Hillary Clinton) if it explicitly mentioned the candidate in positive or negative terms." ></td>
	<td class="line x" title="137:191	This resulted in 49 positively and 58 negatively labeled posts." ></td>
	<td class="line x" title="138:191	Amazon Reviews: The dataset contains product reviews taken from Amazon.com from 4 product types: Kitchen, Books, DVDs, and Electronics (Blitzer et al., 2007)." ></td>
	<td class="line x" title="139:191	The dataset contains about 4000 positive reviews and 4000 negative reviews and can be obtained from http://www.cis.upenn." ></td>
	<td class="line x" title="140:191	edu/mdredze/datasets/sentiment/." ></td>
	<td class="line x" title="141:191	For all datasets, we picked 5000 words with highest document-frequency to generate the vocabulary." ></td>
	<td class="line x" title="142:191	Stopwords were removed and a normalized term-frequency representation was used." ></td>
	<td class="line x" title="143:191	Genuinely unlabeled posts for Political and Lotus were used for semi-supervised learning experiments in section 6.3; they were not used in section 6.2 on the effect of lexical prior knowledge." ></td>
	<td class="line x" title="144:191	In the experiments, we set , the parameter determining the extent to which to enforce the feature labels, to be 1/2, and , the corresponding parameter for enforcing document labels, to be 1." ></td>
	<td class="line x" title="145:191	6.2 Sentiment Analysis with Lexical Knowledge Of course, one can remove all burden on human effort by simply using unsupervised techniques." ></td>
	<td class="line x" title="146:191	Our interest in the  rst set of experiments is to explore the bene ts of incorporating a sentiment lexicon over unsupervised approaches." ></td>
	<td class="line x" title="147:191	Does a one-time effort in compiling a domainindependent dictionary and using it for different sentiment tasks pay off in comparison to simply using unsupervised methods?" ></td>
	<td class="line x" title="148:191	In our case, matrix tri-factorization and other co-clustering methods form the obvious unsupervised baseline for comparison and so we start by comparing our method (MFLK) with the following methods:  Four document clustering methods: Kmeans, Tri-Factor Nonnegative Matrix Factorization (TNMF) (Ding et al., 2006), Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clustering algorithm (ECC) (Cho et al., 2004)." ></td>
	<td class="line x" title="149:191	These methods do not make use of the sentiment lexicon." ></td>
	<td class="line x" title="150:191	Feature Centroid (FC): This is a simple dictionary-based baseline method." ></td>
	<td class="line x" title="151:191	Recall that each word can be expressed as a  bagof-documents vector." ></td>
	<td class="line x" title="152:191	In this approach, we compute the centroids of these vectors, one corresponding to positive words and another corresponding to negative words." ></td>
	<td class="line x" title="153:191	This yields a two-dimensional representation for documents, on which we then perform K-means clustering." ></td>
	<td class="line x" title="154:191	Performance Comparison Figure 1 shows the experimental results on four datasets using accuracy as the performance measure." ></td>
	<td class="line x" title="155:191	The results are obtained by averaging 20 runs." ></td>
	<td class="line x" title="156:191	It can be observed that our MFLK method can effectively utilize the lexical knowledge to improve the quality of sentiment prediction." ></td>
	<td class="line x" title="157:191	Movies Lotus Political Amazon0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy   MFLK FC TNMF ECC ITCC KMeans Figure 1: Accuracy results on four datasets 249 Size of Sentiment Lexicon We also investigate the effects of the size of the sentiment lexicon on the performance of our model." ></td>
	<td class="line x" title="158:191	Figure 2 shows results with random subsets of the lexicon of increasing size." ></td>
	<td class="line x" title="159:191	We observe that generally the performance increases as more and more lexical supervision is provided." ></td>
	<td class="line x" title="160:191	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Fraction of sentiment words labeled Accuracy   Movies Lotus Political Amazon Figure 2: MFLK accuracy as size of sentiment lexicon (i.e., number of words in the lexicon) increases on the four datasets Robustness to Vocabulary Size High dimensionality and noise can have profound impact on the comparative performance of clustering and semi-supervised learning algorithms." ></td>
	<td class="line x" title="161:191	We simulate scenarios with different vocabulary sizes by selecting words based on information gain." ></td>
	<td class="line x" title="162:191	It should, however, be kept in mind that in a truely unsupervised setting document labels are unavailable and therefore information gain cannot be practically computed." ></td>
	<td class="line x" title="163:191	Figure 3 and Figure 4 show results for Lotus and Amazon datasets respectively and are representative of performance on other datasets." ></td>
	<td class="line x" title="164:191	MLFK tends to retain its position as the best performing method even at different vocabulary sizes." ></td>
	<td class="line x" title="165:191	ITCC performance is also noteworthy given that it is a completely unsupervised method." ></td>
	<td class="line x" title="166:191	0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 Fraction of Original Vocabulary Accuracy   MFLK FC TNMF KMeans ITCC ECC Figure 3: Accuracy results on Lotus dataset with increasing vocabulary size 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 Fraction of Original Vocabulary Accuracy   MFLK FC TNMF KMeans ITCC ECC Figure 4: Accuracy results on Amazon dataset with increasing vocabulary size 6.3 Sentiment Analysis with Dual Supervision We now assume that together with labeled features from the sentiment lexicon, we also have access to a few labeled documents." ></td>
	<td class="line x" title="167:191	The natural question is whether the presence of lexical constraints leads to better semi-supervised models." ></td>
	<td class="line x" title="168:191	In this section, we compare our method (SSMFLK) with the following three semi-supervised approaches: (1) The algorithm proposed in (Zhou et al., 2003) which conducts semi-supervised learning with local and global consistency (Consistency Method); (2) Zhu et al.s harmonic Gaussian  eld method coupled with the Class Mass Normalization (HarmonicCMN) (Zhu et al., 2003); and (3) Greens function learning algorithm (Greens Function) proposed in (Ding et al., 2007)." ></td>
	<td class="line x" title="169:191	We also compare the results of SSMFLK with those of two supervised classi cation methods: Support Vector Machine (SVM) and Naive Bayes." ></td>
	<td class="line x" title="170:191	Both of these methods have been widely used in sentiment analysis." ></td>
	<td class="line x" title="171:191	In particular, the use of SVMs in (Pang et al., 2002) initially sparked interest in using machine learning methods for sentiment classi cation." ></td>
	<td class="line x" title="172:191	Note that none of these competing methods utilizes lexical knowledge." ></td>
	<td class="line x" title="173:191	The results are presented in Figure 5, Figure 6, Figure 7, and Figure 8." ></td>
	<td class="line x" title="174:191	We note that our SSMFLK method either outperforms all other methods over the entire range of number of labeled documents (Movies, Political), or ultimately outpaces other methods (Lotus, Amazon) as a few document labels come in." ></td>
	<td class="line x" title="175:191	Learning Domain-Specific Connotations In our  rst set of experiments, we incorporated the sentiment lexicon in our models and learnt the sentiment orientation of words and documents via F,G factors respectively." ></td>
	<td class="line x" title="176:191	In the second set of 250 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 5: Accuracy results with increasing number of labeled documents on Movies dataset 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.3 0.4 0.5 0.6 0.7 0.8 0.9 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bayes Figure 6: Accuracy results with increasing number of labeled documents on Lotus dataset experiments, we additionally introduced labeled documents for domain-speci c adjustments." ></td>
	<td class="line x" title="177:191	Between these experiments, we can now look for words that switch sentiment polarity." ></td>
	<td class="line x" title="178:191	These words are interesting because their domain-speci c connotation differs from their lexical orientation." ></td>
	<td class="line x" title="179:191	For amazon reviews, the following words switched polarity from positive to negative: fan, important, learning, cons, fast, feature, happy, memory, portable, simple, small, work while the following words switched polarity from negative to positive: address, finish, lack, mean, budget, rent, throw." ></td>
	<td class="line x" title="180:191	Note that words like fan, memory probably refer to product or product components (i.e., computer fan and memory) in the amazon review context but have a very different connotation say in the context of movie reviews where they probably refer to movie fanfare and memorable performances." ></td>
	<td class="line x" title="181:191	We were surprised to see happy switch polarity!" ></td>
	<td class="line x" title="182:191	Two examples of its negative-sentiment usage are: I ended up buying a Samsung and I couldnt be more happy and BORING, not one single exciting thing about this book." ></td>
	<td class="line x" title="183:191	I was happy when my lunch break ended so I could go back to work and stop reading." ></td>
	<td class="line x" title="184:191	0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 7: Accuracy results with increasing number of labeled documents on Political dataset 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 Number of documents labeled as a fraction of the original set of labeled documents Accuracy   SSMFLK Consistency Method HomonicCMN Green Function SVM Naive Bays Figure 8: Accuracy results with increasing number of labeled documents on Amazon dataset 7 Conclusion The primary contribution of this paper is to propose and benchmark new methodologies for sentiment analysis." ></td>
	<td class="line x" title="185:191	Non-negative Matrix Factorizations constitute a rich body of algorithms that have found applicability in a variety of machine learning applications: from recommender systems to document clustering." ></td>
	<td class="line x" title="186:191	We have shown how to build effective sentiment models by appropriately constraining the factors using lexical prior knowledge and document annotations." ></td>
	<td class="line x" title="187:191	To more effectively utilize unlabeled data and induce domain-speci c adaptation of our models, several extensions are possible: facilitating learning from related domains, incorporating hyperlinks between documents, incorporating synonyms or co-occurences between words etc. As a topic of vigorous current activity, there are several very recently proposed competing methodologies for sentiment analysis that we would like to benchmark against." ></td>
	<td class="line x" title="188:191	These are topics for future work." ></td>
	<td class="line x" title="189:191	Acknowledgement: The work of T. Li is partially supported by NSF grants DMS-0844513 and CCF-0830659." ></td>
	<td class="line x" title="190:191	We would also like to thank Prem Melville and Richard Lawrence for their support." ></td>
	<td class="line x" title="191:191	251" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1079
Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification
Dasgupta, Sajib;Ng, Vincent;"></td>
	<td class="line x" title="1:254	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 701709, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:254	c2009 ACL and AFNLP Mine the Easy, Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification Sajib Dasgupta and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson, TX 75083-0688 {sajib,vince}@hlt.utdallas.edu Abstract Supervised polarity classification systems are typically domain-specific." ></td>
	<td class="line x" title="3:254	Building these systems involves the expensive process of annotating a large amount of data for each domain." ></td>
	<td class="line x" title="4:254	A potential solution to this corpus annotation bottleneck is to build unsupervised polarity classification systems." ></td>
	<td class="line x" title="5:254	However, unsupervised learning of polarity is difficult, owing in part to the prevalence of sentimentally ambiguous reviews, where reviewers discuss both the positive and negative aspects of a product." ></td>
	<td class="line x" title="6:254	To address this problem, we propose a semi-supervised approach to sentiment classification where we first mine the unambiguous reviews using spectral techniques and then exploit them to classify the ambiguous reviews via a novel combination of active learning, transductive learning, and ensemble learning." ></td>
	<td class="line x" title="7:254	1 Introduction Sentiment analysis has recently received a lot of attention in the Natural Language Processing (NLP) community." ></td>
	<td class="line x" title="8:254	Polarity classification, whose goal is to determine whether the sentiment expressed in a document is thumbs up or thumbs down, is arguably one of the most popular tasks in document-level sentiment analysis." ></td>
	<td class="line x" title="9:254	Unlike topic-based text classification, where a high accuracy can be achieved even for datasets with a large number of classes (e.g., 20 Newsgroups), polarity classification appears to be a more difficult task." ></td>
	<td class="line x" title="10:254	One reason topic-based text classification is easier than polarity classification is that topic clusters are typically well-separated from each other, resulting from the fact that word usage differs considerably between two topically-different documents." ></td>
	<td class="line x" title="11:254	On the other hand, many reviews are sentimentally ambiguous for a variety of reasons." ></td>
	<td class="line x" title="12:254	For instance, an author of a movie review may have negative opinions of the actors but at the same time talk enthusiastically about how much she enjoyed the plot." ></td>
	<td class="line x" title="13:254	Here, the review is ambiguous because she discussed both the positive and negative aspects of the movie, which is not uncommon in reviews." ></td>
	<td class="line x" title="14:254	As another example, a large portion of a movie review may be devoted exclusively to the plot, with the author only briefly expressing her sentiment at the end of the review." ></td>
	<td class="line x" title="15:254	In this case, the review is ambiguous because the objective material in the review, which bears no sentiment orientation, significantly outnumbers its subjective counterpart." ></td>
	<td class="line x" title="16:254	Realizing the challenges posed by ambiguous reviews, researchers have explored a number of techniques to improve supervised polarity classifiers." ></td>
	<td class="line x" title="17:254	For instance, Pang and Lee (2004) train an independent subjectivity classifier to identify and remove objective sentences from a review prior to polarity classification." ></td>
	<td class="line x" title="18:254	Koppel and Schler (2006) use neutral reviews to help improve the classification of positive and negative reviews." ></td>
	<td class="line x" title="19:254	More recently, McDonald et al.(2007) have investigated a model for jointly performing sentenceand document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited." ></td>
	<td class="line x" title="21:254	However, the increased sophistication of supervised polarity classifiers has also resulted in their increased dependence on annotated data." ></td>
	<td class="line x" title="22:254	For instance, Koppel and Schler needed to manually identify neutral reviews to train their polarity classifier, and McDonald et al.s joint model requires that each sentence in a review be labeled with polarity information." ></td>
	<td class="line x" title="23:254	Given the difficulties of supervised polarity classification, it is conceivable that unsupervised polarity classification is a very challenging task." ></td>
	<td class="line x" title="24:254	Nevertheless, a solution to unsupervised polarity classification is of practical significance." ></td>
	<td class="line x" title="25:254	One reason is that the vast majority of supervised polarity 701 classification systems are domain-specific." ></td>
	<td class="line x" title="26:254	Hence, when given a new domain, a large amount of annotated data from the domain typically needs to be collected in order to train a high-performance polarity classification system." ></td>
	<td class="line x" title="27:254	As Blitzer et al.(2007) point out, this data collection process can be prohibitively expensive, especially since product features can change over time." ></td>
	<td class="line x" title="29:254	Unfortunately, to our knowledge, unsupervised polarity classification is largely an under-investigated task in NLP." ></td>
	<td class="line pc" title="30:254	Turneys (2002) work is perhaps one of the most notable examples of unsupervised polarity classification." ></td>
	<td class="line n" title="31:254	However, while his system learns the semantic orientation of phrases in a review in an unsupervised manner, such information is used to heuristically predict the polarity of a review." ></td>
	<td class="line x" title="32:254	At first glance, it may seem plausible to apply an unsupervised clustering algorithm such as kmeans to cluster the reviews according to their polarity." ></td>
	<td class="line x" title="33:254	However, there is reason to believe that such a clustering approach is doomed to fail: in the absence of annotated data, an unsupervised learner is unable to identify which features are relevant for polarity classification." ></td>
	<td class="line x" title="34:254	The situation is further complicated by the prevalence of ambiguous reviews, which may contain a large amount of irrelevant and/or contradictory information." ></td>
	<td class="line x" title="35:254	In light of the difficulties posed by ambiguous reviews, we differentiate between ambiguous and unambiguous reviews in our classification process by addressing the task of semi-supervised polarity classification via a mine the easy, classify the hard approach." ></td>
	<td class="line x" title="36:254	Specifically, we propose a novel system architecture where we first automatically identify and label the unambiguous (i.e., easy) reviews, then handle the ambiguous (i.e., hard) reviews using a discriminative learner to bootstrap from the automatically labeled unambiguous reviews and a small number of manually labeled reviews that are identified by an active learner." ></td>
	<td class="line x" title="37:254	It is worth noting that our system differs from existing work on unsupervised/active learning in two aspects." ></td>
	<td class="line x" title="38:254	First, while existing unsupervised approaches typically rely on clustering or learning via a generative model, our approach distinguishes between easy and hard instances and exploits the strengths of discriminative models to classify the hard instances." ></td>
	<td class="line x" title="39:254	Second, while existing active learners typically start with manually labeled seeds, our active learner relies only on seeds that are automatically extracted from the data." ></td>
	<td class="line x" title="40:254	Experimental results on five sentiment classification datasets demonstrate that our system can generate high-quality labeled data from unambiguous reviews, which, together with a small number of manually labeled reviews selected by the active learner, can be used to effectively classify ambiguous reviews in a discriminative fashion." ></td>
	<td class="line x" title="41:254	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="42:254	Section 2 gives an overview of spectral clustering, which will facilitate the presentation of our approach to unsupervised sentiment classification in Section 3." ></td>
	<td class="line x" title="43:254	We evaluate our approach in Section 4 and present our conclusions in Section 5." ></td>
	<td class="line x" title="44:254	2 Spectral Clustering In this section, we give an overview of spectral clustering, which is at the core of our algorithm for identifying ambiguous reviews." ></td>
	<td class="line x" title="45:254	2.1 Motivation When given a clustering task, an important question to ask is: which clustering algorithm should be used?" ></td>
	<td class="line x" title="46:254	A popular choice is k-means." ></td>
	<td class="line x" title="47:254	Nevertheless, it is well-known that k-means has the major drawback of not being able to separate data points that are not linearly separable in the given feature space (e.g, see Dhillon et al.(2004))." ></td>
	<td class="line x" title="49:254	Spectral clustering algorithms were developed in response to this problem with k-means clustering." ></td>
	<td class="line x" title="50:254	The central idea behind spectral clustering is to (1) construct a low-dimensional space from the original (typically high-dimensional) space while retaining as much information about the original space as possible, and (2) cluster the data points in this lowdimensional space." ></td>
	<td class="line x" title="51:254	2.2 Algorithm Although there are several well-known spectral clustering algorithms in the literature (e.g., Weiss (1999), Meila and Shi (2001), Kannan et al.(2004)), we adopt the one proposed by Ng et al.(2002), as it is arguably the most widely used." ></td>
	<td class="line x" title="54:254	The algorithm takes as input a similarity matrix S created by applying a user-defined similarity function to each pair of data points." ></td>
	<td class="line x" title="55:254	Below are the main steps of the algorithm: 1." ></td>
	<td class="line x" title="56:254	Create the diagonal matrix G whose (i,i)th entry is the sum of the i-th row of S, and then construct the Laplacian matrix L = G1/2SG1/2." ></td>
	<td class="line x" title="57:254	2. Find the eigenvalues and eigenvectors of L. 702 3." ></td>
	<td class="line x" title="58:254	Create a new matrix from the m eigenvectors that correspond to the m largest eigenvalues.1 4." ></td>
	<td class="line x" title="59:254	Each data point is now rank-reduced to a point in the m-dimensional space." ></td>
	<td class="line x" title="60:254	Normalize each point to unit length (while retaining the sign of each value)." ></td>
	<td class="line x" title="61:254	5." ></td>
	<td class="line x" title="62:254	Cluster the resulting data points using kmeans." ></td>
	<td class="line x" title="63:254	In essence, each dimension in the reduced space is defined by exactly one eigenvector." ></td>
	<td class="line x" title="64:254	The reason why eigenvectors with large eigenvalues are retained is that they capture the largest variance in the data." ></td>
	<td class="line x" title="65:254	Therefore, each of them can be thought of as revealing an important dimension of the data." ></td>
	<td class="line x" title="66:254	3 Our Approach While spectral clustering addresses a major drawback of k-means clustering, it still cannot be expected to accurately partition the reviews due to the presence of ambiguous reviews." ></td>
	<td class="line x" title="67:254	Motivated by this observation, rather than attempting to cluster all the reviews at the same time, we handle them in different stages." ></td>
	<td class="line x" title="68:254	As mentioned in the introduction, we employ a mine the easy, classify the hard approach to polarity classification, where we (1) identify and classify the easy (i.e., unambiguous) reviews with the help of a spectral clustering algorithm; (2) manually label a small number of hard (i.e., ambiguous) reviews selected by an active learner; and (3) using the reviews labeled thus far, apply a transductive learner to label the remaining (ambiguous) reviews." ></td>
	<td class="line x" title="69:254	In this section, we discuss each of these steps in detail." ></td>
	<td class="line x" title="70:254	3.1 Identifying Unambiguous Reviews We begin by preprocessing the reviews to be classified." ></td>
	<td class="line x" title="71:254	Specifically, we tokenize and downcase each review and represent it as a vector of unigrams, using frequency as presence." ></td>
	<td class="line x" title="72:254	In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in a single review only." ></td>
	<td class="line x" title="73:254	Finally, following the common practice in the information retrieval community, we remove words with high document frequency, many of which are stopwords or domainspecific general-purpose words (e.g., movies in the movie domain)." ></td>
	<td class="line x" title="74:254	A preliminary examination of our evaluation datasets reveals that these words 1For brevity, we will refer to the eigenvector with the n-th largest eigenvalue simply as the n-th eigenvector." ></td>
	<td class="line x" title="75:254	typically comprise 12% of a vocabulary." ></td>
	<td class="line x" title="76:254	The decision of exactly how many terms to remove from each dataset is subjective: a large corpus typically requires more removals than a small corpus." ></td>
	<td class="line x" title="77:254	To be consistent, we simply sort the vocabulary by document frequency and remove the top 1.5%." ></td>
	<td class="line x" title="78:254	Recall that in this step we use spectral clustering to identify unambiguous reviews." ></td>
	<td class="line x" title="79:254	To make use of spectral clustering, we first create a similarity matrix, defining the similarity between two reviews as the dot product of their feature vectors, but following Ng et al.(2002), we set its diagonal entries to 0." ></td>
	<td class="line x" title="81:254	We then perform an eigen-decomposition of this matrix, as described in Section 2.2." ></td>
	<td class="line x" title="82:254	Finally, using the resulting eigenvectors, we partition the length-normalized reviews into two sets." ></td>
	<td class="line x" title="83:254	As Ng et al. point out, different authors still disagree on which eigenvectors to use, and how to derive clusters from them." ></td>
	<td class="line x" title="84:254	To create two clusters, the most common way is to use only the second eigenvector, as Shi and Malik (2000) proved that this eigenvector induces an intuitively ideal partition of the data  the partition induced by the minimum normalized cut of the similarity graph2, where the nodes are the data points and the edge weights are the pairwise similarity values of the points." ></td>
	<td class="line x" title="85:254	Clustering in a one-dimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points." ></td>
	<td class="line x" title="86:254	A common approach is to set the threshold to zero." ></td>
	<td class="line x" title="87:254	In other words, all points whose value in the second eigenvector is positive are classified as positive, and the remaining points are classified as negative." ></td>
	<td class="line x" title="88:254	However, we found that the second eigenvector does not always induce a partition of the nodes that corresponds to the minimum normalized cut." ></td>
	<td class="line x" title="89:254	One possible reason is that Shi and Maliks proof assumes the use of a Laplacian matrix that is different from the one used by Ng et al. To address this problem, we use the first five eigenvectors: for each eigenvector, we (1) use each of its n elements as a threshold to independently generate n partitions, (2) compute the normalized cut value for each partition, and (3) find the minimum of the n cut values." ></td>
	<td class="line x" title="90:254	We then select the eigenvector that corresponds to the smallest of the five minimum cut values." ></td>
	<td class="line x" title="91:254	Next, we identify the ambiguous reviews from 2Using the normalized cut (as opposed to the usual cut) ensures that the size of the two clusters are relatively balanced, avoiding trivial cuts where one cluster is empty and the other is full." ></td>
	<td class="line x" title="92:254	See Shi and Malik (2000) for details." ></td>
	<td class="line x" title="93:254	703 the resulting partition." ></td>
	<td class="line x" title="94:254	To see how this is done, consider the example in Figure 1, where the goal is to produce two clusters from five data points." ></td>
	<td class="line x" title="95:254	parenleftBigg1 1 1 0 0 1 1 1 0 00 0 1 1 0 0 0 0 1 10 0 0 1 1 parenrightBigg parenleftBigg0.6983 0.7158 0.6983 0.7158 0.9869 0.1616 0.6224 0.7827 0.6224 0.7827 parenrightBigg Figure 1: Sample data and the top two eigenvectors of its Laplacian In the matrix on the left, each row is the feature vector generated for Di, the i-th data point." ></td>
	<td class="line x" title="96:254	By inspection, one can identify two clusters, {D1,D2} and {D4,D5}." ></td>
	<td class="line x" title="97:254	D3 is ambiguous, as it bears resemblance to the points in both clusters and therefore can be assigned to any of them." ></td>
	<td class="line x" title="98:254	In the matrix on the right, the two columns correspond to the top two eigenvectors obtained via an eigendecomposition of the Laplacian matrix formed from the five data points." ></td>
	<td class="line x" title="99:254	As we can see, the second eigenvector gives us a natural cluster assignment: all the points whose corresponding values in the second eigenvector are strongly positive will be in one cluster, and the strongly negative points will be in another cluster." ></td>
	<td class="line x" title="100:254	Being ambiguous, D3 is weakly negative and will be assigned to the negative cluster." ></td>
	<td class="line x" title="101:254	Before describing our algorithm for identifying ambiguous data points, we make two additional observations regarding D3." ></td>
	<td class="line x" title="102:254	First, if we removed D3, we could easily cluster the remaining (unambiguous) points, since the similarity graph becomes more disconnected as we remove more ambiguous data points." ></td>
	<td class="line x" title="103:254	The question then is: why is it important to produce a good clustering of the unambiguous points?" ></td>
	<td class="line x" title="104:254	Recall that the goal of this step is not only to identify the unambiguous reviews, but also to annotate them as POSITIVE or NEGATIVE, so that they can serve as seeds for semi-supervised learning in a later step." ></td>
	<td class="line x" title="105:254	If we have a good 2-way clustering of the seeds, we can simply annotate each cluster (by sampling a handful of its reviews) rather than each seed." ></td>
	<td class="line x" title="106:254	To reiterate, removing the ambiguous data points can help produce a good clustering of their unambiguous counterparts." ></td>
	<td class="line x" title="107:254	Second, as an ambiguous data point, D3 can in principle be assigned to any of the two clusters." ></td>
	<td class="line x" title="108:254	According to the second eigenvector, it should be assigned to the negative cluster; but if feature #4 were irrelevant, it should be assigned to the positive cluster." ></td>
	<td class="line x" title="109:254	In other words, the ability to determine the relevance of each feature is crucial to the accurate clustering of the ambiguous data points." ></td>
	<td class="line x" title="110:254	However, in the absence of labeled data, it is not easy to assess feature relevance." ></td>
	<td class="line x" title="111:254	Even if labeled data were present, the ambiguous points might be better handled by a discriminative learning system than a clustering algorithm, as discriminative learners are more sophisticated, and can handle ambiguous feature space more effectively." ></td>
	<td class="line x" title="112:254	Taking into account these two observations, we aim to (1) remove the ambiguous data points while clustering their unambiguous counterparts, and then (2) employ a discriminative learner to label the ambiguous points in a later step." ></td>
	<td class="line x" title="113:254	The question is: how can we identify the ambiguous data points?" ></td>
	<td class="line x" title="114:254	To do this, we exploit an important observation regarding eigendecomposition." ></td>
	<td class="line x" title="115:254	In the computation of eigenvalues, each data point factors out the orthogonal projections of each of the other data points with which they have an affinity." ></td>
	<td class="line x" title="116:254	Ambiguous data points receive the orthogonal projections from both the positive and negative data points, and hence they have near-zero values in the pivot eigenvectors." ></td>
	<td class="line x" title="117:254	Given this observation, our algorithm uses the eight steps below to remove the ambiguous points in an iterative fashion and produce a clustering of the unambiguous points." ></td>
	<td class="line x" title="118:254	1." ></td>
	<td class="line x" title="119:254	Create a similarity matrix S from the data points D. 2." ></td>
	<td class="line x" title="120:254	Form the Laplacian matrix L from S. 3." ></td>
	<td class="line x" title="121:254	Find the top five eigenvectors of L. 4." ></td>
	<td class="line x" title="122:254	Row-normalize the five eigenvectors." ></td>
	<td class="line x" title="123:254	5." ></td>
	<td class="line x" title="124:254	Pick the eigenvector e for which we get the minimum normalized cut." ></td>
	<td class="line x" title="125:254	6." ></td>
	<td class="line x" title="126:254	Sort D according to e and remove  points in the middle of D (i.e., the points indexed from |D| 2   2 +1 to |D| 2 +  2 ).7." ></td>
	<td class="line x" title="127:254	If |D| = , goto Step 8; else goto Step 1." ></td>
	<td class="line x" title="128:254	8. Run 2-means on e to cluster the points in D. This algorithm can be thought of as the opposite of self-training." ></td>
	<td class="line x" title="129:254	In self-training, we iteratively train a classifier on the data labeled so far, use it to classify the unlabeled instances, and augment the labeled data with the most confidently labeled instances." ></td>
	<td class="line x" title="130:254	In our algorithm, we start with an initial clustering of all of the data points, and then iteratively remove the  most ambiguous points from the dataset and cluster the remaining points." ></td>
	<td class="line x" title="131:254	Given this analogy, it should not be difficult to see the advantage of removing the data points in an iterative fashion (as opposed to removing them in a 704 single iteration): the clusters produced in a given iteration are supposed to be better than those in the previous iterations, as subsequent clusterings are generated from less ambiguous points." ></td>
	<td class="line x" title="132:254	In our experiments, we set  to 50 and  to 500.3 Finally, we label the two clusters." ></td>
	<td class="line x" title="133:254	To do this, we first randomly sample 10 reviews from each cluster and manually label each of them as POSITIVE or NEGATIVE." ></td>
	<td class="line x" title="134:254	Then, we label a cluster as POSITIVE if more than half of the 10 reviews from the cluster are POSITIVE; otherwise, it is labeled as NEGATIVE." ></td>
	<td class="line x" title="135:254	For each of our evaluation datasets, this labeling scheme always produces one POSITIVE cluster and one NEGATIVE cluster." ></td>
	<td class="line x" title="136:254	In the rest of the paper, we will refer to these 500 automatically labeled reviews as seeds." ></td>
	<td class="line x" title="137:254	A natural question is: can this algorithm produce high-quality seeds?" ></td>
	<td class="line x" title="138:254	To answer this question, we show in the middle column of Table 1 the labeling accuracy of the 500 reviews produced by our iterative algorithm for our five evaluation datasets (see Section 4.1 for details on these datasets)." ></td>
	<td class="line x" title="139:254	To better understand whether it is indeed beneficial to remove the ambiguous points in an iterative fashion, we also show the results of a version of this algorithm in which we remove all but the 500 least ambiguous points in just one iteration (see the rightmost column)." ></td>
	<td class="line x" title="140:254	As we can see, for three datasets (Movie, Kitchen, and Electronics), the accuracy is above 80%." ></td>
	<td class="line x" title="141:254	For the remaining two (Book and DVD), the accuracy is not particularly good." ></td>
	<td class="line x" title="142:254	One plausible reason is that the ambiguous reviews in Book and DVD are relatively tougher to identify." ></td>
	<td class="line x" title="143:254	Another reason can be attributed to the failure of the chosen eigenvector to capture the sentiment dimension." ></td>
	<td class="line x" title="144:254	Recall that each eigenvector captures an important dimension of the data, and if the eigenvector that corresponds to the minimum normalized cut (i.e., the eigenvector that we chose) does not reveal the sentiment dimension, the resulting clustering (and hence the seed accuracy) will be poor." ></td>
	<td class="line x" title="145:254	However, even with imperfectly labeled seeds, we will show in the next section how we exploit these seeds to learn a better classifier." ></td>
	<td class="line x" title="146:254	3.2 Incorporating Active Learning Spectral clustering allows us to focus on a small number of dimensions that are relevant as far as creating well-separated clusters is concerned, but 3Additional experiments indicate that the accuracy of our approach is not sensitive to small changes to these values." ></td>
	<td class="line x" title="147:254	Dataset Iterative Single Step Movie 89.3 86.5 Kitchen 87.9 87.1 Electronics 80.4 77.6 Book 68.5 70.3 DVD 66.3 65.4 Table 1: Seed accuracies on five datasets." ></td>
	<td class="line x" title="148:254	they are not necessarily relevant for creating polarity clusters." ></td>
	<td class="line x" title="149:254	In fact, owing to the absence of labeled data, unsupervised clustering algorithms are unable to distinguish between useful and irrelevant features for polarity classification." ></td>
	<td class="line x" title="150:254	Nevertheless, being able to distinguish between relevant and irrelevant information is important for polarity classification, as discussed before." ></td>
	<td class="line x" title="151:254	Now that we have a small, high-quality seed set, we can potentially make better use of the available features by training a discriminative classifier on the seed set and having it identify the relevant and irrelevant features for polarity classification." ></td>
	<td class="line x" title="152:254	Despite the high quality of the seed set, the resulting classifier may not perform well when applied to the remaining (unlabeled) points, as there is no reason to believe that a classifier trained solely on unambiguous reviews can achieve a high accuracy when classifying ambiguous reviews." ></td>
	<td class="line x" title="153:254	We hypothesize that a high accuracy can be achieved only if the classifier is trained on both ambiguous and unambiguous reviews." ></td>
	<td class="line x" title="154:254	As a result, we apply active learning (Cohn et al., 1994) to identify the ambiguous reviews." ></td>
	<td class="line x" title="155:254	Specifically, we train a discriminative classifier using the support vector machine (SVM) learning algorithm (Joachims, 1999) on the set of unambiguous reviews, and then apply the resulting classifier to all the reviews in the training folds4 that are not seeds." ></td>
	<td class="line x" title="156:254	Since this classifier is trained solely on the unambiguous reviews, it is reasonable to assume that the reviews whose labels the classifier is most uncertain about (and therefore are most informative to the classifier) are those that are ambiguous." ></td>
	<td class="line x" title="157:254	Following previous work on active learning for SVMs (e.g., Campbell et al.(2000), Schohn and Cohn (2000), Tong and Koller (2002)), we define the uncertainty of a data point as its distance from the separating hyperplane." ></td>
	<td class="line x" title="159:254	In other words, 4Following Dredze and Crammer (2008), we perform cross-validation experiments on the 2000 labeled reviews in each evaluation dataset, choosing the active learning points from the training folds." ></td>
	<td class="line x" title="160:254	Note that the seeds obtained in the previous step were also acquired using the training folds only." ></td>
	<td class="line x" title="161:254	705 points that are closer to the hyperplane are more uncertain than those that are farther away." ></td>
	<td class="line x" title="162:254	We perform active learning for five iterations." ></td>
	<td class="line x" title="163:254	In each iteration, we select the 10 most uncertain points from each side of the hyperplane for human annotation, and then re-train a classifier on all of the points annotated so far." ></td>
	<td class="line x" title="164:254	This yields a total of 100 manually labeled reviews." ></td>
	<td class="line x" title="165:254	3.3 Applying Transductive Learning Given that we now have a labeled set (composed of 100 manually labeled points selected by active learning and 500 unambiguous points) as well as a larger set of points that are yet to be labeled (i.e., the remaining unlabeled points in the training folds and those in the test fold), we aim to train a better classifier by using a weakly supervised learner to learn from both the labeled and unlabeled data." ></td>
	<td class="line x" title="166:254	As our weakly supervised learner, we employ a transductive SVM." ></td>
	<td class="line x" title="167:254	To begin with, note that the automatically acquired 500 unambiguous data points are not perfectly labeled (see Section 3.1)." ></td>
	<td class="line x" title="168:254	Since these unambiguous points significantly outnumber the manually labeled points, they could undesirably dominate the acquisition of the hyperplane and diminish the benefits that we could have obtained from the more informative and perfectly labeled active learning points otherwise." ></td>
	<td class="line x" title="169:254	We desire a system that can use the active learning points effectively and at the same time is noise-tolerant to the imperfectly labeled unambiguous data points." ></td>
	<td class="line x" title="170:254	Hence, instead of training just one SVM classifier, we aim to reduce classification errors by training an ensemble of five classifiers, each of which uses all 100 manually labeled reviews and a different subset of the 500 automatically labeled reviews." ></td>
	<td class="line x" title="171:254	Specifically, we partition the 500 automatically labeled reviews into five equal-sized sets as follows." ></td>
	<td class="line x" title="172:254	First, we sort the 500 reviews in ascending order of their corresponding values in the eigenvector selected in the last iteration of our algorithm for removing ambiguous points (see Section 3.1)." ></td>
	<td class="line x" title="173:254	We then put point i into set Li mod 5." ></td>
	<td class="line x" title="174:254	This ensures that each set consists of not only an equal number of positive and negative points, but also a mix of very confidently labeled points and comparatively less confidently labeled points." ></td>
	<td class="line x" title="175:254	Each classifier Ci will then be trained transductively, using the 100 manually labeled points and the points in Li as labeled data, and the remaining points (including all points in Lj, where i negationslash= j) as unlabeled data." ></td>
	<td class="line x" title="176:254	After training the ensemble, we classify each unlabeled point as follows: we sum the (signed) confidence values assigned to it by the five ensemble classifiers, labeling it as POSITIVE if the sum is greater than zero (and NEGATIVE otherwise)." ></td>
	<td class="line x" title="177:254	Since the points in the test fold are included in the unlabeled data, they are all classified in this step." ></td>
	<td class="line x" title="178:254	4 Evaluation 4.1 Experimental Setup For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al., 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al., 2007)." ></td>
	<td class="line x" title="179:254	Each dataset has 2000 labeled reviews (1000 positives and 1000 negatives)." ></td>
	<td class="line x" title="180:254	We divide the 2000 reviews into 10 equal-sized folds for cross-validation purposes, maintaining balanced class distributions in each fold." ></td>
	<td class="line x" title="181:254	It is important to note that while the test fold is accessible to the transductive learner (Step 3), only the reviews in training folds (but not their labels) are used for the acquisition of seeds (Step 1) and the selection of active learning points (Step 2)." ></td>
	<td class="line x" title="182:254	We report averaged 10-fold cross-validation results in terms of accuracy." ></td>
	<td class="line x" title="183:254	Following Kamvar et al.(2003), we also evaluate the clusters produced by our approach against the gold-standard clusters using Adjusted Rand Index (ARI)." ></td>
	<td class="line x" title="185:254	ARI ranges from 1 to 1; better clusterings have higher ARI values." ></td>
	<td class="line x" title="186:254	4.2 Baseline Systems Recall that our approach uses 100 hand-labeled reviews chosen by active learning." ></td>
	<td class="line x" title="187:254	To ensure a fair comparison, each of our three baselines has access to 100 labeled points chosen from the training folds." ></td>
	<td class="line x" title="188:254	Owing to the randomness involved in the choice of labeled data, all baseline results are averaged over ten independent runs for each fold." ></td>
	<td class="line x" title="189:254	Semi-supervised spectral clustering." ></td>
	<td class="line x" title="190:254	We implemented Kamvar et al.s (2003) semi-supervised spectral clustering algorithm, which incorporates labeled data into the clustering framework in the form of must-link and cannot-link constraints." ></td>
	<td class="line x" title="191:254	Instead of computing the similarity between each pair of points, the algorithm computes the similarity between a point and its k most similar points only." ></td>
	<td class="line x" title="192:254	Since its performance is highly sensitive to 706 Accuracy Adjusted Rand Index System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD 1 Semi-supervised spectral learning 67.3 63.7 57.7 55.8 56.2 0.12 0.08 0.01 0.02 0.02 2 Transductive SVM 68.7 65.5 62.9 58.7 57.3 0.14 0.09 0.07 0.03 0.02 3 Active learning 68.9 68.1 63.3 58.6 58.0 0.14 0.14 0.08 0.03 0.03 4 Our approach (after 1st step) 69.8 70.8 65.7 58.6 55.8 0.15 0.17 0.10 0.03 0.01 5 Our approach (after 2nd step) 73.5 73.0 69.9 60.6 59.8 0.22 0.21 0.16 0.04 0.04 6 Our approach (after 3rd step) 76.2 74.1 70.6 62.1 62.7 0.27 0.23 0.17 0.06 0.06 Table 2: Results in terms of accuracy and Adjusted Rand Index for the five datasets." ></td>
	<td class="line x" title="193:254	k, we tested values of 10, 15, , 50 for k and reported in row 1 of Table 2 the best results." ></td>
	<td class="line x" title="194:254	As we can see, accuracy ranges from 56.2% to 67.3%, whereas ARI ranges from 0.02 to 0.12." ></td>
	<td class="line x" title="195:254	Transductive SVM." ></td>
	<td class="line x" title="196:254	We employ as our second baseline a transductive SVM5 trained using 100 points randomly sampled from the training folds as labeled data and the remaining 1900 points as unlabeled data." ></td>
	<td class="line x" title="197:254	Results of this baseline are shown in row 2 of Table 3." ></td>
	<td class="line x" title="198:254	As we can see, accuracy ranges from 57.3% to 68.7% and ARI ranges from 0.02 to 0.14, which are significantly better than those of semi-supervised spectral learning." ></td>
	<td class="line x" title="199:254	Active learning." ></td>
	<td class="line x" title="200:254	Our last baseline implements the active learning procedure as described in Tong and Koller (2002)." ></td>
	<td class="line x" title="201:254	Specifically, we begin by training an inductive SVM on one labeled example from each class, iteratively labeling the most uncertain unlabeled point on each side of the hyperplane and re-training the SVM until 100 points are labeled." ></td>
	<td class="line x" title="202:254	Finally, we train a transductive SVM on the 100 labeled points and the remaining 1900 unlabeled points, obtaining the results in row 3 of Table 1." ></td>
	<td class="line x" title="203:254	As we can see, accuracy ranges from 58% to 68.9%, whereas ARI ranges from 0.03 to 0.14." ></td>
	<td class="line x" title="204:254	Active learning is the best of the three baselines, presumably because it has the ability to choose the labeled data more intelligently than the other two." ></td>
	<td class="line x" title="205:254	4.3 Our Approach Results of our approach are shown in rows 46 of Table 2." ></td>
	<td class="line x" title="206:254	Specifically, rows 4 and 5 show the results of the SVM classifier when it is trained on the labeled data obtained after the first step (unsupervised extraction of unambiguous reviews) and the second step (active learning), respectively." ></td>
	<td class="line x" title="207:254	After the first step, our approach can already achieve 5All the SVM classifiers in this paper are trained using the SVMlight package (Joachims, 1999)." ></td>
	<td class="line x" title="208:254	All SVM-related learning parameters are set to their default values, except in transductive learning, where we set p (the fraction of unlabeled examples to be classified as positive) to 0.5 so that the system does not have any bias towards any class." ></td>
	<td class="line x" title="209:254	comparable results to the best baseline." ></td>
	<td class="line x" title="210:254	Performance increases substantially after the second step, indicating the benefits of active learning." ></td>
	<td class="line x" title="211:254	Row 6 shows the results of transductive learning with ensemble." ></td>
	<td class="line x" title="212:254	Comparing rows 5 and 6, we see that performance rises by 0.7%-2.9% for all five datasets after ensembled transduction." ></td>
	<td class="line x" title="213:254	This could be attributed to (1) the unlabeled data, which may have provided the transductive learner with useful information that are not accessible to the other learners, and (2) the ensemble, which is more noise-tolerant to the imperfect seeds." ></td>
	<td class="line x" title="214:254	4.4 Additional Experiments To gain insight into how the design decisions we made in our approach impact performance, we conducted the following additional experiments." ></td>
	<td class="line x" title="215:254	Importance of seeds." ></td>
	<td class="line x" title="216:254	Table 1 showed that for all but one dataset, the seeds obtained through multiple iterations are more accurate than those obtained in a single iteration." ></td>
	<td class="line x" title="217:254	To envisage the importance of seeds, we conducted an experiment where we repeated our approach using the seeds learned in a single iteration." ></td>
	<td class="line x" title="218:254	Results are shown in the first row of Table 3." ></td>
	<td class="line x" title="219:254	In comparison to row 6 of Table 2, we can see that results are indeed better when we bootstrap from higher-quality seeds." ></td>
	<td class="line x" title="220:254	To further understand the role of seeds, we experimented with a version of our approach that bootstraps from no seeds." ></td>
	<td class="line x" title="221:254	Specifically, we used the 500 seeds to guide the selection of active learning points, but trained a transductive SVM using only the active learning points as labeled data (and the rest as unlabeled data)." ></td>
	<td class="line x" title="222:254	As can be seen in row 2 of Table 3, the results are poor, suggesting that our approach yields better performance than the baselines not only because of the way the active learning points were chosen, but also because of contributions from the imperfectly labeled seeds." ></td>
	<td class="line x" title="223:254	We also experimented with training a transductive SVM using only the 100 least ambiguous seeds (i.e., the points with the largest unsigned 707 Accuracy Adjusted Rand Index System Variation MOV KIT ELE BOO DVD MOV KIT ELE BOO DVD 1 Single-step cluster purification 74.9 72.7 70.1 66.9 60.7 0.25 0.21 0.16 0.11 0.05 2 Using no seeds 58.3 55.6 59.7 54.0 56.1 0.04 0.04 0.02 0.01 0.01 3 Using the least ambiguous seeds 74.6 69.7 69.1 60.9 63.3 0.24 0.16 0.14 0.05 0.07 4 No Ensemble 74.1 72.7 68.8 61.5 59.9 0.23 0.21 0.14 0.05 0.04 5 Passive learning 74.1 72.4 68.0 63.7 58.6 0.23 0.20 0.13 0.07 0.03 6 Using 500 active learning points 82.5 78.4 77.5 73.5 73.4 0.42 0.32 0.30 0.22 0.22 7 Fully supervised results 86.1 81.7 79.3 77.6 80.6 0.53 0.41 0.34 0.30 0.38 Table 3: Additional results in terms of accuracy and Adjusted Rand Index for the five datasets." ></td>
	<td class="line x" title="224:254	second eigenvector values) in combination with the active learning points as labeled data (and the rest as unlabeled data)." ></td>
	<td class="line x" title="225:254	Note that the accuracy of these 100 least ambiguous seeds is 45% higher than that of the 500 least ambiguous seeds shown in Table 1." ></td>
	<td class="line x" title="226:254	Results are shown in row 3 of Table 3." ></td>
	<td class="line x" title="227:254	As we can see, using only 100 seeds turns out to be less beneficial than using all of them via an ensemble." ></td>
	<td class="line x" title="228:254	One reason is that since these 100 seeds are the most unambiguous, they may also be the least informative as far as learning is concerned." ></td>
	<td class="line x" title="229:254	Remember that SVM uses only the support vectors to acquire the hyperplane, and since an unambiguous seed is likely to be far away from the hyperplane, it is less likely to be a support vector." ></td>
	<td class="line x" title="230:254	Role of ensemble learning To get a better idea of the role of the ensemble in the transductive learning step, we used all 500 seeds in combination with the 100 active learning points to train a single transductive SVM." ></td>
	<td class="line x" title="231:254	Results of this experiment (shown in row 4 of Table 3) are worse than those in row 6 of Table 2, meaning that the ensemble has contributed positively to performance." ></td>
	<td class="line x" title="232:254	This should not be surprising: as noted before, since the seeds are not perfectly labeled, using all of them without an ensemble might overwhelm the more informative active learning points." ></td>
	<td class="line x" title="233:254	Passive learning." ></td>
	<td class="line x" title="234:254	To better understand the role of active learning in our approach, we replaced it with passive learning, where we randomly picked 100 data points from the training folds and used them as labeled data." ></td>
	<td class="line x" title="235:254	Results, shown in row 5 of Table 3, are averaged over ten independent runs for each fold." ></td>
	<td class="line x" title="236:254	In comparison to row 6 of Table 2, we see that employing points chosen by an active learner yields significantly better results than employing randomly chosen points, which suggests that the way the points are chosen is important." ></td>
	<td class="line x" title="237:254	Using more active learning points." ></td>
	<td class="line x" title="238:254	An interesting question is: how much improvement can we obtain if we employ more active learning points?" ></td>
	<td class="line x" title="239:254	In row 6 of Table 3, we show the results when the experiment in row 6 of Table 2 was repeated using 500 active learning points." ></td>
	<td class="line x" title="240:254	Perhaps not surprisingly, the 400 additional labeled points yield a 4 11% increase in accuracy." ></td>
	<td class="line x" title="241:254	For further comparison, we trained a fully supervised SVM classifier using all of the training data." ></td>
	<td class="line x" title="242:254	Results are shown in row 7 of Table 3." ></td>
	<td class="line x" title="243:254	As we can see, employing only 500 active learning points enables us to almost reach fully-supervised performance for three datasets." ></td>
	<td class="line x" title="244:254	5 Conclusions We have proposed a novel semi-supervised approach to polarity classification." ></td>
	<td class="line x" title="245:254	Our key idea is to distinguish between unambiguous, easy-tomine reviews and ambiguous, hard-to-classify reviews." ></td>
	<td class="line x" title="246:254	Specifically, given a set of reviews, we applied (1) an unsupervised algorithm to identify and classify those that are unambiguous, (2) an active learner that is trained solely on automatically labeled unambiguous reviews to identify a small number of prototypical ambiguous reviews for manual labeling, and (3) an ensembled transductive learner to train a sophisticated classifier on the reviews labeled so far to handle the ambiguous reviews." ></td>
	<td class="line x" title="247:254	Experimental results on five sentiment datasets demonstrate that our mine the easy, classify the hard approach, which only requires manual labeling of a small number of ambiguous reviews, can be employed to train a highperformance polarity classification system." ></td>
	<td class="line x" title="248:254	We plan to extend our approach by exploring two of its appealing features." ></td>
	<td class="line x" title="249:254	First, none of the steps in our approach is designed specifically for sentiment classification." ></td>
	<td class="line x" title="250:254	This makes it applicable to other text classification tasks." ></td>
	<td class="line x" title="251:254	Second, our approach is easily extensible." ></td>
	<td class="line x" title="252:254	Since the semisupervised learner is discriminative, our approach can adopt a richer representation that makes use of more sophisticated features such as bigrams or manually labeled sentiment-oriented words." ></td>
	<td class="line x" title="253:254	708 Acknowledgments We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper." ></td>
	<td class="line x" title="254:254	This work was supported in part by NSF Grant IIS-0812261." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2041
Automatic Satire Detection: Are You Having a Laugh?
Burfoot, Clint;Baldwin, Timothy;"></td>
	<td class="line x" title="1:79	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 161164, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:79	c 2009 ACL and AFNLP Automatic Satire Detection: Are You Having a Laugh?" ></td>
	<td class="line x" title="3:79	Clint Burfoot CSSE University of Melbourne VIC 3010 Australia cburfoot@csse.unimelb.edu.au Timothy Baldwin CSSE University of Melbourne VIC 3010 Australia tim@csse.unimelb.edu.au Abstract We introduce the novel task of determining whether a newswire article is true or satirical." ></td>
	<td class="line x" title="4:79	We experiment with SVMs, feature scaling, and a number of lexical and semantic feature types, and achieve promising results over the task." ></td>
	<td class="line x" title="5:79	1 Introduction Thispaperdescribesamethodforlteringsatirical news articles from true newswire documents." ></td>
	<td class="line x" title="6:79	We dene a satirical article as one which deliberately exposes real-world individuals, organisations and events to ridicule." ></td>
	<td class="line x" title="7:79	Satirical news articles tend to mimic true newswire articles, incorporating irony and non sequitur in an attempt to provide humorous insight." ></td>
	<td class="line x" title="8:79	An example excerpt is: Bank Of England Governor Mervyn King is a Queen, Says Fed Chairman Ben Bernanke During last nights appearance on the American David Letterman Show, Fed Chairman Ben Bernanke let slip that Bank of England (BOE) Governor, Mervyn King, enjoys wearing womens clothing." ></td>
	<td class="line x" title="9:79	Contrast this with a snippet of a true newswire article: Delegates prepare for Cairo conference amid tight security Delegates from 156 countries began preparatory talks here Saturday ahead of the ofcial opening of the UN World Population Conference amid tight security." ></td>
	<td class="line x" title="10:79	The basis for our claim that the rst document is satiricalissurprisinglysubtleinnature, andrelates to the absurdity of the suggestion that a prominent gure would expose another prominent gure as a cross dresser, the implausibility of this story appearing in a reputable news source, and the pun on the name (King being a Queen)." ></td>
	<td class="line x" title="11:79	Satire classication is a novel task to computational linguistics." ></td>
	<td class="line x" title="12:79	It is somewhat similar to the more widely-researched text classication tasks of spam ltering (Androutsopoulos et al., 2000) and sentiment classication (Pang and Lee, 2008), in that: (a) it is a binary classication task, and (b) it is an intrinsically semantic task, i.e. satire news articles are recognisable as such through interpretation and cross-comparison to world knowledge about the entities involved." ></td>
	<td class="line x" title="13:79	Similarly to spam ltering and sentiment classication, a key question asked in this research is whether it is possible to perform the task on the basis of simple lexical features of various types." ></td>
	<td class="line x" title="14:79	That is, is it possible to automatically detect satire without access to the complex inferencing and real-world knowledge that humans make use of." ></td>
	<td class="line x" title="15:79	Theprimarycontributionsofthisresearchareas follows: (1) we introduce a novel task to the arena ofcomputationallinguisticsandmachinelearning, and make available a standardised dataset for research on satire detection; and (2) we develop a method which is adept at identifying satire based on simple bag-of-words features, and further extend it to include richer features." ></td>
	<td class="line x" title="16:79	2 Corpus Our satire corpus consists of a total of 4000 newswire documents and 233 satire news articles, split into xed training and test sets as detailed in Table 1." ></td>
	<td class="line x" title="17:79	The newswire documents were randomly sampled from the English Gigaword Corpus." ></td>
	<td class="line x" title="18:79	The satire documents were selected to relate closely to at least one of the newswire documents by: (1) randomly selecting a newswire document; (2) hand-picking a key individual, institution or event from the selected document, and using it to formulate a phrasal query (e.g. Bill Clinton); (3) using the query to issue a site-restricted query to the 161 Training Test Total TRUE 2505 1495 4000 SATIRE 133 100 233 Table 1: Corpus statistics Google search engine;1 and (4) manually ltering out non-newsy, irrelevant and overly-offensive documents from the top-10 returned documents (i.e. documents not containing satire news articles, or containing satire articles which were not relevant to the original query)." ></td>
	<td class="line x" title="19:79	All newswire and satire documents were then converted to plain text of consistent format using lynx, and all content other than the title and body of the article was manually removed (including web page menus, andheaderandfooterdata)." ></td>
	<td class="line x" title="20:79	Finally,alldocuments were manually post-edited to remove references to the source (e.g. AP or Onion), formatting quirks specic to a particular source (e.g. all caps in the title), and any textual metadata which was indicative of the document source (e.g. editorial notes, dates and locations)." ></td>
	<td class="line x" title="21:79	This was all in an effort to prevent classiers from accessing supercial features which are reliable indicators of the document sourceandhencetrivialisethesatiredetectionprocess." ></td>
	<td class="line x" title="22:79	It is important to note that the number of satirical news articles in the corpus is signicantly less than the number of true newswire articles." ></td>
	<td class="line x" title="23:79	This reects an impressionistic view of the web: there is far more true news content than satirical news content." ></td>
	<td class="line x" title="24:79	The corpus is novel to this research, and is publicly available for download at http://www.csse.unimelb.edu.au/ research/lt/resources/satire/." ></td>
	<td class="line oc" title="25:79	3 Method 3.1 Standard text classication approach We take our starting point from topic-based text classication (Dumais et al., 1998; Joachims, 1998) and sentiment classication (Turney, 2002; Pang and Lee, 2008)." ></td>
	<td class="line x" title="26:79	State-of-the-art results in both elds have been achieved using support vec1The sites queried were satirewire.com, theonion.com, newsgroper.com, thespoof." ></td>
	<td class="line x" title="27:79	com, brokennewz.com, thetoque.com, bbspot.com, neowhig.org, humorfeed.com, satiricalmuslim.com, yunews.com, newsbiscuit.com." ></td>
	<td class="line x" title="28:79	tor machines (SVMs) and bag-of-words features." ></td>
	<td class="line x" title="29:79	We supplement the bag-of-words model with feature weighting, using the two methods described below." ></td>
	<td class="line x" title="30:79	Binary feature weights: Under this scheme all features are given the same weight, regardless of how many times they appear in each article." ></td>
	<td class="line x" title="31:79	The topic and sentiment classication examples cited found binary features gave better performance than other alternatives." ></td>
	<td class="line x" title="32:79	Bi-normal separation feature scaling: BNS (Forman, 2008) has been shown to outperform other established feature representation schemes on a wide range of text classication tasks." ></td>
	<td class="line x" title="33:79	This superiority is especially pronounced for collections with a low proportion of positive class instances." ></td>
	<td class="line x" title="34:79	Under BNS, features are allocated a weight according to the formula: |F1(tpr)F1(fpr)| where F1 is the inverse normal cumulative distribution function, tpr is the true positive rate (P(feature|positive class)) and fpr is the false positive rate (P(feature|negative class))." ></td>
	<td class="line x" title="35:79	BNS produces the highest weights for features that are strongly correlated with either the negative or positive class." ></td>
	<td class="line x" title="36:79	Features that occur evenly across the training instances are given the lowest weight." ></td>
	<td class="line x" title="37:79	This behaviour is particularly helpful for features that correlate with the negative class in a negatively-skewed classication task, so in our caseBNSshouldassisttheclassierinmakinguse of features that identify true articles." ></td>
	<td class="line x" title="38:79	SVM classication is performed with SVMlight (Joachims, 1999) using a linear kernel and the default parameter settings." ></td>
	<td class="line x" title="39:79	Tokens are case folded; currency amounts (e.g. $2.50), abbreviations (e.g. U.S.A.), and punctuation sequences (e.g. a comma, or a closing quote mark followed by a period) are treated as separate features." ></td>
	<td class="line x" title="40:79	3.2 Targeted lexical features This section describe three types of features intended to embody characteristics of satire news documents." ></td>
	<td class="line x" title="41:79	Headline features: Most of the articles in the corpus have a headline as their rst line." ></td>
	<td class="line x" title="42:79	To a human reader, the vast majority of the satire documents in our corpus are immediately recognisable as such from the headline alone, suggesting that ourclassiersmaygetsomethingoutofhavingthe 162 headline contents explicitly identied in the feature vector." ></td>
	<td class="line x" title="43:79	To this end, we add an additional feature for each unigram appearing on the rst line of an article." ></td>
	<td class="line x" title="44:79	In this way the heading tokens are represented twice: once in the overall set of unigrams in the article, and once in the set of heading unigrams." ></td>
	<td class="line x" title="45:79	Profanity: true news articles very occasionally include a verbal quote which contains offensive language, but in practically all other cases it is incumbent on journalists and editors to keep their language clean." ></td>
	<td class="line x" title="46:79	A review of the corpus shows that this is not the case with satirical news, which occasionally uses profanity as a humorous device." ></td>
	<td class="line x" title="47:79	Let P be a binary feature indicating whether or not an article contains profanity, as determined by the Regexp::Common::profanity Perl module.2 Slang: As with profanity, it is intuitively true that true news articles tend to avoid slang." ></td>
	<td class="line x" title="48:79	An impressionistic review of the corpus suggests that informallanguageismuchmorecommontosatirical articles." ></td>
	<td class="line x" title="49:79	We measure the informality of an article as: i def= 1|T| tT s(t) where T is the set of unigram tokens in the article and s is a function taking the value 1 if the token has a dictionary denition marked as slang and 0 if it does not." ></td>
	<td class="line x" title="50:79	It is important to note that this measure of informality is approximate at best." ></td>
	<td class="line x" title="51:79	We do not attempt, e.g., to disambiguate the sense of individual word terms to tell whether the slang sense of a word is the one intended." ></td>
	<td class="line x" title="52:79	Rather, we simply checktoseeifeachwordhasaslangusageinWiktionary.3 A continuous feature is set to the value of i for each article." ></td>
	<td class="line x" title="53:79	Discrete features highi and lowi are set as: highi def= { 1 v >i + 2; 0 lowi def= { 1 v <i2; 0 wherei and  are, respectively, the mean and standard deviation of i across all articles." ></td>
	<td class="line x" title="54:79	2http://search.cpan.org/perldoc?" ></td>
	<td class="line x" title="55:79	Regexp::Common::profanity 3http://www.wiktionary.org 3.3 Semantic validity Lexical approaches are clearly inadequate if we assume that good satirical news articles tend to emulate real news in tone, style, and content." ></td>
	<td class="line x" title="56:79	What is needed is an approach that captures the document semantics." ></td>
	<td class="line x" title="57:79	One common device in satire news articles is absurdity, in terms of describing well-known individuals in unfamiliar settings which parody their viewpoints or public prole." ></td>
	<td class="line x" title="58:79	We attempt to capture this via validity, in the form of the relative frequencyoftheparticularcombinationofkeyparticipants reported in the story." ></td>
	<td class="line x" title="59:79	Our method identies thenamedentitiesinagivendocumentandqueries the web for the conjunction of those entities." ></td>
	<td class="line x" title="60:79	Our expectation is that true news stories will have been reported in various forums, and hence the number of web documents which include the same combination of entities will be higher than with satire documents." ></td>
	<td class="line x" title="61:79	To implement this method, we rst use the Stanford Named Entity Recognizer4 (Finkel et al., 2005)toidentifythesetofpersonandorganisation entities, E, from each article in the corpus." ></td>
	<td class="line x" title="62:79	From this, we estimate the validity of the combination of entities in the article as: v(E) def= |g(E)| where g is the set of matching documents returned by Google using a conjunctive query." ></td>
	<td class="line x" title="63:79	We anticipate that v will have two potentially useful properties: (1) it will be relatively lower when E includes made-up entity names such as Hitler Commemoration Institute, found in one satirical corpus article; and (2) it will be relatively lower when E contains unusual combinations of entities such as, forexample, thoseinthesatiricalarticlebeginning Missing Brazilian balloonist Padre spotted straddling Pink Floyd ying pig." ></td>
	<td class="line x" title="64:79	We include both a continuous representation of v for each article, in the form of log(v(E)), and discrete variants of the feature, based on the same methodology as for highi and lowi." ></td>
	<td class="line x" title="65:79	4 Results The results for our classiers over the satire corpus are shown in Table 2." ></td>
	<td class="line x" title="66:79	The baseline is a naive classier that assigns all instances to the positive 4http://nlp.stanford.edu/software/ CRF-NER.shtml 163 (articleSATIRE?) P R F all-positive baseline 0.063 1.000 0.118 BIN 0.943 0.500 0.654 BIN+lex 0.945 0.520 0.671 BIN+val 0.943 0.500 0.654 BIN+all 0.945 0.520 0.671 BNS 0.944 0.670 0.784 BNS+lex 0.957 0.660 0.781 BNS+val 0.945 0.690 0.798 BNS+all 0.958 0.680 0.795 Table 2: Results for satire detection (P = precision, R = recall, and F = F-score) for binary unigram features (BIN) and BNS unigram features (BNS),optionallyusinglexical(lex), validity(val) or combined lexical and validity (all) features class(i.e. SATIRE)." ></td>
	<td class="line x" title="67:79	AnSVMclassierwithsimple binary unigram word features provides a standard text classication benchmark." ></td>
	<td class="line x" title="68:79	All of the classiers easily outperform the baseline." ></td>
	<td class="line x" title="69:79	This is to be expected given the low proportion of positive instances in the corpus." ></td>
	<td class="line x" title="70:79	The benchmark classier has very good precision, but recall of only 0.500." ></td>
	<td class="line x" title="71:79	Adding the heading, slang, and profanity features provides a small improvement in both precision and recall." ></td>
	<td class="line x" title="72:79	Moving to BNS feature scaling keeps the very high precision and increases the recall to 0.670." ></td>
	<td class="line x" title="73:79	Adding in the heading, slang and profanity lexical features (+lex) actually decreases the F-score slightly, but adding the validity features (+val) provides a near 2 point F-score increase, resulting in the best overall F-score of 0.798." ></td>
	<td class="line x" title="74:79	All of the BNS scores achieve statistically signicant improvements over the benchmark in terms of F-score (using approximate randomisation, p < 0.05)." ></td>
	<td class="line x" title="75:79	The 1-2% gains given by adding inthevariousfeaturetypesarenotstatisticallysignicant due to the small number of satire instances concerned." ></td>
	<td class="line x" title="76:79	All of the classiers achieve very high precision and considerably lower recall." ></td>
	<td class="line x" title="77:79	Error analysis suggests that the reason for the lower recall is subtler satire articles, which require detailed knowledge of the individuals to be fully appreciated as satire." ></td>
	<td class="line x" title="78:79	While they are not perfect, however, the classiers achieve remarkably high performance given the superciality of the features used." ></td>
	<td class="line x" title="79:79	5 Conclusions and future work Thispaperhasintroducedanoveltasktocomputational linguistics and machine learning: determining whether a newswire article is true or satirical. We found that the combination of SVMs with BNS feature scaling achieves high precision and lower recall, and that the inclusion of the notion of validity achieves the best overall F-score." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-2043
Toward finer-grained sentiment identification in product reviews through linguistic and ontological analyses
Min, Hye-Jin;Park, Jong C.;"></td>
	<td class="line x" title="1:107	Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 169172, Suntec, Singapore, 4 August 2009." ></td>
	<td class="line x" title="2:107	c 2009 ACL and AFNLP Toward finer-grained sentiment identification in product reviews through linguistic and ontological analyses   Hye-Jin Min Computer Science Department KAIST, Daejeon, KOREA hjmin@nlp.kiast.ac.kr Jong C. Park Computer Science Department KAIST, Daejeon, KOREA park@nlp.kaist.ac.kr    Abstract  We propose categories of finer-grained polarity for a more effective aspect-based sentiment summary, and describe linguistic and ontological clues that may affect such fine-grained polarity." ></td>
	<td class="line x" title="3:107	We argue that relevance for satisfaction, contrastive weight clues, and certain adverbials work to affect the polarity, as evidenced by the statistical analysis." ></td>
	<td class="line oc" title="4:107	1 Introduction Sentiment analysis have been widely conducted in several domains such as movie reviews, product reviews, news and blog reviews (Pang et al., 2002; Turney, 2002)." ></td>
	<td class="line x" title="5:107	The unit of the sentiment varies from a document level to a sentence level to a phrase-level, where a more fine-grained approach has been receiving more attention for its accuracy." ></td>
	<td class="line x" title="6:107	Sentiment analysis on product reviews identifies or summarizes sentiment from reviews by extracting relevant opinions about certain attributes of products such as their parts, or properties (Hu and Liu, 2004; Popescu and Etzioni, 2005)." ></td>
	<td class="line x" title="7:107	Aspect-based sentiment analysis summarizes sentiments with diverse attributes, so that customers may have to look more closely into analyzed sentiments (Titov and McDonald, 2008)." ></td>
	<td class="line x" title="8:107	However, there are additional problems." ></td>
	<td class="line x" title="9:107	First, it is rather hard to choose the right level of detail." ></td>
	<td class="line x" title="10:107	If concepts corresponding to attributes are too general, the level of detail may not be so much finer than the ones on a document level." ></td>
	<td class="line x" title="11:107	On the other hand, if concepts are too specific, there may be some attributes that are hardly mentioned in the reviews, resulting in the data sparseness problem." ></td>
	<td class="line x" title="12:107	Second, there are cases when some crucial information is lost." ></td>
	<td class="line x" title="13:107	For example, suppose that two product attributes are mentioned in a sentence with a coordinated or subordinated structure." ></td>
	<td class="line x" title="14:107	In this case, the information about their relation may not be shown in the summary if they are classified into different upper-level attributes." ></td>
	<td class="line x" title="15:107	Consider (1)." ></td>
	<td class="line x" title="16:107	(1) a.    /   ,     . osun macciman, sayksangi nemwu etwuweyo." ></td>
	<td class="line x" title="17:107	It fits me okay, but the color is too dark. (size: barely positive, color: negative) b.      ,            ." ></td>
	<td class="line x" title="18:107	sayngkakpota com yalpciman, aney patchye ipnun kenikka nalum kwaynchanhunke kathayo." ></td>
	<td class="line x" title="19:107	Its a bit thinner than I thought, but it is good enough for layering. (thickness: negative but acceptable, overall: positive)  Example (1) shows sample customer reviews about clothes, each first in Korean, followed by a Yale Romanized form, and an English translation." ></td>
	<td class="line x" title="20:107	Note that the weight of the polarity in the sentiment about size e.g. in (1a) is overcome by the one about color." ></td>
	<td class="line x" title="21:107	However, if the overall sentiment is computed by considering only the number of semantically identical phrases in the reviews, it misses the big picture." ></td>
	<td class="line x" title="22:107	In particular, when opinions regarding attributes are described with respect to expressions whose polarities are dependent on the specific contexts such as the weather or user preference, an overestimated or underestimated weight of the sentiment for each attribute may be assigned." ></td>
	<td class="line x" title="23:107	In our example, / yalpta/thin has an ambiguous polarity, i.e., either positive or negative, whose real value depends on the expected utility of the clothes." ></td>
	<td class="line x" title="24:107	In this case, the negative polarity is the intended one, as shown in (1b)." ></td>
	<td class="line x" title="25:107	In order to reflect this possibility, we need to adjust the weight of each polarity accordingly." ></td>
	<td class="line x" title="26:107	In this paper, we propose to look into the kind of linguistic and ontological clues that may in169 fluence the use of polarities, or the relevance for satisfaction of purchase inspired by Kanos theory of quality element classification (Huiskonen and Pirttila, 1998), the conceptual granularities, and such syntactic and lexical clues as conjunction items and adverbs." ></td>
	<td class="line x" title="27:107	They may play significant roles in putting together the identified polarity information, so as to assess correctly what the customers consider most important." ></td>
	<td class="line x" title="28:107	We conducted several one-way Analysis of Variance (ANOVA) tests to identify the effects of each clue on deriving categories of polarity and quantification method 2 to see whether these clues can distinguish fine-grained polarities correctly." ></td>
	<td class="line x" title="29:107	Section 2 introduces categories of polarity." ></td>
	<td class="line x" title="30:107	Section 3 analyzes ontological and linguistic clues for identifying the proper category." ></td>
	<td class="line x" title="31:107	Section 4 describes our method to extract such clues for a statistical analysis." ></td>
	<td class="line x" title="32:107	Section 5 discusses the results of the analysis and implications of the results." ></td>
	<td class="line x" title="33:107	Section 6 concludes the paper." ></td>
	<td class="line x" title="34:107	2 Categories of polarity We suggest two more fine-grained categories of polarity, or barely positive (BP) and acceptably negative (AN), in addition to positive (P), negative (N) and neutral (NEU)." ></td>
	<td class="line x" title="35:107	We distinguish barely positive from normal positive and distinguish acceptably negative from normal negative in order to derive finer-grained sentiments." ></td>
	<td class="line x" title="36:107	Wilson and colleagues (2006) identified the strength of news articles in the MPQA corpus, where they separated intensity (low, medium, high) from categories (private states)." ></td>
	<td class="line x" title="37:107	For the purpose of identifying each attributes contribution to the satisfaction after purchase, we believe that it is not necessary to have so many degrees of intensity." ></td>
	<td class="line x" title="38:107	We argue that the polarity of barely positive may hold attributes that must be satisfied and that acceptably negative may hold those that are somewhat optional." ></td>
	<td class="line x" title="39:107	3 Linguistic and Ontological Analyses In this section, we discuss linguistic and ontological clues that influence the process of identifying finer-grained polarity." ></td>
	<td class="line x" title="40:107	For the purpose of exposition, we build hierarchical and aspect-based review structure as shown in Figure 1." ></td>
	<td class="line x" title="41:107	Major aspects include Price, Delivery, Service, and Product." ></td>
	<td class="line x" title="42:107	If we go down another level, Product is divided into Quality and Comfortableness." ></td>
	<td class="line x" title="43:107	In defining relevant attributes, we consider all the lower-level concepts of major aspects, which contain the characteristics of the product with a description of the associated sentiment." ></td>
	<td class="line x" title="44:107	Figure 1." ></td>
	<td class="line x" title="45:107	Review structure  Relevance for Satisfaction: We consider relevant attributes that affect the quality and satisfaction of the products as one of the important clues." ></td>
	<td class="line x" title="46:107	Quality elements classified by Kano as shown in Table 1 can be base indicators of relevant attributes for satisfaction in real review text." ></td>
	<td class="line x" title="47:107	For example, while completeness of the product may become crucial if the product has a defect, it is usually not the case that it would contribute much to the overall satisfaction of the customer." ></td>
	<td class="line x" title="48:107	Quality Elements  Example features Must-be Quality (MQ) Durability, Completeness 1-dimension Quality (1DQ) Design, Color, Material Attractive Quality (AQ) Luxurious look Table 1." ></td>
	<td class="line x" title="49:107	Kano's Quality Elements  Conceptual Granularity: The concepts corresponding to attributes have a different level of detail." ></td>
	<td class="line x" title="50:107	If the customer wants to comment on some attributes in detail, she could use a finegrained concept (e.g., the width of the thigh part of the pants) rather than a coarse-grained one (e.g., just the size of the pants)." ></td>
	<td class="line x" title="51:107	To deal properly with the changing granularity of such concepts, we constructed a domain specific semihierarchical network for clothes of the ClothingType structure, in addition to the Review structure, by utilizing hierarchical category information in online shopping malls." ></td>
	<td class="line x" title="52:107	Figure 2 shows an example for pants." ></td>
	<td class="line x" title="53:107	ClothingType Bottom Pants Sub_f Sub_p Thigh CalfWaistHip Length+ Material+ Design: Line+ Design: Pattern* Design: Style* Color Size Design: Detail*  Figure 2." ></td>
	<td class="line x" title="54:107	ClothingType structure for pants  Syntactic and Lexical Clues: Descriptions of each attribute in the reviews are often expressed 170 in a phrase or clause, so that conjunctions, or endings of a word with a conjunctive marker in Korean, play a significant role in connecting one attribute to another." ></td>
	<td class="line x" title="55:107	They also convey a subtle meaning of the sentiment about relations between two or more connected attributes." ></td>
	<td class="line x" title="56:107	We classified such syntactic clues into 4 groups of likeness (L), contrary (C), cause-effect (CE), and contrary with contrastive markers (CC)." ></td>
	<td class="line x" title="57:107	Wilson and colleagues (2006) selected some syntactic clues as features for intensity classification." ></td>
	<td class="line x" title="58:107	The selected features are shown to improve the accuracy, but the set of clues may vary to the nature of the given corpus, so that some otherwise useful clues that reflect a particular focused structure may not be selected." ></td>
	<td class="line x" title="59:107	We argue that some syntactic clues such as the use of certain conjunctions can be identified manually to make up for the limitation of feature selection." ></td>
	<td class="line x" title="60:107	Adverbs modifying adjectives or verbs such as too, and very also strengthen the polarity of a given sentiment, so such clues work to differentiate normal positive or negative from barely positive and acceptably negative." ></td>
	<td class="line x" title="61:107	Table 2 summarizes linguistic clues in the present analysis." ></td>
	<td class="line x" title="62:107	Clues  Examples CONJ/ END L -  -ko and C -  -ciman but,   kulena however CE -  -ese so,  kulayse therefore CC -    -kin -ciman  Its , but, though ADV Strong   maywu very,  nemwu too Mild   com a little Table 2." ></td>
	<td class="line x" title="63:107	Syntactic and Lexical Clues  All these three types of clue that appear in the review text may interact with one another." ></td>
	<td class="line x" title="64:107	For example, attributes with barely positive tend to be described with a concept on a coarse level, and may belong to Must-be Quality (e,g.,  size in (1a))." ></td>
	<td class="line x" title="65:107	However, if such attributes are negative, customers may explain them with a very finegrained concept (e.g., the width of thigh is okay, but the calf part is too wide; interaction between relevance for satisfaction and conceptual granularity)." ></td>
	<td class="line x" title="66:107	They may also use adverbs such as too to emphasize such unexpected polarity information." ></td>
	<td class="line x" title="67:107	For emphasis, a contrastive structure can be used to indicate which attribute has a more weight (e.g., A but B; interaction between syntactic clues and relevance for satisfaction)." ></td>
	<td class="line x" title="68:107	In addition, an unfocused attribute A may be the attribute with acceptably negative if the polarity of the attribute B is positive." ></td>
	<td class="line x" title="69:107	We believe that the interaction between lexical and syntactic clues and relevance for satisfaction are the most important and that this correlation information may be utilized with such fine-grained polarity as barely positive or acceptably negative." ></td>
	<td class="line x" title="70:107	4 Clue Acquisition We acquired data semi-automatically for each clue from the extracted attributes and their descriptions from 500 product reviews of several types of pants and annotated polarities manually." ></td>
	<td class="line x" title="71:107	We obtained raw text reviews from one of the major online shopping malls in Korea 1  and performed a morphology analysis and POS-tagging." ></td>
	<td class="line x" title="72:107	After POS-tagging, we collected all the noun phrases as candidates of attributes." ></td>
	<td class="line x" title="73:107	We regarded some of them as attributes with the following guidelines and filtered out the rest: 1) NP with frequent adjectives 2) NP with frequent nonfunctional and intransitive verbs." ></td>
	<td class="line x" title="74:107	In the case of subject omission, we converted adjectives or verbs into their corresponding nouns, such as thin into thickness." ></td>
	<td class="line x" title="75:107	Hu and Liu (2004) identified attributes of IT products based on frequent noun phrases and Popescu and Etzioni (2005) utilized PMI values between product class (hotels and scanners) and some phrases including product." ></td>
	<td class="line x" title="76:107	In our case, we used attributes that belong only to the Product concept in the Review structure, because most attributes we consider are sub-types or sub-attribute of Product." ></td>
	<td class="line x" title="77:107	The total number of <attribute, polarity> pairs is 474." ></td>
	<td class="line x" title="78:107	For relevance for satisfaction, we converted extracted attributes into one of the types of Kanos quality elements by the mapping table we built." ></td>
	<td class="line x" title="79:107	For conceptual granularity we regarded all the attributes with a depth less than 2 as coarse and those more than 2 as fine." ></td>
	<td class="line x" title="80:107	Syntactic and lexical clues are identified from the context information around extracted adjective or verbs by the patterns based on POS information." ></td>
	<td class="line x" title="81:107	5 Statistical Analysis and Discussion We conducted one-way Analysis of Variance (ANOVA) tests using relevance for satisfaction (ReV), conceptual granularity (Granul), and two linguistic clues, ADV and CONJ/END, in order to assess the effects of each clue on identifying categories of polarity." ></td>
	<td class="line x" title="82:107	The ANOVA suggests  1  http://www.11st.co.kr 171 reliable effects of ReV (F(2,474) = 22.2; p = .000), ADV (F(2, 474) = 41.3; p = .000), and CONJ/END (F(3, 474) = 6.1; p = .000)." ></td>
	<td class="line x" title="83:107	We also performed post-hoc tests to test significant differences." ></td>
	<td class="line x" title="84:107	For ReV, there are significant differences between MQ and 1DQ (p=.000), and between MQ and AQ (p =.032)." ></td>
	<td class="line x" title="85:107	AQ is related to positive and MQ to acceptably negative by the result." ></td>
	<td class="line x" title="86:107	For ADV, there are significant differences between all pairs (p <.05)." ></td>
	<td class="line x" title="87:107	For CONJ/END, there are significant differences between likeness and contrary (p = .015), and between likeness and contrary with contrastive markers (p = .025)." ></td>
	<td class="line x" title="88:107	The contrary and contrary with contrastive markers types of conjunctions are related to acceptably negative." ></td>
	<td class="line x" title="89:107	We also conducted Quantification method 2 to see if these clues can discriminate between BP and P and discriminate between AN and N. The regression equation for distinguishing AN from N is statistically significant at the 5% level (F(7,177) = 12,2; R 2 =0.335; Std." ></td>
	<td class="line x" title="90:107	error of the estimate =  0.821; error rate for discriminant = 0.21)." ></td>
	<td class="line x" title="91:107	The coefficients for mild (t 2 =30.8), contrary (t 2 =17.8) and contrary with contrastive markers (t 2 =14.1) are significant." ></td>
	<td class="line x" title="92:107	The results lead us to conclude that we can identify acceptably negative from the clothes reviews by extracting the particular lexical clue, adverbs of mild category and syntactic clue, such as conjunctions of contrary, and contrary with contrastive markers, or contrastive weight." ></td>
	<td class="line x" title="93:107	This clue may convey the customers argumentative intention toward the product, or argumentative orientation, for instance, A and B in A but B. C have different influence on the following discourse C (Elhadad and McKeown, 1990)." ></td>
	<td class="line x" title="94:107	Although contrary with contrastive markers plays an important role in identifying acceptably negative, it could also be used to identify another type of positive as shown in  example (2)." ></td>
	<td class="line x" title="95:107	(2)    ." ></td>
	<td class="line x" title="96:107	   . com twukkeptanun sayngkaki tupnita." ></td>
	<td class="line x" title="97:107	kulayto ttattushakin haneyyo." ></td>
	<td class="line x" title="98:107	It is a bit thick, but it keeps me warm.  It is a positive feature, but neither fully positive nor barely positive." ></td>
	<td class="line x" title="99:107	It seems to be somewhere inbetween." ></td>
	<td class="line x" title="100:107	The order of appearance in reviews may also affect the strength of polarity." ></td>
	<td class="line x" title="101:107	In addition, particular cue phrases such as ~ / kesman ppayko/except that  can also convey acceptably negative, too." ></td>
	<td class="line x" title="102:107	In the future, we need to assess the importance of each proposed clue relative to others and to the existing ones." ></td>
	<td class="line x" title="103:107	We also need to investigate the nature of interactions among linguistic, ontological and relevance for satisfaction clues, which may influence the actual performance for identifying finer-grained polarity." ></td>
	<td class="line x" title="104:107	6 Conclusion and Future Work We proposed further categories of polarity in order to make aspect-based sentiment summary more effective." ></td>
	<td class="line x" title="105:107	Our linguistic and ontological analyses suggest that there are clues, such as relevance for satisfaction, contrastive weight and certain adverbials, that work to affect polarity in a more subtle but crucial manner, as evidenced also by the statistical analysis." ></td>
	<td class="line x" title="106:107	We plan to find out product attributes that contribute most to modeling the interaction among the proposed clues in effective sentiment summarization." ></td>
	<td class="line x" title="107:107	Acknowledgments This work was funded in part by the Intelligent Robotics Development Program, a 21 st  Century Frontier R&D Program by the Ministry of Knowledge Economy in Korea, and in part by the 2 nd  stage of the Brain Korea 21 project." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1606
Investigation in Statistical Language-Independent Approaches for Opinion Detection in English, Chinese and Japanese
Zubaryeva, Olena;Savoy, Jacques;"></td>
	<td class="line x" title="1:178	Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 3845, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:178	c 2009 Association for Computational Linguistics Investigation in Statistical Language-Independent Approaches for Opinion Detection in English, Chinese and Japanese Olena Zubaryeva Jacques Savoy Institute of Informatics Institute of Informatics University of Neuchtel University of Neuchtel Emile-Argand, 11, 2009 Switzerland Emile-Argand, 11, 2009 Switzerland olena.zubaryeva@unine.ch jacques.savoy@unine.ch Abstract In this paper we present a new statistical approach to opinion detection and its evaluation on the English, Chinese and Japanese corpora." ></td>
	<td class="line x" title="3:178	Besides, the proposed method is compared  with  three  baselines,  namely Nave Bayes classifier, a language model and an approach based on significant collocations." ></td>
	<td class="line x" title="4:178	These models being language independent are improved with the use of language-dependent technique on the example of the English corpus." ></td>
	<td class="line x" title="5:178	We show that our method almost always gives better performance compared to the considered baselines." ></td>
	<td class="line x" title="6:178	1 Introduction The task of opinion mining has received attention from the research community and industry lately." ></td>
	<td class="line x" title="7:178	The main reasons for extensive research in the area are the growth of user needs and companies desire to analyze and exploit the user-generated content on the Web in the form of blogs and discussions." ></td>
	<td class="line x" title="8:178	Thus, users want to search for opinions on various topics from products that they want to buy to opinions about events and well-known persons." ></td>
	<td class="line x" title="9:178	A lot of businesses are interested in how their services are perceived by their customers." ></td>
	<td class="line x" title="10:178	Therefore, the detection of subjectivity in the searched information may add the additional value to the interpretation of the results and their relevancy to the searched topic." ></td>
	<td class="line x" title="11:178	The growth of user activity on the Web gives substantial amounts of data for these purposes." ></td>
	<td class="line x" title="12:178	In the context of globalization the possibility to provide search of opinionated information in different natural languages might be of great interest to organizations and communities around the world." ></td>
	<td class="line x" title="13:178	Our goal is to design a fully automatic system capable of working in a language-independent manner." ></td>
	<td class="line x" title="14:178	In order to compare our approach on different languages we chose English, traditional Chinese and Japanese corpora." ></td>
	<td class="line x" title="15:178	As a further possibility to improve the effectiveness of the language independent methods we also consider the additional application of language dependent techniques specific to the particular natural language." ></td>
	<td class="line x" title="16:178	The related work in opinion detection is presented in Section 2." ></td>
	<td class="line x" title="17:178	We describe our approach in detail with the three other baselines in Section 3." ></td>
	<td class="line x" title="18:178	The fourth section describes language specific approach used for the English corpus." ></td>
	<td class="line x" title="19:178	In Section 5 we present the evaluation of the three models using the NTCIR-6 and NTCIR-7 MOAT English, Chinese and Japanese test collections (Seki  et  al., 2008)." ></td>
	<td class="line x" title="20:178	The main findings of our study and future research possibilities are discussed in the last sections." ></td>
	<td class="line x" title="21:178	2 Related Work The focus of our work is to propose a general approach that can be easily deployed for different natural languages." ></td>
	<td class="line x" title="22:178	This task of opinion detection is important  in  many  areas  of  NLP  such  as question/answering, information retrieval, docu38 ment classification and summarization, and information filtering." ></td>
	<td class="line x" title="23:178	There are numerous challenges when trying to solve the task of opinion detection." ></td>
	<td class="line x" title="24:178	Some of them include the fact that the distinction between opinionated and factual could be denoted by a single word in the underlying text (e.g., The iPhone price is $600. vs. The iPhone price is high.)." ></td>
	<td class="line x" title="25:178	Most importantly evaluating whether or not a given sentence conveys an opinion could be questionable when judged by different people." ></td>
	<td class="line x" title="26:178	Further, the opinion classification can be done on different levels, from documents to clauses in the sentence." ></td>
	<td class="line x" title="27:178	We consider the opinion detection task on a sentence level." ></td>
	<td class="line x" title="28:178	After retrieving the relevant sentences using any IR system we automatically classify a sentence according to two classes: opinionated and not opinionated (factual)." ></td>
	<td class="line x" title="29:178	When viewing an opinion-finding task as a classification task, it is usually considered as a supervised learning problem where a statistical model performs a learning task by analyzing a pool of labeled sentences." ></td>
	<td class="line x" title="30:178	Two questions must therefore be solved, namely defining an effective classification algorithm and determining pertinent features that might effectively discriminate between opinionated and factual sentences." ></td>
	<td class="line x" title="31:178	From this perspective, during the last TREC opinion-finding task (Macdonald et  al., 2008) and the last NTCIR-7 workshop (Seki et al., 2008), a series of suggestions surfaced." ></td>
	<td class="line x" title="32:178	As the language-dependent approach various teams proposed using Levin defined verb categories (namely, characterize, declare, conjecture, admire, judge, assess, say, complain, advise) and their features (a verb corresponding to a given category occurring in the analyzed information item) that may be pertinent as a classification feature (Bloom  et  al., 2007)." ></td>
	<td class="line x" title="33:178	However, words such as these cannot always work correctly as clues, for example with the word said in the two sentences There were crowds and crowds of people at the concert, said Ann and There were more than 10,000 people at the concert, said Ann.  Both sentences contain the clue word said but only the first one contains an opinion on the target product." ></td>
	<td class="line oc" title="34:178	Turney (2002) suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon." ></td>
	<td class="line x" title="35:178	Specific to the opinion detection in Chinese language Ku et al.(2006) propose a dictionary-based approach for extraction and summarization." ></td>
	<td class="line x" title="37:178	For the Japanese language in the last NTCIR-6 and NTCIR-7 workshops the opinion finding methods included the use of supervised machine learning approaches with specific selection of certain parts-of-speech (POS) and sentence parts in the form of n-gram features to improve performance." ></td>
	<td class="line x" title="38:178	There has been a trend in applying language models for opinion detection task (Lavrenko, Croft, 2001)." ></td>
	<td class="line x" title="39:178	Pang & Lee (2004) propose the use of language models for sentiment analysis task and subjectivity extraction." ></td>
	<td class="line x" title="40:178	Usually, language models are trained on the labeled data and as an output they give probabilities of classified tokens belonging to the class." ></td>
	<td class="line x" title="41:178	Eguchi & Lavrenko (2006) propose the use of probabilistic language models for ranking the results not only by sentiment but also by the topic relevancy." ></td>
	<td class="line x" title="42:178	As an alternative other teams during the last TREC and NTCIR evaluation campaigns have suggested variations of Nave Bayes classifier, language models and SVM, along with the use of such heuristics as word order, punctuation, sentence length, etc. We might also mention OpinionFinder (Wilson et al., 2005), a more complex system that performs subjectivity analyses to identify opinions as well as sentiments and other private states (speculations, dreams, etc.)." ></td>
	<td class="line x" title="43:178	This system is based on various classical computational linguistics components (tokenization, part-of-speech (POS) tagging (Toutanova & Manning, 2000) as well as classification tools." ></td>
	<td class="line x" title="44:178	For example, a Nave Bayes classifier (Witten & Frank, 2005) is used to distinguish between subjective and objective sentences." ></td>
	<td class="line x" title="45:178	A rule-based system is included to identify both speech events (said, according to) and direct subjective expressions (is happy, fears) within a given sentence." ></td>
	<td class="line x" title="46:178	Of course such learning system requires both a training set and a deeper knowledge of a given natural language (morphological components, syntactic analyses, semantic thesaurus)." ></td>
	<td class="line x" title="47:178	The lack of enough training data for the learning-based systems is clearly a drawback." ></td>
	<td class="line x" title="48:178	Moreover, it is difficult to objectively establish when a complex learning system has enough training data (and to objectively measure the amount of training data needed in a complex ML model)." ></td>
	<td class="line x" title="49:178	39 3 Language Independent Approaches In this section we propose our statistical approach for opinion detection as well as the description of the Nave Bayes and language model (LM) baselines." ></td>
	<td class="line x" title="50:178	3.1  Logistic Model Our system is based on two components: the extraction and weighting of useful features (limited to isolated words in this study) to allow an effective classification, and a classification scheme." ></td>
	<td class="line x" title="51:178	First, we present the feature extraction approach in the Section 3.1.1." ></td>
	<td class="line x" title="52:178	Next, we discuss our classification model." ></td>
	<td class="line x" title="53:178	Sections 3.2 and 3.3 describe the chosen baselines." ></td>
	<td class="line x" title="54:178	3.1.1 Features Extraction  In order to determine the features that can help distinguishing between factual and opinionated documents, we have selected the tokens." ></td>
	<td class="line x" title="55:178	As shown by Kilgarriff (2001), the selection of words (or in general features) in an effort to characterize a particular category is a difficult task." ></td>
	<td class="line x" title="56:178	The goal is therefore to design a method capable of selecting terms that clearly belong to one of the classes." ></td>
	<td class="line x" title="57:178	The approaches that use words and their frequencies or distributions are usually based on a contingency table (see Table 1)." ></td>
	<td class="line x" title="58:178	S C a b a+b not  c d c+d a+c b+d n=a+b+c +d Table 1." ></td>
	<td class="line x" title="59:178	Example of a contingency table." ></td>
	<td class="line x" title="60:178	In this table, the letter a represents the number of occurrences (tokens) of the word  in the document set S (corresponding to a subset of the larger corpus C in the current study)." ></td>
	<td class="line x" title="61:178	The letter b denotes the number of tokens of the same word  in the rest of the corpus (denoted C-) while a+b is the total number of occurrences in the entire corpus (denoted C with C=C-S)." ></td>
	<td class="line x" title="62:178	Similarly, a+c indicates the total number of tokens in S.  The entire corpus C corresponds to the union of the subset S and Cthat contains n tokens (n = a+b+c+d)." ></td>
	<td class="line x" title="63:178	Based on the MLE (Maximum Likelihood Estimation) principle the values shown in a contingency table could be used to estimate various probabilities." ></td>
	<td class="line x" title="64:178	For example we might calculate the probability of the occurrence of the word  in the entire corpus C as Pr() = (a+b)/n or the probability of finding in C a word belonging to the set S as Pr(S) = (a+c)/n. Now to define the discrimination power a term , we suggest defining a weight attached to it according to Muller's method (Muller, 1992)." ></td>
	<td class="line x" title="65:178	We assume that the distribution of the number of tokens of the word  follows a binomial distribution with the parameters p and n'." ></td>
	<td class="line x" title="66:178	The parameter p represented the probability of drawing a word  also denoted in the corpus C (or Pr()) and could be estimated as (a+b)/n.  If we repeat this drawing n' = a+c times, we will have an estimate of the number of word  included in the subset S by Pr().n'." ></td>
	<td class="line x" title="67:178	On the other hand, Table 1 gives also the number of observations of the word  in S, and this value is denoted by a. A large difference between a and the product Pr().n' is clearly an indication that the presence of a occurrences of the term  is not due by chance but corresponds to an intrinsic characteristic of the subset S compared to the subset C-." ></td>
	<td class="line x" title="68:178	In order to obtain a clear rule, we suggest computing the Z score attached to each word ." ></td>
	<td class="line x" title="69:178	If the mean of a binomial distribution is Pr().n', its variance is n'.Pr().(1-Pr())." ></td>
	<td class="line x" title="70:178	These two elements are needed to compute the standard score as described in Equation 1." ></td>
	<td class="line x" title="71:178	))Pr(1()Pr(` )Pr(`)(    = n naZscore         (1) As a decision rule we consider the words having a Z score between -2 and 2 as terms belonging to a common vocabulary, as compared to the reference corpus (as for example will, with, many, friend, or forced in our example)." ></td>
	<td class="line x" title="72:178	This threshold was chosen arbitrary." ></td>
	<td class="line x" title="73:178	A word having a Z score > 2 would be considered as overused (e.g., that, should, must,  not, or government  in MOAT NTCIR-6 English corpus), while a Z score < -2 would be interpreted as an underused term (e.g., police, cell, year, died, or according)." ></td>
	<td class="line x" title="74:178	The arbitrary threshold limit of 2 corresponds to the limit of the standard normal distribution, allowing us to find around 5% of the observa40 tions (around 2.5% less than -2 and 2.5% greater than 2)." ></td>
	<td class="line x" title="75:178	As shown in Figure 1, the difference between our arbitrary limit of 2 (drawn in solid line) and the limits delimiting the 2.5% of the observations (dotted line) are rather close." ></td>
	<td class="line x" title="76:178	Figure 1." ></td>
	<td class="line x" title="77:178	Distribution of the Z score (MOAT NTCIR-6 English corpus, opinionated)." ></td>
	<td class="line x" title="78:178	Based on a training sample, we were able to compute the Z score for different words and retain only those having a large or small Z score value." ></td>
	<td class="line x" title="79:178	Such a procedure is repeated for all classification categories (opinionated and factual)." ></td>
	<td class="line x" title="80:178	It is worth mentioning that such a general scheme may work with isolated words (as applied here) or n-gram (that could be a sequence of either characters or words), as well as with punctuations or other symbols (numbers, dollar signs), syntactic patterns (e.g., verb-adjective in comparative or superlative forms) or other features (presence of proper names, hyperlinks, etc.) 3.1.2 Classification Model When our system needs to determine the opinionatedness of a sentence, we first represent this sentence as a set of words." ></td>
	<td class="line x" title="81:178	For each word, we can then retrieve the Z scores for each category." ></td>
	<td class="line x" title="82:178	If all Z scores for all words are judged as belonging to the general vocabulary, our classification procedure selects the default category." ></td>
	<td class="line x" title="83:178	If not, we may increase the weight associated with the corresponding category (e.g., for the opinionated class if the underlying term is overused in this category)." ></td>
	<td class="line x" title="84:178	Such a simple additive process could be viewed as a first classification scheme, selecting the class having the highest score after enumerating all words occurring in a sentence." ></td>
	<td class="line x" title="85:178	This approach assumes that the word order does not have any impact." ></td>
	<td class="line x" title="86:178	We also assume that each sentence has a similar length." ></td>
	<td class="line x" title="87:178	For this model, we can define two variables, namely SumOP  indicating the sum of the Z score of terms overused in opinionated class (i.e. Z score > 2) and appearing in the input sentence." ></td>
	<td class="line x" title="88:178	Similarly, we can define SumNOOP for the other class." ></td>
	<td class="line x" title="89:178	However, a large SumOP value can be obtained by a single word or by a set of two (or more) words." ></td>
	<td class="line x" title="90:178	Thus, it could be useful to consider also the number of words (features) that are overused (or underused) in a sentence." ></td>
	<td class="line x" title="91:178	Therefore, we can define #OpOver  indicated the number of terms in the evaluated sentence that tends to be overused in opinionated documents (i.e. Z score > 2) while #OpUnder indicated the number of terms that tends to be underused in the class of  opinionated documents (i.e. Z score < -2)." ></td>
	<td class="line x" title="92:178	Similarly, we can define the variables #NoopOver, #NoopUnder, but for the non-opinionated category." ></td>
	<td class="line x" title="93:178	With these additional explanatory variables, we can compute the corresponding subjectivity score for each sentence as follows: NoopUnderNoopOver NoopOverscoreNoop OpUnderOpOver OpOverscoreOp ##  #_ ##  #_ += +=     (2) As a better way to combine different judgments we suggest following Le Calv & Savoy (2000) and normalize the scores using the logistic regression." ></td>
	<td class="line x" title="94:178	The logistic transformation (x) given by each logistic regression model is defined as: +  = = = + + k i ii k i ii x x e ex 10 10 1 )(   pi                   (3) where i are the coefficients obtained from the fitting, xi are the variables, and k is the number of explanatory variables." ></td>
	<td class="line x" title="95:178	These coefficients reflect the relative importance of each variable in the final score." ></td>
	<td class="line x" title="96:178	For each sentence, we can compute the (x) corresponding to the two possible categories and the final decision is simply to classify the sentence according to the max (x) value." ></td>
	<td class="line x" title="97:178	This approach takes account of the fact that some explanatory variables may have more importance than other in assigning the correct category." ></td>
	<td class="line x" title="98:178	41 3.2 Nave Bayes For comparison with our logistic model we chose three baselines: Nave Bayes and language model and finding significant collocations." ></td>
	<td class="line x" title="99:178	Despite its simplicity Nave Bayes classifier tends to perform relatively well for various text categorization problems (Witten, Frank, 2005)." ></td>
	<td class="line x" title="100:178	In accordance with our approach, we used word tokens as classification features for the English corpora." ></td>
	<td class="line x" title="101:178	For the Chinese and Japanese languages overlapping bigram approach was used (Savoy, 2005)." ></td>
	<td class="line x" title="102:178	The training method estimates the relative frequency of the probability that the chosen feature belongs to a specific category using add-one smoothing technique." ></td>
	<td class="line x" title="103:178	3.3 Language Model (LM) As a second baseline we use the classification based on the language model using overlapping ngram sequences (n was set to 8) as suggested by Pang & Lee (2004, 2005) for the English language." ></td>
	<td class="line x" title="104:178	Using the overlapping 4-gram sequence for the word company, we obtain: comp, ompa, mpan, etc. For the Chinese and Japanese corpora bigram approach was applied." ></td>
	<td class="line x" title="105:178	As in Nave Bayes, the language model gives the probability of the sentence belonging to a specific class." ></td>
	<td class="line x" title="106:178	Working with relatively large n allows a lot of word tokens to be processed as is, at least for the English language." ></td>
	<td class="line x" title="107:178	3.4 Significant Collocations (SC) Another promising approach among the supervised learning schemes is the use of collocations of two or more words or features (Manning & Schtze, 2000)." ></td>
	<td class="line x" title="108:178	This method allows classification of instances based on significant collocations learned from the labeled data." ></td>
	<td class="line x" title="109:178	Some examples of the frequent collocations in the corpora would be in the, of the." ></td>
	<td class="line x" title="110:178	The idea of the method is to find significant collocations (SC) that occur more in the opinionated corpus than in the non-opinionated one." ></td>
	<td class="line x" title="111:178	In order to do so the model returns the collocations of two words for the English language based on the degree to which their counts in the opinionated corpus exceed their expected counts in the not opinionated one." ></td>
	<td class="line x" title="112:178	As an example for the English opinionated corpus the following collocations were found: are worried, pleaded guilty, eager to, expressed hope." ></td>
	<td class="line x" title="113:178	Clearly, overlooking the list of new found collocations it is possible to judge their relevancy." ></td>
	<td class="line x" title="114:178	However, it is not clear how to use this method with the Chinese and Japanese texts, since these languages do not have white space or other usual delimiters as in English." ></td>
	<td class="line x" title="115:178	In order to solve the problem of feature selection we chose  bigram  indexing  on  the  Chinese  and Japanese corpora and searched for significant new collocations of bigrams." ></td>
	<td class="line x" title="116:178	4 Language Dependent Approach As the language dependent technique to improve the obtained classification results we suggest the use of SentiWordNet for the English language (Esuli & Sebastiani, 2006)." ></td>
	<td class="line x" title="117:178	Since the vocabulary of words in SentiWordNet is quite limited it is not always clear how to combine the objectivity scores." ></td>
	<td class="line x" title="118:178	The SentiWordNet score was computed in the following way: to define the opinionated score of the sentence the sum of scores representing that the word belongs to opinionated category for each word in the sentence is calculated." ></td>
	<td class="line x" title="119:178	The not opinionated score of the sentence is computed in the same way with the difference that it is divided by the number of words in the sentence." ></td>
	<td class="line x" title="120:178	Thus, if opinionated score is more than not opinionated one, there is an opinion, otherwise not." ></td>
	<td class="line x" title="121:178	This is a heuristic approach that intuitively takes account of the rationalization that there are more not opinionated words than opinionated in the sentence." ></td>
	<td class="line x" title="122:178	At the same time the presence of opinionated word weighs more than the presence of the not opinionated ones." ></td>
	<td class="line x" title="123:178	Especially, this approach seems to give good result." ></td>
	<td class="line x" title="124:178	5 Experiments The experiment was carried out on the NTCIR-6 and NTCIR-7 English news corpora using 10-fold cross-validation model  on a lenient evaluation standard as described in Seki et al. We do not question the construction and structure of opinions in this data set, since those questions were addressed  at the NTCIR workshops." ></td>
	<td class="line x" title="125:178	Using the Chinese and Japanese corpora we can verify the quality of the suggested language-independent approaches." ></td>
	<td class="line x" title="126:178	42 5.1 Feature Selection & Evaluation in English For the evaluation of sentences in English, the assumption of isolated words (bag-of-words) previously stemmed was used by our system." ></td>
	<td class="line x" title="127:178	The corpora are comprised of more than 13,400 sentences, 4,859 (36.3%) of which are opinionated." ></td>
	<td class="line x" title="128:178	As the evaluation metrics precision, recall and F1-measure were used based on gold standard evaluation provided by NTCIR workshops (Seki et al., 2008)." ></td>
	<td class="line x" title="129:178	The precision and recall are weighted equally in our experiment but it should be recognized that based on the system's needs and focus there could be more accent on precision or recall." ></td>
	<td class="line x" title="130:178	Model Precision Recall F1-measure Logistic model 0.583 0.508 0.543 Nave Bayes 0.415 0.364 0.388 LM 0.350 0.339 0.343 SC 0.979 0.360 0.527 Table 2." ></td>
	<td class="line x" title="131:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 English  corpora." ></td>
	<td class="line x" title="132:178	Comparing the results in Table 2 to the baselines of the Nave Bayes classifier and LM evaluated on the same training and testing sets, we see that logistic model outperforms the baselines." ></td>
	<td class="line x" title="133:178	In our opinion, this is due to the use of more explanatory variables that better discriminate between opinionated and factual sentences." ></td>
	<td class="line x" title="134:178	The use of language dependent techniques on the other hand might further improve the results." ></td>
	<td class="line x" title="135:178	Especially, this seems promising observing the results when using the SentiWordNet on the English corpus." ></td>
	<td class="line x" title="136:178	In Table 3 one can see that the first three models show improvement." ></td>
	<td class="line x" title="137:178	Specifically, the precision of the logistic model increased from 0.583 to 0.766 (by 31.4%)." ></td>
	<td class="line x" title="138:178	Model Precision Recall F1-measure Logistic model 0.766 0.488 0.597 Nave Bayes 0.667 0.486 0.562 LM 0.611 0.474 0.534 SC 0.979 0.420 0.588 Table 3." ></td>
	<td class="line x" title="139:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 English corpora with SentiWordNet." ></td>
	<td class="line x" title="140:178	When considering the F1-measure, the impact of the language-dependent approach shows 9% of improvement, from 0.543 to 0.597." ></td>
	<td class="line x" title="141:178	The way that we incorporated the scores provided by SentiWordNet was done with the help of linear combination and normalization of scores for each of the models." ></td>
	<td class="line x" title="142:178	5.2 Feature Selection & Evaluation in Chinese We have assumed until now that words can be extracted from a sentence in order to define the needed features used to determine if the underlying information item conveys an opinion or not." ></td>
	<td class="line x" title="143:178	Working with the Chinese language this assumption does no longer hold." ></td>
	<td class="line x" title="144:178	Therefore, we need to determine indexing units by either applying an automating segmentation approach (based on either a morphological  (e.g.,  CSeg&Tag)  or  a  statistical method (Murata & Isahara, 2003)) or considering n-gram indexing approach (unigram or bigram, for example)." ></td>
	<td class="line x" title="145:178	Finally we may also consider a combination of both n-gram and word-based indexing strategies." ></td>
	<td class="line x" title="146:178	Based on the work of Savoy, 2005 we experimented with overlapping bigram and trigram indexing schemes for Chinese." ></td>
	<td class="line x" title="147:178	The experimental results show that bigram indexing outperforms trigram on all three considered statistical methods." ></td>
	<td class="line x" title="148:178	Therefore, as features for Chinese we used overlapping bigrams." ></td>
	<td class="line x" title="149:178	The NTCIR-6 and NTCIR-7 Chinese corpora consisted of more than 14,507 sentences, 9960 (68.7%) of which are opinionated." ></td>
	<td class="line x" title="150:178	The results of all three statistical models performed on the Chinese corpora are presented in Table 4." ></td>
	<td class="line x" title="151:178	Model Precision Recall F1-measure Logistic model 0.943 0.730 0.823 Nave Bayes 0.729 0.538 0.619 LM 0.581 0.634 0.606 SC 0.313 0.898 0.464 Table 4." ></td>
	<td class="line x" title="152:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 Chinese corpora." ></td>
	<td class="line x" title="153:178	From the results in Table 4 we clearly see that our approach gives better performance and confirms the results presented in Tables 2 and 3." ></td>
	<td class="line x" title="154:178	The significant improvement in scores could be due to the fact that Chinese corpus contains more opinionated sentences in relevance to not opinionated once." ></td>
	<td class="line x" title="155:178	Thus, the training set for opinionated classi43 fication was much larger compared to the English language." ></td>
	<td class="line x" title="156:178	This proves the relevance of more training data for the learning-based systems." ></td>
	<td class="line x" title="157:178	But the direct comparison with the results on the English corpus is not possible." ></td>
	<td class="line x" title="158:178	5.3 Feature Selection & Evaluation in Japanese As with the Chinese language we face the same challenges in feature definition for the Japanese language." ></td>
	<td class="line x" title="159:178	After experimenting with bigram and trigram we chose bigram strategy for indexing and feature selection." ></td>
	<td class="line x" title="160:178	The NTCIR-6 and NTCIR-7 Japanese corpora consisted of more than 11,100 sentences with 4,622 opinionated sentences (representing 41.6% of the corpus)." ></td>
	<td class="line x" title="161:178	The results of the statistical models are shown in Table 5." ></td>
	<td class="line x" title="162:178	Model Precision Recall F-measure Logistic model 0.527 0.761 0.623 Nave Bayes 0.565 0.570 0.567 LM 0.657 0.667 0.662 SC 0.663 0.856 0.747 Table 5." ></td>
	<td class="line x" title="163:178	Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 Japanese corpora." ></td>
	<td class="line x" title="164:178	From the results we can see that the significant collocations model outperforms the others." ></td>
	<td class="line x" title="165:178	This could be due to the fewer number of opinionated sentences compared to the Chinese or English corpora." ></td>
	<td class="line x" title="166:178	This tends to indicate the necessity of an extensive training data for the logistic model in order to provide reliable opinion estimates." ></td>
	<td class="line x" title="167:178	6 Future Work and Conclusion In this paper we presented our language-independent approach based on using Z scores and the logistic model  to identify those terms that adequately characterize subsets of the corpus belonging to opinionated or non-opinionated classes." ></td>
	<td class="line x" title="168:178	In this selection, we focused only on the statistical aspect (distribution difference) of words or bigrams." ></td>
	<td class="line x" title="169:178	Our approach was compared to the three baselines, namely Nave Bayes classifier, language model and an approach based on finding significant collocations." ></td>
	<td class="line x" title="170:178	We have also demonstrated on the English corpora how we can use the language dependent techniques to identify the possibility of opinion expressed in the sentences that otherwise were classified as not opinionated by the system." ></td>
	<td class="line x" title="171:178	The use of SentiWordNet (Esuli & Sebastiani, 2006) in combination with our methods yields better results for the English language." ></td>
	<td class="line x" title="172:178	This study was limited to isolated words in English corpus but in further research we could easily consider longer word sequences to include both noun and verb phrases." ></td>
	<td class="line x" title="173:178	The most useful terms would also then be added to the query to improve the rank of opinionated documents." ></td>
	<td class="line x" title="174:178	As another approach, we could use the evaluation of co-occurrence terms of pronouns I and you mainly with verbs (e.g., believe, feel, think, hate) using part of speech tagging techniques in order to boost the rank of retrieved items." ></td>
	<td class="line x" title="175:178	Using freely available POS taggers, we could take POS information into account (Toutanova & Mannning, 2004) and hopefully develop a better classifier." ></td>
	<td class="line x" title="176:178	For example, the presence of proper names and their frequency or distribution might help us classify a document as being opinionated or not." ></td>
	<td class="line x" title="177:178	The presence of adjectives and adverbs, together with their superlative (e.g., best, most) or comparative (e.g., greater, more) forms could also be useful hints regarding the presence of opinionated versus factual information." ></td>
	<td class="line x" title="178:178	Acknowledgments We would like to thank the MOAT task organizers at NTCIR-7 for their valuable work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1703
Corpus-based Semantic Lexicon Induction with Web-based Corroboration
Igo, Sean;Riloff, Ellen;"></td>
	<td class="line x" title="1:188	Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1826, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:188	c 2009 Association for Computational Linguistics Corpus-based Semantic Lexicon Induction with Web-based Corroboration Sean P. Igo Center for High Performance Computing University of Utah Salt Lake City, UT 84112 USA Sean.Igo@utah.edu Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112 USA riloff@cs.utah.edu Abstract Various techniques have been developed to automatically induce semantic dictionaries from text corpora and from the Web." ></td>
	<td class="line x" title="3:188	Our research combines corpus-based semantic lexicon induction with statistics acquired from the Web to improve the accuracy of automatically acquired domain-specific dictionaries." ></td>
	<td class="line x" title="4:188	We use a weakly supervised bootstrapping algorithm to induce a semantic lexicon from a text corpus, and then issue Web queries to generate co-occurrence statistics between each lexicon entry and semantically related terms." ></td>
	<td class="line x" title="5:188	The Web statistics provide a source of independent evidence to confirm, or disconfirm, that a word belongs to the intended semantic category." ></td>
	<td class="line x" title="6:188	We evaluate this approach on 7 semantic categories representing two domains." ></td>
	<td class="line x" title="7:188	Our results show that the Web statistics dramatically improve the ranking of lexicon entries, and can also be used to filter incorrect entries." ></td>
	<td class="line x" title="8:188	1 Introduction Semantic resources are extremely valuable for many natural language processing (NLP) tasks, as evidenced by the wide popularity of WordNet (Miller, 1990) and a multitude of efforts to create similar WordNets for additional languages (e.g.(Atserias et al., 1997; Vossen, 1998; Stamou et al., 2002))." ></td>
	<td class="line x" title="10:188	Semantic resources can take many forms, but one of the most basic types is a dictionary that associates a word (or word sense) with one or more semantic categories (hypernyms)." ></td>
	<td class="line x" title="11:188	For example, truck might be identified as a VEHICLE, and dog might be identified as an ANIMAL." ></td>
	<td class="line x" title="12:188	Automated methods for generating such dictionaries have been developed under the rubrics of lexical acquisition, hyponym learning, semantic class induction, and Web-based information extraction." ></td>
	<td class="line x" title="13:188	These techniques can be used to rapidly create semantic lexicons for new domains and languages, and to automatically increase the coverage of existing resources." ></td>
	<td class="line x" title="14:188	Techniques for semantic lexicon induction can be subdivided into two groups: corpus-based methods and Web-based methods." ></td>
	<td class="line x" title="15:188	Although the Web can be viewed as a (gigantic) corpus, these two approaches tend to have different goals." ></td>
	<td class="line x" title="16:188	Corpus-based methods are typically designed to induce domain-specific semantic lexicons from a collection of domain-specific texts." ></td>
	<td class="line x" title="17:188	In contrast, Web-based methods are typically designed to induce broad-coverage resources, similar to WordNet." ></td>
	<td class="line x" title="18:188	Ideally, one would hope that broadcoverage resources would be sufficient for any domain, but this is often not the case." ></td>
	<td class="line x" title="19:188	Many domains use specialized vocabularies and jargon that are not adequately represented in broad-coverage resources (e.g., medicine, genomics, etc.)." ></td>
	<td class="line x" title="20:188	Furthermore, even relatively general text genres, such as news, contain subdomains that require extensive knowledge of specific semantic categories." ></td>
	<td class="line x" title="21:188	For example, our work uses a corpus of news articles about terrorism that includes many arcane weapon terms (e.g., M-79, AR-15, an-fo, and gelignite)." ></td>
	<td class="line x" title="22:188	Similarly, our disease-related documents mention obscure diseases (e.g., psittacosis) and contain many informal terms, abbreviations, and spelling variants that do not even occur in most medical dictionaries." ></td>
	<td class="line x" title="23:188	For example, yf refers to yellow fever, tularaemia is an alternative spelling for tularemia, and nv-cjd is frequently used 18 to refer to new variant Creutzfeldt Jacob Disease." ></td>
	<td class="line x" title="24:188	The Web is such a vast repository of knowledge that specialized terminology for nearly any domain probably exists in some niche or cranny, but finding the appropriate corner of the Web to tap into is a challenge." ></td>
	<td class="line x" title="25:188	You have to know where to look to find specialized knowledge." ></td>
	<td class="line x" title="26:188	In contrast, corpus-based methods can learn specialized terminology directly from a domain-specific corpus, but accuracy can be a problem because most corpora are relatively small." ></td>
	<td class="line x" title="27:188	In this paper, we seek to exploit the best of both worlds by combining a weakly supervised corpusbased method for semantic lexicon induction with statistics obtained from the Web." ></td>
	<td class="line x" title="28:188	First, we use a bootstrapping algorithm, Basilisk (Thelen and Riloff, 2002), to automatically induce a semantic lexicon from a domain-specific corpus." ></td>
	<td class="line x" title="29:188	This produces a set of words that are hypothesized to belong to the targeted semantic category." ></td>
	<td class="line x" title="30:188	Second, we use the Web as a source of corroborating evidence to confirm, or disconfirm, whether each term truly belongs to the semantic category." ></td>
	<td class="line x" title="31:188	For each candidate word, we search the Web for pages that contain both the word and a semantically related term." ></td>
	<td class="line x" title="32:188	We expect that true semantic category members will co-occur with semantically similar words more often than non-members." ></td>
	<td class="line x" title="33:188	This paper is organized as follows." ></td>
	<td class="line x" title="34:188	Section 2 discusses prior work on weakly supervised methods for semantic lexicon induction." ></td>
	<td class="line x" title="35:188	Section 3 overviews our approach: we briefly describe the weakly supervised bootstrapping algorithm that we use for corpus-based semantic lexicon induction, and then present our procedure for gathering corroborating evidence from the Web." ></td>
	<td class="line x" title="36:188	Section 4 presents experimental results on seven semantic categories representing two domains: Latin American terrorism and disease-related documents." ></td>
	<td class="line x" title="37:188	Section 5 summarizes our results and discusses future work." ></td>
	<td class="line x" title="38:188	2 Related Work Our research focuses on semantic lexicon induction, where the goal is to create a list of words that belong to a desired semantic class." ></td>
	<td class="line x" title="39:188	A substantial amount of previous work has been done on weakly supervised and unsupervised creation of semantic lexicons." ></td>
	<td class="line x" title="40:188	Weakly supervised corpus-based methods have utilized noun co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Widdows and Dorow, 2002; Phillips and Riloff, 2002; Pantel and Ravichandran, 2004; Tanev and Magnini, 2006), and lexico-syntactic contextual patterns (e.g., resides in <location> or moved to <location>) (Riloff and Jones, 1999; Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="41:188	Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1, although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the Web." ></td>
	<td class="line x" title="42:188	The goal of our work is to improve upon corpus-based bootstrapping algorithms by using cooccurrence statistics obtained from the Web to rerank and filter the hypothesized category members." ></td>
	<td class="line x" title="43:188	Techniques for semantic class learning have also been developed specifically for the Web." ></td>
	<td class="line x" title="44:188	Several Web-based semantic class learners build upon Hearsts early work (Hearst, 1992) with hyponym patterns." ></td>
	<td class="line x" title="45:188	Hearst exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., such authors as <X>) to automatically acquire new hyponyms." ></td>
	<td class="line x" title="46:188	(Pasca, 2004) applied hyponym patterns to the Web and learned semantic class instances and groups by acquiring contexts around the patterns." ></td>
	<td class="line x" title="47:188	Later, (Pasca, 2007) created context vectors for a group of seed instances by searching Web query logs, and used them to learn similar instances." ></td>
	<td class="line x" title="48:188	The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the Web and evaluates them further by computing mutual information scores based on Web queries." ></td>
	<td class="line x" title="49:188	(Kozareva et al., 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns." ></td>
	<td class="line oc" title="50:188	Our work builds upon Turneys work on semantic orientation (Turney, 2002) and synonym learning (Turney, 2001), in which he used a PMI-IR algorithm to measure the similarity of words and phrases based on Web queries." ></td>
	<td class="line o" title="51:188	We use a similar PMI (pointwise mutual information) metric for the purposes of semantic class verification." ></td>
	<td class="line x" title="52:188	There has also been work on fully unsupervised 1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages." ></td>
	<td class="line x" title="53:188	19 semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user." ></td>
	<td class="line x" title="54:188	Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002))." ></td>
	<td class="line x" title="55:188	3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web." ></td>
	<td class="line x" title="56:188	In this section, we describe both of these components." ></td>
	<td class="line x" title="57:188	First, we give a brief overview of the Basilisk bootstrapping algorithm that we use for corpus-based semantic lexicon induction." ></td>
	<td class="line x" title="58:188	Second, we present our new strategies for acquiring and utilizing corroborating statistical evidence from the Web." ></td>
	<td class="line x" title="59:188	3.1 Corpus-based Semantic Lexicon Induction via Bootstrapping For corpus-based semantic lexicon induction, we use a weakly supervised bootstrapping algorithm called Basilisk (Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="60:188	As input, Basilisk requires a small set of seed words for each semantic category, and a collection of (unannotated) texts." ></td>
	<td class="line x" title="61:188	Basilisk iteratively generates new words that are hypothesized to belong to the same semantic class as the seeds." ></td>
	<td class="line x" title="62:188	Here we give an overview of Basilisks algorithm and refer the reader to (Thelen and Riloff, 2002) for more details." ></td>
	<td class="line x" title="63:188	The key idea behind Basilisk is to use pattern contexts around a word to identify its semantic class." ></td>
	<td class="line x" title="64:188	Basilisks bootstrapping process has two main steps: Pattern Pool Creation and Candidate Word Selection." ></td>
	<td class="line x" title="65:188	First, Basilisk applies the AutoSlog pattern generator (Riloff, 1996) to create a set of lexicosyntactic patterns that, collectively, can extract every noun phrase in the corpus." ></td>
	<td class="line x" title="66:188	Basilisk then ranks the patterns according to how often they extract the seed words, under the assumption that patterns which extract known category members are likely to extract other category members as well." ></td>
	<td class="line x" title="67:188	The highest-ranked patterns are placed in a pattern pool." ></td>
	<td class="line x" title="68:188	Second, Basilisk gathers every noun phrase that is extracted by at least one pattern in the pattern pool, and designates each head noun as a candidate for the semantic category." ></td>
	<td class="line x" title="69:188	The candidates are then scored and ranked." ></td>
	<td class="line x" title="70:188	For each candidate, Basilisk collects all of the patterns that extracted that word, computes the logarithm of the number of seeds extracted by each of those patterns, and finally computes the average of these log values as the score for the candidate." ></td>
	<td class="line x" title="71:188	Intuitively, a candidate word receives a high score if it was extracted by patterns that, on average, also extract many known category members." ></td>
	<td class="line x" title="72:188	The N highest ranked candidates are automatically added to the list of seed words, taking a leap of faith that they are true members of the semantic category." ></td>
	<td class="line x" title="73:188	The bootstrapping process then repeats, using the larger set of seed words as known category members in the next iteration." ></td>
	<td class="line x" title="74:188	Basilisk learns many good category members, but its accuracy varies a lot across semantic categories (Thelen and Riloff, 2002)." ></td>
	<td class="line x" title="75:188	One problem with Basilisk, and bootstrapping algorithms in general, is that accuracy tends to deteriorate as bootstrapping progresses." ></td>
	<td class="line x" title="76:188	Basilisk generates candidates by identifying the contexts in which they occur and words unrelated to the desired category can sometimes also occur in those contexts." ></td>
	<td class="line x" title="77:188	Some patterns consistently extract members of several semantic classes; for example, attack on <NP> will extract both people (attack on the president) and buildings (attack on the U.S. embassy)." ></td>
	<td class="line x" title="78:188	Idiomatic expressions and parsing errors can also lead to undesirable words being learned." ></td>
	<td class="line x" title="79:188	Incorrect words tend to accumulate as bootstrapping progresses, which can lead to gradually deteriorating performance." ></td>
	<td class="line x" title="80:188	(Thelen and Riloff, 2002) tried to address this problem by learning multiple semantic categories simultaneously." ></td>
	<td class="line x" title="81:188	This helps to keep the bootstrapping focused by flagging words that are potentially problematic because they are strongly associated with a competing category." ></td>
	<td class="line x" title="82:188	This improved Basilisks accuracy, but by a relatively small amount, and this approach depends on the often unrealistic assumption that a word cannot belong to more than one semantic category." ></td>
	<td class="line x" title="83:188	In our work, we use the single-category version of Basilisk that learns each semantic category independently so that we do not need to make 20 this assumption." ></td>
	<td class="line x" title="84:188	3.2 Web-based Semantic Class Corroboration The novel aspect of our work is that we introduce a new mechanism to independently verify each candidate words category membership using the Web as an external knowledge source." ></td>
	<td class="line x" title="85:188	We gather statistics from the Web to provide evidence for (or against) the semantic class of a word in a manner completely independent of Basilisks criteria." ></td>
	<td class="line x" title="86:188	Our approach is based on the distributional hypothesis (Harris, 1954), which says that words that occur in the same contexts tend to have similar meanings." ></td>
	<td class="line x" title="87:188	We seek to corroborate a words semantic class through statistics that measure how often the word co-occurs with semantically related words." ></td>
	<td class="line x" title="88:188	For each candidate word produced by Basilisk, we construct a Web query that pairs the word with a semantically related word." ></td>
	<td class="line x" title="89:188	Our goal is not just to find Web pages that contain both terms, but to find Web pages that contain both terms in close proximity to one another." ></td>
	<td class="line x" title="90:188	We consider two terms to be collocated if they occur within ten words of each other on the same Web page, which corresponds to the functionality of the NEAR operator used by the AltaVista search engine2." ></td>
	<td class="line oc" title="91:188	Turney (Turney, 2001; Turney, 2002) reported that the NEAR operator outperformed simple page co-occurrence for his purposes; our early experiments informally showed the same for this work." ></td>
	<td class="line x" title="92:188	We want our technique to remain weakly supervised, so we do not want to require additional human input or effort beyond what is already required for Basilisk." ></td>
	<td class="line x" title="93:188	With this in mind, we investigated two types of collocation relations as possible indicators of semantic class membership: Hypernym Collocation: We compute cooccurrence statistics between the candidate word and the name of the targeted semantic class (i.e., the words hypothesized hypernym)." ></td>
	<td class="line x" title="94:188	For example, given the candidate word jeep and the semantic category VEHICLE, we would issue the Web query jeep NEAR vehicle." ></td>
	<td class="line x" title="95:188	Our intuition is that such queries would identify definition-type Web hits." ></td>
	<td class="line x" title="96:188	For example, the query cow NEAR animal might retrieve snippets such as A cow is an animal found 2http://www.altavista.com on dairy farms or An animal such as a cow has." ></td>
	<td class="line x" title="97:188	Seed Collocation: We compute co-occurrence statistics between the candidate word and each seed word that was given to Basilisk as input." ></td>
	<td class="line x" title="98:188	For example, given the candidate word jeep and the seed word truck, we would issue the Web query jeep NEAR truck." ></td>
	<td class="line x" title="99:188	Here the intuition is that members of the same semantic category tend to occur near one another in lists, for example." ></td>
	<td class="line x" title="100:188	As a statistical measure of co-occurrence, we compute a variation of Pointwise Mutual Information (PMI), which is defined as: PMI(x,y) = log( p(x,y)p(x)p(y)) where p(x,y) is the probability that x and y are collocated (near each other) on a Web page, p(x) is the probability that x occurs on a Web page, and p(y) is the probability that y occurs on a Web page." ></td>
	<td class="line x" title="101:188	p(x) is calculated as count(x)N , where count(x) is the number of hits returned by AltaVista, searching for x by itself, and N is the total number of documents on the World Wide Web at the time the query is made." ></td>
	<td class="line x" title="102:188	Similarly, p(x,y) is count(x NEAR y)N . Given this, the PMI equation can be rewritten as: log(N) +log(count(x NEAR y)count(x)count(y)) N is not known, but it is the same for every query (assuming the queries were made at roughly the same time)." ></td>
	<td class="line x" title="103:188	We will use these scores solely to compare the relative goodness of candidates, so we can omit N from the equation because it will not change the relative ordering of the scores." ></td>
	<td class="line x" title="104:188	Thus, our PMI score3 for a candidate word and related term (hypernym or seed) is: log(count(x NEAR y)count(x)count(y)) Finally, we created three different scoring functions that use PMI values in different ways to capture different types of co-occurrence information: Hypernym Score: PMI based on collocation between the hypernym term and candidate word." ></td>
	<td class="line x" title="105:188	3In the rare cases when a term had a zero hit count, we assigned -99999 as the PMI score, which effectively ranks it at the bottom." ></td>
	<td class="line x" title="106:188	21 Average of Seeds Score: The mean of the PMI scores computed for the candidate and each seed word: 1 |seeds| |seeds|summationdisplay i=1 PMI(candidate,seedi) Max of Seeds Score: The maximum (highest) of the PMI scores computed for the candidate and each seed word." ></td>
	<td class="line x" title="107:188	The rationale for the Average of Seeds Score is that the seeds are all members of the semantic category, so we might expect other members to occur near many of them." ></td>
	<td class="line x" title="108:188	Averaging over all of the seeds can diffuse unusually high or low collocation counts that might result from an anomalous seed." ></td>
	<td class="line x" title="109:188	The rationale for the Max of Seeds Score is that a word may naturally co-occur with some category members more often than others." ></td>
	<td class="line x" title="110:188	For example, one would expect dog to co-occur with cat much more frequently than with frog." ></td>
	<td class="line x" title="111:188	A high Max of Seeds Score indicates that there is at least one seed word that frequently co-occurs with the candidate." ></td>
	<td class="line x" title="112:188	Since Web queries are relatively expensive, it is worth taking stock of how many queries are necessary." ></td>
	<td class="line x" title="113:188	Let N be the number of candidate words produced by Basilisk, and S be the number of seed words given to Basilisk as input." ></td>
	<td class="line x" title="114:188	To compute the Hypernym Score for a candidate, we need 3 queries: count(hypernym), count(candidate), and count(hypernym NEAR candidate)." ></td>
	<td class="line x" title="115:188	The first query is the same for all candidates, so for N candidate words we need 2N + 1 queries in total." ></td>
	<td class="line x" title="116:188	To compute the Average or Max of Seeds Score for a candidate, we need S queries for count(seedi), S queries for count(seedi NEAR candidate), and 1 query for count(candidate)." ></td>
	<td class="line x" title="117:188	So for N candidate words we need N (2S + 1) queries." ></td>
	<td class="line x" title="118:188	S is typically small for weakly supervised algorithms (S=10 in our experiments), which means that this Web-based corroboration process requires O(N) queries to process a semantic lexicon of size N. 4 Evaluation 4.1 Data Sets We ran experiments on two corpora: 1700 MUC-4 terrorism articles (MUC-4 Proceedings, 1992) and a combination of 6000 disease-related documents, consisting of 2000 ProMed disease outbreak reports (ProMed-mail, 2006) and 4000 disease-related PubMed abstracts (PubMed, 2009)." ></td>
	<td class="line x" title="119:188	For the terrorism domain, we created lexicons for four semantic categories: BUILDING, HUMAN, LOCATION, and WEAPON." ></td>
	<td class="line x" title="120:188	For the disease domain, we created lexicons for three semantic categories: ANIMAL4, DISEASE, and SYMPTOM." ></td>
	<td class="line x" title="121:188	For each category, we gave Basilisk 10 seed words as input." ></td>
	<td class="line x" title="122:188	The seeds were chosen by applying a shallow parser to each corpus, extracting the head nouns of all the NPs, and sorting the nouns by frequency." ></td>
	<td class="line x" title="123:188	A human then walked down the sorted list and identified the 10 most frequent nouns that belong to each semantic category5." ></td>
	<td class="line x" title="124:188	This strategy ensures that the bootstrapping process is given seed words that occur in the corpus with high frequency." ></td>
	<td class="line x" title="125:188	The seed words are shown in Table 1." ></td>
	<td class="line x" title="126:188	BUILDING: embassy office headquarters church offices house home residence hospital airport HUMAN: people guerrillas members troops Cristiani rebels president terrorists soldiers leaders LOCATION: country El Salvador Salvador United States area Colombia city countries department Nicaragua WEAPON: weapons bomb bombs explosives arms missiles dynamite rifles materiel bullets ANIMAL: bird mosquito cow horse pig chicken sheep dog deer fish DISEASE: SARS BSE anthrax influenza WNV FMD encephalitis malaria pneumonia flu SYMPTOM: fever diarrhea vomiting rash paralysis weakness necrosis chills headaches hemorrhage Table 1: Seed Words To evaluate our results, we used the gold standard answer key that Thelen & Riloff created to evaluate Basilisk on the MUC4 corpus (Thelen and Riloff, 2002); they manually labeled every head noun in the corpus with its semantic class." ></td>
	<td class="line x" title="127:188	For the ProMed / PubMed disease corpus, we created our own answer key." ></td>
	<td class="line x" title="128:188	For all of the lexicon entries hypothesized by Basilisk, a human annotator (not any of the authors) 4ANIMAL was chosen because many of the ProMed disease outbreak stories concerned outbreaks among animal populations." ></td>
	<td class="line x" title="129:188	5The disease domain seed words were chosen from a larger set of ProMed documents, which included the 2000 used for lexicon induction." ></td>
	<td class="line x" title="130:188	22 BUILDING HUMAN LOCATION WEAPON N Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx 25 .40 .56 .52 .56 .40 .72 .80 .84 .68 .88 .88 1.0 .56 .84 1.0 1.0 50 .44 .56 .46 .40 .56 .80 .88 .86 .80 .86 .84 .98 .52 .74 .76 .90 75 .44 .45 .41 .39 .65 .84 .85 .85 .80 .88 .80 .99 .52 .63 .65 .79 100 .42 .41 .38 .36 .69 .81 .80 .87 .81 .85 .78 .95 .55 .55 .56 .63 300 .22 .82 .75 .26 ANIMAL DISEASE SYMPTOM N Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx 25 .48 .88 .92 .92 .64 .84 .80 .84 .64 .84 .92 .80 50 .58 .82 .84 .80 .72 .84 .60 .82 .62 .76 .90 .74 75 .55 .68 .67 .69 .69 .83 .59 .81 .61 .68 .79 .71 100 .45 .55 .54 .57 .69 .78 .58 .80 .59 .71 .77 .64 300 .20 .62 .38 Table 2: Ranking results for 7 semantic categories, showing accuracies for the top-ranked N words." ></td>
	<td class="line x" title="131:188	(Ba=Basilisk, Hy=Hypernym Re-ranking, Av=Average of Seeds Re-ranking, Mx=Max of Seeds Re-ranking labeled each word as either correct or incorrect for the hypothesized semantic class." ></td>
	<td class="line x" title="132:188	A word is considered to be correct if any sense of the word is semantically correct." ></td>
	<td class="line x" title="133:188	4.2 Ranking Results We ran Basilisk for 60 iterations, learning 5 new words in each bootstrapping cycle, which produced a lexicon of 300 words for each semantic category." ></td>
	<td class="line x" title="134:188	The columns labeled Ba in Table 2 show the accuracy results for Basilisk.6 As we explained in Section 3.1, accuracy tends to decrease as bootstrapping progresses, so we computed accuracy scores for the top-ranked 100 words, in increments of 25, and also for the entire lexicon of 300 words." ></td>
	<td class="line x" title="135:188	Overall, we see that Basilisk learns many correct words for each semantic category, and the topranked terms are generally more accurate than the lower-ranked terms." ></td>
	<td class="line x" title="136:188	For the top 100 words, accuracies are generally in the 50-70% range, except for LOCATION which achieves about 80% accuracy." ></td>
	<td class="line x" title="137:188	For the HUMAN category, Basilisk obtained 82% accuracy over all 300 words, but the top-ranked words actually produced lower accuracy." ></td>
	<td class="line x" title="138:188	Basilisks ranking is clearly not as good as it could be because there are correct terms co-mingled with incorrect terms throughout the ranked lists." ></td>
	<td class="line x" title="139:188	This has 6These results are not comparable to the Basilisk results reported by (Thelen and Riloff, 2002) because our implementation only does single-category learning while the results in that paper are based on simultaneously learning multiple categories." ></td>
	<td class="line x" title="140:188	two ramifications." ></td>
	<td class="line x" title="141:188	First, if we want a human to manually review each lexicon before adding the words to an external resource, then the rankings may not be very helpful (i.e., the human will need to review all of the words), and (2) incorrect terms generated during the early stages of bootstrapping may be hindering the learning process because they introduce noise during bootstrapping." ></td>
	<td class="line x" title="142:188	The HUMAN category seems to have recovered from early mistakes, but the lower accuracies for some other categories may be the result of this problem." ></td>
	<td class="line x" title="143:188	The purpose of our Web-based corroboration process is to automatically re-evaluate the lexicons produced by Basilisk, using Web-based statistics to create more separation between the good entries and the bad ones." ></td>
	<td class="line x" title="144:188	Our first set of experiments uses the Web-based co-occurrence statistics to re-rank the lexicon entries." ></td>
	<td class="line x" title="145:188	The Hy, Av, and Mx columns in Table 2 show the re-ranking results using each of the Hypernym, Average of Seeds, and Maximum of Seeds scoring functions." ></td>
	<td class="line x" title="146:188	In all cases, Web-based re-ranking outperforms Basilisks original rankings." ></td>
	<td class="line x" title="147:188	Every semantic category except for BUILDING yielded accuracies of 80-100% among the top candidates." ></td>
	<td class="line x" title="148:188	For each row, the highest accuracy for each semantic category is shown in boldface (as are any tied for highest)." ></td>
	<td class="line x" title="149:188	Overall, the Max of Seeds Scores were best, performing better than or as well as the other scoring functions on 5 of the 7 categories." ></td>
	<td class="line x" title="150:188	It was only out23 BUILDING HUMAN LOCATION WEAPON ANIMAL DISEASE SYMPTOM consulate guerrilla San Salvador shotguns bird-to-bird meningo-encephalitis nausea pharmacies extremists Las Hojas carbines cervids bse).austria diarrhoea aiport sympathizers Tejutepeque armaments goats inhalational myalgias zacamil assassins Ayutuxtepeque revolvers ewes anthrax disease chlorosis airports patrols Copinol detonators ruminants otitis media myalgia parishes militiamen Cuscatancingo pistols swine airport malaria salivation Masariegos battalion Jiboa car bombs calf taeniorhynchus dysentery chancery Ellacuria Chinameca calibers lambs hyopneumonia cramping residences rebel Zacamil M-16 wolsington monkeypox dizziness police station policemen Chalantenango grenades piglets kala-azar inappetance Table 3: Top 10 words ranked by Max of Seeds Scores." ></td>
	<td class="line x" title="151:188	performed once by the Hypernym Scores (BUILDING) and once by the Average of Seeds Scores (SYMPTOM)." ></td>
	<td class="line x" title="152:188	The strong performance of the Max of Seeds scores suggests that one seed is often an especially good collocation indicator for category membership  though it may not be the same seed word for all of the lexicon words." ></td>
	<td class="line x" title="153:188	The relatively poor performance of the Average of Seeds scores may be attributable to the same principle; perhaps even if one seed is especially strong, averaging over the less-effective seeds scores dilutes the results." ></td>
	<td class="line x" title="154:188	Averaging is also susceptible to damage from words that receive the special-case score of -99999 when a hit count is zero (see Section 3.2)." ></td>
	<td class="line x" title="155:188	Table 3 shows the 10 top-ranked candidates for each semantic category based on the Max of Seeds scores." ></td>
	<td class="line x" title="156:188	The table illustrates that this scoring function does a good job of identifying semantically correct words, although of course there are some mistakes." ></td>
	<td class="line x" title="157:188	Mistakes can happen due to parsing errors (e.g., bird-to-bird is an adjective and not a noun, as in bird-to-bird transmission), and some are due to issues associated with Web querying." ></td>
	<td class="line x" title="158:188	For example, the nonsense term bse).austria was ranked highly because Altavista split this term into 2 separate words because of the punctuation, and bse by itself is indeed a disease term (bovine spongiform encephalitis)." ></td>
	<td class="line x" title="159:188	4.3 Filtering Results Table 2 revealed that the 300-word lexicons produced by Basilisk vary widely in the number of true category words that they contain." ></td>
	<td class="line x" title="160:188	The least dense category is ANIMAL, with only 61 correct words, and the most dense is HUMAN with 247 correct words." ></td>
	<td class="line x" title="161:188	Interestingly, the densest categories are not always the easiest to rank." ></td>
	<td class="line x" title="162:188	For example, the HUMAN category is the densest category but Basilisks ranking of the human terms was poor." ></td>
	<td class="line x" title="163:188	 Category Acc Cor/Tot -22 WEAPON .88 46/52 LOCATION .98 59/60 HUMAN .80 8/10 BUILDING .83 5/6 ANIMAL .91 30/33 DISEASE .82 64/78 SYMPTOM .65 64/99 -23 WEAPON .79 59/75 LOCATION .96 82/85 HUMAN .85 23/27 BUILDING .71 12/17 ANIMAL .87 40/46 DISEASE .78 82/105 SYMPTOM .62 86/139 -24 WEAPON .63 63/100 LOCATION .93 111/120 HUMAN .87 54/62 BUILDING .45 17/38 ANIMAL .75 47/63 DISEASE .74 94/127 SYMPTOM .60 100/166 Table 4: Filtering results using the Max of Seeds Scores." ></td>
	<td class="line x" title="164:188	The ultimate goal behind a better ranking mechanism is to completely automate the process of semantic lexicon induction." ></td>
	<td class="line x" title="165:188	If we can produce highquality rankings, then we can discard the lower ranked words and keep only the highest ranked words for our semantic dictionary." ></td>
	<td class="line x" title="166:188	However, this 24 presupposes that we know where to draw the line between the good and bad entries, and Table 2 shows that this boundary varies across categories." ></td>
	<td class="line x" title="167:188	For HUMANS, the top 100 words are 87% accurate, and in fact we get 82% accuracy over all 300 words." ></td>
	<td class="line x" title="168:188	But for ANIMALS we achieve 80% accuracy only for the top 50 words." ></td>
	<td class="line x" title="169:188	It is paramount for semantic dictionaries to have high integrity, so accuracy must be high if we want to use the resulting lexicons without manual review." ></td>
	<td class="line x" title="170:188	As an alternative to ranking, another way that we could use the Web-based corroboration statistics is to automatically filter words that do not receive a high score." ></td>
	<td class="line x" title="171:188	The key question is whether the values of the scores are consistent enough across categories to set a single threshold that will work well across the different categories." ></td>
	<td class="line x" title="172:188	Table 4 shows the results of using the Max of Seeds Scores as a filtering mechanism: given a threshold , all words that have a score <  are discarded." ></td>
	<td class="line x" title="173:188	For each threshold value  and semantic category, we computed the accuracy (Acc) of the lexicon after all words with a score <  have been removed." ></td>
	<td class="line x" title="174:188	The Cor/Tot column shows the number of correct category members and the number of total words that passed the threshold." ></td>
	<td class="line x" title="175:188	We experimented with a variety of threshold values and found that =-22 performed best." ></td>
	<td class="line x" title="176:188	Table 4 shows that this threshold produces a relatively highprecision filtering mechanism, with 6 of the 7 categories achieving lexicon accuracies  80%." ></td>
	<td class="line x" title="177:188	As expected, the Cor/Tot column shows that the number of words varies widely across categories." ></td>
	<td class="line x" title="178:188	Automatic filtering represents a trade-off: a relatively high-precision lexicon can be created, but some correct words will be lost." ></td>
	<td class="line x" title="179:188	The threshold can be adjusted to increase the number of learned words, but with a corresponding drop in precision." ></td>
	<td class="line x" title="180:188	Depending upon a users needs, a high threshold may be desirable to identify only the most confident lexicon entries, or a lower threshold may be desirable to retain most of the correct entries while reliably removing some of the incorrect ones." ></td>
	<td class="line x" title="181:188	5 Conclusions We have demonstrated that co-occurrence statistics gathered from the Web can dramatically improve the ranking of lexicon entries produced by a weakly-supervised corpus-based bootstrapping algorithm, without requiring any additional supervision." ></td>
	<td class="line x" title="182:188	We found that computing Web-based cooccurrence statistics across a set of seed words and then using the highest score was the most successful approach." ></td>
	<td class="line x" title="183:188	Co-occurrence with a hypernym term also performed well for some categories, and could be easily combined with the Max of Seeds approach by choosing the highest value among the seeds as well as the hypernym." ></td>
	<td class="line x" title="184:188	In future work, we would like to incorporate this Web-based re-ranking procedure into the bootstrapping algorithm itself to dynamically clean up the learned words before they are cycled back into the bootstrapping process." ></td>
	<td class="line x" title="185:188	Basilisk could consult the Web-based statistics to select the best 5 words to generate before the next bootstrapping cycle begins." ></td>
	<td class="line x" title="186:188	This integrated approach has the potential to substantially improve Basilisks performance because it would improve the precision of the induced lexicon entries during the earliest stages of bootstrapping when the learning process is most fragile." ></td>
	<td class="line x" title="187:188	Acknowledgments Many thanks to Julia James for annotating the gold standards for the disease domain." ></td>
	<td class="line x" title="188:188	This research was supported in part by Department of Homeland Security Grant N0014-07-1-0152." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-1904
Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria
Hsueh, Pei-Yun;Melville, Prem;Sindhwani, Vikas;"></td>
	<td class="line x" title="1:182	Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 2735, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:182	c 2009 Association for Computational Linguistics Data Quality from Crowdsourcing: A Study of Annotation Selection Criteria Pei-Yun Hsueh, Prem Melville, Vikas Sindhwani IBM T.J. Watson Research Center 1101 Kitchawan Road, Route 134 Yorktown Heights, NY 10598, USA Abstract Annotation acquisition is an essential step in training supervised classifiers." ></td>
	<td class="line x" title="3:182	However, manual annotation is often time-consuming and expensive." ></td>
	<td class="line x" title="4:182	The possibility of recruiting annotators through Internet services (e.g., Amazon Mechanic Turk) is an appealing option that allows multiple labeling tasks to be outsourced in bulk, typically with low overall costs and fast completion rates." ></td>
	<td class="line x" title="5:182	In this paper, we consider the difficult problem of classifying sentiment in political blog snippets." ></td>
	<td class="line x" title="6:182	Annotation data from both expert annotators in a research lab and non-expert annotators recruited from the Internet are examined." ></td>
	<td class="line x" title="7:182	Three selection criteria are identified to select high-quality annotations: noise level, sentiment ambiguity, and lexical uncertainty." ></td>
	<td class="line x" title="8:182	Analysis confirm the utility of these criteria on improving data quality." ></td>
	<td class="line x" title="9:182	We conduct an empirical study to examine the effect of noisy annotations on the performance of sentiment classification models, and evaluate the utility of annotation selection on classification accuracy and efficiency." ></td>
	<td class="line x" title="10:182	1 Introduction Crowdsourcing (Howe, 2008) is an attractive solution to the problem of cheaply and quickly acquiring annotations for the purposes of constructing all kinds of predictive models." ></td>
	<td class="line x" title="11:182	To sense the potential of crowdsourcing, consider an observation in von Ahn et al.(2004): a crowd of 5,000 people playing an appropriately designed computer game 24 hours a day, could be made to label all images on Google (425,000,000 images in 2005) in a matter of just 31 days." ></td>
	<td class="line x" title="13:182	Several recent papers have studied the use of annotations obtained from Amazon Mechanical Turk, a marketplace for recruiting online workers (Su et al., 2007; Kaisser et al., 2008; Kittur et al., 2008; Sheng et al., 2008; Snow et al., 2008; Sorokin and Forsyth, 2008)." ></td>
	<td class="line x" title="14:182	With efficiency and cost-effectiveness, online recruitment of anonymous annotators brings a new set of issues to the table." ></td>
	<td class="line x" title="15:182	These workers are not usually specifically trained for annotation, and might not be highly invested in producing good-quality annotations." ></td>
	<td class="line x" title="16:182	Consequently, the obtained annotations may be noisy by nature, and might require additional validation or scrutiny." ></td>
	<td class="line x" title="17:182	Several interesting questions immediately arise in how to optimally utilize annotations in this setting: How does one handle differences among workers in terms of the quality of annotations they provide?" ></td>
	<td class="line x" title="18:182	How useful are noisy annotations for the end task of creating a model?" ></td>
	<td class="line x" title="19:182	Is it possible to identify genuinely ambiguous examples via annotator disagreements?" ></td>
	<td class="line x" title="20:182	How should these considerations be treated with respect to intrinsic informativeness of examples?" ></td>
	<td class="line x" title="21:182	These questions also hint at a strong connection to active learning, with annotation quality as a new dimension to the problem." ></td>
	<td class="line x" title="22:182	As a challenging empirical testbed for these issues, we consider the problem of sentiment classification on political blogs." ></td>
	<td class="line x" title="23:182	Given a snippet drawn from a political blog post, the desired output is a polarity score that indicates whether the sentiment expressed is positive or negative." ></td>
	<td class="line x" title="24:182	Such an analysis provides a view of the opinion around a subject of interest, e.g., US Presidential candidates, aggregated across the blogsphere." ></td>
	<td class="line x" title="25:182	Recently, sentiment analy27 sis is emerging as a critical methodology for social media analytics." ></td>
	<td class="line oc" title="26:182	Previous research has focused on classifying subjective-versus-objective expressions (Wiebe et al., 2004), and also on accurate sentiment polarity assignment (Turney, 2002; Yi et al., 2003; Pang and Lee, 2004; Sindhwani and Melville, 2008; Melville et al., 2009)." ></td>
	<td class="line p" title="27:182	The success of most prior work relies on the quality of their knowledge bases; either lexicons defining the sentiment polarity of words around a topic (Yi et al., 2003), or quality annotation data for statistical training." ></td>
	<td class="line x" title="28:182	While manual intervention for compiling lexicons has been significantly lessened by bootstrapping techniques (Yu and Hatzivassiloglou, 2003; Wiebe and Riloff, 2005), manual intervention in the annotation process is harder to avoid." ></td>
	<td class="line x" title="29:182	Moreover, the task of annotating blog-post snippets is challenging, particularly in a charged political atmosphere with complex discourse spanning many issues, use of cynicism and sarcasm, and highly domain-specific and contextual cues." ></td>
	<td class="line x" title="30:182	The downside is that high-performance models are generally difficult to construct, but the upside is that annotation and data-quality issues are more clearly exposed." ></td>
	<td class="line x" title="31:182	In this paper we aim to provide an empirical basis for the use of data selection criteria in the context of sentiment analysis in political blogs." ></td>
	<td class="line x" title="32:182	Specifically, we highlight the need for a set of criteria that can be applied to screen untrustworthy annotators and select informative yet unambiguous examples for the end goal of predictive modeling." ></td>
	<td class="line x" title="33:182	In Section 2, we first examine annotation data obtained by both the expert and non-expert annotators to quantify the impact of including non-experts." ></td>
	<td class="line x" title="34:182	Then, in Section 3, we quantify criteria that can be used to select annotators and examples for selective sampling." ></td>
	<td class="line x" title="35:182	Next, in Section 4, we address the questions of whether the noisy annotations are still useful for this task and study the effect of the different selection criteria on the performance of this task." ></td>
	<td class="line x" title="36:182	Finally, in Section 5 we present conclusion and future work." ></td>
	<td class="line x" title="37:182	2 Annotating Blog Sentiment This section introduces the Political Blog Snippet (PBS) corpus, describes our annotation procedure and the sources of noise, and gives an overview of the experiments on political snippet sentiments." ></td>
	<td class="line x" title="38:182	2.1 The Political Blog Snippet Corpus Our dataset comprises of a collection of snippets extracted from over 500,000 blog posts, spanning the activity of 16,741 political bloggers in the time period of Aug 15, 2008 to the election day Nov 4, 2008." ></td>
	<td class="line x" title="39:182	A snippet was defined as a window of text containing four consecutive sentences such that the head sentence contained either the term Obama or the term McCain, but both candidates were not mentioned in the same window." ></td>
	<td class="line x" title="40:182	The global discourse structure of a typical political blog post can be highly complicated with latent topics ranging from policies (e.g., financial situation, economics, the Iraq war) to personalities to voting preferences." ></td>
	<td class="line x" title="41:182	We therefore expected sentiment to be highly nonuniform over a blog post." ></td>
	<td class="line x" title="42:182	This snippetization procedure attempts to localize the text around a presidential candidate with the objective of better estimating aggregate sentiment around them." ></td>
	<td class="line x" title="43:182	In all, we extracted 631,224 snippets." ></td>
	<td class="line x" title="44:182	For learning classifiers, we passed the snippets through a stopword filter, pruned all words that occur in less than 3 snippets and created normalized term-frequency feature vectors over a vocabulary of 3,812 words." ></td>
	<td class="line x" title="45:182	2.2 Annotation Procedure The annotation process consists of two steps: Sentiment-class annotation: In the first step, as we are only interested in detecting sentiments related to the named candidate, the annotators were first asked to mark up the snippets irrelevant to the named candidates election campaign." ></td>
	<td class="line x" title="46:182	Then, the annotators were instructed to tag each relevant snippet with one of the following four sentiment polarity labels: Positive, Negative, Both, or Neutral." ></td>
	<td class="line x" title="47:182	Alignment annotation: In the second step, the annotators were instructed to mark up whether each snippet was written to support or oppose the target candidate therein named." ></td>
	<td class="line x" title="48:182	The motivation of adding this tag comes from our interest in building a classification system to detect positive and negative mentions of each candidate." ></td>
	<td class="line x" title="49:182	For the snippets that do not contain a clear political alignment, the annotators had the freedom to mark it as neutral or simply not alignment-revealing." ></td>
	<td class="line x" title="50:182	In our pilot study many bloggers were observed to endorse a named candidate by using negative ex28 pressions to denounce his opponent." ></td>
	<td class="line x" title="51:182	Therefore, in our annotation procedure, the distinction is made between the coding of manifest content, i.e., sentiments on the surface, and latent political alignment under these surface elements." ></td>
	<td class="line x" title="52:182	2.3 Agreement Study In this section, we compare the annotations obtained from the on-site expert annotators and those from the non-expert AMT annotators." ></td>
	<td class="line x" title="53:182	2.3.1 Expert (On-site) Annotation To assess the reliability of the sentiment annotation procedure, we conducted an agreement study with three expert annotators in our site, using 36 snippets randomly chosen from the PBS Corpus." ></td>
	<td class="line x" title="54:182	Overall agreement among the three annotators on the relevance of snippets is 77.8%." ></td>
	<td class="line x" title="55:182	Overall agreement on the four-class sentiment codings is 70.4%." ></td>
	<td class="line x" title="56:182	Analysis indicate that the annotators agreed better on some codings than the others." ></td>
	<td class="line x" title="57:182	For the task of determining whether a snippet is subjective or not1, the annotators agreed 86.1% of the time." ></td>
	<td class="line x" title="58:182	For the task of determining whether a snippet is positive or negative, they agreed 94.9% of the time." ></td>
	<td class="line x" title="59:182	To examine which pair of codings is the most difficult to distinguish, Table 1 summarizes the confusion matrix for the three pairs of annotators judgements on sentiment codings." ></td>
	<td class="line x" title="60:182	Each column describes the marginal probability of a coding and the probability distribution for this coding being recognized as another coding (including itself)." ></td>
	<td class="line x" title="61:182	As many bloggers use cynical expressions in their writings, the most confusing cases occur when the annotators have to determine whether a snippet is negative or neutral." ></td>
	<td class="line x" title="62:182	The effect of cynical expressions on % Neu Pos Both Neg Marginal 21.9 20.0 10.5 47.6 Neutral (Neu) 47.8 14.3 9.1 16.0 Positive (Pos) 13.0 61.9 18.2 6.0 Both (Both) 4.4 9.5 9.1 14.0 Negative (Neg) 34.8 14.3 63.6 64.0 Table 1: Summary matrix for the three on-site annotators sentiment codings." ></td>
	<td class="line x" title="63:182	1This is done by grouping the codings of Positive, Negative, and Both into the subjective class." ></td>
	<td class="line x" title="64:182	sentiment analysis in the political domain is also revealed in the second step of alignment annotation." ></td>
	<td class="line x" title="65:182	Only 42.5% of the snippets have been coded with alignment coding in the same direction as its sentiment coding  i.e., if a snippet is intended to support (oppose) a target candidate, it will contain positive (negative) sentiment." ></td>
	<td class="line x" title="66:182	The alignment coding task has been shown to be reliable, with the annotators agreeing 76.8% of the time overall on the three-level codings: Support/Against/Neutral." ></td>
	<td class="line x" title="67:182	2.3.2 Amazon Mechanical Turk Annotation To compare the annotation reliability between expert and non-expert annotators, we further conducted an agreement study with the annotators recruited from Amazon Mechanical Turk (AMT)." ></td>
	<td class="line x" title="68:182	We have collected 1,000 snippets overnight, with the cost of 4 cents per annotation." ></td>
	<td class="line x" title="69:182	In the agreement study, a subset of 100 snippets is used, and each snippet is annotated by five AMT annotators." ></td>
	<td class="line x" title="70:182	These annotations were completed by 25 annotators whom were selected based on the approval rate of their previous AMT tasks (over 95% of times).2 The AMT annotators spent on average 40 seconds per snippet, shorter than the average of two minutes reported by the on-site annotators." ></td>
	<td class="line x" title="71:182	The lower overall agreement on all four-class sentiment codings, 35.3%, conforms to the expectation that the non-expert annotators are less reliable." ></td>
	<td class="line x" title="72:182	The Turk annotators also agreed less on the three-level alignment codings, achieving only 47.2% of agreement." ></td>
	<td class="line x" title="73:182	However, a finer-grained analysis reveals that they still agree well on some codings: The overall agreement on whether a snippet is relevant, whether a snippet is subjective or not, and whether a snippet is positive or negative remain within a reasonable range: 81.0%, 81.8% and 61.9% respectively." ></td>
	<td class="line x" title="74:182	2.4 Gold Standard We defined the gold standard (GS) label of a snippet in terms of the coding that receives the majority votes.3 Column 1 in Table 2 (onsite-GS predic2Note that we do not enforce these snippets to be annotated by the same group of annotators." ></td>
	<td class="line x" title="75:182	However, Kappa statistics requires to compute the chance agreement of each annotator." ></td>
	<td class="line x" title="76:182	Due to the violation of this assumption, we do not measure the intercoder agreement with Kappa in this agreement study." ></td>
	<td class="line x" title="77:182	3In this study, we excluded 6 snippets whose annotations failed to reach majority vote by the three onsite annotators." ></td>
	<td class="line x" title="78:182	29 onsite-GS prediction onsite agreement AMT-GS prediction AMT agreement Sentiment (4-class) 0.767 0.704 0.614 0.353 Alignment (3-level) 0.884 0.768 0.669 0.472 Relevant or not 0.889 0.778 0.893 0.810 Subjective or not 0.931 0.861 0.898 0.818 Positive or negative 0.974 0.949 0.714 0.619 Table 2: Average prediction accuracy on gold standard (GS) using one-coder strategy and inter-coder agreement." ></td>
	<td class="line x" title="79:182	tion) shows the ratio of the onsite expert annotations that are consistent with the gold standard, and Column 3 (AMT-GS prediction) shows the same for the AMT annotations." ></td>
	<td class="line x" title="80:182	The level of consistency, i.e., the percentage agreement with the gold standard labels, can be viewed as a proxy of the quality of the annotations." ></td>
	<td class="line x" title="81:182	Among the AMT annotations, Columns 2 (onsite agreement) and 4 (AMT agreement) show the pair-wise intercoder agreement in the on-site expert and AMT annotations respectively." ></td>
	<td class="line x" title="82:182	The results suggest that it is possible to take one single expert annotators coding as the gold standard in a number of annotation tasks using binary classification." ></td>
	<td class="line x" title="83:182	For example, there is a 97.4% chance that one experts coding on the polarity of a snippet, i.e., whether it is positive or negative, will be consistent with the gold standard coding." ></td>
	<td class="line x" title="84:182	However, this one-annotator strategy is less reliable with the introduction of non-expert annotators." ></td>
	<td class="line x" title="85:182	Take the task of polarity annotation as an example, the intercoder agreement among the AMT workers goes down to 61.9% and the one-coder strategy can only yield 71.4% accuracy." ></td>
	<td class="line x" title="86:182	To determine reliable gold standard codings, multiple annotators are still necessary when non-expert annotators are recruited." ></td>
	<td class="line x" title="87:182	3 Annotation Quality Measures Given the noisy AMT annotations, in this section we discuss some summary statistics that are needed to control the quality of annotations." ></td>
	<td class="line x" title="88:182	3.1 Annotator-level noise To study the question of whether there exists a group of annotators who tend to yield more noisy annotations, we evaluate the accumulated noise level introduced by each of the annotators." ></td>
	<td class="line x" title="89:182	We define the noise level as the deviation from the gold standard labels." ></td>
	<td class="line x" title="90:182	Similar to the measure of individual error rates proposed in (Dawid and Skene, 1979), the noise level of a particular annotator j, i.e., noise(annoj), is then estimated by summing up the deviation of the annotations received from this annotator, with a small sampling correction for chance disagreement." ></td>
	<td class="line x" title="91:182	Analysis results demonstrate that there does exist a subset of annotators who yield more noisy annotations than the others." ></td>
	<td class="line x" title="92:182	20% of the annotators (who exceed the noise level 60%) result in annotations that have 70% disagreement with the gold standard." ></td>
	<td class="line x" title="93:182	In addition, we also evaluate how inclusion of noisy annotators reduces the mean agreement with Gold Standard." ></td>
	<td class="line x" title="94:182	The plot (left) in Figure 1 plots the mean agreement rate with GS over the subset of annotators that pass a noise threshold." ></td>
	<td class="line x" title="95:182	These results show that the data quality decreases with the inclusion of more untrustworthy annotators." ></td>
	<td class="line x" title="96:182	3.2 Snippet-level sentiment ambiguity We have observed that not all snippets are equally easy to annotate, with some containing more ambiguous expressions." ></td>
	<td class="line x" title="97:182	To incorporate this concern in the selection process, a key question to be answered is whether there exist snippets whose sentiment is substantially less distinguishable than the others." ></td>
	<td class="line x" title="98:182	We address this question by quantifying ambiguity measures with the two key properties shown as important in evaluating the controversiality of annotation snippets (Carenini and Cheung, 2008): (1) the strength of the annotators judgements and (2) the polarity of the annotations." ></td>
	<td class="line x" title="99:182	The measurement needs to satisfy the constraints demonstrated in the following snippets: (1) An example that has received three positive codings are more ambiguous than that has received five, and (2) an example that has received five positive codings is more ambiguous than the one that has received four positive and one negative coding." ></td>
	<td class="line x" title="100:182	In addition, as some snippets were shown to 30 Annotator noise level Prediction Accuracy 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 Annotator Noise 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Sentiment Ambigity Lexical Uncertainty Prediction Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Lexical Uncertainty Figure 1: Data quality (consistency with GS) as a function of noise level (left), sentiment ambiguity (middle), and lexical uncertainty (right)." ></td>
	<td class="line x" title="101:182	be difficult to tell whether they contain negative or neutral sentiment, the measure of example ambiguity has to go beyond controversiality and incorporate codings of neutral and both." ></td>
	<td class="line x" title="102:182	To satisfy these constraints, we first enumerated through the codings of each snippet and counted the number of neutral, positive, both, and negative codings: We added (1) one to the positive (negative) category for each positive (negative) coding, (2) 0.5 to the neutral category with each neutral coding, and (3) 0.5 to both the positive and negative categories with each both coding." ></td>
	<td class="line x" title="103:182	The strength of codings in the three categories, i.e., str+(snipi), strneu(snipi), and str(snipi), were then summed up into str(snipi)." ></td>
	<td class="line x" title="104:182	The distribution were parameterized with +(snipi) = str+(snipi)/str(snipi) neu(snipi) = strneu(snipi)/str(snipi) (snipi) = str(snipi)/str(snipi) We then quantify the level of ambiguity in the annotators judgement as follows: H((snipi)) =+(snipi)log(+(snipi)) neu(snipi)log(neu(snipi)) (snipi)log((snipi)) Amb(snipi) = str(snipi)str max H((snipi)), where strmax is the maximum value of str among all the snippets in the collection." ></td>
	<td class="line x" title="105:182	The plot (middle) in Figure 1 shows that with the inclusion of snippets that are more ambiguous in sentiment disambiguation, the mean agreement with Gold Standard decreases as expected." ></td>
	<td class="line x" title="106:182	3.3 Combining measures on multiple annotations Having established the impact of noise and sentiment ambiguity on annotation quality, we then set out to explore how to integrate them for selection." ></td>
	<td class="line x" title="107:182	First, the ambiguity scores for each of the snippets are reweighed with respect to the noise level." ></td>
	<td class="line x" title="108:182	w(snipi) = summationdisplay j noise(annoj)(1e)(ij) Conf(snipi) = w(snipi)summationtext iw(snipi) Amb(snipi), where (ij) is an indicator function of whether a coding ofsnipi from annotatorj agrees with its gold standard coding." ></td>
	<td class="line x" title="109:182	w(expi) is thus computed as the aggregated noise level of all the annotators who labeled the ith snippet." ></td>
	<td class="line x" title="110:182	To understand the baseline performance of the selection procedure, we evaluate the the true predictions versus the false alarms resulting from using each of the quality measures separately to select annotations for label predictions." ></td>
	<td class="line x" title="111:182	In this context, a true prediction occurs when an annotation suggested by our measure as high-quality indeed matches the GS label, and a false alarm occurs when a high quality annotation suggested by our measure does not match the GS label." ></td>
	<td class="line x" title="112:182	The ROC (receiver operating characteristics) curves in Figure 2 reflect all the potential operating points with the different measures." ></td>
	<td class="line x" title="113:182	We used data from 2,895 AMT annotations on 579 snippets, including 63 snippets used in the agreement study." ></td>
	<td class="line x" title="114:182	This dataset is obtained by filtering out the snippets with their GS labels as 1 (irrelevant) and the snippets that do not receive any coding that has more than two votes." ></td>
	<td class="line x" title="115:182	31 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm Rate True Prediction Rate a71a71a71 a71 a71 a71 a71 a71 a71 a71 0.10.20.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (a) Match Prediction Before Removing Divisive Snippets 1confusion 1ambiguity 1noise 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 False Alarm Rate True Prediction Rate a71a71a71a71 a71 a71 a71 a71 a71 a71 a71 00.10.20.3 0.4 0.5 0.6 0.7 0.8 0.9 1 (b) Match Prediction After Removing Divisive Snippets 1confusion(all4codings) 1confusion(pos/neg) 1ambiguity(all4codings) 1ambiguity(pos/neg) 1noise Figure 2: Modified ROC curves for quality measures: (a) before removing divisive snippets, (b) after removing divisive snippets." ></td>
	<td class="line x" title="116:182	The numbers shown with the ROC curve are the values of the aggregated quality measure (1-confusion)." ></td>
	<td class="line x" title="117:182	Initially, three quality measures are tested: 1noise, 1-ambiguity, 1-confusion." ></td>
	<td class="line x" title="118:182	Examination of the snippet-level sentiment codings reveals that some snippets (12%) result in divisive codings, i.e., equal number of votes on two codings." ></td>
	<td class="line x" title="119:182	The ROC curves in Figure 2 (a) plot the baseline performance of the different quality measures." ></td>
	<td class="line x" title="120:182	Results show that before removing the subset of divisive snippets, the only effective selection criteria is obtained by monitoring the noise level of annotators." ></td>
	<td class="line x" title="121:182	Figure 2 (b) plots the performance after removing the divisive snippets." ></td>
	<td class="line x" title="122:182	In addition, our ambiguity scores are computed under two settings: (1) with only the polar codings (pos/neg), and (2) with all the four codings (all4codings)." ></td>
	<td class="line x" title="123:182	The ROC curves reveal that analyzing only the polar codings is not sufficient for annotation selection." ></td>
	<td class="line x" title="124:182	The results also demonstrate that confusion, an integrated measure, does perform best." ></td>
	<td class="line x" title="125:182	Confusion is just one way of combining these measures." ></td>
	<td class="line x" title="126:182	One may chose alternative combinations  the results here primarily illustrate the benefit of considering these different dimensions in tandem." ></td>
	<td class="line x" title="127:182	Moreover, the difference between plot (a) and (b) suggests that removing divisive snippets is essential for the quality measures to work well." ></td>
	<td class="line x" title="128:182	How to automatically identify the divisive snippets is therefore important to the success of the annotation selection process." ></td>
	<td class="line x" title="129:182	3.4 Effect of lexical uncertainty on divisive snippet detection In search of measures that can help identify the divisive snippets automatically, we consider the inherent lexical uncertainty of an example." ></td>
	<td class="line x" title="130:182	Uncertainty Sampling (Lewis and Catlett, 1994) is one common heuristic for the selection of informative instances, which select instances that the current classifier is most uncertain about." ></td>
	<td class="line x" title="131:182	Following on these lines we measure the uncertainty on instances, with the assumption that the most uncertain snippets are likely to be divisive." ></td>
	<td class="line x" title="132:182	In particular, we applied a lexical sentiment classifier (c.f. Section 4.1.1) to estimate the likelihood of an unseen snippet being of positive or negative sentiment, i.e., P+(expi), P(expi), by counting the sentiment-indicative word occurrences in the snippet." ></td>
	<td class="line x" title="133:182	As in our dataset the negative snippets far exceed the positive ones, we also take the prior probability into account to avoid class bias." ></td>
	<td class="line x" title="134:182	We then measure lexical uncertainty as follows." ></td>
	<td class="line x" title="135:182	Deviation(snipi) = 1 C|(log(P(+))log(P())) +(log(P+(snipi))log(P(snipi)))|, Uncertainty(snipi) =1Deviation(snipi), where class priors, P(+) and P(), are estimated with the dataset used in the agreement studies, and C is the normalization constant." ></td>
	<td class="line x" title="136:182	We then examine not only the utility of lexical uncertainty in identifying high-quality annotations, but 32 Classifier Accuracy AUC LC 49.60 0.614 NB 83.53 0.653 SVM 83.89 0.647 Pooling 84.51 0.700 Table 3: Accuracy of sentiment classification methods." ></td>
	<td class="line x" title="137:182	also the utility of such measure in identifying divisive snippets." ></td>
	<td class="line x" title="138:182	Figure 1 (right) shows the effect of lexical uncertainty on filtering out low-quality annotations." ></td>
	<td class="line x" title="139:182	Figure 3 demonstrates the effect of lexical uncertainty on divisive snippet detection, suggesting the potential use of lexical uncertainty measures in the selection process." ></td>
	<td class="line x" title="140:182	Lexical Uncertainty Divisive Snippet Detection Accuracy 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Figure 3: Divisive snippet detection accuracy as a function of lexical uncertainty." ></td>
	<td class="line x" title="141:182	4 Empirical Evaluation The analysis in Sec." ></td>
	<td class="line x" title="142:182	3 raises two important questions: (1) how useful are noisy annotations for sentiment analysis, and (2) what is the effect of online annotation selection on improving sentiment polarity classification?" ></td>
	<td class="line x" title="143:182	4.1 Polarity Classifier with Noisy Annotations To answer the first question raised above, we train classifiers based on the noisy AMT annotations to classify positive and negative snippets." ></td>
	<td class="line x" title="144:182	Four different types of classifiers are used: SVMs, Naive Bayes (NB), a lexical classifier (LC), and the lexical knowledge-enhanced Pooling Multinomial classifier, described below." ></td>
	<td class="line x" title="145:182	4.1.1 Lexical Classifier In the absence of any labeled data in a domain, one can build sentiment-classification models that rely solely on background knowledge, such as a lexicon defining the polarity of words." ></td>
	<td class="line x" title="146:182	Given a lexicon of positive and negative terms, one straightforward approach to using this information is to measure the frequency of occurrence of these terms in each document." ></td>
	<td class="line x" title="147:182	The probability that a test document belongs to the positive class can then be computed as P(+|D) = aa+b; where a and b are the number of occurrences of positive and negative terms in the document respectively." ></td>
	<td class="line x" title="148:182	A document is then classified as positive if P(+|D) > P(|D); otherwise, the document is classified as negative." ></td>
	<td class="line x" title="149:182	For this study, we used a lexicon of 1,267 positive and 1,701 negative terms, as labeled by human experts." ></td>
	<td class="line x" title="150:182	4.1.2 Pooling Multinomials The Pooling Multinomials classifier was introduced by the authors as an approach to incorporate prior lexical knowledge into supervised learning for better text classification." ></td>
	<td class="line x" title="151:182	In the context of sentiment analysis, such lexical knowledge is available in terms of the prior sentiment-polarity of words." ></td>
	<td class="line x" title="152:182	Pooling Multinomials classifies unlabeled examples just as in multinomial Nave Bayes classification (McCallum and Nigam, 1998), by predicting the class with the maximum likelihood, given by argmaxcjP(cj)producttextiP(wi|cj); where P(cj) is the prior probability of class cj, and P(wi|cj) is the probability of word wi appearing in a snippet of class cj." ></td>
	<td class="line x" title="153:182	In the absence of background knowledge about the class distribution, we estimate the class priors P(cj) solely from the training data." ></td>
	<td class="line x" title="154:182	However, unlike regular Nave Bayes, the conditional probabilities P(wi|cj) are computed using both the labeled examples and the lexicon of labeled features." ></td>
	<td class="line x" title="155:182	Given two models built using labeled examples and labeled features, the multinomial parameters of such models can be aggregated through a convex combination,P(wi|cj) = Pe(wi|cj)+(1)Pf(wi|cj); where Pe(wi|cj) and Pf(wi|cj) represent the probability assigned by using the example labels and feature labels respectively, and is the weight for combining these distributions." ></td>
	<td class="line x" title="156:182	The weight indicates a level of confidence in each source of information, and can be computed based on the training set accuracy of the two components." ></td>
	<td class="line x" title="157:182	The derivation and details of these models are not directly relevant to this paper, but can be found in (Melville et al., 2009)." ></td>
	<td class="line x" title="158:182	33 Q1 Q2 Q3 Q4 Accuracy AUC Accuracy AUC Accuracy AUC Accuracy AUC Noise 84.62% 0.688 74.36% 0.588 74.36% 0.512 79.49% 0.441 Ambiguity 84.21% 0.715 78.95% 0.618 68.42% 0.624 84.21% 0.691 Confusion 82.50% 0.831 82.50% 0.762 80.00% 0.814 80.00% 0.645 Table 4: Effect of annotation selection on classification accuracy." ></td>
	<td class="line x" title="159:182	4.1.3 Results on Polarity Classification We generated a data set of 504 snippets that had 3 or more labels for either the positive or negative class." ></td>
	<td class="line x" title="160:182	We compare the different classification approaches using 10-fold cross-validation and present our results in Table 3." ></td>
	<td class="line x" title="161:182	Results show that the Pooling Multinomial classifier, which makes predictions based on both the prior lexical knowledge and the training data, can learn the most from the labeled data to classify sentiments of the political blog snippets." ></td>
	<td class="line x" title="162:182	We observe that despite the significant level of noise and ambiguity in the training data, using majority-labeled data for training still results in classifiers with reasonable accuracy." ></td>
	<td class="line x" title="163:182	4.2 Effect of Annotation Selection We then evaluate the utility of the quality measures in a randomly split dataset (with 7.5% of the data in the test set)." ></td>
	<td class="line x" title="164:182	We applied each of the measures to rank the annotation examples and then divide them into 4 equal-sized training sets based on their rankings." ></td>
	<td class="line x" title="165:182	For example, Noise-Q1 contains only the least noisy quarter of annotations and Q4 the most noisy ones." ></td>
	<td class="line x" title="166:182	Results in Table 4 demonstrate that the classification performance declines with the decrease of each quality measure in general, despite exceptions in the subset with the highest sentiment ambiguity (Ambiguity-Q4), the most noisy subset Q4 (NoiseQ4), and the subset yielding less overall confusion (Confusion-Q2)." ></td>
	<td class="line x" title="167:182	The results also reveal the benefits of annotation selection on efficiency: using the subset of annotations predicted in the top quality quarter achieves similar performance as using the whole training set." ></td>
	<td class="line x" title="168:182	These preliminary results suggest that an active learning scheme which considers all three quality measures may indeed be effective in improving label quality and subsequent classification accuracy." ></td>
	<td class="line x" title="169:182	5 Conclusion In this paper, we have analyzed the difference between expert and non-expert annotators in terms of annotation quality, and showed that having a single non-expert annotator is detrimental for annotating sentiment in political snippets." ></td>
	<td class="line x" title="170:182	However, we confirmed that using multiple noisy annotations from different non-experts can still be very useful for modeling." ></td>
	<td class="line x" title="171:182	This finding is consistent with the simulated results reported in (Sheng et al., 2008)." ></td>
	<td class="line x" title="172:182	Given the availability of many non-expert annotators ondemand, we studied three important dimensions to consider when acquiring annotations: (1) the noise level of an annotator compared to others, (2) the inherent ambiguity of an examples class label, and (3) the informativeness of an example to the current classification model." ></td>
	<td class="line x" title="173:182	While the first measure has been studied with annotations obtained from experts (Dawid and Skene, 1979; Clemen and Reilly, 1999), the applicability of their findings on non-expert annotation selection has not been examined." ></td>
	<td class="line x" title="174:182	We showed how quality of labels can be improved by eliminating noisy annotators and ambiguous examples." ></td>
	<td class="line x" title="175:182	Furthermore, we demonstrated the quality measures are useful for selecting annotations that lead to more accurate classification models." ></td>
	<td class="line x" title="176:182	Our results suggest that a good active learning or online learning scheme in this setting should really consider all three dimensions." ></td>
	<td class="line x" title="177:182	The way we use to integrate the different dimensions now is still preliminary." ></td>
	<td class="line x" title="178:182	Also, our empirical findings suggest that some of the dimensions may have to be considered separately." ></td>
	<td class="line x" title="179:182	For example, due to the divisive tendency of the most informative examples, these examples may have to be disregarded in the initial stage of annotation selection." ></td>
	<td class="line x" title="180:182	Also, the way we use to combine these measures is still preliminary." ></td>
	<td class="line x" title="181:182	The design and testing of such schemes are avenues for future work." ></td>
	<td class="line x" title="182:182	34" ></td>
</tr></table>
</div
</body></html>
