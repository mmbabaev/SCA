<html><body><head><link rel="stylesheet" type="text/css" href="style.css" /><script src="map.js"></script><script src="jquery-1.7.1.min.js"></script></head>
<div class="dstPaperData">
W05-0909 <div class="dstPaperTitle">METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments</div><div class="dstPaperAuthors">Banerjee, Satanjeev;Lavie, Alon;</div>
</div>
<table cellspacing="0" cellpadding="0"><tr>
	<td class="srcData" >Source Paper</td>
	<td class="pp legend" ><input type="checkbox" id="cbIPositive" checked="true"/><label for="cbIPositive">Informal +<label></td>
	<td class="nn legend" ><input type="checkbox" id="cbINegative" checked="true"/><label for="cbINegative">Informal -<label></td>
	<td class="oo legend" ><input type="checkbox" id="cbIObjective" checked="true"/><label for="cbIObjective">Informal Neutral<label></td>
	<td class="ppc legend" ><input type="checkbox" id="cbEPositive" checked="true"/><label for="cbEPositive">Formal +</label></td>
	<td class="nnc legend" ><input type="checkbox" id="cbENegative" checked="true"/><label for="cbENegative">Formal -</label></td>
	<td class="ooc legend" ><input type="checkbox" id="cbEObjective" checked="true"/><label for="cbEObjective">Formal Neutral</label></td>
	<td class="lb"><input type="checkbox" id="cbSentenceBoundary"/><label for="cbSentenceBoundary">Sentence Boundary</label></td>
</tr></table>
<div class="dstPaper">
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1031
CDER: Efficient MT Evaluation Using Block Movements
Leusch, Gregor;Ueffing, Nicola;Ney, Hermann;"></td>
	<td class="line x" title="1:291	CDER: Efficient MT Evaluation Using Block Movements Gregor Leusch and Nicola Ueffing and Hermann Ney Lehrstuhl fur Informatik VI, Computer Science Department RWTH Aachen University D-52056 Aachen, Germany {leusch,ueffing,ney}@i6.informatik.rwth-aachen.de Abstract Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks." ></td>
	<td class="line x" title="2:291	In many cases though such movements still result in correct or almost correct sentences." ></td>
	<td class="line x" title="3:291	In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation." ></td>
	<td class="line x" title="4:291	Our measure can be exactly calculated in quadratic time." ></td>
	<td class="line x" title="5:291	Furthermore, we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs." ></td>
	<td class="line x" title="6:291	The correlation of the new measure with human judgment has been investigated systematically on two different language pairs." ></td>
	<td class="line x" title="7:291	The experimental results will show that it significantly outperforms state-of-the-art approaches in sentence-level correlation." ></td>
	<td class="line x" title="8:291	Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment." ></td>
	<td class="line x" title="9:291	1 Introduction Research in machine translation (MT) depends heavily on the evaluation of its results." ></td>
	<td class="line x" title="10:291	Especially for the development of an MT system, an evaluation measure is needed which reliably assesses the quality of MT output." ></td>
	<td class="line x" title="11:291	Such a measure will help analyze the strengths and weaknesses of different translation systems or different versions of the same system by comparing output at the sentence level." ></td>
	<td class="line x" title="12:291	In most applications of MT, understandability for humans in terms of readability as well as semantical correctness should be the evaluation criterion." ></td>
	<td class="line x" title="13:291	But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks." ></td>
	<td class="line x" title="14:291	A high correlation between these automatic evaluation measures and human evaluation is thus desirable." ></td>
	<td class="line x" title="15:291	State-of-the-art measures such as BLEU (Papineni et al. , 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences." ></td>
	<td class="line x" title="16:291	They are thus not well-suited for sentence-level evaluation." ></td>
	<td class="line x" title="17:291	The introduction of smoothing (Lin and Och, 2004) solves this problem only partially." ></td>
	<td class="line x" title="18:291	In this paper, we will present a new automatic error measure for MT  the CDER  which is designed for assessing MT quality on the sentence level." ></td>
	<td class="line x" title="19:291	It is based on edit distance  such as the well-known word error rate (WER)  but allows for reordering of blocks." ></td>
	<td class="line x" title="20:291	Nevertheless, by defining reordering costs, the ordering of the words in a sentence is still relevant for the measure." ></td>
	<td class="line x" title="21:291	In this, the new measure differs significantly from the position independent error rate (PER) by (Tillmann et al. , 1997)." ></td>
	<td class="line x" title="22:291	Generally, finding an optimal solution for such a reordering problem is NP hard, as is shown in (Lopresti and Tomkins, 1997)." ></td>
	<td class="line x" title="23:291	In previous work, researchers have tried to reduce the complexity, for example by restricting the possible permutations on the block-level, or by approximation or heuristics during the calculation." ></td>
	<td class="line x" title="24:291	Nevertheless, most of the resulting algorithms still have high run times and are hardly applied in practice, or give only a rough approximation." ></td>
	<td class="line x" title="25:291	An overview of some better-known measures can be found in Section 3.1." ></td>
	<td class="line x" title="26:291	In contrast to this, our new measure can be calculated very efficiently." ></td>
	<td class="line x" title="27:291	This is achieved by requiring complete and disjoint coverage of the blocks only for the reference sentence, and not for the candidate translation." ></td>
	<td class="line x" title="28:291	We will present an algorithm which computes the new error measure in quadratic time." ></td>
	<td class="line x" title="29:291	The new evaluation measure will be investigated and compared to state-of-the-art methods on two translation tasks." ></td>
	<td class="line x" title="30:291	The correlation with human assessment will be measured for several different statistical MT systems." ></td>
	<td class="line x" title="31:291	We will see that the new measure significantly outperforms the existing approaches." ></td>
	<td class="line x" title="32:291	1The n-gram precisions are measured at the sentence level and then combined into a score over the whole document." ></td>
	<td class="line x" title="33:291	241 As a further improvement, we will introduce word dependent substitution costs." ></td>
	<td class="line x" title="34:291	This method will be applicable to the new measure as well as to established measures like WER and PER." ></td>
	<td class="line x" title="35:291	Starting from the observation that the substitution of a word with a similar one is likely to affect translation quality less than the substitution with a completely different word, we will show how the similarity of words can be accounted for in automatic evaluation measures." ></td>
	<td class="line x" title="36:291	This paper is organized as follows: In Section 2, we will present the state of the art in MT evaluation and discuss the problem of block reordering." ></td>
	<td class="line x" title="37:291	Section 3 will introduce the new error measure CDER and will show how it can be calculated efficiently." ></td>
	<td class="line x" title="38:291	The concept of worddependent substitution costs will be explained in Section 4." ></td>
	<td class="line x" title="39:291	In Section 5, experimental results on the correlation of human judgment with the CDER and other well-known evaluation measures will be presented." ></td>
	<td class="line x" title="40:291	Section 6 will conclude the paper and give an outlook on possible future work." ></td>
	<td class="line x" title="41:291	2 MT Evaluation 2.1 Block Reordering and State of the Art In MT  as opposed to other natural language processing tasks like speech recognition  there is usually more than one correct outcome of a task." ></td>
	<td class="line x" title="42:291	In many cases, alternative translations of a sentence differ from each other mostly by the ordering of blocks of words." ></td>
	<td class="line x" title="43:291	Consequently, an evaluation measure for MT should be able to detect and allow for block reordering." ></td>
	<td class="line x" title="44:291	Nevertheless, a higher amount of reordering between a candidate translation and a reference translation should still be reflected in a worse evaluation score." ></td>
	<td class="line x" title="45:291	In other words, the more blocks there are to be reordered between reference and candidate sentence, the higher we want the measure to evaluate the distance between these sentences." ></td>
	<td class="line x" title="46:291	State-of-the-art evaluation measures for MT penalize movement of blocks rather severely: ngram based scores such as BLEU or NIST still yield a high unigram precision if blocks are reordered." ></td>
	<td class="line x" title="47:291	For higher-order n-grams, though, the precision drops." ></td>
	<td class="line x" title="48:291	As a consequence, this affects the overall score significantly." ></td>
	<td class="line x" title="49:291	WER, which is based on Levenshtein distance, penalizes the reordering of blocks even more heavily." ></td>
	<td class="line x" title="50:291	It measures the distance by substitution, deletion and insertion operations for each word in a relocated block." ></td>
	<td class="line x" title="51:291	PER, on the other hand, ignores the ordering of the words in the sentences completely." ></td>
	<td class="line x" title="52:291	This often leads to an overly optimistic assessment of translation quality." ></td>
	<td class="line x" title="53:291	2.2 Long Jumps The approach we pursue in this paper is to extend the Levenshtein distance by an additional operation, namely block movement." ></td>
	<td class="line x" title="54:291	The number of blocks in a sentence is equal to the number of gaps among the blocks plus one." ></td>
	<td class="line x" title="55:291	Thus, the block movements can equivalently be expressed as long jump operations that jump over the gaps between two blocks." ></td>
	<td class="line x" title="56:291	The costs of a long jump are constant." ></td>
	<td class="line x" title="57:291	The blocks are read in the order of one of the sentences." ></td>
	<td class="line x" title="58:291	These long jumps are combined with the classical Levenshtein edit operations, namely insertion, deletion, substitution, and the zero-cost operation identity." ></td>
	<td class="line x" title="59:291	The resulting long jump distance dLJ gives the minimum number of operations which are necessary to transform the candidate sentence into the reference sentence." ></td>
	<td class="line x" title="60:291	Like the Levenshtein distance, the long jump distance can be depicted using an alignment grid as shown in Figure 1: Here, each grid point corresponds to a pair of inter-word positions in candidate and reference sentence, respectively." ></td>
	<td class="line x" title="61:291	dLJ is the minimum cost of a path between the lower left (first) and the upper right (last) alignment grid point which covers all reference and candidate words." ></td>
	<td class="line x" title="62:291	Deletions and insertions correspond to horizontal and vertical edges, respectively." ></td>
	<td class="line x" title="63:291	Substitutions and identity operations correspond to diagonal edges." ></td>
	<td class="line x" title="64:291	Edges between arbitrary grid points from the same row correspond to long jump operations." ></td>
	<td class="line x" title="65:291	It is easy to see that dLJ is symmetrical." ></td>
	<td class="line x" title="66:291	In the example, the best path contains one deletion edge, one substitution edge, and three long jump edges." ></td>
	<td class="line x" title="67:291	Therefore, the long jump distance between the sentences is five." ></td>
	<td class="line x" title="68:291	In contrast, the best Levenshtein path contains one deletion edge, four identity and five consecutive substitution edges; the Levenshtein distance between the two sentences is six." ></td>
	<td class="line x" title="69:291	The effect of reordering on the BLEU measure is even higher in this example: Whereas 8 of the 10 unigrams from the candidate sentence can be found in the reference sentence, this holds for only 4 bigrams, and 1 trigram." ></td>
	<td class="line x" title="70:291	Not a single one of the 7 candidate four-grams occurs in the reference sentence." ></td>
	<td class="line x" title="71:291	3 CDER: A New Evaluation Measure 3.1 Approach (Lopresti and Tomkins, 1997) showed that finding an optimal path in a long jump alignment grid is an NP-hard problem." ></td>
	<td class="line x" title="72:291	Our experiments showed that the calculation of exact long jump distances becomes impractical for sentences longer than 20 words." ></td>
	<td class="line x" title="73:291	242 we met at the airport at seven oclock." ></td>
	<td class="line x" title="74:291	we met at seven oclockon the airport.have candidate reference deletion insertion substitution identity best path start/ end node long jump block Figure 1: Example of a long jump alignment grid." ></td>
	<td class="line x" title="75:291	All possible deletion, insertion, identity and substitution operations are depicted." ></td>
	<td class="line x" title="76:291	Only long jump edges from the best path are drawn." ></td>
	<td class="line x" title="77:291	A possible way to achieve polynomial runtime is to restrict the number of admissible block permutations." ></td>
	<td class="line x" title="78:291	This has been implemented by (Leusch et al. , 2003) in the inversion word error rate." ></td>
	<td class="line x" title="79:291	Alternatively, a heuristic or approximative distance can be calculated, as in GTM by (Turian et al. , 2003)." ></td>
	<td class="line x" title="80:291	An implementation of both approaches at the same time can be found in TER by (Snover et al. , 2005)." ></td>
	<td class="line x" title="81:291	In this paper, we will present another approach which has a suitable run-time, while still maintaining completeness of the calculated measure." ></td>
	<td class="line x" title="82:291	The idea of the proposed method is to drop some restrictions on the alignment path." ></td>
	<td class="line x" title="83:291	The long jump distance as well as the Levenshtein distance require both reference and candidate translation to be covered completely and disjointly." ></td>
	<td class="line x" title="84:291	When extending the metric by block movements, we drop this constraint for the candidate translation." ></td>
	<td class="line x" title="85:291	That is, only the words in the reference sentence have to be covered exactly once, whereas those in the candidate sentence can be covered zero, one, or multiple times." ></td>
	<td class="line x" title="86:291	Dropping the constraints makes an efficient computation of the distance possible." ></td>
	<td class="line x" title="87:291	We drop the constraints for the candidate sentence and not for the reference sentence because we do not want any information contained in the reference to be omitted." ></td>
	<td class="line x" title="88:291	Moreover, the reference translation will not contain unnecessary repetitions of blocks." ></td>
	<td class="line x" title="89:291	The new measure  which will be called CDER in the following  can thus be seen as a measure oriented towards recall, while measures like BLEU are guided by precision." ></td>
	<td class="line x" title="90:291	The CDER is based on the CDCD distance2 introduced in (Lopresti and Tomkins, 1997)." ></td>
	<td class="line x" title="91:291	The authors show there that the problem of finding the optimal solution can be solved in O(I2  L) time, where I is the length of the candidate sentence and L the length of the reference sentence." ></td>
	<td class="line x" title="92:291	Within this paper, we will refer to this distance as dCD . In the next subsection, we will show how it can be computed in O(I L) time using a modification of the Levenshtein algorithm." ></td>
	<td class="line x" title="93:291	We also studied the reverse direction of the described measure; that is, we dropped the coverage constraints for the reference sentence instead of the candidate sentence." ></td>
	<td class="line x" title="94:291	Additionally, the maximum of both directions has been considered as distance measure." ></td>
	<td class="line x" title="95:291	The results in Section 5.2 will show that the measure using the originally proposed direction has a significantly higher correlation with human evaluation than the other directions." ></td>
	<td class="line x" title="96:291	3.2 Algorithm Our algorithm for calculating dCD is based on the dynamic programming algorithm for the Levenshtein distance (Levenshtein, 1966)." ></td>
	<td class="line x" title="97:291	The Levenshtein distance dLev(eI1, eL1parenrightbig between two strings eI1 and eL1 can be calculated in constant time if the Levenshtein distances of the substrings, dLev(eI11, eL1parenrightbig, dLev(eI1, eL11 parenrightbig, and dLev(eI11, eL11 parenrightbig, are known." ></td>
	<td class="line x" title="98:291	Consequently, an auxiliary quantity DLev(i,l) := dLevparenleftbigei1, el1parenrightbig is stored in an I L table." ></td>
	<td class="line x" title="99:291	This auxiliary quantity can then be calculated recursively from DLev(i  1,l), DLev(i,l  1), and DLev(i  1,l  1)." ></td>
	<td class="line x" title="100:291	Consequently, the Levenshtein distance can be calculated in time O(I L)." ></td>
	<td class="line x" title="101:291	This algorithm can easily be extended for the calculation of dCD as follows: Again we define an auxiliary quantity D(i,l) as D(i,l) := dCDparenleftbigei1, el1parenrightbig Insertions, deletions, and substitutions are handled the same way as in the Levenshtein algorithm." ></td>
	<td class="line x" title="102:291	Now assume that an optimal dCD path has been found: Then, each long jump edge within 2C stands for cover and D for disjoint." ></td>
	<td class="line x" title="103:291	We adopted this notion for our measures." ></td>
	<td class="line x" title="104:291	243  i l deletion insertion subst/id long jump l-1 i-1 Figure 2: Predecessors of a grid point (i,l) in Equation 1 this path will always start at a node with the lowest D value in its row3." ></td>
	<td class="line x" title="105:291	Consequently, we use the following modification of the Levenshtein recursion: D(0,0) = 0 D(i,l) = min    D(i1,l1) + (1(ei, el)), D(i1,l) + 1, D(i,l  1) + 1, min iprime D(iprime,l) + 1    (1) where  is the Kronecker delta." ></td>
	<td class="line x" title="106:291	Figure 2 shows the possible predecessors of a grid point." ></td>
	<td class="line x" title="107:291	The calculation of D(i,l) requires all values of D(iprime,l) to be known, even for iprime > i. Thus, the calculation takes three steps for each l: 1." ></td>
	<td class="line x" title="108:291	For each i, calculate the minimum of the first three terms." ></td>
	<td class="line x" title="109:291	2." ></td>
	<td class="line x" title="110:291	Calculate min iprime D(iprime,l)." ></td>
	<td class="line x" title="111:291	3." ></td>
	<td class="line x" title="112:291	For each i, calculate the minimum according to Equation 1." ></td>
	<td class="line x" title="113:291	Each of these steps can be done in time O(I)." ></td>
	<td class="line x" title="114:291	Therefore, this algorithm calculates dCD in time O(I L) and space O(I)." ></td>
	<td class="line x" title="115:291	3.3 Hypothesis Length and Penalties As the CDER does not penalize candidate translations which are too long, we studied the use of a length penalty or miscoverage penalty." ></td>
	<td class="line x" title="116:291	This determines the difference in sentence lengths between candidate and reference." ></td>
	<td class="line x" title="117:291	Two definitions of such a penalty have been studied for this work." ></td>
	<td class="line x" title="118:291	3Proof: Assume that the long jump edge goes from (iprime,l) to (i,l), and that there exists an iprimeprime such that D(iprimeprime,l) < D(iprime,l)." ></td>
	<td class="line x" title="119:291	This means that the path from (0,0) to (iprimeprime,l) is less expensive than the path from (0,0) to (iprime,l)." ></td>
	<td class="line x" title="120:291	Thus, the path from (0,0) through (iprimeprime,l) to (i,l) is less expensive than the path through (iprime,l)." ></td>
	<td class="line x" title="121:291	This contradicts the assumption." ></td>
	<td class="line x" title="122:291	Length Difference There is always an optimal dCD alignment path that does not contain any deletion edges, because each deletion can be replaced by a long jump, at the same costs." ></td>
	<td class="line x" title="123:291	This is different for a dLJ path, because here each candidate word must be covered exactly once." ></td>
	<td class="line x" title="124:291	Assume now that the candidate sentence consists of I words and the reference sentence consists of L words, with I > L. Then, at most L candidate words can be covered by substitution or identity edges." ></td>
	<td class="line x" title="125:291	Therefore, the remaining candidate words (at least I  L) must be covered by deletion edges." ></td>
	<td class="line x" title="126:291	This means that at least IL deletion edges will be found in any dLJ path, which leads to dLJ  dCD  I  L in this case." ></td>
	<td class="line x" title="127:291	Consequently, the length difference between the two sentences gives us a useful miscoverage penalty lplen: lplen := maxparenleftbigI L,0parenrightbig This penalty is independent of the dCD alignment path." ></td>
	<td class="line x" title="128:291	Thus, an optimal dCD alignment path is optimal for dCD + lplen as well." ></td>
	<td class="line x" title="129:291	Therefore the search algorithm in Section 3.2 will find the optimum for this sum." ></td>
	<td class="line x" title="130:291	Absolute Miscoverage Let coverage(i) be the number of substitution, identity, and deletion edges that cover a candidate word ei in a dCD path." ></td>
	<td class="line x" title="131:291	If we had a complete and disjoint alignment for the candidate word (i.e. , a dLJ path), coverage(i) would be 1 for each i. In general this is not the case." ></td>
	<td class="line x" title="132:291	We can use the absolute miscoverage as a penalty lpmisc for dCD: lpmisc := summationdisplay i |1  coverage(i)| This miscoverage penalty is not independent of the alignment path." ></td>
	<td class="line x" title="133:291	Consequently, the proposed search algorithm will not necessarily find an optimal solution for the sum of dCD and lpmisc." ></td>
	<td class="line x" title="134:291	The idea behind the absolute miscoverage is that one can construct a valid  but not necessarily optimal  dLJ path from a given dCD path." ></td>
	<td class="line x" title="135:291	This procedure is illustrated in Figure 3 and takes place in two steps: 1." ></td>
	<td class="line x" title="136:291	For each block of over-covered candidate words, replace the aligned substitution and/or identity edges by insertion edges; move the long jump at the beginning of the block accordingly." ></td>
	<td class="line x" title="137:291	2." ></td>
	<td class="line x" title="138:291	For each block of under-covered candidate words, add the corresponding number of 244 candidate reference 2 2 0coverage candidate 1 1 1 1 1 1 1 1 1 1 1 dCD dLJ deletion insertion subst/id long jump Figure 3: Transformation of a dCD path into a dLJ path." ></td>
	<td class="line x" title="139:291	deletion edges; move the long jump at the beginning of the block accordingly." ></td>
	<td class="line x" title="140:291	This also shows that there cannot be4 a polynomial time algorithm that calculates the minimum of dCD + lpmisc for arbitrary pairs of sentences, because this minimum is equal to dLJ." ></td>
	<td class="line x" title="141:291	With these miscoverage penalties, inexpensive lower and upper bounds for dLJ can be calculated, because the following inequality holds: (2) dCD + lplen  dLJ  dCD + lpmisc 4 Word-dependent Substitution Costs 4.1 Idea All automatic error measures which are based on the edit distance (i.e. WER, PER, and CDER) apply fixed costs for the substitution of words." ></td>
	<td class="line x" title="142:291	However, this is counter-intuitive, as replacing a word with another one which has a similar meaning will rarely change the meaning of a sentence significantly." ></td>
	<td class="line x" title="143:291	On the other hand, replacing the same word with a completely different one probably will." ></td>
	<td class="line x" title="144:291	Therefore, it seems advisable to make substitution costs dependent on the semantical and/or syntactical dissimilarity of the words." ></td>
	<td class="line x" title="145:291	To avoid awkward case distinctions, we assume that a substitution cost function cSUB for two words e, e meets the following requirements: 1." ></td>
	<td class="line x" title="146:291	cSUB depends only on e and e. 2." ></td>
	<td class="line x" title="147:291	cSUB is a metric; especially (a) The costs are zero if e = e, and larger than zero otherwise." ></td>
	<td class="line x" title="148:291	(b) The triangular inequation holds: it is always cheaper to replace e by e than to replace e by eprime and then eprime by e. 4provided that P negationslash= NP, of course." ></td>
	<td class="line x" title="149:291	3." ></td>
	<td class="line x" title="150:291	The costs of substituting a word e by e are always equal or lower than those of deleting e and then inserting e. In short, cSUB  2." ></td>
	<td class="line x" title="151:291	Under these conditions the algorithms for WER and CDER can easily be modified to use word-dependent substitution costs." ></td>
	<td class="line x" title="152:291	For example, the only necessary modification in the CDER algorithm in Equation 1 is to replace 1  (e, e) by cSUB(e, e)." ></td>
	<td class="line x" title="153:291	For the PER, it is no longer possible to use a linear time algorithm in the general case." ></td>
	<td class="line x" title="154:291	Instead, a modification of the Hungarian algorithm (Knuth, 1993) can be used." ></td>
	<td class="line x" title="155:291	The question is now how to define the worddependent substitution costs." ></td>
	<td class="line x" title="156:291	We have studied two different approaches." ></td>
	<td class="line x" title="157:291	4.2 Character-based Levenshtein Distance A pragmatic approach is to compare the spelling of the words to be substituted with each other." ></td>
	<td class="line x" title="158:291	The more similar the spelling is, the more similar we consider the words to be, and the lower we want the substitution costs between them." ></td>
	<td class="line x" title="159:291	In English, this works well with similar tenses of the same verb, or with genitives or plurals of the same noun." ></td>
	<td class="line x" title="160:291	Nevertheless, a similar spelling is no guarantee for a similar meaning, because prefixes such as mis-, in-, or un- can change the meaning of a word significantly." ></td>
	<td class="line x" title="161:291	An obvious way of comparing the spelling is the Levenshtein distance." ></td>
	<td class="line x" title="162:291	Here, words are compared on character level." ></td>
	<td class="line x" title="163:291	To normalize this distance into a range from 0 (for identical words) to 1 (for completely different words), we divide the absolute distance by the length of the Levenshtein alignment path." ></td>
	<td class="line x" title="164:291	4.3 Common Prefix Length Another character-based substitution cost function we studied is based on the common prefix length of both words." ></td>
	<td class="line x" title="165:291	In English, different tenses of the same verb share the same prefix; which is usually the stem." ></td>
	<td class="line x" title="166:291	The same holds for different cases, numbers and genders of most nouns and adjectives." ></td>
	<td class="line x" title="167:291	However, it does not hold if verb prefixes are changed or removed." ></td>
	<td class="line x" title="168:291	On the other hand, the common prefix length is sensitive to critical prefixes such as mis- for the same reason." ></td>
	<td class="line x" title="169:291	Consequently, the common prefix length, normalized by the average length of both words, gives a reasonable measure for the similarity of two words." ></td>
	<td class="line x" title="170:291	To transform the normalized common prefix length into costs, this fraction is then subtracted from 1." ></td>
	<td class="line x" title="171:291	Table 1 gives an example of these two worddependent substitution costs." ></td>
	<td class="line x" title="172:291	245 Table 1: Example of word-dependent substitution costs." ></td>
	<td class="line x" title="173:291	Levenshtein prefix e e distance substitution cost similarity substitution cost usual unusual 2 27 = 0.29 1 1  16 = 0.83 understanding misunderstanding 3 316 = 0.19 0 1.00 talk talks 1 15 = 0.20 4 1  44.5 = 0.11 4.4 Perspectives More sophisticated methods could be considered for word-dependent substitution costs as well." ></td>
	<td class="line oc" title="174:291	Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="175:291	5 Experimental Results 5.1 Experimental Setting The different evaluation measures were assessed experimentally on data from the ChineseEnglish and the ArabicEnglish task of the NIST 2004 evaluation workshop (Przybocki, 2004)." ></td>
	<td class="line x" title="176:291	In this evaluation campaign, 4460 and 1735 candidate translations, respectively, generated by different research MT systems were evaluated by human judges with regard to fluency and adequacy." ></td>
	<td class="line x" title="177:291	Four reference translations are provided for each candidate translation." ></td>
	<td class="line x" title="178:291	Detailed corpus statistics are listed in Table 2." ></td>
	<td class="line x" title="179:291	For the experiments in this study, the candidate translations from these tasks were evaluated using different automatic evaluation measures." ></td>
	<td class="line x" title="180:291	Pearsons correlation coefficient r between automatic evaluation and the sum of fluency and adequacy was calculated." ></td>
	<td class="line x" title="181:291	As it could be arguable whether Pearsons r is meaningful for categorical data like human MT evaluation, we have also calculated Kendalls correlation coefficient ." ></td>
	<td class="line x" title="182:291	Because of the high number of samples (= sentences, 4460) versus the low number of categories (= outcomes of adequacy+fluency, 9), we calculated  separately for each source sentence." ></td>
	<td class="line x" title="183:291	These experiments showed that Kendalls  reflects the same tendencies as Pearsons r regarding the ranking of the evaluation measures." ></td>
	<td class="line x" title="184:291	But only the latter allows for an efficient calculation of confidence intervals." ></td>
	<td class="line x" title="185:291	Consequently, figures of  are omitted in this paper." ></td>
	<td class="line x" title="186:291	Due to the small number of samples for evaluation on system level (10 and 5, respectively), all correlation coefficients between automatic and human evaluation on system level are very close to 1." ></td>
	<td class="line x" title="187:291	Therefore, they do not show any significant differences for the different evaluation Table 2: Corpus statistics." ></td>
	<td class="line x" title="188:291	TIDES corpora, NIST 2004 evaluation." ></td>
	<td class="line x" title="189:291	Source language Chinese Arabic Target language English English Sentences 446 347 Running words 13 016 10 892 Ref." ></td>
	<td class="line x" title="190:291	translations 4 4 Avg." ></td>
	<td class="line x" title="191:291	ref." ></td>
	<td class="line x" title="192:291	length 29.2 31.4 Candidate systems 10 5 measures." ></td>
	<td class="line x" title="193:291	Additional experiments on data from the NIST 2002 and 2003 workshops and from the IWSLT 2004 evaluation workshop confirm the findings from the NIST 2004 experiments; for the sake of clarity they are not included here." ></td>
	<td class="line x" title="194:291	All correlation coefficients presented here were calculated for sentence level evaluation." ></td>
	<td class="line x" title="195:291	For comparison with state-of-the-art evaluation measures, we have also calculated the correlation between human evaluation and WER and BLEU, which were both measures of choice in several international MT evaluation campaigns." ></td>
	<td class="line x" title="196:291	Furthermore, we included TER (Snover et al. , 2005) as a recent heuristic block movement measure in some of our experiments for comparison with our measure." ></td>
	<td class="line x" title="197:291	As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed." ></td>
	<td class="line x" title="198:291	Additionally, we added sentence boundary symbols for BLEU, and a different reference length calculation scheme for TER, because these changes improved the correlation between human evaluation and the two automatic measures." ></td>
	<td class="line x" title="199:291	Details on this have been described in (Leusch et al. , 2005)." ></td>
	<td class="line x" title="200:291	5.2 CDER Table 3 presents the correlation of BLEU, WER, and CDER with human assessment." ></td>
	<td class="line x" title="201:291	It can be seen that CDER shows better correlation than BLEU and WER on both corpora." ></td>
	<td class="line x" title="202:291	On the ChineseEnglish task, the smoothed BLEU score has a higher sentence-level correlation than WER." ></td>
	<td class="line x" title="203:291	However, this is not the case for the Arabic 246 Table 3: Correlation (r) between human evaluation (adequacy + fluency) and automatic evaluation with BLEU, WER, and CDER (NIST 2004 evaluation; sentence level)." ></td>
	<td class="line x" title="204:291	Automatic measure Chin.E. Arab.E. BLEU 0.615 0.603 WER 0.559 0.589 CDER 0.625 0.623 CDER reverseda 0.222 0.393 CDER maximumb 0.594 0.599 aCD constraints for candidate instead of reference." ></td>
	<td class="line x" title="205:291	bSentence-wise maximum of normal and reversed CDER Table 4: Correlation (r) between human evaluation (adequacy + fluency) and automatic evaluation for CDER with different penalties." ></td>
	<td class="line x" title="206:291	Penalty Chin.E. Arab.E.  0.625 0.623 lplen 0.581 0.567 lpmisc 0.466 0.528 (lplen + lpmisc)/2 0.534 0.557 English task." ></td>
	<td class="line x" title="207:291	So none of these two measures is superior to the other one, but they are both outperformed by CDER." ></td>
	<td class="line x" title="208:291	If the direction of CDER is reversed (i.e, the CD constraints are required for the candidate instead of the reference, such that the measure has precision instead of recall characteristics), the correlation with human evaluation is much lower." ></td>
	<td class="line x" title="209:291	Additionally we studied the use of the maximum of the distances in both directions." ></td>
	<td class="line x" title="210:291	This has a lower correlation than taking the original CDER, as Table 3 shows." ></td>
	<td class="line x" title="211:291	Nevertheless, the maximum still performs slightly better than BLEU and WER." ></td>
	<td class="line x" title="212:291	5.3 Hypothesis Length and Penalties The problem of how to avoid a preference of overly long candidate sentences by CDER remains unsolved, as can be found in Table 4: Each of the proposed penalties infers a significant decrease of correlation between the (extended) CDER and human evaluation." ></td>
	<td class="line x" title="213:291	Future research will aim at finding a suitable length penalty." ></td>
	<td class="line x" title="214:291	Especially if CDER is applied in system development, such a penalty will be needed, as preliminary optimization experiments have shown." ></td>
	<td class="line x" title="215:291	5.4 Substitution Costs Table 5 reveals that the inclusion of worddependent substitution costs yields a raise by more than 1% absolute in the correlation of CDER with human evaluation." ></td>
	<td class="line x" title="216:291	The same is true for Table 5: Correlation (r) between human evaluation (adequacy + fluency) and automatic evaluation for WER and CDER with word-dependent substitution costs." ></td>
	<td class="line x" title="217:291	Measure Subst." ></td>
	<td class="line x" title="218:291	costs Chin.E. Arab.E. WER const (1) 0.559 0.589 prefix 0.571 0.605 Levenshtein 0.580 0.611 CDER const (1) 0.625 0.623 prefix 0.637 0.634 Levenshtein 0.638 0.637 WER: the correlation with human judgment is increased by about 2% absolute on both language pairs." ></td>
	<td class="line x" title="219:291	The Levenshtein-based substitution costs are better suited for WER than the scheme based on common prefix length." ></td>
	<td class="line x" title="220:291	For CDER, there is hardly any difference between the two methods." ></td>
	<td class="line x" title="221:291	Experiments on five more corpora did not give any significant evidence which of the two substitution costs correlates better with human evaluation." ></td>
	<td class="line x" title="222:291	But as the prefix-based substitution costs improved correlation more consistently across all corpora, we employed this method in our next experiment." ></td>
	<td class="line x" title="223:291	5.5 Combination of CDER and PER An interesting topic in MT evaluation research is the question whether a linear combination of two MT evaluation measures can improve the correlation between automatic and human evaluation." ></td>
	<td class="line x" title="224:291	Particularly, we expected the combination of CDER and PER to have a significantly higher correlation with human evaluation than the measures alone." ></td>
	<td class="line x" title="225:291	CDER (as opposed to PER) has the ability to reward correct local ordering, whereas PER (as opposed to CDER) penalizes overly long candidate sentences." ></td>
	<td class="line x" title="226:291	The two measures were combined with linear interpolation." ></td>
	<td class="line x" title="227:291	In order to determine the weights, we performed data analysis on seven different corpora." ></td>
	<td class="line x" title="228:291	The result was consistent across all different data collections and language pairs: a linear combination of about 60% CDER and 40% PER has a significantly higher correlation with human evaluation than each of the measures alone." ></td>
	<td class="line x" title="229:291	For the two corpora studied here, the results of the combination can be found in Table 6: On the ChineseEnglish task, there is an additional gain of more than 1% absolute in correlation over CDER alone." ></td>
	<td class="line x" title="230:291	The combined error measure is the best method in both cases." ></td>
	<td class="line x" title="231:291	The last line in Table 6 shows the 95%confidence interval for the correlation." ></td>
	<td class="line x" title="232:291	We see that the new measure CDER, combined with PER, has a significantly higher correlation with human evaluation than the existing measures BLEU, TER, 247 Table 6: Correlation (r) between human evaluation (adequacy + fluency) and automatic evaluation for different automatic evaluation measures." ></td>
	<td class="line x" title="233:291	Automatic measure Chin.E. Arab.E. BLEU 0.615 0.603 TER 0.548 0.582 WER 0.559 0.589 WER + Lev." ></td>
	<td class="line x" title="234:291	subst." ></td>
	<td class="line x" title="235:291	0.580 0.611 CDER 0.625 0.623 CDER +prefix subst." ></td>
	<td class="line x" title="236:291	0.637 0.634 CDER +prefix+PER 0.649 0.635 95%-confidence 0.018 0.028 and WER on both corpora." ></td>
	<td class="line x" title="237:291	6 Conclusion and Outlook We presented CDER, a new automatic evaluation measure for MT, which is based on edit distance extended by block movements." ></td>
	<td class="line x" title="238:291	CDER allows for reordering blocks of words at constant cost." ></td>
	<td class="line x" title="239:291	Unlike previous block movement measures, CDER can be exactly calculated in quadratic time." ></td>
	<td class="line x" title="240:291	Experimental evaluation on two different translation tasks shows a significantly improved correlation with human judgment in comparison with state-of-the-art measures such as BLEU." ></td>
	<td class="line x" title="241:291	Additionally, we showed how word-dependent substitution costs can be applied to enhance the new error measure as well as existing approaches." ></td>
	<td class="line x" title="242:291	The highest correlation with human assessment was achieved through linear interpolation of the new CDER with PER." ></td>
	<td class="line x" title="243:291	Future work will aim at finding a suitable length penalty for CDER." ></td>
	<td class="line x" title="244:291	In addition, more sophisticated definitions of the word-dependent substitution costs will be investigated." ></td>
	<td class="line x" title="245:291	Furthermore, it will be interesting to see how this new error measure affects system development: We expect it to allow for a better sentence-wise error analysis." ></td>
	<td class="line x" title="246:291	For system optimization, preliminary experiments have shown the need for a suitable length penalty." ></td>
	<td class="line x" title="247:291	Acknowledgement This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="248:291	HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR  Technology and Corpora for Speech to Speech Translation References S. Banerjee and A. Lavie." ></td>
	<td class="line x" title="249:291	2005." ></td>
	<td class="line o" title="250:291	METEOR: An automatic metric for MT evaluation with improved correlation with human judgments." ></td>
	<td class="line x" title="251:291	ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, MI, Jun. G. Doddington." ></td>
	<td class="line x" title="252:291	2002." ></td>
	<td class="line x" title="253:291	Automatic evaluation of machine translation quality using n-gram cooccurrence statistics." ></td>
	<td class="line x" title="254:291	ARPA Workshop on Human Language Technology." ></td>
	<td class="line x" title="255:291	D. E. Knuth, 1993." ></td>
	<td class="line x" title="256:291	The Stanford GraphBase: a platformforcombinatorial computing, pages 7487." ></td>
	<td class="line x" title="257:291	ACM Press, New York, NY." ></td>
	<td class="line x" title="258:291	G. Leusch, N. Ueffing, and H. Ney." ></td>
	<td class="line x" title="259:291	2003." ></td>
	<td class="line x" title="260:291	A novel string-to-string distance measure with applications to machine translation evaluation." ></td>
	<td class="line x" title="261:291	MT Summit IX, pages 240247, New Orleans, LA, Sep. G. Leusch, N. Ueffing, D. Vilar, and H. Ney." ></td>
	<td class="line x" title="262:291	2005." ></td>
	<td class="line x" title="263:291	Preprocessing and normalization for automatic evaluation of machine translation." ></td>
	<td class="line x" title="264:291	ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 1724, Ann Arbor, MI, Jun. V. I. Levenshtein." ></td>
	<td class="line x" title="265:291	1966." ></td>
	<td class="line x" title="266:291	Binary codes capable of correcting deletions, insertions and reversals." ></td>
	<td class="line x" title="267:291	Soviet Physics Doklady, 10(8):707710, Feb. C.-Y." ></td>
	<td class="line x" title="268:291	Lin and F. J. Och." ></td>
	<td class="line x" title="269:291	2004." ></td>
	<td class="line x" title="270:291	Orange: a method for evaluation automatic evaluation metrics for machine translation." ></td>
	<td class="line x" title="271:291	COLING 2004, pages 501 507, Geneva, Switzerland, Aug. D. Lopresti and A. Tomkins." ></td>
	<td class="line x" title="272:291	1997." ></td>
	<td class="line x" title="273:291	Block edit models for approximate string matching." ></td>
	<td class="line x" title="274:291	Theoretical Computer Science, 181(1):159179, Jul. K. Papineni, S. Roukos, T. Ward, and W.-J." ></td>
	<td class="line x" title="275:291	Zhu." ></td>
	<td class="line x" title="276:291	2002." ></td>
	<td class="line x" title="277:291	BLEU: a method for automatic evaluation of machine translation." ></td>
	<td class="line x" title="278:291	40th Annual Meeting of the ACL, pages 311318, Philadelphia, PA, Jul. M. Przybocki." ></td>
	<td class="line x" title="279:291	2004." ></td>
	<td class="line x" title="280:291	NIST machine translation 2004 evaluation: Summary of results." ></td>
	<td class="line x" title="281:291	DARPA Machine Translation Evaluation Workshop, Alexandria, VA. M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micciulla, and R. Weischedel." ></td>
	<td class="line x" title="282:291	2005." ></td>
	<td class="line x" title="283:291	A study of translation error rate with targeted human annotation." ></td>
	<td class="line x" title="284:291	Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland, College Park, MD. C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf." ></td>
	<td class="line x" title="285:291	1997." ></td>
	<td class="line x" title="286:291	Accelerated DP based search for statistical translation." ></td>
	<td class="line x" title="287:291	European Conf." ></td>
	<td class="line x" title="288:291	on Speech Communication and Technology, pages 26672670, Rhodes, Greece, Sep. J. P. Turian, L. Shen, and I. D. Melamed." ></td>
	<td class="line x" title="289:291	2003." ></td>
	<td class="line x" title="290:291	Evaluation of machine translation and its evaluation." ></td>
	<td class="line x" title="291:291	MTSummitIX, pages 2328, New Orleans, LA, Sep. 248" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E06-1032
Re-Evaluation The Role Of Bleu In Machine Translation Research
Callison-Burch, Chris;Osborne, Miles;Koehn, Philipp;"></td>
	<td class="line x" title="1:157	Re-evaluating the Role of BLEU in Machine Translation Research Chris Callison-Burch Miles Osborne Philipp Koehn School on Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW callison-burch@ed.ac.uk Abstract We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric." ></td>
	<td class="line x" title="2:157	We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleus correlation with human judgments of quality." ></td>
	<td class="line x" title="3:157	This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores." ></td>
	<td class="line x" title="4:157	1 Introduction Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al. , 2002)." ></td>
	<td class="line x" title="5:157	The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003)." ></td>
	<td class="line x" title="6:157	Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations." ></td>
	<td class="line x" title="7:157	Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation." ></td>
	<td class="line x" title="8:157	All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003)." ></td>
	<td class="line x" title="9:157	However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements." ></td>
	<td class="line x" title="10:157	If Bleus correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is. In this paper we give a number of counterexamples for Bleus correlation with human judgments." ></td>
	<td class="line x" title="11:157	We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality." ></td>
	<td class="line x" title="12:157	We argue that Bleu is insufficient by showing that Bleu admits a huge amount of variation for identically scored hypotheses." ></td>
	<td class="line x" title="13:157	Typically there are millions of variations on a hypothesis translation that receive the same Bleu score." ></td>
	<td class="line x" title="14:157	Because not all these variations are equally grammatically or semantically plausible there are translations which have the same Bleu score but a worse human evaluation." ></td>
	<td class="line x" title="15:157	We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examplesofBleuvastlyunderestimatingthetranslation quality of systems." ></td>
	<td class="line x" title="16:157	Finally, we discuss appropriate uses for Bleu and suggest that for some research projects it may be preferable to use a focused, manual evaluation instead." ></td>
	<td class="line x" title="17:157	2 BLEU Detailed The rationale behind the development of Bleu (Papinenietal., 2002)isthathumanevaluationofmachine translation can be time consuming and expensive." ></td>
	<td class="line x" title="19:157	An automatic evaluation metric, on the other hand, can be used for frequent tasks like monitoringincrementalsystemchangesduringdevelopment, which are seemingly infeasible in a manual evaluation setting." ></td>
	<td class="line x" title="20:157	The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations." ></td>
	<td class="line x" title="21:157	Machine translation evaluation metrics differ from other metrics that use a reference, like the word error rate metric that is used 249 Orejuela appeared calm as he was led to the American plane which will take him to Miami, Florida." ></td>
	<td class="line x" title="22:157	Orejuela appeared calm while being escorted to the plane that would take him to Miami, Florida." ></td>
	<td class="line x" title="23:157	Orejuela appeared calm as he was being led to the American plane that was to carry him to Miami in Florida." ></td>
	<td class="line x" title="24:157	Orejuela seemed quite calm as he was being led to the American plane that would take him to Miami in Florida." ></td>
	<td class="line x" title="25:157	Appeared calm when he was taken to the American plane, which will to Miami, Florida." ></td>
	<td class="line x" title="26:157	Table 1: A set of four reference translations, and a hypothesis translation from the 2005 NIST MT Evaluation in speech recognition, because translations have a degree of variation in terms of word choice and in terms of variant ordering of some phrases." ></td>
	<td class="line x" title="27:157	Bleu attempts to capture allowable variation in word choice through the use of multiple reference translations (as proposed in Thompson (1991))." ></td>
	<td class="line x" title="28:157	In order to overcome the problem of variation in phrase order, Bleu uses modified n-gram precision instead of WERs more strict string edit distance." ></td>
	<td class="line x" title="29:157	Bleus n-gram precision is modified to eliminate repetitions that occur across sentences." ></td>
	<td class="line x" title="30:157	For example, even though the bigram to Miami is repeated across all four reference translations in Table 1, it is counted only once in a hypothesis translation." ></td>
	<td class="line x" title="31:157	Table 2 shows the n-gram sets created from the reference translations." ></td>
	<td class="line x" title="32:157	Papineni et al.(2002) calculate their modified precision score, pn, for each n-gram length by summing over the matches for every hypothesis sentence S in the complete corpus C as: pn = summationtext SC summationtext ngramS Countmatched(ngram)summationtext SC summationtext ngramS Count(ngram) Counting punctuation marks as separate tokens, the hypothesis translation given in Table 1 has 15 unigram matches, 10 bigram matches, 5 trigram matches (these are shown in bold in Table 2), and three 4-gram matches (not shown)." ></td>
	<td class="line x" title="34:157	The hypothesis translation contains a total of 18 unigrams, 17 bigrams, 16 trigrams, and 15 4-grams." ></td>
	<td class="line x" title="35:157	If the complete corpus consisted of this single sentence 1-grams: American, Florida, Miami, Orejuela, appeared, as, being, calm, carry, escorted, he, him, in, led, plane, quite, seemed, take, that, the, to, to, to, was, was, which, while, will, would,,,." ></td>
	<td class="line x" title="36:157	2-grams: American plane, Florida ., Miami,, Miami in, Orejuela appeared, Orejuela seemed, appeared calm, ashe, beingescorted, beingled, calmas, calmwhile, carry him, escorted to, he was, him to, in Florida, led to, plane that, plane which, quite calm, seemed quite, take him, that was, that would, the American, the plane, to Miami, to carry,tothe, wasbeing, wasled, wasto,whichwill, while being, will take, would take,, Florida 3-grams: American plane that, American plane which, Miami, Florida, Miami in Florida, Orejuela appeared calm, Orejuela seemed quite, appeared calm as, appeared calmwhile,ashewas,beingescortedto,beingledto,calm as he, calm while being, carry him to, escorted to the, he was being, he was led, him to Miami, in Florida ., led to the, plane that was, plane that would, plane which will, quite calm as, seemed quite calm, take him to, that was to, that would take, the American plane, the plane that, to Miami,, to Miami in, to carry him, to the American, to the plane, was being led, was led to, was to carry, which will take, while being escorted, will take him, would take him,, Florida . Table 2: The n-grams extracted from the reference translations, with matches from the hypothesis translation in bold then the modified precisions would be p1 = .83, p2 = .59, p3 = .31, and p4 = .2." ></td>
	<td class="line x" title="37:157	Each pn is combined and can be weighted by specifying a weight wn." ></td>
	<td class="line x" title="38:157	In practice each pn is generally assigned an equal weight." ></td>
	<td class="line x" title="39:157	Because Bleu is precision based, and because recall is difficult to formulate over multiple reference translations, a brevity penalty is introduced to compensate for the possibility of proposing highprecision hypothesis translations which are too short." ></td>
	<td class="line x" title="40:157	The brevity penalty is calculated as: BP = braceleftBigg 1 if c > r e1r/c if c  r where c is the length of the corpus of hypothesis translations, and r is the effective reference corpus length.1 Thus, the Bleu score is calculated as Bleu = BP  exp( Nsummationdisplay n=1 wn logpn) A Bleu score can range from 0 to 1, where higher scores indicate closer matches to the reference translations, and where a score of 1 is assigned to a hypothesis translation which exactly 1The effective reference corpus length is calculated as the sum of the single reference translation from each set which is closest to the hypothesis translation." ></td>
	<td class="line x" title="41:157	250 matches one of the reference translations." ></td>
	<td class="line x" title="42:157	A score of 1 is also assigned to a hypothesis translation which has matches for all its n-grams (up to the maximum n measured by Bleu) in the clipped reference n-grams, and which has no brevity penalty." ></td>
	<td class="line x" title="43:157	TheprimaryreasonthatBleuisviewedasauseful stand-in for manual evaluation is that it has been shown to correlate with human judgments of translation quality." ></td>
	<td class="line x" title="44:157	Papineni et al.(2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations." ></td>
	<td class="line x" title="46:157	Bleus correlation with human judgments has been further tested in the annual NIST Machine Translation Evaluation exercise wherein Bleus rankings of Arabic-to-English and Chinese-to-English systems is verified by manual evaluation." ></td>
	<td class="line x" title="47:157	In the next section we discuss theoretical reasons why Bleu may not always correlate with human judgments." ></td>
	<td class="line x" title="48:157	3 Variations Allowed By BLEU WhileBleuattemptstocaptureallowablevariation in translation, it goes much further than it should." ></td>
	<td class="line x" title="49:157	In order to allow some amount of variant order in phrases, Bleu places no explicit constraints on the order that matching n-grams occur in." ></td>
	<td class="line x" title="50:157	To allow variation in word choice in translation Bleu uses multiple reference translations, but puts very few constraints on how n-gram matches can be drawn from the multiple reference translations." ></td>
	<td class="line x" title="51:157	Because Bleu is underconstrained in these ways, it allows a tremendous amount of variation  far beyond what could reasonably be considered acceptable variation in translation." ></td>
	<td class="line x" title="52:157	Inthissectionweexaminevariouspermutations and substitutions allowed by Bleu." ></td>
	<td class="line x" title="53:157	We show that foranaveragehypothesistranslationtherearemillions of possible variants that would each receive a similar Bleu score." ></td>
	<td class="line x" title="54:157	We argue that because the number of translations that score the same is so large, it is unlikely that all of them will be judged to be identical in quality by human annotators." ></td>
	<td class="line x" title="55:157	This means that it is possible to have items which receive identical Bleu scores but are judged by humans to be worse." ></td>
	<td class="line x" title="56:157	It is also therefore possible to have a higher Bleu score without any genuine improvement in translation quality." ></td>
	<td class="line x" title="57:157	In Sections 3.1 and 3.2 we examine ways of synthetically producing such variant translations." ></td>
	<td class="line x" title="58:157	3.1 Permuting phrases One way in which variation can be introduced is by permuting phrases within a hypothesis translation." ></td>
	<td class="line x" title="59:157	A simple way of estimating a lower bound on thenumber ofways thatphrases ina hypothesis translation can be reordered is to examine bigram mismatches." ></td>
	<td class="line x" title="60:157	Phrases that are bracketed by these bigram mismatch sites can be freely permuted because reordering a hypothesis translation at these points will not reduce the number of matching ngrams and thus will not reduce the overall Bleu score." ></td>
	<td class="line x" title="61:157	Here we denote bigram mismatches for the hypothesis translation given in Table 1 with vertical bars: Appeared calm | when | he was | taken | to the American plane |, | which will | to Miami, Florida . We can randomly produce other hypothesis translations that have the same Bleu score but are radically different from each other." ></td>
	<td class="line x" title="62:157	Because Bleu only takes order into account through rewarding matches of higher order n-grams, a hypothesis sentence may be freely permuted around these bigram mismatch sites and without reducing the Bleu score." ></td>
	<td class="line x" title="63:157	Thus: which will | he was |, | when | taken | Appeared calm | to the American plane | to Miami, Florida . receives an identical score to the hypothesis translation in Table 1." ></td>
	<td class="line x" title="64:157	If b is the number of bigram matches in a hypothesis translation, and k is its length, then there are (k b)!" ></td>
	<td class="line x" title="65:157	(1) possible ways to generate similarly scored items using only the words in the hypothesis translation.2 Thus for the example hypothesis translation there are at least 40,320 different ways of permuting the sentence and receiving a similar Bleu score." ></td>
	<td class="line x" title="66:157	The number of permutations varies with respect to sentence length and number of bigram mismatches." ></td>
	<td class="line x" title="67:157	Therefore as a hypothesis translation approaches being an identical match to one of the reference translations, the amount of variance decreases significantly." ></td>
	<td class="line x" title="68:157	So, as translations improve 2Note that in some cases randomly permuting the sentence in this way may actually result in a greater number of n-gram matches; however, one would not expect random permutation to increase the human evaluation." ></td>
	<td class="line x" title="69:157	251 0 20 40 60 80 100 120 1 1e+10 1e+20 1e+30 1e+40 1e+50 1e+60 1e+70 1e+80 Sentence Length Number of Permutations Figure 1: Scatterplot of the length of each translation against its number of possible permutations due to bigram mismatches for an entry in the 2005 NIST MT Eval spurious variation goes down." ></td>
	<td class="line x" title="70:157	However, at todays levels the amount of variation that Bleu admits is unacceptably high." ></td>
	<td class="line x" title="71:157	Figure 1 gives a scatterplot of each of the hypothesis translations produced by the second best Bleu system from the 2005 NIST MT Evaluation." ></td>
	<td class="line x" title="72:157	The number of possible permutations for some translations is greater than 1073." ></td>
	<td class="line x" title="73:157	3.2 Drawing different items from the reference set In addition to the factorial number of ways that similarly scored Bleu items can be generated by permuting phrases around bigram mismatch points, additional variation may be synthesized by drawing different items from the reference ngrams." ></td>
	<td class="line x" title="74:157	For example, since the hypothesis translation from Table 1 has a length of 18 with 15 unigram matches, 10 bigram matches, 5 trigram matches, and three 4-gram matches, we can artificially construct an identically scored hypothesis by drawing an identical number of matching ngrams from the reference translations." ></td>
	<td class="line x" title="75:157	Therefore the far less plausible: was being led to the | calm as he was | would take | carry him | seemed quite | when | taken would receive the same Bleu score as the hypothesis translation from Table 1, even though human judges would assign it a much lower score." ></td>
	<td class="line x" title="76:157	This problem is made worse by the fact that Bleu equally weights all items in the reference sentences (Babych and Hartley, 2004)." ></td>
	<td class="line x" title="77:157	Therefore omitting content-bearing lexical items does not carry a greater penalty than omitting function words." ></td>
	<td class="line x" title="78:157	The problem is further exacerbated by Bleu not having any facilities for matching synonyms or lexicalvariants." ></td>
	<td class="line x" title="79:157	Thereforewordsinthehypothesis that did not appear in the references (such as when and taken in the hypothesis from Table 1) can be substituted with arbitrary words because they do notcontributetowardstheBleuscore." ></td>
	<td class="line x" title="80:157	UnderBleu, we could just as validly use the words black and helicopters as we could when and taken." ></td>
	<td class="line x" title="81:157	The lack of recall combined with naive token identity means that there can be overlap between similar items in the multiple reference translations." ></td>
	<td class="line x" title="82:157	For example we can produce a translation which contains both the words carry and take even though they arise from the same source word." ></td>
	<td class="line x" title="83:157	The chance of problems of this sort being introduced increases as we add more reference translations." ></td>
	<td class="line x" title="84:157	3.3 Implication: BLEU cannot guarantee correlation with human judgments Bleus inability to distinguish between randomly generated variations in translation hints that it may not correlate with human judgments of translation quality in some cases." ></td>
	<td class="line x" title="85:157	As the number of identically scored variants goes up, the likelihood that they would all be judged equally plausible goes down." ></td>
	<td class="line x" title="86:157	This is a theoretical point, and while the variants are artificially constructed, it does highlight the fact that Bleu is quite a crude measurement of translation quality." ></td>
	<td class="line x" title="87:157	A number of prominent factors contribute to Bleus crudeness:  Synonyms and paraphrases are only handled if they are in the set of multiple reference translations." ></td>
	<td class="line x" title="88:157	 The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty." ></td>
	<td class="line x" title="89:157	 The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall." ></td>
	<td class="line x" title="90:157	Each of these failures contributes to an increased amount of inappropriately indistinguishable translations in the analysis presented above." ></td>
	<td class="line x" title="91:157	Given that Bleu can theoretically assign equal scoring to translations of obvious different quality, it is logical that a higher Bleu score may not 252 Fluency How do you judge the fluency of this translation?" ></td>
	<td class="line x" title="92:157	5 = Flawless English 4 = Good English 3 = Non-native English 2 = Disfluent English 1 = Incomprehensible Adequacy How much of the meaning expressed in the reference translation is also expressed in the hypothesis translation?" ></td>
	<td class="line x" title="93:157	5 = All 4 = Most 3 = Much 2 = Little 1 = None Table 3: The scales for manually assigned adequacy and fluency scores necessarily be indicative of a genuine improvement in translation quality." ></td>
	<td class="line x" title="94:157	This begs the question as to whether this is only a theoretical concern or whether Bleus inadequacies can come into play in practice." ></td>
	<td class="line x" title="95:157	In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice." ></td>
	<td class="line x" title="96:157	4 Failures in Practice: the 2005 NIST MT Eval, and Systran v. SMT The NIST Machine Translation Evaluation exercise has run annually for the past five years as part of DARPAs TIDES program." ></td>
	<td class="line x" title="97:157	The quality of Chinese-to-English and Arabic-to-English translation systems is evaluated both by using Bleu score and by conducting a manual evaluation." ></td>
	<td class="line x" title="98:157	As such, the NIST MT Eval provides an excellent source of data that allows Bleus correlation with human judgments to be verified." ></td>
	<td class="line x" title="99:157	Last years evaluation exercise (Lee and Przybocki, 2005) was startling in that Bleus rankings of the ArabicEnglish translation systems failed to fully correspond to the manual evaluation." ></td>
	<td class="line x" title="100:157	In particular, the entry that was ranked 1st in the human evaluation was ranked 6th by Bleu." ></td>
	<td class="line x" title="101:157	In this section we examine Bleus failure to correctly rank this entry." ></td>
	<td class="line x" title="102:157	The manual evaluation conducted for the NIST MT Eval is done by English speakers without reference to the original Arabic or Chinese documents." ></td>
	<td class="line x" title="103:157	Two judges assigned each sentence in Iran has already stated that Kharazis statements to the conference because of the Jordanian King Abdullah II in which he stood accused Iran of interfering in Iraqi affairs." ></td>
	<td class="line x" title="104:157	n-gram matches: 27 unigrams, 20 bigrams, 15 trigrams, and ten 4-grams human scores: Adequacy:3,2 Fluency:3,2 Iran already announced that Kharrazi will not attend the conference because of the statements made by the Jordanian Monarch Abdullah II who has accused Iran of interfering in Iraqi affairs." ></td>
	<td class="line x" title="105:157	n-gram matches: 24 unigrams, 19 bigrams, 15 trigrams, and 12 4-grams human scores: Adequacy:5,4 Fluency:5,4 Reference: Iran had already announced Kharazi would boycott the conference after Jordans King Abdullah II accused Iran of meddling in Iraqs affairs." ></td>
	<td class="line x" title="106:157	Table 4: Two hypothesis translations with similar Bleu scores but different human scores, and one of four reference translations the hypothesis translations a subjective 15 score along two axes: adequacy and fluency (LDC, 2005)." ></td>
	<td class="line x" title="107:157	Table 3 gives the interpretations of the scores." ></td>
	<td class="line x" title="108:157	When first evaluating fluency, the judges are shown only the hypothesis translation." ></td>
	<td class="line x" title="109:157	They are then shown a reference translation and are asked to judge the adequacy of the hypothesis sentences." ></td>
	<td class="line x" title="110:157	Table 4 gives a comparison between the output of the system that was ranked 2nd by Bleu3 (top) and of the entry that was ranked 6th in Bleu but 1st in the human evaluation (bottom)." ></td>
	<td class="line x" title="111:157	The example is interesting because the number of matching n-grams for the two hypothesis translations is roughly similar but the human scores are quite different." ></td>
	<td class="line x" title="112:157	The first hypothesis is less adequate because it fails to indicated that Kharazi is boycotting the conference, and because it inserts the word stood before accused which makes the Abdullahs actions less clear." ></td>
	<td class="line x" title="113:157	The second hypothesis contains all of the information of the reference, but uses some synonyms and paraphrases which would not picked up on by Bleu: will not attend for would boycott and interfering for meddling." ></td>
	<td class="line x" title="114:157	3The output of the system that was ranked 1st by Bleu is not publicly available." ></td>
	<td class="line x" title="115:157	253 2 2.5 3 3.5 4 0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.52 Human Score Bleu Score Adequacy Correlation Figure 2: Bleu scores plotted against human judgments of adequacy, with R2 = 0.14 when the outlier entry is included Figures 2 and 3 plot the average human score for each of the seven NIST entries against its Bleu score." ></td>
	<td class="line x" title="116:157	It is notable that one entry received a much higher human score than would be anticipated from its low Bleu score." ></td>
	<td class="line x" title="117:157	The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabicsourcesentencesandpost-editingtheresult (Callison-Burch, 2005)." ></td>
	<td class="line x" title="118:157	The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models feature functions (Och and Ney, 2002)." ></td>
	<td class="line x" title="119:157	This opens the possibility that in order for Bleu to bevalid onlysufficiently similarsystems should be compared with one another." ></td>
	<td class="line x" title="120:157	For instance, when measuring correlation using Pearsons we get a very low correlation of R2 = 0.14 when the outlier in Figure 2 is included, but a strongR2 = 0.87 when it is excluded." ></td>
	<td class="line x" title="121:157	Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742." ></td>
	<td class="line x" title="122:157	Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph." ></td>
	<td class="line x" title="123:157	We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu cor2 2.5 3 3.5 4 0.38 0.4 0.42 0.44 0.46 0.48 0.5 0.52 Human Score Bleu Score Fluency Correlation Figure 3: Bleu scores plotted against human judgments of fluency, with R2 = 0.002 when the outlier entry is included rectly ranked the systems." ></td>
	<td class="line x" title="124:157	We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems." ></td>
	<td class="line x" title="125:157	We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004)." ></td>
	<td class="line x" title="126:157	We evaluated it and the Systran system with Bleu using a set of 2,000 held out sentence pairs, using the same normalization and tokenization schemes on both systems output." ></td>
	<td class="line x" title="127:157	We then built a number of SMT systems with various portions of the training corpus, and selected one that was trained with 164 of the data, which had a Bleu score that was close to, but still higher than that for the rule-based system." ></td>
	<td class="line x" title="128:157	We then performed a manual evaluation where we had three judges assign fluency and adequacy ratings for the English translations of 300 French sentences for each of the three systems." ></td>
	<td class="line x" title="129:157	These scores are plotted against the systems Bleu scores in Figure 4." ></td>
	<td class="line x" title="130:157	The graph shows that the Bleu score for the rule-based system (Systran) vastly underestimates its actual quality." ></td>
	<td class="line x" title="131:157	This serves as another significant counter-example to Bleus correlation with human judgments of translation quality, and further increases the concern that Bleu may not be appropriate for comparing systems which employ different translation strategies." ></td>
	<td class="line x" title="132:157	254 2 2.5 3 3.5 4 4.5 0.18 0.2 0.22 0.24 0.26 0.28 0.3 Human Score Bleu Score Adequacy Fluency SMT System 1 SMT System 2 Rule-based System (Systran) Figure 4: Bleu scores plotted against human judgments of fluency and adequacy, showing that Bleu vastly underestimates the quality of a nonstatistical system 5 Related Work A number of projects in the past have looked into ways of extending and improving the Bleu metric." ></td>
	<td class="line x" title="133:157	Doddington(2002)suggestedchangingBleus weighted geometric average of n-gram matches to an arithmetic average, and calculating the brevity penalty in a slightly different manner." ></td>
	<td class="line x" title="134:157	Hovy and Ravichandra (2003) suggested increasing Bleus sensitivity to inappropriate phrase movement by matchingpart-of-speechtagsequencesagainstreference translations in addition to Bleus n-gram matches." ></td>
	<td class="line x" title="135:157	Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases." ></td>
	<td class="line x" title="136:157	Twoalternativeautomatictranslationevaluation metrics do a much better job at incorporating recall than Bleu does." ></td>
	<td class="line x" title="137:157	Melamed et al.(2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty." ></td>
	<td class="line oc" title="139:157	Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match." ></td>
	<td class="line x" title="140:157	LinandHovy(2003)aswellasSoricutandBrill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization." ></td>
	<td class="line n" title="141:157	Both these approaches potentially suffer from the same weaknesses that Bleu has in machine translation evaluation." ></td>
	<td class="line x" title="142:157	Coughlin (2003) performs a large-scale investigation of Bleus correlation with human judgments, and finds one example that fails to correlate." ></td>
	<td class="line x" title="143:157	Her future work section suggests that she has preliminary evidence that statistical machine translation systems receive a higher Bleu score than their non-n-gram-based counterparts." ></td>
	<td class="line x" title="144:157	6 Conclusions In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do." ></td>
	<td class="line x" title="145:157	We have shown that Bleus rather coarse model of allowable variation in translation can mean that an improved Bleu score is not sufficient to reflect a genuine improvement in translation quality." ></td>
	<td class="line x" title="146:157	We have further shown that it is not necessary to receive a higher Bleu score in order to be judged to have better translation quality by human subjects, as illustrated in the 2005 NIST Machine Translation Evaluation and our experiment manually evaluating Systran and SMT translations." ></td>
	<td class="line x" title="147:157	What conclusions can we draw from this?" ></td>
	<td class="line x" title="148:157	Should we give up on using Bleu entirely?" ></td>
	<td class="line x" title="149:157	We think that the advantages of Bleu are still very strong; automatic evaluation metrics are inexpensive, and do allow many tasks to be performed that would otherwise be impossible." ></td>
	<td class="line x" title="150:157	The important thing therefore is to recognize which uses of Bleu are appropriate and which uses are not." ></td>
	<td class="line x" title="151:157	Appropriate uses for Bleu include tracking broad, incremental changes to a single system, comparing systems which employ similar translation strategies (such as comparing phrase-based statistical machine translation systems with other phrase-based statistical machine translation systems), and using Bleu as an objective function to optimize the values of parameters such as feature weights in log linear translation models, until a better metric has been proposed." ></td>
	<td class="line x" title="152:157	Inappropriate uses for Bleu include comparing systems which employ radically different strategies(especiallycomparingphrase-basedstatistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus." ></td>
	<td class="line x" title="153:157	These comments do not apply solely to Bleu." ></td>
	<td class="line nc" title="154:157	255 Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al. , 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation." ></td>
	<td class="line x" title="155:157	Finally, that the fact that Bleus correlation with human judgments has been drawn into question may warrant a re-examination of past work which failed to show improvements in Bleu." ></td>
	<td class="line x" title="156:157	For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al. , 2003; Och et al. , 2004) may deserve a second look with a more targeted manual evaluation." ></td>
	<td class="line x" title="157:157	Acknowledgments The authors are grateful to Amittai Axelrod, Frank Keller, Beata Kouchnir, Jean Senellart, and Matthew Stone for their feedback on drafts of this paper, and to Systran for providing translations of the Europarl test set." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N06-1058
Paraphrasing For Automatic Evaluation
Kauchak, David;Barzilay, Regina;"></td>
	<td class="line x" title="1:238	Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 455462, New York, June 2006." ></td>
	<td class="line x" title="2:238	c2006 Association for Computational Linguistics Paraphrasing for Automatic Evaluation David Kauchak Department of Computer Science University of California, San Diego dkauchak@cs.ucsd.edu Regina Barzilay CSAIL Massachusetts Institute of Technology regina@csail.mit.edu Abstract This paper studies the impact of paraphrases on the accuracy of automatic evaluation." ></td>
	<td class="line x" title="3:238	Given a reference sentence and a machine-generated sentence, we seek to nd a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference." ></td>
	<td class="line x" title="4:238	We apply our paraphrasing method in the context of machine translation evaluation." ></td>
	<td class="line x" title="5:238	Our experiments show that the use of a paraphrased synthetic reference re nes the accuracy of automatic evaluation." ></td>
	<td class="line x" title="6:238	We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation." ></td>
	<td class="line x" title="7:238	1 Introduction The use of automatic methods for evaluating machine-generated text is quickly becoming mainstream in natural language processing." ></td>
	<td class="line x" title="8:238	The most notable examples in this category include measures such as BLEU and ROUGE which drive research in the machine translation and text summarization communities." ></td>
	<td class="line x" title="9:238	These methods assess the quality of a machine-generated output by considering its similarity to a reference text written by a human." ></td>
	<td class="line x" title="10:238	Ideally, the similarity would re ect the semantic proximity between the two." ></td>
	<td class="line x" title="11:238	In practice, this comparison breaks down to n-gram overlap between the reference and the machine output." ></td>
	<td class="line x" title="12:238	1a." ></td>
	<td class="line x" title="13:238	However, Israels reply failed to completely clear the U.S. suspicions." ></td>
	<td class="line x" title="14:238	1b." ></td>
	<td class="line x" title="15:238	However, Israeli answer unable to fully remove the doubts." ></td>
	<td class="line x" title="16:238	Table 1: A reference sentence and corresponding machine translation from the NIST 2004 MT evaluation." ></td>
	<td class="line x" title="17:238	Consider the human-written translation and the machine translation of the same Chinese sentence shown in Table 1." ></td>
	<td class="line x" title="18:238	While the two translations convey the same meaning, they share only auxiliary words." ></td>
	<td class="line x" title="19:238	Clearly, any measure based on word overlap will penalize a system for generating such a sentence." ></td>
	<td class="line x" title="20:238	The question is whether such cases are common phenomena or infrequent exceptions." ></td>
	<td class="line x" title="21:238	Empirical evidence supports the former." ></td>
	<td class="line x" title="22:238	Analyzing 10,728 reference translation pairs1 used in the NIST 2004 machine translation evaluation, we found that only 21 (less than 0.2%) of them are identical." ></td>
	<td class="line x" title="23:238	Moreover, 60% of the pairs differ in at least 11 words." ></td>
	<td class="line x" title="24:238	These statistics suggest that without accounting for paraphrases, automatic evaluation measures may never reach the accuracy of human evaluation." ></td>
	<td class="line x" title="25:238	As a solution to this problem, researchers use multiple references to re ne automatic evaluation." ></td>
	<td class="line x" title="26:238	Papineni et al.(2002) shows that expanding the number of references reduces the gap between automatic and human evaluation." ></td>
	<td class="line x" title="28:238	However, very few human annotated sets are augmented with multiple references and those that are available are relatively 1Each pair included different translations of the same sentence, produced by two human translators." ></td>
	<td class="line x" title="29:238	455 small in size." ></td>
	<td class="line x" title="30:238	Moreover, access to several references does not guarantee that the references will include the same words that appear in machine-generated sentences." ></td>
	<td class="line x" title="31:238	In this paper, we explore the use of paraphrasing methods for re nement of automatic evaluation techniques." ></td>
	<td class="line x" title="32:238	Given a reference sentence and a machine-generated sentence, we seek to nd a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference." ></td>
	<td class="line x" title="33:238	For instance, given the pair of sentences in Table 1, we automatically transform the reference sentence (1a)." ></td>
	<td class="line x" title="34:238	into However, Israels answer failed to completely remove the U.S. suspicions." ></td>
	<td class="line x" title="35:238	Thus, among many possible paraphrases of the reference, we are interested only in those that use words appearing in the system output." ></td>
	<td class="line x" title="36:238	Our paraphrasing algorithm is based on the substitute in context strategy." ></td>
	<td class="line x" title="37:238	First, the algorithm identi es pairs of words from the reference and the system output that could potentially form paraphrases." ></td>
	<td class="line x" title="38:238	We select these candidates using existing lexico-semantic resources such as WordNet." ></td>
	<td class="line x" title="39:238	Next, the algorithm tests whether the candidate paraphrase is admissible in the context of the reference sentence." ></td>
	<td class="line x" title="40:238	Since even synonyms cannot be substituted in any context (Edmonds and Hirst, 2002), this ltering step is necessary." ></td>
	<td class="line x" title="41:238	We predict whether a word is appropriate in a new context by analyzing its distributional properties in a large body of text." ></td>
	<td class="line x" title="42:238	Finally, paraphrases that pass the ltering stage are used to rewrite the reference sentence." ></td>
	<td class="line x" title="43:238	We apply our paraphrasing method in the context of machine translation evaluation." ></td>
	<td class="line x" title="44:238	Using this strategy, we generate a new sentence for every pair of human and machine translated sentences." ></td>
	<td class="line x" title="45:238	This synthetic reference then replaces the original human reference in automatic evaluation." ></td>
	<td class="line x" title="46:238	The key ndings of our work are as follows: (1) Automatically generated paraphrases improve the accuracy of the automatic evaluation methods." ></td>
	<td class="line x" title="47:238	Our experiments show that evaluation based on paraphrased references gives a better approximation of human judgments than evaluation that uses original references." ></td>
	<td class="line x" title="48:238	(2) The quality of automatic paraphrases determines their contribution to automatic evaluation." ></td>
	<td class="line x" title="49:238	By analyzing several paraphrasing resources, we found that the accuracy and coverage of a paraphrasing method correlate with its utility for automatic MT evaluation." ></td>
	<td class="line x" title="50:238	Our results suggest that researchers may nd it useful to augment standard measures such as BLEU and ROUGE with paraphrasing information thereby taking more semantic knowledge into account." ></td>
	<td class="line x" title="51:238	In the following section, we provide an overview of existing work on automatic paraphrasing." ></td>
	<td class="line x" title="52:238	We then describe our paraphrasing algorithm and explain how it can be used in an automatic evaluation setting." ></td>
	<td class="line x" title="53:238	Next, we present our experimental framework and data and conclude by presenting and discussing our results." ></td>
	<td class="line x" title="54:238	2 Related Work Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al. , 2003; Quirk et al. , 2004)." ></td>
	<td class="line x" title="55:238	Most of these approaches learn paraphrases from a parallel or comparable monolingual corpora." ></td>
	<td class="line x" title="56:238	Instances of such corpora include multiple English translations of the same source text written in a foreign language, and different news articles about the same event." ></td>
	<td class="line x" title="57:238	For example, Pang et al.(2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation." ></td>
	<td class="line x" title="59:238	Our approach differs from traditional work on automatic paraphrasing in goal and methodology." ></td>
	<td class="line x" title="60:238	Unlike previous approaches, we are not aiming to produce any paraphrase of a given sentence since paraphrases induced from a parallel corpus do not necessarily produce a rewriting that makes a reference closer to the system output." ></td>
	<td class="line x" title="61:238	Thus, we focus on words that appear in the system output and aim to determine whether they can be used to rewrite a reference sentence." ></td>
	<td class="line x" title="62:238	Our work also has interesting connections with research on automatic textual entailment (Dagan et al. , 2005), where the goal is to determine whether a given sentence can be inferred from text." ></td>
	<td class="line x" title="63:238	While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges." ></td>
	<td class="line x" title="64:238	Methods for entailment 456 recognition extensively rely on lexico-semantic resources (Haghighi et al. , 2005; Harabagiu et al. , 2001), and we believe that our method for contextual substitution can be bene cial in that context." ></td>
	<td class="line x" title="65:238	Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al. , 2003; Papineni et al. , 2002)." ></td>
	<td class="line x" title="66:238	All these metrics compute n-gram overlap between a reference and a system output, but measure the overlap in different ways." ></td>
	<td class="line x" title="67:238	Our method for reference paraphrasing can be combined with any of these metrics." ></td>
	<td class="line x" title="68:238	In this paper, we report experiments with BLEU due to its wide use in the machine translation community." ></td>
	<td class="line p" title="69:238	Recently, researchers have explored additional knowledge sources that could enhance automatic evaluation." ></td>
	<td class="line oc" title="70:238	Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005)." ></td>
	<td class="line o" title="71:238	Our work complements these approaches: we focus on the impact of paraphrases, and study their contribution to the accuracy of automatic evaluation." ></td>
	<td class="line x" title="72:238	3 Methods The input to our method consists of a reference sentence R = r1." ></td>
	<td class="line x" title="73:238	rm and a system-generated sentence W = w1 . . ." ></td>
	<td class="line x" title="74:238	wp whose words form the sets R and W respectively." ></td>
	<td class="line x" title="75:238	The output of the model is a synthetic reference sentence SRW that preserves the meaning of R and has maximal word overlap with W. We generate such a sentence by substituting words from R with contextually equivalent words from W. Our algorithm rst selects pairs of candidate word paraphrases, and then checks the likelihood of their substitution in the context of the reference sentence." ></td>
	<td class="line x" title="76:238	Candidate Selection We assume that words from the reference sentence that already occur in the system generated sentence should not be considered for substitution." ></td>
	<td class="line x" title="77:238	Therefore, we focus on unmatched pairs of the form {(r, w)|r  RW, w  WR}." ></td>
	<td class="line x" title="78:238	From this pool, we select candidate pairs whose members exhibit high semantic proximity." ></td>
	<td class="line x" title="79:238	In our experiments we compute semantic similarity using WordNet, a large-scale lexico-semantic resource employed in many NLP applications for similar pur2a." ></td>
	<td class="line x" title="80:238	It is hard to believe that such tremendous changes have taken place for those people and lands that I have never stopped missing while living abroad." ></td>
	<td class="line x" title="81:238	2b." ></td>
	<td class="line x" title="82:238	For someone born here but has been sentimentally attached to a foreign country far from home, it is dif cult to believe this kind of changes." ></td>
	<td class="line x" title="83:238	Table 2: A reference sentence and a corresponding machine translation." ></td>
	<td class="line x" title="84:238	Candidate paraphrases are in bold." ></td>
	<td class="line x" title="85:238	poses." ></td>
	<td class="line x" title="86:238	We consider a pair as a substitution candidate if its members are synonyms in WordNet." ></td>
	<td class="line x" title="87:238	Applying this step to the two sentences in Table 2, we obtain two candidate pairs (home, place) and (dif cult, hard)." ></td>
	<td class="line x" title="88:238	Contextual Substitution The next step is to determine for each candidate pair (ri, wj) whether wj is a valid substitution for ri in the context of r1 . . ." ></td>
	<td class="line x" title="89:238	ri1a50ri+1 . . ." ></td>
	<td class="line x" title="90:238	rm." ></td>
	<td class="line x" title="91:238	This ltering step is essential because synonyms are not universally substitutable2." ></td>
	<td class="line x" title="92:238	Consider the candidate pair (home, place) from our example (see Table 2)." ></td>
	<td class="line x" title="93:238	Words home and place are paraphrases in the sense of habitat, but in the reference sentence place occurs in a different sense, being part of the collocation take place . In this case, the pair (home, place) cannot be used to rewrite the reference sentence." ></td>
	<td class="line x" title="94:238	We formulate contextual substitution as a binary classi cation task: given a context r1 . . ." ></td>
	<td class="line x" title="95:238	ri1a50ri+1 . . ." ></td>
	<td class="line x" title="96:238	rm, we aim to predict whether wj can occur in this context at position i. For each candidate word wj we train a classi er that models contextual preferences of wj." ></td>
	<td class="line x" title="97:238	To train such a classi er, we collect a large corpus of sentences that contain the word wj and an equal number of randomly extracted sentences that do not contain this word." ></td>
	<td class="line x" title="98:238	The former category forms positive instances, while the latter represents the negative." ></td>
	<td class="line x" title="99:238	For the negative examples, a random position in a sentence is selected for extracting the context." ></td>
	<td class="line x" title="100:238	This corpus is acquired automatically, and does not require any manual annotations." ></td>
	<td class="line x" title="101:238	2This can explain why previous attempts to use WordNet for generating sentence-level paraphrases (Barzilay and Lee, 2003; Quirk et al. , 2004) were unsuccessful." ></td>
	<td class="line x" title="102:238	457 We represent context by n-grams and local collocations, features typically used in supervised word sense disambiguation." ></td>
	<td class="line x" title="103:238	Both n-grams and collocations exclude the word wj." ></td>
	<td class="line x" title="104:238	An n-gram is a sequence of n adjacent words appearing in r1 . . ." ></td>
	<td class="line x" title="105:238	ri1a50ri+1 . . ." ></td>
	<td class="line x" title="106:238	rm." ></td>
	<td class="line x" title="107:238	A local collocation also takes into account the position of an n-gram with respect to the target word." ></td>
	<td class="line x" title="108:238	To compute local collocations for a word at position i, we extract all n-grams (n = 1 . . ." ></td>
	<td class="line x" title="109:238	4) beginning at position i  2 and ending at position i + 2." ></td>
	<td class="line x" title="110:238	To make these position dependent, we prepend each of them with the length and starting position." ></td>
	<td class="line x" title="111:238	Once the classi er3 for wj is trained, we apply it to the context r1 . . ." ></td>
	<td class="line x" title="112:238	ri1a50ri+1 . . ." ></td>
	<td class="line x" title="113:238	rm." ></td>
	<td class="line x" title="114:238	For positive predictions, we rewrite the string as r1 . . ." ></td>
	<td class="line x" title="115:238	ri1wjri+1 . . ." ></td>
	<td class="line x" title="116:238	rm." ></td>
	<td class="line x" title="117:238	In this formulation, all substitutions are tested independently." ></td>
	<td class="line x" title="118:238	For the example from Table 2, only the pair (dif cult, hard) passes this lter, and thus the system produces the following synthetic reference: For someone born here but has been sentimentally attached to a foreign country far from home, it is hard to believe this kind of changes." ></td>
	<td class="line x" title="119:238	The synthetic reference keeps the meaning of the original reference, but has a higher word overlap with the system output." ></td>
	<td class="line x" title="120:238	One of the implications of this design is the need to develop a large number of classi ers to test contextual substitutions." ></td>
	<td class="line x" title="121:238	For each word to be inserted into a reference sentence, we need to train a separate classi er." ></td>
	<td class="line x" title="122:238	In practice, this requirement is not a signi cant burden." ></td>
	<td class="line x" title="123:238	The training is done off-line and only once, and testing for contextual substitution is instantaneous." ></td>
	<td class="line x" title="124:238	Moreover, the rst ltering step effectively reduces the number of potential candidates." ></td>
	<td class="line x" title="125:238	For example, to apply this approach to the 71,520 sentence pairs from the MT evaluation set (described in Section 4.1.2), we had to train 2,380 classi ers." ></td>
	<td class="line x" title="126:238	We also discovered that the key to the success of this approach is the size of the corpus used for training contextual classi ers." ></td>
	<td class="line x" title="127:238	We derived training corpora from the English Gigaword corpus, and the average size of a corpus for one classi er is 255,000 3In our experiments, we used the publicly available BoosTexter classi er (Schapire and Singer, 2000) for this task." ></td>
	<td class="line x" title="128:238	sentences." ></td>
	<td class="line x" title="129:238	We do not attempt to substitute any words that have less that 10,000 appearances in the Gigaword corpus." ></td>
	<td class="line x" title="130:238	4 Experiments Our primary goal is to investigate the impact of machine-generated paraphrases on the accuracy of automatic evaluation." ></td>
	<td class="line x" title="131:238	We focus on automatic evaluation of machine translation due to the availability of human annotated data in that domain." ></td>
	<td class="line x" title="132:238	The hypothesis is that by using a synthetic reference translation, automatic measures approximate better human evaluation." ></td>
	<td class="line x" title="133:238	In section 4.2, we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references." ></td>
	<td class="line x" title="134:238	Our secondary goal is to study the relationship between the quality of paraphrases and their contribution to the performance of automatic machine translation evaluation." ></td>
	<td class="line x" title="135:238	In section 4.3, we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods." ></td>
	<td class="line x" title="136:238	4.1 Experimental Set-Up We begin by describing relevant background information, including the BLEU evaluation method, the test data set, and the alternative paraphrasing methods considered in our experiments." ></td>
	<td class="line x" title="137:238	4.1.1 BLEU BLEU is the basic evaluation measure that we use in our experiments." ></td>
	<td class="line x" title="138:238	It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty." ></td>
	<td class="line x" title="139:238	The BLEU score is computed as follows: BLEU = BP  4 radicaltpradicalvertex radicalvertexradicalbt 4productdisplay n=1 pn BP = min(1, e1r/c), where pn is the n-gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences." ></td>
	<td class="line x" title="140:238	To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference." ></td>
	<td class="line x" title="141:238	458 4.1.2 Data We use the Chinese portion of the 2004 NIST MT dataset." ></td>
	<td class="line x" title="142:238	This portion contains 200 Chinese documents, subdivided into a total of 1788 segments." ></td>
	<td class="line x" title="143:238	Each segment is translated by ten machine translation systems and by four human translators." ></td>
	<td class="line x" title="144:238	A quarter of the machine-translated segments are scored by human evaluators on a one-tove scale along two dimensions: adequacy and uency." ></td>
	<td class="line x" title="145:238	We use only adequacy scores, which measure how well content is preserved in the translation." ></td>
	<td class="line x" title="146:238	4.1.3 Alternative Paraphrasing Techniques To investigate the effect of paraphrase quality on automatic evaluation, we consider two alternative paraphrasing resources: Latent Semantic Analysis (LSA), and Brown clustering (Brown et al. , 1992)." ></td>
	<td class="line x" title="147:238	These techniques are widely used in NLP applications, including language modeling, information extraction, and dialogue processing (Haghighi et al. , 2005; Sera n and Eugenio, 2004; Miller et al. , 2004)." ></td>
	<td class="line x" title="148:238	Both techniques are based on distributional similarity." ></td>
	<td class="line x" title="149:238	The Brown clustering is computed by considering mutual information between adjacent words." ></td>
	<td class="line x" title="150:238	LSA is a dimensionality reduction technique that projects a word co-occurrence matrix to lower dimensions." ></td>
	<td class="line x" title="151:238	This lower dimensional representation is then used with standard similarity measures to cluster the data." ></td>
	<td class="line x" title="152:238	Two words are considered to be a paraphrase pair if they appear in the same cluster." ></td>
	<td class="line x" title="153:238	We construct 1000 clusters employing the Brown method on 112 million words from the North American New York Times corpus." ></td>
	<td class="line x" title="154:238	We keep the top 20 most frequent words for each cluster as paraphrases." ></td>
	<td class="line x" title="155:238	To generate LSA paraphrases, we used the Infomap software4 on a 34 million word collection of articles from the American News Text corpus." ></td>
	<td class="line x" title="156:238	We used the default parameter settings: a 20,000 word vocabulary, the 1000 most frequent words (minus a stoplist) for features, a 15 word context window on either side of a word, a 100 feature reduced representation, and the 20 most similar words as paraphrases." ></td>
	<td class="line x" title="157:238	While we experimented with several parameter settings for LSA and Brown methods, we do not claim that the selected settings are necessarily optimal." ></td>
	<td class="line x" title="158:238	However, these methods present sensible com4http://infomap-nlp.sourceforge.net Method 1 reference 2 references BLEU 0.9657 0.9743 WordNet 0.9674 0.9763 ContextWN 0.9677 0.9764 LSA 0.9652 0.9736 Brown 0.9662 0.9744 Table 4: Pearson adequacy correlation scores for rewriting using one and two references, averaged over ten runs." ></td>
	<td class="line x" title="159:238	Method vs. BLEU vs. ContextWN WordNet trianglelefttriangleleft triangletriangle ContextWN trianglelefttriangleleft LSA X triangletriangle Brown trianglelefttriangleleft triangle Table 5: Paired t-test signi cance for all methods compared to BLEU as well as our method for one reference." ></td>
	<td class="line x" title="160:238	Two triangles indicates signi cant at the 99% con dence level, one triangle at the 95% condence level and X not signi cant." ></td>
	<td class="line x" title="161:238	Triangles point towards the better method." ></td>
	<td class="line x" title="162:238	parison points for understanding the relationship between paraphrase quality and its impact on automatic evaluation." ></td>
	<td class="line x" title="163:238	Table 3 shows synthetic references produced by the different paraphrasing methods." ></td>
	<td class="line x" title="164:238	4.2 Impact of Paraphrases on Machine Translation Evaluation The standard way to analyze the performance of an evaluation metric in machine translation is to compute the Pearson correlation between the automatic metric and human scores (Papineni et al. , 2002; Koehn, 2004; Lin and Och, 2004; Stent et al. , 2005)." ></td>
	<td class="line x" title="165:238	Pearson correlation estimates how linearly dependent two sets of values are." ></td>
	<td class="line x" title="166:238	The Pearson correlation values range from 1, when the scores are perfectly linearly correlated, to -1, in the case of inversely correlated scores." ></td>
	<td class="line x" title="167:238	To calculate the Pearson correlation, we create a document by concatenating 300 segments." ></td>
	<td class="line x" title="168:238	This strategy is commonly used in MT evaluation, because of BLEUs well-known problems with documents of small size (Papineni et al. , 2002; Koehn, 2004)." ></td>
	<td class="line x" title="169:238	For each of the ten MT system translations, 459 Reference: The monthly magazine Choices has won the deep trust of the residents." ></td>
	<td class="line x" title="170:238	The current Internet edition of Choices will give full play to its functions and will help consumers get quick access to market information." ></td>
	<td class="line x" title="171:238	System: The public has a lot of faith in the Choice monthly magazine and the Council is now working on a web version." ></td>
	<td class="line x" title="172:238	This will enhance the magazines function and help consumer to acquire more up-to-date market information." ></td>
	<td class="line x" title="173:238	WordNet The monthly magazine Choices has won the deep faith of the residents." ></td>
	<td class="line x" title="174:238	The current Internet version of Choices will give full play to its functions and will help consumers acquire quick access to market information." ></td>
	<td class="line x" title="175:238	ContextWN The monthly magazine Choices has won the deep trust of the residents." ></td>
	<td class="line x" title="176:238	The current Internet version of Choices will give full play to its functions and will help consumers acquire quick access to market information." ></td>
	<td class="line x" title="177:238	LSA The monthly magazine Choice has won the deep trust of the residents." ></td>
	<td class="line x" title="178:238	The current web edition of Choice will give full play to its functions and will help consumer get quick access to market information." ></td>
	<td class="line x" title="179:238	Brown The monthly magazine Choices has won the deep trust of the residents." ></td>
	<td class="line x" title="180:238	The current Internet version of Choices will give full play to its functions and will help consumers get quick access to market information." ></td>
	<td class="line x" title="181:238	Table 3: Sample of paraphrasings produced by each method based on the corresponding system translation." ></td>
	<td class="line x" title="182:238	Paraphrased words are in bold and ltered words underlined." ></td>
	<td class="line x" title="183:238	the evaluation metric score is calculated on the document and the corresponding human adequacy score is calculated as the average human score over the segments." ></td>
	<td class="line x" title="184:238	The Pearson correlation is calculated over these ten pairs (Papineni et al. , 2002; Stent et al. , 2005)." ></td>
	<td class="line x" title="185:238	This process is repeated for ten different documents created by the same process." ></td>
	<td class="line x" title="186:238	Finally, a paired t-test is calculated over these ten different correlation scores to compute statistical signi cance." ></td>
	<td class="line x" title="187:238	Table 4 shows Pearson correlation scores for BLEU and the four paraphrased augmentations, averaged over ten runs.5 In all ten tests, our method based on contextual rewriting (ContextWN) improves the correlation with human scores over BLEU." ></td>
	<td class="line x" title="188:238	Moreover, in nine out of ten tests ContextWN outperforms the method based on WordNet." ></td>
	<td class="line x" title="189:238	The results of statistical signi cance testing are summarized in Table 5." ></td>
	<td class="line x" title="190:238	All the paraphrasing methods except LSA, exhibit higher correlation with human scores than plain BLEU." ></td>
	<td class="line x" title="191:238	Our method signi cantly outperforms BLEU, and all the other paraphrasebased metrics." ></td>
	<td class="line x" title="192:238	This consistent improvement conrms the importance of contextual ltering." ></td>
	<td class="line x" title="193:238	5Depending on the experimental setup, correlation values can vary widely." ></td>
	<td class="line x" title="194:238	Our scores fall within the range of previous researchers (Papineni et al. , 2002; Lin and Och, 2004)." ></td>
	<td class="line x" title="195:238	The third column in Table 4 shows that automatic paraphrasing continues to improve correlation scores even when two human references are paraphrased using our method." ></td>
	<td class="line x" title="196:238	4.3 Evaluation of Paraphrase Quality In the last section, we saw signi cant variations in MT evaluation performance when different paraphrasing methods were used to generate a synthetic reference." ></td>
	<td class="line x" title="197:238	In this section, we examine the correlation between the quality of automatically generated paraphrases and their contribution to automatic evaluation." ></td>
	<td class="line x" title="198:238	We analyze how the substitution frequency and the accuracy of those substitutions contributes to a methods performance." ></td>
	<td class="line x" title="199:238	We compute the substitution frequency of an automatic paraphrasing method by counting the number of words it rewrites in a set of reference sentences." ></td>
	<td class="line x" title="200:238	Table 6 shows the substitution frequency and the corresponding BLEU score." ></td>
	<td class="line x" title="201:238	The substitution frequency varies greatly across different methods LSA is by far the most proli c rewriter, while Brown produces very few substitutions." ></td>
	<td class="line x" title="202:238	As expected, the more paraphrases identi ed, the higher the BLEU score for the method." ></td>
	<td class="line x" title="203:238	However, this increase does 460 Method Score Substitutions BLEU 0.0913 WordNet 0.0969 994 ContextWN 0.0962 742 LSA 0.992 2080 Brown 0.921 117 Table 6: Scores and the number of substitutions made for all 1788 segments, averaged over the different MT system translations Method Judge 1 Judge 2 Kappa accuracy accuracy WordNet 63.5% 62.5% 0.74 ContextWN 75% 76.0% 0.69 LSA 30% 31.5% 0.73 Brown 56% 56% 0.72 Table 7: Accuracy scores by two human judges as well as the Kappa coef cient of agreement." ></td>
	<td class="line x" title="204:238	not translate into better evaluation performance." ></td>
	<td class="line x" title="205:238	For instance, our contextual ltering method removes approximately a quarter of the paraphrases suggested by WordNet and yields a better evaluation measure." ></td>
	<td class="line x" title="206:238	These results suggest that the substitution frequency cannot predict the utility value of the paraphrasing method." ></td>
	<td class="line x" title="207:238	Accuracy measures the correctness of the proposed substitutions in the context of a reference sentence." ></td>
	<td class="line x" title="208:238	To evaluate the accuracy of different paraphrasing methods, we randomly extracted 200 paraphrasing examples from each method." ></td>
	<td class="line x" title="209:238	A paraphrase example consists of a reference sentence, a reference word to be paraphrased and a proposed paraphrase of that reference (that actually occurred in a corresponding system translation)." ></td>
	<td class="line x" title="210:238	The judge was instructed to mark a substitution as correct only if the substitution was both semantically and grammatically correct in the context of the original reference sentence." ></td>
	<td class="line x" title="211:238	Paraphrases produced by the four methods were judged by two native English speakers." ></td>
	<td class="line x" title="212:238	The pairs were presented in random order, and the judges were not told which system produced a given pair." ></td>
	<td class="line x" title="213:238	We employ a commonly used measure, Kappa, to assess agreement between the judges." ></td>
	<td class="line x" title="214:238	We found that negative positive ltered 40 27 nonltered 33 100 Table 8: Confusion matrix for the context ltering method on a random sample of 200 examples labeled by the rst judge." ></td>
	<td class="line x" title="215:238	on all the four sets the Kappa value was around 0.7, which corresponds to substantial agreement (Landis and Koch, 1977)." ></td>
	<td class="line x" title="216:238	As Table 7 shows, the ranking between the accuracy of the different paraphrasing methods mirrors the ranking of the corresponding MT evaluation methods shown in Table 4." ></td>
	<td class="line x" title="217:238	The paraphrasing method with the highest accuracy, ContextWN, contributes most signi cantly to the evaluation performance of BLEU." ></td>
	<td class="line x" title="218:238	Interestingly, even methods with moderate accuracy, i.e. 63% for WordNet, have a positive in uence on the BLEU metric." ></td>
	<td class="line x" title="219:238	At the same time, poor paraphrasing accuracy, such as LSA with 30%, does hurt the performance of automatic evaluation." ></td>
	<td class="line x" title="220:238	To further understand the contribution of contextual ltering, we compare the substitutions made by WordNet and ContextWN on the same set of sentences." ></td>
	<td class="line x" title="221:238	Among the 200 paraphrases proposed by WordNet, 73 (36.5%) were identi ed as incorrect by human judges." ></td>
	<td class="line x" title="222:238	As the confusion matrix in Table 8 shows, 40 (54.5%) were eliminated during the ltering step." ></td>
	<td class="line x" title="223:238	At the same time, the ltering erroneously eliminates 27 positive examples (21%)." ></td>
	<td class="line x" title="224:238	Even at this level of false negatives, the ltering has an overall positive effect." ></td>
	<td class="line x" title="225:238	5 Conclusion and Future Work This paper presents a comprehensive study of the impact of paraphrases on the accuracy of automatic evaluation." ></td>
	<td class="line x" title="226:238	We found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation." ></td>
	<td class="line x" title="227:238	These results have two important implications: (1) re ning standard measures such as BLEU with paraphrase information moves the automatic evaluation closer to human evaluation and (2) applying paraphrases to MT evaluation provides a task-based assessment for paraphrasing accuracy." ></td>
	<td class="line x" title="228:238	461 We also introduce a novel paraphrasing method based on contextual substitution." ></td>
	<td class="line x" title="229:238	By posing the paraphrasing problem as a discriminative task, we can incorporate a wide range of features that improve the paraphrasing accuracy." ></td>
	<td class="line x" title="230:238	Our experiments show improvement of the accuracy of WordNet paraphrasing and we believe that this method can similarly bene t other approaches that use lexicosemantic resources to obtain paraphrases." ></td>
	<td class="line x" title="231:238	Our ultimate goal is to develop a contextual ltering method that does not require candidate selection based on a lexico-semantic resource." ></td>
	<td class="line x" title="232:238	One source of possible improvement lies in exploring more powerful learning frameworks and more sophisticated linguistic representations." ></td>
	<td class="line x" title="233:238	Incorporating syntactic dependencies and class-based features into the context representation could also increase the accuracy and the coverage of the method." ></td>
	<td class="line x" title="234:238	Our current method only implements rewriting at the word level." ></td>
	<td class="line x" title="235:238	In the future, we would like to incorporate substitutions at the level of phrases and syntactic trees." ></td>
	<td class="line x" title="236:238	Acknowledgments The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168) and DARPA (Kauchak; grant HR0011-06-C-0023)." ></td>
	<td class="line x" title="237:238	Thanks to Michael Collins, Charles Elkan, Yoong Keok Lee, Philip Koehn, Igor Malioutov, Ben Snyder and the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="238:238	Any opinions, ndings and conclusions expressed in this material are those of the author(s) and do not necessarily re ect the views of DARPA or NSF." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-1002
Going Beyond AER: An Extensive Analysis Of Word Alignments And Their Impact On MT
Ayan, Necip Fazil;Dorr, Bonnie Jean;"></td>
	<td class="line x" title="1:186	Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 916, Sydney, July 2006." ></td>
	<td class="line x" title="2:186	c2006 Association for Computational Linguistics Going Beyond AER: An Extensive Analysis of Word Alignments and Their Impact on MT Necip Fazil Ayan and Bonnie J. Dorr Institute of Advanced Computer Studies (UMIACS) University of Maryland College Park, MD 20742 {nfa,bonnie}@umiacs.umd.edu Abstract This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output." ></td>
	<td class="line x" title="3:186	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation." ></td>
	<td class="line x" title="4:186	We show that precision-oriented alignments yield better MT output (translating more words and using longer phrases) than recalloriented alignments." ></td>
	<td class="line x" title="5:186	1 Introduction Word alignments are a by-product of statistical machine translation (MT) and play a crucial role in MT performance." ></td>
	<td class="line x" title="6:186	In recent years, researchers haveproposedseveralalgorithmstogenerateword alignments." ></td>
	<td class="line x" title="7:186	However, evaluating word alignments is difficult because even humans have difficulty performing this task." ></td>
	<td class="line x" title="8:186	The state-of-the art evaluation metric alignment error rate (AER)attempts to balance the precision and recall scores at the level of alignment links (Och and Ney, 2000)." ></td>
	<td class="line oc" title="9:186	Other metrics assess the impact of alignments externally, e.g., different alignments are tested by comparing the corresponding MT outputs using automated evaluation metrics (e.g. , BLEU (Papineni et al. , 2002) or METEOR (Banerjee and Lavie, 2005))." ></td>
	<td class="line x" title="10:186	However, these studies showed that AER and BLEU do not correlate well (Callison-Burch et al. , 2004; Goutte et al. , 2004; Ittycheriah and Roukos, 2005)." ></td>
	<td class="line x" title="11:186	Despite significant AER improvements achievedbyseveralresearchers, theimprovements in BLEU scores are insignificant or, at best, small." ></td>
	<td class="line x" title="12:186	This paper demonstrates the difficulty in assessing whether alignment quality makes a difference in MT performance." ></td>
	<td class="line x" title="13:186	We describe the impact of certain alignment characteristics on MT performance but also identify several alignment-related factors that impact MT performance regardless of the quality of the initial alignments." ></td>
	<td class="line x" title="14:186	In so doing, we begin to answer long-standing questions about the value of alignment in the context of MT. We first evaluate 5 different word alignments intrinsically, using: (1) community-standard metricsprecision, recall and AER; and (2) a new measure called consistent phrase error rate (CPER)." ></td>
	<td class="line x" title="15:186	Next, we observe the impact of different alignments on MT performance." ></td>
	<td class="line x" title="16:186	We present BLEU scores on a phrase-based MT system, Pharaoh (Koehn, 2004), using five different alignments to extract phrases." ></td>
	<td class="line x" title="17:186	We investigate the impact of different settings for phrase extraction, lexical weighting, maximum phrase length and training data." ></td>
	<td class="line x" title="18:186	Finally, we present a quantitative analysis of which phrases are chosen during the actual decoding process and showhow the distribution of thephrasesdifferfromonealignmentintoanother." ></td>
	<td class="line x" title="19:186	Our experiments show that precision-oriented alignmentsyieldbetterphrasesforMTthanrecalloriented alignments." ></td>
	<td class="line x" title="20:186	Specifically, they cover a higher percentage of our test sets and result in fewer untranslated words and selection of longer phrases during decoding." ></td>
	<td class="line x" title="21:186	The next section describes work related to our alignment evaluation approach." ></td>
	<td class="line x" title="22:186	Following this we outline different intrinsic evaluation measures of alignment and we propose a new measure to evaluatewordalignmentswithinphrase-basedMT framework." ></td>
	<td class="line x" title="23:186	We then present several experiments to measure the impact of different word alignments on a phrase-based MT system, and investigate how different alignments change the phrase 9 selection in the same MT system." ></td>
	<td class="line x" title="24:186	2 Related Work Starting with the IBM models (Brown et al. , 1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al. , 1996), log-linear models (Och and Ney, 2003), and similarity-based heuristic methods (Melamed, 2000)." ></td>
	<td class="line x" title="25:186	These methods are unsupervised, i.e., the only input is large parallel corpora." ></td>
	<td class="line x" title="26:186	In recent years, researchers have shown that even using a limited amount of manually aligned data improves word alignment significantly (Callison-Burch et al. , 2004)." ></td>
	<td class="line x" title="27:186	Supervised learning techniques, such as perceptron learning, maximum entropy modeling or maximum weighted bipartite matching, have been shown to provide further improvements on word alignments (Ayan et al. , 2005; Moore, 2005; Ittycheriah and Roukos, 2005; Taskar et al. , 2005)." ></td>
	<td class="line x" title="28:186	The standard technique for evaluating word alignments is to represent alignments as a set of links (i.e. , pairs of words) and to compare the generated alignment against manual alignment of the same data at the level of links." ></td>
	<td class="line x" title="29:186	Manual alignments are represented by two sets: Probable (P) alignments and Sure (S) alignments, where S  P. Given A,P and S, the most commonly used metricsprecision (Pr), recall (Rc) and alignment error rate (AER)are defined as follows: Pr = |AP||A| Rc = |AS||S| AER = 1 |AS|+|AP||A|+|S| Another approach to evaluating alignments is to measure their impact on an external application, e.g., statistical MT. In recent years, phrase-based systems (Koehn, 2004; Chiang, 2005) have been shown to outperform word-based MT systems; therefore,inthispaper,weuseapublicly-available phrase-based MT system, Pharaoh (Koehn, 2004), to investigate the impact of different alignments." ></td>
	<td class="line x" title="30:186	Although it is possible to estimate phrases directly from a training corpus (Marcu and Wong, 2002), most phrase-based MT systems (Koehn, 2004; Chiang, 2005) start with a word alignment and extract phrases that are consistent with the given alignment." ></td>
	<td class="line x" title="31:186	Once the consistent phrases are extracted, they are assigned multiple scores (such Test Lang # of # Words Source Pair Sents (en/fl) en-ch 491 14K/12K NIST MTEval2002 en-ar 450 13K/11K NIST MTEval2003 Training en-ch 107K 4.1M/3.3M FBIS en-ar 44K 1.4M/1.1M News + Treebank Table 1: Test and Training Data Used for Experiments as translation probabilities and lexical weights), and the decoders job is to choose the correct phrases based on those scores using a log-linear model." ></td>
	<td class="line x" title="32:186	3 Intrinsic Evaluation of Alignments Our goal is to compare different alignments and to investigate how their characteristics affect the MT systems." ></td>
	<td class="line x" title="33:186	We evaluate alignments in terms of precision, recall, alignment error rate (AER), and a new measure called consistent phrase error rate (CPER)." ></td>
	<td class="line x" title="34:186	We focus on 5 different alignments obtained by combining two uni-directional alignments." ></td>
	<td class="line x" title="35:186	Each uni-directional alignment is the result of running GIZA++ (Och, 2000b) in one of two directions (source-to-target and vice versa) with default configurations." ></td>
	<td class="line x" title="36:186	The combined alignments that are used in this paper are as follows: 1." ></td>
	<td class="line x" title="37:186	Union of both directions (SU), 2." ></td>
	<td class="line x" title="38:186	Intersection of both directions (SI), 3." ></td>
	<td class="line x" title="39:186	A heuristic based combination technique called grow-diag-final (SG), which is the default alignment combination heuristic employed in Pharaoh (Koehn, 2004), 4-5." ></td>
	<td class="line x" title="40:186	Two supervised alignment combination techniques (SA and SB) using 2 and 4 input alignments as described in (Ayan et al. , 2005)." ></td>
	<td class="line x" title="41:186	This paper examines the impact of alignments according to their orientation toward precision or recall." ></td>
	<td class="line x" title="42:186	Among the five alignments above, SU and SG are recall-oriented while the other three are precision-oriented." ></td>
	<td class="line x" title="43:186	SB is an improved version of SA which attempts to increase recall without a significant sacrifice in precision." ></td>
	<td class="line x" title="44:186	Manually aligned data from two language pairs are used in our intrinsic evaluations using the five combinations above." ></td>
	<td class="line x" title="45:186	A summary of the training and test data is presented in Table 1." ></td>
	<td class="line x" title="46:186	Our gold standard for each language pair is a manually aligned corpus." ></td>
	<td class="line x" title="47:186	English-Chinese an10 notations distinguish between sure and probable alignment links, but English-Arabic annotations do not." ></td>
	<td class="line x" title="48:186	The details of how the annotations are done can be found in (Ayan et al. , 2005) and (Ittycheriah and Roukos, 2005)." ></td>
	<td class="line x" title="49:186	3.1 Precision, Recall and AER Table 2 presents the precision, recall, and AER for 5 different alignments on 2 language pairs." ></td>
	<td class="line x" title="50:186	For each of these metrics, a different system achieves the best score  respectively, these are SI, SU, and SB." ></td>
	<td class="line x" title="51:186	SU and SG yield low precision, high recall alignments." ></td>
	<td class="line x" title="52:186	In contrast, SI yields very high precision but very low recall." ></td>
	<td class="line x" title="53:186	SA and SB attempt to balance these two measures but their precision is still higher than their recall." ></td>
	<td class="line x" title="54:186	Both systems have nearly the same precision but SB yields significantly higher recall than SA." ></td>
	<td class="line x" title="55:186	Align." ></td>
	<td class="line x" title="56:186	en-ch en-ar Sys." ></td>
	<td class="line x" title="57:186	Pr Rc AER Pr Rc AER SU 58.3 84.5 31.6 56.0 84.1 32.8 SG 61.9 82.6 29.7 60.2 83.0 30.2 SI 94.8 53.6 31.2 96.1 57.1 28.4 SA 87.0 74.6 19.5 88.6 71.1 21.1 SB 87.8 80.5 15.9 90.1 76.1 17.5 Table 2: Comparison of 5 Different Alignments using AER (on English-Chinese and English-Arabic) 3.2 Consistent Phrase Error Rate In this section, we present a new method, called consistent phrase error rate (CPER), for evaluating word alignments in the context of phrasebased MT. The idea is to compare phrases consistent with a given alignment against phrases that would be consistent with human alignments." ></td>
	<td class="line x" title="58:186	CPER is similar to AER but operates at the phrase level instead of at the word level." ></td>
	<td class="line x" title="59:186	To compute CPER, we define a link in terms of the position of its start and end words in the phrases." ></td>
	<td class="line x" title="60:186	For instance, the phrase link (i1,i2,j1,j2) indicates that the English phrase ei1,,ei2 and the FL phrase fj1,,fj2 are consistent with the given alignment." ></td>
	<td class="line x" title="61:186	Once we generate the set of phrases PA and PG that are consistent with a given alignment A and a manual alignment G, respectively, we compute precision (Pr), recall (Rc), and CPER as follows:1 Pr = |PA PG||P A| Rc = |PA PG||P G| CPER = 1 2Pr RcPr + Rc 1Note that CPER is equal to 1 F-score." ></td>
	<td class="line x" title="62:186	Chinese Arabic Align." ></td>
	<td class="line x" title="63:186	CPER-3 CPER-7 CPER-3 CPER-7 SU 63.2 73.3 55.6 67.1 SG 59.5 69.4 52.0 62.6 SI 50.8 69.8 50.7 67.6 SA 40.8 51.6 42.0 54.1 SB 36.8 45.1 36.1 46.6 Table 3: Consistent Phrase Error Rates with Maximum Phrase Lengths of 3 and 7 CPER penalizes incorrect or missing alignment links more severely than AER." ></td>
	<td class="line x" title="64:186	While computing AER, an incorrect alignment link reduces the number of correct alignment links by 1, affecting precision and recall slightly." ></td>
	<td class="line x" title="65:186	Similarly, if there is a missing link, only the recall is reduced slightly." ></td>
	<td class="line x" title="66:186	However, when computing CPER, an incorrect or missing alignment link might result in more than one phrase pair being eliminated from or added to the set of phrases." ></td>
	<td class="line x" title="67:186	Thus, the impact is more severe on both precision and recall." ></td>
	<td class="line x" title="68:186	Figure 1: Sample phrases that are generated from a human alignment and an automated alignment: Gray cells show the alignment links, and rectangles show the possible phrases." ></td>
	<td class="line x" title="69:186	In Figure 1, the first box represents a manual alignment and the other two represent automated alignments A. In the case of a missing alignmentlink(Figure1b),PA includes9validphrases." ></td>
	<td class="line x" title="70:186	For this alignment, AER = 1  (2  2/2  2/3)/(2/2 + 2/3) = 0.2 and CPER = 1(2 5/95/6)/(5/9+5/6) = 0.33." ></td>
	<td class="line x" title="71:186	In the case of an incorrect alignment link (Figure 1c), PA includes only 2 valid phrases, which results in a higher CPER (1(22/22/6)/(2/2+2/6) = 0.49) but a lower AER (1  (2  3/4  3/3)/(3/4 + 3/3) = 0.14)." ></td>
	<td class="line x" title="72:186	Table 3 presents the CPER values on two different language pairs, using 2 different maximum phraselengths." ></td>
	<td class="line x" title="73:186	Forbothmaximumphraselengths, SA and SB yield the lowest CPER." ></td>
	<td class="line x" title="74:186	For all 5 alignmentsin both languagesCPER increases as the length of the phrase increases." ></td>
	<td class="line x" title="75:186	For all alignments except SI, this amount of increase is nearly the same on both languages." ></td>
	<td class="line x" title="76:186	Since SI contains very few alignment points, the number of generated phrases dramatically increases, yielding 11 poor precision and CPER as the maximum phrase length increases." ></td>
	<td class="line x" title="77:186	4 Evaluating Alignments within MT We now move from intrinsic measurement to extrinsic measurement using an off-the-shelf phrasebased MT system Pharaoh (Koehn, 2004)." ></td>
	<td class="line x" title="78:186	Our goal is to identify the characteristics of alignments that change MT behavior and the types of changes induced by these characteristics." ></td>
	<td class="line x" title="79:186	All MT system components were kept the same in our experiments except for the component that generates a phrase table from a given alignment." ></td>
	<td class="line x" title="80:186	We used the corpora presented in Table 1 to train the MT system." ></td>
	<td class="line x" title="81:186	The phrases were scored using translation probabilities and lexical weights in two directions and a phrase penalty score." ></td>
	<td class="line x" title="82:186	We also use a language model, a distortion model and a word penalty feature for MT. We measure the impact of different alignments on Pharaoh using three different settings: 1." ></td>
	<td class="line x" title="83:186	Different maximum phrase length, 2." ></td>
	<td class="line x" title="84:186	Different sizes of training data, and 3." ></td>
	<td class="line x" title="85:186	Different lexical weighting." ></td>
	<td class="line x" title="86:186	For maximum phrase length, we used 3 (based onwhatwassuggestedby(Koehn etal., 2003)and 7(thedefaultmaximumphraselengthinPharaoh)." ></td>
	<td class="line x" title="88:186	For lexical weighting, we used the original weighting scheme employed in Pharaoh and a modified version." ></td>
	<td class="line x" title="89:186	We realized that the publiclyavailable implementation of Pharaoh computes the lexical weights only for non-NULL alignment links." ></td>
	<td class="line x" title="90:186	As a consequence, loose phrases containingNULL-alignedwordsalongtheiredgesreceive the same lexical weighting as tight phrases without NULL-aligned words along the edges." ></td>
	<td class="line x" title="91:186	We therefore adopted a modified weighting scheme following(Koehnetal., 2003), whichincorporates NULL alignments." ></td>
	<td class="line x" title="93:186	MT output was evaluated using the standard evaluation metric BLEU (Papineni et al. , 2002).2 The parameters of the MT System were optimized for BLEU metric on NIST MTEval2002 test sets using minimum error rate training (Och, 2003), and the systems were tested on NIST MTEval2003 test sets for both languages." ></td>
	<td class="line x" title="94:186	2We used the NIST script (version 11a) for BLEU with its default settings: case-insensitive matching of n-grams up to n = 4, and the shortest reference sentence for the brevity penalty." ></td>
	<td class="line x" title="95:186	The words that were not translated during decoding were deleted from the MT output before running the BLEU script." ></td>
	<td class="line x" title="96:186	The SRI Language Modeling Toolkit was used totrainatrigrammodelwithmodifiedKneser-Ney smoothing on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus." ></td>
	<td class="line x" title="97:186	During decoding, the number of English phrases per FL phrase was limited to 100 and phrase distortion was limited to 4." ></td>
	<td class="line x" title="98:186	4.1 BLEU Score Comparison Table4presentstheBLEUscoresforPharaohruns on Chinese with five different alignments using different settings for maximum phrase length (3 vs. 7), size of training data (107K vs. 241K), and lexical weighting (original vs. modified).3 The modified lexical weighting yields huge improvements when the alignment leaves several words unaligned: the BLEU score for SA goes from 24.26 to 25.31 and the BLEU score for SB goes from 23.91 to 25.38." ></td>
	<td class="line x" title="99:186	In contrast, when the alignments contain a high number of alignment links (e.g. , SU and SG), modifying lexical weighting does not bring significant improvements because the number of phrases containing unaligned words is relatively low." ></td>
	<td class="line x" title="100:186	Increasing the phrase length increases the BLEU scores for all systems by nearly 0.7 points and increasing the size of the training data increases the BLEU scores by 1.5-2 points for all systems." ></td>
	<td class="line x" title="101:186	For all settings, SU yields the lowest BLEU scores while SB clearly outperforms the others." ></td>
	<td class="line x" title="102:186	Table 5 presents BLEU scores for Pharaoh runs on5differentalignmentsonEnglish-Arabic,using different settings for lexical weighting and maximum phrase lengths.4 Using the original lexical weighting, SA and SB perform better than the others while SU and SI yield the worst results." ></td>
	<td class="line x" title="103:186	Modifying the lexical weighting leads to slight reductions in BLEU scores for SU and SG, but improves the scores for the other 3 alignments significantly." ></td>
	<td class="line x" title="104:186	Finally, increasing the maximum phrase length to 7 leads to additional improvements in BLEU scores, where SG and SU benefit nearly 2 BLEU points." ></td>
	<td class="line x" title="105:186	As in English-Chinese, the worst BLEU scores are obtained by SU while the best scores are produced by SB." ></td>
	<td class="line x" title="106:186	As we see from the tables, the relation between intrinsic alignment measures (AER and CPER) 3We could not run SB on the larger corpus because of the lack of required inputs." ></td>
	<td class="line x" title="107:186	4Due to lack of additional training data, we could not do experiments using different sizes of training data on EnglishArabic." ></td>
	<td class="line x" title="108:186	12 Original Modified Modified Modified Alignment Max Phr Len = 3 Max Phr Len=3 Max Phr Len=7 Max Phr Len=3 |Corpus| = 107K |Corpus| = 107K |Corpus| = 107K |Corpus| = 241K SU 22.56 22.66 23.30 24.40 SG 23.65 23.79 24.48 25.54 SI 23.60 23.97 24.76 26.06 SA 24.26 25.31 25.99 26.92 SB 23.91 25.38 26.14 N/A Table 4: BLEU Scores on English-Chinese with Different Lexical Weightings, Maximum Phrase Lengths and Training Data LW=Org LW=Mod LW=Mod Alignment MPL=3 MPL=3 MPL=7 SU 41.97 41.72 43.50 SG 44.06 43.82 45.78 SI 42.29 42.76 43.88 SA 44.49 45.23 46.06 SB 44.92 45.39 46.66 Table 5: BLEU Scores on English-Arabic with Different Lexical Weightings and Maximum Phrase Lengths and the corresponding BLEU scores varies, depending on the language, lexical weighting, maximum phrase length, and training data size." ></td>
	<td class="line x" title="109:186	For example,usingamodifiedlexicalweighting,thesystems are ranked according to their BLEU scores as follows: SB, SA, SG, SI, SUan ordering that differs from that of AER but is identical to that of CPER (with a phrase length of 3) for Chinese." ></td>
	<td class="line x" title="110:186	On the other hand, in Arabic, both AER and CPER provide a slightly different ranking from that of BLEU, with SG and SI swapping places." ></td>
	<td class="line x" title="111:186	4.2 Tight vs. Loose Phrases To demonstrate how alignment-related components of the MT system might change the translation quality significantly, we did an additional experimenttocomparedifferenttechniquesforextracting phrases from a given alignment." ></td>
	<td class="line x" title="112:186	Specifically, we are comparing two techniques for phrase extraction: 1." ></td>
	<td class="line x" title="113:186	Loose phrases (the original consistent phrase extraction method) 2." ></td>
	<td class="line x" title="114:186	Tight phrases (the set of phrases where thefirst/lastwordsoneachsideareforced to align to some word in the phrase pair) Using tight phrases penalizes alignments with many unaligned words, whereas using loose phrases rewards them." ></td>
	<td class="line x" title="115:186	Our goal is to compare the performance of precision-oriented vs. recalloriented alignments when we allow only tight phrases in the phrase extraction step." ></td>
	<td class="line x" title="116:186	To simplify things, we used only 2 alignments: SG, the best recall-oriented alignment, and SB, the best precision-oriented alignment." ></td>
	<td class="line x" title="117:186	For this experiment, we used modified lexical weighting and a maximum phrase length of 7." ></td>
	<td class="line x" title="118:186	Chinese Arabic Alignment Loose Tight Loose Tight SG 24.48 23.19 45.78 43.67 SB 26.14 22.68 46.66 40.10 Table 6: BLEU Scores with Loose vs. Tight Phrases Table6presentstheBLEUscoresforSG andSB using two different phrase extraction techniques on English-Chinese and English-Arabic." ></td>
	<td class="line x" title="119:186	In both languages, SB outperforms SG significantly when loose phrases are used." ></td>
	<td class="line x" title="120:186	However, when we use only tight phrases, the performance of SB gets significantly worse (3.5 to 6.5 BLEU-score reduction in comparison to loose phrases)." ></td>
	<td class="line x" title="121:186	The performance of SG also gets worse but the degree of BLEUscore reduction is less than that of SB." ></td>
	<td class="line x" title="122:186	Overall SG performs better than SB with tight phrases; forEnglish-Arabic,thedifferencebetweenthetwo systems is more than 3 BLEU points." ></td>
	<td class="line x" title="123:186	Note that, as before, the relation between the alignment measures and the BLEU scores varies, this time depending on whether loose phrases or tight phrases are used: both CPER and AER track the BLEU rankings for loose (but not for tight) phrases." ></td>
	<td class="line x" title="124:186	This suggests that changing alignment-related components of the system (i.e. , phrase extraction and phrase scoring) influences the overall translation quality significantly for a particular alignment." ></td>
	<td class="line x" title="125:186	Therefore, when comparing two alignments in the context of a MT system, it is important to take the alignment characteristics into account." ></td>
	<td class="line x" title="126:186	For instance, alignments with many unaligned words are severely penalized when using tight phrases." ></td>
	<td class="line x" title="127:186	4.3 Untranslated Words We analyzed the percentage of words left untranslated during decoding." ></td>
	<td class="line x" title="128:186	Figure 2 shows the percentage of untranslated words in the FL using the Chinese and Arabic NIST MTEval2003 test sets." ></td>
	<td class="line x" title="129:186	On English-Chinese data (using all four settings given in Table 4) SU and SG yield the highest percentage of untranslated words while SI produces the lowest percentage of untranslated words." ></td>
	<td class="line x" title="130:186	SA and SB leave about 2% of the FL words phrases 13 Figure 2: Percentage of untranslated words out of the total number of FL words without translating them." ></td>
	<td class="line x" title="131:186	Increasing the training data size reduces the percentage of untranslated words by nearly half with all five alignments." ></td>
	<td class="line x" title="132:186	No significant impact on untranslated words is observed from modifying the lexical weights and changing the phrase length." ></td>
	<td class="line x" title="133:186	On English-Arabic data, all alignments result in higher percentages of untranslated words than English-Chinese, most likely due to data sparsity." ></td>
	<td class="line x" title="134:186	As in Chinese-to-English translation, SU is the worst and SB is the best." ></td>
	<td class="line x" title="135:186	SI behaves quite differently, leaving nearly 7% of the words untranslatedan indicator of why it produces a higher BLEU score on Chinese but a lower score on Arabic compared to other alignments." ></td>
	<td class="line x" title="136:186	4.4 Analysis of Phrase Tables This section presents several experiments to analyze how different alignments affect the size of the generated phrase tables, the distribution of the phrases that are used in decoding, and the coverage of the test set with the generated phrase tables." ></td>
	<td class="line x" title="137:186	Size of Phrase Tables The major impact of using different alignments in a phrase-based MT system is that each one results in a different phrase table." ></td>
	<td class="line x" title="138:186	Table 7 presents the number of phrases that are extracted from five alignments using two different maximum phrase lengths (3 vs. 7) in two languages, after filtering the phrase table for MTEval2003 test set." ></td>
	<td class="line x" title="139:186	The size of the phrase table increases dramatically as the number of links in the initial alignment gets smaller." ></td>
	<td class="line x" title="140:186	As a result, for both languages, SU and SG yield a much smaller Chinese Arabic Alignment MPL=3 MPL=7 MPL=3 MPL=7 SU 106 122 32 38 SG 161 181 48 55 SI 1331 3498 377 984 SA 954 1856 297 594 SB 876 1624 262 486 Table 7: Number of Phrases in the Phrase Table Filtered for MTEval2003 Test Sets (in thousands) phrase table than the other three alignments." ></td>
	<td class="line x" title="141:186	As the maximum phrase length increases, the size of the phrase table gets bigger for all alignments; however, the growth of the table is more significant for precision-oriented alignments due to the high number of unaligned words." ></td>
	<td class="line x" title="142:186	Distribution of Phrases To investigate how the decoder chooses phrases of different lengths, we analyzed the distribution of the phrases in the filtered phrase table and the phrases that were used to decode Chinese MTEval2003 test set.5 For the remaining experiments in the paper, we use modified lexical weighting, a maximum phrase length of 7, and 107K sentence pairs for training." ></td>
	<td class="line x" title="143:186	The top row in Figure 3 shows the distribution of the phrases generated by the five alignments (using a maximum phrase length of 7) according to their length." ></td>
	<td class="line x" title="144:186	The j-i designators correspond to the phrase pairs with j FL words and i English words." ></td>
	<td class="line x" title="145:186	For SU and SG, the majority of the phrases contain only one FL word, and the percentage of the phrases with more than 2 FL words is less than 18%." ></td>
	<td class="line x" title="146:186	For the other three alignments, however, the distribution of the phrases is almost inverted." ></td>
	<td class="line x" title="147:186	For SI, nearly 62% of the phrases contain more than 3 words on either FL or English side; for SA and SB, this percentage is around 45-50%." ></td>
	<td class="line x" title="148:186	Given the completely different phrase distribution, the most obvious question is whether the longer phrases generated by SI, SA and SB are actually used in decoding." ></td>
	<td class="line x" title="149:186	In order to investigate this, we did an analysis of the phrases used to decode the same test set." ></td>
	<td class="line x" title="150:186	The bottom row of Figure 3 shows the percentage of phrases used to decode the Chinese MTEval2003 test set." ></td>
	<td class="line x" title="151:186	The distribution of the actual phrases used in decoding is completely the reverse of the distribution of the phrases in the entire filtered table." ></td>
	<td class="line x" title="152:186	For all five alignments, the majority of the used phrases is one-to-one (between 5Due to lack of space, we will present results on ChineseEnglish only in the rest of this paper but the Arabic-English results show the same trends." ></td>
	<td class="line x" title="153:186	14 Figure 3: Distribution of the phrases in the phrase table filtered for Chinese MTEval2003 test set (top row) and the phrases used in decoding the same test set (bottom row) according to their lengths 50-65% of the total number of phrases used in decoding)." ></td>
	<td class="line x" title="154:186	SI, SA and SB use the other phrase pairs (particularly1-to-2phrases)morethanSU andSG." ></td>
	<td class="line x" title="155:186	NotethatSI, SA andSB useonlyasmallportion ofthephraseswithmorethan3wordsalthoughthe majority of the phrase table contains phrases with more than 3 words on one side." ></td>
	<td class="line x" title="156:186	It is surprising that the inclusion of phrase pairs with more than 3 words in the search space increases the BLEU score although the majority of the phrases used in decoding is mostly one-to-one." ></td>
	<td class="line x" title="157:186	Length of the Phrases used in Decoding We also investigated the number and length of phrases that are used to decode the given test set for different alignments." ></td>
	<td class="line x" title="158:186	Table 8 presents the average number of English and FL words in the phrases used in decoding Chinese MTEval2003 test set." ></td>
	<td class="line x" title="159:186	The decoder uses fewer phrases with SI, SA and SB than for the other two, thus yielding a higher number of FL words per phrase." ></td>
	<td class="line x" title="160:186	The number of English words per phrase is also higher for these three systems than the other two." ></td>
	<td class="line x" title="161:186	Coverage of the Test Set Finally, we examine the coverage of a test set using phrases of a specific length in the phrase table." ></td>
	<td class="line x" title="162:186	Table 9 presents Alignment |Eng| |FL| SU 1.39 1.28 SG 1.45 1.33 SI 1.51 1.55 SA 1.54 1.55 SB 1.56 1.52 Table 8: The average length of the phrases that are used in decoding Chinese MTEval2003 test set the coverage of the Chinese MTEval2003 test set (source side) using only phrases of a particular length (from 1 to 7)." ></td>
	<td class="line x" title="163:186	For this experiment, we assume that a word in the test set is covered if it is part of a phrase pair that exists in the phrase table (if a word is part of multiple phrases, it is counted only once)." ></td>
	<td class="line x" title="164:186	Not surprisingly, using only phrases with one FL word, more than 90% of the test set can be covered for all 5 alignments." ></td>
	<td class="line x" title="165:186	As the length of the phrases increases, the coverage of the test set decreases." ></td>
	<td class="line x" title="166:186	For instance, using phrases with 5 FL words results in less than 5% coverage of the test set." ></td>
	<td class="line x" title="167:186	Phrase Length (FL) A 1 2 3 4 5 6 7 SU 92.2 59.5 21.4 6.7 1.3 0.4 0.1 SG 95.5 64.4 24.9 7.4 1.6 0.5 0.3 SI 97.8 75.8 38.0 13.8 4.6 1.9 1.2 SA 97.3 75.3 36.1 12.5 3.8 1.5 0.8 SB 97.5 74.8 35.7 12.4 4.2 1.8 0.9 Table 9: Coverage of Chinese MTEval2003 Test Set Using Phrases with a Specific Length on FL side (in percentages) Table 9 reveals that the coverage of the test set is higher for precision-oriented alignments than recall-oriented alignments for all different lengths of the phrases." ></td>
	<td class="line x" title="168:186	For instance, SI, SA, and SB cover nearly 75% of the corpus using only phrases with 2 FL words, and nearly 36% of the corpus using phraseswith3FLwords." ></td>
	<td class="line x" title="169:186	Thissuggeststhatrecallorientedalignmentsfailtocatchasignificantnumber of phrases that would be useful to decode this test set, and precision-oriented alignments would yield potentially more useful phrases." ></td>
	<td class="line x" title="170:186	Since precision-oriented alignments make a higher number of longer phrases available to the decoder (based on the coverage of phrases presented in Table 9), they are used more during decoding." ></td>
	<td class="line x" title="171:186	Consequently, the major difference between the alignments is the coverage of the phrases extracted from different alignments." ></td>
	<td class="line x" title="172:186	The more the phrase table covers the test set, the more the longer phrases are used during decoding, and precision-oriented alignments are better at generating high-coverage phrases than recall-oriented alignments." ></td>
	<td class="line x" title="173:186	15 5 Conclusions and Future Work This paper investigated how different alignments change the behavior of phrase-based MT. We showed that AER is a poor indicator of MT performance because it penalizes incorrect links less than is reflected in the corresponding phrasebased MT. During phrase-based MT, an incorrect alignment link might prevent extraction of several phrases, but the number of phrases affected by that link depends on the context." ></td>
	<td class="line x" title="174:186	Wedesigned CPER,anew phrase-orientedmetric that is more informative than AER when the alignments are used in a phrase-based MT system because it is an indicator of how the set of phrases differ from one alignment to the next according to a pre-specified maximum phrase length." ></td>
	<td class="line x" title="175:186	Even with refined evaluation metrics (including CPER), we found it difficult to assess the impact of alignment on MT performance because word alignment is not the only factor that affects the choice of the correct words (or phrases) during decoding." ></td>
	<td class="line x" title="176:186	We empirically showed that different phrase extraction techniques result in better MT output for certain alignments but the MT performance gets worse for other alignments." ></td>
	<td class="line x" title="177:186	Similarly, adjusting the scores assigned to the phrases makes a significant difference for certain alignments while it has no impact on some others." ></td>
	<td class="line x" title="178:186	Consequently, whencomparingtwoBLEUscores, itis difficult to determine whether the alignments are bad to start with or the set of extracted phrases is bad or the phrases extracted from the alignments are assigned bad scores." ></td>
	<td class="line x" title="179:186	This suggests that finding a direct correlation between AER (or even CPER) and the automated MT metrics is infeasible." ></td>
	<td class="line x" title="180:186	We demonstrated that recall-oriented alignment methods yield smaller phrase tables and a higher number of untranslated words when compared to precision-oriented methods." ></td>
	<td class="line x" title="181:186	We also showed that the phrases extracted from recall-oriented alignments cover a smaller portion of a given test set when compared to precision-oriented alignments." ></td>
	<td class="line x" title="182:186	Finally, we showed that the decoder with recalloriented alignments uses shorter phrases more frequently as a result of unavailability of longer phrases that are extracted." ></td>
	<td class="line x" title="183:186	Future work will involve an investigation into how the phrase extraction and scoring should be adjusted to take the nature of the alignment into account and how the phrase-table size might be reduced without sacrificing the MT output quality." ></td>
	<td class="line x" title="184:186	Acknowledgments This work has been supported, in part, under ONR MURI Contract FCPO.810548265 and the GALE program of the Defense Advanced Research Projects Agency, Contract No." ></td>
	<td class="line x" title="185:186	HR0011-06-2-0001." ></td>
	<td class="line x" title="186:186	We also thank Adam Lopez for his very helpful comments on earlier drafts of this paper." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2037
Low-Cost Enrichment Of Spanish WordNet With Automatically Translated Glosses: Combining General And Specialized Models
Giménez, Jesús;Màrquez, Lluís;"></td>
	<td class="line x" title="1:205	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287294, Sydney, July 2006." ></td>
	<td class="line x" title="2:205	c2006 Association for Computational Linguistics Low-cost Enrichment of Spanish WordNet with Automatically Translated Glosses: Combining General and Specialized Models Jesus Gimenez and Llus M`arquez TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya Jordi Girona Salgado 13, E-08034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English WordNet glosses using a phrase-based Statistical Machine Translation system." ></td>
	<td class="line x" title="3:205	We construct the English-Spanish translation system from a parallel corpus of proceedings of the European Parliament, and study how to adapt statistical models to the domain of dictionary definitions." ></td>
	<td class="line x" title="4:205	We build specialized language and translation models from a small set of parallel definitions and experiment with robust manners to combine them." ></td>
	<td class="line x" title="5:205	A statistically significant increase in performance is obtained." ></td>
	<td class="line x" title="6:205	The best system is finally used to generate a definition for all Spanish synsets, which are currently ready for a manual revision." ></td>
	<td class="line x" title="7:205	As a complementary issue, we analyze the impact of the amount of in-domain data needed to improve a system trained entirely on out-of-domain data." ></td>
	<td class="line x" title="8:205	1 Introduction Statistical Machine Translation (SMT) is today a very promising approach." ></td>
	<td class="line x" title="9:205	It allows to build very quickly and fully automatically Machine Translation (MT) systems, exhibiting very competitive results, only from a parallel corpus aligning sentences from the two languages involved." ></td>
	<td class="line x" title="10:205	In this work we approach the task of enriching Spanish WordNet with automatically translated glosses1." ></td>
	<td class="line x" title="11:205	The source glosses for these translations are taken from the English WordNet (Fellbaum, 1Glosses are short dictionary definitions that accompany WordNet synsets." ></td>
	<td class="line x" title="12:205	See examples in Tables 5 and 6." ></td>
	<td class="line x" title="13:205	1998), which is linked, at the synset level, to Spanish WordNet." ></td>
	<td class="line x" title="14:205	This resource is available, among other sources, through the Multilingual Central Repository (MCR) developed by the MEANING project (Atserias et al. , 2004)." ></td>
	<td class="line x" title="15:205	We start by empirically testing the performance of a previously developed EnglishSpanish SMT system, built from the large Europarl corpus2 (Koehn, 2003)." ></td>
	<td class="line x" title="16:205	The first observation is that this system completely fails to translate the specific WordNet glosses, due to the large language variations in both domains (vocabulary, style, grammar, etc.)." ></td>
	<td class="line x" title="17:205	Actually, this is confirming one of the main criticisms against SMT, which is its strong domain dependence." ></td>
	<td class="line x" title="18:205	Since parameters are estimated from a corpus in a concrete domain, the performance of the system on a different domain is often much worse." ></td>
	<td class="line x" title="19:205	This flaw of statistical and machine learning approaches is well known and has been largely described in the NLP literature, for a variety of tasks (e.g. , parsing, word sense disambiguation, and semantic role labeling)." ></td>
	<td class="line x" title="20:205	Fortunately, we count on a small set of Spanish hand-developed glosses in MCR3." ></td>
	<td class="line x" title="21:205	Thus, we move to a working scenario in which we introduce a small corpus of aligned translations from the concrete domain of WordNet glosses." ></td>
	<td class="line x" title="22:205	This in-domain corpus could be itself used as a source for constructing a specialized SMT system." ></td>
	<td class="line x" title="23:205	Again, experiments show that this small corpus alone does not suffice, since it does not allow to estimate good translation parameters." ></td>
	<td class="line x" title="24:205	However, it is well suited for combination with the Europarl corpus, to generate combined Language and Translation 2The Europarl Corpus is available at: http://people.csail.mit.edu/people/koehn/publications/europarl 3About 10% of the 68,000 Spanish synsets contain a definition, generated without considering its English counterpart." ></td>
	<td class="line x" title="25:205	287 Models." ></td>
	<td class="line x" title="26:205	A substantial increase in performance is achieved, according to several standard MT evaluation metrics." ></td>
	<td class="line x" title="27:205	Although moderate, this boost in performance is statistically significant according to the bootstrap resampling test described by Koehn (2004b) and applied to the BLEU metric." ></td>
	<td class="line x" title="28:205	The main reason behind this improvement is that the large out-of-domain corpus contributes mainly with coverage and recall and the in-domain corpus provides more precise translations." ></td>
	<td class="line x" title="29:205	We present a qualitative error analysis to support these claims." ></td>
	<td class="line x" title="30:205	Finally, we also address the important question of how much in-domain data is needed to be able to improve the baseline results." ></td>
	<td class="line x" title="31:205	Apart from the experimental findings, our study has generated a very valuable resource." ></td>
	<td class="line x" title="32:205	Currently, we have the complete Spanish WordNet enriched with one gloss per synset, which, far from being perfect, constitutes an axcellent starting point for a posterior manual revision." ></td>
	<td class="line x" title="33:205	Finally, we note that the construction of a SMT system when few domain-specific data are available has been also investigated by other authors." ></td>
	<td class="line x" title="34:205	For instance, Vogel and Tribble (2002) studied whether an SMT system for speech-to-speech translation built on top of a small parallel corpus can be improved by adding knowledge sources which are not domain specific." ></td>
	<td class="line x" title="35:205	In this work, we look at the same problem the other way around." ></td>
	<td class="line x" title="36:205	We study how to adapt an out-of-domain SMT system using in-domain data." ></td>
	<td class="line x" title="37:205	The rest of the paper is organized as follows." ></td>
	<td class="line x" title="38:205	In Section 2 the fundamentals of SMT and the components of our MT architecture are described." ></td>
	<td class="line x" title="39:205	The experimental setting is described in Section 3." ></td>
	<td class="line x" title="40:205	Evaluation is carried out in Section 4." ></td>
	<td class="line x" title="41:205	Finally, Section 5 contains error analysis and Section 6 concludes and outlines future work." ></td>
	<td class="line x" title="42:205	2 Background Current state-of-the-art SMT systems are based on ideas borrowed from the Communication Theory field." ></td>
	<td class="line x" title="43:205	Brown et al.(1988) suggested that MT can be statistically approximated to the transmission of information through a noisy channel." ></td>
	<td class="line x" title="45:205	Given a sentence f = f1fn (distorted signal), it is possible to approximate the sentence e = e1em (original signal) which produced f. We need to estimate P(e|f), the probability that a translator produces f as a translation of e. By applying Bayes rule it is decomposed into: P(e|f) = P(f|e)P(e)P(f)." ></td>
	<td class="line x" title="46:205	To obtain the string e which maximizes the translation probability for f, a search in the probability space must be performed." ></td>
	<td class="line x" title="47:205	Because the denominator is independent of e, we can ignore it for the purpose of the search: e = argmaxeP(f|e)  P(e)." ></td>
	<td class="line x" title="48:205	This last equation devises three components in a SMT system." ></td>
	<td class="line x" title="49:205	First, a language model that estimates P(e)." ></td>
	<td class="line x" title="50:205	Second, a translation model representing P(f|e)." ></td>
	<td class="line x" title="51:205	Last, a decoder responsible for performing the arg-max search." ></td>
	<td class="line x" title="52:205	Language models are typically estimated from large monolingual corpora, translation models are built out from parallel corpora, and decoders usually perform approximate search, e.g., by using dynamic programming and beam search." ></td>
	<td class="line x" title="53:205	However, in word-based models the modeling of the context in which the words occur is very weak." ></td>
	<td class="line x" title="54:205	This problem is significantly alleviated by phrase-based models (Och, 2002), which represent nowadays the state-of-the-art in SMT." ></td>
	<td class="line x" title="55:205	2.1 System Construction Fortunately, there is a number of freely available tools to build a phrase-based SMT system." ></td>
	<td class="line x" title="56:205	We used only standard components and techniques for our basic system, which are all described below." ></td>
	<td class="line x" title="57:205	The SRI Language Modeling Toolkit (SRILM) (Stolcke, 2002) supports creation and evaluation of a variety of language models." ></td>
	<td class="line x" title="58:205	We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing." ></td>
	<td class="line x" title="59:205	In order to build phrase-based translation models, a phrase extraction must be performed on a word-aligned parallel corpus." ></td>
	<td class="line x" title="60:205	We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) to generate word alignments We applied the phraseextract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++." ></td>
	<td class="line x" title="61:205	We work with the union of source-to-target and targetto-source alignments, with no heuristic refinement." ></td>
	<td class="line x" title="62:205	Phrases up to length five are considered." ></td>
	<td class="line x" title="63:205	Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase was more than three times longer than the target/source phrase are ignored." ></td>
	<td class="line x" title="64:205	Finally, phrase pairs are scored by relative frequency." ></td>
	<td class="line x" title="65:205	Note that no smoothing is performed." ></td>
	<td class="line x" title="66:205	Regarding the arg-max search, we used the Pharaoh beam search decoder (Koehn, 2004a), which naturally fits with the previous tools." ></td>
	<td class="line x" title="67:205	4http://www.fjoch.com/GIZA++.html 288 3 Data Sets and Evaluation Metrics As a general source of EnglishSpanish parallel text, we used a collection of 730,740 parallel sentences extracted from the Europarl corpus." ></td>
	<td class="line x" title="68:205	These correspond exactly to the training data from the Shared Task 2: Exploiting Parallel Texts for Statistical Machine Translation from the ACL-2005 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond5." ></td>
	<td class="line x" title="69:205	To be used as specialized source, we extracted, from the MCR, the set of 6,519 EnglishSpanish parallel glosses corresponding to the already defined synsets in Spanish WordNet." ></td>
	<td class="line x" title="70:205	These definitions corresponded to 5,698 nouns, 87 verbs, and 734 adjectives." ></td>
	<td class="line x" title="71:205	Examples and parenthesized texts were removed." ></td>
	<td class="line x" title="72:205	Parallel glosses were tokenized and case lowered." ></td>
	<td class="line x" title="73:205	We discarded some of these parallel glosses based on the difference in length between the source and the target." ></td>
	<td class="line x" title="74:205	The gloss average length for the resulting 5,843 glosses was 8.25 words for English and 8.13 for Spanish." ></td>
	<td class="line x" title="75:205	Finally, gloss pairs were randomly split into training (4,843), development (500) and test (500) sets." ></td>
	<td class="line x" title="76:205	Additionally, we counted on two large monolingual Spanish electronic dictionaries, consisting of 142,892 definitions (2,112,592 tokens) (D1) (Mart, 1996) and 168,779 definitons (1,553,674 tokens) (D2) (Vox, 1990), respectively." ></td>
	<td class="line x" title="77:205	Regarding evaluation, we used up to four different metrics with the aim of showing whether the improvements attained are consistent or not." ></td>
	<td class="line oc" title="78:205	We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al. , 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e = 1,2) (Melamed et al. , 2003), and the METEOR measure (Banerjee and Lavie, 2005)." ></td>
	<td class="line o" title="79:205	These metrics work at the lexical level by rewarding n-gram matches between the candidate translation and a set of human references." ></td>
	<td class="line o" title="80:205	Additionally, METEOR considers stemming, and allows for WordNet synonymy lookup." ></td>
	<td class="line x" title="81:205	The discussion of the significance of the results will be based on the BLEU score, for which we computed a bootstrap resampling test of significance (Koehn, 2004b)." ></td>
	<td class="line x" title="82:205	5http://www.statmt.org/wpt05/." ></td>
	<td class="line x" title="83:205	4 Experimental Evaluation 4.1 Baseline Systems As explained in the introduction we built two individual baseline systems." ></td>
	<td class="line x" title="84:205	The first baseline (EU) system is entirely based on the training data from the Europarl corpus." ></td>
	<td class="line x" title="85:205	The second baseline system (WNG) is entirely based on the training set from of the in-domain corpus of parallel glosses." ></td>
	<td class="line x" title="86:205	In the second case phrase pairs occurring only once in the training corpus are not discarded due to the extremely small size of the corpus." ></td>
	<td class="line x" title="87:205	Table 1 shows results of the two baseline systems, both for the development and test sets." ></td>
	<td class="line x" title="88:205	We compare the performance of the EU baseline on these data sets with respect to the (in-domain) Europarl test set provided by the organizers of the ACL-2005 MT workshop." ></td>
	<td class="line x" title="89:205	As expected, there is a very significant decrease in performance (e.g. , from 0.24 to 0.08 according to BLEU) when the EU baseline system is applied to the new domain." ></td>
	<td class="line x" title="90:205	Some of this decrement is also due to a certain degree of free translation exhibited by the set of available quasi-parallel glosses." ></td>
	<td class="line x" title="91:205	We further discuss this issue in Section 5." ></td>
	<td class="line x" title="92:205	The results obtained by WNG are also very low, though slightly better than those of EU." ></td>
	<td class="line x" title="93:205	This is a very interesting fact." ></td>
	<td class="line x" title="94:205	Although the amount of data utilized to construct the WNG baseline is 150 times smaller than the amount utilized to construct the EU baseline, its performance is higher consistently according to all metrics." ></td>
	<td class="line x" title="95:205	We interpret this result as an indicator that models estimated from in-domain data provide higher precision." ></td>
	<td class="line x" title="96:205	We also compare the results to those of a commercial system such as the on-line version 5.0 of SYSTRAN6, a general-purpose MT system based on manually-defined lexical and syntactic transfer rules." ></td>
	<td class="line x" title="97:205	The performance of the baseline systems is significantly worse than SYSTRANs on both development and test sets." ></td>
	<td class="line x" title="98:205	This means that a rule-based system like SYSTRAN is more robust than the SMT-based systems." ></td>
	<td class="line x" title="99:205	The difference against the specialized WNG also suggests that the amount of data used to train the WNG baseline is clearly insufficient." ></td>
	<td class="line x" title="100:205	4.2 Combining Sources: Language Models In order to improve results, in first place we turned our eyes to language modeling." ></td>
	<td class="line x" title="101:205	In addition to 6http://www.systransoft.com/." ></td>
	<td class="line o" title="102:205	289 system BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR development EU-baseline 0.0737 2.8832 0.3131 0.2216 0.2881 WNG-baseline 0.1149 3.3492 0.3604 0.2605 0.3288 SYSTRAN 0.1625 3.9467 0.4257 0.2971 0.4394 test EU-baseline 0.0790 2.8896 0.3131 0.2262 0.2920 WNG-baseline 0.0951 3.1307 0.3471 0.2510 0.3219 SYSTRAN 0.1463 3.7873 0.4085 0.2921 0.4295 acl05-test EU-baseline 0.2381 6.5848 0.5699 0.2429 0.5153 Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the EU baseline system on the ACL-2005 SMT workshop test set extracted from the Europarl Corpus." ></td>
	<td class="line x" title="103:205	BLEU.n4 shows the accumulated BLEU score for 4-grams." ></td>
	<td class="line x" title="104:205	NIST.n5 shows the accumulated NIST score for 5-grams." ></td>
	<td class="line x" title="105:205	GTM.e1 and GTM.e2 show the GTM F1measure for different values of the e parameter (e = 1, e = 2, respectively)." ></td>
	<td class="line o" title="106:205	METEOR reflects the METEOR score." ></td>
	<td class="line x" title="107:205	the language model built from the Europarl corpus (EU) and the specialized language model based on the small training set of parallel glosses (WNG), two specialized language models, based on the two large monolingual Spanish electronic dictionaries (D1 and D2) were used." ></td>
	<td class="line x" title="108:205	We tried several configurations." ></td>
	<td class="line x" title="109:205	In all cases, language models are combined with equal probability." ></td>
	<td class="line x" title="110:205	See results, for the development set, in Table 2." ></td>
	<td class="line x" title="111:205	As expected, the closer the language model is to the target domain, the better results." ></td>
	<td class="line x" title="112:205	Observe how results using language models D1 and D2 outperform results using EU." ></td>
	<td class="line x" title="113:205	Note also that best results are in all cases consistently attained by using the WNG language model." ></td>
	<td class="line x" title="114:205	This means that language models estimated from small sets of indomain data are helpful." ></td>
	<td class="line x" title="115:205	A second conclusion is that a significant gain is obtained by incrementally adding (in-domain) specialized language models to the baselines, according to all metrics but BLEU for which no combination seems to significantly outperform the WNG baseline alone." ></td>
	<td class="line x" title="116:205	Observe that best results are obtained, except in the case of BLEU, by the system using EU as translation model and WNG as language model." ></td>
	<td class="line x" title="117:205	We interpret this result as an indicator that translation models estimated from out-of-domain data are helpful because they provide recall." ></td>
	<td class="line x" title="118:205	A third interesting point is that adding an out-of-domain language model (EU) does not seem to help, at least combined with equal probability than in-domain models." ></td>
	<td class="line x" title="119:205	Same conclusions hold for the test set, too." ></td>
	<td class="line x" title="120:205	4.3 Tuning the System Adjusting the Pharaoh parameters that control the importance of the different probabilities that govern the search may yield significant improvements." ></td>
	<td class="line x" title="121:205	In our case, it is specially important to properly adjust the contribution of the language models." ></td>
	<td class="line x" title="122:205	We adjusted parameters by means of a software based on the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002)." ></td>
	<td class="line x" title="123:205	The tuning was based on the improvement attained in BLEU score over the development set." ></td>
	<td class="line x" title="124:205	We tuned 6 parameters: 4 language models (lmEU, lmD1, lmD2, lmWNG), the translation model (), and the word penalty (w)7." ></td>
	<td class="line x" title="125:205	Results improve substantially." ></td>
	<td class="line x" title="126:205	See Table 3." ></td>
	<td class="line x" title="127:205	Best results are still attained using the EU translation model." ></td>
	<td class="line x" title="128:205	Interestingly, as suggested by Table 2, the weight of language models is concentrated on the WNG language model (lmWNG = 0.95)." ></td>
	<td class="line x" title="129:205	4.4 Combining Sources: Translation Models In this section we study the possibility of combining out-of-domain and in-domain translation models aiming at achieving a good balance between precision and recall that yields better MT results." ></td>
	<td class="line x" title="130:205	Two different strategies have been tried." ></td>
	<td class="line x" title="131:205	In a first stragegy we simply concatenate the outof-domain corpus (EU) and the in-domain corpus (WNG)." ></td>
	<td class="line x" title="132:205	Then, we construct the translatation model (EUWNG) as detailed in Section 2.1." ></td>
	<td class="line x" title="133:205	A second manner to proceed is to linearly combine the two different translation models into a single translation model (EU+WNG)." ></td>
	<td class="line x" title="134:205	In this case, we can assign different weights () to the contribution of the different models to the search." ></td>
	<td class="line x" title="135:205	We can also determine a certain threshold  which allows us 7Final values when using the EU translation model are lmEU = 0.22, lmD1 = 0, lmD2 = 0.01, lmWNG = 0.95,  = 1, and w = 2.97, while when using the WNG translation model final values are lmEU = 0.17, lmD1 = 0.07, lmD2 = 0.13, lmWNG = 1,  = 0.95, and w = 2.64." ></td>
	<td class="line o" title="136:205	290 Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR EU EU 0.0737 2.8832 0.3131 0.2216 0.2881 EU WNG 0.1062 3.4831 0.3714 0.2631 0.3377 EU D1 0.0959 3.2570 0.3461 0.2503 0.3158 EU D2 0.0896 3.2518 0.3497 0.2482 0.3163 EU D1 + D2 0.0993 3.3773 0.3585 0.2579 0.3244 EU EU + D1 + D2 0.0960 3.2851 0.3472 0.2499 0.3160 EU D1 + D2 + WNG 0.1094 3.4954 0.3690 0.2662 0.3372 EU EU + D1 + D2 + WNG 0.1080 3.4248 0.3638 0.2614 0.3321 WNG EU 0.0743 2.8864 0.3128 0.2202 0.2689 WNG WNG 0.1149 3.3492 0.3604 0.2605 0.3288 WNG D1 0.0926 3.1544 0.3404 0.2418 0.3050 WNG D2 0.0845 3.0295 0.3256 0.2326 0.2883 WNG D1 + D2 0.0917 3.1185 0.3331 0.2394 0.2995 WNG EU + D1 + D2 0.0856 3.0361 0.3221 0.2312 0.2847 WNG D1 + D2 + WNG 0.0980 3.2238 0.3462 0.2479 0.3117 WNG EU + D1 + D2 + WNG 0.0890 3.0974 0.3309 0.2373 0.2941 Table 2: MT Results on development set, for several translation/language model configurations." ></td>
	<td class="line x" title="137:205	EU and WNG refer to the models estimated from the Europarl corpus and the training set of parallel WordNet glosses, respectively." ></td>
	<td class="line x" title="138:205	D1, and D2 denote the specialized language models estimated from the two dictionaries." ></td>
	<td class="line o" title="139:205	Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR development EU EU + D1 + D2 + WNG 0.1272 3.6094 0.3856 0.2727 0.3695 WNG EU + D1 + D2 + WNG 0.1269 3.3740 0.3688 0.2676 0.3452 test EU EU + D1 + D2 + WNG 0.1133 3.4180 0.3720 0.2650 0.3644 WNG EU + D1 + D2 + WNG 0.1015 3.1084 0.3525 0.2552 0.3343 Table 3: MT Results on development and test sets after tuning for the EU + D1 + D2 + WNG language model configuration for the two translation models, EU and WNG." ></td>
	<td class="line x" title="140:205	to discard phrase pairs under a certain probability." ></td>
	<td class="line x" title="141:205	These weights and thresholds were adjusted8 as detailed in Subsection 4.3." ></td>
	<td class="line x" title="142:205	Interestingly, at combination time the importance of the WNG translation model (tmWNG = 0.9) is much higher than that of the EU translation model (tmEU = 0.1)." ></td>
	<td class="line x" title="143:205	Table 4 shows results for the two strategies." ></td>
	<td class="line x" title="144:205	As expected, the EU+WNG strategy consistently obtains the best results according to all metrics both on the development and test sets, since it allows to better adjust the relative importance of each translation model." ></td>
	<td class="line x" title="145:205	However, both techniques achieve a very competitive performance." ></td>
	<td class="line x" title="146:205	Results improve, according to BLEU, from 0.13 to 0.16, and from 0.11 to 0.14, for the development and test sets, respectively." ></td>
	<td class="line x" title="147:205	We measured the statistical signficance of the overall improvement in BLEU.n4 attained with respect to the baseline results by applying the bootstrap resampling technique described by Koehn (2004b)." ></td>
	<td class="line x" title="148:205	The 95% confidence intervals extracted from the test set after 8We used values tmEU = 0.1, tmWNG = 0.9, tmEU = 0.1, and tmWNG = 0.01 10,000 samples are the following: IEUbase = [0.0642,0.0939], IWNGbase = [0.0788,0.1112], IEU+WNGbest = [0.1221,0.1572]." ></td>
	<td class="line x" title="149:205	Since the intervals are not ovelapping, we can conclude that the performance of the best combined method is statistically higher than the ones of the two baseline systems." ></td>
	<td class="line x" title="150:205	4.5 How much in-domain data is needed?" ></td>
	<td class="line x" title="151:205	In principle, the more in-domain data we have the better, but these may be difficult or expensive to collect." ></td>
	<td class="line x" title="152:205	Thus, a very interesting issue in the context of our work is how much in-domain data is needed in order to improve results attained using out-of-domain data alone." ></td>
	<td class="line x" title="153:205	To answer this question we focus on the EU+WNG strategy and analyze the impact on performance (BLEU.n4) of specialized models extracted from an incrementally bigger number of example glosses." ></td>
	<td class="line x" title="154:205	The results are presented in the plot of Figure 1." ></td>
	<td class="line x" title="155:205	We compute three variants separately, by considering the use of the in-domain data: only for the translation model (TM), only for the language model (LM), and simultaneously in both models (TM+LM)." ></td>
	<td class="line o" title="156:205	In order 291 Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR development EUWNG WNG 0.1288 3.7677 0.3949 0.2832 0.3711 EUWNG EU + D1 + D2 + WNG 0.1182 3.6034 0.3835 0.2759 0.3552 EUWNG EU + D1 + D2 + WNG (TUNED) 0.1554 3.8925 0.4081 0.2944 0.3998 EU+WNG WNG 0.1384 3.9743 0.4096 0.2936 0.3804 EU+WNG EU + D1 + D2 + WNG 0.1235 3.7652 0.3911 0.2801 0.3606 EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1618 4.1415 0.4234 0.3029 0.4130 test EUWNG WNG 0.1123 3.6777 0.3829 0.2771 0.3595 EUWNG EU + D1 + D2 + WNG 0.1183 3.5819 0.3737 0.2772 0.3518 EUWNG EU + D1 + D2 + WNG (TUNED) 0.1290 3.6478 0.3920 0.2810 0.3885 EU+WNG WNG 0.1227 3.8970 0.3997 0.2872 0.3723 EU+WNG EU + D1 + D2 + WNG 0.1199 3.7353 0.3846 0.2812 0.3583 EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1400 3.8930 0.4084 0.2907 0.3963 Table 4: MT Results on development and test sets for the two strategies for combining translations models." ></td>
	<td class="line x" title="157:205	0.06 0.07 0.08 0.09 0.1 0.11 0.12 0.13 0.14 0 500 1000 1500 2000 2500 3000 3500 4000 4500 BLEU.n4 # glosses baseline TM + LM impact TM impact LM impact Figure 1: Impact of the size of in-domain data on MT system performance for the test set." ></td>
	<td class="line x" title="158:205	to avoid the possible effect of over-fitting we focus on the behavior on the test set." ></td>
	<td class="line x" title="159:205	Note that the optimization of parameters is performed at each point in the x-axis using only the development set." ></td>
	<td class="line x" title="160:205	A significant initial gain of around 0.3 BLEU points is observed when adding as few as 100 glosses." ></td>
	<td class="line x" title="161:205	In all cases, it is not until around 1,000 glosses are added that the EU+WNG system stabilizes." ></td>
	<td class="line x" title="162:205	After that, results continue improving as more in-domain data are added." ></td>
	<td class="line x" title="163:205	We observe a very significant increase by just adding around 3,000 glosses." ></td>
	<td class="line x" title="164:205	Another interesting observation is the boosting effect of the combination of TM and LM specialized models." ></td>
	<td class="line x" title="165:205	While individual curves for TM and LM tend to be more stable with more than 4,000 added examples, the TM+LM curve still shows a steep increase in this last part." ></td>
	<td class="line x" title="166:205	5 Error Analysis We inspected results at the sentence level based on the GTM F-measure (e = 1) for the best configuration of the EU+WNG system." ></td>
	<td class="line x" title="167:205	196 sentences out from the 500 obtain an F-measure equal to or higher than 0.5 on the development set (181 sentences in the case of test set), whereas only 54 sentences obtain a score lower than 0.1." ></td>
	<td class="line x" title="168:205	These numbers give a first idea of the relative usefulness of our system." ></td>
	<td class="line x" title="169:205	Table 5 shows some translation cases selected for discussion." ></td>
	<td class="line x" title="170:205	For instance, Case 1 is a clear example of unfair low score." ></td>
	<td class="line x" title="171:205	The problem is that source and reference are not parallel but quasi-parallel." ></td>
	<td class="line x" title="172:205	Both glosses define the same concept but in a different way." ></td>
	<td class="line x" title="173:205	Thus, metrics based on rewarding lexical similarities are not well suited for these cases." ></td>
	<td class="line x" title="174:205	Cases 2, 3, 4 are examples of proper cooperation between EU and WNG models." ></td>
	<td class="line x" title="175:205	EU models provides recall, for instance by suggesting translation candidates for bombs or price below." ></td>
	<td class="line x" title="176:205	WNG models provide precision, for instance by choosing the right translation for an attack or the act of." ></td>
	<td class="line x" title="177:205	We also compared the EU+WNG system to SYSTRAN." ></td>
	<td class="line x" title="178:205	In the case of SYSTRAN 167 sentences obtain a score equal to or higher than 0.5 whereas 79 sentences obtain a score lower than 0.1." ></td>
	<td class="line x" title="179:205	These numbers are slightly under the performance of the EU+WNG system." ></td>
	<td class="line x" title="180:205	Table 6 shows some translation cases selected for discussion." ></td>
	<td class="line x" title="181:205	Case 1 is again an example of both systems obtaining very low scores because of quasiparallelism." ></td>
	<td class="line x" title="182:205	Cases 2 and 3 are examples of SYSTRAN outperforming our system." ></td>
	<td class="line x" title="183:205	In case 2 SYSTRAN exhibits higher precision in the translation of accompanying and illustration, whereas in case 3 it shows higher recall by suggesting appropriate translation candidates for fibers, silkworm, cocoon, threads, and knitting." ></td>
	<td class="line x" title="184:205	Cases 292 FE FW FEW Source OutE OutW OutEW Reference 0.0000 0.1333 0.1111 of the younger de acuerdo con de la younger de acuerdo con que tiene of two boys el mas joven de dos boys el mas joven de menos edad with the same de dos boys tiene el mismo dos muchachos family name con la misma nombre familia tiene el mismo familia fama nombre familia 0.2857 0.2500 0.5000 an attack atacar por ataque ataque ataque con by dropping cayendo realizado por realizado por bombas bombs bombas dropping bombs cayendo bombas 0.1250 0.7059 0.5882 the act of acto de la accion y efecto accion y efecto accion y efecto informing by informacion de informing de informaba de informar verbal report por verbales por verbal por verbales con una expliponencia explicacion explicacion cacion verbal 0.5000 0.0000 0.5000 a price below un precio por una price un precio por precio que esta the standard debajo de la below numbero debajo de la por debajo de price norma precio estandar price estandar precio lo normal Table 5: MT output analysis of the EU, WNG and EU+WNG systems." ></td>
	<td class="line x" title="185:205	FE, FW and FEW refer to the GTM (e = 1) F-measure attained by the EU, WNG and EU+WNG systems, respectively." ></td>
	<td class="line x" title="186:205	Source, OutE, OutW and OutEW refer to the input and the output of the systems." ></td>
	<td class="line x" title="187:205	Reference corresponds to the expected output." ></td>
	<td class="line x" title="188:205	4 and 5 are examples where our system outperforms SYSTRAN." ></td>
	<td class="line x" title="189:205	In case 4, our system provides higher recall by suggesting an adequate translation for top of something." ></td>
	<td class="line x" title="190:205	In case 5, our system shows higher precision by selecting a better translation for rate." ></td>
	<td class="line x" title="191:205	However, we observed that SYSTRAN tends in most cases to construct sentences exhibiting a higher degree of grammaticality." ></td>
	<td class="line x" title="192:205	6 Conclusions In this work, we have enriched every synset in Spanish WordNet with a preliminary gloss, which can be later updated in a lighter process of manual revision." ></td>
	<td class="line x" title="193:205	Though imperfect, this material constitutes a very valuable resource." ></td>
	<td class="line x" title="194:205	For instance, WordNet glosses have been used in the past to generate sense tagged corpora (Mihalcea and Moldovan, 1999), or as external knowledge for Question Answering systems (Hovy et al. , 2001)." ></td>
	<td class="line x" title="195:205	We have also shown the importance of using a small set of in-domain parallel sentences in order to adapt a phrase-based general SMT system to a new domain." ></td>
	<td class="line x" title="196:205	In particular, we have worked on specialized language and translation models and on their combination with general models in order to achieve a proper balance between precision (specialized in-domain models) and recall (general out-of-domain models)." ></td>
	<td class="line x" title="197:205	A substantial increase is consistently obtained according to standard MT evaluation metrics, which has been shown to be statistically significant in the case of BLEU." ></td>
	<td class="line x" title="198:205	Broadly speaking, we have shown that around 3,000 glosses (very short sentence fragments) suffice in this domain to obtain a significant improvement." ></td>
	<td class="line x" title="199:205	Besides, all the methods used are language independent, assumed the availability of the required in-domain additional resources." ></td>
	<td class="line x" title="200:205	In the future we plan to work on domain independent translation models built from WordNet itself." ></td>
	<td class="line x" title="201:205	We may use the WordNet topology to provide translation candidates weighted according to the given domain." ></td>
	<td class="line x" title="202:205	Moreover, we are experimenting the applicability of current Word Sense Disambiguation (WSD) technology to MT. We could favor those translation candidates showing a closer semantic relation to the source." ></td>
	<td class="line x" title="203:205	We believe that coarse-grained is sufficient for the purpose of MT. Acknowledgements This research has been funded by the Spanish Ministry of Science and Technology (ALIADO TIC2002-04447-C02) and the Spanish Ministry of Education and Science (TRANGRAM, TIN200407925-C03-02)." ></td>
	<td class="line x" title="204:205	Our research group, TALP Research Center, is recognized as a Quality Research Group (2001 SGR 00254) by DURSI, the Research Department of the Catalan Government." ></td>
	<td class="line x" title="205:205	Authors are grateful to Patrik Lambert for providing us with the implementation of the Simplex Method, and specially to German Rigau for motivating in its origin all this work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P06-2070
Stochastic Iterative Alignment For Machine Translation Evaluation
Liu, Ding;Gildea, Daniel;"></td>
	<td class="line x" title="1:155	Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 539546, Sydney, July 2006." ></td>
	<td class="line x" title="2:155	c2006 Association for Computational Linguistics Stochastic Iterative Alignment for Machine Translation Evaluation Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester, NY 14627 Abstract A number of metrics for automatic evaluation of machine translation have been proposed in recent years, with some metrics focusing on measuring the adequacy of MT output, and other metrics focusing on uency." ></td>
	<td class="line x" title="3:155	Adequacy-oriented metrics such as BLEU measure n-gram overlap of MT outputs and their references, but do not represent sentence-level information." ></td>
	<td class="line x" title="4:155	In contrast, uency-oriented metrics such as ROUGE-W compute longest common subsequences, but ignore words not aligned by the LCS." ></td>
	<td class="line x" title="5:155	We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches." ></td>
	<td class="line x" title="6:155	We compare SIA with existing metrics, and nd that it outperforms them in overall evaluation, and works specially well in uency evaluation." ></td>
	<td class="line x" title="7:155	1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence." ></td>
	<td class="line x" title="8:155	Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years." ></td>
	<td class="line x" title="9:155	In the 2003 Johns Hopkins Workshop on Speech and Language Engineering, experiments on MT evaluation showed that BLEU and NIST do not correlate well with human judgments at the sentence level, even when they correlate well over large test sets (Blatz et al. , 2003)." ></td>
	<td class="line x" title="10:155	Liu and Gildea (2005) also pointed out that due to the limited references for every MT output, using the overlapping ratio of n-grams longer than 2 did not improve sentence level evaluation performance of BLEU." ></td>
	<td class="line x" title="11:155	The problem leads to an even worse result in BLEUS uency evaluation, which is supposed to rely on the long ngrams." ></td>
	<td class="line oc" title="12:155	In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="13:155	ROUGE-W differs from BLEU and NIST in that it doesnt require the common sequence between MT output and the references to be consecutive, and thus longer common sequences can be found." ></td>
	<td class="line x" title="14:155	There is a problem with loose-sequencebased metrics: the words outside the longest common sequence are not considered in the metric, even if they appear both in MT output and the reference." ></td>
	<td class="line x" title="15:155	ROUGE-S is meant to alleviate this problem by computing the common skipped bigrams instead of the LCS." ></td>
	<td class="line x" title="16:155	But the price ROUGES pays is falling back to the shorter sequences and losing the advantage of long common sequences." ></td>
	<td class="line o" title="17:155	METEOR is essentially a unigram based metric, which prefers the monotonic word alignment between MT output and the references by penalizing crossing word alignments." ></td>
	<td class="line n" title="18:155	There are two problems with METEOR." ></td>
	<td class="line n" title="19:155	First, it doesnt consider gaps in the aligned words, which is an important feature for evaluating the sentence uency; second, it cannot use multiple references simultaneously.1 ROUGE and METEOR both use WordNet and Porter Stemmer to increase the chance of the MT output words matching the reference words." ></td>
	<td class="line x" title="20:155	Such morphological processing and synonym extraction tools are available for English, but are not always available for other languages." ></td>
	<td class="line n" title="21:155	In order to take advantage of loose-sequence-based metrics and avoid the problems in ROUGE and METEOR, we propose a new metric SIA, which is based on loose sequence alignment but enhanced with the following features: 1METEOR and ROUGE both compute the score based on the best reference 539  Computing the string alignment score based on the gaps in the common sequence." ></td>
	<td class="line x" title="22:155	Though ROUGE-W also takes into consider the gaps in the common sequence between the MT output and the reference by giving more credits to the n-grams in the common sequence, our method is more exible in that not only do the strict n-grams get more credits, but also the tighter sequences." ></td>
	<td class="line x" title="23:155	 Stochastic word matching." ></td>
	<td class="line o" title="24:155	For the purpose of increasing hitting chance of MT outputs in references, we use a stochastic word matching in the string alignment instead of WORDSTEM and WORD-NET used in METEOR and ROUGE." ></td>
	<td class="line x" title="25:155	Instead of using exact matching, we use a soft matching based on the similarity between two words, which is trained in a bilingual corpus." ></td>
	<td class="line x" title="26:155	The corpus is aligned in the word level using IBM Model4 (Brown et al. , 1993)." ></td>
	<td class="line x" title="27:155	Stochastic word matching is a uniform replacement for both morphological processing and synonym matching." ></td>
	<td class="line x" title="28:155	More importantly, it can be easily adapted for different kinds of languages, as long as there are bilingual parallel corpora available (which is always true for statistical machine translation)." ></td>
	<td class="line x" title="29:155	 Iterative alignment scheme." ></td>
	<td class="line x" title="30:155	In this scheme, the string alignment will be continued until there are no more co-occuring words to be found between the MT output and any one of the references." ></td>
	<td class="line x" title="31:155	In this way, every co-occuring word between the MT output and the references can be considered and contribute to the nal score, and multiple references can be used simultaneously." ></td>
	<td class="line o" title="32:155	The remainder of the paper is organized as follows: section 2 gives a recap of BLEU, ROUGEW and METEOR; section 3 describes the three components of SIA; section 4 compares the performance of different metrics based on experimental results; section 5 presents our conclusion." ></td>
	<td class="line o" title="33:155	2 Recap of BLEU, ROUGE-W and METEOR The most commonly used automatic evaluation metrics, BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002), are based on the assumption that The closer a machine translation is to a promt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 1: Alignment Example for ROUGE-W fessional human translation, the better it is (Papineni et al. , 2002)." ></td>
	<td class="line x" title="34:155	For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty." ></td>
	<td class="line x" title="35:155	NIST uses a similar strategy to BLEU but further considers that n-grams with different frequency should be treated differently in the evaluation (Doddington, 2002)." ></td>
	<td class="line x" title="36:155	BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities (Papineni et al. , 2002; Doddington, 2002)." ></td>
	<td class="line x" title="37:155	ROUGE-W is based on the weighted longest common subsequence (LCS) between the MT output and the reference." ></td>
	<td class="line x" title="38:155	The common subsequences in ROUGE-W are not necessarily strict n-grams, and gaps are allowed in both the MT output and the reference." ></td>
	<td class="line x" title="39:155	Because of the exibility, long common subsequences are feasible in ROUGEW and can help to re ect the sentence-wide similarity of MT output and references." ></td>
	<td class="line x" title="40:155	ROUGE-W uses a weighting strategy where the LCS containing strict n-grams is favored." ></td>
	<td class="line x" title="41:155	Figure 1 gives two examples that show how ROUGE-W searches for the LCS." ></td>
	<td class="line x" title="42:155	For mt1, ROUGE-W will choose either life is like chocolate or life is like box as the LCS, since neither of the sequences like box and like chocolate are strict n-grams and thus make no difference in ROUGE-W (the only strict n-grams in the two candidate LCS is life is)." ></td>
	<td class="line x" title="43:155	For mt2, there is only one choice of the LCS: life is of chocolate." ></td>
	<td class="line x" title="44:155	The LCS of mt1 and mt2 have the same length and the same number of strict n-grams, thus they get the same score in ROUGE-W." ></td>
	<td class="line x" title="45:155	But it is clear to us that mt1 is better than mt2." ></td>
	<td class="line x" title="46:155	It is easy to verify that mt1 and mt2 have the same number of common 1grams, 2-grams, and skipped 2-grams with the reference (they dont have common n-grams longer than 2 words), thus BLEU and ROUGE-S are also not able to differentiate them." ></td>
	<td class="line o" title="47:155	METEOR is a metric sitting in the middle of the n-gram based metrics and the loose se540 mt1: Life is like one nice chocolate in box ref: Life is just like a box of tasty chocolate ref: Life is just like a box of tasty chocolate mt2: Life is of one nice chocolate in box Figure 2: Alignment Example for METEOR quence based metrics." ></td>
	<td class="line x" title="48:155	It has several phases and in each phase different matching techniques (EXACT, PORTER-STEM, WORD-NET) are used to make an alignment for the MT output and the reference." ></td>
	<td class="line o" title="49:155	METEOR doesnt require the alignment to be monotonic, which means crossing word mappings (e.g. a b is mapped to b a) are allowed, though doing so will get a penalty." ></td>
	<td class="line o" title="50:155	Figure 2 shows the alignments of METEOR based on the same example as ROUGE." ></td>
	<td class="line o" title="51:155	Though the two alignments have the same number of word mappings, mt2 gets more crossed word mappings than mt1, thus it will get less credits in METEOR." ></td>
	<td class="line o" title="52:155	Both ROUGE and METEOR normalize their evaluation result based on the MT output length (precision) and the reference length (recall), and the nal score is computed as the F-mean of them." ></td>
	<td class="line x" title="53:155	3 Stochastic Iterative Alignment (SIA) for Machine Translation Evaluation We introduce three techniques to allow more sensitive scores to be computed." ></td>
	<td class="line x" title="54:155	3.1 Modified String Alignment This section introduces how to compute the string alignment based on the word gaps." ></td>
	<td class="line x" title="55:155	Given a pair of strings, the task of string alignment is to obtain the longest monotonic common sequence (where gaps are allowed)." ></td>
	<td class="line x" title="56:155	SIA uses a different weighting strategy from ROUGE-W, which is more exible." ></td>
	<td class="line x" title="57:155	In SIA, the alignments are evaluated based on the geometric mean of the gaps in the reference side and the MT output side." ></td>
	<td class="line x" title="58:155	Thus in the dynamic programming, the state not only includes the current covering length of the MT output and the reference, but also includes the last aligned positions in them." ></td>
	<td class="line x" title="59:155	The algorithm for computing the alignment score in SIA is described in Figure 3." ></td>
	<td class="line x" title="60:155	The subroutine COMPUTE SCORE, which computes the score gained from the current aligned positions, is shown in Figure 4." ></td>
	<td class="line x" title="61:155	From the algorithm, we can function GET ALIGN SCORE(mt, M, ref, N) triangleright Compute the alignment score of the MT output mt with length M and the reference ref with length N for i = 1; i  M; i = i +1 do for j = 1; j  N; j = j +1 do for k = 1; k  i; k = k +1 do for m = 1; m  j; m = m +1 do scorei,j,k,m = maxscorei1,j,k,m,scorei,j1,k,m } ; end for end for scorei,j,i,j = max n=1,M;p=1,N {scorei,j,i,j, scorei1,j1,n,p + COMPUTE SCORE(mt,ref, i, j, n, p)}; end for end for return scoreM,N,M,NM ; end function Figure 3: Alignment Algorithm Based on Gaps function COMPUTE SCORE(mt, ref, i, j, n, p) if mt[i] == ref [j] then return 1/p(i  n)  (j  p); else return 0; end if end function Figure 4: Compute Word Matching Score Based on Gaps see that not only will strict n-grams get higher scores than non-consecutive sequences, but also the non-consecutive sequences with smaller gaps will get higher scores than those with larger gaps." ></td>
	<td class="line x" title="62:155	This weighting method can help SIA capture more subtle difference of MT outputs than ROUGE-W does." ></td>
	<td class="line x" title="63:155	For example, if SIA is used to align mt1 and ref in Figure 1, it will choose life is like box instead of life is like chocolate, because the average distance of box-box to its previous mapping like-like is less than chocolate-chocolate." ></td>
	<td class="line x" title="64:155	Then the score SIA assigns to mt1 is: parenleftbigg 1 1  1 + 1 1  1 + 1 1  2 + 1 2  5 parenrightbigg 18 = 0.399 (1) For mt2, there is only one possible alignment, its score in SIA is computed as: parenleftbigg 1 1  1 + 1 1  1 + 1 1  5 + 1 2  3 parenrightbigg 18 = 0.357 (2) Thus, mt1 will be considered better than mt2 in SIA, which is reasonable." ></td>
	<td class="line x" title="65:155	As mentioned in section 1, though loose-sequence-based metrics give a better re ection of the sentence-wide similarity of the MT output and the reference, they cannot 541 make full use of word-level information." ></td>
	<td class="line x" title="66:155	This defect could potentially lead to a poor performance in adequacy evaluation, considering the case that the ignored words are crucial to the evaluation." ></td>
	<td class="line x" title="67:155	In the later part of this section, we will describe an iterative alignment scheme which is meant to compensate for this defect." ></td>
	<td class="line o" title="68:155	3.2 Stochastic Word Mapping In ROUGE and METEOR, PORTER-STEM and WORD-NET are used to increase the chance of the MT output words matching the references." ></td>
	<td class="line x" title="69:155	We use a different stochastic approach in SIA to achieve the same purpose." ></td>
	<td class="line x" title="70:155	The string alignment has a good dynamic framework which allows the stochastic word matching to be easily incorporated into it." ></td>
	<td class="line x" title="71:155	The stochastic string alignment can be implemented by simply replacing the function COMPUTE SCORE with the function of Figure 5." ></td>
	<td class="line x" title="72:155	The function similarity(word1, word2) returns a ratio which re ects how similar the two words are." ></td>
	<td class="line x" title="73:155	Now we consider how to compute the similarity ratio of two words." ></td>
	<td class="line x" title="74:155	Our method is motivated by the phrase extraction method of Bannard and Callison-Burch (2005), which computes the similarity ratio of two words by looking at their relationship with words in another language." ></td>
	<td class="line x" title="75:155	Given a bilingual parallel corpus with aligned sentences, say English and French, the probability of an English word given a French word can be computed by training word alignment models such as IBM Model4." ></td>
	<td class="line x" title="76:155	Then for every English word e, we have a set of conditional probabilities given each French word: p(e|f1), p(e|f2), , p(e|fN)." ></td>
	<td class="line x" title="77:155	If we consider these probabilities as a vector, the similarities of two English words can be obtained by computing the dot product of their corresponding vectors.2 The formula is described below: similarity(ei, ej) = Nsummationdisplay k=1 p(ei|fk)p(ej|fk) (3) Paraphrasing methods based on monolingual parallel corpora such as (Pang et al. , 2003; Barzilay and Lee, 2003) can also be used to compute the similarity ratio of two words, but they dont have as rich training resources as the bilingual methods do." ></td>
	<td class="line x" title="78:155	2Although the marginalized probability (over all French words) of an English word given the other English word (PNk=1 p(ei|fk)p(fk|ej)) is a more intuitive way of measuring the similarity, the dot product of the vectors p(e|f) described above performed slightly better in our experiments." ></td>
	<td class="line o" title="79:155	function STO COMPUTE SCORE(mt, ref, i, j, n, p) if mt[i] == ref [j] then return 1/p(i  n)  (j  p); else return similarity(mt[i],ref [i])(in)(jp) ; end if end function Figure 5: Compute Stochastic Word Matching Score 3.3 Iterative Alignment Scheme ROUGE-W, METEOR, and WER all score MT output by rst computing a score based on each available reference, and then taking the highest score as the nal score for the MT output." ></td>
	<td class="line x" title="80:155	This scheme has the problem of not being able to use multiple references simultaneously." ></td>
	<td class="line x" title="81:155	The iterative alignment scheme proposed here is meant to alleviate this problem, by doing alignment between the MT output and one of the available references until no more words in the MT output can be found in the references." ></td>
	<td class="line x" title="82:155	In each alignment round, the score based on each reference is computed and the highest one is taken as the score for the round." ></td>
	<td class="line x" title="83:155	Then the words which have been aligned in best alignment will not be considered in the next round." ></td>
	<td class="line x" title="84:155	With the same number of aligned words, the MT output with fewer alignment rounds should be considered better than those requiring more rounds." ></td>
	<td class="line x" title="85:155	For this reason, a decay factor  is multiplied with the scores of each round." ></td>
	<td class="line x" title="86:155	The nal score of the MT output is then computed by summing the weighted scores of each alignment round." ></td>
	<td class="line x" title="87:155	The scheme is described in Figure 6." ></td>
	<td class="line x" title="88:155	The function GET ALIGN SCORE 1 used in GET ALIGN SCORE IN MULTIPLE REFS is slightly different from GET ALIGN SCORE described in the prior subsection." ></td>
	<td class="line x" title="89:155	The dynamic programming algorithm for getting the best alignment is the same, except that it has two more tables as input, which record the unavailable positions in the MT output and the reference." ></td>
	<td class="line x" title="90:155	These positions have already been used in the prior best alignments and should not be considered in the ongoing alignment." ></td>
	<td class="line x" title="91:155	It also returns the aligned positions of the best alignment." ></td>
	<td class="line x" title="92:155	The pseudocode for GET ALIGN SCORE 1 is shown in Figure 7." ></td>
	<td class="line x" title="93:155	The computation of the length penalty is similar to BLEU: it is set to 1 if length of the MT output is longer than the arithmetic mean of length of the 542 function GET ALIGN SCORE IN MULTIPLE REFS(mt, ref 1,  , ref N, ) triangleright Iteratively Compute the Alignment Score Based on Multiple References and the Decay Factor  final score = 0; while max score != 0 do for i = 1,  , N do (score, align) = GET ALIGN SCORE 1(mt, ref i, mt table, ref tablei); if score > max score then max score = score; max align = align; max ref = i; end if end for final score += max score ;   = ; Add the words in align to mt table and ref tablemax ref ; end while return final score length penalty; end function Figure 6: Iterative Alignment Scheme references, and otherwise is set to the ratio of the two." ></td>
	<td class="line x" title="94:155	Figure 8 shows how the iterative alignment scheme works with an evaluation set containing one MT output and two references." ></td>
	<td class="line x" title="95:155	The selected alignment in each round is shown, as well as the unavailable positions in MT output and references." ></td>
	<td class="line x" title="96:155	With the iterative scheme, every common word between the MT output and the reference set can make a contribution to the metric, and by such means SIA is able to make full use of the word-level information." ></td>
	<td class="line x" title="97:155	Furthermore, the order (alignment round) in which the words are aligned provides a way to weight them." ></td>
	<td class="line x" title="98:155	In BLEU, multiple references can be used simultaneously, but the common n-grams are treated equally." ></td>
	<td class="line o" title="99:155	4 Experiments Evaluation experiments were conducted to compare the performance of different metrics including BLEU, ROUGE, METEOR and SIA.3 The test data for the experiments are from the MT evaluation workshop at ACL05." ></td>
	<td class="line x" title="100:155	There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), all of which contain 919 English sentences." ></td>
	<td class="line x" title="101:155	These sentences are the translation of the same Chinese input generated by seven different MT systems." ></td>
	<td class="line x" title="102:155	The uency and adequacy of each sentence are manually ranked from 1 to 5." ></td>
	<td class="line o" title="103:155	For each MT output, there are two sets of human scores available, and 3METEOR and ROUGE can be downloaded at http://www.cs.cmu.edu/ alavie/METEOR and http://www.isi.edu/licensed-sw/see/rouge function GET ALIGN SCORE1(mt, ref, mttable, reftable) triangleright Compute the alignment score of the MT output mt with length M and the reference ref with length N, without considering the positions in mttable and reftable M = |mt|; N = |ref|; for i = 1; i  M; i = i +1 do for j = 1; j  N; j = j +1 do for k = 1; k  i; k = k +1 do for m = 1; m  j; m = m +1 do scorei,j,k,m = maxscorei1,j,k,m, scorei,j1,k,m}; end for end for if i is not in mttable and j is not in reftable then scorei,j,i,j = max n=1,M;p=1,N {scorei,j,i,j, scorei1,j1,n,p + COMPUTE SCORE(mt, ref, i, j, n, p)}; end if end for end for return scoreM,N,M,NM and the corresponding alignment; end function Figure 7: Alignment Algorithm Based on Gaps Without Considering Aligned Positions m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London m: England with France discussed this crisis in London r2: England and France discussed the crisis in London r1: Britain and France consulted about this crisis in London with each other m: England with France discussed this crisis in London r1: Britain and France consulted about this crisis in London with each other r2: England and France discussed the crisis in London Figure 8: Alignment Example for SIA 543 we randomly choose one as the score used in the experiments." ></td>
	<td class="line x" title="104:155	The human overall scores are calculated as the arithmetic means of the human uency scores and adequacy scores." ></td>
	<td class="line x" title="105:155	There are four sets of human translations (E01, E02, E03, E04) serving as references for those MT outputs." ></td>
	<td class="line x" title="106:155	The MT outputs and reference sentences are transformed to lower case." ></td>
	<td class="line x" title="107:155	Our experiments are carried out as follows: automatic metrics are used to evaluate the MT outputs based on the four sets of references, and the Pearsons correlation coef cient of the automatic scores and the human scores is computed to see how well they agree." ></td>
	<td class="line x" title="108:155	4.1 N-gram vs. Loose Sequence One of the problems addressed in this paper is the different performance of n-gram based metrics and loose-sequence-based metrics in sentencelevel evaluation." ></td>
	<td class="line x" title="109:155	To see how they really differ in experiments, we choose BLEU and ROUGEW as the representative metrics for the two types, and used them to evaluate the 6433 sentences in the 7 MT outputs." ></td>
	<td class="line x" title="110:155	The Pearson correlation coef cients are then computed based on the 6433 samples." ></td>
	<td class="line x" title="111:155	The experimental results are shown in Table 1." ></td>
	<td class="line x" title="112:155	BLEU-n denotes the BLEU metric with the longest n-gram of length n. F denotes uency, A denotes adequacy, and O denotes overall." ></td>
	<td class="line x" title="113:155	We see that with the increase of n-gram length, BLEUs performance does not increase monotonically." ></td>
	<td class="line x" title="114:155	The best result in adequacy evaluation is achieved at 2-gram and the best result in uency is achieved at 4-gram." ></td>
	<td class="line x" title="115:155	Using n-grams longer than 2 doesnt buy much improvement for BLEU in uency evaluation, and does not compensate for the loss in adequacy evaluation." ></td>
	<td class="line x" title="116:155	This con rms Liu and Gildea (2005)s nding that in sentence level evaluation, long n-grams in BLEU are not bene cial." ></td>
	<td class="line x" title="117:155	The loose-sequence-based ROUGE-W does much better than BLEU in uency evaluation, but it does poorly in adequacy evaluation and doesnt achieve a signi cant improvement in overall evaluation." ></td>
	<td class="line x" title="118:155	We speculate that the reason is that ROUGE-W doesnt make full use of the available word-level information." ></td>
	<td class="line o" title="119:155	4.2 METEOR vs. SIA SIA is designed to take the advantage of loosesequence-based metrics without losing word-level information." ></td>
	<td class="line x" title="120:155	To see how well it works, we choose E09 as the development set and the sentences in the other 6 sets as the test data." ></td>
	<td class="line o" title="121:155	The decay facB-3 R 1 R 2 M S F 0.167 0.152 0.192 0.167 0.202 A 0.306 0.304 0.287 0.332 0.322 O 0.265 0.256 0.266 0.280 0.292 Table 2: Sentence level evaluation results of BLEU, ROUGE, METEOR and SIA tor in SIA is determined by optimizing the overall evaluation for E09, and then used with SIA to evaluate the other 5514 sentences based on the four sets of references." ></td>
	<td class="line x" title="122:155	The similarity of English words is computed by training IBM Model 4 in an English-French parallel corpus which contains seven hundred thousand sentence pairs." ></td>
	<td class="line x" title="123:155	For every English word, only the entries of the top 100 most similar English words are kept and the similarity ratios of them are then re-normalized." ></td>
	<td class="line x" title="124:155	The words outside the training corpus will be considered as only having itself as its similar word." ></td>
	<td class="line o" title="125:155	To compare the performance of SIA with BLEU, ROUGE and METEOR, the evaluation results based on the same testing data is given in Table 2." ></td>
	<td class="line o" title="126:155	B3 denotes BLEU-3; R 1 denotes the skipped bigram based ROUGE metric which considers all skip distances and uses PORTER-STEM; R 2 denotes ROUGE-W with PORTER-STEM; M denotes the METEOR metric using PORTER-STEM and WORD-NET synonym; S denotes SIA." ></td>
	<td class="line p" title="127:155	We see that METEOR, as the other metric sitting in the middle of n-gram based metrics and loose sequence metrics, achieves improvement over BLEU in both adequacy and uency evaluation." ></td>
	<td class="line n" title="128:155	Though METEOR gets the best results in adequacy evaluation, in uency evaluation, it is worse than the loose-sequence-based metric ROUGE-W-STEM." ></td>
	<td class="line x" title="129:155	SIA is the only one among the 5 metrics which does well in both uency and adequacy evaluation." ></td>
	<td class="line o" title="130:155	It achieves the best results in uency evaluation and comparable results to METEOR in adequacy evaluation, and the balanced performance leads to the best overall evaluation results in the experiment." ></td>
	<td class="line x" title="131:155	To estimate the significance of the correlations, bootstrap resampling (Koehn, 2004) is used to randomly select 5514 sentences with replacement out of the whole test set of 5514 sentences, and then the correlation coef cients are computed based on the selected sentence set." ></td>
	<td class="line x" title="132:155	The resampling is repeated 5000 times, and the 95% con dence intervals are shown in Tables 3, 4, and 5." ></td>
	<td class="line x" title="133:155	We can see that it is very dif 544 BLEU-1 BLEU-2 BLEU-3 BLEU-4 BLEU-5 BLEU-6 ROUGE-W F 0.147 0.162 0.166 0.168 0.165 0.164 0.191 A 0.288 0.296 0.291 0.285 0.279 0.274 0.268 O 0.243 0.256 0.255 0.251 0.247 0.244 0.254 Table 1: Sentence level evaluation results of BLEU and ROUGE-W low mean high B-3 (-16.6%) 0.138 0.165 0.192 (+16.4%) R 1 (-17.8%) 0.124 0.151 0.177 (+17.3%) R 2 (-14.3%) 0.164 0.191 0.218 (+14.2%) M (-15.8%) 0.139 0.166 0.191 (+15.5%) S (-13.3%) 0.174 0.201 0.227 (+13.3%) Table 3: 95% signi cance intervals for sentencelevel uency evaluation low mean high B-3 (-08.2%) 0.280 0.306 0.330 (+08.1%) R 1 (-08.5%) 0.278 0.304 0.329 (+08.4%) R 2 (-09.2%) 0.259 0.285 0.312 (+09.5%) M (-07.3%) 0.307 0.332 0.355 (+07.0%) S (-07.9%) 0.295 0.321 0.346 (+07.8%) Table 4: 95% signi cance intervals for sentencelevel adequacy evaluation cult for one metric to signi cantly outperform another metric in sentence-level evaluation." ></td>
	<td class="line x" title="134:155	The results show that the mean of the correlation factors converges right to the value we computed based on the whole testing set, and the con dence intervals correlate with the means." ></td>
	<td class="line x" title="135:155	While sentence-level evaluation is useful if we are interested in a con dence measure on MT outputs, syste-x level evaluation is more useful for comparing MT systems and guiding their development." ></td>
	<td class="line x" title="136:155	Thus we also present the evaluation results based on the 7 MT output sets in Table 6." ></td>
	<td class="line x" title="137:155	SIA uses the same decay factor as in the sentence-level evaluation." ></td>
	<td class="line o" title="138:155	Its system-level score is computed as the arithmetic mean of the sentence level scores, and low mean high B-3 (-09.8%) 0.238 0.264 0.290 (+09.9%) R 1 (-10.2%) 0.229 0.255 0.281 (+10.0%) R 2 (-10.0%) 0.238 0.265 0.293 (+10.4%) M (-09.0%) 0.254 0.279 0.304 (+08.8%) S (-08.7%) 0.265 0.291 0.316 (+08.8%) Table 5: 95% signi cance intervals for sentencelevel overall evaluation WLS WLS WLS WLS PROB INCS PROB INCS F 0.189 0.202 0.188 0.202 A 0.295 0.310 0.311 0.322 O 0.270 0.285 0.278 0.292 Table 7: Results of different components in SIA WLS WLS WLS WLS INCS INCS INCS INCS STEM WN STEM WN F 0.188 0.188 0.187 0.191 A 0.311 0.313 0.310 0.317 O 0.278 0.280 0.277 0.284 Table 8: Results of SIA working with Porter-Stem and WordNet so are ROUGE, METEOR and the human judgments." ></td>
	<td class="line x" title="139:155	We can see that SIA achieves the best performance in both uency and adequacy evaluation of the 7 systems." ></td>
	<td class="line x" title="140:155	Though the 7-sample based results are not reliable, we can get a sense of how well SIA works in the system-level evaluation." ></td>
	<td class="line x" title="141:155	4.3 Components in SIA To see how the three components in SIA contribute to the nal performance, we conduct experiments where one or two components are removed in SIA, shown in Table 7." ></td>
	<td class="line x" title="142:155	The three components are denoted as WLS (weighted loose sequence alignment), PROB (stochastic word matching), and INCS (iterative alignment scheme) respectively." ></td>
	<td class="line x" title="143:155	WLS without INCS does only one round of alignment and chooses the best alignment score as the nal score." ></td>
	<td class="line o" title="144:155	This scheme is similar to ROUGE-W and METEOR." ></td>
	<td class="line x" title="145:155	We can see that INCS, as expected, improves the adequacy evaluation without hurting the uency evaluation." ></td>
	<td class="line x" title="146:155	PROB improves both adequacy and uency evaluation performance." ></td>
	<td class="line x" title="147:155	The result that SIA works with PORTER-STEM and WordNet is also shown in Table 8." ></td>
	<td class="line o" title="148:155	When PORTER-STEM and WordNet are 545 B-6 R 1 R 2 M S F 0.514 0.466 0.458 0.378 0.532 A 0.876 0.900 0.906 0.875 0.928 O 0.794 0.790 0.792 0.741 0.835 Table 6: Results of BLEU, ROUGE, METEOR and SIA in system level evaluation both used, PORTER-STEM is used rst." ></td>
	<td class="line x" title="149:155	We can see that they are not as good as using the stochastic word matching." ></td>
	<td class="line o" title="150:155	Since INCS and PROB are independent of WLS, we believe they can also be used to improve other metrics such as ROUGE-W and METEOR." ></td>
	<td class="line x" title="151:155	5 Conclusion This paper describes a new metric SIA for MT evaluation, which achieves good performance by combining the advantages of n-gram-based metrics and loose-sequence-based metrics." ></td>
	<td class="line x" title="152:155	SIA uses stochastic word mapping to allow soft or partial matches between the MT hypotheses and the references." ></td>
	<td class="line x" title="153:155	This stochastic component is shown to be better than PORTER-STEM and WordNet in our experiments." ></td>
	<td class="line x" title="154:155	We also analyzed the effect of other components in SIA and speculate that they can also be used in other metrics to improve their performance." ></td>
	<td class="line x" title="155:155	Acknowledgments This work was supported by NSF ITR IIS-09325646 and NSF ITR IIS0428020." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3101
Morpho-Syntactic Information For Automatic Error Analysis Of Statistical Machine Translation Output
Popović, Maja;Gispert, AdriÃ e;Gupta, Deepa;Lambert, Patrik;Ney, Hermann;Marino, Jose B.;Federico, Marcello;Banches, Rafael E.;"></td>
	<td class="line x" title="1:92	Proceedings of the Workshop on Statistical Machine Translation, pages 16, New York City, June 2006." ></td>
	<td class="line x" title="2:92	c2006 Association for Computational Linguistics Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output Maja Popovicstar Hermann Neystar Adri`a de Gispert Jose B. Marino Deepa Gupta Marcello Federico Patrik Lambert Rafael Banchs star Lehrstuhl fur Informatik VI Computer Science Department, RWTH Aachen University, Aachen, Germany  TALP Research Center, Universitat Polit`ecnica de Catalunya (UPC), Barcelona, Spain  ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy {popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es {gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es Abstract Evaluation of machine translation output is an important but difficult task." ></td>
	<td class="line x" title="3:92	Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system." ></td>
	<td class="line x" title="4:92	However, these measures do not give any details about the nature of translation errors." ></td>
	<td class="line x" title="5:92	Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts." ></td>
	<td class="line x" title="6:92	On the other hand, human evaluation is a time consuming and expensive task." ></td>
	<td class="line x" title="7:92	In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements." ></td>
	<td class="line x" title="8:92	1 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT)." ></td>
	<td class="line x" title="9:92	Automatic evaluation is preferred because human evaluation is a time consuming and expensive task." ></td>
	<td class="line x" title="10:92	A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system." ></td>
	<td class="line x" title="11:92	The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al. , 2002) and the NIST score (Doddington, 2002)." ></td>
	<td class="line x" title="12:92	However, none of these measures give any details about the nature of translation errors." ></td>
	<td class="line x" title="13:92	A relationship between these error measures and the actual errors in the translation outputs is not easy to find." ></td>
	<td class="line x" title="14:92	Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts." ></td>
	<td class="line x" title="15:92	A framework for human error analysis and error classification has been proposed in (Vilar et al. , 2006), but like human evaluation, this is also a time consuming task." ></td>
	<td class="line x" title="16:92	The goal of this work is to present a framework for automatic error analysis of machine translation output based on morpho-syntactic information." ></td>
	<td class="line x" title="17:92	2 Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al. , 2002; Babych and Hartley, 2004; Matusov et al. , 2005)." ></td>
	<td class="line x" title="18:92	Semi-automatic evaluation measures have been also investigated, for example in (Nieen et al. , 2000)." ></td>
	<td class="line oc" title="19:92	An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in (Banerjee and Lavie, 2005)." ></td>
	<td class="line o" title="20:92	However, error analysis is still a rather unexplored area." ></td>
	<td class="line x" title="21:92	A framework for human error analysis and error classification has been proposed in (Vilar et al. , 2006) and a detailed analysis of the obtained results has been carried out." ></td>
	<td class="line x" title="22:92	Automatic methods for error analysis to our knowledge have not been studied yet." ></td>
	<td class="line x" title="23:92	Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system." ></td>
	<td class="line x" title="24:92	Various methods for treating morphological and syntactical differences between German and English are investigated in (Nieen and Ney, 2000; Nieen and Ney, 2001a; Nieen and Ney, 2001b)." ></td>
	<td class="line x" title="25:92	Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovic et al. , 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005)." ></td>
	<td class="line x" title="26:92	Inflectional morphology of Spanish verbs is dealt with in (Popovic and Ney, 2004; de Gispert et al. , 2005)." ></td>
	<td class="line x" title="27:92	To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far." ></td>
	<td class="line x" title="28:92	3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the translation errors." ></td>
	<td class="line x" title="29:92	We investigate two types of potential problems for the translation with the Spanish-English language pair:  syntactic differences between the two languages considering nouns and adjectives  inflections in the Spanish language considering mainly verbs, adjectives and nouns As any other automatic evaluation measures, these novel measures will be far from perfect." ></td>
	<td class="line x" title="30:92	Possible POS-tagging errors may introduce additional noise." ></td>
	<td class="line x" title="31:92	However, we expect this noise to be sufficiently small and the new measures to be able to give sufficiently clear ideas about particular errors." ></td>
	<td class="line x" title="32:92	3.1 Syntactic differences Adjectives in the Spanish language are usually placed after the corresponding noun, whereas in English is the other way round." ></td>
	<td class="line x" title="33:92	Although in most cases the phrase based translation system is able to handle these local permutations correctly, some errors are still present, especially for unseen or rarely seen noun-adjective groups." ></td>
	<td class="line x" title="34:92	In order to investigate this type of errors, we extract the nouns and adjectives from both the reference translations and the system output and then calculate WER and PER." ></td>
	<td class="line x" title="35:92	If the difference between the obtained WER and PER is large, this indicates reordering errors: a number of nouns and adjectives is translated correctly but in the wrong order." ></td>
	<td class="line x" title="36:92	3.2 Spanish inflections Spanish has a rich inflectional morphology, especially for verbs." ></td>
	<td class="line x" title="37:92	Person and tense are expressed by the suffix so that many different full forms of one verb exist." ></td>
	<td class="line x" title="38:92	Spanish adjectives, in contrast to English, have four possible inflectional forms depending on gender and number." ></td>
	<td class="line x" title="39:92	Therefore the error rates for those word classes are expected to be higher for Spanish than for English." ></td>
	<td class="line x" title="40:92	Also, the error rates for the Spanish base forms are expected to be lower than for the full forms." ></td>
	<td class="line x" title="41:92	In order to investigate potential inflection errors, we compare the PER for verbs, adjectives and nouns for both languages." ></td>
	<td class="line x" title="42:92	For the Spanish language, we also investigate differences between full form PER and base form PER: the larger these differences, more inflection errors are present." ></td>
	<td class="line x" title="43:92	4 Experimental Settings 4.1 Task and Corpus The corpus analysed in this work is built in the framework of the TC-Star project." ></td>
	<td class="line x" title="44:92	It contains more than one million sentences and about 35 million running words of the Spanish and English European Parliament Plenary Sessions (EPPS)." ></td>
	<td class="line x" title="45:92	A description of the EPPS data can be found in (Vilar et al. , 2005)." ></td>
	<td class="line x" title="46:92	In order to analyse effects of data sparseness, we have randomly extracted a small subset referred to as 13k containing about thirteen thousand sentences and 370k running words (about 1% of the original 2 Training corpus: Spanish English full Sentences 1281427 Running Words 36578514 34918192 Vocabulary 153124 106496 Singletons [%] 35.2 36.2 13k Sentences 13360 Running Words 385198 366055 Vocabulary 22425 16326 Singletons [%] 47.6 43.7 Dev: Sentences 1008 Running Words 25778 26070 Distinct Words 3895 3173 OOVs (full) [%] 0.15 0.09 OOVs (13k) [%] 2.7 1.7 Test: Sentences 840 1094 Running Words 22774 26917 Distinct Words 4081 3958 OOVs (full) [%] 0.14 0.25 OOVs (13k) [%] 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus)." ></td>
	<td class="line x" title="47:92	The statistics of the corpora can be seen in Table 1." ></td>
	<td class="line x" title="48:92	4.2 Translation System The statistical machine translation system used in this work is based on a log-linear combination of seven different models." ></td>
	<td class="line x" title="49:92	The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used." ></td>
	<td class="line x" title="50:92	A more detailed description of the system can be found in (Vilar et al. , 2005; Zens et al. , 2005)." ></td>
	<td class="line x" title="51:92	4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus." ></td>
	<td class="line x" title="52:92	In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovic and Ney, 2006) is also analysed." ></td>
	<td class="line x" title="53:92	Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round." ></td>
	<td class="line x" title="54:92	Therefore, local reorderings of nouns and adSpanishEnglish WER PER BLEU full baseline 34.5 25.5 54.7 reorder 33.5 25.2 56.4 13k baseline 41.8 30.7 43.2 reorder 38.9 29.5 48.5 EnglishSpanish WER PER BLEU full baseline 39.7 30.6 47.8 reorder 39.6 30.5 48.3 13k baseline 49.6 37.4 36.2 reorder 48.1 36.5 37.7 Table 2: Translation Results [%] jective groups in the source language have been applied." ></td>
	<td class="line x" title="55:92	If the source language is Spanish, each noun is moved behind the corresponding adjective group." ></td>
	<td class="line x" title="56:92	If the source language is English, each adjective group is moved behind the corresponding noun." ></td>
	<td class="line x" title="57:92	An adverb followed by an adjective (e.g. more important) or two adjectives with a coordinate conjunction in between (e.g. economic and political) are treated as an adjective group." ></td>
	<td class="line x" title="58:92	Standard translation results are presented in Table 2." ></td>
	<td class="line x" title="59:92	5 Error Analysis 5.1 Syntactic errors As explained in Section 3.1, reordering errors due to syntactic differences between two languages have been measured by the relative difference between WER and PER calculated on nouns and adjectives." ></td>
	<td class="line x" title="60:92	Corresponding relative differences are calculated also for verbs as well as adjectives and nouns separately." ></td>
	<td class="line x" title="61:92	Table 3 presents the relative differences for the English and Spanish output." ></td>
	<td class="line x" title="62:92	It can be seen that the PER/WER difference for nouns and adjectives is relatively high for both language pairs (more than 20%), and for the English output is higher than for the Spanish one." ></td>
	<td class="line x" title="63:92	This corresponds to the fact that the Spanish language has a rather free word order: although the adjective usually is placed behind the noun, this is not always the case." ></td>
	<td class="line x" title="64:92	On the other hand, adjectives in English are always placed before the corresponding noun." ></td>
	<td class="line x" title="65:92	It can also be seen that the difference is higher for the reduced corpus for both outputs indicating that the local reordering problem 3 English output 1 PERWER full nouns+adjectives 24.7 +reordering 20.8 verbs 4.1 adjectives 10.2 nouns 20.1 13k nouns+adjectives 25.7 +reordering 20.1 verbs 4.6 adjectives 8.4 nouns 19.1 Spanish output 1 PERWER full nouns+adjectives 21.5 +reordering 20.3 verbs 3.3 adjectives 5.6 nouns 16.9 13k nouns+adjectives 22.9 +reordering 19.8 verbs 3.9 adjectives 5.4 nouns 19.3 Table 3: Relative difference between PER and WER [%] for different word classes is more important when only small amount of training data is available." ></td>
	<td class="line x" title="66:92	As mentioned in Section 3.1, the phrase based translation system is able to generate frequent noun-adjective groups in the correct word order, but unseen or rarely seen groups introduce difficulties." ></td>
	<td class="line x" title="67:92	Furthermore, the results show that the POS-based reordering of adjectives and nouns leads to a decrease of the PER/WER difference for both outputs and for both corpora." ></td>
	<td class="line x" title="68:92	Relative decrease of the PER/WER difference is larger for the small corpus than for the full corpus." ></td>
	<td class="line x" title="69:92	It can also be noted that the relative decrease for both corpora is larger for the English output than for the Spanish one due to free word order since the Spanish adjective group is not always placed behind the noun, some reorderings in English are not really needed." ></td>
	<td class="line x" title="70:92	For the verbs, PER/WER difference is less than 5% for both outputs and both training corpora, indicating that the word order of verbs is not an imEnglish output PER full verbs 44.8 adjectives 27.3 nouns 23.0 13k verbs 56.1 adjectives 38.1 nouns 31.7 Spanish output PER full verbs 61.4 adjectives 41.8 nouns 28.5 13k verbs 73.0 adjectives 50.9 nouns 37.0 Table 4: PER [%] for different word classes portant issue for the Spanish-English language pair." ></td>
	<td class="line x" title="71:92	PER/WER difference for adjectives and nouns is higher than for verbs, for the nouns being significantly higher than for adjectives." ></td>
	<td class="line x" title="72:92	The reason for this is probably the fact that word order differences involving only the nouns are also present, for example export control = control de exportacion." ></td>
	<td class="line x" title="73:92	5.2 Inflectional errors Table 4 presents the PER for different word classes for the English and Spanish output respectively." ></td>
	<td class="line x" title="74:92	It can be seen that all PERs are higher for the Spanish output than for the English one due to the rich inflectional morphology of the Spanish language." ></td>
	<td class="line x" title="75:92	It can be also seen that the Spanish verbs are especially problematic (as stated in (Vilar et al. , 2006)) reaching 60% of PER for the full corpus and more than 70% for the reduced corpus." ></td>
	<td class="line x" title="76:92	Spanish adjectives also have a significantly higher PER than the English ones, whereas for the nouns this difference is not so high." ></td>
	<td class="line x" title="77:92	Results of the further analysis of inflectional errors are presented in Table 5." ></td>
	<td class="line x" title="78:92	Relative difference between full form PER and base form PER is significantly lower for adjectives and nouns than for verbs, thus showing that the verb inflections are the main source of translation errors into the Spanish language." ></td>
	<td class="line x" title="79:92	Furthermore, it can be seen that for the small cor4 Spanish output 1 PERbPERf full verbs 26.9 adjectives 9.3 nouns 8.4 13k verbs 23.7 adjectives 15.1 nouns 6.5 Table 5: Relative difference between PER of base forms and PER of full forms [%] for the Spanish output pus base/full PER difference for verbs and nouns is basically the same as for the full corpus." ></td>
	<td class="line x" title="80:92	Since nouns in Spanish only have singular and plural form as in English, the number of unseen forms is not particularly enlarged by the reduction of the training corpus." ></td>
	<td class="line x" title="81:92	On the other hand, base/full PER difference of adjectives is significantly higher for the small corpus due to an increased number of unseen adjective full forms." ></td>
	<td class="line x" title="82:92	As for verbs, intuitively it might be expected that the number of inflectional errors for this word class also increases by reducing the training corpus, even more than for adjectives." ></td>
	<td class="line x" title="83:92	However, the base/full PER difference is not larger for the small corpus, but even smaller." ></td>
	<td class="line x" title="84:92	This is indicating that the problem of choosing the right inflection of a Spanish verb apparently is not related to the number of unseen full forms since the number of inflectional errors is very high even when the translation system is trained on a very large corpus." ></td>
	<td class="line x" title="85:92	6 Conclusion In this work, we presented a framework for automatic analysis of translation errors based on the use of morpho-syntactic information." ></td>
	<td class="line x" title="86:92	We carried out a detailed analysis which has shown that the results obtained by our method correspond to those obtained by human error analysis in (Vilar et al. , 2006)." ></td>
	<td class="line x" title="87:92	Additionally, it has been shown that the improvements of the baseline system can be adequately measured as well." ></td>
	<td class="line x" title="88:92	This work is just a first step towards the development of linguistically-informed evaluation measures which provide partial and more specific information of certain translation problems." ></td>
	<td class="line x" title="89:92	Such measures are very important to understand what are the weaknesses of a statistical machine translation system, and what are the best ways and methods for improvements." ></td>
	<td class="line x" title="90:92	For our future work, we plan to extend the proposed measures in order to carry out a more detailed error analysis, for example examinating different types of inflection errors for Spanish verbs." ></td>
	<td class="line x" title="91:92	We also plan to investigate other types of translation errors and other language pairs." ></td>
	<td class="line x" title="92:92	Acknowledgements This work was partly supported by the TC-STAR project by the European Community (FP6-506738) and partly by the Generalitat de Catalunya and the European Social Fund." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3112
Contextual Bitext-Derived Paraphrases In Automatic MT Evaluation
Owczarzak, Karolina;Groves, Declan;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:135	Proceedings of the Workshop on Statistical Machine Translation, pages 8693, New York City, June 2006." ></td>
	<td class="line x" title="2:135	c2006 Association for Computational Linguistics Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation Karolina Owczarzak Declan Groves Josef Van Genabith Andy Way National Centre for Language Technology School of Computing Dublin City University Dublin 9, Ireland {owczarzak,dgroves,josef,away}@computing.dcu.ie Abstract In this paper we present a novel method for deriving paraphrases during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software." ></td>
	<td class="line x" title="3:135	Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation." ></td>
	<td class="line x" title="4:135	The method produces lexical and lowlevel syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system." ></td>
	<td class="line x" title="5:135	1 Introduction Since their appearance, BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating the quality of machine translation." ></td>
	<td class="line x" title="6:135	They both score candidate translations on the basis of the number of n-grams it shares with one or more reference translations provided." ></td>
	<td class="line x" title="7:135	Such automatic measures are indispensable in the development of machine translation systems, because they allow the developers to conduct frequent, cost-effective, and fast evaluations of their evolving models." ></td>
	<td class="line x" title="8:135	These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them." ></td>
	<td class="line x" title="9:135	In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical choices and syntactic structure it contains, even though perfectly legitimate, are not present in at least one of the references." ></td>
	<td class="line x" title="10:135	Necessarily, this score would not reflect a much more favourable human judgment that such a translation would receive." ></td>
	<td class="line x" title="11:135	The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in the BLEUor NIST-based evaluation in the first place." ></td>
	<td class="line x" title="12:135	While (Zhang and Vogel, 2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation." ></td>
	<td class="line x" title="13:135	On the other hand, in practice even a number of references do not capture the whole potential variability of the translation." ></td>
	<td class="line x" title="14:135	Moreover, often it is the case that multiple references are not available or are too difficult and expensive to produce: when designing a statistical machine translation system, the need for large amounts of training data limits the researcher to collections of parallel corpora like Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, often exceeds the resources available." ></td>
	<td class="line x" title="15:135	Therefore, it would be desirable to find a way to automatically generate legitimate translation alternatives not present in the reference(s) already available." ></td>
	<td class="line x" title="16:135	86 In this paper, we present a novel method that automatically derives paraphrases using only the source and reference texts involved in for the evaluation of French-to-English Europarl translations produced by two MT systems: statistical phrase-based Pharaoh (Koehn, 2004) and rulebased Logomedia." ></td>
	<td class="line x" title="17:135	1 In using what is in fact a miniature bilingual corpus our approach differs from the mainstream paraphrase generation based on monolingual resources." ></td>
	<td class="line x" title="18:135	We show that paraphrases produced in this way are more relevant to the task of evaluating machine translation than the use of external lexical knowledge resources like thesauri or WordNet 2, in that our paraphrases contain both lexical equivalents and low-level syntactic variants, and in that, as a side-effect, evaluation bitextderived paraphrasing naturally yields domainspecific paraphrases." ></td>
	<td class="line x" title="19:135	The paraphrases generated from the evaluation bitext are added to the existing reference sentences, in effect creating multiple references and resulting in a higher score for the candidate translation." ></td>
	<td class="line x" title="20:135	Our hypothesis, confirmed by the experiments in this paper, is that the scores raised by additional references produced in this way will correlate better with human judgment than the original scores." ></td>
	<td class="line x" title="21:135	The remainder of this paper is organized as follows: Section 2 describes related work; Section 3 describes our method and presents examples of derived paraphrases; Section 4 presents the results of the comparison between the BLUE and NIST scores for a single-reference translation and the same translation using the paraphrases automatically generated from the bitext, as well as the correlations between the scores and human judgment; Section 5 discusses ongoing work; Section 6 concludes." ></td>
	<td class="line x" title="22:135	2 2.1 Related work Word and phrase alignment Several researchers noted that the word and phrase alignment used in training translation models in Statistical MT can be used for other purposes as well." ></td>
	<td class="line x" title="23:135	(Diab and Resnik, 2002) use second language alignments to tag word senses." ></td>
	<td class="line x" title="24:135	Working on an assumption that separate senses of a L1 word 2.2 1 http://www.lec.com/ 2 http://wordnet.princeton.edu/ can be distinguished by its different translations in L2, they also note that a set of possible L2 translations for a L1 word may contain many synonyms." ></td>
	<td class="line x" title="25:135	(Bannard and Callison-Burch, 2005), on the other hand, conduct an experiment to show that paraphrases derived from such alignments can be semantically correct in more than 70% of the cases." ></td>
	<td class="line x" title="26:135	Automatic MT evaluation The insensitivity of BLEU and NIST to perfectly legitimate variation has been raised, among others, in (Callison-Burch et al. , 2006), but the criticism is widespread." ></td>
	<td class="line x" title="27:135	Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al. , 2002), a problem also noted by (Och et al. , 2003) and (Russo-Lassner et al. , 2005)." ></td>
	<td class="line x" title="28:135	A side effect of this phenomenon is that BLEU is less reliable for smaller data sets, so the advantage it provides in the speed of evaluation is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test data set in order to obtain a reliable score for their system." ></td>
	<td class="line x" title="29:135	Recently a number of attempts to remedy these shortcomings have led to the development of other automatic machine translation metrics." ></td>
	<td class="line x" title="30:135	Some of them concentrate mainly on the word reordering aspect, like Maximum Matching String (Turian et al. , 2003) or Translation Error Rate (Snover et al. , 2005)." ></td>
	<td class="line oc" title="31:135	Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al. , 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al. , 2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching." ></td>
	<td class="line n" title="32:135	A closer examination of these metrics suggests that the accommodation of lexical equivalence is as difficult as the appropriate treatment of syntactic variation, in that it requires considerable external knowledge resources like WordNet, verb class databases, and extensive text preparation: stemming, tagging, etc. The advantage of our method is that it produces relevant paraphrases with nothing more than the evaluation bitext and a widely available word and phrase alignment software, and therefore can be used with any existing evaluation metric." ></td>
	<td class="line x" title="33:135	87 3 Contextual bitext-derived paraphrases The method presented in this paper rests on a combination of two simple ideas." ></td>
	<td class="line x" title="34:135	First, the components necessary for automatic MT evaluation like BLEU or NIST, a source text and a reference text, constitute a miniature parallel corpus, from which word and phrase alignments can be extracted automatically, much like during the training for a statistical machine translation system." ></td>
	<td class="line x" title="35:135	Second, target language words e i1, , e in aligned as the likely translations to a source language word f i are often synonyms or near-synonyms of each other." ></td>
	<td class="line x" title="36:135	This also holds for phrases: target language phrases ep i1, , ep in aligned with a source language phrase fp i are often paraphrases of each other." ></td>
	<td class="line x" title="37:135	For example, in our experiment, for the French word question the most probable automatically aligned English translations are question, matter, and issue, which in English are practically synonyms." ></td>
	<td class="line x" title="38:135	Section 3.2 presents more examples of such equivalent expressions." ></td>
	<td class="line x" title="39:135	3.1 3.2 Experimental design For our experiment, we used two test sets, each consisting of 2000 sentences, drawn randomly from the test section of the Europarl parallel corpus." ></td>
	<td class="line x" title="40:135	The source language was French and the target language was English." ></td>
	<td class="line x" title="41:135	One of the test sets was translated by Pharaoh trained on 156,000 French-English sentence pairs." ></td>
	<td class="line x" title="42:135	The other test set was translated by Logomedia, a commercially available rule-based MT system." ></td>
	<td class="line x" title="43:135	Each test set consisted therefore of three files: the French source file, the English translation file, and the English reference file." ></td>
	<td class="line x" title="44:135	Each translation was evaluated by the BLEU and NIST metrics first with the single reference, then with the multiple references for each sentence using the paraphrases automatically generated from the source-reference mini corpus." ></td>
	<td class="line x" title="45:135	A subset of a 100 sentences was randomly extracted from each test set and evaluated by two independent human judges with respect to accuracy and fluency; the human scores were then compared to the BLEU and NIST scores for the single-reference and the automatically generated multiple-reference files." ></td>
	<td class="line x" title="46:135	Word alignment and phrase extraction We used the GIZA++ word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source French file and the English reference file, and the refined word alignment strategy of (Och and Ney, 2003; Koehn et al. , 2003; Tiedemann, 2004) to obtain improved word and phrase alignments." ></td>
	<td class="line x" title="47:135	For each source word or phrase f i that is aligned with more than one target words or phrases, its possible translations e i1, , e in were placed in a list as equivalent expressions (i.e. synonyms, near-synonyms, or paraphrases of each other)." ></td>
	<td class="line x" title="48:135	A few examples are given in (1)." ></td>
	<td class="line x" title="49:135	(1) agreement accordance adopted implemented matter lot case funds money arms weapons area aspect question  issue  matter we would expect we certainly expect bear on are centred around Alignment divides target words and phrases into equivalence sets; each set corresponds to one source word/phrase that was originally aligned with the target elements." ></td>
	<td class="line x" title="50:135	For example, for the French word citoyens three English words were deemed to be the most appropriate translations: people, public, and citizens; therefore these three words constitute an equivalence set." ></td>
	<td class="line x" title="51:135	Another French word population was aligned with two English translations: population and people; so the word people appears in two equivalence set (this gives rise to the question of equivalence transitivity, which will be discussed in Section 3.3)." ></td>
	<td class="line x" title="52:135	From the 2000-sentence evaluation bitext we derived 769 equivalence sets, containing in total 1658 words or phrases." ></td>
	<td class="line x" title="53:135	Each set contained on average two or three elements." ></td>
	<td class="line x" title="54:135	In effect, we produced at least one equivalent expression for 1658 English words or phrases." ></td>
	<td class="line x" title="55:135	An advantage of our method is that the target paraphrases and words come ordered with re3 http://www.fjoch.com/GIZA++ 88 spect to their likelihood of being the translation of the source word or phrase  each of them is assigned a probability expressing this likelihood, so we are able to choose only the most likely translations, according to some experimentally established threshold." ></td>
	<td class="line x" title="56:135	The experiment reported here was conducted without such a threshold, since the word and phrase alignment was of a very high quality." ></td>
	<td class="line x" title="57:135	3.3 3.4 3.5 Domain-specific lexical and syntactic paraphrases It is important to notice here how the paraphrases produced are more appropriate to the task at hand than synonyms extracted from a generalpurpose thesaurus or WordNet." ></td>
	<td class="line x" title="58:135	First, our paraphrases are contextual they are restricted to only those relevant to the domain of the text, since they are derived from the text itself." ></td>
	<td class="line x" title="59:135	Given the context provided by our evaluation bitext, the word area in (1) turns out to be only synonymous with aspect, and not with land, territory, neighbourhood, division, or other synonyms a general-purpose thesaurus or WordNet would give for this entry." ></td>
	<td class="line x" title="60:135	This allows us to limit our multiple references only to those that are likely to be useful in the context provided by the source text." ></td>
	<td class="line x" title="61:135	Second, the phrase alignment captures something neither a thesaurus nor WordNet will be able to provide: a certain amount of syntactic variation of paraphrases." ></td>
	<td class="line x" title="62:135	Therefore, we know that a string such as we would expect in (1), with the sequence noun-aux-verb, might be paraphrased by we certainly expect, a sequence of noun-adv-verb." ></td>
	<td class="line x" title="63:135	Open and closed class items One important conclusion we draw from analysing the synonyms obtained through word alignment is that equivalence is limited mainly to words that belong to open word classes, i.e. nouns, verbs, adjectives, adverbs, but is unlikely to extend to closed word classes like prepositions or pronouns." ></td>
	<td class="line x" title="64:135	For instance, while the French preposition  can be translated in English as to, in, or at, depending on the context, it is not the case that these three prepositions are synonymous in English." ></td>
	<td class="line x" title="65:135	The division is not that clear-cut, however: within the class of pronouns, he, she, and you are definitely not synonymous, but the demonstrative pronouns this and that might be considered equivalent for some purposes." ></td>
	<td class="line x" title="66:135	Therefore, in our experiment we exclude prepositions and in future work we plan to examine the word alignments more closely to decide whether to exclude any other words." ></td>
	<td class="line x" title="67:135	Creating multiple references After the list of synonyms and paraphrases is extracted from the evaluation bitext, for each reference sentence a string search replaces every eligible word or phrase with its equivalent(s) from the paraphrase list, one at a time, and the resulting string is added to the array of references." ></td>
	<td class="line x" title="68:135	The original string is added to the array as well." ></td>
	<td class="line x" title="69:135	This process results in a different number of reference sentences for every test sentence, depending on whether there was anything to replace in the reference and how many paraphrases we have available for the original substring." ></td>
	<td class="line x" title="70:135	One example of this process is shown in (2)." ></td>
	<td class="line x" title="71:135	(2) Original reference: i admire the answer mrs parly gave this morning but we have turned a blind eye to that Paraphrase 1: i admire the reply mrs parly gave this morning but we have turned a blind eye to that Paraphrase 2: i admire the answer mrs parly gave this morning however we have turned a blind eye to that Paraphrase 3: i admire the answer mrs parly gave this morning but we have turned a blind eye to it Transitivity As mentioned before, an interesting question that arises here is the potential transitivity of our automatically derived synonyms/paraphrases." ></td>
	<td class="line x" title="72:135	It could be argued that if the word people is equivalent to public according to one set from our list, and to the word population according to another set, then public can be thought of as equivalent to population." ></td>
	<td class="line x" title="73:135	In this case, the equivalence is not controversial." ></td>
	<td class="line x" title="74:135	However, consider the following relation: if sure in one of the equivalence sets is synonymous to certain, and certain in a different 89 set is listed as equivalent to some, then treating sure and some as synonyms is a mistake." ></td>
	<td class="line x" title="75:135	In our experiment we do not allow synonym transitivity; we only use the paraphrases from equivalence sets containing the word/phrase we want to replace." ></td>
	<td class="line x" title="76:135	Multiple simultaneous substitution Note that at the moment the references we are producing do not contain multiple simultaneous substitutions of equivalent expressions; for example, in (2) we currently do not produce the following versions: (3) Paraphrase 4: i admire the reply mrs parly gave this morning however we have turned a blind eye to that Paraphrase 5: i admire the answer mrs parly gave this morning however we have turned a blind eye to it Paraphrase 6: i admire the reply mrs parly gave this morning but we have turned a blind eye to it This can potentially prevent higher n-grams being successfully matched if two or more equivalent expressions find themselves within the range of ngrams being tested by BLEU and NIST." ></td>
	<td class="line x" title="77:135	To avoid combinatorial problems, implementing multiple simultaneous substitutions could be done using a lattice, much like in (Pang et al. , 2003)." ></td>
	<td class="line x" title="78:135	4 Results As expected, the use of multiple references produced by our method raises both the BLEU and NIST scores for translations produced by Pharaoh (test set PH) and Logomedia (test set LM)." ></td>
	<td class="line x" title="79:135	The results are presented in Table 1." ></td>
	<td class="line x" title="80:135	BLEU NIST PH single ref 0.2131 6.1625 PH multi ref 0.2407 7.0068 LM single ref 0.1782 5.5406 LM multi ref 0.2043 6.3834 Table 1." ></td>
	<td class="line x" title="81:135	Comparison of single-reference and multireference scores for test set PH and test set LM The hypothesis that the multiple-reference scores reflect better human judgment is also confirmed." ></td>
	<td class="line x" title="82:135	For 100-sentence subsets (Subset PH and Subset LM) randomly extracted from our test sets PH and LM, we calculated Pearsons correlation between the average accuracy and fluency scores that the translations in this subset received from two human judges (for each subset) and the singlereference and multiple-reference sentence-level BLEU and NIST scores." ></td>
	<td class="line x" title="83:135	There are two issues that need to be noted at this point." ></td>
	<td class="line x" title="84:135	First, BLEU scored many of the sentences as zero, artificially leveling many of the weaker translations." ></td>
	<td class="line x" title="85:135	4 This explains the low, although still statistically significant (p value < 0.01 5 ) correlation with BLEU for both single and multiple reference translations." ></td>
	<td class="line x" title="86:135	Using a version of BLEU with add-one smoothing we obtain considerably higher correlations." ></td>
	<td class="line x" title="87:135	Table 2 shows Pearsons correlation coefficient for BLEU, BLEU with add-one smoothing, NIST, and human judgments for Subsets PH." ></td>
	<td class="line x" title="88:135	Multiple paraphrase references produced by our method consistently lead to a higher correlation with human judgment for every metric." ></td>
	<td class="line x" title="89:135	6 Subset PH Metric single ref multi ref H & BLEU 0.297 0.307 H & BLEU smoothed 0.396 0.404 H & NIST 0.323 0.355 Table 2." ></td>
	<td class="line x" title="90:135	Pearsons correlation between human judgment and single-reference and multiplereference BLEU, smoothed BLEU, and NIST for subset PH (of test set PH) The second issue that requires explanation is the lower general scores Logomedias translation received on the full set of 2000 sentences, and the extremely low correlation of its automatic evaluation with human judgment, irrespective of the number of references." ></td>
	<td class="line x" title="91:135	It has been noticed (Calli4 BLEU uses a geometric average while calculating the sentence-level score and will score a sentence as 0 if it does not have at least one 4-gram." ></td>
	<td class="line x" title="92:135	5 A critical value for Pearsons correlation coefficient for the sample size between 90 and 100 is 0.267, with p < 0.01." ></td>
	<td class="line x" title="93:135	6 The significance of the rise in scores was confirmed in a resampling/bootstrapping test, with p < 0.0001." ></td>
	<td class="line x" title="94:135	90 son-Burch et al. , 2006) that BLEU and NIST favour n-gram based MT models such as Pharaoh, so the translation produced by Logomedia scored lower on the automatic evaluation, even though the human judges rated Logomedia output higher than Pharaohs translation." ></td>
	<td class="line x" title="95:135	Both human judges consistently gave very high scores to most sentences in subset LM (Logomedia), and as a consequence there was not enough variation in the scores assigned by them to create a good correlation with the BLEU and NIST scores." ></td>
	<td class="line x" title="96:135	The average human scores for the subsets PH and LM and the coefficients of variation are presented in Table 3." ></td>
	<td class="line x" title="97:135	It is easy to see that Logomedias translation received a higher mean score (on a scale 0 to 5) from the human judges and with less variance than Pharaoh." ></td>
	<td class="line x" title="98:135	Mean score Variation Subset PH 3.815 19.1% Subset LM 4.005 16.25% Table 3." ></td>
	<td class="line x" title="99:135	Human judgment mean scores and coefficients of variation for Subset PH and Subset LM As a result of the consistently high human scores for Logomedia, none of the Pearsons correlations computed for Subset LM is high enough to be significant." ></td>
	<td class="line x" title="100:135	The values are lower than the critical value 0.164 corresponding to p < 0.10." ></td>
	<td class="line x" title="101:135	Subset LM Metric single ref multi ref H & BLEU 0.046* 0.067* H & BLEU smoothed 0.163* 0.151* H & NIST 0.078* 0.116* Table 4." ></td>
	<td class="line x" title="102:135	Pearsons correlation between human judgment and single-reference and multiplereference BLEU, smoothed BLEU, and NIST for subset LM (of test set LM)." ></td>
	<td class="line x" title="103:135	* denotes values with p > 0.10." ></td>
	<td class="line x" title="104:135	5 Current and future work We would like to experiment with the way in which the list of equivalent expressions is produced." ></td>
	<td class="line x" title="105:135	One possible development would be to derive the expressions from a very large training corpus used by a statistical machine translation system, following (Bannard and Callison-Burch, 2005), for instance, and use it as an external widerpurpose knowledge resource (rather than a current domain-tailored resource as in our experiment), which would be nevertheless improve on a thesaurus in that it would also include phrase equivalents with some syntactic variation." ></td>
	<td class="line x" title="106:135	According to (Bannard and Callison-Burch, 2005), who derived their paraphrases automatically from a corpus of over a million German-English Europarl sentences, the baseline syntactic and semantic accuracy of the best paraphrases (those with the highest probability) reaches 48.9% and 64.5%, respectively." ></td>
	<td class="line x" title="107:135	That is, by replacing a phrase with its one most likely paraphrase the sentence remained syntactically well-formed in 48.9% of the cases and retained its meaning in 65% of the cases." ></td>
	<td class="line x" title="108:135	In a similar experiment we generated paraphrases from a French-English Europarl corpus of 700,000 sentences." ></td>
	<td class="line x" title="109:135	The data contained a considerably higher level of noise than our previous experiment on the 2000-sentence test set, even though we excluded any non-word entities from the results." ></td>
	<td class="line x" title="110:135	Like (Bannard and Callison-Burch, 2005), we used the product of probabilities p(f i |e i1 ) and p(e i2 |f i ) to determine the best paraphrase for a given English word e i1 . We then compared the accuracy across four samples of data." ></td>
	<td class="line x" title="111:135	Each sample contained 50 randomly drawn words/phrases and their paraphrases." ></td>
	<td class="line x" title="112:135	For the first two samples, the paraphrases were derived from the initial 2000sentence corpus; for the second two, the paraphrases were derived from the 700,000-sentence corpus." ></td>
	<td class="line x" title="113:135	For each corpus, one of the two samples contained only one best paraphrase for each entry, while the other listed all possible paraphrases." ></td>
	<td class="line x" title="114:135	We then evaluated the quality of each paraphrase with respect to its syntactic and semantic accuracy." ></td>
	<td class="line x" title="115:135	In terms of syntax, we considered the paraphrase accurate either if it had the same category as the original word/phrase; in terms of semantics, we relied on human judgment of similarity." ></td>
	<td class="line x" title="116:135	Tables 5 and 6 summarize the syntactic and semantic accuracy levels in the samples." ></td>
	<td class="line x" title="117:135	Paraphrases Derived from Best All 2000-sent." ></td>
	<td class="line x" title="118:135	corpus 59% 60% 700,000-sent." ></td>
	<td class="line x" title="119:135	corpus 70% 48% Table 5." ></td>
	<td class="line x" title="120:135	Syntactic accuracy of paraphrases 91 Paraphrases Derived from Best All 2000-sent." ></td>
	<td class="line x" title="121:135	corpus 83% 74% 700,000-sent." ></td>
	<td class="line x" title="122:135	corpus 76% 68% Table 6." ></td>
	<td class="line x" title="123:135	Semantic accuracy of paraphrases Although it has to be kept in mind that these percentages were taken from relatively small samples, an interesting pattern emerges from comparing the results." ></td>
	<td class="line x" title="124:135	It seems that the average syntactic accuracy of all paraphrases decreases with increased corpus size, but the syntactic accuracy of the one best paraphrase improves." ></td>
	<td class="line x" title="125:135	This reflects the idea behind word alignment: the bigger the corpus, the more potential alignments there are for a given word, but at the same time the better their order in terms of probability and the likelihood to obtain the correct translation." ></td>
	<td class="line x" title="126:135	Interestingly, the same pattern is not repeated for semantic accuracy, but again, these samples are quite small." ></td>
	<td class="line x" title="127:135	In order to address this issue, we plan to repeat the experiment with more data." ></td>
	<td class="line x" title="128:135	Additionally, it should be noted that certain expressions, although not completely correct syntactically, could be retained in the paraphrase lists for the purposes of machine translation evaluation." ></td>
	<td class="line x" title="129:135	Consider the case where our equivalence set looks like this: (4) abandon  abandoning  abandoned The words in (4) are all inflected forms of the verb abandon, and although they would produce rather ungrammatical paraphrases, those ungrammatical paraphrases still allow us to score our translation higher in terms of BLEU or NIST if it contains one of the forms of abandon than when it contains some unrelated word like piano instead." ></td>
	<td class="line x" title="130:135	This is exactly what other scoring metrics mentioned in Section 2 attempt to obtain with the use of stemming or prefix matching." ></td>
	<td class="line x" title="131:135	6 Conclusions In this paper we present a novel combination of existing ideas from statistical machine translation and paraphrase generation that leads to the creation of multiple references for automatic MT evaluation, using only the source and reference files that are required for the evaluation." ></td>
	<td class="line x" title="132:135	The method uses simple word and phrase alignment software to find possible synonyms and paraphrases for words and phrases of the target text, and uses them to produce multiple reference sentences for each test sentence, raising the BLEU and NIST evaluation scores and reflecting human judgment better." ></td>
	<td class="line x" title="133:135	The advantage of this method over other ways to generate paraphrases is that (1) unlike other methods, it does not require extensive parallel monolingual paraphrase corpora, but it extracts equivalent expressions from the miniature bilingual corpus of the source and reference evaluation files; (2) unlike other ways to accommodate synonymy in automatic evaluation, it does not require external lexical knowledge sources like thesauri or WordNet; (3) it extracts only synonyms that are relevant to the domain in hand; and (4) the equivalent expressions it produces include a certain amount of syntactic paraphrases." ></td>
	<td class="line x" title="134:135	The method is general and it can be used with any automatic evaluation metric that supports multiple references." ></td>
	<td class="line x" title="135:135	In our future work, we plan to apply it to newly developed evaluation metrics like CDER and TER that aim to allow for syntactic variation between the candidate and the reference, therefore bringing together solutions for the two shortcomings of automatic evaluation systems: insensitivity to allowable lexical differences and syntactic variation." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W06-3126
The LDV-COMBO System For SMT
Giménez, Jesús;Màrquez, Lluís;"></td>
	<td class="line x" title="1:85	Proceedings of the Workshop on Statistical Machine Translation, pages 166169, New York City, June 2006." ></td>
	<td class="line x" title="2:85	c2006 Association for Computational Linguistics The LDV-COMBO system for SMT Jesus Gimenez and Llus M`arquez TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya Jordi Girona Salgado 13, E-08034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract We describe the LDV-COMBO system presented at the Shared Task." ></td>
	<td class="line x" title="3:85	Our approach explores the possibility of working with alignments at different levels of abstraction using different degrees of linguistic analysis from the lexical to the shallow syntactic level." ></td>
	<td class="line x" title="4:85	Translation models are built on top of combinations of these alignments." ></td>
	<td class="line x" title="5:85	We present results for the Spanish-to-English and English-toSpanish tasks." ></td>
	<td class="line x" title="6:85	We show that liniguistic information may be helpful, specially when the target language has a rich morphology." ></td>
	<td class="line x" title="7:85	1 Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments." ></td>
	<td class="line x" title="8:85	In the last years, many efforts have been devoted to this matter (Yamada and Knight, 2001; Gildea, 2003)." ></td>
	<td class="line x" title="9:85	Following our previous work (Gimenez and M`arquez, 2005), we use shallow syntactic information to generate more precise alignments." ></td>
	<td class="line x" title="10:85	Far from full syntactic complexity, we suggest going back to the simpler alignment methods first described by IBM (1993)." ></td>
	<td class="line x" title="11:85	Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks)." ></td>
	<td class="line x" title="12:85	Apart from redefining the scope of the alignment unit, we may use different linguistic data views." ></td>
	<td class="line x" title="13:85	We enrich tokens with features further than lexical such as part-of-speech (PoS), lemma, and chunk IOB label." ></td>
	<td class="line x" title="14:85	For instance, suppose the case illustrated in Figure 1 where the lexical item plays is seen acting as a verb and as a noun." ></td>
	<td class="line x" title="15:85	Considering these two words, with the same lexical realization, as a single token adds noise to the word alignment process." ></td>
	<td class="line x" title="16:85	Representing this information, by means of linguistic data views, as playsVBZ and playsNNS would allow us to distinguish between the two cases." ></td>
	<td class="line x" title="17:85	Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses." ></td>
	<td class="line x" title="18:85	Therefore, it would be possible to distinguish for instance between two realizations of plays with different meanings: hePRP playsVBG guitarNN and hePRP playsVBG footballNN." ></td>
	<td class="line x" title="19:85	Of course, there is a natural trade-off between the use of linguistic data views and data sparsity." ></td>
	<td class="line x" title="20:85	Fortunately, we hava data enough so that statistical parameter estimation remains reliable." ></td>
	<td class="line x" title="21:85	The approach which is closest to ours is that by Schafer and Yarowsky (2003) who suggested a combination of models based on shallow syntactic analysis (part-of-speech tagging and phrase chunking)." ></td>
	<td class="line x" title="22:85	They followed a backoff strategy in the application of their models." ></td>
	<td class="line x" title="23:85	Decoding was based on Finite State Automata." ></td>
	<td class="line x" title="24:85	Although no significant improvement in MT quality was reported, results were promising taking into account the short time spent in the development of the linguistic tools utilized." ></td>
	<td class="line x" title="25:85	Our system is further described in Section 2." ></td>
	<td class="line x" title="26:85	Results are reported in Section 3." ></td>
	<td class="line x" title="27:85	Conclusions and further work are briefly outlined in Section 4." ></td>
	<td class="line x" title="28:85	166 Figure 1: A case of word alignment possibilities on top of lexical units (a) and linguistic data views (b)." ></td>
	<td class="line x" title="29:85	2 System Description The LDV-COMBO system follows the SMT architecture suggested by the workshop organizers." ></td>
	<td class="line x" title="30:85	We use the Pharaoh beam-search decoder (Koehn, 2004)." ></td>
	<td class="line x" title="31:85	First, training data are linguistically annotated." ></td>
	<td class="line x" title="32:85	In order to achieve robustness the same tools have been used to linguistically annotate both languages." ></td>
	<td class="line x" title="33:85	The SVMTool1 has been used for PoS-tagging (Gimenez and M`arquez, 2004)." ></td>
	<td class="line x" title="34:85	The Freeling2 package (Carreras et al. , 2004) has been used for lemmatizing." ></td>
	<td class="line x" title="35:85	Finally, the Phreco software (Carreras et al. , 2005) has been used for shallow parsing." ></td>
	<td class="line x" title="36:85	In this paper we focus on data views at the word level." ></td>
	<td class="line x" title="37:85	6 different data views have been built: (W) word, (L) lemma, (WP) word and PoS, (WC) word and chunk IOB label, (WPC) word, PoS and chunk IOB label, (LC) lemma and chunk IOB label." ></td>
	<td class="line x" title="38:85	Then, running GIZA++ (Och and Ney, 2003), we obtain token alignments for each of the data views." ></td>
	<td class="line x" title="39:85	Combined phrase-based translation models are built on top of the Viterbi alignments output by GIZA++." ></td>
	<td class="line x" title="40:85	Phrase extraction is performed following the phraseextract algorithm depicted by Och (2002)." ></td>
	<td class="line x" title="41:85	We do not apply any heuristic refinement." ></td>
	<td class="line x" title="42:85	We work with phrases up to 5 tokens." ></td>
	<td class="line x" title="43:85	Phrase pairs appearing only once have been discarded." ></td>
	<td class="line x" title="44:85	Scoring is performed by relative frequency." ></td>
	<td class="line x" title="45:85	No smoothing is applied." ></td>
	<td class="line x" title="46:85	In this paper we focus on the global phrase extraction (GPHEX) method described by Gimenez 1The SVMTool may be freely downloaded at http://www.lsi.upc.es/nlp/SVMTool/." ></td>
	<td class="line x" title="47:85	2Freeling Suite of Language Analyzers may be downloaded at http://www.lsi.upc.es/nlp/freeling/ and M`arquez (2005)." ></td>
	<td class="line x" title="48:85	We build a single translation model from the union of alignments from the 6 data views described above." ></td>
	<td class="line x" title="49:85	This model must match the input format." ></td>
	<td class="line x" title="50:85	For instance, if the input is annotated with word and PoS (WP), so must be the translation model." ></td>
	<td class="line x" title="51:85	Therefore either the input must be enriched with linguistic annotation or translation models must be post-processed in order to remove the additional linguistic annotation." ></td>
	<td class="line x" title="52:85	We did not observe significant differences in either alternative." ></td>
	<td class="line x" title="53:85	Therefore, we simply adapted translations models to work under the assumption of unannotated inputs (W)." ></td>
	<td class="line x" title="54:85	3 Experimental Work 3.1 Setting We have used only the data sets and language model provided by the organization." ></td>
	<td class="line oc" title="55:85	For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n = 4) (Papineni et al. , 2001), NIST (n = 5) (Lin and Hovy, 2002), GTM F1-measure (e = 1,2) (Melamed et al. , 2003), 1-WER (Nieen et al. , 2000), 1-PER (Leusch et al. , 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="56:85	Optimization of the decoding parameters (tm, lm, w) is performed by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) over the BLEU metric." ></td>
	<td class="line x" title="57:85	3For Spanish-to-English we applied all available modules: exact + stemming + WordNet stemming + WordNet synonymy lookup." ></td>
	<td class="line x" title="58:85	However, for English-to-Spanish we were forced to use the exact module alone." ></td>
	<td class="line o" title="59:85	167 Spanish-to-English System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S* Baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643 LDV-COMBO 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671 English-to-Spanish System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S* Baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028 LDV-COMBO 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240 Table 1: MT results comparing the LDV-COMBO system to a baseline system, for the test set both on the Spanish-to-English and English-to-Spanish tasks." ></td>
	<td class="line x" title="60:85	English Reference: consider germany, where some leaders [] Spanish Reference: pensemos en alemania, donde algunos dirigentes [] English-to-Spanish Baseline estiman que alemania, donde algunos dirigentes [] LDV-COMBO pensemos en alemania, donde algunos dirigentes [] Table 2: A case of error analysis." ></td>
	<td class="line x" title="61:85	3.2 Results Table 1 presents MT results for the test set both for the Spanish-to-English and English-to-Spanish tasks." ></td>
	<td class="line x" title="62:85	The variant of the LDV-COMBO system described in Section 2 is compared to a baseline variant based only on lexical items." ></td>
	<td class="line x" title="63:85	In the case of Spanish-to-English performance varies from metric to metric." ></td>
	<td class="line x" title="64:85	Therefore, an open issue is which metric should be trusted." ></td>
	<td class="line x" title="65:85	In any case, the differences are minor." ></td>
	<td class="line x" title="66:85	However, in the case of English-to-Spanish all metrics but 1-WER agree to indicate that the LDV-COMBO system significantly outperforms the baseline." ></td>
	<td class="line x" title="67:85	We suspect this may be due to the richer morphology of Spanish." ></td>
	<td class="line x" title="68:85	In order to test this hypothesis we performed an error analysys at the sentence level based on the GTM F-measure." ></td>
	<td class="line x" title="69:85	We found many cases where the LDV-COMBO system outperforms the baseline system by choosing a more accurate translation." ></td>
	<td class="line x" title="70:85	For instance, in Table 2 we may see a fragment of the case of sentence 2176 in the test set." ></td>
	<td class="line x" title="71:85	A better translation for consider is provided, pensemos, which corresponds to the right verb and verbal form (instead of estiman)." ></td>
	<td class="line x" title="72:85	By inspecting translation models we confirmed the better adjustment of probabilities." ></td>
	<td class="line x" title="73:85	Interestingly, LDV-COMBO translation models are between 30% and 40% smaller than the models based on lexical items alone." ></td>
	<td class="line x" title="74:85	The reason is that we are working with the union of alignments from different data views, thus adding more constraints into the phrase extraction step." ></td>
	<td class="line x" title="75:85	Fewer phrase pairs are extracted, and as a consequence we are also effectively eliminating noise from translation models." ></td>
	<td class="line x" title="76:85	4 Conclusions and Further Work Many researchers remain sceptical about the usefulness of linguistic information in SMT, because, except in a couple of cases (Charniak et al. , 2003; Collins et al. , 2005), little success has been reported." ></td>
	<td class="line x" title="77:85	In this work we have shown that liniguistic information may be helpful, specially when the target language has a rich morphology (e.g. Spanish)." ></td>
	<td class="line x" title="78:85	Moreover, it has often been argued that linguistic information does not yield significant improvements in MT quality, because (i) linguistic processors introduce many errors and (ii) the BLEU score is not specially sensitive to the grammaticality of MT output." ></td>
	<td class="line x" title="79:85	We have minimized the impact of the first argument by using highly accurate tools for both languages." ></td>
	<td class="line x" title="80:85	In order to solve the second problem more sophisticated metrics are required." ></td>
	<td class="line x" title="81:85	Current MT evaluation metrics fail to capture many aspects of MT 168 quality that characterize human translations with respect to those produced by MT systems." ></td>
	<td class="line x" title="82:85	We are devoting most of our efforts to the deployment of a new MT evaluation framework which allows to combine several similarity metrics into a single measure of quality (Gimenez and Amigo, 2006)." ></td>
	<td class="line x" title="83:85	We also leave for further work the experimentation of new data views such as word senses and semantic roles, as well as their natural porting from the alignment step to phrase extraction and decoding." ></td>
	<td class="line x" title="84:85	Acknowledgements This research has been funded by the Spanish Ministry of Science and Technology (ALIADO TIC2002-04447-C02)." ></td>
	<td class="line x" title="85:85	Authors are thankful to Patrik Lambert for providing us with the implementation of the Simplex Method used for tuning." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1007
Improving Statistical Machine Translation Using Word Sense Disambiguation
Carpuat, Marine;Wu, Dekai;"></td>
	<td class="line x" title="1:218	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:218	6172, Prague, June 2007." ></td>
	<td class="line x" title="3:218	c2007 Association for Computational Linguistics Improving Statistical Machine Translation using Word Sense Disambiguation Marine CARPUAT Dekai WU marine@cs.ust.hk dekai@cs.ust.hk Human Language Technology Center HKUST Department of Computer Science and Engineering University of Science and Technology, Clear Water Bay, Hong Kong Abstract We show for the first time that incorporating the predictions of a word sense disambiguation system within a typical phrase-based statistical machine translation (SMT) model consistently improves translation quality across all three different IWSLT ChineseEnglish test sets, as well as producing statistically significant improvements on the larger NIST Chinese-English MT task and moreover never hurts performance on any test set, according not only to BLEU but to all eight most commonly used automatic evaluation metrics." ></td>
	<td class="line x" title="4:218	Recent work has challenged the assumption that word sense disambiguation (WSD) systems are useful for SMT." ></td>
	<td class="line x" title="5:218	Yet SMT translation quality still obviously suffers from inaccurate lexical choice." ></td>
	<td class="line x" title="6:218	In this paper, we address this problem by investigating a new strategy for integrating WSD into an SMT system, that performs fully phrasal multi-word disambiguation." ></td>
	<td class="line x" title="7:218	Instead of directly incorporating a Senseval-style WSD system, we redefine the WSD task to match the exact same phrasal translation disambiguation task faced by phrase-based SMT systems." ></td>
	<td class="line x" title="8:218	Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT, despite claims to the contrary." ></td>
	<td class="line x" title="9:218	This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No." ></td>
	<td class="line x" title="10:218	HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants 1 Introduction Common assumptions about the role and usefulness of word sense disambiguation (WSD) models in full-scale statistical machine translation (SMT) systems have recently been challenged." ></td>
	<td class="line x" title="11:218	On the one hand, in previous work (Carpuat and Wu, 2005b) we obtained disappointing results when using the predictions of a Senseval WSD system in conjunction with a standard word-based SMT system: we reported slightly lower BLEU scores despite trying to incorporate WSD using a number of apparently sensible methods." ></td>
	<td class="line x" title="12:218	These results cast doubtontheassumptionthatsophisticateddedicated WSD systems that were developed independently from any particular NLP application can easily be integrated into a SMT system so as to improve translation quality through stronger models of context and rich linguistic information." ></td>
	<td class="line x" title="13:218	Rather, it has been argued, SMT systems have managed to achieve significant improvements in translation quality without directly addressing translation disambiguation as a WSD task." ></td>
	<td class="line x" title="14:218	Instead, translation disambiguation decisions are made indirectly, typically using only word surface forms and very local contextual information, forgoing the much richer linguistic information that WSD systems typically take advantage of." ></td>
	<td class="line x" title="15:218	On the other hand, error analysis reveals that the performance of SMT systems still suffers from inaccurate lexical choice." ></td>
	<td class="line x" title="16:218	In subsequent empirical studies, wehaveshownthatSMTsystemsperformmuch worse than dedicated WSD models, both supervised RGC6083/99E, RGC6256/00E, and DAG03/04.EG09." ></td>
	<td class="line x" title="17:218	Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Defense Advanced Research Projects Agency." ></td>
	<td class="line x" title="18:218	61 and unsupervised, on a Senseval WSD task (Carpuat and Wu, 2005a), and therefore suggest that WSD should have a role to play in state-of-the-art SMT systems." ></td>
	<td class="line x" title="19:218	In addition to the Senseval shared tasks, which have provided standard sense inventories and datasets, WSDresearchhasalsoturnedincreasingly to designing specific models for a particular application." ></td>
	<td class="line x" title="20:218	For instance, Vickrey et al.(2005) and Specia (2006) proposed WSD systems designed for French to English, and Portuguese to English translation respectively, and present a more optimistic outlook for the use of WSD in MT, although these WSD systems have not yet been integrated nor evaluated in full-scale machine translation systems." ></td>
	<td class="line x" title="22:218	Taken together, these seemingly contradictory results suggest that improving SMT lexical choice accuracy remains a key challenge to improve current SMT quality, and that it is still unclear what is the most appropriate integration framework for the WSD models in SMT." ></td>
	<td class="line x" title="23:218	In this paper, we present first results with a new architecture that integrates a state-of-the-art WSD model into phrase-based SMT so as to perform multi-word phrasal lexical disambiguation, and show that this new WSD approach not only produces gains across all available Chinese-English IWSLT06 test sets for all eight commonly used automated MT evaluation metrics, but also produces statistically significant gains on the much larger NIST Chinese-English task." ></td>
	<td class="line x" title="24:218	The main difference between this approach and several of our earlier approaches as described in Carpuat and Wu (2005b) and subsequently Carpuat et al.(2006) lies in the fact that we focus on repurposing the WSD system for multi-word phrase-based SMT." ></td>
	<td class="line x" title="26:218	Rather than using a generic Senseval WSD model as we did in Carpuat and Wu (2005b), here both the WSD training and the WSD predictions are integrated into the phrase-based SMT framework." ></td>
	<td class="line x" title="27:218	Furthermore, rather than using a single word based WSD approach to augment a phrase-based SMT model as we did in Carpuat et al.(2006) to improve BLEU and NIST scores, here the WSD training and predictions operate on full multi-word phrasal units, resulting in significantly more reliable and consistent gains as evaluted by many other translation accuracy metrics as well." ></td>
	<td class="line x" title="29:218	Specifically:  Instead of using a Senseval system, we redefine the WSD task to be exactly the same as lexical choice task faced by the multi-word phrasal translation disambiguation task faced by the phrase-based SMT system." ></td>
	<td class="line x" title="30:218	 Instead of using predefined senses drawn from manually constructed sense inventories such as HowNet (Dong, 1998), our WSD for SMT system directly disambiguates between all phrasal translation candidates seen during SMT training." ></td>
	<td class="line x" title="31:218	 Instead of learning from manually annotated training data, our WSD system is trained on the same corpora as the SMT system." ></td>
	<td class="line x" title="32:218	However, despite these adaptations to the SMT task, the core sense disambiguation task remains pure WSD:  The rich context features are typical of WSD and almost never used in SMT." ></td>
	<td class="line x" title="33:218	 The dynamic integration of context-sensitive translation probabilities is not typical of SMT." ></td>
	<td class="line x" title="34:218	 Although it is embedded in a real SMT system, the WSD task is exactly the same as in recent and coming Senseval Multilingual Lexical Sample tasks (e.g. , Chklovski et al.(2004)), where sense inventories represent the semantic distinctions made by another language." ></td>
	<td class="line x" title="36:218	We begin by presenting the WSD module and the SMT integration technique." ></td>
	<td class="line x" title="37:218	We then show that incorporating it into a standard phrase-based SMT baseline system consistently improves translation quality across all three different test sets from the Chinese-English IWSLT text translation evaluation, as well as on the larger NIST Chinese-English translation task." ></td>
	<td class="line x" title="38:218	Depending on the metric, the individual gains are sometimes modest, but remarkably, incorporating WSD never hurts, and helps enough to always make it a worthwile additional component in an SMT system." ></td>
	<td class="line x" title="39:218	Finally, we analyze the reasons for the improvement." ></td>
	<td class="line x" title="40:218	62 2 Problems in context-sensitive lexical choice for SMT To the best of our knowledge, there has been no previous attempt at integrating a state-of-the-art WSD system for fully phrasal multi-word lexical choice into phrase-based SMT, with evaluation of the resulting system on a translation task." ></td>
	<td class="line x" title="41:218	While there are many evaluations of WSD quality, in particular the Senseval series of shared tasks (Kilgarriff and Rosenzweig (1999), Kilgarriff (2001), Mihalcea et al.(2004)), very little work has been done to address the actual integration of WSD in realistic SMT applications." ></td>
	<td class="line x" title="43:218	To fully integrate WSD into phrase-based SMT, it is necessary to perform lexical disambiguation on multi-word phrasal lexical units; in contrast, the model reported in Cabezas and Resnik (2005) can only perform lexical disambiguation on single words." ></td>
	<td class="line x" title="44:218	Like the model proposed in this paper, Cabezas and Resnik attempted to integrate phrasebased WSD models into decoding." ></td>
	<td class="line x" title="45:218	However, althoughtheyreportedthatincorporatingthesepredictions via the Pharaoh XML markup scheme yielded a small improvement in BLEU score over a Pharaoh baselineonasingleSpanish-Englishtranslationdata set, we have determined empirically that applying their single-word based model to several ChineseEnglish datasets does not yield systematic improvements on most MT evaluation metrics (Carpuat and Wu, 2007)." ></td>
	<td class="line x" title="46:218	The single-word model has the disadvantage of forcing the decoder to choose between the baseline phrasal translation probabilities versus the WSD model predictions for single words." ></td>
	<td class="line x" title="47:218	In addition, the single-word model does not generalize to WSD for phrasal lexical choice, as overlapping spans cannot be specified with the XML markup scheme." ></td>
	<td class="line x" title="48:218	Providing WSD predictions for phrases would require committing to a phrase segmentation of the input sentence before decoding, which is likely to hurt translation quality." ></td>
	<td class="line x" title="49:218	It is also necessary to focus directly on translation accuracy rather than other measures such as alignment error rate, which may not actually lead to improved translation quality; in contrast, for example, Garcia-Varea et al.(2001) and Garcia-Varea et al.(2002) show improved alignment error rate with a maximum entropy based context-dependent lexical choice model, but not improved translation accuracy." ></td>
	<td class="line x" title="52:218	In contrast, our evaluation in this paper is conducted on the actual decoding task, rather than intermediate tasks such as word alignment." ></td>
	<td class="line x" title="53:218	Moreover, in the present work, all commonly available automated MT evaluation metrics are used, rather than only BLEU score, so as to maintain a more balanced perspective." ></td>
	<td class="line x" title="54:218	Another problem in the context-sensitive lexical choice in SMT models of Garcia Varea et al. is that their feature set is insufficiently rich to make much better predictions than the SMT model itself." ></td>
	<td class="line x" title="55:218	In contrast, our WSD-based lexical choice models are designed to directly model the lexical choice in the actual translation direction, and take full advantage of not residing strictly within the Bayesian sourcechannel model in order to benefit from the much richer Senseval-style feature set this facilitates." ></td>
	<td class="line x" title="56:218	Garcia Varea et al. found that the best results are obtained when the training of the context-dependent translation model is fully incorporated with the EM training of the SMT system." ></td>
	<td class="line x" title="57:218	As described below, the training of our new WSD model, though not incorporated within the EM training, is also far more closely tied to the SMT model than is the case with traditional standalone WSD models." ></td>
	<td class="line x" title="58:218	In contrast with Brown et al.(1991), our approach incorporates the predictions of state-of-theart WSD models that use rich contextual features for any phrase in the input vocabulary." ></td>
	<td class="line x" title="60:218	In Brown et al.s early study of WSD impact on SMT performance, the authors reported improved translation quality on a French to English task, by choosing an English translation for a French word based on the single contextual feature which is reliably discriminative." ></td>
	<td class="line x" title="61:218	However, this was a pilot study, which is limited to words with exactly two translation candidates, and it is not clear that the conclusions would generalize to more recent SMT architectures." ></td>
	<td class="line x" title="62:218	3 Problems in translation-oriented WSD The close relationship between WSD and SMT has been emphasized since the emergence of WSD as an independent task." ></td>
	<td class="line x" title="63:218	However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy, e.g., DaganandItai(1994), LiandLi(2002), 63 Diab (2004)." ></td>
	<td class="line x" title="64:218	In contrast, this paper focuses on the converse goal of using WSD models to improve actual translation quality." ></td>
	<td class="line x" title="65:218	Recently, several researchers have focused on designing WSD systems for the specific purpose of translation." ></td>
	<td class="line x" title="66:218	Vickrey et al.(2005) train a logistic regression WSD model on data extracted from automaticallywordalignedparallelcorpora, butevaluate on a blank filling task, which is essentially an evaluation of WSD accuracy." ></td>
	<td class="line x" title="68:218	Specia (2006) describes an inductive logic programming-based WSD system,whichwasspecificallydesignedforthepurpose of Portuguese to English translation, but this system was also only evaluated on WSD accuracy, and not integratedinafull-scalemachinetranslationsystem." ></td>
	<td class="line x" title="69:218	Ng et al.(2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models." ></td>
	<td class="line x" title="71:218	The purpose of the study was to lower the annotation cost for supervised WSD, as suggested earlier by Resnik and Yarowsky (1999)." ></td>
	<td class="line x" title="72:218	However this result is also encouraging for the integration of WSD in SMT, since it suggests that accurate WSD can be achieved using training data of the kind needed for SMT." ></td>
	<td class="line x" title="73:218	4 Building WSD models for phrase-based SMT 4.1 WSD models for every phrase in the input vocabulary Just like for the baseline phrase translation model, WSD models are defined for every phrase in the input vocabulary." ></td>
	<td class="line x" title="74:218	Lexical choice in SMT is naturally framed as a WSD problem, so the first step of integration consists of defining a WSD model for every phrase in the SMT input vocabulary." ></td>
	<td class="line x" title="75:218	This differs from traditional WSD tasks, where the WSD target is a single content word." ></td>
	<td class="line x" title="76:218	Senseval for instance has either lexical sample or all word tasks." ></td>
	<td class="line x" title="77:218	The target words for both categories of Senseval WSD tasks are typically only content words primarily nouns, verbs, and adjectiveswhile in the context of SMT, we need to translate entire sentences, and therefore have a WSD model not only for every word in the input sentences, regardless of their POS tag, but for every phrase, including tokens such as articles, prepositions and even punctuation." ></td>
	<td class="line x" title="78:218	Furtherempiricalstudieshavesuggestedthatincluding WSD predictions for those longer phrases is a key factor to help the decoder produce better translations (Carpuat and Wu, 2007)." ></td>
	<td class="line x" title="79:218	4.2 WSD uses the same sense definitions as the SMT system Instead of using pre-defined sense inventories, the WSD models disambiguate between the SMT translation candidates." ></td>
	<td class="line x" title="80:218	In order to closely integrate WSD predictions into the SMT system, we need to formulate WSD models so that they produce features that can directly be used in translation decisions taken by the SMT system." ></td>
	<td class="line x" title="81:218	It is therefore necessary for the WSDandSMTsystemstoconsiderexactlythesame translation candidates for a given word in the input language." ></td>
	<td class="line x" title="82:218	Assuming a standard phrase-based SMT system (e.g. , Koehn et al.(2003)), WSD senses are thus either words or phrases, as learned in the SMT phrasal translation lexicon." ></td>
	<td class="line x" title="84:218	Those sense candidates are very different from those typically used even in dedicated WSD tasks, even in the multilingual Senseval tasks." ></td>
	<td class="line x" title="85:218	Each candidate is a phrase that is not necessarily a syntactic noun or verb phrase as in manually compiled dictionaries." ></td>
	<td class="line x" title="86:218	It is quite possible that distinct senses in our WSD for SMT system could be considered synonyms in a traditional WSD framework, especially in monolingual WSD." ></td>
	<td class="line x" title="87:218	In addition to the consistency requirements for integration, this requirement is also motivated by empirical studies, which show that predefined translations derived from sense distinctions defined in monolingual ontologies do not match translation distinction made by human translators (Specia et al. , 2006)." ></td>
	<td class="line x" title="88:218	4.3 WSD uses the same training data as the SMT system WSD training does not require any other resources than SMT training, nor any manual sense annotation." ></td>
	<td class="line x" title="89:218	We employ supervised WSD systems, since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al.(2004))." ></td>
	<td class="line x" title="91:218	Training examples are annotated using the phrase alignments learned during SMT training." ></td>
	<td class="line x" title="92:218	Every in64 put language phrase is sense-tagged with its aligned output language phrase in the parallel corpus." ></td>
	<td class="line x" title="93:218	The phrase alignment method used to extract the WSD training data therefore depends on the one used by the SMT system." ></td>
	<td class="line x" title="94:218	This presents the advantage of training WSD and SMT models on exactly the same data, thus eliminating domain mismatches between Senseval data and parallel corpora." ></td>
	<td class="line x" title="95:218	But most importantly, this allows WSD training data to be generated entirely automatically, since the parallel corpus is automatically phrase-aligned in order to learn the SMT phrase bilexicon." ></td>
	<td class="line x" title="96:218	4.4 The WSD system The word sense disambiguation subsystem is modeled after the best performing WSD system in the Chinese lexical sample task at Senseval-3 (Carpuat et al. , 2004)." ></td>
	<td class="line x" title="97:218	The features employed are typical of WSD and are therefore far richer than those used in most SMT systems." ></td>
	<td class="line x" title="98:218	The feature set consists of positionsensitive, syntactic, and local collocational features, since these features yielded the best results when combined in a nave Bayes model on several Senseval-2 lexical sample tasks (Yarowsky and Florian, 2002)." ></td>
	<td class="line x" title="99:218	These features scale easily to the bigger vocabulary and sense candidates to be considered in a SMT task." ></td>
	<td class="line x" title="100:218	The Senseval system consists of an ensemble of four combined WSD models: The first model is a nave Bayes model, since Yarowsky and Florian (2002) found this model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data." ></td>
	<td class="line x" title="101:218	The second model is a maximum entropy model (Jaynes, 1978), since Klein and Manning (Klein and Manning, 2002) found that this model yielded higher accuracy than nave Bayes in a subsequent comparison of WSD performance." ></td>
	<td class="line x" title="102:218	The third model is a boosting model (Freund and Schapire, 1997), since boosting has consistently turned in very competitive scores on related tasks such as named entity classification." ></td>
	<td class="line x" title="103:218	We also use the Adaboost.MH algorithm." ></td>
	<td class="line x" title="104:218	The fourth model is a Kernel PCA-based model (Wu et al. , 2004)." ></td>
	<td class="line x" title="105:218	Kernel Principal Component Analysis or KPCA is a nonlinear kernel method for extractingnonlinearprincipalcomponentsfromvector sets where, conceptually, the n-dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F wherelinearPCAisperformed, yieldingatransform by which the input vectors can be mapped nonlinearly to a new set of vectors (Scholkopf et al. , 1998)." ></td>
	<td class="line x" title="106:218	WSD can be performed by a Nearest Neighbor Classifier in the high-dimensional KPCA feature space." ></td>
	<td class="line x" title="107:218	All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant." ></td>
	<td class="line x" title="108:218	Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent." ></td>
	<td class="line x" title="109:218	4.5 Integrating WSD predictions in phrase-based SMT architectures It is non-trivial to incorporate WSD into an existing phrase-based architecture such as Pharaoh (Koehn, 2004), since the decoder is not set up to easily accept multiple translation probabilities that are dynamically computed in context-sensitive fashion." ></td>
	<td class="line x" title="110:218	For every phrase in a given SMT input sentence, the WSD probabilities can be used as additional feature in a loglinear translation model, in combination with typical context-independent SMT bilexicon probabilities." ></td>
	<td class="line x" title="111:218	We overcome this obstacle by devising a calling architecture that reinitializes the decoder with dynamically generated lexicons on a per-sentence basis." ></td>
	<td class="line x" title="112:218	Unlike a n-best reranking approach, which is limited by the lexical choices made by the decoder using only the baseline context-independent translation probabilities, our method allows the system to make full use of WSD information for all competing phrases at all decoding stages." ></td>
	<td class="line x" title="113:218	5 Experimental setup The evaluation is conducted on two standard Chinese to English translation tasks." ></td>
	<td class="line x" title="114:218	We follow standard machine translation evaluation procedure using automatic evaluation metrics." ></td>
	<td class="line x" title="115:218	Since our goal is to evaluate translation quality, we use standard MT evaluation methodology and do not evaluate the accuracy of the WSD model independently." ></td>
	<td class="line o" title="116:218	65 Table 1: Evaluation results on the IWSLT06 dataset: integrating the WSD translation predictions improves BLEU, NIST, METEOR, WER, PER, CDER and TER across all 3 different available test sets." ></td>
	<td class="line x" title="117:218	Test Set Exper." ></td>
	<td class="line o" title="118:218	BLEU NIST METEOR METEOR (no syn) TER WER PER CDER Test 1 SMT 42.21 7.888 65.40 63.24 40.45 45.58 37.80 40.09 SMT+WSD 42.38 7.902 65.73 63.64 39.98 45.30 37.60 39.91 Test 2 SMT 41.49 8.167 66.25 63.85 40.95 46.42 37.52 40.35 SMT+WSD 41.97 8.244 66.35 63.86 40.63 46.14 37.25 40.10 Test 3 SMT 49.91 9.016 73.36 70.70 35.60 40.60 32.30 35.46 SMT+WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58 Table 2: Evaluation results on the NIST test set: integrating the WSD translation predictions improves BLEU, NIST, METEOR, WER, PER, CDER and TER Exper." ></td>
	<td class="line o" title="119:218	BLEU NIST METEOR METEOR (no syn) TER WER PER CDER SMT 20.41 7.155 60.21 56.15 76.76 88.26 61.71 70.32 SMT+WSD 20.92 7.468 60.30 56.79 71.34 83.87 57.29 67.38 5.1 Data set Preliminary experiments are conducted using training and evaluation data drawn from the multilingualBTECcorpus,whichcontainssentencesusedin conversations in the travel domain, and their translations in several languages." ></td>
	<td class="line x" title="120:218	A subset of this data was made available for the IWSLT06 evaluation campaign(Paul,2006); thetrainingsetconsistsof40000 sentence pairs, and each test set contains around 500 sentences." ></td>
	<td class="line x" title="121:218	We used only the pure text data, and not the speech transcriptions, so that speech-specific issueswouldnotinterferewithourprimarygoalofunderstanding the effect of integrating WSD in a fullscale phrase-based model." ></td>
	<td class="line x" title="122:218	A larger scale evaluation is conducted on the standard NIST Chinese-English test set (MT-04), which contains 1788 sentences drawn from newswire corpora, and therefore of a much wider domain than the IWSLT data set." ></td>
	<td class="line x" title="123:218	The training set consists of about 1 million sentence pairs in the news domain." ></td>
	<td class="line x" title="124:218	Basic preprocessing was applied to the corpus." ></td>
	<td class="line x" title="125:218	The English side was simply tokenized and casenormalized." ></td>
	<td class="line x" title="126:218	The Chinese side was word segmented using the LDC segmenter." ></td>
	<td class="line x" title="127:218	5.2 Baseline SMT system Since our focus is not on a specific SMT architecture, we use the off-the-shelf phrase-based decoder Pharaoh (Koehn, 2004) trained on the IWSLT training set." ></td>
	<td class="line x" title="128:218	Pharaoh implements a beam search decoder for phrase-based statistical models, and presents the advantages of being freely available and widely used." ></td>
	<td class="line x" title="129:218	The phrase bilexicon is derived from the intersection of bidirectional IBM Model 4 alignments, obtained with GIZA++ (Och and Ney, 2003), augmented to improve recall using the grow-diag-final heuristic." ></td>
	<td class="line x" title="130:218	The language model is trained on the English side of the corpus using the SRI language modeling toolkit (Stolcke, 2002)." ></td>
	<td class="line x" title="131:218	The loglinear model weights are learned using Chiangs implementation of the maximum BLEU training algorithm (Och, 2003), both for the baseline, and the WSD-augmented system." ></td>
	<td class="line x" title="132:218	Due to time constraints, this optimization was only conducted on the IWSLT task." ></td>
	<td class="line x" title="133:218	The weights used in the WSD-augmented NIST model are based on the best IWSLT model." ></td>
	<td class="line x" title="134:218	Given that the two tasks are quite different, we expect further improvements on the WSD-augmented system after running maximum BLEU optimization for the NIST task." ></td>
	<td class="line x" title="135:218	6 Results and discussion Using WSD predictions in SMT yields better translation quality on all test sets, as measured by all eight commonly used automatic evaluation metrics." ></td>
	<td class="line x" title="136:218	66 Table 3: Translation examples with and without WSD for SMT, drawn from IWSLT data sets." ></td>
	<td class="line x" title="137:218	Input lX-. Ref." ></td>
	<td class="line x" title="138:218	Please transfer to the Chuo train line." ></td>
	<td class="line x" title="139:218	SMT Please turn to the Central Line." ></td>
	<td class="line x" title="140:218	SMT+WSD Please transfer to Central Line." ></td>
	<td class="line x" title="141:218	Input fh(f p Ref." ></td>
	<td class="line x" title="142:218	Do I pay on the bus?" ></td>
	<td class="line x" title="143:218	SMT Please get on the bus?" ></td>
	<td class="line x" title="144:218	SMT+WSD I buy a ticket on the bus?" ></td>
	<td class="line x" title="145:218	Input  Ref." ></td>
	<td class="line x" title="146:218	Do I need a reservation?" ></td>
	<td class="line x" title="147:218	SMT I need a reservation?" ></td>
	<td class="line x" title="148:218	SMT+WSD Do I need a reservation?" ></td>
	<td class="line x" title="149:218	Input n h Ref." ></td>
	<td class="line x" title="150:218	I want to reconfirm this ticket." ></td>
	<td class="line x" title="151:218	SMT I would like to reconfirm a flight for this ticket." ></td>
	<td class="line x" title="152:218	SMT+WSD I would like to reconfirm my reservation for this ticket." ></td>
	<td class="line x" title="153:218	Input eL0 Ref." ></td>
	<td class="line x" title="154:218	Can I get there on foot?" ></td>
	<td class="line x" title="155:218	SMT Is there on foot?" ></td>
	<td class="line x" title="156:218	SMT+WSD Can I get there on foot?" ></td>
	<td class="line x" title="157:218	Input  * @ Ref." ></td>
	<td class="line x" title="158:218	I have another appointment, so please hurry." ></td>
	<td class="line x" title="159:218	SMT I have an appointment for a, so please hurry." ></td>
	<td class="line x" title="160:218	SMT+WSD I have another appointment, so please hurry." ></td>
	<td class="line x" title="161:218	Input  w`J0~" ></td>
	<td class="line x" title="162:218	G Ref." ></td>
	<td class="line x" title="163:218	Excuse me. Could you tell me the way to Broadway?" ></td>
	<td class="line x" title="164:218	SMT Could you tell me the way to Broadway?" ></td>
	<td class="line x" title="165:218	I am sorry." ></td>
	<td class="line x" title="166:218	SMT+WSD Excuse me, could you tell me the way to Broadway?" ></td>
	<td class="line x" title="167:218	Input  w *&7 Ref." ></td>
	<td class="line x" title="168:218	Excuse me, I want to open an account." ></td>
	<td class="line x" title="169:218	SMT Excuse me, I would like to have an account." ></td>
	<td class="line x" title="170:218	SMT+WSD Excuse me, I would like to open an account." ></td>
	<td class="line x" title="171:218	The results are shown in Table 1 for IWSLT and Table 2 for the NIST task." ></td>
	<td class="line x" title="172:218	Paired bootstrap resampling shows that the improvements on the NIST test set are statistically significant at the 95% level." ></td>
	<td class="line x" title="173:218	Remarkably, integrating WSD predictions helps all the very different metrics." ></td>
	<td class="line oc" title="174:218	In addition to the widely used BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Positionindependent word Error Rate (PER) (Tillmann et al. , 1997), CDER, which allows block reordering (Leusch et al. , 2006), and Translation Edit Rate (TER) (Snover et al. , 2006)." ></td>
	<td class="line o" title="175:218	Note that we report Meteor scores computed both with and without using WordNet synonyms to match translation candidates and references, showing that the improvement is not due to context-independent synonym matches at evaluation time." ></td>
	<td class="line x" title="176:218	Comparison of the 1-Best decoder output with and without the WSD feature shows that the sentences differ by one or more token respectively for 25.49%, 30.40% and 29.25% of IWSLT test sets 1, 67 Table 4: Translation examples with and without WSD for SMT, drawn from the NIST test set." ></td>
	<td class="line x" title="177:218	Input  UXh SMT Without any congressmen voted against him." ></td>
	<td class="line x" title="178:218	SMT+WSD No congressmen voted against him." ></td>
	<td class="line x" title="179:218	Input (fL?VTS" ></td>
	<td class="line x" title="180:218	/ SMT Russias policy in Chechnya and CIS neighbors attitude is even more worried that the United States." ></td>
	<td class="line x" title="181:218	SMT+WSD Russias policy in Chechnya and its attitude toward its CIS neighbors cause the United States still more anxiety." ></td>
	<td class="line x" title="182:218	Input Cb SMT As for the U.S. human rights conditions?" ></td>
	<td class="line x" title="183:218	SMT+WSD As for the human rights situation in the U.S.?" ></td>
	<td class="line x" title="184:218	Input /:HB,sAc SMT The purpose of my visit to Japan is pray for peace and prosperity." ></td>
	<td class="line x" title="185:218	SMT+WSD The purpose of my visit is to pray for peace and prosperity for Japan." ></td>
	<td class="line x" title="186:218	Input :2P; IfM@* % SMT InordertopreventterroristactivitiesLosAngeles,thepolicehavetakenunprecedented tight security measures." ></td>
	<td class="line x" title="187:218	SMT+WSD In order to prevent terrorist activities Los Angeles, the police to an unprecedented tight security measures." ></td>
	<td class="line x" title="188:218	2 and 3, and 95.74% of the NIST test set." ></td>
	<td class="line x" title="189:218	Tables 3 and 4 show examples of translations drawn from the IWSLT and NIST test sets respectively." ></td>
	<td class="line x" title="190:218	A more detailed analysis reveals WSD predictions give better rankings and are more discriminative than baseline translation probabilities, which helps the final translation in three different ways." ></td>
	<td class="line x" title="191:218	 The rich context features help rank the correct translation first with WSD while it is ranked lower according to baseline translation probability scores." ></td>
	<td class="line x" title="192:218	 Even when WSD andbaseline translation probabilities agree on the top translation candidate, the stronger WSD scores help override wrong language model predictions." ></td>
	<td class="line x" title="193:218	 The strong WSD scores for phrases help the decoder pick longer phrase translations, while using baseline translation probabilities often translate those phrases in smaller chunks that include a frequent (and incorrect) translation candidate." ></td>
	<td class="line x" title="194:218	For instance, the top 4 Chinese sentences in Table 4, are better translated by the WSD-augmented system because the WSD scores help the decoder to choose longer phrases." ></td>
	<td class="line x" title="195:218	In the first example, the phrase  U is correctly translated as a whole as No by the WSD-augmented system, while the baseline translates each word separately yielding an incorrect translation." ></td>
	<td class="line x" title="196:218	In the following three examples, the WSD system encourages the decoder to translate the long phrases / , C, and HB, sAc as single units, while the baseline introduces errors by breaking them down into shorter phrases." ></td>
	<td class="line x" title="197:218	The last sentence in the table shows an example where the WSD predictions do not help the baseline system." ></td>
	<td class="line x" title="198:218	The translation quality is actually much worse, since the verb  is incorrectly translated as to, despite the fact that the top candidate predicted by the WSD system alone is the much better translation has taken, but with a relatively low probability of 0.509." ></td>
	<td class="line x" title="199:218	7 Conclusion We have shown for the first time that integrating multi-word phrasal WSD models into phrase-based 68 SMT consistently helps on all commonly available automated translation quality evaluation metrics on all three different test sets from the Chinese-English IWSLT06 text translation task, and yields statistically significant gains on the larger NIST ChineseEnglish task." ></td>
	<td class="line x" title="200:218	It is important to note that the WSD models never hurt translation quality, and always yield individual gains of a level that makes their integration always worthwile." ></td>
	<td class="line x" title="201:218	We have proposed to consistently integrate WSD models both during training, where sense definitions andsense-annotateddataareautomaticallyextracted from the word-aligned parallel corpora from SMT training, and during testing, where the phrasal WSD probabilities are used by the SMT system just like all the other lexical choice features." ></td>
	<td class="line x" title="202:218	Context features are derived from state-of-the-art WSDmodels, andtheevaluationisconductedonthe actual translation task, rather than intermediate tasks such as word alignment." ></td>
	<td class="line x" title="203:218	It is to be emphasized that this approach does not merely consist of adding a source sentence feature in the log linear model for translation." ></td>
	<td class="line x" title="204:218	On the contrary, it remains a real WSD task, defined just as in the Senseval Multilingual Lexical Sample tasks (e.g. , Chklovski et al.(2004))." ></td>
	<td class="line x" title="206:218	Ourmodelmakesuse of typical WSD features that are almost never used in SMT systems, and requires a dynamically created translation lexicon on a per-sentence basis." ></td>
	<td class="line x" title="207:218	To our knowledge this constitues the first attempt at fully integrating state-of-the-art WSD with conventional phrase-based SMT." ></td>
	<td class="line x" title="208:218	Unlike previous approaches,theWSDtargetsarenotonlysinglewords, but multi-word phrases, just as in the SMT system." ></td>
	<td class="line x" title="209:218	This means that WSD senses are unusually predicted not only for a limited set of single words or very short phrases, but for all phrases of arbitrarily length that are in the SMT translation lexicon." ></td>
	<td class="line x" title="210:218	Thesinglewordapproach, aswereportedinCarpuat et al.(2006), improved BLEU and NIST scores for phrase-based SMT, but subsequent detailed empirical studies we have performed since then suggest that single word WSD approaches are less successful when evaluated under all other MT metrics (Carpuat and Wu, 2007)." ></td>
	<td class="line x" title="212:218	Thus, fully phrasal WSD predictions for longer phrases, as reported in this paper, are particularly important to improve translation quality." ></td>
	<td class="line x" title="213:218	Theresultsreportedinthispapercastnewlighton the WSD vs. SMT debate, suggesting that a close integration of WSD and SMT decisions should be incorporated in a SMT model that successfully uses WSD predictions." ></td>
	<td class="line x" title="214:218	Our objective here is to demonstrate that this technique works for the widest possible class of models, so we have chosen as the baseline the most widely used phrase-based SMT model." ></td>
	<td class="line x" title="215:218	Our positive results suggest that our experiments could be tried on other current statistical MT models, especially the growing family of treestructured SMT models employing stochastic transduction grammars of various sorts (Wu and Chiang, 2007)." ></td>
	<td class="line x" title="216:218	For instance, incorporating WSD predictions into an MT decoder based on inversion transduction grammars (Wu, 1997)such as the Bracketing ITG based models of Wu (1996), Zens et al.(2004), or CherryandLin(2007)wouldpresentanintriguing comparison with the present work." ></td>
	<td class="line x" title="218:218	It would also be interesting to assess whether a more grammatically structured statistical MT model that is less reliant on an n-gram language model, such as the syntactic ITG based grammatical channel translation model of (Wu and Wong, 1998), could make more effective use of WSD predictions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1055
A Systematic Comparison of Training Criteria for Statistical Machine Translation
Zens, Richard;Hasan, SaÅ¡a;Ney, Hermann;"></td>
	<td class="line x" title="1:198	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:198	524532, Prague, June 2007." ></td>
	<td class="line x" title="3:198	c2007 Association for Computational Linguistics A Systematic Comparison of Training Criteria for Statistical Machine Translation Richard Zens and Sasa Hasan and Hermann Ney Human Language Technology and Pattern Recognition Lehrstuhl fur Informatik 6  Computer Science Department RWTH Aachen University, D-52056 Aachen, Germany {zens,hasan,ney}@cs.rwth-aachen.de Abstract We address the problem of training the free parameters of a statistical machine translation system." ></td>
	<td class="line x" title="4:198	We show significant improvements over a state-of-the-art minimum error rate training baseline on a large ChineseEnglish translation task." ></td>
	<td class="line x" title="5:198	We present novel training criteria based on maximum likelihood estimation and expected loss computation." ></td>
	<td class="line x" title="6:198	Additionally, we compare the maximum a-posteriori decision rule and the minimum Bayes risk decision rule." ></td>
	<td class="line x" title="7:198	We show that, not only from a theoretical point of view but also in terms of translation quality, the minimum Bayes risk decision rule is preferable." ></td>
	<td class="line x" title="8:198	1 Introduction Once we specified the Bayes decision rule for statistical machine translation, we have to address three problems (Ney, 2001):  the search problem, i.e. how to find the best translation candidate among all possible target language sentences;  the modeling problem, i.e. how to structure the dependencies of source and target language sentences;  the training problem, i.e. how to estimate the free parameters of the models from the training data." ></td>
	<td class="line x" title="9:198	Here, the main focus is on the training problem." ></td>
	<td class="line x" title="10:198	We will compare a variety of training criteria for statistical machine translation." ></td>
	<td class="line x" title="11:198	In particular, we are considering criteria for the log-linear parameters or model scaling factors." ></td>
	<td class="line x" title="12:198	We will introduce new training criteria based on maximum likelihood estimation and expected loss computation." ></td>
	<td class="line x" title="13:198	We will show that some achieve significantly better results than the standard minimum error rate training of (Och, 2003)." ></td>
	<td class="line x" title="14:198	Additionally, we will compare two decision rules, the common maximum a-posteriori (MAP) decision rule and the minimum Bayes risk (MBR) decision rule (Kumar and Byrne, 2004)." ></td>
	<td class="line x" title="15:198	We will show that the minimum Bayes risk decision rule results in better translation quality than the maximum aposteriori decision rule for several training criteria." ></td>
	<td class="line x" title="16:198	The remaining part of this paper is structured as follows: first, we will describe related work in Sec." ></td>
	<td class="line x" title="17:198	2." ></td>
	<td class="line x" title="18:198	Then, we will briefly review the baseline system, Bayes decision rule for statistical machine translation and automatic evaluation metrics for machine translation in Sec." ></td>
	<td class="line x" title="19:198	3 and Sec." ></td>
	<td class="line x" title="20:198	4, respectively." ></td>
	<td class="line x" title="21:198	The novel training criteria are described in Sec." ></td>
	<td class="line x" title="22:198	5 and Sec." ></td>
	<td class="line x" title="23:198	6." ></td>
	<td class="line x" title="24:198	Experimental results are reported in Sec." ></td>
	<td class="line x" title="25:198	7 and conclusions are given in Sec." ></td>
	<td class="line x" title="26:198	8." ></td>
	<td class="line x" title="27:198	2 Related Work The most common modeling approach in statistical machine translation is to use a log-linear combination of several sub-models (Och and Ney, 2002)." ></td>
	<td class="line x" title="28:198	In (Och and Ney, 2002), the log-linear weights were tuned to maximize the mutual information criterion (MMI)." ></td>
	<td class="line x" title="29:198	The current state-of-the-art is to optimize these parameters with respect to the final evaluation criterion; this is the so-called minimum error rate training (Och, 2003)." ></td>
	<td class="line x" title="30:198	Minimum Bayes risk decoding for machine trans524 lation was introduced in (Kumar and Byrne, 2004)." ></td>
	<td class="line x" title="31:198	It was shown that MBR outperforms MAP decoding for different evaluation criteria." ></td>
	<td class="line x" title="32:198	Further experiments using MBR for Bleu were performed in (Venugopal et al. , 2005; Ehling et al. , 2007)." ></td>
	<td class="line x" title="33:198	Here, we will present additional evidence that MBR decoding is preferable over MAP decoding." ></td>
	<td class="line x" title="34:198	Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features." ></td>
	<td class="line x" title="35:198	Here, we focus on the comparison of different training criteria." ></td>
	<td class="line x" title="36:198	Shen et al.(2004) compared different algorithms for tuning the log-linear weights in a reranking framework and achieved results comparable to the standard minimum error rate training." ></td>
	<td class="line x" title="38:198	An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training." ></td>
	<td class="line x" title="39:198	The parameters are estimated iteratively using an annealing technique that minimizes the risk of an expected-BLEU approximation, which is similar to the one presented in this paper." ></td>
	<td class="line x" title="40:198	3 Baseline System In statistical machine translation, we are given a source language sentence fJ1 = f1fj fJ, which is to be translated into a target language sentence eI1 = e1 ei eI." ></td>
	<td class="line x" title="41:198	Statistical decision theory tells us that among all possible target language sentences, we should choose the sentence which minimizes the expected loss, also called Bayes risk: eI1 = argmin I,eI1 braceleftBigg summationdisplay Iprime,eprimeIprime1 Pr(eprimeIprime1 |fJ1 )  L(eI1,eprimeIprime1 ) bracerightBigg Here, L(eI1,eprimeIprime1 ) denotes the loss function under consideration." ></td>
	<td class="line x" title="42:198	It measures the loss (or errors) of a candidate translation eI1 assuming the correct translation is eprimeIprime1 . In the following, we will call this decision rule the MBR rule (Kumar and Byrne, 2004)." ></td>
	<td class="line x" title="43:198	This decision rule is optimal in the sense that any other decision rule will result (on average) in at least as many errors as the MBR rule." ></td>
	<td class="line x" title="44:198	Despite this, most SMTsystemsdonotusetheMBRdecisionrule." ></td>
	<td class="line x" title="45:198	The most common approach is to use the maximum aposteriori (MAP) decision rule." ></td>
	<td class="line x" title="46:198	Thus, we select the hypothesis which maximizes the posterior probability Pr(eI1|fJ1 ): eI1 = argmax I,eI1 braceleftBig Pr(eI1|fJ1 ) bracerightBig This is equivalent to the MBR decision rule under a 0-1 loss function: L01(eI1,eprimeIprime1 ) = braceleftbigg 0 if eI 1 = eprime Iprime 1 1 else Hence, the MAP decision rule is optimal for the sentence or string error rate." ></td>
	<td class="line x" title="47:198	It is not necessarily optimal for other evaluation metrics such as the Bleu score." ></td>
	<td class="line x" title="48:198	One reason for the popularity of the MAP decision rule might be that, compared to the MBR rule, its computation is simpler." ></td>
	<td class="line x" title="49:198	The posterior probability Pr(eI1|fJ1 ) is modeled directly using a log-linear combination of several models (Och and Ney, 2002): pM 1 (eI1|fJ1 ) = exp parenleftBigsummationtextM m=1 mhm(e I1,fJ1 ) parenrightBig summationtext Iprime,eprimeIprime1 exp parenleftBigsummationtextM m=1 mhm(eprime Iprime1,fJ 1 ) parenrightBig (1) This approach is a generalization of the sourcechannel approach (Brown et al. , 1990)." ></td>
	<td class="line x" title="50:198	It has the advantage that additional models h() can be easily integrated into the overall system." ></td>
	<td class="line x" title="51:198	The denominator represents a normalization factor that depends only on the source sentence fJ1 . Therefore, we can omit it in case of the MAP decision rule during the search process and obtain: eI1 = argmax I,eI1 braceleftBigg Msummationdisplay m=1 mhm(eI1,fJ1 ) bracerightBigg Notethatthedenominatoraffectstheresultsofthe MBR decision rule and, thus, cannot be omitted in that case." ></td>
	<td class="line x" title="52:198	We use a state-of-the-art phrase-based translation system similar to (Koehn, 2004; Mauser et al. , 2006) including the following models: an n-gram languagemodel, aphrasetranslationmodelandawordbased lexicon model." ></td>
	<td class="line x" title="53:198	The latter two models are used for both directions: p(f|e) andp(e|f)." ></td>
	<td class="line x" title="54:198	Additionally, we use a word penalty, phrase penalty and a distortion penalty." ></td>
	<td class="line x" title="55:198	525 In the following, we will discuss the so-called training problem (Ney, 2001): how do we train the free parameters M1 of the model?" ></td>
	<td class="line x" title="56:198	The current state-of-the-art is to use minimum error rate training (MERT) as described in (Och, 2003)." ></td>
	<td class="line x" title="57:198	The free parameters are tuned to directly optimize the evaluation criterion." ></td>
	<td class="line x" title="58:198	Except for the MERT, the training criteria that we will consider are additive at the sentence-level." ></td>
	<td class="line x" title="59:198	Thus, the training problem for a development set with S sentences can be formalized as: M1 = argmax M1 Ssummationdisplay s=1 F(M1,(eI1,fJ1 )s) (2) Here, F(,) denotes the training criterion that we would like to maximize and (eI1,fJ1 )s denotes a sentence pair in the development set." ></td>
	<td class="line x" title="60:198	The optimization is done using the Downhill Simplex algorithm from the Numerical Recipes book (Press et al. , 2002)." ></td>
	<td class="line x" title="61:198	This is a general purpose optimization procedure withtheadvantagethatitdoesnotrequirethederivative information." ></td>
	<td class="line x" title="62:198	Before we will describe the details of the different training criteria in Sec." ></td>
	<td class="line x" title="63:198	5 and 6, we will discuss evaluation metrics in the following section." ></td>
	<td class="line x" title="64:198	4 Evaluation Metrics The automatic evaluation of machine translation is currently an active research area." ></td>
	<td class="line oc" title="65:198	There exists a variety of different metrics, e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al. , 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al. , 2003)." ></td>
	<td class="line n" title="66:198	Each of them has advantages and shortcomings." ></td>
	<td class="line x" title="67:198	A popular metric for evaluating machine translation quality is the Bleu score (Papineni et al. , 2002)." ></td>
	<td class="line x" title="68:198	It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al. , 2006)." ></td>
	<td class="line x" title="69:198	On the other hand, Callison-Burch concluded that the Bleu score is reliable for comparing variants of the same machine translation system." ></td>
	<td class="line x" title="70:198	As this is exactly what we will need in our experiments and as Bleu is currently the most popular metric, we have chosen it as our primary evaluation metric." ></td>
	<td class="line x" title="71:198	Nevertheless, most of the methods we will present can be easily adapted to other automatic evaluation metrics." ></td>
	<td class="line x" title="72:198	In the following, we will briefly review the computation of the Bleu score as some of the training criteria are motivated by this." ></td>
	<td class="line x" title="73:198	The Bleu score is a combination of the geometric mean of n-gram precisions and a brevity penalty for too short translation hypotheses." ></td>
	<td class="line x" title="74:198	The Bleu score for a translation hypothesiseI1 and a reference translation eI1 is computed as: Bleu(eI1,eI1) = BP(I, I)  4productdisplay n=1 Precn(eI1,eI1)1/4 with BP(I, I) = braceleftbigg 1 if I  I exp(1  I/I) if I < I Precn(eI1,eI1) = summationtext wn1 minC(wn1|eI1),C(wn1|eI1)} summationtext wn1 C(wn1|eI1) (3) Here, C(wn1|eI1) denotes the number of occurrences of an n-gram wn1 in a sentence eI1." ></td>
	<td class="line x" title="75:198	The denominators of the n-gram precisions evaluate to the number of n-grams in the hypothesis, i.e. I n+1." ></td>
	<td class="line x" title="76:198	The n-gram counts for the Bleu score computation are usually collected over a whole document." ></td>
	<td class="line x" title="77:198	For our purposes, a sentence-level computation is preferable." ></td>
	<td class="line x" title="78:198	A problem with the sentence-level Bleu score is that the score is zero if not at least one fourgram matches." ></td>
	<td class="line x" title="79:198	As we would like to avoid this problem, we use the smoothed sentence-level Bleu score as suggested in (Lin and Och, 2004)." ></td>
	<td class="line x" title="80:198	Thus, we increase the nominator and denominator of Precn(,) by one for n > 1." ></td>
	<td class="line x" title="81:198	Note that we will use the sentence-level Bleu score only during training." ></td>
	<td class="line x" title="82:198	The evaluation on the development and test sets will be carried out using the standard Bleu score, i.e. at the corpuslevel." ></td>
	<td class="line x" title="83:198	AstheMERTbaselinedoesnotrequire the use of the sentence-level Bleu score, we use the standard Bleu score for training the baseline system." ></td>
	<td class="line x" title="84:198	In the following, we will describe several criteria for training the log-linear parameters M1 of our model." ></td>
	<td class="line x" title="85:198	For notational convenience, we assume that there is just one reference translation." ></td>
	<td class="line x" title="86:198	Nevertheless, themethodscanbeeasilyadaptedtothecaseofmultiple references." ></td>
	<td class="line x" title="87:198	526 5 Maximum Likelihood 5.1 Sentence-Level Computation A popular approach for training parameters is maximum likelihood estimation (MLE)." ></td>
	<td class="line x" title="88:198	Here, the goal is to maximize the joint likelihood of the parameters and the training data." ></td>
	<td class="line x" title="89:198	For log-linear models, this results in a nice optimization criterion which is convex and has a single optimum." ></td>
	<td class="line x" title="90:198	It is equivalent to the maximum mutual information (MMI) criterion." ></td>
	<td class="line x" title="91:198	We obtain the following training criterion: FMLS(M1,(eI1,fJ1 )) = logpM 1 (eI1|fJ1 ) A problem that we often face in practice is that the correct translation might not be among the candidates that our MT system produces." ></td>
	<td class="line x" title="92:198	Therefore, (Och and Ney, 2002; Och, 2003) defined the translation candidate with the minimum word-error rate as pseudo reference translation." ></td>
	<td class="line x" title="93:198	This has some bias towards minimizing the word-error rate." ></td>
	<td class="line x" title="94:198	Here, we will use the translation candidate with the maximum Bleu score as pseudo reference to bias the system towards the Bleu score." ></td>
	<td class="line x" title="95:198	However, as pointed out in (Och, 2003), there is no reason to believe that the resulting parameters are optimal with respect to translation quality measured with the Bleu score." ></td>
	<td class="line x" title="96:198	The goal of this sentence-level criterion is to discriminatethesinglecorrecttranslationagainstallthe other incorrect translations." ></td>
	<td class="line x" title="97:198	This is problematic as, even for human experts, it is very hard to define a single best translation of a sentence." ></td>
	<td class="line x" title="98:198	Furthermore, the alternative target language sentences are not all equally bad translations." ></td>
	<td class="line x" title="99:198	Some of them might be very close to the correct translation or even equivalent whereas other sentences may have a completely different meaning." ></td>
	<td class="line x" title="100:198	The sentence-level MLE criterion does not distinguish these cases and is therefore a rather harsh training criterion." ></td>
	<td class="line x" title="101:198	5.2 N-gram Level Computation As an alternative to the sentence-level MLE, we performed experiments with an n-gram level MLE." ></td>
	<td class="line x" title="102:198	Here, we limit the order of the n-grams and assume conditional independence among the n-gram probabilities." ></td>
	<td class="line x" title="103:198	We define the log-likelihood (LLH) of a target language sentence eI1 given a source language sentence fJ1 as: FMLN(M1,(eI1,fJ1 )) = Nsummationdisplay n=1 summationdisplay wn1eI1 logpM 1 (wn1|fJ1 ) Here, we use the n-gram posterior probability pM 1 (wn1|fJ1 ) as defined in (Zens and Ney, 2006)." ></td>
	<td class="line x" title="104:198	The n-gram posterior distribution is smoothed using a uniform distribution over all possible n-grams." ></td>
	<td class="line x" title="105:198	pM 1 (wn1|fJ1 ) =   NM 1 (wn1,fJ1 )summationtext wprimen1 NM1 (wprime n1,fJ 1 ) + (1  )  1V n Here, V denotes the vocabulary size of the target language; thus, V n is the number of possible n-grams in the target language." ></td>
	<td class="line x" title="106:198	We define NM 1 (wn1,fJ1 ) as in (Zens and Ney, 2006): NM 1 (wn1,fJ1 ) = summationdisplay I,eI1 In+1summationdisplay i=1 pM 1 (eI1|fJ1 )(ei+n1i,wn1) (4) The sum over the target language sentences is limited to an N-best list, i.e. the N best translation candidates according to the baseline model." ></td>
	<td class="line x" title="107:198	In this equation, we use the Kronecker function (,), i.e. the term (ei+n1i,wn1) evaluates to one if and only if the n-gram wn1 occurs in the target sentence eI1 starting at position i. An advantage of the n-gram level computation of the likelihood is that we do not have to define pseudo-references as for the sentence-level MLE." ></td>
	<td class="line x" title="108:198	We can easily compute the likelihood for the human reference translation." ></td>
	<td class="line x" title="109:198	Furthermore, this criterion has thedesirablepropertythatittakespartialcorrectness into account, i.e. it is not as harsh as the sentencelevel criterion." ></td>
	<td class="line x" title="110:198	6 Expected Bleu Score According to statistical decision theory, one should maximize the expected gain (or equivalently minimize the expected loss)." ></td>
	<td class="line x" title="111:198	For machine translation, this means that we should optimize the expected Bleu score, or any other preferred evaluation metric." ></td>
	<td class="line x" title="112:198	527 6.1 Sentence-Level Computation The expected Bleu score for a given source sentence fJ1 and a reference translation eI1 is defined as: E[Bleu|eI1,fJ1 ] = summationdisplay eI1 Pr(eI1|fJ1 )  Bleu(eI1,eI1) Here, Pr(eI1|fJ1 ) denotes the true probability distribution over the possible translations eI1 of the given source sentence fJ1 . As this probability distribution is unknown, we approximate it using the log-linear translation modelpM 1 (eI1|fJ1 ) from Eq." ></td>
	<td class="line x" title="113:198	1." ></td>
	<td class="line x" title="114:198	Furthermore, the computation of the expected Bleu score involves a sum over all possible translations eI1." ></td>
	<td class="line x" title="115:198	This sum is approximated using an N-best list, i.e. the N best translation hypotheses of the MT system." ></td>
	<td class="line x" title="116:198	Thus, the training criterion for the sentencelevel expected Bleu computation is: FEBS(M1,(eI1,fJ1 )) = summationdisplay eI1 pM 1 (eI1|fJ1 )Bleu(eI1,eI1) An advantage of the sentence-level computation is that it is straightforward to plug in alternative evaluation metrics instead of the Bleu score." ></td>
	<td class="line x" title="117:198	Note that the minimum error rate training (Och, 2003) uses only the target sentence with the maximum posterior probability whereas, here, the whole probability distribution is taken into account." ></td>
	<td class="line x" title="118:198	6.2 N-gram Level Computation In this section, we describe a more fine grained computation of the expected Bleu score by exploiting its particular structure." ></td>
	<td class="line x" title="119:198	Hence, this derivation is specific for the Bleu score but should be easily adaptable to other n-gram based metrics." ></td>
	<td class="line x" title="120:198	We can rewrite the expected Bleu score as: E[Bleu|eI1,fJ1 ] = E[BP|I,fJ1 ]  4productdisplay n=1 E[Precn|eI1,fJ1 ]1/4 We assumed conditional independence between the brevity penalty BP and the n-gram precisions Precn." ></td>
	<td class="line x" title="121:198	Note that although these independence assumptions do not hold, the resulting parameters might work well for translation." ></td>
	<td class="line x" title="122:198	In fact, we will show that this criterion is among the best performing ones in Sec." ></td>
	<td class="line x" title="123:198	7." ></td>
	<td class="line x" title="124:198	This type of independence assumption is typical within the naive Bayes classifier framework." ></td>
	<td class="line x" title="125:198	The resulting training criterion that we will use in Eq." ></td>
	<td class="line x" title="126:198	2 is then: FEBN(M1,(eI1,fJ1 )) = EM 1 [BP|I,fJ1 ]  4productdisplay n=1 EM 1 [Precn|eI1,fJ1 ]1/4 We still have to define the estimators for the expected brevity penalty as well as the expected ngram precision: EM 1 [BP|I,fJ1 ] = summationdisplay I BP(I, I)  pM 1 (I|fJ1 ) EM 1 [Precn|eI1,fJ1 ] = (5) summationtext wn1 pM 1 (wn1|fJ1 )summationtext c minc,C(wn1|eI1)}  pM 1 (c|wn1,fJ1 ) summationtext wn1 pM 1 (wn1|fJ1 )summationtext c c  pM 1 (c|wn1,fJ1 ) Here, we use the sentence length posterior probability pM 1 (I|fJ1 ) as defined in (Zens and Ney, 2006) andthen-gramposteriorprobabilitypM 1 (wn1|fJ1 )as described in Sec." ></td>
	<td class="line x" title="127:198	5.2." ></td>
	<td class="line x" title="128:198	Additionally, we predict the number of occurrences c of an n-gram." ></td>
	<td class="line x" title="129:198	This information is necessary for the so-called clipping in the Bleu score computation, i.e. the min operator in the nominator of formulae Eq." ></td>
	<td class="line x" title="130:198	3 and Eq." ></td>
	<td class="line x" title="131:198	5." ></td>
	<td class="line x" title="132:198	The denominator of Eq." ></td>
	<td class="line x" title="133:198	5 is the expected number of n-grams in the target sentence, whereas the nominator denotes the expected number of correct n-grams." ></td>
	<td class="line x" title="134:198	To predict the number of occurrences within a translation hypothesis, we use relative frequencies smoothed with a Poisson distribution." ></td>
	<td class="line x" title="135:198	The mean of the Poisson distribution (wn1,fJ1,M1 ) is chosen to be the mean of the unsmoothed distribution." ></td>
	<td class="line x" title="136:198	pM 1 (c|wn1,fJ1 ) =   NM 1 (c,wn1,fJ1 ) NM 1 (wn1,fJ1 ) + (1  )  (w n1,fJ1,M1 )c  ec c!" ></td>
	<td class="line x" title="137:198	528 Table 1: Chinese-English TC-Star task: corpus statistics." ></td>
	<td class="line x" title="138:198	Chinese English Train Sentence pairs 8.3M Running words 197M 238M Vocabulary size 224K 389K Dev Sentences 1019 2038 Running words 26K 51K Eval 2006 Sentences 1232 2464 Running words 30K 62K 2007 Sentences 917 1834 Running words 21K 45K with (wn1,fJ1,M1 ) = summationdisplay c c  NM 1 (c,wn1,fJ1 ) NM 1 (wn1,fJ1 ) Note that in case the mean(wn1,fJ1,M1 ) is zero, we do not need the distribution pM 1 (c|wn1,fJ1 )." ></td>
	<td class="line x" title="139:198	The smoothing parameters  and  are both set to 0.9." ></td>
	<td class="line x" title="140:198	7 Experimental Results 7.1 Task Description We perform translation experiments on the ChineseEnglish TC-Star task." ></td>
	<td class="line x" title="141:198	This is a broadcast news speech translation task used within the European Union project TC-Star1." ></td>
	<td class="line x" title="142:198	The bilingual training data consists of virtually all publicly available LDC Chinese-English corpora." ></td>
	<td class="line x" title="143:198	The 6-gram language model was trained on the English part of the bilingual training data and additional monolingual English parts from the GigaWord corpus." ></td>
	<td class="line x" title="144:198	We use the modified Kneser-Ney discounting as implemented in the SRILM toolkit (Stolcke, 2002)." ></td>
	<td class="line x" title="145:198	Annual public evaluations are carried out for this task within the TC-Star project." ></td>
	<td class="line x" title="146:198	We will report results on manual transcriptions, i.e. the so-called verbatim condition, of the official evaluation test sets of the years 2006 and 2007." ></td>
	<td class="line x" title="147:198	There are two reference translations available for the development and test sets." ></td>
	<td class="line x" title="148:198	The corpus statistics are shown in Table 1." ></td>
	<td class="line x" title="149:198	7.2 Translation Results In Table 2, we present the translation results for different training criteria for the development 1http://www.tc-star.org set and the two blind test sets." ></td>
	<td class="line x" title="150:198	The reported case-sensitive Bleu scores are computed using the mteval-v11b.pl2 tool using two reference translations, i.e. BLEUr2n4c." ></td>
	<td class="line x" title="151:198	Note that already the baseline system (MERT-Bleu) would have achieved the first rank in the official TC-Star evaluation 2006; the best Bleu score in that evaluation was 16.1%." ></td>
	<td class="line x" title="152:198	The MBR hypotheses were generated using the algorithm described in (Ehling et al. , 2007) on a 10000-best list." ></td>
	<td class="line x" title="153:198	On the development data, the MERT-Bleu achieves the highest Bleu score." ></td>
	<td class="line x" title="154:198	This seems reasonable as it is the objective of this training criterion." ></td>
	<td class="line x" title="155:198	The maximum likelihood (MLE) criteria perform somewhat worse under MAP decoding." ></td>
	<td class="line x" title="156:198	Interestingly, the MBR decoding can compensate this to a large extent: all criteria achieve a Bleu score of about 18.9% on the development set." ></td>
	<td class="line x" title="157:198	The benefits of MBR decoding become even more evident on the two test sets." ></td>
	<td class="line x" title="158:198	Here, the MAP results for the sentence-level MLE criterion are rather poor compared to the MERT-Bleu." ></td>
	<td class="line x" title="159:198	Nevertheless, using MBR decoding results in very similar Bleu scores for most of the criteria on these two test sets." ></td>
	<td class="line x" title="160:198	We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities." ></td>
	<td class="line x" title="161:198	Then-gramlevelMLEcriterionseemstoperform better than the sentence-level MLE criterion, especially on the test sets." ></td>
	<td class="line x" title="162:198	The reasons might be that there is no need for the use of pseudo references as described in Sec." ></td>
	<td class="line x" title="163:198	5 and that partial correctness is taken into account." ></td>
	<td class="line x" title="164:198	The best results are achieved using the expected BleuscorecriteriadescribedinSec.6." ></td>
	<td class="line x" title="165:198	Here,thesentence level and n-gram level variants achieve more or less the same results." ></td>
	<td class="line x" title="166:198	The overall improvement on the Eval06 set is about 1.0% Bleu absolute for MAP decoding and 0.9% for MBR decoding." ></td>
	<td class="line x" title="167:198	On the Eval07 set, the improvements are even larger, about 1.8% Bleu absolute for MAP and 1.1% Bleu for MBR." ></td>
	<td class="line x" title="168:198	All these improvements are statistically significant at the 99% level using a pairwise significance test3." ></td>
	<td class="line x" title="169:198	Given that currently the most popular approach is to use MERT-Bleu MAP decoding, the overall im2http://www.nist.gov/speech/tests/mt/resources/scoring.htm 3Thetoolforcomputingthesignificancetestwaskindlyprovided by the National Research Council Canada." ></td>
	<td class="line x" title="170:198	529 Table 2: Translation results: Bleu scores [%] for the Chinese-English TC-Star task for various training criteria (MERT: minimum error rate training; MLE: maximum likelihood estimation; E[Bleu]: expected Bleu score) and the maximum a-posteriori (MAP) as well as the minimum Bayes risk (MBR) decision rule." ></td>
	<td class="line x" title="171:198	Development Eval06 Eval07 Decision Rule MAP MBR MAP MBR MAP MBR Training Criterion MERT-Bleu (baseline) 19.5 19.4 16.7 17.2 22.2 23.0 MLE sentence-level 17.8 18.9 14.8 17.1 18.9 22.7 n-gram level 18.6 18.8 17.0 17.8 22.8 23.5 E[Bleu] sentence-level 19.1 18.9 17.5 18.1 23.5 24.1 n-gram level 18.6 18.8 17.7 17.6 24.0 24.0 provement is about 1.4% absolute for the Eval06 set and 1.9% absolute on the Eval07 set." ></td>
	<td class="line x" title="172:198	Note that the MBR decision rule almost always outperformstheMAPdecisionrule." ></td>
	<td class="line x" title="173:198	Intherarecases where the MAP decision rule yields better results, the difference in terms of Bleu score are small and not statistically significant." ></td>
	<td class="line x" title="174:198	We also investigated the effect of the maximum n-gram order for the n-gram level maximum likelihood estimation (MLE)." ></td>
	<td class="line x" title="175:198	The results are shown in Figure 1." ></td>
	<td class="line x" title="176:198	We observe an increase of the Bleu score with increasing maximum n-gram order for the development corpus." ></td>
	<td class="line x" title="177:198	On the evaluation sets, however, the maximum is achieved if the maximum n-gram order is limited to four." ></td>
	<td class="line x" title="178:198	This seems intuitive as the Bleu score usesn-grams up to length four." ></td>
	<td class="line x" title="179:198	However, one should be careful here: the differences are rather small, so it might be just statistical noise." ></td>
	<td class="line x" title="180:198	Some translation examples from the Eval07 test set are shown in Table 3 for different training criteria under the maximum a-posteriori decision rule." ></td>
	<td class="line x" title="181:198	8 Conclusions We have presented a systematic comparison of several criteria for training the log-linear parameters of a statistical machine translation system." ></td>
	<td class="line x" title="182:198	Additionally, we have compared the maximum a-posteriori with the minimum Bayes risk decision rule." ></td>
	<td class="line x" title="183:198	We can conclude that the expected Bleu score is not only a theoretically sound training criterion, but also achieves the best results in terms of Bleu score." ></td>
	<td class="line x" title="184:198	The improvement over a state-of-the-art MERT baseline is 1.3% Bleu absolute for the MAP decision rule and 1.1% Bleu absolute for the MBR decision rule for the large Chinese-English TC-Star speech translation task." ></td>
	<td class="line x" title="185:198	1 2 3 4 5 6 7 8 9 max." ></td>
	<td class="line x" title="186:198	n-gram order 14 16 18 20 22 24 Bleu [%] Dev Eval'06 Eval'07 Figure 1: Effect of the maximum n-gram order on the Bleu score for the n-gram level maximum likelihood estimation under the maximum a-posteriori decision rule." ></td>
	<td class="line x" title="187:198	We presented two methods for computing the expected Bleu score: a sentence-level and an n-gram level approach." ></td>
	<td class="line x" title="188:198	Both yield similar results." ></td>
	<td class="line x" title="189:198	We think that the n-gram level computation has certain advantages: The n-gram posterior probabilities could be computed from a word graph which would result in more reliable estimates." ></td>
	<td class="line x" title="190:198	Whether this pays off in terms of translation quality is left open for future work." ></td>
	<td class="line x" title="191:198	Another interesting result of our experiments is that the MBR decision rule seems to be less affected by sub-optimal parameter settings." ></td>
	<td class="line x" title="192:198	Although it is well-known that the MBR decision rule is more appropriate than the MAP decision rule, the latter is more popular in the SMT community (and many other areas of natural language processing)." ></td>
	<td class="line x" title="193:198	Our results show that it can be beneficial to 530 Table 3: Translation examples from the Eval07 test set for different training criteria and the maximum aposteriori decision rule." ></td>
	<td class="line x" title="194:198	(MERT: minimum error rate training, MLE-S: sentence-level maximum likelihood estimation, E[Bleu]: sentence-level expected Bleu) Criterion Translation Reference 1 Saving Private Ryan ranks the third on the box office revenue list which is also a movie that is possible to win an 1999 Oscar award 2 Saving Private Ryan ranked third in the box office income is likely to compete in the nineteen ninety-nine Oscar Awards MERT-Bleu Saving private Ryan in box office income is possible ranked third in 1999 Oscar a film MLE-S Saving private Ryan box office revenue ranked third is possible in 1999 Oscar a film E[Bleu]-S Saving private Ryan ranked third in the box office income is also likely to run for the 1999 Academy Awards a film Reference 1 The following problem is whether people in countries like China and Japan and other countries will choose Euros rather than US dollars in international business activities in the future 2 The next question is whether China or Japan or other countries will choose to use Euros instead of US dollars when they conduct international business in the future MERT-Bleu The next question is in China or Japan international business activities in the future they will not use the Euro dollar MLE-S The next question was either in China or Japan international business activities in the future they will adopt the Euro instead of the dollar E[Bleu]-S The next question was in China or Japan in the international business activities in the future they will adopt the Euro instead of the US dollar Reference 1 The Chairman of the European Commission Jacques Santer pointed out in this September that the financial crisis that happened in Russia has not affected peoples confidence in adopting the Euro 2 European Commission President Jacques Santer pointed out in September this year that Russias financial crisis did not shake peoples confidence for planning the use of the Euro MERT-Bleu President of the European Commission Jacques Santer on September this year that the Russian financial crisis has not shaken people s confidence in the introduction of the Euro MLE-S President of the European Commission Jacques Santer September that the Russian financial crisis has not affected people s confidence in the introduction of the Euro E[Bleu]-S President of the European Commission Jacques Santer pointed out that Russia s financial crisis last September has not shaken people s confidence in the introduction of the Euro Reference 1 After many years of friction between Dutch and French speaking Belgians all of them now hope to emphasize their European identities 2 After years of friction between Belgiums Dutch-speaking and French-speaking people they now all wish to emphasize their European identity MERT-Bleu Belgiums Dutch-speaking and French-speaking after many years of civil strife emphasized that they now hope that Europeans MLE-S Belgiums Dutch-speaking and francophone after years of civil strife that they now hope that Europeans E[Bleu]-S Belgiums Dutch-speaking and French-speaking after many years of civil strife it is now want to emphasize their European identity 531 use the MBR decision rule." ></td>
	<td class="line x" title="195:198	On the other hand, the computation of the MBR hypotheses is more time consuming." ></td>
	<td class="line x" title="196:198	Therefore, it would be desirable to have a more efficient algorithm for computing the MBR hypotheses." ></td>
	<td class="line x" title="197:198	Acknowledgments This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No." ></td>
	<td class="line x" title="198:198	HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D07-1105
An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems
Macherey, Wolfgang;Och, Franz Josef;"></td>
	<td class="line x" title="1:270	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp." ></td>
	<td class="line x" title="2:270	986995, Prague, June 2007." ></td>
	<td class="line x" title="3:270	c2007 Association for Computational Linguistics An Empirical Study on Computing Consensus Translations from Multiple Machine Translation Systems Wolfgang Macherey Google Inc. 1600 Amphitheatre Parkway Mountain View, CA 94043, USA wmach@google.com Franz Josef Och Google Inc. 1600 Amphitheatre Parkway Mountain View, CA 94043, USA och@google.com Abstract This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination." ></td>
	<td class="line x" title="4:270	We give empirical evidence that the systems to be combined should be of similar quality and need to be almost uncorrelated in order to be beneficial for system combination." ></td>
	<td class="line x" title="5:270	Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the bestranked machine translation engines in the 2006 NIST machine translation evaluation." ></td>
	<td class="line x" title="6:270	1 Introduction Computing consensus translations from the outputs of multiple machine translation engines has become a powerful means to improve translation quality in many machine translation tasks." ></td>
	<td class="line x" title="7:270	Analogous to the ROVER approach in automatic speech recognition (Fiscus, 1997), a composite translation is computed by voting on the translation outputs of multiple machine translation systems." ></td>
	<td class="line x" title="8:270	Depending on how the translations are combined and how the voting scheme is implemented, the composite translation may differ from any of the original hypotheses." ></td>
	<td class="line x" title="9:270	While elementary approaches simply select for each sentence one of the original translations, more sophisticated methods allow for combining translations on a word or a phrase level." ></td>
	<td class="line x" title="10:270	Although system combination could be shown to result in substantial improvements in terms of translation quality (Matusov et al. , 2006; Sim et al. , 2007), not every possible ensemble of translation outputs has the potential to outperform the primary translation system." ></td>
	<td class="line x" title="11:270	In fact, an adverse combination of translation systems may even deteriorate translation quality." ></td>
	<td class="line x" title="12:270	This holds to a greater extent, when the ensemble of translation outputs contains a significant number of translations produced by low performing but highly correlated systems." ></td>
	<td class="line x" title="13:270	In this paper we present an empirical study on how different ensembles of translation outputs affect performance in system combination." ></td>
	<td class="line x" title="14:270	In particular, we will address the following questions:  To what extent can translation quality benefit from combining systems developed by multiple research labs?" ></td>
	<td class="line x" title="15:270	Despite an increasing number of translation engines, most state-of-the-art systems in statistical machine translation are nowadays based on implementations of the same techniques." ></td>
	<td class="line x" title="16:270	For instance, word alignment models are often trained using the GIZA++ toolkit (Och and Ney, 2003); error minimizing training criteria such as the Minimum Error Rate Training (Och, 2003) are employed in order to learn feature function weights for log-linear models; and translation candidates are produced using phrase-based decoders (Koehn et al. , 2003) in combination with n-gram language models (Brants et al. , 2007)." ></td>
	<td class="line x" title="17:270	All these methods are established as de facto standards and form an integral part of most statistical machine translation systems." ></td>
	<td class="line x" title="18:270	This, however, raises the question as to what extent translation quality can be expected to improve when similarly designed systems are combined." ></td>
	<td class="line x" title="19:270	 How can a set of diverse translation systems be built from a single translation engine?" ></td>
	<td class="line x" title="20:270	Without having access to different translation 986 engines, it is desirable to build a large number of diverse translation systems from a single translation engine that are useful in system combination." ></td>
	<td class="line x" title="21:270	The mere use of N-best lists and word lattices is often not effective, because N-best candidates may be highly correlated, thus resulting in small diversity compared to the first best hypothesis." ></td>
	<td class="line x" title="22:270	Therefore, we need a canonical way to build a large pool of diverse translation systems from a single translation engine." ></td>
	<td class="line x" title="23:270	 How can an ensemble of translation outputs be selected from a large pool of translation systems?" ></td>
	<td class="line x" title="24:270	Once a large pool of translation systems is available, we need an effective means to select a small ensemble of translation outputs for which the combined system outperforms the best individual system." ></td>
	<td class="line x" title="25:270	These questions will be investigated on the basis of three approaches to system combination: (i) an MBR-like candidate selection method based on BLEU correlation matrices, (ii) confusion networks built from word sausages, and (iii) a novel twopass search algorithm that aims at finding consensus translations by reordering bags of words constituting the consensus hypothesis." ></td>
	<td class="line x" title="26:270	Experiments were performed on two ChineseEnglish text translation corpora under the conditions of the large data track as defined for the 2006 NIST machine translation evaluation (MT06)." ></td>
	<td class="line x" title="27:270	Results are reported for consensus translations built from system outputs provided by MT06 participants as well as systems derived from one of the best-ranked translation engines." ></td>
	<td class="line x" title="28:270	The remainder of this paper is organized as follows: in Section 2, we describe three combination methods for computing consensus translations." ></td>
	<td class="line x" title="29:270	In Sections 3.1 and 3.2, we present experimental results on combining system outputs provided by MT06 participants." ></td>
	<td class="line x" title="30:270	Section 3.3 shows how correlation among translation systems affects performance in system combination." ></td>
	<td class="line x" title="31:270	In Section 3.4, we discuss how a single translation engine can be modified in order to produce a large number of diverse translation systems." ></td>
	<td class="line x" title="32:270	First experimental results using a greedy search algorithm to select a small ensemble of translation outputs from a large pool of canonically built translation systems are reported." ></td>
	<td class="line x" title="33:270	A summary presented in Section 4 concludes the paper." ></td>
	<td class="line x" title="34:270	2 Methods for System Combination System combination in machine translation aims to build a composite translation from system outputs of multiple machine translation engines." ></td>
	<td class="line x" title="35:270	Depending on how the systems are combined and which voting scheme is implemented, the consensus translation may differ from any of the original candidate translations." ></td>
	<td class="line x" title="36:270	In this section, we discuss three approaches to system combination." ></td>
	<td class="line x" title="37:270	2.1 System Combination via Candidate Selection The easiest and most straightforward approach to system combination simply returns one of the original candidate translations." ></td>
	<td class="line x" title="38:270	Typically, this selection is made based on translation scores, confidence estimations, language and other models (Nomoto, 2004; Paul et al. , 2005)." ></td>
	<td class="line x" title="39:270	For many machine translation systems, however, the scores are often not normalized or may even not be available, which makes it difficult to apply this technique." ></td>
	<td class="line x" title="40:270	We therefore propose an alternative method based on correlation matrices computed from the BLEU performance measure (Papineni et al. , 2001)." ></td>
	<td class="line x" title="41:270	Lete1,,eM denote the outputs of M translation systems, each given as a sequence of words in the target language." ></td>
	<td class="line x" title="42:270	An element of the BLEU correlation matrix B a16 a112bija113 is defined as the sentence-based BLEU score between a candidate translation ei and a pseudo-reference translation ej a112i,j a16 1,,Ma113: bij a16 BPa112ei,eja113a4exp a36a39 a39a371 4 4a184 na161 logna112ei,eja113 a44a47 a47a45." ></td>
	<td class="line x" title="43:270	(1) Here, BP denotes the brevity penalty factor with n designating the n-gram precisions." ></td>
	<td class="line x" title="44:270	Because the BLEU score is computed on a sentence rather than a corpus-level, n-gram precisions are capped by the maximum over 12a4a124eia124 and n in order to avoid singularities, where a124eia124 is the length of the candidate translation 1." ></td>
	<td class="line x" title="45:270	Due to the following properties, B can be interpreted as a correlation matrix, although the term does not hold in a strict mathematical sense: (i) bij a80 a1140,1a115; (ii) bij a16 1.0 a240a241 ei a16 ej; (iii) bij a16 0.0 a240a241 eia88ej a16 a72, i.e., bij is zero if and only if none of the words which constitute ei can be found 1 Note that for non-zero n-gram precisions, n is always larger than 12a4a124ea124." ></td>
	<td class="line x" title="46:270	987 in ej and vice versa." ></td>
	<td class="line x" title="47:270	The BLEU correlation matrix is in general, however, not symmetric, although in practice, a124a124bij a1bjia124a124 is typically negligible." ></td>
	<td class="line x" title="48:270	Each translation system m is assigned to a system prior weight m a80 a1140,1a115, which reflects the performance of system m relatively to all other translation systems." ></td>
	<td class="line x" title="49:270	If no prior knowledge is available, m is set to 1a123M." ></td>
	<td class="line x" title="50:270	Now, let  a16 a1121,,Ma113a74 denote a vector of system prior weights and let b1,,bM denote the row vectors of the matrix B. Then the translation system with the highest consensus is given by: ea6 a16 ema6 with ma6 a16 argmax em a33 a74 a4bm a41 (2) The candidate selection rule in Eq." ></td>
	<td class="line x" title="51:270	(2) has two useful properties:  The selection does not depend on scored translation outputs; the mere target word sequence is sufficient." ></td>
	<td class="line x" title="52:270	Hence, this technique is also applicable to rule-based translation systems 2." ></td>
	<td class="line x" title="53:270	 Using the components of the row-vector bm as feature function values for the candidate translation em (m a16 1,,M), the system prior weights  can easily be trained using the Minimum Error Rate Training described in (Och, 2003)." ></td>
	<td class="line x" title="54:270	Note that the candidate selection rule in Eq." ></td>
	<td class="line x" title="55:270	(2) is equivalent to re-ranking candidate translations according to the Minimum Bayes Risk (MBR) decision rule (Kumar and Byrne, 2004), provided that the system prior weights are used as estimations of the posterior probabilities pa112ea124fa113 for a source sentence f. Due to the proximity of this method to the MBR selection rule, we call this combination scheme MBR-like system combination." ></td>
	<td class="line x" title="56:270	2.2 ROVER-Like Combination Schemes ROVER-like combination schemes aim at computing a composite translation by voting on confusion networks that are built from translation outputs of multiple machine translation engines via an iterative application of alignments (Fiscus, 1997)." ></td>
	<td class="line x" title="57:270	To accomplish this, one of the original candidate translations, e.g. em, is chosen as the primary translation hypothesis, while all other candidates en a112n a24 ma113 are aligned with the word sequence of 2 This property is not exclusive to this combination scheme but also holds for the methods discussed in Sections 2.2 and 2.3." ></td>
	<td class="line x" title="58:270	the primary translation." ></td>
	<td class="line x" title="59:270	To limit the costs when aligning a permutation of the primary translation, the alignment metric should allow for small shifts of contiguous word sequences in addition to the standard edit operations deletions, insertions, and substitutions." ></td>
	<td class="line x" title="60:270	These requirements are met by the Translation Edit Rate (TER) (Snover et al. , 2006): TERa112ei,eja113a21 Del a0 Ins a0 Sub a0 Shifta124e ja124 (3) The outcome of the iterated alignments is a word transition network which is also known as word sausage because of the linear sequence of correspondence sets that constitute the network." ></td>
	<td class="line x" title="61:270	Since both the order and the elements of a correspondence set depend on the choice of the primary translation, each candidate translation is chosen in turn as the primary system." ></td>
	<td class="line x" title="62:270	This results in a total of M word sausages that are combined into a single super network." ></td>
	<td class="line x" title="63:270	The word sequence along the costminimizing path defines the composite translation." ></td>
	<td class="line x" title="64:270	To further optimize the word sausages, we replace each system prior weight m with the lp-norm over the normalized scalar product between the weight vector  and the row vector bm: a49m a21 a112 a74 a4bma113lscript a184 m a112a74 a4bma113lscript, lscript a80 a1140,a0a56a113 (4) As lscript approaches a0a56, a49m a16 1 if and only if system m has the highest consensus among all input systems; otherwise, a49m a16 0." ></td>
	<td class="line x" title="65:270	Thus, the word sausages are able to emulate the candidate selection rule described in Section 2.1." ></td>
	<td class="line x" title="66:270	Setting lscript a16 0 yields uniform system prior weights, and setting B to the unity matrix provides the original prior weights vector." ></td>
	<td class="line x" title="67:270	Word sausages which take advantage of the refined system prior weights are denoted by word sausages+." ></td>
	<td class="line x" title="68:270	2.3 A Two-Pass Search Algorithm The basic idea of the two-pass search algorithm is to compute a consensus translation by reordering words that are considered to be constituents of the final consensus translation." ></td>
	<td class="line x" title="69:270	Initially, the two-pass search is given a repository of candidate translations which serve as pseudo references together with a vector of system prior weights." ></td>
	<td class="line x" title="70:270	In the first pass, the algorithm uses a greedy strategy to determine a bag of words which minimizes the position-independent word error rate (PER)." ></td>
	<td class="line x" title="71:270	These words are considered to be 988 constituents of the final consensus translation." ></td>
	<td class="line x" title="72:270	The greedy strategy implicitly ranks the constituents, i.e., words selected at the beginning of the first phase reduce the PER the most and are considered to be more important than constituents selected in the end." ></td>
	<td class="line x" title="73:270	The first pass finishes when putting further constituents into the bag of words does not improve the PER." ></td>
	<td class="line x" title="74:270	The list of constituents is then passed to a second search algorithm, which starts with the empty string and then expands all active hypotheses by systematically inserting the next unused word from the list of constituents at different positions in the current hypothesis." ></td>
	<td class="line x" title="75:270	For instance, a partial consensus hypothesis of length l expands into l a0 1 new hypotheses of length la01." ></td>
	<td class="line x" title="76:270	The resulting hypotheses are scored with respect to the TER measure based on the repository of weighted pseudo references." ></td>
	<td class="line x" title="77:270	Lowscoring hypotheses are pruned to keep the space of active hypotheses small." ></td>
	<td class="line x" title="78:270	The algorithm will finish if either no constituents are left or if expanding the set of active hypotheses does not further decrease the TER score." ></td>
	<td class="line x" title="79:270	Optionally, the best consensus hypothesis found by the two-pass search is combined with all input translation systems via the MBR-like combination scheme described in Section 2.1." ></td>
	<td class="line x" title="80:270	This refinement is called two-pass+." ></td>
	<td class="line x" title="81:270	2.4 Related Work Research on multi-engine machine translation goes back to the early nineties." ></td>
	<td class="line x" title="82:270	In (Robert and Nirenburg, 1994), a semi-automatic approach is described that combines outputs from three translation systems to build a consensus translation." ></td>
	<td class="line x" title="83:270	(Nomoto, 2004) and (Paul et al. , 2005) used translation scores, language and other models to select one of the original translations as consensus translation." ></td>
	<td class="line x" title="84:270	(Bangalore et al. , 2001) used a multiple string alignment algorithm in order to compute a single confusion network, on which a consensus hypothesis was computed through majority voting." ></td>
	<td class="line x" title="85:270	Because the alignment procedure was based on the Levenshtein distance, it was unable to align translations with significantly different word orders." ></td>
	<td class="line x" title="86:270	(Jayaraman and Lavie, 2005) tried to overcome this problem by using confidence scores and language models in order to rank a collection of synthetic combinations of words extracted from the original translation hypotheses." ></td>
	<td class="line oc" title="87:270	Experimental results were only reported for the METEOR metric (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="88:270	In (Matusov et al. , 2006), pairwise word alignments of the original translation hypotheses were estimated for an enhanced statistical alignment model in order Table 1: Corpus statistics for two Chinese-English text translation sets: ZHEN-05 is a random selection of test data used in NIST evaluations prior to 2006; ZHEN-06 comprises the NIST portion of the Chinese-English evaluation data used in the 2006 NIST machine translation evaluation." ></td>
	<td class="line x" title="89:270	corpus Chinese English ZHEN-05 sentences 2390 chars / words 110647 67737 ZHEN-06 sentences 1664 chars / words 64292 41845 to explicitly capture word re-ordering." ></td>
	<td class="line x" title="90:270	Although the proposed method was not compared with other approaches to system combination, it resulted in substantial gains and provided new insights into system combination." ></td>
	<td class="line x" title="91:270	3 Experimental Results Experiments were conducted on two corpora for Chinese-English text translations, the first of which is compiled from a random selected subset of evaluation data used in the NIST MT evaluations up to the year 2005." ></td>
	<td class="line x" title="92:270	The second data set consists of the NIST portion of the Chinese-English data used in the MT06 evaluation and comprises 1664 Chinese sentences collected from broadcast news articles (565 sentences), newswire texts (616 sentences), and news group texts (483 sentences)." ></td>
	<td class="line x" title="93:270	Both corpora provide 4 reference translations per source sentence." ></td>
	<td class="line x" title="94:270	Table 1 summarizes some corpus statistics." ></td>
	<td class="line x" title="95:270	For all experiments, system performance was measured in terms of the IBM-BLEU score (Papineni et al. , 2001)." ></td>
	<td class="line x" title="96:270	Compared to the NIST implementation of the BLEU score, IBM-BLEU follows the original definition of the brevity penalty (BP) factor: while in the NIST implementation the BP is always based on the length of the shortest reference translation, the BP in the IBM-BLEU score is based on the length of the reference translation which is closest to the candidate translation length." ></td>
	<td class="line x" title="97:270	Typically, IBM-BLEU scores tend to be smaller than NISTBLEU scores." ></td>
	<td class="line x" title="98:270	In the following, BLEU always refers to the IBM-BLEU score." ></td>
	<td class="line x" title="99:270	Except for the results reported in Section 3.2, we used uniform system prior weights throughout all experiments." ></td>
	<td class="line x" title="100:270	This turned out to be more stable when combining different sets of translation systems and helped to improve generalization." ></td>
	<td class="line x" title="101:270	989 Table 2: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for primary systems together with consensus systems for the MBR-like candidate selection method obtained by combining each three adjacent systems with uniform system prior weights." ></td>
	<td class="line x" title="102:270	Primary systems are sorted in descending order with respect to their BLEU score." ></td>
	<td class="line x" title="103:270	The 95% confidence intervals are computed using the bootstrap re-sampling normal approximation method (Noreen, 1989)." ></td>
	<td class="line x" title="104:270	combination primary system consensus oracle BLEU CI 95% BP BLEU  BP pair-CI 95% BLEU BP 01, 02, 03 32.10 (a80.88) 0.93 32.97 (+0.87) 0.92 [+0.29, +1.46] 38.54 0.94 01, 15, 16a6 32.10 (a80.88) 0.93 23.55 ( -8.54) 0.92 [ -9.29, -7.80] 33.55 0.95 02, 03, 04 31.71 (a80.90) 0.96 31.55 ( -0.16) 0.92 [ -0.65, +0.29] 37.23 0.95 03, 04, 05 29.59 (a80.88) 0.87 29.55 ( -0.04) 0.88 [ -0.53, +0.41] 35.55 0.92 03, 04, 06a6 29.59 (a80.88) 0.87 29.83 (+0.24) 0.90 [ -0.29, +0.71] 35.69 0.93 04, 05, 06 27.70 (a80.87) 0.94 28.52 (+0.82) 0.91 [+0.15, +1.49] 34.67 0.94 05, 06, 07 27.05 (a80.81) 0.88 28.21 (+1.16) 0.92 [+0.63, +1.66] 33.89 0.94 05, 06, 08a6 27.05 (a80.81) 0.88 28.47 (+1.42) 0.91 [+0.95, +1.95] 34.18 0.93 06, 07, 08 27.02 (a80.76) 0.92 28.12 (+1.10) 0.94 [+0.59, +1.59] 33.87 0.95 07, 08, 09 26.75 (a80.79) 0.97 27.79 (+1.04) 0.94 [+0.52, +1.51] 33.54 0.95 08, 09, 10 26.41 (a80.81) 0.92 26.78 (+0.37) 0.94 [ -0.07, +0.86] 32.47 0.96 09, 10, 11 25.05 (a80.84) 0.90 24.96 ( -0.09) 0.94 [ -0.59, +0.46] 30.92 0.97 10, 11, 12 23.48 (a80.68) 1.00 24.24 (+0.76) 0.94 [+0.27, +1.30] 30.08 0.96 11, 12, 13 23.26 (a80.74) 0.95 24.05 (+0.79) 0.92 [+0.40, +1.23] 29.56 0.93 12, 13, 14 22.38 (a80.78) 0.87 22.68 (+0.30) 0.89 [ -0.28, +0.95] 28.58 0.91 13, 14, 15 22.13 (a80.72) 0.89 21.29 ( -0.84) 0.90 [ -1.33, -0.33] 26.61 0.92 14, 15, 16 17.42 (a80.66) 0.93 18.45 (+1.03) 0.92 [+0.45, +1.56] 23.30 0.95 15 17.20 (a80.64) 0.91       16 15.21 (a80.63) 0.96       3.1 Combining Multiple Research Systems In a first experiment, we investigated the effect of combining translation outputs provided from different research labs." ></td>
	<td class="line x" title="105:270	Each translation system corresponds to a primary system submitted to the NIST MT06 evaluation 3." ></td>
	<td class="line x" title="106:270	Table 2 shows the BLEU scores together with their corresponding BP factors for the primary systems of 16 research labs (site names were anonymized)." ></td>
	<td class="line x" title="107:270	Primary systems are sorted in descending order with respect to their BLEU score." ></td>
	<td class="line x" title="108:270	Table 2 also shows the consensus translation results for the MBR-like candidate selection method." ></td>
	<td class="line x" title="109:270	Except where marked with an asterisk, all consensus systems are built from the outputs of three adjacent systems." ></td>
	<td class="line x" title="110:270	While only few combined systems show a degradation, the majority of all consensus translations achieve substantial gains between 0.2% and 1.4% absolute in terms of BLEU score on top of the best individual (primary) system." ></td>
	<td class="line x" title="111:270	The column CI provides 95% confidence intervals for BLEU scores with respect to the primary system baseline using the bootstrap re-sampling normal 3 For more information see http://www.nist.gov/ speech/tests/mt/mt06eval_official_results." ></td>
	<td class="line x" title="112:270	html approximation method (Noreen, 1989)." ></td>
	<td class="line x" title="113:270	The column pair-CI shows 95% confidence intervals relative to the primary system using the paired bootstrap re-sampling method (Koehn, 2004)." ></td>
	<td class="line x" title="114:270	The principle of the paired bootstrap method is to create a large number of corresponding virtual test sets by consistently selecting candidate translations with replacement from both the consensus and the primary system." ></td>
	<td class="line x" title="115:270	The confidence interval is then estimated over the differences between the BLEU scores of corresponding virtual test sets." ></td>
	<td class="line x" title="116:270	Improvements are considered to be significant if the left boundary of the confidence interval is larger than zero." ></td>
	<td class="line x" title="117:270	Oracle BLEU scores shown in Table 2 are computed by selecting the best translation among the three candidates." ></td>
	<td class="line x" title="118:270	The oracle scores might indicate a larger potential of the MBR-like selection rule, and further gains could be expected if the candidate selection rule is combined with confidence measures." ></td>
	<td class="line x" title="119:270	Table 2 shows that it is important that all translation systems achieve nearly equal quality; combining high-performing systems with low-quality translations typically results in clear performance losses compared to the primary system, which is the case when combining, e.g., systems 01, 15, and 16." ></td>
	<td class="line x" title="120:270	990 Table 3: BLEU scores and brevity penalty (BP) factors determined on the ZHEN-06 test set for the combination of multiple research systems using the MBR-like selection method with uniform and trained system prior weights." ></td>
	<td class="line x" title="121:270	Prior weights are trained using 5-fold cross validation." ></td>
	<td class="line x" title="122:270	The 95% confidence intervals realtive to uniform weights are computed using the paired bootstrap re-sampling method (Koehn, 2004)." ></td>
	<td class="line x" title="123:270	# systems combination uniform  opt." ></td>
	<td class="line x" title="124:270	on dev." ></td>
	<td class="line x" title="125:270	 opt." ></td>
	<td class="line x" title="126:270	on test BLEU BP BLEU BP pair-CI 95% BLEU BP 3 01  03 32.98 0.92 33.03 0.93 [ -0.23, +0.34] 33.60 0.93 4 01  04 33.44 0.93 33.46 0.93 [ -0.26, +0.29] 34.97 0.94 5 01  05 33.07 0.92 33.14 0.93 [ -0.29, +0.43] 34.33 0.93 6 01  06 32.86 0.92 33.53 0.93 [+0.26, +1.08] 34.43 0.93 7 01  07 33.08 0.93 33.51 0.93 [+0.04, +0.82] 34.49 0.93 8 01  08 33.12 0.93 33.47 0.93 [ -0.06, +0.75] 34.50 0.94 9 01  09 33.15 0.93 33.22 0.93 [ -0.35, +0.51] 34.68 0.93 10 01  10 33.01 0.93 33.59 0.94 [+0.18, +0.96] 34.79 0.94 11 01  11 32.84 0.94 33.40 0.94 [+0.13, +0.98] 34.76 0.94 12 01  12 32.73 0.93 33.49 0.94 [+0.34, +1.18] 34.83 0.94 13 01  13 32.71 0.93 33.54 0.94 [+0.39, +1.26] 34.91 0.94 14 01  14 32.66 0.93 33.69 0.94 [+0.58, +1.47] 34.97 0.94 15 01  15 32.47 0.93 33.57 0.94 [+0.63, +1.57] 34.99 0.94 16 01  16 32.51 0.93 33.62 0.94 [+0.62, +1.59] 35.00 0.94 3.2 Non-Uniform System Prior Weights As pointed out in Section 2.1, a useful property of the MBR-like system selection method is that system prior weights can easily be trained using the Minimum Error Rate Training (Och, 2003)." ></td>
	<td class="line x" title="127:270	In this section, we investigate the effect of using non-uniform system weights for the combination of multiple research systems." ></td>
	<td class="line x" title="128:270	Since for each research system, only the first best translation candidate was provided, we used a five-fold cross validation scheme in order to train and evaluate the system prior weights." ></td>
	<td class="line x" title="129:270	For this purpose, all research systems were consistently split into five random partitions of almost equal size." ></td>
	<td class="line x" title="130:270	The partitioning procedure was document preserving, i.e., sentences belonging to the same document were guaranteed to be assigned to the same partition." ></td>
	<td class="line x" title="131:270	Each of the five partitions played once the role of the evaluation set while the other four partitions were used as development data to train the system prior weights." ></td>
	<td class="line x" title="132:270	Consensus systems were computed for each held out set using the system prior weights estimated on the respective development sets." ></td>
	<td class="line x" title="133:270	The combination results determined on all held out sets were then concatenated and evaluated with respect to the ZHEN-06 reference translations." ></td>
	<td class="line x" title="134:270	Table 3 shows the results for the combinations of up to 16 research systems using either uniform or trained system prior weights." ></td>
	<td class="line x" title="135:270	System 01 achieved the highest BLEU score on all five constellations of development partitions and is therefore the primary system to which all results in Table 3 compare." ></td>
	<td class="line x" title="136:270	In comparison to uniform weights, consensus translations using trained weights are more robust toward the integration of low performing systems into the combination scheme." ></td>
	<td class="line x" title="137:270	The best combined system obtained with trained system prior weights (01-14) is, however, not significantly better than the best combined system using uniform weights (01-04), for which the 95% confidence interval yields a114a10.17, 0.66a115 according to the paired bootstrap re-sampling method." ></td>
	<td class="line x" title="138:270	Table 3 also shows the theoretically achievable BLEU scores when optimizing the system prior weights on the held out data." ></td>
	<td class="line x" title="139:270	This provides an upper bound to what extent system combination might benefit if an ideal set of system prior weights were used." ></td>
	<td class="line x" title="140:270	3.3 Effect of Correlation on System Combination The degree of correlation among input translation systems is a key factor which decides whether translation outputs can be combined such a way that the overall system performance improves." ></td>
	<td class="line x" title="141:270	Correlation can be considered as a reciprocal measure of diversity: if the correlation is too large (a161 90%), there will be insufficient diversity among the input systems and the consensus system will at most be able to only marginally outperform the best indi991 Table 4: BLEU scores obtained on ZHEN-05 with uniform prior weights and a 10-way system combination using the MBR-like candidate selection rule, word sausages, and the two-pass search algorithm together with their improved versions sausages+ and two-pass+, respectively for different sample sizes of the FBIS training corpus." ></td>
	<td class="line x" title="142:270	sampling primary mbr-like sausages sausages+ two-pass two-pass+ a114%a115 BLEU CI 95% BP BLEU BP BLEU BP BLEU BP BLEU BP BLEU BP 5 27.82 (a80.65) 1.00 29.51 1.00 29.00 0.97 30.25 0.99 29.58 0.94 29.93 0.96 10 29.70 (a80.69) 1.00 31.42 1.00 30.74 0.98 31.99 0.99 31.30 0.95 31.75 0.97 20 31.37 (a80.69) 1.00 32.56 1.00 32.64 1.00 33.17 0.99 32.60 0.96 32.76 0.98 40 32.66 (a80.66) 1.00 33.52 1.00 33.23 0.99 33.98 1.00 33.65 0.97 33.88 0.99 80 33.67 (a80.66) 1.00 34.17 1.00 33.93 0.99 34.38 1.00 34.20 0.99 34.35 1.00 100 33.90 (a80.67) 1.00 34.03 1.00 33.98 1.00 34.02 1.00 33.90 1.00 34.08 1.00 vidual translation system." ></td>
	<td class="line x" title="143:270	If the correlation is too low (a160 5%), there might be no consensus among the input systems and the quality of the consensus translations will hardly differ from a random selection of the candidates." ></td>
	<td class="line x" title="144:270	To study how correlation affects performance in system combination, we built a large number of systems trained on randomly sampled portions of the FBIS 4 training data collection." ></td>
	<td class="line x" title="145:270	Sample sizes ranged between 5% and 100% with each larger data set doubling the size of the next smaller collection." ></td>
	<td class="line x" title="146:270	For each sample size, we created 10 data sets, thus resulting in a total of 6a210 training corpora." ></td>
	<td class="line x" title="147:270	On each data set, a new translation system was trained from scratch and 4 LDC catalog number: LDC2003E14 27 28 29 30 31 32 33 34 35 0 10 20 30 40 50 60 70 80 90 100 BLEU [%] sampling [%] consensus system: 109 87 65 43 primary system: 1 Figure 1: Incremental system combination on ZHEN-05 using the MBR-like candidate selection rule and uniform prior weights." ></td>
	<td class="line x" title="148:270	Systems were trained with different sample sizes of the FBIS data." ></td>
	<td class="line x" title="149:270	used for decoding the ZHEN-05 test sentences." ></td>
	<td class="line x" title="150:270	All 60 systems applied the MBR decision rule (Kumar and Byrne, 2004), which gave an additional 0.5% gain on average on top of using the maximum aposteriori (MAP) decision rule." ></td>
	<td class="line x" title="151:270	Systems trained on equally amounts of training data were incrementally combined." ></td>
	<td class="line x" title="152:270	Figure 1 shows the evolution of the BLEU scores as a function of the number of systems as the sample size is increased from 5100%." ></td>
	<td class="line x" title="153:270	Table 4 shows the BLEU scores obtained with a 10way system combination using the MBR-like candidate selection rule, word sausages, and the twopass search algorithm together with their improved versions sausages+ and two-pass+, respectively." ></td>
	<td class="line x" title="154:270	In order to measure the correlation between the individual translation systems, we computed the intersystem BLEU score matrix as shown exemplary 0 10 20 30 40 50 60 70 80 90 100 35 40 45 50 55 60 65 70 75 80 85 correlation [%] sampling [%] consensus Figure 2: Evolution of the correlation on ZHEN-05 averaged over 10 systems in the course of the sample size." ></td>
	<td class="line x" title="155:270	992 Table 5: Minimum, maximum, and average inter-system BLEU score correlations for (i) the primary systems of the 2006 NIST machine translation evaluation on the ZHEN-06 test data, (ii) different training corpus sizes (FBIS), and (iii) a greedy strategy which chooses 15 systems out of a pool of 200 translation systems." ></td>
	<td class="line x" title="156:270	ZHEN-6 ZHEN-5 ZHEN-5 ZHEN-6 16 primary FBIS sampling, 10 systems 15 systems 15 systems systems 5% 10% 20% 40% 80% 100% greedy selection ZHEN-5 selection min 0.08 0.38 0.44 0.47 0.53 0.60 0.72 0.55 0.50 mean 0.18 0.40 0.45 0.50 0.56 0.66 0.79 0.65 0.61 median 0.19 0.40 0.45 0.49 0.56 0.64 0.78 0.63 0.58 max 0.28 0.42 0.47 0.53 0.58 0.70 0.88 0.85 0.83 in Table 6 for the 16 MT06 primary submissions." ></td>
	<td class="line x" title="157:270	Figure 2 shows the evolution of the correlation averaged over 10 systems as the sample size is increased from 5100%." ></td>
	<td class="line x" title="158:270	Note that all systems were optimized using a non-deterministic implementation of the Minimum Error Rate Training described in (Och, 2003)." ></td>
	<td class="line x" title="159:270	Hence, using all of the FBIS corpus data does not necessarily result in fully correlated systems, since the training procedure may pick a different solution for same training data in order to increase diversity." ></td>
	<td class="line x" title="160:270	Both Table 4 and Figure 1 clearly indicate that increasing the correlation (and thus reducing the diversity) substantially reduces the potential of a consensus system to outperform the primary translation system." ></td>
	<td class="line x" title="161:270	Ideally, the correlation should not be larger than 30%." ></td>
	<td class="line x" title="162:270	Especially for low inter-system correlations and reduced translation quality, both the enhanced versions of the word sausage combination method and the two-pass search outperform the MBR-like candidate selection scheme." ></td>
	<td class="line x" title="163:270	This advantage, however, diminishes as soon as the correlation increases and translations produced by the individual systems become more similar." ></td>
	<td class="line x" title="164:270	3.4 Toward Automatic System Generation and Selection Sampling the training data is an effective means to investigate the effect of system correlation on consensus performance." ></td>
	<td class="line x" title="165:270	However, this is done at the expense of the overall system quality." ></td>
	<td class="line x" title="166:270	What we need instead is a method to reduce correlation without sacrificing system performance." ></td>
	<td class="line x" title="167:270	A simple, though computationally very expensive way to build an ensemble of low-correlated statistical machine translation systems from a single translation engine is to train a large pool of systems, in which each of the systems is trained with a slightly different set of parameters." ></td>
	<td class="line x" title="168:270	Changing only few parameters at a time typically results in only small changes in system performance but may have a strong impact on system correlation." ></td>
	<td class="line x" title="169:270	In our experiments we observed that changing parameters which affect the training procedure at a very early stage, are most effective and introduce larger diversity." ></td>
	<td class="line x" title="170:270	For instance, changing the training procedure for word alignment models turned out to be most beneficial; for details see (Och and Ney, 2003)." ></td>
	<td class="line x" title="171:270	Other parameters that were changed include the maximum jump width in word re-ordering, the choice of feature function weights for the log-linear translation models, and the set of language models used in decoding." ></td>
	<td class="line x" title="172:270	Once a large pool of translation systems has been generated, we need a method to select a small ensemble of diverse translation outputs that are beneficial for computing consensus translations." ></td>
	<td class="line x" title="173:270	Here, we used a greedy strategy to rank the systems with respect to their ability to improve system Table 6: Inter-system BLEU score matrix for primary systems of the NIST 2006 TIDES machine translation evaluation on the ZHEN-06 test data." ></td>
	<td class="line x" title="174:270	Id 01 02 03 04 05 a4a4a4 14 15 16 01 1.00 0.27 0.26 0.23 0.26 a4a4a4 0.15 0.15 0.12 02 0.27 1.00 0.27 0.22 0.25 a4a4a4 0.15 0.15 0.12 03 0.26 0.27 1.00 0.21 0.28 a4a4a4 0.15 0.15 0.10 04 0.23 0.22 0.21 1.00 0.19 a4a4a4 0.14 0.12 0.12 05 0.26 0.25 0.28 0.19 1.00 a4a4a4 0.16 0.17 0.11 06 0.27 0.24 0.25 0.21 0.26 a4a4a4 0.16 0.18 0.13    14 0.15 0.15 0.15 0.14 0.16 a4a4a4 1.00 0.12 0.08 15 0.15 0.15 0.15 0.12 0.17 a4a4a4 0.12 1.00 0.09 16 0.12 0.12 0.10 0.12 0.11 a4a4a4 0.08 0.09 1.00 993 37.7 37.8 37.9 38 38.1 38.2 38.3 38.4 5 10 15 20 25 30 35 40 BLEU[%] number of systems ZHEN-5: consensus systemprimary system 5 10 15 20 25 30 35 40 31.2 31.4 31.6 31.8 32 32.2 32.4 32.6 BLEU[%] number of systems ZHEN-6: oracle selectionconsensus system primary system Figure 3: BLEU score of the consensus translation as a function of the number of systems on the ZHEN-05 sentences (left) and ZHEN-06 sentences (right)." ></td>
	<td class="line x" title="175:270	The middle curve (right) shows the variation of the BLEU score on the ZHEN-06 data when the greedy selection of the ZHEN-05 is used." ></td>
	<td class="line x" title="176:270	combination." ></td>
	<td class="line x" title="177:270	Initially, the greedy strategy selected the best individual system and then continued by adding those systems to the ensemble, which gave the highest gain in terms of BLEU score according to the MBR-like system combination method." ></td>
	<td class="line x" title="178:270	Note that the greedy strategy is not guaranteed to increase the BLEU score of the combined system when a new system is added to the ensemble of translation systems." ></td>
	<td class="line x" title="179:270	In a first experiment, we trained approximately 200 systems using different parameter settings in training." ></td>
	<td class="line x" title="180:270	Each system was then used to decode both the ZHEN-05 and the ZHEN-06 test sentences using the MBR decision rule." ></td>
	<td class="line x" title="181:270	The upper curve in Figure 3 (left) shows the evolution of the BLEU score on the ZHEN-05 sentences in the course of the number of selected systems." ></td>
	<td class="line x" title="182:270	The upper curve in Figure 3 (right) shows the BLEU score of the consensus translation as a function of the number of systems when the selection is done on the ZHEN-06 set." ></td>
	<td class="line x" title="183:270	This serves as an oracle." ></td>
	<td class="line x" title="184:270	The middle curve (right) shows the function of the BLEU score when the system selection made on the ZHEN-05 set is used in order to combine the translation outputs for the ZHEN-06 data." ></td>
	<td class="line x" title="185:270	Although system combination gave moderate improvements on top of the primary system, the greedy strategy still needs further refinements in order to improve generalization." ></td>
	<td class="line x" title="186:270	While the correlation statistics shown in Table 5 indicate that changing the training parameters helps to substantially decrease system correlation, there is still need for additional methods in order to reduce the level of inter-system BLEU scores such that they fall within the range of a1140.2, 0.3a115." ></td>
	<td class="line x" title="187:270	4 Conclusions In this paper, we presented an empirical study on how different selections of translation outputs affect translation quality in system combination." ></td>
	<td class="line x" title="188:270	Composite translations were computed using (i) a candidate selection method based on inter-system BLEU score matrices, (ii) an enhanced version of word sausage networks, and (iii) a novel two-pass search algorithm which determines and re-orders bags of words that build the constituents of the final consensus hypothesis." ></td>
	<td class="line x" title="189:270	All methods gave statistically significant improvements." ></td>
	<td class="line x" title="190:270	We showed that both a high diversity among the original translation systems and a similar translation quality among the translation systems are essential in order to gain substantial improvements on top of the best individual translation systems." ></td>
	<td class="line x" title="191:270	Experiments were conducted on the NIST portion of the Chinese English text translation corpus used for the 2006 NIST machine translation evaluation." ></td>
	<td class="line x" title="192:270	Combined systems were built from primary systems of up to 16 different research labs as well as systems derived from one of the best-ranked translation engines." ></td>
	<td class="line x" title="193:270	We trained a large pool of translation systems from a single translation engine and presented first experimental results for a greedy search to select an ensemble of translation systems for system combination." ></td>
	<td class="line x" title="194:270	994 References S. Banerjee and A. Lavie." ></td>
	<td class="line x" title="195:270	2005." ></td>
	<td class="line o" title="196:270	METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments." ></td>
	<td class="line x" title="197:270	In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, 43th Annual Meeting of the Association of Computational Linguistics (ACL-2005), Ann Arbor, MI, USA, June." ></td>
	<td class="line x" title="198:270	S. Bangalore, G. Bodel, and G. Riccardi." ></td>
	<td class="line x" title="199:270	2001." ></td>
	<td class="line x" title="200:270	Computing Consensus Translation from Multiple Machine Translation Systems." ></td>
	<td class="line x" title="201:270	In 2001 Automatic Speech Recognition and Understanding (ASRU) Workshop, Madonna di Campiglio, Trento, Italy, December." ></td>
	<td class="line x" title="202:270	T. Brants, A. Popat, P. Xu, F. Och, and J. Dean." ></td>
	<td class="line x" title="203:270	2007." ></td>
	<td class="line x" title="204:270	Large Language Models in Machine Tranlation." ></td>
	<td class="line x" title="205:270	In Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing, Prague, Czech Republic." ></td>
	<td class="line x" title="206:270	Association for Computational Linguistics." ></td>
	<td class="line x" title="207:270	J. G. Fiscus." ></td>
	<td class="line x" title="208:270	1997." ></td>
	<td class="line x" title="209:270	A Post-Processing System to Yield Reduced Word Error Rates: Recognizer Output Voting Error Reduction (ROVER)." ></td>
	<td class="line x" title="210:270	In Proceedings 1997 IEEE Workshop on Automatic Speech Recognition and Understanding, pages 347352, Santa Barbara, CA, USA, December." ></td>
	<td class="line x" title="211:270	S. Jayaraman and A. Lavie." ></td>
	<td class="line x" title="212:270	2005." ></td>
	<td class="line x" title="213:270	Multi-Engine Machine Translation Guided by Explicit Word Matching." ></td>
	<td class="line x" title="214:270	In 10th Conference of the European Association for Machine Translation (EAMT), pages 143152, Budapest, Hungary." ></td>
	<td class="line x" title="215:270	P. Koehn, F. J. Och, and D. Marcu." ></td>
	<td class="line x" title="216:270	2003." ></td>
	<td class="line x" title="217:270	Statistical Phrase-Based Translation." ></td>
	<td class="line x" title="218:270	In NAACL 03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 4854, Edmonton, Canada." ></td>
	<td class="line x" title="219:270	Association for Computational Linguistics." ></td>
	<td class="line x" title="220:270	P. Koehn." ></td>
	<td class="line x" title="221:270	2004." ></td>
	<td class="line x" title="222:270	Statistical Significance Tests for Machine Translation Evaluation." ></td>
	<td class="line x" title="223:270	In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388395, Barcelona, Spain, August." ></td>
	<td class="line x" title="224:270	Association for Computational Linguistics." ></td>
	<td class="line x" title="225:270	S. Kumar and W. Byrne." ></td>
	<td class="line x" title="226:270	2004." ></td>
	<td class="line x" title="227:270	Minimum Bayes-Risk Decoding for Statistical Machine Translation." ></td>
	<td class="line x" title="228:270	In Proc." ></td>
	<td class="line x" title="229:270	HLT-NAACL, pages 196176, Boston, MA, USA, May. E. Matusov, N. Ueffing, and H. Ney." ></td>
	<td class="line x" title="230:270	2006." ></td>
	<td class="line x" title="231:270	Computing Consensus Translation from Multiple Machine Translation Systems Using Enhanced Hypotheses Alignment." ></td>
	<td class="line x" title="232:270	In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 3340, Trento, Italy, April." ></td>
	<td class="line x" title="233:270	T. Nomoto." ></td>
	<td class="line x" title="234:270	2004." ></td>
	<td class="line x" title="235:270	Multi-Engine Machine Translation with Voted Language Model." ></td>
	<td class="line x" title="236:270	In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL04), Main Volume, pages 494501, Barcelona, Spain, July." ></td>
	<td class="line x" title="237:270	E. W. Noreen." ></td>
	<td class="line x" title="238:270	1989." ></td>
	<td class="line x" title="239:270	Computer-Intensive Methods for Testing Hypotheses." ></td>
	<td class="line x" title="240:270	John Wiley & Sons, Canada." ></td>
	<td class="line x" title="241:270	F. J. Och and H. Ney." ></td>
	<td class="line x" title="242:270	2003." ></td>
	<td class="line x" title="243:270	A Systematic Comparison of Various Statistical Alignment Models." ></td>
	<td class="line x" title="244:270	Computational Linguistics, 29(1):1951." ></td>
	<td class="line x" title="245:270	F. J. Och." ></td>
	<td class="line x" title="246:270	2003." ></td>
	<td class="line x" title="247:270	Minimum Error Rate Training in Statistical Machine Translation." ></td>
	<td class="line x" title="248:270	In 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160167, Sapporo, Japan, July." ></td>
	<td class="line x" title="249:270	K. Papineni, S. Roukos, T. Ward, and W. Zhu." ></td>
	<td class="line x" title="250:270	2001." ></td>
	<td class="line x" title="251:270	Bleu: a Method for Automatic Evaluation of Machine Translation." ></td>
	<td class="line x" title="252:270	Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, USA." ></td>
	<td class="line x" title="253:270	M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma, and E. Sumita." ></td>
	<td class="line x" title="254:270	2005." ></td>
	<td class="line x" title="255:270	Nobody is Perfect: ATRs Hybrid Approach to Spoken Language Translation." ></td>
	<td class="line x" title="256:270	In International Workshop on Spoken Language Translation, pages 5562, Pittsburgh, PA, USA, October." ></td>
	<td class="line x" title="257:270	F. Robert and S. Nirenburg." ></td>
	<td class="line x" title="258:270	1994." ></td>
	<td class="line x" title="259:270	Three Heads are Better than One." ></td>
	<td class="line x" title="260:270	In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing, Stuttgart, Germany, October." ></td>
	<td class="line x" title="261:270	K. C. Sim, W. Byrne, M. Gales, H. Sahbi, and P.C. Woodland." ></td>
	<td class="line x" title="262:270	2007." ></td>
	<td class="line x" title="263:270	Consensus network decoding for statistical machine translation system combination." ></td>
	<td class="line x" title="264:270	In IEEE Int." ></td>
	<td class="line x" title="265:270	Conf." ></td>
	<td class="line x" title="266:270	on Acoustics, Speech, and Signal Processing, Honolulu, HI, USA, April." ></td>
	<td class="line x" title="267:270	M. Snover, B. J. Dorr, R. Schwartz, J. Makhoul, L. Micciulla, and R. Weischedel." ></td>
	<td class="line x" title="268:270	2006." ></td>
	<td class="line x" title="269:270	A Study of Translation Edit Rate with Targeted Human Annotation." ></td>
	<td class="line x" title="270:270	In Proceedings of Association for Machine Translation in the Americas ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1005
Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-Goals
Uchimoto, Kiyotaka;Kotani, Katsunori;Zhang, Yujie;Isahara, Hitoshi;"></td>
	<td class="line x" title="1:194	Proceedings of NAACL HLT 2007, pages 3340, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:194	c2007 Association for Computational Linguistics Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate." ></td>
	<td class="line x" title="3:194	We propose a method for automatically evaluating the quality of each translation." ></td>
	<td class="line x" title="4:194	In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality." ></td>
	<td class="line x" title="5:194	In EnglishJapanese translation, for example, prepositions and infinitives must be appropriately translated." ></td>
	<td class="line x" title="6:194	We show several procedures that enable evaluating the quality of a translated sentence more appropriately than using conventional methods." ></td>
	<td class="line x" title="7:194	The first procedure is constructing a test set where the conditions are assigned to each test-set sentence in the form of yes/no questions." ></td>
	<td class="line x" title="8:194	The second procedure is developing a system that determines an answer to each question." ></td>
	<td class="line x" title="9:194	The third procedure is combining a measure based on the questions and conventional measures." ></td>
	<td class="line x" title="10:194	We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals." ></td>
	<td class="line x" title="11:194	Promising results are shown." ></td>
	<td class="line x" title="12:194	1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important issue." ></td>
	<td class="line oc" title="13:194	In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al. , 2000; Akiba et al. , 2001; Papineni et al. , 2002; NIST, 2002; Leusch et al. , 2003; Turian et al. , 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimenez et al. , 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently." ></td>
	<td class="line x" title="14:194	For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003)." ></td>
	<td class="line x" title="15:194	This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves." ></td>
	<td class="line x" title="16:194	MT systems can be ranked if a set of MT results for each system and their reference translations are given." ></td>
	<td class="line x" title="17:194	Usually, about 300 or more sentences are used to automatically rank MT systems (Koehn, 2004)." ></td>
	<td class="line x" title="18:194	However, the quality of a sentence translated by an MT system is difficult to evaluate." ></td>
	<td class="line x" title="19:194	For example, the results of five MTs into Japanese of the sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers. are shown in Table 1." ></td>
	<td class="line x" title="20:194	A conventional automatic evaluation method ranks the fifth MT result first although its human subjective evaluation is the lowest." ></td>
	<td class="line x" title="21:194	This is because conventional methods are based on the similarity between a translated sentence and its reference translation, and they give the translated sentence a high score when the two sentences are globally similar to each other in terms of lexical overlap." ></td>
	<td class="line x" title="22:194	However, in the case of the above example, 33 Table 1: Examples of conventional automatic evaluations." ></td>
	<td class="line x" title="23:194	Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers." ></td>
	<td class="line x" title="24:194	reference translation (in Japanese) roudousha no igan no wariai wa, asubesuto roudousha no tame ni saikou to naru youda." ></td>
	<td class="line x" title="25:194	System MT results BLEU NIST Fluency Adequacy 1 roudousha no aida no igan no paasenteeji wa, donoyouna ishiwata roudousha no tame ni demo mottomo ookii youdearu . 0.2111 2.1328 2 3 2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto roudousha no tame ni mottomo takai youni omowa re masu . 0.2572 2.1234 2 3 3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame ni mo mottomo takai youni mie masu 0 1.8094 1 2 4 roudousha no aida no igan no paasenteeji wa ninino ishiwata ni wa mottomo takaku mie masu . 0 1.5902 1 2 5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mottomo takai youni mieru . 0.2692 2.2640 1 2 the most important thing to maintain a high translation quality is to correctly translate for into the target language, and it would be difficult to detect the importance just by comparing an MT result and its reference translations even if the number of reference translations is increased." ></td>
	<td class="line x" title="26:194	In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality." ></td>
	<td class="line x" title="27:194	In this paper, we show that constructing a test set where the conditions that are mainly established from a linguistic point of view are assigned to each test-set sentence in the form of yes/no questions, developing a system that determines an answer to each question, and combining a measure based on the questions and conventional measures enable the evaluation of the quality of a translated sentence more appropriately than using conventional methods." ></td>
	<td class="line x" title="28:194	We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals." ></td>
	<td class="line x" title="29:194	2 Test Set for Evaluating Machine Translation Quality 2.1 Test Set Two main types of data are used for evaluating MT quality." ></td>
	<td class="line x" title="30:194	One type of data is constructed by arbitrarily collecting sentence pairs in the sourceand target-languages, and the other is constructed by intensively collecting sentence pairs that include linguistic phenomena that are difficult to automatically translate." ></td>
	<td class="line x" title="31:194	Recently, MT evaluation campaigns such as the International Workshop on Spoken Language Translation 1, NIST Machine Translation Evaluation 2, and HTRDP Evaluation 3 were organized to support the improvement of MT techniques." ></td>
	<td class="line x" title="32:194	The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conversation data for fair evaluation." ></td>
	<td class="line x" title="33:194	They are classified as the former type of data mentioned above." ></td>
	<td class="line x" title="34:194	On the other hand, the data provided by NTT (Ikehara et al. , 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type." ></td>
	<td class="line x" title="35:194	Almost all the data mentioned above consist of only parallel translations in two languages." ></td>
	<td class="line x" title="36:194	Data with information for evaluating MT results, such as JEIDAs are rarely found." ></td>
	<td class="line x" title="37:194	In this paper, we call data that consist of parallel translations collected for MT evaluation and that the information for MT evaluation is assigned to, a test set." ></td>
	<td class="line x" title="38:194	The most characteristic information assigned to the JEIDA test set is the yes/no question for assessing the translation results." ></td>
	<td class="line x" title="39:194	For example, a yes/no question such as Is for translated into an expression representing a cause/reason such as de? (in Japanese) is assigned to a test-set sentence." ></td>
	<td class="line x" title="40:194	We can evaluate MT results objectively by answering the question." ></td>
	<td class="line x" title="41:194	An example of a test-set sample consisting of an ID, a source-language sample sentence, its reference translation, and a question is as follows." ></td>
	<td class="line x" title="42:194	1 http://www.slt.atr.jp/IWSLT2006/ 2 http://www.nist.gov/speech/tests/mt/index.htm 3 http://www.863data.org.cn/ 34 ID 1.1.7.1.3-1 Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers." ></td>
	<td class="line x" title="43:194	reference translation (in Japanese) roudousha no igan no wariai wa, asubesuto roudousha no tame ni saikou to naru youda . Question Is appear to translated into an auxiliary verb such as youda?" ></td>
	<td class="line x" title="44:194	The questions are classified mainly in terms of grammar, and the numbers to the left of the hyphenation of each ID such as 1.1.7.1.3 represent the categories of the questions." ></td>
	<td class="line x" title="45:194	For example, the above question is related to catenative verbs." ></td>
	<td class="line x" title="46:194	The JEIDA test set consists of two parts, one for the evaluation of English-Japanese MT and the other for that of Japanese-English MT. We focused on the part for English-Japanese MT. This part consists of 769 sample sentences, each of which has a yes/no question." ></td>
	<td class="line x" title="47:194	The 769 sentences were translated by using five commercial MT systems to investigate the relationship between subjective evaluation based on yes/no questions and conventional subjective evaluation based on fluency and adequacy." ></td>
	<td class="line x" title="48:194	The instruction for the subjective evaluation based on fluency and adequacy followed that given in the TIDES specification (TIDES, 2002)." ></td>
	<td class="line x" title="49:194	The subjective evaluation based on yes/no questions was done by manually answering each question for each translation." ></td>
	<td class="line x" title="50:194	The subjective evaluation based on the yes/no questions was stable; namely, it was almost independent of the human subjects in our preliminary investigation." ></td>
	<td class="line x" title="51:194	There were only two questions for which the answers generated inconsistency in the subjective evaluation when 1,500 question-answer pairs were randomly sampled and evaluated by two human subjects." ></td>
	<td class="line x" title="52:194	Then, we investigated the correlation between the two types of subjective evaluation." ></td>
	<td class="line x" title="53:194	The correlation coefficients mentioned in this paper are statistically significant at the 1% or less significance level." ></td>
	<td class="line x" title="54:194	The Spearman rank-order correlation coefficient is used in this paper." ></td>
	<td class="line x" title="55:194	In the subjective evaluation based on yes/no questions, yes and no were numerically transformed into 1 and 1." ></td>
	<td class="line x" title="56:194	For 3,845 translations obtained by using five MT systems, the correlation coefficients between the subjective evaluations based on yes/no questions and based on fluency and adequacy were 0.48 for fluency and 0.63 for adequacy." ></td>
	<td class="line x" title="57:194	These results indicate that the two subjective evaluations have relatively strong correlations." ></td>
	<td class="line x" title="58:194	The correlation is especially strong between the subjective evaluation based on yes/no questions and adequacy." ></td>
	<td class="line x" title="59:194	2.2 Expansion of JEIDA Test Set Each sample sentence in the JEIDA test set has only one question." ></td>
	<td class="line x" title="60:194	Therefore, in the subjective evaluation using the JEIDA test set, translation errors that do not involve the pre-assigned question are ignored even if they are serious." ></td>
	<td class="line x" title="61:194	Therefore, translations that have serious errors that are not related to the question tend to be evaluated as being of high quality." ></td>
	<td class="line x" title="62:194	To solve this problem, we expanded the test set by adding new questions about translations with the serious errors." ></td>
	<td class="line x" title="63:194	Sentences whose average grades were three or less for fluency and adequacy for the translation results of the five MT systems were selected for the expansion." ></td>
	<td class="line x" title="64:194	Besides them, sentences whose average grades were more than three for fluency and adequacy for the translation results of the five MT systems were selected when a majority of evaluation results based on yes/no questions about the translations of the five MT systems were no." ></td>
	<td class="line x" title="65:194	The number of selected sentences was 150." ></td>
	<td class="line x" title="66:194	The expansion was manually performed using the following steps." ></td>
	<td class="line x" title="67:194	1." ></td>
	<td class="line x" title="68:194	Serious translation errors are extracted from the MT results." ></td>
	<td class="line x" title="69:194	2." ></td>
	<td class="line x" title="70:194	For each extracted error, questions strongly related to the error are searched for in the test set." ></td>
	<td class="line x" title="71:194	If related questions are found, the same types of questions are generated for the selected sentence, and the same ID as that of the related question is assigned to each generated question." ></td>
	<td class="line x" title="72:194	Otherwise, questions are newly generated, and a new ID is assigned to each generated question." ></td>
	<td class="line x" title="73:194	3." ></td>
	<td class="line x" title="74:194	Each MT result is evaluated according to each added question." ></td>
	<td class="line x" title="75:194	Eventually, one or more questions were assigned to each selected sentence in the test set." ></td>
	<td class="line x" title="76:194	Among the 150 35 Table 2: Expanded test-set samples." ></td>
	<td class="line x" title="77:194	ID 1.1.7.1.3-1 Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers." ></td>
	<td class="line x" title="78:194	reference translation (in Japanese) roudousha no igan no wariai wa, asubesuto roudousha no tame ni saikou to naru youda . Question (Q-0) Is appear to translated into an auxiliary verb such as youda?" ></td>
	<td class="line x" title="79:194	ID 1.1.6.1.3-5 Expanded Translation error For is not translated appropriately." ></td>
	<td class="line x" title="80:194	Question-1 (Q-1) Is for translated into an expression representing a cause/reason such as de?" ></td>
	<td class="line x" title="81:194	ID Additional-1 Expanded Translation error Some expressions are not translated." ></td>
	<td class="line x" title="82:194	Question-2 (Q-2) Are all English words translated into Japanese?" ></td>
	<td class="line x" title="83:194	Table 3: Examples of subjective evaluations based on yes/no questions." ></td>
	<td class="line x" title="84:194	Answer System MT results Q-0 Q-1 Q-2 Fluency Adequacy 1 roudousha no aida no igan no paasenteeji wa, donoyouna ishiwata roudousha no tame ni demo mottomo ookii youdearu . YesNoYes 2 3 2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto roudousha no tame ni mottomo takai youni omowa re masu . Yes Yes Yes 2 3 3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame ni mo mottomo takai youni mie masu Yes No No 1 2 4 roudousha no aida no igan no paasenteeji wa ninino ishiwata ni wa mottomo takaku mie masu . Yes No No 1 2 5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mottomo takai youni mieru . Yes No No 1 2 selected sentences, questions were newly assigned to 103 sentences." ></td>
	<td class="line x" title="85:194	The number of added questions was 148." ></td>
	<td class="line x" title="86:194	The maximum number of questions added to a sentence was five." ></td>
	<td class="line x" title="87:194	After expanding the test set, the correlation coefficients between the subjective evaluations based on yes/no questions and based on fluency and adequacy increased from 0.48 to 0.51 for fluency and from 0.63 to 0.66 for adequacy." ></td>
	<td class="line x" title="88:194	The differences between the correlation coefficients obtained before and after the expansion are statistically significant at the 5% or less significance level for adequacy." ></td>
	<td class="line x" title="89:194	These results indicate that the expansion of the test set significantly improves the correlation between the subjective evaluations based on yes/no questions and based on adequacy." ></td>
	<td class="line x" title="90:194	When two or more questions were assigned to a test-set sentence, the subjective evaluation based on the questions was decided by the majority answer." ></td>
	<td class="line x" title="91:194	The majority answers, yes and no, were numerically transformed into 1 and 1." ></td>
	<td class="line x" title="92:194	Ties between yes and no were transformed into 0." ></td>
	<td class="line x" title="93:194	Examples of added questions and the subjective evaluations based on the questions are shown in Tables 2 and 3." ></td>
	<td class="line x" title="94:194	3 Automatic Evaluation of Machine Translation Based on Rate of Accomplishment of Sub-goals 3.1 A New Measure for Evaluating Machine Translation Quality The JEIDA test set was not designed for automatic evaluation but for human subjective evaluation." ></td>
	<td class="line x" title="95:194	However, a measure for automatic MT evaluation that strongly correlates fluency and adequacy is likely to be established because the subjective evaluation based on yes/no questions has a relatively strong correlation with the subjective evaluation based on fluency and adequacy, as mentioned in Section 2." ></td>
	<td class="line x" title="96:194	In this section, we describe a method for automatically evaluating MT quality by predicting an answer to each yes/no question and using those answers." ></td>
	<td class="line x" title="97:194	Hereafter, we assume that each yes/no question is defined as a sub-goal that a given translation should satisfy and that the sub-goal is accomplished if the answer to the corresponding yes/no question to the sub-goal is yes." ></td>
	<td class="line x" title="98:194	We also assume that the sub-goal is unaccomplished if the answer is no." ></td>
	<td class="line x" title="99:194	A new evaluation score, A, is defined based on a multiple lin36 Table 4: Examples of Patterns." ></td>
	<td class="line x" title="100:194	Sample sentence She lived there by herself." ></td>
	<td class="line x" title="101:194	Question Is by herself translated as hitori de?" ></td>
	<td class="line x" title="102:194	Pattern The answer is yes if the pattern [hitori dake de|hitori kiri de |tandoku de|tanshin de] is included in a translation." ></td>
	<td class="line x" title="103:194	Otherwise, the answer is no." ></td>
	<td class="line x" title="104:194	Sample sentence They speak English in New Zealand." ></td>
	<td class="line x" title="105:194	Question The personal pronoun they is omitted in a translation like nyuujiilando de wa eigo wo hanasu?" ></td>
	<td class="line x" title="106:194	Pattern The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation." ></td>
	<td class="line x" title="107:194	Otherwise, the answer is no." ></td>
	<td class="line x" title="108:194	ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation." ></td>
	<td class="line x" title="109:194	The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981)." ></td>
	<td class="line x" title="110:194	A = m summationdisplay i=1  S i  S i (1) + n summationdisplay j=1 ( Q j  Q j +  Q prime j  Q prime j )+ epsilon1 Q j = braceleftBigg 1 : if subgoal is accomplished 0:otherwise (2) Q prime j = braceleftBigg 1 : if subgoal is unaccomplished 0:otherwise (3) Here, the term Q j corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and  Q j is a weight for the rate of accomplishment." ></td>
	<td class="line x" title="111:194	The term Q prime j corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and  Q prime j is a weight for the rate of unaccomplishment." ></td>
	<td class="line x" title="112:194	The value n indicates the number of types of sub-goals." ></td>
	<td class="line x" title="113:194	The term  epsilon1 is constant." ></td>
	<td class="line x" title="114:194	The term S i indicates a similarity between a translated sentence and its reference translation, and  S i is a weight for the similarity." ></td>
	<td class="line oc" title="115:194	Many methods for calculating the similarity have been proposed (Niessen et al. , 2000; Akiba et al. , 2001; Papineni et al. , 2002; NIST, 2002; Leusch et al. , 2003; Turian et al. , 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gimenez et al. , 2005)." ></td>
	<td class="line oc" title="116:194	In our research, 23 scores, namely BLEU (Papineni et al. , 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al. , 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et al. , 2000), PER (Leusch et al. , 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S,SU, W-1.2), were used to calculate each similarity S i . Therefore, the value of m in Eq." ></td>
	<td class="line x" title="117:194	(1) was 23." ></td>
	<td class="line x" title="118:194	Japanese word segmentation was performed by using JUMAN 4 in our experiments." ></td>
	<td class="line x" title="119:194	As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information." ></td>
	<td class="line x" title="120:194	3.2 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determining the answer to each question as yes or no." ></td>
	<td class="line x" title="121:194	This section describes a method based on simple patterns for determining the answers." ></td>
	<td class="line x" title="122:194	An answer to each question is automatically determined by checking whether patterns are included in a translation or not." ></td>
	<td class="line x" title="123:194	The patterns are constructed for each question." ></td>
	<td class="line x" title="124:194	All of the patterns are expressed in hiragana characters." ></td>
	<td class="line x" title="125:194	Before applying the patterns to a given translation, the translation is transformed into hiragana characters, and all punctuation is eliminated." ></td>
	<td class="line x" title="126:194	The transformation to hiragana characters was performed by using JUMAN in our experiments." ></td>
	<td class="line x" title="127:194	Test-set sentences, the questions assigned to them, and the patterns constructed for the questions are shown in Table 4." ></td>
	<td class="line x" title="128:194	In the patterns, the symbol | represents OR." ></td>
	<td class="line x" title="129:194	3.3 Automatic Sub-goal Generation and Automatic Estimation of Rate of Accomplishment of Sub-goals We found that expressions important for maintaining a high translation quality were often commonly 4 http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html 37 included in the reference translations for each testset sentence." ></td>
	<td class="line x" title="130:194	We also found that the expression was also related to the yes/no question assigned to the test-set sentence." ></td>
	<td class="line x" title="131:194	Therefore, we automatically generate yes/no questions in the following steps." ></td>
	<td class="line x" title="132:194	1." ></td>
	<td class="line x" title="133:194	For each test-set sentence, a set of words commonly appearing in the reference translations are extracted." ></td>
	<td class="line x" title="134:194	2." ></td>
	<td class="line x" title="135:194	For each combination of n words in the set of words extracted in the first step, skip word n-grams commonly appearing in the reference translations in the same word order are selected as a set of common skip word n-grams." ></td>
	<td class="line x" title="136:194	3." ></td>
	<td class="line x" title="137:194	For each test-set sentence, the sub-goal is defined as the yes/no question Are all of the common skip word n-grams included in the translation? If no common skip word n-grams are found, the yes/no question is not generated." ></td>
	<td class="line x" title="138:194	The answer to the yes/no question is determined to be yes if all of the common skip word n-grams are included in a translation." ></td>
	<td class="line x" title="139:194	Otherwise, the answer is determined to be no." ></td>
	<td class="line x" title="140:194	This scheme assigns greater weight to important phrases that should be included in the translation to maintain a high translation quality." ></td>
	<td class="line x" title="141:194	Our observation is that those important phrases are often common between human translations." ></td>
	<td class="line x" title="142:194	A similar scheme was proposed by Babych and Hartley (Babych and Hartley, 2004) for BLEU." ></td>
	<td class="line x" title="143:194	In their scheme, greater weight is assigned to components that are salient throughout the document." ></td>
	<td class="line x" title="144:194	Therefore, their scheme focuses on global context while our scheme focuses on local context." ></td>
	<td class="line x" title="145:194	We believe that the two schemes are complementary to each other." ></td>
	<td class="line x" title="146:194	4 Experiments and Discussion In our experiments, the translation results of three MT systems and their subjective evaluation results were used as a development set for constructing the patterns described in Section 3.2 and for tuning the parameters  S i,  Q j,  Q prime j, and  epsilon1 in Eq." ></td>
	<td class="line x" title="147:194	(1)." ></td>
	<td class="line x" title="148:194	The translations and evaluation results of the remaining two MT systems were used as an evaluation set for testing." ></td>
	<td class="line x" title="149:194	In the development set, each test-set sentence has at least one question, at least one reference translation, three MT results, and subjective evaluation results of the three MT results." ></td>
	<td class="line x" title="150:194	The patterns for determining yes/no answers were manually constructed for the questions assigned to the 769 test-set sentences." ></td>
	<td class="line x" title="151:194	There were 917 questions assigned to them." ></td>
	<td class="line x" title="152:194	Among them, the patterns could be constructed for 898 questions assigned to 767 test-set sentences." ></td>
	<td class="line x" title="153:194	The remaining 19 questions were skipped because making simple patterns as described in Section 3.2 was difficult; for example, one of the questions was Is the whole sentence translated into one sentence?." ></td>
	<td class="line x" title="154:194	The yes/no answer determination accuracies obtained by using the patterns are shown in Table 5." ></td>
	<td class="line x" title="155:194	Table 5: Results of yes/no answer determination." ></td>
	<td class="line x" title="156:194	Test set Accuracy Development 97.6% (2,629/2,694) Evaluation 82.8% (1,487/1,796) We investigated the correlation between the evaluation score, A in Eq." ></td>
	<td class="line x" title="157:194	(1) and the subjective evaluations, fluency and adequacy, for the 769 test-set sentences." ></td>
	<td class="line x" title="158:194	First, to maximize the correlation coefficients between the evaluation score, A, and the human subjective evaluations, fluency and adequacy, the optimal values of  S i,  Q j,  Q prime j, and  epsilon1 in Eq." ></td>
	<td class="line x" title="159:194	(1) were investigated using the development set within a framework of multiple linear regression modeling (Draper and Smith, 1981)." ></td>
	<td class="line x" title="160:194	Then, the correlation coefficients were investigated by using the optimal value set." ></td>
	<td class="line x" title="161:194	The results are shown in Table 6, 7, and 8." ></td>
	<td class="line x" title="162:194	In these tables, Conventional method indicates the correlation coefficients obtained when A was calculated by using only similarities S i . Conventional method (combination) is a combination of existing automatic evaluation methods from the literature." ></td>
	<td class="line x" title="163:194	Our method (automatic) indicates the correlation coefficients obtained when the results of the automatic determination of yes/no answers were used to calculate Q j and Q prime j in Eq." ></td>
	<td class="line x" title="164:194	(1)." ></td>
	<td class="line x" title="165:194	For the 19 questions for which the patterns could not be constructed, Q j was set at 0." ></td>
	<td class="line x" title="166:194	Our method (full automatic) indicates the correlation coefficients obtained when the results of the automatic sub-goal generation and determination of rate of accomplish38 Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy." ></td>
	<td class="line x" title="167:194	(A reference translation is used to calculate S i .) Method fluency adequacy Development set Evaluation set Development set Evaluation set Conventional method (WER) 0.43 0.48 0.42 0.48 Conventional method (combination) 0.52 0.51 0.49 0.47 Our method (automatic) 0.90 0.59 0.89 0.62 Our method (upper bound) 0.90 0.62 0.90 0.68 Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy." ></td>
	<td class="line x" title="168:194	(Three reference translations are used to calculate S i .) Method fluency adequacy Development set Evaluation set Development set Evaluation set Conventional method (WER) 0.47 0.51 0.45 0.51 Conventional method (combination) 0.54 0.54 0.51 0.52 Our method (automatic) 0.90 0.60 0.90 0.64 Our method (full automatic) 0.85 0.58 0.84 0.60 Our method (upper bound) 0.90 0.62 0.90 0.69 Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy." ></td>
	<td class="line x" title="169:194	(Five reference translations are used to calculate S i .) Method fluency adequacy Development set Evaluation set Development set Evaluation set Conventional method (WER) 0.49 0.53 0.46 0.53 Conventional method (combination) 0.56 0.56 0.52 0.54 Our method (automatic) 0.90 0.60 0.90 0.63 Our method (full automatic) 0.86 0.59 0.85 0.60 Our method (upper bound) 0.91 0.63 0.90 0.69 In these tables,  indicates significance at the 5% or less significance level." ></td>
	<td class="line x" title="170:194	ment of sub-goals were used to calculate Q j and Q prime j in Eq." ></td>
	<td class="line x" title="171:194	(1)." ></td>
	<td class="line x" title="172:194	Skip word trigrams, skip word bigrams, and skip word unigrams were used for generating the sub-goals according to our preliminary experiments." ></td>
	<td class="line x" title="173:194	Our method (upper bound) indicates the correlation coefficients obtained when human judgments on the questions were used to calculate Q j and Q prime j . As shown in Table 6, 7, and 8, our methods significantly outperform the conventional methods from literature." ></td>
	<td class="line x" title="174:194	Note that WER outperformed other individual measures like BLEU and NIST in our experiments, and the combination of existing automatic evaluation methods from the literature outperformed individual lexical similarity measures by themselves in almost all cases." ></td>
	<td class="line x" title="175:194	The differences between the correlation coefficients obtained using our method and the conventional methods are statistically significant at the 5% or less significance level for fluency and adequacy, even if the number of reference translations increases, except in three cases shown in Table 7 and 8." ></td>
	<td class="line x" title="176:194	This indicates that considering the rate of accomplishment of sub-goals to automatically evaluate the quality of each translation is useful, especially when the number of reference translations is small." ></td>
	<td class="line x" title="177:194	The differences between the correlation coefficients obtained using two automatic methods are not significant." ></td>
	<td class="line x" title="178:194	These results indicate that we can reduce the development cost for constructing sub-goals." ></td>
	<td class="line x" title="179:194	However, there are still significant gaps between the correlation coefficients obtained using a fully automatic method and upper bounds." ></td>
	<td class="line x" title="180:194	These gaps indicate that we need further improvement in automatic sub-goal generation and automatic estimation of rate of accomplishment of sub-goals, which is our future work." ></td>
	<td class="line x" title="181:194	Human judgments of adequacy and fluency are known to be noisy, with varying levels of intercoder agreement." ></td>
	<td class="line x" title="182:194	Recent work has tended to apply crossjudge normalization to address this issue (Blatz et al. , 2003)." ></td>
	<td class="line x" title="183:194	We would like to evaluate against the normalized data in the future." ></td>
	<td class="line x" title="184:194	39 5 Conclusion and Future Work We demonstrated that the quality of a translated sentence can be evaluated more appropriately than by using conventional methods." ></td>
	<td class="line x" title="185:194	That was demonstrated by constructing a test set where the conditions that should be satisfied to maintain a high translation quality are assigned to each test-set sentence in the form of a question, by developing a system that determines an answer to each question, and by combining a measure based on the questions and conventional measures." ></td>
	<td class="line x" title="186:194	We also presented a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals." ></td>
	<td class="line x" title="187:194	Promising results were obtained." ></td>
	<td class="line x" title="188:194	In the near future, we would like to expand the test set to improve the upper bound obtained by our method." ></td>
	<td class="line x" title="189:194	We are also planning to expand the method and improve the accuracy of the automatic sub-goal generation and determination of the rate of accomplishment of sub-goals." ></td>
	<td class="line x" title="190:194	The sub-goals of a given sentence should be generated by considering the complexity of the sentence and the alignment information between the original source-language sentence and its translation." ></td>
	<td class="line x" title="191:194	Further advanced generation and estimation would give us information about the erroneous parts of MT results and their quality." ></td>
	<td class="line x" title="192:194	We believe that future research would allow us to develop high-quality MT systems by tuning the system parameters based on the automatic MT evaluation measures." ></td>
	<td class="line x" title="193:194	Acknowledgments The guideline for expanding the test set is based on that constructed by the Technical Research Committee of the AAMT (Asia-Pacific Association for Machine Translation) The authors would like to thank the committee members, especially, Mr. Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro, Mr. Masaru Fuji, and Ms. Yoshiko Matsukawa for their cooperation." ></td>
	<td class="line x" title="194:194	This research is partially supported by special coordination funds for promoting science and technology." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N07-1006
Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation
Liu, Ding;Gildea, Daniel;"></td>
	<td class="line x" title="1:159	Proceedings of NAACL HLT 2007, pages 4148, Rochester, NY, April 2007." ></td>
	<td class="line x" title="2:159	c2007 Association for Computational Linguistics Source-Language Features and Maximum Correlation Training for Machine Translation Evaluation Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester, NY 14627 Abstract We propose three new features for MT evaluation: source-sentence constrained n-gram precision, source-sentence reordering metrics, and discriminative unigram precision, as well as a method of learning linear feature weights to directly maximize correlation with human judgments." ></td>
	<td class="line x" title="3:159	By aligning both the hypothesis and the reference with the sourcelanguage sentence, we achieve better correlation with human judgments than previously proposed metrics." ></td>
	<td class="line x" title="4:159	We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework." ></td>
	<td class="line x" title="5:159	1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence." ></td>
	<td class="line x" title="6:159	The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al. , 2002), but does not perform as well on sentence-level evaluation (Blatz et al. , 2003)." ></td>
	<td class="line x" title="7:159	Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types:  Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006)." ></td>
	<td class="line x" title="8:159	Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002)." ></td>
	<td class="line x" title="9:159	 Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005)." ></td>
	<td class="line x" title="10:159	Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs." ></td>
	<td class="line oc" title="11:159	 Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="12:159	Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006)." ></td>
	<td class="line x" title="13:159	 Combination of metrics based on machine learning." ></td>
	<td class="line x" title="14:159	Kulesza and Shieber (2004) used SVMs to combine several metrics." ></td>
	<td class="line x" title="15:159	Their method is based on the assumption that higher classification accuracy in discriminating humanfrom machine-generated translations will yield closer correlation with human judgment." ></td>
	<td class="line x" title="16:159	This assumption may not always hold, particularly when classification is difficult." ></td>
	<td class="line x" title="17:159	Lita et al.(2005) proposed a log-linear model to combine features, but they only did preliminary experiments based on 2 features." ></td>
	<td class="line x" title="19:159	Following the track of previous work, to improve evaluation performance, one could either propose new metrics, or find more effective ways to combine the metrics." ></td>
	<td class="line x" title="20:159	We explore both approaches." ></td>
	<td class="line x" title="21:159	Much work has been done on computing MT scores based 41 on the pair of MT output/reference, and we aim to investigate whether some other information could be used in the MT evaluation, such as source sentences." ></td>
	<td class="line x" title="22:159	We propose two types of source-sentence related features as well as a feature based on part of speech." ></td>
	<td class="line x" title="23:159	The three new types of feature can be summarized as follows:  Source-sentence constrained n-gram precision." ></td>
	<td class="line x" title="24:159	Overlapping n-grams between an MT hypothesis and its references do not necessarily indicate correct translation segments, since they could correspond to different parts of the source sentence." ></td>
	<td class="line x" title="25:159	Thus our constrained n-gram precision counts only overlapping n-grams in MT hypothesis and reference which are aligned to the same words in the source sentences." ></td>
	<td class="line x" title="26:159	 Source-sentence reordering agreement." ></td>
	<td class="line x" title="27:159	With the alignment information, we can compare the reorderings of the source sentence in the MT hypothesis and in its references." ></td>
	<td class="line x" title="28:159	Such comparison only considers the aligned positions of the source words in MT hypothesis and references, and thus is oriented towards evaluating the sentence structure." ></td>
	<td class="line x" title="29:159	 Discriminative unigram precision." ></td>
	<td class="line x" title="30:159	We divide the normal n-gram precision into many subprecisions according to their part of speech (POS)." ></td>
	<td class="line x" title="31:159	The division gives us flexibility to train the weights of each sub-precision in frameworks such as SVM and Maximum Correlation Training, which will be introduced later." ></td>
	<td class="line x" title="32:159	The motivation behind such differentiation is that different sub-precisions should have different importance in MT evaluation, e.g., subprecision of nouns, verbs, and adjectives should be important for evaluating adequacy, and sub-precision in determiners and conjunctions should mean more in evaluating fluency." ></td>
	<td class="line x" title="33:159	Along the direction of feature combination, since indirect weight training using SVMs, based on reducing classification error, cannot always yield good performance, we train the weights by directly optimizing the evaluation performance, i.e., maximizing the correlation with the human judgment." ></td>
	<td class="line x" title="34:159	This type of direct optimization is known as Minimum Error Rate Training (Och, 2003) in the MT community, and is an essential component in building the stateof-art MT systems." ></td>
	<td class="line x" title="35:159	It would seem logical to apply similar methods to MT evaluation." ></td>
	<td class="line x" title="36:159	What is more, Maximum Correlation Training (MCT) enables us to train the weights based on human fluency judgments and adequacy judgments respectively, and thus makes it possible to make a fluency-oriented or adequacy-oriented metric." ></td>
	<td class="line x" title="37:159	It surpasses previous MT metrics approach, where a a single metric evaluates both fluency and adequacy." ></td>
	<td class="line x" title="38:159	The rest of the paper is organized as follows: Section 2 gives a brief recap of n-gram precision-based metrics and introduces our three extensions to them; Section 3 introduces MCT for MT evaluation; Section 4 describes the experimental results, and Section 5 gives our conclusion." ></td>
	<td class="line x" title="39:159	2 Three New Features for MT Evaluation Since our source-sentence constrained n-gram precision and discriminative unigram precision are both derived from the normal n-gram precision, it is worth describing the original n-gram precision metric, BLEU (Papineni et al. , 2002)." ></td>
	<td class="line x" title="40:159	For every MT hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty." ></td>
	<td class="line x" title="41:159	The formula for computing BLEU is shown below: BLEU = BPN NX n=1 P C P ngramC Countclip(ngram)P C P ngramC Count(ngram) where C denotes the set of MT hypotheses." ></td>
	<td class="line x" title="42:159	Countclip(ngram) denotes the clipped number of n-grams in the candidates which also appear in the references." ></td>
	<td class="line x" title="43:159	BP in the above formula denotes the brevity penalty, which is set to 1 if the accumulated length of the MT outputs is longer than the arithmetic mean of the accumulated length of the references, and otherwise is set to the ratio of the two." ></td>
	<td class="line x" title="44:159	For sentence-level evaluation with BLEU, we compute the score based on each pair of MT hypothesis/reference." ></td>
	<td class="line x" title="45:159	Later approaches, as described in Section 1, use different ways to manipulate the morphological similarity between the MT hypothesis and its references." ></td>
	<td class="line x" title="46:159	Most of them, except NIST, consider the words in MT hypothesis as the same, i.e., as long as the words in MT hypothesis appear in the references, 42 they make no difference to the metrics.1 NIST computes the n-grams weights as the logarithm of the ratio of the n-gram frequency and its one word lower n-gram frequency." ></td>
	<td class="line x" title="47:159	From our experiments, NIST is not generally better than BLEU, and the reason, we conjecture, is that it differentiates the n-grams too much and the frequency estimated upon the evaluation corpus is not always reliable." ></td>
	<td class="line x" title="48:159	In this section we will describe two other strategies for differentiating the n-grams, one of which uses the alignments with the source sentence as a further constraint, while the other differentiates the n-gram precisions according to POS." ></td>
	<td class="line x" title="49:159	2.1 Source-sentence Constrained N-gram Precision The quality of an MT sentence should be independent of the source sentence given the reference translation, but considering that current metrics are all based on shallow morphological similarity of the MT outputs and the reference, without really understanding the meaning in both sides, the source sentences could have some useful information in differentiating the MT outputs." ></td>
	<td class="line x" title="50:159	Consider the ChineseEnglish translation example below: Source: wo bu neng zhe me zuo Hypothesis: I must hardly not do this ~eference: I must not do this It is clear that the word not in the MT output cannot co-exist with the word hardly while maintaining the meaning of the source sentence." ></td>
	<td class="line x" title="51:159	None of the metrics mentioned above can prevent not from being counted in the evaluation, due to the simple reason that they only compute shallow morphological similarity." ></td>
	<td class="line x" title="52:159	Then how could the source sentence help in the example?" ></td>
	<td class="line x" title="53:159	If we reveal the alignment of the source sentence with both the reference and the MT output, the Chinese word bu neng would be aligned to must not in the reference and must hardly in the MT output respectively, leaving the word not in the MT output not aligned to any word in the source sentence." ></td>
	<td class="line o" title="54:159	Therefore, if we can somehow find the alignments between the source sentence and the reference/MT output, we could be smarter in selecting the overlapping words to be counted in the 1In metrics such as METEOR, ROUGE, SIA (Liu and Gildea, 2006), the positions of words do make difference, but it has nothing to do with the word itself." ></td>
	<td class="line x" title="55:159	for all n-grams wi,,wi+n1 in MT hypothesis do max val = 0; for all reference sentences do for all n-grams rj,,rj+n1 in current reference sentence do val=0; for k=0; k  n-1; k ++ do if wi+k equals rj+k AND MTaligni equals REFalignj then val += 1n; if val  max val then max val = val; hit count += max val; return hit countMThypothesislength length penalty; Figure 1: Algorithm for Computing Sourcesentence Constrained n-gram Precision metric: only select the words which are aligned to the same source words." ></td>
	<td class="line x" title="56:159	Now the question comes to how to find the alignment of source sentence and MT hypothesis/references, since the evaluation data set usually does not contain alignment information." ></td>
	<td class="line x" title="57:159	Our approach uses GIZA++2 to construct the manyto-one alignments between source sentences and the MT hypothesis/references respectively.3 GIZA++ could generate many-to-one alignments either from source sentence to the MT hypothesis, in which case every word in MT hypothesis is aligned to a set of (or none) words in the source sentence, or from the reverse direction, in which case every word in MT hypothesis is aligned to exactly one word (or none) word in the source sentence." ></td>
	<td class="line x" title="58:159	In either case, using MTaligni and REFaligni to denote the positions of the words in the source sentences which are aligned to a word in the MT hypothesis and a word in the reference respectively, the algorithm for computing source-sentence constrained n-gram precision of length n is described in Figure 1." ></td>
	<td class="line x" title="59:159	Since source-sentence constrained n-gram precision (SSCN) is a precision-based metric, the vari2GIZA++ is available at http://www.fjoch.com/GIZA++.html 3More refined alignments could be got for source-hypothesis from the MT system, and for source-references by using manual proof-reading after the automatic alignment." ></td>
	<td class="line x" title="60:159	Doing so, however, requires the MT systems cooperation and some costly human labor." ></td>
	<td class="line x" title="61:159	43 able length penalty is used to avoid assigning a short MT hypothesis a high score, and is computed in the same way as BLEU." ></td>
	<td class="line x" title="62:159	Note that in the algorithm for computing the precision of n-grams longer than one word, not all words in the n-grams should satisfy the source-sentence constraint." ></td>
	<td class="line x" title="63:159	The reason is that the high order n-grams are already very sparse in the sentence-level evaluation." ></td>
	<td class="line x" title="64:159	To differentiate the SSCNs based on the source-to-MT/Ref (many-toone) alignments and the MT/Ref-to-source (manyto-one) alignments, we use SSCN1 and SSCN2 to denote them respectively." ></td>
	<td class="line x" title="65:159	Naturally, we could combine the constraint in SSCN1 and SSCN2 by either taking their union (the combined constrained is satisfied if either one is satisfied) or intersecting them (the combined constrained is satisfied if both constraints are satisfied)." ></td>
	<td class="line x" title="66:159	We use SSCN u and SSCN i to denote the SSCN based on unioned constraints and intersected constraints respectively." ></td>
	<td class="line x" title="67:159	We could also apply the stochastic word mapping proposed in SIA (Liu and Gildea, 2006) to replace the hard word matching in Figure 1, and the corresponding metrics are denoted as pSSCN1, pSSCN2, pSSCN u, pSSCN i, with the suffixed number denoting different constraints." ></td>
	<td class="line x" title="68:159	2.2 Metrics Based on Source Word Reordering Most previous MT metrics concentrate on the cooccurrence of the MT hypothesis words in the references." ></td>
	<td class="line x" title="69:159	Our metrics based on source sentence reorderings, on the contrary, do not take words identities into account, but rather compute how similarly the source words are reordered in the MT output and the references." ></td>
	<td class="line x" title="70:159	For simplicity, we only consider the pairwise reordering similarity." ></td>
	<td class="line x" title="71:159	That is, for the source word pair wi and wj, if their aligned positions in the MT hypothesis and a reference are in the same order, we call it a consistent word pair." ></td>
	<td class="line x" title="72:159	Our pairwise reordering similarity (PRS) metric computes the fraction of the consistent word pairs in the source sentence." ></td>
	<td class="line x" title="73:159	Figure 2 gives the formal description of PRS." ></td>
	<td class="line x" title="74:159	SrcMTi and SrcRefk,i denote the aligned position of source word wi in the MT hypothesis and the kth reference respectively, and N denotes the length of the source sentence." ></td>
	<td class="line x" title="75:159	Another criterion for evaluating the reordering of the source sentence in the MT hypothesis is how well it maintains the original word order in the for all word pair wi,wj in the source sentence such that i < j do for all reference sentences rk do if (SrcMTi == SrcMTj AND SrcRefk,i == SrcRefk,j) OR ((SrcMTi  SrcMTj)  (SrcRefk,i  SrcRefk,j) > 0) then count+ +; break; return 2countN(N1); Figure 2: Compute Pairwise Reordering Similarity for all word pair wi,wj in the source sentence, such that i < j do if SrcMTi SrcMTj < 0 then count+ +; return 2countN(N1); Figure 3: Compute Source Sentence Monotonic Reordering Ratio source sentence." ></td>
	<td class="line x" title="76:159	We know that most of the time, the alignment of the source sentence and the MT hypothesis is monotonic." ></td>
	<td class="line x" title="77:159	This idea leads to the metric of monotonic pairwise ratio (MPR), which computes the fraction of the source word pairs whose aligned positions in the MT hypothesis are of the same order." ></td>
	<td class="line x" title="78:159	It is described in Figure 3." ></td>
	<td class="line x" title="79:159	2.3 Discriminative Unigram Precision Based on POS The Discriminative Unigram Precision Based on POS (DUPP) decomposes the normal unigram precision into many sub-precisions according to their POS." ></td>
	<td class="line x" title="80:159	The algorithm is described in Figure 4." ></td>
	<td class="line x" title="81:159	These sub-precisions by themselves carry the same information as standard unigram precision, but they provide us the opportunity to make a better combined metric than the normal unigram precision with MCT, which will be introduced in next section." ></td>
	<td class="line x" title="82:159	for all unigram s in the MT hypothesis do if s is found in any of the references then countPOS(s) += 1 precisionx = countxmt hypothesis length x  POS Figure 4: Compute DUPP for N-gram with length n 44 Such division could in theory be generalized to work with higher order n-grams, but doing so would make the n-grams in each POS set much more sparse." ></td>
	<td class="line x" title="83:159	The preprocessing step for the metric is tagging both the MT hypothesis and the references with POS." ></td>
	<td class="line x" title="84:159	It might elicit some worries about the robustness of the POS tagger on the noise-containing MT hypothesis." ></td>
	<td class="line x" title="85:159	This should not be a problem for two reasons." ></td>
	<td class="line x" title="86:159	First, compared with other preprocessing steps like parsing, POS tagging is easier and has higher accuracy." ></td>
	<td class="line x" title="87:159	Second, because the counts for each POS are accumulated, the correctness of a single words POS will not affect the result very much." ></td>
	<td class="line x" title="88:159	3 Maximum Correlation Training for Machine Translation Evaluation Maximum Correlation Training (MCT) is an instance of the general approach of directly optimizing the objective function by which a model will ultimately be evaluated." ></td>
	<td class="line x" title="89:159	In our case, the model is the linear combination of the component metrics, the parameters are the weights for each component metric, and the objective function is the Pearsons correlation of the combined metric and the human judgments." ></td>
	<td class="line x" title="90:159	The reason to use the linear combination of the metrics is that the component metrics are usually of the same or similar order of magnitude, and it makes the optimization problem easy to solve." ></td>
	<td class="line x" title="91:159	Using w to denote the weights, and m to denote the component metrics, the combined metric x is computed as: x(w) = summationdisplay j wjmj (1) Using hi and x(w)i denote the human judgment and combined metric for a sentence respectively, and N denote the number of sentences in the evaluation set, the objective function is then computed as: Pearson(X(w),H) = PN i=1 x(w)ihi  PN i=1 x(w)i PN i=1 hi Nq (PNi=1 x(w)2i  ( PN i=1 x(w)i)2 N )( PN i=1 h 2 i  (PNi=1 hi)2 N ) Now our task is to find the weights for each component metric so that the correlation of the combined metric with the human judgment is maximized." ></td>
	<td class="line x" title="92:159	It can be formulated as: w = argmax w Pearson(X(w),H) (2) The function Pearson(X(w),H) is differentiable with respect to the vector w, and we compute this derivative analytically and perform gradient ascent." ></td>
	<td class="line x" title="93:159	Our objective function not always convex (one can easily create a non-convex function by setting the human judgments and individual metrics to some particular value)." ></td>
	<td class="line x" title="94:159	Thus there is no guarantee that, starting from a random w, we will get the globally optimal w using optimization techniques such as gradient ascent." ></td>
	<td class="line x" title="95:159	The easiest way to avoid ending up with a bad local optimum to run gradient ascent by starting from different random points." ></td>
	<td class="line x" title="96:159	In our experiments, the difference in each run is very small, i.e., by starting from different random initial values of w, we end up with, not the same, but very similar values for Pearsons correlation." ></td>
	<td class="line x" title="97:159	4 Experiments Experiments were conducted to evaluate the performance of the new metrics proposed in this paper, as well as the MCT combination framework." ></td>
	<td class="line x" title="98:159	The data for the experiments are from the MT evaluation workshop at ACL05." ></td>
	<td class="line x" title="99:159	There are seven sets of MT outputs (E09 E11 E12 E14 E15 E17 E22), each of which contains 919 English sentences translated from the same set of Chinese sentences." ></td>
	<td class="line x" title="100:159	There are four references (E01, E02, E03, E04) and two sets of human scores for each MT hypothesis." ></td>
	<td class="line x" title="101:159	Each human score set contains a fluency and an adequacy score, both of which range from 1 to 5." ></td>
	<td class="line x" title="102:159	We create a set of overall human scores by averaging the human fluency and adequacy scores." ></td>
	<td class="line x" title="103:159	For evaluating the automatic metrics, we compute the Pearsons correlation of the automatic scores and the averaged human scores (over the two sets of available human scores), for overall score, fluency, and adequacy." ></td>
	<td class="line x" title="104:159	The alignment between the source sentences and the MT hypothesis/references is computed by GIZA++, which is trained on the combined corpus of the evaluation data and a parallel corpus of Chinese-English newswire text." ></td>
	<td class="line x" title="105:159	The parallel newswire corpus contains around 75,000 sentence pairs, 2,600,000 English words and 2,200,000 Chinese words." ></td>
	<td class="line x" title="106:159	The 45 stochastic word mapping is trained on a FrenchEnglish parallel corpus containing 700,000 sentence pairs, and, following Liu and Gildea (2005), we only keep the top 100 most similar words for each English word." ></td>
	<td class="line x" title="107:159	4.1 Performance of the Individual Metrics To evaluate our source-sentence based metrics, they are used to evaluate the 7 MT outputs, with the 4 sets of human references." ></td>
	<td class="line x" title="108:159	The sentence-level Pearsons correlation with human judgment is computed for each MT output, and the averaged results are shown in Table 1." ></td>
	<td class="line o" title="109:159	As a comparison, we also show the results of BLEU, NIST, METEOR, ROUGE, WER, and HWCM." ></td>
	<td class="line o" title="110:159	For METEOR and ROUGE, WORDNET and PORTER-STEMMER are enabled, and for SIA, the decay factor is set to 0.6." ></td>
	<td class="line x" title="111:159	The number in brackets, for BLEU, shows the n-gram length it counts up to, and for SSCN, shows the length of the n-gram it uses." ></td>
	<td class="line x" title="112:159	In the table, the top 3 results in each column are marked bold and the best result is also underlined." ></td>
	<td class="line x" title="113:159	The results show that the SSCN2 metrics are better than the SSCN1 metrics in adequacy and overall score." ></td>
	<td class="line x" title="114:159	This is understandable since what SSCN metrics need is which words in the source sentence are aligned to an n-gram in the MT hypothesis/references." ></td>
	<td class="line x" title="115:159	This is directly modeled in the alignment used in SSCN2." ></td>
	<td class="line x" title="116:159	Though we could also get such information from the reverse alignment, as in SSCN1, it is rather an indirect way and could contain more noise." ></td>
	<td class="line x" title="117:159	It is interesting that SSCN1 gets better fluency evaluation results than SSCN2." ></td>
	<td class="line x" title="118:159	The SSCN metrics with the unioned constraint, SSCN u, by combining the strength of SSCN1 and SSCN2, get even better results in all three aspects." ></td>
	<td class="line x" title="119:159	We can see that SSCN metrics, even without stochastic word mapping, get significantly better results than their relatives, BLEU, which indicates the source sentence constraints do make a difference." ></td>
	<td class="line p" title="120:159	SSCN2 and SSCN u are also competitive to the state-of-art MT metrics such as METEOR and SIA." ></td>
	<td class="line x" title="121:159	The best SSCN metric, pSSCN u(2), achieves the best performance among all the testing metrics in overall and adequacy, and the second best performance in fluency, which is just a little bit worse than the best fluency metric SIA." ></td>
	<td class="line o" title="122:159	The two reordering based metrics, PRS and MPR, are not as good as the other testing metrics, in terms Fluency Adequacy Overall ROUGE W 24.8 27.8 29.0 ROUGE S 19.7 30.9 28.5 METEOR 24.4 34.8 33.1 SIA 26.8 32.1 32.6 NIST 1 09.6 22.6 18.5 WER 22.5 27.5 27.7 PRS 14.2 19.4 18.7 MPR 11.0 18.2 16.5 BLEU(1) 18.4 29.6 27.0 BLEU(2) 20.4 31.1 28.9 BLEU(3) 20.7 30.4 28.6 HWCM(2) 22.1 30.3 29.2 SSCN1(1) 24.2 29.6 29.8 SSCN2(1) 22.9 33.0 31.3 SSCN u(1) 23.8 34.2 32.5 SSCN i(1) 23.4 28.0 28.5 pSSCN1(1) 24.9 30.2 30.6 pSSCN2(1) 23.8 34.0 32.4 pSSCN u(1) 24.5 34.6 33.1 pSSCN i(1) 24.1 28.8 29.3 SSCN1(2) 24.0 29.6 29.7 SSCN2(2) 23.3 31.5 31.8 SSCN u(2) 24.1 34.5 32.8 SSCN i(2) 23.1 27.8 28.2 pSSCN1(2) 24.9 30.2 30.6 pSSCN2(2) 24.3 34.4 32.8 pSSCN u(2) 25.2 35.4 33.9 pSSCN i(2) 23.9 28.7 29.1 Table 1: Performance of Component Metrics of the individual performance." ></td>
	<td class="line x" title="123:159	It should not be surprising since they are totally different kind of metrics, which do not count the overlapping n-grams, but the consistent/monotonic word pair reorderings." ></td>
	<td class="line x" title="124:159	As long as they capture some property of the MT hypothesis, they might be able to boost the performance of the combined metric under the MCT framework." ></td>
	<td class="line x" title="125:159	4.2 Performance of the Combined Metrics To test how well MCT works, the following scheme is used: each set of MT outputs is evaluated by MCT, which is trained on the other 6 sets of MT outputs and their corresponding human judgment; the averaged correlation of the 7 sets of MT outputs with the human judgment is taken as the final result." ></td>
	<td class="line x" title="126:159	4.2.1 Discriminative Unigram Precision based on POS We first use MCT to combine the discriminative unigram precisions." ></td>
	<td class="line x" title="127:159	To reduce the sparseness of the unigrams of each POS, we do not use the original POS set, but use a generalized one by combining 46 all POS tags with the same first letter (e.g. , the different verb forms such as VBN, VBD, and VBZ are transformed to V)." ></td>
	<td class="line x" title="128:159	The unified POS set contains 23 POS tags." ></td>
	<td class="line x" title="129:159	To give a fair comparison of DUPP with BLEU, the length penalty is also added into it as a component." ></td>
	<td class="line x" title="130:159	Results are shown in Table 2." ></td>
	<td class="line x" title="131:159	DUPP f, DUPP a and DUPP o denote DUPP trained on human fluency, adequacy and overall judgment respectively." ></td>
	<td class="line x" title="132:159	This shows that DUPP achieves obvious improvement over BLEU, with only the unigrams and length penalty, and DUPP f/ a/ o gets the best result in fluency/adequacy/overall evaluation, showing that MCT is able to make a fluencyor adequacyoriented metric." ></td>
	<td class="line x" title="133:159	4.2.2 Putting It All Together The most interesting question in this paper is, with all these metrics, how well we can do in the MT evaluation." ></td>
	<td class="line x" title="134:159	To answer the question, we put all the metrics described into the MCT framework and use the combined metric to evaluate the 7 MT outputs." ></td>
	<td class="line x" title="135:159	Note that to speed up the training process, we do not directly use 24 DUPP components, instead, we use the 3 combined DUPP metrics." ></td>
	<td class="line x" title="136:159	With the metrics shown in Table 1, we then have in total 31 metrics." ></td>
	<td class="line x" title="137:159	Table 2 shows the results of the final combined metric." ></td>
	<td class="line x" title="138:159	We can see that MCT trained on fluency, adequacy and overall human judgment get the best results among all the testing metrics in fluency, adequacy and overall evaluation respectively." ></td>
	<td class="line x" title="139:159	We did a t-test with Fishers z transform for the combined results and the individual results to see how significant the difference is. The combined results in adequacy and overall are significantly better at 99.5% confidence than the best results of the individual metrics (pSSCN u(2)), and the combined result in fluency is significantly better at 96.9% confidence than the best individual metric (SIA)." ></td>
	<td class="line x" title="140:159	We also give the upper bound for each evaluation aspect by training MCT on the testing MT outputs, e.g., we train MCT on E09 and then use it to evaluate E09." ></td>
	<td class="line x" title="141:159	The upperbound is the best we can do with the MCT based on linear combination." ></td>
	<td class="line x" title="142:159	Another linear framework, Classification SVM (CSVM),4 is also used to combine the testing metrics except DUPP." ></td>
	<td class="line x" title="143:159	Since DUPP is based on MCT, to make a neat comparison, we rule out DUPP in the experiments with CSVM." ></td>
	<td class="line x" title="144:159	The 4http://svmlight.joachims.org/ Fluency Adequacy Overall DUPP f 23.6 30.1 30.1 DUPP a 22.1 32.9 30.9 DUPP o 23.2 32.8 31.3 MCT f(4) 30.3 36.7 37.2 MCT a(4) 28.0 38.9 37.4 MCT o(4) 29.4 38.8 38.0 Upper bound 35.3 43.4 42.2 MCT f(3) 29.2 34.7 35.3 MCT a(3) 27.4 38.4 36.8 MCT o(3) 28.8 38.0 37.2 CSVM(3) 27.3 36.9 35.5 Table 2: Combination of the Testing Metrics testing scheme is the same as MCT, except that we only use 3 references for each MT hypothesis, and the positive samples for training CSVM are computed as the scores of one of the 4 references based on the other 3 references." ></td>
	<td class="line x" title="145:159	The slack parameter of CSVM is chosen so as to maximize the classification accuracy of a heldout set of 800 negative and 800 positive samples, which are randomly selected from the training set." ></td>
	<td class="line x" title="146:159	The results are shown in Table 2." ></td>
	<td class="line x" title="147:159	We can see that MCT, with the same number of reference sentences, is better than CSVM." ></td>
	<td class="line x" title="148:159	Note that the resources required by MCT and CSVM are different." ></td>
	<td class="line x" title="149:159	MCT uses human judgments to adjust the weights, while CSVM needs extra human references to produce positive training samples." ></td>
	<td class="line x" title="150:159	To have a rough idea of how the component metrics contribute to the final performance of MCT, we incrementally add metrics into the MCT in descending order of their overall evaluation performance, with the results shown in Figure 5." ></td>
	<td class="line x" title="151:159	We can see that the performance improves as the number of metrics increases, in a rough sense." ></td>
	<td class="line o" title="152:159	The major improvement happens in the 3rd, 4th, 9th, 14th, and 30th metrics, which are METEOR, SIA, DUPP a, pSSCN1(1), and PRS." ></td>
	<td class="line x" title="153:159	It is interesting to note that these are not the metrics with the highest individual performance." ></td>
	<td class="line x" title="154:159	Another interesting observation is that there are no two metrics belonging to the same series in the most beneficial metrics, indicating that to get better combined metrics, individual metrics showing different sentence properties are preferred." ></td>
	<td class="line x" title="155:159	5 Conclusion This paper first describes two types of new approaches to MT evaluation, which includes making 47 0 5 10 15 20 25 30 350.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 the number of metrics (o: adequacy, x: fluency, +: overall) correlation with human fluency/overall/adequacy judgements Figure 5: Performance as a Function of the Number of Interpolated Metrics use of source sentences, and discriminating unigram precisions based on POS." ></td>
	<td class="line o" title="156:159	Among all the testing metrics including BLEU, NIST, METEOR, ROUGE, and SIA, our new metric, pSSCN u(2), based on source-sentence constrained bigrams, achieves the best adequacy and overall evaluation results, and the second best result in fluency evaluation." ></td>
	<td class="line x" title="157:159	We further improve the performance by combining the individual metrics under the MCT framework, which is shown to be better than a classification based framework such as SVM." ></td>
	<td class="line x" title="158:159	By examining the contribution of each component metric, we find that metrics showing different properties of a sentence are more likely to make a good combined metric." ></td>
	<td class="line x" title="159:159	Acknowledgments This work was supported by NSF grants IIS-0546554, IIS-0428020, and IIS0325646." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1038
Regression for Sentence-Level MT Evaluation with Pseudo References
Albrecht, Joshua;Hwa, Rebecca;"></td>
	<td class="line x" title="1:203	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 296303, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:203	c2007 Association for Computational Linguistics Regression for Sentence-Level MT Evaluation with Pseudo References Joshua S. Albrecht and Rebecca Hwa Department of Computer Science University of Pittsburgh {jsa8,hwa}@cs.pitt.edu Abstract Many automatic evaluation metrics for machine translation (MT) rely on making comparisons to human translations, a resource that may not always be available." ></td>
	<td class="line x" title="3:203	We present a method for developing sentence-level MT evaluation metrics that do not directly rely on human reference translations." ></td>
	<td class="line x" title="4:203	Our metrics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy (pseudo references)." ></td>
	<td class="line x" title="5:203	Experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances." ></td>
	<td class="line x" title="6:203	1 Introduction Automatic assessment of translation quality is a challenging problem because the evaluation task, at its core, is based on subjective human judgments." ></td>
	<td class="line x" title="7:203	Reference-based metrics such as BLEU (Papineni et al. , 2002) have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source?" ></td>
	<td class="line x" title="8:203	This approach requires the participation of human translators, who provide the gold standard reference sentences." ></td>
	<td class="line x" title="9:203	However, keeping humans in the evaluation loop represents a significant expenditure both in terms of time and resources; therefore it is worthwhile to explore ways of reducing the degree of human involvement." ></td>
	<td class="line x" title="10:203	To this end, Gamon et al.(2005) proposed a learning-based evaluation metric that does not compare against reference translations." ></td>
	<td class="line x" title="12:203	Under a learning framework, the input (i.e. , the sentence to be evaluated) is represented as a set of features." ></td>
	<td class="line x" title="13:203	These are measurements that can be extracted from the input sentence (and may be individual metrics themselves)." ></td>
	<td class="line x" title="14:203	The learning algorithm combines the features to form a model (a composite evaluation metric) that produces the final score for the input." ></td>
	<td class="line x" title="15:203	Without human references, the features in the model proposed by Gamon et al. were primarily language model features and linguistic indicators that could be directly derived from the input sentence alone." ></td>
	<td class="line x" title="16:203	Although their initial results were not competitive with standard reference-based metrics, their studies suggested that a referenceless metric may still provide useful information about translation fluency." ></td>
	<td class="line x" title="17:203	However, a potential pitfall is that systems might game the metric by producing fluent outputs that are not adequate translations of the source." ></td>
	<td class="line x" title="18:203	This paper proposes an alternative approach to evaluate MT outputs without comparing against human references." ></td>
	<td class="line x" title="19:203	While our metrics are also trained, our model consists of different features and is trained under a different learning regime." ></td>
	<td class="line x" title="20:203	Crucially, our model includes features that capture some notions of adequacy by comparing the input against pseudo references: sentences from other MT systems (such as commercial off-the-shelf systems or open sourced research systems)." ></td>
	<td class="line x" title="21:203	To improve fluency judgments, the model also includes features that compare the input against target-language references such as large text corpora and treebanks." ></td>
	<td class="line x" title="22:203	Unlike human translations used by standard reference-based metrics, pseudo references are not 296 gold standards and can be worse than the sentences being evaluated; therefore, these references in-and-of themselves are not necessarily informative enough for MT evaluation." ></td>
	<td class="line x" title="23:203	The main insight of our approach is that through regression, the trained metrics can make more nuanced comparisons between the input and pseudo references." ></td>
	<td class="line x" title="24:203	More specifically, our regression objective is to infer a function that maps a feature vector (which measures an inputs similarity to the pseudo references) to a score that indicates the quality of the input." ></td>
	<td class="line x" title="25:203	This is achieved by optimizing the models output to correlate against a set of training examples, which are translation sentences labeled with quantitative assessments of their quality by human judges." ></td>
	<td class="line x" title="26:203	Although this approach does incur some human effort, it is primarily for the development of training data, which, ideally, can be amortized over a long period of time." ></td>
	<td class="line x" title="27:203	To determine the feasibility of the proposed approach, we conducted empirical studies that compare our trained metrics against standard referencebased metrics." ></td>
	<td class="line x" title="28:203	We report three main findings." ></td>
	<td class="line x" title="29:203	First, pseudo references are informative comparison points." ></td>
	<td class="line x" title="30:203	Experimental results suggest that a regression-trained metric that compares against pseudo references can have higher correlations with human judgments than applying standard metrics with multiple human references." ></td>
	<td class="line x" title="31:203	Second, the learning model that uses both adequacy and fluency features performed the best, with adequacy being the more important factor." ></td>
	<td class="line x" title="32:203	Third, when the pseudo references are multiple MT systems, the regressiontrained metric is predictive even when the input is from a better MT system than those providing the references." ></td>
	<td class="line x" title="33:203	We conjecture that comparing MT outputs against other imperfect translations allows for a more nuanced discrimination of quality." ></td>
	<td class="line x" title="34:203	2 Background and Related Work For a formally organized event, such as the annual MT Evaluation sponsored by National Institute of Standard and Technology (NIST MT Eval), it may be worthwhile to recruit multiple human translators to translate a few hundred sentences for evaluation references." ></td>
	<td class="line x" title="35:203	However, there are situations in which multiple human references are not practically available (e.g. , the source may be of a large quantity, and no human translation exists)." ></td>
	<td class="line x" title="36:203	One such instance is translation quality assurance, in which one wishes to identify poor outputs in a large body of machine translated text automatically for human to post-edit." ></td>
	<td class="line x" title="37:203	Another instance is in day-to-day MT research and development, where new test set with multiple references are also hard to come by." ></td>
	<td class="line x" title="38:203	One could work with previous datasets from events such as the NIST MT Evals, but there is a danger of over-fitting." ></td>
	<td class="line x" title="39:203	One also could extract a single reference from parallel corpora, although it is known that automatic metrics are more reliable when comparing against multiple references." ></td>
	<td class="line x" title="40:203	The aim of this work is to develop a trainable automatic metric for evaluation without human references." ></td>
	<td class="line x" title="41:203	This can be seen as a form of confidence estimation on MT outputs (Blatz et al. , 2003; Ueffing et al. , 2003; Quirk, 2004)." ></td>
	<td class="line x" title="42:203	The main distinction is that confidence estimation is typically performed with a particular system in mind, and may rely on systeminternal information in estimation." ></td>
	<td class="line x" title="43:203	In this study, we draw on only system-independent indicators so that the resulting metric may be more generally applied." ></td>
	<td class="line x" title="44:203	This allows us to have a clearer picture of the contributing factors as they interact with different types of MT systems." ></td>
	<td class="line x" title="45:203	Also relevant is previous work that applied machine learning approaches to MT evaluation, both with human references (Corston-Oliver et al. , 2001; Kulesza and Shieber, 2004; Albrecht and Hwa, 2007; Liu and Gildea, 2007) and without (Gamon et al. , 2005)." ></td>
	<td class="line x" title="46:203	One motivation for the learning approach is the ease of combining multiple criteria." ></td>
	<td class="line x" title="47:203	Literature in translation evaluation reports a myriad of criteria that people use in their judgments, but it is not clear how these factors should be combined mathematically." ></td>
	<td class="line x" title="48:203	Machine learning offers a principled and unified framework to induce a computational model of humans decision process." ></td>
	<td class="line x" title="49:203	Disparate indicators can be encoded as one or more input features, and the learning algorithm tries to find a mapping from input features to a score that quantifies the inputs quality by optimizing the model to match human judgments on training examples." ></td>
	<td class="line x" title="50:203	The framework is attractive because its objective directly captures the goal of MT evaluation: how would a user rate the quality of these translations?" ></td>
	<td class="line x" title="51:203	This work differs from previous approaches in 297 two aspects." ></td>
	<td class="line x" title="52:203	One is the representation of the model; our model treats the metric as a distance measure even though there are no human references." ></td>
	<td class="line x" title="53:203	Another is the training of the model." ></td>
	<td class="line x" title="54:203	More so than when human references are available, regression is central to the success of the approach, as it determines how much we can trust the distance measures against each pseudo reference system." ></td>
	<td class="line x" title="55:203	While our model does not use human references directly, its features are adapted from the following distance-based metrics." ></td>
	<td class="line x" title="56:203	The well-known BLEU (Papineni et al. , 2002) is based on the number of common n-grams between the translation hypothesis and human reference translations of the same sentence." ></td>
	<td class="line o" title="57:203	Metrics such as ROUGE, Head Word Chain (HWC), METEOR, and other recently proposed methods all offer different ways of comparing machine and human translations." ></td>
	<td class="line x" title="58:203	ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a)." ></td>
	<td class="line oc" title="59:203	METEOR uses the Porter stemmer and synonymmatching via WordNet to calculate recall and precision more accurately (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="60:203	The HWC metrics compare dependency and constituency trees for both reference and machine translations (Liu and Gildea, 2005)." ></td>
	<td class="line x" title="61:203	3 MT Evaluation with Pseudo References using Regression Reference-based metrics are typically thought of as measurements of similarity to good translations because human translations are used as references, but in more general terms, they are distance measurements between two sentences." ></td>
	<td class="line x" title="62:203	The distance between a translation hypothesis and an imperfect reference is still somewhat informative." ></td>
	<td class="line x" title="63:203	As a toy example, consider a one-dimensional line segment." ></td>
	<td class="line x" title="64:203	A distance from the end-point uniquely determines the position of a point." ></td>
	<td class="line x" title="65:203	When the reference location is anywhere else on the line segment, a relative distance to the reference does not uniquely specify a location on the line segment." ></td>
	<td class="line x" title="66:203	However, the position of a point can be uniquely determined if we are given its relative distances to two reference locations." ></td>
	<td class="line x" title="67:203	The problem space for MT evaluation, though more complex, is not dissimilar to the toy scenario." ></td>
	<td class="line x" title="68:203	There are two main differences." ></td>
	<td class="line x" title="69:203	First, we do not know the actual distance function  this is what we are trying to learn." ></td>
	<td class="line x" title="70:203	The distance functions we have at our disposal are all heuristic approximations to the true translational distance function." ></td>
	<td class="line x" title="71:203	Second, unlike human references, whose quality value is assumed to be maximum, the quality of a pseudo reference sentence is not known." ></td>
	<td class="line x" title="72:203	In fact, prior to training, we do not even know the quality of the reference systems." ></td>
	<td class="line x" title="73:203	Although the direct way to calibrate a reference system is to evaluate its outputs, this is not practically ideal, since human judgments would be needed each time we wish to incorporate a new reference system." ></td>
	<td class="line x" title="74:203	Our proposed alternative is to calibrate the reference systems against an existing set of human judgments for a range of outputs from different MT systems." ></td>
	<td class="line x" title="75:203	That is, if many of the reference systems outputs are similar to those MT outputs that received low assessments, we conclude this reference system may not be of high quality." ></td>
	<td class="line x" title="76:203	Thus, if a new translation is found to be similar with this reference systems output, it is more likely for the new translation to also be bad." ></td>
	<td class="line x" title="77:203	Both issues of combining evidences from heuristic distances and calibrating the quality of pseudo reference systems can be addressed by a probabilistic learning model." ></td>
	<td class="line x" title="78:203	In particular, we use regression because its problem formulation fits naturally with the objective of MT evaluations." ></td>
	<td class="line x" title="79:203	In regression learning, we are interested in approximating a function f that maps a multi-dimensional input vector, x, to a continuous real value, y, such that the error over a set of m training examples, {(x1,y1),,(xm,ym)}, is minimized according to a loss function." ></td>
	<td class="line x" title="80:203	In the context of MT evaluation, y is the true quantitative measure of translation quality for an input sentence1." ></td>
	<td class="line x" title="81:203	The function f represents a mathematical model of human judgments of translations; an input sentence is represented as a feature vector, x, which contains the information that can be extracted from the input sentence (possibly including comparisons against some reference sentences) that are relevant to computing y. Determining the set of relevant features for this modeling is on-going re1Perhaps even more so than grammaticality judgments, there is variability in peoples judgments of translation quality." ></td>
	<td class="line x" title="82:203	However, like grammaticality judgments, people do share some similarities in their judgments at a coarse-grained level." ></td>
	<td class="line x" title="83:203	Ideally, what we refer to as the true value of translational quality should reflect the consensus judgments of all people." ></td>
	<td class="line x" title="84:203	298 search." ></td>
	<td class="line x" title="85:203	In this work, we consider some of the more widely used metrics as features." ></td>
	<td class="line o" title="86:203	Our full feature vector consists of r  18 adequacy features, where r is the number of reference systems used, and 26 fluency features: Adequacy features: These include features derived from BLEU (e.g. , n-gram precision, where 1  n  5, length ratios), PER, WER, features derived from METEOR (precision, recall, fragmentation), and ROUGE-related features (nonconsecutive bigrams with a gap size of g, where 1  g  5 and longest common subsequence)." ></td>
	<td class="line x" title="87:203	Fluency features: We consider both string-level features such as computing n-gram precision against a target-language corpus as well as several syntaxbased features." ></td>
	<td class="line x" title="88:203	We parse each input sentence into a dependency tree and compared aspects of it against a large target-language dependency treebank." ></td>
	<td class="line x" title="89:203	In addition to adapting the idea of Head Word Chains (Liu and Gildea, 2005), we also compared the input sentences argument structures against the treebank for certain syntactic categories." ></td>
	<td class="line x" title="90:203	Due to the large feature space to explore, we chose to work with support vector regression as the learning algorithm." ></td>
	<td class="line x" title="91:203	As its loss function, support vector regression uses an epsilon1-insensitive error function, which allows for errors within a margin of a small positive value, epsilon1, to be considered as having zero error (cf.Bishop (2006), pp.339-344)." ></td>
	<td class="line x" title="93:203	Like its classification counterpart, this is a kernel-based algorithm that finds sparse solutions so that scores for new test instances are efficiently computed based on a subset of the most informative training examples." ></td>
	<td class="line x" title="94:203	In this work, Gaussian kernels are used." ></td>
	<td class="line x" title="95:203	The cost of regression learning is that it requires training examples that are manually assessed by human judges." ></td>
	<td class="line x" title="96:203	However, compared to the cost of creating new references whenever new (test) sentences are evaluated, the effort of creating human assessment training data is a limited (ideally, one-time) cost." ></td>
	<td class="line x" title="97:203	Moreover, there is already a sizable collection of human assessed data for a range of MT systems through multiple years of the NIST MT Eval efforts." ></td>
	<td class="line x" title="98:203	Our experiments suggest that there is enough assessed data to train the proposed regression model." ></td>
	<td class="line x" title="99:203	Aside from reducing the cost of developing human reference translations, the proposed metric also provides an alternative perspective on automatic MT evaluation that may be informative in its own right." ></td>
	<td class="line x" title="100:203	We conjecture that a metric that compares inputs against a diverse population of differently imperfect sentences may be more discriminative in judging translation systems than solely comparing against gold standards." ></td>
	<td class="line x" title="101:203	That is, two sentences may be considered equally bad from the perspective of a gold standard, but subtle differences between them may become more prominent if they are compared against sentences in their peer group." ></td>
	<td class="line x" title="102:203	4 Experiments We conducted experiments to determine the feasibility of the proposed approach and to address the following questions: (1) How informative are pseudo references in-and-of themselves?" ></td>
	<td class="line x" title="103:203	Does varying the number and/or the quality of the references have an impact on the metrics?" ></td>
	<td class="line x" title="104:203	(2) What are the contributions of the adequacy features versus the fluency features to the learning-based metric?" ></td>
	<td class="line x" title="105:203	(3) How do the quality and distribution of the training examples, together with the quality of the pseudo references, impact the metric training?" ></td>
	<td class="line x" title="106:203	(4) Do these factors impact the metrics ability in assessing sentences produced within a single MT system?" ></td>
	<td class="line x" title="107:203	How does that systems quality affect metric performance?" ></td>
	<td class="line x" title="108:203	4.1 Data preparation and Experimental Setup The implementation of support vector regression used for these experiments is SVM-Light (Joachims, 1999)." ></td>
	<td class="line x" title="109:203	We performed all experiments using the 2004 NIST Chinese MT Eval dataset." ></td>
	<td class="line x" title="110:203	It consists of 447 source sentences that were translated by four human translators as well as ten MT systems." ></td>
	<td class="line x" title="111:203	Each machine translated sentence was evaluated by two human judges for their fluency and adequacy on a 5-point scale2." ></td>
	<td class="line x" title="112:203	To remove the bias in the distributions of scores between different judges, we follow the normalization procedure described by Blatz et al.(2003)." ></td>
	<td class="line x" title="114:203	The two judges total scores (i.e. , sum of the normalized fluency and adequacy scores) are then averaged." ></td>
	<td class="line x" title="115:203	2The NIST human judges use human reference translations when making assessments; however, our approach is generally applicable when the judges are bilingual speakers who compare source sentences with translation outputs." ></td>
	<td class="line x" title="116:203	299 We chose to work with this NIST dataset because it contains numerous systems that span over a range of performance levels (see Table 1 for a ranking of the systems and their averaged human assessment scores)." ></td>
	<td class="line x" title="117:203	This allows us to have control over the variability of the experiments while answering the questions we posed above (such as the quality of the systems providing the pseudo references, the quality of MT systems being evaluated, and the diversity over the distribution of training examples)." ></td>
	<td class="line x" title="118:203	Specifically, we reserved four systems (MT2, MT5, MT6, and MT9) for the role of pseudo references." ></td>
	<td class="line x" title="119:203	Sentences produced by the remaining six systems are used as evaluative data." ></td>
	<td class="line x" title="120:203	This set includes the best and worst systems so that we can see how well the metrics performs on sentences that are better (or worse) than the pseudo references." ></td>
	<td class="line x" title="121:203	Metrics that require no learning are directly applied onto all sentences of the evaluative set." ></td>
	<td class="line x" title="122:203	For the learningbased metrics, we perform six-fold cross validation on the evaluative dataset." ></td>
	<td class="line x" title="123:203	Each fold consists of sentences from one MT system." ></td>
	<td class="line x" title="124:203	In a round robin fashion, each fold serves as the test set while the other five are used for training and heldout." ></td>
	<td class="line x" title="125:203	Thus, the trained models have seen neither the test instances nor other instances from the MT system that produced them." ></td>
	<td class="line x" title="126:203	A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data." ></td>
	<td class="line x" title="127:203	The correlation coefficient is a real number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations." ></td>
	<td class="line x" title="128:203	To compare the relative quality of different metrics, we apply bootstrapping re-sampling on the data, and then use paired t-test to determine the statistical significance of the correlation differences (Koehn, 2004)." ></td>
	<td class="line x" title="129:203	For the results we report, unless explicitly mentioned, all stated comparisons are statistically significant with 99.8% confidence." ></td>
	<td class="line o" title="130:203	We include two standard reference-based metrics, BLEU and METEOR, as baseline comparisons." ></td>
	<td class="line x" title="131:203	BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bigrams because this has higher correlations with human judgments than when higher-ordered n-grams are included." ></td>
	<td class="line x" title="132:203	SysID Human-assessment score MT1 0.661 MT2 0.626 MT3 0.586 MT4 0.578 MT5 0.537 MT6 0.530 MT7 0.530 MT8 0.375 MT9 0.332 MT10 0.243 Table 1: The human-judged quality of ten participating systems in the NIST 2004 Chinese MT Evaluation." ></td>
	<td class="line x" title="133:203	We used four systems as references (highlighted in boldface) and the data from the remaining six for training and evaluation." ></td>
	<td class="line x" title="134:203	4.2 Pseudo Reference Variations vs. Metrics We first compare different metrics performance on the six-system evaluative dataset under different configurations of human and/or pseudo references." ></td>
	<td class="line x" title="135:203	For the case when only one human reference is used, the reference was chosen at random from the 2004 NIST Eval dataset3." ></td>
	<td class="line x" title="136:203	The correlation results on the evaluative dataset are summarized in Table 2." ></td>
	<td class="line x" title="137:203	Some trends are as expected: comparing within a metric, having four references is better than having just one; having human references is better than an equal number of system references; having a high quality system as reference is better than one with low quality." ></td>
	<td class="line x" title="138:203	Perhaps more surprising is the consistent trend that metrics do significantly better with four MT references than with one human reference, and they do almost as well as using four human references." ></td>
	<td class="line x" title="139:203	The results show that pseudo references are informative, as standard metrics were able to make use of the pseudo references and achieve higher correlations than judging from fluency alone." ></td>
	<td class="line x" title="140:203	However, higher correlations are achieved when learning with regression, suggesting that the trained metrics are better at interpreting comparisons against pseudo references." ></td>
	<td class="line x" title="141:203	Comparing within each reference configuration, the regression-trained metric that includes both ad3One reviewer asked about the quality this humans translations." ></td>
	<td class="line x" title="142:203	Although we were not given official rankings of the human references, we compared each person against the other three using MT evaluation metrics and found this particular translator to rank third, though the quality of all four are significantly higher than even the best MT systems." ></td>
	<td class="line x" title="143:203	300 equacy and fluency features always has the highest correlations." ></td>
	<td class="line x" title="144:203	If the metric consists of only adequacy features, its performance degrades with the decreasing quality of the references." ></td>
	<td class="line x" title="145:203	At another extreme, a metric based only on fluency features has an overall correlation rate of 0.459, which is lower than most correlations reported in Table 2." ></td>
	<td class="line x" title="146:203	This confirms the importance of modeling adequacy; even a single mid-quality MT system may be an informative pseudo reference." ></td>
	<td class="line x" title="147:203	Finally, we note that a regressiontrained metric with the full features set that compares against 4 pseudo references has a higher correlation than BLEU with four human references." ></td>
	<td class="line x" title="148:203	These results suggest that the feedback from the human assessed training examples was able to help the learning algorithm to combine different features to form a better composite metric." ></td>
	<td class="line x" title="149:203	4.3 Sentence-Level Evaluation on Single Systems To explore the interaction between the quality of the reference MT systems and that of the test MT systems, we further study the following pseudo reference configurations: all four systems, a highquality system with a medium quality system, two systems of medium-quality, one medium with one poor system, and only the high-quality system." ></td>
	<td class="line o" title="150:203	For each pseudo reference configuration, we consider three metrics: BLEU, METEOR, and the regressiontrained metric (using the full feature set)." ></td>
	<td class="line x" title="151:203	Each metric evaluates sentences from four test systems of varying quality: the best system in the dataset (MT1), the worst in the set (MT10), and two midranged systems (MT4 and MT7)." ></td>
	<td class="line x" title="152:203	The correlation coefficients are summarized in Table 3." ></td>
	<td class="line x" title="153:203	Each row specifies a metric/reference-type combination; each column specifies an MT system being evaluated (using sentences from all other systems as training examples)." ></td>
	<td class="line x" title="154:203	The fluency-only metric and standard metrics using four human references are baselines." ></td>
	<td class="line x" title="155:203	The overall trends at the dataset level generally also hold for the per-system comparisons." ></td>
	<td class="line x" title="156:203	With the exception of the evaluation of MT10, regressionbased metrics always has higher correlations than standard metrics that use the same reference configuration (comparing correlation coefficients within each cell)." ></td>
	<td class="line o" title="157:203	When the best MT reference system (MT2) is included as pseudo references, regressionbased metrics are typically better than or not statistically different from standard applications of BLEU and METEOR with 4 human references." ></td>
	<td class="line x" title="158:203	Using the two mid-quality MT systems as references (MT5 and MT6), regression metrics yield correlations that are only slightly lower than standard metrics with human references." ></td>
	<td class="line x" title="159:203	These results support our conjecture that comparing against multiple systems is informative." ></td>
	<td class="line x" title="160:203	The poorer performances of the regression-based metrics on MT10 point out an asymmetry in the learning approach." ></td>
	<td class="line x" title="161:203	The regression model aims to learn a function that approximates human judgments of translated sentences through training examples." ></td>
	<td class="line x" title="162:203	In the space of all possible MT outputs, the neighborhood of good translations is much smaller than that of bad translations." ></td>
	<td class="line x" title="163:203	Thus, as long as the regression models sees some examples of sentences with high assessment scores during training, it should have a much better estimation of the characteristics of good translations." ></td>
	<td class="line x" title="164:203	This idea is supported by the experimental data." ></td>
	<td class="line x" title="165:203	Consider the scenario of evaluating MT1 while using two mid-quality MT systems as references." ></td>
	<td class="line x" title="166:203	Although the reference systems are not as high quality as the system under evaluation, and although the training examples shown to the regression model were also generated by systems whose overall quality was rated lower, the trained metric was reasonably good at ranking sentences produced by MT1." ></td>
	<td class="line x" title="167:203	In contrast, the task of evaluating sentences from MT10 is more difficult for the learning approach, perhaps because it is sufficiently different from all training and reference systems." ></td>
	<td class="line x" title="168:203	Correlations might be improved with additional reference systems." ></td>
	<td class="line x" title="169:203	4.4 Discussions The design of these experiments aims to simulate practical situations to use our proposed metrics." ></td>
	<td class="line x" title="170:203	For the more frequently encountered language pairs, it should be possible to find at least two mid-quality (or better) MT systems to serve as pseudo references." ></td>
	<td class="line x" title="171:203	For example, one might use commercial offthe-shelf systems, some of which are free over the web." ></td>
	<td class="line x" title="172:203	For less commonly used languages, one might use open source research systems (Al-Onaizan et al. , 1999; Burbank et al. , 2005)." ></td>
	<td class="line x" title="173:203	Datasets from formal evaluation events such as 301 Ref type and # Ref Sys." ></td>
	<td class="line o" title="174:203	BLEU-S(2) METEOR Regr (adq." ></td>
	<td class="line x" title="175:203	only) Regr (full) 4 Humans all humans 0.628 0.591 0.588 0.644 1 Human HRef #3 0.536 0.512 0.487 0.597 4 Systems all MTRefs 0.614 0.583 0.584 0.632 2 Systems Best 2 MTRefs 0.603 0.577 0.573 0.620 Mid 2 MTRefs 0.579 0.555 0.528 0.608 Worst 2 MTRefs 0.541 0.508 0.467 0.581 1 System Best MTRef 0.576 0.559 0.534 0.596 Mid MTRef (MT5) 0.538 0.528 0.474 0.577 Worst MTRef 0.371 0.329 0.151 0.495 Table 2: Comparisons of metrics (columns) using different types of references (rows)." ></td>
	<td class="line x" title="176:203	The full regressiontrained metric has the highest correlation (shown in boldface) when four human references are used; it has the second highest correlation rate (shown in italic) when four MT system references are used instead." ></td>
	<td class="line x" title="177:203	A regression-trained metric with only fluency features has a correlation coefficient of 0.459." ></td>
	<td class="line x" title="178:203	Ref Type Metric MT-1 MT-4 MT-7 MT-10 No ref Regr." ></td>
	<td class="line x" title="179:203	0.367 0.316 0.301 -0.045 4 human refs Regr." ></td>
	<td class="line o" title="180:203	0.538* 0.473* 0.459* 0.247 BLEU-S(2) 0.466 0.419 0.397 0.321* METEOR 0.464 0.418 0.410 0.312 4 MTRefs Regr." ></td>
	<td class="line o" title="181:203	0.498 0.429 0.421 0.243 BLEU-S(2) 0.386 0.349 0.404 0.240 METEOR 0.445 0.354 0.333 0.243 Best 2 MTRefs Regr." ></td>
	<td class="line o" title="182:203	0.492 0.418 0.403 0.201 BLEU-S(2) 0.391 0.330 0.394 0.268 METEOR 0.430 0.333 0.327 0.267 Mid 2 MTRefs Regr." ></td>
	<td class="line o" title="183:203	0.450 0.413 0.388 0.219 BLEU-S(2) 0.362 0.314 0.310 0.282 METEOR 0.391 0.315 0.284 0.274 Worst 2 MTRefs Regr." ></td>
	<td class="line o" title="184:203	0.430 0.386 0.365 0.158 BLEU-S(2) 0.320 0.298 0.316 0.223 METEOR 0.351 0.306 0.302 0.228 Best MTRef Regr." ></td>
	<td class="line o" title="185:203	0.461 0.401 0.414 0.122 BLEU-S(2) 0.371 0.330 0.380 0.242 METEOR 0.375 0.318 0.392 0.283 Table 3: Correlation comparisons of metrics by test systems." ></td>
	<td class="line x" title="186:203	For each test system (columns) the overall highest correlations is distinguished by an asterisk (*); correlations higher than standard metrics using human-references are highlighted in boldface; those that are statistically comparable to them are italicized." ></td>
	<td class="line x" title="187:203	NIST MT Evals, which contains human assessed MT outputs for a variety of systems, can be used for training examples." ></td>
	<td class="line x" title="188:203	Alternatively, one might directly recruit human judges to assess sample sentences from the system(s) to be evaluated." ></td>
	<td class="line x" title="189:203	This should result in better correlations than what we reported here, since the human assessed training examples will be more similar to the test instances than the setup in our experiments." ></td>
	<td class="line x" title="190:203	In developing new MT systems, pseudo references may supplement the single human reference translations that could be extracted from a parallel text." ></td>
	<td class="line x" title="191:203	Using the same setup as Exp. 1 (see Table 2), adding pseudo references does improve correlations." ></td>
	<td class="line x" title="192:203	Adding four pseudo references to the single human reference raises the correlation coefficient to 0.650 (from 0.597) for the regression metric." ></td>
	<td class="line x" title="193:203	Adding them to four human references results in a correlation coefficient of 0.660 (from 0.644)4." ></td>
	<td class="line x" title="194:203	5 Conclusion In this paper, we have presented a method for developing sentence-level MT evaluation metrics without using human references." ></td>
	<td class="line x" title="195:203	We showed that by learning from human assessed training examples, 4BLEU with four human references has a correlation of 0.628." ></td>
	<td class="line x" title="196:203	Adding four pseudo references increases BLEU to 0.650." ></td>
	<td class="line x" title="197:203	302 the regression-trained metric can evaluate an input sentence by comparing it against multiple machinegenerated pseudo references and other target language resources." ></td>
	<td class="line x" title="198:203	Our experimental results suggest that the resulting metrics are robust even when the sentences under evaluation are from a system of higher quality than the systems serving as references." ></td>
	<td class="line x" title="199:203	We observe that regression metrics that use multiple pseudo references often have comparable or higher correlation rates with human judgments than standard reference-based metrics." ></td>
	<td class="line x" title="200:203	Our study suggests that in conjunction with regression training, multiple imperfect references may be as informative as gold-standard references." ></td>
	<td class="line x" title="201:203	Acknowledgments This work has been supported by NSF Grants IIS-0612791 and IIS-0710695." ></td>
	<td class="line x" title="202:203	We would like to thank Ric Crabbe, Dan Gildea, Alon Lavie, Stuart Shieber, and Noah Smith and the anonymous reviewers for their suggestions." ></td>
	<td class="line x" title="203:203	We are also grateful to NIST for making their assessment data available to us." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1040
Improved Word-Level System Combination for Machine Translation
Rosti, Antti-Veikko I.;Matsoukas, Spyros;Schwartz, Richard M.;"></td>
	<td class="line x" title="1:212	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312319, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:212	c2007 Association for Computational Linguistics Improved Word-Level System Combination for Machine Translation Antti-Veikko I. Rosti and Spyros Matsoukas and Richard Schwartz BBN Technologies, 10 Moulton Street Cambridge, MA 02138 a0 arosti,smatsouk,schwartz a1 @bbn.com Abstract Recently, confusion network decoding has been applied in machine translation system combination." ></td>
	<td class="line x" title="3:212	Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs." ></td>
	<td class="line x" title="4:212	This paper describes an improved confusion network based method to combine outputs from multiple MT systems." ></td>
	<td class="line x" title="5:212	In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring." ></td>
	<td class="line x" title="6:212	Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed." ></td>
	<td class="line x" title="7:212	A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR." ></td>
	<td class="line x" title="8:212	The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods." ></td>
	<td class="line x" title="9:212	1 Introduction System combination has been shown to improve classification performance in various tasks." ></td>
	<td class="line x" title="10:212	There are several approaches for combining classifiers." ></td>
	<td class="line x" title="11:212	In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting (Schapire, 1990)." ></td>
	<td class="line x" title="12:212	Another approach is to combine outputs from a few highly specialized classifiers." ></td>
	<td class="line x" title="13:212	The classifiers may be based on the same basic modeling techniques but differ by, for example, alternative feature representations." ></td>
	<td class="line x" title="14:212	Combination of speech recognition outputs is an example of this approach (Fiscus, 1997)." ></td>
	<td class="line x" title="15:212	In speech recognition, confusion network decoding (Mangu et al. , 2000) has become widely used in system combination." ></td>
	<td class="line x" title="16:212	Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems." ></td>
	<td class="line x" title="17:212	The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994)." ></td>
	<td class="line x" title="18:212	Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al. , 2001)." ></td>
	<td class="line x" title="19:212	To generate confusion networks, hypotheses have to be aligned against each other." ></td>
	<td class="line x" title="20:212	In (Bangalore et al. , 2001), Levenshtein alignment was used to generate the network." ></td>
	<td class="line x" title="21:212	As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses." ></td>
	<td class="line x" title="22:212	In (Matusov et al. , 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003)." ></td>
	<td class="line x" title="23:212	The size of the test set may influence the quality of these alignments." ></td>
	<td class="line x" title="24:212	Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments." ></td>
	<td class="line x" title="25:212	A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al. , 2006) was used to align hy312 potheses in (Sim et al. , 2007)." ></td>
	<td class="line x" title="26:212	The alignments from TER are consistent as they do not depend on the test set size." ></td>
	<td class="line x" title="27:212	Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005)." ></td>
	<td class="line x" title="28:212	A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering." ></td>
	<td class="line x" title="29:212	Confusion networks are generated by choosing one hypothesis as the skeleton, and other hypotheses are aligned against it." ></td>
	<td class="line x" title="30:212	The skeleton defines the word order of the combination output." ></td>
	<td class="line x" title="31:212	Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al. , 2007)." ></td>
	<td class="line x" title="32:212	The average TER score was computed between each systems a0 -best hypothesis and all other hypotheses." ></td>
	<td class="line x" title="33:212	The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER." ></td>
	<td class="line x" title="34:212	This work was extended in (Rosti et al. , 2007) by introducing system weights for word confidences." ></td>
	<td class="line x" title="35:212	However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton." ></td>
	<td class="line x" title="36:212	In this work, confusion networks are generated by using the a0 -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses." ></td>
	<td class="line x" title="37:212	All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights." ></td>
	<td class="line x" title="38:212	The combination outputs from confusion network decoding may be ungrammatical due to alignment errors." ></td>
	<td class="line x" title="39:212	Also the word-level decoding may break coherent phrases produced by the individual systems." ></td>
	<td class="line x" title="40:212	In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences." ></td>
	<td class="line x" title="41:212	This allows a log-linear addition of arbitrary features such as language model (LM) scores." ></td>
	<td class="line x" title="42:212	The LM scores should increase the total log-posterior of more grammatical hypotheses." ></td>
	<td class="line x" title="43:212	Powells method (Brent, 1973) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set." ></td>
	<td class="line x" title="44:212	Tuning is fully automatic, as opposed to (Matusov et al. , 2006) where global system weights were set manually." ></td>
	<td class="line x" title="45:212	This paper is organized as follows." ></td>
	<td class="line x" title="46:212	Three evaluation metrics used in weights tuning and reporting the test set results are reviewed in Section 2." ></td>
	<td class="line x" title="47:212	Section 3 describes confusion network decoding for MT system combination." ></td>
	<td class="line x" title="48:212	The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively." ></td>
	<td class="line x" title="49:212	Section 6 details the weights optimization algorithm and the experimental results are reported in Section 7." ></td>
	<td class="line x" title="50:212	Conclusions and future work are discussed in Section 8." ></td>
	<td class="line x" title="51:212	2 Evaluation Metrics Currently, the most widely used automatic MT evaluation metric is the NIST BLEU-4 (Papineni et al. , 2002)." ></td>
	<td class="line x" title="52:212	It is computed as the geometric mean of a1 gram precisions up to a2 -grams between the hypothesis a3 and reference a3a5a4 as follows a6a8a7a10a9a12a11a14a13 a3a16a15a17a3a18a4a20a19a22a21 (1) a23a25a24a27a26a29a28 a0 a2 a30 a31 a32a34a33a36a35a38a37a40a39a42a41a44a43 a32 a13 a3a16a15a17a3a45a4a20a19a47a46a49a48 a13 a3a16a15a17a3a45a4a20a19 where a48 a13 a3a16a15a17a3a18a4a50a19a52a51 a0 is the brevity penalty and a43 a32 a13 a3a16a15a17a3a45a4a20a19 are the a1 -gram precisions." ></td>
	<td class="line x" title="53:212	When multiple references are provided, the a1 -gram counts against all references are accumulated to compute the precisions." ></td>
	<td class="line x" title="54:212	Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs." ></td>
	<td class="line x" title="55:212	The BLEU scores are between a53 and a0, higher being better." ></td>
	<td class="line x" title="56:212	Often BLEU scores are reported as percentages and one BLEU point gain usually means a BLEU increase of a53a27a54a55a53 a0." ></td>
	<td class="line x" title="57:212	Other evaluation metrics have been proposed to replace BLEU." ></td>
	<td class="line pc" title="58:212	It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision (Banerjee and Lavie, 2005)." ></td>
	<td class="line o" title="59:212	METEOR is based on the weighted harmonic mean of the precision and recall measured on unigram matches as follows a56a58a57a45a59a60a13 a3a16a15a17a3a18a4a61a19a12a21 a0 a53a34a62 a63a65a64a18a66a68a67a34a63 a4 a28 a0a70a69 a53a27a54a72a71 a13a74a73a61a75 a62 a19a47a76a20a46 (2) where a62 is the total number of unigram matches, a63a77a64 is the hypothesis length, a63 a4 is the reference length and a73 is the minimum number of a1 -gram matches that covers the alignment." ></td>
	<td class="line o" title="60:212	The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to a53a27a54a72a71 when a73 a21 a62 ; i.e., 313 there are no matching a1 -grams higher than a1 a21 a0 . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer." ></td>
	<td class="line x" title="61:212	Additional matching modules including WordNet stemming and synonymy may also be used." ></td>
	<td class="line x" title="62:212	When multiple references are provided, the lowest score is reported." ></td>
	<td class="line x" title="63:212	Full test set scores are obtained by accumulating statistics over all test sentences." ></td>
	<td class="line o" title="64:212	The METEOR scores are also between a53 and a0, higher being better." ></td>
	<td class="line x" title="65:212	The scores in the results section are reported as percentages." ></td>
	<td class="line x" title="66:212	Translation edit rate (TER) (Snover et al. , 2006) has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference." ></td>
	<td class="line x" title="67:212	The TER score is computed as follows a57a45a9a12a59a60a13 a3a16a15a17a3a45a4a20a19a22a21 a1a3a2a5a4a36a66a7a6 a23 a37 a66a9a8a11a10a13a12a65a66a9a8a13a14a13a15a17a16 a63 a4 (3) where a63 a4 is the reference length." ></td>
	<td class="line x" title="68:212	The only difference to word error rate is that the TER allows shifts." ></td>
	<td class="line x" title="69:212	A shift of a sequence of words is counted as a single edit." ></td>
	<td class="line x" title="70:212	The minimum translation edit alignment is usually found through a beam search." ></td>
	<td class="line x" title="71:212	When multiple references are provided, the edits from the closest reference are divided by the average reference length." ></td>
	<td class="line x" title="72:212	Full test set scores are obtained by accumulating the edits and the average reference lengths." ></td>
	<td class="line x" title="73:212	The perfect TER score is 0, and otherwise higher than zero." ></td>
	<td class="line x" title="74:212	The TER score may also be higher than 1 due to insertions." ></td>
	<td class="line x" title="75:212	Also TER is reported as a percentage in the results section." ></td>
	<td class="line x" title="76:212	3 Confusion Network Decoding Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination." ></td>
	<td class="line x" title="77:212	The other hypotheses are aligned against the skeleton." ></td>
	<td class="line x" title="78:212	Either votes or some form of confidences are assigned to each word in the network." ></td>
	<td class="line x" title="79:212	For example using cat sat the mat as the skeleton, aligning cat sitting on the mat and hat on a mat against it might yield the following alignments: cat sat a18 the mat cat sitting on the mat hat a18 on a mat where a18 represents a NULL word." ></td>
	<td class="line x" title="80:212	In graphical form, the resulting confusion network is shown in Figure 1." ></td>
	<td class="line x" title="81:212	Each arc represents an alternative word at that position in the sentence and the number of votes for each word is marked in parentheses." ></td>
	<td class="line x" title="82:212	Confusion network decoding usually requires finding the path with the highest confidence in the network." ></td>
	<td class="line x" title="83:212	Based on vote counts, there are three alternatives in the example: cat sat on the mat, cat on the mat and cat sitting on the mat, each having accumulated 10 votes." ></td>
	<td class="line x" title="84:212	The alignment procedure plays an important role, as by switching the position of the word sat and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, cat on the mat." ></td>
	<td class="line x" title="85:212	1 2 3 4 5 6 cat (2) hat (1)  (1) sitting (1)  (1) on (2) a (1) the (2)sat (1) mat (3) Figure 1: Example consensus network with votes on word arcs." ></td>
	<td class="line x" title="86:212	Different alignment methods yield different confusion networks." ></td>
	<td class="line x" title="87:212	The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning." ></td>
	<td class="line x" title="88:212	As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton." ></td>
	<td class="line x" title="89:212	Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score." ></td>
	<td class="line x" title="90:212	The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton a3a20a19 as follows a3a21a19 a21a23a22a25a24 a41 a26a28a27 a2 a29a31a30a32a29a34a33 a35a37a36 a31 a38 a33a36a35 a57a45a9a12a59a65a13 a3 a38 a15a17a3a40a39 a19 (4) where a63 a19 is the number of systems." ></td>
	<td class="line x" title="91:212	This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al. , 2007)." ></td>
	<td class="line x" title="92:212	Other evaluation metrics may also be used as the MBR loss function." ></td>
	<td class="line o" title="93:212	For BLEU and METEOR, the loss function would be a0 a69 a6a8a7a10a9 a11a60a13 a3 a38 a15a17a3a41a39 a19 and a0 a69 a56 a57a45a59a65a13 a3 a38 a15a17a3a41a39 a19 . It has been found that multiple hypotheses from each system may be used to improve the quality of 314 the combination output (Sim et al. , 2007)." ></td>
	<td class="line x" title="94:212	When using a63 -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis." ></td>
	<td class="line x" title="95:212	In (Rosti et al. , 2007), simple a0 a75a27a13 a0 a66a1a0 a19 score was assigned to the word coming from the a0 thbest hypothesis." ></td>
	<td class="line x" title="96:212	Due to the computational burden of the TER alignment, only a0 -best hypotheses were considered as possible skeletons, and a1 a21 a0 a53 hypotheses per system were aligned." ></td>
	<td class="line x" title="97:212	Similar approach to estimate word posteriors is adopted in this work." ></td>
	<td class="line x" title="98:212	System weights may be used to assign a system specific confidence on each word in the network." ></td>
	<td class="line x" title="99:212	The weights may be based on the systems relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set." ></td>
	<td class="line x" title="100:212	In (Rosti et al. , 2007), the total confidence of the a1 th best confusion network hypothesis a3 a38a3a2a32, including NULL words, given the a4 th source sentence a5 a38 was given by a73a42a13 a3 a38a3a2 a32a7a6 a5 a38 a19a22a21 (5) a35a9a8a11a10 a35 a31 a39 a33a36a35 a35a37a36 a31 a12 a33a36a35 a13 a12 a73a15a14 a12 a39 a66a1a16a44a63 a32a18a17 a12a19a12 a19 a13 a3 a38a3a2 a32 a19 where a63 a38 is the number of nodes in the confusion network for the source sentence a5 a38, a63 a19 is the number of translation systems, a13 a12 is the a20 th system weight, a73a21a14 a12a39 is the accumulated confidence for word a22 produced by system a20 between nodes a23 and a23 a66 a0, and a16 is a weight for the number of NULL links along the hypothesis a63 a32a24a17 a12a25a12 a19 a13 a3 a38a26a2a32 a19 . The word confidences a73a21a14 a12a39 were increased by a0 a75a27a13 a0 a66a27a0 a19 if the word a22 aligns between nodes a23 and a23 a66 a0 in the network." ></td>
	<td class="line x" title="101:212	If no word aligns between nodes a23 and a23 a66 a0, the NULL word confidence at that position was increased by a0 a75a27a13 a0 a66a28a0 a19 . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty." ></td>
	<td class="line x" title="102:212	Each arc in the confusion network carries the word label a22 and a63 a19 scores a73a29a14 a12 a39 . The decoder outputs the hypothesis with the highest a73a42a13 a3 a38a26a2a32 a6a5 a38 a19 given the current set of weights." ></td>
	<td class="line x" title="103:212	3.1 Discussion There are several problems with the previous confusion network decoding approaches." ></td>
	<td class="line x" title="104:212	First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the word-level decoding." ></td>
	<td class="line x" title="105:212	For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output." ></td>
	<td class="line x" title="106:212	Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores." ></td>
	<td class="line x" title="107:212	Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding." ></td>
	<td class="line x" title="108:212	Third, the system weights are independent of the skeleton selection." ></td>
	<td class="line x" title="109:212	Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton." ></td>
	<td class="line x" title="110:212	4 Log-Linear Combination with Arbitrary Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified." ></td>
	<td class="line x" title="111:212	Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows a37 a39a42a41a10a43 a13 a3 a38a3a2 a32a30a6 a5 a38 a19a12a21 (6) a35a9a8a29a10 a35 a31 a39 a33a36a35 a37 a39a42a41 a28 a35a37a36 a31 a12 a33a36a35 a13 a12 a43 a13 a22 a6 a20a47a15a31a23a47a19a47a46 a66a33a32a35a34a5a13 a3 a38a26a2 a32 a19 a66a36a16a44a63 a32a24a17 a12a25a12 a19 a13 a3 a38a3a2 a32 a19 a66a1a37a42a63a38a14a7a39 a4a41a40 a19 a13 a3 a38a3a2 a32 a19 where a32 is the language model weight, a34a5a13 a3 a38a3a2a32 a19 is the LM log-probability and a63a42a14a7a39 a4a31a40 a19 a13 a3 a38a3a2 a32 a19 is the number of words in the hypothesis a3 a38a3a2a32 . The word posteriors a43 a13 a22 a6 a20a47a15a31a23a47a19 are estimated by scaling the confidences a73a21a14 a12a39 to sum to one for each system a20 over all words a22 in between nodes a23 and a23 a66 a0 . The system weights are also constrained to sum to one." ></td>
	<td class="line x" title="112:212	Equation 6 may be viewed as a log-linear sum of sentencelevel features." ></td>
	<td class="line x" title="113:212	The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score." ></td>
	<td class="line x" title="114:212	The last two terms are not completely independent but seem to help based on experimental results." ></td>
	<td class="line x" title="115:212	The number of paths through a confusion network grows exponentially with the number of nodes." ></td>
	<td class="line x" title="116:212	Therefore expanding a network with an a1 -gram language model may result in huge lattices if a1 is high." ></td>
	<td class="line x" title="117:212	Instead of high order a1 -grams with heavy pruning, a bi-gram may first be used to expand the lattice." ></td>
	<td class="line x" title="118:212	After optimizing one set of weights for the expanded 315 confusion network, a second set of weights for a63 best list re-scoring with a higher order a1 -gram model may be optimized." ></td>
	<td class="line x" title="119:212	On a test set, the first set of weights is used to generate an a63 -best list from the bi-gram expanded lattice." ></td>
	<td class="line x" title="120:212	This a63 -best list is then re-scored with the higher order a1 -gram." ></td>
	<td class="line x" title="121:212	The second set of weights is used to find the final a0 -best from the re-scored a63 -best list." ></td>
	<td class="line x" title="122:212	5 Multiple Confusion Network Decoding As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation." ></td>
	<td class="line x" title="123:212	To prevent the a0 -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network." ></td>
	<td class="line x" title="124:212	All a63 a19 confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network." ></td>
	<td class="line x" title="125:212	All confusion network are connected to a common end node with NULL arcs." ></td>
	<td class="line x" title="126:212	The final arcs have a probability of one." ></td>
	<td class="line x" title="127:212	The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a a0 -best from a system with a zero weight will not be chosen." ></td>
	<td class="line x" title="128:212	The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities." ></td>
	<td class="line x" title="129:212	These log-probabilities are scaled so that the priors sum to one." ></td>
	<td class="line x" title="130:212	There is a concern that the prior probabilities estimated this way may be inaccurate." ></td>
	<td class="line x" title="131:212	Therefore, the priors may have to be smoothed by a tunable exponent." ></td>
	<td class="line x" title="132:212	However, the optimization experiments showed that the best performance was obtained by having a smoothing factor of 1 which is equivalent to the original priors." ></td>
	<td class="line x" title="133:212	Thus, no smoothing was used in the experiments presented later in this paper." ></td>
	<td class="line x" title="134:212	An example joint network with the priors is shown in Figure 2." ></td>
	<td class="line x" title="135:212	This example has three confusion networks with priors a53a27a54a72a71, a53a27a54a1a0 and a53a27a54a1a2 . The total number of nodes in the network is represented by a63a4a3 . Similar combination of multiple confusion networks was presented in (Matusov et al. , 2006)." ></td>
	<td class="line x" title="136:212	However, this approach did not include sentence  (1)  (1)  (1)  (0.2)  (0.3)  (0.5) 1 Na Figure 2: Three confusion networks with prior probabilities." ></td>
	<td class="line x" title="137:212	specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights." ></td>
	<td class="line x" title="138:212	6 Weights Optimization The optimization of the system and feature weights may be carried out using a63 -best lists as in (Ostendorf et al. , 1991)." ></td>
	<td class="line x" title="139:212	A confusion network may be represented by a word lattice and standard tools may be used to generate a63 -best hypothesis lists including word confidence scores, language model scores and other features." ></td>
	<td class="line x" title="140:212	The a63 -best list may be re-ordered using the sentence-level posteriors a43 a13 a3 a38a3a2 a32a30a6 a5 a38 a19 from Equation 6 for the a4 th source sentence a5 a38 and the corresponding a1 th hypothesis a3 a38a3a2 a32 . The current a0 -best hypothesis a5 a3 a38 given a set of weights a6 a21 a0 a13 a35 a15 a54 a54 a54 a15 a13 a35a37a36 a15 a32 a15 a16 a15 a37 a1 may be represented as follows a5 a3 a38 a13 a5 a38 a6 a6 a19 a21a23a22a25a24 a41 a26 a22 a24 a29a31a30a32a29 a8a8a7a9 a43 a13 a3 a38a26a2 a32a7a6 a5 a38 a19 (7) The objective is to optimize the a0 -best score on a development set given a set of reference translations." ></td>
	<td class="line x" title="141:212	For example, estimating weights which minimize TER between a set of a0 -best hypothesis a5 a10 and reference translations a10 a4 can be written as a5 a6 a21 a22a25a24 a41 a26a28a27 a2 a11 a57a45a9a12a59 a13 a5 a10 a15 a10 a4a61a19 (8) This objective function is very complicated, so gradient-based optimization methods may not be used." ></td>
	<td class="line x" title="142:212	In this work, modified Powells method as proposed by (Brent, 1973) is used." ></td>
	<td class="line x" title="143:212	The algorithm explores better weights iteratively starting from a set of initial weights." ></td>
	<td class="line x" title="144:212	First, each dimension is optimized using a grid-based line minimization algorithm." ></td>
	<td class="line x" title="145:212	Then, a new direction based on the changes in the objective function is estimated to speed up the search." ></td>
	<td class="line x" title="146:212	To improve the chances of finding a 316 global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs." ></td>
	<td class="line x" title="147:212	Since the a63 -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new a63 -best list from the lattice for the next iteration." ></td>
	<td class="line o" title="148:212	Similarly, weights which maximize BLEU or METEOR may be optimized." ></td>
	<td class="line x" title="149:212	The same Powells method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003)." ></td>
	<td class="line x" title="150:212	A more efficient algorithm for log-linear models was also proposed." ></td>
	<td class="line x" title="151:212	In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used." ></td>
	<td class="line x" title="152:212	7 Results The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al. , 2007) on the Arabic to English and Chinese to English NIST MT05 tasks." ></td>
	<td class="line x" title="153:212	Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al. , 2006)." ></td>
	<td class="line x" title="154:212	All systems were trained on the same data and the outputs used the same tokenization." ></td>
	<td class="line x" title="155:212	The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU." ></td>
	<td class="line x" title="156:212	All decoder weight tuning was done on the NIST MT02 task." ></td>
	<td class="line x" title="157:212	The joint confusion network was expanded with a bi-gram language model and a a2a34a53a42a53 -best list was generated from the lattice for each tuning iteration." ></td>
	<td class="line x" title="158:212	The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks." ></td>
	<td class="line x" title="159:212	All four reference translations available for the tuning and test sets were used." ></td>
	<td class="line x" title="160:212	A first set of weights with the bigram LM was optimized with three iterations." ></td>
	<td class="line x" title="161:212	A second set of weights was tuned for 5-gram a63 -best list re-scoring." ></td>
	<td class="line x" title="162:212	The bi-gram and 5-gram English language models were trained on about 7 billion words." ></td>
	<td class="line x" title="163:212	The final combination outputs were detokenized and cased before scoring." ></td>
	<td class="line x" title="164:212	The tuning set results on the Arabic to English NIST MT03+MT04 task are shown in Table 1." ></td>
	<td class="line o" title="165:212	The Arabic tuning TER BLEU MTR system A 44.93 45.71 66.09 system B 46.41 43.07 64.79 system C 46.10 46.41 65.33 system D 44.36 46.83 66.91 system E 45.35 45.44 65.69 system F 47.10 44.52 65.28 no weights 42.35 48.91 67.76 baseline 42.19 49.86 68.34 TER tuned 41.88 51.45 68.62 BLEU tuned 42.12 51.72 68.59 MTR tuned 54.08 38.93 71.42 Table 1: Mixed-case TER and BLEU, and lower-case METEOR scores on Arabic NIST MT03+MT04." ></td>
	<td class="line o" title="166:212	Arabic test TER BLEU MTR system A 42.98 49.58 69.86 system B 43.79 47.06 68.62 system C 43.92 47.87 66.97 system D 40.75 52.09 71.23 system E 42.19 50.86 70.02 system F 44.30 50.15 69.75 no weights 39.33 53.66 71.61 baseline 39.29 54.51 72.20 TER tuned 39.10 55.30 72.53 BLEU tuned 39.13 55.48 72.81 MTR tuned 51.56 41.73 74.79 Table 2: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT05." ></td>
	<td class="line x" title="167:212	best score on each metric is shown in bold face fonts." ></td>
	<td class="line x" title="168:212	The row labeled as no weights corresponds to Equation 5 with uniform system weights a13 a12 and zero NULL weight." ></td>
	<td class="line x" title="169:212	The baseline corresponds to Equation 5 with TER tuned weights." ></td>
	<td class="line x" title="170:212	The following three rows correspond to the improved confusion network decoding with different optimization metrics." ></td>
	<td class="line x" title="171:212	As expected, the scores on the metric used in tuning are the best on that metric." ></td>
	<td class="line x" title="172:212	Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning." ></td>
	<td class="line p" title="173:212	However, the METEOR tuning yields extremely high TER and low BLEU scores." ></td>
	<td class="line o" title="174:212	This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR 317 Chinese tuning TER BLEU MTR system A 56.56 29.39 54.54 system B 55.88 30.45 54.36 system C 58.35 32.88 56.72 system D 57.09 36.18 57.11 system E 57.69 33.85 58.28 system F 56.11 36.64 58.90 no weights 53.11 37.77 59.19 baseline 53.40 38.52 59.56 TER tuned 52.13 36.87 57.30 BLEU tuned 53.03 39.99 58.97 MTR tuned 70.27 28.60 63.10 Table 3: Mixed-case TER and BLEU, and lower-case METEOR scores on Chinese NIST MT03+MT04." ></td>
	<td class="line x" title="175:212	score." ></td>
	<td class="line n" title="176:212	Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision." ></td>
	<td class="line x" title="177:212	The Arabic test set results are shown in Table 2." ></td>
	<td class="line x" title="178:212	The TER and BLEU optimized combination results beat all single system scores on all metrics." ></td>
	<td class="line x" title="179:212	The best results on a given metric are again obtained by the combination optimized for the corresponding metric." ></td>
	<td class="line x" title="180:212	It should be noted that the TER optimized combination has significantly higher BLEU score than the TER optimized baseline." ></td>
	<td class="line x" title="181:212	Compared to the baseline system which is also optimized for TER, the BLEU score is improved by 0.97 points." ></td>
	<td class="line o" title="182:212	Also, the METEOR score using the METEOR optimized weights is very high." ></td>
	<td class="line x" title="183:212	However, the other scores are worse in common with the tuning set results." ></td>
	<td class="line x" title="184:212	The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3." ></td>
	<td class="line x" title="185:212	The baseline combination weights were tuned to optimize BLEU." ></td>
	<td class="line x" title="186:212	Again, the best scores on each metric are obtained by the combination tuned for that metric." ></td>
	<td class="line o" title="187:212	Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F other combinations are better than any single system on all metrics apart from the METEOR tuned combinations." ></td>
	<td class="line o" title="188:212	The test set results follow clearly the tuning results again the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Chinese test TER BLEU MTR system A 56.57 29.63 56.63 system B 56.30 29.62 55.61 system C 59.48 31.32 57.71 system D 58.32 33.77 57.92 system E 58.46 32.40 59.75 system F 56.79 35.30 60.82 no weights 53.80 36.17 60.75 baseline 54.34 36.44 61.05 TER tuned 52.90 35.76 58.60 BLEU tuned 54.05 37.91 60.31 MTR tuned 72.59 26.96 64.35 Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05." ></td>
	<td class="line o" title="189:212	terms of METEOR." ></td>
	<td class="line x" title="190:212	Compared to the baseline, the BLEU score of the BLEU tuned combination is improved by 1.47 points." ></td>
	<td class="line n" title="191:212	Again, the METEOR tuned weights hurt the other metrics significantly." ></td>
	<td class="line x" title="192:212	8 Conclusions An improved confusion network decoding method combining the word posteriors with arbitrary features was presented." ></td>
	<td class="line x" title="193:212	This allows the addition of language model scores by expanding the lattices or re-scoring a63 -best lists." ></td>
	<td class="line x" title="194:212	The LM integration should result in more grammatical combination outputs." ></td>
	<td class="line x" title="195:212	Also, confusion networks generated by using the a0 -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores." ></td>
	<td class="line x" title="196:212	This guarantees that the best path will not be found from a network generated for a system with zero weight." ></td>
	<td class="line x" title="197:212	Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights." ></td>
	<td class="line x" title="198:212	The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks." ></td>
	<td class="line x" title="199:212	Compared to the baseline from (Rosti et al. , 2007), the new method improves the BLEU scores significantly." ></td>
	<td class="line o" title="200:212	The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR." ></td>
	<td class="line x" title="201:212	The TER tuning seems to yield very good results on Arabic the BLEU tuning seems to be better on Chinese." ></td>
	<td class="line n" title="202:212	It also seems like 318 METEOR should not be used in tuning due to high insertion rate and low precision." ></td>
	<td class="line x" title="203:212	It would be interesting to know which tuning metric results in the best translations in terms of human judgment." ></td>
	<td class="line x" title="204:212	However, this would require time consuming evaluations such as human mediated TER post-editing (Snover et al. , 2006)." ></td>
	<td class="line x" title="205:212	The improved confusion network decoding approach allows arbitrary features to be used in the combination." ></td>
	<td class="line x" title="206:212	New features may be added in the future." ></td>
	<td class="line x" title="207:212	Hypothesis alignment is also very important in confusion network generation." ></td>
	<td class="line x" title="208:212	Better alignment methods which take synonymy into account should be investigated." ></td>
	<td class="line x" title="209:212	This method could also benefit from more sophisticated word posterior estimation." ></td>
	<td class="line x" title="210:212	Acknowledgments This work was supported by DARPA/IPTO Contract No." ></td>
	<td class="line x" title="211:212	HR0011-06-C-0022 under the GALE program (approved for public release, distribution unlimited)." ></td>
	<td class="line x" title="212:212	The authors would like to thank ISI and University of Edinburgh for sharing their MT system outputs." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P07-1111
A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation
Albrecht, Joshua;Hwa, Rebecca;"></td>
	<td class="line x" title="1:176	Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880887, Prague, Czech Republic, June 2007." ></td>
	<td class="line x" title="2:176	c2007 Association for Computational Linguistics A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation Joshua S. Albrecht and Rebecca Hwa Department of Computer Science University of Pittsburgh {jsa8,hwa}@cs.pitt.edu Abstract Recent studies suggest that machine learning can be applied to develop good automatic evaluation metrics for machine translated sentences." ></td>
	<td class="line x" title="3:176	This paper further analyzes aspects of learning that impact performance." ></td>
	<td class="line x" title="4:176	We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality, but that regression-based learning produces more reliable metrics." ></td>
	<td class="line x" title="5:176	We demonstrate the feasibility of regression-based metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics." ></td>
	<td class="line x" title="6:176	1 Introduction As machine translation (MT) research advances, the importance of its evaluation also grows." ></td>
	<td class="line x" title="7:176	Efficient evaluation methodologies are needed both for facilitating the system development cycle and for providing an unbiased comparison between systems." ></td>
	<td class="line x" title="8:176	To this end, a number of automatic evaluation metrics have been proposed to approximate human judgments of MT output quality." ></td>
	<td class="line x" title="9:176	Although studies have shown them to correlate with human judgments at the document level, they are not sensitive enough to provide reliable evaluations at the sentence level (Blatz et al. , 2003)." ></td>
	<td class="line x" title="10:176	This suggests that current metrics do not fully reflect the set of criteria that people use in judging sentential translation quality." ></td>
	<td class="line x" title="11:176	A recent direction in the development of metrics for sentence-level evaluation is to apply machine learning to create an improved composite metric out of less indicative ones (Corston-Oliver et al. , 2001; Kulesza and Shieber, 2004)." ></td>
	<td class="line x" title="12:176	Under the assumption that good machine translation will produce human-like sentences, classifiers are trained to predict whether a sentence is authored by a human or by a machine based on features of that sentence, which may be the sentences scores from individual automatic evaluation metrics." ></td>
	<td class="line x" title="13:176	The confidence of the classifiers prediction can then be interpreted as a judgment on the translation quality of the sentence." ></td>
	<td class="line x" title="14:176	Thus, the composite metric is encoded in the confidence scores of the classification labels." ></td>
	<td class="line x" title="15:176	While the learning approach to metric design offers the promise of ease of combining multiple metrics and the potential for improved performance, several salient questions should be addressed more fully." ></td>
	<td class="line x" title="16:176	First, is learning a Human Likeness classifier the most suitable approach for framing the MTevaluation question?" ></td>
	<td class="line x" title="17:176	An alternative is regression, in which the composite metric is explicitly learned as a function that approximates humans quantitative judgments, based on a set of human evaluated training sentences." ></td>
	<td class="line x" title="18:176	Although regression has been considered on a small scale for a single system as confidence estimation (Quirk, 2004), this approach has not been studied as extensively due to scalability and generalization concerns." ></td>
	<td class="line x" title="19:176	Second, how does the diversity of the model features impact the learned metric?" ></td>
	<td class="line x" title="20:176	Third, how well do learning-based metrics generalize beyond their training examples?" ></td>
	<td class="line x" title="21:176	In particular, how well can a metric that was developed based 880 on one group of MT systems evaluate the translation qualities of new systems?" ></td>
	<td class="line x" title="22:176	In this paper, we argue for the viability of a regression-based framework for sentence-level MTevaluation." ></td>
	<td class="line x" title="23:176	Through empirical studies, we first show that having an accurate Human-Likeness classifier does not necessarily imply having a good MTevaluation metric." ></td>
	<td class="line x" title="24:176	Second, we analyze the resource requirement for regression models for different sizes of feature sets through learning curves." ></td>
	<td class="line x" title="25:176	Finally, we show that SVM-regression metrics generalize better than SVM-classification metrics in their evaluation of systems that are different from those in the training set (by languages and by years), and their correlations with human assessment are higher than standard automatic evaluation metrics." ></td>
	<td class="line x" title="26:176	2 MT Evaluation Recent automatic evaluation metrics typically frame the evaluation problem as a comparison task: how similar is the machine-produced output to a set of human-produced reference translations for the same source text?" ></td>
	<td class="line x" title="27:176	However, as the notion of similarity is itself underspecified, several different families of metrics have been developed." ></td>
	<td class="line x" title="28:176	First, similarity can be expressed in terms of string edit distances." ></td>
	<td class="line x" title="29:176	In addition to the well-known word error rate (WER), more sophisticated modifications have been proposed (Tillmann et al. , 1997; Snover et al. , 2006; Leusch et al. , 2006)." ></td>
	<td class="line x" title="30:176	Second, similarity can be expressed in terms of common word sequences." ></td>
	<td class="line x" title="31:176	Since the introduction of BLEU (Papineni et al. , 2002) the basic n-gram precision idea has been augmented in a number of ways." ></td>
	<td class="line oc" title="32:176	Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al. , 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used." ></td>
	<td class="line x" title="33:176	Finally, researchers have begun to look for similarities at a deeper structural level." ></td>
	<td class="line x" title="34:176	For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees." ></td>
	<td class="line x" title="35:176	With this wide array of metrics to choose from, MT developers need a way to evaluate them." ></td>
	<td class="line x" title="36:176	One possibility is to examine whether the automatic metric ranks the human reference translations highly with respect to machine translations (Lin and Och, 2004b; Amigo et al. , 2006)." ></td>
	<td class="line x" title="37:176	The reliability of a metric can also be more directly assessed by determining how well it correlates with human judgments of the same data." ></td>
	<td class="line x" title="38:176	For instance, as a part of the recent NIST sponsored MT Evaluation, each translated sentence by participating systems is evaluated by two (non-reference) human judges on a five point scale for its adequacy (does the translation retain the meaning of the original source text)?" ></td>
	<td class="line x" title="39:176	and fluency (does the translation sound natural in the target language?)." ></td>
	<td class="line x" title="40:176	These human assessment data are an invaluable resource for measuring the reliability of automatic evaluation metrics." ></td>
	<td class="line x" title="41:176	In this paper, we show that they are also informative in developing better metrics." ></td>
	<td class="line x" title="42:176	3 MT Evaluation with Machine Learning A good automatic evaluation metric can be seen as a computational model that captures a humans decision process in making judgments about the adequacy and fluency of translation outputs." ></td>
	<td class="line x" title="43:176	Inferring a cognitive model of human judgments is a challenging problem because the ultimate judgment encompasses a multitude of fine-grained decisions, and the decision process may differ slightly from person to person." ></td>
	<td class="line x" title="44:176	The metrics cited in the previous section aim to capture certain aspects of human judgments." ></td>
	<td class="line x" title="45:176	One way to combine these metrics in a uniform and principled manner is through a learning framework." ></td>
	<td class="line x" title="46:176	The individual metrics participate as input features, from which the learning algorithm infers a composite metric that is optimized on training examples." ></td>
	<td class="line x" title="47:176	Reframing sentence-level translation evaluation as a classification task was first proposed by Corston-Oliver et al.(2001)." ></td>
	<td class="line x" title="49:176	Interestingly, instead of recasting the classification problem as a Human Acceptability test (distinguishing good translations outputs from bad one), they chose to develop a Human-Likeness classifier (distinguishing outputs seem human-produced from machine-produced ones) to avoid the necessity of obtaining manually labeled training examples." ></td>
	<td class="line x" title="50:176	Later, Kulesza and Shieber (2004) noted that if a classifier provides a 881 confidence score for its output, that value can be interpreted as a quantitative estimate of the input instances translation quality." ></td>
	<td class="line x" title="51:176	In particular, they trained an SVM classifier that makes its decisions based on a set of input features computed from the sentence to be evaluated; the distance between input feature vector and the separating hyperplane then serves as the evaluation score." ></td>
	<td class="line x" title="52:176	The underlying assumption for both is that improving the accuracy of the classifier on the Human-Likeness test will also improve the implicit MT evaluation metric." ></td>
	<td class="line x" title="53:176	A more direct alternative to the classification approach is to learn via regression and explicitly optimize for a function (i.e. MT evaluation metric) that approximates human judgments in training examples." ></td>
	<td class="line x" title="54:176	Kulesza and Shieber (2004) raised two main objections against regression for MT evaluations." ></td>
	<td class="line x" title="55:176	One is that regression requires a large set of labeled training examples." ></td>
	<td class="line x" title="56:176	Another is that regression may not generalize well over time, and re-training may become necessary, which would require collecting additional human assessment data." ></td>
	<td class="line x" title="57:176	While these are legitimate concerns, we show through empirical studies (in Section 4.2) that the additional resource requirement is not impractically high, and that a regression-based metric has higher correlations with human judgments and generalizes better than a metric derived from a Human-Likeness classifier." ></td>
	<td class="line x" title="58:176	3.1 Relationship between Classification and Regression Classification and regression are both processes of function approximation; they use training examples as sample instances to learn the mapping from inputs to the desired outputs." ></td>
	<td class="line x" title="59:176	The major difference between classification and regression is that the function learned by a classifier is a set of decision boundaries by which to classify its inputs; thus its outputs are discrete." ></td>
	<td class="line x" title="60:176	In contrast, a regression model learns a continuous function that directly maps an input to a continuous value." ></td>
	<td class="line x" title="61:176	An MT evaluation metric is inherently a continuous function." ></td>
	<td class="line x" title="62:176	Casting the task as a 2-way classification may be too coarse-grained." ></td>
	<td class="line x" title="63:176	The Human-Likeness formulation of the problem introduces another layer of approximation by assuming equivalence between Like Human-Produced and Well-formed sentences." ></td>
	<td class="line x" title="64:176	In Section 4.1, we show empirically that high accuracy in the HumanLikeness test does not necessarily entail good MT evaluation judgments." ></td>
	<td class="line x" title="65:176	3.2 Feature Representation To ascertain the resource requirements for different model sizes, we considered two feature models." ></td>
	<td class="line x" title="66:176	The smaller one uses the same nine features as Kulesza and Shieber, which were derived from BLEU and WER." ></td>
	<td class="line x" title="67:176	The full model consists of 53 features: some are adapted from recently developed metrics; others are new features of our own." ></td>
	<td class="line o" title="68:176	They fall into the following major categories1: String-based metrics over references These include the nine Kulesza and Shieber features as well as precision, recall, and fragmentation, as calculated in METEOR; ROUGE-inspired features that are non-consecutive bigrams with a gap size of m, where 1  m  5 (skip-m-bigram), and ROUGE-L (longest common subsequence)." ></td>
	<td class="line x" title="69:176	Syntax-based metrics over references We unrolled HWCM into their individual chains of length c (where 2  c  4); we modified STM so that it is computed over unlexicalized constituent parse trees as well as over dependency parse trees." ></td>
	<td class="line x" title="70:176	String-based metrics over corpus Features in this category are similar to those in String-based metric over reference except that a large English corpus is used as reference instead." ></td>
	<td class="line x" title="71:176	Syntax-based metrics over corpus A large dependency treebank is used as the reference instead of parsed human translations." ></td>
	<td class="line x" title="72:176	In addition to adaptations of the Syntax-based metrics over references, we have also created features to verify the argument structures for certain syntactic categories." ></td>
	<td class="line x" title="73:176	4 Empirical Studies In these studies, the learning models used for both classification and regression are support vector machines (SVM) with Gaussian kernels." ></td>
	<td class="line x" title="74:176	All models are trained with SVM-Light (Joachims, 1999)." ></td>
	<td class="line x" title="75:176	Our primary experimental dataset is from NISTs 2003 1As feature engineering is not the primary focus of this paper, the features are briefly described here, but implementational details will be made available in a technical report." ></td>
	<td class="line x" title="76:176	882 Chinese MT Evaluations, in which the fluency and adequacy of 919 sentences produced by six MT systems are scored by two human judges on a 5-point scale2." ></td>
	<td class="line x" title="77:176	Because the judges evaluate sentences according to their individual standards, the resulting scores may exhibit a biased distribution." ></td>
	<td class="line x" title="78:176	We normalize human judges scores following the process described by Blatz et al.(2003)." ></td>
	<td class="line x" title="80:176	The overall human assessment score for a translation output is the average of the sum of two judges normalized fluency and adequacy scores." ></td>
	<td class="line x" title="81:176	The full dataset (6  919 = 5514 instances) is split into sets of training, heldout and test data." ></td>
	<td class="line x" title="82:176	Heldout data is used for parameter tuning (i.e. , the slack variable and the width of the Gaussian)." ></td>
	<td class="line x" title="83:176	When training classifiers, assessment scores are not used, and the training set is augmented with all available human reference translation sentences (4  919 = 3676 instances) to serve as positive examples." ></td>
	<td class="line x" title="84:176	To judge the quality of a metric, we compute Spearman rank-correlation coefficient, which is a real number ranging from -1 (indicating perfect negative correlations) to +1 (indicating perfect positive correlations), between the metrics scores and the averaged human assessments on test sentences." ></td>
	<td class="line x" title="85:176	We use Spearman instead of Pearson because it is a distribution-free test." ></td>
	<td class="line x" title="86:176	To evaluate the relative reliability of different metrics, we use bootstrapping re-sampling and paired t-test to determine whether the difference between the metrics correlation scores has statistical significance (at 99.8% confidence level)(Koehn, 2004)." ></td>
	<td class="line x" title="87:176	Each reported correlation rate is the average of 1000 trials; each trial consists of n sampled points, where n is the size of the test set." ></td>
	<td class="line x" title="88:176	Unless explicitly noted, the qualitative differences between metrics we report are statistically significant." ></td>
	<td class="line o" title="89:176	As a baseline comparison, we report the correlation rates of three standard automatic metrics: BLEU, METEOR, which incorporates recall and stemming, and HWCM, which uses syntax." ></td>
	<td class="line x" title="90:176	BLEU is smoothed to be more appropriate for sentencelevel evaluation (Lin and Och, 2004b), and the bigram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included." ></td>
	<td class="line x" title="91:176	This phenomenon has 2This corpus is available from the Linguistic Data Consortium as Multiple Translation Chinese Part 4." ></td>
	<td class="line x" title="92:176	0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 45 50 55 60 65 70 75 80 85Correlation Coefficient with Human Judgement (R) Human-Likeness Classifier Accuracy (%) Figure 1: This scatter plot compares classifiers accuracy with their corresponding metrics correlations with human assessments been previously observed by Liu and Gildea (2005)." ></td>
	<td class="line x" title="93:176	4.1 Relationship between Classification Accuracy and Quality of Evaluation Metric A concern in using a metric derived from a HumanLikeness classifier is whether it would be predictive for MT evaluation." ></td>
	<td class="line x" title="94:176	Kulesza and Shieber (2004) tried to demonstrate a positive correlation between the Human-Likeness classification task and the MT evaluation task empirically." ></td>
	<td class="line x" title="95:176	They plotted the classification accuracy and evaluation reliability for a number of classifiers, which were generated as a part of a greedy search for kernel parameters and found some linear correlation between the two." ></td>
	<td class="line x" title="96:176	This proof of concept is a little misleading, however, because the population of the sampled classifiers was biased toward those from the same neighborhood as the local optimal classifier (so accuracy and correlation may only exhibit linear relationship locally)." ></td>
	<td class="line x" title="97:176	Here, we perform a similar study except that we sampled the kernel parameter more uniformly (on a log scale)." ></td>
	<td class="line x" title="98:176	As Figure 1 confirms, having an accurate Human-Likeness classifier does not necessarily entail having a good MT evaluation metric." ></td>
	<td class="line x" title="99:176	Although the two tasks do seem to be positively related, and in the limit there may be a system that is good at both tasks, one may improve classification without improving MT evaluation." ></td>
	<td class="line x" title="100:176	For this set of heldout data, at the near 80% accuracy range, a derived metric might have an MT evaluation correlation coefficient anywhere between 0.25 (on par with 883 unsmoothed BLEU, which is known to be unsuitable for sentence-level evaluation) and 0.35 (competitive with standard metrics)." ></td>
	<td class="line x" title="101:176	4.2 Learning Curves To investigate the feasibility of training regression models from assessment data that are currently available, we consider both a small and a large regression model." ></td>
	<td class="line x" title="102:176	The smaller model consists of nine features (same as the set used by Kulesza and Shieber); the other uses the full set of 53 features as described in Section 3.2." ></td>
	<td class="line x" title="103:176	The reliability of the trained metrics are compared with those developed from Human-Likeness classifiers." ></td>
	<td class="line x" title="104:176	We follow a similar training and testing methodology as previous studies: we held out 1/6 of the assessment dataset for SVM parameter tuning; five-fold cross validation is performed with the remaining sentences." ></td>
	<td class="line x" title="105:176	Although the metrics are evaluated on unseen test sentences, the sentences are produced by the same MT systems that produced the training sentences." ></td>
	<td class="line x" title="106:176	In later experiments, we investigate generalizing to more distant MT systems." ></td>
	<td class="line x" title="107:176	Figure 2(a) shows the learning curves for the two regression models." ></td>
	<td class="line o" title="108:176	As the graph indicates, even with a limited amount of human assessment data, regression models can be trained to be comparable to standard metrics (represented by METEOR in the graph)." ></td>
	<td class="line x" title="109:176	The small feature model is close to convergence after 1000 training examples3." ></td>
	<td class="line n" title="110:176	The model with a more complex feature set does require more training data, but its correlation began to overtake METEOR after 2000 training examples." ></td>
	<td class="line x" title="111:176	This study suggests that the start-up cost of building even a moderately complex regression model is not impossibly high." ></td>
	<td class="line x" title="112:176	Although we cannot directly compare the learning curves of the Human-Likeness classifiers to those of the regression models (since the classifiers training examples are automatically labeled), training examples for classifiers are not entirely free: human reference translations still must be developed for the source sentences." ></td>
	<td class="line x" title="113:176	Figure 2(c) shows the learning curves for training Human-Likeness classifiers (in terms of improving a classifiers accuracy) using the same two feature sets, and Figure 2(b) shows the 3The total number of labeled examples required is closer to 2000, since the heldout set uses 919 labeled examples." ></td>
	<td class="line x" title="114:176	correlations of the metrics derived from the corresponding classifiers." ></td>
	<td class="line p" title="115:176	The pair of graphs show, especially in the case of the larger feature set, that a large improvement in classification accuracy does not bring proportional improvement in its corresponding metricss correlation; with an accuracy of near 90%, its correlation coefficient is 0.362, well below METEOR." ></td>
	<td class="line x" title="116:176	This experiment further confirms that judging Human-Likeness and judging Human-Acceptability are not tightly coupled." ></td>
	<td class="line x" title="117:176	Earlier, we have shown in Figure 1 that different SVM parameterizations may result in classifiers with the same accuracy rate but different correlations rates." ></td>
	<td class="line x" title="118:176	As a way to incorporate some assessment information into classification training, we modify the parameter tuning process so that SVM parameters are chosen to optimize for assessment correlations in the heldout data." ></td>
	<td class="line x" title="119:176	By incurring this small amount of human assessed data, this parameter search improves the classifiers correlations: the metric using the smaller feature set increased from 0.423 to 0.431, and that of the larger set increased from 0.361 to 0.422." ></td>
	<td class="line x" title="120:176	4.3 Generalization We conducted two generalization studies." ></td>
	<td class="line x" title="121:176	The first investigates how well the trained metrics evaluate systems from other years and systems developed for a different source language." ></td>
	<td class="line x" title="122:176	The second study delves more deeply into how variations in the training examples affect a learned metrics ability to generalize to distant systems." ></td>
	<td class="line x" title="123:176	The learning models for both experiments use the full feature set." ></td>
	<td class="line x" title="124:176	Cross-Year Generalization To test how well the learning-based metrics generalize to systems from different years, we trained both a regression-based metric (R03) and a classifier-based metric (C03) with the entire NIST 2003 Chinese dataset (using 20% of the data as heldout4)." ></td>
	<td class="line x" title="125:176	All metrics are then applied to three new datasets: NIST 2002 Chinese MT Evaluation (3 systems, 2634 sentences total), NIST 2003 Arabic MT Evaluation (2 systems, 1326 sentences total), and NIST 2004 Chinese MT Evaluation (10 systems, 4470 sentences total)." ></td>
	<td class="line x" title="126:176	The results 4Here, too, we allowed the classifiers parameters to be tuned for correlation with human assessment on the heldout data rather than accuracy." ></td>
	<td class="line x" title="127:176	884 (a) (b) (c) Figure 2: Learning curves: (a) correlations with human assessment using regression models; (b) correlations with human assessment using classifiers; (c) classifier accuracy on determining Human-Likeness." ></td>
	<td class="line x" title="128:176	Dataset R03 C03 BLEU MET." ></td>
	<td class="line x" title="129:176	HWCM 2002 Ara 0.466 0.384 0.423 0.431 0.424 2002 Chn 0.309 0.250 0.269 0.290 0.260 2004 Chn 0.602 0.566 0.588 0.563 0.546 Table 1: Correlations for cross-year generalization." ></td>
	<td class="line x" title="130:176	Learning-based metrics are developed from NIST 2003 Chinese data." ></td>
	<td class="line x" title="131:176	All metrics are tested on datasets from 2003 Arabic, 2002 Chinese and 2004 Chinese." ></td>
	<td class="line x" title="132:176	are summarized in Table 1." ></td>
	<td class="line x" title="133:176	We see that R03 consistently has a better correlation rate than the other metrics." ></td>
	<td class="line x" title="134:176	At first, it may seem as if the difference between R03 and BLEU is not as pronounced for the 2004 dataset, calling to question whether a learned metric might become quickly out-dated, we argue that this is not the case." ></td>
	<td class="line x" title="135:176	The 2004 dataset has many more participating systems, and they span a wider range of qualities." ></td>
	<td class="line x" title="136:176	Thus, it is easier to achieve a high rank correlation on this dataset than previous years because most metrics can qualitatively discern that sentences from one MT system are better than those from another." ></td>
	<td class="line x" title="137:176	In the next experiment, we examine the performance of R03 with respect to each MT system in the 2004 dataset and show that its correlation rate is higher for better MT systems." ></td>
	<td class="line x" title="138:176	Relationship between Training Examples and Generalization Table 2 shows the result of a generalization study similar to before, except that correlations are performed on each system." ></td>
	<td class="line x" title="139:176	The rows order the test systems by their translation qualities from the best performing system (2004-Chn1, whose average human assessment score is 0.655 out of 1.0) to the worst (2004-Chn10, whose score is 0.255)." ></td>
	<td class="line x" title="140:176	In addition to the regression metric from the previous experiment (R03-all), we consider two more regression metrics trained from subsets of the 2003 dataset: R03-Bottom5 is trained from the subset that excludes the best 2003 MT system, and R03Top5 is trained from the subset that excludes the worst 2003 MT system." ></td>
	<td class="line x" title="141:176	We first observe that on a per test-system basis, the regression-based metrics generally have better correlation rates than BLEU, and that the gap is as wide as what we have observed in the earlier crossyears studies." ></td>
	<td class="line x" title="142:176	The one exception is when evaluating 2004-Chn8." ></td>
	<td class="line x" title="143:176	None of the metrics seems to correlate very well with human judges on this system." ></td>
	<td class="line x" title="144:176	Because the regression-based metric uses these individual metrics as features, its correlation also suffers." ></td>
	<td class="line x" title="145:176	During regression training, the metric is optimized to minimize the difference between its prediction and the human assessments of the training data." ></td>
	<td class="line x" title="146:176	If the input feature vector of a test instance is in a very distant space from training examples, the chance for error is higher." ></td>
	<td class="line x" title="147:176	As seen from the results, the learned metrics typically perform better when the training examples include sentences from higher-quality systems." ></td>
	<td class="line x" title="148:176	Consider, for example, the differences between R03-all and R03-Top5 versus the differences between R03-all and R03-Bottom5." ></td>
	<td class="line x" title="149:176	Both R03-Top5 and R03-Bottom5 differ from R03all by one subset of training examples." ></td>
	<td class="line x" title="150:176	Since R03alls correlation rates are generally closer to R03Top5 than to R03-Bottom5, we see that having seen extra training examples from a bad system is not as harmful as having not seen training examples from a good system." ></td>
	<td class="line o" title="151:176	This is expected, since there are many ways to create bad translations, so seeing a partic885 R03-all R03-Bottom5 R03-Top5 BLEU METEOR HWCM 2004-Chn1 0.495 0.460 0.518 0.456 0.457 0.444 2004-Chn2 0.398 0.330 0.440 0.352 0.347 0.344 2004-Chn3 0.425 0.389 0.459 0.369 0.402 0.369 2004-Chn4 0.432 0.392 0.434 0.400 0.400 0.362 2004-Chn5 0.452 0.441 0.443 0.370 0.426 0.326 2004-Chn6 0.405 0.392 0.406 0.390 0.357 0.380 2004-Chn7 0.443 0.432 0.448 0.390 0.408 0.392 2004-Chn8 0.237 0.256 0.256 0.265 0.259 0.179 2004-Chn9 0.581 0.569 0.591 0.527 0.537 0.535 2004-Chn10 0.314 0.313 0.354 0.321 0.303 0.358 2004-all 0.602 0.567 0.617 0.588 0.563 0.546 Table 2: Metric correlations within each system." ></td>
	<td class="line x" title="152:176	The columns specify which metric is used." ></td>
	<td class="line x" title="153:176	The rows specify which MT system is under evaluation; they are ordered by human-judged system quality, from best to worst." ></td>
	<td class="line x" title="154:176	For each evaluated MT system (row), the highest coefficient in bold font, and those that are statistically comparable to the highest are shown in italics." ></td>
	<td class="line x" title="155:176	ular type of bad translations from one system may not be very informative." ></td>
	<td class="line x" title="156:176	In contrast, the neighborhood of good translations is much smaller, and is where all the systems are aiming for; thus, assessments of sentences from a good system can be much more informative." ></td>
	<td class="line x" title="157:176	4.4 Discussion Experimental results confirm that learning from training examples that have been doubly approximated (class labels instead of ordinals, humanlikeness instead of human-acceptability) does negatively impact the performance of the derived metrics." ></td>
	<td class="line x" title="158:176	In particular, we showed that they do not generalize as well to new data as metrics trained from direct regression." ></td>
	<td class="line x" title="159:176	We see two lingering potential objections toward developing metrics with regression-learning." ></td>
	<td class="line x" title="160:176	One is the concern that a system under evaluation might try to explicitly game the metric5. This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al. , 2006)." ></td>
	<td class="line x" title="161:176	In a learning framework, potential pitfalls for individual metrics are ameliorated through a combination of evidences." ></td>
	<td class="line x" title="162:176	That said, it is still prudent to defend against the potential of a system gaming a subset of the features." ></td>
	<td class="line x" title="163:176	For example, our fluency-predictor features are not strong indicators of translation qualities by themselves." ></td>
	<td class="line x" title="164:176	We want to avoid training a metric that as5Or, in a less adversarial setting, a system may be performing minimum error-rate training (Och, 2003) signs a higher than deserving score to a sentence that just happens to have many n-gram matches against the target-language reference corpus." ></td>
	<td class="line x" title="165:176	This can be achieved by supplementing the current set of human assessed training examples with automatically assessed training examples, similar to the labeling process used in the Human-Likeness classification framework." ></td>
	<td class="line x" title="166:176	For instance, as negative training examples, we can incorporate fluent sentences that are not adequate translations and assign them low overall assessment scores." ></td>
	<td class="line x" title="167:176	A second, related concern is that because the metric is trained on examples from current systems using currently relevant features, even though it generalizes well in the near term, it may not continue to be a good predictor in the distant future." ></td>
	<td class="line x" title="168:176	While periodic retraining may be necessary, we see value in the flexibility of the learning framework, which allows for new features to be added." ></td>
	<td class="line x" title="169:176	Moreover, adaptive learning methods may be applicable if a small sample of outputs of some representative translation systems is manually assessed periodically." ></td>
	<td class="line x" title="170:176	5 Conclusion Human judgment of sentence-level translation quality depends on many criteria." ></td>
	<td class="line x" title="171:176	Machine learning affords a unified framework to compose these criteria into a single metric." ></td>
	<td class="line x" title="172:176	In this paper, we have demonstrated the viability of a regression approach to learning the composite metric." ></td>
	<td class="line x" title="173:176	Our experimental results show that by training from some human as886 sessments, regression methods result in metrics that have better correlations with human judgments even as the distribution of the tested population changes." ></td>
	<td class="line x" title="174:176	Acknowledgments This work has been supported by NSF Grants IIS-0612791 and IIS-0710695." ></td>
	<td class="line x" title="175:176	We would like to thank Regina Barzilay, Ric Crabbe, Dan Gildea, Alex Kulesza, Alon Lavie, and Matthew Stone as well as the anonymous reviewers for helpful comments and suggestions." ></td>
	<td class="line x" title="176:176	We are also grateful to NIST for making their assessment data available to us." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0411
Dependency-Based Automatic Evaluation for Machine Translation
Owczarzak, Karolina;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:166	Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 8087, Rochester, New York, April 2007." ></td>
	<td class="line x" title="2:166	c2007 Association for Computational Linguistics Dependency-Based Automatic Evaluation for Machine Translation Karolina Owczarzak Josef van Genabith Andy Way National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland {owczarzak,josef,away}@computing.dcu.ie Abstract We present a novel method for evaluating the output of Machine Translation (MT), based on comparing the dependency structures of the translation and reference rather than their surface string forms." ></td>
	<td class="line x" title="3:166	Our method uses a treebank-based, widecoverage, probabilistic Lexical-Functional Grammar (LFG) parser to produce a set of structural dependencies for each translation-reference sentence pair, and then calculates the precision and recall for these dependencies." ></td>
	<td class="line x" title="4:166	Our dependencybased evaluation, in contrast to most popular string-based evaluation metrics, will not unfairly penalize perfectly valid syntactic variations in the translation." ></td>
	<td class="line x" title="5:166	In addition to allowing for legitimate syntactic differences, we use paraphrases in the evaluation process to account for lexical variation." ></td>
	<td class="line x" title="6:166	In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores." ></td>
	<td class="line x" title="7:166	An experiment with two translations of 4,000 sentences from Spanish-English Europarl shows that, in contrast to most other metrics, our method does not display a high bias towards statistical models of translation." ></td>
	<td class="line x" title="8:166	1 Introduction Since their appearance, string-based evaluation metrics such as BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002) have been the standard tools used for evaluating MT quality." ></td>
	<td class="line x" title="9:166	Both score a candidate translation on the basis of the number of n-grams shared with one or more reference translations." ></td>
	<td class="line x" title="10:166	Automatic measures are indispensable in the development of MT systems, because they allow MT developers to conduct frequent, costeffective, and fast evaluations of their evolving models." ></td>
	<td class="line x" title="11:166	These advantages come at a price, though: an automatic comparison of n-grams measures only the string similarity of the candidate translation to one or more reference strings, and will penalize any divergence from them." ></td>
	<td class="line x" title="12:166	In effect, a candidate translation expressing the source meaning accurately and fluently will be given a low score if the lexical and syntactic choices it contains, even though perfectly legitimate, are not present in at least one of the references." ></td>
	<td class="line x" title="13:166	Necessarily, this score would differ from a much more favourable human judgement that such a translation would receive." ></td>
	<td class="line x" title="14:166	The limitations of string comparison are the reason why it is advisable to provide multiple references for a candidate translation in BLEUor NIST-based evaluations." ></td>
	<td class="line x" title="15:166	While Zhang and Vogel (2004) argue that increasing the size of the test set gives even more reliable system scores than multiple references, this still does not solve the inadequacy of BLEU and NIST for sentence-level or small set evaluation." ></td>
	<td class="line x" title="16:166	In addition, in practice even a number of references do not capture the whole potential variability of the translation." ></td>
	<td class="line x" title="17:166	Moreover, when designing a statistical MT system, the need for large amounts of training data limits the researcher to collections of parallel corpora such as Europarl (Koehn, 2005), which provides only one reference, namely the target text; and the cost of creating additional reference translations of the test set, usually a few thousand sentences long, is often prohibitive." ></td>
	<td class="line x" title="18:166	Therefore, it would be desirable to find an evaluation method that accepts legitimate syntactic and lexical differences 80 between the translation and the reference, thus better mirroring human assessment." ></td>
	<td class="line x" title="19:166	In this paper, we present a novel method that automatically evaluates the quality of translation based on the dependency structure of the sentence, rather than its surface form." ></td>
	<td class="line x" title="20:166	Dependencies abstract away from the particulars of the surface string (and CFG tree) realization and provide a normalized representation of (some) syntactic variants of a given sentence." ></td>
	<td class="line x" title="21:166	The translation and reference files are analyzed by a treebank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al. , 2004), which produces a set of dependency triples for each input." ></td>
	<td class="line x" title="22:166	The translation set is compared to the reference set, and the number of matches is calculated, giving the precision, recall, and f-score for that particular translation." ></td>
	<td class="line x" title="23:166	In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al.(2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score." ></td>
	<td class="line oc" title="25:166	Comparing the LFG-based evaluation method with other popular metrics: BLEU, NIST, General Text Matcher (GTM) (Turian et al. , 2003), Translation Error Rate (TER) (Snover et al. , 2006)1, and METEOR (Banerjee and Lavie, 2005), we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment." ></td>
	<td class="line x" title="26:166	The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of two experiments on different sets of data: 4,000 sentences from Spanish-English Europarl and 16,800 sentences of Chinese-English newswire text from the Linguistic Data Consortiums (LDC) Multiple Translation project; Section 5 discusses ongoing work; Section 6 concludes." ></td>
	<td class="line x" title="27:166	1 As we focus on purely automatic metrics, we omit HTER (Human-Targeted Translation Error Rate) here." ></td>
	<td class="line x" title="28:166	2 Lexical-Functional Grammar In Lexical-Functional Grammar (Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="29:166	C-structure represents the surface string word order and the hierarchical organisation of phrases in terms of CFG trees." ></td>
	<td class="line x" title="30:166	F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), approximating to predicate-argument structure or simple logical forms." ></td>
	<td class="line x" title="31:166	C-structure and f-structure are related in terms of functional annotations (attribute-value structure equations) in c-structure trees, describing f-structures." ></td>
	<td class="line x" title="32:166	While c-structure is sensitive to surface word order, f-structure is not." ></td>
	<td class="line x" title="33:166	The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures, shown in (1)." ></td>
	<td class="line x" title="34:166	(1) C-structure: F-structure: S NP VP | John V NP-TMP | | resigned yesterday SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} S NP NP VP | | | Yesterday John V | resigned SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} Notice that if these two sentences were a translation-reference pair, they would receive a less-than-perfect score from string-based metrics." ></td>
	<td class="line x" title="35:166	For example, BLEU with add-one smoothing2 gives this pair a score of barely 0.3781." ></td>
	<td class="line x" title="36:166	The f-structure can also be described as a flat set of triples." ></td>
	<td class="line x" title="37:166	In triples format, the f-structure in (1) could be represented as follows: {subj(resign, john), pers(john, 3), num(john, sg), tense(resign, 2 We use smoothing because the original BLEU gives zero points to sentences with fewer than one four-gram." ></td>
	<td class="line x" title="38:166	81 past), adj(resign, yesterday), pers(yesterday, 3), num(yesterday, sg)}." ></td>
	<td class="line x" title="39:166	Cahill et al.(2004) presents Penn-II Treebankbased LFG parsing resources." ></td>
	<td class="line x" title="41:166	Her approach distinguishes 32 types of dependencies, including grammatical functions and morphological information." ></td>
	<td class="line x" title="42:166	This set can be divided into two major groups: a group of predicate-only dependencies and non-predicate dependencies." ></td>
	<td class="line x" title="43:166	Predicate-only dependencies are those whose path ends in a predicate-value pair, describing grammatical relations." ></td>
	<td class="line x" title="44:166	For example, for the f-structure in (1), predicate-only dependencies would include: {subj(resign, john), adj(resign, yesterday)}.3 In parser evaluation, the quality of the fstructures produced automatically can be checked against a set of gold standard sentences annotated with f-structures by a linguist." ></td>
	<td class="line x" title="45:166	The evaluation is conducted by calculating the precision and recall between the set of dependencies produced by the parser, and the set of dependencies derived from the human-created f-structure." ></td>
	<td class="line x" title="46:166	Usually, two versions of f-score are calculated: one for all the dependencies for a given input, and a separate one for the subset of predicate-only dependencies." ></td>
	<td class="line x" title="47:166	In this paper, we use the parser developed by Cahill et al.(2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates." ></td>
	<td class="line x" title="49:166	4 3 Related work The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al.(2006), but the criticism is widespread." ></td>
	<td class="line x" title="51:166	Even the creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al. , 2002)." ></td>
	<td class="line x" title="52:166	A side 3 Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, relative clause pronoun." ></td>
	<td class="line x" title="53:166	The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, number, verbal particle, participle, passive, person, pronoun surface form, tense, infinitival clause." ></td>
	<td class="line x" title="54:166	4 http://lfg-demo.computing.dcu.ie/lfgparser.html effect of this phenomenon is that BLEU is less reliable for smaller data sets, so the advantage it provides in the speed of evaluation is to some extent counterbalanced by the time spent by developers on producing a sufficiently large test set in order to obtain a reliable score for their system." ></td>
	<td class="line x" title="55:166	Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics." ></td>
	<td class="line x" title="56:166	Some of them concentrate mainly on word order, like General Text Matcher (Turian et al. , 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al. , 2005), which computes the number of substitutions, inserts, deletions, and shifts necessary to transform the translation text to match the reference." ></td>
	<td class="line oc" title="57:166	Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al. , 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy." ></td>
	<td class="line x" title="58:166	Kauchak and Barzilay (2006) and Owczarzak et al.(2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet5 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="61:166	Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al.(2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching." ></td>
	<td class="line x" title="63:166	Kulesza and Schieber (2004), on the other hand, train a Support Vector Machine using features like proportion of n-gram matches and word error rate to judge a given translations distance from human-level quality." ></td>
	<td class="line x" title="64:166	Nevertheless, these metrics use only stringbased comparisons, even while taking into consideration reordering." ></td>
	<td class="line x" title="65:166	By contrast, our dependency-based method concentrates on utilizing linguistic structure to establish a comparison between translated sentences and their reference." ></td>
	<td class="line x" title="66:166	5 http://wordnet.princeton.edu/ 82 4 LFG f-structure in MT evaluation The process underlying the evaluation of fstructure quality against a gold standard can be used in automatic MT evaluation as well: we parse the translation and the reference, and then, for each sentence, we check the set of translation dependencies against the set of reference dependencies, counting the number of matches." ></td>
	<td class="line x" title="67:166	As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair." ></td>
	<td class="line x" title="68:166	Because we are comparing two outputs that were produced automatically, there is a possibility that the result will not be noise-free." ></td>
	<td class="line x" title="69:166	To assess the amount of noise that the parser may introduce we conducted an experiment where 100 English Europarl sentences were modified by hand in such a way that the position of adjuncts was changed, but the sentence remained grammatical and the meaning was not changed." ></td>
	<td class="line x" title="70:166	This way, an ideal parser should give both the source and the modified sentence the same fstructure, similarly to the case presented in (1)." ></td>
	<td class="line x" title="71:166	The modified sentences were treated like a translation file, and the original sentences played the part of the reference." ></td>
	<td class="line x" title="72:166	Each set was run through the parser." ></td>
	<td class="line o" title="73:166	We evaluated the dependency triples obtained from the translation against the dependency triples for the reference, calculating the f-score, and applied other metrics (TER, METEOR, BLEU, NIST, and GTM) to the set in order to compare scores." ></td>
	<td class="line x" title="74:166	The results, inluding the distinction between f-scores for all dependencies and predicate-only dependencies, appear in Table 1." ></td>
	<td class="line o" title="75:166	baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1." ></td>
	<td class="line x" title="76:166	Scores for sentences with reordered adjuncts The baseline column shows the upper bound for a given metric: the score which a perfect translation, word-for-word identical to the reference, would obtain.6 In the other column we list the scores that the metrics gave to the translation containing reordered adjunct." ></td>
	<td class="line x" title="77:166	As can be seen, the dependency and predicate-only dependency scores are lower than the perfect 100, reflecting the noise introduced by the parser." ></td>
	<td class="line x" title="78:166	To show the difference between the scoring based on LFG dependencies and other metrics in an ideal situation, we created another set of a hundred sentences with reordered adjuncts, but this time selecting only those reordered sentences that were given the same set of dependencies by the parser (in other words, we simulated having the ideal parser)." ></td>
	<td class="line x" title="79:166	As can be seen in Table 2, other metrics are still unable to tolerate legitimate variation in the position of adjuncts, because the sentence surface form differs from the reference; however, it is not treated as an error by the parser." ></td>
	<td class="line o" title="80:166	baseline modified TER 0.0 7.841 METEOR 1.0 0.9956 BLEU 1.0000 0.8485 NIST 11.1690 10.7422 (96.18%) GTM 100 99.35 dep f-score 100 100 dep_preds f-score 100 100 Table 2." ></td>
	<td class="line x" title="81:166	Scores for sentences with reordered adjuncts in an ideal situation 4.1 Initial experiment  Europarl In the first experiment, we attempted to determine whether the dependency-based measure is biased towards statistical MT output, a problem that has been observed for n-gram-based metrics like BLEU and NIST." ></td>
	<td class="line x" title="82:166	Callison-Burch et al.(2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaohs translation." ></td>
	<td class="line x" title="84:166	Others repeatedly 6 Two things have to be noted here: (1) in case of NIST the perfect score differs from text to text, which is why we provide the percentage points as well, and (2) in case of TER the lower the score, the better the translation, so the perfect translation will receive 0, and there is no upper bound on the score, which makes this particular metric extremely difficult to directly compare with others." ></td>
	<td class="line x" title="85:166	83 observed this tendency in previous research as well; in one experiment, reported in Owczarzak et al.(2006), where the rule-based system Logomedia7 was compared with Pharaoh, BLEU scored Pharaoh 0.0349 points higher, NIST scored Pharaoh 0.6219 points higher, but human judges scored Logomedia output 0.19 points higher (on a 5-point scale)." ></td>
	<td class="line x" title="87:166	4.1.1 Experimental design In order to check for the existence of a bias in the dependency-based metric, we created a set of 4,000 sentences drawn randomly from the SpanishEnglish subset of Europarl (Koehn, 2005), and we produced two translations: one by a rule-based system Logomedia, and the other by the standard phrase-based statistical decoder Pharaoh, using alignments produced by GIZA++8 and the refined word alignment strategy of Och and Ney (2003)." ></td>
	<td class="line o" title="88:166	The translations were scored with a range of metrics: BLEU, NIST, GTM, TER, METEOR, and the dependency-based method." ></td>
	<td class="line x" title="89:166	4.1.2 Adding synonyms Besides the ability to allow syntactic variants as valid translations, a good metric should also be able to accept legitimate lexical variation." ></td>
	<td class="line x" title="90:166	We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al.(2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006))." ></td>
	<td class="line x" title="92:166	Bitext-derived paraphrases Owczarzak et al.(2006) describe a simple way to produce a list of paraphrases, which can be useful in MT evaluation, by running word alignment software on the test set that is being evaluated." ></td>
	<td class="line x" title="94:166	Paraphrases derived in this way are specific to the domain at hand and contain low-level syntactic variants in addition to word-level synonymy." ></td>
	<td class="line x" title="95:166	Using the standard GIZA++ software and the refined word alignment strategy of Och and Ney (2003) on our test set of 4,000 Spanish-English sentences, the method generated paraphrases for just over 1100 items." ></td>
	<td class="line x" title="96:166	These paraphrases served to 7 http://www.lec.com/ 8 http://www.fjoch.com/GIZA++ create new individual best-matching references for the Logomedia and Pharaoh translations." ></td>
	<td class="line x" title="97:166	Due to the small size of the paraphrase set, only about 20% of reference sentences were actually modified to better reflect the translation." ></td>
	<td class="line x" title="98:166	This, in turn, led to little difference in scores." ></td>
	<td class="line x" title="99:166	WordNet synonyms To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation." ></td>
	<td class="line o" title="100:166	In addition, METEOR also has an option of including WordNet in the evaluation process." ></td>
	<td class="line x" title="101:166	As in the case of bitext-derived paraphrases, we used WordNet synonyms to create new best-matching references for each of the two translations." ></td>
	<td class="line x" title="102:166	This time, given the extensive database containing synonyms for over 150,000 items, around 70% of reference sentences were modified: 67% for Pharaoh, and 75% for Logomedia." ></td>
	<td class="line x" title="103:166	Note that the number of substitutions is higher for Logomedia; this confirms the intuition that the translation produced by Pharaoh, trained on the domain which is also the source of the reference text, will need fewer lexical replacements than Logomedia, which is based on a general non-domain-specific model." ></td>
	<td class="line x" title="104:166	4.1.3 Results Table 3 shows the difference between the scores which Pharaohs and Logomedias translations obtained from each metric: a positive number shows by how much Pharaohs score was higher than Logomedias, and a negative number reflects Logomedias higher score (the percentages are absolute values)." ></td>
	<td class="line o" title="105:166	As can be seen, all the metrics scored Pharaoh higher, inlcuding METEOR and the dependency-based method that were boosted with WordNet." ></td>
	<td class="line x" title="106:166	The values in the table are sorted in descending order, from the largest to the lowest advantage of Pharaoh over Logomedia." ></td>
	<td class="line p" title="107:166	Interestingly, next to METEOR boosted with WordNet, it is the dependency-based method, and especially the predicates-only version, that shows the least bias towards the phrase-based translation." ></td>
	<td class="line x" title="108:166	In the next step, we selected from this set smaller subsets of sentences that were more and more similar in terms of translation quality (as determined by a sentences BLEU score)." ></td>
	<td class="line x" title="109:166	As the similarity of the translation quality increased, most metrics lowered their bias, as is shown in Table 4." ></td>
	<td class="line x" title="110:166	The first column shows the case where the sentences chosen differed at the most by 0.05 84 points BLEU score; in the second column the difference was lowered to 0.01; and in the third column to 0.005." ></td>
	<td class="line x" title="111:166	The numbers following the hash signs in the header row indicate the number of sentences in a given set." ></td>
	<td class="line o" title="112:166	metric PH score  LM score TER 1.997 BLEU 7.16% NIST 6.58% dep 4.93% dep+paraphr 4.80% GTM 3.89% METEOR 3.80% dep_preds 3.79% dep+paraphr_preds 3.70% dep+WordNet 3.55% dep+WordNet_preds 2.60% METEOR+WordNet 1.56% Table 3." ></td>
	<td class="line x" title="113:166	Difference between scores assigned to Pharaoh and Logomedia." ></td>
	<td class="line x" title="114:166	Positive numbers show by how much Pharaohs score was higher than Logomedias. Legend: dep = dependency f-score, paraph = paraphrases, _preds = predicate-only f-score." ></td>
	<td class="line x" title="115:166	~ 0.05 #1692 ~ 0.01 #567 ~ 0.005 #335 NIST 2.29% NIST 1.76% NIST 1.48% BLEU 0.95% BLEU 0.42% BLEU 0.59% GTM 0.94% GTM 0.29% GTM -0.09% d+p 0.67% d 0.04% d+p -0.15% d 0.61% d+p 0.02% d -0.24% d+WN -0.29% d+WN -0.78% d+WN -0.99% d+p_pr -0.70% M -0.99% d+p_pr -1.30% d_pr -0.75% d_pr -1.37% d_pr -1.43% M -1.03% d+p_pr -1.38% M -1.57% d+WN_pr -1.43% d+WN_pr -1.97% d+WN_pr -1.94% M+WN -2.51% M+WN -2.21% M+WN -2.74% TER -1.579 TER -1.228 TER -1.739 Table 4." ></td>
	<td class="line x" title="116:166	Difference between scores assigned to Pharaoh and Logomedia for sets of increasing similarity." ></td>
	<td class="line x" title="117:166	Positive numbers show Pharaohs advantage, negative numbers show Logomedias advantage." ></td>
	<td class="line x" title="118:166	Legend: d = dependency fscore, p = paraphrases, _pr = predicate-only f-score, M = METEOR, WN = WordNet." ></td>
	<td class="line x" title="119:166	These results confirm earlier suggestions that the predicate-only version of the dependencybased evaluation is less biased in favour of the statistical MT system than the version that includes all dependency types." ></td>
	<td class="line x" title="120:166	Adding a sufficient number of lexical choices reduces the bias even further; although again, paraphrases generated from the test set only are too few to make a significant difference." ></td>
	<td class="line p" title="121:166	Similarly to METEOR, the dependency-based method shows on the whole lower bias than other metrics." ></td>
	<td class="line x" title="122:166	However, we cannot be certain that the underlying scores vary linearly with each other and with human judgements, as we have no framework of reference such as human segment-level assessment of translation quality in this case." ></td>
	<td class="line x" title="123:166	Therefore, the correlation with human judgement is analysed in our next experiment." ></td>
	<td class="line x" title="124:166	4.2 Correlation with human judgement  MultiTrans To calculate how well the dependency-based method correlates with human judgement, and how it compares to the correlation shown by other metrics, we conducted an experiment on ChineseEnglish newswire text." ></td>
	<td class="line x" title="125:166	4.2.1 Experimental design We used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4." ></td>
	<td class="line x" title="126:166	The data consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores for a subset of the translation-reference pairs." ></td>
	<td class="line x" title="127:166	Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment." ></td>
	<td class="line x" title="128:166	In effect, the test set created from this data contained 16,800 segments." ></td>
	<td class="line o" title="129:166	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and the dependency-based method." ></td>
	<td class="line x" title="130:166	4.2.2 Results We calculated Pearsons correlation coefficient for segment-level scores that were given by each metric and by human judges." ></td>
	<td class="line x" title="131:166	The results of the correlation are shown in Table 5." ></td>
	<td class="line x" title="132:166	Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other." ></td>
	<td class="line x" title="133:166	Rows are ordered 85 by the highest value of the (absolute) correlation with the human score." ></td>
	<td class="line x" title="134:166	First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy." ></td>
	<td class="line x" title="135:166	However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area." ></td>
	<td class="line x" title="136:166	This can be explained by the methods sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent." ></td>
	<td class="line x" title="137:166	H_FL H_AC H_AVE d+WN 0.168 M+WN 0.294 M+WN 0.255 d 0.162 M 0.278 d+WN 0.244 d+WN_pr 0.162 NIST 0.273 M 0.242 BLEU 0.155 d+WN 0.266 NIST 0.238 d_pr 0.154 GTM 0.260 d 0.236 M+WN 0.153 d 0.257 GTM 0.230 M 0.149 d+WN_pr 0.232 d+WN_pr 0.220 NIST 0.146 d_pr 0.224 d_pr 0.212 GTM 0.146 BLEU 0.199 BLEU 0.197 TER -0.133 TER -0.192 TER -0.182 Table 5." ></td>
	<td class="line x" title="138:166	Pearsons correlation between human scores and evaluation metrics." ></td>
	<td class="line x" title="139:166	Legend: d = dependency f-score, _pr = predicate-only f-score, M = METEOR, WN = WordNet, H_FL = human fluency score, H_AC = human accuracy score, H_AVE = human average score.9 Second, and somewhat surprisingly, in this detailed examination the relative order of the metrics changed." ></td>
	<td class="line x" title="140:166	The predicate-only version of the dependency-based method appears to be less adequate for correlation with human scores than its non-restricted versions." ></td>
	<td class="line p" title="141:166	As to the correlation with human evaluation of translation accuracy, our method currently falls short of METEOR and even NIST." ></td>
	<td class="line o" title="142:166	This is caused by the fact that both METEOR and NIST assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic form." ></td>
	<td class="line x" title="143:166	For our dependency-based method, the noise introduced by the parser might be the reason for low correlation: if even one side of the translation-reference pair contains parsing errors, this may lead to a less reliable score." ></td>
	<td class="line x" title="144:166	An obvious solution to this problem, 9 In general terms, an increase of 0.015 between any two scores is significant with a 95% confidence interval." ></td>
	<td class="line x" title="145:166	which we are examining at the moment, is to include a number of best parses for each side of the evaluation." ></td>
	<td class="line x" title="146:166	High correlation with human judgements of fluency and lower correlation with accuracy results in a high second place for our dependency-based method when it comes to the average correlation coefficient." ></td>
	<td class="line p" title="147:166	The WordNet-boosted dependencybased method scores only slightly lower than METEOR with WordNet." ></td>
	<td class="line x" title="148:166	These results are very encouraging, especially as we see a number of ways the dependency-based method could be further developed." ></td>
	<td class="line x" title="149:166	5 Current and future work While the idea of a dependency-based method is a natural step in the direction of a deeper linguistic analysis for MT evaluation, it does require an LFG grammar and parser for the target language." ></td>
	<td class="line x" title="150:166	There are several obvious areas for improvement with respect to the method itself." ></td>
	<td class="line x" title="151:166	First, we would also like to adapt the process of translation-reference dependency comparison to include n-best parsers for the input sentences, as well as some basic transformations which would allow an even deeper logical analysis of input (e.g. passive to active voice transformation)." ></td>
	<td class="line x" title="152:166	Second, we want to repeat both experiments using a paraphrase set derived from a large parallel corpus, rather than the test set, as described in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="154:166	While retaining the advantage of having a similar size to a corresponding set of WordNet synonyms, this set will also capture low-level syntactic variations, which can increase the number of matches and the correlation with human scores." ></td>
	<td class="line x" title="155:166	Finally, we want to take advantage of the fact that the score produced by the dependencybased method is the proportional average of fscores for a group of up to 32 (but usually far fewer) different dependency types." ></td>
	<td class="line x" title="156:166	We plan to implement a set of weights, one for each dependency type, trained in such a way as to maximize the correlation of the final dependency fscore with human evaluation." ></td>
	<td class="line x" title="157:166	6 Conclusions In this paper we present a novel way of evaluating MT output." ></td>
	<td class="line x" title="158:166	So far, all metrics relied on 86 comparing translation and reference on a string level." ></td>
	<td class="line x" title="159:166	Even given reordering, stemming, and synonyms for individual words, current methods are still far from reaching human ability to assess the quality of translation." ></td>
	<td class="line x" title="160:166	Our method compares the sentences on the level of their grammatical structure, as exemplified by their f-structure dependency triples produced by an LFG parser." ></td>
	<td class="line x" title="161:166	The dependency-based method can be further augmented by using paraphrases or WordNet synonyms, and is available in full version and predicate-only version." ></td>
	<td class="line x" title="162:166	In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score." ></td>
	<td class="line x" title="163:166	The use of dependencies in MT evaluation is a rather new idea and requires more research to improve it, but the method shows potential to become an accurate evaluation metric." ></td>
	<td class="line x" title="164:166	Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper." ></td>
	<td class="line x" title="165:166	We would also like to thank our reviewers for their insightful comments." ></td>
	<td class="line x" title="166:166	All remaining errors are our own." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0704
Exploring Different Representational Units in English-to-Turkish Statistical Machine Translation
Oflazer, Kemal;Durgar El-Kahlout, Ilknur;"></td>
	<td class="line x" title="1:182	Proceedings of the Second Workshop on Statistical Machine Translation, pages 25??2, Prague, June 2007." ></td>
	<td class="line x" title="2:182	c2007 Association for Computational Linguistics Exploring Different Representational Units in English-to-Turkish Statistical Machine Translation Kemal Oflazer????" ></td>
	<td class="line x" title="3:182	?Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA oflazer@sabanciuniv.edu ?Ilknur Durgar El-Kahlout??" ></td>
	<td class="line x" title="4:182	??Faculty of Engineering and Natural Sciences Sabanc University Istanbul, Tuzla, 34956, Turkey ilknurdurgar@su.sabanciuniv.edu Abstract We investigate different representational granularities for sub-lexical representation in statistical machine translation work from English to Turkish." ></td>
	<td class="line x" title="5:182	We find that (i) representing both Turkish and English at the morpheme-level but with some selective morpheme-grouping on the Turkish side of the training data, (ii) augmenting the training data with ?sentences??comprising only the content words of the original training data to bias root word alignment, (iii) reranking the n-best morpheme-sequence outputs of the decoder with a word-based language model, and (iv) using model iteration all provide a non-trivial improvement over a fully word-based baseline." ></td>
	<td class="line x" title="6:182	Despite our very limited training data, we improve from 20.22 BLEU points for our simplest model to 25.08 BLEU points for an improvement of 4.86 points or 24% relative." ></td>
	<td class="line x" title="7:182	1 Introduction Statistical machine translation (SMT) from Englishto-Turkish poses a number of difficulties." ></td>
	<td class="line x" title="8:182	Typologically English and Turkish are rather distant languages: while English has very limited morphology and rather fixed SVO constituent order, Turkish is an agglutinative language with a very rich and productive derivational and inflectional morphology, and a very flexible (but SOV dominant) constituent order." ></td>
	<td class="line x" title="9:182	Another issue of practical significance is the lack of large scale parallel text resources, with no substantial improvement expected in the near future." ></td>
	<td class="line x" title="10:182	In this paper, we investigate different representational granularities for sub-lexical representation of parallel data for English-to-Turkish phrase-based SMT and compare them with a word-based baseline." ></td>
	<td class="line x" title="11:182	We also employ two-levels of language models: the decoder uses a morpheme based LM while it is generating an n-best list." ></td>
	<td class="line x" title="12:182	The n-best lists are then rescored using a word-based LM." ></td>
	<td class="line x" title="13:182	The paper is structured as follows: We first briefly discuss issues in SMT and Turkish, and review related work." ></td>
	<td class="line x" title="14:182	We then outline how we exploit morphology, and present results from our baseline and morphologically segmented models, followed by some sample outputs." ></td>
	<td class="line x" title="15:182	We then describe discuss model iteration." ></td>
	<td class="line x" title="16:182	Finally, we present a comprehensive discussion of our approach and results, and briefly discuss word-repair ??fixing morphologicaly malformed words ??and offer a few ideas about the adaptation of BLEU to morphologically complex languages like Turkish." ></td>
	<td class="line x" title="17:182	2 Turkish and SMT Our previous experience with SMT into Turkish (Durgar El-Kahlout and Oflazer, 2006) hinted that exploiting sub-lexical structure would be a fruitful avenue to pursue." ></td>
	<td class="line x" title="18:182	This was based on the observation that a Turkish word would have to align with a complete phrase on the English side, and that sometimes these phrases on the English side could be discontinuous." ></td>
	<td class="line x" title="19:182	Figure 1 shows a pair of English and Turkish sentences that are aligned at the word (top) and morpheme (bottom) levels." ></td>
	<td class="line x" title="20:182	At the morpheme level, we have split the Turkish words into their lexical morphemes while English words with overt morphemes have been stemmed, and such morphemes have been marked with a tag." ></td>
	<td class="line x" title="21:182	The productive morphology of Turkish implies potentially a very large vocabulary size." ></td>
	<td class="line x" title="22:182	Thus, sparseness which is more acute when very modest 25 Figure 1: Word and morpheme alignments for a pair of English-Turkish sentences parallel resources are available becomes an important issue." ></td>
	<td class="line x" title="23:182	However, Turkish employs about 30,000 root words and about 150 distinct suffixes, so when morphemes are used as the units in the parallel texts, the sparseness problem can be alleviated to some extent." ></td>
	<td class="line x" title="24:182	Our approach in this paper is to represent Turkish words with their morphological segmentation." ></td>
	<td class="line x" title="25:182	We use lexical morphemes instead of surface morphemes, as most surface distinctions are manifestations of word-internal phenomena such as vowel harmony, and morphotactics." ></td>
	<td class="line x" title="26:182	With lexical morpheme representation, we can abstract away such word-internal details and conflate statistics for seemingly different suffixes, as at this level of representation words that look very different on the surface, look very similar.1 For instance, although the words evinde ?in his house??and masasnda ?on his table??look quite different, the lexical morphemes except for the root are the same: ev+sH+ndA vs. masa+sH+ndA." ></td>
	<td class="line x" title="27:182	We should however note that although employing a morpheme based representations dramatically reduces the vocabulary size on the Turkish side, it also runs the risk of overloading distortion mechanisms to account for both word-internal morpheme sequencing and sentence level word ordering." ></td>
	<td class="line x" title="28:182	The segmentation of a word in general is not unique." ></td>
	<td class="line x" title="29:182	We first generate a representation that contains both the lexical segments and the morphological features encoded for all possible segmenta1This is in a sense very similar to the more general problem of lexical redundancy addressed by Talbot and Osborne (2006) but our approach does not require the more sophisticated solution there." ></td>
	<td class="line x" title="30:182	tions and interpretations of the word." ></td>
	<td class="line x" title="31:182	For the word emeli for instance, our morphological analyzer generates the following with lexical morphemes bracketed with (): (em)em+Verb+Pos(+yAlH)?DB+Adverb+Since since (someone) sucked (something) (emel)emel+Noun+A3sg(+sH)+P3sg+Nom his/her ambition (emel)emel+Noun+A3sg+Pnon(+yH)+Acc ambition (as object of a transitive verb) These analyses are then disambiguated with a statistical disambiguator (Yuret and Ture, 2006) which operates on the morphological features.2 Finally, the morphological features are removed from each parse leaving the lexical morphemes." ></td>
	<td class="line x" title="32:182	Using morphology in SMT has been recently addressed by researchers translation from or into morphologically rich(er) languages." ></td>
	<td class="line x" title="33:182	Niessen and Ney (2004) have used morphological decomposition to improve alignment quality." ></td>
	<td class="line x" title="34:182	Yang and Kirchhoff (2006) use phrase-based backoff models to translate words that are unknown to the decoder, by morphologically decomposing the unknown source word." ></td>
	<td class="line x" title="35:182	They particularly apply their method to translating from Finnish ??another language with very similar structural characteristics to Turkish." ></td>
	<td class="line x" title="36:182	Corston-Oliver and Gamon (2004) normalize inflectional morphology by stemming the word for German-English word alignment." ></td>
	<td class="line x" title="37:182	Lee (2004) uses a morphologically analyzed and tagged parallel corpus for ArabicEnglish SMT." ></td>
	<td class="line x" title="38:182	Zolmann et al.(2006) also exploit morphology in Arabic-English SMT." ></td>
	<td class="line x" title="40:182	Popovic and Ney (2004) investigate improving translation qual2This disambiguator has about 94% accuracy." ></td>
	<td class="line x" title="41:182	26 ity from inflected languages by using stems, suffixes and part-of-speech tags." ></td>
	<td class="line x" title="42:182	Goldwater and McClosky (2005) use morphological analysis on Czech text to get improvements in Czech to English SMT." ></td>
	<td class="line x" title="43:182	Recently, Minkov et al.(2007) have used morphologicalpostprocessingontheoutputsideusingstructural information and information from the source side, to improve SMT quality." ></td>
	<td class="line x" title="45:182	3 Exploiting Morphology Our parallel data consists mainly of documents in international relations and legal documents from sources such as the Turkish Ministry of Foreign Affairs, EU, etc. We process these as follows: (i) We segment the words in our Turkish corpus into lexical morphemes whereby differences in the surface representations of morphemes due to word-internal phenomena are abstracted out to improve statistics during alignment.3 (ii) We tag the English side using TreeTagger (Schmid, 1994), which provides a lemma and a part-of-speech for each word." ></td>
	<td class="line x" title="46:182	We then remove any tags which do not imply an explicit morpheme or an exceptional form." ></td>
	<td class="line x" title="47:182	So for instance, if the word book gets tagged as +NN, we keep book in the text, but remove +NN." ></td>
	<td class="line x" title="48:182	For books tagged as +NNS or booking tagged as +VVG, we keep book and +NNS, and book and +VVG." ></td>
	<td class="line x" title="49:182	A word like went is replaced by go +VVD.4 (iii) From these morphologically segmented corpora, we also extract for each sentence, the sequence of roots for open class content words (nouns, adjectives, adverbs, and verbs)." ></td>
	<td class="line x" title="50:182	For Turkish, this corresponds to removing all morphemes and any roots for closed classes." ></td>
	<td class="line x" title="51:182	For English, this corresponds to removing all words tagged as closed class words along with the tags such as +VVG above that signal a morpheme on an open class content word." ></td>
	<td class="line x" title="52:182	We use this to augment the training corpus and bias content word alignments, with the hope that such roots may get a chance to align without any additional ?noise??from morphemes and other function words." ></td>
	<td class="line x" title="53:182	From such processed data, we compile the data sets whose statistics are listed in Table 1." ></td>
	<td class="line x" title="54:182	One can notethatTurkishhasmanymoredistinctwordforms (about twice as many as English), but has much less 3So for example, the surface plural morphemes +ler and +lar get conflated to +lAr and their statistics are hence combined." ></td>
	<td class="line x" title="55:182	4Ideally, it would have been very desirable to actually do derivational morphological analysis on the English side, so that one could for example analyze accession into access plus a marker indicating nominalization." ></td>
	<td class="line x" title="56:182	Turkish Sent." ></td>
	<td class="line x" title="57:182	Words (UNK) Uniq." ></td>
	<td class="line x" title="58:182	Words Train 45,709 557,530 52,897 Train-Content 56,609 436,762 13,767 Tune 200 3,258 1,442 Test 649 10,334 (545) 4,355 English Train 45,709 723,399 26,747 Train-Content 56,609 403,162 19,791 Test 649 13,484 (231) 3,220 MorphUniq." ></td>
	<td class="line x" title="59:182	Morp./ Uniq." ></td>
	<td class="line x" title="60:182	Uniq." ></td>
	<td class="line x" title="61:182	Turkish emes Morp." ></td>
	<td class="line x" title="62:182	Word Roots Suff." ></td>
	<td class="line x" title="63:182	Train 1,005,045 15,081 1.80 14,976 105 Tune 6,240 859 1.92 810 49 Test 18,713 2,297 1.81 2,220 77 Table 1: Statistics on Turkish and English training and test data, and Turkish morphological structure number of distinct content words than English.5 For language models in decoding and n-best list rescoring, we use, in addition to the training data, a monolingual Turkish text of about 100,000 sentences (in a segmented and disambiguated form)." ></td>
	<td class="line x" title="64:182	A typical sentence pair in our data looks like the following, where we have highlighted the content root words with bold font, coindexed them toshow their alignments and bracketed the ?words??" ></td>
	<td class="line x" title="65:182	that evaluation on test would consider." ></td>
	<td class="line x" title="66:182	??T: [kat1 +hl +ma] [ortaklk2 +sh +nhn] [uygula3 +hn +ma +sh] [,] [ortaklk4] [anlasma5 +sh] [cerceve6 +sh +nda] [izle7 +hn +yacak +dhr] [.]" ></td>
	<td class="line x" title="67:182	??E: the implementation3 of the accession1 partnership2 will be monitor7 +vvn in the framework6 of the association4 agreement5." ></td>
	<td class="line x" title="68:182	Note that when the morphemes/tags (starting with a +) are concatenated, we get the ?word-based??" ></td>
	<td class="line x" title="69:182	version of the corpus, since surface words are directly recoverable from the concatenated representation." ></td>
	<td class="line x" title="70:182	We use this word-based representation also for word-based language models used for rescoring." ></td>
	<td class="line x" title="71:182	We employ the phrase-based SMT framework (Koehn et al. , 2003), and use the Moses toolkit (Koehn et al. , 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al. , 2002), using a single reference translation." ></td>
	<td class="line x" title="72:182	5The training set in the first row of 1 was limited to sentences on the Turkish side which had at most 90 tokens (roots and bound morphemes) in total in order to comply with requirements of the GIZA++ alignment tool." ></td>
	<td class="line x" title="73:182	However when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction when morphemes/function word are removed." ></td>
	<td class="line x" title="74:182	27 Moses Dec. Parms." ></td>
	<td class="line x" title="75:182	BLEU BLEU-c Default 16.29 16.13 dl = -1, -weight-d = 0.1 20.16 19.77 Table 2: BLEU results for baseline experiments." ></td>
	<td class="line x" title="76:182	BLEU is for the model trained on the training set BLEU-Cisforthemodeltrainedontrainingsetaugmentedwith the content words." ></td>
	<td class="line x" title="77:182	3.1 The Baseline System As a baseline system, we trained a model using default Moses parameters (e.g. , maximum phrase length = 7), using the word-based training corpus." ></td>
	<td class="line x" title="78:182	The English test set was decoded with both default decoder parameters and with the distortion limit (-dl in Moses) set to unlimited (-1 in Moses) and distortion weight (-weight-d in Moses) set to a very low value of 0.1 to allow for long distance distortions.6 We also augmented the training set with the content word data and trained a second baseline model." ></td>
	<td class="line x" title="79:182	Minimum error rate training with the tune set did not provide any tangible improvements.7 Table 2 shows the BLEU results for baseline performance." ></td>
	<td class="line x" title="80:182	It can be seen that adding the content word training data actually hampers the baseline performance." ></td>
	<td class="line x" title="81:182	3.2 Fully Morphologically Segmented Model We now trained a model using the fully morphologically segmented training corpus with and without content word parallel corpus augmentation." ></td>
	<td class="line x" title="82:182	For decoding, we used a 5-gram morpheme-based language model with the hope of capturing local morphotactic ordering constraints, and perhaps some sentence level ordering of words.8 We then decoded and obtained 1000-best lists." ></td>
	<td class="line x" title="83:182	The 1000-best sentences were then converted to ?words??(by concatenating the morphemes) and then rescored with a 4gram word-based language model with the hope of enforcing more distant word sequencing constraints." ></td>
	<td class="line x" title="84:182	For this, we followed the following procedure: We 6We arrived at this combination by experimenting with the decoder to avoid the almost monotonic translation we were getting with the default parameters." ></td>
	<td class="line x" title="85:182	7We ran MERT on the baseline model and the morphologically segmented models forcing -weight-d to range a very small around 0.1, but letting the other parameters range in their suggested ranges." ></td>
	<td class="line x" title="86:182	Even though the procedure came back claiming that it achieved a better BLEU score on the tune set, running the new model on the test set did not show any improvement at all." ></td>
	<td class="line x" title="87:182	This may have been due to the fact that the initial choice of -weight-d along with -dl set to 1 provides such a drastic improvement that perturbations in the other parameters do not have much impact." ></td>
	<td class="line x" title="88:182	8Given that on the average we have almost two bound morphemes per ?word??(for inflecting word classes), a morpheme 5-gram would cover about 2 ?words??" ></td>
	<td class="line x" title="89:182	tried various linear combinations of the word-based language model and the translation model scores on the tune corpus, and used the combination that performed best to evaluate the test corpus." ></td>
	<td class="line x" title="90:182	We also experimented with both the default decoding parameters, and the modified parameters used in the baseline model decoding above." ></td>
	<td class="line x" title="91:182	The results in Table 3 indicate that the default decoding parameters used by the Moses decoder provide a very dismal results ??much below the baseline scores." ></td>
	<td class="line x" title="92:182	We can speculate that as the constituent orders of Turkish and English are very different, (root) words may have to be scrambled to rather long distances along with the translations of functions words and tags on the English side, to morphemes on the Turkish side." ></td>
	<td class="line x" title="93:182	Thus limiting maximum distortion and penalizing distortions with the default higher weight, result in these low BLEU results." ></td>
	<td class="line x" title="94:182	Allowing the decoder to consider longer range distortions and penalizing such distortions much less with the modified decoding parameters, seem to make an enormous difference in this case, providing close to almost 7 BLEU points improvement.9 We can also see that, contrary to the case with the baseline word-based experiments, using the additional content word corpus for training actually provides a tangible improvement (about 6.2% relative (w/o rescoring)), most likely due to slightly better alignments when content words are used.10 Rescoring the 1000-best sentence output with a 4gram word-based language model provides an additional 0.79 BLEU points (about 4% relative) ??from 20.22 to 21.01 ??for the model with the basic training set, and an additional 0.71 BLEU points (about 3% relative) ??from 21.47 to 22.18??for the model with the augmented training set." ></td>
	<td class="line x" title="95:182	The cumulative improvement is 1.96 BLEU points or about 9.4% relative." ></td>
	<td class="line x" title="96:182	3.3 Selectively Segmented Model A systematic analysis of the alignment files produced by GIZA++ for a small subset of the training sentences showed that certain morphemes on the 9The ?morpheme??BLEU scores are much higher (34.43 on the test set) where we measure BLEU using decoded morphemes as tokens." ></td>
	<td class="line x" title="97:182	This is just indicative and but correlates with word-level BLEU which we report in Table 3, and can be used to gauge relative improvements to the models." ></td>
	<td class="line x" title="98:182	10We also constructed phrase tables only from the actual training set (w/o the content word section) after the alignment phase." ></td>
	<td class="line x" title="99:182	The resulting models fared slightly worse though we do not yet understand why." ></td>
	<td class="line x" title="100:182	28 Moses Dec. Parms." ></td>
	<td class="line x" title="101:182	BLEU BLEU-c Default 13.55 NA dl = -1, -weight-d = 0.1 20.22 21.47 dl = -1, -weight-d = 0.1 + word-level LM rescoring 21.01 22.18 Table 3: BLEU results for experiments with fully morphologically segmented training set Turkish side were almost consistently never aligned with anything on the English side: e.g., the compoundnounmarkermorphemeinTurkish(+sh)does not have a corresponding unit on the English side since English noun-noun compounds do not carry any overt markers." ></td>
	<td class="line x" title="102:182	Such markers were never aligned to anything or were aligned almost randomly to tokens on the English side." ></td>
	<td class="line x" title="103:182	Since we perform derivational morphological analysis on the Turkish side but not on the English side, we noted that most verbal nominalizations on the English side were just aligned to the verb roots on the Turkish side and the additional markers on the Turkish side indicating the nominalization and agreement markers etc. , were mostly unaligned." ></td>
	<td class="line x" title="104:182	For just these cases, we selectively attached such morphemes (and in the case of verbs, the intervening morphemes) to the root, but otherwise kept other morphemes, especially any case morphemes, still by themselves, as they almost often align with prepositions on the English side quite accurately.11 This time, we trained a model on just the contentword augmented training corpus, with the better performing parameters for the decoder and again did 1000-best rescoring.12 The results for this experiment are shown in Table 4." ></td>
	<td class="line x" title="105:182	The resulting BLEU represents 2.43 points (11% relative) improvement overthebestfullysegmentedmodel(and4.39points 21.7% compared to the very initial morphologically segmented model)." ></td>
	<td class="line x" title="106:182	This is a very encouraging result that indicates we should perhaps consider a much more detailed analysis of morpheme alignments to uncover additional morphemes with similar status." ></td>
	<td class="line x" title="107:182	Table 5 provides additional details on the BLEU 11It should be noted that what to selectively attach to the root should be considered on a per-language basis; if Turkish were to be aligned with a language with similar morphological markers, this perhaps would not have been needed." ></td>
	<td class="line x" title="108:182	Again one perhaps can use methods similar to those suggested by Talbot and Osborne (2006)." ></td>
	<td class="line x" title="109:182	12Decoders for the fully-segmented model and selectively segmented model use different 5-gram language models, since the language model corpus should have the same selectively segmented units as those in the training set." ></td>
	<td class="line x" title="110:182	However, the wordlevel language models used in rescoring are the same." ></td>
	<td class="line x" title="111:182	Moses Dec. Parms." ></td>
	<td class="line x" title="112:182	BLEU-c dl = -1, -weight-d = 0.1 + word-level LM rescoring 22.18 (Full Segmentation (from Table 3)) dl = -1, -weight-d = 0.1 23.47 dl = -1, -weight-d = 0.1 + word-level LM rescoring 24.61 Table 4: BLEU results for experiments with selectivelysegmentedandcontent-wordaugmentedtraining set Range Sent." ></td>
	<td class="line x" title="113:182	BLEU-c 1 10 172 44.36 1 15 276 34.63 5 15 217 33.00 1 20 369 28.84 1 30 517 27.88 1 40 589 24.90 All 649 24.61 Table 5: BLEU Scores for different ranges of (source) sentence length for the result in Table 4 scores for this model, for different ranges of (English source) sentence length." ></td>
	<td class="line x" title="114:182	4 Sample Rules and Translations We have extracted some additional statistics from the translations produced from English test set." ></td>
	<td class="line x" title="115:182	Of the 10,563 words in the decoded test set, a total of 957 words (9.0 %) were not seen in the training corpus." ></td>
	<td class="line x" title="116:182	However, interestingly, of these 957 words, 432 (45%) were actually morphologically well-formed (some as complex as having 4-5 morphemes)!" ></td>
	<td class="line x" title="117:182	This indicates that the phrase-based translation model is able to synthesize novel complex words.13 In fact, some phrase table entries seem to capture morphologically marked subcategorization patterns." ></td>
	<td class="line x" title="118:182	An example is the phrase translation pair after examine +vvg ??" ></td>
	<td class="line x" title="119:182	+acc incele+dhk +abl sonra which very much resembles a typical structural transfer rule one would find in a symbolic machine translation system PP(after examine +vvg NPeng) ??" ></td>
	<td class="line x" title="120:182	PP(NPturk+acc incele+dhk +abl sonra) in that the accusative marker is tacked to the translation of the English NP." ></td>
	<td class="line x" title="121:182	Figure 2 shows how segments are translated to Turkish for a sample sentence." ></td>
	<td class="line x" title="122:182	Figure 3 shows the translations of three sentences from the test data 13Though whether such words are actually correct in their context is not necessarily clear." ></td>
	<td class="line x" title="123:182	29 cocuk [[ child ]] hak+lar+sh +nhn [[ +nns +pos right ]] koru+hn+ma+sh [[ protection ]] +nhn [[ of ]] tesvik et+hl+ma+sh [[ promote ]] +loc [[ +nns in ]] ab [[ eu ]] ve ulus+lararasi standart +lar [[ and international standard +nns ]] +dat uygun [[ line with ]] +dhr . [[ .]] Figure 2: Phrasal translations selected for a sample sentence Inp.: 1 . everyone?s right to life shall be protected by law . Trans.: 1 . herkesin yasama hakk kanunla korunur." ></td>
	<td class="line x" title="124:182	Lit.: everyone?s living right is protected with law . Ref.: 1 . herkesin yasam hakk yasann korumas altndadr . Lit.: everyone?s life right is under the protection of the law." ></td>
	<td class="line x" title="125:182	Inp.: promote protection of children?s rights in line with eu and international standards . Trans.: cocuk haklarnn korunmasnn ab ve uluslararas standartlara uygun sekilde gelistirilmesi." ></td>
	<td class="line x" title="126:182	Lit.: develop protection of children?s rights in accordance with eu and international standards . Ref.: ab ve uluslararas standartlar do?grultusunda cocuk haklarnn korunmasnn tesvik edilmesi." ></td>
	<td class="line x" title="127:182	Lit.: in line with eu and international standards promote/motivate protection of children?s rights . Inp.: as a key feature of such a strategy, an accession partnership will be drawn up on the basis of previous european council conclusions." ></td>
	<td class="line x" title="128:182	Trans.: bu stratejinin kilit unsuru bir katlm ortakl?g belgesi hazrlanacak kadarn temelinde, bir onceki avrupa konseyi sonuclardr . Lit.: as a key feature of this strategy, accession partnership document will be prepared ???" ></td>
	<td class="line x" title="129:182	based are previous european council resolutions . Ref.: bu stratejinin kilit unsuru olarak, daha onceki ab zirve sonuclarna dayanlarak bir katlm ortakl?g olusturulacaktr. Lit.: as a key feature of this strategy an accession partnership based on earlier eu summit resolutions will be formed . Figure 3: Some sample translations along with the literal paraphrases of the translation and the reference versions." ></td>
	<td class="line x" title="130:182	The first two are quite accurate and acceptable translations while the third clearly has missing and incorrect parts." ></td>
	<td class="line x" title="131:182	5 Model Iteration We have also experimented with an iterative approach to use multiple models to see if further improvements are possible." ></td>
	<td class="line x" title="132:182	This is akin to post-editing (though definitely not akin to the much more sophisticated approach in described in Simard et al.(2007))." ></td>
	<td class="line x" title="134:182	We proceeded as follows: We used the selective segmentation based model above and decoded our English training data ETrain and English test data ETest to obtain T1Train and T1Test reStep BLEU From Table 4 24.61 Iter." ></td>
	<td class="line x" title="135:182	1 24.77 Iter." ></td>
	<td class="line x" title="136:182	2 25.08 Table 6: BLEU results for two model iterations spectively." ></td>
	<td class="line x" title="137:182	We then trained the next model using T1Train and TTrain, to build a model that hopefully will improve upon the output of the previous model, T1Test, to bring it closer to TTest." ></td>
	<td class="line x" title="138:182	This model when applied toT1Train andT1Test produceT2Train and T2Test respectively." ></td>
	<td class="line x" title="139:182	We have not included the content word corpus in these experiments, as (i) our few very preliminary experiments indicated that using a morphemebased models in subsequent iterations would perform worse than word-based models, and (ii) that for word-basedmodels addingthecontent wordtraining data was not helpful as our baseline experiments indicated." ></td>
	<td class="line x" title="140:182	The models were tested by decoding the output of the previous model for original test data." ></td>
	<td class="line x" title="141:182	For word-based decoding in the additional iterations we used a 3-gram word-based language model but reranked the 1000-best outputs using a 4-gram language model." ></td>
	<td class="line x" title="142:182	Table 6 provides the BLEU results for these experiments corresponding to two additional model iterations." ></td>
	<td class="line x" title="143:182	The BLEU result for the second iteration, 25.08, represents a cumulative 4.86 points (24% relative) improvement over the initial fully morphologically segmented model using only the basic training set and no rescoring." ></td>
	<td class="line x" title="144:182	6 Discussion Translation into Turkish seems to involve processes that are somewhat more complex than standard statistical translation models: sometimes words on the Turkish side are synthesized from the translations of two or more (SMT) phrases, and errors in any translated morpheme or its morphotactic position render the synthesized word incorrect, even though the rest of the word can be quite fine." ></td>
	<td class="line x" title="145:182	If we just extract the root words (not just for content words but all words) in the decoded test set and the reference set, and compute root word BLEU, we obtain 30.62, [64.6/35.7/23.4/16.3]." ></td>
	<td class="line x" title="146:182	The unigram precision score shows that we are getting almost 65% of the root words correct." ></td>
	<td class="line x" title="147:182	However, the unigram precision score with full words is about 52% for our best model." ></td>
	<td class="line x" title="148:182	Thus we are missing about 13% of the words although we seem to be getting their roots 30 correct." ></td>
	<td class="line x" title="149:182	With a tool that we have developed, BLEU+ (Tantu?g et al. , 2007), we have investigated such mismatches and have found that most of these are actually morphologically bogus, in that, although they have the root word right, the morphemes are either not the applicable ones or are in a morphotactically wrong position." ></td>
	<td class="line x" title="150:182	These can easily be identified with the morphological generator that we have." ></td>
	<td class="line x" title="151:182	In many cases, such morphologically bogus words are one morpheme edit distance away from the correct form in the reference file." ></td>
	<td class="line x" title="152:182	Another avenue that could be pursued is the use of skip language models (supported by the SRILM toolkit) so that the content word order could directly be used by the decoder.14 Atthispointitisveryhardtocomparehowourresults fare in the grand scheme of things, since there isnotmuchpriorresultsforEnglishtoTurkishSMT." ></td>
	<td class="line x" title="153:182	Koehn (2005) reports on translation from English to Finnish, another language that is morphologically as complex as Turkish, with the added complexity of compounding and stricter agreement between modifiers and head nouns." ></td>
	<td class="line x" title="154:182	A standard phrase-based system trained with 941,890 pairs of sentences (about 20 times the data that we have)!" ></td>
	<td class="line x" title="155:182	gives a BLEU score of 13.00." ></td>
	<td class="line x" title="156:182	However, in this study, nothing specific for Finnish was employed, and one can certainly employ techniques similar to presented here to improve upon this." ></td>
	<td class="line x" title="157:182	6.1 Word Repair The fact that there are quite many erroneous words which are actually easy to fix suggests some ideas to improve unigram precision." ></td>
	<td class="line x" title="158:182	One can utilize a morpheme level ?spelling corrector??that operates on segmented representations, and corrects such forms to possible morphologically correct words in order to form a lattice which can again be rescored to select the contextually correct one.15 With the BLEU+ tool, we have done one experiment that shows that if we could recover all morphologically bogus words that are 1 and 2 morpheme edit distance from the correct form, the word BLEU score could rise to 29.86, [60.0/34.9/23.3/16.]" ></td>
	<td class="line x" title="159:182	and 30.48 [63.3/35.6/23.4/16.4] respectively." ></td>
	<td class="line x" title="160:182	Obviously, these are upper-bound oracle scores, as subsequent candidate generation and lattice rescoring could make er14This was suggested by one of the reviewers." ></td>
	<td class="line x" title="161:182	15It would however perhaps be much better if the decoder could be augmented with a filter that could be invoked at much earlier stages of sentence generation to check if certain generated segments violate hard-constraints (such as morphotactic constraints) regardless of what the statistics say." ></td>
	<td class="line x" title="162:182	rors, but nevertheless they are very close to the root word BLEU scores above." ></td>
	<td class="line x" title="163:182	Another path to pursue in repairing words is to identify morphologically correct words which are either OOVs in the language model or for which the language model has low confidence." ></td>
	<td class="line x" title="164:182	One can perhaps identify these using posterior probabilities (e.g. , using techniques in Zens and Ney (2006)) and generate additional morphologically valid words that are ?close??and construct a lattice that can be rescored." ></td>
	<td class="line x" title="165:182	6.2 Some Thoughts on BLEU BLEU is particularly harsh for Turkish and the morpheme based-approach, because of the all-or-none nature of token comparison, as discussed above." ></td>
	<td class="line x" title="166:182	There are also cases where words with different morphemes have very close morphosemantics, convey the relevant meaning and are almost interchangeable: ??gel+hyor (geliyor he is coming) vs. gel+makta (gelmekte he is (in a state of) coming) are essentially the same." ></td>
	<td class="line x" title="167:182	On a scale of 0 to 1, one could rate these at about 0.95 in similarity." ></td>
	<td class="line x" title="168:182	??gel+yacak (gelecek he will come) vs. gel+yacak+dhr (gelecektir he will come) in a sentence final position." ></td>
	<td class="line x" title="169:182	Such pairs could be rated perhaps at 0.90 in similarity." ></td>
	<td class="line x" title="170:182	??gel+dh (geldi he came (past tense)) vs. gel+mhs (gelmis he came (hearsay past tense))." ></td>
	<td class="line x" title="171:182	These essentially mark past tense but differ in how the speaker relates to the event and could be rated at perhaps 0.70 similarity." ></td>
	<td class="line oc" title="172:182	Note that using stems and their synonyms as used in METEOR (Banerjee and Lavie, 2005) could also be considered for word similarity." ></td>
	<td class="line x" title="173:182	Again using the BLEU+ tool and a slightly different formulation of token similarity in BLEU computation, we find that using morphological similarity our best score above, 25.08 BLEU increases to 25.14 BLEU, while using only root word synonymy and very close hypernymy from Wordnet, gives us 25.45 BLEU." ></td>
	<td class="line x" title="174:182	The combination of rules and Wordnet match gives 25.46 BLEU." ></td>
	<td class="line x" title="175:182	Note that these increases are much less than what can (potentially) be gained from solving the word-repair problem above." ></td>
	<td class="line x" title="176:182	7 Conclusions We have presented results from our investigation into using different granularity of sub-lexical representations for English to Turkish SMT." ></td>
	<td class="line x" title="177:182	We have found that employing a language-pair specific representation somewhere in between using full wordforms and fully morphologically segmented representations and using content words as additional 31 data provide a significant boost in BLEU scores, in addition to contributions of word-level rescoring of 1000-best outputs and model iteration, to give a BLEU score of 25.08 points with very modest parallel text resources." ></td>
	<td class="line x" title="178:182	Detailed analysis of the errors point at a few directions such as word-repair, to improve word accuracy." ></td>
	<td class="line x" title="179:182	This also suggests perhaps hooking into the decoder, a mechanism for imposing hard constraints (such as morphotactic constraints) during decoding to avoid generating morphologically bogus words." ></td>
	<td class="line x" title="180:182	Another direction is to introduce exploitation of limited structures such as bracketed noun phrases before considering full-fledged syntactic structure." ></td>
	<td class="line x" title="181:182	Acknowledgements This work was supported by TUB?ITAK ??The Turkish National Science and Technology Foundation under project grant 105E020." ></td>
	<td class="line x" title="182:182	We thank the anonymous reviewer for some very useful comments and suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0707
Word Error Rates: Decomposition over POS classes and Applications for Error Analysis
Popović, Maja;Ney, Hermann;"></td>
	<td class="line x" title="1:251	Proceedings of the Second Workshop on Statistical Machine Translation, pages 48??5, Prague, June 2007." ></td>
	<td class="line x" title="2:251	c2007 Association for Computational Linguistics Word Error Rates: Decomposition over POS Classes and Applications for Error Analysis Maja Popovic Lehrstuhl fur Informatik 6 RWTH Aachen University Aachen, Germany popovic@cs.rwth-aachen.de Hermann Ney Lehrstuhl fur Informatik 6 RWTH Aachen University Aachen, Germany ney@cs.rwth-aachen.de Abstract Evaluation and error analysis of machine translation output are important but difficult tasks." ></td>
	<td class="line x" title="3:251	In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (WER) and Position independent word Error Rate (PER) over different Partof-Speech (POS) classes." ></td>
	<td class="line x" title="4:251	Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors and distribution of missing words over POS classes." ></td>
	<td class="line x" title="5:251	The obtained results are shown to correspond to the results of a human error analysis." ></td>
	<td class="line x" title="6:251	The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system." ></td>
	<td class="line x" title="7:251	1 Introduction Evaluation of machine translation output is a very important but difficult task." ></td>
	<td class="line x" title="8:251	Human evaluation is expensive and time consuming." ></td>
	<td class="line x" title="9:251	Therefore a variety of automatic evaluation measures have been studied over the last years." ></td>
	<td class="line x" title="10:251	The most widely used are Word Error Rate (WER), Position independent word Error Rate (PER), the BLEU score (Papineni et al. , 2002) and the NIST score (Doddington, 2002)." ></td>
	<td class="line x" title="11:251	These measures have shown to be valuable tools for comparing different systems as well as for evaluating improvements within one system." ></td>
	<td class="line x" title="12:251	However, these measures do not give any details about the nature of translation errors." ></td>
	<td class="line x" title="13:251	Therefore some more detailed analysis of the generated output is needed in order to identify the main problems and to focus the research efforts." ></td>
	<td class="line x" title="14:251	A framework for human error analysis has been proposed in (Vilar et al. , 2006), but as every human evaluation, this is also a time consuming task." ></td>
	<td class="line x" title="15:251	This article presents a framework for calculating the decomposition of WER and PER over different POS classes, i.e. for estimating the contribution of each POS class to the overall word error rate." ></td>
	<td class="line x" title="16:251	Although this work focuses on POS classes, the method can be easily extended to other types of linguistic information." ></td>
	<td class="line x" title="17:251	In addition, two methods for error analysis using the WER and PER decompositons together with base forms are proposed: estimation of inflectional errors and distribution of missing words over POS classes." ></td>
	<td class="line x" title="18:251	The translation corpus used for our error analysis is built in the framework of the TC-STAR project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (EPPS) in Spanish and English." ></td>
	<td class="line x" title="19:251	The translation system used is the phrase-based statistical machine translation system described in (Vilar et al. , 2005; Matusov et al. , 2006)." ></td>
	<td class="line x" title="20:251	2 Related Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years." ></td>
	<td class="line x" title="21:251	The BLEU metric (Papineni et al. , 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER 48 have been widely used by many machine translation researchers." ></td>
	<td class="line x" title="22:251	An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004)." ></td>
	<td class="line x" title="23:251	(Leusch et al. , 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST." ></td>
	<td class="line x" title="24:251	The same set of measures is examined in (Matusov et al. , 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output)." ></td>
	<td class="line oc" title="25:251	A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words." ></td>
	<td class="line o" title="26:251	This measure counts the number of exact word matches between the output and the reference." ></td>
	<td class="line x" title="27:251	In a second step, unmatched words are converted into stems or synonyms and then matched." ></td>
	<td class="line x" title="28:251	The TER metric (Snover et al. , 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference." ></td>
	<td class="line x" title="29:251	The CDER measure (Leusch et al. , 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks." ></td>
	<td class="line p" title="30:251	Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar et al. , 2006) and a detailed analysis of the obtained results has been carried out." ></td>
	<td class="line x" title="31:251	However, human error analysis, like any human evaluation, is a time consuming task." ></td>
	<td class="line x" title="32:251	Whereas the use of linguistic knowledge for improving the performance of a statistical machine translation system is investigated in many publications for various language pairs (like for example (Nieen and Ney, 2000), (Goldwater and McClosky, 2005)), its use for the analysis of translation errors is still a rather unexplored area." ></td>
	<td class="line x" title="33:251	Some automatic methods for error analysis using base forms and POS tags are proposed in (Popovic et al. , 2006; Popovic and Ney, 2006)." ></td>
	<td class="line x" title="34:251	These measures are based on differences between WER and PER which are calculated separately for each POS class using subsets extracted from the original texts." ></td>
	<td class="line x" title="35:251	Standard overall WER and PER of the original texts are not at all taken into account." ></td>
	<td class="line x" title="36:251	In this work, the standard WER and PER are decomposed and analysed." ></td>
	<td class="line x" title="37:251	3 Decomposition of WER and PER over POS classes The standard procedure for evaluating machine translation output is done by comparing the hypothesis document hyp with given reference translations ref, each one consisting of K sentences (or segments)." ></td>
	<td class="line x" title="38:251	The reference document ref consists of R reference translations for each sentence." ></td>
	<td class="line x" title="39:251	Let the length of the hypothesis sentence hypk be denoted as Nhypk, and the reference lengths of each sentence Nref k,r. Then, the total hypothesis length of the document is Nhyp =summationtextk Nhypk, and the total reference length is Nref = summationtextk N?ref k where N?ref k is defined as the length of the reference sentence with the lowest sentence-level error rate as shown to be optimal in (Leusch et al. , 2005)." ></td>
	<td class="line x" title="40:251	3.1 Standard word error rates (overview) The word error rate (WER) is based on the Levenshtein distance (Levenshtein, 1966) the minimum number of substitutions, deletions and insertions that have to be performed to convert the generated text hyp into the reference text ref . A shortcoming of the WER is the fact that it does not allow reorderings of words, whereas the word order of the hypothesis can be different from word order of the reference even though it is correct translation." ></td>
	<td class="line x" title="41:251	In order to overcome this problem, the position independent word error rate (PER) compares the words in the two sentences without taking the word order into account." ></td>
	<td class="line x" title="42:251	The PER is always lower than or equal to the WER." ></td>
	<td class="line x" title="43:251	On the other hand, shortcoming of the PER is the fact that the word order can be important in some cases." ></td>
	<td class="line x" title="44:251	Therefore the best solution is to calculate both word error rates." ></td>
	<td class="line x" title="45:251	Calculation of WER: The WER of the hypothesis hyp with respect to the reference ref is calculated as: WER = 1N??" ></td>
	<td class="line x" title="46:251	ref Ksummationdisplay k=1 minr dL(ref k,r,hypk) where dL(ref k,r,hypk) is the Levenshtein distance between the reference sentence ref k,r and the hypothesis sentence hypk." ></td>
	<td class="line x" title="47:251	The calculation of WER 49 is performed using a dynamic programming algorithm." ></td>
	<td class="line x" title="48:251	Calculation of PER: The PER can be calculated using the counts n(e,hypk) and n(e,ref k,r) of a word e in the hypothesis sentence hypk and the reference sentence ref k,r respectively: PER = 1N??" ></td>
	<td class="line x" title="49:251	ref Ksummationdisplay k=1 minr dPER(ref k,r,hypk) where dPER(ref k,r,hypk) = 12 parenleftbigg |Nref k,r ??Nhypk|+ summationdisplay e |n(e,ref k,r) ??n(e,hypk)| parenrightbigg 3.2 WER decomposition over POS classes The dynamic programming algorithm for WER enables a simple and straightforward identification of each erroneous word which actually contributes to WER." ></td>
	<td class="line x" title="50:251	Let errk denote the set of erroneous words in sentence k with respect to the best reference and p be a POS class." ></td>
	<td class="line x" title="51:251	Then n(p,errk) is the number of errors in errk produced by words with POS class p. It should be noted that for the substitution errors, the POS class of the involved reference word is taken into account." ></td>
	<td class="line x" title="52:251	POS tags of the reference words are also used for the deletion errors, and for the insertion errors the POS class of the hypothesis word is taken." ></td>
	<td class="line x" title="53:251	The WER for the word class p can be calculated as: WER(p) = 1N??" ></td>
	<td class="line x" title="54:251	ref Ksummationdisplay k=1 n(p,errk) The sum over all classes is equal to the standard overall WER." ></td>
	<td class="line x" title="55:251	An example of a reference sentence and hypothesis sentence along with the corresponding POS tags is shown in Table 1." ></td>
	<td class="line x" title="56:251	The WER errors, i.e. actual words participating in WER together with their POS classes can be seen in Table 2." ></td>
	<td class="line x" title="57:251	The reference words involved in WER are denoted as reference errors, and hypothesis errors refer to the hypothesis words participating in WER." ></td>
	<td class="line x" title="58:251	Standard WER of the whole sentence is equal to 4/12 = 33.3%." ></td>
	<td class="line x" title="59:251	The contribution of nouns is reference: Mister#N Commissioner#N,#PUN twenty-four#NUM hours#N sometimes#ADV can#V be#V too#ADV much#PRON time#N .#PUN hypothesis: Mrs#N Commissioner#N,#PUN twenty-four#NUM hours#N is#V sometimes#ADV too#ADV much#PRON time#N .#PUN Table 1: Example for illustration of actual errors: a POS tagged reference sentence and a corresponding hypothesis sentence reference errors hypothesis errors error type Mister#N Mrs#N substitution sometimes#ADV is#V substitution can#V deletion be#V sometimes#ADV substitution Table 2: WER errors: actual words which are participating in the word error rate and their corresponding POS classes WER(N) = 1/12 = 8.3%, of verbs WER(V) = 2/12 = 16.7% and of adverbs WER(ADV) = 1/12 = 8.3% 3.3 PER decomposition over POS classes In contrast to WER, standard efficient algorithms for the calculation of PER do not give precise information about contributing words." ></td>
	<td class="line x" title="60:251	However, it is possible to identify all words in the hypothesis which do not have a counterpart in the reference, and vice versa." ></td>
	<td class="line x" title="61:251	These words will be referred to as PER errors." ></td>
	<td class="line x" title="62:251	reference errors hypothesis errors Mister#N Mrs#N be#V is#V can#V Table 3: PER errors: actual words which are participating in the position independent word error rate and their corresponding POS classes An illustration of PER errors is given in Table 3." ></td>
	<td class="line x" title="63:251	50 The number of errors contributing to the standard PER according to the algorithm described in 3.1 is 3 there are two substitutions and one deletion." ></td>
	<td class="line x" title="64:251	The problem with standard PER is that it is not possible to detect which words are the deletion errors, which are the insertion errors, and which words are the substitution errors." ></td>
	<td class="line x" title="65:251	Therefore we introduce an alternative PER based measure which corresponds to the F-measure." ></td>
	<td class="line x" title="66:251	Let herrk refer to the set of words in the hypothesis sentence k which do not appear in the reference sentence k (referred to as hypothesis errors)." ></td>
	<td class="line x" title="67:251	Analogously, let rerrk denote the set of words in the reference sentence k which do not appear in the hypothesis sentence k (referred to as reference errors)." ></td>
	<td class="line x" title="68:251	Then the following measures can be calculated: ??reference PER (RPER) (similar to recall): RPER(p) = 1N??" ></td>
	<td class="line x" title="69:251	ref Ksummationdisplay k=1 n(p,rerrk) ??hypothesis PER (HPER) (similar to precision): HPER(p) = 1N hyp Ksummationdisplay k=1 n(p,herrk) ??F-based PER (FPER): FPER(p) = 1N??" ></td>
	<td class="line x" title="70:251	ref + Nhyp   Ksummationdisplay k=1 (n(p,rerrk) + n(p,herrk)) Since we are basically interested in all words without a counterpart, both in the reference and in the hypothesis, this work will be focused on FPER." ></td>
	<td class="line x" title="71:251	The sum of FPER over all POS classes is equal to the overall FPER, and the latter is always less or equal to the standard PER." ></td>
	<td class="line x" title="72:251	For the example sentence presented in Table 1, the number of hypothesis errors n(e,herrk) is 2 and the number of reference errors n(e,rerrk) is 3 where e denotes the word." ></td>
	<td class="line x" title="73:251	The number of errors contributing to the standard PER is 3, since |Nref ??Nhyp| = 1 and summationtexte |n(e,ref k) ??n(e,hypk)| = 5." ></td>
	<td class="line x" title="74:251	The standard PER is normalised over the reference length Nref = 12 thus being equal to 25%." ></td>
	<td class="line x" title="75:251	The FPER is the sum of hypothesis and reference errors divided by the sum of hypothesis and reference length: FPER = (2 + 3)/(11 + 12) = 5/23 = 21.7%." ></td>
	<td class="line x" title="76:251	The contribution of nouns is FPER(N) = 2/23 = 8.7% and the contribution of verbs is FPER(V) = 3/23 = 13%." ></td>
	<td class="line x" title="77:251	4 Applications for error analysis The decomposed error rates described in Section 3.2 and Section 3.3 contain more details than the standard error rates." ></td>
	<td class="line x" title="78:251	However, for more precise information about certain phenomena some kind of further analysis is required." ></td>
	<td class="line x" title="79:251	In this work, we investigate two possible aspects for error analysis: ??estimation of inflectional errors by the use of FPER errors and base forms ??extracting the distribution of missing words over POS classes using WER errors, FPER errors and base forms." ></td>
	<td class="line x" title="80:251	4.1 Inflectional errors Inflectional errors can be estimated using FPER errors and base forms." ></td>
	<td class="line x" title="81:251	From each referencehypothesis sentence pair, only erroneous words which have the common base forms are taken into account." ></td>
	<td class="line x" title="82:251	The inflectional error rate of each POS class is then calculated in the same way as FPER." ></td>
	<td class="line x" title="83:251	For example, from the PER errors presented in Table 3, the words ?is??and ?be??are candidates for an inflectional error because they are sharing the same base form ?be??" ></td>
	<td class="line x" title="84:251	Inflectional error rate in this example is present only for the verbs, and is calculated in the same way as FPER, i.e. IFPER(V) = 2/23 = 8.7%." ></td>
	<td class="line x" title="85:251	4.2 Missing words Distribution of missing words over POS classes can be extracted from the WER and FPER errors in the following way: the words considered as missing are those which occur as deletions in WER errors and at the same time occur only as reference PER errors without sharing the base form with any hypothesis error." ></td>
	<td class="line x" title="86:251	The use of both WER and PER errors is much more reliable than using only the WER deletion erros because not all deletion errors are produced by missing words: a number of WER deletions appears 51 due to reordering errors." ></td>
	<td class="line x" title="87:251	The information about the base form is used in order to eliminate inflectional errors." ></td>
	<td class="line x" title="88:251	The number of missing words is extracted for each word class and then normalised over the sum of all classes." ></td>
	<td class="line x" title="89:251	For the example sentence pair presented in Table 1, from the WER errors in Table 2 and the PER errors in Table 3 the word ?can??will be identified as missing." ></td>
	<td class="line x" title="90:251	5 Experimental settings 5.1 Translation System The machine translation system used in this work is based on the statistical aproach." ></td>
	<td class="line x" title="91:251	It is built as a log-linear combination of seven different statistical models: phrase based models in both directions, IBM1 models at the phrase level in both directions, as well as target language model, phrase penalty and length penalty are used." ></td>
	<td class="line x" title="92:251	A detailed description of the system can be found in (Vilar et al. , 2005; Matusov et al. , 2006)." ></td>
	<td class="line x" title="93:251	5.2 Task and corpus The corpus analysed in this work is built in the framework of the TC-STAR project." ></td>
	<td class="line x" title="94:251	The training corpus contains more than one million sentences and about 35 million running words of the European Parliament Plenary Sessions (EPPS) in Spanish and English." ></td>
	<td class="line x" title="95:251	The test corpus contains about 1 000 sentences and 28 000 running words." ></td>
	<td class="line x" title="96:251	The OOV rates are low, about 0.5% of the running words for Spanish and 0.2% for English." ></td>
	<td class="line x" title="97:251	The corpus statistics can be seen in Table 4." ></td>
	<td class="line x" title="98:251	More details about the EPPS data can be found in (Vilar et al. , 2005)." ></td>
	<td class="line x" title="99:251	TRAIN Spanish English Sentences 1 167 627 Running words 35 320 646 33 945 468 Vocabulary 159 080 110 636 TEST Sentences 894 1 117 Running words 28 591 28 492 OOVs 0.52% 0.25% Table 4: Statistics of the training and test corpora of the TC-STAR EPPS Spanish-English task." ></td>
	<td class="line x" title="100:251	Test corpus is provided with two references." ></td>
	<td class="line x" title="101:251	6 Error analysis The translation is performed in both directions (Spanish to English and English to Spanish) and the error analysis is done on both the English and the Spanish output." ></td>
	<td class="line x" title="102:251	Morpho-syntactic annotation of the English references and hypotheses is performed using the constraint grammar parser ENGCG (Voutilainen, 1995), and the Spanish texts are annotated using the FreeLing analyser (Carreras et al. , 2004)." ></td>
	<td class="line x" title="103:251	In this way, all references and hypotheses are provided with POS tags and base forms." ></td>
	<td class="line x" title="104:251	The decomposition of WER and FPER is done over the ten main POS classes: nouns (N), verbs (V), adjectives (A), adverbs (ADV), pronouns (PRON), determiners (DET), prepositions (PREP), conjunctions (CON), numerals (NUM) and punctuation marks (PUN)." ></td>
	<td class="line x" title="105:251	Inflectional error rates are also estimated for each POS class using FPER counts and base forms." ></td>
	<td class="line x" title="106:251	Additionally, details about the verb tense and person inflections for both languages as well as about the adjective gender and person inflections for the Spanish output are extracted." ></td>
	<td class="line x" title="107:251	Apart from that, the distribution of missing words over the ten POS classes is estimated using the WER and FPER errors." ></td>
	<td class="line x" title="108:251	6.1 WER and PER (FPER) decompositions Figure 1 presents the decompositions of WER and FPER over the ten basic POS classes for both languages." ></td>
	<td class="line x" title="109:251	The largest part of both word error rates comes from the two most important word classes, namely nouns and verbs, and that the least critical classes are punctuations, conjunctions and numbers." ></td>
	<td class="line x" title="110:251	Adjectives, determiners and prepositions are significantly worse in the Spanish output." ></td>
	<td class="line x" title="111:251	This is partly due to the richer morphology of the Spanish language." ></td>
	<td class="line x" title="112:251	Furthermore, the histograms indicate that the number of erroneus nouns and pronouns is higher in the English output." ></td>
	<td class="line x" title="113:251	As for verbs, WER is higher for English and FPER for Spanish." ></td>
	<td class="line x" title="114:251	This indicates that there are more problems with word order in the English output, and more problems with the correct verb or verb form in the Spanish output." ></td>
	<td class="line x" title="115:251	In addition, the decomposed error rates give an idea of where to put efforts for possible improvements of the system." ></td>
	<td class="line x" title="116:251	For example, working on improvements of verb translations could reduce up to about 10% WER and 7% FPER, working on nouns 52 0 1 2 3 4 5 6 7 8 9 10 11 PUNNUMPREP CONDETPRONADVAVN WER over POS classes [%] English Spanish 0 1 2 3 4 5 6 7 8 9 PUNNUMPREP CONDETPRONADVAVN FPER over POS classes [%] English Spanish Figure 1: Decomposition of WER and FPER [%] over the ten basic POS classes for English and Spanish output up to 8% WER and 5% FPER, whereas there is no reason to put too much efforts on e.g. adverbs since this could lead only to about 2% of WER and FPER reduction." ></td>
	<td class="line x" title="117:251	1 6.2 Inflectional errors Inflectional error rates for the ten POS classes are presented in Figure 2." ></td>
	<td class="line x" title="118:251	For the English language, these errors are significant only for two POS classes: nouns and verbs." ></td>
	<td class="line x" title="119:251	The verbs are the most problematic category in both languages, for Spanish having almost two times higher error rate than for English." ></td>
	<td class="line x" title="120:251	This is due to the very rich morphology of Spanish verbs one base form might have up to about fourty different inflections." ></td>
	<td class="line x" title="121:251	1Reduction of FPER leads to a similar reduction of PER." ></td>
	<td class="line x" title="122:251	0 0.5 1 1.5 2 2.5 PUNNUMPREP CONDETPRONADVAVN inflectional errors [%] English Spanish Figure 2: Inflectional error rates [%] for English and Spanish output Nouns have a higher error rate for English than for Spanish." ></td>
	<td class="line x" title="123:251	The reason for this difference is not clear, since the noun morphology of neither of the languages is particularly rich there is only distinction between singular and plural." ></td>
	<td class="line x" title="124:251	One possible explanation might be the numerous occurences of different variants of the same word, like for example ?Mr??and ?Mister??" ></td>
	<td class="line x" title="125:251	In the Spanish output, two additional POS classes are showing significant error rate: determiners and adjectives." ></td>
	<td class="line x" title="126:251	This is due to the gender and number inflections of those classes which do not exist in the English language for each determiner or adjective, there are four variants in Spanish and only one in English." ></td>
	<td class="line x" title="127:251	Working on inflections of Spanish verbs might reduce approximately 2% of FPER, on English verbs about 1%." ></td>
	<td class="line x" title="128:251	Improvements of Spanish determiners could lead up to about 2% of improvements." ></td>
	<td class="line x" title="129:251	6.2.1 Comparison with human error analysis The results obtained for inflectional errors are comparable with the results of a human error analysis carried out in (Vilar et al. , 2006)." ></td>
	<td class="line x" title="130:251	Although it is difficult to compare all the numbers directly, the overall tendencies are the same: the largest number of translation errors are caused by Spanish verbs, and much less but still a large number of errors by English verbs." ></td>
	<td class="line x" title="131:251	A much smaller but still significant number of errors is due to Spanish adjectives, and only a few errors of English adjectives are present." ></td>
	<td class="line x" title="132:251	Human analysis was done also for the tense and 53 person of verbs, as well as for the number and gender of adjectives." ></td>
	<td class="line x" title="133:251	We use more detailed POS tags in order to extract this additional information and calculate inflectional error rates for such tags." ></td>
	<td class="line x" title="134:251	It should be noted that in contrast to all previous error rates, these error rates are not disjunct but overlapping: many words are contributing to both." ></td>
	<td class="line x" title="135:251	The results are shown in Figure 3, and the tendencies are again the same as those reported in (Vilar et al. , 2006)." ></td>
	<td class="line x" title="136:251	As for verbs, tense errors are much more frequent than person errors for both languages." ></td>
	<td class="line x" title="137:251	Adjective inflections cause certain amount of errors only in the Spanish output." ></td>
	<td class="line x" title="138:251	Contributions of gender and of number are aproximately equal." ></td>
	<td class="line x" title="139:251	0 0.5 1 1.5 2 A numberA genderV personV tense inflectional errors of verbs and adjectives [%] English Spanish Figure 3: More details about inflections: verb tense and person error rates and adjective gender and number error rates [%] 6.3 Missing words Figure 4 presents the distribution of missing words over POS classes." ></td>
	<td class="line x" title="140:251	This distribution has a same behaviour as the one obtained by human error analysis." ></td>
	<td class="line x" title="141:251	Most missing words for both languages are verbs." ></td>
	<td class="line x" title="142:251	For English, the percentage of missing verbs is significantly higher than for Spanish." ></td>
	<td class="line x" title="143:251	The same thing happens for pronouns." ></td>
	<td class="line x" title="144:251	The probable reason for this is the nature of Spanish verbs." ></td>
	<td class="line x" title="145:251	Since person and tense are contained in the suffix, Spanish pronouns are often omitted, and auxiliary verbs do not exist for all tenses." ></td>
	<td class="line x" title="146:251	This could be problematic for a translation system, because it processes only one Spanish word which actually contains two (or more) English words." ></td>
	<td class="line x" title="147:251	0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 PUNNUMPREP CONDETPRONADVAVN missing words [%] eng esp Figure 4: Distribution of missing words over POS classes [%] for English and Spanish output Prepositions are more often missing in Spanish than in English, as well as determiners." ></td>
	<td class="line x" title="148:251	A probable reason is the disproportion of the number of occurrences for those classes between two languages." ></td>
	<td class="line x" title="149:251	7 Conclusions This work presents a framework for extraction of linguistic details from standard word error rates WER and PER and their use for an automatic error analysis." ></td>
	<td class="line x" title="150:251	We presented a method for the decomposition of standard word error rates WER and PER over ten basic POS classes." ></td>
	<td class="line x" title="151:251	We also carried out a detailed analysis of inflectional errors which has shown that the results obtained by our method correspond to those obtained by a human error analysis." ></td>
	<td class="line x" title="152:251	In addition, we proposed a method for analysing missing word errors." ></td>
	<td class="line x" title="153:251	We plan to extend the proposed methods in order to carry out a more detailed error analysis, for example examining different types of verb inflections." ></td>
	<td class="line x" title="154:251	We also plan to examine other types of translation errors like for example errors caused by word order." ></td>
	<td class="line x" title="155:251	Acknowledgements This work was partly funded by the European Union under the integrated project TC-STAR??Technology and Corpora for Speech to Speech Translation (IST2002-FP6-506738)." ></td>
	<td class="line x" title="156:251	54 References Bogdan Babych and Anthony Hartley." ></td>
	<td class="line x" title="157:251	2004." ></td>
	<td class="line x" title="158:251	Extending BLEU MT Evaluation Method with Frequency Weighting." ></td>
	<td class="line x" title="159:251	In Proc." ></td>
	<td class="line x" title="160:251	of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain, July." ></td>
	<td class="line x" title="161:251	Satanjeev Banerjee and Alon Lavie." ></td>
	<td class="line x" title="162:251	2005." ></td>
	<td class="line o" title="163:251	METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgements." ></td>
	<td class="line x" title="164:251	In 43rd Annual Meeting of the Assoc." ></td>
	<td class="line x" title="165:251	for Computational Linguistics: Proc." ></td>
	<td class="line x" title="166:251	Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65??2, Ann Arbor, MI, June." ></td>
	<td class="line x" title="167:251	Xavier Carreras, Isaac Chao, Llus Padro, and Muntsa Padro. 2004." ></td>
	<td class="line x" title="168:251	FreeLing: An Open-Source Suite of Language Analyzers." ></td>
	<td class="line x" title="169:251	In Proc." ></td>
	<td class="line x" title="170:251	4th Int." ></td>
	<td class="line x" title="171:251	Conf." ></td>
	<td class="line x" title="172:251	on Language Resources and Evaluation (LREC), pages 239??" ></td>
	<td class="line x" title="173:251	242, Lisbon, Portugal, May. George Doddington." ></td>
	<td class="line x" title="174:251	2002." ></td>
	<td class="line x" title="175:251	Automatic evaluation of machine translation quality using n-gram co-occurrence statistics." ></td>
	<td class="line x" title="176:251	In Proc." ></td>
	<td class="line x" title="177:251	ARPA Workshop on Human Language Technology, pages 128??32, San Diego." ></td>
	<td class="line x" title="178:251	Sharon Goldwater and David McClosky." ></td>
	<td class="line x" title="179:251	2005." ></td>
	<td class="line x" title="180:251	Improving stastistical machine translation through morphological analysis." ></td>
	<td class="line x" title="181:251	In Proc." ></td>
	<td class="line x" title="182:251	of the Conf." ></td>
	<td class="line x" title="183:251	on Empirical Methods for Natural Language Processing (EMNLP), Vancouver, Canada, October." ></td>
	<td class="line x" title="184:251	Gregor Leusch, Nicola Ueffing, David Vilar, and Hermann Ney." ></td>
	<td class="line x" title="185:251	2005." ></td>
	<td class="line x" title="186:251	Preprocessing and Normalization for Automatic Evaluation of Machine Translation." ></td>
	<td class="line x" title="187:251	In 43rd Annual Meeting of the Assoc." ></td>
	<td class="line x" title="188:251	for Computational Linguistics: Proc." ></td>
	<td class="line x" title="189:251	Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 17??4, Ann Arbor, MI, June." ></td>
	<td class="line x" title="190:251	Association for Computational Linguistics." ></td>
	<td class="line x" title="191:251	Gregor Leusch, Nicola Ueffing, and Hermann Ney." ></td>
	<td class="line x" title="192:251	2006." ></td>
	<td class="line x" title="193:251	CDER: Efficient MT Evaluation Using Block Movements." ></td>
	<td class="line x" title="194:251	In EACL06, pages 241??48, Trento, Italy, April." ></td>
	<td class="line x" title="195:251	Vladimir Iosifovich Levenshtein." ></td>
	<td class="line x" title="196:251	1966." ></td>
	<td class="line x" title="197:251	Binary Codes Capable of Correcting Deletions, Insertions and Reversals." ></td>
	<td class="line x" title="198:251	Soviet Physics Doklady, 10(8):707??10, February." ></td>
	<td class="line x" title="199:251	Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney." ></td>
	<td class="line x" title="200:251	2005." ></td>
	<td class="line x" title="201:251	Evaluating Machine Translation Output with Automatic Sentence Segmentation." ></td>
	<td class="line x" title="202:251	In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 148??54, Pittsburgh, PA, October." ></td>
	<td class="line x" title="203:251	Evgeny Matusov, Richard Zens, David Vilar, Arne Mauser, Maja Popovic, and Hermann Ney." ></td>
	<td class="line x" title="204:251	2006." ></td>
	<td class="line x" title="205:251	The RWTH Machine Translation System." ></td>
	<td class="line x" title="206:251	In TC-Star Workshop on Speech-to-Speech Translation, pages 31??36, Barcelona, Spain, June." ></td>
	<td class="line x" title="207:251	Sonja Nieen and Hermann Ney." ></td>
	<td class="line x" title="208:251	2000." ></td>
	<td class="line x" title="209:251	Improving SMT quality with morpho-syntactic analysis." ></td>
	<td class="line x" title="210:251	In COLING ??0: The 18th Int." ></td>
	<td class="line x" title="211:251	Conf." ></td>
	<td class="line x" title="212:251	on Computational Linguistics, pages 1081??085, Saarbrucken, Germany, July." ></td>
	<td class="line x" title="213:251	Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu." ></td>
	<td class="line x" title="214:251	2002." ></td>
	<td class="line x" title="215:251	BLEU: a method for automatic evaluation of machine translation." ></td>
	<td class="line x" title="216:251	In Proc." ></td>
	<td class="line x" title="217:251	of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311??18, Philadelphia, PA, July." ></td>
	<td class="line x" title="218:251	Maja Popovic and Hermann Ney." ></td>
	<td class="line x" title="219:251	2006." ></td>
	<td class="line x" title="220:251	Error Analysis of Verb Inflections in Spanish Translation Output." ></td>
	<td class="line x" title="221:251	In TC-Star Workshop on Speech-to-Speech Translation, pages 99??03, Barcelona, Spain, June." ></td>
	<td class="line x" title="222:251	Maja Popovic, Adri`a de Gispert, Deepa Gupta, Patrik Lambert, Hermann Ney, Jose B. Mari?no, Marcello Federico, and Rafael Banchs." ></td>
	<td class="line x" title="223:251	2006." ></td>
	<td class="line x" title="224:251	Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output." ></td>
	<td class="line x" title="225:251	In Proc." ></td>
	<td class="line x" title="226:251	of the HLT-NAACL Workshop on Statistical Machine Translation, pages 1??, New York, NY, June." ></td>
	<td class="line x" title="227:251	Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul." ></td>
	<td class="line x" title="228:251	2006." ></td>
	<td class="line x" title="229:251	A Study of Translation Error Rate with Targeted Human Annotation." ></td>
	<td class="line x" title="230:251	In Proc." ></td>
	<td class="line x" title="231:251	of the 7th Conf." ></td>
	<td class="line x" title="232:251	of the Association for Machine Translation in the Americas (AMTA 06), pages 223??31, Boston, MA." ></td>
	<td class="line x" title="233:251	2005." ></td>
	<td class="line x" title="234:251	TC-STAR technology and corpora for speech to speech translation." ></td>
	<td class="line x" title="235:251	Integrated project TCSTAR (IST2002-FP6-506738) funded by the European Commission." ></td>
	<td class="line x" title="236:251	http://www.tc-star.org/." ></td>
	<td class="line x" title="237:251	David Vilar, Evgeny Matusov, Sa?sa Hasan, Richard Zens, and Hermann Ney." ></td>
	<td class="line x" title="238:251	2005." ></td>
	<td class="line x" title="239:251	Statistical Machine Translation of European Parliamentary Speeches." ></td>
	<td class="line x" title="240:251	In Proc." ></td>
	<td class="line x" title="241:251	MT Summit X, pages 259??66, Phuket, Thailand, September." ></td>
	<td class="line x" title="242:251	David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney." ></td>
	<td class="line x" title="243:251	2006." ></td>
	<td class="line x" title="244:251	Error Analysis of Statistical Machine Translation Output." ></td>
	<td class="line x" title="245:251	In Proc." ></td>
	<td class="line x" title="246:251	of the Fifth Int." ></td>
	<td class="line x" title="247:251	Conf." ></td>
	<td class="line x" title="248:251	on Language Resources and Evaluation (LREC), pages 697??02, Genoa, Italy, May. Atro Voutilainen." ></td>
	<td class="line x" title="249:251	1995." ></td>
	<td class="line x" title="250:251	ENGCG -Constraint Grammar Parser of English." ></td>
	<td class="line x" title="251:251	http://www2.lingsoft.fi/doc/engcg/intro/ ." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0714
Labelled Dependencies in Machine Translation Evaluation
Owczarzak, Karolina;Van Genabith, Josef;Way, Andy;"></td>
	<td class="line x" title="1:140	Proceedings of the Second Workshop on Statistical Machine Translation, pages 104??11, Prague, June 2007." ></td>
	<td class="line x" title="2:140	c2007 Association for Computational Linguistics Labelled Dependencies in Machine Translation Evaluation Karolina Owczarzak Josef van Genabith Andy Way National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland {owczarzak,josef,away}@computing.dcu.ie Abstract We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser." ></td>
	<td class="line x" title="3:140	Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation." ></td>
	<td class="line x" title="4:140	In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores." ></td>
	<td class="line x" title="5:140	1 Introduction Since the creation of BLEU (Papineni et al. , 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention." ></td>
	<td class="line x" title="6:140	Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al. , 2006)." ></td>
	<td class="line x" title="7:140	As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references." ></td>
	<td class="line x" title="8:140	A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference." ></td>
	<td class="line x" title="9:140	Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement." ></td>
	<td class="line x" title="10:140	Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a ?normalized??representation of (some) syntactic variants of a given sentence." ></td>
	<td class="line x" title="11:140	While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a LexicalFunctional Grammar (LFG) parser." ></td>
	<td class="line x" title="12:140	These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. The presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when a lexical item finds itself in a correct relation but with an incorrect partner." ></td>
	<td class="line x" title="13:140	Moreover, we use a number of best parses for the translation and the reference, which serves to decrease the amount of noise that can be introduced by the process of parsing and extracting dependency information." ></td>
	<td class="line x" title="14:140	The translation and reference files are analyzed by a treebank-based, probabilistic LFG parser (Cahill et al. , 2004), which produces a set of dependency triples for each input." ></td>
	<td class="line x" title="15:140	The translation set is compared to the reference set, and the number of matches is calculated, giving the 104 precision, recall, and f-score for each particular translation." ></td>
	<td class="line x" title="16:140	In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score." ></td>
	<td class="line pc" title="17:140	In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al. , 2003), Translation Error Rate (TER) (Snover et al. , 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment." ></td>
	<td class="line x" title="18:140	Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005)." ></td>
	<td class="line x" title="19:140	The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes." ></td>
	<td class="line x" title="20:140	2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="21:140	C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of CFG trees." ></td>
	<td class="line x" title="22:140	F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), etc. , approximating to predicate-argument structure or simple logical forms." ></td>
	<td class="line x" title="23:140	C-structure and f-structure are related in 1 We omit HTER (Human-Targeted Translation Error Rate), as it is not fully automatic and requires human input." ></td>
	<td class="line x" title="24:140	terms of functional annotations (attribute-value structure equations) in c-structure trees, describing f-structures." ></td>
	<td class="line x" title="25:140	While c-structure is sensitive to surface rearrangement of constituents, f-structure abstracts away from the particulars of the surface realization." ></td>
	<td class="line x" title="26:140	The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures, shown in (1)." ></td>
	<td class="line x" title="27:140	(1) C-structure: F-structure: S NP VP | John V NP-TMP | | resigned yesterday SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} S NP NP VP | | | Yesterday John V | resigned SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} Note that if these sentences were a translationreference pair, they would receive a less-thanperfect score from string-based metrics." ></td>
	<td class="line x" title="28:140	For example, BLEU with add-one smoothing2 gives this pair a score of barely 0.3781." ></td>
	<td class="line x" title="29:140	This is because, although all three unigrams from the ?translation??" ></td>
	<td class="line x" title="30:140	(John; resigned; yesterday) are present in the reference, which contains four items including the comma (Yesterday;,; John; resigned), the ?translation??contains only one bigram (John resigned) that matches the ?reference??(Yesterday,;, John; John resigned), and no matching trigrams." ></td>
	<td class="line x" title="31:140	The f-structure can also be described in terms of a flat set of triples." ></td>
	<td class="line x" title="32:140	In triples format, the fstructure in (1) is represented as follows: {subj(resign, john), pers(john, 3), num(john, sg), tense(resign, past), adj(resign, yesterday), pers(yesterday, 3), num(yesterday, sg)}." ></td>
	<td class="line x" title="33:140	2 We use smoothing because the original BLEU metric gives zero points to sentences with fewer than one fourgram." ></td>
	<td class="line x" title="34:140	105 Cahill et al.(2004) presents a set of Penn-II Treebank-based LFG parsing resources." ></td>
	<td class="line x" title="36:140	Their approach distinguishes 32 types of dependencies, including grammatical functions and morphological information." ></td>
	<td class="line x" title="37:140	This set can be divided into two major groups: a group of predicate-only dependencies and non-predicate dependencies." ></td>
	<td class="line x" title="38:140	Predicate-only dependencies are those whose path ends in a predicate-value pair, describing grammatical relations." ></td>
	<td class="line x" title="39:140	For example, for the fstructure in (1), predicate-only dependencies would include: {subj(resign, john), adj(resign, yesterday)}." ></td>
	<td class="line x" title="40:140	Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, and relative clause pronoun." ></td>
	<td class="line x" title="41:140	The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, number, verbal particle, participle, passive, person, pronoun surface form, tense, and infinitival clause." ></td>
	<td class="line x" title="42:140	In parser evaluation, the quality of the fstructures produced automatically can be checked against a set of gold standard sentences annotated with f-structures by a linguist." ></td>
	<td class="line x" title="43:140	The evaluation is conducted by calculating the precision and recall between the set of dependencies produced by the parser, and the set of dependencies derived from the human-created f-structure." ></td>
	<td class="line x" title="44:140	Usually, two versions of f-score are calculated: one for all the dependencies for a given input, and a separate one for the subset of predicate-only dependencies." ></td>
	<td class="line x" title="45:140	In this paper, we use the parser developed by Cahill et al.(2004), which automatically annotates input text with c-structure trees and f-structure dependencies, obtaining high precision and recall rates." ></td>
	<td class="line x" title="47:140	3 3 Related work 3.1 String-based metrics The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al.(2006), but the criticism is widespread." ></td>
	<td class="line x" title="49:140	Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al. , 2002)." ></td>
	<td class="line x" title="50:140	Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics." ></td>
	<td class="line x" title="51:140	Some of them concentrate mainly on word order, like General Text Matcher (Turian et al. , 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al. , 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference." ></td>
	<td class="line oc" title="52:140	Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al. , 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy." ></td>
	<td class="line x" title="53:140	Kauchak and Barzilay (2006) and Owczarzak et al.(2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="56:140	Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al.(2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching." ></td>
	<td class="line x" title="58:140	Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation?s distance from human-level quality." ></td>
	<td class="line x" title="59:140	3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering." ></td>
	<td class="line x" title="60:140	By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information." ></td>
	<td class="line x" title="61:140	Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 4 http://wordnet.princeton.edu/ 106 is based on matching headword chains, i.e. sequences of words that correspond to a path in the unlabelled dependency tree of the sentence." ></td>
	<td class="line x" title="62:140	Dependency trees are created by extracting a headword for each node of the syntactic tree, according to the rules used by the parser of Collins (1999), where every subtree represents the modifier information for its root headword." ></td>
	<td class="line x" title="63:140	The dependency trees for the translation and the reference are converted into flat headword chains, and the number of overlapping n-grams between the translation and the reference chains is calculated." ></td>
	<td class="line x" title="64:140	Our method, extending this line of research with the use of labelled LFG dependencies, partial matching, and n-best parses, allows us to considerably outperform Liu and Gildea?s (2005) highest correlations with human judgement (they report 0.144 for the correlation with human fluency judgement, 0.202 for the correlation with human overall judgement), although it has to be kept in mind that such comparison is only tentative, as their correlation is calculated on a different test set." ></td>
	<td class="line x" title="65:140	4 LFG f-structure in MT evaluation LFG-based automatic MT evaluation reflects the same process that underlies the evaluation of parser-produced f-structure quality against a gold standard: we parse the translation and the reference, and then, for each sentence, we check the set of labelled translation dependencies against the set of labelled reference dependencies, counting the number of matches." ></td>
	<td class="line x" title="66:140	As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair." ></td>
	<td class="line x" title="67:140	4.1 Determining parser noise Because we are comparing two outputs that were produced automatically, there is a possibility that the result will not be noise-free, even if the parser fails to provide a parse only in 0.1% of cases." ></td>
	<td class="line x" title="68:140	To assess the amount of noise that the parser introduces, Owczarzak et al.(2006) conducted an experiment where 100 English sentences were hand-modified so that the position of adjuncts was changed, but the sentence remained grammatical and the meaning was not influenced." ></td>
	<td class="line x" title="70:140	This way, an ideal parser should give both the source and the modified sentence the same f-structure, similarly to the example presented in (1)." ></td>
	<td class="line x" title="71:140	The modified sentences were treated like a translation file, and the original sentences played the part of the reference." ></td>
	<td class="line x" title="72:140	Each set was run through the parser, and the dependency triples obtained from the ?translation??were compared against the dependency triples for the ?reference??" ></td>
	<td class="line x" title="73:140	calculating the f-score." ></td>
	<td class="line o" title="74:140	Additionally, the same ?translationreference??set was scored with other metrics (TER, METEOR, BLEU, NIST, and GTM)." ></td>
	<td class="line x" title="75:140	The results, including the distinction between f-scores for all dependencies and predicate-only dependencies, appear in Table 1." ></td>
	<td class="line o" title="76:140	baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1." ></td>
	<td class="line x" title="77:140	Scores for sentences with reordered adjuncts The baseline column shows the upper bound for a given metric: the score which a perfect translation, word-for-word identical to the reference, would obtain.5 The other column lists the scores that the metrics gave to the ?translation??containing reordered adjunct." ></td>
	<td class="line x" title="78:140	As can be seen, the dependency and predicate-only dependency scores are lower than the perfect 100, reflecting the noise introduced by the parser." ></td>
	<td class="line x" title="79:140	We propose that the problem of parser noise can be alleviated by introducing a number of best parses into the comparison between the translation and the reference." ></td>
	<td class="line x" title="80:140	Table 2 shows how increasing the number of parses available for comparison brings our method closer to an ideal noise-free parser." ></td>
	<td class="line x" title="81:140	5 Two things have to be noted here: (1) in the case of NIST the perfect score differs from text to text, which is why the percentage points are provided along the numerical score, and (2) in the case of TER the lower the score, the better the translation, so the perfect translation will receive 0, and there is no upper bound on the score, which makes this particular metric extremely difficult to directly compare with others." ></td>
	<td class="line x" title="82:140	107 dependency f-score 1 best 96.56 2 best 97.31 5 best 97.90 10 best 98.31 20 best 98.59 30 best 98.74 50 best 98.79 baseline 100 Table 2." ></td>
	<td class="line x" title="83:140	Dependency f-scores for sentences with reordered adjuncts with n-best parses available It has to be noted, however, that increasing the number of parses beyond a certain threshold does little to further improve results, and at the same time it considerably decreases the efficiency of the method, so it is important to find the right balance between these two factors." ></td>
	<td class="line x" title="84:140	In our opinion, the optimal value would be 10-best parses." ></td>
	<td class="line x" title="85:140	4.2 Correlation with human judgement ??" ></td>
	<td class="line x" title="86:140	MultiTrans 4.2.1 Experimental design To evaluate the correlation with human assessment, we used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4, which consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores for a subset of the translation-reference pairs." ></td>
	<td class="line x" title="87:140	Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment." ></td>
	<td class="line x" title="88:140	In effect, the test set created from this data contained 16,800 segments." ></td>
	<td class="line o" title="89:140	As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method." ></td>
	<td class="line x" title="90:140	4.2.2 Labelled dependency-based method We examined a number of modifications of the dependency-based method in order to find out which one gives the highest correlation with human scores." ></td>
	<td class="line x" title="91:140	The correlation differences between immediate neighbours in the ranking were often too small to be statistically significant; however, there is a clear overall trend towards improvement." ></td>
	<td class="line x" title="92:140	Besides the plain version of the dependency fscore, we also looked at the f-score calculated on predicate dependencies only (ignoring ?atomic??" ></td>
	<td class="line x" title="93:140	features such as person, number, tense, etc.), which turned out not to correlate well with human judgements." ></td>
	<td class="line x" title="94:140	Another addition was the use of 2-, 10-, or 50best parses of the translation and reference sentences, which partially neutralized parser noise and resulted in increased correlations." ></td>
	<td class="line x" title="95:140	We also created a version where predicate dependencies of the type subj(resign,John) are split into two parts, each time replacing one of the elements participating in the relation with a variable, giving in effect subj(resign,x) and subj(y,John)." ></td>
	<td class="line x" title="96:140	This lets us score partial matches, where one correct lexical object happens to find itself in the correct relation, but with an incorrect ?partner??" ></td>
	<td class="line o" title="97:140	Lastly, we added WordNet synonyms into the matching process to accommodate lexical variation, and to compare our WordNet-enhanced method with the WordNet-enhanced version of METEOR." ></td>
	<td class="line x" title="98:140	4.2.3 Results We calculated Pearson?s correlation coefficient for segment-level scores that were given by each metric and by human judges." ></td>
	<td class="line x" title="99:140	The results of the correlation are shown in Table 3." ></td>
	<td class="line x" title="100:140	Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other." ></td>
	<td class="line x" title="101:140	Rows are ordered by the highest value of the (absolute) correlation with the human score." ></td>
	<td class="line x" title="102:140	First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy." ></td>
	<td class="line x" title="103:140	This finding has been previously reported, among others, in Liu and Gildea (2005)." ></td>
	<td class="line x" title="104:140	However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area." ></td>
	<td class="line x" title="105:140	This can be explained by the method?s sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent." ></td>
	<td class="line p" title="106:140	As to the correlation with human evaluation of translation accuracy, our method currently falls 108 short of METEOR." ></td>
	<td class="line o" title="107:140	This is caused by the fact that METEOR assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic form." ></td>
	<td class="line n" title="108:140	Interestingly, while METEOR, with or without WordNet, considerably outperforms all other metrics when it comes to the correlation with human judgements of translation accuracy, it falls well behind most versions of our dependency-based method in correlation with human scores of translation fluency." ></td>
	<td class="line x" title="109:140	Surprisingly, adding partial matching to the dependency-based method resulted in the greatest increase in correlation levels, to the extent that the partial-match versions consistently outperformed versions with a larger number of parses available but without the partial match." ></td>
	<td class="line x" title="110:140	The most interesting effect was that the partial-match versions (even those with just a single parse) offered results comparable to or higher than the addition of WordNet to the matching process when it comes to accuracy and overall judgement." ></td>
	<td class="line x" title="111:140	5 Current and future work Fluency and accuracy are two very different aspects of translation quality, each with its own set of conditions along which the input is evaluated." ></td>
	<td class="line x" title="112:140	Therefore, it seems unfair to expect a single automatic metric to correlate highly with human judgements of both at the same time." ></td>
	<td class="line o" title="113:140	This pattern is very noticeable in Table 3: if a metric is (relatively) good at correlating with fluency, its accuracy correlation suffers (GTM might serve as an example here), and the opposite holds as well (see METEOR?s scores)." ></td>
	<td class="line x" title="114:140	It does not mean that any improvement that increases the method?s correlation with one aspect will result in a decrease in the correlation with the other aspect; but it does suggest that a possible way of development would be to target these correlations separately, if we want our automated metrics to reflect human scores better." ></td>
	<td class="line x" title="115:140	At the same time, string-based metrics might have already exhausted their potential when it comes to increasing their correlation with human evaluation; as has been pointed out before, these metrics can only tell us that two strings differ, but they cannot distinguish legitimate grammatical variance from ungrammatical variance." ></td>
	<td class="line x" title="116:140	As the quality of MT Table 3." ></td>
	<td class="line x" title="117:140	Pearson?s correlation between human scores and evaluation metrics." ></td>
	<td class="line o" title="118:140	Legend: d = dependency f-score, _pr = predicate-only f-score, 2, 10, 50 = n-best parses; var = partial-match version; M = METEOR, WN = WordNet6 improves, the community will need metrics that are more sensitive in this respect." ></td>
	<td class="line x" title="119:140	After all, the true quality of MT depends on producing grammatical output which describes the same concept as the source utterance, and the string identity with a reference is only a very selective approximation of this goal." ></td>
	<td class="line x" title="120:140	6 In general terms, an increase of 0.022 or more between any two scores in the same column is significant with a 95% confidence interval." ></td>
	<td class="line x" title="121:140	The statistical significance of correlation differences was calculated using Fisher?s z??" ></td>
	<td class="line x" title="122:140	transformation and the general formula for confidence interval." ></td>
	<td class="line x" title="123:140	fluency accuracy average d_50+WN 0.177 M+WN 0.294 M+WN 0.255 d+WN 0.175 M 0.278 d_50_var 0.252 d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250 GTM 0.172 NIST 0.273 d_10_var 0.250 d_10_var 0.172 d_10_var 0.273 d_2_var 0.247 d_50 0.171 d_2_var 0.270 d+WN 0.244 d_2_var 0.168 d_50+WN 0.269 d_50 0.243 d_10 0.168 d_var 0.266 d_var 0.243 d_var 0.165 d_50 0.262 M 0.242 d_2 0.164 d_10 0.262 d_10 0.242 d 0.161 d+WN 0.260 NIST 0.238 BLEU 0.155 d_2 0.257 d_2 0.237 M+WN 0.153 d 0.256 d 0.235 M 0.149 d_pr 0.240 d_pr 0.216 NIST 0.146 GTM 0.203 GTM 0.208 d_pr 0.143 BLEU 0.199 BLEU 0.197 TER -0.133 TER -0.192 TER -0.182 109 In order to maximize the correlation with human scores of fluency, we plan to look more closely at the parser output, and implement some basic transformations which would allow an even deeper logical analysis of input (e.g. passive to active voice transformation)." ></td>
	<td class="line x" title="124:140	Additionally, we want to take advantage of the fact that the score produced by the dependencybased method is the proportional average of matches for a group of up to 32 (but usually far fewer) different dependency types." ></td>
	<td class="line x" title="125:140	We plan to implement a set of weights, one for each dependency type, trained in such a way as to maximize the correlation of the final dependency fscore with human evaluation." ></td>
	<td class="line x" title="126:140	In a preliminary experiment, for example, assigning a low weight to the topic dependency increases our correlations slightly (this particular case can also be seen as a transformation into a more basic logical form by removing non-elementary dependency types)." ></td>
	<td class="line x" title="127:140	In a similar direction, we want to experiment more with the f-score calculations." ></td>
	<td class="line x" title="128:140	Initial check shows that assigning a higher weight to recall than to precision improves results." ></td>
	<td class="line x" title="129:140	To improve the correlation with accuracy judgements, we would like to experiment using a paraphrase set derived from a large parallel corpus, as described in Owczarzak et al.(2006)." ></td>
	<td class="line x" title="131:140	While retaining the advantage of having a similar size to a corresponding set of WordNet synonyms, this set will also capture low-level syntactic variations, which can increase the number of matches." ></td>
	<td class="line x" title="132:140	6 Conclusions In this paper we present a linguisticallymotivated method for automatically evaluating the output of Machine Translation." ></td>
	<td class="line x" title="133:140	Most currently used popular metrics rely on comparing translation and reference on a string level." ></td>
	<td class="line x" title="134:140	Even given reordering, stemming, and synonyms for individual words, current methods are still far from reaching human ability to assess the quality of translation, and there exists a need in the community to develop more dependable metrics." ></td>
	<td class="line x" title="135:140	Our method explores one such direction of development, comparing the sentences on the level of their grammatical structure, as exemplified by their fstructure labelled dependency triples produced by an LFG parser." ></td>
	<td class="line x" title="136:140	In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score." ></td>
	<td class="line x" title="137:140	The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric." ></td>
	<td class="line x" title="138:140	Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper." ></td>
	<td class="line x" title="139:140	We would also like to thank our reviewers and Dan Melamed for their insightful comments." ></td>
	<td class="line x" title="140:140	All remaining errors are our own." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0716
Using Paraphrases for Parameter Tuning in Statistical Machine Translation
Madnani, Nitin;Ayan, Necip Fazil;Resnik, Philip;Dorr, Bonnie Jean;"></td>
	<td class="line x" title="1:171	Proceedings of the Second Workshop on Statistical Machine Translation, pages 120??27, Prague, June 2007." ></td>
	<td class="line x" title="2:171	c2007 Association for Computational Linguistics Using Paraphrases for Parameter Tuning in Statistical Machine Translation Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD, 20742 {nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu Abstract Most state-of-the-art statistical machine translation systems use log-linear models, whicharedefinedintermsofhypothesisfeatures and weights for those features." ></td>
	<td class="line x" title="3:171	It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations." ></td>
	<td class="line x" title="4:171	However, obtaining reference translations is expensive." ></td>
	<td class="line x" title="5:171	In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resultingparaphrasescanbeusedtodrasticallyreducethenumberofhumanreferencetranslations needed for parameter tuning, without a significant decrease in translation quality." ></td>
	<td class="line x" title="6:171	1 Introduction Viewed at a very high level, statistical machine translationinvolvesfourphases: languageandtranslation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al. , 2003)." ></td>
	<td class="line x" title="7:171	SincetheirintroductioninstatisticalMTbyOchand Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems." ></td>
	<td class="line x" title="8:171	Typically such a model takes the form summationdisplay i i?i( f,e) (1) where ?i are features of the hypothesis e and i are weights associated with those features." ></td>
	<td class="line x" title="9:171	Selecting appropriate weights i is essential in order to obtain good translation performance." ></td>
	<td class="line x" title="10:171	Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear modelparametersrelativetoameasureoftranslation quality." ></td>
	<td class="line x" title="11:171	This has become much more standard than optimizing the conditional probability of the trainingdatagiventhemodel(i.e. ,amaximumlikelihood criterion), as was common previously." ></td>
	<td class="line oc" title="12:171	Och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation; BLEU (Papineni et al. , 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al. , 2006)." ></td>
	<td class="line x" title="13:171	Minimum error rate training?and more generally, optimization of parameters relative to a translation quality measure?relies on data sets in which source language sentences are paired with (sets of) reference translations." ></td>
	<td class="line x" title="14:171	It is widely agreed that, at least for the widely used BLEU criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translationsaspossible." ></td>
	<td class="line x" title="15:171	Intuitivelythismakessense: if there are alternative ways to phrase the meaning of the source sentence in the target language, then the translation quality criterion should take as many of those variations into account as possible." ></td>
	<td class="line x" title="16:171	To do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided." ></td>
	<td class="line x" title="17:171	This reliance on multiple reference translations createsaproblem, becausereferencetranslationsare labor intensive and expensive to obtain." ></td>
	<td class="line x" title="18:171	A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al. , 2006)." ></td>
	<td class="line x" title="19:171	Some 120 efforts have been made to develop alternative processes for eliciting translations, e.g., from users on the Web (Oard, 2003) or from informants in lowdensity languages (Probst et al. , 2002)." ></td>
	<td class="line x" title="20:171	However, reference translations for parameter tuning and evaluation remain a severe data bottleneck for such approaches." ></td>
	<td class="line x" title="21:171	Note, however, one crucial property of reference translations: they are paraphrases, i.e., multiple expressions of the same meaning." ></td>
	<td class="line x" title="22:171	Automatic techniques exist for generating paraphrases." ></td>
	<td class="line x" title="23:171	Although one would clearly like to retain human translations as the benchmark for evaluation of translation, might it be possible to usefully increase the number of reference translations for tuning by using automatic paraphrase techniques?" ></td>
	<td class="line x" title="24:171	In this paper, we demonstrate that it is, in fact, possible to do so." ></td>
	<td class="line x" title="25:171	Section 2 briefly describes our translation framework." ></td>
	<td class="line x" title="26:171	Section 3 lays out a novel technique for paraphrasing, designed with the application to parameter tuning in mind." ></td>
	<td class="line x" title="27:171	Section 4 presentsevaluationresultsusingastateoftheartstatistical MT system, demonstrating that half the human reference translations in a standard 4-reference tuning set can be replaced with automatically generated paraphrases, with nosignificant decrease inMT system performance." ></td>
	<td class="line x" title="28:171	In Section 5 we discuss related work, and in Section 6 we summarize the results and discuss plans for future research." ></td>
	<td class="line x" title="29:171	2 Translation Framework The work described in this paper makes use of the Hiero statistical MT framework (Chiang, 2007)." ></td>
	<td class="line x" title="30:171	Hiero is formally based on a weighted synchronous context-free grammar (CFG), containing synchronous rules of the form X ???e, f,?k1( f,e,X)??(2) where X is a symbol from the nonterminal alphabet, and e and f can contain both words (terminals) andvariables(nonterminals)thatserveasplaceholders for other phrases." ></td>
	<td class="line x" title="31:171	In the context of statistical MT,wherephrase-basedmodelsarefrequentlyused, these synchronous rules can be interpreted as pairs of hierarchical phrases." ></td>
	<td class="line x" title="32:171	The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexical re-orderings, but phrasal re-orderings, as well." ></td>
	<td class="line x" title="33:171	Each ?(e, f,X) denotes a feature function defined on the pair of hierarchical phrases.1 Feature functions represent conditional and joint co-occurrence probabilities over the hierarchical paraphrase pair." ></td>
	<td class="line x" title="34:171	The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data." ></td>
	<td class="line x" title="35:171	Briefly, training a Hiero model proceeds as follows: ??GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence." ></td>
	<td class="line x" title="36:171	??Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al. , 2003; Och and Ney, 2004)." ></td>
	<td class="line x" title="37:171	??Grammar rules in the form of equation (2) are induced by ?subtracting??out hierarchical phrase pairs from these initial phrase pairs." ></td>
	<td class="line x" title="38:171	??Fractional counts are assigned to each produced rule: c(X ???e, f??" ></td>
	<td class="line x" title="39:171	= msummationdisplay j=1 1 njr (3) where m is the number of initial phrase pairs that give rise to this grammar rule and njr is the number of grammar rules produced by the jth initial phrase pair." ></td>
	<td class="line x" title="40:171	??Feature functions ?k1( f,e,X) are calculated for each rule using the accumulated counts." ></td>
	<td class="line x" title="41:171	Oncetraininghastakenplace,minimumerrorrate training (Och, 2003) is used to tune the parameters i. Finally, decoding in Hiero takes place using a CKY synchronous parser with beam search, augmented to permit efficient incorporation of language model scores (Chiang, 2007)." ></td>
	<td class="line x" title="42:171	Given a source language sentence f, the decoder parses the source language sentence using the grammar it has learned 1Currently only one nonterminal symbol is used in Hiero productions." ></td>
	<td class="line x" title="43:171	121 during training, with parser search guided by the model; a target-language hypothesis is generated simultaneously via the synchronous rules, and the yieldofthathypothesizedanalysisrepresentsthehypothesized string e in the target language." ></td>
	<td class="line x" title="44:171	3 Generating Paraphrases As discussed in Section 1, our goal is to make it possible to accomplish the parameter-tuning phase using fewer human reference translations." ></td>
	<td class="line x" title="45:171	We accomplish this by beginning with a small set of human reference translations for each sentence in the development set, and expanding that set by automatically paraphrasing each member of the set rather than by acquiring more human translations." ></td>
	<td class="line x" title="46:171	Most previous work on paraphrase has focused on high quality rather than coverage (Barzilay and Lee, 2003; Quirk et al. , 2004), but generating artificial references for MT parameter tuning in our setting has two unique properties compared to other paraphrase applications." ></td>
	<td class="line x" title="47:171	First, we would like to obtain 100% coverage, in order to avoid modifications to our minimum error rate training infrastructure.2 Second, we prefer that paraphrases be as distinct as possible from the original sentences, while retaining as much of the original meaning as possible." ></td>
	<td class="line x" title="48:171	In order to satisfy these two properties, we approach sentence-level paraphrase for English as a problem of English-to-English translation, constructing the model using English-F translation, for a second language F, as a pivot." ></td>
	<td class="line x" title="49:171	Following Bannard and Callison-Burch (2005), we first identify English-to-F correspondences, then map from English to English by following translation units from English to F and back." ></td>
	<td class="line x" title="50:171	Then, generalizing their approach, we use those mappings to create a well defined English-to-English translation model." ></td>
	<td class="line x" title="51:171	The parameters of this model are tuned using MERT, and then the model is used in an the (unmodified) statistical MT system, yielding sentence-level English paraphrases by means of decoding input English sentences." ></td>
	<td class="line x" title="52:171	The remainder of this section presents this process in detail." ></td>
	<td class="line x" title="53:171	2Strictly speaking, this was not a requirement of the approach, but rather a concession to practical considerations." ></td>
	<td class="line x" title="54:171	3.1 Mapping and Backmapping We employ the following strategy for the induction oftherequiredmonolingualgrammar." ></td>
	<td class="line x" title="55:171	First,wetrain the Hiero system in standard fashion on a bilingual English-F training corpus." ></td>
	<td class="line x" title="56:171	Then, for each existing production in the resulting Hiero grammar, we create multiple new English-to-English productions by pivoting on the foreign hierarchical phrase in the rule." ></td>
	<td class="line x" title="57:171	For example, assume that we have the following toy grammar for English-F, as produced by Hiero: X ????e1, f1??" ></td>
	<td class="line x" title="58:171	X ????e3, f1??" ></td>
	<td class="line x" title="59:171	X ????e1, f2??" ></td>
	<td class="line x" title="60:171	X ????e2, f2??" ></td>
	<td class="line x" title="61:171	X ????e4, f2??" ></td>
	<td class="line x" title="62:171	If we use the foreign phrase f1 as a pivot and backmap, we can extract the two English-to-English rules: X ????e1, e3??and X ????e3, e1??" ></td>
	<td class="line x" title="63:171	Backmapping using both f1 and f2 produces the following new rules (ignoring duplicates and rules that map any English phrase to itself): X ????e1, e2??" ></td>
	<td class="line x" title="64:171	X ????e1, e3??" ></td>
	<td class="line x" title="65:171	X ????e1, e4??" ></td>
	<td class="line x" title="66:171	X ????e2, e1??" ></td>
	<td class="line x" title="67:171	X ????e2, e4??" ></td>
	<td class="line x" title="68:171	3.2 Feature values Each rule production in a Hiero grammar is weighted by several feature values defined on the rule themselves." ></td>
	<td class="line x" title="69:171	In order to perform accurate backmapping, we must recompute these feature functions for the newly created English-to-English grammar." ></td>
	<td class="line x" title="70:171	Rather than computing approximations based on feature values already existing in the bilingual Hiero grammar, we calculate these features in a more principled manner, by computing maximum likelihood estimates directly from the fractional counts that Hiero accumulates in the penultimate training step." ></td>
	<td class="line x" title="71:171	We use the following features in our induced English-to-English grammar:3 3Hiero also uses lexical weights (Koehn et al. , 2003) in both 122 ??The joint probability of the two English hierarchical paraphrases, conditioned on the nonterminal symbol, as defined by this formula: p(e1, e2|x) = c(X ???e1, e2??summationtext e1prime, e2prime c(X ???e1prime, e2prime??" ></td>
	<td class="line x" title="72:171	= c(X ???e1, e2??c(X) (4) where the numerator is the fractional count of the rule under consideration and the denominator represents the marginal count over all the English hierarchical phrase pairs." ></td>
	<td class="line x" title="73:171	??The conditionals p(e1,x|e2) and p(e2,x|e1) defined as follows: p(e1,x|e2) = c(X ???e1, e2??summationtext e1prime c(X ???e1prime, e2??" ></td>
	<td class="line x" title="74:171	(5) p(e2,x|e1) = c(X ???e1, e2??summationtext e2prime c(X ???e1, e2prime??" ></td>
	<td class="line x" title="75:171	(6) Finally, for all induced rules, we calculate a word penalty exp(?T(e2)), where T(e2) just counts the number of terminal symbols in e2." ></td>
	<td class="line x" title="76:171	This feature allows the model to learn whether it should produce shorter or longer paraphrases." ></td>
	<td class="line x" title="77:171	Inadditiontothefeaturesabovethatareestimated from the training data, we also use a trigram language model." ></td>
	<td class="line x" title="78:171	Since we are decoding to produce English sentences, we can use the same language model employed in a standard statistical MT setting." ></td>
	<td class="line x" title="79:171	Calculating the proposed features is complicated by the fact that we don?t actually have the counts for English-to-English rules because there is no English-to-English parallel corpus." ></td>
	<td class="line x" title="80:171	This is where the counts provided by Hiero come into the picture." ></td>
	<td class="line x" title="81:171	We estimate the counts that we need as follows: c(X ???e1, e2??" ></td>
	<td class="line x" title="82:171	= summationdisplay f c(X ???e1, f??c(X ???e2, f??" ></td>
	<td class="line x" title="83:171	(7) An intuitive way to think about the formula above is by using an example at the corpus level." ></td>
	<td class="line x" title="84:171	Assume that, in the given bilingual parallel corpus, there are m sentences in which the English phrase directions as features but we don?t use them for our grammar." ></td>
	<td class="line x" title="85:171	e1 co-occurs with the foreign phrase f and n sentences in which the same foreign phrase f co-occurs with the English phrase e2." ></td>
	<td class="line x" title="86:171	The problem can then be thought of as defining a function g(m,n) which computes the number of sentences in a hypothetical English-to-English parallel corpus wherein the phrases e1 and e1 co-occur." ></td>
	<td class="line x" title="87:171	For this paper, we define g(m,n) to be the upper bound mn." ></td>
	<td class="line x" title="88:171	Tables 1 and 2 show some examples of paraphrases generated by our system across a range of paraphrase quality for two different pivot languages." ></td>
	<td class="line x" title="89:171	3.3 Tuning Model Parameters Although the goal of the paraphrasing approach is to make it less data-intensive to tune log-linear model parameters for translation, our paraphrasing approach, since it is based on an English-to-English log-linear model, also requires its own parameter tuning." ></td>
	<td class="line x" title="90:171	This, however, is straightforward: regardless of how the paraphrasing model will be used in statistical MT, e.g., irrespective of source language,itispossibletouseanyexistingsetofEnglish paraphrases as the tuning set for English-to-English translation." ></td>
	<td class="line x" title="91:171	We used the 2002 NIST MT evaluation test set reference translations." ></td>
	<td class="line x" title="92:171	For every item in the set, we randomly chose one sentence as the source sentence, and the remainder as the ?reference translations?forpurposesofminimumerrorratetraining." ></td>
	<td class="line x" title="93:171	4 Evaluation Havingdevelopedaparaphrasingapproachbasedon English-to-English translation, we evaluated its use in improving minimum error rate training for translation from a second language into English." ></td>
	<td class="line x" title="94:171	Generating paraphrases via English-to-English translation makes use of a parallel corpus, from which a weighted synchronous grammar is automatically acquired." ></td>
	<td class="line x" title="95:171	Although nothing about our approachrequiresthattheparaphrasesystem?straining bitext be the same one used in the translation experiments (see Section 6), doing so is not precluded, either, and it is a particularly convenient choice when the paraphrasing is being done in support of MT.4 The training bitext comprised of Chinese-English 4The choice of the foreign language used as the pivot should not really matter but it is worth exploring this using other language pairs as our bitext." ></td>
	<td class="line x" title="96:171	123 O: we must bear in mind the community as a whole." ></td>
	<td class="line x" title="97:171	P: we must remember the wider community . O: thirdly, the implications of enlargement for the union ?s regional policy cannot be overlooked . P: finally, the impact of enlargement for eu regional policy cannot be ignored . O: how this works in practice will become clear when the authority has to act . P: how this operate in practice will emerge when the government has to play . O: this is an ill-advised policy . P: this is an unwelcome in europe . Table 1: Example paraphrases with French as the pivot language." ></td>
	<td class="line x" title="98:171	O = Original Sentence, P = Paraphrase." ></td>
	<td class="line x" title="99:171	O: alcatel added that the company?s whole year earnings would be announced on february 4 . P: alcatel said that the company?s total annual revenues would be released on february 4 . O: he was now preparing a speech concerning the us policy for the upcoming world economic forum . P: he was now ready to talk with regard to the us policies for the forthcoming international economic forum . O: tibet has entered an excellent phase of political stability, ethnic unity and people living in peace . P: tibetans have come to cordial political stability, national unity and lived in harmony . O: its ocean and blue-sky scenery and the mediterranean climate make it world?s famous scenic spot . P: its harbour and blue-sky appearance and the border situation decided it world?s renowned tourist attraction . Table 2: Example paraphrases with Chinese as the pivot language." ></td>
	<td class="line x" title="100:171	O = Original Sentence, P = Paraphrase." ></td>
	<td class="line x" title="101:171	Corpus # Sentences # Words HK News 542540 11171933 FBIS 240996 9121210 Xinhua 54022 1497562 News1 9916 314121 Treebank 3963 125848 Total 851437 22230674 Table 3: Chinese-English corpora used as training bitext both for paraphrasing and for evaluation." ></td>
	<td class="line x" title="102:171	parallelcorporacontaining850,000sentencepairs??" ></td>
	<td class="line x" title="103:171	approx." ></td>
	<td class="line x" title="104:171	22 million words (details shown in Table 3)." ></td>
	<td class="line x" title="105:171	As the source of development data for minimum error rate training, we used the 919 source sentences and human reference translations from the 2003 NIST Chinese-English MT evaluation exercise." ></td>
	<td class="line x" title="106:171	As raw material for experimentation, we generated a paraphrase for each reference sentence via 1-best decoding using the English-to-English translation approach of Section 3." ></td>
	<td class="line x" title="107:171	As our test data, we used the 1082 source sentences and human reference translations from the 2005 NIST Chinese-English MT evaluation." ></td>
	<td class="line x" title="108:171	Our core experiment involved three conditions where the only difference was the set of references for the development set used for tuning feature weights." ></td>
	<td class="line x" title="109:171	For each condition, once the weights were tuned, they were used to decode the test set." ></td>
	<td class="line x" title="110:171	Note that for all the conditions, the decoded test set was alwaysscoredagainstthesamefourhigh-qualityhuman reference translations included with the set." ></td>
	<td class="line x" title="111:171	The three experimental conditions were designed around the constraint that our development set contains a total of four human reference translations per sentence, and therefore a maximum of four human references with which to compute an upper bound: ??Baseline (2H): For each item in the development set, we randomly chose two of the four human-constructed reference translations as references for minimum error rate training." ></td>
	<td class="line x" title="112:171	??Expanded (2H + 2P): For each of the two human references in the baseline tuning set, we automatically generated a corresponding paraphrase using (1-best) English-to-English translation, decoding using the model developed in Section 3." ></td>
	<td class="line x" title="113:171	This condition represents the critical case in which you have a limited number of hu124 man references (two, in this case) and augment themwithartificiallygeneratedreferencetranslations." ></td>
	<td class="line x" title="114:171	This yields a set of four references for minimum error rate training (two human, two paraphrased), which permits a direct comparison against the upper bound of four humangenerated reference translations." ></td>
	<td class="line x" title="115:171	??Upper bound: 4H: We performed minimum error rate training using the four human references from the development set." ></td>
	<td class="line x" title="116:171	In addition to these core experimental conditions, we added a fourth condition to assess the effect on performance when all four human reference translations are used in expanding the reference set via paraphrase: ??Expanded(4H+4P): ThisisthesameasCondition 2, but using all four human references." ></td>
	<td class="line x" title="117:171	Note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references." ></td>
	<td class="line x" title="118:171	Table 4 shows BLEU and TER scores on the test set for all four conditions.5 If only two human references were available (simulated by using only two of the available four), expanding to four using paraphrases would yield a clear improvement." ></td>
	<td class="line x" title="119:171	Using bootstrap resampling to compute confidence intervals (Koehn, 2004), we find that the improvement in BLEU score is statistically significant at p < .01." ></td>
	<td class="line x" title="120:171	Equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing MERT using all four human reference translations." ></td>
	<td class="line x" title="121:171	The difference in BLEU between conditions 2 and 3 is not significant." ></td>
	<td class="line x" title="122:171	Finally, our fourth condition asks whether it is possible to improve MT performance given the typical four human reference translations used for MERT in most statistical MT systems, by adding a paraphrase to each one for a total eight references per translation." ></td>
	<td class="line x" title="123:171	There is indeed further improvement, although the difference in BLEU score does not reach significance." ></td>
	<td class="line o" title="124:171	5We plan to include METEOR scores in future experiments." ></td>
	<td class="line x" title="125:171	Condition References used BLEU TER 1 2 H 30.43 59.82 2 2 H + 2 P 31.10 58.79 3 4 H 31.26 58.66 4 4 H + 4 P 31.68 58.24 Table 4: BLEU and TER scores showing utility of paraphrased reference translations." ></td>
	<td class="line x" title="126:171	H = human references, P = paraphrased references." ></td>
	<td class="line x" title="127:171	We also evaluated our test set using TER (Snover etal.,2006)andobservedthattheTERscoresfollow the same trend as the BLEU scores." ></td>
	<td class="line x" title="129:171	Specifically, the TER scores demonstrate that using paraphrases to artificially expand the reference set is better than using only 2 human reference translations and as good as using 4 human reference translations.6 5 Related Work The approach we have taken here arises from a typical situation in NLP systems: the lack of sufficient data to accurately estimate a model based on supervised training data." ></td>
	<td class="line x" title="130:171	In a structured prediction problem such as MT, we have an example input and a single labeled, correct output." ></td>
	<td class="line x" title="131:171	However, this output is chosen from a space in which the number of possible outputs is exponential in the input size, and in which there are many good outputs in this space (although they are vastly outnumbered by the bad outputs)." ></td>
	<td class="line x" title="132:171	Various discriminative learning methods have attempted to deal with the first of these issues, often by restricting the space of examples." ></td>
	<td class="line x" title="133:171	For instance, some max-margin methods restrict their computations to a set of examples from a ?feasible set,??" ></td>
	<td class="line x" title="134:171	where they are expected to be maximally discriminative (Tillmann and Zhang, 2006)." ></td>
	<td class="line x" title="135:171	The present approach deals with the second issue: in a learning problem where the use of a single positive example is likely to be highly biased, how can we produce a set of positive examples that is more representative of the space of correct outcomes?" ></td>
	<td class="line x" title="136:171	Our method exploits alternative sources of information to produce new positive examples that are, we hope, reasonably likely to represent a consensus of good examples." ></td>
	<td class="line x" title="137:171	Quite a bit of work has been done on paraphrase, 6We anticipate doing significance tests for differences in TER in future work." ></td>
	<td class="line x" title="138:171	125 some clearly related to our technique, although in general previous work has been focused on human readability rather than high coverage, noisy paraphrases for use downstream in an automatic process." ></td>
	<td class="line x" title="139:171	At the sentence level, (Barzilay and Lee, 2003) employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora." ></td>
	<td class="line x" title="140:171	Their technique produces a paraphrase only if the input sentence matches any of the extracted lattice pairs, leading to a bias strongly favoring quality over coverage." ></td>
	<td class="line x" title="141:171	They were able to generate paraphrases for 59 sentences (12%) out of a 484-sentence test set, generating no paraphrases at all for the remainder." ></td>
	<td class="line x" title="142:171	Quirk et al.(2004) also generate sentential paraphrases using a monolingual corpus." ></td>
	<td class="line x" title="144:171	They use IBM Model-1 scores as the only feature, and employ a monotone decoder (i.e. , one that cannot produce phrase-level reordering)." ></td>
	<td class="line x" title="145:171	This approach emphasizes very simple ?substitutions of words and short phrases,??and, in fact, almost a third of their best sentential ?paraphrases??are identical to the input sentence." ></td>
	<td class="line x" title="146:171	A number of other approaches rely on parallel monolingual data and, additionally, require parsing of the training sentences (Ibrahim et al. , 2003; Pang et al. , 2003)." ></td>
	<td class="line x" title="147:171	Lin and Pantel (2001) use a non-parallelcorpusandemployadependencyparser and computation of distributional similarity to learn paraphrases." ></td>
	<td class="line x" title="148:171	There has also been recent work on using paraphrases to improve statistical machine translation." ></td>
	<td class="line x" title="149:171	Callison-Burch et al.(2006) extract phrase-level paraphrases by mapping input phrases into a phrase table and then mapping back to the source language." ></td>
	<td class="line x" title="151:171	However, they do not generate paraphrases of entire sentences,butinsteademployparaphrasestoaddentries to an existing phrase table solely for the purpose of increasing source-language coverage." ></td>
	<td class="line oc" title="152:171	Other work has incorporated paraphrases into MT evaluation: Russo-Lassner et al.(2005) use a combination of paraphrase-based features to evaluate translation output; Zhou et al.(2006) propose a new metric that extends n-gram matching to include synonyms and paraphrases; and Lavie?s METEOR metric (Banerjee and Lavie, 2005) can be used with additionalknowledgesuchasWordNetinordertosupport inexact lexical matches." ></td>
	<td class="line x" title="155:171	6 Conclusions and Future Work We introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and demonstrated that, using this technique, it is possible to cut in half the usual number of reference translations used for minimum error rate training with no significant loss in translation quality." ></td>
	<td class="line x" title="156:171	Our method enables the generation of paraphrases for thousands of sentences in a very short amount of time (much shorter than creating other low-cost human references)." ></td>
	<td class="line x" title="157:171	This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006)." ></td>
	<td class="line x" title="158:171	This has important implications for data acquisition strategies For example, it suggests that rather than obtaining four reference translations per sentence for development sets, it may be more worthwhile to obtain fewer translations for a wider range of sentences, e.g., expanding into new topics and genres." ></td>
	<td class="line x" title="159:171	In addition, this approach can significantly increase the utility of datasets which include only a single reference translation." ></td>
	<td class="line x" title="160:171	A number of future research directions are possible." ></td>
	<td class="line x" title="161:171	First, since we have already demonstrated that noisy paraphrases can nonetheless add value, it would be straightforward to explore the quantity/quality tradeoff by expanding the MERT reference translations with n-best paraphrases for n > 1." ></td>
	<td class="line x" title="162:171	We also plan to conduct an intrinsic evaluation of the quality of paraphrases that our technique generates." ></td>
	<td class="line x" title="163:171	It is important to note that a different tradeoff ratio may lead to even better results, e.g, using only the paraphrased references when they pass some goodness threshold, as used in Ueffing?s (2006) selftraining MT approach." ></td>
	<td class="line x" title="164:171	We have also observed that named entities are usually paraphrased incorrectly if there is a genre mismatchbetweenthetrainingandthetestdata." ></td>
	<td class="line x" title="165:171	The Hiero decoder allows spans of source text to be annotated with inline translations using XML." ></td>
	<td class="line x" title="166:171	We plan to identify and annotate named entities in the English source so that they are left unchanged." ></td>
	<td class="line x" title="167:171	Also,sincethelanguageF forEnglish-F pivoting is arbitrary, we plan to investigate using English-toEnglish grammars created using multiple English-F grammars based on different languages, both indi126 vidually and in combination, in order to improve paraphrase quality." ></td>
	<td class="line x" title="168:171	We also plan to explore a wider range of paraphrase-creation techniques, ranging from simple word substitutions (e.g. , based on WordNet) to usingthepivottechniquewithothertranslationssystems." ></td>
	<td class="line x" title="169:171	7 Acknowledgments We are indebted to David Chiang, Adam Lopez and Smaranda Muresan for insights and comments." ></td>
	<td class="line x" title="170:171	This work has been supported under the GALE program of the Defense Advaned Research Projects Agency, ContractNo.HR0011-06-2-001." ></td>
	<td class="line x" title="171:171	Anyopinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of DARPA." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0718
(Meta-) Evaluation of Machine Translation
Callison-Burch, Chris;Fordyce, Cameron;Koehn, Philipp;Monz, Christof;Schroeder, Josh;"></td>
	<td class="line x" title="1:240	Proceedings of the Second Workshop on Statistical Machine Translation, pages 136??58, Prague, June 2007." ></td>
	<td class="line x" title="2:240	c2007 Association for Computational Linguistics (Meta-) Evaluation of Machine Translation Chris Callison-Burch Johns Hopkins University ccb clsp jhu edu Cameron Fordyce CELCT fordyce celct it Philipp Koehn University of Edinburgh pkoehn inf ed ac uk Christof Monz Queen Mary, University of London christof dcs qmul ac uk Josh Schroeder University of Edinburgh j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back." ></td>
	<td class="line x" title="3:240	We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process." ></td>
	<td class="line x" title="4:240	We measured timing and intraand inter-annotator agreement for three types of subjective evaluation." ></td>
	<td class="line x" title="5:240	We measured the correlationofautomaticevaluationmetricswith human judgments." ></td>
	<td class="line x" title="6:240	This meta-evaluation reveals surprising facts about the most commonly used methodologies." ></td>
	<td class="line x" title="7:240	1 Introduction This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation." ></td>
	<td class="line x" title="8:240	The goals of this paper are twofold: First, we evaluate the shared task entries in order to determine which systems produce translations with the highest quality." ></td>
	<td class="line x" title="9:240	Second, we analyze the evaluation measures themselves in order to try to determine ?best practices??when evaluating machine translation research." ></td>
	<td class="line x" title="10:240	Previous ACL Workshops on Machine Translation were more limited in scope (Koehn and Monz, 2005; Koehn and Monz, 2006)." ></td>
	<td class="line x" title="11:240	The 2005 workshop evaluated translation quality only in terms of Bleu score." ></td>
	<td class="line x" title="12:240	The 2006 workshop additionally included a limited manual evaluation in the style of NIST machine translation evaluation workshop." ></td>
	<td class="line x" title="13:240	Here we apply eleven different automatic evaluation metrics, and conduct three different types of manual evaluation." ></td>
	<td class="line x" title="14:240	Beyond examining the quality of translations produced by various systems, we were interested in examining the following questions about evaluation methodologies: How consistent are people when they judge translation quality?" ></td>
	<td class="line x" title="15:240	To what extent do they agree with other annotators?" ></td>
	<td class="line x" title="16:240	Can we improve human evaluation?" ></td>
	<td class="line x" title="17:240	Which automatic evaluation metrics correlate most strongly with human judgments of translation quality?" ></td>
	<td class="line x" title="18:240	This paper is organized as follows: ??Section 2 gives an overview of the shared task." ></td>
	<td class="line x" title="19:240	It describes the training and test data, reviews the baseline system, and lists the groups that participated in the task." ></td>
	<td class="line x" title="20:240	??Section 3 describes the manual evaluation." ></td>
	<td class="line x" title="21:240	We performed three types of evaluation: scoring with five point scales, relative ranking of translations of sentences, and ranking of translations of phrases." ></td>
	<td class="line x" title="22:240	??Section 4 lists the eleven different automatic evaluation metrics which were also used to score the shared task submissions." ></td>
	<td class="line x" title="23:240	??Section5presentstheresultsofthesharedtask, giving scores for each of the systems in each of the different conditions." ></td>
	<td class="line x" title="24:240	??Section 6 provides an evaluation of the different types of evaluation, giving intraand 136 inter-annotator agreement figures for the manual evaluation, and correlation numbers for the automatic metrics." ></td>
	<td class="line x" title="25:240	2 Shared task overview This year?s shared task changed in some aspects from last year?s: ??We gave preference to the manual evaluation of system output in the ranking of systems." ></td>
	<td class="line x" title="26:240	Manual evaluation was done by the volunteers from participating groups and others." ></td>
	<td class="line x" title="27:240	Additionally, there were three modalities of manual evaluation." ></td>
	<td class="line x" title="28:240	??Automatic metrics were also used to rank the systems." ></td>
	<td class="line x" title="29:240	In total eleven metrics were applied, and their correlation with the manual scores was measured." ></td>
	<td class="line x" title="30:240	??As in 2006, translation was from English, and into English." ></td>
	<td class="line x" title="31:240	English was again paired with German, French, and Spanish." ></td>
	<td class="line x" title="32:240	We additionally included Czech (which was fitting given the location of the WS)." ></td>
	<td class="line x" title="33:240	Similar to the IWSLT International Workshop on Spoken Language Translation (Eck and Hori, 2005; Paul, 2006), and the NIST Machine Translation Evaluation Workshop (Lee, 2006) we provide the shared task participants with a common set of training and test data for all language pairs." ></td>
	<td class="line x" title="34:240	The major part of data comes from current and upcoming full releases of the Europarl data set (Koehn, 2005)." ></td>
	<td class="line x" title="35:240	2.1 Description of the Data The data used in this year?s shared task was similar tothedatausedinlastyear?ssharedtask." ></td>
	<td class="line x" title="36:240	Thisyear?s data included training and development sets for the NewsCommentarydata, whichwasthesurpriseoutof-domain test set last year." ></td>
	<td class="line x" title="37:240	The majority of the training data for the Spanish, French, and German tasks was drawn from a new version of the Europarl multilingual corpus." ></td>
	<td class="line x" title="38:240	Additional training data was taken from the News Commentary corpus." ></td>
	<td class="line x" title="39:240	Czech language resources were drawn from the News Commentary data." ></td>
	<td class="line x" title="40:240	Additional resources for Czech came from the CzEng Parallel Corpus (Bojar and ?Zabokrtsky, 2006)." ></td>
	<td class="line x" title="41:240	Overall, there are over 30 million words of training data per language from the Europarl corpus and 1 million words from the News Commentary corpus." ></td>
	<td class="line x" title="42:240	Figure 1 provides some statistics about the corpora used this year." ></td>
	<td class="line x" title="43:240	2.2 Baseline system To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources." ></td>
	<td class="line x" title="44:240	To summarize, we provided: ??sentence-aligned training corpora ??development and dev-test sets ??language models trained for each language ??an open source decoder for phrase-based SMT called Moses (Koehn et al. , 2006), which replaces the Pharaoh decoder (Koehn, 2004) ??a training script to build models for Moses Theperformanceofthisbaselinesystemissimilar to the best submissions in last year?s shared task." ></td>
	<td class="line x" title="45:240	2.3 Test Data The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data." ></td>
	<td class="line x" title="46:240	Participants were also provided with three sets of parallel text to be used for system development and tuning." ></td>
	<td class="line x" title="47:240	In addition to the Europarl test set, we also collected editorials from the Project Syndicate website1, which are published in all the five languages of the shared task." ></td>
	<td class="line x" title="48:240	We aligned the texts at a sentence level across all five languages, resulting in 2,007 sentences per language." ></td>
	<td class="line x" title="49:240	For statistics on this test set, refer to Figure 1." ></td>
	<td class="line x" title="50:240	The News Commentary test set differs from the Europarl data in various ways." ></td>
	<td class="line x" title="51:240	The text type are editorials instead of speech transcripts." ></td>
	<td class="line x" title="52:240	The domain is general politics, economics and science." ></td>
	<td class="line x" title="53:240	However, it is also mostly political content (even if not focused ontheinternalworkingsoftheEuropeanUnion)and opinion." ></td>
	<td class="line x" title="54:240	2.4 Participants We received submissions from 15 groups from 14 institutions, as listed in Table 1." ></td>
	<td class="line x" title="55:240	This is a slight 1http://www.project-syndicate.com/ 137 Europarl Training corpus Spanish?English French?English German?English Sentences 1,259,914 1,288,901 1,264,825 Foreign words 33,159,337 33,176,243 29,582,157 English words 31,813,692 32,615,285 31,929,435 Distinct foreign words 345,944 344,287 510,544 Distinct English words 266,976 268,718 250,295 News Commentary Training corpus Spanish?English French?English German?English Czech?English Sentences 51,613 43,194 59,975 57797 Foreign words 1,263,067 1,028,672 1,297,673 1,083,122 English words 1,076,273 906,593 1,238,274 1,188,006 Distinct foreign words 84,303 68,214 115,589 142,146 Distinct English words 70,755 63,568 76,419 74,042 Language model data English Spanish French German Sentence 1,407,285 1,431,614 1,435,027 1,478,428 Words 34,539,822 36,426,542 35,595,199 32,356,475 Distinct words 280,546 385,796 361,205 558,377 Europarl test set English Spanish French German Sentences 2,000 Words 53,531 55,380 53,981 49,259 Distinct words 8,558 10,451 10,186 11,106 News Commentary test set English Spanish French German Czech Sentences 2,007 Words 43,767 50,771 49,820 45,075 39,002 Distinct words 10,002 10,948 11,244 12,322 15,245 Figure 1: Properties of the training and test sets used in the shared task." ></td>
	<td class="line x" title="56:240	The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages." ></td>
	<td class="line x" title="57:240	138 ID Participant cmu-uka Carnegie Mellon University, USA (Paulik et al. , 2007) cmu-syntax Carnegie Mellon University, USA (Zollmann et al. , 2007) cu Charles University, Czech Republic (Bojar, 2007) limsi LIMSI-CNRS, France (Schwenk, 2007) liu University of Linkoping, Sweden(Holmqvist et al. , 2007) nrc National Research Council, Canada (Ueffing et al. , 2007) pct a commercial MT provider from the Czech Republic saar Saarland University & DFKI, Germany (Chen et al. , 2007) systran SYSTRAN, France & U. Edinburgh, UK (Dugast et al. , 2007) systran-nrc National Research Council, Canada (Simard et al. , 2007) ucb University of California at Berkeley, USA (Nakov and Hearst, 2007) uedin University of Edinburgh, UK (Koehn and Schroeder, 2007) umd University of Maryland, USA (Dyer, 2007) upc University of Catalonia, Spain (Costa-Juss`a and Fonollosa, 2007) upv University of Valencia, Spain (Civera and Juan, 2007) Table 1: Participants in the shared task." ></td>
	<td class="line x" title="58:240	Not all groups participated in all translation directions." ></td>
	<td class="line x" title="59:240	increase over last year?s shared task where submissions were received from 14 groups from 11 institutions." ></td>
	<td class="line x" title="60:240	Of the 11 groups that participated in last year?s shared task, 6 groups returned this year." ></td>
	<td class="line x" title="61:240	This year, most of these groups follow a phrasebased statistical approach to machine translation." ></td>
	<td class="line x" title="62:240	However, several groups submitted results from systems that followed a hybrid approach." ></td>
	<td class="line x" title="63:240	While building a machine translation system is a serious undertaking we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible." ></td>
	<td class="line x" title="64:240	The creation of parallel corpora suchastheEuroparl,theCzEng,andtheNewsCommentarycorporashouldhelpinthisdirectionbyproviding freely available language resources for buildingsystems." ></td>
	<td class="line x" title="65:240	Thecreationofanopensourcebaseline system should also go a long way towards achieving this goal." ></td>
	<td class="line x" title="66:240	For more on the participating systems, please refer to the respective system description in the proceedings of the workshop." ></td>
	<td class="line x" title="67:240	3 Human evaluation Weevaluatedthesharedtasksubmissionsusingboth manual evaluation and automatic metrics." ></td>
	<td class="line x" title="68:240	While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are an imperfect substitute for human assessment of translation quality." ></td>
	<td class="line x" title="69:240	Manual evaluation is time consuming and expensive to perform, so comprehensive comparisons of multiple systems are rare." ></td>
	<td class="line x" title="70:240	For our manual evaluation we distributed the workload across a number of people, including participants in the shared task, interested volunteers, and a small number of paid annotators." ></td>
	<td class="line x" title="71:240	More than 100 people participated in the manual evaluation, with 75 of those people putting in at least an hour?s worth of effort." ></td>
	<td class="line x" title="72:240	A total of 330 hours of labor was invested, nearly doubling last year?s all-volunteer effort which yielded 180 hours of effort." ></td>
	<td class="line x" title="73:240	Beyond simply ranking the shared task submissions, we had a number of scientific goals for the manual evaluation." ></td>
	<td class="line x" title="74:240	Firstly, we wanted to collect data which could be used to assess how well automatic metrics correlate with human judgments." ></td>
	<td class="line x" title="75:240	Secondly, we wanted to examine different types of manual evaluation and assess which was the best." ></td>
	<td class="line x" title="76:240	A number of criteria could be adopted for choosing among different types of manual evaluation: the ease with which people are able to perform the task, their agreement with other annotators, their reliability when asked to repeat judgments, or the number of judgments which can be collected in a fixed time period." ></td>
	<td class="line x" title="77:240	There are a range of possibilities for how human 139 evaluation of machine translation can be done." ></td>
	<td class="line x" title="78:240	For instance, it can be evaluated with reading comprehension tests (Jones et al. , 2005), or by assigning subjective scores to the translations of individual sentences (LDC, 2005)." ></td>
	<td class="line x" title="79:240	We examined three differentwaysofmanuallyevaluatingmachinetranslation quality: ??Assigning scores based on five point adequacy and fluency scales ??Ranking translated sentences relative to each other ??Ranking the translations of syntactic constituents drawn from the source sentence 3.1 Fluency and adequacy The most widely used methodology when manually evaluatingMTistoassignvaluesfromtwofivepoint scales representing fluency and adequacy." ></td>
	<td class="line x" title="80:240	These scales were developed for the annual NIST Machine Translation Evaluation Workshop by the Linguistics Data Consortium (LDC, 2005)." ></td>
	<td class="line x" title="81:240	The five point scale for adequacy indicates how much of the meaning expressed in the reference translation is also expressed in a hypothesis translation: 5 = All 4 = Most 3 = Much 2 = Little 1 = None The second five point scale indicates how fluent the translation is. When translating into English the values correspond to: 5 = Flawless English 4 = Good English 3 = Non-native English 2 = Disfluent English 1 = Incomprehensible Separate scales for fluency and adequacy were developed under the assumption that a translation might be disfluent but contain all the information from the source." ></td>
	<td class="line x" title="82:240	However, in principle it seems that people have a hard time separating these two aspects of translation." ></td>
	<td class="line x" title="83:240	The high correlation between people?s fluency and adequacy scores (given in Tables 17 and 18) indicate that the distinction might be false." ></td>
	<td class="line x" title="84:240	people 's Iraq to services basic other and, care health, food provide cannot it if occupation its sustain US the Can ?knnenanbietenDienstleistungengrundlegendeandereundGesundheitsfrsorge,NahrungnichtV olk irakischendemsiewenn,USAdieKnnen aufrechterhaltenBesetzung ihre Reference translation NP NP NP VP NP VP S S CNP NP Constituents selected for evaluation Target phrases highlighted via word alignments Parsed source sentence Figure 2: In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems??translations Another problem with the scores is that there are no clear guidelines on how to assign values to translations." ></td>
	<td class="line x" title="85:240	No instructions are given to evaluators in terms of how to quantify meaning, or how many grammatical errors (or what sort) separates the different levels of fluency." ></td>
	<td class="line x" title="86:240	Because of this many judges either develop their own rules of thumb, or use the scales as relative rather than absolute." ></td>
	<td class="line x" title="87:240	These are borne out in our analysis of inter-annotator agreement in Section 6." ></td>
	<td class="line x" title="88:240	3.2 Ranking translations of sentences Because fluency and adequacy were seemingly difficult things for judges to agree on, and because many peoplefromlastyear?sworkshopseemedtobeusing them as a way of ranking translations, we decided to try a separate evaluation where people were simply 140 asked to rank translations." ></td>
	<td class="line x" title="89:240	The instructions for this task were: Rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed)." ></td>
	<td class="line x" title="90:240	These instructions were just as minimal as for fluency and adequacy, but the task was considerably simplified." ></td>
	<td class="line x" title="91:240	Rather than having to assign each translation a value along an arbitrary scale, people simply had to compare different translations of a single sentence and rank them." ></td>
	<td class="line x" title="92:240	3.3 Ranking translations of syntactic constituents In addition to having judges rank the translations of whole sentences, we also conducted a pilot study of a new type of evaluation methodology, which we call constituent-based evaluation." ></td>
	<td class="line x" title="93:240	In our constituent-based evaluation we parsed the source language sentence, selected constituents from the tree, and had people judge the translations of those syntactic phrases." ></td>
	<td class="line x" title="94:240	In order to draw judges??attention to these regions, we highlighted the selected source phrasesandthecorrespondingphrasesinthetranslations." ></td>
	<td class="line x" title="95:240	The corresponding phrases in the translations were located via automatic word alignments." ></td>
	<td class="line x" title="96:240	Figure 2 illustrates the constituent based evaluation when applied to a German source sentence." ></td>
	<td class="line x" title="97:240	The German source sentence is parsed, and various phrases are selected for evaluation." ></td>
	<td class="line x" title="98:240	Word alignments are created between the source sentence and the reference translation (shown), and the source sentence and each of the system translations (not shown)." ></td>
	<td class="line x" title="99:240	We parsed the test sentences for each of the languages aside from Czech." ></td>
	<td class="line x" title="100:240	We used Cowan and Collins (2005)?s parser for Spanish, Arun and Keller (2005)?s for French, Dubey (2005)?s for German, and Bikel (2002)?s for English." ></td>
	<td class="line x" title="101:240	The word alignments were created with Giza++ (Och and Ney, 2003) applied to a parallel corpus containing 200,000 sentence pairs of the training data, plus sets of 4,007 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations." ></td>
	<td class="line x" title="102:240	The phrases in the translations were located using techniques from phrase-based statistical machine translation which extract phrase pairsfromwordalignments(Koehnetal., 2003; Och and Ney, 2004)." ></td>
	<td class="line x" title="104:240	Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase." ></td>
	<td class="line x" title="105:240	We noted this in the instructions to judges: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed)." ></td>
	<td class="line x" title="106:240	Grade only the highlighted part of each translation." ></td>
	<td class="line x" title="107:240	Please note that segments are selected automatically, and they should be taken as an approximate guide." ></td>
	<td class="line x" title="108:240	They might includeextrawordsthatarenotintheactual alignment, or miss words on either end." ></td>
	<td class="line x" title="109:240	The criteria that we used to select which constituents were to be evaluated were: ??The constituent could not be the whole source sentence ??The constituent had to be longer three words, and be no longer than 15 words ??The constituent had to have a corresponding phrase with a consistent word alignment in each of the translations The final criterion helped reduce the number of alignment errors." ></td>
	<td class="line x" title="110:240	3.4 Collecting judgments We collected judgments using a web-based tool." ></td>
	<td class="line x" title="111:240	Shared task participants were each asked to judge 200 sets of sentences." ></td>
	<td class="line x" title="112:240	The sets consisted of 5 system outputs, as shown in Figure 3." ></td>
	<td class="line x" title="113:240	The judges were presented with batches of each type of evaluation." ></td>
	<td class="line x" title="114:240	We presented them with five screens of adequacy/fluency scores, five screens of sentence rankings, and ten screens of constituent rankings." ></td>
	<td class="line x" title="115:240	The order of the types of evaluation were randomized." ></td>
	<td class="line x" title="116:240	In order to measure intra-annotator agreement 10% of the items were repeated and evaluated twice by each judge." ></td>
	<td class="line x" title="117:240	In order to measure inter-annotator agreement 40% of the items were randomly drawn from a common pool that was shared across all 141 http://www.statmt.org/wmt07/shared-task/judge/do_task.php WMT07 Manual Evaluation Rank Segments You have judged 25 sentences for WMT07 German-English News Corpus, 190 sentences total taking 64.9 seconds per sentence." ></td>
	<td class="line x" title="118:240	Source: Knnen die USA ihre Besetzung aufrechterhalten, wenn sie dem irakischen Volk nicht Nahrung, Gesundheitsfrsorge und andere grundlegende Dienstleistungen anbieten knnen?" ></td>
	<td class="line x" title="119:240	Reference: Can the US sustain its occupation if it cannot provide food, health care, and other basic services to Iraq's people?" ></td>
	<td class="line x" title="120:240	Translation Rank The United States can maintain its employment when it the Iraqi people not food, health care and other basic services on offer?." ></td>
	<td class="line x" title="121:240	1 Worst 2 3 4 5 Best The US can maintain its occupation, if they cannot offer the Iraqi people food, health care and other basic services?" ></td>
	<td class="line x" title="122:240	1 Worst 2 3 4 5 Best Can the US their occupation sustained if it to the Iraqi people not food, health care and other basic services can offer?" ></td>
	<td class="line x" title="123:240	1 Worst 2 3 4 5 Best Can the United States maintain their occupation, if the Iraqi people do not food, health care and other basic services can offer?" ></td>
	<td class="line x" title="124:240	1 Worst 2 3 4 5 Best The United States is maintained, if the Iraqi people, not food, health care and other basic services can offer?" ></td>
	<td class="line x" title="125:240	1 Worst 2 3 4 5 Best Annotator: ccb Task: WMT07 German-English News Corpus Instructions: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed)." ></td>
	<td class="line x" title="126:240	Grade only the highlighted part of each translation." ></td>
	<td class="line x" title="127:240	Please note that segments are selected automatically, and they should be taken as an approximate guide." ></td>
	<td class="line x" title="128:240	They might include extra words on either end that are not in the actual alignment, or miss words." ></td>
	<td class="line x" title="129:240	Figure 3: For each of the types of evaluation, judges were shown screens containing up to five different system translations, along with the source sentence and reference translation." ></td>
	<td class="line x" title="130:240	annotators so that we would have items that were judged by multiple annotators." ></td>
	<td class="line x" title="131:240	Judges were allowed to select whichever data set they wanted, and to evaluate translations into whatever languages they were proficient in." ></td>
	<td class="line x" title="132:240	Shared task participants were excluded from judging their own systems." ></td>
	<td class="line x" title="133:240	Table 2 gives a summary of the number of judgments that we collected for translations of individual sentences." ></td>
	<td class="line x" title="134:240	Since we had 14 translation tasks and four different types of scores, there were 55 different conditions.2 In total we collected over 81,000 judgments." ></td>
	<td class="line x" title="135:240	Despite the large number of conditions we managed to collect more than 1,000 judgments for most of them." ></td>
	<td class="line x" title="136:240	This provides a rich source of data for analyzing the quality of translations produced by different systems, the different types of human evaluation, and the correlation of automatic metrics with human judgments.3 2We did not perform a constituent-based evaluation for Czech to English because we did not have a syntactic parser for Czech." ></td>
	<td class="line x" title="137:240	We considered adapting our method to use Bojar (2004)?sdependencyparserforCzech,butdidnothavethetime." ></td>
	<td class="line x" title="138:240	3The judgment data along with all system translations are available at http://www.statmt.org/wmt07/ 4 Automatic evaluation The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality." ></td>
	<td class="line x" title="139:240	Bleu was used exclusively since it is the most widely used metric in the field and has been shown to correlate with human judgments of translation quality in many instances (Doddington, 2002; Coughlin, 2003; Przybocki, 2004)." ></td>
	<td class="line x" title="140:240	However, recent work suggests that Bleu?s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al. , 2006)." ></td>
	<td class="line x" title="141:240	The results of last year?s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)." ></td>
	<td class="line x" title="142:240	We used the manual evaluation data as a means of testing the correlation of a range of automatic metrics in addition to Bleu." ></td>
	<td class="line x" title="143:240	In total we used eleven different automatic evaluation measures to rank the shared task submissions." ></td>
	<td class="line oc" title="144:240	They are: ??Meteor (Banerjee and Lavie, 2005)?Meteor measures precision and recall of unigrams when comparing a hypothesis translation 142 Language Pair Test Set Adequacy Fluency Rank Constituent English-German Europarl 1,416 1,418 1,419 2,626 News Commentary 1,412 1,413 1,412 2,755 German-English Europarl 1,525 1,521 1,514 2,999 News Commentary 1,626 1,620 1,601 3,084 English-Spanish Europarl 1,000 1,003 1,064 1,001 News Commentary 1,272 1,272 1,238 1,595 Spanish-English Europarl 1,174 1,175 1,224 1,898 News Commentary 947 949 922 1,339 English-French Europarl 773 772 769 1,456 News Commentary 729 735 728 1,313 French-English Europarl 834 833 830 1,641 News Commentary 1,041 1,045 1,035 2,036 English-Czech News Commentary 2,303 2,304 2,331 3,968 Czech-English News Commentary 1,711 1,711 1,733 0 Totals 17,763 17,771 17,820 27,711 Table 2: The number of items that were judged for each task during the manual evaluation against a reference." ></td>
	<td class="line p" title="145:240	It flexibly matches words using stemming and WordNet synonyms." ></td>
	<td class="line o" title="146:240	Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007)." ></td>
	<td class="line x" title="147:240	??Bleu (Papineni et al. , 2002)?Bleu is currently the de facto standard in machine translation evaluation." ></td>
	<td class="line x" title="148:240	It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation." ></td>
	<td class="line x" title="149:240	We use a single reference translation in our experiments." ></td>
	<td class="line x" title="150:240	??GTM (Melamed et al. , 2003)?GTM generalizes precision, recall, and F-measure to measure overlap between strings, rather than overlap between bags of items." ></td>
	<td class="line x" title="151:240	An ?exponent??parameter which controls the relative importance of word order." ></td>
	<td class="line x" title="152:240	A value of 1.0 reduces GTM to ordinary unigram overlap, with higher values emphasizing order.4 ??Translation Error Rate (Snover et al. , 2006)??" ></td>
	<td class="line x" title="153:240	4The GTM scores presented here are an F-measure with a weight of 0.1, which counts recall at 10x the level of precision." ></td>
	<td class="line x" title="154:240	The exponent is set at 1.2, which puts a mild preference towards items with words in the correct order." ></td>
	<td class="line x" title="155:240	These parameters could be optimized empirically for better results." ></td>
	<td class="line x" title="156:240	TER calculates the number of edits required to change a hypothesis translation into a reference translation." ></td>
	<td class="line x" title="157:240	The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words." ></td>
	<td class="line x" title="158:240	??ParaEval precision and ParaEval recall (Zhou et al. , 2006)?ParaEval matches hypothesis and reference translations using paraphrases that are extracted from parallel corpora in an unsupervised fashion (Bannard and Callison-Burch, 2005)." ></td>
	<td class="line x" title="159:240	It calculates precision and recall using a unigram counting strategy." ></td>
	<td class="line x" title="160:240	??Dependency overlap (Amigo et al. , 2006)??" ></td>
	<td class="line x" title="161:240	This metric uses dependency trees for the hypothesis and reference translations, by computing the average overlap between words in the two trees which are dominated by grammatical relationships of the same type." ></td>
	<td class="line x" title="162:240	??Semantic role overlap (Gimenez and M`arquez, 2007)?This metric calculates the lexical overlap between semantic roles (i.e. , semantic arguments or adjuncts) of the same type in the the hypothesis and reference translations." ></td>
	<td class="line x" title="163:240	It uniformly averages lexical overlap over all semantic role types." ></td>
	<td class="line x" title="164:240	143 ??Word Error Rate over verbs (Popovic and Ney, 2007)?WER??creates a new reference and a new hypothesis for each POS class by extracting all words belonging to this class, and then tocalculatethestandardWER.Weshowresults for this metric over verbs." ></td>
	<td class="line o" title="165:240	??Maximumcorrelationtrainingonadequacyand on fluency (Liu and Gildea, 2007)?a linear combination of different evaluation metrics (Bleu, Meteor, Rouge, WER, and stochastic iterative alignment) with weights set to maximize Pearson?s correlation with adequacy and fluency judgments." ></td>
	<td class="line x" title="166:240	Weights were trained on WMT-06 data." ></td>
	<td class="line x" title="167:240	The scores produced by these are given in the tables at the end of the paper, and described in Section 5." ></td>
	<td class="line x" title="168:240	We measured the correlation of the automatic evaluation metrics with the different types of human judgments on 12 data conditions, and report these in Section 6." ></td>
	<td class="line x" title="169:240	5 Shared task results The results of the human evaluation are given in Tables 9, 10, 11 and 12." ></td>
	<td class="line x" title="170:240	Each of those tables present four scores: ??FLUENCY and ADEQUACY are normalized versions of the five point scores described in Section 3.1." ></td>
	<td class="line x" title="171:240	The tables report an average of the normalized scores.5 ??RANK is the average number of times that a system was judged to be better than any other system in the sentence ranking evaluation described in Section 3.2." ></td>
	<td class="line x" title="172:240	??CONSTITUENT is the average number of times that a system was judged to be better than any other system in the constituent-based evaluation described in Section 3.3." ></td>
	<td class="line x" title="173:240	There was reasonably strong agreement between these four measures at which of the entries was the best in each data condition." ></td>
	<td class="line x" title="174:240	There was complete 5Since different annotators can vary widely in how they assign fluency and adequacy scores, we normalized these scores on a per-judge basis using the method suggested by Blatz et al.(2003) in Chapter 5, page 97." ></td>
	<td class="line x" title="176:240	SYSTRAN (systran) 32% University of Edinburgh (uedin) 20% University of Catalonia (upc) 15% LIMSI-CNRS (limsi) 13% University of Maryland (umd) 5% National Research Council of Canada?s joint entry with SYSTRAN (systran-nrc) 5% Commercial Czech-English system (pct) 5% University of Valencia (upv) 2% Charles University (cu) 2% Table 3: The proportion of time that participants??" ></td>
	<td class="line x" title="177:240	entries were top-ranked in the human evaluation University of Edinburgh (uedin) 41% University of Catalonia (upc) 12% LIMSI-CNRS (limsi) 12% University of Maryland (umd) 9% Charles University (cu) 4% Carnegie Mellon University (cmu-syntax) 4% Carnegie Mellon University (cmu-uka) 4% University of California at Berkeley (ucb) 3% National Research Council?s joint entry with SYSTRAN (systran-nrc) 2% SYSTRAN (systran) 2% Saarland University (saar) 0.8% Table 4: The proportion of time that participants??" ></td>
	<td class="line x" title="178:240	entries were top-ranked by the automatic evaluation metrics agreement between them in 5 of the 14 conditions, and agreement between at least three of them in 10 of the 14 cases." ></td>
	<td class="line x" title="179:240	Table 3 gives a summary of how often different participants??entries were ranked #1 by any of the four human evaluation measures." ></td>
	<td class="line x" title="180:240	SYSTRAN?s entries were ranked the best most often, followed by University of Edinburgh, University of Catalonia and LIMSI-CNRS." ></td>
	<td class="line x" title="181:240	The following systems were the best performing for the different language pairs: SYSTRAN was ranked the highest in German-English, University of Catalonia was ranked the highest in Spanish-English, LIMSI-CNRS was ranked highest in French-English, and the University of Maryland and a commercial system were the highest for 144 Evaluation type P(A) P(E) K Fluency (absolute).400 .2 .250 Adequacy (absolute) .380 .2 .226 Fluency (relative) .520 .333 .281 Adequacy (relative) .538 .333 .307 Sentence ranking .582 .333 .373 Constituent ranking .693 .333 .540 Constituent ranking .712 .333 .566 (w/identical constituents) Table 5: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation Czech-English." ></td>
	<td class="line x" title="182:240	While we consider the human evaluation to be primary, it is also interesting to see how the entries were ranked by the various automatic evaluationmetrics." ></td>
	<td class="line x" title="183:240	Thecompletesetofresultsfortheautomatic evaluation are presented in Tables 13, 14, 15, and 16." ></td>
	<td class="line x" title="184:240	An aggregate summary is provided in Table 4." ></td>
	<td class="line x" title="185:240	The automatic evaluation metrics strongly favor the University of Edinburgh, which garners 41% of the top-ranked entries (which is partially due to the fact it was entered in every language pair)." ></td>
	<td class="line x" title="186:240	Significantly, the automatic metrics disprefer SYSTRAN, whichwasstronglyfavoredinthehumanevaluation." ></td>
	<td class="line x" title="187:240	6 Meta-evaluation In addition to evaluating the translation quality of the shared task entries, we also performed a ?metaevaluation??of our evaluation methodologies." ></td>
	<td class="line x" title="188:240	6.1 Interand Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient(K)whichiswidelyused in computational linguistics for measuring agreement in category judgments (Carletta, 1996)." ></td>
	<td class="line x" title="189:240	It is defined as K = P(A)?P(E)1?P(E) where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance." ></td>
	<td class="line x" title="190:240	We define chance agreement for fluency and adequacy as 15, since they are based on five point scales, and for ranking as 13 Evaluation type P(A) P(E) K Fluency (absolute) .630 .2 .537 Adequacy (absolute) .574 .2 .468 Fluency (relative) .690 .333 .535 Adequacy (relative) .696 .333 .544 Sentence ranking .749 .333 .623 Constituent ranking .825 .333 .738 Constituent ranking .842 .333 .762 (w/identical constituents) Table 6: Kappa coefficient values for intra-annotator agreement for the different types of manual evaluation since there are three possible out comes when ranking the output of a pair of systems: A > B, A = B, A < B. For inter-annotator agreement we calculated P(A) for fluency and adequacy by examining all items that were annotated by two or more annotators, and calculating the proportion of time they assigned identical scores to the same items." ></td>
	<td class="line x" title="191:240	For the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time that they agreed that A > B, A = B, or A < B. For intra-annotator agreement we did similarly, but gathered items that were annotated on multiple occasions by a single annotator." ></td>
	<td class="line x" title="192:240	Table 5 gives K values for inter-annotator agreement, and Table 6 gives K values for intra-annoator agreement." ></td>
	<td class="line x" title="193:240	These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively." ></td>
	<td class="line x" title="194:240	The interpretation of Kappa varies, but according to Landis and Koch (1977) 0?.2 is slight, .21?.4 is fair,.41?.6 is moderate,.61?.8 is substantial and the rest almost perfect." ></td>
	<td class="line x" title="195:240	The K values for fluency and adequacy should give us pause about using these metrics in the future." ></td>
	<td class="line x" title="196:240	Whenweanalyzedthemastheyareintendedto be?scores classifying the translations of sentences into different types?the inter-annotator agreement was barely considered fair, and the intra-annotator agreement was only moderate." ></td>
	<td class="line x" title="197:240	Even when we reassessed fluency and adequacy as relative ranks the agreements increased only minimally." ></td>
	<td class="line x" title="198:240	145 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0 10 20 30 40 50 60 num sentences taking this long (%) time to judge one sentence (seconds) constituent rank sentence rank fluency+adequacy scoring Figure 4: Distributions of the amount of time it took to judge single sentences for the three types of manual evaluation The agreement on the other two types of manual evaluation that we introduced were considerably better." ></td>
	<td class="line x" title="199:240	Theboththesentenceandconstituentranking had moderate inter-annotator agreement and substantialintra-annotatoragreement." ></td>
	<td class="line x" title="200:240	Becausetheconstituent ranking examined the translations of short phrases, often times all systems produced the same translations." ></td>
	<td class="line x" title="201:240	Since these trivially increased agreement (since they would always be equally ranked) we also evaluated the interand intra-annotator agreement when those items were excluded." ></td>
	<td class="line x" title="202:240	The agreement remained very high for constituent-based evaluation." ></td>
	<td class="line x" title="203:240	6.2 Timing We used the web interface to collect timing information." ></td>
	<td class="line x" title="204:240	The server recorded the time when a set of sentences was given to a judge and the time when the judge returned the sentences." ></td>
	<td class="line x" title="205:240	We divided the time that it took to do a set by the number of sentences in the set." ></td>
	<td class="line x" title="206:240	The average amount of time that it took to assign fluency and adequacy to a single sentence was 26 seconds.6 The average amount of time it took to rank a sentence in a set was 20 seconds." ></td>
	<td class="line x" title="207:240	The average amount of time it took to rank a highlighted constituent was 11 seconds." ></td>
	<td class="line x" title="208:240	Figure 4 shows the distribution of times for these tasks." ></td>
	<td class="line x" title="209:240	6Sets which took longer than 5 minutes were excluded from these calculations, because there was a strong chance that annotators were interrupted while completing the task." ></td>
	<td class="line x" title="210:240	These timing figures are promising because they indicate that the tasks which the annotators were the most reliable on (constituent ranking and sentence ranking) were also much quicker to complete than the ones that they were unreliable on (assigning fluency and adequacy scores)." ></td>
	<td class="line x" title="211:240	This suggests that fluency and adequacy should be replaced with ranking tasks in future evaluation exercises." ></td>
	<td class="line x" title="212:240	6.3 Correlation between automatic metrics and human judgments To measure the correlation of the automatic metrics with the human judgments of translation quality we used Spearman?s rank correlation coefficient ?." ></td>
	<td class="line x" title="213:240	We opted for Spearman rather than Pearson because it makes fewer assumptions about the data." ></td>
	<td class="line x" title="214:240	Importantly, it can be applied to ordinal data (such as the fluency and adequacy scales)." ></td>
	<td class="line x" title="215:240	Spearman?s rank correlation coefficient is equivalent to Pearson correlation on ranks." ></td>
	<td class="line x" title="216:240	Aftertherawscoresthatwereassignedtosystems by an automatic metric and by one of our manual evaluation techniques have been converted to ranks, we can calculate ? using the simplified equation: ? = 1??6 summationtextd2 i n(n2 ??)" ></td>
	<td class="line x" title="217:240	where di is the difference between the rank for systemi and n is the number of systems." ></td>
	<td class="line x" title="218:240	The possible values of?range between 1 (where all systems arerankedinthesameorder)and??(wherethesystems are ranked in the reverse order)." ></td>
	<td class="line x" title="219:240	Thus an automatic evaluation metric with a higher value for ? is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower ?." ></td>
	<td class="line x" title="220:240	Table 17 reports ? for the metrics which were used to evaluate translations into English.7." ></td>
	<td class="line x" title="221:240	Table 7 summarizes the results by averaging the correlation numbers by equally weighting each of the data conditions." ></td>
	<td class="line x" title="222:240	The table ranks the automatic evaluation metrics based on how well they correlated with human judgments." ></td>
	<td class="line oc" title="223:240	While these are based on a relatively few number of items, and while we have not performed any tests to determine whether the differences in ? are statistically significant, the results 7The Czech-English conditions were excluded since there were so few systems 146 are nevertheless interesting, since three metrics have higher correlation than Bleu: ??Semantic role overlap (Gimenez and M`arquez, 2007), which makes its debut in the proceedings of this workshop ??ParaEval measuring recall (Zhou et al. , 2006), which has a model of allowable variation in translation that uses automatically generated paraphrases (Callison-Burch, 2007) ??Meteor (Banerjee and Lavie, 2005) which also allows variation by introducing synonyms and by flexibly matches words using stemming." ></td>
	<td class="line x" title="224:240	Tables 18 and 8 report ? for the six metrics which were used to evaluate translations into the other languages." ></td>
	<td class="line x" title="225:240	Here we find that Bleu and TER are the closest to human judgments, but that overall the correlations are much lower than for translations into English." ></td>
	<td class="line x" title="226:240	7 Conclusions Similar to last year?s workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from four European languages into English, and vice versa." ></td>
	<td class="line x" title="227:240	This year we substantially increased the number of automatic evaluation metrics and were also able to nearly double the efforts of producing the human judgments." ></td>
	<td class="line x" title="228:240	There were substantial differences in the results results of the human and automatic evaluations." ></td>
	<td class="line x" title="229:240	We take the human judgments to be authoritative, and used them to evaluate the automatic metrics." ></td>
	<td class="line x" title="230:240	We measured correlation using Spearman?s coefficient and found that three less frequently used metrics were stronger predictors of human judgments than Bleu." ></td>
	<td class="line o" title="231:240	They were: semantic role overlap (newly introduced in this workshop) ParaEval-recall and Meteor." ></td>
	<td class="line x" title="232:240	Although we do not claim that our observations are indisputably conclusive, they again indicate that the choice of automatic metric can have a significant impact on comparing systems." ></td>
	<td class="line x" title="233:240	Understanding the exact causes of those differences still remains an important issue for future research." ></td>
	<td class="line o" title="234:240	metric AD EQ UA CY FL UE NC Y RA NK CO NS TI TU EN TO VE RA LL Semantic role overlap .774 .839 .803 .741 .789 ParaEvalRecall .712 .742 .768 .798 .755 Meteor .701 .719 .745 .669 .709 Bleu .690 .722 .672 .602 .671 1-TER .607 .538 .520 .514 .644 Max adequcorrelation .651 .657 .659 .534 .626 Max fluency correlation .644 .653 .656 .512 .616 GTM .655 .674 .616 .495 .610 Dependency overlap .639 .644 .601 .512 .599 ParaEvalPrecision .639 .654 .610 .491 .598 1-WER of verbs .378 .422 .431 .297 .382 Table 7: Average corrections for the different automatic metrics when they are used to evaluate translations into English metric AD EQ UA CY FL UE NC Y RA NK CO NS TI TU EN TO VE RA LL Bleu .657 .445 .352 .409 .466 1-TER .589 .419 .361 .380 .437 Max fluency correlation .534 .419 .368 .400 .430 Max adequcorrelation .498 .414 .385 .409 .426 Meteor .490 .356 .279 .304 .357 1-WER of verbs .371 .304 .359 .359 .348 Table 8: Average corrections for the different automatic metrics when they are used to evaluate translations into the other languages 147 This year?s evaluation also measured the agreement between human assessors by computing the Kappa coefficient." ></td>
	<td class="line x" title="235:240	One striking observation is that inter-annotator agreement for fluency and adequacy can be called ?fair??at best." ></td>
	<td class="line x" title="236:240	On the other hand, comparing systems by ranking them manually (constituents or entire sentences), resulted in much higher inter-annotator agreement." ></td>
	<td class="line x" title="237:240	Acknowledgments This work was supported in part by the EuroMatrix project funded by the European Commission (6th Framework Programme), and in part by the GALE program of the US Defense Advanced Research Projects Agency, Contract No." ></td>
	<td class="line x" title="238:240	HR0011-06C-0022." ></td>
	<td class="line x" title="239:240	We are grateful to Jesus Gimenez, Dan Melamed, Maja Popvic, Ding Liu, Liang Zhou, and Abhaya Agarwal for scoring the entries with their automatic evaluation metrics." ></td>
	<td class="line x" title="240:240	Thanks to Brooke Cowan for parsing the Spanish test sentences, to Josh Albrecht for his script for normalizing fluency and adequacy on a per judge basis, and to Dan Melamed, Rebecca Hwa, AlonLavie, ColinBannardandMirellaLapata for their advice about statistical tests." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0719
Context-aware Discriminative Phrase Selection for Statistical Machine Translation
Giménez, Jesús;Màrquez, Lluís;"></td>
	<td class="line x" title="1:244	Proceedings of the Second Workshop on Statistical Machine Translation, pages 159??66, Prague, June 2007." ></td>
	<td class="line x" title="2:244	c2007 Association for Computational Linguistics Context-aware Discriminative Phrase Selection for Statistical Machine Translation Jesus Gimenez and Llus M`arquez TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya Jordi Girona Salgado 1??, E-08034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract In this work we revise the application of discriminative learning to the problem of phrase selection in Statistical Machine Translation." ></td>
	<td class="line x" title="3:244	Inspired by common techniques used in Word Sense Disambiguation, we train classifiers based on local context to predict possible phrase translations." ></td>
	<td class="line x" title="4:244	Our work extends that of Vickrey et al.(2005) in two main aspects." ></td>
	<td class="line x" title="6:244	First, we move from word translation to phrase translation." ></td>
	<td class="line x" title="7:244	Second, we move from the ?blank-filling??task to the ?full translation??task." ></td>
	<td class="line x" title="8:244	We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation." ></td>
	<td class="line x" title="9:244	1 Introduction Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al. , 2003)." ></td>
	<td class="line x" title="10:244	In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context." ></td>
	<td class="line x" title="11:244	Following the approach by Vickrey et al.(2005), we deal with the ?phrase translation??problem as a classification problem." ></td>
	<td class="line x" title="13:244	We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence." ></td>
	<td class="line x" title="14:244	We extend the work by Vickrey et al.(2005) in two main aspects." ></td>
	<td class="line x" title="16:244	First, we move from ?word translation??to ?phrase translation??" ></td>
	<td class="line x" title="17:244	Second, we move from the ?blank-filling??task to the ?full translation??task." ></td>
	<td class="line x" title="18:244	Our approach is fully described in Section 2." ></td>
	<td class="line x" title="19:244	We apply it to the Spanish-to-English translation of European Parliament Proceedings." ></td>
	<td class="line x" title="20:244	In Section 3, prior to considering the ?full translation??task, we analyze the impact of using DPT models for the isolated ?phrase translation??task." ></td>
	<td class="line x" title="21:244	In spite of working on a very specific domain, a large room for improvement, coherent with WSD performance, and results by Vickrey et al.(2005), is predicted." ></td>
	<td class="line x" title="23:244	Then, in Section 4, we tackle the full translation task." ></td>
	<td class="line x" title="24:244	DPT models are integrated in a ?soft??manner, by making them available to the decoder so they can fully interact with other models." ></td>
	<td class="line x" title="25:244	Results using a reduced set of highly frequent source phrases show a significant improvement, according to several automatic evaluation metrics." ></td>
	<td class="line x" title="26:244	Interestingly, the BLEU metric (Papineni et al. , 2001) is not able to reflect this improvement." ></td>
	<td class="line x" title="27:244	Through a rigorous process of manual evaluation we have verified the gain." ></td>
	<td class="line x" title="28:244	We have also observed that it is mainly related to adequacy." ></td>
	<td class="line x" title="29:244	These results confirm that better phrase translation probabilities may be helpful for the full translation task." ></td>
	<td class="line x" title="30:244	However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study." ></td>
	<td class="line x" title="31:244	2 Discriminative Phrase Translation In this section we describe the phrase-based SMT baseline system and how DPT models are built and integrated into this system in a ?soft??manner." ></td>
	<td class="line x" title="32:244	159 2.1 Baseline System The baseline system is a phrase-based SMT system (Koehn et al. , 2003), built almost entirely using freely available components." ></td>
	<td class="line x" title="33:244	We use the SRI Language Modeling Toolkit (Stolcke, 2002) for language modeling." ></td>
	<td class="line x" title="34:244	We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing." ></td>
	<td class="line x" title="35:244	Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e. , lemma, part-of-speech, and base phrase chunks) as described by Gimenez and M`arquez (2005)." ></td>
	<td class="line x" title="36:244	Text is automatically annotated, using the SVMTool (Gimenez and M`arquez, 2004), Freeling (Carreras et al. , 2004), and Phreco (Carreras et al. , 2005) packages." ></td>
	<td class="line x" title="37:244	We used the GIZA++ SMT Toolkit1 (Och and Ney, 2003) to generate word alignments." ></td>
	<td class="line x" title="38:244	We apply the phrase-extract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++ following the ?global phrase extraction??" ></td>
	<td class="line x" title="39:244	strategy described by Gimenez and M`arquez (2005) (i.e. , a single phrase translation table is built on top of the union of alignments corresponding to different linguistic data views)." ></td>
	<td class="line x" title="40:244	We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement." ></td>
	<td class="line x" title="41:244	Phrases up to length five are considered." ></td>
	<td class="line x" title="42:244	Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase is more than three times longer than the target/source phrase are ignored." ></td>
	<td class="line x" title="43:244	Phrase pairs are scored on the basis of unsmoothed relative frequency (i.e. , MLE)." ></td>
	<td class="line x" title="44:244	Regarding the argmax search, we used the Pharaoh beam search decoder (Koehn, 2004), which naturally fits with the previous tools." ></td>
	<td class="line x" title="45:244	2.2 DPT for SMT Instead of relying on MLE estimation to score the phrase pairs (fi,ej) in the translation table, we suggest considering the translation of every source phrase fi as a multi-class classification problem, where every possible translation of fi is a class." ></td>
	<td class="line x" title="46:244	We use local linear SVMs 2." ></td>
	<td class="line x" title="47:244	Since SVMs are binary classifiers, the problem must be binarized." ></td>
	<td class="line x" title="48:244	We 1http://www.fjoch.com/GIZA++.html 2We use the SVMlight package, which is freely available at http://svmlight.joachims.org(Joachims, 1999)." ></td>
	<td class="line x" title="49:244	have applied a simple one-vs-all binarization, i.e., a SVM is trained for every possible translation candidate ej." ></td>
	<td class="line x" title="50:244	Training examples are extracted from the same training data as in the case of MLE models, i.e., an aligned parallel corpus, obtained as described in Section 2.1." ></td>
	<td class="line x" title="51:244	We use each sentence pair in which the source phrase fi occurs to generate a positive example for the classifier corresponding to the actual translation of fi in that sentence, according to the automatic alignment." ></td>
	<td class="line x" title="52:244	This will be as well a negative example for the classifiers corresponding to the rest of possible translations of fi." ></td>
	<td class="line x" title="53:244	2.2.1 Feature Set We consider different kinds of information, always from the source sentence, based on standard WSD methods (Yarowsky et al. , 2001)." ></td>
	<td class="line x" title="54:244	As to the local context, inside the source phrase to disambiguate, and 5 tokens to the left and to the right, we use n-grams (n ??{1,2,3}) of: words, partsof-speech, lemmas and base phrase chunking IOB labels." ></td>
	<td class="line x" title="55:244	As to the global context, we collect topical information by considering the source sentence as a bag of lemmas." ></td>
	<td class="line x" title="56:244	2.2.2 Decoding." ></td>
	<td class="line x" title="57:244	A Trick." ></td>
	<td class="line x" title="58:244	At translation time, we consider every instance of fi as a separate case." ></td>
	<td class="line x" title="59:244	In each case, for all possible translations of fi, we collect the SVM score, according to the SVM classification rule." ></td>
	<td class="line x" title="60:244	We are in fact modeling P(ej|fi)." ></td>
	<td class="line x" title="61:244	However, these scores are not probabilities." ></td>
	<td class="line x" title="62:244	We transform them into probabilities by applying the softmax function described by Bishop (1995)." ></td>
	<td class="line x" title="63:244	We do not constrain the decoder to use the translation ej with highest probability." ></td>
	<td class="line x" title="64:244	Instead, we make all predictions available and let the decoder choose." ></td>
	<td class="line x" title="65:244	We have avoided implementing a new decoder by pre-computing all the SVM predictions for all possible translations for all source phrases appearing in the test set." ></td>
	<td class="line x" title="66:244	We input this information onto the decoder by replicating the entries in the translation table." ></td>
	<td class="line x" title="67:244	In other words, each distinct occurrence of every single source phrase has a distinct list of phrase translation candidates with their corresponding scores." ></td>
	<td class="line x" title="68:244	Accordingly, the source sentence is transformed into a sequence of identifiers, 160 in our case a sequence of (w,i) pairs3, which allow us to uniquely identify every distinct instance of every word in the test set during decoding, and to retrieve DPT predictions in the translation table." ></td>
	<td class="line x" title="69:244	For that purpose, source phrases in the translation table must comply with the same format." ></td>
	<td class="line x" title="70:244	This imaginative trick4 saved us in the short run a gigantic amount of work." ></td>
	<td class="line x" title="71:244	However, it imposes a severe limitation on the kind of features which the DPT system may use." ></td>
	<td class="line x" title="72:244	In particular, features from the target sentence under construction and from the correspondence between source and target (i.e. , alignments) can not be used." ></td>
	<td class="line x" title="73:244	3 Phrase Translation Analogously to the ?word translation??definition by Vickrey et al.(2005), rather than predicting the sense of a word according to a given sense inventory, in ?phrase translation??" ></td>
	<td class="line x" title="75:244	the goal is to predict the correct translation of a phrase, for a given target language, in the context of a sentence." ></td>
	<td class="line x" title="76:244	This task is simpler than the ?full translation??task, but provides an insight to the gain prospectives." ></td>
	<td class="line x" title="77:244	We used the data from the Openlab 2006 Initiative5 promoted by the TC-STAR Consortium6." ></td>
	<td class="line x" title="78:244	This test suite is entirely based on European Parliament Proceedings." ></td>
	<td class="line x" title="79:244	We have focused on the Spanish-toEnglish task." ></td>
	<td class="line x" title="80:244	The training set consists of 1,281,427 parallel sentences." ></td>
	<td class="line x" title="81:244	Performing phrase extraction over the training data, as described in Section 2.1, we obtained translation candidates for 1,729,191 source phrases." ></td>
	<td class="line x" title="82:244	We built classifiers for all the source phrases with more than one possible translation and more than 10 occurrences." ></td>
	<td class="line x" title="83:244	241,234 source phrases fulfilled this requirement." ></td>
	<td class="line x" title="84:244	For each source phrase, we used 80% of the instances for training, 10% for development, and 10% for test." ></td>
	<td class="line x" title="85:244	Table 1 shows ?phrase translation??results over the test set." ></td>
	<td class="line x" title="86:244	We compare the performance, in terms of accuracy, of DPT models and the ?most frequent translation??baseline (?MFT??." ></td>
	<td class="line x" title="87:244	The MFT base3w is a word and i corresponds to the number of instances of word w seen in the test set before the current instance." ></td>
	<td class="line x" title="88:244	4We have checked that results following this type of decoding when translation tables are estimated on the basis of MLE are identical to regular decoding results." ></td>
	<td class="line x" title="89:244	5http://tc-star.itc.it/openlab2006/ 6http://www.tc-star.org/ phrase set model macro micro all MFT 0.66 0.70 DPT 0.68 0.76 frequent MFT 0.76 0.75 DPT 0.86 0.86 Table 1: ?Phrase Translation??Accuracy (test set)." ></td>
	<td class="line x" title="90:244	line is equivalent to selecting the translation candidate with highest probability according to MLE." ></td>
	<td class="line x" title="91:244	The ?macro??column shows macro-averaged results over all phrases, i.e., the accuracy for each phrase counts equally towards the average." ></td>
	<td class="line x" title="92:244	The ?micro??column shows micro-averaged accuracy, where each test example counts equally." ></td>
	<td class="line x" title="93:244	The ?all??set includes results for the 241,234 phrases, whereas the ?frequent??set includes results for a selection of 41 very frequent phrases ocurring more than 50,000 times." ></td>
	<td class="line x" title="94:244	A priori, DPT models seem to offer a significant room for potential improvement." ></td>
	<td class="line x" title="95:244	Although phrase translation differs from WSD in a number of aspects, the increase with respect to the MFT baseline is comparable." ></td>
	<td class="line x" title="96:244	Results are also coherent with those attained by Vickrey et al.(2005)." ></td>
	<td class="line x" title="98:244	-1 -0.5 0 0.5 1 0 50000 100000 150000 200000 250000 300000 accuracy(DPT) accuracy(MLE) #examples Figure 1: Analysis of ?Phrase Translation??Results on the development set (Spanish-to-English)." ></td>
	<td class="line x" title="99:244	Figure 1 shows the relationship between the accuracy7 gain and the number of training examples." ></td>
	<td class="line x" title="100:244	In general, with a sufficient number of examples (over 10,000), DPT outperforms the MFT baseline." ></td>
	<td class="line x" title="101:244	7We focus on micro-averaged accuracy." ></td>
	<td class="line x" title="102:244	161 4 Full Translation In the ?phrase translation??task the predicted phrase does not interact with the rest of the target sentence." ></td>
	<td class="line x" title="103:244	In this section we analyze the impact of DPT models when the goal is to translate the whole sentence." ></td>
	<td class="line x" title="104:244	For evaluation purposes we count on a set of 1,008 sentences." ></td>
	<td class="line x" title="105:244	Three human references per sentence are available." ></td>
	<td class="line x" title="106:244	We randomly split this set in two halves, and use them for development and test, respectively." ></td>
	<td class="line x" title="107:244	4.1 Evaluation Evaluating the effects of using DPT predictions, directed towards a better word selection, in the full translation task presents two serious difficulties." ></td>
	<td class="line x" title="108:244	In first place, the actual room for improvement caused by a better translation modeling is smaller than estimated in Section 3." ></td>
	<td class="line x" title="109:244	This is mainly due to the SMT architecture itself which relies on a search over a probability space in which several models cooperate." ></td>
	<td class="line x" title="110:244	For instance, in many cases errors caused by a poor translation modeling may be corrected by the language model." ></td>
	<td class="line x" title="111:244	In a recent study, Vilar et al.(2006) found that only around 25% of the errors are related to word selection." ></td>
	<td class="line x" title="113:244	In half of these cases errors are caused by a wrong word sense disambiguation, and in the other half the word sense is correct but the lexical choice is wrong." ></td>
	<td class="line x" title="114:244	In second place, most conventional automatic evaluation metrics have not been designed for this purpose." ></td>
	<td class="line x" title="115:244	For instance, metrics such as BLEU (Papineni et al. , 2001) tend to favour longer n-gram matchings, and are, thus, biased towards word ordering." ></td>
	<td class="line pc" title="116:244	We might find better suited metrics, such as METEOR (Banerjee and Lavie, 2005), which is oriented towards word selection8." ></td>
	<td class="line x" title="117:244	However, a new problem arises." ></td>
	<td class="line x" title="118:244	Because different metrics are biased towards different aspects of quality, scores conferred by different metrics are often controversial." ></td>
	<td class="line x" title="119:244	In order to cope with evaluation difficulties we have applied several complementary actions: 1." ></td>
	<td class="line x" title="120:244	Based on the results from Section 3, we focus on a reduced set of 41 very promising phrases trained on more than 50,000 examples." ></td>
	<td class="line x" title="121:244	This set covers 25.8% of the words in the test set, 8METEOR works at the unigram level, may consider word stemming and, for the case of English is also able to perform a lookup for synonymy in WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="122:244	and exhibits a potential absolute accuracy gain around 11% (See Table 1)." ></td>
	<td class="line x" title="123:244	2." ></td>
	<td class="line x" title="124:244	With the purpose of evaluating the changes related only to this small set of very promising phrases, we introduce a new measure, Apt, which computes ?phrase translation??accuracy for a given list of source phrases." ></td>
	<td class="line x" title="125:244	For every test case, Apt counts the proportion of phrases from the list appearing in the source sentence which have a valid9 translation both in the target sentence and in any of the reference translations." ></td>
	<td class="line x" title="126:244	In fact, because in general source-totarget alignments are not known, Apt calculates an approximate10 solution." ></td>
	<td class="line x" title="127:244	3." ></td>
	<td class="line x" title="128:244	We evaluate overall MT quality on the basis of ?Human Likeness??" ></td>
	<td class="line x" title="129:244	In particular, we use the QUEEN11 meta-measure from the QARLA Framework (Amigo et al. , 2005)." ></td>
	<td class="line x" title="130:244	QUEEN operates under the assumption that a good translation must be similar to all human references according to all metrics." ></td>
	<td class="line x" title="131:244	Given a set of automatic translations A, a set of similarity metrics X, and a set of human references R, QUEEN is defined as the probability, over RRR, that for every metric in X the automatic translation a is more similar to a reference r than two other references rprime and rprimeprime to each other." ></td>
	<td class="line x" title="132:244	Formally: QUEENX,R(a) = Prob(?x ??X : x(a,r) ??x(rprime,rprimeprime)) QUEEN captures the features that are common to all human references, rewarding those automatic translations which share them, and penalizing those which do not." ></td>
	<td class="line x" title="133:244	Thus, QUEEN provides a robust means of combining several metrics into a single measure of quality." ></td>
	<td class="line x" title="134:244	Following the methodology described by Gimenez and Amigo (2006), we compute the QUEEN measure over the metric combination with highest KING, i.e., discriminative power." ></td>
	<td class="line x" title="135:244	We have considered all the lexical metrics12 provided by 9Valid translations are provided by the translation table." ></td>
	<td class="line x" title="136:244	10Current Apt implementation searches phrases from left to right in decreasing length order." ></td>
	<td class="line x" title="137:244	11QUEEN is available inside the IQMT package for MT Evaluation based on ?Human Likeness??(Gimenez and Amigo, 2006)." ></td>
	<td class="line x" title="138:244	http://www.lsi.upc.edu/?nlp/IQMT 12Consult the IQMT Technical Manual v1.3 for a detailed description of the metric set." ></td>
	<td class="line x" title="139:244	http://www.lsi.upc.edu/ ?nlp/IQMT/IQMT.v1.3.pdf 162 QUEEN Apt BLEU METEOR ROUGE P(e) +PMLE(f|e) 0.43 0.86 0.59 0.77 0.42 P(e) +PMLE(e|f) 0.45 0.87 0.62 0.77 0.43 P(e) +PDPT(e|f) 0.47 0.89 0.62 0.78 0.44 Table 2: Automatic evaluation of the ?full translation??results on the test set." ></td>
	<td class="line x" title="140:244	IQMT." ></td>
	<td class="line x" title="141:244	The optimal set is: { METEORwnsyn, ROUGEw 1.2 } which includes variants of METEOR, and ROUGE (Lin and Och, 2004)." ></td>
	<td class="line x" title="142:244	4.2 Adjustment of Parameters Models are combined in a log-linear fashion: logP(e|f) ??lmlogP(e) + glogPMLE(f|e) + dlogPMLE(e|f) + DPTlogPDPT(e|f) P(e) is the language model probability." ></td>
	<td class="line x" title="143:244	PMLE(f|e) corresponds to the MLE-based generative translation model, whereas PMLE(e|f) corresponds to the analogous discriminative model." ></td>
	<td class="line x" title="144:244	PDPT(e|f) corresponds to the DPT model which uses SVM-based predictions in a wider feature context." ></td>
	<td class="line x" title="145:244	In order to perform fair comparisons, model weights must be adjusted." ></td>
	<td class="line x" title="146:244	Because we have focused on a reduced set of frequent phrases, in order to translate the whole test set we must provide alternative translation probabilities for all the source phrases in the vocabulary which do not have a DPT prediction." ></td>
	<td class="line x" title="147:244	We have used MLE predictions to complete the model." ></td>
	<td class="line x" title="148:244	However, interaction between DPT and MLE models is problematic." ></td>
	<td class="line x" title="149:244	Problems arise when, for a given source phrase, fi, DPT predictions must compete with MLE predictions for larger phrases fj overlapping with or containing fi (See Section 4.3)." ></td>
	<td class="line x" title="150:244	We have alleviated these problems by splitting DPT tables in 3 subtables: (1) phrases with DPT prediction, (2) phrases with DPT prediction only for subphrases of it, and (3) phrases with no DPT prediction for any subphrase; and separately adjusting their weights." ></td>
	<td class="line x" title="151:244	Counting on a reliable automatic measure of quality is a crucial issue for system development." ></td>
	<td class="line x" title="152:244	Optimal configurations may vary very significantly depending on the metric governing the optimization process." ></td>
	<td class="line x" title="153:244	We optimize the system parameters over the QUEEN measure, which has proved to lead to more robust system configurations than BLEU (Lambert et al. , 2006)." ></td>
	<td class="line x" title="154:244	We exhaustively try all possible parameter configurations, at a resolution of 0.1, over the development set and select the best one." ></td>
	<td class="line x" title="155:244	In order to keep the optimization process feasible, in terms of time, the search space is pruned13 during decoding." ></td>
	<td class="line x" title="156:244	4.3 Results We compare the systems using the generative and discriminative MLE-based translation models to the discriminative translation model which uses DPT predictions for the set of 41 very ?frequent??source phrases." ></td>
	<td class="line x" title="157:244	Table 2 shows automatic evaluation results on the test set, according to several metrics." ></td>
	<td class="line x" title="158:244	Phrase translation accuracy (over the ?frequent??set of phrases) and MT quality are evaluated by means of the Apt and QUEEN measures, respectively." ></td>
	<td class="line x" title="159:244	For the sake of informativeness, BLEU, METEORwnsyn and ROUGEw 1.2 scores are provided as well." ></td>
	<td class="line x" title="160:244	Interestingly, discriminative models outperform the (noisy-channel) default generative model." ></td>
	<td class="line x" title="161:244	Improvement in Apt measure also reveals that DPT predictions provide a better translation for the set of ?frequent??phrases than the MLE models." ></td>
	<td class="line x" title="162:244	This improvement remains when measuring overall translation quality via QUEEN." ></td>
	<td class="line x" title="163:244	If we take into account that DPT predictions are available for only 25% of the words in the test set, we can say that the gain reported by the QUEEN and Apt measures is consistent with the accuracy prospectives predicted in Table 1." ></td>
	<td class="line x" title="164:244	METEORwnsyn and ROUGEw 1.2 reflect a slight improvement as well." ></td>
	<td class="line x" title="165:244	However, according to BLEU there is no difference between both systems." ></td>
	<td class="line x" title="166:244	We suspect that BLEU is unable to accurately reflect the possible gains attained by a better ?phrase selection??" ></td>
	<td class="line x" title="167:244	over a small set of phrases because of its tendency 13For each phrase only the 30 top-scoring translations are used." ></td>
	<td class="line x" title="168:244	At all times, only the 100 top-scoring solutions are kept." ></td>
	<td class="line x" title="169:244	We also disabled distortion and word penalty models." ></td>
	<td class="line x" title="170:244	Therefore, translations are monotonic, and source and target tend to have the same number of words (that is not mandatory)." ></td>
	<td class="line x" title="171:244	163 to reward long n-gram matchings." ></td>
	<td class="line x" title="172:244	In order to clarify this scenario a rigorous process of manual evaluation has been conducted." ></td>
	<td class="line x" title="173:244	We have selected a subset of sentences based on the following criteria: ??sentence length between 10 and 30 words." ></td>
	<td class="line x" title="174:244	??at least 5 words have a DPT prediction." ></td>
	<td class="line x" title="175:244	??DPT and MLE outputs differ." ></td>
	<td class="line x" title="176:244	A total of 114 sentences fulfill these requirements." ></td>
	<td class="line x" title="177:244	In each translation case, assessors must judge whether the output by the discriminative ?MLE??system is better, equal to or worse than the output by the ?DPT??system, with respect to adequacy, fluency, and overall quality." ></td>
	<td class="line x" title="178:244	In order to avoid any bias in the evaluation, we have randomized the respective position in the display of the sentences corresponding to each system." ></td>
	<td class="line x" title="179:244	Four judges participated in the evaluation." ></td>
	<td class="line x" title="180:244	Each judge evaluated only half of the cases." ></td>
	<td class="line x" title="181:244	Each case was evaluated by two different judges." ></td>
	<td class="line x" title="182:244	Therefore, we count on 228 human assessments." ></td>
	<td class="line x" title="183:244	Table 3 shows the results of the manual system comparison." ></td>
	<td class="line x" title="184:244	Statistical significance has been determined using the sign-test (Siegel, 1956)." ></td>
	<td class="line x" title="185:244	According to human assessors, the ?DPT??system outperforms the ?MLE??system very significantly with respect to adequacy, whereas for fluency there is a slight advantage in favor of the ?MLE??system." ></td>
	<td class="line x" title="186:244	Overall, there is a slight but significant advantage in favor of the ?DPT??system." ></td>
	<td class="line x" title="187:244	Manual evaluation confirms our suspicion that the BLEU metric is less sensitive than QUEEN to improvements related to adequacy." ></td>
	<td class="line x" title="188:244	Error Analysis Guided by the QUEEN measure, we carefully inspect particular cases." ></td>
	<td class="line x" title="189:244	We start, in Table 4, by showing a positive case." ></td>
	<td class="line x" title="190:244	The three phrases highlighted in the source sentence (?tiene??" ></td>
	<td class="line x" title="191:244	?se?nora??and ?una cuestion??" ></td>
	<td class="line x" title="192:244	find a better translation with the help of the DPT models: ?tiene??translates into ?has??instead of ?i give??" ></td>
	<td class="line x" title="193:244	?se?nora??into ?mrs??instead of ?lady??" ></td>
	<td class="line x" title="194:244	and ?una cuestion??into ?a point??instead of ?a motion??" ></td>
	<td class="line x" title="195:244	In contrast, Table 5 shows a negative case." ></td>
	<td class="line x" title="196:244	The translation of the Spanish word ?se?nora??as ?mrs??is acceptable." ></td>
	<td class="line x" title="197:244	However, it influences very negatively the translation of the following word ?diputada??" ></td>
	<td class="line x" title="198:244	whereas the ?MLE??system translates the phrase ?se?nora diputada??" ></td>
	<td class="line x" title="199:244	which does not have a DPT prediction, as a whole." ></td>
	<td class="line x" title="200:244	Similarly, the translation of Adequacy Fluency Overall MLE > DPT 39 84 83 MLE = DPT 100 76 46 MLE < DPT 89 68 99 Table 3: Manual evaluation of the ?full translation??" ></td>
	<td class="line x" title="201:244	results on the test set." ></td>
	<td class="line x" title="202:244	Counts on the number of translation cases for which the ?MLE??system is better than (>), equal to (=), or worse than (<) the ?DPT??system, with respect to adequacy, fluency, and overall MT quality, are presented." ></td>
	<td class="line x" title="203:244	?cuestion??as ?matter??" ></td>
	<td class="line x" title="204:244	although acceptable, is breaking the phrase ?cuestion de orden??of high cohesion, which is commonly translated as ?point of order??" ></td>
	<td class="line x" title="205:244	The cause underlying these problems is that DPT predictions are available only for a subset of phrases." ></td>
	<td class="line x" title="206:244	Thus, during decoding, for these cases our DPT models may be in disadvantage." ></td>
	<td class="line x" title="207:244	5 Related Work Recently, there is a growing interest in the application of WSD technology to MT. For instance, Carpuat and Wu (2005b) suggested integrating WSD predictions into a SMT system in a ?hard??" ></td>
	<td class="line x" title="208:244	manner, either for decoding, by constraining the set of acceptable translation candidates for each given source word, or for post-processing the SMT system output, by directly replacing the translation of each selected word with the WSD system prediction." ></td>
	<td class="line x" title="209:244	They did not manage to improve MT quality." ></td>
	<td class="line x" title="210:244	They encountered several problems inherent to the SMT architecture." ></td>
	<td class="line x" title="211:244	In particular, they described what they called the ?language model effect??in SMT: ?The lexical choices are made in a way that heavily prefers phrasal cohesion in the output target sentence, as scored by the language model.??" ></td>
	<td class="line x" title="212:244	This problem is a direct consequence of the ?hard??interaction between their WSD and SMT systems." ></td>
	<td class="line x" title="213:244	WSD predictions cannot adapt to the surrounding target context." ></td>
	<td class="line x" title="214:244	In a later work, Carpuat and Wu (2005a) analyzed the converse question, i.e. they measured the WSD performance of SMT models." ></td>
	<td class="line x" title="215:244	They showed that dedicated WSD models significantly outperform current state-of-the-art SMT models." ></td>
	<td class="line x" title="216:244	Consequently, SMT should benefit from WSD predictions." ></td>
	<td class="line x" title="217:244	Simultaneously, Vickrey et al.(2005) studied the 164 Source tiene la palabra la se?nora mussolini para una cuestion de orden . Ref 1 mrs mussolini has the floor for a point of order . Ref 2 you have the floor, missus mussolini, for a question of order . Ref 3 ms mussolini has now the floor for a point of order . P(e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion . P(e) + PDPT(e|f) has the floor the mrs mussolini on a point of order . Table 4: Case of Analysis of sentence #422." ></td>
	<td class="line x" title="219:244	DPT models help." ></td>
	<td class="line x" title="220:244	Source se?nora diputada, esta no es una cuestion de orden . Ref 1 mrs mussolini, that is not a point of order . Ref 2 honourable member, this is not a question of order . Ref 3 my honourable friend, this is not a point of order . P(e) + PMLE(e|f) honourable member, this is not a point of order . P(e) + PDPT(e|f) mrs karamanou, this is not a matter of order . Table 5: Case of Analysis of sentence #434." ></td>
	<td class="line x" title="221:244	DPT models fail." ></td>
	<td class="line x" title="222:244	application of discriminative models based on WSD technology to the ?blank-filling??task, a simplified version of the translation task, in which the target context surrounding the word translation is available." ></td>
	<td class="line x" title="223:244	They did not encounter the ?language model effect??because they approached the task in a ?soft??" ></td>
	<td class="line x" title="224:244	way, i.e., allowing their WSD models to interact with other models during decoding." ></td>
	<td class="line x" title="225:244	Similarly, our DPT models are, as described in Section 2.2, softly integrated in the decoding step, and thus do not suffer from the detrimental ?language model effect??either, in the context of the ?full translation??task." ></td>
	<td class="line x" title="226:244	Besides, DPT models enforce phrasal cohesion by considering disambiguation at the level of phrases." ></td>
	<td class="line x" title="227:244	6 Conclusions and Further Work Despite the fact that measuring improvements in word selection is a very delicate issue, we have showed that dedicated discriminative translation models considering a wider feature context provide a useful mechanism in order to improve the quality of current phrase-based SMT systems, specially with regard to adequacy." ></td>
	<td class="line x" title="228:244	However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study." ></td>
	<td class="line x" title="229:244	Moreover, there are several open issues." ></td>
	<td class="line x" title="230:244	First, for practical reasons, we have limited to a reduced set of ?frequent??phrases, and we have disabled reordering and word penalty models." ></td>
	<td class="line x" title="231:244	We are currently studying the impact of a larger set of phrases, covering over 99% of the words in the test set." ></td>
	<td class="line x" title="232:244	Experiments with enabled reordering and word penalty models should be conducted as well." ></td>
	<td class="line x" title="233:244	Second, automatic evaluation of the results revealed a low agreement between BLEU and other metrics." ></td>
	<td class="line x" title="234:244	For system comparison, we solved this through a process of manual evaluation." ></td>
	<td class="line x" title="235:244	However, this is impractical for the adjustment of parameters, where hundreds of different configurations are tried." ></td>
	<td class="line x" title="236:244	In this work we have relied on automatic evaluation based on ?Human Likeness??which allows for metric combinations and provides a stable and robust criterion for the metric set selection." ></td>
	<td class="line x" title="237:244	Other alternatives could be tried." ></td>
	<td class="line x" title="238:244	The crucial issue, in our opinion, is that the metric guiding the optimization is able to capture the changes." ></td>
	<td class="line x" title="239:244	Finally, we argue that, if DPT models considered features from the target side, and from the correspondence between source and target, results could further improve." ></td>
	<td class="line x" title="240:244	However, at the short term, the incorporation of these type of features will force us to either build a new decoder or extend an existing one, or to move to a new MT architecture, for instance, in the fashion of the architectures suggested by Tillmann and Zhang (2006) or Liang et al.(2006)." ></td>
	<td class="line x" title="242:244	Acknowledgements This research has been funded by the Spanish Ministry of Education and Science, projects OpenMT (TIN2006-15307-C03-02) and TRAN165 GRAM (TIN2004-07925-C03-02)." ></td>
	<td class="line x" title="243:244	We are recognized as a Quality Research Group (2005 SGR00130) by DURSI, the Research Department of the Catalan Government." ></td>
	<td class="line x" title="244:244	Authors are thankful to the TC-STAR Consortium for providing such very valuable data sets." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0734
METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments
Lavie, Alon;Agarwal, Abhaya;"></td>
	<td class="line x" title="1:94	Proceedings of the Second Workshop on Statistical Machine Translation, pages 228??31, Prague, June 2007." ></td>
	<td class="line p" title="2:94	c2007 Association for Computational Linguistics Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments Alon Lavie and Abhaya Agarwal Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA {alavie,abhayaa}@cs.cmu.edu Abstract Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric." ></td>
	<td class="line o" title="3:94	It is one of several automatic metrics used in this year?s shared task within the ACL WMT-07 workshop." ></td>
	<td class="line o" title="4:94	This paper recaps the technical details underlying the metric and describes recent improvements in the metric." ></td>
	<td class="line p" title="5:94	The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English." ></td>
	<td class="line x" title="6:94	1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years." ></td>
	<td class="line x" title="7:94	Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators." ></td>
	<td class="line x" title="8:94	Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development." ></td>
	<td class="line x" title="9:94	The most commonly used MT evaluation metric in recent years has been IBM?s Bleu metric (Papineni et al. , 2002)." ></td>
	<td class="line x" title="10:94	Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003)." ></td>
	<td class="line x" title="11:94	Various researchers have noted, however, various weaknesses in the metric." ></td>
	<td class="line x" title="12:94	Most notably, Bleu does not produce very reliable sentence-level scores." ></td>
	<td class="line o" title="13:94	Meteor, as well as several other proposed metrics such as GTM (Melamed et al. , 2003), TER (Snover et al. , 2006) and CDER (Leusch et al. , 2006) aim to address some of these weaknesses." ></td>
	<td class="line o" title="14:94	Meteor, initially proposed and released in 2004 (Lavie et al. , 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level." ></td>
	<td class="line oc" title="15:94	Previous publications on Meteor (Lavie et al. , 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics." ></td>
	<td class="line o" title="16:94	This paper recaps the technical details underlying Meteor and describes recent improvements in the metric." ></td>
	<td class="line o" title="17:94	The latest release extends Meteor to support evaluation of MT output in Spanish, French and German, in addition to English." ></td>
	<td class="line x" title="18:94	Furthermore, several parameters within the metric have been optimized on language-specific training data." ></td>
	<td class="line x" title="19:94	We present experimental results that demonstrate the improvements in correlations with human judgments that result from these parameter tunings." ></td>
	<td class="line o" title="20:94	2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation." ></td>
	<td class="line x" title="21:94	If more than one reference translation is available, the translation is scored against each reference independently, and the best scoring pair is used." ></td>
	<td class="line o" title="22:94	Given a pair of strings to be compared, Meteor creates a word alignment between the two strings." ></td>
	<td class="line x" title="23:94	An alignment is mapping between words, such that every word in each string maps to at most one word in the other string." ></td>
	<td class="line x" title="24:94	This alignment is incrementally produced by a sequence of word-mapping modules." ></td>
	<td class="line x" title="25:94	The ?exact??module maps two words if they are exactly the same." ></td>
	<td class="line x" title="26:94	The ?porter stem??module maps two words if they are the same after they are stemmed using the Porter stemmer." ></td>
	<td class="line x" title="27:94	The ?WN synonymy??module maps two words if they are considered synonyms, based on the fact that they both belong to the same ?synset??in WordNet." ></td>
	<td class="line x" title="28:94	The word-mapping modules initially identify all 228 possible word matches between the pair of strings." ></td>
	<td class="line x" title="29:94	We then identify the largest subset of these word mappings such that the resulting set constitutes an alignment as defined above." ></td>
	<td class="line x" title="30:94	If more than one maximal cardinality alignment is found, Meteor selects the alignment for which the word order in the two strings is most similar (the mapping that has the least number of ?crossing??unigram mappings)." ></td>
	<td class="line x" title="31:94	The order in which the modules are run reflects wordmatching preferences." ></td>
	<td class="line x" title="32:94	The default ordering is to first apply the ?exact??mapping module, followed by ?porter stemming??and then ?WN synonymy??" ></td>
	<td class="line o" title="33:94	Once a final alignment has been produced between the system translation and the reference translation, the Meteor score for this pairing is computed as follows." ></td>
	<td class="line x" title="34:94	Based on the number of mapped unigrams found between the two strings (m), the total number of unigrams in the translation (t) and the total number of unigrams in the reference (r), we calculate unigram precision P = m/t and unigram recall R = m/r. We then compute a parameterized harmonic mean of P and R (van Rijsbergen, 1979): Fmean = P  R  P +(1??) R Precision, recall and Fmean are based on singleword matches." ></td>
	<td class="line o" title="35:94	To take into account the extent to which the matched unigrams in the two strings are in the same word order, Meteorcomputes a penalty for a given alignment as follows." ></td>
	<td class="line x" title="36:94	First, the sequence of matched unigrams between the two strings is divided into the fewest possible number of ?chunks??" ></td>
	<td class="line x" title="37:94	such that the matched unigrams in each chunk are adjacent (in both strings) and in identical word order." ></td>
	<td class="line x" title="38:94	The number of chunks (ch) and the number of matches (m) is then used to calculate a fragmentation fraction: frag = ch/m. The penalty is then computed as: Pen =   frag The value of  determines the maximum penalty (0 ?? ??1)." ></td>
	<td class="line x" title="39:94	The value of  determines the functional relation between fragmentation and the penalty." ></td>
	<td class="line o" title="40:94	Finally, the Meteor score for the alignment between the two strings is calculated as: score = (1??Pen) Fmean In all previous versions of Meteor, the values of the three parameters mentioned above were set to be:  = 0.9,  = 3.0 and  = 0.5, based on experimentation performed in early 2004." ></td>
	<td class="line x" title="41:94	In the latest release, we tuned these parameters to optimize correlation with human judgments based on more extensive experimentation, as reported in section 4." ></td>
	<td class="line o" title="42:94	3 Meteor Implementations for Spanish, French and German We have recently expanded the implementation of Meteor to support evaluation of translations in Spanish, French and German, in addition to English." ></td>
	<td class="line x" title="43:94	Two main language-specific issues required adaptation within the metric: (1) language-specific wordmatching modules; and (2) language-specific parameter tuning." ></td>
	<td class="line o" title="44:94	The word-matching component within the English version of Meteor uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference." ></td>
	<td class="line x" title="45:94	The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007)." ></td>
	<td class="line o" title="46:94	In order to construct instances of Meteor for Spanish, French and German, we created new languagespecific ?stemming??modules." ></td>
	<td class="line x" title="47:94	We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007)." ></td>
	<td class="line x" title="48:94	Unfortunately, we have so far been unable to obtain freely available WordNet resources for these three languages." ></td>
	<td class="line o" title="49:94	Meteor versions for Spanish, French and German therefore currently include only ?exact??" ></td>
	<td class="line x" title="50:94	and ?stemming??matching modules." ></td>
	<td class="line x" title="51:94	We are investigating the possibility of developing new synonymy modules for the various languages based on alternative methods, which could then be used in place of WordNet." ></td>
	<td class="line o" title="52:94	The second main language-specific issue which required adaptation is the tuning of the three parameters within Meteor, described in section 4." ></td>
	<td class="line oc" title="53:94	4 Optimizing Metric Parameters The original version of Meteor (Banerjee and Lavie, 2005) has instantiated values for three parameters in the metric: one for controlling the relative weight of precision and recall in computing the Fmean score (); one governing the shape of the penalty as a function of fragmentation () and one for the relative weight assigned to the fragmentation penalty ()." ></td>
	<td class="line o" title="54:94	In all versions of Meteor to date, these parameters were instantiated with the values  = 0.9,  = 3.0 and  = 0.5, based on early data experimentation." ></td>
	<td class="line x" title="55:94	We recently conducted a more thorough investigation aimed at tuning these parameters based on several available data sets, with the goal of finding parameter settings that maximize correlation with human judgments." ></td>
	<td class="line x" title="56:94	Human judgments come in the form of ?adequacy??and ?fluency??quantitative scores." ></td>
	<td class="line x" title="57:94	In our experiments, we looked at optimizing parameters for each of these human judgment types separately, as well as optimizing parameters for the sum of adequacy and fluency." ></td>
	<td class="line o" title="58:94	Parameter adapta229 Corpus Judgments Systems NIST 2003 Ara-to-Eng 3978 6 NIST 2004 Ara-to-Eng 347 5 WMT-06 Eng-to-Fre 729 4 WMT-06 Eng-to-Ger 756 5 WMT-06 Eng-to-Spa 1201 7 Table 1: Corpus Statistics for Various Languages tion is also an issue in the newly created Meteor instances for other languages." ></td>
	<td class="line x" title="59:94	We suspected that parameters that were optimized to maximize correlation with human judgments for English would not necessarily be optimal for other languages." ></td>
	<td class="line x" title="60:94	4.1 Data For English, we used the NIST 2003 Arabic-toEnglish MT evaluation data for training and the NIST 2004 Arabic-to-English evaluation data for testing." ></td>
	<td class="line x" title="61:94	For Spanish, German and French we used the evaluation data provided by the shared task at last year?s WMT workshop." ></td>
	<td class="line x" title="62:94	Sizes of various corpora are shown in Table 1." ></td>
	<td class="line x" title="63:94	Some, but not all, of these data sets have multiple human judgments per translation hypothesis." ></td>
	<td class="line x" title="64:94	To partially address human bias issues, we normalize the human judgments, which transforms the raw judgment scores so that they have similar distributions." ></td>
	<td class="line x" title="65:94	We use the normalization method described in (Blatz et al. , 2003)." ></td>
	<td class="line x" title="66:94	Multiple judgments are combined into a single number by taking their average." ></td>
	<td class="line x" title="67:94	4.2 Methodology We performed a ?hill climbing??search to find the parameters that achieve maximum correlation with human judgments on the training set." ></td>
	<td class="line x" title="68:94	We use Pearson?s correlation coefficient as our measure of correlation." ></td>
	<td class="line x" title="69:94	We followed a ?leave one out??training procedure in order to avoid over-fitting." ></td>
	<td class="line x" title="70:94	When n systems were available for a particular language, we train the parameters n times, leaving one system out in each training, and pooling the segments from all other systems." ></td>
	<td class="line x" title="71:94	The final parameter values are calculated as the mean of the n sets of trained parameters that were obtained." ></td>
	<td class="line x" title="72:94	When evaluating a set of parameters on test data, we compute segment-level correlation with human judgments for each of the systems in the test set and then report the mean over all systems." ></td>
	<td class="line x" title="73:94	4.3 Results 4.3.1 Optimizing for Adequacy and Fluency We trained parameters to obtain maximum correlation with normalized adequacy and fluency judgAdequacy Fluency Sum  0.82 0.78 0.81  1.0 0.75 0.83  0.21 0.38 0.28 Table 2: Optimal Values of Tuned Parameters for Different Criteria for English Adequacy Fluency Sum Original 0.6123 0.4355 0.5704 Adequacy 0.6171 0.4354 0.5729 Fluency 0.6191 0.4502 0.5818 Sum 0.6191 0.4425 0.5778 Table 3: Pearson Correlation with Human Judgments on Test Data for English ments separately and also trained for maximal correlation with the sum of the two." ></td>
	<td class="line x" title="74:94	The resulting optimal parameter values on the training corpus are shown in Table 2." ></td>
	<td class="line x" title="75:94	Pearson correlations with human judgments on the test set are shown in Table 3." ></td>
	<td class="line x" title="76:94	The optimal parameter values found are somewhat different than our previous metric parameters (lower values for all three parameters)." ></td>
	<td class="line x" title="77:94	The new parameters result in moderate but noticeable improvements in correlation with human judgments on both training and testing data." ></td>
	<td class="line x" title="78:94	Tests for statistical significance using bootstrap sampling indicate that the differences in correlation levels are all significant at the 95% level." ></td>
	<td class="line x" title="79:94	Another interesting observation is that precision receives slightly more ?weight??when optimizing correlation with fluency judgments (versus when optimizing correlation with adequacy)." ></td>
	<td class="line x" title="80:94	Recall, however, is still given more weight than precision." ></td>
	<td class="line x" title="81:94	Another interesting observation is that the value of  is higher for fluency optimization." ></td>
	<td class="line x" title="82:94	Since the fragmentation penalty reflects word-ordering, which is closely related to fluency, these results are consistent with our expectations." ></td>
	<td class="line x" title="83:94	When optimizing correlation with the sum of adequacy and fluency, optimal values fall in between the values found for adequacy and fluency." ></td>
	<td class="line x" title="84:94	4.3.2 Parameters for Other Languages Similar to English, we trained parameters for Spanish, French and German on the available WMT06 training data." ></td>
	<td class="line x" title="85:94	We optimized for maximum correlation with human judgments of adequacy, fluency and for the sum of the two." ></td>
	<td class="line x" title="86:94	Resulting parameters are shown in Table 4.3.2." ></td>
	<td class="line x" title="87:94	For all three languages, the parameters that were found to be optimal were quite different than those that were found for English, and using the language-specific optimal parameters re230 Adequacy Fluency Sum French: 0.86 0.74 0.76  0.5 0.5 0.5  1.0 1.0 1.0 German: 0.95 0.95 0.95  0.5 0.5 0.5  0.6 0.8 0.75 Spanish: 0.95 0.62 0.95  1.0 1.0 1.0  0.9 1.0 0.98 Table 4: Tuned Parameters for Different Languages sults in significant gains in Pearson correlation levels with human judgments on the training data (compared with those obtained using the English optimal parameters)1." ></td>
	<td class="line x" title="88:94	Note that the training sets used for these optimizations are comparatively very small, and that we currently do not have unseen test data to evaluate the parameters for these three languages." ></td>
	<td class="line x" title="89:94	Further validation will need to be performed once additional data becomes available." ></td>
	<td class="line o" title="90:94	5 Conclusions In this paper we described newly developed language-specific instances of the Meteor metric and the process of optimizing metric parameters for different human measures of translation quality and for different languages." ></td>
	<td class="line x" title="91:94	Our evaluations demonstrate that parameter tuning improves correlation with human judgments." ></td>
	<td class="line x" title="92:94	The stability of the optimized parameters on different data sets remains to be investigated for languages other than English." ></td>
	<td class="line o" title="93:94	We are currently exploring broadening the set of features used in Meteor to include syntax-based features and alternative notions of synonymy." ></td>
	<td class="line o" title="94:94	The latest release of Meteor is freely available on our website at: http://www.cs.cmu.edu/~alavie/METEOR/ Acknowledgements The work reported in this paper was supported by NSF Grant IIS-0534932." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0737
Localization of Difficult-to-Translate Phrases
Mohit, Behrang;Hwa, Rebecca;"></td>
	<td class="line x" title="1:228	Proceedings of the Second Workshop on Statistical Machine Translation, pages 248??55, Prague, June 2007." ></td>
	<td class="line x" title="2:228	c2007 Association for Computational Linguistics Localization of Difficult-to-Translate Phrases Behrang Mohit1 and Rebecca Hwa1,2 Intelligent Systems Program1 Department of Computer Science2 University of Pittsburgh Pittsburgh, PA 15260 U.S.A. {behrang, hwa}@cs.pitt.edu Abstract This paper studies the impact that difficult-totranslate source-language phrases might have on the machine translation process." ></td>
	<td class="line x" title="3:228	We formulate the notion of difficulty as a measurable quantity; we show that a classifier can be trained to predict whether a phrase might be difficult to translate; and we develop a framework that makes use of the classifier and external resources (such as human translators) to improve the overall translation quality." ></td>
	<td class="line x" title="4:228	Through experimental work, we verify that by isolating difficult-to-translate phrases and processing them as special cases, their negative impact on the translation of the rest of the sentences can be reduced." ></td>
	<td class="line x" title="5:228	1 Introduction For translators, not all source sentences are created equal." ></td>
	<td class="line x" title="6:228	Some are straight-forward enough to be automatically translated by a machine, while others may stump even professional human translators." ></td>
	<td class="line x" title="7:228	Similarly, within a single sentence there may be some phrases that are more difficult to translate than others." ></td>
	<td class="line x" title="8:228	The focus of this paper is on identifying Difficult-to-Translate Phrases (DTPs) within a source sentence and determining their impact on the translation process." ></td>
	<td class="line x" title="9:228	We investigate three questions: (1) how should we formalize the notion of difficulty as a measurable quantity over an appropriately defined phrasal unit?" ></td>
	<td class="line x" title="10:228	(2) To what level of accuracy can we automatically identify DTPs?" ></td>
	<td class="line x" title="11:228	(3) To what extent do DTPs affect an MT system's performance on other (not-as-difficult) parts of the sentence?" ></td>
	<td class="line x" title="12:228	Conversely, would knowing the correct translation for the DTPs improve the system?s translation for the rest of the sentence?" ></td>
	<td class="line x" title="13:228	In this work, we model difficulty as a measurement with respect to a particular MT system." ></td>
	<td class="line x" title="14:228	We further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the MT system, which can be approximated using an automatic evaluation metric, such as BLEU (Papineni et al. , 2002)." ></td>
	<td class="line x" title="15:228	Using this formulation of difficulty, we build a framework that augments an off-the-shelf phrasebased MT system with a DTP classifier that we developed." ></td>
	<td class="line x" title="16:228	We explore the three questions in a set of experiments, using the framework as a testbed." ></td>
	<td class="line x" title="17:228	In the first experiment, we verify that our proposed difficulty measurement is sensible." ></td>
	<td class="line x" title="18:228	The second experiment evaluates the classifier's accuracy in predicting whether a source phrase is a DTP." ></td>
	<td class="line x" title="19:228	For that, we train a binary SVM classifier via a series of lexical and system dependent features." ></td>
	<td class="line x" title="20:228	The third is an oracle study in which the DTPs are perfectly identified and human translations are obtained." ></td>
	<td class="line x" title="21:228	These human-translated phrases are then used to constrain the MT system as it translates the rest of the sentence." ></td>
	<td class="line x" title="22:228	We evaluate the translation quality of the entire sentence and also the parts that are not translated by humans." ></td>
	<td class="line x" title="23:228	Finally, the framework is evaluated as a whole." ></td>
	<td class="line x" title="24:228	Results from our experiments suggest that improved handling of DTPs will have a positive impact the overall MT output quality." ></td>
	<td class="line x" title="25:228	Moreover, we find the SVMtrained DTP classifier to have a promising rate of accuracy, and that the incorporation of DTP information can improve the outputs of the underlying MT system." ></td>
	<td class="line x" title="26:228	Specifically, we achieve an improvement of translation quality for non-difficult seg248 ments of a sentence when the DTPs are translated by humans." ></td>
	<td class="line x" title="27:228	2 Motivation There are several reasons for investigating ways to identify DTPs." ></td>
	<td class="line x" title="28:228	For instance, it can help to find better training examples in an active learning framework; it can be used to coordinate outputs of multiple translation systems; or it can be used as means of error analysis for MT system development." ></td>
	<td class="line x" title="29:228	It can also be used as a preprocessing step, an alternative to post-editing." ></td>
	<td class="line x" title="30:228	For many languages, MT output requires posttranslation editing that can be cumbersome task for low quality outputs, long sentences, complicated structures and idioms." ></td>
	<td class="line x" title="31:228	Pre-translation might be viewed as a kind of preventive medicine; that is, a system might produce an overall better output if it were not thwarted by some small portion of the input." ></td>
	<td class="line x" title="32:228	By identifying DTPs and passing those cases off to an expensive translation resource (e.g. humans) first, we might avoid problems further down the MT pipeline." ></td>
	<td class="line x" title="33:228	Moreover, pre-translation might not always have to be performed by humans." ></td>
	<td class="line x" title="34:228	What is considered difficult for one system might not be difficult for another system; thus, pretranslation might also be conducted using multiple MT systems." ></td>
	<td class="line x" title="35:228	3 Our Approach Figure 1 presents the overall dataflow of our system." ></td>
	<td class="line x" title="36:228	The input is a source sentence (a1 an), from which DTP candidates are proposed." ></td>
	<td class="line x" title="37:228	Because the DTPs will have to be translated by humans as independent units, we limit the set of possible phrases to be syntactically meaningful units." ></td>
	<td class="line x" title="38:228	Therefore, the framework requires a sourcelanguage syntactic parser or chunker." ></td>
	<td class="line x" title="39:228	In this paper, we parse the source sentence with an off-the-shelf syntactic parser (Bikel, 2002)." ></td>
	<td class="line x" title="40:228	From the parse tree produced for the source sentence, every constituent whose string span is between 25% and 75% of the full sentence length is considered a DTP candidate." ></td>
	<td class="line x" title="41:228	Additionally we have a tree node depth constraint that requires the constituent to be at least two levels above the tree?s yield and two levels below the root." ></td>
	<td class="line x" title="42:228	These two constraints ensure that the extracted phrases have balanced lengths." ></td>
	<td class="line x" title="43:228	We apply the classifier on each candidate and select the one labeled as difficult with the highest classification score." ></td>
	<td class="line x" title="44:228	Depending on the underlying classifier, the score can be in various formats such as class probablity, confidence measure, etc. In our SVM based classifier, the score is the distance from the margin." ></td>
	<td class="line x" title="45:228	Figure 1: An overview of our translation framework." ></td>
	<td class="line x" title="46:228	The chosen phrase (aj  ak) is translated by a human (ei  em)." ></td>
	<td class="line x" title="47:228	We constrain the underlying phrase-based MT system (Koehn, 2003) so that its decoding of the source sentence must contain the human translation for the DTP." ></td>
	<td class="line x" title="48:228	In the following subsections, we describe how we develop the DTP classifier with machine learning techniques and how we constrain the underlying MT system with human translated DTPs." ></td>
	<td class="line x" title="49:228	3.1 Training the DTP Classifier Given a phrase in the source language, the DTP classifier extracts a set of features from it and predicts whether it is difficult or not based on its feature values." ></td>
	<td class="line x" title="50:228	We use an SVM classifier in this work." ></td>
	<td class="line x" title="51:228	We train the SVM-Light implementation of the 249 algorithm (Joachims 1999)." ></td>
	<td class="line x" title="52:228	To train the classifier, we need to tackle two challenges." ></td>
	<td class="line x" title="53:228	First, we need to develop some appropriate training data because there is no corpus with annotated DTPs." ></td>
	<td class="line x" title="54:228	Second, we need to determine a set of predictive features for the classifier." ></td>
	<td class="line x" title="55:228	Development of the Gold Standard Unlike the typical SVM training scenario, labeled training examples of DTPs do not exist." ></td>
	<td class="line x" title="56:228	Manual creation of such data requires deep understanding of the linguistics differences of source and target languages and also deep knowledge about the MT system and its training data." ></td>
	<td class="line x" title="57:228	Such resources are not accessible to us." ></td>
	<td class="line x" title="58:228	Instead, we construct the gold standard automatically." ></td>
	<td class="line x" title="59:228	We make the strong assumption that difficulty is directly correlated to translation quality and that translation quality can be approximately measured by automatic metrics such as BLEU." ></td>
	<td class="line x" title="60:228	We have two resource requirements ??a sentence-aligned parallel corpus (different from the data used to train the underlying MT system), and a syntactic parser for the source language." ></td>
	<td class="line x" title="61:228	The procedure for creating the gold standard data is as follows: 1." ></td>
	<td class="line x" title="62:228	Each source sentence is parsed." ></td>
	<td class="line x" title="63:228	2." ></td>
	<td class="line x" title="64:228	Phrase translations are extracted from the parallel corpus." ></td>
	<td class="line x" title="65:228	Specifically, we generate wordalignments using GIZA++ (Och 2001) in both directions and combine them using the refined methodology (Och and Ney 2003), and then we applied Koehn?s toolkit (2004) to extract parallel phrases." ></td>
	<td class="line x" title="66:228	We have relaxed the length constraints of the toolkit to ensure the extraction of long phrases (as long as 16 words)." ></td>
	<td class="line x" title="67:228	3." ></td>
	<td class="line x" title="68:228	Parallel phrases whose source parts are not well-formed constituents are filtered out." ></td>
	<td class="line x" title="69:228	4." ></td>
	<td class="line x" title="70:228	The source phrases are translated by the underlying MT system, and a baseline BLEU score is computed over this set of MT outputs." ></td>
	<td class="line x" title="71:228	5." ></td>
	<td class="line x" title="72:228	To label each source phrase, we remove that phrase and its translation from the MT output and calculate the set?s new BLEU score." ></td>
	<td class="line x" title="73:228	If new-score is greater than the baseline score by some threshold value (a tunable parameter), we label the phrase as difficult, otherwise we label it as not difficult." ></td>
	<td class="line x" title="74:228	Rather than directly calculating the BLEU score for each phrase, we performed the round-robin procedure described in steps 4 and 5 because BLEU is not reliable for short phrases." ></td>
	<td class="line x" title="75:228	BLEU is calculated as a geometric mean over n-gram matches with references, assigning a score of zero to an entire phrase if no higher-ordered n-gram matches were found against the references." ></td>
	<td class="line x" title="76:228	However, some phrases with a score of 0 might have more matches in the lower-ordered n-grams than other phrases (and thus ought to be considered ?easier??." ></td>
	<td class="line x" title="77:228	A comparison of the relative changes in BLEU scores while holding out a phrase from the corpus gives us a more sensitive measurement than directly computing BLEU for each phrase." ></td>
	<td class="line x" title="78:228	Features By analyzing the training corpus, we have found 18 features that are indicative of DTPs." ></td>
	<td class="line x" title="79:228	Some phrase-level feature values are computed as an average of the feature values of the individual words." ></td>
	<td class="line x" title="80:228	The following first four features use some probabilities that are collected from a parallel data and word alignments." ></td>
	<td class="line x" title="81:228	Such a resource does not exist at the time of testing." ></td>
	<td class="line x" title="82:228	Instead we use the history of the source words (estimated from the large parallel corpus) to predict the feature value." ></td>
	<td class="line x" title="83:228	(I) Average probability of word alignment crossings: word alignment crossings are indicative of word order differences and generally structural difference across two languages." ></td>
	<td class="line x" title="84:228	We collect word alignment crossing statistics from the training corpus to estimate the crossing probability for each word in a new source phrase." ></td>
	<td class="line x" title="85:228	For example the Arabic word rhl has 67% probability of alignment crossing (word movement across English)." ></td>
	<td class="line x" title="86:228	These probabilities are then averaged into one value for the entire phrase." ></td>
	<td class="line x" title="87:228	(II) Average probability of translation ambiguity: words that have multiple equally-likely translations contribute to translation ambiguity." ></td>
	<td class="line x" title="88:228	For example a word that has 4 different translations with similar frequencies tends to be more ambiguous than a word that has one dominant translation." ></td>
	<td class="line x" title="89:228	We collect statistics about the lexical translational ambiguities from the training corpus and lexical translation tables and use them to predict the ambiguity of each word in a new source phrase." ></td>
	<td class="line x" title="90:228	The score for the phrase is the average of the scores for the individual words." ></td>
	<td class="line x" title="91:228	(III) Average probability of POS tag changes: Change of a word?s POS tagging is an indication of deep structural differences between the source phrase and the target phrase." ></td>
	<td class="line x" title="92:228	Using the POS tagging information for both sides of the training corpus, we learn the probability that each source word?s POS gets changed after the translation." ></td>
	<td class="line x" title="93:228	To 250 overcome data sparseness, we only look at the collapsed version of POS tags on both sides of the corpus." ></td>
	<td class="line x" title="94:228	The phrase?s score is the average the individual word probabilities." ></td>
	<td class="line x" title="95:228	(IV) Average probability of null alignments: In many cases null alignments of the source words are indicative of the weakness of information about the word." ></td>
	<td class="line x" title="96:228	This feature is similar to average ambiguity probability." ></td>
	<td class="line x" title="97:228	The difference is that we use the probability of null alignments instead of lexical probabilities." ></td>
	<td class="line x" title="98:228	(V-IX) Normalized number of unknown words, content words, numbers, punctuations: For each of these features we normalize the count (e.g.: unknown words) with the length of the phrase." ></td>
	<td class="line x" title="99:228	The normalization of the features helps the classifier to not have length preference for the phrases." ></td>
	<td class="line x" title="100:228	(X) Number of proper nouns: Named entities tend to create translation difficulty, due to their diversity of spellings and also domain differences." ></td>
	<td class="line x" title="101:228	We use the number of proper nouns to estimate the occurrence of the named entities in the phrase." ></td>
	<td class="line x" title="102:228	(XI Depth of the subtree: The feature is used as a measure of syntactic complexity of the phrase." ></td>
	<td class="line x" title="103:228	For example continuous right branching of the parse tree which adds to the depth of the subtree can be indicative of a complex or ambiguous structure that might be difficult to translate." ></td>
	<td class="line x" title="104:228	(XII) Constituency type of the phrase: We observe that the different types of constituents have varied effects on the translations of the phrase." ></td>
	<td class="line x" title="105:228	For example prepositional phrases tend to belong to difficult phrases." ></td>
	<td class="line x" title="106:228	(XIII) Constituency type of the parent phrase (XIV) Constituency types of the children nodes of the phrase: We form a set from the children nodes of the phrase (on the parse tree)." ></td>
	<td class="line x" title="107:228	(XV) Length of the phrase: The feature is based on the number of the words in the phrase." ></td>
	<td class="line x" title="108:228	(XVI) Proportional length of the phrase: The proportion of the length of the phrase to the length of the sentence." ></td>
	<td class="line x" title="109:228	As this proportion gets larger, the contextual effect on the translation of the phrase becomes less." ></td>
	<td class="line x" title="110:228	(XVII) Distance from the start of the sentence and: Phrases that are further away from the start of the sentence tend to not be translated as well due to compounding translational errors." ></td>
	<td class="line x" title="111:228	(XVIII) Distance from a learned translation phrase: The feature measure the number of words before reaching a learned phrase." ></td>
	<td class="line x" title="112:228	In other words it s an indication of the level of error that is introduced in the early parts of the phrase translation." ></td>
	<td class="line x" title="113:228	3.2 Constraining the MT System Once human translations have been obtained for the DTPs, we want the MT system to only consider output candidates that contain the human translations." ></td>
	<td class="line x" title="114:228	The additional knowledge can be used by the phrase-based system without any code modification." ></td>
	<td class="line x" title="115:228	Figure 2 shows the data-flow for this process." ></td>
	<td class="line x" title="116:228	First, we append the pre-trained phrase-translation table with the DTPs and their human translations with a probability of 1.0." ></td>
	<td class="line x" title="117:228	We also include the human translations for the DTPs as training data for the language model to ensure that the phrase vocabulary is familiar to the decoder and relax the phrase distortion parameter that the decoder can include all phrase translations with any length in the decoding." ></td>
	<td class="line x" title="118:228	Thus, candidates that contain the human translations for the DTPs will score higher and be chosen by the decoder." ></td>
	<td class="line x" title="119:228	Figure 2: Human translations for the DTPs can be incorporated into the MT system?s phrase table and language model." ></td>
	<td class="line x" title="120:228	4 Experiments The goal of these four experiments is to gain a better understanding of the DTPs and their impact on the translation process." ></td>
	<td class="line x" title="121:228	All our studies are conducted for Arabic-to-English MT. We formed a one-million word parallel text out of two corpora released by the Linguistic Data Consortium: Ara251 bic News Translation Text Part 1 and Arabic English Parallel News Part 1." ></td>
	<td class="line x" title="122:228	The majority of the data was used to train the underlying phrase-based MT system." ></td>
	<td class="line x" title="123:228	We reserve 2000 sentences for development and experimentation." ></td>
	<td class="line x" title="124:228	Half of these are used for the training and evaluation of the DTP classifier (Sections 4.1 and 4.2); the other half is used for translation experiments on the rest of the framework (Sections 4.3 and 4.4)." ></td>
	<td class="line x" title="125:228	In both cases, translation phrases are extracted from the sentences and assigned ?gold standard??" ></td>
	<td class="line x" title="126:228	labels according to the procedure described in Section 3.1." ></td>
	<td class="line x" title="127:228	It is necessary to keep two separate datasets because the later experiments make use of the trained DTP classifier." ></td>
	<td class="line x" title="128:228	For the two translation experiments, we also face a practical obstacle: we do not have an army of human translators at our disposal to translate the identified phrases." ></td>
	<td class="line x" title="129:228	To make the studies possible, we rely on a pre-translated parallel corpus to simulate the process of asking a human to translate a phrase." ></td>
	<td class="line x" title="130:228	That is, we use the phrase extraction toolkit to find translation phrases corresponding to each DTP candidate (note that the data used for this experiment is separate from the main parallel corpus used to train the MT system, so the system has no knowledge about these translations)." ></td>
	<td class="line x" title="131:228	4.1 Automatic Labeling of DTP In this first experiment, we verify whether our method for creating positive and negative labeled examples of DTPs (as described in Section 3.1) is sound." ></td>
	<td class="line x" title="132:228	Out of 2013 extracted phrases, we found 949 positive instances (DTPs) and 1064 negative instances." ></td>
	<td class="line x" title="133:228	The difficult phrases have an average length of 8.8 words while the other phrases have an average length of 7.8 words1." ></td>
	<td class="line x" title="134:228	We measured the BLEU scores for the MT outputs for both groups of phrases (Table 1)." ></td>
	<td class="line x" title="135:228	Experiment BLEU Score DTPs 14.34 Non-DTPs 61.22 Table 1: Isolated Translation of the selected training phrases The large gap between the translation qualities of the two phrase groups suggests that the DTPs are indeed much more ?difficult??than the other phrases." ></td>
	<td class="line x" title="136:228	1 Arabic words are tokenized and lemmatized by Diab?s Arabic Toolset (Diab 2004)." ></td>
	<td class="line x" title="137:228	4.2 Evaluation of the DTP Classifier We now perform a local evaluation of the trained DTP classifier for its classification accuracy." ></td>
	<td class="line x" title="138:228	The classifier is trained as an SVM using a linear kernel." ></td>
	<td class="line x" title="139:228	The ?gold standard??phrases from the section 4.1 are split into three groups: 2013 instances are used as training data for the classifier; 100 instances are used for development (e.g. , parameter tuning and feature engineering); and 200 instances are used as test instances." ></td>
	<td class="line x" title="140:228	The test set has an equal number of difficult and non-difficult phrases (50% baseline accuracy)." ></td>
	<td class="line x" title="141:228	In order to optimize the accuracy of classification, we used a development set for feature engineering and trying various SVM kernels and associated parameters." ></td>
	<td class="line x" title="142:228	For the feature engineering part, we used the all-but-one heuristic to test the contribution of each individual feature." ></td>
	<td class="line x" title="143:228	Table 2 presents the most and least contributing four features that we used in our classification." ></td>
	<td class="line x" title="144:228	Among various features, we observed that the syntactic features are the most contributing sources of information for our classification." ></td>
	<td class="line x" title="145:228	Least Useful Features Most Useful Features Ft1: Align Crossing Ft 2: Lexical Ambiguity Ft 8: Count of Nums Ft 11: Depth of subtree Ft:9: Count of Puncs Ft 12: Const type of Phr Ft 10: Count of NNPs Ft 13: Const type of Par Table 2: The most and least useful features The DTP classifier achieves an average accuracy of 71.5%, using 10 fold cross validation on the test set." ></td>
	<td class="line x" title="146:228	4.3 Study on the effect of DTPs This experiment concentrates on the second half of the framework: that of constraining the MT system to use human-translations for the DTPs." ></td>
	<td class="line x" title="147:228	Our objective is to assess to what degree do the DTPs negatively impact the MT process." ></td>
	<td class="line x" title="148:228	We compare the MT outputs of two groups of sentences." ></td>
	<td class="line x" title="149:228	Group I is made up of 242 sentences that contain the most difficult to translate phrases in the 1000 sentences we reserved for this study." ></td>
	<td class="line x" title="150:228	Group II is a control group made up of 242 sentences with the least difficult to translate phrases." ></td>
	<td class="line x" title="151:228	The DTPs make up about 9% of word counts in the above 484 sentences." ></td>
	<td class="line x" title="152:228	We follow the procedure described in Section 3.1 to identify and score all the phrases; thus, 252 this experiment can be considered an oracle study." ></td>
	<td class="line x" title="153:228	We compare four scenarios: 1." ></td>
	<td class="line x" title="154:228	Adding phrase translations for Group I: MT system is constrained using the method described in Section 3.2 to incorporate human translations of the pre-identified DTPs in Group I.2 2." ></td>
	<td class="line x" title="155:228	Adding phrase translations for Group II: MT system is constrained to use human translations for the identified (non-difficult) phrases in Group II." ></td>
	<td class="line x" title="156:228	3." ></td>
	<td class="line x" title="157:228	Adding translations for random phrases: randomly replace 242 phrases from either Group I or Group II." ></td>
	<td class="line x" title="158:228	4." ></td>
	<td class="line x" title="159:228	Adding translations for classifier labeled DTPs: human translations for phrases that our trained classifier has identified as DTPs from both Group I and Group II." ></td>
	<td class="line x" title="160:228	All of the above scenarios are evaluated on a combined set of 484 sentences (group 1 + group 2)." ></td>
	<td class="line x" title="161:228	This set up normalizes the relative difficulty of each grouping." ></td>
	<td class="line x" title="162:228	If the DTPs negatively impact the MT process, we would expect to see a greater improvement when Group I phrases are translated by humans than when Group II phrases are translated by humans." ></td>
	<td class="line x" title="163:228	The baseline for the comparisons is to evaluate the outputs of the MT system without using any human translations." ></td>
	<td class="line x" title="164:228	This results in a BLEU score of 24.0." ></td>
	<td class="line x" title="165:228	When human translations are used, the BLEU score of the dataset increases, as shown in Table 3." ></td>
	<td class="line x" title="166:228	Experiment BLEU Baseline (no human trans) 24.0 w/ translated DTPs (Group I) 39.6 w/ translated non-DTPs (Group II) 33.7 w/ translated phrases (random) 35.1 w/ translated phrases (classifier) 37.0 Table 3: A comparison of BLEU scores for the entire set of sentences under the constraints of using human translations for different types of phrases." ></td>
	<td class="line x" title="167:228	While it is unsurprising that the inclusion of human translations increases the overall BLEU score, this comparison shows that the boost is sharper when more DTPs are translated." ></td>
	<td class="line x" title="168:228	This is 2 In this study, because the sentences are from the training parallel corpus, we can extract human translations directly from the corpus." ></td>
	<td class="line x" title="169:228	consistent with our conjecture that pre-translating difficult phrases may be helpful." ></td>
	<td class="line x" title="170:228	A more interesting question is whether the human translations still provide any benefit once we factor out their direct contributions to the increase in BLEU scores." ></td>
	<td class="line x" title="171:228	To answer this question, we compute the BLEU scores for the outputs again, this time filtering out all 484 identified phrases from the evaluation." ></td>
	<td class="line x" title="172:228	In other words in this experiment we focus on the part of the sentence that is not labeled and does include any human translations." ></td>
	<td class="line x" title="173:228	Table 4 presents the results." ></td>
	<td class="line x" title="174:228	Experiment BLEU Baseline (no human trans) 23.0 w/ translated DTPs (Group I) 25.4 w/ translated non-DTPs (Group II) 23.9 w/ translated phrases (random) 24.5 w/ translated phrases (classifier) 25.1 Table 4: BLEU scores for the translation outputs excluding the 484 (DTP and non-DTP) phrases." ></td>
	<td class="line x" title="175:228	The largest gain (2.4 BLEU increment from baseline) occurs when all and only the DTPs were translated." ></td>
	<td class="line x" title="176:228	In contrast, replacing phrases from Group II did not improve the BLEU score very much." ></td>
	<td class="line x" title="177:228	These results suggest that better handling of DTPs will have a positive effect on the overall MT process." ></td>
	<td class="line x" title="178:228	We also note that using our SVM-trained classifier to identify the DTPs, the constrained MT system?s outputs obtained a BLEU score that is nearly as high as if a perfect classifier was used." ></td>
	<td class="line x" title="179:228	4.4 Full evaluation of the framework This final experiment evaluates the complete framework as described in Section 3." ></td>
	<td class="line x" title="180:228	The setup of this study is similar to that of the previous section." ></td>
	<td class="line x" title="181:228	The main difference is that now, we rely on the classifier to predict which phrase would be the most difficult to translate and use human translations for those phrases." ></td>
	<td class="line x" title="182:228	Out of 1000 sentences, 356 have been identified to contain DTPs (that are in the phrase extraction list)." ></td>
	<td class="line x" title="183:228	In other words, only 356 sentences hold DTPs that we can find their human translations through phrase projection." ></td>
	<td class="line x" title="184:228	For the remaining sentences, we do not use any human translation." ></td>
	<td class="line x" title="185:228	253 Table 5 presents the increase in BLEU scores when human translations for the 356 DTPs are used." ></td>
	<td class="line x" title="186:228	As expected the BLEU score increases, but the improvement is less dramatic than in the previous experiment because most sentences are unchanged." ></td>
	<td class="line x" title="187:228	Experiment BLEU Baseline (no human trans) 24.9 w/ human translations 29.0 Table 5: Entire Corpus level evaluation (1000 sentences) when replacing DTPs in the hit list Table 6 summarizes the experimental results on the subset of the 356 sentences." ></td>
	<td class="line x" title="188:228	The first two rows compare the translation quality at the sentence level (similar to Table 3); the next two rows compare the translation quality of the non-DTP parts (similar to Table 4)." ></td>
	<td class="line x" title="189:228	Rows 1 and 3 are conditions when we do not use human translation; and rows 2 and 4 are conditions when we replace DTPs with their associated human translations." ></td>
	<td class="line x" title="190:228	The improvements of the BLEU score for the hit list are similar to the results we have previously seen." ></td>
	<td class="line x" title="191:228	Experiment on 356 sentences BLEU Baseline: full sent." ></td>
	<td class="line x" title="192:228	25.1 w/ human translation: full sent." ></td>
	<td class="line x" title="193:228	37.6 Baseline: discount DTPs 26.0 w/ human translation: discount DTPs 27.8 Table 6: Evaluation of the subset of 356 sentences: both for the full sentence and for non-DTP parts, with and without human translation replacement of DTPs." ></td>
	<td class="line x" title="194:228	5 Related Work Our work is related to the problem of confidence estimation for MT (Blatz et." ></td>
	<td class="line x" title="195:228	al. 2004; Zen and Ney 2006)." ></td>
	<td class="line x" title="196:228	The confidence measure is a score for ngrams generated by a decoder3." ></td>
	<td class="line x" title="197:228	The measure is based on the features like lexical probabilities (word posterior), phrase translation probabilities, N-best translation hypothesis, etc. Our DTP classification differs from the confidence measuring in several aspects: one of the main purposes of our classification of DTPs is to optimize the usage of outside resources." ></td>
	<td class="line x" title="198:228	To do so, we focus on classification of phrases which are syntactically meaningful, because those syntactic constituent units have 3 Most of the confidence estimation measures are for unigrams (word level measures)." ></td>
	<td class="line x" title="199:228	less dependency to the whole sentence structure and can be translated independently." ></td>
	<td class="line x" title="200:228	Our classification relies on syntactic features that are important source of information about the MT difficulty and also are useful for further error tracking (reasons behind the difficulty)." ></td>
	<td class="line x" title="201:228	Our classification is performed as a pre-translation step, so it does not rely on the output of the MT system for a test sentence; instead, it uses a parallel training corpus and the characteristics of the underlying MT system (e.g.: phrase translations, lexical probabilities)." ></td>
	<td class="line x" title="202:228	Confidence measures have been used for error correction and interactive MT systems." ></td>
	<td class="line x" title="203:228	Ueffing and Ney (2005) employed confidence measures within a trans-type-style interactive MT system." ></td>
	<td class="line x" title="204:228	In their system, the MT system iteratively generates the translation and the human translator accepts a part of the proposed translation by typing one or more prefix characters." ></td>
	<td class="line x" title="205:228	The system regenerates a new translation based on the human prefix input and word level confidence measures." ></td>
	<td class="line x" title="206:228	In contrast, our proposed usage of human knowledge is for translation at the phrase level." ></td>
	<td class="line x" title="207:228	We use syntactic restrictions to make the extracted phrases meaningful and easy to translate in isolation." ></td>
	<td class="line x" title="208:228	In other words, by the usage of our framework trans-type systems can use human knowledge at the phrase level for the most difficult segments of a sentence." ></td>
	<td class="line x" title="209:228	Additionally by the usage of our framework, the MT system performs the decoding task only once." ></td>
	<td class="line x" title="210:228	The idea of isolated phrase translation has been explored successfully in MT community." ></td>
	<td class="line x" title="211:228	Koehn and Knight (2003) used isolated translation of NP and PP phrases and merge them with the phrase based MT system to translate the complete sentence." ></td>
	<td class="line x" title="212:228	In our work, instead of focusing on specific type of phrases (NP or PP), we focus on isolated translation of difficult phrases with an aim to improve the translation quality of non-difficult segments too." ></td>
	<td class="line x" title="213:228	6 Conclusion and Future Work We have presented an MT framework that makes use of additional information about difficult-totranslate source phrases." ></td>
	<td class="line x" title="214:228	Our framework includes an SVM-based phrase classifier that finds the segment of a sentence that is most difficult to translate." ></td>
	<td class="line x" title="215:228	Our classifier achieves a promising 71.5% accuracy." ></td>
	<td class="line x" title="216:228	By asking external sources (such as human translators) to pre-translate these DTPs and using them to constrain the MT process, we im254 prove the system outputs for the other parts of the sentences." ></td>
	<td class="line x" title="217:228	We plan to extend this work in several directions." ></td>
	<td class="line x" title="218:228	First, our framework can be augmented to include multiple MT systems." ></td>
	<td class="line x" title="219:228	We expect different systems will have difficulties with different constructs, and thus they may support each other, and thus reducing the need to ask human translators for help with the difficult phrases." ></td>
	<td class="line x" title="220:228	Second, our current metric for phrasal difficulty depends on BLEU." ></td>
	<td class="line x" title="221:228	Considering the recent debates about the shortcomings of the BLEU score (Callison-Burch et." ></td>
	<td class="line oc" title="222:228	al. 2006), we are interested in applying alternative metrics such a Meteor (Banerjee and Lavie 2005)." ></td>
	<td class="line x" title="223:228	Third, we believe that there is more room for improvement and extension of our classification features." ></td>
	<td class="line x" title="224:228	Specifically, we believe that our syntactic analysis of source sentences can be improved by including richer parsing features." ></td>
	<td class="line x" title="225:228	Finally, the framework can also be used to diagnose recurring problems in the MT system." ></td>
	<td class="line x" title="226:228	We are currently developing methods for improving the translation of the difficult phrases for the phrase-based MT system used in our experiments." ></td>
	<td class="line x" title="227:228	Acknowledgements This work is supported by NSF Grant IIS-0612791." ></td>
	<td class="line x" title="228:228	We would like to thank Alon Lavie, Mihai Rotaru and the NLP group at Pitt as well as the anonymous reviewers for their valuable comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W07-0738
Linguistic Features for Automatic Evaluation of Heterogenous MT Systems
Giménez, Jesús;Màrquez, Lluís;"></td>
	<td class="line x" title="1:270	Proceedings of the Second Workshop on Statistical Machine Translation, pages 256??64, Prague, June 2007." ></td>
	<td class="line x" title="2:270	c2007 Association for Computational Linguistics Linguistic Features for Automatic Evaluation of Heterogenous MT Systems Jesus Gimenez and Llus M`arquez TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya Jordi Girona Salgado 1??, E-08034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract Evaluation results recently reported by Callison-Burch et al.(2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator." ></td>
	<td class="line x" title="4:270	This happens, for instance, when the systems under evaluation are based on different paradigms, and therefore, do not share the same lexicon." ></td>
	<td class="line x" title="5:270	The reason is that, while MT quality aspects are diverse, BLEU limits its scope to the lexical dimension." ></td>
	<td class="line x" title="6:270	In this work, we suggest using metrics which take into account linguistic features at more abstract levels." ></td>
	<td class="line x" title="7:270	We provide experimental results showing that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than metrics based on lexical matching alone, specially when the systems under evaluation are of a different nature." ></td>
	<td class="line x" title="8:270	1 Introduction Most metrics used in the context of Automatic Machine Translation (MT) Evaluation are based on the assumption that ?acceptable??translations tend to share the lexicon (i.e. , word forms) in a predefined set of manual reference translations." ></td>
	<td class="line x" title="9:270	This assumption works well in many cases." ></td>
	<td class="line x" title="10:270	However, several results in recent MT evaluation campaigns have cast some doubts on its general validity." ></td>
	<td class="line x" title="11:270	For instance, Callison-Burch et al.(2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al. , 2001)." ></td>
	<td class="line x" title="13:270	In particular, they noted that when the systems under evaluation are of a different nature (e.g. , rule-based vs. statistical, human-aided vs. fully automatical, etc)." ></td>
	<td class="line x" title="14:270	BLEU may not be a reliable MT quality indicator." ></td>
	<td class="line x" title="15:270	The reason is that BLEU favours MT systems which share the expected reference lexicon (e.g. , statistical systems), and penalizes those which use a different one." ></td>
	<td class="line x" title="16:270	Indeed, the underlying cause is much simpler." ></td>
	<td class="line x" title="17:270	In general, lexical similarity is nor a sufficient neither a necessary condition so that two sentences convey the same meaning." ></td>
	<td class="line x" title="18:270	On the contrary, natural languages are expressive and ambiguous at different levels." ></td>
	<td class="line x" title="19:270	Consequently, the similarity between two sentences may involve different dimensions." ></td>
	<td class="line x" title="20:270	In this work, we hypothesize that, in order to ?fairly??evaluate MT systems based on different paradigms, similarities at more abstract linguistic levels must be analyzed." ></td>
	<td class="line x" title="21:270	For that purpose, we have compiled a rich set of metrics operating at the lexical, syntactic and shallow-semantic levels (see Section 2)." ></td>
	<td class="line x" title="22:270	We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al.(2006) (see Section 3)." ></td>
	<td class="line x" title="24:270	We show that metrics based on deeper linguistic information (syntactic/shallow-semantic) are able to produce more reliable system rankings than those produced by metrics which limit their scope to the lexical dimension, specially when the systems under evaluation are of a different nature." ></td>
	<td class="line x" title="25:270	256 2 A Heterogeneous Metric Set For our experiments, we have compiled a representative set of metrics1 at different linguistic levels." ></td>
	<td class="line x" title="26:270	We have resorted to several existing metrics, and we have also developed new ones." ></td>
	<td class="line x" title="27:270	Below, we group them according to the level at which they operate." ></td>
	<td class="line x" title="28:270	2.1 Lexical Similarity Most of the current metrics operate at the lexical level." ></td>
	<td class="line x" title="29:270	We have selected 7 representatives from different families which have been shown to obtain high levels of correlation with human assessments: BLEU We use the default accumulated score up to the level of 4-grams (Papineni et al. , 2001)." ></td>
	<td class="line x" title="30:270	NIST We use the default accumulated score up to the level of 5-grams (Doddington, 2002)." ></td>
	<td class="line x" title="31:270	GTM We set to 1 the value of the e parameter (Melamed et al. , 2003)." ></td>
	<td class="line o" title="32:270	METEOR We run all modules: ?exact??" ></td>
	<td class="line x" title="33:270	?porterstem??" ></td>
	<td class="line x" title="34:270	?wn stem??and ?wn synonymy??" ></td>
	<td class="line oc" title="35:270	in that order (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="36:270	ROUGE We used the ROUGE-S* variant (skip bigrams with no max-gap-length)." ></td>
	<td class="line x" title="37:270	Stemming is enabled (Lin and Och, 2004a)." ></td>
	<td class="line x" title="38:270	mWER We use 1 ??mWER (Nieen et al. , 2000)." ></td>
	<td class="line x" title="39:270	mPER We use 1 ??mPER (Tillmann et al. , 1997)." ></td>
	<td class="line o" title="40:270	Let us note that ROUGE and METEOR may consider stemming (i.e. , morphological variations)." ></td>
	<td class="line o" title="41:270	Additionally, METEOR may perform a lookup for synonyms in WordNet (Fellbaum, 1998)." ></td>
	<td class="line x" title="42:270	2.2 Beyond Lexical Similarity Modeling linguistic features at levels further than the lexical level requires the usage of more complex linguistic structures." ></td>
	<td class="line x" title="43:270	We have defined what we call ?linguistic elements??(LEs)." ></td>
	<td class="line x" title="44:270	2.2.1 Linguistic Elements LEs are linguistic units, structures, or relationships, such that a sentence may be partially seen as a ?bag??of LEs." ></td>
	<td class="line x" title="45:270	Possible kinds of LEs are: word forms, parts-of-speech, dependency relationships, syntactic phrases, named entities, semantic roles, etc. Each 1All metrics used in this work are publicly available inside the IQMT Framework (Gimenez and Amigo, 2006)." ></td>
	<td class="line x" title="46:270	http:// www.lsi.upc.edu/?nlp/IQMT LE may consist, in its turn, of one or more LEs, which we call ?items??inside the LE." ></td>
	<td class="line x" title="47:270	For instance, a ?phrase??LE may consist of ?phrase??items, ?part-ofspeech??(PoS) items, ?word form??items, etc. Items may be also combinations of LEs." ></td>
	<td class="line x" title="48:270	For instance, a ?phrase??LE may be seen as a sequence of ?wordform:PoS??items." ></td>
	<td class="line x" title="49:270	2.2.2 Similarity Measures We are interested in comparing linguistic structures, and linguistic units." ></td>
	<td class="line x" title="50:270	LEs allow for comparisons at different granularity levels, and from different viewpoints." ></td>
	<td class="line x" title="51:270	For instance, we might compare the semantic structure of two sentences (i.e. , which actions, semantic arguments and adjuncts exist) or we might compare lexical units according to the semantic role they play inside the sentence." ></td>
	<td class="line x" title="52:270	For that purpose, we use two very simple kinds of similarity measures over LEs: ?Overlapping??and ?Matching??" ></td>
	<td class="line x" title="53:270	We provide a general definition: Overlapping between items inside LEs, according to their type." ></td>
	<td class="line x" title="54:270	Formally: Overlapping(t) = X i?itemst(hyp) countprimehyp(i,t) X i?itemst(ref) countref(i,t) where t is the LE type2, itemst(s) refers to the set of items occurring inside LEs of type t in sentence s, countref(i,t) denotes the number of times item i appears in the reference translation inside a LE of type t, and countprimehyp(i,t) denotes the number of times i appears in the candidate translation inside a LE of type t, limited by the number of times i appears in the reference translation inside a LE of type t. Thus, ?Overlapping??provides a rough measure of the proportion of items inside elements of a certain type which have been ?successfully??translated." ></td>
	<td class="line x" title="55:270	We also introduce a coarser metric, ?Overlapping(*)??" ></td>
	<td class="line x" title="56:270	which considers the uniformly averaged ?overlapping??over all types: Overlapping(star) = 1|T| X t?T Overlapping(t) where T is the set of types." ></td>
	<td class="line x" title="57:270	2LE types vary according to the specific LE class." ></td>
	<td class="line x" title="58:270	For instance, in the case of Named Entities types may be ?PER??(i.e. , person), ?LOC??(i.e. , location), ?ORG??(i.e. , organization), etc. 257 Matching between items inside LEs, according to their type." ></td>
	<td class="line x" title="59:270	Its definition is analogous to the ?Overlapping??definition, but in this case the relative order of the items is important." ></td>
	<td class="line x" title="60:270	All items inside the same element are considered as a single unit (i.e. , a sequence in left-to-right order)." ></td>
	<td class="line x" title="61:270	In other words, we are computing the proportion of ?fully??translated elements, according to their type." ></td>
	<td class="line x" title="62:270	We also introduce a coarser metric, ?Matching(*)??" ></td>
	<td class="line x" title="63:270	which considers the uniformly averaged ?Matching??over all types." ></td>
	<td class="line x" title="64:270	notes: ???Overlapping??and ?Matching??operate on the assumption of a single reference translation." ></td>
	<td class="line x" title="65:270	The extension to the multi-reference setting is computed by assigning the maximum value attained over all human references individually." ></td>
	<td class="line x" title="66:270	???Overlapping??and ?Matching??are general metrics." ></td>
	<td class="line x" title="67:270	We may apply them to specific scenarios by defining the class of linguistic elements and items to be used." ></td>
	<td class="line x" title="68:270	Below, we instantiate these measures over several particular cases." ></td>
	<td class="line x" title="69:270	2.3 Shallow Syntactic Similarity Metrics based on shallow parsing (?SP??" ></td>
	<td class="line x" title="70:270	analyze similarities at the level of PoS-tagging, lemmatization, and base phrase chunking." ></td>
	<td class="line x" title="71:270	Outputs and references are automatically annotated using stateof-the-art tools." ></td>
	<td class="line x" title="72:270	PoS-tagging and lemmatization are provided by the SVMTool package (Gimenez and M`arquez, 2004), and base phrase chunking is provided by the Phreco software (Carreras et al. , 2005)." ></td>
	<td class="line x" title="73:270	Tag sets for English are derived from the Penn Treebank (Marcus et al. , 1993)." ></td>
	<td class="line x" title="74:270	We instantiate ?Overlapping??over parts-of-speech and chunk types." ></td>
	<td class="line x" title="75:270	The goal is to capture the proportion of lexical items correctly translated, according to their shallow syntactic realization: SP-Op-t Lexical overlapping according to the partof-speech ?t??" ></td>
	<td class="line x" title="76:270	For instance, ?SP-Op-NN??roughly reflects the proportion of correctly translated singular nouns." ></td>
	<td class="line x" title="77:270	We also introduce a coarser metric, ?SP-Op-*??which computes average overlapping over all parts-of-speech." ></td>
	<td class="line x" title="78:270	SP-Oc-t Lexical overlapping according to the chunk type ?t??" ></td>
	<td class="line x" title="79:270	For instance, ?SP-Oc-NP??roughly reflects the successfully translated proportion of noun phrases." ></td>
	<td class="line x" title="80:270	We also introduce a coarser metric, ?SP-Oc-*??which considers the average overlapping over all chunk types." ></td>
	<td class="line x" title="81:270	At a more abstract level, we use the NIST metric (Doddington, 2002) to compute accumulated/individual scores over sequences of: Lemmas ??SP-NIST(i)l-n Parts-of-speech ??SP-NIST(i)p-n Base phrase chunks ??SP-NIST(i)c-n For instance, ?SP-NISTl-5??corresponds to the accumulated NIST score for lemma n-grams up to length 5, whereas ?SP-NISTip-5??corresponds to the individual NIST score for PoS 5-grams." ></td>
	<td class="line x" title="82:270	2.4 Syntactic Similarity We have incorporated, with minor modifications, some of the syntactic metrics described by Liu and Gildea (2005) and Amigo et al.(2006) based on dependency and constituency parsing." ></td>
	<td class="line x" title="84:270	2.4.1 On Dependency Parsing (DP) ?DP??metrics capture similarities between dependency trees associated to automatic and reference translations." ></td>
	<td class="line x" title="85:270	Dependency trees are provided by the MINIPAR dependency parser (Lin, 1998)." ></td>
	<td class="line x" title="86:270	Similarities are captured from different viewpoints: DP-HWC(i)-l This metric corresponds to the HWC metric presented by Liu and Gildea (2005)." ></td>
	<td class="line x" title="87:270	All head-word chains are retrieved." ></td>
	<td class="line x" title="88:270	The fraction of matching head-word chains of a given length, ?l??" ></td>
	<td class="line x" title="89:270	is computed." ></td>
	<td class="line x" title="90:270	We have slightly modified this metric in order to distinguish three different variants according to the type of items headword chains may consist of: Lexical forms ??DP-HWC(i)w-l Grammatical categories ??DP-HWC(i)c-l Grammatical relationships ??DP-HWC(i)r-l Average accumulated scores up to a given chain length may be used as well." ></td>
	<td class="line x" title="91:270	For instance, ?DP-HWCiw-4??retrieves the proportion of matching length-4 word-chains, whereas ?DP-HWCw4??retrieves average accumulated proportion of matching word-chains up to length-4." ></td>
	<td class="line x" title="92:270	Analogously, ?DP-HWCc-4??" ></td>
	<td class="line x" title="93:270	and ?DP-HWCr-4??com258 pute average accumulated proportion of category/relationship chains up to length-4." ></td>
	<td class="line x" title="94:270	DP-Ol|Oc|Or These metrics correspond exactly to the LEVEL, GRAM and TREE metrics introduced by Amigo et al.(2006)." ></td>
	<td class="line x" title="96:270	DP-Ol-l Overlapping between words hanging at level ?l??" ></td>
	<td class="line x" title="97:270	or deeper." ></td>
	<td class="line x" title="98:270	DP-Oc-t Overlapping between words directly hanging from terminal nodes (i.e. grammatical categories) of type ?t??" ></td>
	<td class="line x" title="99:270	DP-Or-t Overlapping between words ruled by non-terminal nodes (i.e. grammatical relationships) of type ?t??" ></td>
	<td class="line x" title="100:270	Node types are determined by grammatical categories and relationships defined by MINIPAR." ></td>
	<td class="line x" title="101:270	For instance, ?DP-Or-s??reflects lexical overlapping between subtrees of type ?s??(subject)." ></td>
	<td class="line x" title="102:270	?DPOc-A??reflects lexical overlapping between terminal nodes of type ?A??(Adjective/Adverbs)." ></td>
	<td class="line x" title="103:270	?DP-Ol-4??reflects lexical overlapping between nodes hanging at level 4 or deeper." ></td>
	<td class="line x" title="104:270	Additionally, we consider three coarser metrics (?DP-Ol*??" ></td>
	<td class="line x" title="105:270	?DP-Oc-*??and ?DP-Or-*??" ></td>
	<td class="line x" title="106:270	which correspond to the uniformly averaged values over all levels, categories, and relationships, respectively." ></td>
	<td class="line x" title="107:270	2.4.2 On Constituency Parsing (CP) ?CP??metrics capture similarities between constituency parse trees associated to automatic and reference translations." ></td>
	<td class="line x" title="108:270	Constituency trees are provided by the Charniak-Johnson?s Max-Ent reranking parser (Charniak and Johnson, 2005)." ></td>
	<td class="line x" title="109:270	CP-STM(i)-l This metric corresponds to the STM metric presented by Liu and Gildea (2005)." ></td>
	<td class="line x" title="110:270	All syntactic subpaths in the candidate and the reference trees are retrieved." ></td>
	<td class="line x" title="111:270	The fraction of matching subpaths of a given length, ?l??" ></td>
	<td class="line x" title="112:270	is computed." ></td>
	<td class="line x" title="113:270	For instance, ?CP-STMi-5??retrieves the proportion of length-5 matching subpaths." ></td>
	<td class="line x" title="114:270	Average accumulated scores may be computed as well." ></td>
	<td class="line x" title="115:270	For instance, ?CP-STM-9??retrieves average accumulated proportion of matching subpaths up to length-9." ></td>
	<td class="line x" title="116:270	2.5 Shallow-Semantic Similarity We have designed two new families of metrics, ?NE??" ></td>
	<td class="line x" title="117:270	and ?SR??" ></td>
	<td class="line x" title="118:270	which are intended to capture similarities over Named Entities (NEs) and Semantic Roles (SRs), respectively." ></td>
	<td class="line x" title="119:270	2.5.1 On Named Entities (NE) ?NE??metrics analyze similarities between automatic and reference translations by comparing the NEs which occur in them." ></td>
	<td class="line x" title="120:270	Sentences are automatically annotated using the BIOS package (Surdeanu et al. , 2005)." ></td>
	<td class="line x" title="121:270	BIOS requires at the input shallow parsed text, which is obtained as described in Section 2.3." ></td>
	<td class="line x" title="122:270	See the list of NE types in Table 1." ></td>
	<td class="line x" title="123:270	Type Description ORG Organization PER Person LOC Location MISC Miscellaneous O Not-a-NE DATE Temporal expressions NUM Numerical expressions ANGLE QUANTITY DISTANCE QUANTITY SIZE QUANTITY Quantities SPEED QUANTITY TEMPERATURE QUANTITY WEIGHT QUANTITY METHOD MONEY LANGUAGE Other PERCENT PROJECT SYSTEM Table 1: Named Entity types." ></td>
	<td class="line x" title="124:270	We define two types of metrics: NE-Oe-t Lexical overlapping between NEs according to their type t. For instance, ?NE-Oe-PER??reflects lexical overlapping between NEs of type ?PER??(i.e. , person), which provides a rough estimate of the successfully translated proportion of person names." ></td>
	<td class="line x" title="125:270	The ?NE-Oe-*??metric considers the average lexical overlapping over all NE types." ></td>
	<td class="line x" title="126:270	This metric includes the NE type ?O??" ></td>
	<td class="line x" title="127:270	(i.e., Not-a-NE)." ></td>
	<td class="line x" title="128:270	We introduce another variant, ?NE-Oe-**??" ></td>
	<td class="line x" title="129:270	which considers only actual NEs." ></td>
	<td class="line x" title="130:270	NE-Me-t Lexical matching between NEs according to their type t. For instance, ?NE-Me-LOC??" ></td>
	<td class="line x" title="131:270	reflects the proportion of fully translated NEs of type ?LOC??(i.e. , location)." ></td>
	<td class="line x" title="132:270	The ?NE-Me-*??" ></td>
	<td class="line x" title="133:270	259 metric considers the average lexical matching over all NE types, this time excluding type ?O??" ></td>
	<td class="line x" title="134:270	Other authors have measured MT quality over NEs in the recent literature." ></td>
	<td class="line x" title="135:270	In particular, the ?NEMe-*??metric is similar to the ?NEE??metric defined by Reeder et al.(2001)." ></td>
	<td class="line x" title="137:270	2.5.2 On Semantic Roles (SR) ?SR??metrics analyze similarities between automatic and reference translations by comparing the SRs (i.e. , arguments and adjuncts) which occur in them." ></td>
	<td class="line x" title="138:270	Sentences are automatically annotated using the SwiRL package (M`arquez et al. , 2005)." ></td>
	<td class="line x" title="139:270	This package requires at the input shallow parsed text enriched with NEs, which is obtained as described in Section 2.5.1." ></td>
	<td class="line x" title="140:270	See the list of SR types in Table 2." ></td>
	<td class="line x" title="141:270	Type Description A0 A1 A2 arguments associated with a verb predicate, A3 defined in the PropBank Frames scheme." ></td>
	<td class="line x" title="142:270	A4 A5 AA Causative agent AM-ADV Adverbial (general-purpose) adjunct AM-CAU Causal adjunct AM-DIR Directional adjunct AM-DIS Discourse marker AM-EXT Extent adjunct AM-LOC Locative adjunct AM-MNR Manner adjunct AM-MOD Modal adjunct AM-NEG Negation marker AM-PNC Purpose and reason adjunct AM-PRD Predication adjunct AM-REC Reciprocal adjunct AM-TMP Temporal adjunct Table 2: Semantic Roles." ></td>
	<td class="line x" title="143:270	We define three types of metrics: SR-Or-t Lexical overlapping between SRs according to their type t. For instance, ?SR-Or-A0??reflects lexical overlapping between ?A0??arguments." ></td>
	<td class="line x" title="144:270	?SR-Or-*??considers the average lexical overlapping over all SR types." ></td>
	<td class="line x" title="145:270	SR-Mr-t Lexical matching between SRs according to their type t. For instance, the metric ?SR-Mr-AM-MOD??reflects the proportion of fully translated modal adjuncts." ></td>
	<td class="line x" title="146:270	The ?SR-Mr-*??" ></td>
	<td class="line x" title="147:270	metric considers the average lexical matching over all SR types." ></td>
	<td class="line x" title="148:270	SR-Or This metric reflects ?role overlapping??" ></td>
	<td class="line x" title="149:270	i.e overlapping between semantic roles independently from their lexical realization." ></td>
	<td class="line x" title="150:270	Note that in the same sentence several verbs, with their respective SRs, may co-occur." ></td>
	<td class="line x" title="151:270	However, the metrics described above do not distinguish between SRs associated to different verbs." ></td>
	<td class="line x" title="152:270	In order to account for such a distinction we introduce a more restrictive version of these metrics (?SR-Mrv-t??" ></td>
	<td class="line x" title="153:270	?SR-Orv-t??" ></td>
	<td class="line x" title="154:270	?SR-Mrv-*??" ></td>
	<td class="line x" title="155:270	?SR-Orv-*??" ></td>
	<td class="line x" title="156:270	and ?SR-Orv??, which require SRs to be associated to the same verb." ></td>
	<td class="line x" title="157:270	3 Experimental Work In this section, we study the behavior of some of the metrics described in Section 2, according to the linguistic level at which they operate." ></td>
	<td class="line x" title="158:270	We have selected a set of coarse-grained metric variants (i.e. , accumulated/average scores over linguistic units and structures of different kinds)3." ></td>
	<td class="line x" title="159:270	We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al.(2006)." ></td>
	<td class="line x" title="161:270	We distinguish different evaluation contexts." ></td>
	<td class="line x" title="162:270	In Section 3.1, we study the case of a single reference translation being available." ></td>
	<td class="line x" title="163:270	In principle, this scenario should diminish the reliability of metrics based on lexical matching alone, and favour metrics based on deeper linguistic features." ></td>
	<td class="line x" title="164:270	In Section 3.2, we study the case of several reference translations available." ></td>
	<td class="line x" title="165:270	This scenario should alleviate the deficiencies caused by the shallowness of metrics based on lexical matching." ></td>
	<td class="line x" title="166:270	We also analyze separately the case of ?homogeneous??systems (i.e. , all systems being of the same nature), and the case of ?heterogenous??systems (i.e. , there exist systems based on different paradigms)." ></td>
	<td class="line x" title="167:270	As to the metric meta-evaluation criterion, the two most prominent criteria are: Human Acceptability Metrics are evaluated on the basis of correlation with human evaluators." ></td>
	<td class="line x" title="168:270	Human Likeness Metrics are evaluated in terms of descriptive power, i.e., their ability to distinguish between human and automatic translations (Lin and Och, 2004b; Amigo et al. , 2005)." ></td>
	<td class="line x" title="169:270	In our case, metrics are evaluated on the basis of ?Human Acceptability??" ></td>
	<td class="line x" title="170:270	Specifically, we use Pearson correlation coefficients between metric scores 3When computing ?lexical??overlapping/matching, we use lemmas instead of word forms." ></td>
	<td class="line x" title="171:270	260 and the average sum of adequacy and fluency assessments at the document level." ></td>
	<td class="line x" title="172:270	The reason is that meta-evaluation based on ?Human Likeness??requires the availability of heterogenous test beds (i.e. , representative sets of automatic outputs and human references), which, unfortunately, is not the case of all the tasks under study." ></td>
	<td class="line x" title="173:270	First, because most translation systems are statistical." ></td>
	<td class="line x" title="174:270	Second, because in most cases only one reference translation is available." ></td>
	<td class="line x" title="175:270	3.1 Single-reference Scenario We use some of the test beds corresponding to the ?NAACL 2006 Workshop on Statistical Machine Translation??(WMT 2006) (Koehn and Monz, 2006)." ></td>
	<td class="line x" title="176:270	Since linguistic features described in Section 2 are so far implemented only for the case of English being the target language, among the 12 translation tasks available, we studied only the 6 tasks corresponding to the Foreign-to-English direction." ></td>
	<td class="line x" title="177:270	A single reference translation is available." ></td>
	<td class="line x" title="178:270	System outputs consist of 2000 and 1064 sentences for the ?indomain??and ?out-of-domain??test beds, respectively." ></td>
	<td class="line x" title="179:270	In each case, human assessments on adequacy and fluency are available for a subset of systems and sentences." ></td>
	<td class="line x" title="180:270	Table 3 shows the number of sentences assessed in each case." ></td>
	<td class="line x" title="181:270	Each sentence was evaluated by two different human judges." ></td>
	<td class="line x" title="182:270	System scores have been obtained by averaging over all sentence scores." ></td>
	<td class="line x" title="183:270	in out sys French-to-English 2,247 1,274 11/14 German-to-English 2,401 1,535 10/12 Spanish-to-English 1,944 1,070 11/15 Table 3: WMT 2006." ></td>
	<td class="line x" title="184:270	?in??and ?out??columns show the number of sentences assessed for the ?indomain??and ?out-of-domain??subtasks." ></td>
	<td class="line x" title="185:270	The ?sys??" ></td>
	<td class="line x" title="186:270	column shows the number of systems counting on human assessments with respect to the total number of systems which presented to each task." ></td>
	<td class="line x" title="187:270	Evaluation of Heterogeneous Systems In four of the six translation tasks under study, all the systems are statistical except ?Systran??" ></td>
	<td class="line x" title="188:270	which is rule-based." ></td>
	<td class="line x" title="189:270	This is the case of the German/Frenchto-English in-domain/out-of-domain tasks." ></td>
	<td class="line x" title="190:270	Table 4 shows correlation with human assessments for some metric representatives at different linguistic levels." ></td>
	<td class="line o" title="191:270	fr2en de2en Level Metric in out in out 1-PER 0.73 0.64 0.57 0.46 1-WER 0.73 0.73 0.32 0.38 BLEU 0.71 0.87 0.60 0.67 Lexical NIST 0.74 0.82 0.56 0.63 GTM 0.84 0.86 0.12 0.70 METEOR 0.92 0.95 0.76 0.81 ROUGE 0.85 0.89 0.65 0.79 SP-Op-* 0.81 0.88 0.64 0.71 SP-Oc-* 0.81 0.89 0.65 0.75 Shallow SP-NISTl-5 0.75 0.81 0.56 0.64 Syntactic SP-NISTp-5 0.75 0.91 0.77 0.77 SP-NISTc-5 0.73 0.88 0.71 0.54 DP-HWCw-4 0.76 0.88 0.64 0.74 DP-HWCc-4 0.93 0.97 0.88 0.72 DP-HWCr-4 0.92 0.96 0.91 0.76 Syntactic DP-Ol-* 0.87 0.94 0.84 0.84 DP-Oc-* 0.91 0.95 0.88 0.87 DP-Or-* 0.87 0.97 0.91 0.88 CP-STM-9 0.93 0.95 0.93 0.87 NE-Me-* 0.80 0.79 0.93 0.63 NE-Oe-* 0.79 0.76 0.91 0.59 NE-Oe-** 0.81 0.87 0.63 0.70 SR-Mr-* 0.83 0.95 0.92 0.84 Shallow SR-Or-* 0.89 0.95 0.88 0.90 Semantic SR-Or 0.95 0.85 0.80 0.75 SR-Mrv-* 0.77 0.92 0.72 0.85 SR-Orv-* 0.81 0.93 0.76 0.94 SR-Orv 0.84 0.93 0.81 0.92 Table 4: WMT 2006." ></td>
	<td class="line x" title="192:270	Evaluation of Heterogeneous Systems." ></td>
	<td class="line x" title="193:270	French-to-English (fr2en) / German-toEnglish (de2en), in-domain and out-of-domain." ></td>
	<td class="line x" title="194:270	Although the four cases are different, we have identified several regularities." ></td>
	<td class="line o" title="195:270	For instance, BLEU and, in general, all metrics based on lexical matching alone, except METEOR, obtain significantly lower levels of correlation than metrics based on deeper linguistic similarities." ></td>
	<td class="line n" title="196:270	The problem with lexical metrics is that they are unable to capture the actual quality of the ?Systran??system." ></td>
	<td class="line p" title="197:270	Interestingly, METEOR obtains a higher correlation, which, in the case of French-to-English, rivals the top-scoring metrics based on deeper linguistic features." ></td>
	<td class="line o" title="198:270	The reason, however, does not seem to be related to its additional linguistic operations (i.e. , stemming or synonymy lookup), but rather to the METEOR matching strategy itself (unigram precision/recall)." ></td>
	<td class="line x" title="199:270	Metrics at the shallow syntactic level are in the same range of lexical metrics." ></td>
	<td class="line x" title="200:270	At the properly syntactic level, metrics obtain in most cases high correlation coefficients." ></td>
	<td class="line x" title="201:270	However, the ?DP-HWCw-4??" ></td>
	<td class="line x" title="202:270	metric, which, although from the viewpoint of de261 pendency relationships, still considers only lexical matching, obtains a lower level of correlation." ></td>
	<td class="line x" title="203:270	This reinforces the idea that metrics based on rewarding long n-grams matchings may not be a reliable quality indicator in these cases." ></td>
	<td class="line x" title="204:270	At the level of shallow semantics, while ?NE??" ></td>
	<td class="line x" title="205:270	metrics are not equally useful in all cases, ?SR??metrics prove very effective." ></td>
	<td class="line x" title="206:270	For instance, correlation attained by ?SR-Or-*??reveals that it is important to translate lexical items according to the semantic role they play inside the sentence." ></td>
	<td class="line x" title="207:270	Moreover, correlation attained by the ?SR-Mr-*??metric is a clear indication that in order to achieve a high quality, it is important to ?fully??translate ?whole??semantic structures (i.e. , arguments/adjuncts)." ></td>
	<td class="line x" title="208:270	The existence of all the semantic structures (?SR-Or??, specially associated to the same verb (?SR-Orv??, is also important." ></td>
	<td class="line x" title="209:270	Evaluation of Homogeneous Systems In the two remaining tasks, Spanish-to-English in-domain/out-of-domain, all the systems are statistical." ></td>
	<td class="line x" title="210:270	Table 5 shows correlation with human assessments for some metric representatives." ></td>
	<td class="line x" title="211:270	In this case, BLEU proves very effective, both in-domain and out-of-domain." ></td>
	<td class="line x" title="212:270	Indeed, all metrics based on lexical matching obtain high levels of correlation with human assessments." ></td>
	<td class="line x" title="213:270	However, still metrics based on deeper linguistic analysis attain in most cases higher correlation coefficients, although not as significantly higher as in the case of heterogeneous systems." ></td>
	<td class="line x" title="214:270	3.2 Multiple-reference Scenario We study the case reported by Callison-Burch et al.(2006) in the context of the Arabic-to-English exercise of the ??005 NIST MT Evaluation Campaign??" ></td>
	<td class="line x" title="216:270	(Le and Przybocki, 2005)." ></td>
	<td class="line x" title="217:270	In this case all systems are statistical but ?LinearB??" ></td>
	<td class="line x" title="218:270	a human-aided MT system (Callison-Burch, 2005)." ></td>
	<td class="line x" title="219:270	Five reference translations are available." ></td>
	<td class="line x" title="220:270	System outputs consist of 1056 sentences." ></td>
	<td class="line x" title="221:270	We obtained permission5 to use 7 system outputs." ></td>
	<td class="line x" title="222:270	For six of these systems we counted 4http://www.nist.gov/speech/tests/ summaries/2005/mt05.htm 5Due to data confidentiality, we contacted each participant individually and asked for permission to use their data." ></td>
	<td class="line x" title="223:270	A number of groups and companies responded positively: University of Southern California Information Sciences Institute (ISI), University of Maryland (UMD), Johns Hopkins University & University of Cambridge (JHU-CU), IBM, University of Edinburgh, MITRE and LinearB." ></td>
	<td class="line o" title="224:270	es2en Level Metric in out 1-PER 0.82 0.78 1-WER 0.88 0.83 BLEU 0.89 0.87 Lexical NIST 0.88 0.84 GTM 0.86 0.80 METEOR 0.84 0.81 ROUGE 0.89 0.83 SP-Op-* 0.88 0.80 SP-Oc-* 0.89 0.84 Shallow SP-NISTl-5 0.88 0.85 Syntactic SP-NISTp-5 0.85 0.86 SP-NISTc-5 0.84 0.83 DP-HWCw-4 0.94 0.83 DP-HWCc-4 0.91 0.87 DP-HWCr-4 0.91 0.88 Syntactic DP-Ol-* 0.91 0.84 DP-Oc-* 0.88 0.83 DP-Or-* 0.88 0.84 CP-STM-9 0.89 0.86 NE-Me-* 0.75 0.76 NE-Oe-* 0.71 0.71 NE-Oe-** 0.88 0.80 SR-Mr-* 0.86 0.82 Shallow SR-Or-* 0.92 0.92 Semantic SR-Or 0.91 0.92 SR-Mrv-* 0.89 0.88 SR-Orv-* 0.91 0.92 SR-Orv 0.91 0.91 Table 5: WMT 2006." ></td>
	<td class="line x" title="225:270	Evaluation of Homogeneous Systems." ></td>
	<td class="line x" title="226:270	Spanish-to-English (es2en), in-domain and out-of-domain." ></td>
	<td class="line x" title="227:270	on a subjective manual evaluation based on adequacy and fluency for a subset of 266 sentences (i.e. , 1596 sentences were assessed)." ></td>
	<td class="line x" title="228:270	Each sentence was evaluated by two different human judges." ></td>
	<td class="line x" title="229:270	System scores have been obtained by averaging over all sentence scores." ></td>
	<td class="line x" title="230:270	Table 6 shows the level of correlation with human assessments for some metric representatives (see ?ALL??column)." ></td>
	<td class="line x" title="231:270	In this case, lexical metrics obtain extremely low levels of correlation." ></td>
	<td class="line x" title="232:270	Again, the problem is that lexical metrics are unable to capture the actual quality of ?LinearB??" ></td>
	<td class="line x" title="233:270	At the shallow syntactic level, only metrics which do not consider any lexical information (?SP-NISTp-5??and ?SP-NISTc5??" ></td>
	<td class="line x" title="234:270	attain a significantly higher quality." ></td>
	<td class="line x" title="235:270	At the properly syntactic level, all metrics attain a higher correlation." ></td>
	<td class="line x" title="236:270	At the shallow semantic level, again, while ?NE??metrics are not specially useful, ?SR??metrics prove very effective." ></td>
	<td class="line o" title="237:270	On the other hand, if we remove ?LinearB??(see 262 ar2en Level Metric ALL SMT 1-PER -0.35 0.75 1-WER -0.50 0.69 BLEU 0.06 0.83 Lexical NIST 0.04 0.81 GTM 0.03 0.92 ROUGE -0.17 0.81 METEOR 0.05 0.86 SP-Op-* 0.05 0.84 SP-Oc-* 0.12 0.89 Shallow SP-NISTl-5 0.04 0.82 Syntactic SP-NISTp-5 0.42 0.89 SP-NISTc-5 0.44 0.68 DP-HWCw-4 0.52 0.86 DP-HWCc-4 0.80 0.75 DP-HWCr-4 0.88 0.86 Syntactic DP-Ol-* 0.51 0.94 DP-Oc-* 0.53 0.91 DP-Or-* 0.72 0.93 CP-STM-9 0.74 0.95 NE-Me-* 0.33 0.78 NE-Oe-* 0.24 0.82 NE-Oe-** 0.04 0.81 SR-Mr-* 0.72 0.96 Shallow SR-Or-* 0.61 0.87 Semantic SR-Or 0.66 0.75 SR-Mrv-* 0.68 0.97 SR-Orv-* 0.47 0.84 SR-Orv 0.46 0.81 Table 6: NIST 2005." ></td>
	<td class="line x" title="238:270	Arabic-to-English (ar2en) exercise." ></td>
	<td class="line x" title="239:270	?ALL??refers to the evaluation of all systems." ></td>
	<td class="line x" title="240:270	?SMT??refers to the evaluation of statistical systems alone (i.e. , removing ?LinearB??." ></td>
	<td class="line x" title="241:270	?SMT??column), lexical metrics attain a much higher correlation, in the same range of metrics based on deeper linguistic information." ></td>
	<td class="line x" title="242:270	However, still metrics based on syntactic parsing, and semantic roles, exhibit a slightly higher quality." ></td>
	<td class="line x" title="243:270	4 Conclusions We have presented a comparative study on the behavior of a wide set of metrics for automatic MT evaluation at different linguistic levels (lexical, shallow-syntactic, syntactic, and shallow-semantic) under different scenarios." ></td>
	<td class="line x" title="244:270	We have shown, through empirical evidence, that linguistic features at more abstract levels may provide more reliable system rankings, specially when the systems under evaluation do not share the same lexicon." ></td>
	<td class="line x" title="245:270	We strongly believe that future MT evaluation campaigns should benefit from these results, by including metrics at different linguistic levels." ></td>
	<td class="line x" title="246:270	For instance, the following set could be used: { ?DP-HWCr-4??" ></td>
	<td class="line x" title="247:270	?DP-Oc-*??" ></td>
	<td class="line x" title="248:270	?DP-Ol-*??" ></td>
	<td class="line x" title="249:270	?DP-Or-*??" ></td>
	<td class="line x" title="250:270	?CPSTM-9??" ></td>
	<td class="line x" title="251:270	?SR-Or-*??" ></td>
	<td class="line x" title="252:270	?SR-Orv??}" ></td>
	<td class="line x" title="253:270	All these metrics are among the top-scoring in all the translation tasks studied." ></td>
	<td class="line x" title="254:270	However, none of these metrics provides, in isolation, a ?global??measure of quality." ></td>
	<td class="line x" title="255:270	Indeed, all these metrics focus on ?partial??" ></td>
	<td class="line x" title="256:270	aspects of quality." ></td>
	<td class="line x" title="257:270	We believe that, in order to perform ?global??evaluations, different quality dimensions should be integrated into a single measure of quality." ></td>
	<td class="line x" title="258:270	With that purpose, we are currently exploring several metric combination strategies." ></td>
	<td class="line x" title="259:270	Preliminary results, based on the QUEEN measure inside the QARLA Framework (Amigo et al. , 2005), indicate that metrics at different linguistic levels may be robustly combined." ></td>
	<td class="line x" title="260:270	Experimental results also show that metrics requiring linguistic analysis seem very robust against parsing errors committed by automatic linguistic processors, at least at the document level." ></td>
	<td class="line x" title="261:270	That is very interesting, taking into account that, while reference translations are supposedly well formed, that is not always the case of automatic translations." ></td>
	<td class="line x" title="262:270	However, it remains pending to test the behaviour at the sentence level, which could be very useful for error analysis." ></td>
	<td class="line x" title="263:270	Moreover, relying on automatic processors implies two other important limitations." ></td>
	<td class="line x" title="264:270	First, these tools are not available for all languages." ></td>
	<td class="line x" title="265:270	Second, usually they are too slow to allow for massive evaluations, as required, for instance, in the case of system development." ></td>
	<td class="line x" title="266:270	In the future, we plan to incorporate more accurate, and possibly faster, linguistic processors, also for languages other than English, as they become publicly available." ></td>
	<td class="line x" title="267:270	Acknowledgements This research has been funded by the Spanish Ministry of Education and Science, projects OpenMT (TIN2006-15307-C03-02) and TRANGRAM (TIN2004-07925-C03-02)." ></td>
	<td class="line x" title="268:270	We are recognized as a Quality Research Group (2005 SGR00130) by DURSI, the Research Department of the Catalan Government." ></td>
	<td class="line x" title="269:270	Authors are thankful to the WMT organizers for providing such valuable test beds." ></td>
	<td class="line x" title="270:270	Authors are also thankful to Audrey Le (from NIST), and to the 2005 NIST MT Evaluation Campaign participants who agreed to share their system 263 outputs and human assessments for the purpose of this research." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-1141
Diagnostic Evaluation of Machine Translation Systems Using Automatically Constructed Linguistic Check-Points
Zhou, Ming;Wang, Bo;Liu, Shujie;Li, Mu;Zhang, Dongdong;Zhao, Tiejun;"></td>
	<td class="line x" title="1:183	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 11211128 Manchester, August 2008 Diagnostic Evaluation of Machine Translation Systems Using Automatically Constructed Linguistic Check-Points Ming Zhou1, Bo Wang2, Shujie Liu2, Mu Li1, Dongdong Zhang1, Tiejun Zhao2 1Microsoft Research Asia Beijing, China {mingzhou,muli,dozhang} @microsoft.com  2Harbin Institute of Technology Harbin, China {bowang,Shujieliu,tjzhao} @mtlab.hit.edu.cn   Abstract We present a diagnostic evaluation platform which provides multi-factored evaluation based on automatically constructed check-points." ></td>
	<td class="line x" title="2:183	A check-point is a linguistically motivated unit (e.g. an ambiguous word, a noun phrase, a verb~obj collocation, a prepositional phrase etc.), which are pre-defined in a linguistic taxonomy." ></td>
	<td class="line x" title="3:183	We present a method that automatically extracts check-points from parallel sentences." ></td>
	<td class="line x" title="4:183	By means of checkpoints, our method can monitor a MT system in translating important linguistic phenomena to provide diagnostic evaluation." ></td>
	<td class="line x" title="5:183	The effectiveness of our approach for diagnostic evaluation is verified through experiments on various types of MT systems." ></td>
	<td class="line x" title="6:183	1 Introduction Automatic MT evaluation is a crucial issue for MT system developers." ></td>
	<td class="line x" title="7:183	The state-of-the-art methods for automatic MT evaluation are using an n-gram based metric represented by BLEU (Papineni et al., 2002) and its variants." ></td>
	<td class="line x" title="8:183	Ever since its invention, the BLEU score has been a widely accepted benchmark for MT system evaluation." ></td>
	<td class="line x" title="9:183	Nevertheless, the research community has been aware of the deficiencies of the BLEU metric (Callison-Burch et al., 2006)." ></td>
	<td class="line x" title="10:183	For instance, BLEU fails to sufficiently capture the vitality of natural languages: all grams of a sentence are   2008." ></td>
	<td class="line x" title="11:183	Licensed under the Creative Commons AttributionNoncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/)." ></td>
	<td class="line x" title="12:183	Some rights reserved." ></td>
	<td class="line x" title="13:183	treated equally ignoring their linguistic significance; only consecutive grams are considered ignoring the skipped grams of certain linguistic relations; candidate translation gets acknowledged only if it uses exactly the same lexicon as the reference ignoring the variation in lexical choice." ></td>
	<td class="line x" title="14:183	Furthermore, BLEU is useful for optimizing and improving statistical MT systems but it has shown to be ineffective in comparing systems with different architectures (e.g., rule-based vs. phrase-based) (Callison-Burch et al.,  2006)." ></td>
	<td class="line x" title="15:183	Another common deficiency of the state-ofthe-art evaluation approaches is that they cannot clearly inform MT developers on the detailed strengths and flaws of an MT system, and therefore there is no way for us to understand the capability of certain modules of an MT system, and the capability of translating certain kinds of language phenomena." ></td>
	<td class="line x" title="16:183	For the purpose of system development, MT developers need a diagnostic evaluation approach to provide the feedback on the translation ability of an MT system with regard to various important linguistic phenomena." ></td>
	<td class="line x" title="17:183	We propose a novel diagnostic evaluation approach." ></td>
	<td class="line x" title="18:183	Instead of assigning a general score to an MT system we evaluate the capability of the system in handling various important linguistic test cases called Check-Points." ></td>
	<td class="line x" title="19:183	A check-point is a linguistically motivated unit, (e.g. an ambiguous word, a noun phrase, a verb~obj collocation, a prepositional phrase etc.) which are pre-defined in a linguistic taxonomy for diagnostic evaluation." ></td>
	<td class="line x" title="20:183	The reference of a check-point is its corresponding part in the target sentence." ></td>
	<td class="line x" title="21:183	The evaluation is performed by matching the candidate translation corresponding to the references of the check-points." ></td>
	<td class="line x" title="22:183	The extraction of the check-points is an automatic process using word aligner and parsers." ></td>
	<td class="line x" title="23:183	We control the noise of the word aligner and parsers within tolerable scope by selecting 1121 reliable subset of the check-points and weighting the references with confidence." ></td>
	<td class="line x" title="24:183	The check-points of various kinds extracted in this way have shown to be effective in performing diagnostic evaluation of MT systems." ></td>
	<td class="line x" title="25:183	In addition, scores of check-points are also approved to be useful to improve the ranking of MT systems as additional features at sentence level and document level." ></td>
	<td class="line x" title="26:183	The rest of the paper is structured in the following way:  Section 2 gives the overview of the process of the diagnostic evaluation." ></td>
	<td class="line x" title="27:183	Section 3 introduces the design of check-point taxonomy." ></td>
	<td class="line x" title="28:183	Section 4 explains the details of construction of check-point database and the methods of reducing the noise of aligner and parsers." ></td>
	<td class="line x" title="29:183	Section 5 explains the matching approach." ></td>
	<td class="line x" title="30:183	In Section 6, we introduce the experiments on different MT systems to demonstrate the capability of the diagnostic evaluation." ></td>
	<td class="line x" title="31:183	In Section 7, we show that the check-points can be used to improve the current ranking methods of MT systems." ></td>
	<td class="line x" title="32:183	Section 8 compares our approach with related evaluation approaches." ></td>
	<td class="line x" title="33:183	We conclude this work in Section 9." ></td>
	<td class="line x" title="34:183	2 Overview of Diagnostic Evaluation In our implementation, we first build a checkpoint database encoded in XML by associating a test sentence with qualified check-points it contains." ></td>
	<td class="line x" title="35:183	This process can be described as following steps:    Collect a large amount of parallel sentences from the web or book collections." ></td>
	<td class="line x" title="36:183	  Parse the sentences of source language and target language." ></td>
	<td class="line x" title="37:183	  Perform the word alignments between each sentence pair." ></td>
	<td class="line x" title="38:183	  For each category of check-points, extract the check-points from the parsed sentence pairs." ></td>
	<td class="line x" title="39:183	  Determine the references of each checkpoint in source language based on the word alignment." ></td>
	<td class="line x" title="40:183	With the extracted check-point database, the diagnostic evaluation of an MT system is performed with the following steps:    The test sentences are selected from the database based on the selected categories of check-points to be evaluated." ></td>
	<td class="line x" title="41:183	  For each check-point, we calculate the number of matched n-grams of the references against the translated sentence of the MT system." ></td>
	<td class="line x" title="42:183	The credit of the MT system in translating this check-point is obtained after necessary normalization." ></td>
	<td class="line x" title="43:183	  The credit of a category can be obtained by summing up the credits of all checkpoints of this category." ></td>
	<td class="line x" title="44:183	Then the credit of an MT system can be obtained by summing up the credits of all categories." ></td>
	<td class="line x" title="45:183	  Finally, scores of system, category groups (e.g. Words), single category (e.g. Noun), and detail information of n-gram matching of each check-point are all provided to the developers to diagnose the MT system." ></td>
	<td class="line x" title="46:183	3 Linguistic Check-Point Taxonomy The taxonomy of automatic diagnostic evaluation should be widely accepted so that the diagnostic results can be explained and shared with each other." ></td>
	<td class="line x" title="47:183	We will also need to remove the sophisticated categories that are out of the capability of current NLP tools to recognize." ></td>
	<td class="line x" title="48:183	In light of this consideration, for ChineseEnglish machine translation, we adopted the manual taxonomy introduced by (Lv, 2000; Liu, 2002) and removed items that are beyond the capability of our parsers." ></td>
	<td class="line x" title="49:183	The taxonomy includes typical check-pints at word, phrase and sentence levels." ></td>
	<td class="line x" title="50:183	Some examples of the representative check-points at different levels are provided below:     Word level check-points:     a. Preposition word e.g.,  (in),  (at)     b. Ambiguous word e.g.,  (play)     c. New word1 e.g.,  (Punk)    Phrase level check-points:     a. Collocation." ></td>
	<td class="line x" title="51:183	e.g.,  - (fired  food)     b. Repetitive word combination." ></td>
	<td class="line x" title="52:183	e.g.,  (have a look)     c. Subjective-predicate phrase e.g.,  * , (he*said)        Sentence level check-points: a. BA sentence2 :  (BA) ." ></td>
	<td class="line x" title="53:183	(He took away the book.)" ></td>
	<td class="line x" title="54:183	b. BEI sentence3  (BEI) ." ></td>
	<td class="line x" title="55:183	(The vase was broken.)" ></td>
	<td class="line x" title="56:183	1 New words are the terms extracted from web which can be a name entity or popular words emerging recently." ></td>
	<td class="line x" title="57:183	2 In a BA sentence, the object which normally follows the verb occurs preverbally, marked by word BA." ></td>
	<td class="line x" title="58:183	3 BEI sentence is a kind of passive voice in Chinese marked by word BEI." ></td>
	<td class="line x" title="59:183	1122     Our implementation of Chinese-English check-point taxonomy contains 22 categories and English-Chinese check-point taxonomy contains 20 categories." ></td>
	<td class="line x" title="60:183	Table 1 and 2 show the two taxonomies." ></td>
	<td class="line x" title="61:183	In practice, any tag in parsers (e.g. NP) can be easily added as new category." ></td>
	<td class="line x" title="62:183	Word level Ambiguous word New word Idiom Noun Verb Adjective Pronoun Adverb Preposition Quantifier Repetitive word Collocation Phrase level Subject-predicate phrase Predicate-object  phrase Prepositionobject phrase Measure phrase Location phrase Sentence level BA sentence BEI sentence SHI sentence YOU sentence Compound sentence Table 1:  Chinese check-point taxonomy  Word level Noun Verb (with Tense) Modal verb Adjective Adverb Pronoun Preposition Ambiguous word Plurality Possessive Comparative & Superlative  degree Phrase level Noun phrase Verb phrase Adjective phrase Adverb phrase Preposition phrase Sentence level Attribute clause Adverbial clause Noun clause Hyperbaton Table 2: English check-point taxonomy 4 Construction of Check-Point Database Given a bilingual corpus with word alignment, the construction of check-point database consists of following two steps." ></td>
	<td class="line x" title="63:183	First, the information of pos-tag, dependency structure and constituent structure can be identified with parsers." ></td>
	<td class="line x" title="64:183	Then check-points of different categories are identified." ></td>
	<td class="line x" title="65:183	Check-points of word-level categories such as Chinese idiom and English ambiguous words are extracted with human-made dictionaries, and the check-points of New-Word are extracted with a new word list mined from the web." ></td>
	<td class="line x" title="66:183	A set of human-made rules are employed to extract certain categories involving sentence types such as compound sentence." ></td>
	<td class="line x" title="67:183	Second, for a check-point, with the word alignment information, the corresponding target language portion is identified as the reference of this check-point." ></td>
	<td class="line x" title="68:183	The following example illustrates the process of extracting check-points from a parallel sentence pair." ></td>
	<td class="line x" title="69:183	   A Chinese-English sentence pair:    .     They opposed the building of reserve funds." ></td>
	<td class="line x" title="70:183	   Word segmentation and pos-tagging:    /R  /V  /V  /N ./P    Parsing result (e.g.  a dependency result):     (SUB, 1/ , 0/ )  (OBJ, 1/ , 2/  ) (OBJ, 2/ , 3/ )    Word alignment:      (1; 1); (2; 2); (3; 4); (4; 6,7);     The check-points in table 3 are extracted:  Table 3: Example of check-point extraction      To extract the categories of check-points of different schema of syntactic analysis such as constitute structure and dependency structure, three parsers including a Chinese skeleton parser (a kind of dependency parser) (Zhou, 2000), Stanford statistical parser and Berkeley statistical parser (Klein 2003) are used to parse the Chinese and English sentences." ></td>
	<td class="line x" title="71:183	As explained in next section, these multiple parsers are also used to select high confident check-points." ></td>
	<td class="line x" title="72:183	To get word alignment, an existing tool GIZA++ (Och 2003) is used." ></td>
	<td class="line x" title="73:183	4.1 Reducing the Noise of the Parser The reliability of the check-points mainly depends on the accuracy of the parsers." ></td>
	<td class="line x" title="74:183	We can achieve high quality word level check-points with the state-of-the-art POS tagger (94% precision) and dictionaries of various purposes." ></td>
	<td class="line x" title="75:183	For sentence level categories, the parser tags and manually compiled rules can also achieve 95% accuracy." ></td>
	<td class="line x" title="76:183	For some kinds of categories at phrase level which parsers cannot produce high accuracy, we only select the check-points which can be identified by multiple parsers, that is, adopt the intersection of the parsers results." ></td>
	<td class="line x" title="77:183	Table 4 shows the improvement brought by this approach." ></td>
	<td class="line x" title="78:183	Column 1 and 2 shows the precision of 6 major types of phrases in Stanford and Berkeley parser." ></td>
	<td class="line x" title="79:183	Column 3 shows the precision of intersection and column 4 shows the reduction of the number of check-points when adopting the intersection results." ></td>
	<td class="line x" title="80:183	The test corpus is a part of Category Check-point Reference New word   reserve funds Ambiguous word   building Predicate  object phrase   the building of reserve funds Subject-predicate phrase   They opposed 1123 Penn Chinese Treebank which is not contained in the training corpus of two statistical parsers." ></td>
	<td class="line x" title="81:183	(Klein 2003)." ></td>
	<td class="line x" title="82:183	Stf% Brk% Inter% Tpts redu% NP 87.37 86.03 95.83 17.06 VP 87.34 82.87 95.23 19.68 PP 90.60 88.56 96.00 11.50 QP 98.12 92.90 99.21 6.31 ADJP 91.95 90.87 96.41 10.20 ADVP 95.21 94.25 92.64 3.92 Table 4:  Precision of parsers and their intersection (Stf is Stanford, Brk is Berkelry)  4.2 Alleviating the Impact of Alignment Noise Except for sentence level check-points whose references are the whole sentences and New Word, Idiom check-points whose references are extracted from dictionary, the quality of the references are impacted by the alignment accuracy." ></td>
	<td class="line x" title="83:183	To alleviate the noise of aligner we use the lexical dictionary to check the reliability of references." ></td>
	<td class="line x" title="84:183	Suppose c is a check-point, for each reference c.r of c we calculate the dictionary matching degree DM(c.r) with the source side c.s of c:  )1()).( )).(,.(,1.0().( rcW o r d C n t scD icrcC o C n tMa xrcDM       Where Dic(x) is a word bag contains all words in the dictionary translations of each source word in x. CoCnt(x, y) is the count of the common words in x and y. WordCnt(x) is the count of words in x. Specially, if c.r is not obtained based on alignment DM(c.r) will be 1." ></td>
	<td class="line x" title="85:183	Because the limitation of dictionary, a zero DM score not always means the reference is completely wrong, so we force the DM score to be not less than a minimum value (e.g. 0.1)." ></td>
	<td class="line x" title="86:183	DM score will further be used in evaluation in section 5." ></td>
	<td class="line x" title="87:183	To better understand the reliability of the references and explore whether increasing the number of check-points could also alleviate the impact of noise, we built two check-point databases from a human-aligned corpus (with 60,000 sentence pairs) and an automatically aligned corpus (using GIZA++) respectively and tested 10 different SMT systems4 with them." ></td>
	<td class="line x" title="88:183	The Spearman correlation is calculated between two ranked lists of the 10 evaluation results against the two data 4 These systems are derived from an in-house phrase based SMT engine with different parameter sets." ></td>
	<td class="line x" title="89:183	bases." ></td>
	<td class="line x" title="90:183	A higher correlation score means that the impact of the mistakes in word alignment is weaker." ></td>
	<td class="line x" title="91:183	The experiment is repeated on 6 subsets of the database with the size from 500 sentences to 16K sentences to check the impact of the corpus size." ></td>
	<td class="line x" title="92:183	At system level, high correlations are found at different corpus sizes." ></td>
	<td class="line x" title="93:183	At category level, correlations are found to be low for some categories at small size and become higher at larger corpus size." ></td>
	<td class="line x" title="94:183	The results indicate that the impact of the alignment quality can be ignored if the corpus size is at large scale." ></td>
	<td class="line x" title="95:183	As the check-points can be extracted fully automatically, increasing the size of check-point database will not bring extra cost and efforts." ></td>
	<td class="line x" title="96:183	Empirically, the proper scale is set to be 2000 or more sentences according to the Table 6." ></td>
	<td class="line x" title="97:183	Table 6: Impact of word alignment at different sizes of test corpus." ></td>
	<td class="line x" title="98:183	5 Matching Check-Points for Evaluation Evaluation can be carried out at multiple options: for certain linguistic category, a group of categories or entire taxonomy." ></td>
	<td class="line x" title="99:183	For instance, in ChineseEnglish translation task, if a MT developer would like to know the ability to translate idiom, then a number of parallel sentences containing idiom check-points are selected from the database." ></td>
	<td class="line x" title="100:183	Then the system translation sentences are matched to the references of the check-points of idioms." ></td>
	<td class="line x" title="101:183	500 1K 2K 4K 8K 16K Ambiguous word 0.98 0.98 0.98 0.98 0.96 0.98 Noun 0.93 0.99 0.99 0.89 0.8 0.86 Verb 0.97 0.97 0.99 0.99 0.95 0.92 Adjective 0.16 0.19 0.57 0.75 0.77 0.97 Pronoun 0.96 1 0.93 0.99 0.97 0.99 Adverb 0.38 0.77 0.8 0.96 0.72 0.84 Preposition 0.56 0.86 0.9 0.9 0.97 0.96 Quantifier 1 0.46 0.46 0.98 0.85 0.96 Repetitive Word 0.99 0.99 0.97 0.89 0.73 0.95 Collocation 0.42 0.77 0.77 0.77 0.73 0.88 Subjectpredicate phrase 0.06 0.8 0.95 1 0.96 0.84 Predicateobject phrase 0.84 0.96 0.78 0.7 0.78 0.88 Prepositionobject phrase 0.51 0.5 0.93 0.95 0.87 0.99 Measure phrase 0.91 0.67 0.95 0.95 0.87 0.97 Location phrase 0.62 0.54 0.55 0.55 0.85 0.89 SYSTEM 0.95 0.95 0.98 0.99 0.97 0.98 1124 To calculate the credit at different occasions of matching, similar to BLEU, we split each reference of a check-point into a set of n-grams and sum up the gains over all grams as the credit of this check-point." ></td>
	<td class="line x" title="102:183	Especially, if the check-point is not consecutive we use a special token (e.g. *) to represent a component which can be wildcard matched by any word sequence." ></td>
	<td class="line x" title="103:183	We use the following examples to demonstrate the splitting and matching of grams." ></td>
	<td class="line x" title="104:183	  Consecutive check-point:     Checkpoint:        Reference: playing a drum     Candidate translation:  He is playing a drum." ></td>
	<td class="line x" title="105:183	Matched n-grams: playing; a; drum; playing a; a drum; playing a drum     Not consecutive check-point:     Checkpoint:  *     Reference: They*playing     Candidate translation: They are playing cop per drum." ></td>
	<td class="line x" title="106:183	Matched n-grams: They; playing; They * playing     Additionally, to match word inflections, 3 different options of matching granularity are defined as follows." ></td>
	<td class="line x" title="107:183	  Normal: matching with exact form." ></td>
	<td class="line x" title="108:183	  Lower-case: matching with lowercase." ></td>
	<td class="line x" title="109:183	  Stem: matching with the stem of the word." ></td>
	<td class="line x" title="110:183	For a check-point c and references set R of c, we select the r* in R which matches the translation best based on formula (2)." ></td>
	<td class="line x" title="111:183	When we calculate the recall of a set of checkpoints C (C can be a single check-point, a category or a category group), r* of each check-point c in C are merged into one reference set R* and the recall of C is obtained using formula (3) on R*." ></td>
	<td class="line x" title="112:183	A penalty is also introduced to punish the redundancy of candidate sentences, where length(T) is the average length of all translation sentences and length(R) is the average length of all reference sentences." ></td>
	<td class="line x" title="113:183	Then, the final score of C will be:  )5()(Re)( P e n a l t yCCS c o r e   6 Experiments on MT System Diagnoses In this section, to demonstrate the ability of our approach in the diagnoses of MT systems, we apply diagnostic evaluation to 3 statistical MT (SMT) systems and a rule-based MT (RMT) system respectively." ></td>
	<td class="line x" title="114:183	We compare two SMT systems to understand the strength and shortcoming of each of them, and also compare a SMT system with the RMT system." ></td>
	<td class="line x" title="115:183	The test corpus is NIST05 test data with 54852 check-points." ></td>
	<td class="line x" title="116:183	First SMT system (system A) is an implementation of classical phrase based SMT." ></td>
	<td class="line x" title="117:183	The second SMT system (system B) shares the same decoder with system A and introduces a preprocess to reorder the long phrases in source sentences according to the syntax structure before decoding (Chiho Li et al., 2007)." ></td>
	<td class="line x" title="118:183	The third SMT system (system C) is a popular internet service and the RMT system (system D) is a popular commercial system." ></td>
	<td class="line x" title="119:183	In the first experiment, we diagnose the system A and B and compare the results as shown in table 7." ></td>
	<td class="line x" title="120:183	When evaluated using BLEU, system B achieved a 0.005 points increase on top of system A which is not a very significant difference." ></td>
	<td class="line x" title="121:183	The diagnostic results in table 7 provide much richer information." ></td>
	<td class="line x" title="122:183	The results indicate that two systems perform similar at the word level categories while at all phrase level categories, system B performs better." ></td>
	<td class="line x" title="123:183	This result reflects the benefit from the reordering of complex phrases in system B. Paired t-statistic score for each pair of category scores is also calculated by repeating the evaluation on a random copy of the test set with replacement (Koehn 2004)." ></td>
	<td class="line x" title="124:183	An absolute score beyond 2.17 of paired t-statistic means the difference of the samples is statistically significant (above 95%)." ></td>
	<td class="line x" title="125:183	Table 8 and 9 show an instance of the check-point and its evaluation in this experiment." ></td>
	<td class="line x" title="126:183	)2())'( )( )((m a xa r g ' *         rg r a mn rg r a mn Rr g r a mnC o u n t g r a mnM a tc h rDMr )4( 1 )()()( )(    O th e r w is e Rle n g thTle n g thifTle n g th Rle n g thP e n a lt y )3())'()(( ))()(( )R e ( * * ' '' '           Rr rg r a mn Rr rg r a mn g r a mnC o u n trDM g r a mnM a tc hrDM C 1125  System A System B T score WORDs Idiom 0.1933 0.2370 13.38 Adjective 0.5836 0.5577 -17.43 Pronoun 0.7566 0.7344 -13.49 Adverb 0.5365 0.5433 7.11 Preposition 0.6529 0.6456 -6.21 Repetitive word 0.3363 0.3958 9.86 PHRASEs Subject-predicate 0.5117 0.5206 7.36 Predicate-object 0.4041 0.4180 15.52 Predicate-complement 0.4409 0.5125 9.51 Measure phrase 0.5030 0.5092 3.56 Location phrase 0.5245 0.5338 2.83 GROUPs WORDs 0.4839 0.4855 2.03 PHRASEs 0.4744 0.4964 13.97 SYSTEM (Linguistic) 0.4263 0.4370 16.50 SYSTEM (BLEU) 0.3564 0.3614 7.91 Table 7: Diagnose of SMT systems  Source Sentence   Category Preposition_Object_Phrase Check-Point      Reference 1 in this country  DM = 0.5 Reference 2 in his country  DM = 0.5 System A Translation but the prime minister of thailand Dexin vowed to continue in domestic the search." ></td>
	<td class="line x" title="127:183	System B Translation but the prime minister of thailand Dexin vowed to continue the search in his country." ></td>
	<td class="line x" title="128:183	Table 8: An instance of the check-point." ></td>
	<td class="line x" title="129:183	System A System B Ref 1: Match/Total 1/6 2/6 Ref 2: Match/Total 1/6 6/6 Score 0.17 1 Table 9: N-gram matching rate and scores." ></td>
	<td class="line x" title="130:183	Table 10: Diagnose of SMT and RMT." ></td>
	<td class="line x" title="131:183	In the second experiment, we diagnose system C and D and compare the results." ></td>
	<td class="line x" title="132:183	The BLEU score of system C is 0.3005 and system D is 0.2606." ></td>
	<td class="line x" title="133:183	Table 10 shows the diagnostic results on categories with significant differences." ></td>
	<td class="line x" title="134:183	Scores calculated with 3 matching options described in section 5 are given (Lower means Lowercase." ></td>
	<td class="line x" title="135:183	The scores are listed in the form SMT score/RMT score)." ></td>
	<td class="line x" title="136:183	The diagnostic results indicate that system C performs better on most categories than system D, but system D performs better on categories like idiom, pronoun and preposition." ></td>
	<td class="line x" title="137:183	This result reveals a key difference between two types of MT systems: the SMT works well on the open categories that can be handled by context, while the RMT works well on closed categories which are easily translated by linguistic rules." ></td>
	<td class="line x" title="138:183	As the results of two experiments demonstrate, the diagnostic evaluation provides rich information of the capability of translating various important linguistic categories beyond a single system score." ></td>
	<td class="line x" title="139:183	It successfully distinguishes the specific difference between the MT systems whose system level performance is similar." ></td>
	<td class="line x" title="140:183	It can also diagnose the MT system with different architectures." ></td>
	<td class="line x" title="141:183	Diagnostic evaluation tells the developers about the direction to improve the system." ></td>
	<td class="line x" title="142:183	Along with the scores of categories, the diagnostic evaluation provides the system translation and references at every check-point so that the developers can trace and understand about how the MT system works on every single instance." ></td>
	<td class="line x" title="143:183	7 Experiments on Ranking MT Systems Offering a general evaluation at system level is the major goal of state-of-the-art evaluation methods including widely accepted n-gram metrics." ></td>
	<td class="line x" title="144:183	The absence of linguistic knowledge in BLEU motivated many work to integrate linguistic features into evaluation metric." ></td>
	<td class="line x" title="145:183	In (Yang 2007), the evaluation of SMT systems is alternately formulated as a ranking problem." ></td>
	<td class="line x" title="146:183	Different linguistic features are combined with BLEU such as matching rate of dependency relations of translation candidates against the reference sentences." ></td>
	<td class="line x" title="147:183	The experiments demonstrate that the dependency matching rate feature can increase the ranking accuracy in some cases." ></td>
	<td class="line x" title="148:183	Compared to dependency structure, the linguistic categories in our approach showcase more extensive features." ></td>
	<td class="line x" title="149:183	It would be interesting to see whether the linguistic categories can be used to further improve the ranking of SMT systems." ></td>
	<td class="line x" title="150:183	In experiments, we use the scores of linguistic categories, dependency matching rate, scores of BLEU and other popular metrics as ranking features of MT systems and trained by Ranking SVM of SVMlight (Joachims, 1998)." ></td>
	<td class="line x" title="151:183	We performed the ranking experiments on ACL 2005 workshop data, ranking 7 MT translations with three-fold cross-validation both on sentence level and document level." ></td>
	<td class="line x" title="152:183	The Spearman score is used Type Normal Lower Stem Ambiguous word 0.49/0.42 0.50/0.42 0.53/0.46 New word 0.13/0.13 0.37/0.32 0.42/0.35 Idiom 0.43/0.66 0.46/0.67 0.51/0.71 Pronoun 0.60/0.68 0.69/0.75 0.66/0.75 Preposition 0.38/0.42 0.42/0.45 0.43/0.46 Collocation 0.66/0.54 0.66/0.55 0.70/0.56 Subject-predicate phrase 0.46/0.30 0.51/0.36 0.58/0.42 Predicate-object phrase 0.37/0.25 0.37/0.26 0.47/0.29 Compound sentence 0.22/0.16 0.23/0.16 0.23/0.17 1126 to calculate the correlation with human assessments." ></td>
	<td class="line x" title="153:183	Table 11 and 12 show the results of the different feature sets on sentence level and document level respectively." ></td>
	<td class="line x" title="154:183	As shown in experiment results linguistic categories (LC), when used alone, are better related with human assessments than BLEU and GTM." ></td>
	<td class="line x" title="155:183	When combined with the baseline metrics (BLEU & NIST), LC scores further improve the correlation score, better than dependence matching rate (DP)." ></td>
	<td class="line o" title="156:183	LC scores are obtained by matching the exact form of the words as METEOR(exact) does." ></td>
	<td class="line o" title="157:183	NIST+LC combination score is better than METEOR(exact) at sentence and document level, and also better than METEOR(exact&syn) (syn means wn_synonymy module in METEOR) at document level." ></td>
	<td class="line x" title="158:183	This results indicate the ability of linguistic features in improving the performance of ranking task." ></td>
	<td class="line o" title="159:183	Mean Correlation BLEU 4 0.245 NIST 5 0.307 GTM (e=2) 0.251 METEOR(exact) 0.306 METEOR(exact&syn) 0.327 DP 0.246 LC 0.263 BLEU+DP 0.270 BLEU+ LC 0.288 BLEU+ DP +LC 0.307 NIST+ LC 0.322 NIST+ DP +LC 0.333  Table11: Sentence level ranking (DP means dependency and LC means linguistic categories)   Mean Correlation BLEU 4 0.305 NIST 5 0.373 GTM (e=2) 0.327 METEOR(exact) 0.363 METEOR(exact&syn) 0.394 DP 0.323 LC 0.369 BLEU+DP 0.325 BLEU+ LC 0.387 BLEU+ DP +LC 0.332 NIST+ LC 0.409 NIST+ DP +LC 0.359 Table 12: Document level ranking 8 Comparison with Related Work This work is inspired by (Yu, 1993) with many extensions." ></td>
	<td class="line x" title="160:183	(Yu, 1993) proposed MTE evaluation system based on check-points for EnglishChinese machine translation systems with human craft linguistic taxonomy including 3,200 pairs of sentences containing 6 classes of check-points." ></td>
	<td class="line x" title="161:183	Their check-points were manually constructed by human experts, therefore it will be costly to build new test corpus while the check-points in our approach are constructed automatically." ></td>
	<td class="line x" title="162:183	Another limitation of their work is that only binary score is used for credits while we use n-gram matching rate which provides a broader coverage of different levels of matching." ></td>
	<td class="line o" title="163:183	There are many recent work motivated by ngram based approach." ></td>
	<td class="line x" title="164:183	(Callison-Burch et al., 2006) criticized the inadequate accuracy of evaluation at the sentence level." ></td>
	<td class="line x" title="165:183	(Lin and Och, 2004) used longest common subsequence and skipbigram statistics." ></td>
	<td class="line oc" title="166:183	(Banerjee and Lavie, 2005) calculated the scores by matching the unigrams on the surface forms, stemmed forms and senses." ></td>
	<td class="line x" title="167:183	(Liu et al., 2005) used syntactic features and unlabeled head-modifier dependencies to evaluate MT quality, outperforming BLEU on sentence level correlations with human judgment." ></td>
	<td class="line x" title="168:183	(Gimenez and Marquez, 2007) showed that linguistic features at more abstract levels such as dependency relation may provide more reliable system rankings." ></td>
	<td class="line x" title="169:183	(Yang et al., 2007) formulates MT evaluation as a ranking problems leading to greater correlation with human assessment at the sentence level." ></td>
	<td class="line o" title="170:183	There are many differences between these ngram based methods and our approach." ></td>
	<td class="line o" title="171:183	In ngram approach, a sentence is viewed as a collection of n-grams with different length without differentiating the specific linguistic phenomena." ></td>
	<td class="line x" title="172:183	In our approach, a sentence is viewed as a collection of check-points with different types and depth, conforming to a clear linguistic taxonomy." ></td>
	<td class="line n" title="173:183	Furthermore, in n-gram approach, only one general score at the system level is provided which make it not suitable for system diagnoses, while in our approach we can give scores of linguistic categories and provide much richer information to help developers to find the concrete strength and flaws of the system, in addition to the general score." ></td>
	<td class="line n" title="174:183	The n-gram based metric is not very effective when comparing the systems with different architectures or systems with similar general score, while our approach is more effective in both cases by digging into the multiple linguistic levels and disclosing the latent differences of the systems." ></td>
	<td class="line x" title="175:183	9 Conclusion and Future Work This paper presents an automatically diagnostic evaluation methods on MT based on linguistic check-points automatically constructed." ></td>
	<td class="line x" title="176:183	In contrast with the metrics which only give a general score, our evaluation system can give developers 1127 feedback about the faults and strength of an MT system regarding specific linguistic category or category group." ></td>
	<td class="line x" title="177:183	Different with the existing work based on check-points, our work presents an approach to automatically generate the check-point database." ></td>
	<td class="line x" title="178:183	We show that although there is some noise brought from word alignment and parsing, we can effectively alleviate the problem by refining the parser results, weighting the reference with confidence score and providing large quantity of check-points." ></td>
	<td class="line x" title="179:183	The experiments demonstrate that this method can uncover the specific difference between MT systems with similar architectures and different architectures." ></td>
	<td class="line x" title="180:183	It is also demonstrated that the linguistic check-points can be used as new features to improve the ranking task of MT systems." ></td>
	<td class="line x" title="181:183	Although we present the diagnostic evaluation method with Chinese-English language pair, our approach can be applied to other language pair if syntax parser and word aligner are available." ></td>
	<td class="line x" title="182:183	The taxonomy used in current proposal is based on the human-made linguistic system." ></td>
	<td class="line x" title="183:183	An interesting problem to be explored in the future is whether the taxonomy could be constructed automatically from the parsing results." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="C08-3006
Multilingual Mobile-Phone Translation Services for World Travelers
Paul, Michael;Okuma, Hideo;Yamamoto, Hirofumi;Sumita, Eiichiro;Matsuda, Shigeki;Shimizu, Tohru;Nakamura, Satoshi;"></td>
	<td class="line x" title="1:57	Coling 2008: Companion volume  Posters and Demonstrations, pages 165168 Manchester, August 2008 Multilingual Mobile-Phone Translation Services for World Travelers Michael Paul, Hideo Okuma, Hirofumi Yamamoto, Eiichiro Sumita, Shigeki Matsuda, Tohru Shimizu, Satoshi Nakamura  NICT Spoken Language Communication Group  ATR Spoken Language Communication Research Labs Hikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, Japan Michael.Paul@nict.go.jp Abstract This demonstration introduces two new multilingual translation services for mobile phones." ></td>
	<td class="line x" title="2:57	The  rst translation service provides state-of-the-art text-to-text translations of Japanese as well as English conversational spoken language in the travel domain into 17 languages using statistical machine translation technologies trained automatically from a large-scale multilingual corpus." ></td>
	<td class="line x" title="3:57	The second demonstration is a speech translation service between Japanese and English for real environments." ></td>
	<td class="line x" title="4:57	It is based on distributed speech recognition with noise suppression." ></td>
	<td class="line x" title="5:57	Flexible interfaces between internal and external speech translation resources ease the portability of the system to other languages and enable real-time location-free communication world-wide." ></td>
	<td class="line x" title="6:57	1 Introduction Spoken language translation technologies attempt to bridge the language barriers between people with different native languages who each want to engage in conversation by using their mothertongue." ></td>
	<td class="line x" title="7:57	The importance of these technologies is increasing due to increases in the number of opportunities for cross-language communication in face-to-face conversation, especially in the domain of tourism." ></td>
	<td class="line x" title="8:57	We demonstrate two multilingual translation services for mobile phones that are built on corpusbased speech recognition and translation technologies." ></td>
	<td class="line x" title="9:57	These services enable smooth and locationfree communication in real environments covering the major languages of most nations (see Figure 1)." ></td>
	<td class="line x" title="10:57	cNICT/ATR, 2008." ></td>
	<td class="line x" title="11:57	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/)." ></td>
	<td class="line x" title="12:57	Some rights reserved." ></td>
	<td class="line x" title="13:57	Figure 1: Global Language Coverage The  rst multilingual translation service described in this paper is a text-to-text translation service that enables users to translate Japanese and English conversational spoken language sentences in the travel domain into 17 other languages." ></td>
	<td class="line x" title="14:57	The systems core components consist of a multilingual, sentence-aligned spoken language corpus covering 18 of the major world languages and state-of-the-art statistical machine translation (SMT) engines that are trained automatically from this corpus covering 306 (=18x17) translation directions." ></td>
	<td class="line x" title="15:57	A graphical user-interface (GUI) allows 24x7 world-wide access to the translation service (see Section 2)." ></td>
	<td class="line x" title="16:57	The second multilingual translation service is an extension of the text-based translation service that additionally provides speech recognition capabilities." ></td>
	<td class="line x" title="17:57	This is the  rst commercial speech translation service in the world." ></td>
	<td class="line x" title="18:57	The system is based on distributed speech recognition and operates as follows: (1) front-end processing (noise suppression, feature extraction, and feature parameter compression) is carried out on the mobile phone, (2) backend processing (recognition, translation) is done on a server and (3) translation results are sent back and displayed on the mobile phone (see Section 3)." ></td>
	<td class="line x" title="19:57	2 Multilingual Text Translation Service (MTTS) The multilingual text translation service for mobile phones can be accessed via http://atrlangue.jp/smlt or by using the QR code in Figure 2 that also illustrates the graphical user interface of 165 Figure 2: QR Code and GUI of MTTS the translation service." ></td>
	<td class="line x" title="20:57	Two different modes are distinguished: (1) the multilingual mode where the input is translated into all 17 languages simultaneously and the translation results are displayed sideby-side and (2) the bilingual mode where a single language out of 17 languages can be selected as the target language of the Japanese or English input text translation." ></td>
	<td class="line x" title="21:57	The bilingual mode also features back-translation functionality, i.e., a reverse translation of the generated translation output into the source language, that enables immediate feedback on the quality of the translation output." ></td>
	<td class="line x" title="22:57	In order to solve font problems of mobile phones, the translated sentences are rendered on the server side and an image is sent and displayed in the mobile phone." ></td>
	<td class="line x" title="23:57	2.1 Multilingual Travel Conversation Corpus The translation engines used for the translation service are trained on the Basic Travel Expressions Corpus (ATR-BTEC) which is a collection of sentences that travel experts consider useful for people Table 1: Language Characteristics Language Order Segments Morphology Arabic (ar) SVO phrase rich Danish (da) SVO words medium German (de) SVO words medium English (en) SVO words poor Spanish (es) SVO words medium French (fr) SVO words medium Indonesian (id) SVO words rich Italian (it) SVO words medium Japanese (ja) SOV none poor Korean (ko) SOV phrase poor Malay (ms) SVO words rich Dutch (nl) SVO words medium Portuguese (pt) SVO words medium Brazilian (pt-b) SVO words medium Portuguese Russian (ru) SVO words rich Thai (th) SVO none none Vietnamese (vi) SVO phrase none Chinese (zh) SVO none none going abroad and cover a large variety of topics in travel situations like shopping or stay (Kikui et al., 2006)." ></td>
	<td class="line x" title="24:57	The multilingual corpus consists of 160K sentences for each of the 18 languages, aligned at the sentence-level." ></td>
	<td class="line x" title="25:57	The characteristics of all ATR-BTEC corpus languages are summarized in Table 1." ></td>
	<td class="line x" title="26:57	These languages differ largely in word order (SVO, SOV), segmentation unit (phrase, word, none), and morphology (poor, medium, rich)." ></td>
	<td class="line x" title="27:57	Concerning word segmentation, the corpora were preprocessed using simple tokenization tools for all European languages and language-speci c wordsegmentation tools for languages like Chinese, Japanese, Korean, or Thai that do not use whitespace to separate word/phrase tokens." ></td>
	<td class="line x" title="28:57	All data sets were lower-cased and punctuation marks were removed." ></td>
	<td class="line x" title="29:57	2.2 Statistical Machine Translation Engines Phrase-based statistical machine translation approaches continue to dominate the  eld of machine translation." ></td>
	<td class="line x" title="30:57	The translation service makes use of state-of-the-art phrase-based SMT systems within the framework of feature-based exponential models containing the following features:  Phrase translation probability  Inverse phrase translation probability  Lexical weighting probability  Inverse lexical weighting probability  Phrase penalty  Language model probability  Simple distance-based distortion model  Word penalty 166 Table 2: Language Model Perplexity Lang Entropy Total Eval Data uage Entropy Words Vocab ar 5.73 21,663 3,780 1,067 da 5.66 17,411 3,077 884 de 5.58 16,698 2,995 910 en 4.53 14,370 3,169 807 es 5.35 15,622 2,919 943 fr 4.77 16,793 3,521 929 id 6.09 18,145 2,977 908 it 5.52 16,078 2,914 956 ja 4.03 15,080 3,745 929 ko 4.21 15,011 3,567 943 ms 6.43 19,144 2,977 909 nl 5.66 17,609 3,110 909 pt-b 5.73 16,981 2,962 932 pt 5.54 16,064 2,900 946 ru 6.20 16,040 2,587 1,143 th 5.12 20,230 3,953 738 vi 4.84 19,531 4,034 792 zh 5.11 14,748 2,887 944 The basic framework within which all the MT systems were constructed is shown in Figure 3." ></td>
	<td class="line x" title="31:57	Figure 3: SMT Framework Translation examples from the respective bilingual text corpus are aligned in order to extract phrasal equivalences and to calculate the bilingual feature probabilities." ></td>
	<td class="line x" title="32:57	Monolingual features like the language model probability are trained on monolingual text corpora of the target language whereby standard word alignment and language modeling tools were used." ></td>
	<td class="line x" title="33:57	For decoding, the CleopATRa decoder (Finch et al., 2007), a multi-stack phrasebased SMT decoder is used." ></td>
	<td class="line x" title="34:57	2.3 Evaluation In order to get an idea of how dif cult the translation tasks are, we trained standard 5gram language models on 160K sentence pairs and evaluated the entropy and total entropy, i.e., the entropy multiplied by word counts, of each language on an evaluation data set of 510 sentences each." ></td>
	<td class="line o" title="35:57	Table 2 shows that the total entropy of European Table 3: Automatic Evaluation Results BLEU (%) METEOR (%) en-* *-en ja-* *-ja en-* *-en ja-* *-ja ar 18.21 51.01 13.03 46.09 40.90 69.01 37.52 58.02 da 59.70 70.90 45.94 55.34 75.08 82.56 64.41 65.83 de 56.48 69.25 41.99 59.20 74.01 81.48 63.69 69.61 en   61.56 68.53   78.19 75.39 es 65.22 73.82 51.77 63.24 78.15 85.28 68.30 72.17 fr 64.69 71.04 52.36 63.16 79.28 83.05 71.14 72.82 id 48.35 59.69 40.59 57.24 66.82 75.83 62.33 69.00 it 56.80 70.43 43.45 60.77 72.41 82.96 62.35 70.70 ja 68.53 61.56   75.39 78.19 ko 37.00 58.82 69.96 85.10 57.89 75.92 83.25 89.73 ms 40.99 57.63 36.13 55.84 61.08 74.75 58.73 67.33 nl 57.46 72.85 41.43 59.70 75.88 84.52 63.42 72.19 pt-b 59.99 69.41 46.50 58.07 72.77 80.70 64.68 69.14 pt 62.81 70.25 48.24 59.20 75.65 83.32 67.38 68.32 ru 44.46 61.23 36.08 55.13 66.41 73.75 60.59 64.55 th 46.49 51.35 43.75 50.85 62.47 73.12 60.25 62.91 vi 55.18 57.42 50.86 55.07 71.04 73.98 68.67 70.81 zh 53.08 59.33 51.68 69.43 69.85 74.68 65.88 77.62 languages like Danish, German, English, Spanish, etc. does not differ much." ></td>
	<td class="line x" title="36:57	Moreover, languages with phrasal segments and/or rich morphology like Arabic, Malay, Russian or Vietnamese have a high total entropy and thus can be expected to be more dif cult to translate." ></td>
	<td class="line oc" title="37:57	This is con rmed by the translation experiments in which the evaluation data sets were translated using the servers translation engines and the translation quality was evaluated using the standard automatic evaluation metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) where scores range between 0 (worst) and 1 (best)." ></td>
	<td class="line x" title="38:57	Besides Korean (single references only), all languages were evaluated using 16 reference translations." ></td>
	<td class="line x" title="39:57	The evaluation results in Table 3 show that closely related language pairs like Japanese-Korean or Portuguese-Brazilian can be translated very accurately, whereas translations into languages with high total entropy are of lower quality." ></td>
	<td class="line x" title="40:57	3 Multilingual Speech Translation Service (MSTS) The speech translation service1 can be accessed via http://www.atr-trek.co.jp/contents html or using the QR code in Figure 4 that also illustrates the graphical user interface of the translation service." ></td>
	<td class="line x" title="41:57	After connecting to the top page, the translation service is activated by selecting the  Translation option." ></td>
	<td class="line x" title="42:57	In order to achieve robust speech recogni1The speech translation service for JapaneseEnglish on Docomo 905i mobile phones started November 2007." ></td>
	<td class="line x" title="43:57	167  Figure 4: QR Code and GUI of MSTS tion, the service features a push-to-talk functionality, i.e., the user (1) presses the key to start the service (2) speaks freely into the integrated microphone of the mobile phone, and (3) presses the key again after the speech input is  nished." ></td>
	<td class="line x" title="44:57	Fast and accurate front-end and back-end processing algorithms enable high-speed speech translation of the input." ></td>
	<td class="line x" title="45:57	Both, the speech recognition results as well as the translation results are sent back to and displayed on the mobile phone." ></td>
	<td class="line x" title="46:57	3.1 Multilingual Speech Corpus Similar to the statistical machine translation approach introduced in Section 2.2, the speech recognition components are based on large-sized multilingual speech corpora." ></td>
	<td class="line x" title="47:57	For Japanese, speech recordings of 4000 speakers were collected resulting on a total of 200 hours of speech." ></td>
	<td class="line x" title="48:57	For English, almost the same amount of speech data were collected from 500 speakers in North America (300 speakers), the UK (100 speakers), and Australia (100 speakers)." ></td>
	<td class="line x" title="49:57	3.2 Distributed Speech Recognition The speech interface is based on distributed speech recognition (DSR) that is integrated as a clientserver architecture compatible with the ETSI ES 202 050 standards." ></td>
	<td class="line x" title="50:57	The usage of Speech Translation Markup Language (STML) enables  exible connections between internal and external speech translation resources like speech recognition and translation servers via a network." ></td>
	<td class="line x" title="51:57	Figure 5 illustrates the architecture of the utilized DSR system." ></td>
	<td class="line x" title="52:57	The front-end processing includes noise suppression, feature extraction and feature parameter compression and is carried out on the mobile phone." ></td>
	<td class="line x" title="53:57	The data stream is then sent via internet to the application service provider (ASP) for back-end processing, i.e. speech recognition and statistical machine translation." ></td>
	<td class="line x" title="54:57	The recognition and translation results are sent back to the mobile phone for display to the user." ></td>
	<td class="line x" title="55:57	! '     '  ' #     '       '     $     Figure 5: MSTS Architecture 4 Conclusion This paper introduced the  rst commercial speech translation service in the world." ></td>
	<td class="line x" title="56:57	State-of-theart spoken language translation technologies (distributed speech recognition with noise suppression, multilingual statistical machine translation) are implemented into a  exible client-server architecture that covers the major languages of most countries and enables users to communicate in real environments all over the world using their own mobile phones." ></td>
	<td class="line x" title="57:57	5 Acknowledgments This work is partly supported by the Grant-in-Aid for Scienti c Research (C) and the Special Coordination Funds for Promoting Science and Technology of the Ministry of Education, Culture, Sports, Science and Technology, Japan." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D08-1064
Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms
Chiang, David;DeNeefe, Steve;Chan, Yee Seng;Ng, Hwee Tou;"></td>
	<td class="line x" title="1:146	Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610619, Honolulu, October 2008." ></td>
	<td class="line x" title="2:146	c2008 Association for Computational Linguistics Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms David Chiang and Steve DeNeefe Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA {chiang,sdeneefe}@isi.edu Yee Seng Chan and Hwee Tou Ng Department of Computer Science National University of Singapore Law Link Singapore 117590 {chanys,nght}@comp.nus.edu.sg Abstract B is the de facto standard for evaluation and development of statistical machine translation systems." ></td>
	<td class="line x" title="3:146	We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd." ></td>
	<td class="line x" title="4:146	These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications." ></td>
	<td class="line x" title="5:146	We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments." ></td>
	<td class="line oc" title="6:146	1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MTliterature.Callison-Burchetal.(2006)havesubjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments.Bothcasesinvolvecomparisonsbetween statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restrictedtocomparisonsbetweenrelatedsystemsor different versions of the same systems." ></td>
	<td class="line x" title="7:146	In Bs defense, comparisons between different versions of the same system were exactly what B was designed for." ></td>
	<td class="line x" title="8:146	However, we show that even in such situations, difficulties with B can arise." ></td>
	<td class="line x" title="9:146	We illustrate three ways that properties of B can be exploited to yield improvements that are questionable or even absurd." ></td>
	<td class="line x" title="10:146	All of these scenarios arose in actual practice and involve comparisons between different versions of the same statistical MT systems." ></td>
	<td class="line x" title="11:146	They can be traced to the fact that B is not decomposable at the sentence level: that is, it lacks the property that improving a sentence in a test set leads to an increase in overall score, and degrading a sentence leads to a decrease in the overall score." ></td>
	<td class="line x" title="12:146	This property is not only intuitive, but also computationally convenient for various applications such as translation reranking and discriminative training." ></td>
	<td class="line x" title="13:146	We propose a minimal modification to B that reduces its nondecomposability, as well as a cross between B and word error rate (WER) that is decomposable down to the subsentential level (in a sense to be made more precise below)." ></td>
	<td class="line x" title="14:146	Both metrics correct the observed problems and correlate with human judgments better than B. 2 The B metric Let gk(w) be the multiset of all k-grams of a sentence w. We are given a sequence of candidate translations c to be scored against a set of sequences of reference translations, {rj} = r1,,rR: c = c1, c2, c3,,cN r1 = r11, r12, r13, ,r1N  rR = rR1,rR2,rR3,,rRN 610 Then the B score of c is defined to be B(c,{rj}) = 4productdisplay k=1 prk(c,{rj})14  bp(c,{rj}) (1) where1 prk(c,{rj}) = summationtext i vextendsinglevextendsinglevextendsingle vextendsinglegk(ci)  uniontextj gk(rji ) vextendsinglevextendsinglevextendsingle vextendsinglesummationtext i |gk(ci)| (2) is the k-gram precision of c with respect to {rj}, and bp(c,r), known as the brevity penalty, is defined as follows." ></td>
	<td class="line x" title="15:146	Let (x) = exp(1  1/x)." ></td>
	<td class="line x" title="16:146	In the case of a single reference r, bp(c,r) =  parenleftBigg min braceleftBigg 1, summationtext i |ci|summationtext i |ri| bracerightBiggparenrightBigg (3) In the multiple-reference case, the length |ri| is replaced with an effective reference length, which can be calculated in several ways." ></td>
	<td class="line x" title="17:146	 In the original definition (Papineni et al., 2002), it is the length of the reference sentence whose length is closest to the test sentence." ></td>
	<td class="line x" title="18:146	 In the NIST definition, it is the length of the shortest reference sentence." ></td>
	<td class="line x" title="19:146	 A third possibility would be to take the average length of the reference sentences." ></td>
	<td class="line x" title="20:146	The purpose of the brevity penalty is to prevent a system from generating very short but precise translations, and the definition of effective reference length impacts how strong the penalty is. The NIST definition is the most tolerant of short translations and becomes more tolerant with more reference sentences." ></td>
	<td class="line x" title="21:146	The original definition is less tolerant but has the counterintuitive property that decreasing the length of a test sentence can eliminate the brevity penalty." ></td>
	<td class="line x" title="22:146	Using the average reference length seems attractive but has the counterintuitive property that 1We use the following definitions about multisets: if X is a multiset, let #X(a) be the number of times a occurs in X. Then: |X|  summationdisplay a #X(a) #XY(a)  min{#X(a),#Y(a)} #XY(a)  max{#X(a),#Y(a)} an exact match with one of the references may not get a 100% score." ></td>
	<td class="line x" title="23:146	Throughout this paper we use the NIST definition, as it is currently the definition most used in the literature and in evaluations." ></td>
	<td class="line x" title="24:146	The brevity penalty can also be seen as a standin for recall." ></td>
	<td class="line x" title="25:146	The fraction summationtext i |ci|summationtext i |ri| in the definition of the brevity penalty (3) indeed resembles a weak recall score in which every guessed item counts as a match." ></td>
	<td class="line x" title="26:146	However, with recall, the per-sentence score |ci| |ri| would never exceed unity, but with the brevitypenalty, it can." ></td>
	<td class="line x" title="27:146	This means that if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty." ></td>
	<td class="line x" title="28:146	This is a serious weakness in the B metric, as we demonstrate below using three scenarios, encountered in actual practice." ></td>
	<td class="line x" title="29:146	3 Exploiting the B metric 3.1 The sign test We are aware of two methods that have been proposed for significance testing with B: bootstrap resampling (Koehn, 2004b; Zhang et al., 2004) and the sign test (Collins et al., 2005)." ></td>
	<td class="line x" title="30:146	In bootstrap resampling, we sample with replacement from the test set to synthesize a large number of test sets, and then we compare the performance of two systems on those synthetic test sets to see whether one is better 95% (or 99%) of the time." ></td>
	<td class="line x" title="31:146	But Collins et al.(2005) note that it is not clear whether the conditions requiredbybootstrapresamplingaremetinthecaseof B, and recommend the sign test instead." ></td>
	<td class="line x" title="33:146	Suppose wewanttodeterminewhetherasetofoutputscfrom a test system is better or worse than a set of baseline outputs b. The sign test requires a function f(bi,ci) that indicates whether ci is a better, worse, or samequality translation relative to bi." ></td>
	<td class="line x" title="34:146	However, because B is not defined on single sentences, Collins et al. use an approximation: for each i, form a composite set of outputs bprime = {b1,,bi1,ci,bi+1,,bN}, and compare the B scores of b and bprime." ></td>
	<td class="line x" title="35:146	The goodness of this approximation depends on to what extent the comparison between b and bprime is dependent only on bi and ci, and independent of the other sentences." ></td>
	<td class="line x" title="36:146	However, B scores are highly context-dependent: for example, if the sentences in b are on average epsilon1 words longer than the reference sentences, then ci can be as short as (N  1)epsilon1 words 611 shorter than ri without incurring the brevity penalty." ></td>
	<td class="line x" title="37:146	Moreover, since the ci are substituted in one at a time, we can do this for all of the ci." ></td>
	<td class="line x" title="38:146	Hence, c could have a disastrously low B score (because of the brevity penalty) yet be found by the sign test to be significantly better than the baseline." ></td>
	<td class="line x" title="39:146	We have encountered this situation in practice: two versions of the same system with B scores of 29.6 (length ratio 1.02) and 29.3 (length ratio 0.97), where the sign test finds the second system to be significantly better than the first (and the first system significantly better than the second)." ></td>
	<td class="line x" title="40:146	Clearly, in orderforasignificancetesttobesensible,itshouldnot contradict the observed scores, and should certainly not contradict itself." ></td>
	<td class="line x" title="41:146	In the rest of this paper, except where indicated, all significance tests are performed using bootstrap resampling." ></td>
	<td class="line x" title="42:146	3.2 Genre-specific training For several years, much statistical MT research has focused on translating newswire documents." ></td>
	<td class="line x" title="43:146	One likely reason is that the DARPA TIDES program used newswire documents for evaluation for several years." ></td>
	<td class="line x" title="44:146	But more recent evaluations have included other genres such as weblogs and conversation." ></td>
	<td class="line x" title="45:146	The conventional wisdom has been that if one uses a single statistical translation system to translate text from several different genres, it may perform poorly, and it is better to use several systems optimized separately for each genre." ></td>
	<td class="line x" title="46:146	However, if our task is to translate documents from multiple known genres, but they are evaluated together, the B metric allows us to use that fact to our advantage." ></td>
	<td class="line x" title="47:146	To understand how, notice that our system has an optimal number of words that it should generate for the entire corpus: too few and it will be penalized by Bs brevity penalty, and too many increases the risk of additional non-matching k-grams." ></td>
	<td class="line x" title="48:146	But these words can be distributed among the sentences (and genres) in any way we like." ></td>
	<td class="line x" title="49:146	Instead of translating sentences from each genre with the best genre-specific systems possible, we can generate longer outputs for the genre we have more confidence in, while generating shorter outputs for the harder genre." ></td>
	<td class="line x" title="50:146	This strategy will have mediocre performance on each individual genre (according to both intuition and B), yet will receive a higher B score on the combined test set than the combined systems optimized for each genre." ></td>
	<td class="line x" title="51:146	In fact, knowing which sentence is in which genre is not even always necessary." ></td>
	<td class="line x" title="52:146	In one recent task, we translated documents from two different genres, without knowing the genre of any given sentence." ></td>
	<td class="line x" title="53:146	The easier genre, newswire, also tended to have shorter reference sentences (relative to the source sentences) than the harder genre, weblogs." ></td>
	<td class="line x" title="54:146	For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word." ></td>
	<td class="line x" title="55:146	Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score." ></td>
	<td class="line x" title="56:146	This phenomenon has subsequently been observed by Och (2008) as well." ></td>
	<td class="line x" title="57:146	We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-B training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres." ></td>
	<td class="line x" title="58:146	We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genrespecific system, and (2) all documents with the system trained on the combined (mixed-genre) development set." ></td>
	<td class="line x" title="59:146	In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre trainings." ></td>
	<td class="line x" title="60:146	The genre-specific systems each outperform the mixed system on their own genre as expected, but when the same results are combined, the mixed systemsoutputisafullB pointhigherthanthecombination of the genre-specific systems." ></td>
	<td class="line x" title="61:146	This is because the mixed system produces outputs that have about 1.35 English words per Arabic word on average: longer than the shortest newswire references, but shorter than the weblog references." ></td>
	<td class="line x" title="62:146	The mixed system does worse on each genre but better on the combined test set, whereas, according to intuition, a system that does worse on the two subsets should also do worse on the combined test set." ></td>
	<td class="line x" title="63:146	3.3 Word deletion A third way to take advantage of the B metric is to permit an MT system to delete arbitrary words 612 in the input sentence." ></td>
	<td class="line x" title="64:146	We can do this by introducing new phrases or rules into the system that match words in the input sentence but generate no output; to these rules we attach a feature whose weight is tuned during max-B training." ></td>
	<td class="line x" title="65:146	Such rules have been in use for some time but were only recently discussed by Li et al.(2008)." ></td>
	<td class="line x" title="67:146	When we add word-deletion rules to our MT system, we find that the B increases significantly (Table 6, line 2)." ></td>
	<td class="line x" title="68:146	Figure 1 shows some examples of deletion in Chinese-English translation." ></td>
	<td class="line x" title="69:146	The first sentence has a proper name,<[[/maigesaisai Magsaysay,whichhasbeenmistokenizedintofour tokens." ></td>
	<td class="line x" title="70:146	The baseline system attempts to translate the first two phonetic characters as wheat Georgia, whereas the other system simply deletes them." ></td>
	<td class="line x" title="71:146	On the other hand, the second sentence shows how word deletion can sacrifice adequacy for the sake of fluency, and the third sentence shows that sometimes word deletion removes words that could have been translated well (as seen in the baseline translation)." ></td>
	<td class="line x" title="72:146	Does B reward word deletion fairly?" ></td>
	<td class="line x" title="73:146	We note two reasons why word deletion might be desirable." ></td>
	<td class="line x" title="74:146	First, some function words should truly be deleted: for example, the Chinese particle/de and Chinese measure words often have no counterpart in English (Li et al., 2008)." ></td>
	<td class="line x" title="75:146	Second, even content word deletion might be helpful if it allows a more fluent translation to be assembled from the remnants." ></td>
	<td class="line x" title="76:146	We observe that in the above experiment, word deletion caused the absolute number of k-gram matches, and not just kgram precision, to increase for all 1  k  4." ></td>
	<td class="line x" title="77:146	Human evaluation is needed to conclusively determine whether B rewards deletion fairly." ></td>
	<td class="line x" title="78:146	But to control for these potentially positive effects of deletion, we tested a sentence-deletion system, which is the same as the word-deletion system but constrained to delete all of the words in a sentence or none of them." ></td>
	<td class="line x" title="79:146	This system (Table 6, line 3) deleted 810% of its input and yielded a B score with no significant decrease (p  0.05) from the baseline systems. Given that our model treats sentences independently, so that it cannot move information from one sentence to another, we claim that deletion of nearly 10% of the input is a grave translation deficiency, yet B is insensitive to it." ></td>
	<td class="line x" title="80:146	What does this tell us about word deletion?" ></td>
	<td class="line x" title="81:146	While acknowledging that some word deletions can improve translation quality, we suggest in addition that because word deletion provides a way for the system to translate the test set selectively, a behavior which we have shown that B is insensitive to, part of the score increase due to word deletion is likely an artifact of B. 4 Other metrics Are other metrics susceptible to the same problems as the B metric?" ></td>
	<td class="line x" title="82:146	In this section we examine several other popular metrics for these problems, propose two of our own, and discuss some desirable characteristics for any new MT evaluation metric." ></td>
	<td class="line x" title="83:146	4.1 Previous metrics We ran a suite of other metrics on the above problem cases to see whether they were affected." ></td>
	<td class="line oc" title="84:146	In none of these cases did we repeat minimum-error-rate training; all these systems were trained using max-B. The metrics we tested were:  METEOR (Banerjee and Lavie, 2005), version 0.6,usingtheexact,Porter-stemmer,andWordNet synonmy stages, and the optimized parameters  = 0.81,  = 0.83,  = 0.28 as reported in (Lavie and Agarwal, 2007)." ></td>
	<td class="line x" title="85:146	 GTM (Melamed et al., 2003), version 1.4, with default settings, except e = 1.2, following the WMT 2007 shared task (Callison-Burch et al., 2007)." ></td>
	<td class="line x" title="86:146	 MS (Chan and Ng, 2008), more specifically MSn, which skips the dependency relations." ></td>
	<td class="line x" title="87:146	On the sign test (Table 2), all metrics found significant differences consistent with the difference in score between the two systems." ></td>
	<td class="line x" title="88:146	The problem related to genre-specific training does not seem to affect the other metrics (see Table 4), but they still manifest the unintuitive result that genre-specific training is sometimesworsethanmixed-genretraining.Finally, all metrics but GTM disfavored both word deletion and sentence deletion (Table 7)." ></td>
	<td class="line x" title="89:146	4.2 Strict brevity penalty AveryconservativewayofmodifyingtheB metric to combat the effects described above is to im613 (a) source 9]<[[V reference fei xiaotong awarded magsaysay prize baseline fei xiaotong was awarded the wheat georgia xaixai prize delete fei xiaotong was awarded xaixai award (b) source c-/EAp-NqHa reference the center of the yuhua stone bears an image which very much resembles the territory of the people s republic of china . baseline rain huashi center is a big clear images of chinese territory . delete rain is a clear picture of the people s republic of china ." ></td>
	<td class="line x" title="90:146	(c) source :FDRw  reference urban construction becomes new hotspot for foreign investment in qinghai baseline urban construction become new hotspot for foreign investment qinghai delete become new foreign investment hotspot Figure 1: Examples of word deletion." ></td>
	<td class="line x" title="91:146	Underlined Chinese words were deleted in the word-deletion system; underlined English words correspond to deleted Chinese words." ></td>
	<td class="line x" title="92:146	pose a stricter brevity penalty." ></td>
	<td class="line x" title="93:146	In Section 2, we presented the brevity penalty as a stand-in for recall, but noted that unlike recall, the per-sentence score |ci| |ri| can exceed unity." ></td>
	<td class="line x" title="94:146	This suggests the simple fix ofclipping the per-sentence recall scores in a similar fashion to the clipping of precision scores: bp(c,r) =  parenleftBiggsummationtext i min{|ci|,|ri|}summationtext i |ri| parenrightBigg (4) Then if a translation system produces overlong translations for some sentences, it cannot use those translations to license short translations for other sentences." ></td>
	<td class="line x" title="95:146	Call this revised metric B- (for B with strict brevity penalty)." ></td>
	<td class="line x" title="96:146	We can test this revised definition on the problem cases described above." ></td>
	<td class="line x" title="97:146	Table 2 shows that B resolves the inconsistency observed between B and the sign test, using the example test sets fromSection3.1(nomax-B- trainingwasperformed)." ></td>
	<td class="line x" title="98:146	Table 5 shows the new scores of the mixedgenre example from Section 3.2 after max-B- training." ></td>
	<td class="line x" title="99:146	These results fall in line with intuition tuning separately for each genre leads to slightly better scores in all cases." ></td>
	<td class="line x" title="100:146	Finally, Table 8 shows the B- scores for the word-deletion example from Section 3.3, using both max-B training and maxB- training." ></td>
	<td class="line x" title="101:146	We see that B- reduces the benefit of word deletion to an insignificant level on the test set, and severely punishes sentence deletion." ></td>
	<td class="line x" title="102:146	When we retrain using max-B-, the rate of word deletion is reduced and sentence deletion is all but eliminated, and there are no significant differences on the test set." ></td>
	<td class="line x" title="103:146	4.3 4-gram recognition rate All of the problems we have examinedexcept for word deletionare traceable to the fact that B is not a sentence-level metric." ></td>
	<td class="line x" title="104:146	Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems." ></td>
	<td class="line x" title="105:146	Note that any metricinvolvingmicro-averagedprecision(inwhich the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property." ></td>
	<td class="line x" title="106:146	Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nieen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006)." ></td>
	<td class="line x" title="107:146	Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang 614 et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis." ></td>
	<td class="line x" title="108:146	A variation on B is often used for these purposes, in which the k-gram precisions are smoothed by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above." ></td>
	<td class="line x" title="109:146	The remaining issue, word deletion, is more difficult to assess." ></td>
	<td class="line x" title="110:146	It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained." ></td>
	<td class="line x" title="111:146	It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim." ></td>
	<td class="line x" title="112:146	However, there is again a dovetailing engineering concern which is quite legitimate." ></td>
	<td class="line x" title="113:146	If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples containedinalattice(Taskaretal.,2004),onewould need a metric that can be calculated on the edges of the lattice." ></td>
	<td class="line x" title="114:146	Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nieen et al., 2000)." ></td>
	<td class="line x" title="115:146	Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1  WER = 1  min I + D+S|r| = max M  I|r| (5) where I is the number of insertions, D of deletions, S of substitutions, and M = |r|  D  S the number of matches." ></td>
	<td class="line x" title="116:146	The dynamic program for WRR can be formulated as a Viterbi search through a finite-state automaton: given a candidate sentence c and a referencesentencer,findthehighest-scoringpathmatching c through the automaton with states 0,,|r|, initial state 0, final state |r|, and the following transitions (a star matches any symbol): For 0  i < |r|: i ri+1:1 i+1 match i epsilon1:0 i+1 deletion i star:0 i+1 substitution For 0  i  |r|: i star:1 i insertion This automaton can be intersected with a typical stack-based phrase-based decoder lattice (Koehn, 2004a) or CKY-style shared forest (Chiang, 2007) in much the same way that a language model can, yielding a polynomial-time algorithm for extracting the best-scoring translation from a lattice or forest (Wagner, 1974)." ></td>
	<td class="line x" title="117:146	Intuitively, the reason for this is that WRR, like most metrics, implicitly constructs a word alignment between c and r and only counts matches between aligned words; but unlike other metrics, this alignment is constrained to be monotone." ></td>
	<td class="line x" title="118:146	We can combine WRR with the idea of k-gram matching in B to yield a new metric, the 4-gram recognition rate: 4-GRR = max summationtext4 k=1 Mk  I  Dsummationtext 4 k=1 |gk(r)| (6) where Mk is the number of k-gram matches,  and  control the penalty for insertions and deletions, and gk is as defined in Section 2." ></td>
	<td class="line x" title="119:146	We presently set  = 1, = 0 by analogy with WRR, but explore other settings below." ></td>
	<td class="line x" title="120:146	To calculate 4-GRR on a whole test set, we sum the numerators and denominators as in micro-averaged recall." ></td>
	<td class="line o" title="121:146	The 4-GRR can also be formulated as a finitestate automaton, with states {(i,m) | 0  i  |r|,0  m  3}, initial state (0,0), final states (|r|,m), and the following transitions: For 0  i < |r|, 0  m  3: (i,m) ri+1:m+1 (i+1,min{m+1,3}) match (i,m) epsilon1: (i+1,0) deletion (i,m) star:0 (i+1,0) substitution 615 Metric Adq Flu Rank Con Avg Sem.roleoverlap 77.4 83.9 80.3 74.1 78.9 ParaEvalrecall 71.2 74.2 76.8 79.8 75.5 METEOR 70.1 71.9 74.5 66.9 70.9 B 68.9 72.1 67.2 60.2 67.1 WER 51.0 54.2 34.5 52.4 48.0 B- 73.9 76.7 73.5 63.4 71.9 4-GRR 72.3 75.5 74.3 64.2 71.6 Table 1: Our new metrics correlate with human judgmentsbetterthanB (case-sensitive).Adq=Adequacy, Flu = Fluency, Con = Constituent, Avg = Average." ></td>
	<td class="line x" title="122:146	For 0  i  |r|, 0  m  3: (i,m) star: (i,0) insertion Therefore 4-GRR can also be calculated efficiently on lattices or shared forests." ></td>
	<td class="line x" title="123:146	We did not attempt max-4-GRR training, but we evaluated the word-deletion test sets obtained by max-B and max-B- training using 4-GRR." ></td>
	<td class="line x" title="124:146	The results are shown in Table 7." ></td>
	<td class="line x" title="125:146	In general, the results are very similar to B- except that 4-GRR sometimes scores word deletion slightly lower than baseline." ></td>
	<td class="line x" title="126:146	5 Correlation with human judgments The shared task of the 2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007) was conducted with several aims, one of which was to measure the correlation of several automatic MT evaluation metrics (including B) against human judgments." ></td>
	<td class="line x" title="127:146	The task included two datasets (one drawn from the Europarl corpus and the other from the News Commentary corpus) and across three language pairs (from German, Spanish, and French to English, and back)." ></td>
	<td class="line x" title="128:146	In our experiments, we focus on the tasks where the target language is English." ></td>
	<td class="line x" title="129:146	For human evaluations of the MT submissions, four different criteria were used:  Adequacy: how much of the meaning expressed in the reference translation is also expressed in the hypothesis translation." ></td>
	<td class="line x" title="130:146	 Fluency: how well the translation reads in the target language." ></td>
	<td class="line x" title="131:146	 Rank: each translation is ranked from best to worst, relative to the other translations of the same sentence." ></td>
	<td class="line x" title="132:146	 Constituent: constituents are selected from source-side parse-trees, and human judges are asked to rank their translations." ></td>
	<td class="line x" title="133:146	We scored the workshop shared task submissions with B- and 4-GRR, then converted the raw scores to rankings and calculated the Spearman correlations with the human judgments." ></td>
	<td class="line oc" title="134:146	Table 1 shows theresultsalongwithB andthethreemetricsthat achieved higher correlations than B: semantic role overlap (Gimenez and Marquez, 2007), ParaEval recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="135:146	We find that both our proposed metrics correlate with human judgments better than B does." ></td>
	<td class="line x" title="136:146	However, recall the parameters  and  in the definitionof4-GRRthatcontrolthepenaltyforinserted and deleted words." ></td>
	<td class="line x" title="137:146	Experimenting with this parameter reveals that  = 0.9, = 1 yields a correlation of 78.9%." ></td>
	<td class="line x" title="138:146	In other words, a metric that unboundedly rewards spuriously inserted words correlates better with human judgments than a metric that punishes them." ></td>
	<td class="line x" title="139:146	We assume this is because there are not enough data points (systems) in the sample and ask that all these figures be taken with a grain of salt." ></td>
	<td class="line x" title="140:146	As a general remark, it may be beneficial for humancorrelation datasets to include a few straw-man systems that have very short or very long translations." ></td>
	<td class="line x" title="141:146	6 Conclusion We have described three real-world scenarios involving comparisons between different versions of the same statistical MT systems where B gives counterintuitive results." ></td>
	<td class="line x" title="142:146	All these issues center around the issue of decomposability: the sign test fails because substituting translations one sentence at a time can improve the overall score yet substituting them all at once can decrease it; genre-specific training fails because improving the score of two halves of a test set can decrease the overall score; and sentence deletion is not harmful because generating empty translations for selected sentences does not necessarily decrease the overall score." ></td>
	<td class="line x" title="143:146	We proposed a minimal modification to B, calledB-,andshowedthatitamelioratesthese 616 problems." ></td>
	<td class="line x" title="144:146	We also proposed a metric, 4-GRR, that is decomposable at the sentence level and is therefore guaranteed to solve the sign test, genre-specific tuning, and sentence deletion problems; moreoever, it is decomposable at the subsentential level, which has potential implications for evaluating word deletion and promising applications to translation reranking and discriminative training." ></td>
	<td class="line x" title="145:146	Acknowledgments Our thanks go to Daniel Marcu for suggesting modifying the B brevity penalty, and to Jonathan May and Kevin Knight for their insightful comments." ></td>
	<td class="line x" title="146:146	ThisresearchwassupportedinpartbyDARPAgrant HR0011-06-C-0022 under BBN Technologies subcontract 9500008412." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="I08-1042
Heterogeneous Automatic MT Evaluation Through Non-Parametric Metric Combinations
Giménez, Jesús;Màrquez, Lluís;"></td>
	<td class="line x" title="1:165	Heterogeneous Automatic MT Evaluation Through Non-Parametric Metric Combinations Jesus Gimenez and Llus M`arquez TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya Jordi Girona Salgado 13, E-08034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics." ></td>
	<td class="line x" title="2:165	Recently, several approaches have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a)." ></td>
	<td class="line x" title="3:165	Although based on different assumptions, these approaches share the common characteristic of being parametric." ></td>
	<td class="line x" title="4:165	Their models involve a number of parameters whose weight must be adjusted." ></td>
	<td class="line x" title="5:165	As an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance." ></td>
	<td class="line x" title="6:165	Besides, rather than limiting to the lexical dimension, we work on a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic)." ></td>
	<td class="line x" title="7:165	Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation." ></td>
	<td class="line x" title="8:165	1 Introduction Automatic evaluation metrics have notably accelerated the development cycle of MT systems in the last decade." ></td>
	<td class="line x" title="9:165	There exist a large number of metrics based on different similarity criteria." ></td>
	<td class="line x" title="10:165	By far, the most widely used metric in recent literature is BLEU (Papineni et al., 2001)." ></td>
	<td class="line pc" title="11:165	Other well-known metrics are WER (Nieen et al., 2000), NIST (Doddington, 2002), GTM (Melamed et al., 2003), ROUGE (Lin and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), just to name a few." ></td>
	<td class="line n" title="12:165	All these metrics take into account information at the lexical level1, and, therefore, their reliability depends very strongly on the heterogeneity/representativity of the set of reference translations available (Culy and Riehemann, 2003)." ></td>
	<td class="line x" title="13:165	In order to overcome this limitation several authors have suggested taking advantage of paraphrasing support (Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006)." ></td>
	<td class="line x" title="14:165	Other authors have tried to exploit information at deeper linguistic levels." ></td>
	<td class="line x" title="15:165	For instance, we may find metrics based on full constituent parsing (Liu and Gildea, 2005), and on dependency parsing (Liu and Gildea, 2005; Amigo et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007)." ></td>
	<td class="line x" title="16:165	We may find also metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gimenez and M`arquez, 2007), and at the properly semantic level, e.g., over discourse representations (Gimenez, 2007)." ></td>
	<td class="line n" title="17:165	However, none of current metrics provides, in isolation, a global measure of quality." ></td>
	<td class="line o" title="18:165	Indeed, all metrics focus on partial aspects of quality." ></td>
	<td class="line n" title="19:165	The main problem of relying on partial metrics is that we may obtain biased evaluations, which may lead us to derive inaccurate conclusions." ></td>
	<td class="line x" title="20:165	For instance, CallisonBurch et al.(2006) and Koehn and Monz (2006) have recently reported several problematic cases related to the automatic evaluation of systems oriented towards maximizing different quality aspects." ></td>
	<td class="line o" title="22:165	Corroborating the findings by Culy and Riehemann (2003), they showed that BLEU overrates SMT systems with respect to other types of systems, such 1ROUGE and METEOR may consider morphological variations." ></td>
	<td class="line o" title="23:165	METEOR may also look up for synonyms in WordNet." ></td>
	<td class="line x" title="24:165	319 as rule-based, or human-aided." ></td>
	<td class="line x" title="25:165	The reason is that SMT systems are likelier to match the sublanguage (e.g., lexical choice and order) represented by the set of reference translations." ></td>
	<td class="line x" title="26:165	We argue that, in order to perform more robust, i.e., less biased, automatic MT evaluations, different quality dimensions should be jointly taken into account." ></td>
	<td class="line x" title="27:165	A natural solution to this challenge consists in combining the scores conferred by different metrics, ideally covering a heterogeneous set of quality aspects." ></td>
	<td class="line x" title="28:165	In the last few years, several approaches to metric combination have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a)." ></td>
	<td class="line x" title="29:165	In spite of working on a limited set of quality aspects, mostly lexical features, these approaches have provided effective means of combining different metrics into a single measure of quality." ></td>
	<td class="line x" title="30:165	All these methods implement a parametric combination scheme." ></td>
	<td class="line x" title="31:165	Their models involve a number of parameters whose weight must be adjusted (see further details in Section 2)." ></td>
	<td class="line x" title="32:165	As an alternative path towards heterogeneous MT evaluation, in this work, we explore the possibility of relying on non-parametric combination schemes, in which metrics are combined without having to adjust their relative importance (see Section 3)." ></td>
	<td class="line x" title="33:165	We have studied their ability to integrate a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic) over several evaluation scenarios (see Section 4)." ></td>
	<td class="line x" title="34:165	We show that nonparametric schemes offer a valid means of putting different quality dimensions together, effectively yielding a significantly improved evaluation quality, both in terms of human likeness and human acceptability." ></td>
	<td class="line x" title="35:165	We have also verified that these methods port well across test beds." ></td>
	<td class="line x" title="36:165	2 Related Work Approaches to metric combination require two important ingredients: Combination Scheme, i.e., how to combine several metric scores into a single score." ></td>
	<td class="line x" title="37:165	As pointed out in Section 1, we distinguish between parametric and non-parametric schemes." ></td>
	<td class="line x" title="38:165	Meta-Evaluation Criterion, i.e., how to evaluate the quality of a metric combination." ></td>
	<td class="line x" title="39:165	The two most prominent meta-evaluation criteria are:  Human Acceptability: Metrics are evaluated in terms of their ability to capture the degree of acceptability to humans of automatic translations, i.e., their ability to emulate human assessors." ></td>
	<td class="line x" title="40:165	The underlying assumption is that good translations should be acceptable to human evaluators." ></td>
	<td class="line x" title="41:165	Human acceptability is usually measured on the basis of correlation between automatic metric scores and human assessments of translation quality2." ></td>
	<td class="line x" title="42:165	 Human Likeness: Metrics are evaluated in terms of their ability to capture the features which distinguish human from automatic translations." ></td>
	<td class="line x" title="43:165	The underlying assumption is that good translations should resemble human translations." ></td>
	<td class="line x" title="44:165	Human likeness is usually measured on the basis of discriminative power (Lin and Och, 2004b; Amigo et al., 2005)." ></td>
	<td class="line x" title="45:165	In the following, we describe the most relevant approaches to metric combination suggested in recent literature." ></td>
	<td class="line x" title="46:165	All are parametric, and most of them are based on machine learning techniques." ></td>
	<td class="line x" title="47:165	We distinguish between approaches relying on human likeness and approaches relying on human acceptability." ></td>
	<td class="line x" title="48:165	2.1 Approaches based on Human Likeness The first approach to metric combination based on human likeness was that by Corston-Oliver et al.(2001) who used decision trees to distinguish between human-generated (good) and machinegenerated (bad) translations." ></td>
	<td class="line x" title="50:165	They focused on evaluating only the well-formedness of automatic translations (i.e., subaspects of fluency), obtaining high levels of classification accuracy." ></td>
	<td class="line x" title="51:165	Kulesza and Shieber (2004) extended the approach by Corston-Oliver et al.(2001) to take into account other aspects of quality further than fluency alone." ></td>
	<td class="line x" title="53:165	Instead of decision trees, they trained Support Vector Machine (SVM) classifiers." ></td>
	<td class="line x" title="54:165	They used features inspired by well-known metrics such as BLEU, NIST, WER, and PER." ></td>
	<td class="line x" title="55:165	Metric quality was evaluated both in terms of classification accuracy and correlation with human assessments at the sentence level." ></td>
	<td class="line x" title="56:165	2Usually adequacy, fluency, or a combination of the two." ></td>
	<td class="line x" title="57:165	320 A significant improvement with respect to standard individual metrics was reported." ></td>
	<td class="line x" title="58:165	Gamon et al.(2005) presented a similar approach which, in addition, had the interesting property that the set of human and automatic translations could be independent, i.e., human translations were not required to correspond, as references, to the set of automatic translations." ></td>
	<td class="line x" title="60:165	2.2 Approaches based on Human Acceptability Quirk (2004) applied supervised machine learning algorithms (e.g., perceptrons, SVMs, decision trees, and linear regression) to approximate human quality judgements instead of distinguishing between human and automatic translations." ></td>
	<td class="line x" title="61:165	Similarly to the work by Gamon et al.(2005) their approach does not require human references." ></td>
	<td class="line x" title="63:165	More recently, Albrecht and Hwa (2007a; 2007b) re-examined the SVM classification approach by Kulesza and Shieber (2004) and, inspired by the work of Quirk (2004), suggested a regression-based learning approach to metric combination, with and without human references." ></td>
	<td class="line x" title="64:165	The regression model learns a continuous function that approximates human assessments in training examples." ></td>
	<td class="line x" title="65:165	As an alternative to methods based on machine learning techniques, Liu and Gildea (2007) suggested a simpler approach based on linear combinations of metrics." ></td>
	<td class="line x" title="66:165	They followed a Maximum Correlation Training, i.e., the weight for the contribution of each metric to the overall score was adjusted so as to maximize the level of correlation with human assessments at the sentence level." ></td>
	<td class="line x" title="67:165	As expected, all approaches based on human acceptability have been shown to outperform that of Kulesza and Shieber (2004) in terms of human acceptability." ></td>
	<td class="line x" title="68:165	However, no results in terms of human likeness have been provided, thus leaving these comparative studies incomplete." ></td>
	<td class="line x" title="69:165	3 Non-Parametric Combination Schemes In this section, we provide a brief description of the QARLA framework (Amigo et al., 2005), which is, to our knowledge, the only existing non-parametric approach to metric combination." ></td>
	<td class="line x" title="70:165	QARLA is nonparametric because, rather than assigning a weight to the contribution of each metric, the evaluation of a given automatic output a is addressed through a set of independent probabilistic tests (one per metric) in which the goal is to falsify the hypothesis that a is a human reference." ></td>
	<td class="line x" title="71:165	The input for QARLA is a set of test cases A (i.e., automatic translations), a set of similarity metrics X, and a set of models R (i.e., human references) for each test case." ></td>
	<td class="line x" title="72:165	With such a testbed, QARLA provides the two essential ingredients required for metric combination: Combination Scheme Metrics are combined inside the QUEEN measure." ></td>
	<td class="line x" title="73:165	QUEEN operates under the unanimity principle, i.e., the assumption that a good translation must be similar to all human references according to all metrics." ></td>
	<td class="line x" title="74:165	QUEENX(a) is defined as the probability, over R  R  R, that, for every metric in X, the automatic translation a is more similar to a human reference r than two other references, rprime and rprimeprime, to each other." ></td>
	<td class="line x" title="75:165	Formally: QUEENX,R(a) = Prob(x  X : x(a,r)  x(rprime,rprimeprime)) where x(a,r) stands for the similarity between a and r according to the metric x. Thus, QUEEN allows us to combine different similarity metrics into a single measure, without having to adjust their relative importance." ></td>
	<td class="line x" title="76:165	Besides, QUEEN offers two other important advantages which make it really suitable for metric combination: (i) it is robust against metric redundancy, i.e., metrics covering similar aspects of quality, and (ii) it is not affected by the scale properties of metrics." ></td>
	<td class="line x" title="77:165	The main drawback of the QUEEN measure is that it requires at least three human references, when in most cases only a single reference translation is available." ></td>
	<td class="line x" title="78:165	Meta-evaluation Criterion Metric quality is evaluated using the KING measure of human likeness." ></td>
	<td class="line x" title="79:165	All human references are assumed to be equally optimal and, while they are likely to be different, the best similarity metric is the one that identifies and uses the features that are common to all human references, grouping them and separating them from automatic translations." ></td>
	<td class="line x" title="80:165	Based on QUEEN, KING represents the probability that a human reference 321 does not receive a lower score than the score attained by any automatic translation." ></td>
	<td class="line x" title="81:165	Formally: KINGA,R(X) = Prob(a  A : QUEENX,R{r}(r)  QUEENX,R{r}(a)) KING operates, therefore, on the basis of discriminative power." ></td>
	<td class="line x" title="82:165	The closest measure to KING is ORANGE (Lin and Och, 2004b), which is, however, not intended for the purpose of metric combination." ></td>
	<td class="line x" title="83:165	Apart from being non-parametric, QARLA exhibits another important feature which differentiates it form other approaches; besides considering the similarity between automatic translations and human references, QARLA also takes into account the distribution of similarities among human references." ></td>
	<td class="line x" title="84:165	However, QARLA is not well suited to port from human likeness to human acceptability." ></td>
	<td class="line x" title="85:165	The reason is that QUEEN is, by definition, a very restrictive measure a good translation must be similar to all human references according to all metrics." ></td>
	<td class="line x" title="86:165	Thus, as the number of metrics increases, it becomes easier to find a metric which does not satisfy the QUEEN assumption." ></td>
	<td class="line x" title="87:165	This causes QUEEN values to get close to zero, which turns correlation with human assessments into an impractical meta-evaluation measure." ></td>
	<td class="line x" title="88:165	We have simulated a non-parametric scheme based on human acceptability by working on uniformly averaged linear combinations (ULC) of metrics." ></td>
	<td class="line x" title="89:165	Our approach is similar to that of Liu and Gildea (2007) except that in our case all the metrics in the combination are equally important3." ></td>
	<td class="line x" title="90:165	In other words, ULC is indeed a particular case of a parametric scheme, in which the contribution of each metric is not adjusted." ></td>
	<td class="line x" title="91:165	Formally: ULCX(a,R) = 1|X| summationdisplay xX x(a,R) where X is the metric set, and x(a,R) is the similarity between the automatic translation a and the set of references R, for the given test case, according to the metric x. Since correlation with human assessments at the system level is vaguely informative (it is often estimated on very few system samples), we 3That would be assuming that all metrics operate in the same range of values, which is not always the case." ></td>
	<td class="line x" title="92:165	AE04 CE04 AE05 CE05 #human references 5 5 5 4 #system outputs 5 10 7 10 #outputsassessed 5 10 6 5 #sentences 1,353 1,788 1,056 1,082 #sentencesassessed 347 447 266 272 Table 1: Description of the test beds evaluate metric quality in terms of correlation with human assessments at the sentence level (Rsnt)." ></td>
	<td class="line x" title="93:165	We use the sum of adequacy and fluency to simulate a global assessment of quality." ></td>
	<td class="line x" title="94:165	4 Experimental Work In this section, we study the behavior of the two combination schemes presented in Section 3 in the context of four different evaluation scenarios." ></td>
	<td class="line x" title="95:165	4.1 Experimental Settings We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przybocki, 2005)4." ></td>
	<td class="line x" title="96:165	Both campaigns include two different translations exercises: Arabic-to-English (AE) and Chinese-to-English (CE)." ></td>
	<td class="line x" title="97:165	Human assessments of adequacy and fluency are available for a subset of sentences, each evaluated by two different human judges." ></td>
	<td class="line x" title="98:165	See, in Table 1, a brief numerical description including the number of human references and system outputs available, as well as the number of sentences per output, and the number of system outputs and sentences per system assessed." ></td>
	<td class="line x" title="99:165	For metric computation, we have used the IQMT v2.1, which includes metrics at different linguistic levels (lexical, shallow-syntactic, syntactic, shallowsemantic, and semantic)." ></td>
	<td class="line x" title="100:165	A detailed description may be found in (Gimenez, 2007)5." ></td>
	<td class="line x" title="101:165	4.2 Evaluating Individual Metrics Prior to studying the effects of metric combination, we study the isolated behaviour of individual metrics." ></td>
	<td class="line x" title="102:165	We have selected a set of metric representatives from each linguistic level." ></td>
	<td class="line x" title="103:165	Table 2 shows metaevaluation results for the test beds described in Section 4.1, according both to human likeness (KING) 4http://www.nist.gov/speech/tests/ summaries/2005/mt05.htm 5The IQMT Framework may be freely downloaded from http://www.lsi.upc.edu/nlp/IQMT." ></td>
	<td class="line o" title="104:165	322 KING Rsnt Level Metric AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05 1-WER 0.70 0.51 0.48 0.61 0.53 0.47 0.38 0.47 1-PER 0.64 0.43 0.45 0.58 0.50 0.51 0.29 0.40 1-TER 0.73 0.54 0.53 0.66 0.54 0.50 0.38 0.49 BLEU 0.70 0.49 0.52 0.59 0.50 0.46 0.36 0.39 NIST 0.74 0.53 0.55 0.68 0.53 0.55 0.37 0.46 Lexical GTM.e1 0.67 0.49 0.48 0.61 0.41 0.50 0.26 0.29 GTM.e2 0.69 0.52 0.51 0.64 0.49 0.54 0.43 0.48 ROUGEL 0.73 0.59 0.49 0.65 0.58 0.60 0.41 0.52 ROUGEW 0.75 0.62 0.54 0.68 0.59 0.57 0.48 0.54 METEORwnsyn 0.75 0.56 0.57 0.69 0.56 0.56 0.35 0.41 SP-Op-* 0.66 0.48 0.49 0.59 0.51 0.57 0.38 0.41 SP-Oc-* 0.65 0.44 0.46 0.59 0.55 0.58 0.42 0.41 Shallow SP-NISTl 0.73 0.51 0.55 0.66 0.53 0.54 0.38 0.44 Syntactic SP-NISTp 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39 SP-NISTiob 0.69 0.48 0.49 0.59 0.32 0.36 0.27 0.26 SP-NISTc 0.60 0.42 0.39 0.52 0.26 0.27 0.16 0.16 DP-HWCw 0.58 0.40 0.42 0.53 0.41 0.08 0.35 0.40 DP-HWCc 0.50 0.32 0.33 0.41 0.41 0.17 0.38 0.32 DP-HWCr 0.56 0.40 0.37 0.46 0.42 0.16 0.39 0.43 DP-Ol-* 0.58 0.48 0.41 0.52 0.52 0.48 0.36 0.37 Syntactic DP-Oc-* 0.65 0.45 0.44 0.55 0.49 0.51 0.43 0.41 DP-Or-* 0.71 0.57 0.54 0.64 0.55 0.55 0.50 0.50 CP-Op-* 0.67 0.47 0.47 0.60 0.53 0.57 0.38 0.46 CP-Oc-* 0.66 0.51 0.49 0.62 0.57 0.59 0.45 0.50 CP-STM 0.64 0.42 0.43 0.58 0.39 0.13 0.34 0.30 NE-Oe-** 0.65 0.45 0.46 0.57 0.47 0.56 0.32 0.39 Shallow SR-Or-* 0.48 0.22 0.34 0.41 0.28 0.10 0.32 0.21 Semantic SR-Orv 0.36 0.13 0.24 0.27 0.27 0.12 0.25 0.24 DR-Or-* 0.62 0.47 0.50 0.55 0.47 0.46 0.43 0.37 Semantic DR-Orp-* 0.58 0.42 0.43 0.50 0.37 0.35 0.36 0.26 Optimal Combination 0.79 0.64 0.61 0.70 0.64 0.63 0.54 0.61 Table 2: Metric Meta-evaluation and human acceptability (Rsnt), computed over the subsets of sentences for which human assessments are available." ></td>
	<td class="line x" title="105:165	The first observation is that the two metaevaluation criteria provide very similar metric quality rankings for a same test bed." ></td>
	<td class="line x" title="106:165	This seems to indicate that there is a relationship between the two meta-evaluation criteria employed." ></td>
	<td class="line x" title="107:165	We have confirmed this intuition by computing the Pearson correlation coefficient between values in columns 1 to 4 and their counterparts in columns 5 to 8." ></td>
	<td class="line x" title="108:165	There exists a high correlation (R = 0.79)." ></td>
	<td class="line x" title="109:165	A second observation is that metric quality varies significantly from task to task." ></td>
	<td class="line x" title="110:165	This is due to the significant differences among the test beds employed." ></td>
	<td class="line x" title="111:165	These are related to three main aspects: language pair, translation domain, and system typology." ></td>
	<td class="line x" title="112:165	For instance, notice that most metrics exhibit a lower quality in the case of the AE05 test bed." ></td>
	<td class="line x" title="113:165	The reason is that, while in the rest of test beds all systems are statistical, the AE05 test bed presents the particularity of providing automatic translations produced by heterogeneous MT systems (i.e., systems belonging to different paradigms)6." ></td>
	<td class="line x" title="114:165	The fact that most systems are statistical also explains why, in general, lexical metrics exhibit a higher quality." ></td>
	<td class="line x" title="115:165	However, highest levels of quality are not in all cases attained by metrics at the lexical level (see highlighted values)." ></td>
	<td class="line x" title="116:165	In fact, there is only one metric, ROUGEW (based on lexical matching), which is consistently among the top-scoring in all test beds according to both meta-evaluation criteria." ></td>
	<td class="line x" title="117:165	The underlying cause is simple: current metrics do not provide a global measure of quality, but account only for partial aspects of it." ></td>
	<td class="line x" title="118:165	Apart from evincing the importance of the meta-evaluation process, these results strongly suggest the need for conducting heterogeneous MT evaluations." ></td>
	<td class="line x" title="119:165	6Specifically, all systems are statistical except one which is human-aided." ></td>
	<td class="line o" title="120:165	323 Opt.K(AE.04) = {SP-NISTp} Opt.K(CE.04) = {ROUGEW,SP-NISTp, ROUGEL} Opt.K(AE.05) = {METEORwnsyn,SP-NISTp, DP-Or-*} Opt.K(CE.05) = {SP-NISTp} Opt.R(AE.04) = {ROUGEW,ROUGEL,CP-Oc-*,METEORwnsyn,DP-Or-*,DP-Ol-*,GTM.e2,DR-Or-*,CP-STM} Opt.R(CE.04) = {ROUGEL,CP-Oc-*,ROUGEW,SP-Op-*,METEORwnsyn,DP-Or-*,GTM.e2,1-WER,DR-Or-*} Opt.R(AE.05) = {DP-Or-*,ROUGEW} Opt.R(CE.05) = {ROUGEW,ROUGEL,DP-Or-*,CP-Oc-*,1-TER,GTM.e2,DP-HWCr,CP-STM} Table 3: Optimal metric sets 4.3 Finding Optimal Metric Combinations In that respect, we study the applicability of the two combination strategies presented." ></td>
	<td class="line x" title="121:165	Optimal metric sets are determined by maximizing over the corresponding meta-evaluation measure (KING or Rsnt)." ></td>
	<td class="line x" title="122:165	However, because exploring all possible combinations was not viable, we have used a simple algorithm which performs an approximate search." ></td>
	<td class="line x" title="123:165	First, individual metrics are ranked according to their quality." ></td>
	<td class="line x" title="124:165	Then, following that order, metrics are added to the optimal set only if in doing so the global quality increases." ></td>
	<td class="line x" title="125:165	Since no training is required it has not been necessary to keep a held-out portion of the data for test (see Section 4.4 for further discussion)." ></td>
	<td class="line x" title="126:165	Optimal metric sets are displayed in Table 3." ></td>
	<td class="line x" title="127:165	Inside each set, metrics are sorted in decreasing quality order." ></td>
	<td class="line x" title="128:165	The Optimal Combination line in Table 2 shows the quality attained by these sets, combined under QUEEN in the case of KING optimization, and under ULC in the case of optimizing over Rsnt." ></td>
	<td class="line x" title="129:165	In most cases optimal sets consist of metrics operating at different linguistic levels, mostly at the lexical and syntactic levels." ></td>
	<td class="line x" title="130:165	This is coherent with the findings in Section 4.2." ></td>
	<td class="line x" title="131:165	Metrics at the semantic level are selected only in two cases, corresponding to the Rsnt optimization in AE04 and CE04 test beds." ></td>
	<td class="line x" title="132:165	Also in two cases, corresponding to the KING optimization in AE04 and CE05 test beds, it has not been possible to find any metric combination which outperforms the best individual metric." ></td>
	<td class="line x" title="133:165	This is not a discouraging result." ></td>
	<td class="line x" title="134:165	After all, in these cases, the best metric alone achieves already a very high quality (0.79 and 0.70, respectively)." ></td>
	<td class="line x" title="135:165	The fact that a single feature suffices to discern between manual and automatic translations indicates that MT systems are easily distinguishable, possibly because of their low quality and/or because they are all based on the same translation paradigm." ></td>
	<td class="line x" title="136:165	4.4 Portability It can be argued that metric set optimization is itself a training process; each metric would have an associated binary parameter controlling whether it is selected or not." ></td>
	<td class="line x" title="137:165	For that reason, in Table 4, we have analyzed the portability of optimal metric sets (i) across test beds and (ii) across combination strategies." ></td>
	<td class="line x" title="138:165	As to portability across test beds (i.e., across language pairs and years), the reader must focus on the cells for which the meta-evaluation criterion guiding the metric set optimization matches the criterion used in the evaluation, i.e., the top-left and bottom-right 16-cell quadrangles." ></td>
	<td class="line x" title="139:165	The fact that the 4 values in each subcolumn are in a very similar range confirms that optimal metric sets port well across test beds." ></td>
	<td class="line x" title="140:165	We have also studied the portability of optimal metric sets across combination strategies." ></td>
	<td class="line x" title="141:165	In other words, although QUEEN and ULC are thought to operate on metric combinations respectively optimized on the basis of human likeness and human acceptability, we have studied the effects of applying either measure over metric combinations optimized on the basis of the alternative metaevaluation criterion." ></td>
	<td class="line x" title="142:165	In this case, the reader must compare top-left vs. bottom-left (KING) and topright vs. bottom-right (Rsnt) 16-cell quadrangles." ></td>
	<td class="line x" title="143:165	It can be clearly seen that optimal metric sets, in general, do not port well across meta-evaluation criteria, particularly from human likeness to human acceptability." ></td>
	<td class="line x" title="144:165	However, interestingly, in the case of AE05 (i.e., heterogeneous systems), the optimal metric set ports well from human acceptability to human likeness." ></td>
	<td class="line x" title="145:165	We speculate that system heterogeneity has contributed positively for the sake of robustness." ></td>
	<td class="line x" title="146:165	5 Conclusions As an alternative to current parametric combination techniques, we have presented two different meth324 Metric KING Rsnt Set AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05 Opt.K(AE.04) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39 Opt.K(CE.04) 0.78 0.64 0.57 0.67 0.49 0.51 0.39 0.43 Opt.K(AE.05) 0.74 0.63 0.61 0.66 0.48 0.51 0.39 0.42 Opt.K(CE.05) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39 Opt.R(AE.04) 0.62 0.56 0.52 0.49 0.64 0.61 0.53 0.58 Opt.R(CE.04) 0.68 0.59 0.55 0.56 0.63 0.63 0.51 0.57 Opt.R(AE.05) 0.75 0.64 0.59 0.69 0.62 0.60 0.54 0.57 Opt.R(CE.05) 0.64 0.56 0.51 0.52 0.63 0.57 0.53 0.61 Table 4: Portability of combination strategies ods: a genuine non-parametric method based on human likeness, and a parametric method based human acceptability in which the parameter weights are set equiprobable." ></td>
	<td class="line x" title="147:165	We have shown that both strategies may yield a significantly improved quality by combining metrics at different linguistic levels." ></td>
	<td class="line x" title="148:165	Besides, we have shown that these methods generalize well across test beds." ></td>
	<td class="line x" title="149:165	Thus, a valid path towards heterogeneous automatic MT evaluation has been traced." ></td>
	<td class="line x" title="150:165	We strongly believe that future MT evaluation campaigns should benefit from these results specially for the purpose of comparing systems based on different paradigms." ></td>
	<td class="line x" title="151:165	These techniques could also be used to build better MT systems by allowing system developers to perform more accurate error analyses and less biased adjustments of system parameters." ></td>
	<td class="line x" title="152:165	As an additional result, we have found that there is a tight relationship between human acceptability and human likeness." ></td>
	<td class="line x" title="153:165	This result, coherent with the findings by Amigo et al.(2006), suggests that the two criteria are interchangeable." ></td>
	<td class="line x" title="155:165	This would be a point in favour of combination schemes based on human likeness, since human assessments which are expensive to acquire, subjective and not reusable are not required." ></td>
	<td class="line x" title="156:165	We also interpret this result as an indication that human assessors probably behave in many cases in a discriminative manner." ></td>
	<td class="line x" title="157:165	For each test case, assessors would inspect the source sentence and the set of human references trying to identify the features which good translations should comply with, for instance regarding adequacy and fluency." ></td>
	<td class="line x" title="158:165	Then, they would evaluate automatic translations roughly according to the number and relevance of the features they share and the ones they do not." ></td>
	<td class="line x" title="159:165	For future work, we plan to study the integration of finer features as well as to conduct a rigorous comparison between parametric and nonparametric combination schemes." ></td>
	<td class="line x" title="160:165	This may involve reproducing the works by Kulesza and Shieber (2004) and Albrecht and Hwa (2007a)." ></td>
	<td class="line x" title="161:165	This would also allow us to evaluate their approaches in terms of both human likeness and human acceptability, and not only on the latter criterion as they have been evaluated so far." ></td>
	<td class="line x" title="162:165	Acknowledgements This research has been funded by the Spanish Ministry of Education and Science, project OpenMT (TIN2006-15307-C03-02)." ></td>
	<td class="line x" title="163:165	Our NLP group has been recognized as a Quality Research Group (2005 SGR-00130) by DURSI, the Research Department of the Catalan Government." ></td>
	<td class="line x" title="164:165	We are thankful to Enrique Amigo, for his generous help and valuable comments." ></td>
	<td class="line x" title="165:165	We are also grateful to the NIST MT Evaluation Campaign organizers, and participants who agreed to share their system outputs and human assessments for the purpose of this research." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1007
MAXSIM: A Maximum Similarity Metric for Machine Translation Evaluation
Chan, Yee Seng;Ng, Hwee Tou;"></td>
	<td class="line x" title="1:175	Proceedings of ACL-08: HLT, pages 5562, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:175	c2008 Association for Computational Linguistics MAXSIM: A Maximum Similarity Metric for Machine Translation Evaluation Yee Seng Chan and Hwee Tou Ng Department of Computer Science National University of Singapore Law Link, Singapore 117590 {chanys, nght}@comp.nus.edu.sg Abstract We propose an automatic machine translation (MT) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences." ></td>
	<td class="line x" title="3:175	Unlike most metrics, we compute a similarity score between items across the two sentences." ></td>
	<td class="line x" title="4:175	We then find a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence." ></td>
	<td class="line x" title="5:175	This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as n-grams, dependency relations, etc. When evaluated on data from the ACL-07 MT workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic MT evaluation metrics that were evaluated during the workshop." ></td>
	<td class="line x" title="6:175	1 Introduction In recent years, machine translation (MT) research has made much progress, which includes the introduction of automatic metrics for MT evaluation." ></td>
	<td class="line x" title="7:175	Since human evaluation of MT output is time consuming and expensive, having a robust and accurate automatic MT evaluation metric that correlates well with human judgement is invaluable." ></td>
	<td class="line x" title="8:175	Among all the automatic MT evaluation metrics, BLEU (Papineni et al., 2002) is the most widely used." ></td>
	<td class="line x" title="9:175	Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores." ></td>
	<td class="line x" title="10:175	During the recent ACL-07 workshop on statistical MT (Callison-Burch et al., 2007), a total of 11 automatic MT evaluation metrics were evaluated for correlation with human judgement." ></td>
	<td class="line pc" title="11:175	The results show that, as compared to BLEU, several recently proposed metrics such as Semantic-role overlap (Gimenez and Marquez, 2007), ParaEval-recall (Zhou et al., 2006), and METEOR (Banerjee and Lavie, 2005) achieve higher correlation." ></td>
	<td class="line x" title="12:175	In this paper, we propose a new automatic MT evaluation metric, MAXSIM, that compares a pair of system-reference sentences by extracting n-grams and dependency relations." ></td>
	<td class="line x" title="13:175	Recognizing that different concepts can be expressed in a variety of ways, we allow matching across synonyms and also compute a score between two matching items (such as between two n-grams or between two dependency relations), which indicates their degree of similarity with each other." ></td>
	<td class="line x" title="14:175	Having weighted matches between items means that there could be many possible ways to match, or link items from a system translation sentence to a reference translation sentence." ></td>
	<td class="line x" title="15:175	To match each system item to at most one reference item, we model the items in the sentence pair as nodes in a bipartite graph and use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to find a maximum weight matching (or alignment) between the items in polynomial time." ></td>
	<td class="line x" title="16:175	The weights (from the edges) of the resulting graph will then be added to determine the final similarity score between the pair of sentences." ></td>
	<td class="line x" title="17:175	55 Although a maximum weight bipartite graph was also used in the recent work of (Taskar et al., 2005), their focus was on learning supervised models for single word alignment between sentences from a source and target language." ></td>
	<td class="line x" title="18:175	The contributions of this paper are as follows." ></td>
	<td class="line x" title="19:175	Current metrics (such as BLEU, METEOR, Semantic-role overlap, ParaEval-recall, etc.) do not assign different weights to their matches: either two items match, or they dont. Also, metrics such as METEOR determine an alignment between the items of a sentence pair by using heuristics such as the least number of matching crosses." ></td>
	<td class="line x" title="20:175	In contrast, we propose weighting different matches differently, and then obtain an optimal set of matches, or alignments, by using a maximum weight matching framework." ></td>
	<td class="line x" title="21:175	We note that this framework is not used by any of the 11 automatic MT metrics in the ACL-07 MT workshop." ></td>
	<td class="line x" title="22:175	Also, this framework allows for defining arbitrary similarity functions between two matching items, and we could match arbitrary concepts (such as dependency relations) gathered from a sentence pair." ></td>
	<td class="line x" title="23:175	In contrast, most other metrics (notably BLEU) limit themselves to matching based only on the surface form of words." ></td>
	<td class="line x" title="24:175	Finally, when evaluated on the datasets of the recent ACL07 MT workshop (Callison-Burch et al., 2007), our proposed metric achieves higher correlation with human judgements than all of the 11 automatic MT evaluation metrics evaluated during the workshop." ></td>
	<td class="line x" title="25:175	In the next section, we describe several existing metrics." ></td>
	<td class="line x" title="26:175	In Section 3, we discuss issues to consider when designing a metric." ></td>
	<td class="line x" title="27:175	In Section 4, we describe our proposed metric." ></td>
	<td class="line x" title="28:175	In Section 5, we present our experimental results." ></td>
	<td class="line x" title="29:175	Finally, we outline future work in Section 6, before concluding in Section 7." ></td>
	<td class="line x" title="30:175	2 Automatic Evaluation Metrics In this section, we describe BLEU, and the three metrics which achieved higher correlation results than BLEU in the recent ACL-07 MT workshop." ></td>
	<td class="line x" title="31:175	2.1 BLEU BLEU (Papineni et al., 2002) is essentially a precision-based metric and is currently the standard metric for automatic evaluation of MT performance." ></td>
	<td class="line x" title="32:175	To score a system translation, BLEU tabulates the number of n-gram matches of the system translation against one or more reference translations." ></td>
	<td class="line x" title="33:175	Generally, more n-gram matches result in a higher BLEU score." ></td>
	<td class="line x" title="34:175	When determining the matches to calculate precision, BLEU uses a modified, or clipped n-gram precision." ></td>
	<td class="line x" title="35:175	With this, an n-gram (from both the system and reference translation) is considered to be exhausted or used after participating in a match." ></td>
	<td class="line x" title="36:175	Hence, each system n-gram is clipped by the maximum number of times it appears in any reference translation." ></td>
	<td class="line x" title="37:175	To prevent short system translations from receiving too high a score and to compensate for its lack of a recall component, BLEU incorporates a brevity penalty." ></td>
	<td class="line x" title="38:175	This penalizes the score of a system if the length of its entire translation output is shorter than the length of the reference text." ></td>
	<td class="line x" title="39:175	2.2 Semantic Roles (Gimenez and Marquez, 2007) proposed using deeper linguistic information to evaluate MT performance." ></td>
	<td class="line x" title="40:175	For evaluation in the ACL-07 MT workshop, the authors used the metric which they termed as SR-Or-*1." ></td>
	<td class="line x" title="41:175	This metric first counts the number of lexical overlaps SR-Or-t for all the different semantic roles t that are found in the system and reference translation sentence." ></td>
	<td class="line x" title="42:175	A uniform average of the counts is then taken as the score for the sentence pair." ></td>
	<td class="line x" title="43:175	In their work, the different semantic roles t they considered include the various core and adjunct arguments as defined in the PropBank project (Palmer et al., 2005)." ></td>
	<td class="line x" title="44:175	For instance, SR-Or-A0 refers to the number of lexical overlaps between the A0 arguments." ></td>
	<td class="line x" title="45:175	To extract semantic roles from a sentence, several processes such as lemmatization, partof-speech tagging, base phrase chunking, named entity tagging, and finally semantic role tagging need to be performed." ></td>
	<td class="line x" title="46:175	2.3 ParaEval The ParaEval metric (Zhou et al., 2006) uses a large collection of paraphrases, automatically extracted from parallel corpora, to evaluate MT performance." ></td>
	<td class="line x" title="47:175	To compare a pair of sentences, ParaEval first locates paraphrase matches between the two 1Verified through personal communication as this is not evident in their paper." ></td>
	<td class="line x" title="48:175	56 sentences." ></td>
	<td class="line x" title="49:175	Then, unigram matching is performed on the remaining words that are not matched using paraphrases." ></td>
	<td class="line x" title="50:175	Based on the matches, ParaEval will then elect to use either unigram precision or unigram recall as its score for the sentence pair." ></td>
	<td class="line x" title="51:175	In the ACL-07 MT workshop, ParaEval based on recall (ParaEval-recall) achieves good correlation with human judgements." ></td>
	<td class="line oc" title="52:175	2.4 METEOR Given a pair of strings to compare (a system translation and a reference translation), METEOR (Banerjee and Lavie, 2005) first creates a word alignment between the two strings." ></td>
	<td class="line o" title="53:175	Based on the number of word or unigram matches and the amount of string fragmentation represented by the alignment, METEOR calculates a score for the pair of strings." ></td>
	<td class="line x" title="54:175	In aligning the unigrams, each unigram in one string is mapped, or linked, to at most one unigram in the other string." ></td>
	<td class="line x" title="55:175	These word alignments are created incrementally through a series of stages, where each stage only adds alignments between unigrams which have not been matched in previous stages." ></td>
	<td class="line x" title="56:175	At each stage, if there are multiple different alignments, then the alignment with the most number of mappings is selected." ></td>
	<td class="line x" title="57:175	If there is a tie, then the alignment with the least number of unigram mapping crosses is selected." ></td>
	<td class="line x" title="58:175	The three stages of exact, porter stem, and WN synonymy are usually applied in sequence to create alignments." ></td>
	<td class="line x" title="59:175	The exact stage maps unigrams if they have the same surface form." ></td>
	<td class="line x" title="60:175	The porter stem stage then considers the remaining unmapped unigrams and maps them if they are the same after applying the Porter stemmer." ></td>
	<td class="line x" title="61:175	Finally, the WN synonymy stage considers all remaining unigrams and maps two unigrams if they are synonyms in the WordNet sense inventory (Miller, 1990)." ></td>
	<td class="line o" title="62:175	Once the final alignment has been produced, unigram precision P (number of unigram matches m divided by the total number of system unigrams) and unigram recall R (m divided by the total number of reference unigrams) are calculated and combined into a single parameterized harmonic mean (Rijsbergen, 1979): Fmean = P  RP + (1  )R (1) To account for longer matches and the amount of fragmentation represented by the alignment, METEOR groups the matched unigrams into as few chunks as possible and imposes a penalty based on the number of chunks." ></td>
	<td class="line o" title="63:175	The METEOR score for a pair of sentences is: score = bracketleftBigg 1   parenleftbiggno." ></td>
	<td class="line x" title="64:175	of chunks m parenrightbiggbracketrightBigg Fmean where  parenleftbigno." ></td>
	<td class="line x" title="65:175	of chunksm parenrightbig represents the fragmentation penalty of the alignment." ></td>
	<td class="line o" title="66:175	Note that METEOR consists of three parameters that need to be optimized based on experimentation: , , and ." ></td>
	<td class="line x" title="67:175	3 Metric Design Considerations We first review some aspects of existing metrics and highlight issues that should be considered when designing an MT evaluation metric." ></td>
	<td class="line x" title="68:175	Intuitive interpretation: To compensate for the lack of recall, BLEU incorporates a brevity penalty." ></td>
	<td class="line x" title="69:175	This, however, prevents an intuitive interpretation of its scores." ></td>
	<td class="line oc" title="70:175	To address this, standard measures like precision and recall could be used, as in some previous research (Banerjee and Lavie, 2005; Melamed et al., 2003)." ></td>
	<td class="line x" title="71:175	Allowing for variation: BLEU only counts exact word matches." ></td>
	<td class="line x" title="72:175	Languages, however, often allow a great deal of variety in vocabulary and in the ways concepts are expressed." ></td>
	<td class="line x" title="73:175	Hence, using information such as synonyms or dependency relations could potentially address the issue better." ></td>
	<td class="line x" title="74:175	Matches should be weighted: Current metrics either match, or dont match a pair of items." ></td>
	<td class="line x" title="75:175	We note, however, that matches between items (such as words, n-grams, etc.) should be weighted according to their degree of similarity." ></td>
	<td class="line x" title="76:175	4 The Maximum Similarity Metric We now describe our proposed metric, Maximum Similarity (MAXSIM), which is based on precision and recall, allows for synonyms, and weights the matches found." ></td>
	<td class="line x" title="77:175	57 Given a pair of English sentences to be compared (a system translation against a reference translation), we perform tokenization2, lemmatization using WordNet3, and part-of-speech (POS) tagging with the MXPOST tagger (Ratnaparkhi, 1996)." ></td>
	<td class="line x" title="78:175	Next, we remove all non-alphanumeric tokens." ></td>
	<td class="line x" title="79:175	Then, we match the unigrams in the system translation to the unigrams in the reference translation." ></td>
	<td class="line x" title="80:175	Based on the matches, we calculate the recall and precision, which we then combine into a single Fmean unigram score using Equation 1." ></td>
	<td class="line x" title="81:175	Similarly, we also match the bigrams and trigrams of the sentence pair and calculate their corresponding Fmean scores." ></td>
	<td class="line x" title="82:175	To obtain a single similarity score scores for this sentence pair s, we simply average the three Fmean scores." ></td>
	<td class="line x" title="83:175	Then, to obtain a single similarity score sim-score for the entire system corpus, we repeat this process of calculating a scores for each system-reference sentence pair s, and compute the average over all jSj sentence pairs: sim-score = 1jSj |S|summationdisplay s=1 bracketleftBigg 1 N Nsummationdisplay n=1 Fmeans,n bracketrightBigg where in our experiments, we set N=3, representing calculation of unigram, bigram, and trigram scores." ></td>
	<td class="line x" title="84:175	If we are given access to multiple references, we calculate an individual sim-score between the system corpus and each reference corpus, and then average the scores obtained." ></td>
	<td class="line x" title="85:175	4.1 Using N-gram Information In this subsection, we describe in detail how we match the n-grams of a system-reference sentence pair." ></td>
	<td class="line x" title="86:175	Lemma and POS match Representing each ngram by its sequence of lemma and POS-tag pairs, we first try to perform an exact match in both lemma and POS-tag." ></td>
	<td class="line x" title="87:175	In all our n-gram matching, each ngram in the system translation can only match at most one n-gram in the reference translation." ></td>
	<td class="line x" title="88:175	Representing each unigram (lipi) at position i by its lemma li and POS-tag pi, we count the number matchuni of system-reference unigram pairs where both their lemma and POS-tag match." ></td>
	<td class="line x" title="89:175	To find matching pairs, we proceed in a left-to-right fashion 2http://www.cis.upenn.edu/ treebank/tokenizer.sed 3http://wordnet.princeton.edu/man/morph.3WN r1 r2 r3 00.5 0.750.75 0.75 11 1 s3s2s1 0.5 r1 r2 r3 0.75 11 s3s1 s2 Figure 1: Bipartite matching." ></td>
	<td class="line x" title="90:175	(in both strings)." ></td>
	<td class="line x" title="91:175	We first compare the first system unigram to the first reference unigram, then to the second reference unigram, and so on until we find a match." ></td>
	<td class="line x" title="92:175	If there is a match, we increment matchuni by 1 and remove this pair of system-reference unigrams from further consideration (removed items will not be matched again subsequently)." ></td>
	<td class="line x" title="93:175	Then, we move on to the second system unigram and try to match it against the reference unigrams, once again proceeding in a left-to-right fashion." ></td>
	<td class="line x" title="94:175	We continue this process until we reach the last system unigram." ></td>
	<td class="line x" title="95:175	To determine the number matchbi of bigram matches, a system bigram (lsipsi,lsi+1psi+1) matches a reference bigram (lripri,lri+1pri+1) if lsi = lri, psi = pri, lsi+1 = lri+1, and psi+1 = pri+1." ></td>
	<td class="line x" title="96:175	For trigrams, we similarly determine matchtri by counting the number of trigram matches." ></td>
	<td class="line x" title="97:175	Lemma match For the remaining set of n-grams that are not yet matched, we now relax our matching criteria by allowing a match if their corresponding lemmas match." ></td>
	<td class="line x" title="98:175	That is, a system unigram (lsipsi) matches a reference unigram (lripri) if lsi = lri." ></td>
	<td class="line x" title="99:175	In the case of bigrams, the matching conditions are lsi = lri and lsi+1 = lri+1." ></td>
	<td class="line x" title="100:175	The conditions for trigrams are similar." ></td>
	<td class="line x" title="101:175	Once again, we find matches in a left-to-right fashion." ></td>
	<td class="line x" title="102:175	We add the number of unigram, bigram, and trigram matches found during this phase to matchuni, matchbi, and matchtri respectively." ></td>
	<td class="line x" title="103:175	Bipartite graph matching For the remaining ngrams that are not matched so far, we try to match them by constructing bipartite graphs." ></td>
	<td class="line x" title="104:175	During this phase, we will construct three bipartite graphs, one 58 each for the remaining set of unigrams, bigrams, and trigrams." ></td>
	<td class="line x" title="105:175	Using bigrams to illustrate, we construct a weighted complete bipartite graph, where each edge e connecting a pair of system-reference bigrams has a weight w(e), indicating the degree of similarity between the bigrams connected." ></td>
	<td class="line x" title="106:175	Note that, without loss of generality, if the number of system nodes and reference nodes (bigrams) are not the same, we can simply add dummy nodes with connecting edges of weight 0 to obtain a complete bipartite graph with equal number of nodes on both sides." ></td>
	<td class="line x" title="107:175	In an n-gram bipartite graph, the similarity score, or the weight w(e) of the edge e connecting a system n-gram (ls1ps1, ,lsnpsn) and a reference n-gram (lr1pr1, ,lrnprn) is calculated as follows: Si = I(psi,pri) + Syn(lsi,lri)2 w(e) = 1n nsummationdisplay i=1 Si where I(psi,pri) evaluates to 1 if psi = pri, and 0 otherwise." ></td>
	<td class="line x" title="108:175	The function Syn(lsi,lri) checks whether lsi is a synonym of lri." ></td>
	<td class="line x" title="109:175	To determine this, we first obtain the set WNsyn(lsi) of WordNet synonyms for lsi and the set WNsyn(lri) of WordNet synonyms for lri." ></td>
	<td class="line x" title="110:175	Then, Syn(lsi,lri) =    1, WNsyn(lsi) \ WNsyn(lri) 6= ; 0, otherwise In gathering the set WNsyn for a word, we gather all the synonyms for all its senses and do not restrict to a particular POS category." ></td>
	<td class="line x" title="111:175	Further, if we are comparing bigrams or trigrams, we impose an additional condition: Si 6= 0, for 1  i  n, else we will set w(e) = 0." ></td>
	<td class="line x" title="112:175	This captures the intuition that in matching a system n-gram against a reference ngram, where n > 1, we require each system token to have at least some degree of similarity with the corresponding reference token." ></td>
	<td class="line x" title="113:175	In the top half of Figure 1, we show an example of a complete bipartite graph, constructed for a set of three system bigrams (s1,s2,s3) and three reference bigrams (r1,r2,r3), and the weight of the connecting edge between two bigrams represents their degree of similarity." ></td>
	<td class="line x" title="114:175	Next, we aim to find a maximum weight matching (or alignment) between the bigrams such that each system (reference) bigram is connected to exactly one reference (system) bigram." ></td>
	<td class="line x" title="115:175	This maximum weighted bipartite matching problem can be solved in O(n3) time (where n refers to the number of nodes, or vertices in the graph) using the KuhnMunkres algorithm (Kuhn, 1955; Munkres, 1957)." ></td>
	<td class="line x" title="116:175	The bottom half of Figure 1 shows the resulting maximum weighted bipartite graph, where the alignment represents the maximum weight matching, out of all possible alignments." ></td>
	<td class="line x" title="117:175	Once we have solved and obtained a maximum weight matching M for the bigram bipartite graph, we sum up the weights of the edges to obtain the weight of the matching M: w(M) = summationtexteM w(e), and add w(M) to matchbi." ></td>
	<td class="line x" title="118:175	From the unigram and trigram bipartite graphs, we similarly calculate their respective w(M) and add to the corresponding matchuni and matchtri." ></td>
	<td class="line x" title="119:175	Based on matchuni, matchbi, and matchtri, we calculate their corresponding precision P and recall R, from which we obtain their respective Fmean scores via Equation 1." ></td>
	<td class="line x" title="120:175	Using bigrams for illustration, we calculate its P and R as: P = matchbino." ></td>
	<td class="line x" title="121:175	of bigrams in system translation R = matchbino." ></td>
	<td class="line x" title="122:175	of bigrams in reference translation 4.2 Dependency Relations Besides matching a pair of system-reference sentences based on the surface form of words, previous work such as (Gimenez and Marquez, 2007) and (Rajman and Hartley, 2002) had shown that deeper linguistic knowledge such as semantic roles and syntax can be usefully exploited." ></td>
	<td class="line x" title="123:175	In the previous subsection, we describe our method of using bipartite graphs for matching of ngrams found in a sentence pair." ></td>
	<td class="line x" title="124:175	This use of bipartite graphs, however, is a very general framework to obtain an optimal alignment of the corresponding information items contained within a sentence pair." ></td>
	<td class="line x" title="125:175	Hence, besides matching based on n-gram strings, we can also match other information items, such as dependency relations." ></td>
	<td class="line o" title="126:175	59 Metric Adequacy Fluency Rank Constituent Average MAXSIMn+d 0.780 0.827 0.875 0.760 0.811 MAXSIMn 0.804 0.845 0.893 0.766 0.827 Semantic-role 0.774 0.839 0.804 0.742 0.790 ParaEval-recall 0.712 0.742 0.769 0.798 0.755 METEOR 0.701 0.719 0.746 0.670 0.709 BLEU 0.690 0.722 0.672 0.603 0.672 Table 1: Overall correlations on the Europarl and News Commentary datasets." ></td>
	<td class="line x" title="127:175	The Semantic-role overlap metric is abbreviated as Semantic-role." ></td>
	<td class="line x" title="128:175	Note that each figure above represents 6 translation tasks: the Europarl and News Commentary datasets each with 3 language pairs (German-English, Spanish-English, French-English)." ></td>
	<td class="line x" title="129:175	In our work, we train the MSTParser4 (McDonald et al., 2005) on the Penn Treebank Wall Street Journal (WSJ) corpus, and use it to extract dependency relations from a sentence." ></td>
	<td class="line x" title="130:175	Currently, we focus on extracting only two relations: subject and object." ></td>
	<td class="line x" title="131:175	For each relation (ch,dp,pa) extracted, we note the child lemma ch of the relation (often a noun), the relation type dp (either subject or object), and the parent lemma pa of the relation (often a verb)." ></td>
	<td class="line x" title="132:175	Then, using the system relations and reference relations extracted from a system-reference sentence pair, we similarly construct a bipartite graph, where each node is a relation (ch,dp,pa)." ></td>
	<td class="line x" title="133:175	We define the weight w(e) of an edge e between a system relation (chs,dps,pas) and a reference relation (chr,dpr,par) as follows: Syn(chs,chr) + I(dps,dpr) + Syn(pas,par) 3 where functions I and Syn are defined as in the previous subsection." ></td>
	<td class="line x" title="134:175	Also, w(e) is non-zero only if dps = dpr." ></td>
	<td class="line x" title="135:175	After solving for the maximum weight matching M, we divide w(M) by the number of system relations extracted to obtain a precision score P, and divide w(M) by the number of reference relations extracted to obtain a recall score R. P and R are then similarly combined into a Fmean score for the sentence pair." ></td>
	<td class="line x" title="136:175	To compute the similarity score when incorporating dependency relations, we average the Fmean scores for unigrams, bigrams, trigrams, and dependency relations." ></td>
	<td class="line o" title="137:175	5 Results To evaluate our metric, we conduct experiments on datasets from the ACL-07 MT workshop and NIST 4Available at: http://sourceforge.net/projects/mstparser Europarl Metric Adq Flu Rank Con Avg MAXSIMn+d 0.749 0.786 0.857 0.651 0.761 MAXSIMn 0.749 0.786 0.857 0.651 0.761 Semantic-role 0.815 0.854 0.759 0.612 0.760 ParaEval-recall 0.701 0.708 0.737 0.772 0.730 METEOR 0.726 0.741 0.770 0.558 0.699 BLEU 0.803 0.822 0.699 0.512 0.709 Table 2: Correlations on the Europarl dataset." ></td>
	<td class="line x" title="138:175	Adq=Adequacy, Flu=Fluency, Con=Constituent, and Avg=Average." ></td>
	<td class="line o" title="139:175	News Commentary Metric Adq Flu Rank Con Avg MAXSIMn+d 0.812 0.869 0.893 0.869 0.861 MAXSIMn 0.860 0.905 0.929 0.881 0.894 Semantic-role 0.734 0.824 0.848 0.871 0.819 ParaEval-recall 0.722 0.777 0.800 0.824 0.781 METEOR 0.677 0.698 0.721 0.782 0.720 BLEU 0.577 0.622 0.646 0.693 0.635 Table 3: Correlations on the News Commentary dataset." ></td>
	<td class="line x" title="140:175	MT 2003 evaluation exercise." ></td>
	<td class="line x" title="141:175	5.1 ACL-07 MT Workshop The ACL-07 MT workshop evaluated the translation quality of MT systems on various translation tasks, and also measured the correlation (with human judgement) of 11 automatic MT evaluation metrics." ></td>
	<td class="line x" title="142:175	The workshop used a Europarl dataset and a News Commentary dataset, where each dataset consisted of English sentences (2,000 English sentences for Europarl and 2,007 English sentences for News Commentary) and their translations in various languages." ></td>
	<td class="line x" title="143:175	As part of the workshop, correlations of the automatic metrics were measured for the tasks 60 of translating German, Spanish, and French into English." ></td>
	<td class="line x" title="144:175	Hence, we will similarly measure the correlation of MAXSIM on these tasks." ></td>
	<td class="line x" title="145:175	5.1.1 Evaluation Criteria For human evaluation of the MT submissions, four different criteria were used in the workshop: Adequacy (how much of the original meaning is expressed in a system translation), Fluency (the translations fluency), Rank (different translations of a single source sentence are compared and ranked from best to worst), and Constituent (some constituents from the parse tree of the source sentence are translated, and human judges have to rank these translations)." ></td>
	<td class="line x" title="146:175	During the workshop, Kappa values measured for interand intra-annotator agreement for rank and constituent are substantially higher than those for adequacy and fluency, indicating that rank and constituent are more reliable criteria for MT evaluation." ></td>
	<td class="line x" title="147:175	5.1.2 Correlation Results We follow the ACL-07 MT workshop process of converting the raw scores assigned by an automatic metric to ranks and then using the Spearmans rank correlation coefficient to measure correlation." ></td>
	<td class="line p" title="148:175	During the workshop, only three automatic metrics (Semantic-role overlap, ParaEval-recall, and METEOR) achieve higher correlation than BLEU." ></td>
	<td class="line x" title="149:175	We gather the correlation results of these metrics from the workshop paper (Callison-Burch et al., 2007), and show in Table 1 the overall correlations of these metrics over the Europarl and News Commentary datasets." ></td>
	<td class="line x" title="150:175	In the table, MAXSIMn represents using only n-gram information (Section 4.1) for our metric, while MAXSIMn+d represents using both ngram and dependency information." ></td>
	<td class="line x" title="151:175	We also show the breakdown of the correlation results into the Europarl dataset (Table 2) and the News Commentary dataset (Table 3)." ></td>
	<td class="line o" title="152:175	In all our results for MAXSIM in this paper, we follow METEOR and use =0.9 (weighing recall more than precision) in our calculation of Fmean via Equation 1, unless otherwise stated." ></td>
	<td class="line x" title="153:175	The results in Table 1 show that MAXSIMn and MAXSIMn+d achieve overall average (over the four criteria) correlations of 0.827 and 0.811 respectively." ></td>
	<td class="line o" title="154:175	Note that these results are substantially Metric Adq Flu Avg MAXSIMn+d 0.943 0.886 0.915 MAXSIMn 0.829 0.771 0.800 METEOR (optimized) 1.000 0.943 0.972 METEOR 0.943 0.886 0.915 BLEU 0.657 0.543 0.600 Table 4: Correlations on the NIST MT 2003 dataset." ></td>
	<td class="line x" title="155:175	higher than BLEU, and in particular higher than the best performing Semantic-role overlap metric in the ACL-07 MT workshop." ></td>
	<td class="line x" title="156:175	Also, Semantic-role overlap requires more processing steps (such as base phrase chunking, named entity tagging, etc.) than MAXSIM." ></td>
	<td class="line x" title="157:175	For future work, we could experiment with incorporating semantic-role information into our current framework." ></td>
	<td class="line x" title="158:175	We note that the ParaEvalrecall metric achieves higher correlation on the constituent criterion, which might be related to the fact that both ParaEval-recall and the constituent criterion are based on phrases: ParaEval-recall tries to match phrases, and the constituent criterion is based on judging translations of phrases." ></td>
	<td class="line x" title="159:175	5.2 NIST MT 2003 Dataset We also conduct experiments on the test data (LDC2006T04) of NIST MT 2003 Chinese-English translation task." ></td>
	<td class="line x" title="160:175	For this dataset, human judgements are available on adequacy and fluency for six system submissions, and there are four English reference translation texts." ></td>
	<td class="line o" title="161:175	Since implementations of the BLEU and METEOR metrics are publicly available, we score the system submissions using BLEU (version 11b with its default settings), METEOR, and MAXSIM, showing the resulting correlations in Table 4." ></td>
	<td class="line oc" title="162:175	For METEOR, when used with its originally proposed parameter values of (=0.9, =3.0, =0.5), which the METEOR researchers mentioned were based on some early experimental work (Banerjee and Lavie, 2005), we obtain an average correlation value of 0.915, as shown in the row METEOR." ></td>
	<td class="line x" title="163:175	In the recent work of (Lavie and Agarwal, 2007), the values of these parameters were tuned to be (=0.81, =0.83, =0.28), based on experiments on the NIST 2003 and 2004 Arabic-English evaluation datasets." ></td>
	<td class="line o" title="164:175	When METEOR was run with these new parameter values, it returned an average correlation value of 61 0.972, as shown in the row METEOR (optimized)." ></td>
	<td class="line x" title="165:175	MAXSIM using only n-gram information (MAXSIMn) gives an average correlation value of 0.800, while adding dependency information (MAXSIMn+d) improves the correlation value to 0.915." ></td>
	<td class="line x" title="166:175	Note that so far, the parameters of MAXSIM are not optimized and we simply perform uniform averaging of the different n-grams and dependency scores." ></td>
	<td class="line o" title="167:175	Under this setting, the correlation achieved by MAXSIM is comparable to that achieved by METEOR." ></td>
	<td class="line x" title="168:175	6 Future Work In our current work, the parameters of MAXSIM are as yet un-optimized." ></td>
	<td class="line x" title="169:175	We found that by setting =0.7, MAXSIMn+d could achieve a correlation of 0.972 on the NIST MT 2003 dataset." ></td>
	<td class="line x" title="170:175	Also, we have barely exploited the potential of weighted similarity matching." ></td>
	<td class="line x" title="171:175	Possible future directions include adding semantic role information, using the distance between item pairs based on the token position within each sentence as additional weighting consideration, etc. Also, we have seen that dependency relations help to improve correlation on the NIST dataset, but not on the ACL-07 MT workshop datasets." ></td>
	<td class="line x" title="172:175	Since the accuracy of dependency parsers is not perfect, a possible future work is to identify when best to incorporate such syntactic information." ></td>
	<td class="line x" title="173:175	7 Conclusion In this paper, we present MAXSIM, a new automatic MT evaluation metric that computes a similarity score between corresponding items across a sentence pair, and uses a bipartite graph to obtain an optimal matching between item pairs." ></td>
	<td class="line x" title="174:175	This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison." ></td>
	<td class="line x" title="175:175	When evaluated for correlation with human judgements, MAXSIM achieves superior results when compared to current automatic MT evaluation metrics." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-1010
Phrase Table Training for Precision and Recall: What Makes a Good Phrase and a Good Phrase Pair?
Deng, Yonggang;Xu, Jia;Gao, Yuqing;"></td>
	<td class="line x" title="1:204	Proceedings of ACL-08: HLT, pages 8188, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:204	c2008 Association for Computational Linguistics Phrase Table Training For Precision and Recall: What Makes a Good Phrase and a Good Phrase Pair?" ></td>
	<td class="line x" title="3:204	Yonggang Deng , Jia Xu+ and Yuqing Gao IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA {ydeng,yuqing}@us.ibm.com +Chair of Computer Science VI, RWTH Aachen University, D-52056 Aachen, Germany xujia@cs.rwth-aachen.de Abstract In this work, the problem of extracting phrase translation is formulated as an information retrieval process implemented with a log-linear model aiming for a balanced precision and recall." ></td>
	<td class="line x" title="4:204	We present a generic phrase training algorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance." ></td>
	<td class="line x" title="5:204	Multiple data-driven feature functions are proposed to capture the quality and confidence of phrases andphrasepairs." ></td>
	<td class="line x" title="6:204	Experimentalresultsdemonstrate consistent and significant improvement over the widely used method that is based on word alignment matrix only." ></td>
	<td class="line x" title="7:204	1 Introduction Phrase has become the standard basic translation unit in Statistical Machine Translation (SMT) since it naturally captures context dependency and models internal word reordering." ></td>
	<td class="line x" title="8:204	In a phrase-based SMT system, the phrase translation table is the defining component which specifies alternative translations and their probabilities for a given source phrase." ></td>
	<td class="line x" title="9:204	In learning such a table from parallel corpus, two related issues need to be addressed (either separately or jointly): which pairs are considered valid translations and how to assign weights, such as probabilities, to them." ></td>
	<td class="line x" title="10:204	The first problem is referred to as phrase pair extraction, which identifies phrase pairs that are supposed to be translations of each other." ></td>
	<td class="line x" title="11:204	Methods have been proposed, based on syntax, that take advantage of linguistic constraints and alignment of grammatical structure, such as in Yamada and Knight (2001) and Wu (1995)." ></td>
	<td class="line x" title="12:204	The most widely used approach derives phrase pairs from word alignment matrix (Och and Ney, 2003; Koehn et al., 2003)." ></td>
	<td class="line x" title="13:204	Other methods do not depend on word alignments only, such as directly modeling phrase alignment in a joint generative way (Marcu and Wong, 2002), pursuing information extraction perspective (Venugopal et al., 2003), or augmenting with modelbased phrase pair posterior (Deng and Byrne, 2005)." ></td>
	<td class="line x" title="14:204	Using relative frequency as translation probability is a common practice to measure goodness of a phrase pair." ></td>
	<td class="line x" title="15:204	Since most phrases appear only a few times in training data, a phrase pair translation is also evaluated by lexical weights (Koehn et al., 2003) or term weighting (Zhao et al., 2004) as additional features to avoid overestimation." ></td>
	<td class="line x" title="16:204	The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006)." ></td>
	<td class="line x" title="17:204	The focus of this paper is the phrase pair extraction problem." ></td>
	<td class="line x" title="18:204	As in information retrieval, precision and recall issues need to be addressed with a right balance for building a phrase translation table." ></td>
	<td class="line x" title="19:204	High precision requires that identified translation candidates are accurate, while high recall wants as much valid phrase pairs as possible to be extracted, which is important and necessary for online translation that requires coverage." ></td>
	<td class="line x" title="20:204	In the word-alignment derived phrase extraction approach, precision can be improved by filtering out most of the entries by using a statistical significance test (Johnson et al., 2007)." ></td>
	<td class="line x" title="21:204	On the other hand, there are valid translation pairs in the training corpus that are not learned due to word alignment errors as shown in Deng and Byrne (2005)." ></td>
	<td class="line x" title="22:204	81 We would like to improve phrase translation accuracy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments." ></td>
	<td class="line x" title="23:204	One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006)." ></td>
	<td class="line x" title="24:204	In this work, we present a generic discriminative phrase pair extraction framework that can integrate multiple features aiming to identify correct phrase translation candidates." ></td>
	<td class="line x" title="25:204	A significant deviation from most other approaches is that the framework is parameterized and can be optimized jointly with the decoder to maximize translation performance on a development set." ></td>
	<td class="line x" title="26:204	Within the general framework, the main work is on investigating useful metrics." ></td>
	<td class="line x" title="27:204	We employ features based on word alignment models and alignment matrix." ></td>
	<td class="line x" title="28:204	We also propose information metrics that are derived from both bilingual and monolingual perspectives." ></td>
	<td class="line x" title="29:204	Allthesefeaturesaredata-drivenandindependentof languages." ></td>
	<td class="line x" title="30:204	The proposed phrase extraction framework is general to apply linguistic features such as semantic, POS tags and syntactic dependency." ></td>
	<td class="line x" title="31:204	2 A Generic Phrase Training Procedure Let e = eI1 denote an English sentence and let f = fJ1 denote its translation in a foreign language, say Chinese." ></td>
	<td class="line x" title="32:204	Phrase extraction begins with sentence-aligned parallel corpora {(ei,fi)}." ></td>
	<td class="line x" title="33:204	We use E = eieib and F = fjejb to denote an English and foreign phrases respectively, where ib(jb) is the position in the sentence of the beginning word of the English(foreign) phrase and ie(je) is the position of the ending word of the phrase." ></td>
	<td class="line x" title="34:204	We first train word alignment models and will use them to evaluate the goodness of a phrase and a phrase pair." ></td>
	<td class="line x" title="35:204	Let fk(E,F),k = 1,2,,K be K feature functions to be used to measure the quality of a given phrase pair (E,F)." ></td>
	<td class="line x" title="36:204	The generic phrase extraction procedure is an evaluation, ranking, filtering, estimation and tuning process, presented in Algorithm 1." ></td>
	<td class="line x" title="37:204	Step 1 (line 1) is the preparation stage." ></td>
	<td class="line x" title="38:204	Beginning with a flat lexicon, we train IBM Model-1 word alignment model with 10 iterations for each translation direction." ></td>
	<td class="line x" title="39:204	We then train HMM word alignment models (Vogel et al., 1996) in two directions simultaneously by merging statistics collected in the Algorithm 1 A Generic Phrase Training Procedure 1: Train Model-1 and HMM word alignment models 2: for all sentence pair (e,f) do 3: Identify candidate phrases on each side 4: for all candidate phrase pair (E,F) do 5: Calculate its feature function values fk 6: Obtain the score q(E,F) =summationtextKk=1 kfk(E,F) 7: end for 8: Sort candidate phrase pairs by their final scores q 9: Find the maximum score qm = maxq(E,F) 10: for all candidate phrase pair (E,F) do 11: If q(E,F)  qm, dump the pair into the pool 12: end for 13: end for 14: Built a phrase translation table from the phrase pair pool 15: Discriminatively train feature weights k and threshold  E-step from two directions motivated by Zens et al.(2004) with 5 iterations." ></td>
	<td class="line x" title="41:204	We use these models to define the feature functions of candidate phrase pairs such as phrase pair posterior distribution." ></td>
	<td class="line x" title="42:204	More details will be given in Section 3." ></td>
	<td class="line x" title="43:204	Step 2 (line 2) consists of phrase pair evaluation, ranking and filtering." ></td>
	<td class="line x" title="44:204	Usually all n-grams up to a pre-defined length limit are considered as candidate phrases." ></td>
	<td class="line x" title="45:204	This is also the place where linguistic constraints can be applied, say to avoid noncompositionalphrases(Lin,1999)." ></td>
	<td class="line x" title="46:204	Eachnormalized feature score derived from word alignment models or language models will be log-linearly combined to generate the final score." ></td>
	<td class="line x" title="47:204	Phrase pair filtering is simply thresholding on the final score by comparing to the maximum within the sentence pair." ></td>
	<td class="line x" title="48:204	Note that under the log-linear model, applying threshold for filtering is equivalent to comparing the likelihood ratio." ></td>
	<td class="line x" title="49:204	Step 3 (line 14) pools all candidate phrase pairs that pass the threshold testing and estimates the final phrase translation table by maximum likelihood criterion." ></td>
	<td class="line x" title="50:204	For each candidate phrase pair which is above the threshold, we assign HMM-based phrase pair posterior as its soft count when dumping them into the global phrase pair pool." ></td>
	<td class="line x" title="51:204	Other possibilities for the weighting include assigning constant one or the exponential of the final score etc. One of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the trans82 lation engine to minimize the final translation errors measured by automatic metrics such as BLEU (Papineni et al., 2002)." ></td>
	<td class="line x" title="52:204	In the final step 4 (line 15), parameters {k,} are discriminatively trained on a development set using the downhill simplex method (Nelder and Mead, 1965)." ></td>
	<td class="line x" title="53:204	This phrase training procedure is general in the sense that it is configurable and trainable with different feature functions and their parameters." ></td>
	<td class="line x" title="54:204	The commonly used phrase extraction approach based on word alignment heuristics (referred as ViterbiExtract algorithm for comparison in this paper) as described in (Och, 2002; Koehn et al., 2003) is a special case of the algorithm, where candidate phrase pairs are restricted to those that respect word alignment boundaries." ></td>
	<td class="line x" title="55:204	We rely on multiple feature functions that aim to describe the quality of candidate phrase translations and the generic procedure to figure out the best way of combining these features." ></td>
	<td class="line x" title="56:204	A good feature function pops up valid translation pairs and pushes down incorrect ones." ></td>
	<td class="line x" title="57:204	3 Features Now we present several feature functions that we investigated to help extracting correct phrase translations." ></td>
	<td class="line x" title="58:204	All these features are data-driven and defined based on models, such as statistical word alignment model or language model." ></td>
	<td class="line x" title="59:204	3.1 Model-based Phrase Pair Posterior In a statistical generative word alignment model (Brown et al., 1993), it is assumed that (i) a random variable a specifies how each target word fj is generated by (therefore aligned to) a source 1 word eaj; and (ii) the likelihood function f(f,a|e) specifies a generativeprocedurefromthesourcesentencetothe target sentence." ></td>
	<td class="line x" title="60:204	Given a phrase pair in a sentence pair, there will be many generative paths that align thesourcephrasetothetargetphrase." ></td>
	<td class="line x" title="61:204	Thelikelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair (Deng and Byrne, 2005)." ></td>
	<td class="line x" title="62:204	This is implemented as the summation of the likelihood function over all valid hidden word alignments." ></td>
	<td class="line x" title="63:204	1The word source and target are in the sense of word alignment direction, not as in the source-channel formulation." ></td>
	<td class="line x" title="64:204	More specifically, let A(j1,j2)(i1,i2) be the set of word alignment a that aligns the source phrase ej1i1 to the target phrase fj2j1 (links to NULL word are ignored for simplicity): A(j1,j2)(i1,i2) = {a : aj  [i1,i2] iff j  [j1,j2]} The alignment set given a phrase pair ignores those pairs with word links across the phrase boundary." ></td>
	<td class="line x" title="65:204	Consequently, the phrase-pair posterior distribution is defined as P(ei2i1  fj2j1 |e,f) = summationtext aA(j1,j2)(i1,i2) f(a,f|e;) summationtext a f(a,f|e;) .(1) Switching the source and the target, we can obtain the posterior distribution in another translation direction." ></td>
	<td class="line x" title="66:204	This distribution is applicable to all word alignment models that follow assumptions (i) and (ii)." ></td>
	<td class="line x" title="67:204	However, the complexity of the likelihood function could make it impractical to calculate the summations in Equation 1 unless an approximation is applied." ></td>
	<td class="line x" title="68:204	Several feature functions will be defined on top of the posterior distribution." ></td>
	<td class="line x" title="69:204	One of them is based on HMM word alignment model." ></td>
	<td class="line x" title="70:204	We use the geometric mean of posteriors in two translation directions as a symmetric metric for phrase pair quality evaluation function under HMM alignment models." ></td>
	<td class="line x" title="71:204	Table 1 shows the phrase pair posterior matrix of the example." ></td>
	<td class="line x" title="72:204	Replacing the word alignment model with IBM Model-1 is another feature function that we added." ></td>
	<td class="line x" title="73:204	IBM Model-1 is simple yet has been shown to be effective in many applications (Och et al., 2004)." ></td>
	<td class="line x" title="74:204	There is a close form solution to calculate the phrase pair posterior under Model-1." ></td>
	<td class="line x" title="75:204	Moreover, word to word translation table under HMM is more concentrated than that under Model-1." ></td>
	<td class="line x" title="76:204	Therefore, the posterior distribution evaluated by Model-1 is smoother and potentially it can alleviate the overestimation problem in HMM especially when training data size is small." ></td>
	<td class="line x" title="77:204	3.2 Bilingual Information Metric Trying to find phrase translations for any possible ngram is not a good idea for two reasons." ></td>
	<td class="line x" title="78:204	First, due to data sparsity and/or alignment models capability, there would exist n-grams that cannot be aligned 83     f 1 f 2 f 3  1(that)   2(is)   34(what)   whats   that   e 1 e 2  e11 e21 e22 HBL(fj2j1 ) f11 0.0006 0.012 0.89 0.08 f21 0.0017 0.035 0.343 0.34 f31 0.07 0.999 0.0004 0.24 f22 0.03 0.0001 0.029 0.7 f32 0.89 0.006 0.006 0.05 f33 0.343 0.002 0.002 0.06 HBL(ei2i1) 0.869 0.26 0.70 Table1: Phrasepairposteriordistributionfortheexample well, for instance, n-grams that are part of a paraphrase translation or metaphorical expression." ></td>
	<td class="line x" title="79:204	To giveanexample, theunigramtomorrowintheday after tomorrow whose Chinese translation is a single word _d_1614_d_2102." ></td>
	<td class="line x" title="80:204	Extracting candidate translations for such kind of n-grams for the sake of improving coverage (recall) might hurt translation quality (precision)." ></td>
	<td class="line x" title="81:204	We will define a confidence metric to estimate how reliably the model can align an n-gram in one side to a phrase on the other side given a parallel sentence." ></td>
	<td class="line x" title="82:204	Second, some n-grams themselves carrynolinguisticmeaning; theirphrasetranslations can be misleading, for example non-compositional phrases (Lin, 1999)." ></td>
	<td class="line x" title="83:204	We will address this in section 3.3." ></td>
	<td class="line x" title="84:204	Given a sentence pair, the basic assumption is that if the HMM word alignment model can align an English phrase well to a foreign phrase, the posterior distribution of the English phrase generating all foreign phrases on the other side is significantly biased." ></td>
	<td class="line x" title="85:204	For instance, the posterior of one foreign phrase is far larger than that of the others." ></td>
	<td class="line x" title="86:204	We use the entropy oftheposteriordistributionastheconfidencemetric: HBL(ei2i1|e,f) = H( PHMM(ei2i1  )) (2) where H(P) = summationtextx P(x)logP(x) is the entropy of a distribution P(x), PHMM(ei2i1  ) is the normalized probability (sum up to 1) of the posterior PHMM(ei2i1  ) as defined in Equation 1." ></td>
	<td class="line x" title="87:204	Low entropy signals a high confidence that the English phrase can be aligned correctly." ></td>
	<td class="line x" title="88:204	On the other hand, high entropy implies ambiguity presented in discriminating the correct foreign phrase from the others from the viewpoint of the model." ></td>
	<td class="line x" title="89:204	Similarly we calculate the confidence metric of aligning a foreign phrase correctly with the word alignment model in foreign to English direction." ></td>
	<td class="line x" title="90:204	Table 1 shows the entropy of phrases." ></td>
	<td class="line x" title="91:204	The unigram of foreign side f22 is unlikely to survive with such high ambiguity." ></td>
	<td class="line x" title="92:204	Adding the entropy in two directions defines the bilingual information metric as another feature function, which describes the reliability of aligning each phrase correctly by the model." ></td>
	<td class="line x" title="93:204	Note that we used HMM word alignment model to find the posterior distribution." ></td>
	<td class="line x" title="94:204	Other models such as Model-1 can be applied in the same way." ></td>
	<td class="line x" title="95:204	This feature function quantitatively captures the goodness of phrases." ></td>
	<td class="line x" title="96:204	During phrase pair ranking, it can help to move upward phrases that can be aligned well and push downward phrases that are difficult for the model to find correct translations." ></td>
	<td class="line x" title="97:204	3.3 Monolingual Information Metric Now we turn to monolingual resources to evaluate the quality of an n-gram being a good phrase." ></td>
	<td class="line x" title="98:204	A phrase in a sentence is specified by its boundaries." ></td>
	<td class="line x" title="99:204	We assume that the boundaries of a good phrase should bethe rightplace to break." ></td>
	<td class="line x" title="100:204	Moregenerally, we want to quantify how effective a word boundary is as a phrase boundary." ></td>
	<td class="line x" title="101:204	One would perform say NP-chunking or parsing to avoid splitting a linguistic constituent." ></td>
	<td class="line x" title="102:204	We apply a language model (LM) to describe the predictive uncertainty (PU) between words in two directions." ></td>
	<td class="line x" title="103:204	Given a history wn11 , a language model specifies a conditional distribution of the future word being predicted to follow the history." ></td>
	<td class="line x" title="104:204	We can find the entropy of such pdf: HLM(wn11 ) = H(P(|wn11 ))." ></td>
	<td class="line x" title="105:204	SogivenasentencewN1 , thePU oftheboundarybetween word wi and wi+1 is established by two-way entropysumusingaforwardandbackwardlanguage model: PU(wN1 ,i) = HLMF(wi1) + HLMB(wi+1N ) We assume that the higher the predictive uncertainty is, the more likely the left or right part of the word boundary can be cut-and-pasted to form another reasonable sentence." ></td>
	<td class="line x" title="106:204	So a good phrase is characterized with high PU values on the boundaries." ></td>
	<td class="line x" title="107:204	For example, in we want to have a table near the window, the PU value of the point after table is 0.61, higher than that between near and the 0.3, using trigram LMs." ></td>
	<td class="line x" title="108:204	With this, the feature function derived from 84 monolingual clue for a phrase pair can be defined as the product of PUs of the four word boundaries." ></td>
	<td class="line x" title="109:204	3.4 Word Alignments Induced Metric The widely used ViterbiExtract algorithm relies on word alignment matrix and no-crossing-link assumption to extract phrase translation candidates." ></td>
	<td class="line x" title="110:204	Practically it has been proved to work well." ></td>
	<td class="line x" title="111:204	However, discarding correct phrase pairs due to incorrect word links leaves room for improving recall." ></td>
	<td class="line x" title="112:204	This is especially true for not significantly large training corpora." ></td>
	<td class="line x" title="113:204	Provided with a word alignment matrix, we define within phrase pair consistency ratio (WPPCR) as another feature function." ></td>
	<td class="line x" title="114:204	WPPCR was used as one of the scores in (Venugopal et al., 2003) for phrase extraction." ></td>
	<td class="line x" title="115:204	It is defined as the number of consistent word links associated with any words within the phrase pair divided by the number of all word links associated with any words within the phrase pair." ></td>
	<td class="line x" title="116:204	An inconsistent link connects a word within the phrase pair to a word outside the phrase pair." ></td>
	<td class="line x" title="117:204	For example, the WPPCR for (e21,f21) in Table 1 is 2/3." ></td>
	<td class="line x" title="118:204	As a special case, the ViterbiExtract algorithm extracts only phrase pairs with WPPCR is 1." ></td>
	<td class="line x" title="119:204	To further discriminate the pairs with higher WPPCR from those with lower ratio, we apply a BiLinear Transform (BLT) (Oppenheim and Schafer, 1989) mapping." ></td>
	<td class="line x" title="120:204	BLT is commonly used in signal processing to attenuate the low frequency parts." ></td>
	<td class="line x" title="121:204	When used to map WPPCR, it exaggerates the difference between phrase pairs with high WPPCR and those with low WPPCR, making the pairs with low ratio more unlikely to be selected as translation candidates." ></td>
	<td class="line x" title="122:204	One of the nice properties of BLT is that there is a parameter that can be changed to adjust thedegreeofattenuation, whichprovidesanotherdimension for system optimization." ></td>
	<td class="line x" title="123:204	4 Experimental Results Weevaluatetheeffectoftheproposedphraseextraction algorithm with translation performance." ></td>
	<td class="line x" title="124:204	We do experiments on IWSLT (Paul, 2006) 2006 ChineseEnglish corpus." ></td>
	<td class="line x" title="125:204	The task is to translate Chinese utterances in travel domain into English." ></td>
	<td class="line x" title="126:204	We report only text (speech transcription) translation results." ></td>
	<td class="line x" title="127:204	The training corpus consists of 40K ChineseEnglish parallel sentences in travel domain with toEval Set 04dev 04test 05test 06dev 06test # of sentences 506 500 506 489 500 # of words 2808 2906 3209 5214 5550 # of refs 16 16 16 7 7 Table 2: Dev/test set statistics tal 306K English words and 295K Chinese words." ></td>
	<td class="line x" title="128:204	In the data processing step, Chinese characters are segmented into words." ></td>
	<td class="line x" title="129:204	English text are normalized and lowercased." ></td>
	<td class="line x" title="130:204	All punctuation is removed." ></td>
	<td class="line x" title="131:204	There are five sets of evaluation sentences in tourism domain for development and test." ></td>
	<td class="line x" title="132:204	Their statistics are shown in Table 2." ></td>
	<td class="line x" title="133:204	We will tune training anddecodingparameterson06devandreportresults on other sets." ></td>
	<td class="line x" title="134:204	4.1 Training and Translation Setup Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003)." ></td>
	<td class="line x" title="135:204	Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized distortion models, sentence length penalty and other heuristics." ></td>
	<td class="line x" title="136:204	These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method." ></td>
	<td class="line x" title="137:204	The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data." ></td>
	<td class="line x" title="138:204	Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments." ></td>
	<td class="line x" title="139:204	By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed." ></td>
	<td class="line x" title="140:204	Basedonalignmentmodelsandwordalignment matrices, we compare different approaches of building a phrase translation table and show the final translation results." ></td>
	<td class="line oc" title="141:204	We measure translation performance by the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores with multiple translation references." ></td>
	<td class="line o" title="142:204	85 BLEU Scores Table 04dev 04test 05test 06dev 06test HMM 0.367 0.407 0.473 0.200 0.190 Model-4 0.380 0.403 0.485 0.210 0.204 New 0.411 0.427 0.500 0.216 0.208 METEOR Scores Table 04dev 04test 05test 06dev 06test HMM 0.532 0.586 0.675 0.482 0.471 Model-4 0.540 0.593 0.682 0.492 0.480 New 0.568 0.614 0.691 0.505 0.487 Table 3: Translation Results 4.2 Translation Results Our baseline phrase table training method is the ViterbiExtract algorithm." ></td>
	<td class="line x" title="143:204	All phrase pairs with respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion." ></td>
	<td class="line x" title="144:204	We prune phrase translation entries by their probabilities." ></td>
	<td class="line x" title="145:204	The maximum number of words in Chinese and English phrases is set to 8 and 25 respectively for all conditions2." ></td>
	<td class="line x" title="146:204	We perform online style phrase training, i.e., phrase extraction is not particular for any evaluation set." ></td>
	<td class="line x" title="147:204	Two different word alignment models are trained as the baseline, one is symmetric HMM word alignment model, the other is IBM Model-4 as implemented in the GIZA++ toolkit (Och and Ney, 2003)." ></td>
	<td class="line o" title="148:204	The translation results as measured by BLEU and METEORscoresarepresentedinTable3." ></td>
	<td class="line o" title="149:204	Wenotice that Model-4 based phrase table performs roughly 1% better in terms of both BLEU and METEOR scores than that based on HMM." ></td>
	<td class="line x" title="150:204	We follow the generic phrase training procedure as described in section 2." ></td>
	<td class="line x" title="151:204	The most time consuming part is calculating posteriors, which is carried out in parallel with 30 jobs in less than 1.5 hours." ></td>
	<td class="line x" title="152:204	We use the Viterbi word alignments from HMM to define within phrase pair consistency ratio as discussed in section 3.4." ></td>
	<td class="line x" title="153:204	Although Table 3 implies that Model-4 word alignment quality is better than that of HMM, we did not get benefits by switching to Model-4 to compute word alignments based feature values." ></td>
	<td class="line x" title="154:204	In estimating phrase translation probability, we use accumulated HMM-based phrase pair posteriors 2We chose large numbers for phrase length limit to build a strong baseline and to avoid impact of longer phase length." ></td>
	<td class="line x" title="155:204	as their soft frequencies and then the final translation probability is the relative frequency." ></td>
	<td class="line x" title="156:204	HMMbased posterior was shown to be better than treating each occurrence as count one." ></td>
	<td class="line x" title="157:204	Once we have computed all feature values for all phrase pairs in the training corpus, we discriminatively train feature weights ks and the threshold  using the downhill simplex method to maximize the BLEU score on 06dev set." ></td>
	<td class="line x" title="158:204	Since the translation engine implements a log-linear model, the discriminative training of feature weights in the decoder should be embedded in the whole end-to-end system jointly with the discriminative phrase table training process." ></td>
	<td class="line x" title="159:204	This is globally optimal but computationally demanding." ></td>
	<td class="line x" title="160:204	As a compromise, we fix the decoder feature weights and put all efforts on optimizing phrase training parameters to find out the best phrase table." ></td>
	<td class="line x" title="161:204	The translation results with the discriminatively trained phrase table are shown as the row of New in Table 3." ></td>
	<td class="line x" title="162:204	We observe that the new approach is consistentlybetterthanthebaselineViterbiExtractalgorithmwitheitherModel-4orHMMwordalignments on all sets." ></td>
	<td class="line x" title="163:204	Roughly, it has 0.5% higher BLEU score on 2006 sets and 1.5% to 3% higher on other sets than Model-4 based ViterbiExtract method." ></td>
	<td class="line o" title="164:204	Similar superior results are observed when measured with METEOR score." ></td>
	<td class="line x" title="165:204	5 Discussions The generic phrase training algorithm follows an information retrieval perspective as in (Venugopal et al., 2003) but aims to improve both precision and recall with the trainable log-linear model." ></td>
	<td class="line x" title="166:204	A clear advantage of the proposed approach over the widely used ViterbiExtractmethod is trainability." ></td>
	<td class="line x" title="167:204	Under the general framework, one can put as many features as possible together under the log-linear model to evaluate the quality of a phrase and a phase pair." ></td>
	<td class="line x" title="168:204	The phrase table extracting procedure is trainable and can be optimized jointly with the translation engine." ></td>
	<td class="line x" title="169:204	Another advantage is flexibility, which is provided partially by the threshold ." ></td>
	<td class="line x" title="170:204	As the figure 1 shows, when we increase the threshold by allowing more candidate phrase pair hypothesized as validtranslation, weobservethephrasetablesizeincreases monotonically." ></td>
	<td class="line x" title="171:204	On the other hand, we notice 86 12 34 56 78 910 1112 1314 15 0.170.180.190.2 Threshold Thresholding Effects Translation Performance 0 5 10 1555.566.5 Log10 of the number of Entries in the PhraseTable BLEU Phrasetable Size Figure 1: Thresholding effects on translation performance and phrase table size that the translation performance improves gradually." ></td>
	<td class="line x" title="172:204	After reaching its peak, the BLEU score drops as the threshold  increases." ></td>
	<td class="line x" title="173:204	When  is large enough, the translation performance is not changing much but still worse than the peak value." ></td>
	<td class="line x" title="174:204	It implies a balancing process between precision and recall." ></td>
	<td class="line x" title="175:204	The final optimal threshold  is around 5." ></td>
	<td class="line x" title="176:204	The flexibility is also enabled by multiple configurable features used to evaluate the quality of a phrase and a phrase pair." ></td>
	<td class="line x" title="177:204	Ideally, a perfect combination of feature functions divides the correct and incorrect candidate phrase pairs within a parallel sentence into two ordered separate sets." ></td>
	<td class="line x" title="178:204	We use feature functions to decide the order and the threshold  to locate the boundary guided with a development set." ></td>
	<td class="line x" title="179:204	So the main issue to investigate now is which features are important and valuable in ranking candidate phrase pairs." ></td>
	<td class="line x" title="180:204	We propose several information metrics derived from posterior distribution, language model and word alignments as feature functions." ></td>
	<td class="line x" title="181:204	The ViterbiExtract is a special case where a single binary feature function defined from word alignments is used." ></td>
	<td class="line x" title="182:204	Its good performance (as shown in Table 3) suggests that word alignments are very indicative of phrase pair quality." ></td>
	<td class="line x" title="183:204	So we design comparative experiments to capture word alignment impact only." ></td>
	<td class="line x" title="184:204	We start with basic features that include model-based posterior, bilingual and monolingual information metrics." ></td>
	<td class="line x" title="185:204	Its results on different test sets are presented in the basic row of Table 4." ></td>
	<td class="line x" title="186:204	We add word alignment feature (+align row), and Features 04dev 04test 05test 06dev 06test basic 0.393 0.406 0.496 0.205 0.199 +align 0.401 0.429 0.502 0.208 0.196 +align BLT 0.411 0.427 0.500 0.216 0.208 Table 4: Translation Results (BLEU) of discriminative phrase training approach using different features 75K250K 132K PP1 PP3PP2 Model4 New Features 04dev 04test 05test 06dev 06test PP2 0.380 0.395 0.480 0.207 0.202 PP1+PP2 0.380 0.403 0.485 0.210 0.204 PP2+PP3 0.411 0.427 0.500 0.216 0.208 PP1+PP2+PP3 0.412 0.432 0.500 0.217 0.214 Table 5: Translation Results (BLEU) of Different Phrase Pair Combination thenapplybilineartransformtotheconsistencyratio WPPCR as described in section 3.4 (+align BLT row)." ></td>
	<td class="line x" title="187:204	The parameter controlling the degree of attenuation in BLT is also optimized together with other feature weights." ></td>
	<td class="line x" title="188:204	With the basic features, the new phrase extraction approach performs better than the baseline method with HMM word alignment models but similar to the baseline method with Model-4." ></td>
	<td class="line x" title="189:204	With the word alignment based feature WPPCR, we obtain a 2% improvement on 04test set but not much on other sets except slight degradation on 06test." ></td>
	<td class="line x" title="190:204	Finally, applying BLT transform to WPPCR leads to additional 0.8 BLEU point on 06dev set and 1.2 point on 06test set." ></td>
	<td class="line x" title="191:204	This confirms the effectiveness of word alignment based features." ></td>
	<td class="line x" title="192:204	Now we compare the phrase table using the proposed method to that extracted using the baseline ViterbiExtract method with Model-4 word alignments." ></td>
	<td class="line x" title="193:204	The Venn diagram in Table 5 shows how the two phrase tables overlap with each other and size of each part." ></td>
	<td class="line x" title="194:204	As expected, they have a large number of common phrase pairs (PP2)." ></td>
	<td class="line x" title="195:204	The new method is able to extract more phrase pairs than the baseline with Model-4." ></td>
	<td class="line x" title="196:204	PP1 is the set of phrase pairs found by Model-4 alignments." ></td>
	<td class="line x" title="197:204	Removing PP1 from the baseline phrase table (comparing the first group of scores) or adding PP1 to the new phrase table 87 (the second group of scores) overall results in no or marginal performance change." ></td>
	<td class="line x" title="198:204	On the other hand, adding phrase pairs extracted by the new method only (PP3) can lead to significant BLEU score increases (comparing row 1 vs. 3, and row 2 vs. 4)." ></td>
	<td class="line x" title="199:204	6 Conclusions In this paper, the problem of extracting phrase translation is formulated as an information retrieval processimplementedwithalog-linearmodelaimingfor a balanced precision and recall." ></td>
	<td class="line x" title="200:204	We have presented a generic phrase translation extraction procedure which is parameterized with feature functions." ></td>
	<td class="line x" title="201:204	It can be optimized jointly with the translation engine to directly maximize the end-to-end translation performance." ></td>
	<td class="line x" title="202:204	Multiple feature functions were investigated." ></td>
	<td class="line x" title="203:204	Our experimental results on IWSLT ChineseEnglish corpus have demonstrated consistent and significant improvement over the widely used word alignment matrix based extraction method." ></td>
	<td class="line x" title="204:204	3 Acknowledgement We would like to thank Xiaodong Cui, Radu Florian and other IBM colleagues for useful discussions and the anonymous reviewers for their constructive suggestions." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P08-3005
A Re-examination on Features in Regression Based Approach to Automatic MT Evaluation
Sun, Shuqi;Chen, Yin;Li, Jufeng;"></td>
	<td class="line x" title="1:160	Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 2530, Columbus, June 2008." ></td>
	<td class="line x" title="2:160	c2008 Association for Computational Linguistics A Re-examination on Features in Regression Based Approach to Automatic MT Evaluation  Shuqi Sun, Yin Chen and Jufeng Li School of Computer Science and Technology Harbin Institute of Technology, Harbin, China {sqsun, chenyin, jfli}@mtlab.hit.edu.cn  Abstract Machine learning methods have been extensively employed in developing MT evaluation metrics and several studies show that it can help to achieve a better correlation with human assessments." ></td>
	<td class="line x" title="3:160	Adopting the regression SVM framework, this paper discusses the linguistic motivated feature formulation strategy." ></td>
	<td class="line x" title="4:160	We argue that blind combination of available features does not yield a general metrics with high correlation rate with human assessments." ></td>
	<td class="line x" title="5:160	Instead, certain simple intuitive features serve better in establishing the regression SVM evaluation model." ></td>
	<td class="line x" title="6:160	With six features selected, we show evidences to support our view through a few experiments in this paper." ></td>
	<td class="line x" title="7:160	1 Introduction The automatic evaluation of machine translation (MT) system has become a hot research issue in MT circle." ></td>
	<td class="line x" title="8:160	Compared with the huge amount of manpower cost and time cost of human evaluation, the automatic evaluations have lower cost and reusability." ></td>
	<td class="line x" title="9:160	Although the automatic evaluation metrics have succeeded in the system level, there are still on-going investigations to get reference translation better (Russo-Lassner et al., 2005) or to deal with sub-document level evaluation (Kulesza et al., 2004; Leusch et al, 2006)." ></td>
	<td class="line x" title="10:160	N-grams co-occurrence based metrics such as BLEU and NIST can reach a fairly good correlation with human judgments, but due to their consideration for the capability of generalization across multiple languages, they discard the inherent linguistic knowledge of the sentence evaluated." ></td>
	<td class="line x" title="11:160	Actually, for a certain target language, one could exploit this knowledge to help us developing a more human-like metric." ></td>
	<td class="line x" title="12:160	Gimnez and Mrquez (2007) showed that compared with metrics limited in lexical dimension, metrics integrating deep linguistic information will be more reliable." ></td>
	<td class="line x" title="13:160	The introduction of machine learning methods aimed at the improvement of MT evaluation metrics precision is a recent trend." ></td>
	<td class="line x" title="14:160	Corston-Oliver et al.(2001) treated the evaluation of MT outputs as classification problem between human translation and machine translation." ></td>
	<td class="line x" title="16:160	Kulesza et al.(2004) proposed a SVM classifier based on confidence score, which takes the distance between feature vector and the decision surface as the measure of the MT systems output." ></td>
	<td class="line x" title="18:160	Joshua S. Albrecht et al.(2007) adopted regression SVM to improve the evaluation metric." ></td>
	<td class="line x" title="20:160	In the rest of this paper, we will first discuss some pitfalls of the n-gram based metrics such as BLEU and NIST, together with the intuition that factors from the linguist knowledge can be used to evaluate MT systems outputs." ></td>
	<td class="line x" title="21:160	Then, we will propose a MT evaluation metric based on SVM regression using information from various linguistic levels (lexical level, phrase level, syntax level and sentence-level) as features." ></td>
	<td class="line x" title="22:160	Finally, from empirical studies, we will show that this metric, with less simple linguistic motivated features, will result in a better correlation with human judgments than previous regression-based methods." ></td>
	<td class="line x" title="23:160	2 N-gram Based vs Linguistic Motivated Metrics N-gram co-occurrence based metrics is the main trend of MT evaluation." ></td>
	<td class="line x" title="24:160	The basic idea is to compute the similarity between MT system output and 25 several human reference translations through the co-occurrence of n-grams." ></td>
	<td class="line x" title="25:160	BLEU (Papineni et al., 2002) is one of the most popular automatic evaluation metrics currently used." ></td>
	<td class="line x" title="26:160	Although with a good correlation with human judgment, it still has some defects:   BLEU considers precision regardless of recall." ></td>
	<td class="line x" title="27:160	To avoid a low recall, BLEU introduces a brevity penalty factor, but this is only an approximation." ></td>
	<td class="line x" title="28:160	  Though BLEU makes use of high order ngrams to assess the fluency of a sentence, it does not exploit information from inherent structures of a sentence." ></td>
	<td class="line x" title="29:160	  BLEU is a perfect matching only metric." ></td>
	<td class="line x" title="30:160	This is a serious problem." ></td>
	<td class="line x" title="31:160	Although it can be alleviated by adding more human reference translations, there may be still a number of informative words that will be labeled as unmatched." ></td>
	<td class="line x" title="32:160	  BLEU lacks models determining each ngrams own contribution to the meaning of the sentence." ></td>
	<td class="line x" title="33:160	Correct translations of the headwords which express should be attached more importance to than that of accessory words e.g.   While computing geometric average of precisions from unigram to n-gram, if a certain precision is zero, the whole score will be zero." ></td>
	<td class="line x" title="34:160	In the evaluation task of a MT system with certain target language, the intuition is that we can fully exploit linguistic information, making the evaluation progress more human-like while leaving the capability of generalization across multiple languages (just the case that BLEU considers) out of account." ></td>
	<td class="line x" title="35:160	Following this intuition, from the plentiful linguist information, we take the following factors in to consideration:   Content words are important to the semantic meaning of a sentence." ></td>
	<td class="line x" title="36:160	A better translation will include more substantives translated from the source sentence than worse ones." ></td>
	<td class="line x" title="37:160	In a similar way, a machine translation should be considered a better one, if more content words in human reference translations are included in it." ></td>
	<td class="line x" title="38:160	  At the phrase level, the situation above remains the same, and what is more, real phrases are used to measure the quality of the machine translations instead of merely using n-grams which are of little semantic information." ></td>
	<td class="line x" title="39:160	  In addition, the length of translation is usually in good proportion to the source language." ></td>
	<td class="line x" title="40:160	We believe that a human reference translation sentence has a moderate byte-length ratio to the source sentence." ></td>
	<td class="line x" title="41:160	So a machine translation will be depreciated if it has a ratio considerably different from the ratio calculated from reference sentences." ></td>
	<td class="line x" title="42:160	  Finally, a good translation must be a wellformed sentence, which usually brings a high probability score in language models, e.g. n-gram model." ></td>
	<td class="line x" title="43:160	In the next section, using regression SVM, we will build a MT evaluation metric for ChineseEnglish translation with features selected from above aspects." ></td>
	<td class="line x" title="44:160	3 A Regression SVM Approach Based on Linguistic Motivated Features Introducing machine learning methods to establish MT evaluation metric is a recent trend." ></td>
	<td class="line x" title="45:160	Provided that we could get many factors of human judgments, machine learning will be a good method to combine these factors together." ></td>
	<td class="line x" title="46:160	As proved in the recent literature, learning from regression is of a better quality than from classifier (Albrecht and Hwa, 2007; Russo-Lassner et al., 2005; Quirk, 2004)." ></td>
	<td class="line x" title="47:160	In this paper, we choose regression support vector machine (SVM) as the learning model." ></td>
	<td class="line x" title="48:160	3.1 Learning from human assessment data The machine translated sentences for model training are provided with human assessment data score together with several human references." ></td>
	<td class="line x" title="49:160	Each sentence is treated as a training example." ></td>
	<td class="line x" title="50:160	We extract feature vectors from training examples, and human assessment score will act as the output of the target function." ></td>
	<td class="line x" title="51:160	The regression SVM will generate an approximated function which maps multidimensional feature vectors to a continuous real value with a minimal error rate according to a loss function." ></td>
	<td class="line x" title="52:160	This value is the result of the evaluation process." ></td>
	<td class="line x" title="53:160	Figure 1 shows our general framework for regression based learning, in which we train the SVM with a number of sentences x 1 , x 2 ,  with human assessment scores y 1 , y 2 ,  and use the trained model to evaluate an test sentence x with feature vector (f 1 , f 2 ,, f n )." ></td>
	<td class="line x" title="54:160	To determine which indicators of a sentence are chosen as features is research in progress, but we contend that the more features, the better quality is not always true." ></td>
	<td class="line x" title="55:160	Large feature sets require more computation cost, though maybe result in a metric with a better corre26 lation with human judgments, it can also be achieved by introducing a much smaller feature set." ></td>
	<td class="line x" title="56:160	Moreover, features may conflict with each others, and bring down the performance of the metric." ></td>
	<td class="line x" title="57:160	We will show this in the next section, using less than 10 features stated in section 3.2." ></td>
	<td class="line x" title="58:160	Some details of the implementation will also be described." ></td>
	<td class="line x" title="59:160	Figure 1: SVM based model of automatic MT evaluation metric 3.2 Feature selection A great deal of information can be extracted from the MT systems output using linguistic knowledge." ></td>
	<td class="line x" title="60:160	Some of them can be very informative while easy to obtain." ></td>
	<td class="line x" title="61:160	As considered in section 2, we choose factors from lexical level, phrase level, syntax level and sentence-level as features to train the SVM." ></td>
	<td class="line x" title="62:160	  Features based on translation quality of content words The motivation is that content words are carrying more important information of a sentence compared with function words." ></td>
	<td class="line x" title="63:160	In this paper, content words include nouns, verbs, adjectives, adverbials, pronouns and cardinal numerals." ></td>
	<td class="line x" title="64:160	The corresponding features are the precision of content words defined in Eq." ></td>
	<td class="line x" title="65:160	1 and the recall defined in Eq." ></td>
	<td class="line x" title="66:160	2 where ref means reference translation." ></td>
	<td class="line x" title="67:160	() #_ ___ #__ con precision t correctly translated cons in t cons in t =         (1) () #___ _ __ #___ con recall t cons in ref correctly translated in t cons in the ref =     (2)   Features based on cognate words matching English words have plenty of morphological changes." ></td>
	<td class="line x" title="68:160	So if a machine translation sentence shares with a human reference sentence some cognates, it contains at least some basic information correct." ></td>
	<td class="line x" title="69:160	And if we look at it in another way, words that do not match in the original text maybe match after morphological reduction." ></td>
	<td class="line x" title="70:160	Thus, differences between poor translations will be revealed." ></td>
	<td class="line x" title="71:160	Similarly, we here define the content word precision and recall after morphological reduction in Eq." ></td>
	<td class="line x" title="72:160	3 and Eq." ></td>
	<td class="line x" title="73:160	4 where mr_cons means content words after morphological reduction: _ () #_ ____ #_ __ mr con precision t correctly translated mr cons in t mr cons in t =   (3) _ () #_ __ _ _ __ #_ ___ mr con recall t mr cons in ref correctly translated in t mr cons in the ref =  (4)   Features based on translation quality of phrases Phrases are baring the weight of semantic information more than words." ></td>
	<td class="line x" title="74:160	In manual evaluation, or rather, in a humans mind, phrases are paid special attention to." ></td>
	<td class="line x" title="75:160	Here we parse every sentence 1  and extract several types of phrases, then, compute the precision and recall of each type of phrase according to Eq." ></td>
	<td class="line x" title="76:160	5 and Eq." ></td>
	<td class="line x" title="77:160	6 2 : tinphrs tinphrstranslatedcorrectly tprecision phr __# ____# )( =      (5) reftheinphr tintranslatedcorrectlyrefinphr trecall phr ___# ______# )( =    (6) In practice, we found that if we compute these two indicators by matching phrases caseinsensitive, we will receive a metric with higher performance." ></td>
	<td class="line x" title="78:160	We speculate that by doing this the difference between poor translations is revealed just like morphological reduction." ></td>
	<td class="line x" title="79:160	  Features based on byte-length ratio Gale and Church (1991) noted that he bytelength ratio of target sentence to source sentence is normally distributed." ></td>
	<td class="line x" title="80:160	We employ this observation by computing the ratio of reference sentences to  1  The parser we used is proposed by Michael Collins in Collins (1999)." ></td>
	<td class="line x" title="81:160	2  Only precision and recall of NP are used so far." ></td>
	<td class="line x" title="82:160	Other types of phrase will be added in future study." ></td>
	<td class="line x" title="83:160	Machine Translation Sentence Feature extraction x = (f 1 , f 2 ,, f n )  Regression SVM   y = g(x) Assessment x 2 =(f 1 , f 2 ,, f n ), y = y 2 x 1 =(f 1 , f 2 ,, f n ), y = y 1 Training Set  27 source sentences, and then calculating the mean c and variance s of this ratio." ></td>
	<td class="line x" title="84:160	So if we take the ratio r as a random variable, (r-c)/s has a normal distribution with mean 0 and variance 1." ></td>
	<td class="line x" title="85:160	Then we compute the same ratio of machine translation sentence to source sentence, and take the output of p-norm function as a feature: ) __/__ ()( s csrcoflengthtoflenght Ptf norm  =      (7)   Features based on parse score The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005)." ></td>
	<td class="line x" title="86:160	However, the language model is widely adopted in MT, resulting less discrimination power." ></td>
	<td class="line x" title="87:160	And the present parser is still not satisfactory, leading much noise in parse structure matching." ></td>
	<td class="line x" title="88:160	To avoid these pitfalls in using LM and parser, here we notice that the score of a parse by the parser also reflects the quality of a sentence." ></td>
	<td class="line x" title="89:160	It may be regarded as a syntactic based language model score as well as an approximate representation of parse structure." ></td>
	<td class="line x" title="90:160	Here we introduce the feature based on parsers score as: parserbygiventofmark tscorepaser _____ 100 )(_  =            (8) 4 Experiments We use SVM-Light (Joachims 1999) to train our learning models." ></td>
	<td class="line x" title="91:160	Our main dataset is NISTs 2003 Chinese MT evaluations." ></td>
	<td class="line x" title="92:160	There are 6919=5514 sentences generated by six systems together with human assessment data which contains a fluency score and adequacy score marked by two human judges." ></td>
	<td class="line x" title="93:160	Because there is bias in the distributions of the two judges assessment, we normalize the scores following Blatz et al.(2003)." ></td>
	<td class="line x" title="95:160	The normalized score is the average of the sum of the normalized fluency score and the normalized adequacy score." ></td>
	<td class="line x" title="96:160	To determine the quality of a metric, we use Spearman rank correlation coefficient which is distribution-independent between the score given to the evaluative data and human assessment data." ></td>
	<td class="line x" title="97:160	The Spearman coefficient is a real number ranging from -1 to +1, indicating perfect negative correlations or perfect positive correlations." ></td>
	<td class="line x" title="98:160	We take the correlation rates of the metrics reported in Albrecht and Hwa (2007) and a standard automatic metric BLEU as a baseline comparison." ></td>
	<td class="line x" title="99:160	Among the features described in section 3.2, we finally adopted 6 features:   Content words precision and recall after morphological reduction defined in Eq." ></td>
	<td class="line x" title="100:160	3 and Eq." ></td>
	<td class="line x" title="101:160	4." ></td>
	<td class="line x" title="102:160	  Noun-phrases case insensitive precision and recall." ></td>
	<td class="line x" title="103:160	  P-norm (Eq." ></td>
	<td class="line x" title="104:160	7) functions output." ></td>
	<td class="line x" title="105:160	  Rescaled parser score defined in Eq." ></td>
	<td class="line x" title="106:160	8." ></td>
	<td class="line x" title="107:160	Our first experiment will compare the correlation rate between metric using rescaled parser score and that using parser score directly." ></td>
	<td class="line x" title="108:160	4.1 Different kernels Intuitively, features and the resulting assessment are not in a linear correlation." ></td>
	<td class="line x" title="109:160	We trained two SVM, one with linear kernel and the other with Gaussian kernel, using NIST 2003 Chinese dataset." ></td>
	<td class="line x" title="110:160	Then we apply the two metrics on NIST 2002 Chinese Evaluation dataset which has 3878=2634 sentences (3 systems total)." ></td>
	<td class="line x" title="111:160	The results are summarized in Table 1." ></td>
	<td class="line x" title="112:160	For comparison, the result from BLEU is also included." ></td>
	<td class="line x" title="113:160	Feature Linear Gaussian  BLEU Rescale 0.320 0.329 Direct 0.317 0.224 0.244 Table 1: Spearman rank-correlation coefficients for regression based metrics using linear and Gaussian kernel, and using rescaled parser score or directly the parser score." ></td>
	<td class="line x" title="114:160	Coefficient for BLEU is also involved." ></td>
	<td class="line x" title="115:160	Table 1 shows that the metric with Gaussian kernel using rescaled parser score gains the highest correlation rate." ></td>
	<td class="line x" title="116:160	That is to say, Gaussian kernel function can capture characteristics of the relation better, and rescaling the parser score can help to increase the correlation with human judgments." ></td>
	<td class="line x" title="117:160	Moreover, as other features range from 0 to 1, we can discover in the second row of Table 1 that Gaussian kernel is suffering more seriously from the parser score which is ranging distinctly." ></td>
	<td class="line x" title="118:160	In following experiments, we will adopt Gaussian kernel to train the SVM and rescaled parser score as a feature." ></td>
	<td class="line x" title="119:160	4.2 Comparisons within the year 2003 We held out 1/6 of the assessment dataset for parameter turning, and on the other 5/6 of dataset, we perform a five-fold cross validation to verify the metrics performance." ></td>
	<td class="line oc" title="120:160	In comparison we introduce 28 several metrics coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set." ></td>
	<td class="line o" title="121:160	The results are summarized in Table 2: Metric Coefficient Our Metric 0.515 Albrecht, 2007 0.520 Smoothed BLEU 0.272 METEOR 0.318 HWCM 0.288 Table 2: Comparison among various metrics." ></td>
	<td class="line x" title="122:160	Learningbased metrics are developed from NIST 2003 Chinese Evaluation dataset and tested under five-fold cross validation." ></td>
	<td class="line x" title="123:160	Compared with reference based metrics such as BLEU, the regression based metrics yield a higher correlation rate." ></td>
	<td class="line x" title="124:160	Generally speaking, for a given source sentence, there is usually a lot of feasible translations, but reference translations are always limited though this can be eased by adding references." ></td>
	<td class="line x" title="125:160	On the other hand, regression based metrics is independent of references and make the assessment by mapping features to the score, so it can make a better judgment even dealing with a translation that doesnt match the reference well." ></td>
	<td class="line x" title="126:160	We can also see that our metric which uses only 6 features can reach a pretty high correlation rate which is close to the metric proposed in Albrecht and Hwa (2007) using 53 features." ></td>
	<td class="line x" title="127:160	That confirms our speculation that a small feature set can also result in a metric having a good correlation with human judgments." ></td>
	<td class="line x" title="128:160	4.3 Crossing years Though the training set and test set in the experiment described above are not overlapping, in the last, they come from the same dataset (NIST 2003)." ></td>
	<td class="line x" title="129:160	The content of this dataset are Xinhua news and AFC news from Jan. 2003 to Feb. 2003 which has an inherent correlation." ></td>
	<td class="line x" title="130:160	To test the capability of generalization of our metric, we trained a metric on the whole NIST 2003 Chinese dataset (20% data are held out for parameter tuning) and applied it onto NIST 2002 Chinese Evaluation dataset." ></td>
	<td class="line x" title="131:160	We use the same metrics introduced in section 4.2 for comparison." ></td>
	<td class="line o" title="132:160	The results are summarized in Table 3:  Metric Coefficient Our Metric 0.329 Albrecht, 2007 0.309 Smoothed BLEU 0.269 METEOR 0.290 HWCM 0.260 Table 3: Cross year experiment result." ></td>
	<td class="line x" title="133:160	All the learning based metrics are developed from NIST 2003." ></td>
	<td class="line x" title="134:160	The content of NIST 2002 Chinese dataset is Xinhua news and Zaobaos online news from Mar. 2002 to Apr. 2002." ></td>
	<td class="line x" title="135:160	The most remarkable characteristic of news is its timeliness." ></td>
	<td class="line x" title="136:160	News come from the year 2002 are nearly totally unrelated to that from the year 2003." ></td>
	<td class="line x" title="137:160	It can be seen from Table 3 that we have got the expected results." ></td>
	<td class="line x" title="138:160	Our metric can generalize well across years and yields a better correlation with human judgments." ></td>
	<td class="line x" title="139:160	4.4 Discussions Albrecht and Hwa (2007) and this paper both adopted a regression-based learning method." ></td>
	<td class="line x" title="140:160	In fact, the preliminary experiment is strictly set according to their paper." ></td>
	<td class="line x" title="141:160	The most distinguishing difference is that the features in Albrecht and Hwa (2007) are collections of existing automatic evaluation metrics." ></td>
	<td class="line o" title="142:160	The total 53 features are computationally heavy (for the features from METEOR, ROUGE, HWCM and STM)." ></td>
	<td class="line x" title="143:160	In comparison, our metric made use of six features coming from linguistic knowledge which can be easily obtained." ></td>
	<td class="line x" title="144:160	Moreover, the experiments show that our metric can reach a correlation with human judgments nearly as good as the metric described in Albrecht and Hwa (2007), with a much lower computation cost." ></td>
	<td class="line x" title="145:160	And when we applied it to a different years dataset, its correlation rate is much better than that of the metric from Albrecht and Hwa (2007), showing us a good capability of generalization." ></td>
	<td class="line x" title="146:160	To account for this, we deem that the regression model is not resistant to data overfiting." ></td>
	<td class="line x" title="147:160	If provided too much cross-dependent features for a limited training data, the model is prone to a less generalized result." ></td>
	<td class="line x" title="148:160	But, it is difficult in practice to locate those key features in human perception of translation quality because we are lack of explicit evidences on what human actually use in translation evaluation." ></td>
	<td class="line x" title="149:160	In such cases, this paper uses only simple feature in key linguistic aspects, which reduces the risk of overfitting and bring a more generalized regression results." ></td>
	<td class="line x" title="150:160	29 Compared with the literature, the byte-length ratio between source and translation and the parse score are original in automatic MT evaluation modeling." ></td>
	<td class="line x" title="151:160	The parse score is proved to be a good alternative to LM." ></td>
	<td class="line x" title="152:160	And it helps to avoid the errors of parser in parse structure (the experiment to verify this claim is still on-going)." ></td>
	<td class="line x" title="153:160	It should be noted that feature selection is accomplished by empirically exhaustive test on the combination of the candidate features." ></td>
	<td class="line x" title="154:160	In future work, we will test if this strategy will help to get better results for MT evaluation, e.g. try-on the selection between the 53 features in Albrecht and Hwa (2007)." ></td>
	<td class="line x" title="155:160	And, we will also test to see if linguistic motivated feature augmentation would bring further benefit." ></td>
	<td class="line x" title="156:160	5 Conclusion For the metrics based on regressing, it is not always true that more features and complex features will help in performance." ></td>
	<td class="line x" title="157:160	If we choose features elaborately, simple features are also effective." ></td>
	<td class="line x" title="158:160	In this paper we proposed a regression based metric with a considerably small feature set that yield performance of the same level to the metrics with a large set of 53 features." ></td>
	<td class="line x" title="159:160	And the experiment of the cross-year validation proves that our metric bring a more generalized evaluation results by correlating with human judgments better." ></td>
	<td class="line x" title="160:160	Acknowledgements This research is support by Natural Science Foundation of China (Grant No. 60773066) and National 863 Project (Grant No. 2006AA01Z150)" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0301
An Empirical Study in Source Word Deletion for Phrase-Based Statistical Machine Translation
Li, Chi-Ho;Zhang, Hailei;Zhang, Dongdong;Li, Mu;Zhou, Ming;"></td>
	<td class="line x" title="1:195	Proceedings of the Third Workshop on Statistical Machine Translation, pages 18, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:195	c2008 Association for Computational Linguistics An Empirical Study in Source Word Deletion for Phrase-based Statistical Machine Translation Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou Microsoft Research Asia Beijing, China chl, dozhang@microsoft.com muli, mingzhou@microsoft.com Hailei Zhang Northeastern University of China Shenyang, China hailei.zh@gmail.com Abstract The treatment of spurious words of source language is an important problem but often ignored in the discussion on phrase-based SMT." ></td>
	<td class="line x" title="3:195	This paper explains why it is important and why it is not a trivial problem, and proposes three models to handle spurious source words." ></td>
	<td class="line x" title="4:195	Experiments show that any source word deletion model can improve a phrase-based system by at least 1.6 BLEU points and the most sophisticated model improves by nearly 2 BLEU points." ></td>
	<td class="line x" title="5:195	This paper also explores the impact of training data size and training data domain/genre on source word deletion." ></td>
	<td class="line x" title="6:195	1 Introduction It is widely known that translation is by no means word-to-word conversion." ></td>
	<td class="line x" title="7:195	Not only because sometimes a word in some language translates as more than one word in another language, also every language has some spurious words which do not have any counterpart in other languages." ></td>
	<td class="line x" title="8:195	Consequently, an MT system should be able to identify the spurious words of the source language and not translate them, as well as to generate the spurious words of the target language." ></td>
	<td class="line x" title="9:195	This paper focuses on the first task and studies how it can be handled in phrase-based SMT." ></td>
	<td class="line x" title="10:195	An immediate reaction to the proposal of investigating source word deletion (henceforth SWD) is: Is SWD itself worth our attention?" ></td>
	<td class="line x" title="11:195	Isnt it a trivial task that can be handled easily by existing techniques?" ></td>
	<td class="line x" title="12:195	One of the reasons why we need to pay attention to SWD is its significant improvement to translation performance, which will be shown by the experiments results in section 4.2." ></td>
	<td class="line x" title="13:195	Another reason is that SWD is not a trivial task." ></td>
	<td class="line x" title="14:195	While some researchers think that the spurious words of a language are merely function words or grammatical particles, which can be handled by some simple heuristics or statistical means, there are in fact some tricky cases of SWD which need sophisticated solution." ></td>
	<td class="line x" title="15:195	Consider the following example in Chinese-to-English translation: in English we have the subordinate clause according to NP, where NP refers to some source of information." ></td>
	<td class="line x" title="16:195	The Chinese equivalent of this clause can sometimes be ACCORDING-TO/ NP EXPRESS/,+; that is, in Chinese we could have a clause rather than a noun phrase following the preposition ACCORDING-TO/ . Therefore, when translating Chinese into English, the content word EXPRESS/,+ should be considered spurious and not to be translated." ></td>
	<td class="line x" title="17:195	Of course, the verb EXPRESS/,+ is not spurious in other contexts." ></td>
	<td class="line x" title="18:195	It is an example that SWD is not only about a few function words, and that the solution to SWD has to take context-sensitive factors into account." ></td>
	<td class="line x" title="19:195	Moreover, the solution needed for such tricky cases seems to be beyond the scope of current phrase-based SMT, unless we have a very large amount of training data which covers all possible variations of the Chinese pattern ACCORDING-TO/ NP EXPRESS/,+." ></td>
	<td class="line x" title="20:195	Despite the obvious need for handling spurious source words, it is surprising that phrasebased SMT, which is a major approach to SMT, does not well address the problem." ></td>
	<td class="line x" title="21:195	There are two possible ways for a phrase-based system to deal with SWD." ></td>
	<td class="line x" title="22:195	The first one is to allow a source 1 language phrase to translate to nothing." ></td>
	<td class="line x" title="23:195	However, no existing literature has mentioned such a possibility and discussed the modifications required by such an extension." ></td>
	<td class="line x" title="24:195	The second way is to capture SWD within the phrase pairs in translation table." ></td>
	<td class="line x" title="25:195	That is, suppose there is a foreign phrase F = (fAfBfC) and an English phrase E = (eAeC), where fA is aligned to eA and fC to eC, then the phrase pair (F, E) tacitly deletes the spurious word fB." ></td>
	<td class="line x" title="26:195	Such a SWD mechanism fails when data sparseness becomes a problem." ></td>
	<td class="line x" title="27:195	If the training data does not have any word sequence containing fB, then the spurious fB cannot associate with other words to form a phrase pair, and therefore cannot be deleted tacitly in some phrase pair." ></td>
	<td class="line x" title="28:195	Rather, the decoder can only give a phrase segmentation that treats fB itself as a phrase, and this phrase cannot translate into nothing, as far as the SMT training and decoding procedure reported by existing literature are used." ></td>
	<td class="line x" title="29:195	In sum, the current mechanism of phrase-based SMT is not capable of handling all cases of SWD." ></td>
	<td class="line x" title="30:195	In this paper, we will present, in section 3, three SWD models and elaborate how to apply each of them to phrase-based SMT." ></td>
	<td class="line x" title="31:195	Experiment settings are described in section 4.1, followed by the report and analysis of experiment results, using BLEU as evaluation metric, in section 4.2, which also discusses the impact of training data size and training data domain on SWD models." ></td>
	<td class="line x" title="32:195	Before making our conclusions, the effect of SWD on another evaluation metric, viz." ></td>
	<td class="line o" title="33:195	METEOR, is examined in section 5." ></td>
	<td class="line x" title="34:195	2 Literature Review Research work in SMT seldom treats SWD as a problem separated from other factors in translation." ></td>
	<td class="line x" title="35:195	However, it can be found in different SMT paradigms the mechanism of handling SWD." ></td>
	<td class="line x" title="36:195	As to the pioneering IBM word-based SMT models (Brown et al., 1990), IBM models 3, 4 and 5 handle spurious source words by considering them as corresponding to a particular EMPTY word token on the English side, and by the fertility model which allows the English EMPTY to generate a certain number of foreign words." ></td>
	<td class="line x" title="37:195	As to the hierarchical phrase-based approach (Chiang, 2007), its hierarchical rules are more powerful in SWD than the phrase pairs in conventional phrase-based approach." ></td>
	<td class="line x" title="38:195	For instance, the ACCORDING-TO/ NP EXPRESS/,+ example in the last section can be handled easily by the hierarchical rule X < X,+, according to X > . In general, if the deletion of a source word depends on some context cues, then the hierarchical approach is, at least in principle, capable of handling it correctly." ></td>
	<td class="line x" title="39:195	However, it is still confronted by the same problem as the conventional phrase-based approach regarding those words whose spuriousness does not depend on any context." ></td>
	<td class="line x" title="40:195	3 Source Word Deletion Models This section presents a number of solutions to the problem of SWD." ></td>
	<td class="line x" title="41:195	These solutions share the same property that a specific empty symbol epsilon1 on the target language side is posited and any source word is allowed to translate into epsilon1." ></td>
	<td class="line x" title="42:195	This symbol is invisible in every module of the decoder except the translation model." ></td>
	<td class="line x" title="43:195	That is, epsilon1 is not counted when calculating language model score, word penalty and any other feature values, and it is omitted in the final output of the decoder." ></td>
	<td class="line x" title="44:195	It is only used to delete spurious source words and refine translation model scores accordingly." ></td>
	<td class="line x" title="45:195	It must be noted that in our approach phrases comprising more than one source word are not allowed to translate into epsilon1." ></td>
	<td class="line x" title="46:195	This constraint is based on our subjective evaluation of alignment matrix, which indicates that the un-alignment of a continuous sequence of two or more source words is far less accurate than the un-alignment of a single source word lying within aligned neighbors." ></td>
	<td class="line x" title="47:195	Consequently, in order to treat a source word as spurious, the decoder must give a phrase segmentation that treats the word itself as a phrase." ></td>
	<td class="line x" title="48:195	Another important modification to the phrasebased architecture is a new feature added to the log-linear model." ></td>
	<td class="line x" title="49:195	The new feature, epsilon1-penalty, represents how many source words translate into epsilon1." ></td>
	<td class="line x" title="50:195	The purpose of this feature is the same as that of the feature of word penalty." ></td>
	<td class="line x" title="51:195	As many features used in the log-linear model have values of logarithm of probability, candidate translations with more words have, in general, lower scores, and 2 Model 1 P(epsilon1) Model 2 P(epsilon1|f) Model 3 PCRF(epsilon1|vectorF(f) Table 1: Summary of the Three SWD Models therefore the decoder has a bias towards shorter translations." ></td>
	<td class="line x" title="52:195	Word penalty (in fact, it should be renamed as word reward) is used to neutralize this bias." ></td>
	<td class="line x" title="53:195	Similarly, the more source words translate into epsilon1, the shorter the translation will be, and therefore the higher score the translation will have." ></td>
	<td class="line x" title="54:195	The epsilon1-penalty is proposed to neutralize the bias towards shorter translations." ></td>
	<td class="line x" title="55:195	The core of the solutions is the SWD model, which calculates P(epsilon1|f), the probability distribution of translating some source word f to epsilon1." ></td>
	<td class="line x" title="56:195	Three SWD models will be elaborated in the following subsections." ></td>
	<td class="line x" title="57:195	They differ from each other by the conditions of the probability distribution, as summarized in Table 1." ></td>
	<td class="line x" title="58:195	Model 1 is a uniform probability distribution that does not take the source word f into account." ></td>
	<td class="line x" title="59:195	Model 2 is a simple probability distribution conditioned on the lexical form of f only." ></td>
	<td class="line x" title="60:195	Model 3 is a more complicated distribution conditioned on a feature vector of f, and the distribution is estimated by the method of Conditional Random Field." ></td>
	<td class="line x" title="61:195	3.1 Model 1: Uniform Probability The first model assumes a uniform probability of translation to epsilon1." ></td>
	<td class="line x" title="62:195	This model is inspired by the HMM-based alignment model (Och and Ney, 2000a), which posits a probability P0 for alignment of some source word to the empty word on the target language side, and weighs all other alignment probabilities by the factor 1  P0." ></td>
	<td class="line x" title="63:195	In the same style, SWD model 1 posits a probability P(epsilon1) for the translation of any source word to epsilon1." ></td>
	<td class="line x" title="64:195	The probabilities of normal phrase pairs should be weighed accordingly." ></td>
	<td class="line x" title="65:195	For a source phrase containing only one word, its weight is simply P(epsilon1) = 1  P(epsilon1)." ></td>
	<td class="line x" title="66:195	As to a source phrase containing more than one word, it implies that every word in the phrase does not translate into epsilon1, and therefore the weighing factor P(epsilon1) should be multiplied as many times as the number of words in the source phrase." ></td>
	<td class="line x" title="67:195	In sum, for any phrase pair < F, E >, its probability is P( E|F) = braceleftBigg P(epsilon1) if E = (epsilon1) P(epsilon1)|F|PT( E|F) otherwise where PT( E|F) is the probability of the phrase pair as registered in the translation table, and |F| is the length of the phrase F. The estimation of P(epsilon1) is done by MLE: P(epsilon1) = number of unaligned source word tokensnumber of source word tokens . 3.2 Model 2: EMPTY as Normal Word Model 1 assumes that every word is as likely to be spurious as any other word." ></td>
	<td class="line x" title="68:195	Definitely this is not a reasonable assumption, since certain function words and grammatical particles are more likely to be spurious than other words." ></td>
	<td class="line x" title="69:195	Therefore, in our second SWD model the probability of translating a source word f to epsilon1 is conditioned on f itself." ></td>
	<td class="line x" title="70:195	This probability, P(epsilon1|f), is in the same form as the probability of a normal phrase pair, P( E|F), if we consider epsilon1 as some special phrase of the target language and f as a source language phrase on its own." ></td>
	<td class="line x" title="71:195	Thus P(epsilon1|f) can be estimated and recorded in the same way as the probability of normal phrase pairs." ></td>
	<td class="line x" title="72:195	During the phase of phrase enumeration, in addition to enumerating all normal phrase pairs, we also enumerate all unaligned source words f and add phrase pairs of the form < (f),(epsilon1) >." ></td>
	<td class="line x" title="73:195	These special phrase pairs, TOEMPTY phrase pairs, are fed to the module of phrase scoring along with the normal phrase pairs." ></td>
	<td class="line x" title="74:195	Both types of phrase pairs are then stored in the translation table with corresponding phrase translation probabilities." ></td>
	<td class="line x" title="75:195	It can be seen that, since the probabilities of normal phrase pairs are estimated in the same procedure as those of TO-EMPTY phrase pairs, they do not need re-weighing as in the case of SWD model 1." ></td>
	<td class="line x" title="76:195	3.3 Model 3: Context-sensitive Model Although model 2 is much more informative than model 1, it is still unsatisfactory if we consider the problem of SWD as a problem of tagging." ></td>
	<td class="line x" title="77:195	The decoder can be conceived as if it carries out a tagging task over the source language sentence: each source word is tagged either as spurious or non-spurious." ></td>
	<td class="line x" title="78:195	Under such a perspective, SWD 3 model 2 is merely a unigram tagging model, and it uses only one feature template, viz." ></td>
	<td class="line x" title="79:195	the lexical form of the source word in hand." ></td>
	<td class="line x" title="80:195	Such a model can by no means encode any contextual information, and therefore it cannot handle the ACCORDING-TO/ NP EXPRESS/,+ example in section 1." ></td>
	<td class="line x" title="81:195	An obvious solution to this limitation is a more powerful tagging model augmented with contextsensitive feature templates." ></td>
	<td class="line x" title="82:195	Inspired by research work like (Lafferty et al., 2001) and (Sha and Pereira, 2003), our SWD model 3 uses first-order Conditional Random Field (CRF) to tackle the tagging task.1 The CRF model uses the following feature templates: 1." ></td>
	<td class="line x" title="83:195	the lexical form and the POS of the foreign word f itself; 2." ></td>
	<td class="line x" title="84:195	the lexical forms and the POSs of f2, f1, f+1, and f+2, where f2 and f1 are the two words to the left of f, and f+1 and f+2 are the two words to the right of f; 3." ></td>
	<td class="line x" title="85:195	the lexical form and the POS of the head word of f; 4." ></td>
	<td class="line x" title="86:195	the lexical forms and the POSs of the dependent words of f. The lexical forms are the major source of information whereas the POSs are employed to alleviate data sparseness." ></td>
	<td class="line x" title="87:195	The neighboring words are used to capture local context information." ></td>
	<td class="line x" title="88:195	For example, in Chinese there is often a comma after verbs like said or stated, and such a comma is not translated to any word or punctuation in English." ></td>
	<td class="line x" title="89:195	These spurious commas are therefore identified by their immediate left neighbors." ></td>
	<td class="line x" title="90:195	The head and dependent words are employed to capture non-local context information found by some dependency parser." ></td>
	<td class="line x" title="91:195	For the ACCORDING-TO/  NP EXPRESS/,+ example in section 1, the Chinese word ACCORDING-TO/ is the head word of EXPRESS/,+." ></td>
	<td class="line x" title="92:195	The spurious token of EXPRESS/,+ in this pattern can be distinguished from the non-spurious tokens through the feature template of head word." ></td>
	<td class="line x" title="93:195	1Maximum Entropy was also tried in our experiments but its performance is not as good as CRF." ></td>
	<td class="line x" title="94:195	The training data for the CRF model comprises the alignment matrices of the bilingual training data for the MT system." ></td>
	<td class="line x" title="95:195	A source word (token) in the training data is tagged as non-spurious if it is aligned to some target word(s), otherwise it is tagged as spurious." ></td>
	<td class="line x" title="96:195	The sentences in the training data are also POS-tagged and parsed by some dependency parser, so that each word can be assigned values for the POS-based feature templates as well as the feature templates of head word and dependency words." ></td>
	<td class="line x" title="97:195	The trained CRF model can then be used to augment the decoder to tackle the SWD problem." ></td>
	<td class="line x" title="98:195	An input source sentence should first be POStagged and parsed for assigning feature values." ></td>
	<td class="line x" title="99:195	The probability for f being spurious, P(epsilon1|f), is then calculated by the trained CRF model as PCRF(spurious|vectorF(f))." ></td>
	<td class="line x" title="100:195	The probability for f being non-spurious is simply 1  P(epsilon1|f)." ></td>
	<td class="line x" title="101:195	For a normal phrase pair < F, E > recorded in the translation table, its phrase translation probability and the lexical weight should be re-weighed by the probabilities of non-spuriousness." ></td>
	<td class="line x" title="102:195	The weighing factor is productdisplay fiepsilon1F (1P(epsilon1|fi)), since the translation of F into E means the decoder considers every word in F as non-spurious." ></td>
	<td class="line x" title="103:195	4 Experiments 4.1 Experiment Settings A series of experiments were run to compare the performance of the three SWD models against the baseline, which is the standard phrase-based approach to SMT as elaborated in (Koehn et al., 2003)." ></td>
	<td class="line x" title="104:195	The experiments are about Chinese-toEnglish translation." ></td>
	<td class="line x" title="105:195	The bilingual training data is the one for NIST MT-2006." ></td>
	<td class="line x" title="106:195	The GIGAWORD corpus is used for training language model." ></td>
	<td class="line x" title="107:195	The development/test corpora are based on the test sets for NIST MT-2005/6." ></td>
	<td class="line x" title="108:195	The alignment matrices of the training data are produced by the GIZA++ (Och and Ney, 2000b) word alignment package with its default settings." ></td>
	<td class="line x" title="109:195	The subsequent construction of translation table was done in exactly the same way as explained 4 in (Koehn et al., 2003)." ></td>
	<td class="line x" title="110:195	For SWD model 2, the phrase enumeration step is modified as described in section 3.2." ></td>
	<td class="line x" title="111:195	We used the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar for its POS-tagging as well as finding the head/dependent words of all source words." ></td>
	<td class="line x" title="112:195	The CRF toolkit used for model 3 is CRF++2." ></td>
	<td class="line x" title="113:195	The training data for the CRF model should be the same as that for translation table construction." ></td>
	<td class="line x" title="114:195	However, since there are too many instances (every single word in the training data is an instance) with a huge feature space, no publicly available CRF toolkit can handle the entire training set of NIST MT-2006.3 Therefore, we can use at most only about one-third of the NIST training set (comprising the FBIS, B1, and T10 sections) for CRF training." ></td>
	<td class="line x" title="115:195	The decoder in the experiments is our reimplementation of HIERO (Chiang, 2007), augmented with a 5-gram language model and a reordering model based on (Zhang et al., 2007)." ></td>
	<td class="line x" title="116:195	Note that no hierarchical rule is used with the decoder; the phrase pairs used are still those used in conventional phrase-based SMT." ></td>
	<td class="line x" title="117:195	Note also that the decoder does not translate OOV at all even in the baseline case, and thus the SWD models do not improve performance simply by removing OOVs." ></td>
	<td class="line x" title="118:195	In order to test the effect of training data size on the performance of the SWD models, three variations of training data were used: FBIS Only the FBIS section of the NIST training set is used as training data (for both translation table and the CRF model in model 3)." ></td>
	<td class="line x" title="119:195	This section constitutes about 10% of the entire NIST training set." ></td>
	<td class="line x" title="120:195	The purpose of this variation is to test the performance of each model when very small amount of data are available." ></td>
	<td class="line x" title="121:195	BFT Only the B1, FBIS, and T10 sections of the NIST training set are used as training data." ></td>
	<td class="line x" title="122:195	These sections are about one-third of the entire NIST training set." ></td>
	<td class="line x" title="123:195	The purpose of this 2http://crfpp.sourceforge.net/ 3Apart from CRF++, we also tried FLEXCRF (http://flexcrfs.sourceforge.net) and MALLET (http://mallet.cs.umass.edu)." ></td>
	<td class="line x" title="124:195	Data baseline model 1 model 2 model 3 FBIS 28.01 29.71 29.48 29.64 BFT 29.82 31.55 31.61 31.75 NIST 29.77 31.39 31.33 31.71 Table 2: BLEU scores in Experiment 1: NIST05 as dev and NIST06 as test variation is to test each model when medium size of data are available.4 NIST All the sections of the NIST training set are used." ></td>
	<td class="line x" title="125:195	The purpose of this variation is to test each model when a large amount of data are available." ></td>
	<td class="line x" title="126:195	(Case-insensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric." ></td>
	<td class="line x" title="127:195	In each test in our experiments, maximum BLEU training were run 10 times, and thus there are 10 BLEU scores for the test set." ></td>
	<td class="line x" title="128:195	In the following we will report the mean scores only." ></td>
	<td class="line x" title="129:195	4.2 Experiment Results and Analysis Table 2 shows the results of the first experiment, which uses the NIST MT-2005 test set as development data and the NIST MT-2006 test set as test data." ></td>
	<td class="line x" title="130:195	The most obvious observation is that any SWD model achieves much higher BLEU score than the baseline, as there is at least 1.6 BLEU point improvement in each case, and in some case the improvement of using SWD is nearly 2 BLEU points." ></td>
	<td class="line x" title="131:195	This clearly proves the importance of SWD in phrase-based SMT." ></td>
	<td class="line x" title="132:195	The difference between the performance of the various SWD models is much smaller." ></td>
	<td class="line x" title="133:195	Yet there are still some noticeable facts." ></td>
	<td class="line x" title="134:195	The first one is that model 1 gives the best result in the case of using only FBIS as training data but it fails to do so when more training data is available." ></td>
	<td class="line x" title="135:195	This phenomenon is not strange since model 2 and model 3 are conditioned on more information and therefore they need more training data." ></td>
	<td class="line x" title="136:195	The second observation is about the strength of SWD model 3, which achieves the best BLEU score in both the BFT and NIST cases." ></td>
	<td class="line x" title="137:195	While its improvement over models 1 and 2 is marginal in the case of BFT, its performance in the NIST 4Note also that the BFT data set is the largest training data that the CRF model in model 3 can handle." ></td>
	<td class="line x" title="138:195	5 case is remarkable." ></td>
	<td class="line x" title="139:195	A suspicion to the strength of model 3 is that in the NIST case both models 1 and 2 use the entire NIST training set for estimating P(epsilon1), while model 3 uses only the BFT sections to train its CRF model." ></td>
	<td class="line x" title="140:195	It may be that the BFT sections are more consistent with the test data set than the other NIST sections, and therefore a SWD model trained on BFT sections only is better than that trained on the entire NIST." ></td>
	<td class="line x" title="141:195	This conjecture is supported by the fact that in all four settings the BLEU scores in the NIST case are lower than those in the BFT case, which suggests that other NIST sections are noisy." ></td>
	<td class="line x" title="142:195	While it is impossible to test model 3 with the entire NIST, it is possible to restrict the data for the estimation of P(epsilon1|f) in model 1 to the BFT sections only and check if such a restriction helps.5 We estimated the uniform probability P(epsilon1) from only the BFT sections and used it with the translation table constructed from the complete NIST training set." ></td>
	<td class="line x" title="143:195	The BLEU score thus obtained is 31.24, which is even lower than the score (31.39) of the original case of using the entire NIST for both translation table and P(epsilon1|f) estimation." ></td>
	<td class="line x" title="144:195	In sum, the strength of model 3 is not simply due to the choice of training data." ></td>
	<td class="line x" title="145:195	The test set used in Experiment 1 distinguishes itself from the development data and the training data by its characteristics of combining text from different genres." ></td>
	<td class="line x" title="146:195	There are three sources of the NIST MT-2006 test set, viz." ></td>
	<td class="line x" title="147:195	newswire, newsgroup, and broadcast news, while our development data and the NIST training set comprises only newswire text and text of similar style." ></td>
	<td class="line x" title="148:195	It is an interesting question whether SWD only works for some genres (say, newswire) but not for other genres." ></td>
	<td class="line x" title="149:195	In fact, it is dubious whether SWD fits the test set to the same extent as it fits the development set." ></td>
	<td class="line x" title="150:195	That is, perhaps SWD contributes to the improvement in Experiment 1 simply by improving the translation of the development set which is composed of newswire text only, and SWD may not benefit the translation of the test data at all." ></td>
	<td class="line x" title="151:195	In order to test this conjecture, we ran Experiment 2, in which the SWD models were still applied to the development data during training, but 5Unfortunately this way does not work for model 2 as the estimation of P(epsilon1|f) and the construction of translation table are tied together." ></td>
	<td class="line x" title="152:195	Data model 1 model 2 model 3 FBIS 29.85 29.91 29.95 BFT 31.73 31.84 32.08 NIST 31.70 31.82 32.05 Table 3: BLEU scores in Experiment 2, which is the same as Experiment 1 but no word is deleted for test corpus." ></td>
	<td class="line x" title="153:195	Note: the baseline scores are the same as the baselines in Experiment 1 (Table 2)." ></td>
	<td class="line x" title="154:195	all SWD models stopped working when translating the test data with the trained parameters." ></td>
	<td class="line x" title="155:195	The results are shown in Table 3." ></td>
	<td class="line x" title="156:195	These results are very discouraging if we compare each cell in Table 3 against the corresponding cell in Table 2: in all cases SWD seems harmful to the translation of the test data." ></td>
	<td class="line x" title="157:195	It is tempting to accept the conclusion that SWD works for newswire text only." ></td>
	<td class="line x" title="158:195	To scrutinize the problem, we split up the test data set into two parts, viz." ></td>
	<td class="line x" title="159:195	the newswire section and the non-newswire section, and ran experiments separately." ></td>
	<td class="line x" title="160:195	Table 4 shows the results of Experiment 3, in which the development data is still the NIST MT-2005 test set and the test data is the newswire section of NIST MT-2006 test set." ></td>
	<td class="line x" title="161:195	It is confirmed that if test data shares the same genre as the training/development data, then SWD does improve translation performance a lot." ></td>
	<td class="line x" title="162:195	It is also observed that more sophisticated SWD models perform better when provided with sufficient training data, and that model 3 exhibits remarkable improvement when it comes to the NIST case." ></td>
	<td class="line x" title="163:195	Of course, the figures in Table 5, which shows the results of Experiment 4 where the nonnewswire section of NIST MT-2006 test set is used as test data, still leave us the doubt that SWD is useful for a particular genre only." ></td>
	<td class="line x" title="164:195	After all, it is reasonable to assume that a model trained from data of a particular domain can give good performance only to data of the same domain." ></td>
	<td class="line x" title="165:195	On the other hand, the language model is another cause of the poor performance, as the GIGAWORD corpus is also of the newswire style." ></td>
	<td class="line x" title="166:195	While we cannot prove the value of SWD with respect to training data of other genres in the mean time, we could test the effect of using development data of other genres." ></td>
	<td class="line x" title="167:195	In our last experiment, the first halves of both the newswire 6 apply SWD for test set no SWD for test set Data model 1 model 2 model 3 model 1 model 2 model 3 FBIS 30.81 30.81 30.68 29.23 29.61 29.46 BFT 33.57 33.74 33.71 31.88 31.87 32.25 NIST 33.65 34.01 34.42 32.14 32.59 32.87 Table 4: BLEU scores in Experiment 3, which is the same as Experiments 1 and 2 but only the newswire section of NIST06 test set is used." ></td>
	<td class="line x" title="168:195	Note: the baseline scores are the same as the baselines in Experiment 1 (Table 2)." ></td>
	<td class="line x" title="169:195	apply SWD for test set no SWD for test set Data model 1 model 2 model 3 model 1 model 2 model 3 FBIS 29.19 28.86 29.16 30.07 29.67 30.08 BFT 30.62 30.64 30.86 31.66 31.83 32.00 NIST 30.34 30.10 30.46 31.50 31.45 31.66 Table 5: BLEU scores in Experiment 4, which is the same as Experiments 1 and 2 but only the non-newswire section of NIST06 test set is used." ></td>
	<td class="line x" title="170:195	Note: the baseline scores are the same as the baselines in Experiment 1 (Table 2)." ></td>
	<td class="line x" title="171:195	Data baseline model 1 model 2 model 3 FBIS 26.87 27.79 27.51 27.61 BFT 29.11 30.38 30.49 30.41 NIST 29.34 30.63 30.95 31.00 Table 6: BLEU scores in Experiment 5: which is the same as Experiment 1 but uses half of NIST06 as development set and another half of NIST06 as test set." ></td>
	<td class="line x" title="172:195	and non-newswire sections of NIST MT-2006 test set are combined to form the new development data, and the second halves of the two sections are combined to form the new test data." ></td>
	<td class="line x" title="173:195	The new development data is therefore consistent with the new test data." ></td>
	<td class="line x" title="174:195	If SWD, or at least a SWD model from newswire, is harmful to the non-newswire section, which constitutes about 60% of the development/test data, then it will be either that the parameter training process minimizes the impact of SWD, or that the SWD model will make the parameter training process fail to search for good parameter values." ></td>
	<td class="line x" title="175:195	The consequence of either case is that the baseline setting should produce similar or even higher BLEU score than the settings that employ some SWD model." ></td>
	<td class="line x" title="176:195	Experiment results, as shown in Table 6, illustrate that SWD is still very useful even when both development and test sets contain texts of different genres from the training text." ></td>
	<td class="line x" title="177:195	It is also observed, however, that the three SWD models give rise to roughly the same BLEU scores, indicating that the SWD training data do not fit the test/development data very well as even the more sophisticated models are not benefited from more data." ></td>
	<td class="line x" title="178:195	5 Experiments using METEOR The results in the last section are all evaluated using the BLEU metric only." ></td>
	<td class="line oc" title="179:195	It is dubious whether SWD is useful regarding recall-oriented metrics like METEOR (Banerjee and Lavie, 2005), since SWD removes information in source sentences." ></td>
	<td class="line o" title="180:195	This suspicion is to certain extent confirmed by our application of METEOR to the translation outputs of Experiment 1 (c.f. Table 7), which shows that all SWD models achieve lower METEOR scores than the baseline." ></td>
	<td class="line o" title="181:195	However, SWD is not entirely harmful to METEOR: if SWD is applied to parameter tuning only but not for the test set, (i.e. Experiment 2), even higher METEOR scores can be obtained." ></td>
	<td class="line x" title="182:195	This puzzling observation may be because the parameters of the decoder are optimized with respect to BLEU score, and SWD benefits parameter tuning by improving BLEU score." ></td>
	<td class="line o" title="183:195	In future experiments, maximum METEOR training should be used instead of maximum BLEU training so as to examine if SWD is really useful for parameter tuning." ></td>
	<td class="line o" title="184:195	7 Experiment 1 Experiment 2 SWD for both dev/test SWD for dev only Data baseline model 1 model 2 model 3 model 1 model 2 model 3 FBIS 50.07 47.90 49.83 49.34 51.58 51.08 51.17 BFT 52.47 50.55 51.89 52.10 54.72 54.43 54.30 NIST 52.12 49.86 50.97 51.59 54.14 53.82 54.01 Table 7: METEOR scores in Experiments 1 and 2 6 Conclusion and Future Work In this paper, we have explained why the handling of spurious source words is not a trivial problem and how important it is. Three solutions, with increasing sophistication, to the problem of SWD are presented." ></td>
	<td class="line x" title="185:195	Experiment results show that, in our setting of using NIST MT-2006 test set, any SWD model leads to an improvement of at least 1.6 BLEU points, and SWD model 3, which makes use of contextual information, can improve up to nearly 2 BLEU points." ></td>
	<td class="line x" title="186:195	If only the newswire section of the test set is considered, SWD model 3 is even more superior to the other two SWD models." ></td>
	<td class="line x" title="187:195	The effect of training data size on SWD has also been examined, and it is found that more sophisticated SWD models do not outperform unless they are provided with sufficient amount of data." ></td>
	<td class="line x" title="188:195	As to the effect of training data domain/genre on SWD, it is clear that SWD models trained on text of certain genre perform the best when applied to text of the same genre." ></td>
	<td class="line x" title="189:195	While it is infeasible for the time being to test if SWD works well for non-newswire style of training data, we managed to illustrate that SWD based on newswire text still to certain extent benefits the training and translation of non-newswire text." ></td>
	<td class="line x" title="190:195	In future, two extensions of our system are needed for further examination of SWD." ></td>
	<td class="line o" title="191:195	The first one is already mentioned in the last section: maximum METEOR training should be implemented in order to fully test the effect of SWD regarding METEOR." ></td>
	<td class="line x" title="192:195	The second extension is about the weighing factor in models 1 and 3." ></td>
	<td class="line x" title="193:195	The current implementation assumes that all source words in a normal phrase pair need to be weighed by 1P(epsilon1)." ></td>
	<td class="line x" title="194:195	However, in fact some source words in a source phrase are tacitly deleted (as explained in the Introduction)." ></td>
	<td class="line x" title="195:195	Thus the word alignment information within phrase pairs need to be recorded and the weighing of a normal phrase pair should be done in accordance with such alignment information." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0302
Rich Source-Side Context for Statistical Machine Translation
Gimpel, Kevin;Smith, Noah A.;"></td>
	<td class="line x" title="1:197	Proceedings of the Third Workshop on Statistical Machine Translation, pages 917, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:197	c2008 Association for Computational Linguistics Rich Source-Side Context for Statistical Machine Translation Kevin Gimpel and Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated." ></td>
	<td class="line x" title="3:197	This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007)." ></td>
	<td class="line x" title="4:197	The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrases translation." ></td>
	<td class="line x" title="5:197	Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios." ></td>
	<td class="line x" title="6:197	We report significant improvements in automatic evaluation scores for Chineseto-EnglishandEnglish-to-Germantranslation, and also describe our entry in the WMT-08 shared task based on this approach." ></td>
	<td class="line x" title="7:197	1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years." ></td>
	<td class="line x" title="8:197	Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007)." ></td>
	<td class="line x" title="9:197	Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007)." ></td>
	<td class="line x" title="10:197	End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007)." ></td>
	<td class="line x" title="11:197	We follow the approach of Koehn et al.(2003), in which we translate a source-language sentence f into the target-language sentence e that maximizes a linear combination of features and weights:1 e,a = argmax e,a score(e,a,f) (1) = argmax e,a Msummationdisplay m=1 mhm(e,a,f) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight m. The translation is typically found using beam search (Koehn et al., 2003)." ></td>
	<td class="line x" title="13:197	The weights 1,,M are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003)." ></td>
	<td class="line x" title="14:197	Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(eji | flscriptk) and p(flscriptk | eji) for each phrase pair eji,flk in the candidate sentence pair.2 In this paper, we add and evaluate fea1In the statistical MT literature, this is often referred to as a log-linearmodel, butsincethescoreisnormalizedduringneither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions." ></td>
	<td class="line x" title="15:197	Since many of the features are actually probabilities, this linear combination is closer to a mixture model." ></td>
	<td class="line x" title="16:197	2We will use xj i to denote the subsequence of x containing the ith through jth elements of x, inclusive." ></td>
	<td class="line x" title="17:197	9 tures that condition on additional context features on the source (f) side: p(eji | Phrase = flscriptk,Context = fk11 ,fFlscript+1,) The advantage of considering context is wellknownandexploitedintheexample-basedMTcommunity (Carl and Way, 2003)." ></td>
	<td class="line x" title="18:197	Recently researchers have begun to use source phrase context information in statistical MT systems (Stroppa et al., 2007)." ></td>
	<td class="line x" title="19:197	Statistical NLP researchers understand that conditioning a probability model on more information is helpful only if there are sufficient training data to accurately estimate the context probabilities.3 Sparse data are often the death of elaborate models, though this can be remedied through careful smoothing." ></td>
	<td class="line x" title="20:197	In this paper we leverage the existing linear model (Equation 2) to bring source-side context into phrase-based MT in a way that is robust to data sparseness." ></td>
	<td class="line x" title="21:197	We interpret the linear model as a mixtureofmanyprobabilityestimatesbasedondifferent context features, some of which may be very sparse." ></td>
	<td class="line x" title="22:197	The mixture coefficients are trained in the usual way (minimum error-rate training, Och, 2003), so that the additional context is exploited when it is useful and ignored when it isnt. The paper proceeds as follows." ></td>
	<td class="line x" title="23:197	We first review related work that enriches statistical translation models using context (2)." ></td>
	<td class="line x" title="24:197	We then propose a set of source-side features to be incorporated into the translation model, including the novel use of syntactic context from source-side parse trees and global position within f (3)." ></td>
	<td class="line x" title="25:197	We explain why analogous target-side features pose a computational challenge (4)." ></td>
	<td class="line x" title="26:197	Specific modifications to the standard training and evaluation paradigm are presented in 5." ></td>
	<td class="line x" title="27:197	Experimental results are reported in 6." ></td>
	<td class="line x" title="28:197	2 Related Work Stroppa et al.(2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use." ></td>
	<td class="line x" title="30:197	They consider up to two words and/or POS tags of context on either side." ></td>
	<td class="line x" title="31:197	Because of the aforementioned data sparseness problem, they use a decision3An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004)." ></td>
	<td class="line x" title="32:197	tree classifier that implicitly smooths relative frequencyestimates." ></td>
	<td class="line x" title="33:197	Themethodimprovedoverastandardphrase-basedbaselinetrainedonsmallamounts of data (< 50K sentence pairs) for ItalianEnglish and Chinese  English." ></td>
	<td class="line x" title="34:197	We explore a significantly largerspaceofcontextfeatures, asmoothingmethod that more naturally fits into the widely used, errordriven linear model, and report a more comprehensiveexperimentalevaluation(includingfeaturecomparison and scaling up to very large datasets)." ></td>
	<td class="line x" title="35:197	Recent research on the use of word-sense disambiguation in machine translation also points toward our approach." ></td>
	<td class="line x" title="36:197	For example, Vickrey et al.(2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation." ></td>
	<td class="line x" title="38:197	Gimenez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks." ></td>
	<td class="line x" title="39:197	They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear." ></td>
	<td class="line x" title="40:197	Carpuat and Wu (2007) and Chan et al.(2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese  English translation." ></td>
	<td class="line x" title="42:197	Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them." ></td>
	<td class="line x" title="43:197	Indeed, like theexample-basedcommunity, wedonotbeginwith any set of assumptions about which kinds of phrases require additional disambiguation (cf.the application of word-sense disambiguation, which is motivated by lexical ambiguity)." ></td>
	<td class="line x" title="45:197	Our feature-rich approach is omnivorous and can exploit any linguistic analysis of an input sentence." ></td>
	<td class="line x" title="46:197	3 Source-Side Context Features Adding features to the linear model (Equation 2) that consider more of the source sentence requires changing the decoder very little, if at all." ></td>
	<td class="line x" title="47:197	The reason is that the source sentence is fully observed, so the information to be predicted is the same as before the difference is that we are using more clues to carry out the prediction." ></td>
	<td class="line x" title="48:197	10 We see this as an opportunity to include many more features in phrase-based MT without increasing the cost of decoding at runtime." ></td>
	<td class="line x" title="49:197	This discussion is reminiscent of an advantage gained by moving from hidden Markov models to conditional random fields for sequence labeling tasks." ></td>
	<td class="line x" title="50:197	While the same core algorithm is used for decoding with both models, a CRF allows inclusion of features that consider the entire observed sequencei.e., more of the observable context of each label to be predicted." ></td>
	<td class="line x" title="51:197	Although this same advantage was already obtained in statistical MT through the transition from noisy channel translation models to (log-)linear models, the customary set of features used in most phrasebased systems does not take full advantage of the observed data." ></td>
	<td class="line x" title="52:197	The standard approach to estimating the phrase translationconditionalprobabilityfeatures isviarelative frequencies (here e and f are phrases): p(e | f) = count(e,f)summationtext eprime count(eprime,f) Our new features all take the form p(e | f,fcontext), where e is the target language phrase, f is the source language phrase, and fcontext is the contextofthesourcelanguagephraseinthesentence in which it was observed." ></td>
	<td class="line x" title="53:197	Like the context-bare conditional probabilities, we estimate probability features using relative frequencies: p(e | f,fcontext) = count(e,f,fcontext)summationtext eprime count(eprime,f,fcontext) Since we expect that adding conditioning variables will lead to sparser counts and therefore more zero estimates, we compute features for many different types of context." ></td>
	<td class="line x" title="54:197	To combine the many differently-conditioned features into a single model, we provide them as features to the linear model (Equation 2) and use minimum error-rate training (Och, 2003) to obtain interpolation weights m. This is similar to an interpolation of backed-off estimates, if we imagine that all of the different contextsaredifferently-backedoffestimatesofthe complete context." ></td>
	<td class="line x" title="55:197	The error-driven weight training effectively smooths one implicit context-rich estimate p(e | f,fcontext) so that all of the backed-off estimates are taken into account, including the original p(e | f)." ></td>
	<td class="line x" title="56:197	Our approach is asymmetrical; we havenot, forexample, estimatedfeaturesoftheform p(f,fcontext | e)." ></td>
	<td class="line x" title="57:197	We next discuss the specific source-side context features used in our model." ></td>
	<td class="line x" title="58:197	3.1 Lexical Context Features The most obvious kind of context of a source phrase flscriptk is the m-length sequence before it (fk1km) and the m-length sequence after it (flscript+mlscript+1 )." ></td>
	<td class="line x" title="59:197	We include context features for m  {1,2}, padding sentences with m special symbols at the beginning and at the end." ></td>
	<td class="line x" title="60:197	For each value of m, we include three features:  p(e | f,fk1km), the left lexical context;  p(e | f,flscript+mlscript+1 ), the right lexical context;  p(e | f,fk1km,flscript+mlscript+1 ), both sides." ></td>
	<td class="line x" title="61:197	3.2 Shallow Syntactic Features Lexical context features, especially when m > 1, are expected to be sparse." ></td>
	<td class="line x" title="62:197	Representing the context by part-of-speech (POS) tags is one way to overcome that sparseness." ></td>
	<td class="line x" title="63:197	We used the same set of the lexical context features described above, but with POS tags replacing words in the context." ></td>
	<td class="line x" title="64:197	We also include a feature which conditions on the POS tag sequence of the actual phrase being translated." ></td>
	<td class="line x" title="65:197	3.3 Syntactic Features If a robust parser is available for the source language, we can include context features from parse trees." ></td>
	<td class="line x" title="66:197	We used the following parse tree features:  Is the phrase (exactly) a constituent?" ></td>
	<td class="line x" title="67:197	 What is the nonterminal label of the lowest node in the parse tree that covers the phrase?" ></td>
	<td class="line x" title="68:197	 What is the nonterminal label or POS of the highest nonterminal node that ends immediately before the phrase?" ></td>
	<td class="line x" title="69:197	Begins immediately after the phrase?" ></td>
	<td class="line x" title="70:197	 Is the phrase strictly to the left of the root word, does it contain the root word, or is it strictly to the right of the root word?" ></td>
	<td class="line x" title="71:197	(Requires a parse with head annotations.)" ></td>
	<td class="line x" title="72:197	We also used a feature that conditions on both features in the third bullet point above." ></td>
	<td class="line x" title="73:197	11 S[support] Syntactic Features: Not a constituent NP covers phrase VBP to left PP to right Right of root word PP Shallow Lexical Phrase PP NP VP IN     DT       NN        ,  PRP VBP     DT   NN   IN  DT      NN         IN     NN     CC    NN    ,  NP SBAR NPNPNP NP . . ." ></td>
	<td class="line x" title="74:197	Positional Features: Not at start Not at end Second fifth of sentence Covers 18.5% of sentence   (quantized to 20%) Against this background ,  we   support   the report  of  the committee   on transport and tourism , which In dieser Hinsicht untersttzen wir   den Bericht des Ausschusses   fr Verkehr und Fremdenverkehr , in Syntactic NP VBP PP Figure 1: A (partial) sentence pair from the WMT-07 Europarl training corpus." ></td>
	<td class="line x" title="75:197	Processing of the data (parsing, word alignment) was done as discussed in 6." ></td>
	<td class="line x" title="76:197	The phrase pair of interest is boxed and context features are shown in dotted shapes." ></td>
	<td class="line x" title="77:197	The context features help determine whether the phrase should be translated as der Bericht des Ausschusses (nominative case) or den Bericht des Ausschusses (accusative case)." ></td>
	<td class="line x" title="78:197	See text for details." ></td>
	<td class="line x" title="79:197	3.4 Positional Features We include features based on the position of the phrase in the source sentence, the phrase length, and the sentence length." ></td>
	<td class="line x" title="80:197	These features use information from the entire source sentence, but are not syntactic." ></td>
	<td class="line x" title="81:197	For a phrase flscriptk in a sentence f of length n:  Is the phrase at the start of the sentence (k = 1)?" ></td>
	<td class="line x" title="82:197	 Is the phrase at the end of the sentence (lscript = n)?" ></td>
	<td class="line x" title="83:197	 A quantization of r = k+ lscriptk+1 2n , the relative position in (0,1) of the phrases midpoint within f. We choose the smallest q  {0.2,0.4,0.6,0.8,1} such that q > r.  A quantization of c = lscriptk+1n , the fraction of the words in f that are covered by the phrase." ></td>
	<td class="line x" title="84:197	We choose the smallest q  { 140, 120." ></td>
	<td class="line x" title="85:197	110, 15, 13,1} such that q > c. An illustration of the context features is shown in Fig." ></td>
	<td class="line x" title="86:197	1." ></td>
	<td class="line x" title="87:197	Consider the phrase pair the report of the committee/den Bericht des Ausschusses extracted by our English  German baseline MT system (described in 6.3)." ></td>
	<td class="line x" title="88:197	The German word Bericht is a masculine noun; therefore, it takes the article der in the nominative case, den in the accusative case, and dem in the dative case." ></td>
	<td class="line x" title="89:197	These three translations are indeed available in the phrase table for the report of the committee (see Table 1, no context column), with relatively high entropy." ></td>
	<td class="line x" title="90:197	The choice between den and der must be made by the language model alone." ></td>
	<td class="line x" title="91:197	Knowing that the phrase follows a verb, or appears to the right of the sentences root word, or within the second fifth of the sentence should help." ></td>
	<td class="line x" title="92:197	Indeed, a probability distribution that conditions on context features gives more peaked distributions that give higher probability to the correct translation, given this context, and lower probability given some other contexts (see Table 1)." ></td>
	<td class="line x" title="93:197	4 Why Not Target-Side Context?" ></td>
	<td class="line x" title="94:197	While source context is straightforward to exploit in a model, including target-side context features breaks one of the key independence assumptions made by phrase-based translation models: the translations of the source-side phrases are conditionally independent of each other, given f, thereby requiring new algorithms for decoding (Marino et al., 2006)." ></td>
	<td class="line x" title="95:197	We suggest that target-side context may already be well accounted for in current MT systems." ></td>
	<td class="line x" title="96:197	Indeed, language models pay attention to the local context of phrases, as do reordering models." ></td>
	<td class="line x" title="97:197	The recent emphasis on improving these components of a translation system (Brants et al., 2007) is likely due in part to the widespread availability of NLP tools for the language that is most frequently the target: English." ></td>
	<td class="line x" title="98:197	We will demonstrate that NLP tools (tag12 Shallow: 2 POS on left Syntax: of root Positional: rel." ></td>
	<td class="line x" title="99:197	pos." ></td>
	<td class="line x" title="100:197	g no context PRP VBP VBN IN right left 2nd fifth 1st fifth den bericht des ausschusses 0.3125 1.0000 0.3333 0.5000 0.0000 0.6000 0.0000 der bericht des ausschusses 0.3125 0.0000 0.0000 0.1000 0.6667 0.2000 0.6667 dem bericht des ausschusses 0.2500 0.0000 0.6667 0.3000 0.1667 0.0000 0.1667 Table 1: Phrase table entries for the report of the committee and their scores under different contexts." ></td>
	<td class="line x" title="101:197	These are the top three phrases in the baseline English  German system (no context column)." ></td>
	<td class="line x" title="102:197	Contexts from the source sentence in Fig." ></td>
	<td class="line x" title="103:197	1 (starred) predict correctly; we show also alternative contexts that give very different distributions." ></td>
	<td class="line x" title="104:197	gers and parsers) for the source side can be used to improve the translation model, exploiting analysis tools for other languages." ></td>
	<td class="line x" title="105:197	5 Implementation The additional data required to compute the context features is extracted along with the phrase pairs during execution of the standard phrase extraction algorithm, affecting phrase extraction and scoring time by a constant factor." ></td>
	<td class="line x" title="106:197	We avoid the need to modify the standard phrase-based decoder to handle context features by appending a unique identifier to each token in the sentences to be translated." ></td>
	<td class="line x" title="107:197	Then, we precompute a phrase table for the phrases in these sentences according to the phrase contexts." ></td>
	<td class="line x" title="108:197	To avoid extremely long lists of translations of common tokens, we filter the generated phrase tables, removing entries for which the estimate of p(e | f) < c, for some small c. In our experiments, we fixed c = 0.0002." ></td>
	<td class="line x" title="109:197	This filtering reduced time for experimentation dramatically and had no apparent effect on the translation output." ></td>
	<td class="line x" title="110:197	We did not perform any filtering for the baseline system." ></td>
	<td class="line x" title="111:197	6 Experiments In this section we present experimental results using our context-endowed phrase translation model with a variety of different context features, on Chinese  English, German  English, and English  GerChinese  English (UN) Context features BLEU NIST METEOR None 0.3715 7.918 0.6486 Lexical 0.4030 8.367 0.6716 Shallow 0.3807 7.981 0.6523 Lexical + Shallow 0.4030 8.403 0.6703 Syntactic 0.3823 7.992 0.6531 Positional 0.3775 7.958 0.6510 Table 2: Chinese  English experiments: training and testing on UN data." ></td>
	<td class="line x" title="112:197	Boldface marks scores significantly higher than None. man translation tasks." ></td>
	<td class="line x" title="113:197	Dataset details are given in Appendices A (Chinese) and B (German)." ></td>
	<td class="line x" title="114:197	Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the grow-diag-finaland heuristic for symmetrization and use a maximum phrase length of 7." ></td>
	<td class="line x" title="115:197	In addition to the two phrase translation conditionals p(e | f) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998)." ></td>
	<td class="line x" title="116:197	Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (m in Equation 2) for these features." ></td>
	<td class="line x" title="117:197	A recaser is trained on the target side of the parallelcorpususingthescriptprovidedwithMoses." ></td>
	<td class="line x" title="118:197	All output is recased and detokenized prior to evaluation." ></td>
	<td class="line oc" title="119:197	Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions." ></td>
	<td class="line x" title="120:197	The version of BLEU used was that provided by NIST." ></td>
	<td class="line o" title="121:197	Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p < 0.05).6 4http://www.statmt.org/wmt08 5METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching." ></td>
	<td class="line x" title="122:197	For German, weuseexactmatchingandPorterstemming." ></td>
	<td class="line x" title="123:197	Thesearethe same settings that were used to evaluate systems for the WMT07 shared task." ></td>
	<td class="line o" title="124:197	6Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu.edu/MT. 13 Chinese  English Testing on UN Testing on News (NIST 2005) Context features BLEU NIST METEOR BLEU NIST METEOR Training on in-domain data only: None 0.3715 7.918 0.6486 0.2700 7.986 0.5314 Training on all data: None 0.3615 7.797 0.6414 0.2593 7.697 0.5200 Lexical 0.3898 8.231 0.6697 0.2522 7.852 0.5273 Shallow:  1 POS tag 0.3611 7.713 0.6430 0.2669 8.243 0.5526 Shallow:  2 POS tags 0.3657 7.808 0.6455 0.2591 7.843 0.5288 Lexical + Shallow 0.3886 8.245 0.6675 0.2628 7.881 0.5290 Syntactic 0.3717 7.899 0.6531 0.2653 8.123 0.5403 Lexical + Syntactic 0.3926 8.224 0.6636 0.2572 7.774 0.5234 Positional 0.3647 7.766 0.6469 0.2648 7.891 0.5275 All 0.3772 8.176 0.6582 0.2566 7.775 0.5225 Feature selection (see Sec." ></td>
	<td class="line x" title="125:197	6.4) 0.3843 8.079 0.6594 0.2730 8.059 0.5343 Table 3: Chinese  English experiments: first row shows baseline performance when training only on in-domain data for each task; all other rows show results when training on all data (UN and News)." ></td>
	<td class="line x" title="126:197	Left half shows results when tuning and testing on UN test sets; right half shows results when tuning on NIST 2004 News test set and testing on NIST 2005." ></td>
	<td class="line x" title="127:197	For feature selection, an additional set of unseen data was used: 2000 held-out sentences from the UN data for the left half and the NIST 2003 test set for the right half." ></td>
	<td class="line x" title="128:197	Boldface marks scores that are significantly higher than the first row, in-domain baseline." ></td>
	<td class="line x" title="129:197	6.1 Chinese  English For our Chinese  English experiments, two kinds of data were used: UN proceedings, and newswire as used in NIST evaluations." ></td>
	<td class="line x" title="130:197	UN Data UN data results are reported in Table 2." ></td>
	<td class="line x" title="131:197	Significant improvements are obtained on all three evaluation measurese.g., more than 3 BLEU pointsusing lexical or lexical and shallow features." ></td>
	<td class="line x" title="132:197	Whileimprovementsaresmallerforotherfeatures and feature combinations, performance is not harmed by conditioning on context features." ></td>
	<td class="line x" title="133:197	Note that using syntactic features gave 1 BLEU point of improvement." ></td>
	<td class="line x" title="134:197	NewsData In News data experiments, none of our features obtained BLEU performance statistically distinguishable from the baseline of 0.2700 BLEU (neither better, nor worse)." ></td>
	<td class="line x" title="135:197	The News training corpus is less than half the size of the UN training corpus (in words); unsurprisingly, the context features were too sparse to be helpful." ></td>
	<td class="line x" title="136:197	Further, newswire are less formulaic and repetitive than UN proceedings, so contexts do not generalize as well from training to test data." ></td>
	<td class="line x" title="137:197	Fortunately, our error-minimizing mixture approach protects the BLEU score, which the m are tuned to optimize." ></td>
	<td class="line x" title="138:197	Combined UN + News Data Our next experiment used all of the available training data (> 200M words on each side) to train the models, in-domain m tuning, and testing for each domain separately; see Table 3." ></td>
	<td class="line x" title="139:197	Without context features, training on mixed-domain data consistently harms performance." ></td>
	<td class="line x" title="140:197	With contexts that include lexical features, the mixed-domain model significantly outperforms the in-domain baseline for UN data." ></td>
	<td class="line x" title="141:197	These results suggestthatcontextfeaturesenablebetteruseofoutof-domain data, an important advantage for statistical MT since parallel data often arise from very different sources than those of real-world translation scenarios." ></td>
	<td class="line o" title="142:197	On News data, context features did not give a significant advantage on the BLEU score, though syntactic and  1 POS contexts did give significant NIST and METEOR improvements over the in-domain baseline." ></td>
	<td class="line x" title="143:197	6.2 German  English We do not report full results for this task, because the context features neither helped nor hurt performance significantly." ></td>
	<td class="line x" title="144:197	We believe this is due to data sparsenessresultingfromthesizeofthetrainingcorpus (26M German words), Germans relatively rich morphology, and the challenges of German parsing." ></td>
	<td class="line o" title="145:197	14 English  German Context features BLEU NIST METEOR None 0.2069 6.020 0.2811 Lexical 0.2018 6.031 0.2772 Shallow 0.2017 5.911 0.2748 Syntactic 0.2077 6.049 0.2829 Positional 0.2045 5.930 0.2772 Lex." ></td>
	<td class="line x" title="146:197	+ Shal." ></td>
	<td class="line x" title="147:197	+ Syn." ></td>
	<td class="line x" title="148:197	0.2045 6.061 0.2817 All 0.2053 6.009 0.2797 Feature selection 0.2080 6.009 0.2807 Table 4: English  German experiments: training and testing on Europarl data." ></td>
	<td class="line x" title="149:197	WMT-07 Europarl parallel training data was used for training, dev06 was used for tuning, devtest06 was used for feature selection and developmental testing, and test07 was used for final testing." ></td>
	<td class="line x" title="150:197	Boldface marks scores significantly higher than None. 6.3 English  German English  German results are shown in Table 4." ></td>
	<td class="line x" title="151:197	The baseline system here is highly competitive, having scored higher on automatic evaluation measures than any other system in the WMT-07 shared task (Callison-Burch et al., 2007)." ></td>
	<td class="line x" title="152:197	Though most results are not statistically significant, small improvements do tend to come from syntactic context features." ></td>
	<td class="line x" title="153:197	Comparing with the GermanEnglish experiment, we attribute this effect to the high accuracy of the English parser compared to the German parser." ></td>
	<td class="line x" title="154:197	6.4 Feature Selection Translation performance does not always increase when features are added to the model." ></td>
	<td class="line x" title="155:197	This motivates the use of feature selection algorithms to choose a subset of features to optimize performance." ></td>
	<td class="line x" title="156:197	We experimented with several feature selection algorithms based on information-theoretic quantities computed among the source phrase, the target phrase, and the context, but found that a simple forward variable selection algorithm (Guyon and Elisseeff, 2003) worked best." ></td>
	<td class="line x" title="157:197	In this procedure, we start with no context features and, at each iteration, add the single feature that results in the largest increase in BLEU score on an unseen development set after m tuning." ></td>
	<td class="line x" title="158:197	The algorithm terminates if no features are left or if none result in an increase in BLEU." ></td>
	<td class="line x" title="159:197	We ran this algorithm to completion for the twoChineseEnglishtune/testsets(trainingon all data in each case) and the English  German task; see Tables 3 and 4." ></td>
	<td class="line x" title="160:197	In all cases, the algorithm finishes after  4 iterations." ></td>
	<td class="line x" title="161:197	Feature selection for Chinese  English (UN) first chose the lexical feature 1 word on each side, then the positional feature indicating which fifth of the sentence contains the phrase, and finally the lexical feature 1 word on right. For News, the features chosen were the shallow syntactic feature 1 POS on each side, then the positional beginningof-sentence feature, then the position relative to the root (a syntactic feature)." ></td>
	<td class="line x" title="162:197	For English  German, the shallow syntactic feature 2 POS on left, then the lexical feature 1 word on right were selected." ></td>
	<td class="line x" title="163:197	In the case where context features were most helpful (Chinese  English UN data), we found feature selection to be competitive at 2.28 BLEU pointsabovetheno-contextbaseline, butnotthebest achieved." ></td>
	<td class="line x" title="164:197	Intheothertwocases(ChineseEnglish News and English  German Europarl), our best results were achieved using these automatically selected features, and in the Chinese  English News case, improvements on all three scores (including 1.37 BLEU points) are significant compared to the no-context baseline trained on the same data." ></td>
	<td class="line x" title="165:197	6.5 WMT-08 Shared Task: English  German Since we began this research before the release of the data for the WMT-08 shared task, we performed the majority of our experiments using the data released for the WMT-07 shared task (see Appendix B)." ></td>
	<td class="line x" title="166:197	To prepare our entry for the 2008 shared task, we trained a baseline system on the 2008 data usinganearlyidenticalconfiguration.7 Table5compares performance of the baseline system (with no context features) to performance with the two context features chosen automatically as described in 6.4." ></td>
	<td class="line x" title="167:197	In addition to the devtest06 data, we report results on the 2007 and 2008 Europarl test sets." ></td>
	<td class="line x" title="168:197	Most improvements were statistically significant." ></td>
	<td class="line x" title="169:197	7 Future Work In future work, we plan to apply more sophisticated learning algorithms to rich-feature phrase table estimation." ></td>
	<td class="line x" title="170:197	Context features can also be used as conditioning variables in other components of translation 7The only differences were the use of a larger max sentence length threshold of 55 tokens instead of 50, and the use of the better-performing englishFactored Stanford parser model." ></td>
	<td class="line o" title="171:197	15 devtest06 test07 test08 System BLEU NIST METEOR BLEU NIST METEOR BLEU NIST METEOR Baseline 0.2009 5.866 0.2719 0.2051 5.957 0.2782 0.2003 5.889 0.2720 Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772 Table 5: English  German shared task system results using WMT-08 Europarl parallel data for training, dev06 for tuning, and three test sets, including the final 2008 test set." ></td>
	<td class="line x" title="172:197	The row labeled Context uses the top-performing feature set {2 POS on left, 1 word on right}." ></td>
	<td class="line x" title="173:197	Boldface marks scores that are significantly higher than the baseline." ></td>
	<td class="line x" title="174:197	models, including the lexicalized reordering model and the lexical translation model in the Moses MT system, or hierarchical or syntactic models (Chiang, 2005)." ></td>
	<td class="line x" title="175:197	Additional linguistic analysis (e.g., morphological disambiguation, named entity recognition, semantic role labeling) can be used to define new context features." ></td>
	<td class="line x" title="176:197	8 Conclusion We have described a straightforward, scalable method for improving phrase translation models by modeling features of a phrases source-side context." ></td>
	<td class="line x" title="177:197	Our method allows incorporation of features from anykindofsource-sideannotationandbarelyaffects the decoding algorithm." ></td>
	<td class="line x" title="178:197	Experiments show performance rivaling or exceeding strong, state-of-the-art baselines on standard translation tasks." ></td>
	<td class="line x" title="179:197	Automatic feature selection can be used to achieve performance gains with just two or three context features." ></td>
	<td class="line x" title="180:197	Performance is strongest when large in-domain training sets and high-accuracy NLP tools for the source language are available." ></td>
	<td class="line x" title="181:197	Acknowledgments This research was supported in part by an ARCS award to the first author, NSF IIS-0713265, supercomputing resources provided by Yahoo, and a Google grant." ></td>
	<td class="line x" title="182:197	We thank Abhaya Agarwal, Ashish Venugopal, and Andreas Zollmann for helpful conversations and Joy Zhang for his Chinese segmenter." ></td>
	<td class="line x" title="183:197	We also thank the anonymous reviewers for helpful comments." ></td>
	<td class="line x" title="184:197	A Dataset Details (Chinese-English) We trained on data from the NIST MT 2008 constrained Chinese-English track: Hong Kong Hansards and news (LDC2004T08), Sinorama (LDC2005T10), FBIS (LDC2003E14), Xinhua (LDC2002E18), and financial news (LDC2006E26)total 2.5M sents., 66M Chinese words, 68M English." ></td>
	<td class="line x" title="185:197	For news experiments, the newswire portion of the NIST 2004 test set was used for tuning, the full NIST 2003 test set was used for developmental testing and feature selection, and the NIST 2005 test set was used for testing (900-1000 sents." ></td>
	<td class="line x" title="186:197	each)." ></td>
	<td class="line x" title="187:197	We also used the United Nations parallel text (LDC2004E12), divided into training (4.7M sents.; words: 136M Chinese, 144M English), tuning (2K sents.), and test sets (2K sents.)." ></td>
	<td class="line x" title="188:197	We removed sentence pairs where either side was longer than 80 words, segmented all Chinese text automatically,8 and parsed/tagged using the Stanford parser with the pre-trained xinhuaPCFG model (Klein and Manning, 2003)." ></td>
	<td class="line x" title="189:197	Trigram language models were trained on the English side of the parallel corpus along with approximately 115M words from the Xinhua section of the English Gigaword corpus (LDC2005T12), years 19952000 (total 326M words)." ></td>
	<td class="line x" title="190:197	B Dataset Details (German-English) For German  English experiments, we used data provided for the WMT-07 shared task (1.1M sents., 26M German words, 27M English)." ></td>
	<td class="line x" title="191:197	We used dev06 for tuning, devtest06 for feature selection and developmental testing, and test07 for final testing (2K sents." ></td>
	<td class="line x" title="192:197	each)." ></td>
	<td class="line x" title="193:197	We removed sentence pairs where either side was longer than 50 words and parsed/tagged the German and English data using theStanfordparser(KleinandManning,2003)(with pre-trained germanFactored and englishPCFG models)." ></td>
	<td class="line x" title="194:197	5-gram language models were trained on the entire target side of the parallel corpus (37M German words, 38M English)." ></td>
	<td class="line x" title="195:197	8Available at http://projectile.is.cs.cmu." ></td>
	<td class="line x" title="196:197	edu/research/public/tools/segmentation/ lrsegmenter/lrSegmenter.perl." ></td>
	<td class="line x" title="197:197	16" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0305
Learning Performance of a Machine Translation System: a Statistical and Computational Analysis
Turchi, Marco;De Bie, Tijl;Cristianini, Nello;"></td>
	<td class="line x" title="1:200	Proceedings of the Third Workshop on Statistical Machine Translation, pages 3543, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:200	c2008 Association for Computational Linguistics Learning Performance of a Machine Translation System: a Statistical and Computational Analysis Marco Turchi Tijl De Bie Dept. of Engineering Mathematics University of Bristol, Bristol, BS8 1TR, UK {Marco.Turchi, Tijl.DeBie}@bristol.ac.uk nello@support-vector.net Nello Cristianini Abstract We present an extensive experimental study of a Statistical Machine Translation system, Moses (Koehn et al., 2007), from the point of view of its learning capabilities." ></td>
	<td class="line x" title="3:200	Very accurate learning curves are obtained, by using high-performance computing, and extrapolations are provided of the projected performance of the system under different conditions." ></td>
	<td class="line x" title="4:200	We provide a discussion of learning curves, and we suggest that: 1) the representation power of the system is not currently a limitation to its performance, 2) the inference of its models from finite sets of i.i.d. data is responsible for current performance limitations, 3) it is unlikely that increasing dataset sizes will result in significant improvements (at least in traditional i.i.d. setting), 4) it is unlikely that novel statistical estimation methods will result in significant improvements." ></td>
	<td class="line x" title="5:200	The current performance wall is mostly a consequence of Zipfs law, and this should be taken into account when designing a statistical machine translation system." ></td>
	<td class="line x" title="6:200	A few possible research directions are discussed as a result of this investigation, most notably the integration of linguistic rules into the model inference phase, and the development of active learning procedures." ></td>
	<td class="line x" title="7:200	1 Introduction and Background The performance of every learning system is the result of (at least) two combined effects: the representation power of the hypothesis class, determining how well the system can approximate the target behaviour; and statistical effects, determining how well the system can approximate the best element of the hypothesis class, based on finite and noisy training information." ></td>
	<td class="line x" title="8:200	The two effects interact, with richer classes being better approximators of the target behaviour but requiring more training data to reliably identify the best hypothesis." ></td>
	<td class="line x" title="9:200	The resulting tradeoff, equally well known in statistics and in machine learning, can be expressed in terms of bias variance, capacity-control, or model selection." ></td>
	<td class="line x" title="10:200	Various theories on learning curves have been proposed to deal with it, where a learning curve is a plot describing performance as a function of some parameters, typically training set size." ></td>
	<td class="line x" title="11:200	In the context of Statistical Machine Translation (SMT), where large bilingual corpora are used to train adaptive software to translate text, this task is further complicated by the peculiar distribution underlying the data, where the probability of encountering new words or expressions never vanishes." ></td>
	<td class="line x" title="12:200	If we want to understand the potential and limitations of the current technology, we need to understand the interplay between these two factors affecting performance." ></td>
	<td class="line x" title="13:200	In an age where the creation of intelligent behaviour is increasingly data driven, this is a question of great importance to all of Artificial Intelligence." ></td>
	<td class="line x" title="14:200	These observations lead us to an analysis of learning curves in machine translation, and to a number of related questions, including an analysis of the flexibility of the representation class used, an analysis of the stability of the models with respect to perturbations of the parameters, and an analysis of the computational resources needed to train these systems." ></td>
	<td class="line x" title="15:200	Using the open source package Moses (Koehn et 35 al., 2007) and the Spanish-English Europarl corpus (Koehn, 2005) we have performed a complete investigation of the influence of training set size on the quality of translations and on the cost of training; the influence of several design choices; the role of data sizes in training various components of the system." ></td>
	<td class="line x" title="16:200	We use this data to inform a discussion about learning curves." ></td>
	<td class="line x" title="17:200	An analysis of learning curves has previously been proposed by (Al-Onaizan et al., 1999)." ></td>
	<td class="line x" title="18:200	Recent advances in software, data availability and computing power have enabled us to undertake the present study, where very accurate curves are obtained on a large corpus." ></td>
	<td class="line x" title="19:200	Since our goal was to obtain high accuracy learning curves, that can be trusted both for comparing different system settings, and to extrapolate performance under unseen conditions, we conducted a large-scale series of tests, to reduce uncertainty in the estimations and to obtain the strongest possible signals." ></td>
	<td class="line x" title="20:200	This was only possible, to the degree of accuracy needed by our analysis, by the extensive use of a high performance computer cluster over several weeks of computation." ></td>
	<td class="line x" title="21:200	One of our key findings is that the current performance is not limited by the representation power of the hypothesis class, but rather by model estimation from data." ></td>
	<td class="line x" title="22:200	And that increasing of the size of the dataset is not likely to bridge that gap (at least not for realistic amounts in the i.i.d. setting), nor is the development of new parameter estimation principles." ></td>
	<td class="line x" title="23:200	The main limitation seems to be a direct consequence of Zipfs law, and the introduction of constraints from linguistics seems to be an unavoidable step, to help the system in the identification of the optimal models without resorting to massive increases in training data, which would also result in significantly higher training times, and model sizes." ></td>
	<td class="line x" title="24:200	2 Statistical Machine Translation What is the best function class to map Spanish documents into English documents?" ></td>
	<td class="line x" title="25:200	This is a question of linguistic nature, and has been the subject of a long debate." ></td>
	<td class="line x" title="26:200	The de-facto answer came during the 1990s from the research community on Statistical Machine Translation, who made use of statistical tools based on a noisy channel model originally developed for speech recognition (Brown et al., 1994; Och and Weber, 1998; R.Zens et al., 2002; Och and Ney, 2001; Koehn et al., 2003)." ></td>
	<td class="line x" title="27:200	A Markovian language model, based on phrases rather than words, coupled with a phrase-to-phrase translation table are at the heart of most modern systems." ></td>
	<td class="line x" title="28:200	Translating a text amounts to computing the most likely translation based on the available model parameters." ></td>
	<td class="line x" title="29:200	Inferring the parameters of these models from bilingual corpora is a matter of statistics." ></td>
	<td class="line x" title="30:200	By model inference we mean the task of extracting all tables, parameters and functions, from the corpus, that will be used to translate." ></td>
	<td class="line x" title="31:200	How far can this representation take us towards the target of achieving human-quality translations?" ></td>
	<td class="line x" title="32:200	Are the current limitations due to the approximation error of this representation, or to lack of sufficient training data?" ></td>
	<td class="line x" title="33:200	How much space for improvement is there, given new data or new statistical estimation methods or given different models with different complexities?" ></td>
	<td class="line x" title="34:200	We investigate both the approximation and the estimation components of the error in machine translation systems." ></td>
	<td class="line x" title="35:200	After analysing the two contributions, we focus on the role of various design choices in determining the statistical part of the error." ></td>
	<td class="line x" title="36:200	We investigate learning curves, measuring both the role of the training set and the optimization set size, as well as the importance of accuracy in the numeric parameters." ></td>
	<td class="line x" title="37:200	We also address the trade-off between accuracy and computational cost." ></td>
	<td class="line x" title="38:200	We perform a complete analysis of Moses as a learning system, assessing the various contributions to its performance and where improvements are more likely, and assessing computational and statistical aspects of the system." ></td>
	<td class="line x" title="39:200	A general discussion of learning curves in Moseslike systems and an extrapolation of performance are provided, showing that the estimation gap is unlikely to be closed by adding more data in realistic amounts." ></td>
	<td class="line x" title="40:200	3 Experimental Setup We have performed a large number of detailed experiments." ></td>
	<td class="line x" title="41:200	In this paper we report just a few, leaving the complete account of our benchmarking to a full journal version (Turchi et al., In preparation)." ></td>
	<td class="line x" title="42:200	Three experiments allow us to assess the most promis36 ing directions of research, from a machine learning point of view." ></td>
	<td class="line x" title="43:200	1." ></td>
	<td class="line x" title="44:200	Learning curve showing translation performance as a function of training set size, where translation is performed on unseen sentences." ></td>
	<td class="line x" title="45:200	The curves, describing the statistical part of the performance, are seen to grow very slowly with training set size." ></td>
	<td class="line x" title="46:200	2." ></td>
	<td class="line x" title="47:200	Learning curve showing translation performance as a function of training set size, where translation is performed on known sentences." ></td>
	<td class="line x" title="48:200	This was done to verify that the hypothesis class is indeed capable of representing high quality translations in the idealized case when all the necessary phrases have been observed in training phase." ></td>
	<td class="line x" title="49:200	By limiting phrase length to 7 words, and using test sentences mostly longer than 20 words, we have ensured that this was a genuine task of decoding." ></td>
	<td class="line x" title="50:200	We observed that translation in these idealized conditions is worse than human translation, but much better than machine translation of unseen sentences." ></td>
	<td class="line x" title="51:200	3." ></td>
	<td class="line x" title="52:200	Plot of performance of a model when the numeric parameters are corrupted by an increasing amount of noise." ></td>
	<td class="line x" title="53:200	This was done to simulate the effect of inaccurate parameter estimation algorithms (due either to imprecise objective functions, or to lack of sufficient statistics from the corpus)." ></td>
	<td class="line x" title="54:200	We were surprised to observe that accurate estimation of these parameters accounts for at most 10% of the final score." ></td>
	<td class="line x" title="55:200	It is the actual list of phrases that forms the bulk of the knowledge in the system." ></td>
	<td class="line x" title="56:200	We conclude that the availability of the right models in the system would allow the system to have a much higher performance, but these models will not come from increased datasets or estimation procedures." ></td>
	<td class="line x" title="57:200	Instead, they will come from the results of either the introduction of linguistic knowledge, or the introduction of query algorithms, themselves resulting necessarily from confidence estimation methods." ></td>
	<td class="line x" title="58:200	Hence these appear to be the two most pressing questions in this research area." ></td>
	<td class="line x" title="59:200	3.1 Software Moses (Koehn et al., 2007) is a complete translation toolkit for academic purposes." ></td>
	<td class="line x" title="60:200	It provides all the components needed to create a machine translation system from one language to another." ></td>
	<td class="line x" title="61:200	It contains different modules to preprocess data, train the language models and the translation models." ></td>
	<td class="line x" title="62:200	These models can be tuned using minimum error rate training (Och, 2003)." ></td>
	<td class="line x" title="63:200	Moses uses standard external tools for some of these tasks, such as GIZA++ (Och and Ney, 2003) for word alignments and SRILM (Stolcke, 2002) for language modeling." ></td>
	<td class="line x" title="64:200	Notice that Moses is a very sophisticated system, capable of learning translation tables, language models and decoding parameters from data." ></td>
	<td class="line x" title="65:200	We analyse the contribution of each component to the overall score." ></td>
	<td class="line x" title="66:200	Given a parallel training corpus, Moses preprocesses it removing long sentences, lowercasing and tokenizing sentences." ></td>
	<td class="line x" title="67:200	These sentences are used to train the language and translation models." ></td>
	<td class="line x" title="68:200	This phase requires several steps as aligning words, computing the lexical translation, extracting phrases, scoring the phrases and creating the reordering model." ></td>
	<td class="line x" title="69:200	When the models have been created, the development set is used to run the minimum error rate training algorithm to optimize their weights." ></td>
	<td class="line x" title="70:200	We refer to that step as the optimization step in the rest of the paper." ></td>
	<td class="line x" title="71:200	Test set is used to evaluate the quality of models on the data." ></td>
	<td class="line x" title="72:200	The translated sentences are embedded in a sgm format, such that the quality of the translation can be evaluated using the most common machine translation scores." ></td>
	<td class="line oc" title="73:200	Moses provides BLEU (K.Papineni et al., 2001) and NIST (Doddington, 2002), but Meteor (Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) and TER (Snover et al., 2006) can easily be used instead." ></td>
	<td class="line x" title="74:200	NIST is used in this paper as evaluation score after we observed its high correlation to the other scores on the corpus (Turchi et al., In preparation)." ></td>
	<td class="line x" title="75:200	All experiments have been run using the default parameter configuration of Moses." ></td>
	<td class="line x" title="76:200	It means that Giza++ has used IBM model 1, 2, 3, and 4 with number of iterations for model 1 equal to 5, model 2 equal to 0, model 3 and 4 equal to 3; SRILM has used n-gram order equal to 3 and the KneserNey smoothing algorithm; Mert has been run fixing to 100 the number of nbest target sentence for 37 each develop sentence, and it stops when none of the weights changed more than 1e-05 or the nbest list does not change." ></td>
	<td class="line x" title="77:200	The training, development and test set sentences are tokenized and lowercased." ></td>
	<td class="line x" title="78:200	The maximum number of tokens for each sentence in the training pair has been set to 50, whilst no limit is applied to the development or test set." ></td>
	<td class="line x" title="79:200	TMs were limited to a phrase-length of 7 words and LMs were limited to 3." ></td>
	<td class="line x" title="80:200	3.2 Data The Europarl Release v3 Spanish-English corpus has been used for the experiments." ></td>
	<td class="line x" title="81:200	All the pairs of sentences are extracted from the proceedings of the European Parliament." ></td>
	<td class="line x" title="82:200	This dataset is made of three sets of pairs of sentences." ></td>
	<td class="line x" title="83:200	Each of them has a different role: training, development and test set." ></td>
	<td class="line x" title="84:200	The training set contains 1,259,914 pairs, while there are 2,000 pairs for development and test sets." ></td>
	<td class="line x" title="85:200	This work contains several experiments on different types and sizes of data set." ></td>
	<td class="line x" title="86:200	To be consistent and to avoid anomalies due to overfitting or particular data combinations, each set of pairs of sentences have been randomly sampled." ></td>
	<td class="line x" title="87:200	The number of pairs is fixed and a software selects them randomly from the whole original training, development or test set using a uniform distribution (bootstrap)." ></td>
	<td class="line x" title="88:200	Redundancy of pairs is allowed inside each subset." ></td>
	<td class="line x" title="89:200	3.3 Hardware All the experiments have been run on a cluster machine, http://www.acrc.bris.ac.uk/acrc/hpc.htm." ></td>
	<td class="line x" title="90:200	It includes 96 nodes each with two dual-core opteron processors, 8 GB of RAM memory per node (2 GB per core); 4 thick nodes each with four dual-core opteron processors, 32 GB of RAM memory per node (4 GB per core); ClearSpeed accelerator boards on the thick nodes; SilverStorm Infiniband highspeed connectivity throughout for parallel code message passing; General Parallel File System (GPFS) providing data access from all the nodes; storage 11 terabytes." ></td>
	<td class="line x" title="91:200	Each experiment has been run using one core and allocating 4Gb of RAM." ></td>
	<td class="line x" title="92:200	4 Experiments 4.1 Experiment 1: role of training set size on performance on new sentences In this section we analyse how performance is affected by training set size, by creating learning curves (NIST score vs training set size)." ></td>
	<td class="line x" title="93:200	We have created subsets of the complete corpus by sub-sampling sentences from a uniform distribution, with replacement." ></td>
	<td class="line x" title="94:200	We have created 10 random subsets for each of the 20 chosen sizes, where each size represents 5%, 10%, etc of the complete corpus." ></td>
	<td class="line x" title="95:200	For each subset a new instance of the SMT system has been created, for a total of 200 models." ></td>
	<td class="line x" title="96:200	These have been optimized using a fixed size development set (of 2,000 sentences, not included in any other phase of the experiment)." ></td>
	<td class="line x" title="97:200	Two hundred experiments have then been run on an independent test set (of 2,000 sentences, also not included in any other phase of the experiment)." ></td>
	<td class="line x" title="98:200	This allowed us to calculate the mean and variance of NIST scores." ></td>
	<td class="line x" title="99:200	This has been done for the models with and without the optimization step, hence producing the learning curves with error bars plotted in Figure 1, representing translation performance versus training set size, in the two cases." ></td>
	<td class="line x" title="100:200	The growth of the learning curve follows a typical pattern, growing fast at first, then slowing down (traditional learning curves are power laws, in theoretical models)." ></td>
	<td class="line x" title="101:200	In this case it appears to be growing even slower than a power law, which would be a surprise under traditional statistical learning theory models." ></td>
	<td class="line x" title="102:200	In any case, the addition of massive amounts of data from the same distribution will result into smaller improvements in the performance." ></td>
	<td class="line x" title="103:200	The small error bars that we have obtained also allow us to neatly observe the benefits of the optimization phase, which are small but clearly significant." ></td>
	<td class="line x" title="104:200	4.2 Experiment 2: role of training set size on performance on known sentences The performance of a learning system depends both on the statistical estimation issues discussed in the previous subsection, and on functional approximation issues: how well can the function class reproduce the desired behaviour?" ></td>
	<td class="line x" title="105:200	In order to measure this quantity, we have performed an experiment much like the one described above, with one key differ38 0 2 4 6 8 10 12 14 x 105 6.8 6.9 7 7.1 7.2 7.3 7.4 7.5 7.6 Nist Score vs Training Size Training Size Nist Score   Optimized Not Optimized Figure 1: Not Optimized has been obtained using a fixed test set and no optimization phase." ></td>
	<td class="line x" title="106:200	Optimized using a fixed test set and the optimization phase." ></td>
	<td class="line x" title="107:200	ence: the test set was selected randomly from the training set (after cleaning phase)." ></td>
	<td class="line x" title="108:200	In this way we are guaranteed that the system has seen all the necessary information in training phase, and we can assess its limitations in these very ideal conditions." ></td>
	<td class="line x" title="109:200	We are aware this condition is extremely idealized and it will never happen in real life, but we wanted to have an upper bound on the performance achievable by this architecture if access to ideal data was not an issue." ></td>
	<td class="line x" title="110:200	We also made sure that the performance on translating training sentences was not due to simple memorization of the entire sentence, verifying that the vast majority of the sentences were not present in the translation table (where the maximal phrase size was 7), not even in reduced form." ></td>
	<td class="line x" title="111:200	Under these favourable conditions, the system obtained a NIST score of around 11, against a score of about 7.5 on unseen sentences." ></td>
	<td class="line x" title="112:200	This suggests that the phrase-based Markov-chain representation is sufficiently rich to obtain a high score, if the necessary information is contained in the translation and language models." ></td>
	<td class="line x" title="113:200	For each model to be tested on known sentences, we have sampled ten subsets of 2,000 sentences each from the training set." ></td>
	<td class="line x" title="114:200	The Optimized, Test on Training Set learning curve, see figure 2, represents a possible upper bound on the best performance of this SMT system, since it has been computed in favourable conditions." ></td>
	<td class="line x" title="115:200	It does suggest that this hypothesis class has the power of approximating the target behaviour more accurately than we could think based on performance on unseen sentences." ></td>
	<td class="line x" title="116:200	If the right information has been seen, the system can reconstruct the sentences rather accurately." ></td>
	<td class="line x" title="117:200	The NIST score computed using the reference sentences as target sentences is around 15, we identify the relative curve as Human Translation." ></td>
	<td class="line x" title="118:200	At this point, it seems likely that the process with which we learn the necessary tables representing the knowledge of the system is responsible for the performance limitations." ></td>
	<td class="line x" title="119:200	The gap between the Optimized, Test on Training Set and the Optimized curves is even more interesting if related to the slow growth rate in the previous learning curve: although the system can represent internally a good model of translation, it seems unlikely that this will ever be inferred by increasing the size of training datasets in realistic amounts." ></td>
	<td class="line x" title="120:200	The training step results in various forms of knowledge: translation table, language model and parameters from the optimization." ></td>
	<td class="line x" title="121:200	The internal models learnt by the system are essentially lists of phrases, with probabilities associated to them." ></td>
	<td class="line x" title="122:200	Which of these components is mostly responsible for performance limitations?" ></td>
	<td class="line x" title="123:200	4.3 Experiment 3: effect on performance of increasing noise levels in parameters Much research has focused on devising improved principles for the statistical estimation of the parameters in language and translation models." ></td>
	<td class="line x" title="124:200	The introduction of discriminative graphical models has marked a departure from traditional maximum likelihood estimation principles, and various approaches have been proposed." ></td>
	<td class="line x" title="125:200	The question is: how much information is contained in the fine grain structure of the probabilities estimated by the model?" ></td>
	<td class="line x" title="126:200	Is the performance improving with more data because certain parameters are estimated better, or just because the lists are growing?" ></td>
	<td class="line x" title="127:200	In the second case, it is likely that more sophisticated statistical algorithms to improve the estimation of probabilities will have limited impact." ></td>
	<td class="line x" title="128:200	In order to simulate the effect of inaccurate estimation of the numeric parameters, we have added increasing amount of noise to them." ></td>
	<td class="line x" title="129:200	This can either represent the effect of insufficient statistics in estimating them, or the use of imperfect parameter esti39 0 2 4 6 8 10 12 14 x 105 6 7 8 9 10 11 12 13 14 15 16 Nist Score vs Training Size Training Size Nist Score   Not Optimized Optimized Optimized, Test On Training Set Human Translation Figure 2: Four learning curves have been compared." ></td>
	<td class="line x" title="130:200	Not Optimized has been obtained using a fixed test set and no optimization phase." ></td>
	<td class="line x" title="131:200	Optimized using a fixed test set and the optimization phase." ></td>
	<td class="line x" title="132:200	Optimized Test On Training Set a test set selected by the training set for each training set size and the optimization phase." ></td>
	<td class="line x" title="133:200	Human Translation has been obtained by computing NIST using the reference English sentence of the test set as target sentences." ></td>
	<td class="line x" title="134:200	mation biases." ></td>
	<td class="line x" title="135:200	We have corrupted the parameters in the language and translation models, by adding increasing levels of noise to them, and measured the effect of this on performance." ></td>
	<td class="line x" title="136:200	One model trained with 62,995 pairs of sentences has been chosen from the experiments in Section 4.1." ></td>
	<td class="line x" title="137:200	A percentage of noise has been added to each probability in the language model, including conditional probability and back off, translation model, bidirectional translation probabilities and lexicalized weighting." ></td>
	<td class="line x" title="138:200	Given a probability p and a percentage of noise, pn, a value has been randomly selected from the interval [-x,+x], where x = p * pn, and added to p. If this quantity is bigger than one it has been approximated to one." ></td>
	<td class="line x" title="139:200	Different values of percentage have been used." ></td>
	<td class="line x" title="140:200	For each value of pn, five experiment have been run." ></td>
	<td class="line x" title="141:200	The optimization step has not been run." ></td>
	<td class="line x" title="142:200	We see from Figure 3 that the performance does not seem to depend crucially on the fine structure of the parameter vectors, and that even a large addition of noise (100%) produces a 10% decline in NIST score." ></td>
	<td class="line x" title="143:200	This suggests that it is the list itself, rather 0 10 20 30 40 50 60 70 80 90 100 1106.6 6.65 6.7 6.75 6.8 6.85 'Perturbed' Nist Score vs Percentage of Perturbation Percentage of Perturbation 'Perturbed' Nist Score Figure 3: Each probability of the language and translation models has been perturbed adding a percentage of noise." ></td>
	<td class="line x" title="144:200	This learning curve reports the not optimized NIST score versus the percentage of perturbation applied." ></td>
	<td class="line x" title="145:200	These results have been obtained using a fixed training set size equal to 62,995 pairs of sentences." ></td>
	<td class="line x" title="146:200	40 0 2 4 6 8 10 12 14 x 105 0 500 1000 1500 2000 2500 CPU Computational Time in minutes vs Training Size Training Size CPU Computational Time in minutes   Training Time Tuning Time Figure 4: Training and tuning user time vs training set size." ></td>
	<td class="line x" title="147:200	Time quantities are expressed in minutes." ></td>
	<td class="line x" title="148:200	than the probabilities in it, that controls the performance." ></td>
	<td class="line x" title="149:200	Different estimation methods can produce different parameters, but this does not seem to matter very much." ></td>
	<td class="line x" title="150:200	The creation of a more complete list of words, however, seems to be the key to improve the score." ></td>
	<td class="line x" title="151:200	Combined with the previous findings, this would mean that neither more data nor better statistics will bridge the performance gap." ></td>
	<td class="line x" title="152:200	The solution might have to be found elsewhere, and in our Discussion section we outline a few possible avenues." ></td>
	<td class="line x" title="153:200	5 Computational Cost The computational cost of models creation and development-phase has been measured during the creation of the learning curves." ></td>
	<td class="line x" title="154:200	Despite its efficiency in terms of data usage, the development phase has a high cost in computational terms, if compared with the cost of creating the complete language and translation models." ></td>
	<td class="line x" title="155:200	For each experiment, the user CPU time is computed as the sum of the user time of the main process and the user time of the children." ></td>
	<td class="line x" title="156:200	These quantities are collected for training, development, testing and evaluation phases." ></td>
	<td class="line x" title="157:200	In figure 4, training and tuning user times are plotted as a function of the training set size." ></td>
	<td class="line x" title="158:200	It is evident that increasing the training size causes an increase in training time in a roughly linear fashion." ></td>
	<td class="line x" title="159:200	It is hard to find a similar relationship for the tuning time of the development phase." ></td>
	<td class="line x" title="160:200	In fact, the tuning time is strictly connected with the optimization algorithm and the sentences in the development set." ></td>
	<td class="line x" title="161:200	We can also see in figure 4 that even a small development set size can require a large amount of tuning time." ></td>
	<td class="line x" title="162:200	Each point of the tuning time curve has a big variance." ></td>
	<td class="line x" title="163:200	The tuning phase involves translating the development set many times and hence its cost depends very weakly on the training set size, since a large training set leads to larger tables and these lead to slightly longer test times." ></td>
	<td class="line x" title="164:200	6 Discussion The impressive capability of current machine translation systems is not only a testament to an incredibly productive and creative research community, but can also be seen as a paradigm for other Artificial Intelligence tasks." ></td>
	<td class="line x" title="165:200	Data driven approaches to all main areas of AI currently deliver the state of the art performance, from summarization to speech recognition to machine vision to information retrieval." ></td>
	<td class="line x" title="166:200	And statistical learning technology is central to all approaches to data driven AI." ></td>
	<td class="line x" title="167:200	Understanding how sophisticated behaviour can be learnt from data is hence not just a concern for machine learning, or to individual applied communities, such as Statistical Machine Translation, but rather a general concern for modern Artificial Intelligence." ></td>
	<td class="line x" title="168:200	The analysis of learning curves, and the identification of the various limitations to performance is a crucial part of the machine learning method, and one where statistics and algorithmics interact closely." ></td>
	<td class="line x" title="169:200	In the case of Statistical Machine Translation, the analysis of Moses suggests that the current bottleneck is the lack of sufficient data, not the function class used for the representation of translation systems." ></td>
	<td class="line x" title="170:200	The clear gap between performance on training and testing set, together with the rate of the learning curves, suggests that improvements may be possible but not by adding more data in i.i.d. way as done now." ></td>
	<td class="line x" title="171:200	The perturbation analysis suggests that improved statistical principles are unlikely to make a big difference either." ></td>
	<td class="line x" title="172:200	Since it is unlikely that sufficient data will be available by simply sampling a distribution, one needs to address a few possible ways to transfer large amounts of knowledge into the system." ></td>
	<td class="line x" title="173:200	All of them lead to open problems either in machine learn41 ing or in machine translation, most of them having been already identified by their respective communities as important questions." ></td>
	<td class="line x" title="174:200	They are actively being worked on." ></td>
	<td class="line x" title="175:200	The gap between performances on training and on test sets is typically affected by model selection choices, ultimately controlling the trade off between overfitting and underfitting." ></td>
	<td class="line x" title="176:200	In these experiments the system used phrases of length 7 or less." ></td>
	<td class="line x" title="177:200	Changing this parameter might reflect on the gap and this is the focus of our current work." ></td>
	<td class="line x" title="178:200	A research programme naturally follows from our analysis." ></td>
	<td class="line x" title="179:200	The first obvious approach is an effort to identify or produce datasets on demand (active learning, where the learning system can request translations of specific sentences, to satisfy its information needs)." ></td>
	<td class="line x" title="180:200	This is a classical machine learning question, that however comes with the need for further theoretical work, since it breaks the traditional i.i.d. assumptions on the origin of data." ></td>
	<td class="line x" title="181:200	Furthermore, it would also require an effective way to do confidence estimation on translations, as traditional active learning approaches are effectively based on the identification (or generation) of instances where there is low confidence in the output (Blatz et al., 2004; Ueffing and Ney, 2004; Ueffing and Ney, 2005b; Ueffing and Ney, 2005a)." ></td>
	<td class="line x" title="182:200	The second natural direction involves the introduction of significant domain knowledge in the form of linguistic rules, so to dramatically reduce the amount of data needed to essentially reconstruct them by using statistics." ></td>
	<td class="line x" title="183:200	These rules could take the form of generation of artificial training data, based on existing training data, or a posteriori expansion of translation and language tables." ></td>
	<td class="line x" title="184:200	Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007)." ></td>
	<td class="line x" title="185:200	Obviously, it is always possible that the identification of radically different representations of language might introduce totally different constraints on both approximation and estimation error, and this might be worth considering." ></td>
	<td class="line x" title="186:200	What is not likely to work." ></td>
	<td class="line x" title="187:200	It does not seem that the introduction of more data will change the situation significantly, as long as the data is sampled i.i.d. from the same distribution." ></td>
	<td class="line x" title="188:200	It also does not seem that more flexible versions of Markov models would be likely to change the situation." ></td>
	<td class="line x" title="189:200	Finally, it does not seem that new and different methods to estimate probabilities would make much of a difference." ></td>
	<td class="line x" title="190:200	Our perturbation studies show that significant amounts of noise in the parameters result into very small variations in the performance." ></td>
	<td class="line x" title="191:200	Note also that the current algorithm is not even working on refining the probability estimates, as the rate of growth of the tables suggests that new n-grams are constantly appearing, reducing the proportion of time spent refining probabilities of old n-grams." ></td>
	<td class="line x" title="192:200	It does seem that the control of the performance relies on the length of the translation and language tables." ></td>
	<td class="line x" title="193:200	Ways are needed to make these tables grow much faster as a function of training set size; they can either involve active selection of documents to translate, or the incorporation of linguistic rules to expand the tables without using extra data." ></td>
	<td class="line x" title="194:200	It is important to note that many approaches suggested above are avenues currently being actively pursued, and this analysis might be useful to decide which one of them should be given priority." ></td>
	<td class="line x" title="195:200	7 Conclusions We have started a series of extensive experimental evaluations of performance of Moses, using high performance computing, with the goal of understanding the system from a machine learning point of view, and use this information to identify weaknesses of the system that can lead to improvements." ></td>
	<td class="line x" title="196:200	We have performed many more experiments that cannot be reported in this workshop paper, and will be published in a longer report (Turchi et al., In preparation)." ></td>
	<td class="line x" title="197:200	In general, our goal is to extrapolate the performance of the system under many conditions, to be able to decide which directions of research are most likely to deliver improvements in performance." ></td>
	<td class="line x" title="198:200	Acknowledgments Marco Turchi is supported by the EU Project SMART." ></td>
	<td class="line x" title="199:200	The authors thank Callum Wright, Bristol HPC Systems Administrator, and Moses mailing list." ></td>
	<td class="line x" title="200:200	42" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0307
Using Shallow Syntax Information to Improve Word Alignment and Reordering for SMT
Crego, Josep M.;Habash, Nizar;"></td>
	<td class="line x" title="1:224	Proceedings of the Third Workshop on Statistical Machine Translation, pages 5361, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:224	c2008 Association for Computational Linguistics Using Shallow Syntax Information to Improve Word Alignment and Reordering for SMT Josep M. Crego TALP Research Center Universitat Polit`ecnica de Catalunya 08034 Barcelona, Spain jmcrego@gps.tsc.upc.edu Nizar Habash Center for Computational Learning Systems Columbia University New York, NY 10115, USA habash@ccls.columbia.edu Abstract We describe two methods to improve SMT accuracy using shallow syntax information." ></td>
	<td class="line x" title="3:224	First, we use chunks to refine the set of word alignments typically used as a starting point in SMT systems." ></td>
	<td class="line x" title="4:224	Second, we extend an N-grambased SMT system with chunk tags to better account for long-distance reorderings." ></td>
	<td class="line x" title="5:224	Experiments are reported on an Arabic-English task showing significant improvements." ></td>
	<td class="line x" title="6:224	A human error analysis indicates that long-distance reorderings are captured effectively." ></td>
	<td class="line x" title="7:224	1 Introduction Much research has been done on using syntactic information in statistical machine translation (SMT)." ></td>
	<td class="line x" title="8:224	In this paper we use chunks (shallow syntax information) to improve an N-gram-based SMT system." ></td>
	<td class="line x" title="9:224	We tackle both the alignment and reordering problems of a language pair with important differences in word order (Arabic-English)." ></td>
	<td class="line x" title="10:224	These differences lead to noisy word alignments, which lower the accuracy of the derived translation table." ></td>
	<td class="line x" title="11:224	Additionally, word order differences, especially those spanning long distances and/or including multiple levels of reordering, are a challenge for SMT decoding." ></td>
	<td class="line x" title="12:224	Two improvements are presented here." ></td>
	<td class="line x" title="13:224	First, we reduce the number of noisy alignments by using the idea that chunks, like raw words, have a translation correspondence in the source and target sentences." ></td>
	<td class="line x" title="14:224	Hence, word links are constrained (i.e., noisy links are pruned) using chunk information." ></td>
	<td class="line x" title="15:224	Second, we introduce rewrite rules which can handle both short/medium and long distance reorderings as well as different degrees of recursive application." ></td>
	<td class="line x" title="16:224	We build our rules with two different linguistic annotations, (local) POS tags and (long-spanning) chunk tags." ></td>
	<td class="line x" title="17:224	Despite employing an N-gram-based SMT system, the methods described here can also be applied to any phrase-based SMT system." ></td>
	<td class="line x" title="18:224	Alignment and reordering are similarly used in both approaches." ></td>
	<td class="line x" title="19:224	In Section 2 we discuss previous related work." ></td>
	<td class="line x" title="20:224	In Section 3, we discuss Arabic linguistic issues and motivate some of our decisions." ></td>
	<td class="line x" title="21:224	In Section 4, we describe the N-gram based SMT system which we extend in this paper." ></td>
	<td class="line x" title="22:224	Sections 5 and 6 detail the main contributions of this work." ></td>
	<td class="line x" title="23:224	In Section 7, we carry out evaluation experiments reporting on the accuracy results and give details of a human evaluation error analysis." ></td>
	<td class="line x" title="24:224	2 Related Work In the SMT community, it is widely accepted that there is a need for structural information to account for differences in word order between different language pairs." ></td>
	<td class="line x" title="25:224	Structural information offers a greater potential to learn generalizations about relationships between languages than flat-structure models." ></td>
	<td class="line x" title="26:224	The need for these mappings is specially relevant when handling language pairs with very different word order, such as Arabic-English or Chinese-English." ></td>
	<td class="line x" title="27:224	Many alternatives have been proposed on using syntactic information in SMT systems." ></td>
	<td class="line x" title="28:224	They range from those aiming at harmonizing (monotonizing) the word order of the considered language pairs by means of a set of linguistically-motivated reordering patterns (Xia and McCord, 2004; Collins et al., 2005) to others considering translation a synchronous parsing process where reorderings introduced in the overall search are syntactically motivated (Galley et al., 2004; Quirk et al., 2005)." ></td>
	<td class="line x" title="29:224	The work presented here follows the word order harmonization strategy." ></td>
	<td class="line x" title="30:224	53 Collins et al.(2005) describe a technique for preprocessing German to look more like English syntactically." ></td>
	<td class="line x" title="32:224	They used six transformations that are applied on German parsed text to reorder it before passing it on to a phrase-based system." ></td>
	<td class="line x" title="33:224	They show a moderate statistically significant improvement." ></td>
	<td class="line x" title="34:224	Our work differs from theirs crucially in that our preprocessing rules are learned automatically." ></td>
	<td class="line x" title="35:224	Xia and McCord (2004) describe an approach for translation from French to English, where reordering rules are acquired automatically using source and target parses and word alignment." ></td>
	<td class="line x" title="36:224	The reordering rules they use are in a context-free constituency representation with marked heads." ></td>
	<td class="line x" title="37:224	The rules are mostly lexicalized." ></td>
	<td class="line x" title="38:224	Xia and McCord (2004) use source and target parses to constrain word alignments used for rule extraction." ></td>
	<td class="line x" title="39:224	Their results show that there is a positive effect on reordering when the decoder is run monotonically (i.e., without additional distortion-based reordering)." ></td>
	<td class="line x" title="40:224	The value of reordering is diminished if the decoder is run in a non-monotonic way." ></td>
	<td class="line x" title="41:224	Recently, Crego and Marino (2007b) employ POS tags to automatically learn reorderings in training." ></td>
	<td class="line x" title="42:224	They allow all possible learned reorderings to be used to create a lattice that is input to the decoder, which further improves translation accuracy." ></td>
	<td class="line x" title="43:224	Similarly, Costa-juss`a and Fonollosa (2006) use statistical word classes to generalize reorderings, which are learned/introduced in a translation process that transforms the source language into the target language word order." ></td>
	<td class="line x" title="44:224	Zhang et al.(2007) describe a similar approach using unlexicalized context-free chunk tags (XPs) to learn reordering rules for Chinese-English SMT." ></td>
	<td class="line x" title="46:224	Crego and Marino (2007c) extend their previous work using syntax trees (dependency parsing) to learn reorderings on a Chinese-English task." ></td>
	<td class="line x" title="47:224	Habash (2007) applies automatically-learned syntactic reordering rules (for Arabic-English SMT) to preprocess the input before passing it to a phrase-based SMT decoder." ></td>
	<td class="line x" title="48:224	As in (Zhang et al., 2007), (Costa-juss`a and Fonollosa, 2006) and (Crego and Marino, 2007b), we employ a word graph for a tight coupling between reordering and decoding." ></td>
	<td class="line x" title="49:224	However, we differ on the language pair (Arabic-English) and the rules employed to learn reorderings." ></td>
	<td class="line x" title="50:224	Rules are built using both POS tags and chunk tags in order to balance the higher generalization power of chunks with the higher accuracy of POS tags." ></td>
	<td class="line x" title="51:224	Additionally, we introduce a method to use chunks for refining word alignments employed in the system." ></td>
	<td class="line x" title="52:224	3 Arabic Linguistic Issues Arabic is a morpho-syntactically complex language with many differences from English." ></td>
	<td class="line x" title="53:224	We describe here three prominent syntactic features of Arabic that are relevant to Arabic-English translation and that motivate some of our decisions in this work." ></td>
	<td class="line x" title="54:224	First, Arabic words are morphologically complex containing clitics whose translations are represented separately in English and sometimes in a different order." ></td>
	<td class="line x" title="55:224	For instance, possessive pronominal enclitics are attached to the noun they modify in Arabic but their translation precedes the English translation of the noun: kitAbu+hu1 book+his  his book." ></td>
	<td class="line x" title="56:224	Other clitics include the definite article Al+ the, the conjunction w+ and and the preposition l+ of/for, among others." ></td>
	<td class="line x" title="57:224	We use the Penn Arabic Treebank tokenization scheme which splits three classes of clitics only." ></td>
	<td class="line x" title="58:224	This scheme is compatible with the chunker we use (Diab et al., 2004)." ></td>
	<td class="line x" title="59:224	Secondly, Arabic verb subjects may be: prodropped (verb conjugated), pre-verbal (SVO), or post-verbal (VSO)." ></td>
	<td class="line x" title="60:224	The VSO order is quite challenging in the context of translation to English." ></td>
	<td class="line x" title="61:224	For small noun phrases (NP), small phrase pairs in a phrase table and some degree of distortion can easily move the verb to follow the NP." ></td>
	<td class="line x" title="62:224	But this becomes much less likely with very long NPs that exceed the size of phrases in a phrase table." ></td>
	<td class="line x" title="63:224	Finally, Arabic adjectival modifiers typically follow their nouns (with a small exception of some superlative adjectives)." ></td>
	<td class="line x" title="64:224	For example, rajul Tawiyl (lit." ></td>
	<td class="line x" title="65:224	man tall) translates as a tall man." ></td>
	<td class="line x" title="66:224	These three syntactic features of Arabic-English translation are not independent of each other." ></td>
	<td class="line x" title="67:224	As we reorder the verb and the subject NP, we also have to reorder the NPs adjectival components." ></td>
	<td class="line x" title="68:224	This brings new challenges to previous implementations of Ngram based SMT which had worked with language pairs that are more similar than Arabic and English, e.g., Spanish and English." ></td>
	<td class="line x" title="69:224	Although Spanish is like Arabic in terms of its noun-adjective order; Spanish is similar to English in terms of its subject-verb order." ></td>
	<td class="line x" title="70:224	Spanish morphology is more complex than English but not as complex as Arabic: Spanish is like Arabic in terms of being pro-drop but has a smaller 1All Arabic transliterations in this paper are provided in the Buckwalter transliteration scheme (Buckwalter, 2004)." ></td>
	<td class="line x" title="71:224	54 number of clitics." ></td>
	<td class="line x" title="72:224	We do not focus on morphology issues in this work." ></td>
	<td class="line x" title="73:224	Table 1 illustrates these dimensions of variations." ></td>
	<td class="line x" title="74:224	The more variations, the harder the translation." ></td>
	<td class="line x" title="75:224	Morph." ></td>
	<td class="line x" title="76:224	Subj-Verb Noun-Adj AR hard VSO, SVO, pro-drop N-A, A-N ES medium SVO, pro-drop N-A EN simple SVO A-N Table 1: Arabic (AR), Spanish (ES) and English (EN) linguistic features." ></td>
	<td class="line x" title="77:224	4 N-gram-based SMT System The baseline translation system described in this paper implements a log-linear combination of six models: a translation model, a surface target language model, a target tag language model, a word bonus model, a source-to-target lexicon model, and a target-to-source lexicon model." ></td>
	<td class="line x" title="78:224	In contrast to standard phrase-based approaches, the translation model is expressed in tuples, bilingual translation units, and is estimated as an N-gram language model (Marino et al., 2006)." ></td>
	<td class="line x" title="79:224	4.1 Translation Units Translation units (or tuples) are extracted after reordering source words following the unfold method for monotonizing word alignments (Crego et al., 2005)." ></td>
	<td class="line x" title="80:224	Figure 1 shows an example of tuple extraction with the original source-side word order resulting in one tuple (regular); and after reordering the source words resulting in three tuples (unfold)." ></td>
	<td class="line x" title="81:224	Figure 1: Regular Vs. Unfold translation units." ></td>
	<td class="line x" title="82:224	In general, the unfold extraction method outperforms the regular method because it produces smaller, less sparse and more reusable units, which is specially relevant for languages with very different word order." ></td>
	<td class="line x" title="83:224	On the other hand, the unfold method needs the input source words to be reordered during decoding similarly to how source words were reordered in training." ></td>
	<td class="line x" title="84:224	If monotonic decoding were used with unfolded units, translation hypotheses would follow the source language word order." ></td>
	<td class="line x" title="85:224	4.2 Reordering Framework In training time, a set of reordering rules are automatically learned from word alignments." ></td>
	<td class="line x" title="86:224	These rules are used in decoding time to provide the decoder with a set of reordering hypotheses in the form of a reordering input graph." ></td>
	<td class="line x" title="87:224	Rule Extraction Following the unfold technique, source side reorderings are introduced into the training corpus in order to harmonize the word order of the source and target sentences." ></td>
	<td class="line x" title="88:224	For each reordering produced in this step a record is taken in the form of a reordering rule: s1,,sn  i1,,in, where s1,,sn is a sequence of of source words, and i1,,in is a sequence of index positions into which the source words (left-hand side of the rule) are reordered." ></td>
	<td class="line x" title="89:224	It is worth noticing that translation units and reordering rules are tightly coupled." ></td>
	<td class="line x" title="90:224	The reordering rules described so far can only handle reorderings of word sequences already seen in training." ></td>
	<td class="line x" title="91:224	In order to improve the generalization power of these rules, linguistic classes (POS tags, chunks, syntax trees, etc.) can be used instead of raw words in the left-hand side of the rules." ></td>
	<td class="line x" title="92:224	For example, the reordering introduced to unfold the alignments of the regular tuple AEln Almdyr AlEAm  AlEAm Almdyr AEln in Figure 1 can produce the rule: VBD NN JJ  2 1 0, where the left-hand side of the rule contains the sequence of POS tags (verb noun adjective) belonging to the source words involved in reordering." ></td>
	<td class="line x" title="93:224	Search Graph Extension In decoding, the input sentence is handled as a word graph." ></td>
	<td class="line x" title="94:224	A monotonic search graph contains a single path, composed of arcs covering the input words in the original word order." ></td>
	<td class="line x" title="95:224	To allow for reordering, the graph is extended with new arcs, covering the source words in the desired word order." ></td>
	<td class="line x" title="96:224	For a given test sentence, any sequence of input tags fulfilling a left-hand side reordering rule leads to the 55 Figure 2: Linguistic information, reordering graph and translation composition of an Arabic sentence." ></td>
	<td class="line x" title="97:224	addition of a reordering path." ></td>
	<td class="line x" title="98:224	Figure 2 shows an example of an input search graph extension (middle)." ></td>
	<td class="line x" title="99:224	The monotonic search graph is expanded following three different reordering rules." ></td>
	<td class="line x" title="100:224	5 Rules with Chunk Information The generalization power of POS-based reordering rules is somehow limited to short rules (less sparse) which fail to capture many real examples." ></td>
	<td class="line x" title="101:224	Longer rules are needed to model reorderings between full (linguistic) phrases, which are not restricted to any size." ></td>
	<td class="line x" title="102:224	In order to capture such long-distance reorderings, we introduce rules with tags referring to arbitrary large sequences of words: chunk tags." ></td>
	<td class="line x" title="103:224	Chunkbased rules allow the introduction of chunk tags in the left-hand side of the rule." ></td>
	<td class="line x" title="104:224	For instance, the rule: VP NP  1 0 indicates that a verb phrase VP preceding a noun phrase NP are to be swapped." ></td>
	<td class="line x" title="105:224	That is, the sequence of words composing the verb phrase are reordered at the end of the sequence of words composing the noun phrase." ></td>
	<td class="line x" title="106:224	In training, like POS-based rules, a record is taken in the form of a rule whenever a source reordering is introduced by the unfold technique." ></td>
	<td class="line x" title="107:224	To account for chunk-based rules, a chunk tag is used instead of the corresponding POS tags when the words composing the phrase remain consecutive (not necessarily in the same order) after reordering." ></td>
	<td class="line x" title="108:224	Notice that rules are built using POS tags as well as chunk tags." ></td>
	<td class="line x" title="109:224	Since both approaches are based on the same reorderings introduced in training, both POS-based and chunkbased rules collect the same number of training rule instances." ></td>
	<td class="line x" title="110:224	Figure 3 illustrates the process of POS-based and chunk-based rule extraction." ></td>
	<td class="line x" title="111:224	Here, the reordering Figure 3: POS-based and chunk-based Rule extraction: word-alignments, chunk and POS information (top), translation units (middle) and reordering rules (bottom) are shown." ></td>
	<td class="line x" title="112:224	rule is applied over the sequence s2 s3 s4 s5 s6, which is transformed into s6 s3 s4 s5 s2." ></td>
	<td class="line x" title="113:224	As for the chunk rule, the POS tags p3 p4 p5 of the POS rule are replaced by the corresponding chunk tag c2 since words within the phrase remain consecutive after being reordered." ></td>
	<td class="line x" title="114:224	The vocabulary of chunk tags is typically smaller than that of POS tags." ></td>
	<td class="line x" title="115:224	Hence, in order to increase the accuracy of the rules, we always use the POS tag instead of the chunk tag for single word chunks." ></td>
	<td class="line x" title="116:224	In the example in Figure 3, the resulting chunk rule contains the POS tag p6 instead of the corresponding chunk tag c3." ></td>
	<td class="line x" title="117:224	Any sequence of input POS/chunk tags fulfilling a left-hand side reordering rule entails the extension of the permutation graph with a new reordering path." ></td>
	<td class="line x" title="118:224	Figure 2 shows the permutation graph (middle) computed for an Arabic sentence (top) af56 ter applying three reordering rules." ></td>
	<td class="line x" title="119:224	The best path is drawn in bold arcs." ></td>
	<td class="line x" title="120:224	It is important to notice that rules are recursively applied on top of sequences of already reordered words." ></td>
	<td class="line x" title="121:224	Chunk rules are applied over phrases (sequences of words) which may need additional reorderings." ></td>
	<td class="line x" title="122:224	Larger rules are applied before shorter ones in order to allow for an easy implementation of recursive reordering." ></td>
	<td class="line x" title="123:224	Rules are allowed to match any path of the permutation graph consisting of a sequence of words in the original order." ></td>
	<td class="line x" title="124:224	For example, the sequence Almdyr AlEAm is reordered into AlEAm Almdyr following the rule NN JJ  1 0 on top of the monotonic path as well as on top of the path previously reordered by rule VP NP PP PP NP  1 2 3 4 0." ></td>
	<td class="line x" title="125:224	In Figure 2, the best reordering path (bold arcs) could not be hypothesized without recursive reorderings." ></td>
	<td class="line x" title="126:224	6 Refinement of Word Alignments As stated earlier, the Arabic-English language pair presents important word order disparities." ></td>
	<td class="line x" title="127:224	These strong differences make word alignment a very difficult task, typically producing a large number of noisy (wrong) alignments." ></td>
	<td class="line x" title="128:224	The N-gram-based SMT approach suffers highly from the presence of noisy alignments since translation units are extracted out of single alignment-based segmentations of training sentences." ></td>
	<td class="line x" title="129:224	Noisy alignments lead to large translation units, which cause a loss of translation information and add to sparseness problems." ></td>
	<td class="line x" title="130:224	We propose an alignment refinement method to reduce the number of wrong alignments." ></td>
	<td class="line x" title="131:224	The method employs two initial alignment sets: one with high precision, the other with high recall." ></td>
	<td class="line x" title="132:224	We use the Intersection and Union (Och and Ney, 2000) of both alignment directions2 as the high precision and high recall alignment sets, respectively." ></td>
	<td class="line x" title="133:224	We will study the effect of various initial alignment sets (such as grow-diag-final instead of Union) in the future." ></td>
	<td class="line x" title="134:224	The method is based on the fact that linguistic phrases (chunks), like raw words, have translation correspondences and can therefore be aligned." ></td>
	<td class="line x" title="135:224	We use chunk information to reduce the number of allowed alignments for a given word." ></td>
	<td class="line x" title="136:224	The simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2We use IBM-1 to IBM-5 models (Brown et al., 1993) implemented with GIZA++ (Och and Ney, 2003)." ></td>
	<td class="line x" title="137:224	distant chunks." ></td>
	<td class="line x" title="138:224	Since limiting alignments to one-toone chunk links is perhaps too strict, we extend the number of allowed alignments by permitting words in a chunk to be aligned to words in a target range of words." ></td>
	<td class="line x" title="139:224	This target range is computed as a projection of the source chunk under consideration." ></td>
	<td class="line x" title="140:224	The resulting refined set contains all the Intersection alignments and some of the Union." ></td>
	<td class="line x" title="141:224	t1       t2       t3       t4       t5       t6      t7       t8 s3      s4      s5 s6 s7      s8      s9s1     s2 c2 c2c1 c3 c4 c1c3 c4 Figure 4: Chunk projection: solid link are Intersection links and all links (solid and dashed) are Union links." ></td>
	<td class="line x" title="142:224	We outline the algorithm next." ></td>
	<td class="line x" title="143:224	The method can be decomposed in two steps." ></td>
	<td class="line x" title="144:224	In the first step, using the Intersection set of alignments and source-side chunks, each chunk is projected into the target side." ></td>
	<td class="line x" title="145:224	Figure 4 shows an example of word alignment refinement." ></td>
	<td class="line x" title="146:224	The projection ck of the chunk ck is composed of the sequence of consecutive target words [tleft,tright] which can be determined as follows:  All target words tj contained in Intersection links (si,tj) with source word si within ck are considered projection anchors." ></td>
	<td class="line x" title="147:224	In the example in Figure 4, source words of chunk (c2) are aligned into the target side by means of two Intersection alignments, (s3,t3) and (s4,t5), and producing two anchors (t3 and t5)." ></td>
	<td class="line x" title="148:224	 For each source chunk ck, tleft/tright is set by extending its leftmost/rightmost anchor in the left/right direction up to the word before the next anchor (or the first/last word if at sentence edge)." ></td>
	<td class="line x" title="149:224	In the example in Figure 4, c1, c2, c3 and c4 are respectively [t4,t4],[t2,t6],[t1,t2] and [t6,t8]." ></td>
	<td class="line x" title="150:224	In the second step, for every alignment of the Union set, the alignment is discarded if it links a 57 source word si to a target word tj that falls out of the projection of the chunk containing the source word." ></td>
	<td class="line x" title="151:224	Notice that all the Intersection links are contained in the resulting refined set." ></td>
	<td class="line x" title="152:224	In the example in Figure 4, the link (s1,t2) is discarded as t2 falls out of the projection of chunk c1 ([t4,t4])." ></td>
	<td class="line x" title="153:224	A further refinement can be done using the chunks of the target side." ></td>
	<td class="line x" title="154:224	The same technique is applied by switching the role of source and target words/chunks in the algorithm described above and using the output of the basic source-based refinement (described above) as the high-recall alignment set, i.e., instead of Union." ></td>
	<td class="line x" title="155:224	7 Evaluation 7.1 Experimental Framework All of the training data used here is available from the Linguistic Data Consortium (LDC).3 We use an Arabic-English parallel corpus4 consisting of 131K sentence pairs, with approximately 4.1M Arabic tokens and 4.4M English tokens." ></td>
	<td class="line x" title="156:224	Word alignment is done with GIZA++ (Och and Ney, 2003)." ></td>
	<td class="line x" title="157:224	All evaluated systems use the same surface trigram language model, trained on approximately 340 million words of English newswire text from the English Gigaword corpus (LDC2003T05)." ></td>
	<td class="line x" title="158:224	Additionally, we use a 5-gram language model computed over the POS tagged English side of the training corpus." ></td>
	<td class="line x" title="159:224	Language models are implemented using the SRILM toolkit (Stolcke, 2002)." ></td>
	<td class="line x" title="160:224	For Arabic tokenization, we use the Arabic TreeBank tokenization scheme: 4-way normalized segments into conjunction, particle, word and pronominal clitic." ></td>
	<td class="line x" title="161:224	For POS tagging, we use the collapsed tagset for PATB (24 tags)." ></td>
	<td class="line x" title="162:224	Tokenization and POS tagging are done using the publicly available Morphological Analysis and Disambiguation of Arabic (MADA) tool (Habash and Rambow, 2005)." ></td>
	<td class="line x" title="163:224	For chunking Arabic, we use the AMIRA (ASVMT) toolkit (Diab et al., 2004)." ></td>
	<td class="line x" title="164:224	English preprocessing simply included down-casing, separating punctuation from words and splitting off s." ></td>
	<td class="line x" title="165:224	The English side is POS-tagged with TNT(Brants, 2000) and chunked with the freely available OpenNlp5 tools." ></td>
	<td class="line x" title="166:224	3http://www.ldc.upenn.edu 4The parallel text includes Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18)." ></td>
	<td class="line oc" title="167:224	5http://opennlp.sourceforge.net/ We use the standard four-reference NIST MTEval data sets for the years 2003, 2004 and 2005 (henceforth MT03, MT04 and MT05, respectively) for testing and the 2002 data set for tuning.6 BLEU4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and multiple-reference Word Error Rate scores are reported." ></td>
	<td class="line x" title="168:224	SMT decoding is done using MARIE,7 a freely available N-gram-based decoder implementing a beam search strategy with distortion/reordering capabilities (Crego and Marino, 2007a)." ></td>
	<td class="line x" title="169:224	Optimization is done with an in-house implementation of the SIMPLEX (Nelder and Mead, 1965) algorithm." ></td>
	<td class="line x" title="170:224	7.2 Results In this section we assess the accuracy results of the techniques introduced in this paper for alignment refinement and word reordering." ></td>
	<td class="line x" title="171:224	Alignment Refinement Experiment We contrast three systems built from different word alignments: (a.) the Union alignment set of both translation directions (U); (b.) the refined alignment set, detailed in Section 6, employing only source-side chunks (rS); (c.) the refined alignment set employing source as well as target-side chunks (rST)." ></td>
	<td class="line x" title="172:224	For this experiment, the system employs an ngram bilingual translation model (TM) with n = 3 and n = 4." ></td>
	<td class="line x" title="173:224	We also vary the use of a 5-gram targettag language model (ttLM)." ></td>
	<td class="line x" title="174:224	The reordering graph is built using POS-based rules restricted to a maximum size of 6 tokens (POS tags in the left-hand side of the rule)." ></td>
	<td class="line x" title="175:224	The results are shown in Table 2." ></td>
	<td class="line x" title="176:224	Results from the refined alignment (rS) system clearly outperform the results from the alignment union (U) system." ></td>
	<td class="line x" title="177:224	All measures agree in all test sets." ></td>
	<td class="line x" title="178:224	Results further improve when we employ target-side chunks to refine the alignments (rST), although not statistically significantly." ></td>
	<td class="line x" title="179:224	BLEU 95% confidence intervals for the best configuration (last row) are .0162, .0210 and .0135 respectively for MT03, MT04 and MT05." ></td>
	<td class="line x" title="180:224	As anticipated, the N-gram system suffers under high reordering needs when noisy alignments produce long (sparse) tuples." ></td>
	<td class="line x" title="181:224	This can be seen by the increase in translation unit counts when refined links are used to alleviate the sparseness problem." ></td>
	<td class="line o" title="182:224	The number of links of each alignment set over all 6http://www.nist.gov/speech/tests/mt/ 7http://gps-tsc.upc.es/veu/soft/soft/marie/ 58 Align TM ttLM BLEU mWER METEOR MT03 U 3 .4453 51.94 .6356 rS 3 .4586 50.67 .6401 rST 3 .4600 50.64 .6416 rST 4 .4610 50.20 .6401 rST 4 5 .4689 49.36 .6411 MT04 U 3 .4244 50.12 .6055 rS 3 .4317 49.89 .6085 rST 3 .4375 49.69 .6109 rST 4 .4370 49.07 .6093 rST 4 5 .4366 48.70 .6092 MT05 U 3 .4366 50.40 .6306 rS 3 .4447 49.77 .6353 rST 3 .4484 49.09 .6386 rST 4 .4521 48.69 .6377 rST 4 5 .4561 48.07 .6401 Table 2: Evaluation results for experiments on translation units, alignment and modeling." ></td>
	<td class="line x" title="183:224	training data is 5.5M (U), 4.9M (rS) and 4.6M (rST)." ></td>
	<td class="line x" title="184:224	Using the previous sets, the number of unique extracted translation units is 265.5K (U), 346.3K (rS) and 407.8K (rST)." ></td>
	<td class="line x" title="185:224	Extending the TM to order 4 and introducing the ttLM seems to further boost the accuracy results for all sets in terms of mWER and for MT03 and MT05 only in terms of BLEU." ></td>
	<td class="line x" title="186:224	Chunk Reordering Experiment We compare POS-based reordering rules with chunk-based reordering rules under different maximum rule-size constraints." ></td>
	<td class="line x" title="187:224	Results are obtained using TM n = 4, ttLM n=5 and rST refinement alignment." ></td>
	<td class="line x" title="188:224	BLEU scores are shown in Table 3 for all test sets and rule sizes." ></td>
	<td class="line x" title="189:224	Rule size 7R indicates that chunk rules are used with recursive reorderings." ></td>
	<td class="line x" title="190:224	BLEU 2 3 4 5 6 7 8 7R MT03 POS .4364 .4581 .4656 .4690 .4689 .4686 .4685 Chunk .4426 .4637 .4680 .4698 .4703 .4714 .4714 .4725 MT04 POS .4105 .4276 .4332 .4355 .4366 .4362 .4368 Chunk .4125 .4316 .4358 .4381 .4373 .4372 .4373 .4364 MT05 POS .4206 .4465 .4532 .4549 .4561 .4562 .4565 Chunk .4236 .4507 .4561 .4571 .4574 .4575 .4575 .4579 Table 3: BLEU scores according to the maximum size of rules employed." ></td>
	<td class="line x" title="191:224	Table 4 measures the impact of introducing reordering rules limited to a given size (Y axis) on the permutation graphs of input sentences from the MT03 data set (composed of 663 sentences containing 18,325 words)." ></td>
	<td class="line x" title="192:224	Column Total shows the number of additional (extended) paths introduced into the test set permutation graph (i.e., 2,971 additional paths of size 3 POS tags were introduced)." ></td>
	<td class="line x" title="193:224	Columns 3 to 8 show the number of moves made in the 1-best translation output according to the size of the move in words (i.e., 1,652 moves of size 2 words appeared when considering POS rules of up to size 3 words)." ></td>
	<td class="line x" title="194:224	The rows in Table 4 correspond to the columns associated with MT03 in Table 3." ></td>
	<td class="line x" title="195:224	Notice that a chunk tag may refer to multiple words, which explains, for instance, how 42 moves of size 4 appear using chunk rules of size 2." ></td>
	<td class="line x" title="196:224	Overall, short-size reorderings are far more abundant than larger ones." ></td>
	<td class="line x" title="197:224	Size Total 2 3 4 [5,6] [7,8] [9,14] POS rules 2 8,142 2,129 3 +2,971 1,652 707 4 +1,628 1,563 631 230 5 +964 1,531 615 210 82 6 +730 1,510 604 200 123 7 +427 1,497 600 191 121 24 8 +159 1,497 599 191 120 26 Chunk rules 2 9,201 2,036 118 42 20 1 0 3 +4,977 1,603 651 71 42 5 2 4 +1,855 1,542 593 200 73 7 0 5 +1,172 1,514 578 187 118 15 1 6 +760 1,495 573 178 130 20 5 7 +393 1,488 568 173 129 27 10 8 +112 1,488 568 173 129 27 10 7R +393 1,405 546 179 152 54 25 Table 4: Reorderings hypothesized and employed in the 1-best translation output according to their size." ></td>
	<td class="line x" title="198:224	Differences in BLEU (Table 3) are very small across the alternative configurations (POS/chunk)." ></td>
	<td class="line x" title="199:224	It seems that larger reorderings, size 7 to 14, (shown in Table 4) introduce very small accuracy variations when measured using BLEU." ></td>
	<td class="line x" title="200:224	POS rules are able to account for most of the necessary moves (size 2 to 6)." ></td>
	<td class="line x" title="201:224	However, the presence of the larger moves when considering chunk-based rules (together with accuracy improvements) show that long-size reorderings can only be captured by chunk rules." ></td>
	<td class="line x" title="202:224	The largest moves taken by the decoder using POS rules consist of 2 sequences of 8 words (Table 4, column 7, row 9 minus row 8)." ></td>
	<td class="line x" title="203:224	The increase in the number of 59 long moves when considering recursive chunks (7R) means that longer chunk rules provide only valid reordering paths if further (recursive) reorderings are also considered." ></td>
	<td class="line x" title="204:224	The corresponding BLEU score (Table 3, last column) indicates that the new set of moves improves the resulting accuracy." ></td>
	<td class="line x" title="205:224	The general lower scores and inconsistent behavior of MT04 compared to MT03/MT05 may be a result of MT04 being a mix of genres (newswire, speeches and editorials)." ></td>
	<td class="line x" title="206:224	7.3 Error Analysis We conducted a human error analysis by comparing the best results from the POS system to those of the best chunk system." ></td>
	<td class="line x" title="207:224	We used a sample of 155 sentences from MT03." ></td>
	<td class="line x" title="208:224	In this sample, 25 sentences (16%) were actually different between the two analyzed systems." ></td>
	<td class="line x" title="209:224	The differences were determined to involve 30 differing reorderings." ></td>
	<td class="line x" title="210:224	In all of these cases, the chunk system made a move, but the POS system only moved (from source word order) in 60% of the cases." ></td>
	<td class="line x" title="211:224	We manually judged the relative quality of the move (or lack thereof)." ></td>
	<td class="line x" title="212:224	We found that 47% of the time, chunk moves were superior to POS choice." ></td>
	<td class="line x" title="213:224	In 27% of the time POS moves were better." ></td>
	<td class="line x" title="214:224	In the rest of the time, the two systems were equally good or bad." ></td>
	<td class="line x" title="215:224	The main challenge for chunk reordering seems to be the lack of syntactic constraints: in many cases of errors the chunk reordering did not go far enough or went too far, breaking up NPs or passing multiple NPs, respectively." ></td>
	<td class="line x" title="216:224	Additional syntactic features to constrain the reordering model may be needed." ></td>
	<td class="line x" title="217:224	8 Conclusions and Future Work In this work we have described two methods to improve SMT accuracy using shallow syntax information." ></td>
	<td class="line x" title="218:224	First, alignment quality has been improved (in terms of translation accuracy) by pruning out noisy links which do not respect a chunk-tochunk alignment correspondence." ></td>
	<td class="line x" title="219:224	Second, rewrite rules built with two different linguistic annotations, (local) POS tags and (long-spanning) chunk tags, can handle both short/medium and long distance reorderings as well as different degrees of recursive application." ></td>
	<td class="line x" title="220:224	In order to better assess the suitability of chunk rules we carried out a human error analysis which confirmed that long reorderings were effectively captured by chunk rules." ></td>
	<td class="line x" title="221:224	However, the error analysis also revealed that additional syntactic features to constrain the reordering model may be needed." ></td>
	<td class="line x" title="222:224	In the future, we plan to introduce weights into the permutations graph to more accurately drive the search process as well as extend the rules with full syntactic information (parse trees)." ></td>
	<td class="line x" title="223:224	Acknowledgments The first author has been partially funded by the Spanish Government under the AVIVAVOZ project (TEC2006-13694-C03) the Catalan Government under BE-2007 grant and the Universitat Polit`ecnica de Catalunya under UPC-RECERCA grant." ></td>
	<td class="line x" title="224:224	The second author was funded under the DARPA GALE program, contract HR0011-06-C-0023." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0312
Meteor, M-BLEU and M-TER: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output
Agarwal, Abhaya;Lavie, Alon;"></td>
	<td class="line x" title="1:85	Proceedings of the Third Workshop on Statistical Machine Translation, pages 115118, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:85	c2008 Association for Computational Linguistics Meteor, m-bleu and m-ter: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output Abhaya Agarwal and Alon Lavie Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA {abhayaa,alavie}@cs.cmu.edu Abstract This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08." ></td>
	<td class="line x" title="3:85	Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses." ></td>
	<td class="line x" title="4:85	We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments." ></td>
	<td class="line x" title="5:85	We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor . 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years." ></td>
	<td class="line x" title="6:85	Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators." ></td>
	<td class="line x" title="7:85	The most commonly used MT evaluation metric in recent years has been IBMs Bleu metric (Papineni et al., 2002)." ></td>
	<td class="line x" title="8:85	Bleu is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003)." ></td>
	<td class="line x" title="9:85	Various researchers have noted, however, various weaknesses in the metric." ></td>
	<td class="line x" title="10:85	Most notably, Bleu does not produce very reliable sentence-level scores." ></td>
	<td class="line x" title="11:85	Meteor , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses." ></td>
	<td class="line x" title="12:85	Meteor , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level." ></td>
	<td class="line oc" title="13:85	Previous publications on Meteor (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics." ></td>
	<td class="line x" title="14:85	In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English." ></td>
	<td class="line x" title="15:85	This paper provides a brief technical description of Meteor and describes our experiments in re-tuning the metric for improving correlation with the human rankings of translation hypotheses corresponding to a single source sentence." ></td>
	<td class="line x" title="16:85	Our experiments show significant improvement in correlation as a result of retuning which shows the importance of having a metric tunable to different testing conditions." ></td>
	<td class="line x" title="17:85	Also, in order to establish the usefulness of the flexible matching based on stemming and Wordnet, we extend two other widely used metrics bleu and ter which use exact word matching, with the matcher module of Meteor . 2 The Meteor Metric Meteor evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation." ></td>
	<td class="line x" title="18:85	If more than one reference translation is available, the translation is scored against each reference independently, and the best scoring pair is used." ></td>
	<td class="line x" title="19:85	Given a pair of strings to be compared, Meteor creates a word alignment between the two strings." ></td>
	<td class="line x" title="20:85	An alignment is mapping between words, such that every word in each string maps to at most one word in the other string." ></td>
	<td class="line x" title="21:85	This alignment is incrementally produced by a sequence of word-mapping modules." ></td>
	<td class="line x" title="22:85	The exact module maps two words if they are exactly the same." ></td>
	<td class="line x" title="23:85	The porter stem module maps two words if they are the same after they are stemmed us115 ing the Porter stemmer." ></td>
	<td class="line x" title="24:85	The WN synonymy module maps two words if they are considered synonyms, based on the fact that they both belong to the same synset in WordNet." ></td>
	<td class="line x" title="25:85	The word-mapping modules initially identify all possible word matches between the pair of strings." ></td>
	<td class="line x" title="26:85	We then identify the largest subset of these word mappings such that the resulting set constitutes an alignment as defined above." ></td>
	<td class="line x" title="27:85	If more than one maximal cardinality alignment is found, Meteor selects the alignment for which the word order in the two strings is most similar (the mapping that has the least number of crossing unigram mappings)." ></td>
	<td class="line x" title="28:85	The order in which the modules are run reflects wordmatching preferences." ></td>
	<td class="line x" title="29:85	The default ordering is to first apply the exact mapping module, followed by porter stemming and then WN synonymy." ></td>
	<td class="line x" title="30:85	Once a final alignment has been produced between the system translation and the reference translation, the Meteor score for this pairing is computed as follows." ></td>
	<td class="line x" title="31:85	Based on the number of mapped unigrams found between the two strings (m), the total number of unigrams in the translation (t) and the total number of unigrams in the reference (r), we calculate unigram precision P = m/t and unigram recall R = m/r. We then compute a parametrized harmonic mean of P and R (van Rijsbergen, 1979): Fmean = P RP + (1)R Precision, recall and Fmean are based on singleword matches." ></td>
	<td class="line x" title="32:85	To take into account the extent to which the matched unigrams in the two strings are in the same word order, Meteorcomputes a penalty for a given alignment as follows." ></td>
	<td class="line x" title="33:85	First, the sequence of matched unigrams between the two strings is divided into the fewest possible number of chunks such that the matched unigrams in each chunk are adjacent (in both strings) and in identical word order." ></td>
	<td class="line x" title="34:85	The number of chunks (ch) and the number of matches (m) is then used to calculate a fragmentation fraction: frag = ch/m. The penalty is then computed as: Pen =  frag The value of  determines the maximum penalty (0    1)." ></td>
	<td class="line x" title="35:85	The value of  determines the functional relation between fragmentation and the penalty." ></td>
	<td class="line x" title="36:85	Finally, the Meteor score for the alignment between the two strings is calculated as: score = (1 Pen)Fmean The free parameters in the metric, ,  and  are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007)." ></td>
	<td class="line x" title="37:85	3 Extending Bleu and Ter with Flexible Matching Many widely used metrics like Bleu (Papineni et al., 2002) and Ter (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor . Most of them, however, depend on finding exact matches between the words in two strings." ></td>
	<td class="line pc" title="38:85	Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria." ></td>
	<td class="line x" title="39:85	In the following experiments, we extend the Bleu and Ter metrics to use the stemming and Wordnet based word mapping modules from Meteor . Given a translation hypothesis and reference pair, we first align them using the word mapping modules from Meteor . We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis." ></td>
	<td class="line x" title="40:85	We now compute Bleu and Ter with these new references without changing anything inside the metrics." ></td>
	<td class="line x" title="41:85	To get meaningful Bleu scores at segment level, we compute smoothed Bleu as described in (Lin and Och, 2004)." ></td>
	<td class="line x" title="42:85	4 Re-tuning Meteor for Rankings (Callison-Burch et al., 2007) reported that the intercoder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation." ></td>
	<td class="line x" title="43:85	Based on that finding, in WMT-08, only ranking judgments are being collected from the human judges." ></td>
	<td class="line x" title="44:85	The current version of Meteor uses parameters optimized towards maximizing the Pearsons correlation with human judgments of adequacy scores." ></td>
	<td class="line x" title="45:85	It is not clear that the same parameters would be optimal for correlation with human rankings." ></td>
	<td class="line x" title="46:85	So we would like to re-tune the parameters in the metric for maximizing the correlation with ranking judgments instead." ></td>
	<td class="line x" title="47:85	This requires computing full rankings according to the metric and the humans and then computing a suitable correlation measure on those rankings." ></td>
	<td class="line x" title="48:85	4.1 Computing Full Rankings Meteor assigns a score between 0 and 1 to every translation hypothesis." ></td>
	<td class="line x" title="49:85	This score can be converted 116 Language Judgments Binary Sentences English 3978 365 German 2971 334 French 1903 208 Spanish 2588 284 Table 1: Corpus Statistics for Various Languages to rankings trivially by assuming that a higher score indicates a better hypothesis." ></td>
	<td class="line x" title="50:85	In development data, human rankings are available as binary judgments indicating the preferred hypothesis between a given pair." ></td>
	<td class="line x" title="51:85	There are also cases where both the hypotheses in the pair are judged to be equal." ></td>
	<td class="line x" title="52:85	In order to convert these binary judgments into full rankings, we do the following: 1." ></td>
	<td class="line x" title="53:85	Throw out all the equal judgments." ></td>
	<td class="line x" title="54:85	2." ></td>
	<td class="line x" title="55:85	Construct a directed graph where nodes correspond to the translation hypotheses and every binary judgment is represented by a directed edge between the corresponding nodes." ></td>
	<td class="line x" title="56:85	3." ></td>
	<td class="line x" title="57:85	Do a topological sort on the resulting graph and assign ranks in the sort order." ></td>
	<td class="line x" title="58:85	The cycles in the graph are broken by assigning same rank to all the nodes in the cycle." ></td>
	<td class="line x" title="59:85	4.2 Measuring Correlation Following (Ye et al., 2007), we first compute the Spearman correlation between the human rankings and Meteor rankings of the translation hypotheses corresponding to a single source sentence." ></td>
	<td class="line x" title="60:85	Let N be the number of translation hypotheses and D be the difference in ranks assigned to a hypothesis by two rankings, then Spearman correlation is given by: r = 1 6 summationtextD2 N(N2 1) The final score for the metric is the average of the Spearman correlations for individual sentences." ></td>
	<td class="line x" title="61:85	5 Experiments 5.1 Data We use the human judgment data from WMT-07 which was released as development data for the evaluation shared task." ></td>
	<td class="line x" title="62:85	Amount of data available for various languages is shown in Table 1." ></td>
	<td class="line x" title="63:85	Development data contains the majority judgments (not every hypotheses pair was judged by same number of judges) which means that in the cases where multiple judges judged the same pair of hypotheses, the judgment given by majority of the judges was considered." ></td>
	<td class="line x" title="64:85	English German French Spanish  0.95 0.9 0.9 0.9  0.5 3 0.5 0.5  0.45 0.15 0.55 0.55 Table 2: Optimal Values of Tuned Parameters for Various Languages Original Re-tuned English 0.3813 0.4020 German 0.2166 0.2838 French 0.2992 0.3640 Spanish 0.2021 0.2186 Table 3: Average Spearman Correlation with Human Rankings for Meteor on Development Data 5.2 Methodology We do an exhaustive grid search in the feasible ranges of parameter values, looking for parameters that maximize the average Spearman correlation over the training data." ></td>
	<td class="line x" title="65:85	To get a fair estimate of performance, we use 3-fold cross validation on the development data." ></td>
	<td class="line x" title="66:85	Final parameter values are chosen as the best performing set on the data pooled from all the folds." ></td>
	<td class="line x" title="67:85	5.3 Results 5.3.1 Re-tuning Meteor for Rankings The re-tuned parameter values are shown in Table 2 while the average Spearman correlations for various languages with original and re-tuned parameters are shown in Table 3." ></td>
	<td class="line x" title="68:85	We get significant improvements for all the languages." ></td>
	<td class="line x" title="69:85	Gains are specially pronounced for German and French." ></td>
	<td class="line x" title="70:85	Interestingly, weight for recall becomes even higher than earlier parameters where it was already high." ></td>
	<td class="line x" title="71:85	So it seems that ranking judgments are almost entirely driven by the recall in all the languages." ></td>
	<td class="line x" title="72:85	Also the re-tuned parameters for all the languages except German are quite similar." ></td>
	<td class="line x" title="73:85	5.3.2 m-bleu and m-ter Table 4 shows the average Spearman correlations of m-bleu and m-ter with human rankings." ></td>
	<td class="line x" title="74:85	For English, both m-bleu and m-ter show considerable improvements." ></td>
	<td class="line x" title="75:85	For other languages, improvements in m-ter are smaller but consistent." ></td>
	<td class="line x" title="76:85	m-bleu , however, doesnt shows any improvements in this case." ></td>
	<td class="line x" title="77:85	A possible reason for this behavior can be the lack of a WN synonymy module for languages other than English which results in fewer extra matches over the exact matching baseline." ></td>
	<td class="line x" title="78:85	Additionally, French, German and Spanish have a richer morphology as compared to English." ></td>
	<td class="line x" title="79:85	The morphemes in these languages 117 Exact Match Flexible Match English: Bleu 0.2486 0.2747 Ter 0.1598 0.2033 French: Bleu 0.2906 0.2889 Ter 0.2472 0.2604 German: Bleu 0.1829 0.1806 Ter 0.1509 0.1668 Spanish: Bleu 0.1804 0.1847 Ter 0.1787 0.1839 Table 4: Average Spearman Correlation with Human Rankings for m-bleu and m-ter carry much more information and different forms of the same word may not be as freely replaceable as in English." ></td>
	<td class="line x" title="80:85	A more fine grained strategy for matching words in these languages remains an area of further investigation." ></td>
	<td class="line x" title="81:85	6 Conclusions In this paper, we described the re-tuning of Meteor parameters to better correlate with human rankings of translation hypotheses." ></td>
	<td class="line x" title="82:85	Results on the development data indicate that the re-tuned version is significantly better at predicting ranking than the earlier version." ></td>
	<td class="line x" title="83:85	We also presented enhanced Bleu and Ter that use the flexible word matching module from Meteor and show that this results in better correlations as compared to the default exact matching versions." ></td>
	<td class="line x" title="84:85	The new version of Meteor will be soon available on our website at: http://www.cs.cmu.edu/~alavie/METEOR/ . This release will also include the flexible word matcher module which can be used to extend any metric with the flexible matching." ></td>
	<td class="line x" title="85:85	Acknowledgments The work reported in this paper was supported by NSF Grant IIS-0534932." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0322
Kernel Regression Framework for Machine Translation: UCL System Description for WMT 2008 Shared Translation Task
Wang, Zhuoran;Shawe-Taylor, John;"></td>
	<td class="line x" title="1:68	Proceedings of the Third Workshop on Statistical Machine Translation, pages 155158, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:68	c2008 Association for Computational Linguistics Kernel Regression Framework for Machine Translation: UCL System Description for WMT 2008 Shared Translation Task Zhuoran Wang University College London Dept. of Computer Science Gower Street, London, WC1E 6BT United Kingdom z.wang@cs.ucl.ac.uk John Shawe-Taylor University College London Dept. of Computer Science Gower Street, London, WC1E 6BT United Kingdom jst@cs.ucl.ac.uk Abstract The novel kernel regression model for SMT only demonstrated encouraging results on small-scale toy data sets in previous works due to the complexities of kernel methods." ></td>
	<td class="line x" title="3:68	It is the first time results based on the real-world data from the shared translation task will be reported at ACL 2008 Workshop on Statistical Machine Translation." ></td>
	<td class="line x" title="4:68	This paper presents the key modules of our system, including the kernel ridge regression model, retrieval-based sparse approximation, the decoding algorithm, as well as language modeling issues under this framework." ></td>
	<td class="line x" title="5:68	1 Introduction This paper follows the work in (Wang et al., 2007; Wang and Shawe-Taylor, 2008) which applied the kernel regression method with high-dimensional outputs proposed originally in (Cortes et al., 2005) to statistical machine translation (SMT) tasks." ></td>
	<td class="line x" title="6:68	In our approach, the machine translation problem is viewed as a string-to-string mapping, where both the source and the target strings are embedded into their respective kernel induced feature spaces." ></td>
	<td class="line x" title="7:68	Then kernel ridge regression is employed to learn the mapping from the input feature space to the output one." ></td>
	<td class="line x" title="8:68	As a kernel method, this model offers the potential advantages of capturing very high-dimensional correspondences among the features of the source and target languages as well as easy integration of additional linguistic knowledge via selecting particular kernels." ></td>
	<td class="line x" title="9:68	However, unlike the sequence labeling tasks such as optical character recognition in (Cortes et al., 2005), the complexity of the SMT problem itself together with the computational complexities of kernel methods significantly complicate the implementation of the regression technique in this field." ></td>
	<td class="line x" title="10:68	Our system is actually designed as a hybrid of the classic phrase-based SMT model (Koehn et al., 2003) and the kernel regression model as follows: First, for each source sentence a small relevant set of sentence pairs are retrieved from the large-scale parallel corpus." ></td>
	<td class="line x" title="11:68	Then, the regression model is trained on this small relevant set only as a sparse approximation of the regression hyperplane trained on the entire training set, as proposed in (Wang and ShaweTaylor, 2008)." ></td>
	<td class="line x" title="12:68	Finally, a beam search algorithm is utilized to decode the target sentence from the very noisy output feature vector we predicted, with the support of a pre-trained phrase table to generate possible hypotheses (candidate translations)." ></td>
	<td class="line x" title="13:68	In addition, a language model trained on a monolingual corpus can be integrated either directly into the regression model or during the decoding procedure as an extra scoring function." ></td>
	<td class="line x" title="14:68	Before describing each key component of our system in detail, we give a block diagram overview in Figure 1." ></td>
	<td class="line x" title="15:68	2 Problem Formulation Concretely, the machine translation problem in our method is formulated as follows." ></td>
	<td class="line x" title="16:68	If we define a feature space Hx of our source language X, and define the mapping  : X  Hx, then a sentence x  X can be expressed by its feature vector (x)  Hx." ></td>
	<td class="line x" title="17:68	The definition of the feature space Hy of our target language Y can be made in a similar way, with cor155 AlignmentParallel Corpus Retriever Phrase Table Phrase Extraction Kernel Regression Decoder Monolingual Corpus Language Modeling N-gram Model Target Text Relevant Set Source Text Figure 1: System overview." ></td>
	<td class="line x" title="18:68	The processes in gray blocks are pre-performed for the whole system, while the white blocks are online processes for each input sentence." ></td>
	<td class="line x" title="19:68	The two dash-line arrows represent two possible ways of language model integration in our system described in Section 6." ></td>
	<td class="line x" title="20:68	responding mapping  : Y  Hy." ></td>
	<td class="line x" title="21:68	Now in the machine translation task, we are trying to seek a matrix represented linear operator W, such that: (y) = W(x) (1) to predict the translation y for an arbitrary source sentence x. 3 Kernel Ridge Regression Based on a set of training samples, i.e. bilingual sentence pairs S = {(xi,yi) : xi  X,yi  Y,i = 1,,m.}, we use ridge regression to learn the W in Equation (1), as: min bardblWM Mbardbl2F + bardblWbardbl2F (2) where M = [(x1),,(xm)], M = [(y1),,(ym)], bardbl  bardblF denotes the Frobenius norm that is a matrix norm defined as the square root of the sum of the absolute squares of the elements in that matrix, and  is a regularization coefficient." ></td>
	<td class="line x" title="22:68	Differentiating the expression and setting it to zero gives the explicit solution of the ridge regression problem: W = M(K + I)1M (3) where I is the identity matrix, and K = MM = ((xi,xj)1i,jm)." ></td>
	<td class="line x" title="23:68	Note here, we use the kernel function: (xi,xj) = (xi),(xj) = (xi)(xj) (4) to denote the inner product between two feature vectors." ></td>
	<td class="line x" title="24:68	If the feature spaces are properly defined, the kernel trick will allow us to avoid dealing with the very high-dimensional feature vectors explicitly (Shawe-Taylor and Cristianini, 2004)." ></td>
	<td class="line x" title="25:68	Inserting Equation (3) into Equation (1), we obtain our prediction as: (y) = M(K + I)1k(x) (5) where k(x) = ((x,xi)1im) is an m  1 column matrix." ></td>
	<td class="line x" title="26:68	Note here, we will use the exact matrix inversion instead of iterative approximations." ></td>
	<td class="line x" title="27:68	3.1 N-gram String Kernel In the practical learning and prediction processes, only the inner products of feature vectors are required, which can be computed with the kernel function implicitly without evaluating the explicit coordinates of points in the feature spaces." ></td>
	<td class="line x" title="28:68	Here, we define our features of a sentence as its word n-gram counts, so that a blended n-gram string kernel can be used." ></td>
	<td class="line x" title="29:68	That is, if we denote by xi:j a substring of sentence x starting with the ith word and ending with the jth, then for two sentences x and z, the blended n-gram string kernel is computed as: (x,z) = nsummationdisplay p=1 |x|p+1summationdisplay i=1 |z|p+1summationdisplay j=1 [[xi:i+p1 = zj:j+p1]] (6) Here, |  | denotes the length of the sentence, and [[]] is the indicator function for the predicate." ></td>
	<td class="line x" title="30:68	In our system, the blended tri-gram kernel is used, which means we count the n-grams of length up to 3." ></td>
	<td class="line x" title="31:68	4 Retrieval-based Sparse Approximation For SMT, we are not able to use the entire training set that contains millions of sentences to train our regression model." ></td>
	<td class="line x" title="32:68	Fortunately, it is not necessary either." ></td>
	<td class="line x" title="33:68	Wang and Shawe-Taylor (2008) suggested that a small set of sentences whose source is relevant to the input can be retrieved, and the regression model can be trained on this small-scale relevant set only." ></td>
	<td class="line x" title="34:68	156 Src n y a-t-il pas ici deux poids , deux mesures Rlv pourquoi y a-t-il deux poids , deux mesures pourquoi deux poids et deux mesures peut-etre n y a-t-il pas d epidemie non plus pourquoi n y a-t-il pas urgence cette directive doit exister d ici deux mois Table 1: A sample input (Src) and some of the retrieved relevant examples (Rlv)." ></td>
	<td class="line x" title="35:68	In our system, we take each sentence as a document and use the tf-idf metric that is frequently used in information retrieval tasks to retrieve the relevant set." ></td>
	<td class="line x" title="36:68	Preliminary experiments show that the size of the relevant set should be properly controlled, as if many sentences that are not very close to the source text are involved, they will correspond to adding noise." ></td>
	<td class="line x" title="37:68	Hence, we use a threshold of the tf-idf score to filter the relevant set." ></td>
	<td class="line x" title="38:68	On average, around 1500 sentence pairs are extracted for each source sentence." ></td>
	<td class="line x" title="39:68	Table 1 shows a sample input and some of its top relevant sentences retrieved." ></td>
	<td class="line x" title="40:68	5 Decoding After the regression, we have a prediction of the target feature vector as in Equation (1)." ></td>
	<td class="line x" title="41:68	To obtain the target sentence, a decoding algorithm is still required to solve the pre-image problem." ></td>
	<td class="line x" title="42:68	This is achieved in our system by seeking the sentence y whose feature vector has the minimum Euclidean distance to the prediction, as: y = arg min yY(x) bardblW(x)  (y)bardbl (7) where Y(x)  Y denotes a finite set covering all potential translations for the given source sentence x. To obtain a smaller search space and more reliable translations, Y(x) is generated with the support of a phrase table extracted from the whole training set." ></td>
	<td class="line x" title="43:68	Then a modified beam search algorithm is employed, in which we restricted the distortion of the phrases by only allowing adjacent phrases to exchange their positions, and rank the search states in the beams according to Equation (7) but applied directly to the partial translations and their corresponding source parts." ></td>
	<td class="line x" title="44:68	A more detailed explanation of the decoding algorithm can be found in (Wang et al., 2007)." ></td>
	<td class="line x" title="45:68	In addition, Wang and Shawe-Taylor (2008) further showed that the search error rate of this algorithm is acceptable." ></td>
	<td class="line x" title="46:68	6 Language Model Integration In previous works (Wang et al., 2007; Wang and Shawe-Taylor, 2008), there was no language model utilized in the regression framework for SMT, as similar function can be achieved by the correspondences among the n-gram features." ></td>
	<td class="line x" title="47:68	It was demonstrated to work well on small-scale toy data, however, real-world data are much more sparse and noisy, where a language model will help significantly." ></td>
	<td class="line x" title="48:68	There are two ways to integrate a language model in our framework." ></td>
	<td class="line x" title="49:68	First, the most straightforward solution is to add a weight to adjust the strength of the regression based translation scores and the language model score during the decoding procedure." ></td>
	<td class="line x" title="50:68	Alternatively, as language model is n-gram-based which matches the definition of our feature space, we can add a langauge model loss to the objective function of our regression model as follows." ></td>
	<td class="line x" title="51:68	We define our language score for a target sentence y as: LM(y) = V(y) (8) where V is a vector whose components Vyyy will typically be log-probabilities logP(y|yy), and y, y and y are arbitrary words." ></td>
	<td class="line x" title="52:68	Note here, in order to match our blended tri-gram induced feature space, we can make V of the same dimension as (y), while zero the components corresponding to uni-grams and bi-grams." ></td>
	<td class="line x" title="53:68	Then the regression problem can be defined as: minbardblWMMbardbl2F +1bardblWbardbl2F 2VWM1 (9) where 2 is a coefficient balancing between the prediction being close to the target feature vector and being a fluent target sentence, and 1 denotes a vector with components 1." ></td>
	<td class="line x" title="54:68	By differentiating the expression with respect to W and setting the result to zero, we can obtain the explicit solution as: W = (M + 2V1)(K + 1I)1M (10) 7 Experimental Results Preliminary experiments are carried out on the French-English portion of the Europarl corpus." ></td>
	<td class="line o" title="55:68	We 157 System BLEU (%) NIST METEOR (%) TER (%) WER (%) PER (%) Kernel Regression 26.59 7.00 52.63 55.98 60.52 43.20 Moses 31.15 7.48 56.80 55.14 59.85 42.79 Table 3: Evaluations based on different metrics with comparison to Moses." ></td>
	<td class="line x" title="56:68	train our regression model on the training set, and test the effects of different language models on the development set (test2007)." ></td>
	<td class="line x" title="57:68	The results evaluated by BLEU score (Papineni et al., 2002) is shown in Table 2." ></td>
	<td class="line x" title="58:68	It can be found that integrating the language model into the regression framework works slightly better than just using it as an additional score component during decoding." ></td>
	<td class="line x" title="59:68	But language models of higher-order than the n-gram kernel cannot be formulated to the regression problem, which would be a drawback of our system." ></td>
	<td class="line oc" title="60:68	Furthermore, the BLEU score performance suggests that our model is not very powerful, but some interesting hints can be found in Table 3 when we compare our method with a 5-gram language model to a state-of-the-art system Moses (Koehn and Hoang, 2007) based on various evaluation metrics, including BLEU score, NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER and PER." ></td>
	<td class="line o" title="61:68	It is shown that our systems TER, WER and PER scores are very close to Moses, though the gaps in BLEU, NIST and METEOR are significant, which suggests that we would be able to produce accurate translations but might not be good at making fluent sentences." ></td>
	<td class="line x" title="62:68	8 Conclusion This work is a novel attempt to apply the advanced kernel method to SMT tasks." ></td>
	<td class="line x" title="63:68	The contribution at this stage is still preliminary." ></td>
	<td class="line x" title="64:68	When applied to real-world data, this approach is not as powerful as the state-ofthe-art phrase-based log-linear model." ></td>
	<td class="line x" title="65:68	However, interesting prospects can be expected from the shared translation task." ></td>
	<td class="line x" title="66:68	Acknowledgements This work is supported by the European Commission under the IST Project SMART (FP6-033917)." ></td>
	<td class="line x" title="67:68	no-LM LM13gram LM23gram LM15gram BLEU 23.27 25.19 25.66 26.59 Table 2: BLEU score performance of different language models." ></td>
	<td class="line x" title="68:68	LM1 denotes adding the language model during decoding process, while LM2 represents integrating the language model into the regression framework as described in Problem (9)." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0331
Ranking vs. Regression in Machine Translation Evaluation
Duh, Kevin;"></td>
	<td class="line x" title="1:100	Proceedings of the Third Workshop on Statistical Machine Translation, pages 191194, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:100	c2008 Association for Computational Linguistics Ranking vs. Regression in Machine Translation Evaluation Kevin Duh Dept. of Electrical Engineering University of Washington Seattle, WA 98195 kevinduh@u.washington.edu Abstract Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology." ></td>
	<td class="line x" title="3:100	Most automatic evaluation methods proposed to date are score-based: they compute scores that represent translation quality, and MT systems are compared on the basis of these scores." ></td>
	<td class="line x" title="4:100	We advocate an alternative perspective of automatic MT evaluation based on ranking." ></td>
	<td class="line x" title="5:100	Instead of producing scores, we directly produce a ranking over the set of MT systems to be compared." ></td>
	<td class="line x" title="6:100	This perspective is often simpler when the evaluation goal is system comparison." ></td>
	<td class="line x" title="7:100	We argue that it is easier to elicit human judgments of ranking and develop a machine learning approach to train on rank data." ></td>
	<td class="line x" title="8:100	We compare this ranking method to a score-based regression method on WMT07 data." ></td>
	<td class="line x" title="9:100	Results indicate that ranking achieves higher correlation to human judgments, especially in cases where ranking-specific features are used." ></td>
	<td class="line x" title="10:100	1 Motivation Automatic evaluation of machine translation (MT) systems is an important research topic for the advancement of MT technology, since automatic evaluation methods can be used to quickly determine the (approximate) quality of MT system outputs." ></td>
	<td class="line x" title="11:100	This is useful for tuning system parameters and for comparing different techniques in cases when human judgments for each MT output are expensivie to obtain." ></td>
	<td class="line x" title="12:100	Many automatic evaluation methods have been proposed to date." ></td>
	<td class="line x" title="13:100	Successful methods such as BLEU Work supported by an NSF Graduate Research Fellowship." ></td>
	<td class="line x" title="14:100	(Papineni et al., 2002) work by comparing MT output with one or more human reference translations and generating a similarity score." ></td>
	<td class="line x" title="15:100	Methods differ by the definition of similarity." ></td>
	<td class="line oc" title="16:100	For instance, BLEU and ROUGE (Lin and Och, 2004) are based on n-gram precisions, METEOR (Banerjee and Lavie, 2005) and STM (Liu and Gildea, 2005) use word-class or structural information, Kauchak (2006) leverages on paraphrases, and TER (Snover et al., 2006) uses edit-distances." ></td>
	<td class="line x" title="17:100	Currently, BLEU is the most popular metric; it has been shown that it correlates well with human judgments on the corpus level." ></td>
	<td class="line x" title="18:100	However, finding a metric that correlates well with human judgments on the sentence-level is still an open challenge (Blatz and others, 2003)." ></td>
	<td class="line x" title="19:100	Machine learning approaches have been proposed to address the problem of sentence-level evaluation." ></td>
	<td class="line x" title="20:100	(Corston-Oliver et al., 2001) and (Kulesza and Shieber, 2004) train classifiers to discriminate between human-like translations and automatic translations, using features from the aforementioned metrics (e.g. n-gram precisions)." ></td>
	<td class="line x" title="21:100	In contrast, (Albrecht and Hwa, 2007) argues for a regression approach that directly predicts human adequecy/fluency scores." ></td>
	<td class="line x" title="22:100	All the above methods are score-based in the sense that they generate a score for each MT system output." ></td>
	<td class="line x" title="23:100	When the evaluation goal is to compare multiple MT systems, scores are first generated independently for each system, then systems are ranked by their respective scores." ></td>
	<td class="line x" title="24:100	We think that this twostep process may be unnecessarily complex." ></td>
	<td class="line x" title="25:100	Why solve a more difficult problem of predicting the quality of MT system outputs, when the goal is simply 191 to compare systems?" ></td>
	<td class="line x" title="26:100	In this regard, we propose a ranking-based approach that directly ranks a set of MT systems without going through the intermediary of system-specific scores." ></td>
	<td class="line x" title="27:100	Our approach requires (a) training data in terms of human ranking judgments of MT outputs, and (b) a machine learning algorithm for learning and predicting rankings.1 The advantages of a ranking approach are:  It is often easier for human judges to rank MT outputs by preference than to assign absolute scores (Vilar et al., 2007)." ></td>
	<td class="line x" title="28:100	This is because it is difficult to quantify the quality of a translation accurately, but relative easy to tell which one of several translations is better." ></td>
	<td class="line x" title="29:100	Thus humanannotated data based on ranking may be less costly to acquire." ></td>
	<td class="line x" title="30:100	 The interand intra-annotator agreement for ranking is much more reasonable than that of scoring." ></td>
	<td class="line x" title="31:100	For instance, Callison-Burch (2007) found the inter-annotator agreement (Kappa) for scoring fluency/adequency to be around .22-.25, whereas the Kappa for ranking is around .37-.56." ></td>
	<td class="line x" title="32:100	Thus human-annotated data based on ranking may be more reliable to use." ></td>
	<td class="line x" title="33:100	 As mentioned earlier, when the final goal of the evaluation is comparing systems, ranking more directly solves the problem." ></td>
	<td class="line x" title="34:100	A scoring approach essentially addresses a more difficult problem of estimating MT output quality." ></td>
	<td class="line x" title="35:100	Nevertheless, we note that score-based approaches remain important in cases when the absolute difference between MT quality is desired." ></td>
	<td class="line x" title="36:100	For instance, one might wonder by how much does the top-ranked MT system outperform the secondranked system, in which case a ranking-based approach provide no guidance." ></td>
	<td class="line x" title="37:100	In the following, Section 2 formulates the sentence-level MT evaluation problem as a ranking problem; Section 3 explains a machine learning approach for training and predicting rankings; this is our submission to the WMT2008 Shared Evaluation 1Our ranking approach is similar to Ye et." ></td>
	<td class="line x" title="38:100	al." ></td>
	<td class="line x" title="39:100	(2007), who was the first to advocate MT evaluation as a ranking problem." ></td>
	<td class="line x" title="40:100	Here we focus on comparing ranking vs. scoring approaches, which was not done in previous work." ></td>
	<td class="line x" title="41:100	task." ></td>
	<td class="line x" title="42:100	Ranking vs. scoring approaches are compared in Section 4." ></td>
	<td class="line x" title="43:100	2 Formulation of the Ranking Problem We formulate the sentence-level MT evaluation problem as follows: Suppose there are T source sentences to be translated." ></td>
	<td class="line x" title="44:100	Let rt, t = 1T be the set of references2." ></td>
	<td class="line x" title="45:100	Corresponding to each source sentence, there are N MT system outputs o(n)t , n = 1N and Mt (Mt  N) human evaluations." ></td>
	<td class="line x" title="46:100	The evaluations are represented as Mt-dimensional label vectors yt." ></td>
	<td class="line x" title="47:100	In a scoring approach, the elements of yt may correspond to, e.g. a fluency score on a scale of 1 to 5." ></td>
	<td class="line x" title="48:100	In a ranking approach, they may correspond to relative scores that are used to represent ordering (e.g. yt = [6;1;3] means that there are three outputs, and the first is ranked best, followed by third, then second.)" ></td>
	<td class="line x" title="49:100	In order to do machine learning, we extract feature vectors x(n)t from each pair of rt and o(n)t .3 The set {(x(n)t ,yt)}t=1T forms the training set." ></td>
	<td class="line x" title="50:100	In a scoring approach, we train a function f with f(x(n)t )  y(n)." ></td>
	<td class="line x" title="51:100	In a ranking approach, we train f such that higher-ranked outputs have higher function values." ></td>
	<td class="line x" title="52:100	In the example above, we would want: f(x(n=1)t ) > f(x(n=3)t ) > f(x(n=2)t )." ></td>
	<td class="line x" title="53:100	Once f is trained, it can be applied to rank any new data: this is done by extracting features from references/outputs and sorting by function values." ></td>
	<td class="line x" title="54:100	3 Implementation 3.1 Sentence-level scoring and ranking We now describe the particular scoring and ranking implementations we examined and submitted to the WMT2008 Shared Evaluation task." ></td>
	<td class="line x" title="55:100	In the scoring approach, f is trained using RegressionSVM (Drucker and others, 1996); in the ranking approach, we examined RankSVM (Joachims, 2002) and RankBoost (Freund et al., 2003)." ></td>
	<td class="line x" title="56:100	We used only linear kernels for RegressionSVM and RankSVM, while allowed RankBoost to produce non-linear f based on a feature thresholds." ></td>
	<td class="line x" title="57:100	2Here we assume single reference for ease of notation; this can be easily extended for multiple reference 3Only Mt (notN) features vectors are extracted in practice." ></td>
	<td class="line x" title="58:100	192 ID Description 1-4 log of ngram precision, n=14 5 ratio of hypothesis and reference length 6-9 ngram precision, n=14 10-11 hypothesis and reference length 12 BLEU 13 Smooth BLEU 14-20 Intra-set features for ID 5-9, 12,13 Table 1: Feature set: Features 1-5 can be combined (with uniform weights) to form the log(BLEU) score." ></td>
	<td class="line x" title="59:100	Features 6-11 are redundant statistics, but scaled differently." ></td>
	<td class="line x" title="60:100	Feature 12 is sentence-level BLEU; Feature 13 is a modified version with add-1 count to each ngram precision (this avoids prevalent zeros)." ></td>
	<td class="line x" title="61:100	Features 14-20 are only available in the ranking approach; they are derived by comparing different outputs within the same set to be ranked." ></td>
	<td class="line x" title="62:100	The complete feature set is shown in Table 1." ></td>
	<td class="line x" title="63:100	We restricted our feature set to traditional BLEU statistics since our experimental goal is to directly compare regression, ranking, and BLEU." ></td>
	<td class="line x" title="64:100	Features 1420 are the only novel features proposed here." ></td>
	<td class="line x" title="65:100	We wanted to examine features that are enabled by a ranking approach, but not possible for a scoring approach." ></td>
	<td class="line x" title="66:100	We thus introduce intra-set features, which are statistics computed by observing the entire set of existing features {x(n)t }n=1Mt." ></td>
	<td class="line x" title="67:100	For instance: We define Feature 14 by looking at the relative 1-gram precision (Feature 1) in the set of Mt outputs." ></td>
	<td class="line x" title="68:100	Feature 14 is set to value 1 for the output which has the best 1-gram precision, and value 0 otherwise." ></td>
	<td class="line x" title="69:100	Similarly, Feature 15 is a binary variable that is 1 for the output with the best 2-gram precision, and 0 for all others." ></td>
	<td class="line x" title="70:100	The advantage of intra-set features is calibration." ></td>
	<td class="line x" title="71:100	e.g. If the outputs for rt=1 all have relatively high BLEU compared to those of rt=2, the basic BLEU features will vary widely across the two sets, making it more difficult to fit a ranking function." ></td>
	<td class="line x" title="72:100	On the other hand, intra-set features are of the same scale ([0,1] in this case) across the two sets and therefore induce better margins." ></td>
	<td class="line x" title="73:100	While we have only explored one particular instantiation of intra-set features, many other definitions are imaginable." ></td>
	<td class="line x" title="74:100	Novel intra-set features is a promising research direction; experiments indicate that they are most important in helping ranking outperform regression." ></td>
	<td class="line x" title="75:100	3.2 Corpus-level ranking Sentence-level evaluation generates a ranking for each source sentence." ></td>
	<td class="line x" title="76:100	How does one produce an overall corpus-level ranking based on a set of sentence-level rankings?" ></td>
	<td class="line x" title="77:100	This is known as the consensus ranking or rank aggregation problem, which can be NP-hard under certain formulations (Meila et al., 2007)." ></td>
	<td class="line x" title="78:100	We use the FV heuristic (Fligner and Verducci, 1988), which estimates the empirical probability Pij that system i ranks above system j from sentence-level rankings (i.e. Pij = number of sentences where i ranks better than j, divided by total number of sentences)." ></td>
	<td class="line x" title="79:100	The corpuslevel ranking of system i is then defined assummationtextjprime Pijprime." ></td>
	<td class="line x" title="80:100	4 Experiments For experiments, we split the provided development data into train, dev, and test sets (see Table 2)." ></td>
	<td class="line x" title="81:100	The data split is randomized at the level of different evaluation tracks (e.g. en-es.test, de-en.test are different tracks) in order to ensure that dev/test are sufficiently novel with respect to the training data." ></td>
	<td class="line x" title="82:100	This is important since machine learning approaches have the risk of overfitting and spreading data from the same track to both train and test could lead to overoptimistic results." ></td>
	<td class="line x" title="83:100	Train Dev Test # tracks 8 3 3 # sets 1504 (63%) 514 (21%) 390 (16%) # sent 6528 (58%) 2636 (23%) 2079 (19%) Table 2: Data characteristics: the training data contains 8 tracks, which contained 6528 sentence evaluations or 1504 sets of human rankings (T = 1504)." ></td>
	<td class="line x" title="84:100	In the first experiment, we compared Regression SVM and Rank SVM (both used Features 1-12) by training on varying amounts of training data." ></td>
	<td class="line x" title="85:100	The sentence-level rankings produced by each are compared to human judgments using the Spearman rank correlation coefficient (see Figure 1)." ></td>
	<td class="line x" title="86:100	In the second experiment, we compared all ranking and scoring methods discussed thus far." ></td>
	<td class="line x" title="87:100	The full training set is used; the dev set is used to tune the cost parameter for the SVMs and number of iterations for RankBoost, which is then applied without modification to the test set." ></td>
	<td class="line x" title="88:100	Table 3 shows the aver193 0 10 20 30 40 50 60 70 80 90 1000.22 0.24 0.26 0.28 0.3 0.32 0.34 0.36 Percentage of training data Spearman coeff Data ablation results on Dev Set   RankSVM RegressionSVM Figure 1: Ranking slightly outperforms Regression for various amounts of training data." ></td>
	<td class="line x" title="89:100	Regression results appear to be less stable, with a rise/fall in average Spearman coefficent around 20%, possibly because linear regression functions become harder to fit with more data." ></td>
	<td class="line x" title="90:100	age Spearman coefficient for different methods and different feature sets." ></td>
	<td class="line x" title="91:100	There are several interesting observations: 1." ></td>
	<td class="line x" title="92:100	BLEU performs poorly, but SmoothedBLEU is almost as good as the machine learning methods that use same set of basic BLEU features." ></td>
	<td class="line x" title="93:100	2." ></td>
	<td class="line x" title="94:100	Rank SVM slightly outperforms RankBoost." ></td>
	<td class="line x" title="95:100	3." ></td>
	<td class="line x" title="96:100	Regression SVM and Rank SVM gave similar results under the same feature set." ></td>
	<td class="line x" title="97:100	However, Rank SVM gave significant improvements when intra-set features are incorporated." ></td>
	<td class="line x" title="98:100	The last observation is particularly important: it shows that the training criteria differences between the ranking and regression is actually not critical." ></td>
	<td class="line x" title="99:100	Ranking can outperform regression, but only when ranking-specific features are considered." ></td>
	<td class="line x" title="100:100	Without intra-set features, ranking methods may be suffering the same calibration problems as regression." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0334
Dynamic Model Interpolation for Statistical Machine Translation
Finch, Andrew;Sumita, Eiichiro;"></td>
	<td class="line x" title="1:154	Proceedings of the Third Workshop on Statistical Machine Translation, pages 208215, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:154	c2008 Association for Computational Linguistics Abstract This paper presents a technique for classdependent decoding for statistical machine translation (SMT)." ></td>
	<td class="line x" title="3:154	The approach differs from previous methods of class-dependent translation in that the class-dependent forms of all models are integrated directly into the decoding process." ></td>
	<td class="line x" title="4:154	We employ probabilistic mixture weights between models that can change dynamically on a segment-by-segment basis depending on the characteristics of the source segment." ></td>
	<td class="line x" title="5:154	The effectiveness of this approach is demonstrated by evaluating its performance on travel conversation data." ></td>
	<td class="line x" title="6:154	We used the approach to tackle the translation of questions and declarative sentences using classdependent models." ></td>
	<td class="line x" title="7:154	To achieve this, our system integrated two sets of models specifically built to deal with sentences that fall into one of two classes of dialog sentence: questions and declarations, with a third set of models built to handle the general class." ></td>
	<td class="line x" title="8:154	The technique was thoroughly evaluated on data from 17 language pairs using 6 machine translation evaluation metrics." ></td>
	<td class="line x" title="9:154	We found the results were corpus-dependent, but in most cases our system was able to improve translation performance, and for some languages the improvements were substantial." ></td>
	<td class="line x" title="10:154	1 Introduction Topic-dependent modeling has proven to be an effective way to improve quality the quality of models in speech recognition (Iyer and Osendorf, 1994; Carter, 1994)." ></td>
	<td class="line x" title="11:154	Recently, experiments in the field of machine translation (Hasan and Ney, 2005; Yamamoto and Sumita, 2007; Finch et al. 2007, Foster and Kuhn, 2007) have shown that classspecific models are also useful for translation." ></td>
	<td class="line x" title="12:154	In the method proposed by Yamamoto and Sumita (2007), topic dependency was implemented by partitioning the data into sets before the decoding process commenced, and subsequently decoding these sets independently using different models that were specific to the class predicted for the source sentence by a classifier that was run over the source sentences in a pre-processing pass." ></td>
	<td class="line x" title="13:154	Our approach is in many ways a generalization of this work." ></td>
	<td class="line x" title="14:154	Our technique allows the use of multiplemodel sets within the decoding process itself." ></td>
	<td class="line x" title="15:154	The contributions of each model set can be controlled dynamically during the decoding through a set of interpolation weights." ></td>
	<td class="line x" title="16:154	These weights can be changed on a sentence-by-sentence basis." ></td>
	<td class="line x" title="17:154	The previous approach is, in essence, the case where the interpolation weights are either 1 (indicating that the source sentence is the same topic as the model) or 0 (the source sentence is a different topic)." ></td>
	<td class="line x" title="18:154	One advantage of our proposed technique is that it is a soft approach." ></td>
	<td class="line x" title="19:154	That is, the source sentence can belong to multiple classes to varying degrees." ></td>
	<td class="line x" title="20:154	In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distancebased weights." ></td>
	<td class="line x" title="21:154	These probabilities were used directly as the mixture weights for the respective models in an interpolated model-set." ></td>
	<td class="line x" title="22:154	A second difference between our approach and that of Foster and Kuhn, is that we include a general model built from all of the data along with the set of classspecific models." ></td>
	<td class="line x" title="23:154	Our approach differs from all previous approaches in the models that are class-dependent." ></td>
	<td class="line x" title="24:154	Hasan and Ney (2005) used only a class-dependent language model." ></td>
	<td class="line x" title="25:154	Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model." ></td>
	<td class="line x" title="26:154	In our approach we combine all of the models, including the distortion and target length models, in the SMT system within  a single framework." ></td>
	<td class="line x" title="27:154	The contribution of this paper is two-fold." ></td>
	<td class="line x" title="28:154	The first is the proposal of a technique for combining Dynamic Model Interpolation for Statistical Machine Translation Andrew FINCH NICT  -ATR  Kyoto, Japan andrew.finch@atr.jp Eiichiro SUMITA NICT  -ATR   Kyoto, Japan eiichiro.sumita@atr.jp   National Institute for Science and Technology   Advanced Telecommunications Research Laboratories 208 multiple SMT systems in a weighted manner to allow probabilistic soft weighting between topicdependent models for all models in the system." ></td>
	<td class="line x" title="29:154	The second is the application of this technique to improve the quality of dialog systems by building and combing class-based models for interrogative and declarative sentences." ></td>
	<td class="line x" title="30:154	For the purposes of this paper, we wish to make the distinction between interrogative sentences and those which are not." ></td>
	<td class="line x" title="31:154	For the sake of simplicity of expression we will call those sentences which are interrogative, questions and those which are not, declarations for the remainder of this article." ></td>
	<td class="line x" title="32:154	The techniques proposed here were evaluated on a variety of different languages." ></td>
	<td class="line x" title="33:154	We enumerate them below as a key: Arabic (ar), Danish (da), German (de), English (en), Spanish (es), French (fr), Indonesian (Malay) (id), Italian (it), Japanese (ja), Korean (ko), Malaysian (Malay) (ms), Dutch (nl), Portugese (pt), Russian (ru), Thai (th), Vietnamese (vi) and Chinese (zh)." ></td>
	<td class="line x" title="34:154	2 System Overview 2.1 Experimental Data To evaluate the proposed technique, we conducted experiments on a travel conversation corpus." ></td>
	<td class="line x" title="35:154	The experimental corpus was the travel arrangement task of the BTEC corpus (Kikui et al., 2003) and used English as the target and each of the other languages as source languages." ></td>
	<td class="line x" title="36:154	The training, development, and evaluation corpus statistics are shown in Table 1." ></td>
	<td class="line x" title="37:154	The evaluation corpus had sixteen reference translations per sentence." ></td>
	<td class="line x" title="38:154	This training corpus was also used in the IWSLT06 Evaluation Campaign on Spoken Language Translation (Paul 2006) J-E open track, and the evaluation corpus was used as the IWSLT05 evaluation set." ></td>
	<td class="line x" title="39:154	2.2 System Architecture Figure 1 shows the overall structure of our system." ></td>
	<td class="line x" title="40:154	We used punctuation (a sentence-final ? character) on the target-side as the ground truth as to the class of the target sentence." ></td>
	<td class="line x" title="41:154	Neither punctuation nor case information was used for any other purpose in the experiments." ></td>
	<td class="line x" title="42:154	The data were partitioned into classes, and further sub-divided into training and development sets for each class." ></td>
	<td class="line x" title="43:154	1000 sentences were set aside as development data, and the remainder was used for training." ></td>
	<td class="line x" title="44:154	Three complete SMT  systems were built: one for each class, and one on the data from both classes." ></td>
	<td class="line x" title="45:154	A probabilistic classifier (described in the next section) was also trained from the full set of training data." ></td>
	<td class="line x" title="46:154	The machine translation decoder used is able to linearly interpolate all of the models models from Figure 1." ></td>
	<td class="line x" title="47:154	The architecture of the class-based SMT system used in our experiments Model interpolating decoder Labeled bilingual corpus All data bilingual declarations Probabilistic Classifier Question-specific SMT System General SMT System Declaration-specific SMT System DEVTRAIN bilingual questions DEVTRAIN DEV TRAIN Unlabeled test corpus General weight (fixed during decoding) Question weight (dynamic) Declaration weight (dynamic) sentence sentence sentence 209 all of the sub-systems according to a vector of interpolation weights supplied for each source word sequence to be decoded." ></td>
	<td class="line x" title="48:154	To do this, prior to the search, the decoder must first merge the phrasetables from each sub-system." ></td>
	<td class="line x" title="49:154	Every phrase from all of the phrase-tables is used during the decoding." ></td>
	<td class="line x" title="50:154	Phrases that occur in one sub-systems table, but do not occur in another sub-systems table will be used, but will receive no support (zero probability) from those sub-systems that did not acquire this phrase during training." ></td>
	<td class="line x" title="51:154	The search process proceeds as in a typical multi-stack phrase-based decoder." ></td>
	<td class="line x" title="52:154	The weight for the general model was set by tuning the parameter on the general development set in order to maximize performance in terms of BLEU score." ></td>
	<td class="line x" title="53:154	This weight determines the amount of probability mass to be assigned to the general model, and it  remains fixed during the decoding of all sentences." ></td>
	<td class="line x" title="54:154	The remainder of the probability mass is divided among the class-specific models dynamically sentence-by-sentence at run-time." ></td>
	<td class="line x" title="55:154	The proportion that is assigned to each class is simply the class membership probability of the source sequence assigned by the classifier." ></td>
	<td class="line x" title="56:154	3 Question Prediction 3.1 Outline of the Problem Given a source sentence of a particular class (interrogative or declarative in our case), we wish to ensure that the target sentence generated is of an appropriate class." ></td>
	<td class="line x" title="57:154	Note that this does not necessarily mean that given a question in the source, a question should be generated in the target." ></td>
	<td class="line x" title="58:154	However, it seems reasonable to assume that, intuitively at least, one should be able to generate a target question from a source question, and a target declaration from a source declaration." ></td>
	<td class="line x" title="59:154	This is reasonable because the role of a machine translation engine is not to be able to generate every possible translation from the source, but to be able to generate one acceptable translation." ></td>
	<td class="line x" title="60:154	This assumption leads us to two plausible ways to proceed." ></td>
	<td class="line x" title="61:154	1." ></td>
	<td class="line x" title="62:154	To predict the class of the source sentence, and use this to constrain the decoding process used to generate the target 2." ></td>
	<td class="line x" title="63:154	To predict the class of the target In our experiments, we chose the second method, as it seemed the most correct, but feel there is some merit in both strategies." ></td>
	<td class="line x" title="64:154	3.2 The Maximum Entropy Classifier We used a Maximum Entropy (ME) classifier to determine which class to which the input source sentence belongs using a set  of lexical features." ></td>
	<td class="line x" title="65:154	That is, we use the classifier to set  the mixture weights of the class-specific models." ></td>
	<td class="line x" title="66:154	In recent years such classifiers have produced powerful models utilizing large numbers of lexical features in a variety of natural language processing tasks, for example Rosenfeld (1996)." ></td>
	<td class="line x" title="67:154	An ME model is an exponential model with the following form: where: t is the class being predicted; c is the context of t;  is a normalization coefficient; K is the number of features in the model; k is the weight of feature fk; fk are binary feature functions;     p0 is the default model p(t, c)= K productdisplay k=0  f k (c,t) k p 0 Questions + Decls." ></td>
	<td class="line x" title="68:154	Questions Declarations Test Train Dev Train Dev Train Dev Sentences 161317 1000 69684 1000 90633 1000 510 Words 1001671 6112 445676 6547 549375 6185 3169 Table 1." ></td>
	<td class="line x" title="69:154	The corpus statistics of the target language corpus (en)." ></td>
	<td class="line x" title="70:154	The number of sentences is the same as these values for all source languaes." ></td>
	<td class="line x" title="71:154	The number of words in the source language differs, and depends on the segmentation granularity." ></td>
	<td class="line x" title="72:154	210 We used the set of all n-grams (n3) occurring in the source sentences as features to predict the sentences class." ></td>
	<td class="line x" title="73:154	Additionally we introduced beginning of sentence tokens (<s>) and end of sentence tokens into the word sequence to distinguish n-grams occurring at the start and end of sentences from those occurring within the sentence." ></td>
	<td class="line x" title="74:154	This was based on the observation that question words or words that indicate that the sentence is a question will frequently be found either at the start of the sentence (as in the wh<what, where, when> words in English or the -kah words in Malay <apakah, dimanakah, kapankah>), or at the end of the sentence (for example the Japanese ka or the Chinese ma)." ></td>
	<td class="line x" title="75:154	In fact, in earlier models we used features consisting of n-grams occurring only at the start and end of the source sentence." ></td>
	<td class="line x" title="76:154	These classifiers performed quite well (approximately 4% lower than the classifiers that used features from all of the n-grams in the source), but an error analysis showed that n-grams from the interior of the sentence were necessary to handle sentences such as excuse me please where is ." ></td>
	<td class="line x" title="77:154	A simple example sentence and the set of features generated from the sentence is shown in Figure 2." ></td>
	<td class="line x" title="78:154	We used the ME modeling toolkit of (Zhang, 2004) to implement our ME models." ></td>
	<td class="line x" title="79:154	The models were trained by using L-BFGS parameter estimation, and a Gaussian prior was used for smoothing during training." ></td>
	<td class="line x" title="80:154	3.3 Forcing the target to conform Before adopting the mixture-based approach set out in this paper, we first pursued an obvious and intuitively appealing way of using this classifier." ></td>
	<td class="line x" title="81:154	We applied it as a filter to the output of the decoder, to force source sentences that the classifier predicts should generate questions in the target to actually generate questions in the target." ></td>
	<td class="line x" title="82:154	This approach was unsuccessful due to a number of issues." ></td>
	<td class="line x" title="83:154	We took the n-best output from the decoder and selected the highest translation hypothesis on the list that had agreement on class according to source and target classifiers." ></td>
	<td class="line x" title="84:154	The issues we encountered included, too much similarity in the n-best hypotheses, errors of the MT system were correlated with errors of the classifier, and the number of cases that were corrected by the system was small <2%." ></td>
	<td class="line x" title="85:154	As a consequence, the method proposed in this paper was preferred." ></td>
	<td class="line x" title="86:154	4 Experiments 4.1 Experimental Conditions Decoder The decoder used to in the experiments, CleopATRa is an in-house phrase-based statistical decoder that can operate on the same principles as the PHARAOH (Koehn, 2004) and MOSES (Koehn et Source Language English Punctuation Own Punctuation ar 98.0 N/A da 97.3 98.0 de 98.1 98.6 en 98.9 98.9 es 96.3 96.7 fr 97.7 98.7 id 97.9 98.5 it 94.9 95.4 ja 94.1 N/A ko 94.2 99.4 ms 98.1 99.0 nl 98.1 99.0 pt 96.2 96.0 ru 95.9 96.6 th 98.2 N/A vi 97.7 98.0 zh 93.2 98.8 Table 2." ></td>
	<td class="line x" title="87:154	The classifcation accuracy (%) of the classifier used to predict whether or not an input sentence either is or should give rise to a question in the target." ></td>
	<td class="line x" title="88:154	<s> where is the <s> where is <s> where is the is the station </s> is the station </s> the station </s> Figure 2." ></td>
	<td class="line x" title="89:154	The set of n-gram (n3) features extracted from the sentence <s> where is the station </s> for use as predicates in the ME model to predict target sentence class." ></td>
	<td class="line x" title="90:154	211 al, 2007) decoders." ></td>
	<td class="line x" title="91:154	The decoder was configured to produce near-identical output to MOSES for these experiments." ></td>
	<td class="line x" title="92:154	The decoder was modified in order to handle multiple-sets of models, accept weighted input, and to incorporate the dynamic interpolation process during the decoding." ></td>
	<td class="line x" title="93:154	Practical Issues Perhaps the largest concerns about the proposed approach come from the heavy resource requirements that could potentially occur when dealing with large numbers of models." ></td>
	<td class="line o" title="94:154	However, one important characteristic of the decoder used in our experiments is its ability to leave its models on disk, loading only the parts of the models necesSource BLEU NIST WER PER GTM METEOR ar 0.4457 (0.00) 8.9386 (0.00) 0.4458 (0.00) 0.3742 (0.00) 0.7469 (0.00) 0.6766 (0.00) da 0.6640 (0.64) 11.4500 (1.64) 0.2560 (0.08) 0.2174 (2.42) 0.8338 (0.68) 0.8154 (1.23) de 0.6642 (0.79) 11.4107 (0.44) 0.2606 (2.18) 0.2105 (0.14) 0.8348 (-0.13) 0.8132 (-0.07) es 0.7345 (0.00) 12.1384 (0.00) 0.2117 (0.00) 0.1668 (0.00) 0.8519 (0.00) 0.8541 (0.00) fr 0.6666 (0.95) 11.7443 (0.63) 0.2548 (4.82) 0.2172 (6.50) 0.8408 (0.48) 0.8293 (1.29) id 0.5295 (9.56) 10.3459 (4.11) 0.3899 (21.17) 0.3239 (4.65) 0.7960 (1.35) 0.7521 (2.35) it 0.6702 (1.01) 11.5604 (0.41) 0.2590 (3.25) 0.2090 (0.62) 0.8351 (0.36) 0.8171 (0.05) ja 0.5971 (3.47) 10.6346 (2.56) 0.3779 (5.53) 0.2842 (2.80) 0.8125 (0.74) 0.7669 (0.67) ko 0.5898 (1.78) 10.2151 (1.31) 0.3891 (0.74) 0.3138 (-0.10) 0.7880 (0.36) 0.7397 (0.35) ms 0.5102 (10.19) 9.9775 (2.75) 0.4058 (18.53) 0.3355 (3.59) 0.7815 (0.18) 0.7247 (2.49) nl 0.6906 (2.55) 11.9092 (1.47) 0.2415 (3.21) 0.1872 (1.73) 0.8548 (0.39) 0.8399 (0.36) pt 0.6623 (0.35) 11.6913 (0.26) 0.2549 (2.52) 0.2110 (2.68) 0.8396 (0.02) 0.8265 (-0.07) ru 0.5877 (0.34) 10.1233 (-1.10) 0.3447 (1.99) 0.2928 (1.71) 0.7900 (0.15) 0.7537 (-0.40) th 0.4857 (1.50) 9.5901 (1.17) 0.4883 (-0.23) 0.3579 (2.03) 0.7608 (0.45) 0.7104 (1.23) vi 0.5118 (0.67) 9.8588 (1.85) 0.4274 (-0.05) 0.3301 (0.12) 0.7806 (1.05) 0.7254 (0.43) zh 0.5742 (0.00) 10.1263 (0.00) 0.3937 (0.00) 0.3172 (0.00) 0.7936 (0.00) 0.7343 (0.00) Table 3." ></td>
	<td class="line x" title="95:154	Performance results translating from a number of source languages into English." ></td>
	<td class="line x" title="96:154	Figures in parentheses are the percentage improvement in the score relative to the original score." ></td>
	<td class="line x" title="97:154	Bold-bordered cells indicate those conditions where performance degraded." ></td>
	<td class="line x" title="98:154	White cells indicate the proposed systems performance is significanly different from the baseline (using 2000-sample bootstrap resampling with a 95% confidence level)." ></td>
	<td class="line x" title="99:154	TER scores were not tested for significance due to technical difficulties." ></td>
	<td class="line x" title="100:154	ar, es and zh were also omitted since the systems were identical." ></td>
	<td class="line x" title="101:154	212 sary to decode the sentence in hand." ></td>
	<td class="line x" title="102:154	This reduced the memory overhead considerably when loading multiple models, without noticeably affecting decoding time." ></td>
	<td class="line x" title="103:154	Moreover, it is also possible to precompute the interpolated probabilities for most of the models for each sentence before the search commences, reducing both search memory and processing time." ></td>
	<td class="line x" title="104:154	Decoding Conditions For tuning of the decoder's parameters, minimum error training (Och 2003) with respect to the BLEU score using was conducted using the respective development corpus." ></td>
	<td class="line x" title="105:154	A 5-gram language model, built using the SRI language modeling toolkit (Stolcke, 1999) with Witten-Bell smoothing was used." ></td>
	<td class="line x" title="106:154	The model included a length model, and also the simple distance-based distortion model used by the PHARAOH decoder (Koehn, 2004)." ></td>
	<td class="line x" title="107:154	Source Baseline No Classifier Hard Proposed ar 0.4457 (0.00) 0.4457 (0.00) 0.4457 (0.00) 0.4457 da 0.6598 (0.64) 0.6647 (-0.11) 0.6591 (0.74) 0.664 de 0.6590 (0.79) 0.6651 (-0.14) 0.6634 (0.12) 0.6642 es 0.7345 (0.00) 0.7345 (0.00) 0.7345 (0.00) 0.7345 fr 0.6603 (0.95) 0.6594 (1.09) 0.6605 (0.92) 0.6666 id 0.4833 (9.56) 0.5029 (5.29) 0.5276 (0.36) 0.5295 it 0.6635 (1.01) 0.6660 (0.63) 0.6644 (0.87) 0.6702 ja 0.5771 (3.47) 0.5796 (3.02) 0.5667 (5.36) 0.5971 ko 0.5795 (1.78) 0.5837 (1.05) 0.5922 (-0.41) 0.5898 ms 0.4630 (10.19) 0.5015 (1.73) 0.5057 (0.89) 0.5102 nl 0.6734 (2.55) 0.6902 (0.06) 0.6879 (0.39) 0.6906 pt 0.6600 (0.35) 0.6643 (-0.30) 0.6598 (0.38) 0.6623 ru 0.5857 (0.34) 0.5885 (-0.14) 0.5844 (0.56) 0.5877 th 0.4785 (1.50) 0.4815 (0.87) 0.4831 (0.54) 0.4857 vi 0.5084 (0.67) 0.5095 (0.45) 0.5041 (1.53) 0.5118 zh 0.5742 (0.00) 0.5742 (0.00) 0.5742 (0.00) 0.5742 Table 4." ></td>
	<td class="line x" title="108:154	Performance results comaparing our proposes method with other techniques." ></td>
	<td class="line x" title="109:154	The column labeled Baseline is the same as in Table 3, for reference." ></td>
	<td class="line x" title="110:154	The column lableled No Classifier, is the same system as our proposed method, except that the classifier was replaced with a default model that assigned a class membership probability of 0.5 in every case." ></td>
	<td class="line x" title="111:154	The column labeled Hard corresponds to a system that used hard weights (either 1 or 0) for the class-dependent models." ></td>
	<td class="line x" title="112:154	The column labeled Proposed are the results from our proposed method." ></td>
	<td class="line x" title="113:154	Figures in parentheses represent the percentage improvement of the proposed methods score relative to the alternative method." ></td>
	<td class="line x" title="114:154	Cells with bold borders indicate those conditions where performance was degraded." ></td>
	<td class="line x" title="115:154	213 Tuning the interpolation weights The interpolation weights were tuned by maximizing the BLEU score on the development set over a set of weights ranging from 0 to 1 in increments of 0.1." ></td>
	<td class="line x" title="116:154	Figure 1 shows the behavior of two of our models with respect to their weight parameter." ></td>
	<td class="line o" title="117:154	Evaluation schemes To obtain a balanced view of the merits of our proposed approach, in our experiments we used 6 evaluation techniques to evaluate our systems." ></td>
	<td class="line oc" title="118:154	These were: BLEU (Papineni, 2001), NIST (Doddington, 2002), WER (Word Error Rate), PER (Position-independent WER), GTM (General Text Matcher), and METEOR (Banerjee and Lavie, 2005)." ></td>
	<td class="line x" title="119:154	4.2 Classification Accuracy The performance of the classifier (from 10-fold cross-validation on the training set) is shown in Table 2." ></td>
	<td class="line x" title="120:154	We give classification accuracy figures for predicting both source (same language) and target (English) punctuation." ></td>
	<td class="line x" title="121:154	Unsurprisingly, all systems were better at predicting their own punctuation." ></td>
	<td class="line x" title="122:154	The poorer scores in the table might reflect linguistic characteristics (perhaps questions in the source language are often expressed as statements in the target), or characteristics of the corpus itself." ></td>
	<td class="line x" title="123:154	For all languages the accuracy of the classifier seemed satisfactory, especially considering the possibility of inconsistencies in the corpus itself (and therefore our test data for this experiment)." ></td>
	<td class="line x" title="124:154	4.3 Translation Quality The performance of the SMT systems are shown in Table 3." ></td>
	<td class="line x" title="125:154	It is clear from the table that for most of the experimental conditions evaluated the system outperformed a baseline system that consisted of an SMT system trained on all of the data." ></td>
	<td class="line x" title="126:154	For those metrics in which performance degraded, in all-butone the results were statistically insignificant, and in all cases most of the other MT evaluation metrics showed an improvement." ></td>
	<td class="line x" title="127:154	Some of the language pairs showed striking improvements, in particular both of the Malay languages id and ms improved by over 3.5 BLEU points each using our technique." ></td>
	<td class="line x" title="128:154	Interestingly Dutch, a relative of Malay, also improved substantially." ></td>
	<td class="line x" title="129:154	This evidence points to a linguistic explanation for the gains." ></td>
	<td class="line x" title="130:154	Malay has very simple and regular question structure, the question words appear at the front of question sentences (in the same way as the target language) and do not take any other function in the language (unlike the English do for example)." ></td>
	<td class="line x" title="131:154	Perhaps this simplicity of expression allowed our class-specific models to model the data well in spite of the reduced data caused by dividing the data." ></td>
	<td class="line x" title="132:154	Another factor might be the performance of the classifier which was high for all these languages (around 98%)." ></td>
	<td class="line x" title="133:154	Unfortunately, it is hard to know the reasons behind the variety of scores in the table." ></td>
	<td class="line x" title="134:154	One large factor is likely to be differences in corpus quality, and also the relationship between the source and target corpus." ></td>
	<td class="line x" title="135:154	Some corpora are direct translations of each other, whereas others are translated through another language." ></td>
	<td class="line x" title="136:154	Chinese was one such language, and this may explain why we were unable to improve on the baseline for this language even though we were very successful for both Japanese and Thai, which are relatives of Chinese." ></td>
	<td class="line x" title="137:154	4.4  Comparison to Previous Methods We ran an experiment to compare our proposed method to an instance of our system that used hard weights." ></td>
	<td class="line x" title="138:154	The aim was to come as close as possible within our framework to the system proposed by Yamamoto and Sumita (2007)." ></td>
	<td class="line x" title="139:154	We used weights of 1 and 0, instead of the classification probabilities to weight the class-specific models." ></td>
	<td class="line x" title="140:154	To achieve this, we thresholded the probabilities from the classifier such that probabilities >0.5 gave a weight of 1, otherwise a weight of 0 was used." ></td>
	<td class="line x" title="141:154	The performance of this system is shown in Table 4 under the column heading Hard." ></td>
	<td class="line x" title="142:154	In all-but-one of the conFigure 3." ></td>
	<td class="line x" title="143:154	Graph showing the BLEU score on the developmment set plotted against the general models interpolation weight (a weight of 0 meaning no contribution from the general model) for two systems in our experiments." ></td>
	<td class="line x" title="144:154	0 0.2 0.4 0.6 0.8 1 Model interpolation weight 0.38 0.4 0.42 0.44 0.46 BLEU score zh id 214 ditions this system was outperformed by or equal to the proposed approach." ></td>
	<td class="line x" title="145:154	The column labeled No Classifier in Table 4 illustrates the effectiveness of the classifier in our system." ></td>
	<td class="line x" title="146:154	These results show the effect of using equal weights (0.5) to interpolate between the Question and Declaration models." ></td>
	<td class="line x" title="147:154	This system, although not as effective as the system with the classifier, gave a respectable performance." ></td>
	<td class="line x" title="148:154	5 Conclusion In this paper we have presented a technique for combining all models from multiple SMT  engines into a single decoding process." ></td>
	<td class="line x" title="149:154	This technique allows for topic-dependent decoding with probabilistic soft weighting between the component  models." ></td>
	<td class="line x" title="150:154	We demonstrated the effectiveness of our approach on conversational data by building class-specific models for interrogative and declarative sentence classes." ></td>
	<td class="line x" title="151:154	We carried out an extensive evaluation of the technique using a large number of language pairs and MT evaluation metrics." ></td>
	<td class="line x" title="152:154	In most cases we were able to show significant improvements over a system without model interpolation, and for some language pairs the approach excelled." ></td>
	<td class="line x" title="153:154	The best improvement of all the language pairs was for Malaysian (Malay)-English which outperformed the baseline system by 4.7 BLEU points (from 0.463 to 0.510)." ></td>
	<td class="line x" title="154:154	In future research we would like to try the approach with larger sets of models, and also (possibly overlapping) subsets of the data produced using automatic clustering methods." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W08-0913
Diagnosing Meaning Errors in Short Answers to Reading Comprehension Questions
Bailey, Stacey;Meurers, Detmar;"></td>
	<td class="line x" title="1:190	Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 107115, Columbus, Ohio, USA, June 2008." ></td>
	<td class="line x" title="2:190	c2008 Association for Computational Linguistics Diagnosingmeaningerrors in shortanswersto readingcomprehensionquestions StaceyBailey Departmentof Linguistics The Ohio State University 1712 Neil Avenue Columbus, Ohio 43210,USA s.bailey@ling.osu.edu DetmarMeurers Seminarfur Sprachwissenschaft Universitat Tubingen Wilhelmstrasse19 72074Tubingen,Germany dm@sfs.uni-tuebingen.de Abstract A common focus of systems in Intelligent Computer-Assisted Language Learning (ICALL) is to provide immediatefeedback to languagelearnersworkingon exercises." ></td>
	<td class="line x" title="3:190	Most ofthisresearchhasfocusedonprovidingfeedback on the form of the learnerinput." ></td>
	<td class="line x" title="4:190	Foreign languagepracticeand secondlanguageacquisitionresearch,on the other hand, emphasizes the importance of exercises that require the learnerto manipulatemeaning." ></td>
	<td class="line x" title="5:190	The ability of an ICALL system to diagnose and provide feedback on the meaning conveyed by a learner response depends on how well it can deal with the response variation allowed by an activity." ></td>
	<td class="line x" title="6:190	We focus on short-answerreadingcomprehensionquestions which have a clearly defined target response but the learner may convey the meaning of the target in multiple ways." ></td>
	<td class="line x" title="7:190	As empirical basis of our work, we collectedan English as a Second Language (ESL) learner corpus of short-answerreading comprehensionquestions, for which two graders provided target answers and correctness judgments." ></td>
	<td class="line x" title="8:190	On this basis, we developed a Content-Assessment Module (CAM), which performs shallow semanticanalysisto diagnosemeaningerrors." ></td>
	<td class="line x" title="9:190	It reachesan accuracy of 88%forsemanticerror detection and 87% on semantic error diagnosis on a held-outtest data set." ></td>
	<td class="line x" title="10:190	1 Introduction Languagepracticethat includesmeaningfulinteraction is a critical component of many current language teaching theories." ></td>
	<td class="line x" title="11:190	At the same time, existing researchon intelligentcomputer-aidedlanguage learning(ICALL)systemshas focusedprimarilyon providing practice with grammatical forms." ></td>
	<td class="line x" title="12:190	For mostICALLsystems,althoughformassessmentoften involves the use of natural language processing (NLP) techniques, the need for sophisticated content assessment of a learner response is limited by restricting the kinds of activities offered in order to tightly control the variation allowed in learner responses,i.e.,onlyoneorveryfewformscanbeused by the learner to express the correct content." ></td>
	<td class="line x" title="13:190	Yet many of the activities that language instructors typically use in real language-learningsettings support a significant degree of variation in correct answers and in turn require both form and content assessment for answer evaluation." ></td>
	<td class="line x" title="14:190	Thus, there is a real need for ICALL systems that provide accurate content assessment." ></td>
	<td class="line x" title="15:190	While some meaningful activities are too unrestrictedforICALLsystemstoprovideeffective content assessment, where the line should be drawn on a spectrum of language exercises is an open question." ></td>
	<td class="line x" title="16:190	Different language-learning exercises carry different expectations with respect to the level and type of linguistic variation possible across learner responses." ></td>
	<td class="line x" title="17:190	In turn, theseexpectationsmay be linked to the learning goals underlyingthe activity design, the cognitive skills required to respond to the activity, or other properties of the activity." ></td>
	<td class="line x" title="18:190	To develop adequate processingstrategies for content assessment, it is important to understand the connection between exercises and expected variation, as conceptualized by the exercise spectrum shown in Figure 1, because the level of variation imposes re107 Tightly Restricted Responses Loosely Restricted Responses Decontextualized grammar fill-inthe-blanks Short-answe r reading comprehension questions Essa ys on individualized topics The Middle Ground Viable Processing Ground Figure1: LanguageLearningExerciseSpectrum quirements and limitations on different processing strategies." ></td>
	<td class="line x" title="19:190	At oneextremeof the spectrum,thereare tightly restrictedexercises requiringminimalanalysis in order to assess content." ></td>
	<td class="line x" title="20:190	At the other extreme are unrestricted exercises requiring extensive form and contentanalysisto assess content." ></td>
	<td class="line x" title="21:190	In this work, we focus on determining whether shallow contentanalysis techniques can be used to perform content assessment for activities in the space between the extremes." ></td>
	<td class="line x" title="22:190	A good test case in this middle ground are loosely restricted reading comprehension (RC) questions." ></td>
	<td class="line x" title="23:190	From a teaching perspective, they are a task that is common in real-life learning situations, they combine elements of comprehension and production, and they are a meaningful activity suited to an ICALL setting." ></td>
	<td class="line x" title="24:190	From a processing perspective, responsesexhibitlinguisticvariationonlexical, morphological, syntactic and semantic levels  yet the intendedcontentsof the answeris predictableso that an instructorcan definetarget responses." ></td>
	<td class="line x" title="25:190	Since variation is possible across learner responses in activities in the middle ground of the spectrum,we propose a shallow content assessment approach which supports the comparison of target and learnerresponseson several levels includingtoken, chunk and relation." ></td>
	<td class="line x" title="26:190	We present an architecture for a content assessmentmodule(CAM)which providesthisflexibilityusingmultiplesurface-based matching strategies and existing language processing tools." ></td>
	<td class="line x" title="27:190	For an empiricalevaluation, we collected a corpus of language learner data consisting exclusively of responsesto short-answerreadingcomprehensionquestionsby intermediateEnglishlanguage learners." ></td>
	<td class="line x" title="28:190	2 TheData The learner corpus consists of 566 responses to short-answer comprehension questions." ></td>
	<td class="line x" title="29:190	The responses, written by intermediate ESL students as part of their regular homework assignments, were typically 1-3 sentences in length." ></td>
	<td class="line x" title="30:190	Students had access to their textbooks for all activities." ></td>
	<td class="line x" title="31:190	For developmentand testing,the corpuswas dividedinto two sets." ></td>
	<td class="line x" title="32:190	The development set contains 311 responses from 11 students answering 47 different questions; the test set contains255 responsesfrom 15 students to 28 questions." ></td>
	<td class="line x" title="33:190	The developmentand test sets were collected in two different classes of the same intermediatereading/writingcourse." ></td>
	<td class="line x" title="34:190	Two graders annotated the learner answers with a binary code for semantic correctness and one of several diagnosis codes to be discussed below." ></td>
	<td class="line x" title="35:190	Target responses (i.e., correct answers) and keywords from the target responses were also identified by the graders.1 Because we focus on content assessment, learner responses containing grammatical errors were only marked as incorrect if the grammatical errors impactedthe understandingof the meaning." ></td>
	<td class="line x" title="36:190	The graders did not agree on correctness judgments for 31 responses (12%) in the test set." ></td>
	<td class="line x" title="37:190	These wereeliminatedfromthe test set in orderto obtaina gold standardfor evaluation." ></td>
	<td class="line x" title="38:190	The remainingresponses in the development and test sets showed a rangeof variationfor many of the prompts." ></td>
	<td class="line x" title="39:190	As the following examplefrom the corpus illustrates, even straightforward questions based on 1Keywords refer to terms in the target response essential to a correctanswer." ></td>
	<td class="line x" title="40:190	108 an explicit short reading passage yield both linguistic and contentvariation: CUE: What are the methods of propaganda mentionedin the article?" ></td>
	<td class="line x" title="41:190	TARGET: The methods include use of labels, visual images, and beautiful or famous people promoting the idea or product." ></td>
	<td class="line x" title="42:190	Also used is linkingthe product toconceptsthatareadmiredordesiredandtocreate theimpressionthateveryonesupportstheproductor idea." ></td>
	<td class="line x" title="43:190	LEARNER RESPONSES:  A number of methods of propaganda are used in the media." ></td>
	<td class="line x" title="44:190	 Bositiveor negative labels." ></td>
	<td class="line x" title="45:190	 Giving positive or negative labels." ></td>
	<td class="line x" title="46:190	Using visualimages." ></td>
	<td class="line x" title="47:190	Havinga beautifulor famousperson to promote." ></td>
	<td class="line x" title="48:190	Creating the impression that everyonesupportsthe product or idea." ></td>
	<td class="line x" title="49:190	While the third answer was judged to be correct, the syntacticstructures,word order, forms,and lexical items used (e.g., famous person vs. famous people) vary from the string provided as target." ></td>
	<td class="line x" title="50:190	Of the learner responsesin the corpus, only one was string identical with the teacher-provided target and nine wereidenticalwhentreatedasbags-of-words." ></td>
	<td class="line x" title="51:190	Inthe test set, none of the learner responses was string or bag-of-word identical with the correspondingtarget sentence." ></td>
	<td class="line x" title="52:190	To classify the variation exhibited in learner responses, we developed an annotationscheme based on target modification, with the meaning error labels being adapted from those identified by James (1998) for grammatical mistakes." ></td>
	<td class="line x" title="53:190	Target modification encodes how the learner response varies from the target, but makes the sometimes incorrect assumption that the learner is actually trying to hit the meaning of the target." ></td>
	<td class="line x" title="54:190	The annotation scheme distinguishes correct answers, omissions (of relevant concepts), overinclusions (of incorrect concepts), blends (both omissions and overinclusions), and non-answers." ></td>
	<td class="line x" title="55:190	These error types are exemplified below with examples from the corpus." ></td>
	<td class="line x" title="56:190	In addition, the graders used the label alternate answer for responses that were correct given the question and reading passage, but that differed significantly in meaning from what was conveyed by the target answer.2 1." ></td>
	<td class="line x" title="57:190	Necessaryconceptsleftoutoflearnerresponse." ></td>
	<td class="line x" title="58:190	CUE: Name the features that are used in the designof advertisements." ></td>
	<td class="line x" title="59:190	TARGET: The features are eye contact, color, famous people, language and cultural references." ></td>
	<td class="line x" title="60:190	RESPONSE: Eye contact,color 2." ></td>
	<td class="line x" title="61:190	Responsewith extraneous,incorrect concepts." ></td>
	<td class="line x" title="62:190	CUE: Which form of programming on TV shows that highestlevel of violence?" ></td>
	<td class="line x" title="63:190	TARGET: Cartoonsshow the most violentacts." ></td>
	<td class="line x" title="64:190	RESPONSE: Television drama, childrens programs and cartoons." ></td>
	<td class="line x" title="65:190	3." ></td>
	<td class="line x" title="66:190	An incorrect blend/substitution (correct concept missing,incorrectone present)." ></td>
	<td class="line x" title="67:190	CUE: What is alliteration?" ></td>
	<td class="line x" title="68:190	TARGET: Alliteration is where sequential words begin with the same letter or sound." ></td>
	<td class="line x" title="69:190	RESPONSE: The worlds are often chosen to make some pattern or play on works." ></td>
	<td class="line x" title="70:190	Sequential worksbeginswiththe sameletteror sound." ></td>
	<td class="line x" title="71:190	4." ></td>
	<td class="line x" title="72:190	Multipleincorrectconcepts." ></td>
	<td class="line x" title="73:190	CUE: What was the major moral question raised by the Clintonincident?3 TARGET: The moral question raised by the Clinton incident was whether a politicians personal life is relevant to their job performance." ></td>
	<td class="line x" title="74:190	RESPONSE: The scandal was about the relationshipbetweenClintonand Lewinsky." ></td>
	<td class="line x" title="75:190	3 Method The CAM design integrates multiple matching strategies at different levels of representation and various abstractions from the surface form to compare meanings across a range of response variations." ></td>
	<td class="line x" title="76:190	Theapproachisrelatedtothemethodsusedin 2We use the term concept to refer to an entity or a relation between entities in a representation of the meaning of a sentence." ></td>
	<td class="line x" title="77:190	Thus,a responsegenerallycontainsmultipleconcepts." ></td>
	<td class="line x" title="78:190	3Note the incorrect presupposition in the cue provided by the instructor." ></td>
	<td class="line oc" title="79:190	109 machine translation evaluation (e.g., Banerjee and Lavie, 2005; Lin and Och, 2004),paraphraserecognition (e.g., Brockett and Dolan, 2005; Hatzivassiloglou et al., 1999), and automatic grading (e.g., Leacock,2004;Marn, 2004)." ></td>
	<td class="line x" title="80:190	To illustrate the general idea, consider the example from our corpusin Figure2." ></td>
	<td class="line x" title="81:190	Figure2: Basicmatchingexample Wefindonestringidenticalmatchbetweenthetoken was occurringin the target and the learnerresponse." ></td>
	<td class="line x" title="82:190	At the noun chunk level we can match home with his house." ></td>
	<td class="line x" title="83:190	And finally, after pronounresolutionit is possibleto matchBob Hope with he." ></td>
	<td class="line x" title="84:190	The overall architectureof CAMis shown in Figure 3." ></td>
	<td class="line x" title="85:190	Generally speaking, CAM compares the learner response to a stored target response and decides whetherthe two responsesare possiblydifferent realizations of the same semantic content." ></td>
	<td class="line x" title="86:190	The design relies on a series of increasingly complex comparisonmodulesto alignor matchcompatible concepts." ></td>
	<td class="line x" title="87:190	Aligned and unaligned concepts are used to diagnose content errors." ></td>
	<td class="line x" title="88:190	The CAM design supportsthe comparisonof target and learnerresponses on token, chunk and relation levels." ></td>
	<td class="line x" title="89:190	At the token level, the natureof the comparisonincludesabstractions of the stringto its lemma(i.e., uninflectedroot formof a word),semantictype(e.g.,date,location), synonyms, and a more general notion of similarity supportingcomparisonacrosspart-of-speech." ></td>
	<td class="line x" title="90:190	Thesystemtakesasinputthelearnerresponseand one or more target responses, along with the question and the source reading passage." ></td>
	<td class="line x" title="91:190	The comparison of the target and learnerinputpair proceedsfirst with an analysis filter, which determines whether linguistic analysis is required for diagnosis." ></td>
	<td class="line x" title="92:190	Essentially, thisfilteridentifieslearnerresponsesthatwere copieddirectlyfrom the sourcetext." ></td>
	<td class="line x" title="93:190	Then, for any learner-target response pair that requires linguistic analysis, CAM assessment proceeds in three phases  Annotation, Alignment and Diagnosis." ></td>
	<td class="line x" title="94:190	The Annotationphase uses NLP tools to enrich the learner and target responses, as well as the question text, with linguistic information, such as lemmas and part-of-speech tags." ></td>
	<td class="line x" title="95:190	The question text is used for pronoun resolution and to eliminate conceptsthat are given (cf.Halliday, 1967, p.204 and many others since)." ></td>
	<td class="line x" title="97:190	Here given information refers to conceptsfrom the questiontext that are reused in the learner response." ></td>
	<td class="line x" title="98:190	They may be necessary for forming complete sentences, but contribute no new information." ></td>
	<td class="line x" title="99:190	For example, if the questionis What is alliteration?" ></td>
	<td class="line x" title="100:190	and the response is Alliteration is the repetitionof initialletters or sounds, then the concept represented by the word alliteration is given and the rest is new." ></td>
	<td class="line x" title="101:190	For CAM, responses are neither penalizednor rewarded for containinggiven information." ></td>
	<td class="line x" title="102:190	Table 1 contains an overview of the annotations and the resources, tools or algorithms used." ></td>
	<td class="line x" title="103:190	The choiceoftheparticularalgorithmorimplementation wasprimarilybasedonavailabilityandperformance on our developmentcorpus otherimplementations could generallybe substituted without changingthe overall approach." ></td>
	<td class="line x" title="104:190	AnnotationTask LanguageProcessingTool SentenceDetection, MontyLingua(Liu,2004) Tokenization, Lemmatization Lemmatization PC-KIMMO(Antworth,1993) SpellChecking Editdistance(Levenshtein,1966), SCOWLwordlist(Atkinson,2004) Part-of-speechTagging TreeTagger(Schmid,1994) NounPhraseChunkingCASS(Abney,1997) LexicalRelations WordNet(Miller,1995) SimilarityScores PMI-IR(Turney,2001; Mihalceaetal.,2006) DependencyRelations StanfordParser (KleinandManning,2003) Table 1: NLP Tools used in CAM Afterthe Annotationphase,Alignmentmapsnew (i.e., not given) concepts in the learner response to concepts in the target response using the annotated information." ></td>
	<td class="line x" title="105:190	The final Diagnosis phase analyzes the alignment to determine whether the learner re110 Annotation Alignment Diagnosis Punctuation Input Learner Response Target Response(s) Question Output Source Text Activity Model Settings Sentence Detection Tokenization Lemmatization POS Tagging Chunking Dependency Parsing Spelling Correction Similarity Scoring Pronoun Resolution Type Recognition Analysis Filter Givenness Pre-Alignment Filters Token-level Alignment Chunk-level Alignment Relation-level Alignment Error Reporting Detection Classification Diagnosis Classification Figure3: Architectureof the ContentAssessmentModule(CAM) sponsecontainscontenterrors." ></td>
	<td class="line x" title="106:190	If multipletarget responses are supplied, then each is compared to the learner response and the target response with the most matches is selected as the model used in diagnosis." ></td>
	<td class="line x" title="107:190	The output is a diagnosis of the input pair, whichmightbe usedin a numberof waysto provide feedbackto the learner." ></td>
	<td class="line x" title="108:190	3.1 Combiningthe evidence To combine the evidence from these different levels of analysis for content evaluation and diagnosis, we tried two methods." ></td>
	<td class="line x" title="109:190	In the first, we handwrote rules and set thresholds to maximize performance on the developmentset." ></td>
	<td class="line x" title="110:190	On the development set, the hand-tuned method resulted in an accuracy of 81% for the semantic error detection task, a binary judgment task." ></td>
	<td class="line x" title="111:190	However, performance on the test set (which was collected in a later quarter with a different instructor and different students) made clearthattherulesandthresholdsthusobtainedwere overly specific to the development set, as accuracy dropped down to 63% on the test set." ></td>
	<td class="line x" title="112:190	The handwritten rules apparentlywere not general enough to transferwellfromthedevelopmentsettothetestset, i.e., they relied on propertiesof the developmentset thatwherenot sharedacrossdatasets." ></td>
	<td class="line x" title="113:190	Given the variety of features and the many different options for combiningand weighingthem that might have been explored, we decided that rather than hand-tuning therulesto additionaldata,wewouldtryto machine learn the best way of combining the evidence collected." ></td>
	<td class="line x" title="114:190	We thus decided to explore machine learning, even though the set of development data for trainingclearlyis very small." ></td>
	<td class="line x" title="115:190	Machine learning has been used for equivalence recognitionin relatedfields." ></td>
	<td class="line x" title="116:190	For instance,Hatzivassiloglou et al.(1999) trained a classifier for paraphrase detection, though their performance only reached roughly 37% recall and 61% precision." ></td>
	<td class="line x" title="118:190	In a different approach, Finch et al.(2005) found that MT evaluation techniques combined with machine learning improves equivalence recognition." ></td>
	<td class="line x" title="120:190	They usedtheoutputofseveralMTevaluationapproaches based on matching concepts (e.g., BLEU) as features/values for training a support vector machine (SVM)classifier." ></td>
	<td class="line x" title="121:190	Matchedconceptsand unmatched 111 conceptsalike were used as featuresfor trainingthe classifier." ></td>
	<td class="line x" title="122:190	Tested against the Microsoft Research Paraphrase(MSRP) Corpus, the SVM classifierobtained 75% accuracy on identifying paraphrases." ></td>
	<td class="line x" title="123:190	But it does not appear that machine learning techniqueshave so far been appliedto or even discussed inthecontextoflanguagelearnercorpora,wherethe availabledata sets typicallyare very small." ></td>
	<td class="line x" title="124:190	To begin to address the application of machine learning to meaning error diagnosis, the alignment data computedby CAMwas converted into features suitablefor machinelearning." ></td>
	<td class="line x" title="125:190	For example,the first feature calculated is the relative overlap of aligned keywords from the target response." ></td>
	<td class="line x" title="126:190	The full list of featuresare listed in Table 2." ></td>
	<td class="line x" title="127:190	Features Description 1.KeywordOverlap Percentofkeywordsaligned (relativetotarget) 2.TargetOverlap Percentofalignedtargettokens 3.LearnerOverlap Percentofalignedlearnertokens 4.T-Chunk Percentofalignedtargetchunks 5.L-Chunk Percentofalignedlearnerchunks 6.T-Triple Percentofalignedtargettriples 7.L-Triple Percentofalignedlearnertriples 8.TokenMatch Percentoftokenalignments thatweretoken-identical 9.SimilarityMatch Percentoftokenalignments thatweresimilarity-resolved 10.TypeMatch Percentoftokenalignments thatweretype-resolved 11.LemmaMatch Percentoftokenalignments thatwerelemma-resolved 12.SynonymMatch Percentoftokenalignments thatweresynonym-resolved 13.VarietyofMatch Numberofkindsoftoken-level (0-5) alignments Table 2: Featuresused for MachineLearning Features1-7reflectrelativenumbersofmatches(relative to length of either the target or learner response)." ></td>
	<td class="line x" title="128:190	Features2, 4, and 6 are relatedto the target responseoverlap." ></td>
	<td class="line x" title="129:190	Features3, 5, and 7 are related to overlap in the learner response." ></td>
	<td class="line x" title="130:190	Features 813 reflect the natureof the matches." ></td>
	<td class="line x" title="131:190	Thevaluesforthe13featuresinTable2wereused totrainthedetectionclassifier." ></td>
	<td class="line x" title="132:190	Fordiagnosis,afourteenthfeatureadetectionfeature(1or0depending onwhetherthedetectionclassifierdetectedan error)  was added to the developmentdata to train the diagnosisclassifier." ></td>
	<td class="line x" title="133:190	Given that token-level alignments are usedin identifyingchunk-and triple-level alignments,thatkindsof alignmentsarerelatedto variety of matches,etc., thereis clearredundancy and interdependence among features." ></td>
	<td class="line x" title="134:190	But each feature adds some new information to the overall diagnosis picture." ></td>
	<td class="line x" title="135:190	The machine learning suite used in all the developmentandtestingrunsisTiMBL(Daelemansetal., 2007)." ></td>
	<td class="line x" title="136:190	AswiththeNLPtoolsused,TiMBLwaschosenmainlytoillustratetheapproach." ></td>
	<td class="line x" title="137:190	Itwasnotevaluated against several learning algorithms to determine the best performingalgorithm for the task, althoughthisiscertainlyanavenueforfutureresearch." ></td>
	<td class="line x" title="138:190	In fact, TiMBL itself offers several algorithms and options for training and testing." ></td>
	<td class="line x" title="139:190	Experiments with these optionson the developmentset includedvaryinghow similaritybetweeninstanceswas measured, how importance (i.e., weight) was assigned to features and how many neighbors(i.e., instances)were examined in classifying new instances." ></td>
	<td class="line x" title="140:190	Given the very small development set available, making empirical tuning on the development set difficult, we decided to use the default learning algorithm (knearest neighbor) and majority voting based on the top-performingtraining runs for each available distance measure." ></td>
	<td class="line x" title="141:190	4 Results Turning to the results obtained by the machinelearning based CAM, for the binary semantic error detectiontask,thesystemobtainsanoverall87%accuracy on the developmentset (usingthe leave-oneout option of TiMBL to avoid training on the test item)." ></td>
	<td class="line x" title="142:190	Interestingly, even forthissmalldevelopment set, machinelearningthus outperformsthe accuracy obtained for the manual method of combining the evidence reported above." ></td>
	<td class="line x" title="143:190	On the test set, the final TiMBL-based CAM performance for detection improved slightly to 88% accuracy." ></td>
	<td class="line x" title="144:190	These results suggest that detection using the CAM design is viable, though more extensive testing with a larger corpus is needed." ></td>
	<td class="line x" title="145:190	Balancedsets Both the developmentand test sets contained a high proportion of correct answers  71% of the development set and 84% of the test set weremarkedas correctbythehumangraders." ></td>
	<td class="line x" title="146:190	Thus, 112 we also sampled a balanced set consisting of 50% correct and 50% incorrect answers by randomly including correct answers plus all the incorrect answers to obtain a set with 152 cases (development subset)and 72 (test subset)sentences." ></td>
	<td class="line x" title="147:190	The accuracy obtained for this balanced set was 78% (leave-oneout-testingwithdevelopmentset)and67%(testset)." ></td>
	<td class="line x" title="148:190	The fact that the results for the balanced developmentset usingleave-one-out-testingare comparable to the generalresultsshows thatthe machinelearner was not biased towards the ratio of correct and incorrect responses, even though there is a clear drop from developmentto test set, possiblyrelated to the small size of the data sets available for training and testing." ></td>
	<td class="line x" title="149:190	Alternate answers Another interesting aspect to discuss is the treatmentof alternateanswers." ></td>
	<td class="line x" title="150:190	Recall that alternate answers are those learner responses that are correct but significantlydissimilarfrom the given target." ></td>
	<td class="line x" title="151:190	Of the developmentset responsepairs, 15 were labeled as alternate answers." ></td>
	<td class="line x" title="152:190	One would expectthat given that theseresponsesviolatethe assumption that the learner is trying to hit the given target,usingtheseitemsintrainingwouldnegatively effect the results." ></td>
	<td class="line x" title="153:190	This turns out to be the case; performanceon the trainingset dropsslightlywhenthe alternateanswerpairs are included." ></td>
	<td class="line x" title="154:190	We thus did not include them in the development set used for training the classifier." ></td>
	<td class="line x" title="155:190	In other words, the diagnosisclassifier was trained to label the data with one of five codes  correct, omissions (of relevant concepts), overinclusions(of incorrect concepts), blends (both omissions and overinclusions), and non-answers." ></td>
	<td class="line x" title="156:190	Because it cannot be determined beforehand which itemsinunseendataarealternateanswerpairs,these pairs were not removed from the test set in the final evaluation." ></td>
	<td class="line x" title="157:190	Were these items eliminated, the detection performancewould improve slightlyto 89%." ></td>
	<td class="line x" title="158:190	Form errors Interestingly, the form errors frequently occurring in the student utterances did not negatively impact the CAM results." ></td>
	<td class="line x" title="159:190	On average, a learner response in the test set contained 2.7 form errors." ></td>
	<td class="line x" title="160:190	Yet, 68% of correctly diagnosed sentences included at least one form error, but only 53% of incorrectly diagnosed ones did so." ></td>
	<td class="line x" title="161:190	In other words, correct responses had more form errors than incorrect responses." ></td>
	<td class="line x" title="162:190	Looking at numbers and combinations of form errors, no clear pattern emerges that would suggest that form errors are linked to meaning errors in a clear way." ></td>
	<td class="line x" title="163:190	One conclusion to draw based on these data is that form and content assessment can be treated as distinct in the evaluation of learner responses." ></td>
	<td class="line x" title="164:190	Even in the presence of a range of form-basederrors,humangraderscan clearlyextract the intendedmeaningto be able to evaluate semanticcorrectness." ></td>
	<td class="line x" title="165:190	The CAM approachis similarly able to provide meaning evaluation in the presence of grammaticalerrors." ></td>
	<td class="line x" title="166:190	Diagnosis For diagnosis with five codes, CAM obtained overall 87% accuracy both on the developmentandon thetestset." ></td>
	<td class="line x" title="167:190	Given thatthenumberof labelsincreasesfrom2to5,theslightdropinoverall performancein diagnosisas comparedto the detection of semantic errors (from 88% to 87%) is both unsurprising in the decline and encouraging in the smallnessof thedecline." ></td>
	<td class="line x" title="168:190	However, given thesample sizeandfew numbersofinstancesofany givenerror in the test (and development)set, additionalquantitative analysis of the diagnosisresults would not be particularlymeaningful." ></td>
	<td class="line x" title="169:190	5 RelatedWork The need for semantic error diagnosis in previous CALL work has been limited by the narrow range of acceptable response variation in the supported language activity types." ></td>
	<td class="line x" title="170:190	The few ICALL systems that have been successfully integrated into real-life language teaching, such as German Tutor (Heift, 2001) and BANZAI (Nagata, 2002), also tightly control expected response variation through deliberate exercise type choices that limit acceptable responses." ></td>
	<td class="line x" title="171:190	Content assessment in the German Tutor is performed by string matching against the stored targets." ></td>
	<td class="line x" title="172:190	Because of the tightly controlled exercise types and lack of variation in the expected input, the assumption that any variation in a learner response is due to form error, rather than legitimate variation, is a reasonable one." ></td>
	<td class="line x" title="173:190	The recently developed TAGARELA system for learners of Portuguese(Amaraland Meurers, 2006; Amaral,2007) liftssomeof therestrictionsonexercisetypes,while relying on shallow semantic processing." ></td>
	<td class="line x" title="174:190	Using strategies inspired by our work, TAGARELA incorporatessimplecontentassessmentfor evaluating 113 learnerresponsesin short-answerquestions." ></td>
	<td class="line x" title="175:190	ICALL system designs that do incorporate more sophisticated content assessment include FreeText (LHaire and Faltin, 2003), the Military Language Tutor (MILT) Program (Kaplan et al., 1998), and Herr Kommissar (DeSmedt, 1995)." ></td>
	<td class="line x" title="176:190	These systems restrictboththeexercisetypesand domainsto make content assessment feasible using deeper semantic processingstrategies." ></td>
	<td class="line x" title="177:190	Beyond the ICALL domain, work in automatic grading of short answers and essays has addressed whether the students answers convey the correct meaning, but these systems focus on largely scoring rather than diagnosis (e.g., E-rater, Burstein and Chodorow, 1999), do not specifically address language learning contexts and/or are designed to work specifically with longer texts (e.g., AutoTutor, Wiemer-Hastingset al., 1999)." ></td>
	<td class="line x" title="178:190	Thus,the extent to which ICALL systems can diagnose meaning errors in languagelearnerresponseshas beenfar from clear." ></td>
	<td class="line x" title="179:190	As far as we are aware, no directly comparable systems performing content-assessment on related languagelearner data exist." ></td>
	<td class="line x" title="180:190	The closest related system that does a similar kind of detection is the Crater system (Leacock, 2004)." ></td>
	<td class="line x" title="181:190	That system obtains 85%accuracy." ></td>
	<td class="line x" title="182:190	However, thetestsetandscoringsystem were different, and the system was applied to responsesfromnative Englishspeakers." ></td>
	<td class="line x" title="183:190	In addition, theirwork focusedon detectionof errorsratherthan diagnosis." ></td>
	<td class="line x" title="184:190	So, the results are not directly comparable." ></td>
	<td class="line x" title="185:190	Nevertheless, theCAMdetectionresultsclearly are competitive." ></td>
	<td class="line x" title="186:190	6 Summary After motivating the need for content assessmentin ICALL,inthispaperwehavediscussedanapproach for content assessment of English language learner responses to short answer reading comprehension questions, which is worked out in detail in Bailey (2008)." ></td>
	<td class="line x" title="187:190	Wediscussedanarchitecturewhichrelieson shallow processingstrategies and achieves an accuracy approaching90%forcontenterrordetectionon alearnercorpuswecollectedfromlearnerscompleting the exercises assigned in a real-life ESL class." ></td>
	<td class="line x" title="188:190	Even for the small data sets available in the area of language learning, it turns out that machine learningcanbeeffectiveforcombiningtheevidencefrom variousshallowmatchingfeatures." ></td>
	<td class="line x" title="189:190	Thegoodperformance confirms the viability of using shallow NLP techniques for meaning error detection." ></td>
	<td class="line x" title="190:190	By developing and testing this model, we hope to contribute to bridging the gap between what is practical and feasible from a processing perspective and what is desirablefrom the perspective of current theoriesof languageinstruction." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1023
Feature-Rich Translation by Quasi-Synchronous Lattice Parsing
Gimpel, Kevin;Smith, Noah A.;"></td>
	<td class="line x" title="1:263	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219228, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:263	c 2009 ACL and AFNLP Feature-Rich Translation by Quasi-Synchronous Lattice Parsing Kevin Gimpel and Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We present a machine translation framework that can incorporate arbitrary features of both input and output sentences." ></td>
	<td class="line x" title="3:263	The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic." ></td>
	<td class="line x" title="4:263	Using generic approximate dynamic programming techniques, this decoder can handle non-local features." ></td>
	<td class="line x" title="5:263	Similar approximate inference techniques support efficient parameter estimation with hidden variables." ></td>
	<td class="line x" title="6:263	We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism." ></td>
	<td class="line x" title="7:263	1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001)." ></td>
	<td class="line x" title="8:263	Hence a tension is visible in the many recent research efforts aiming to decode with non-local features (Chiang, 2007; Huang and Chiang, 2007)." ></td>
	<td class="line x" title="9:263	Lopez (2009) recently argued for a separation between features/formalisms (and the indepen1Informally,featuresarepartsofaparallelsentencepair and/or their mutual derivation structure (trees, alignments, etc.)." ></td>
	<td class="line x" title="10:263	Features are often implied by a choice of formalism." ></td>
	<td class="line x" title="11:263	dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning." ></td>
	<td class="line x" title="12:263	Here we take first steps towardsuchauniversaldecoder, makingthefollowing contributions: Arbitraryfeaturemodel(2): Wedefineasingle, direct log-linear translation model (Papineni etal., 1997; OchandNey, 2002)thatencodesmost popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments." ></td>
	<td class="line x" title="13:263	The trees are optional and can be easily removed, allowing simulation of string-to-tree, tree-to-string, treeto-tree, and phrase-based models, among many others." ></td>
	<td class="line x" title="14:263	We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model." ></td>
	<td class="line x" title="15:263	Decoding as QG parsing (34): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inferencetechniquestoincorporatearbitrarynonlocal features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009)." ></td>
	<td class="line x" title="16:263	Parameter estimation (5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hiddenvariablestodiscriminativelyandefficiently train our model." ></td>
	<td class="line x" title="17:263	Because we start with inference (the key subroutine in training), many other learning algorithms are possible." ></td>
	<td class="line x" title="18:263	Experimental platform (6): The flexibility of our model/decoder permits carefully controlled experiments." ></td>
	<td class="line x" title="19:263	We compare lexical phrase and dependency syntax features, as well as a novel com2To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 , T source and target language vocabularies, respectively Trans :  {NULL}  2T function mapping each source word to target words to which it may translate s = s0,,sn  n source language sentence (s0 is the NULL word) t = t1,,tm  Tm target language sentence, translation of s s : {1,,n}  {0,,n} dependency tree of s, where s(i) is the index of the parent of si (0 is the root, $) t : {1,,m}  {0,,m} dependency tree of t, where t(i) is the index of the parent of ti (0 is the root, $) a : {1,,m}  2{1,,n} alignments from words in t to words in s;  denotes alignment to NULL  parameters of the model gtrans(s,a,t) lexical translation features (2.1): flex(s,t) word-to-word translation features for translating s as t fphr(sji,tlscriptk) phrase-to-phrase translation features for translating sji as tlscriptk glm(t) language model features (2.2): fN(tjjN+1) N-gram probabilities gsyn(t,t) target syntactic features (2.3): fatt(t,j,tprime,k) syntactic features for attaching target word tprime at position k to target word t at position j fval(t,j,I) syntactic valence features with word t at position j having children I  {1,,m} greor(s,s,a,t,t) reordering features (2.4): fdist(i,j) distortion features for a source word at position i aligned to a target word at position j gtree2(s,a,t) tree-to-tree syntactic features (3): fqg(i,iprime,j,k) configuration features for source pair si/siprime being aligned to target pair tj/tk gcov(a) coverage features (4.2) fscov(a), fzth(a), fsunc(a) counters for covering each s word each time, the zth time, and leaving it uncovered Table 1: Key notation." ></td>
	<td class="line x" title="20:263	Feature factorings are elaborated in Tab." ></td>
	<td class="line x" title="21:263	2." ></td>
	<td class="line x" title="22:263	bination of the two." ></td>
	<td class="line x" title="23:263	We quantify the effects of our approximate inference." ></td>
	<td class="line x" title="24:263	We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG." ></td>
	<td class="line x" title="25:263	We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research." ></td>
	<td class="line x" title="26:263	2 Model (Table 1 explains notation.)" ></td>
	<td class="line x" title="27:263	Given a sentence s and its parse tree s, we formulate the translation problem as finding the target sentence t (along with its parse tree t and alignment a to the source tree) such that3 t,t ,a = argmax t,t,a p(t,t,a | s,s) (1) Inordertoincludeoverlappingfeaturesandpermit hidden variables during training, we use a single globally-normalized conditional log-linear model." ></td>
	<td class="line x" title="28:263	That is, p(t,t,a | s,s) = exp{latticetopg(s,s,a,t,t)}summationtext aprime,tprime,primet exp{ latticetopg(s,s,aprime,tprime,prime t)} (2) where the g are arbitrary feature functions and the  are feature weights." ></td>
	<td class="line x" title="29:263	If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables." ></td>
	<td class="line x" title="30:263	In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3We assume in this work that s is parsed." ></td>
	<td class="line x" title="31:263	In principle, we might include source-side parsing as part of decoding." ></td>
	<td class="line x" title="32:263	on the feasibility of inference, including decoding." ></td>
	<td class="line x" title="33:263	Typicallythesefeaturefunctionsarechosentofactor into local parts of the overall structure." ></td>
	<td class="line x" title="34:263	We next define some key features used in current MT systems, explaining how they factor." ></td>
	<td class="line x" title="35:263	We will use subscripts on g to denote different groups of features, which may depend on subsets of the structures t, t, a, s, and s. When these features factor into parts, we will use f to denote the factored vectors, so that if x is an object that breaks into parts {xi}i, then g(x) =summationtexti f(xi).4 2.1 Lexical Translations Classical lexical translation features depend on s andtandthealignmentabetweenthem." ></td>
	<td class="line x" title="36:263	Thesimplest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s   and t  T. Phrase-to-phrase features generalize these, estimated as p(tprime | sprime) and p(sprime | tprime) where sprime (respectively, tprime) is a substring of s (t)." ></td>
	<td class="line x" title="37:263	A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences 4There are two conventional definitions of feature functions." ></td>
	<td class="line x" title="38:263	One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002)." ></td>
	<td class="line x" title="39:263	These estimates are usually heuristic and inconsistent (Koehn et al., 2003)." ></td>
	<td class="line x" title="40:263	An alternative is to instantiate features for different structural patterns(Liangetal.,2006; Blunsometal.,2008)." ></td>
	<td class="line x" title="41:263	Thisoffers more expressive power but may require much more training data to avoid overfitting." ></td>
	<td class="line x" title="42:263	For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice." ></td>
	<td class="line x" title="43:263	220 (Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007)." ></td>
	<td class="line x" title="44:263	Lexical translation features factor as in Eq." ></td>
	<td class="line x" title="45:263	3 (Tab." ></td>
	<td class="line x" title="46:263	2)." ></td>
	<td class="line x" title="47:263	We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in the target phrase; ifuniontextk:ikj a(k) = , no phrase feature fires for tji." ></td>
	<td class="line x" title="48:263	2.2 N-gram Language Model N-gram language models have become standard in machine translation systems." ></td>
	<td class="line x" title="49:263	For bigrams and trigrams (used in this paper), the factoring is in Eq." ></td>
	<td class="line x" title="50:263	4 (Tab." ></td>
	<td class="line x" title="51:263	2)." ></td>
	<td class="line x" title="52:263	2.3 Target Syntax There have been many features proposed that consider sourceand target-language syntax during translation." ></td>
	<td class="line x" title="53:263	Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible." ></td>
	<td class="line x" title="54:263	For example, Quirk et al.(2005) use features involving phrases and sourceside dependency trees and Mi et al.(2008) use features from a forest of parses of the source sentence." ></td>
	<td class="line x" title="57:263	There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008)." ></td>
	<td class="line x" title="58:263	In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008)." ></td>
	<td class="line x" title="59:263	In this work, we focus on syntactic features of target-side dependency trees, t, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features." ></td>
	<td class="line x" title="60:263	They factor as in Eq." ></td>
	<td class="line x" title="61:263	5 (Tab." ></td>
	<td class="line x" title="62:263	2)." ></td>
	<td class="line x" title="63:263	Features that consider only target-side syntax and words without considering s can be seen as syntactic language model features (Shen et al., 2008)." ></td>
	<td class="line x" title="64:263	5Segmentation might be modeled as a hidden variable in future work." ></td>
	<td class="line x" title="65:263	gtrans(s,a,t) = Pmj=1Pia(j) flex(si,tj) (3) +Pi,j:1i<jm fphr(slast(i,j)first(i,j),tji) glm(t) = PN{2,3}Pm+1j=1 fN(tjjN+1) (4) gsyn(t,t) = Pmj=1 fatt(tj,j,tt(j),t(j)) +fval(tj,j,1t (j)) (5) greor(s,s,a,t,t) = Pmj=1Pia(j) fdist(i,j) (6) gtree2(s,a,t) = mX j=1 fqg(a(j),a(t(j)),j,t(j)) (7) Table 2: Factoring of global feature collections g into f. xji denotes xi,xj in sequence x = x1,." ></td>
	<td class="line x" title="66:263	first(i,j) = mink:ikj (min(a(k))) and last(i,j) = maxk:ikj (max(a(k)))." ></td>
	<td class="line x" title="67:263	2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007)." ></td>
	<td class="line x" title="68:263	In syntax-based systems, reordering is typically parameterized by grammar rules." ></td>
	<td class="line x" title="69:263	For generality we permit these features to see all structures and denote them greor(s,s,a,t,t)." ></td>
	<td class="line x" title="70:263	Eq." ></td>
	<td class="line x" title="71:263	6 (Tab." ></td>
	<td class="line x" title="72:263	2) shows a factoring of reordering features based on absolute positions of aligned words." ></td>
	<td class="line x" title="73:263	We turn next to the backbone model for our decoder; the formalism and the properties of its decodingalgorithmwillinspiretwoadditionalsets of features." ></td>
	<td class="line x" title="74:263	3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t,t,a | s,s)." ></td>
	<td class="line x" title="75:263	Given a source sentence s and its parse s, a QDG induces a probabilistic monolingual dependency grammar over sentences inspired by the source sentence and tree." ></td>
	<td class="line x" title="76:263	We denote this grammar by Gs,s; its (weighted) language is the set of translations of s. Each word generated by Gs,s is annotated with a sense, which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in t and nodes in s. In principle, any portion of t may align to any portion of s, but in practice we often make restrictions on the alignments to simplify computation." ></td>
	<td class="line x" title="77:263	Smith and Eisner, for example, restricted |a(j)| for all words 221 tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 Which translations are possible depends heavily on the configurations that the QDG permits." ></td>
	<td class="line x" title="78:263	Formally, for a parent-child pair tt(j),tj in t, we consider the relationship betweena(t(j)) and a(j), the source-side words to which tt(j) and tj align." ></td>
	<td class="line x" title="79:263	If, for example, we require that, for all j, a(t(j)) = s(a(j)) or a(j) = 0, and that the root of t must align to the root of s or to NULL, then strict isomorphism must hold between s and t, and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005)." ></td>
	<td class="line x" title="80:263	Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment." ></td>
	<td class="line x" title="81:263	(a(t(j)) = s(a(j)) corresponds to their parent-child configuration; see Fig." ></td>
	<td class="line x" title="82:263	3 in Smith and Eisner (2006) for illustrations of the rest.)" ></td>
	<td class="line x" title="83:263	More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq." ></td>
	<td class="line x" title="84:263	7 (Tab." ></td>
	<td class="line x" title="85:263	2)." ></td>
	<td class="line x" title="86:263	Note that the QDG instantiates the model in Eq." ></td>
	<td class="line x" title="87:263	2." ></td>
	<td class="line x" title="88:263	Of the features discussed in 2, flex, fatt, fval, and fdist can be easily incorporated into the QDG as described while respecting the independence assumptions implied by the configuration features." ></td>
	<td class="line x" title="89:263	The others (fphr, f2, and f3) are nonlocal, or involve parts of the structure that, from the QDGs perspective, are conditionally independent given intervening material." ></td>
	<td class="line x" title="90:263	Note that nonlocality is relative to a choice of formalism; in 2 we did not commit to any formalism, so it is only now that we can describe phrase and N-gram features as non-local." ></td>
	<td class="line x" title="91:263	Non-local features will present a challenge for decoding and training (4.3)." ></td>
	<td class="line x" title="92:263	4 Decoding Given a sentence s and its parse s, at decoding time we seek the target sentence t, the target tree t , and the alignments a that are most probable, as defined in Eq." ></td>
	<td class="line x" title="93:263	1.7 (In 5 we will consider kbest and all-translations variations on this prob6I.e., from here on, a : {1,,m}  {0,,n} where 0 denotes alignment to NULL." ></td>
	<td class="line x" title="94:263	7Arguably, we seek argmax t p(t | s), marginalizing outeverything else." ></td>
	<td class="line x" title="95:263	Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work." ></td>
	<td class="line x" title="96:263	lem.)" ></td>
	<td class="line x" title="97:263	As usual, the normalization constant is not required for decoding; it suffices to solve: t,t ,a = argmax t,t,a latticetopg(s,s,a,t,t) (8) For a QDG model, the decoding problem has notbeenaddressedbefore." ></td>
	<td class="line x" title="98:263	Itequatestofindingthe most probable derivation under the s/s-specific grammar Gs,s. We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known." ></td>
	<td class="line x" title="99:263	The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widelyunderstoodinNLPandforwhichpractical, efficient, generic techniques exist." ></td>
	<td class="line x" title="100:263	A major advantage of DP is that, with small modifications, summing over structures is also possible with inside DP algorithms." ></td>
	<td class="line x" title="101:263	We will exploit this in training (5)." ></td>
	<td class="line x" title="102:263	Efficient summing opens up many possibilities for training , such as likelihood and pseudolikelihood, and provides principled ways to handle hidden variables during learning." ></td>
	<td class="line x" title="103:263	4.1 Translation as Monolingual Parsing We decode by performing lattice parsing on a lattice encoding the set of possible translations." ></td>
	<td class="line x" title="104:263	The lattice is a weighted sausage lattice that permits sentences up to some maximum length lscript; lscript is derived from the source sentence length." ></td>
	<td class="line x" title="105:263	Let the states be numbered 0 to lscript; states from floorleftlscriptfloorright to lscript are final states (for some   (0,1))." ></td>
	<td class="line x" title="106:263	For every position between consecutive states j  1 and j (0 < j  lscript), and for every word si in s, and for every word t  Trans(si), we instantiate an arc annotated with t and i. The weight of such an arc is exp{latticetopf}, where f is the sum of feature functions that fire when si translates as t in target position j (e.g., flex(si,t) and fdist(i,j))." ></td>
	<td class="line x" title="107:263	Given the lattice and Gs,s, lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms (Eisner, 1997)." ></td>
	<td class="line x" title="108:263	This decoder accounts for flex, fatt, fval, fdist, and fqg as local features." ></td>
	<td class="line x" title="109:263	Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice representing possible translations." ></td>
	<td class="line x" title="110:263	In each bundle, the arcs are listed in decreasing order according to weight and for clarity only the first five are shown." ></td>
	<td class="line x" title="111:263	The output of the decoder consists of lattice arcs 222 knnen:can knnen:may sie:you es:it  vorbei:by $   knnen   sie   es  vorbei    leifern    morgen  frh  ? can you deliver it by tomorrow morning ? can     you     deliver  it    by     tomorrow morning ? CAN     YOU  IT      BY     DELIVER  TOMORROW-MORNING  ?      knnen:can liefern:deliver sie:you sie:it es:it knnen:can knnen:can liefern:deliver sie:you es:it vorbei:by morgen:tomorrow morgen:tomorrow liefern:deliver es:it vorbei:by frh:morning  es:it morgen:tomorrow liefern:deliver vorbei:by frh:morning frh:early ?:?" ></td>
	<td class="line x" title="112:263	morgen:morning konnten:could konnten:could es:it sie:you konnten:might  konnten:couldn     sie:let sie:you sie:them es:it sie:you konnten:could bersetzen: translate bersetzen: translate bersetzen: translated bersetzen: translate bersetzen: translated ?:?" ></td>
	<td class="line x" title="113:263	konnten:could es:it es:it ?:?" ></td>
	<td class="line x" title="114:263	es:it ?:?" ></td>
	<td class="line x" title="115:263	NULL:to knnen:can knnen:may sie:you es:it  vorbei:by      knnen:can liefern:deliver sie:you sie:it es:it knnen:can knnen:can liefern:deliver sie:you es:it vorbei:by morgen:tomorrow morgen:tomorrow liefern:deliver es:it vorbei:by frh:morning frh:early ?:?" ></td>
	<td class="line x" title="116:263	morgen:morning  frh:morning morgen:tomorrow morgen:morning liefern:deliver vorbei:by $ $   knnen   sie   es  vorbei    leifern    morgen  frh  ? can     you     deliver  it    by     tomorrow morning ? $   knnen   sie   es  vorbei    leifern    morgen  frh  ? can     you     deliver  it    by     tomorrow morning ? Source:          $  konnten  sie  es  bersetzen  ? Reference:         could  you  translate  it  ? Decoder output: Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them." ></td>
	<td class="line x" title="117:263	selected at each position and a dependency tree over them." ></td>
	<td class="line x" title="118:263	4.2 Source-Side Coverage Features Most MT decoders enforce a notion of coverage of the source sentence during translation: all parts ofsshouldbealignedtosomepartoft(alignment to NULL incursanexplicitcost)." ></td>
	<td class="line x" title="119:263	Phrase-basedsystems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated." ></td>
	<td class="line x" title="120:263	Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in t (or a deliberate choice is made by the decoder to translate it to NULL)." ></td>
	<td class="line x" title="121:263	In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder." ></td>
	<td class="line x" title="122:263	Our QDG decoder has no way to enforce coverage; it does not track any kind of state in s apart from a single recently aligned word." ></td>
	<td class="line x" title="123:263	This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993)." ></td>
	<td class="line x" title="124:263	This sacrifice is the result of our choice to use a conditional model (2)." ></td>
	<td class="line x" title="125:263	The solution is to introduce a set of coverage features gcov(a)." ></td>
	<td class="line x" title="126:263	Here, these include:  A counter for the number of times each source word is covered: fscov(a) =summationtextni=1 |a1(i)|." ></td>
	<td class="line x" title="127:263	 Features that fire once when a source word is covered the zth time (z  {2,3,4}) and fire again all subsequent times it is covered; these are denoted f2nd, f3rd, and f4th." ></td>
	<td class="line x" title="128:263	 A counter of uncovered source words: fsunc(a) =summationtextni=1 (|a1(i)|,0)." ></td>
	<td class="line x" title="129:263	Of these, only fscov is local." ></td>
	<td class="line x" title="130:263	4.3 Non-Local Features The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them." ></td>
	<td class="line x" title="131:263	Phrase lexicon features fphr, language model features fN for N > 1, and most coverage features are non-local with respect to our QDG." ></td>
	<td class="line x" title="132:263	Recently Chiang (2007) introduced cube pruning as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits." ></td>
	<td class="line x" title="133:263	Techniques like cube pruning can be used to include the non-local features in our decoder.8 5 Training Training requires us to learn values for the parameters  in Eq." ></td>
	<td class="line x" title="134:263	2." ></td>
	<td class="line x" title="135:263	Given T training examples of the form t(i),(i)t ,s(i),(i)s , for i = 1,,T, maximum likelihood estimation for this model consists of solving Eq." ></td>
	<td class="line x" title="136:263	9 (Tab." ></td>
	<td class="line x" title="137:263	3).9 Note that the 8A full discussion is omitted for space, but in fact we use cube decoding, a slightly less approximate, slightly more expensive method that is more closely related to the approximate inference methods we use for training, discussed in 5." ></td>
	<td class="line x" title="138:263	9In practice, we regularize by including a term cbardblbardbl2 2." ></td>
	<td class="line x" title="139:263	223 LL() = TX i=1 logp(t(i),(i)t | s(i),(i)s ) = TX i=1 log P a exp{ latticetopg(s(i),(i)s ,a,t(i),(i) t )}P t,t,a exp{ latticetopg(s(i),(i)s ,a,t,t)} = TX i=1 log numeratordenominator (9) PL() = TX i=1 log X a p(t(i),a | (i)t ,s(i),(i)s )  + TX i=1 log X a p((i)t ,a | t(i),s(i),(i)s )  (10) denominator of term 1 in Eq." ></td>
	<td class="line x" title="140:263	10 = nX i=0 X tprimeTrans(si) S(1t (0),i,tprime)  exp n latticetop`flex(si,tprime) + fatt($,0,tprime,k) + fqg(0,i,0,k) o (11) S(j,i,t) = Y k1t (j) nX iprime=0 X tprimeTrans(siprime) S(k,iprime,tprime)  exp  latticetop  f lex(siprime,t prime) + f att(t,j,t prime,k)+ fval(t,j,1t (j)) + fqg(i,iprime,j,k) ff (12) S(j,i,t) = exp n latticetop`fval(t,j,1t (j)) o if 1t (j) =  (13) Table 3: Eq." ></td>
	<td class="line x" title="141:263	9: Log-likelihood." ></td>
	<td class="line x" title="142:263	Eq." ></td>
	<td class="line x" title="143:263	10: Pseudolikelihood." ></td>
	<td class="line x" title="144:263	In both cases we maximize w.r.t. ." ></td>
	<td class="line x" title="145:263	Eqs." ></td>
	<td class="line x" title="146:263	1113: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machinetranslationaswell(Blunsometal.,2008)." ></td>
	<td class="line x" title="147:263	Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA)." ></td>
	<td class="line x" title="148:263	This requires us to calculate the functions gradient (vector of first derivatives) with respect to .11 Computing the numerator in Eq." ></td>
	<td class="line x" title="149:263	9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast inside DP solution is known (Smith and Eisner, 2006; Wang et al., 2007)." ></td>
	<td class="line x" title="150:263	It runs in O(mn2) time and O(mn) space." ></td>
	<td class="line x" title="151:263	Computing the denominator in Eq." ></td>
	<td class="line x" title="152:263	9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences." ></td>
	<td class="line x" title="153:263	With a maximum length imposed, this is tractable using the inside version of the maximizing DP algorithm of Sec." ></td>
	<td class="line x" title="154:263	4, but it is prohibitively expensive." ></td>
	<td class="line x" title="155:263	We therefore optimize pseudo-likelihood instead, making the following approximation (Be10Alignments could be supplied by automatic word alignment algorithms." ></td>
	<td class="line x" title="156:263	We chose to leave them hidden so that we couldmake thebestuse ofour parsed trainingdata whenconfiguration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses." ></td>
	<td class="line x" title="157:263	11When the functions value is computed by inside DP, the corresponding outside algorithm can be used to obtain the gradient." ></td>
	<td class="line x" title="158:263	Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al.(2005)." ></td>
	<td class="line x" title="160:263	sag, 1975): p(t,t | s,s)  p(t | t,s,s)  p(t | t,s,s) Plugging this into Eq." ></td>
	<td class="line x" title="161:263	9, we arrive at Eq." ></td>
	<td class="line x" title="162:263	10 (Tab." ></td>
	<td class="line x" title="163:263	3)." ></td>
	<td class="line x" title="164:263	The two parenthesized terms in Eq." ></td>
	<td class="line x" title="165:263	10 each have their own numerators and denominators (not shown)." ></td>
	<td class="line x" title="166:263	The numerators are identical to each other and to that in Eq." ></td>
	<td class="line x" title="167:263	9." ></td>
	<td class="line x" title="168:263	The denominators are much more manageable than in Eq." ></td>
	<td class="line x" title="169:263	9, never requiring summation over more than two structures at a time." ></td>
	<td class="line x" title="170:263	We must sum over target word sequences and word alignments (with fixed t), and separately over target trees and word alignments (with fixed t)." ></td>
	<td class="line x" title="171:263	5.1 Summing over t and a The summation over target word sequences and alignments given fixed t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992)." ></td>
	<td class="line x" title="172:263	Let S(j,i,t) denote the sum of all translations rooted at position j in t such that a(j) = i and tj = t. Tab." ></td>
	<td class="line x" title="173:263	3 gives the equations for this DP: Eq." ></td>
	<td class="line x" title="174:263	11 is the quantity of interest, Eq." ></td>
	<td class="line x" title="175:263	12 is the recursion, and Eq." ></td>
	<td class="line x" title="176:263	13 shows the base cases for leaves of t. Letting q = max0in |Trans(si)|, this algorithm runs in O(mn2q2) time and O(mnq) space." ></td>
	<td class="line x" title="177:263	For efficiency we place a hard upper bound on q during training (details in 6)." ></td>
	<td class="line x" title="178:263	5.2 Summing over t and a For the summation over dependency trees and alignments given fixed t, required for p(t | t,s,s), we perform inside lattice parsing with Gs,s. The technique is the summing variant of the decoding method in 4, except for each state j, 224 the sausage lattice only includes arcs from j1 to j that are labeled with the known target word tj." ></td>
	<td class="line x" title="179:263	If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3) time and requires O(a2) space." ></td>
	<td class="line x" title="180:263	Because we use a hard upper bound on|Trans(s)|for all s  , this summation is much faster in practice than the one over words and alignments." ></td>
	<td class="line x" title="181:263	5.3 Handling Non-Local Features So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., fphr, fN for N > 1, fzth, fsunc)." ></td>
	<td class="line x" title="182:263	We recently proposed cube summing, an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009)." ></td>
	<td class="line x" title="183:263	Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007)that maintainsk-best lists of derivations for each DP chart item." ></td>
	<td class="line x" title="184:263	Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features." ></td>
	<td class="line x" title="185:263	Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudolikelihood, as well as to compute their approximate gradients." ></td>
	<td class="line x" title="186:263	Our approach permits an alternative to minimum error-rate training (MERT; Och, 2003); it is discriminativebuthandleslatentstructureandregularization in more principled ways." ></td>
	<td class="line x" title="187:263	The pseudolikelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGAs inner loop faster than MERTs inner loop." ></td>
	<td class="line x" title="188:263	6 Experiments Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output." ></td>
	<td class="line x" title="189:263	6.1 Data and Evaluation We use the German-English portion of the Basic Travel Expression Corpus (BTEC)." ></td>
	<td class="line x" title="190:263	The corpus has approximately 100K sentence pairs." ></td>
	<td class="line x" title="191:263	We filter sentences of length more than 15 words, which only removes 6% of the data." ></td>
	<td class="line x" title="192:263	We end up with a training set of 82,299 sentences, a development set of 934 sentences, and a test set of 500 sentences." ></td>
	<td class="line oc" title="193:263	We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching." ></td>
	<td class="line x" title="194:263	6.2 Features Our base system uses features as discussed in 2." ></td>
	<td class="line x" title="195:263	To obtain lexical translation features gtrans(s,a,t), we use the Moses pipeline (Koehn et al., 2007)." ></td>
	<td class="line x" title="196:263	We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the grow-diag-final-and heuristic, and extract phrases up to length 3." ></td>
	<td class="line x" title="197:263	We define flex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized alignments." ></td>
	<td class="line x" title="198:263	After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define fphr by 8 features: {2,3} target words  phrase conditional and lexical smoothing probabilities  two conditional directions." ></td>
	<td class="line x" title="199:263	Bigram and trigam language model features, f2 and f3, are estimated using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998)." ></td>
	<td class="line x" title="200:263	For our target-language syntactic features gsyn, we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004)." ></td>
	<td class="line x" title="201:263	These include probabilities associated with individual attachments (fatt) and child-generation valence probabilities (fval)." ></td>
	<td class="line x" title="202:263	These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003)." ></td>
	<td class="line x" title="203:263	The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003)." ></td>
	<td class="line x" title="204:263	In total, there are 7 lexical and 7 word-class syntax features." ></td>
	<td class="line x" title="205:263	For reordering, we use a single absolute distortionfeaturefdist(i,j)thatreturns|ij|whenever a(j) = i and i,j > 0." ></td>
	<td class="line x" title="206:263	(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.)" ></td>
	<td class="line x" title="207:263	The tree-to-tree syntactic features gtree2 in our model are binary features fqg that fire for particularQGconfigurations." ></td>
	<td class="line x" title="208:263	Weuseonefeatureforeach of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura225 Phrase Syntactic Features: features: +fatt fval +fqg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +fphr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU)." ></td>
	<td class="line x" title="209:263	tions involving root words and NULL-alignments more finely." ></td>
	<td class="line x" title="210:263	There are 14 features in this category." ></td>
	<td class="line x" title="211:263	Coverage features gcov are as described in 4.2." ></td>
	<td class="line x" title="212:263	In all, 46 feature weights are learned." ></td>
	<td class="line x" title="213:263	6.3 Experimental Procedure Our model permits training the system on the full set of parallel data, but we instead use the parallel data to estimate feature functions and learn  on the development set.12 We trained using three iterations of SGA over the development data with a batch size of 1 and a fixed step size of 0.01." ></td>
	<td class="line x" title="214:263	We used lscript2 regularization with a fixed, untuned coefficient of 0.1." ></td>
	<td class="line x" title="215:263	Cube summing used a 10-best list for training and a 7-best list for decoding unless otherwise specified." ></td>
	<td class="line x" title="216:263	To obtain the translation lexicon (Trans) we first included the top three target words t for each s using p(s | t)  p(t | s) to score target words." ></td>
	<td class="line x" title="217:263	For any training sentence s,t and tj for which tj negationslash uniontextni=1 Trans(si), we added tj to Trans(si) for i = argmaxiprimeI p(siprime|tj)  p(tj|siprime), where I = {i : 0  i  n  |Trans(si)| < qi}." ></td>
	<td class="line x" title="218:263	We used q0 = 10 and q>0 = 5, restricting |Trans(NULL)|  10 and |Trans(s)|  5 for any s  ." ></td>
	<td class="line x" title="219:263	This made 191 of the development sentences unreachable by the model, leaving 743 sentences for learning ." ></td>
	<td class="line x" title="220:263	During decoding, we generated lattices with all t  Trans(si) for 0  i  n, for every position." ></td>
	<td class="line x" title="221:263	We used  = 0.9, causing states within 90% of the source sentence length to be final states." ></td>
	<td class="line x" title="222:263	Between each pair of consecutive states, we pruned edges that fell outside a beam of 70% of the sum of edge weights (see 4.1; edge weights use flex, fdist, and fscov) of all edges between those two states." ></td>
	<td class="line x" title="223:263	6.4 Feature Set Comparison Our first set of experiments compares feature sets commonlyusedinphrase-andsyntax-basedtranslation." ></td>
	<td class="line x" title="224:263	In particular, we compare the effects of combining phrase features and syntactic features." ></td>
	<td class="line x" title="225:263	The base model contains flex, glm, greor, and 12We made this choice both for similarity to standard MT systems and a more rapid experiment cycle." ></td>
	<td class="line x" title="226:263	gcov." ></td>
	<td class="line x" title="227:263	The results are shown in Table 4." ></td>
	<td class="line x" title="228:263	The second row contains scores when adding in the eight fphr features." ></td>
	<td class="line x" title="229:263	The second column shows scores when adding the 14 target syntax features (fatt and fval), and the third column adds to them the 14 additional tree-to-tree features (fqg)." ></td>
	<td class="line x" title="230:263	We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality." ></td>
	<td class="line x" title="231:263	6.5 Varying k During Decoding For models without syntactic features, we constrained the decoder to produce dependency trees in which every words parent is immediately to its right and ignored syntactic features while scoring structures." ></td>
	<td class="line x" title="232:263	This causes decoding to proceed leftto-right in the lattice, the way phrase-based decoders operate." ></td>
	<td class="line x" title="233:263	Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice." ></td>
	<td class="line x" title="234:263	Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig." ></td>
	<td class="line x" title="235:263	2." ></td>
	<td class="line x" title="236:263	Scores improve when we increase k up to 10, but not much beyond, and there is still a substantial gap (2.5 BLEU) between using phrase features with k = 20 and using all features with k = 5." ></td>
	<td class="line x" title="237:263	Models without syntax perform poorly when using a very small k, due to their reliance on non-local language model and phrase features." ></td>
	<td class="line x" title="238:263	By contrast, models with syntactic features, which are local in our decoder, perform relatively well even with k = 1." ></td>
	<td class="line x" title="239:263	6.6 QG Configuration Comparison We next compare different constraints on isomorphism between the source and target dependency 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0 5 10 15 20 Value of k  for Decoding B L E U Phrase + Syntactic Phrase Syntactic Neither Figure 2: Comparison of size of k-best list for cube decoding with various feature sets." ></td>
	<td class="line o" title="240:263	226 QDG Configurations BLEU METEOR synchronous 0.4008 0.6949 + nulls, root-any 0.4108 0.6931 + child-parent, same node 0.4337 0.6815 + sibling 0.4881 0.7216 + grandparent/child 0.5015 0.7365 + c-command 0.5156 0.7441 + other 0.5142 0.7472 Table 5: QG configuration comparison." ></td>
	<td class="line x" title="241:263	The name of each configuration, following Smith and Eisner (2006), refers to the relationship between a(t(j)) and a(j) in s. trees." ></td>
	<td class="line x" title="242:263	To do this, we impose harsh penalties on some QDG configurations (3) by fixing their feature weights to 1000." ></td>
	<td class="line x" title="243:263	Hence they are permitted only when absolutely necessary in training and rarely in decoding.13 Each model uses all phrase and syntactic features; they differ only in the sets of configurations which have fixed negative weights." ></td>
	<td class="line x" title="244:263	Tab." ></td>
	<td class="line x" title="245:263	5 shows experimental results." ></td>
	<td class="line x" title="246:263	The base synchronous model permits parent-child (a(t(j)) = s(a(j))), any configuration where a(j) = 0, including both words being linked to NULL, andrequirestherootwordint tobelinked to the root word in s or to NULL(5 of our 14 configurations)." ></td>
	<td class="line x" title="247:263	The second row allows any configuration involving NULL, including those where tj aligns to a non-NULL word in s and its parent aligns to NULL, and allows the root in t to be linked to any word in s. Each subsequent row adds additional configurations (i.e., trains its  rather than fixing it to 1000)." ></td>
	<td class="line x" title="248:263	In general, we see large improvements as we permit more configurations, and the largest jump occurs when we add the sibling configuration (s(a(t(j))) = s(a(j)))." ></td>
	<td class="line o" title="249:263	The BLEU score does not increase, however, when we permit all configurations in the final row of the table, and the METEOR score increases only slightly." ></td>
	<td class="line x" title="250:263	While allowing certain categories of non-isomorphism clearly seems helpful, permitting arbitrary violations does not appear to be necessary for this dataset." ></td>
	<td class="line o" title="251:263	6.7 Discussion We note that these results are not state-of-theart on this dataset (on this task, Moses/MERT achieves0.6838BLEUand0.8523METEORwith maximum phrase length 3).14 Our aim has been to 13In fact, the strictest synchronous model used the almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized." ></td>
	<td class="line x" title="252:263	14Webelieveonecauseforthisperformancegapisthegeneration of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation." ></td>
	<td class="line x" title="253:263	illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints." ></td>
	<td class="line x" title="254:263	Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-totree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of nonisomorphism." ></td>
	<td class="line x" title="255:263	We have validated cube summing and decoding as practical methods for approximate inference." ></td>
	<td class="line x" title="256:263	Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Moses phrase segmentation variable), and, of course, additional feature representations." ></td>
	<td class="line x" title="257:263	The system is publicly available at www.ark.cs." ></td>
	<td class="line x" title="258:263	cmu.edu/Quipu." ></td>
	<td class="line x" title="259:263	7 Conclusion We presented feature-rich MT using a principled probabilistic framework that separates features from inference." ></td>
	<td class="line x" title="260:263	Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle non-local features using generic techniques that also support efficient parameter estimation." ></td>
	<td class="line x" title="261:263	Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints." ></td>
	<td class="line x" title="262:263	Acknowledgments We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper." ></td>
	<td class="line x" title="263:263	This research was supported by NSF IIS-0836431 and IIS-0844507, a grant from Google, and computational resources provided by Yahoo." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="D09-1117
Bidirectional Phrase-based Statistical Machine Translation
Finch, Andrew;Sumita, Eiichiro;"></td>
	<td class="line x" title="1:159	Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11241132, Singapore, 6-7 August 2009." ></td>
	<td class="line x" title="2:159	c 2009 ACL and AFNLP Bidirectional Phrase-based Statistical Machine Translation Andrew Finch NICT, Keihanna Science City, Kyoto, 619-0288, Japan andrew.finch@nict.go.jp Eiichiro Sumita NICT, Keihanna Science City, Kyoto, 619-0288, Japan eiichiro.sumita@nict.go.jp Abstract This paper investigates the effect of direction in phrase-based statistial machine translation decoding." ></td>
	<td class="line x" title="3:159	We compare a typical phrase-based machine translation decoder using a left-to-right decoding strategy to a right-to-left decoder." ></td>
	<td class="line x" title="4:159	We also investigate the effectiveness of a bidirectional decoding strategy that integrates both mono-directional approaches, with the aim of reducing the effects due to language specificity." ></td>
	<td class="line x" title="5:159	Our experimental evaluation was extensive, based on 272 different language pairs, and gave the surprising result that for most of the language pairs, it was better decode from right-to-left than from left-to-right." ></td>
	<td class="line x" title="6:159	As expected the relative performance of left-to-right and rightto-left strategies proved to be highly language dependent." ></td>
	<td class="line x" title="7:159	The bidirectional approach outperformed the both the left-toright strategy and the right-to-left strategy, showing consistent improvements that appeared to be unrelated to the specific languages used for translation." ></td>
	<td class="line x" title="8:159	Bidirectional decoding gave rise to an improvement in performance over a left-to-right decoding strategy in terms of the BLEU score in 99% of our experiments." ></td>
	<td class="line x" title="9:159	1 Introduction Human language production by its very nature is an ordered process." ></td>
	<td class="line x" title="10:159	That is to say, words are written/uttered in a sequence." ></td>
	<td class="line x" title="11:159	The current generation of phrase-based statistical machine translation (SMT) systems also generate their target word sequences according to an order." ></td>
	<td class="line x" title="12:159	Since the generation process is symmetrical, there are two possible strategies that could be used to generate the target: from beginning to end; or from end to beginning." ></td>
	<td class="line x" title="13:159	Generating the target in the wrong direction (the opposite direction to the way in which humans do) is counter intuitive, and possibly as a result of this, SMT systems typically generate the target word sequence in the same order as human language production." ></td>
	<td class="line x" title="14:159	However it is not necessarily the case that this is most effective strategy for all language pairs." ></td>
	<td class="line x" title="15:159	In this paper we investigate the effect of direction in phrase-based SMT decoding." ></td>
	<td class="line x" title="16:159	For the purposes of this paper, we will refer to target word sequence generation that follows the same order as human language production as forward generation, and generation in the opposite direction to human language production as reverse generation." ></td>
	<td class="line x" title="17:159	These are often referred left-toright and right-to-left respectively in the literature, but we avoid this notation as many languages are naturally written from right-to-left." ></td>
	<td class="line x" title="18:159	In earlier work (Watanabe and Sumita, 2002), it was hypothesized that the optimal direction for decoding was dependent on the characteristics of the target language." ></td>
	<td class="line x" title="19:159	Their results show that for Japanese to English translation a reverse decoding strategy was the most effective, whereas for English to Japanese translation, a forward decoding strategy proved superior." ></td>
	<td class="line x" title="20:159	In addition they implemented a bidirectional decoder, but their results were mixed." ></td>
	<td class="line x" title="21:159	For English to Japanese translation, decoding bidirectionally gives higher performance, but for Japanese to English translation they were unable to improve performance by decoding bidirectionally." ></td>
	<td class="line x" title="22:159	Their experiments were performed using a decoder based on IBM Model 4 using the translation techniques developed at IBM (Brown et al., 1993)." ></td>
	<td class="line x" title="23:159	This work is closely related to the techniques proposed in (Watanabe and Sumita, 2002), but in our case we decode within the framework of a phrase-based SMT system, rather than the IBM model." ></td>
	<td class="line x" title="24:159	Our intention was to explore the effect of direction in decoding within the context of a more 1124 contemporary machine translation paradigm, and to experiment with a broader range of languages." ></td>
	<td class="line x" title="25:159	The underlying motivation for our studies however remains the same." ></td>
	<td class="line x" title="26:159	Languages have considerably different structure, and certain grammatical constructs tend to occupy particular positions within sentences of the same language, but different positions across languages." ></td>
	<td class="line x" title="27:159	These differences may make it easier to tackle the automatic translation of a sentence in a given language from a particular direction." ></td>
	<td class="line x" title="28:159	Our approach differs in that the decoding process of a phrased-based decoder is quite different from that used by (Watanabe and Sumita, 2002) since decoding is done using larger units making the re-ordering process much simpler." ></td>
	<td class="line x" title="29:159	In (Watanabe and Sumita, 2002) only one language pair is considered, for our experiments we extended this to include translation among 17 different languages including the Japanese and English pair used in (Watanabe and Sumita, 2002)." ></td>
	<td class="line x" title="30:159	We felt that it was important to consider as many languages as possible in this study, as intuition and evidence from the original study suggests that the effect of direction in decoding is likely to be strongly language dependent." ></td>
	<td class="line x" title="31:159	The next section briefly describes the mechanisms underlying phrase-based decoding." ></td>
	<td class="line x" title="32:159	Then we explain the principles behind the forward, reverse and bidirectional decoding strategies used in our experiments." ></td>
	<td class="line x" title="33:159	Section 3 presents the experiments we performed." ></td>
	<td class="line x" title="34:159	Section 4 gives the results and some analysis." ></td>
	<td class="line x" title="35:159	Finally in Section 5, we conclude and offer possible directions for future research." ></td>
	<td class="line x" title="36:159	2 Phrase-based Translation For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al., 2007), integrating our models within a log-linear framework (Och and Ney, 2002)." ></td>
	<td class="line x" title="37:159	One of the advantages of a log-linear model is that it is possible to integrate a diverse set of features into the model." ></td>
	<td class="line x" title="38:159	For the decoders used in the experiments in this paper, we included the following feature functions:  An n-gram language model over the target word sequence Ensures the target word sequence is a likely sequence of words in the target language  A phrase translation model Effects the segmentation of the source word sequence, and is also responsible for the transformation of source phrases into target phrases." ></td>
	<td class="line x" title="39:159	 A target word sequence length model Controls the length of the target word sequence." ></td>
	<td class="line x" title="40:159	This is usually a constant term added for each word in the translation hypothesis." ></td>
	<td class="line x" title="41:159	 A lexicalized distortion model Influences the reordering of the translated source phrases in the target word sequence using lexical context on the boundaries of the phrases being reordered." ></td>
	<td class="line x" title="42:159	2.1 Decoding In a phrase-based SMT decoder, the word sequence of the target language is typically generated in order in a forward manner." ></td>
	<td class="line x" title="43:159	The words at the start of the translation are generated first, then the subsequent words, in order until the final word of the target word sequence is generated." ></td>
	<td class="line x" title="44:159	As the process is phrase-based, the translation is generated in a phrase-by-phrase manner, rather word-by-word." ></td>
	<td class="line x" title="45:159	The basic idea is to segment the source word sequence into subsequences (phrases), then translate each phrase individually, and finally compose the target word sequence by reordering the translations of the source phrases." ></td>
	<td class="line x" title="46:159	This composition must occur in a particular order, such that target words are generated sequentially from the start (or end in the case of reverse decoding) of the sentence." ></td>
	<td class="line x" title="47:159	The reason that the target needs to be generated sequentially is to allow an n-gram language model to be applied to the partial target word sequence at each step of the decoding process." ></td>
	<td class="line x" title="48:159	This process is illustrated in Figure 1." ></td>
	<td class="line x" title="49:159	In the decoding for both forward and reverse decoders the source sentence is segmented into 2 phrases: where is and the station (although in this example the segmentation is the same for both decoding strategies, it is not necessarily the case since the search processes are different)." ></td>
	<td class="line x" title="50:159	In the forward decoding process, first the English phrase the station is translated into the Japanese phrase eki wa." ></td>
	<td class="line x" title="51:159	Initially the target sequence consists 1125 Left to rightRight to left where is the station <s> <s> eki wa <s> eki wa doko </s> </s> doko </s> <s> eki wa doko </s> P(eki | <s> ) P(wa | eki, <s>) P(doko | wa, eki, <s>) P(</s> | doko, wa, eki, <s>) PLM = Generation P(doko | </s> ) P(wa | doko, </s>) P(eki | wa, doko, </s>) P(<s> | eki, wa, doko, </s>) PLM = where is the station Figure 1: The phrase-based decoding process for an English to Japanese translation, in both forward and reverse directions." ></td>
	<td class="line x" title="52:159	The n-gram language model probability calculation for the completed translation hypotheses are also shown on the bottom of the figure." ></td>
	<td class="line x" title="53:159	See Section 2.1 for a description of the decoding process." ></td>
	<td class="line x" title="54:159	of only the start of sentence marker s." ></td>
	<td class="line x" title="55:159	This marker only serves as context to indicate the start of the sequence for the benefit of the language model." ></td>
	<td class="line x" title="56:159	The first target phrase is separated into its component words and each word is added in order to the target word sequence." ></td>
	<td class="line x" title="57:159	Each addition causes an application of the language model, hence in Figure 1 the first term of PLM is P(eki|s), the second is P(wa|s) and so on." ></td>
	<td class="line x" title="58:159	For reverse decoding, the target sentence is generated starting from the end of sentence marker/swith the language model context being to the right of the current word." ></td>
	<td class="line x" title="59:159	For the case of bidirectional decoding, the model probability for the hypothesis is a linear interpolation of the scores for both forward and reverse hypotheses." ></td>
	<td class="line x" title="60:159	2.2 Direction in Decoding Direction in decoding influences both the models used by the decoder and the search process itself." ></td>
	<td class="line x" title="61:159	The direction of decoding determines the order in which target words are generated, the source phrases being translated in any order, therefore it is likely to be features of the target language rather than those of the the source language that determine the effect that the decoding direction has on decoder performance." ></td>
	<td class="line x" title="62:159	2.2.1 The Language Model The fundamental difference between the language models of a forward decoder and that of a reverse decoder is the direction in which the model looks for its context." ></td>
	<td class="line x" title="63:159	The forward model looks back to the start of the sentence, whereas the reverse model looks forward to the end of the sentence." ></td>
	<td class="line x" title="64:159	2.2.2 The Search Assuming a full search, a unigram language model and no limitations on reordering, the forward and reverse decoding processes are equivalent." ></td>
	<td class="line x" title="65:159	When these constraints are lifted, as is the case in the experiments in this paper, the two search processes diverge and can give rise to hypotheses that are different in character." ></td>
	<td class="line x" title="66:159	The partial hypotheses from early in the search process for forward decoding represent hypotheses for the first few words of the target word sequence, whereas the early partial hypotheses of a reverse decoder hold the last few words." ></td>
	<td class="line x" title="67:159	This has two consequences for the search." ></td>
	<td class="line x" title="68:159	The first is that (assuming a beam search as used in our experiments), certain candidate word sequences in the early stages of the search might be outside the beam and be pruned." ></td>
	<td class="line x" title="69:159	The consequence of this is that sentences that start with (or end with in the case of reverse decoding) the pruned word sequence will not be considered during the remainder of the search." ></td>
	<td class="line x" title="70:159	The second is that word se1126 quences in the partial hypotheses are used in the context of the models used in the subsequent decoding." ></td>
	<td class="line x" title="71:159	Thus, correctly decoding the start (or end for reverse decoding) of the sentence will benefit the subsequent decoding process." ></td>
	<td class="line x" title="72:159	3 Experiments 3.1 Experimental Data The experiments were conducted on all possible pairings among 17 languages." ></td>
	<td class="line x" title="73:159	A key to the acronyms used for languages together with information about their respective characteristics is given in Table 1." ></td>
	<td class="line x" title="74:159	We used all of the first ATR Basic Travel Expression Corpus (BTEC1) (Kikui et al., 2003) for these experiments." ></td>
	<td class="line x" title="75:159	This corpus contains the kind of expressions that one might expect to find in a phrase-book for travelers." ></td>
	<td class="line x" title="76:159	The corpus is similar in character to the IWSLT06 Evaluation Campaign on Spoken Language Translation (Paul, 2006) J-E open track." ></td>
	<td class="line x" title="77:159	The sentences are relatively short (see Table 1) with a simple structure and a fairly narrow range of vocabulary due to the limited domain." ></td>
	<td class="line x" title="78:159	The experiments were conducted on data that contained no case information, and also no punctuation (this was an arbitrary decision that we believe had no impact on the results)." ></td>
	<td class="line x" title="79:159	We used a 1000 sentence development corpus for all experiments, and the corpus used for evaluation consisted of 5000 sentences with a single reference for each sentence." ></td>
	<td class="line x" title="80:159	3.2 Training Each instance of the decoder is a standard phrasebased machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al., 2007) SMT decoders." ></td>
	<td class="line x" title="81:159	In these experiments 5-gram language models built with Witten-Bell smoothing were used along with a lexicalized distortion model." ></td>
	<td class="line x" title="82:159	The system was trained in a standard manner, using a minimum error-rate training (MERT) procedure (Och, 2003) with respect to the BLEU score (Papineni et al., 2001) on held-out development data to optimize the loglinear model weights." ></td>
	<td class="line x" title="83:159	For simplicity, the MERT procedure was performed on independently on the forward and reverse decoders for the bidirectional system, rather them attempting to tune the parameters for the full system." ></td>
	<td class="line x" title="84:159	3.3 Translation Engines 3.3.1 Forward The forward decoding translation systems used in these experiments represent the baseline of our experiments." ></td>
	<td class="line x" title="85:159	They consist of phrase-based, multistack, beam search decoders commonly used in the field." ></td>
	<td class="line x" title="86:159	3.3.2 Reverse The reverse decoding translation systems used in these experiments were exactly the same as the forward decoding systems." ></td>
	<td class="line x" title="87:159	The difference being the that word sequences in the training, development, and source side of the test corpora were reversed prior to training the systems." ></td>
	<td class="line x" title="88:159	The final output of the reverse decoders was reordered in a post processing step before evaluation." ></td>
	<td class="line x" title="89:159	3.3.3 Bidirectional The decoder used for the bidirectional decoding experiments was modified in order to be able to decode both forward and reverse in separate instances of the decoder." ></td>
	<td class="line x" title="90:159	Models for decoding in forward and reverse directions are loaded, and two decoding instances created." ></td>
	<td class="line x" title="91:159	Scores for hypotheses that share the same target word sequence from the two decoders were combined at the end of the decoding process linearly using equal interpolation weights." ></td>
	<td class="line x" title="92:159	Hypotheses that were generated by only one of the component decoders were not pruned." ></td>
	<td class="line x" title="93:159	The scores from these hypotheses only had a contribution from the decoder that was able to generate them, the contribution from the other decoder being zero." ></td>
	<td class="line x" title="94:159	3.4 Decoding Constraints The experiments reported in this paper were conducted with loose constraints on the decoding as overconstraining the decoding process could lead to differences between unidirectional and bidirectional strategies." ></td>
	<td class="line x" title="95:159	More specificially, the decoding was done with a beam width of 100, no beam thresholding and no constraints on the reordering process." ></td>
	<td class="line x" title="96:159	Figure 2 shows the effect of varying the beam width (stack size) in the search for forward decoder of the English to Japanese translation experiment." ></td>
	<td class="line x" title="97:159	At the beam width of 100 used in our experiments, the gains from doubling the beam with are small (0.07 BLEU percentage points)." ></td>
	<td class="line x" title="98:159	It is also important to note that a future cost identical to that used in the MOSES decoder 1127 Abbreviation Language #Words Avg." ></td>
	<td class="line x" title="99:159	sent length Vocabulary Order ar Arabic 806853 5.16 47093 SVO da Danish 806853 5.16 47093 SVO de German 907354 5.80 23443 SVO en English 970252 6.21 12900 SVO es Spanish 881709 5.64 18128 SVO fr French 983402 6.29 17311 SVO id Indonesian (Malay) 865572 5.54 15527 SVO it Italian 865572 5.54 15527 SVO ja Japanese 1149065 7.35 15405 SOV ko Korean 1091874 6.98 17015 SOV ms Malaysian (Malay) 873959 5.59 16182 SVO nl Dutch 927861 5.94 19775 SVO pt Portuguese 881428 5.64 18217 SVO ru Russian 781848 5.00 32199 SVO th Thai 1211690 7.75 6921 SVO vi Vietnamese 1223341 7.83 8055 SVO zh Chinese 873375 5.59 14854 SVO Table 1: Key to the languages, corpus statistics and word order." ></td>
	<td class="line x" title="100:159	SVO denotes a language that predominantly has subject-verb-object order, and SOV denotes a language that predominantly has subject-objectverb order Stack size BLEU Score 1 0.3954 2 0.4032 4 0.4075 8 0.4115 16 0.4149 32 0.4161 64 0.4181 128 0.4188 256 0.4197 512 0.4197 1024 0.4197 0.39 0.398 0.406 0.414 0.422 0 256 512 768 1024 BLEU Scor e Stack size Figure 2: The performance of a forward decoder (En-Ja) with increasing stack size." ></td>
	<td class="line x" title="101:159	(Koehn et al., 2007) was also included in the scores for partial hypothesis during the decoding." ></td>
	<td class="line x" title="102:159	3.5 Computational Overhead In the current implementation, bidirectional decoding takes twice as long as a mono-directional system." ></td>
	<td class="line x" title="103:159	However, in a multi-threaded environment, each instance of the decoder is able to run on its own thread in parallel, and so this slowdown can be mitigated in some circumstances." ></td>
	<td class="line x" title="104:159	Future generations of the bidirectional decoder will more tightly couple the two decoders, and we believe this will lead to faster and more effective search." ></td>
	<td class="line x" title="105:159	3.6 Evaluation The results presented in this paper are given in terms of the BLEU score (Papineni et al., 2001)." ></td>
	<td class="line x" title="106:159	This metric measures the geometric mean of ngram precision of n-grams drawn from the output translation and a set of reference translations for that translation." ></td>
	<td class="line x" title="107:159	There are large number of proposed methods for carrying out machine translation evaluation." ></td>
	<td class="line x" title="108:159	Methods differ in their focus of characteristics of the translation (for example fluency or adequacy), and moreover anomolous results can occur if a single metric is relied on." ></td>
	<td class="line oc" title="109:159	Therefore, we also carried out evaluations using the NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), WER (Hunt, 1989), PER (Tillmann et al., 1997) and TER (Snover et al., 2005) machine translation evaluation techniques." ></td>
	<td class="line x" title="110:159	4 Results The results of the experiments in terms of the BLEU score are given in Tables ??, 5, 3 and 3." ></td>
	<td class="line x" title="111:159	These results show the performance of the reverse and bidirectional decoding strategies relative to the usual forward decoding strategy." ></td>
	<td class="line x" title="112:159	The cells in the tables that represent experiments in which 1128 ar da de en es fr id it ja ko ms nl pt ru th vi zh ar 47.8 48.8 51.7 48.8 47.3 46.5 49.2 29.8 27.8 46.9 49.0 49.0 47.8 39.7 43.0 27.8 da 58.3 58.7 63.0 58.6 55.7 53.5 58.5 37.5 35.1 54.4 59.6 59.0 55.4 48.1 51.7 35.2 de 53.8 55.5 59.4 55.9 51.9 50.3 55.3 34.2 32.0 50.8 57.0 55.9 51.2 45.7 48.9 32.7 en 63.6 65.8 64.8 67.0 61.0 58.4 65.8 41.1 38.7 59.1 67.6 66.7 58.7 52.8 57.7 38.6 es 57.6 58.2 58.0 65.6 56.6 54.2 61.1 38.3 36.4 54.3 59.6 62.6 55.1 47.6 51.3 36.0 fr 57.8 58.3 58.0 62.3 58.9 52.7 57.4 39.1 37.7 53.8 58.3 57.9 54.8 47.7 50.4 37.6 id 54.7 52.8 52.8 56.6 53.7 51.0 53.1 37.2 35.6 86.4 53.8 53.0 51.3 46.4 48.4 34.9 it 54.1 53.4 54.4 59.4 56.4 51.8 49.2 34.4 32.8 49.9 55.1 56.2 50.5 44.0 47.0 33.6 ja 38.2 39.2 38.6 41.9 39.9 40.2 40.7 39.5 69.4 40.4 39.5 39.7 37.8 37.3 37.2 52.1 ko 34.4 35.3 34.6 38.2 36.3 36.2 36.8 35.6 66.4 36.6 35.6 36.3 34.5 34.2 34.1 46.4 ms 54.5 52.7 52.6 56.2 53.4 50.6 82.5 53.2 36.8 34.9 53.6 53.4 51.3 46.7 49.2 34.8 nl 55.1 57.3 58.8 63.2 58.5 54.5 52.4 57.1 36.7 34.1 53.4 58.3 53.5 48.7 50.7 35.2 pt 56.8 57.7 57.6 63.8 62.0 55.5 52.7 59.7 37.8 36.4 53.4 58.7 54.2 47.1 50.6 35.8 ru 51.4 49.1 50.2 53.3 52.0 48.7 48.6 51.6 31.9 29.5 49.1 50.9 50.5 41.8 43.7 30.0 th 53.8 55.0 54.8 58.2 55.8 53.3 55.0 54.8 41.4 39.2 55.4 55.9 55.5 53.0 56.0 40.4 vi 53.6 53.6 54.2 57.4 54.2 51.4 52.3 53.3 37.6 35.8 53.3 54.6 54.4 51.7 50.3 36.2 zh 32.0 33.0 32.6 34.6 33.2 33.7 34.2 33.2 47.8 43.5 33.9 33.4 32.6 32.2 31.1 29.7 Table 2: Baseline BLEU scores for all systems." ></td>
	<td class="line x" title="113:159	The figures represent the scores in BLEU percentage points of the baseline left-to-right decoding systems." ></td>
	<td class="line x" title="114:159	Source languages are indicated by the column headers, the row headers denoting the target languages." ></td>
	<td class="line x" title="115:159	the forward strategy outperformed the contrasting strategy are shaded in gray." ></td>
	<td class="line x" title="116:159	The numbers in the cells represent the difference in BLEU percentage points for the systems being compared in that cell." ></td>
	<td class="line o" title="117:159	It is clear from Table 3 that for most of the language pairs (67% of them for BLEU, and a similar percentage for all the other metrics except METEOR), better evaluation scores were achieved by using a reverse decoding strategy than a forward strategy." ></td>
	<td class="line x" title="118:159	This is a surprising result because language is produced naturally in a forward manner (by definition), and therefore one might expect this to also be the optimal direction for word sequence generation in decoding." ></td>
	<td class="line x" title="119:159	4.1 Word Order Typography Following (Watanabe and Sumita, 2002), to explain the effects we observe in our results we look to the word order typography of the target language (Comrie and Vogel, 2000)." ></td>
	<td class="line x" title="120:159	The word order of a language is defined in terms of the order in which you would expect to encounter the finite verb (V) and its arguments, subject (S) and object (O)." ></td>
	<td class="line x" title="121:159	In most languages S precedes O and V. Whether or not O precedes or follows V defines the two most prevalent word order types SOV and SVO (Comrie and Vogel, 2000)." ></td>
	<td class="line x" title="122:159	Two of the target languages in this study (Japanese and Korean) have the SOV word type, the remainder having the SVO word order type." ></td>
	<td class="line x" title="123:159	In Table 3 looking at the rows for ja and ko we can see that for both of these languages reverse decoding outperformed forward decoding in only 4 out of 12 experiments." ></td>
	<td class="line x" title="124:159	Furthermore these two languages were the two languages that benefited the most (in terms of the number of experimental cases) from forward decoding." ></td>
	<td class="line x" title="125:159	The two languages also agree on the best decoding direction for 12 of the 16 language pairs." ></td>
	<td class="line x" title="126:159	This apparent correlation may reflect similarities between the two languages (word order type, or other common features of the languages)." ></td>
	<td class="line x" title="127:159	Given this evidence, it seems plausible that word order does account in part for the differences in performance when decoding in differing directions, but this can only be part of the explanation since there are 4 source languages for which reverse decoding yielded higher performance." ></td>
	<td class="line x" title="128:159	It should be noted that our results differ from those of (Watanabe and Sumita, 2002) for English to Japanese translation, who observed gains when decoding in the reverse direction for this language pair." ></td>
	<td class="line x" title="129:159	It is hard to compare our results directly with theirs however, due to the differences in the decoders used in the experiments (ours being phrase-based, and theirs based on the IBM ap1129 ar da de en es fr id it ja ko ms nl pt ru th vi zh ar 0.87 0.34 1.30 0.93 1.63 0.66 0.58 0.12 0.36 0.85 0.33 0.88 0.22 1.33 1.04 0.88 da 0.25 0.41 0.71 0.56 0.70 1.10 0.31 0.46 0.07 0.96 0.13 0.62 0.17 1.28 0.71 0.29 de 0.41 0.04 0.38 0.52 0.15 0.80 0.01 0.47 0.72 0.60 0.25 0.21 0.05 0.47 0.68 0.20 en 0.04 0.05 0.21 0.05 0.13 0.58 0.02 0.73 0.35 0.39 0.07 0.52 0.05 0.67 0.63 0.29 es 0.14 0.19 0.05 0.35 0.68 0.01 0.08 0.25 0.31 0.25 0.25 0.17 0.07 0.43 0.44 0.78 fr 0.37 0.57 0.38 0.66 0.21 0.36 0.28 0.15 0.45 0.22 0.46 0.64 0.10 0.25 0.58 0.31 id 0.16 0.02 0.31 1.45 0.58 0.50 0.34 0.03 0.27 0.00 0.42 0.57 0.36 0.53 1.04 0.59 it 0.28 0.72 0.36 0.27 0.08 0.30 0.11 0.07 0.12 0.37 0.23 0.05 0.37 0.04 0.63 0.37 ja 0.36 0.22 0.03 0.03 0.22 0.13 0.64 0.36 0.21 0.57 0.46 0.08 0.33 0.08 0.83 0.70 ko 0.35 0.01 0.31 0.03 0.12 0.07 0.13 0.21 0.42 0.29 0.07 0.42 0.40 0.44 0.62 0.05 ms 0.06 0.49 0.53 1.38 0.99 0.71 0.47 0.34 0.11 0.32 0.62 0.27 0.10 0.83 0.99 0.11 nl 0.26 0.03 0.26 0.30 0.20 0.19 0.47 0.23 0.13 0.06 0.06 0.08 0.09 0.06 1.00 0.15 pt 0.03 0.34 0.06 0.51 0.07 0.17 0.06 0.18 0.13 0.65 0.08 0.10 0.06 0.09 0.85 0.35 ru 0.25 0.58 0.67 0.74 0.01 0.48 0.50 0.27 0.41 0.38 0.13 0.38 0.46 0.88 0.56 0.49 th 0.19 0.28 0.21 0.41 0.05 0.23 0.30 0.00 0.34 0.04 0.25 0.07 0.21 0.08 0.46 0.25 vi 0.21 0.34 0.24 0.65 0.72 0.34 0.06 0.59 0.24 0.22 0.19 0.12 0.11 0.18 0.63 0.15 zh 0.43 0.26 0.42 0.05 0.15 0.31 0.16 0.28 0.00 0.31 0.40 0.14 0.67 0.18 0.39 0.21 Table 3: Gains in BLEU score from reverse decoding over a forward decoding strategy The numbers in the cells are the differences in BLEU percentage points between the systems." ></td>
	<td class="line x" title="130:159	Shaded cells indicate the cases where forward decoding give a higher score." ></td>
	<td class="line x" title="131:159	Source languages are indicated by the column headers, the row headers denoting the target languages." ></td>
	<td class="line o" title="132:159	Metric Bi>For Bi>Rev Rev>For BLEU 98.90 84.93 67.65 NIST 98.53 78.31 75.00 METEOR 99.63 95.96 50.74 WER 99.26 92.85 66.18 PER 98.53 84.97 70.59 TER 99.63 91.18 68.75 Table 4: Summary of the results using several automatic metrics for evaluation." ></td>
	<td class="line x" title="133:159	Numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true (for example figure in the first row and first column means that for 98.9 percent of the language pairs the BLEU score for the bidirectional decoder was better than that of the forward decoder) proach (Brown et al., 1993))." ></td>
	<td class="line x" title="134:159	The results were the similar in character when other MT evaluation methods were used." ></td>
	<td class="line x" title="135:159	These results are summarized in Table 3." ></td>
	<td class="line x" title="136:159	4.2 Bidirectional Decoding Table 5 shows the performance of the bidirectional decoder relative to a forward decoder." ></td>
	<td class="line x" title="137:159	As can be seen from the table, in 269 out of the 272 experiments the bidirectional decoder outperformed the unidirectional decoder." ></td>
	<td class="line x" title="138:159	The gains ranged from a maximum of 1.81 BLEU (translating from Thai to Arabic) points, to a minimum of -0.04 BLEU points (translating from Indonesian to Japanese) with the average gain over all experiments being 0.56 BLEU points." ></td>
	<td class="line x" title="139:159	It is clear from our experiments that there is much to be gained from decoding bidirectionally." ></td>
	<td class="line x" title="140:159	Our results were almost unanimously positive, and in all three negative cases the drop in performance was small." ></td>
	<td class="line x" title="141:159	5 Conclusion In this paper we have investigated the effects on phrase-based machine translation performance of three different decoding strategies: forward, reverse and bidirectional." ></td>
	<td class="line x" title="142:159	The experiments were conducted on a large set of source and target languages consisting of 272 experiments representing all possible pairings from a set of 17 languages." ></td>
	<td class="line x" title="143:159	These languages were very diverse in character and included a broad selection of European and Asian languages." ></td>
	<td class="line x" title="144:159	The experimental results revealed that for SVO word order languages it is usually better to decode in a reverse manner, and in contrast, for SOV word order languages it is usu1130 ar da de en es fr id it ja ko ms nl pt ru th vi zh ar 0.66 0.51 1.03 0.65 0.75 0.59 0.47 0.46 0.85 0.59 0.69 0.39 0.30 1.81 1.30 0.85 da 0.27 0.61 0.63 0.38 0.60 0.59 0.29 1.04 0.79 0.69 0.45 0.89 0.27 1.28 0.87 0.47 de 0.52 0.51 0.54 0.44 0.42 0.70 0.40 0.74 0.45 0.83 0.37 0.28 0.34 0.77 0.90 0.84 en 0.53 0.01 0.32 0.23 0.25 0.56 0.19 1.11 0.59 0.28 0.27 0.45 0.60 0.89 0.61 0.58 es 0.28 0.48 0.45 0.56 0.43 0.12 0.26 0.57 0.64 0.56 0.06 0.04 0.24 1.16 1.23 0.68 fr 0.70 0.33 0.54 0.66 0.46 0.49 0.57 0.24 0.13 0.11 0.43 0.33 0.55 0.91 1.09 0.57 id 0.24 0.32 0.36 0.93 0.70 0.65 0.35 0.75 0.77 0.11 0.46 0.69 0.57 0.99 0.85 0.47 it 0.13 0.55 0.32 0.43 0.47 0.51 0.64 0.65 0.42 0.77 0.51 0.51 0.69 0.85 0.98 0.58 ja 0.38 0.62 0.60 0.61 0.38 0.73 0.04 0.43 0.35 0.05 0.70 0.30 0.38 0.53 0.17 0.02 ko 0.49 0.62 0.90 0.40 0.34 0.57 0.47 0.47 0.02 0.23 0.52 0.20 0.83 0.70 0.44 0.83 ms 0.37 0.57 0.63 0.92 0.81 0.75 0.36 0.54 0.70 1.31 0.76 0.35 0.51 1.14 0.70 0.35 nl 0.35 0.14 0.54 0.33 0.30 0.46 0.68 0.69 0.77 0.63 0.44 0.42 0.67 0.71 1.13 0.55 pt 0.46 0.21 0.37 0.21 0.17 0.49 0.47 0.24 0.88 0.45 0.54 0.39 0.41 0.94 1.15 0.90 ru 0.69 0.63 0.69 0.77 0.26 0.50 0.79 0.52 0.69 0.90 0.66 0.69 0.40 1.19 1.23 0.47 th 0.90 0.49 0.53 0.77 0.64 0.38 0.21 0.60 0.37 0.96 0.38 0.63 0.68 0.72 0.33 0.45 vi 0.64 0.61 0.42 1.09 0.84 0.63 0.34 0.70 0.59 0.39 0.16 0.56 0.36 0.50 0.77 0.53 zh 0.23 0.48 0.96 0.33 0.49 0.32 0.27 0.43 0.43 0.69 0.31 0.97 0.85 0.23 0.40 0.50 Table 5: Gains in BLEU score from decoding bidirectionally over a forward decoding strategy." ></td>
	<td class="line x" title="145:159	The numbers in the cells are the differences in BLEU percentage points between the systems." ></td>
	<td class="line x" title="146:159	Shaded cells indicate the cases where forward decoding gave a higher score." ></td>
	<td class="line x" title="147:159	Source languages are indicated by the column headers, the row headers denoting the target languages." ></td>
	<td class="line x" title="148:159	ally better to decode in a forward direction." ></td>
	<td class="line x" title="149:159	Our main contribution has been to show that a bidirectional decoding strategy is superior to both monodirectional decoding strategies." ></td>
	<td class="line x" title="150:159	It might be argued that the gains arise simply from system combination." ></td>
	<td class="line x" title="151:159	However, our systems are combined in a simple linear fashion, and gains will only arise when the second system contributes novel and useful information to into the combination." ></td>
	<td class="line x" title="152:159	Furthermore, our systems are trained on two copies of the same data, no additional data is required." ></td>
	<td class="line x" title="153:159	The gains from decoding bidirectionally were obtained very consistently, with only loose constraints on the decoding." ></td>
	<td class="line x" title="154:159	This can be seen clearly in Table 5 where the results are almost unanimously positive." ></td>
	<td class="line x" title="155:159	Moreover, these gains appear to be independent of the linguistic characteristics of the source and target languages." ></td>
	<td class="line x" title="156:159	In the future we would like to explore the possibilities created by more tightly coupling the forward and reverse components of the bidirectional decoder." ></td>
	<td class="line x" title="157:159	Scores from partial hypotheses of both processes could be combined and used at each step of the decoding, making the search more informed." ></td>
	<td class="line x" title="158:159	Furthermore, forward partial hypotheses and reverse hypotheses would meet during decoding (when one decoding direction has covered words in the source that the other has yet to cover), and provide paths for each other to a final state in the search." ></td>
	<td class="line x" title="159:159	Acknowledgment This work is partly supported by the Grant-inAid for Scientific Research (C) Number 19500137 and Construction of speech translation foundation aiming to overcome the barrier between Asian languages, the Special Coordination Funds for Promoting Science and Technology of the Ministry of Education, Culture, Sports, Science and Technology, Japan." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="E09-1063
Bilingually Motivated Domain-Adapted Word Segmentation for Statistical Machine Translation
Ma, Yanjun;Way, Andy;"></td>
	<td class="line x" title="1:190	Proceedings of the 12th Conference of the European Chapter of the ACL, pages 549557, Athens, Greece, 30 March  3 April 2009." ></td>
	<td class="line x" title="2:190	c2009 Association for Computational Linguistics Bilingually Motivated Domain-Adapted Word Segmentation for Statistical Machine Translation Yanjun Ma Andy Way National Centre for Language Technology School of Computing Dublin City University Dublin 9, Ireland {yma, away}@computing.dcu.ie Abstract We introduce a word segmentation approach to languages where word boundaries are not orthographically marked, with application to Phrase-Based Statistical Machine Translation (PB-SMT)." ></td>
	<td class="line x" title="3:190	Instead of using manually segmented monolingual domain-specific corpora to train segmenters, we make use of bilingual corpora and statistical word alignment techniques." ></td>
	<td class="line x" title="4:190	First of all, our approach is adapted for the specific translation task at hand by taking the corresponding source (target) language into account." ></td>
	<td class="line x" title="5:190	Secondly, this approach does not rely on manually segmented training data so that it can be automatically adapted for different domains." ></td>
	<td class="line x" title="6:190	We evaluate the performance of our segmentation approach on PB-SMT tasks from two domains and demonstrate that our approach scores consistently among the best results across different data conditions." ></td>
	<td class="line x" title="7:190	1 Introduction State-of-the-art Statistical Machine Translation (SMT) requires a certain amount of bilingual corpora as training data in order to achieve competitive results." ></td>
	<td class="line x" title="8:190	The only assumption of most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) is that the aligned sentences in such corpora should be segmented into sequences of tokens that are meant to be words." ></td>
	<td class="line x" title="9:190	Therefore, for languages where word boundaries are not orthographically marked, tools which segment a sentence into words are required." ></td>
	<td class="line x" title="10:190	However, this segmentation is normally performed as a preprocessing step using various word segmenters." ></td>
	<td class="line x" title="11:190	Moreover, most of these segmenters are usually trained on a manually segmented domainspecific corpus, which is not adapted for the specific translation task at hand given that the manual segmentation is performed in a monolingual context." ></td>
	<td class="line x" title="12:190	Consequently, such segmenters cannot produce consistently good results when used across different domains." ></td>
	<td class="line x" title="13:190	A substantial amount of research has been carried out to address the problems of word segmentation." ></td>
	<td class="line x" title="14:190	However, most research focuses on combining various segmenters either in SMT training or decoding (Dyer et al., 2008; Zhang et al., 2008)." ></td>
	<td class="line x" title="15:190	One important yet often neglected fact is that the optimal segmentation of the source (target) language is dependent on the target (source) language itself, its domain and its genre." ></td>
	<td class="line x" title="16:190	Segmentation considered to be good from a monolingual point of view may be unadapted for training alignment models or PB-SMT decoding (Ma et al., 2007)." ></td>
	<td class="line x" title="17:190	The resulting segmentation will consequently influence the performance of an SMT system." ></td>
	<td class="line x" title="18:190	In this paper, we propose a bilingually motivated automatically domain-adapted approach for SMT." ></td>
	<td class="line x" title="19:190	We utilise a small bilingual corpus with the relevant language segmented into basic writing units (e.g. characters for Chinese or kana for Japanese)." ></td>
	<td class="line x" title="20:190	Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidate words." ></td>
	<td class="line x" title="21:190	We evaluate the reliability of these candidates using simple metrics based on co-occurrence frequencies, similar to those used in associative approaches to word alignment (Melamed, 2000)." ></td>
	<td class="line x" title="22:190	We then modify the segmentation of the respective sentences in the parallel corpus according to these candidate words; these modified sentences are then given back to the word aligner, which produces new alignments." ></td>
	<td class="line x" title="23:190	We evaluate the validity of our approach by measuring the influence of the segmentation process on Chinese-to-English Machine Translation (MT) tasks in two different domains." ></td>
	<td class="line x" title="24:190	The remainder of this paper is organised as fol549 lows." ></td>
	<td class="line x" title="25:190	In section 2, we study the influence of word segmentation on PB-SMT across different domains." ></td>
	<td class="line x" title="26:190	Section 3 describes the working mechanism of our bilingually motivated word segmentation approach." ></td>
	<td class="line x" title="27:190	In section 4, we illustrate the adaptation of our decoder to this segmentation scheme." ></td>
	<td class="line x" title="28:190	The experiments conducted in two different domains are reported in Section 5 and 6." ></td>
	<td class="line x" title="29:190	We discuss related work in section 7." ></td>
	<td class="line x" title="30:190	Section 8 concludes and gives avenues for future work." ></td>
	<td class="line x" title="31:190	2 The Influence of Word Segmentation on SMT: A Pilot Investigation The monolingual word segmentation step in traditional SMT systems has a substantial impact on the performance of such systems." ></td>
	<td class="line x" title="32:190	A considerable amount of recent research has focused on the influence of word segmentation on SMT (Ma et al., 2007; Chang et al., 2008; Zhang et al., 2008); however, most explorations focused on the impact of various segmentation guidelines and the mechanisms of the segmenters themselves." ></td>
	<td class="line x" title="33:190	A current research interest concerns consistency of performance across different domains." ></td>
	<td class="line x" title="34:190	From our experiments, we show that monolingual segmenters cannot produce consistently good results when applied to a new domain." ></td>
	<td class="line x" title="35:190	Our pilot investigation into the influence of word segmentation on SMT involves three offthe-shelf Chinese word segmenters including ICTCLAS (ICT) Olympic version1, LDC segmenter2 and Stanford segmenter version 2006-05113." ></td>
	<td class="line x" title="36:190	Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al., 2003) and conditional random fields for the Stanford segmenter (Tseng et al., 2005)." ></td>
	<td class="line x" title="37:190	Both segmentation models were trained on news domain data with named entity recognition functionality." ></td>
	<td class="line x" title="38:190	The LDC segmenter is dictionary-based with word frequency information to help disambiguation, both of which are collected from data in the news domain." ></td>
	<td class="line x" title="39:190	We used Chinese character-based and manual segmentations as contrastive segmentations." ></td>
	<td class="line x" title="40:190	The experiments were carried out on a range of data sizes from news and dialogue domains using a state-of-the-art Phrase-Based SMT (PB-SMT) 1http://ictclas.org/index.html 2http://www.ldc.upenn.edu/Projects/ Chinese 3http://nlp.stanford.edu/software/ segmenter.shtml systemMoses (Koehn et al., 2007)." ></td>
	<td class="line x" title="41:190	The performance of PB-SMT system is measured with BLEU score (Papineni et al., 2002)." ></td>
	<td class="line x" title="42:190	We firstly measure the influence of word segmentation on in-domain data with respect to the three above mentioned segmenters, namely UN data from the NIST 2006 evaluation campaign." ></td>
	<td class="line x" title="43:190	As can be seen from Table 1, using monolingual segmenters achieves consistently better SMT performance than character-based segmentation (CS) on different data sizes, which means character-based segmentation is not good enough for this domain where the vocabulary tends to be large." ></td>
	<td class="line x" title="44:190	We can also observe that the ICT and Stanford segmenter consistently outperform the LDC segmenter." ></td>
	<td class="line x" title="45:190	Even using 3M sentence pairs for training, the differences between them are still statistically significant (p < 0.05) using approximate randomisation (Noreen, 1989) for significance testing." ></td>
	<td class="line x" title="46:190	40K 160K 640K 3M CS 8.33 12.47 14.40 17.80 ICT 10.17 14.85 17.20 20.50 LDC 9.37 13.88 15.86 19.59 Stanford 10.45 15.26 16.94 20.64 Table 1: Word segmentation on NIST data sets However, when tested on out-of-domain data, i.e. IWSLT data in the dialogue domain, the results seem to be more difficult to predict." ></td>
	<td class="line x" title="47:190	We trained the system on different sizes of data and evaluated the system on two test sets: IWSLT 2006 and 2007." ></td>
	<td class="line x" title="48:190	From Table 2, we can see that on the IWSLT 2006 test sets, LDC achieves consistently good results and the Stanford segmenter is the worst.4 Furthermore, the character-based segmentation also achieves competitive results." ></td>
	<td class="line x" title="49:190	On IWSLT 2007, all monolingual segmenters outperform character-based segmentation and the LDC segmenter is only slightly better than the other segmenters." ></td>
	<td class="line x" title="50:190	From the experiments reported above, we can reach the following conclusions." ></td>
	<td class="line x" title="51:190	First of all, character-based segmentation cannot achieve state-of-the-art results in most experimental SMT settings." ></td>
	<td class="line x" title="52:190	This also motivates the necessity to work on better segmentation strategies." ></td>
	<td class="line x" title="53:190	Second, monolingual segmenters cannot achieve consis4Interestingly, the developers themselves also note the sensitivity of the Stanford segmenter and incorporate external lexical information to address such problems (Chang et al., 2008)." ></td>
	<td class="line x" title="54:190	550 40K 160K IWSLT06 CS 19.31 23.06 Manual 19.94 ICT 20.34 23.36 LDC 20.37 24.34 Stanford 18.25 21.40 IWSLT07 CS 29.59 30.25 Manual 33.85 ICT 31.18 33.38 LDC 31.74 33.44 Stanford 30.97 33.41 Table 2: Word segmentation on IWSLT data sets tently good results when used in another domain." ></td>
	<td class="line x" title="55:190	In the following sections, we propose a bilingually motivated segmentation approach which can be automatically derived from a small representative data set and the experiments show that we can consistently obtain state-of-the-art results in different domains." ></td>
	<td class="line x" title="56:190	3 Bilingually Motivated Word Segmentation 3.1 Notation While in this paper, we focus on ChineseEnglish, the method proposed is applicable to other language pairs." ></td>
	<td class="line x" title="57:190	The notation, however, assumes ChineseEnglish MT. Given a Chinese sentence cJ1 consisting of J characters {c1,,cJ} and an English sentence eI1 consisting of I words {e1,,eI}, ACE will denote a Chinese-toEnglish word alignment between cJ1 and eI1." ></td>
	<td class="line x" title="58:190	Since we are primarily interested in 1-to-n alignments, ACE can be represented as a set of pairs ai = Ci,ei denoting a link between one single English word ei and a few Chinese characters Ci.The set Ci is empty if the word ei is not aligned to any character in cJ1 . 3.2 Candidate Extraction In the following, we assume the availability of an automatic word aligner that can output alignments ACE for any sentence pair (cJ1,eI1) in a parallel corpus." ></td>
	<td class="line x" title="59:190	We also assume that ACE contain 1-to-n alignments." ></td>
	<td class="line x" title="60:190	Our method for Chinese word segmentation is as follows: whenever a single English word is aligned with several consecutive Chinese characters, they are considered candidates for grouping." ></td>
	<td class="line x" title="61:190	Formally, given an alignment ACE between cJ1 and eI1, if ai = Ci,ei  ACE, with Ci = {ci1,,cim} and k  llbracket1,m  1rrbracket, ik+1  ik = 1, then the alignment ai between ei and the sequence of words Ci is considered a candidate word." ></td>
	<td class="line x" title="62:190	Some examples of such 1-to-n alignments between Chinese and English we can derive automatically are displayed in Figure 1.5 Figure 1: Example of 1-to-n word alignments between English words and Chinese characters 3.3 Candidate Reliability Estimation Of course, the process described above is errorprone, especially on a small amount of training data." ></td>
	<td class="line x" title="63:190	If we want to change the input segmentation to give to the word aligner, we need to make sure that we are not making harmful modifications." ></td>
	<td class="line x" title="64:190	We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary." ></td>
	<td class="line x" title="65:190	To perform this filtering, we use two simple statistical measures." ></td>
	<td class="line x" title="66:190	In the following, ai = Ci,ei denotes a candidate." ></td>
	<td class="line x" title="67:190	The first measure we consider is co-occurrence frequency (COOC(Ci,ei)), i.e. the number of times Ci and ei co-occur in the bilingual corpus." ></td>
	<td class="line x" title="68:190	This very simple measure is frequently used in associative approaches (Melamed, 2000)." ></td>
	<td class="line x" title="69:190	The second measure is the alignment confidence (Ma et al., 2007), defined as AC(ai) = C(ai)COOC(C i,ei) , where C(ai) denotes the number of alignments proposed by the word aligner that are identical to ai." ></td>
	<td class="line x" title="70:190	In other words, AC(ai) measures how often the aligner aligns Ci and ei when they co-occur." ></td>
	<td class="line x" title="71:190	We also impose that |Ci|  k, where k is a fixed integer that may depend on the language pair (between 3 and 5 in practice)." ></td>
	<td class="line x" title="72:190	The rationale behind this is that it is very rare to get reliable alignments between one word and k consecutive words when k is high." ></td>
	<td class="line x" title="73:190	5While in this paper we are primarily concerned with languages where the word boundaries are not orthographically marked, this approach, however, can also be applied to languages marked with word boundaries to construct bilingually motivated words." ></td>
	<td class="line x" title="74:190	551 The candidates are included in our bilingual dictionary if and only if their measures are above some fixed thresholds tCOOC and tAC, which allow for the control of the size of the dictionary and the quality of its contents." ></td>
	<td class="line x" title="75:190	Some other measures (including the Dice coefficient) could be considered; however, it has to be noted that we are more interested here in the filtering than in the discovery of alignments per se, since our method builds upon an existing aligner." ></td>
	<td class="line x" title="76:190	Moreover, we will see that even these simple measures can lead to an improvement in the alignment process in an MT context." ></td>
	<td class="line x" title="77:190	3.4 Bootstrapped word segmentation Once the candidates are extracted, we perform word segmentation using the bilingual dictionaries constructed using the method described above; this provides us with an updated training corpus, in which some character sequences have been replaced by a single token." ></td>
	<td class="line x" title="78:190	This update is totally naive: if an entry ai = Ci,ei is present in the dictionary and matches one sentence pair (cJ1,eI1) (i.e. Ci and ei are respectively contained in cJ1 and eI1), then we replace the sequence of characters Ci with a single token which becomes a new lexical unit.6 Note that this replacement occurs even if no alignment was found between Ci and ei for the pair (cJ1,eI1)." ></td>
	<td class="line x" title="79:190	This is motivated by the fact that the filtering described above is quite conservative; we trust the entry ai to be correct." ></td>
	<td class="line x" title="80:190	This process can be applied several times: once we have grouped some characters together, they become the new basic unit to consider, and we can re-run the same method to get additional groupings." ></td>
	<td class="line x" title="81:190	However, we have not seen in practice much benefit from running it more than twice (few new candidates are extracted after two iterations)." ></td>
	<td class="line x" title="82:190	4 Word Lattice Decoding 4.1 Word Lattices In the decoding stage, the various segmentation alternatives can be encoded into a compact representation of word lattices." ></td>
	<td class="line x" title="83:190	A word lattice G = V,E is a directed acyclic graph that formally is a weighted finite state automaton." ></td>
	<td class="line x" title="84:190	In the case of word segmentation, each edge is a candidate word associated with its weights." ></td>
	<td class="line x" title="85:190	A straightforward es6In case of overlap between several groups of words to replace, we select the one with the highest confidence (according to tAC)." ></td>
	<td class="line x" title="86:190	timation of the weights is to distribute the probability mass for each node uniformly to each outgoing edge." ></td>
	<td class="line x" title="87:190	The single node having no outgoing edges is designated the end node." ></td>
	<td class="line x" title="88:190	An example of word lattices for a Chinese sentence is shown in Figure 2." ></td>
	<td class="line x" title="89:190	4.2 Word Lattice Generation Previous research on generating word lattices relies on multiple monolingual segmenters (Xu et al., 2005; Dyer et al., 2008)." ></td>
	<td class="line x" title="90:190	One advantage of our approach is that the bilingually motivated segmentation process facilitates word lattice generation without relying on other segmenters." ></td>
	<td class="line x" title="91:190	As described in section 3.4, the update of the training corpus based on the constructed bilingual dictionary requires that the sentence pair meets the bilingual constraints." ></td>
	<td class="line x" title="92:190	Such a segmentation process in the training stage facilitates the utilisation of word lattice decoding." ></td>
	<td class="line x" title="93:190	4.3 Phrase-Based Word Lattice Decoding Given a Chinese input sentence cJ1 consisting of J characters, the traditional approach is to determine the best word segmentation and perform decoding afterwards." ></td>
	<td class="line x" title="94:190	In such a case, we first seek a single best segmentation: fK1 = argmax fK1 ,K {Pr(fK1 |cJ1)} Then in the decoding stage, we seek: eI1 = argmax eI1,I {Pr(eI1| fK1 )} In such a scenario, some segmentations which are potentially optimal for the translation may be lost." ></td>
	<td class="line x" title="95:190	This motivates the need for word lattice decoding." ></td>
	<td class="line x" title="96:190	The search process can be rewritten as: eI1 = argmax eI1,I {max fK1 ,K Pr(eI1,fK1 |cJ1)} = argmax eI1,I {max fK1 ,K Pr(eI1)Pr(fK1 |eI1,cJ1)} = argmax eI1,I {max fK1 ,K Pr(eI1)Pr(fK1 |eI1)Pr(fK1 |cJ1)} Given the fact that the number of segmentations fK1 grows exponentially with respect to the number of characters K, it is impractical to firstly enumerate all possible fK1 and then to decode." ></td>
	<td class="line x" title="97:190	However, it is possible to enumerate all the alternative segmentations for a substring of cJ1, making the utilisation of word lattices tractable in PB-SMT." ></td>
	<td class="line x" title="98:190	552 Figure 2: Example of a word lattice 5 Experimental Setting 5.1 Evaluation The intrinsic quality of word segmentation is normally evaluated against a manually segmented gold-standard corpus using F-score." ></td>
	<td class="line x" title="99:190	While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations." ></td>
	<td class="line x" title="100:190	First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996)." ></td>
	<td class="line x" title="101:190	Second, an increase in F-score does not necessarily imply an improvement in translation quality." ></td>
	<td class="line x" title="102:190	It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008)." ></td>
	<td class="line x" title="103:190	Consequently, we chose to extrinsically evaluate the performance of our approach via the ChineseEnglish translation task, i.e. we measure the influence of the segmentation process on the final translation output." ></td>
	<td class="line oc" title="104:190	The quality of the translation output is mainly evaluated using BLEU, with NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) as complementary metrics." ></td>
	<td class="line x" title="105:190	5.2 Data The data we used in our experiments are from two different domains, namely news and travel dialogues." ></td>
	<td class="line x" title="106:190	For the news domain, we trained our system using a portion of UN data for NIST 2006 evaluation campaign." ></td>
	<td class="line x" title="107:190	The system was developed on LDC Multiple-Translation Chinese (MTC) Corpus and tested on MTC part 2, which was also used as a test set for NIST 2002 evaluation campaign." ></td>
	<td class="line x" title="108:190	For the dialogue data, we used the Chinese English datasets provided within the IWSLT 2007 evaluation campaign." ></td>
	<td class="line x" title="109:190	Specifically, we used the standard training data, to which we added devset1 and devset2." ></td>
	<td class="line x" title="110:190	Devset4 was used to tune the parameters and the performance of the system was tested on both IWSLT 2006 and 2007 test sets." ></td>
	<td class="line x" title="111:190	We used both test sets because they are quite different in terms of sentence length and vocabulary size." ></td>
	<td class="line x" title="112:190	To test the scalability of our approach, we used HIT corpus provided within IWSLT 2008 evaluation campaign." ></td>
	<td class="line x" title="113:190	The various statistics for the corpora are shown in Table 3." ></td>
	<td class="line x" title="114:190	5.3 Baseline System We conducted experiments using different segmenters with a standard log-linear PB-SMT model: GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-errorrate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al., 2007; Dyer et al., 2008) to translate both single best segmentation and word lattices." ></td>
	<td class="line x" title="115:190	6 Experiments 6.1 Results The initial word alignments are obtained using the baseline configuration described above by segmenting the Chinese sentences into characters." ></td>
	<td class="line x" title="116:190	From these we build a bilingual 1-to-n dictionary, and the training corpus is updated by grouping the characters in the dictionaries into a single word, using the method presented in section 3.4." ></td>
	<td class="line x" title="117:190	As previously mentioned, this process can be repeated several times." ></td>
	<td class="line x" title="118:190	We then extract aligned phrases using the same procedure as for the baseline system; the only difference is the basic unit we are considering." ></td>
	<td class="line x" title="119:190	Once the phrases are extracted, we perform the estimation of weights for the features of the log-linear model." ></td>
	<td class="line x" title="120:190	We then use a simple dictionary-based maximum matching algorithm to obtain a single-best segmentation for the Chinese sentences in the development set so that 553 Train Dev." ></td>
	<td class="line x" title="121:190	Eval." ></td>
	<td class="line x" title="122:190	Zh En Zh En Zh En Dialogue Sentences 40,958 489 (7 ref.)" ></td>
	<td class="line x" title="123:190	489 (6 ref.)/489 (7 ref.)" ></td>
	<td class="line x" title="124:190	Running words 488,303 385,065 8,141 46,904 8,793/4,377 51,500/23,181 Vocabulary size 2,742 9,718 835 1,786 936/772 2,016/1,339 News Sentences 40,000 993 (9 ref.)" ></td>
	<td class="line x" title="125:190	878 (4 ref.)" ></td>
	<td class="line x" title="126:190	Running words 1,412,395 956,023 41,466 267,222 38,700 105,530 Vocabulary size 6057 20,068 1,983 10,665 1,907 7,388 Table 3: Corpus statistics for Chinese (Zh) character segmentation and English (En) minimum-error-rate training can be performed.7 Finally, in the decoding stage, we use the same segmentation algorithm to obtain the single-best segmentation on the test set, and word lattices can also be generated using the bilingual dictionary." ></td>
	<td class="line x" title="127:190	The various parameters of the method (k, tCOOC, tAC, cf.section 3) were optimised on the development set." ></td>
	<td class="line x" title="129:190	One iteration of character grouping on the NIST task was found to be enough; the optimal set of values was found to be k = 3, tAC = 0.0 and tCOOC = 0, meaning that all the entries in the bilingually dictionary are kept." ></td>
	<td class="line x" title="130:190	On IWSLT data, we found that two iterations of character grouping were needed: the optimal set of values was found to be k = 3, tAC = 0.3, tCOOC = 8 for the first iteration, and tAC = 0.2, tCOOC = 15 for the second." ></td>
	<td class="line x" title="131:190	As can be seen from Table 4, our bilingually motivated segmenter (BS) achieved statistically significantly better results than character-based segmentation when enhanced with word lattice decoding.8 Compared to the best in-domain segmenter, namely the Stanford segmenter on this particular task, our approach is inferior according to BLEU and NIST." ></td>
	<td class="line x" title="132:190	We firstly attribute this to the small amount of training data, from which a high quality bilingual dictionary cannot be obtained due to data sparseness problems." ></td>
	<td class="line x" title="133:190	We also attribute this to the vast amount of named entity terms in the test sets, which is extremely difficult for our approach.9 We expect to see better results when a larger amount of data is used and the segmenter is enhanced with a named entity recogniser." ></td>
	<td class="line x" title="134:190	On IWSLT data (cf.Tables 5 and 6), our 7In order to save computational time, we used the same set of parameters obtained above to decode both the singlebest segmentation and the word lattice." ></td>
	<td class="line x" title="136:190	8Note the BLEU scores are particularly low due to the number of references used (4 references), in addition to the small amount of training data available." ></td>
	<td class="line x" title="137:190	9As we previously point out, both ICT and Stanford segmenters are equipped with named entity recognition functionality." ></td>
	<td class="line x" title="138:190	This may risk causing data sparseness problems on small training data." ></td>
	<td class="line x" title="139:190	However, this is beneficial in the translation process compared to character-based segmentation." ></td>
	<td class="line x" title="140:190	approach yielded a consistently good performance on both translation tasks compared to the best indomain segmenterthe LDC segmenter." ></td>
	<td class="line x" title="141:190	Moreover, the good performance is confirmed by all three evaluation measures." ></td>
	<td class="line o" title="142:190	BLEU NIST METEOR CS 8.43 4.6272 0.3778 Stanford 10.45 5.0675 0.3699 BS-SingleBest 7.98 4.4374 0.3510 BS-WordLattice 9.04 4.6667 0.3834 Table 4: BS on NIST task BLEU NIST METEOR CS 0.1931 6.1816 0.4998 LDC 0.2037 6.2089 0.4984 BS-SingleBest 0.1865 5.7816 0.4602 BS-WordLattice 0.2041 6.2874 0.5124 Table 5: BS on IWSLT 2006 task BLEU NIST METEOR CS 0.2959 6.1216 0.5216 LDC 0.3174 6.2464 0.5403 BS-SingleBest 0.3023 6.0476 0.5125 BS-WordLattice 0.3171 6.3518 0.5603 Table 6: BS on IWSLT 2007 task 6.2 Parameter Search Graph The reliability estimation process is computationally intensive." ></td>
	<td class="line x" title="143:190	However, this can be easily parallelised." ></td>
	<td class="line x" title="144:190	From our experiments, we observed that the translation results are very sensitive to the parameters and this search process is essential to achieve good results." ></td>
	<td class="line x" title="145:190	Figure 3 is the search graph on the IWSLT data set in the first iteration step." ></td>
	<td class="line x" title="146:190	From this graph, we can see that filtering of the bilingual dictionary is essential in order to achieve better performance." ></td>
	<td class="line x" title="147:190	554 Figure 3: The search graph on development set of IWSLT task 6.3 Vocabulary Size Our bilingually motivated segmentation approach has to overcome another challenge in order to produce competitive results, i.e. data sparseness." ></td>
	<td class="line x" title="148:190	Given that our segmentation is based on bilingual dictionaries, the segmentation process can significantly increase the size of the vocabulary, which could potentially lead to a data sparseness problem when the size of the training data is small." ></td>
	<td class="line x" title="149:190	Tables 7 and 8 list the statistics of the Chinese side of the training data, including the total vocabulary (Voc), number of character vocabulary (Char.voc) in Voc, and the running words (Run.words) when different word segmentations were used." ></td>
	<td class="line x" title="150:190	From Table 7, we can see that our approach suffered from data sparseness on the NIST task, i.e. a large vocabulary was generated, of which a considerable amount of characters still remain as separate words." ></td>
	<td class="line x" title="151:190	On the IWSLT task, since the dictionary generation process is more conservative, we maintained a reasonable vocabulary size, which contributed to the final good performance." ></td>
	<td class="line x" title="152:190	Voc." ></td>
	<td class="line x" title="153:190	Char.voc Run." ></td>
	<td class="line x" title="154:190	Words CS 6,057 6,057 1,412,395 ICT 16,775 1,703 870,181 LDC 16,100 2,106 881,861 Stanford 22,433 1,701 880,301 BS 18,111 2,803 927,182 Table 7: Vocabulary size of NIST task (40K) 6.4 Scalability The experimental results reported above are based on a small training corpus containing roughly 40,000 sentence pairs." ></td>
	<td class="line x" title="155:190	We are particularly interested in the performance of our segmentation apVoc." ></td>
	<td class="line x" title="156:190	Char.voc Run." ></td>
	<td class="line x" title="157:190	Words CS 2,742 2,742 488,303 ICT 11,441 1,629 358,504 LDC 9,293 1,963 364,253 Stanford 18,676 981 348,251 BS 3,828 2,740 402,845 Table 8: Vocabulary size of IWSLT task (40K) proach when it is scaled up to larger amounts of data." ></td>
	<td class="line x" title="158:190	Given that the optimisation of the bilingual dictionary is computationally intensive, it is impractical to directly extract candidate words and estimate their reliability." ></td>
	<td class="line x" title="159:190	As an alternative, we can use the obtained bilingual dictionary optimised on the small corpus to perform segmentation on the larger corpus." ></td>
	<td class="line x" title="160:190	We expect competitive results when the small corpus is a representative sample of the larger corpus and large enough to produce reliable bilingual dictionaries without suffering severely from data sparseness." ></td>
	<td class="line x" title="161:190	As we can see from Table 9, our segmentation approach achieved consistent results on both IWSLT 2006 and 2007 test sets." ></td>
	<td class="line x" title="162:190	On the NIST task (cf.Table 10), our approach outperforms the basic character-based segmentation; however, it is still inferior compared to the other in-domain monolingual segmenters due to the low quality of the bilingual dictionary induced (cf.section 6.1)." ></td>
	<td class="line x" title="165:190	IWSLT06 IWSLT07 CS 23.06 30.25 ICT 23.36 33.38 LDC 24.34 33.44 Stanford 21.40 33.41 BS-SingleBest 22.45 30.76 BS-WordLattice 24.18 32.99 Table 9: Scale-up to 160K on IWSLT data sets 160K 640K CS 12.47 14.40 ICT 14.85 17.20 LDC 13.88 15.86 Stanford 15.26 16.94 BS-SingleBest 12.58 14.11 BS-WordLattice 13.74 15.33 Table 10: Scalability of BS on NIST task 555 6.5 Using different word aligners The above experiments rely on GIZA++ to perform word alignment." ></td>
	<td class="line x" title="166:190	We next show that our approach is not dependent on the word aligner given that we have a conservative reliability estimation procedure." ></td>
	<td class="line x" title="167:190	Table 11 shows the results obtained on the IWSLT data set using the MTTK alignment tool (Deng and Byrne, 2005; Deng and Byrne, 2006)." ></td>
	<td class="line x" title="168:190	IWSLT06 IWSLT07 CS 21.04 31.41 ICT 20.48 31.11 LDC 20.79 30.51 Stanford 17.84 29.35 BS-SingleBest 19.22 29.75 BS-WordLattice 21.76 31.75 Table 11: BS on IWSLT data sets using MTTK 7 Related Work (Xu et al., 2004) were the first to question the use of word segmentation in SMT and showed that the segmentation proposed by word alignments can be used in SMT to achieve competitive results compared to using monolingual segmenters." ></td>
	<td class="line x" title="169:190	Our approach differs from theirs in two aspects." ></td>
	<td class="line x" title="170:190	Firstly, (Xu et al., 2004) use word aligners to reconstruct a (monolingual) Chinese dictionary and reuse this dictionary to segment Chinese sentences as other monolingual segmenters." ></td>
	<td class="line x" title="171:190	Our approach features the use of a bilingual dictionary and conducts a different segmentation." ></td>
	<td class="line x" title="172:190	In addition, we add a process which optimises the bilingual dictionary according to translation quality." ></td>
	<td class="line x" title="173:190	(Ma et al., 2007) proposed an approach to improve word alignment by optimising the segmentation of both source and target languages." ></td>
	<td class="line x" title="174:190	However, the reported experiments still rely on some monolingual segmenters and the issue of scalability is not addressed." ></td>
	<td class="line x" title="175:190	Our research focuses on avoiding the use of monolingual segmenters in order to improve the robustness of segmenters across different domains." ></td>
	<td class="line x" title="176:190	(Xu et al., 2005) were the first to propose the use of word lattice decoding in PB-SMT, in order to address the problems of segmentation." ></td>
	<td class="line x" title="177:190	(Dyer et al., 2008) extended this approach to hierarchical SMT systems and other language pairs." ></td>
	<td class="line x" title="178:190	However, both of these methods require some monolingual segmentation in order to generate word lattices." ></td>
	<td class="line x" title="179:190	Our approach facilitates word lattice generation given that our segmentation is driven by the bilingual dictionary." ></td>
	<td class="line x" title="180:190	8 Conclusions and Future Work In this paper, we introduced a bilingually motivated word segmentation approach for SMT." ></td>
	<td class="line x" title="181:190	The assumption behind this motivation is that the language to be segmented can be tokenised into basic writing units." ></td>
	<td class="line x" title="182:190	Firstly, we extract 1-to-n word alignments using statistical word aligners to construct a bilingual dictionary in which each entry indicates a correspondence between one English word and n Chinese characters." ></td>
	<td class="line x" title="183:190	This dictionary is then filtered using a few simple association measures and the final bilingual dictionary is deployed for word segmentation." ></td>
	<td class="line x" title="184:190	To overcome the segmentation problem in the decoding stage, we deployed word lattice decoding." ></td>
	<td class="line x" title="185:190	We evaluated our approach on translation tasks from two different domains and demonstrate that our approach is (i) not as sensitive as monolingual segmenters, and (ii) that the SMT system using our word segmentation can achieve state-of-the-art performance." ></td>
	<td class="line x" title="186:190	Moreover, our approach can easily be scaled up to larger data sets and achieves competitive results if the small data used is a representative sample." ></td>
	<td class="line x" title="187:190	As for future work, firstly we plan to integrate some named entity recognisers into our approach." ></td>
	<td class="line x" title="188:190	We also plan to try our approach in more domains and on other language pairs (e.g. Japanese English)." ></td>
	<td class="line x" title="189:190	Finally, we intend to explore the correlation between vocabulary size and the amount of training data needed in order to achieve good results using our approach." ></td>
	<td class="line x" title="190:190	Acknowledgments This work is supported by Science Foundation Ireland (O5/IN/1732) and the Irish Centre for HighEnd Computing.10 We would like to thank the reviewers for their insightful comments." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="N09-1058
Streaming for large scale NLP: Language Modeling
Goyal, Amit;Daume III, Hal;Venkatasubramanian, Suresh;"></td>
	<td class="line x" title="1:229	Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 512520, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:229	c 2009 Association for Computational Linguistics Streaming for large scale NLP: Language Modeling Amit Goyal, Hal Daume III, and Suresh Venkatasubramanian University of Utah, School of Computing {amitg,hal,suresh}@cs.utah.edu Abstract In this paper, we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems." ></td>
	<td class="line x" title="3:229	We present an efficient low-memory method for constructing high-order approximate n-gram frequency counts." ></td>
	<td class="line x" title="4:229	The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint." ></td>
	<td class="line x" title="5:229	We show that this method easily scales to billion-word monolingual corpora using a conventional (8 GB RAM) desktop machine." ></td>
	<td class="line x" title="6:229	Statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods." ></td>
	<td class="line x" title="7:229	1 Introduction In many NLP problems, we are faced with the challenge of dealing with large amounts of data." ></td>
	<td class="line x" title="8:229	Many problems boil down to computing relative frequencies of certain items on this data." ></td>
	<td class="line x" title="9:229	Items can be words, patterns, associations, n-grams, and others." ></td>
	<td class="line x" title="10:229	Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al., 2005), constructing syntactic rules for SMT (Galley et al., 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies." ></td>
	<td class="line x" title="11:229	We use language modeling as a canonical example of a large-scale task that requires relative frequency estimation." ></td>
	<td class="line x" title="12:229	Computing relative frequencies seems like an easy problem." ></td>
	<td class="line x" title="13:229	However, as corpus sizes grow, it becomes a highly computational expensive task." ></td>
	<td class="line x" title="14:229	Cutoff Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 2 229.8m 28.23 7.613 56.03 3 143.6m 28.17 7.571 56.53 5 59.4m 28.33 7.636 56.03 10 18.3m 27.91 7.546 55.64 100 1.1m 28.03 7.607 55.91 200 0.5m 27.62 7.550 55.67 Table 1: Effect of count-based pruning on SMT performance using EAN corpus." ></td>
	<td class="line o" title="15:229	Results are according to BLEU, NIST and METEOR (MET) metrics." ></td>
	<td class="line x" title="16:229	Bold #s are not statistically significant worse than exact model." ></td>
	<td class="line x" title="17:229	Brants et al.(2007) used 1500 machines for a day to compute the relative frequencies of n-grams (summed over all orders from 1 to 5) from 1.8TB of web data." ></td>
	<td class="line x" title="19:229	Their resulting model contained 300 million unique n-grams." ></td>
	<td class="line x" title="20:229	It is not realistic using conventional computing resources to use all the 300 million n-grams for applications like speech recognition, spelling correction, information extraction, and statistical machine translation (SMT)." ></td>
	<td class="line x" title="21:229	Hence, one of the easiest way to reduce the size of this model is to use count-based pruning which discards all n-grams whose count is less than a pre-defined threshold." ></td>
	<td class="line x" title="22:229	Although countbased pruning is quite simple, yet it is effective for machine translation." ></td>
	<td class="line x" title="23:229	As we do not have a copy of the web, we will use a portion of gigaword i.e. EAN (see Section 4.1) to show the effect of count-based pruning on performance of SMT (see Section 5.1)." ></td>
	<td class="line x" title="24:229	Table 1 shows that using a cutoff of 100 produces a model of size 1.1 million n-grams with a Bleu score of 28.03." ></td>
	<td class="line x" title="25:229	If we compare this with an exact model of size 367.6 million n-grams, we see an increase of 0.8 points in Bleu (95% statistical significance level 512  Size BLEU NIST MET Exact 367.6m 28.73 7.691 56.32 1e-10 218.4m 28.64 7.669 56.33 5e-10 171.0m 28.48 7.666 56.38 1e-9 148.0m 28.56 7.646 56.51 5e-9 91.9m 28.27 7.623 56.16 1e-8 69.4m 28.15 7.609 56.19 5e-7 28.5m 28.08 7.595 55.91 Table 2: Effect of entropy-based pruning on SMT performance using EAN corpus." ></td>
	<td class="line x" title="26:229	Results are as in Table 1 is  0.53 Bleu)." ></td>
	<td class="line x" title="27:229	However, we need 300 times bigger model to get such an increase." ></td>
	<td class="line x" title="28:229	Unfortunately, it is not possible to integrate such a big model inside a decoder using normal computation resources." ></td>
	<td class="line x" title="29:229	A better way of reducing the size of n-grams is to use entropy pruning (Stolcke, 1998)." ></td>
	<td class="line x" title="30:229	Table 2 shows the results with entropy pruning with different settings of ." ></td>
	<td class="line x" title="31:229	We see that for three settings of  equal to 1e-10, 5e-10 and 1e-9, we get Bleu scores comparable to the exact model." ></td>
	<td class="line x" title="32:229	However, the size of all these models is not at all small." ></td>
	<td class="line x" title="33:229	The size of smallest model is 25% of the exact model." ></td>
	<td class="line x" title="34:229	Even with this size it is still not feasible to integrate such a big model inside a decoder." ></td>
	<td class="line x" title="35:229	If we take a model of size comparable to count cutoff of 100, i.e., with  = 5e-7, we see both count-based pruning as well as entropy pruning performs the same." ></td>
	<td class="line x" title="36:229	There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a; Talbot and Osborne, 2007b; Talbot and Brants, 2008)) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately." ></td>
	<td class="line x" title="37:229	There are two difficulties with scaling all the above approaches as the order of the LM increases." ></td>
	<td class="line x" title="38:229	Firstly, the computation time to build the database of counts increases rapidly." ></td>
	<td class="line x" title="39:229	Secondly, the initial disk storage required to maintain these counts, prior to building the compressed representation is enormous." ></td>
	<td class="line x" title="40:229	The method we propose solves both of these problems." ></td>
	<td class="line x" title="41:229	We do this by making use of the streaming algorithm paradigm (Muthukrishnan, 2005)." ></td>
	<td class="line x" title="42:229	Working under the assumption that multiple-GB models are infeasible, our goal is to instead of estimating a large model and then compressing it, we directly estimate a small model." ></td>
	<td class="line x" title="43:229	We use a deterministic streaming algorithm (Manku and Motwani, 2002) that computes approximate frequency counts of frequently occurring n-grams." ></td>
	<td class="line x" title="44:229	This scheme is considerably more accurate in getting the actual counts as compared to other schemes (Demaine et al., 2002; Karp et al., 2003) that find the set of frequent items without caring about the accuracy of counts." ></td>
	<td class="line x" title="45:229	We use these counts directly as features in an SMT system, and propose a direct way to integrate these features into an SMT decoder." ></td>
	<td class="line x" title="46:229	Experiments show that directly storing approximate counts of frequent 5-grams compared to using count or entropybased pruning counts gives equivalent SMT performance, while dramatically reducing the memory usage and getting rid of pre-computing a large model." ></td>
	<td class="line x" title="47:229	2 Background 2.1 n-gram Language Models Language modeling is based on assigning probabilities to sentences." ></td>
	<td class="line x" title="48:229	It can either compute the probability of an entire sentence or predict the probability of the next word in a sequence." ></td>
	<td class="line x" title="49:229	Let wm1 denote a sequence of words (w1,,wm)." ></td>
	<td class="line x" title="50:229	The probability of estimating word wm depends on previous n-1 words where n denotes the size of n-gram." ></td>
	<td class="line x" title="51:229	This assumption that probability of predicting a current word depends on the previous words is called a Markov assumption, typically estimated by relative frequency: P(wm | wm1mn+1) = C(w m1 mn+1wm) C(wm1mn+1) (1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix." ></td>
	<td class="line x" title="52:229	This is precisely the relative frequency estimate we seek." ></td>
	<td class="line x" title="53:229	2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new." ></td>
	<td class="line x" title="54:229	(Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests." ></td>
	<td class="line x" title="55:229	Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models 513 for SMT." ></td>
	<td class="line x" title="56:229	(Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions." ></td>
	<td class="line x" title="57:229	A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem." ></td>
	<td class="line x" title="58:229	(Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees." ></td>
	<td class="line x" title="59:229	(Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used." ></td>
	<td class="line x" title="60:229	(Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow." ></td>
	<td class="line x" title="61:229	Now using higher order LMs at time of re-ranking looks like a good option." ></td>
	<td class="line x" title="62:229	However, the target n-best hypothesis list is not diverse enough." ></td>
	<td class="line x" title="63:229	Hence if possible it is always better to integrate LMs directly into the decoder." ></td>
	<td class="line x" title="64:229	2.3 Streaming Consider an algorithm that reads the input from a read-only stream from left to right, with no ability to go back to the input that it has already processed." ></td>
	<td class="line x" title="65:229	This algorithm has working storage that it can use to store parts of the input or other intermediate computations." ></td>
	<td class="line x" title="66:229	However, (and this is a critical constraint), this working storage space is significantly smaller than the input stream length." ></td>
	<td class="line x" title="67:229	For typical algorithms, the storage size is of the order of logk N, where N is the input size and k is some constant." ></td>
	<td class="line x" title="68:229	Stream algorithms were first developed in the early 80s, but gained in popularity in the late 90s as researchers first realized the challenges of dealing with massive data sets." ></td>
	<td class="line x" title="69:229	A good survey of the model and core challenges can be found in (Muthukrishnan, 2005)." ></td>
	<td class="line x" title="70:229	There has been considerable work on the problem of identifying high-frequency items (items with frequency above a threshold), and a detailed review of these methods is beyond the scope of this article." ></td>
	<td class="line x" title="71:229	A new survey by (Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature." ></td>
	<td class="line x" title="72:229	3 Space-Efficient Approximate Frequency Estimation Prior work on approximate frequency estimation for language models provide a no-false-negative guarantee, ensuring that counts for n-grams in the model are returned exactly, while working to make sure the false-positive rate remains small (Talbot and Osborne, 2007a)." ></td>
	<td class="line x" title="73:229	The notion of approximation we use is different: in our approach, it is the actual count values that will be approximated." ></td>
	<td class="line x" title="74:229	We also exploit the fact that low-frequency n-grams, while constituting the vast majority of the set of unique n-grams, are usually smoothed away and are less likely to influence the language model significantly." ></td>
	<td class="line x" title="75:229	Discarding low-frequency n-grams is particularly important in a stream setting, because it can be shown in general that any algorithm that generates approximate frequency counts for all n-grams requires space linear in the input stream (Alon et al., 1999)." ></td>
	<td class="line x" title="76:229	We employ an algorithm for approximate frequency counting proposed by (Manku and Motwani, 2002) in the context of database management." ></td>
	<td class="line x" title="77:229	Fix parameters s  (0,1), and   (0,1),  s. Our goal is to approximately find all n-grams with frequency at least sN." ></td>
	<td class="line x" title="78:229	For an input stream of n-grams of length N, the algorithm outputs a set of items (and frequencies) and guarantees the following:  All items with frequencies exceeding sN are output (no false negatives)." ></td>
	<td class="line x" title="79:229	 No item with frequency less than (s  )N is output (few false positives)." ></td>
	<td class="line x" title="80:229	 All reported frequencies are less than the true frequencies by at most N (close-to-exact frequencies)." ></td>
	<td class="line x" title="81:229	 The space used by the algorithm is O(1 logN)." ></td>
	<td class="line x" title="82:229	A simple example illustrates these properties." ></td>
	<td class="line x" title="83:229	Let us fix s = 0.01, = 0.001." ></td>
	<td class="line x" title="84:229	Then the algorithm guarantees that all n-grams with frequency at least 1% will be returned, no element with frequency less than 0.9% will be returned, and all frequencies will be no more than 0.1% away from the true frequencies." ></td>
	<td class="line x" title="85:229	The space used by the algorithm is O(logN), which can be compared to the much larger (close to N) space 514 needed to store the initial frequency counts." ></td>
	<td class="line x" title="86:229	In addition, the algorithm runs in linear time by definition, requiring only one pass over the input." ></td>
	<td class="line x" title="87:229	Note that there might be 1 elements with frequency at least N, and so the algorithm uses optimal space (up to a logarithmic factor)." ></td>
	<td class="line x" title="88:229	3.1 The Algorithm We present a high-level overview of the algorithm; for more details, the reader is referred to (Manku and Motwani, 2002)." ></td>
	<td class="line x" title="89:229	The algorithm proceeds by conceptually dividing the stream into epochs, each containing 1/ elements." ></td>
	<td class="line x" title="90:229	Note that there are N epochs." ></td>
	<td class="line x" title="91:229	Each such epoch has an ID, starting from 1." ></td>
	<td class="line x" title="92:229	The algorithm maintains a list of tuples1 of the form (e,f,), where e is an n-gram, f is its reported frequency, and  is the maximum error in the frequency estimation." ></td>
	<td class="line x" title="93:229	While the algorithm reads ngrams associated with the current epoch, it does one of two things: if the new element e is contained in the list of tuples, it merely increments the frequency count f. If not, it creates a new tuple of the form (e,1,T 1), where T is the ID of the current epoch." ></td>
	<td class="line x" title="94:229	After each epoch, the algorithm cleans house by eliminating tuples whose maximum true frequency is small." ></td>
	<td class="line x" title="95:229	Formally, if the epoch that just ended has ID T, then the algorithm deletes all tuples satisfying condition f +   T. Since T  N, this ensures that no low-frequency tuples are retained." ></td>
	<td class="line x" title="96:229	When all elements in the stream have been processed, the algorithm returns all tuples (e,f,) where f  (s)N. In practice, however we do not care about s and return all tuples." ></td>
	<td class="line x" title="97:229	At a high level, the reason the algorithm works is that if an element has high frequency, it shows up more than once each epoch, and so its frequency gets updated enough to stave off elimination." ></td>
	<td class="line x" title="98:229	4 Intrinsic Evaluation We conduct a set of experiments with approximate n-gram counts (stream counts) produced by the stream algorithm." ></td>
	<td class="line x" title="99:229	We define various metrics on which we evaluate the quality of stream counts compared with exact n-gram counts (true counts)." ></td>
	<td class="line x" title="100:229	To 1We use hash tables to store tuples; however smarter data structures like suffix trees could also be used." ></td>
	<td class="line x" title="101:229	Corpus Gzip-MB M-wrds Perplexity EP 63 38 1122.69 afe 417 171 1829.57 apw 1213 540 1872.96 nyt 2104 914 1785.84 xie 320 132 1885.33 Table 3: Corpus Statistics and perplexity of LMs made with each of these corpuses on development set evaluate the quality of stream counts on these metrics, we carry out three experiments." ></td>
	<td class="line x" title="102:229	4.1 Experimental Setup The freely available English side of Europarl (EP) and Gigaword corpus (Graff, 2003) is used for computing n-gram counts." ></td>
	<td class="line x" title="103:229	We only use EP along with two sections of the Gigaword corpus: Agence France Press English Service(afe) and The New York Times Newswire Service (nyt)." ></td>
	<td class="line x" title="104:229	The unigram language models built using these corpuses yield better perplexity scores on the development set (see Section 5.1) compared to The Xinhua News Agency English Service (xie) and Associated Press Worldstream English Service (apw) as shown in Table 3." ></td>
	<td class="line x" title="105:229	The LMs are build using the SRILM language modelling toolkit (Stolcke, 2002) with modified KneserNey discounting and interpolation." ></td>
	<td class="line x" title="106:229	The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, consisting of 1.1 billion words." ></td>
	<td class="line x" title="107:229	4.2 Description of the metrics To evaluate the quality of counts produced by our stream algorithm four different metrics are used." ></td>
	<td class="line x" title="108:229	The accuracy metric measures the quality of top N stream counts by taking the fraction of top N stream counts that are contained in the top N true counts." ></td>
	<td class="line x" title="109:229	Accuracy = Stream Counts  True CountsTrue Counts Spearmans rank correlation coefficient or Spearmans rho() computes the difference between the ranks of each observation (i.e. n-gram) on two variables (that are top N stream and true counts)." ></td>
	<td class="line x" title="110:229	This measure captures how different the stream count ordering is from the true count ordering." ></td>
	<td class="line x" title="111:229	 = 1 6 summationtextd2 i N(N2 1) 515 di is the difference between the ranks of corresponding elements Xi and Yi; N is the number of elements found in both sets; Xi and Yi in our case denote the stream and true counts." ></td>
	<td class="line x" title="112:229	Mean square error (MSE) quantifies the amount by which a predicted value differs from the true value." ></td>
	<td class="line x" title="113:229	In our case, it estimates how different the stream counts are from the true counts." ></td>
	<td class="line x" title="114:229	MSE = 1N Nsummationdisplay i=1 (truei  predictedi)2 true and predicted denotes values of true and stream counts; N denotes the number of stream counts contained in true counts." ></td>
	<td class="line x" title="115:229	4.3 Varying  experiments In our first experiment, we use accuracy,  and MSE metrics for evaluation." ></td>
	<td class="line x" title="116:229	Here, we compute 5-gram stream counts with different settings of  on the EAN corpus." ></td>
	<td class="line x" title="117:229	 controls the number of stream counts produced by the algorithm." ></td>
	<td class="line x" title="118:229	The results in Table 4 support the theory that decreasing the value of  improves the quality of stream counts." ></td>
	<td class="line x" title="119:229	Also, as expected, the algorithm produces more stream counts with smaller values of ." ></td>
	<td class="line x" title="120:229	The evaluation of stream counts obtained with  = 50e-8 and 20e-8 reveal that the stream counts learned with this large value are more susceptible to errors." ></td>
	<td class="line x" title="121:229	If we look closely at the counts for  = 50e-8, we see that we get at least 30% of the stream counts from 245k true counts." ></td>
	<td class="line x" title="122:229	This number is not significantly worse than the 36% of stream counts obtained from 4,018k true counts for the smallest value of  = 5e-8." ></td>
	<td class="line x" title="123:229	However, if we look at the other two metrics, the ranking correlation  of stream counts compared with true counts on  = 50e-8 and 20e-8 is low compared to other  values." ></td>
	<td class="line x" title="124:229	For the MSE, the error with stream counts on these m values is again high compared to other values." ></td>
	<td class="line x" title="125:229	As we decrease the value of  we continually get better results: decreasing  pushes the stream counts towards the true counts." ></td>
	<td class="line x" title="126:229	However, using a smaller  increases the memory usage." ></td>
	<td class="line x" title="127:229	Looking at the evaluation, it is therefore advisable to use 5-gram stream counts produced with at most  10e-7 for the EAN corpus." ></td>
	<td class="line x" title="128:229	Since it is not possible to compute true 7-grams counts on EAN with available computing resources,  5-gram Acc  MSEproduced 50e-8 245k 0.294 -3.6097 0.4954 20e-8 726k 0.326 -2.6517 0.1155 10e-8 1655k 0.352 -1.9960 0.0368 5e-8 4018k 0.359 -1.7835 0.0114 Table 4: Evaluating quality of 5-gram stream counts for different settings of  on EAN corpus  7-gram Acc  MSEproduced 50e-8 44k 0.509 0.3230 0.0341 20e-8 128k 0.596 0.5459 0.0063 10e-8 246k 0.689 0.7413 0.0018 5e-8 567k 0.810 0.8599 0.0004 Table 5: Evaluating quality of 7-gram stream counts for different settings of  on EP corpus we carry out a similar experiment for 7-grams on EP to verify the results for higher order n-grams 2." ></td>
	<td class="line x" title="129:229	The results in Table 5 tell a story similar to our results for 7-grams." ></td>
	<td class="line x" title="130:229	The size of EP corpus is much smaller than EAN and so we see even better results on each of the metrics with decreasing the value of ." ></td>
	<td class="line x" title="131:229	The overall trend remains the same; here too, setting   10e8 is the most effective strategy." ></td>
	<td class="line x" title="132:229	The fact that these results are consistent across two datasets of different sizes and different n-gram sizes suggests that they will carry over to other tasks." ></td>
	<td class="line x" title="133:229	4.4 Varying top K experiments In the second experiment, we evaluate the quality of the top K (sorted by frequency) 5-gram stream counts." ></td>
	<td class="line x" title="134:229	Here again, we use accuracy,  and MSE for evaluation." ></td>
	<td class="line x" title="135:229	We fix the value of  to 5e-8 and compute 5-gram stream counts on the EAN corpus." ></td>
	<td class="line x" title="136:229	We vary the value of K between 100k and 4,018k (i.e all the n-gram counts produced by the stream algorithm)." ></td>
	<td class="line x" title="137:229	The experimental results in Table 6 support the theory that stream count algorithm computes the exact count of most of the high frequency n-grams." ></td>
	<td class="line x" title="138:229	Looking closer, we see that if we evaluate the algorithm on just the top 100k 5-grams (roughly 5% of all 5-grams produced), we see almost perfect results." ></td>
	<td class="line x" title="139:229	Further, if we take the top 1,000k 5-grams (approximately 25% of all 5-grams) we again see excellent 2Similar evaluation scores are observed for 9-gram stream counts with different values of  on EP corpus." ></td>
	<td class="line x" title="140:229	516 Top K Accuracy  MSE 100k 0.994 0.9994 0.01266 500k 0.934 0.9795 0.0105 1000k 0.723 0.8847 0.0143 2000k 0.504 0.2868 0.0137 4018k 0.359 -1.7835 0.0114 Table 6: Evaluating top K sorted 5-gram stream counts for =5e-8 on EAN corpus performance on all metrics." ></td>
	<td class="line x" title="141:229	The accuracy of the results decrease slightly, but the  and MSE metrics are not decreased that much in comparison." ></td>
	<td class="line x" title="142:229	Performance starts to degrade as we get to 2,000k (over 50% of all 5-grams), a result that is not too surprising." ></td>
	<td class="line x" title="143:229	However, even here we note that the MSE is low, suggesting that the frequencies of stream counts (found in top K true counts) are very close to the true counts." ></td>
	<td class="line x" title="144:229	Thus, we conclude that the quality of the 5-gram stream counts produced for this value of  is quite high (in relation to the true counts)." ></td>
	<td class="line x" title="145:229	As before, we corroborate our results with higher order n-grams." ></td>
	<td class="line x" title="146:229	We evaluate the quality of top K 7gram stream counts on EP.3 Since EP is a smaller corpus, we evaluate the stream counts produced by setting  to 10e-8." ></td>
	<td class="line x" title="147:229	Here we vary the value of K between 10k and 246k (the total number produced by the stream algorithm)." ></td>
	<td class="line x" title="148:229	Results are shown in Table 7." ></td>
	<td class="line x" title="149:229	As we saw earlier with 5-grams, the top 10k (i.e. approximately 5% of all 7-grams) are of very high quality." ></td>
	<td class="line x" title="150:229	Results, and this remains true even when we increase K to 100k." ></td>
	<td class="line x" title="151:229	There is a drop in the accuracy and a slight drop in , while the MSE remains the same." ></td>
	<td class="line x" title="152:229	Taking all counts again shows a significant decrease in both accuracy and  scores, but this does not affect MSE scores significantly." ></td>
	<td class="line x" title="153:229	Hence, the 7-gram stream counts i.e. 246k counts produced by  = 10e-8 are quite accurate when compared to the top 246k true counts." ></td>
	<td class="line x" title="154:229	4.5 Analysis of tradeoff between coverage and space In our third experiment, we investigate whether a large LM can help MT performance." ></td>
	<td class="line x" title="155:229	We evaluate the coverage of stream counts built on the EAN corpus on the test data for SMT experiments (see Sec3Similar evaluation scores are observed for different top K sorted 9-gram stream counts with =10e-8 on EP corpus." ></td>
	<td class="line x" title="156:229	Top K Accuracy  MSE 10k 0.996 0.9997 0.0015 20k 0.989 0.9986 0.0016 50k 0.950 0.9876 0.0016 100k 0.876 0.9493 0.0017 246k 0.689 0.7413 0.0018 Table 7: Evaluating top K sorted 7-gram stream counts for =10e-8 on EP corpus tion 5.1) with different values of m. We compute the recall of each model against 3071 sentences of test data where recall is the fraction of number of n-grams of a dataset found in stream counts." ></td>
	<td class="line x" title="157:229	Recall = Number of n-grams found in stream countsNumber of n-grams in dataset We build unigram, bigram, trigram, 5-gram and 7-gram with four different values of ." ></td>
	<td class="line x" title="158:229	Table 8 contains the gzip size of the count file and the recall of various different stream count n-grams." ></td>
	<td class="line x" title="159:229	As expected, the recall with respect to true counts is maximum for unigrams, bigrams, trigrams and 5-grams." ></td>
	<td class="line x" title="160:229	However the amount of space required to store all true counts in comparison to stream counts is extremely high: we need 4.8GB of compressed space to store all the true counts for 5-grams." ></td>
	<td class="line x" title="161:229	For unigram models, we see that the recall scores are good for all values of ." ></td>
	<td class="line x" title="162:229	If we compare the approximate stream counts produced by largest  (which is worst) to all true counts, we see that the stream counts compressed size is 50 times smaller than the true counts size, and is only three points worse in recall." ></td>
	<td class="line x" title="163:229	Similar trends hold for bigrams, although the loss in recall is higher." ></td>
	<td class="line x" title="164:229	As with unigrams, the loss in recall is more than made up for by the memory savings (a factor of nearly 150)." ></td>
	<td class="line x" title="165:229	For trigrams, we see a 14 point loss in recall for the smallest , but a memory savings of 400 times." ></td>
	<td class="line x" title="166:229	For 5-grams, the best recall value is .020 (1.2k out of 60k 5-gram stream counts are found in the test set)." ></td>
	<td class="line x" title="167:229	However, compared with the true counts we only loss a recall of 0.05 (4.3k out of 60k) points but memory savings of 150 times." ></td>
	<td class="line x" title="168:229	In extrinsic evaluations, we will show that integrating 5-gram stream counts with an SMT system performs slightly worse than the true counts, while dramatically reducing the memory usage." ></td>
	<td class="line x" title="169:229	517 N-gram unigram bigram trigram 5-gram 7-gram  Gzip Recall Gzip Recall Gzip Recall Gzip Recall Gzip RecallMB MB MB MB MB 50e-8 .352 .785 2.3 .459 3.3 .167 1.9 .006 .864 5.6e-5 20e-8 .568 .788 4.5 .494 7.6 .207 5.3 .011 2.7 1.3e-4 10e-8 .824 .791 7.6 .518 15 .237 13 .015 9.7 4.1e-4 5e-8 1.3 .794 13 .536 30 .267 31 .020 43 5.9e-4 all 17 .816 228 .596 1200 .406 4800 .072 NA Table 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different m For 7-gram we can not compute the true n-gram counts due to limitations of available computational resources." ></td>
	<td class="line x" title="170:229	The memory requirements with smallest value of  are similar to those of 5-gram, but the recall values are quite small." ></td>
	<td class="line x" title="171:229	For 7-grams, the best recall value is 5.9e-4 which means that stream counts contains only 32 out of 54k 7-grams contained in test set." ></td>
	<td class="line x" title="172:229	The small recall value for 7-grams suggests that these counts may not be that useful in SMT." ></td>
	<td class="line x" title="173:229	We further substantiate our findings in our extrinsic evaluations." ></td>
	<td class="line x" title="174:229	There we show that integrating 7-gram stream counts with an SMT system does not affect its overall performance significantly." ></td>
	<td class="line x" title="175:229	5 Extrinsic Evaluation 5.1 Experimental Setup All the experiments conducted here make use of publicly available resources." ></td>
	<td class="line x" title="176:229	Europarl (EP) corpus French-English section is used as parallel data." ></td>
	<td class="line x" title="177:229	The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007)." ></td>
	<td class="line x" title="178:229	The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set." ></td>
	<td class="line x" title="179:229	Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality." ></td>
	<td class="line oc" title="180:229	The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores." ></td>
	<td class="line x" title="181:229	The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4http://www.statmt.org/moses/ 5http://www.statmt.org/wmt07/ 6We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact." ></td>
	<td class="line x" title="182:229	This suggests that for SMT performance, more data 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities." ></td>
	<td class="line x" title="183:229	We can either turn these counts into conditional probabilities (by using SRILM) or use the counts directly." ></td>
	<td class="line x" title="184:229	We observed no significant difference in performance between these two approaches." ></td>
	<td class="line x" title="185:229	However, using the counts directly consumes significantly less memory at run-time and is therefore preferable." ></td>
	<td class="line x" title="186:229	Due to space constraints, SRILM results are omitted." ></td>
	<td class="line x" title="187:229	The only remaining open question is: how should we turn the counts into a feature that can be used in an SMT system?" ></td>
	<td class="line x" title="188:229	We considered several alternatives; the most successful was a simple weighted count of n-gram matches of varying size, appropriately backed-off." ></td>
	<td class="line x" title="189:229	Specifically, consider an n-gram model." ></td>
	<td class="line x" title="190:229	For every sequence of words wi,,wi+N1, we obtain a feature score computed recursively according to Eq (2)." ></td>
	<td class="line x" title="191:229	f(wi) = log C(w i) Z  (2) f(wi,,wi+k) = log C(w i,,wi+k) Z  + 12f(wi+1,,wi+k) Here, 12 is the backoff factor and Z is the largest count in the count set (the presence of Z is simply to ensure that these values remain manageable)." ></td>
	<td class="line x" title="192:229	In order to efficiently compute these features, we store the counts in a suffix-tree." ></td>
	<td class="line x" title="193:229	The computation proceeds by first considering wi+N1 alone and then expanding to consider the bigram, then trigram and so on." ></td>
	<td class="line x" title="194:229	The advantage to this order of computation is that the recursive calls can cease whenever a is better only if it comes from the right domain." ></td>
	<td class="line x" title="195:229	518 n-gram() BLEU NIST MET MemGB 3 EP(exact) 25.57 7.300 54.48 2.7 5 EP(exact) 25.79 7.286 54.44 2.9 3 EAN(exact) 27.04 7.428 55.07 4.6 5 EAN(exact) 28.73 7.691 56.32 20.5 4(10e-8) 27.36 7.506 56.19 2.7 4(5e-8) 27.40 7.507 55.90 2.8 5(10e-8) 27.97 7.605 55.52 2.8 5(5e-8) 27.98 7.611 56.07 2.8 7(10e-8) 27.97 7.590 55.88 2.9 7(5e-8) 27.88 7.577 56.01 2.9 9(10e-8) 28.18 7.611 55.95 2.9 9(5e-8) 27.98 7.608 56.08 2.9 Table 9: Evaluating SMT with different LMs on EAN." ></td>
	<td class="line x" title="196:229	Results are according to BLEU, NIST and MET metrics." ></td>
	<td class="line x" title="197:229	Bold #s are not statistically significant worse than exact." ></td>
	<td class="line x" title="198:229	zero count is reached." ></td>
	<td class="line x" title="199:229	(Extending Moses to include this required only about 100 lines of code.)" ></td>
	<td class="line x" title="200:229	5.3 Results Table 9 summarizes SMT results." ></td>
	<td class="line x" title="201:229	We have 4 baseline LMs that are conventional LMs smoothed using modified Kneser-Ney smoothing." ></td>
	<td class="line x" title="202:229	The first two trigram and 5-gram LMs are built on EP corpus and the other two are built on EAN corpus." ></td>
	<td class="line x" title="203:229	Table 9 show that there is not much significant difference in SMT results of 5-gram and trigram LM on EP." ></td>
	<td class="line x" title="204:229	As expected, the trigram built on the large corpus EAN gets an improvement of 1.5 Bleu Score." ></td>
	<td class="line x" title="205:229	However, unlike the EP corpus, building a 5-gram LM on EAN (huge corpus) gets an improvement of 3.2 Bleu Score." ></td>
	<td class="line o" title="206:229	(The 95% statistical significance boundary is about  0.53 Bleu on the test data, 0.077 Nist and 0.16 Meteor according to bootstrap resampling) We see similar gains in Nist and Meteor metrics as shown in Table 9." ></td>
	<td class="line x" title="207:229	We use stream counts computed with two values of , 5e-8 and 10e-8 on EAN corpus." ></td>
	<td class="line x" title="208:229	We use all the stream counts produced by the algorithm." ></td>
	<td class="line x" title="209:229	4, 5, 7 and 9 order n-gram stream counts are computed with these settings of ." ></td>
	<td class="line x" title="210:229	These counts are used along with a trigram LM built on EP to improve SMT performance." ></td>
	<td class="line x" title="211:229	The memory usage (Mem) shown in Table 9 is the full memory size required to run on the test data (including phrase tables)." ></td>
	<td class="line x" title="212:229	Adding 4-gram and 5-gram stream counts as feature helps the most." ></td>
	<td class="line x" title="213:229	The performance gain by using 5-gram stream counts is slightly worse than compared to true 5-gram LM on EAN." ></td>
	<td class="line x" title="214:229	However, using 5-gram stream counts directly is more memory efficient." ></td>
	<td class="line x" title="215:229	Also, the gains for stream counts are exactly the same as we saw for same sized countbased and entropy-based pruning counts in Table 1 and 2 respectively." ></td>
	<td class="line x" title="216:229	Moreover, unlike the pruning methods, our algorithm directly computes a small model, as opposed to compressing a pre-computed large model." ></td>
	<td class="line x" title="217:229	Adding 7-gram and 9-gram does not help significantly, a fact anticipated by the low recall of 7-grambased counts that we saw in Section 4.5." ></td>
	<td class="line x" title="218:229	The results with two different settings of  are largely the same." ></td>
	<td class="line x" title="219:229	This validates our intrinsic evaluation results in Section 4.3 that stream counts learned using   10e-8 are of good quality, and that the quality of the stream counts is high." ></td>
	<td class="line x" title="220:229	6 Conclusion We have proposed an efficient, low-memory method to construct high-order approximate n-gram LMs." ></td>
	<td class="line x" title="221:229	Our method easily scales to billion-word monolingual corpora on conventional (8GB) desktop machines." ></td>
	<td class="line x" title="222:229	We have demonstrated that approximate ngram features could be used as a direct replacement for conventional higher order LMs in SMT with significant reductions in memory usage." ></td>
	<td class="line x" title="223:229	In future, we will be looking into building streaming skip ngrams, and other variants (like cluster n-grams)." ></td>
	<td class="line x" title="224:229	In NLP community, it has been shown that having more data results in better performance (Ravichandran et al., 2005; Brants et al., 2007; Turney, 2008)." ></td>
	<td class="line x" title="225:229	At web scale, we have terabytes of data and that can capture broader knowledge." ></td>
	<td class="line x" title="226:229	Streaming algorithm paradigm provides a memory and space-efficient platform to deal with terabytes of data." ></td>
	<td class="line x" title="227:229	We hope that other NLP applications (where we need to compute relative frequencies) like noun-clustering, constructing syntactic rules for SMT, finding analogies, and others can also benefit from streaming methods." ></td>
	<td class="line x" title="228:229	We also believe that stream counts can be applied to other problems involving higher order LMs such as speech recognition, information extraction, spelling correction and text generation." ></td>
	<td class="line x" title="229:229	519" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1022
DEPEVAL(summ): Dependency-based Evaluation for Automatic Summaries
Owczarzak, Karolina;"></td>
	<td class="line x" title="1:180	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 190198, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:180	c2009 ACL and AFNLP DEPEVAL(summ): Dependency-based Evaluation for Automatic Summaries Karolina Owczarzak Information Access Division National Institute of Standards and Technology Gaithersburg, MD 20899 karolina.owczarzak@nist.gov Abstract This paper presents DEPEVAL(summ), a dependency-based metric for automatic evaluation of summaries." ></td>
	<td class="line x" title="3:180	Using a reranking parser and a Lexical-Functional Grammar (LFG) annotation, we produce a set of dependency triples for each summary." ></td>
	<td class="line x" title="4:180	The dependency set for each candidate summary is then automatically compared against dependencies generated from model summaries." ></td>
	<td class="line x" title="5:180	We examine a number of variations of the method, including the addition of WordNet, partial matching, or removing relation labels from the dependencies." ></td>
	<td class="line x" title="6:180	In a test on TAC 2008 and DUC 2007 data, DEPEVAL(summ) achieves comparable or higher correlations with human judgments than the popular evaluation metrics ROUGE and Basic Elements (BE)." ></td>
	<td class="line x" title="7:180	1 Introduction Evaluation is a crucial component in the area of automatic summarization; it is used both to rank multiple participant systems in shared summarization tasks, such as the Summarization track at Text Analysis Conference (TAC) 2008 and its Document Understanding Conference (DUC) predecessors, and to provide feedback to developers whose goal is to improve their summarization systems." ></td>
	<td class="line x" title="8:180	However, manual evaluation of a large number of documents necessary for a relatively unbiased view is often unfeasible, especially in the contexts where repeated evaluations are needed." ></td>
	<td class="line x" title="9:180	Therefore, there is a great need for reliable automatic metrics that can perform evaluation in a fast and consistent manner." ></td>
	<td class="line x" title="10:180	In this paper, we explore one such evaluation metric, DEPEVAL(summ), based on the comparison of Lexical-Functional Grammar (LFG) dependencies between a candidate summary and one or more model (reference) summaries." ></td>
	<td class="line x" title="11:180	The method is similar in nature to Basic Elements (Hovy et al., 2005), in that it extends beyond a simple string comparison of word sequences, reaching instead to a deeper linguistic analysis of the text." ></td>
	<td class="line x" title="12:180	Both methods use hand-written extraction rules to derive dependencies from constituent parses produced by widely available Penn II Treebank parsers." ></td>
	<td class="line x" title="13:180	The difference between DEPEVAL(summ) and BE is that in DEPEVAL(summ) the dependency extraction is accomplished through an LFG annotation of Cahill et al.(2004) applied to the output of the reranking parser of Charniak and Johnson (2005), whereas in BE (in the version presented here) dependencies are generated by the Minipar parser (Lin, 1995)." ></td>
	<td class="line x" title="15:180	Despite relying on a the same concept, our approach outperforms BE in most comparisons, and it often achieves higher correlations with human judgments than the string-matching metric ROUGE (Lin, 2004)." ></td>
	<td class="line x" title="16:180	A more detailed description of BE and ROUGE is presented in Section 2, which also gives an account of manual evaluation methods employed at TAC 2008." ></td>
	<td class="line x" title="17:180	Section 3 gives a short introduction to the LFG annotation." ></td>
	<td class="line x" title="18:180	Section 4 describes in more detail DEPEVAL(summ) and its variants." ></td>
	<td class="line x" title="19:180	Section 5 presents the experiment in which we compared the perfomance of all three metrics on the TAC 2008 data (consisting of 5,952 100-words summaries) and on the DUC 2007 data (1,620 250-word summaries) and discusses the correlations these metrics achieve." ></td>
	<td class="line x" title="20:180	Finally, Section 6 presents conclusions and some directions for future work." ></td>
	<td class="line x" title="21:180	2 Current practice in summary evaluation In the first Text Analysis Conference (TAC 2008), as well as its predecessor, the Document Understanding Conference (DUC) series, the evaluation 190 of summarization tasks was conducted using both manual and automatic methods." ></td>
	<td class="line x" title="22:180	Since manual evaluation is still the undisputed gold standard, both at TAC and DUC there was much effort to evaluate manually as much data as possible." ></td>
	<td class="line x" title="23:180	2.1 Manual evaluation Manual assessment, performed by human judges, usually centers around two main aspects of summary quality: content and form." ></td>
	<td class="line x" title="24:180	Similarly to MachineTranslation,wherethesetwoaspectsarerepresented by the categories of Accuracy and Fluency, in automatic summarization evaluation performed at TAC and DUC they surface as (Content) Responsiveness and Readability." ></td>
	<td class="line x" title="25:180	In TAC 2008 (Dang and Owczarzak, 2008), however, Content Responsiveness was replaced by Overall Responsiveness, conflating these two dimensions and reflecting the overall quality of the summary: the degree to which a summary was responding to the information need contained in the topic statement, as well as its linguistic quality." ></td>
	<td class="line x" title="26:180	A separate Readability score was still provided, assessingthefluencyandstructureindependentlyofcontent,basedonsuchaspectsasgrammaticality,nonredundancy, referential clarity, focus, structure, and coherence." ></td>
	<td class="line x" title="27:180	Both Overall Responsiveness and Readability were evaluated according to a fivepoint scale, ranging from Very Poor to Very Good." ></td>
	<td class="line x" title="28:180	ContentwasevaluatedmanuallybyNISTassessors using the Pyramid framework (Passonneau et al., 2005)." ></td>
	<td class="line x" title="29:180	In the Pyramid evaluation, assessors first extract all possible information nuggets, or Summary Content Units (SCUs) from the four human-crafted model summaries on a given topic." ></td>
	<td class="line x" title="30:180	EachSCUisassignedaweightinproportiontothe number of model summaries in which it appears, on the assumption that information which appears in most or all human-produced model summaries is more essential to the topic." ></td>
	<td class="line x" title="31:180	Once all SCUs are harvested from the model summaries, assessors determine how many of these SCUs are present in each of the automatic peer summaries." ></td>
	<td class="line x" title="32:180	The final score for an automatic summary is its total SCUweightdividedbythemaximumSCUweight available to a summary of average length (where the average length is determined by the mean SCU count of the model summaries for this topic)." ></td>
	<td class="line x" title="33:180	All types of manual assessment are expensive and time-consuming, which is why it can be rarely provided for all submitted runs in shared tasks such as the TAC Summarization track." ></td>
	<td class="line x" title="34:180	It is also not a viable tool for system developers who ideally would like a fast, reliable, and above all automatic evaluation method that can be used to improve their systems." ></td>
	<td class="line x" title="35:180	The creation and testing of automatic evaluation methods is, therefore, an important research venue, and the goal is to produce automatic metrics that will correlate with manual assessment as closely as possible." ></td>
	<td class="line x" title="36:180	2.2 Automatic evaluation Automatic metrics, because of their relative speed, can be applied more widely than manual evaluation." ></td>
	<td class="line x" title="37:180	In TAC 2008 Summarization track, all submitted runs were scored with the ROUGE (Lin, 2004) and Basic Elements (BE) metrics (Hovy et al., 2005)." ></td>
	<td class="line x" title="38:180	ROUGE is a collection of string-comparison techniques, based on matching n-grams between a candidate string and a reference string." ></td>
	<td class="line x" title="39:180	The string in question might be a single sentence (as in the case of translation), or a set of sentences (as in the case of summaries)." ></td>
	<td class="line x" title="40:180	The variations of ROUGE range from matching unigrams (i.e. single words) to matching four-grams, with or without lemmatization and stopwords, with the options of using different weights or skip-n-grams (i.e. matchingn-gramsdespiteinterveningwords)." ></td>
	<td class="line x" title="41:180	The two versions used in TAC 2008 evaluations were ROUGE-2 and ROUGE-SU4, where ROUGE-2 calculates the proportion of matching bigrams between the candidate summary and the reference summaries, and ROUGE-SU4 is a combination of unigram match and skip-bigram match with skip distance of 4 words." ></td>
	<td class="line x" title="42:180	BE, on the other hand, employs a certain degree of linguistic analysis in the assessment process,asitrestsoncomparingtheBasicElements between the candidate and the reference." ></td>
	<td class="line x" title="43:180	Basic Elements are syntactic in nature, and comprise the heads of major syntactic constituents in the text (noun, verb, adjective, etc.) and their modifiers in a dependency relation, expressed as a triple (head, modifier, relation type)." ></td>
	<td class="line x" title="44:180	First, the input text is parsed with a syntactic parser, then Basic Elements are extracted from the resulting parse, and the candidate BEs are matched against the reference BEs." ></td>
	<td class="line x" title="45:180	In TAC 2008 and DUC 2008 evaluations the BEs were extracted with Minipar (Lin, 1995)." ></td>
	<td class="line x" title="46:180	Since BE, contrary to ROUGE, does not 191 rely solely on the surface sequence of words to determine similarity between summaries, but delves intowhatcouldbecalledashallowsemanticstructure,comprisingthematicrolessuchassubjectand object, it is likely to notice identity of meaning where such identity is obscured by variations in word order." ></td>
	<td class="line x" title="47:180	In fact, when it comes to evaluation of automatic summaries, BE shows higher correlations with human judgments than ROUGE, although the difference is not large enough to be statistically significant." ></td>
	<td class="line x" title="48:180	In the TAC 2008 evaluations, BE-HM (a version of BE where the words are stemmed and the relation type is ignored) obtained a correlation of 0.911 with human assessment of overall responsiveness and 0.949 with the Pyramid score, whereas ROUGE-2 showed correlations of 0.894 and 0.946, respectively." ></td>
	<td class="line x" title="49:180	While using dependency information is an important step towards integrating linguistic knowledge into the evaluation process, there are many ways in which this could be approached." ></td>
	<td class="line x" title="50:180	Since this type of evaluation processes information in stages (constituent parser, dependency extraction, and the method of dependency matching between a candidate and a reference), there is potential for variance in performance among dependencybased evaluation metrics that use different components." ></td>
	<td class="line x" title="51:180	Therefore, it is interesting to compare our method, which relies on the Charniak-Johnson parser and the LFG annotation, with BE, which uses Minipar to parse the input and produce dependencies." ></td>
	<td class="line x" title="52:180	3 Lexical-Functional Grammar and the LFG parser The method discussed in this paper rests on the assumptions of Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) (LFG)." ></td>
	<td class="line x" title="53:180	In LFG sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure." ></td>
	<td class="line x" title="54:180	C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of trees." ></td>
	<td class="line x" title="55:180	F-structures are recursive feature structures, representing abstract grammatical relations such as subject, object, oblique, adjunct, etc., approximating to predicateargument structure or simple logical forms." ></td>
	<td class="line x" title="56:180	Cstructure and f-structure are related by means of functional annotations in c-structure trees, which describe f-structures." ></td>
	<td class="line x" title="57:180	While c-structure is sensitive to surface rearrangement of constituents, f-structure abstracts away from (some of) the particulars of surface realization." ></td>
	<td class="line x" title="58:180	The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures." ></td>
	<td class="line x" title="59:180	The f-structure can also be described in terms of a flat set of triples, or dependencies." ></td>
	<td class="line x" title="60:180	In triples format, the f-structure for these two sentences is represented in 1." ></td>
	<td class="line x" title="61:180	(1) subject(resign,john) person(john,3) number(john,sg) tense(resign,past) adjunct(resign,yesterday) person(yesterday,3) number(yesterday,sg) Cahill et al.(2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups: a group of predicate-only dependencies and nonpredicate dependencies." ></td>
	<td class="line x" title="63:180	Predicate-only dependencies are those whose path ends in a predicatevalue pair, describing grammatical relations." ></td>
	<td class="line x" title="64:180	For instance, in the sentence John resigned yesterday, predicate-only dependencies would include: subject(resign, john) and adjunct(resign, yesterday), while non-predicate dependencies are person(john,3), number(john,sg), tense(resign,past), person(yesterday,3), num(yesterday,sg)." ></td>
	<td class="line x" title="65:180	Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, and relative clause pronoun." ></td>
	<td class="line x" title="66:180	The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, verbal particle, participle, passive, pronoun surface form, and infinitival clause." ></td>
	<td class="line x" title="67:180	These 32 dependencies, produced by LFG annotation, and the overlap between the set of dependencies derived from the candidate summary and the reference summaries, form the basis of our evaluation method, which we present in Section 4." ></td>
	<td class="line x" title="68:180	First, a summary is parsed with the CharniakJohnson reranking parser (Charniak and Johnson, 2005) to obtain the phrase-structure tree." ></td>
	<td class="line x" title="69:180	Then, a sequence of scripts annotates the output, translating the relative phrase position into f-structural dependencies." ></td>
	<td class="line x" title="70:180	The treebank-based LFG annotation used in this paper and developed by Cahill et al.(2004) obtains high precision and recall rates." ></td>
	<td class="line x" title="72:180	As reported in Cahill et al.(2008), the version of 192 the LFG parser which applies the LFG annotation algorithm to the earlier Charniaks parser (Charniak, 2000) obtains an f-score of 86.97 on the Wall Street Journal Section 23 test set." ></td>
	<td class="line x" title="74:180	The LFG parser is robust as well, with coverage levels exceeding 99.9%, measured in terms of complete spanning parse." ></td>
	<td class="line x" title="75:180	4 Dependency-based evaluation Our dependency-based evaluation method, similarly to BE, compares two unordered sets of dependencies: one bag contains dependencies harvested from the candidate summary and the other contains dependencies from one or more reference summaries." ></td>
	<td class="line x" title="76:180	Overlap between the candidate bag and the reference bag is calculated in the form of precision, recall, and the f-measure (with precision and recall equally weighted)." ></td>
	<td class="line x" title="77:180	Since for ROUGE and BE the only reported score is recall, we present recall results here as well, calculated as in 2: (2) DEPEVAL(summ) Recall = |Dcand||Dref||Dref| where Dcand are the candidate dependencies andDref are the reference dependencies." ></td>
	<td class="line x" title="78:180	The dependency-based method using LFG annotation has been successfully employed in the evaluation of Machine Translation (MT)." ></td>
	<td class="line nc" title="79:180	In Owczarzak (2008), the method achieves equal or higher correlations with human judgments than METEOR (Banerjee and Lavie, 2005), one of the best-performingautomaticMTevaluationmetrics." ></td>
	<td class="line x" title="80:180	However, it is not clear that the method can be applied without change to the task of assessing automatic summaries; after all, the two tasks of summarization and translation produce outputs that are different in nature." ></td>
	<td class="line x" title="81:180	In MT, the unit of text is a sentence; text is translated, and the translation evaluated, sentence by sentence." ></td>
	<td class="line x" title="82:180	In automatic summarization, the output unit is a summary with length varying depending on task, but which most often consists of at least several sentences." ></td>
	<td class="line x" title="83:180	This has bearing on the matching process: with several sentences on the candidate and reference side each, there is increased possibility of trivial matches, such as dependencies containing function words, which might inflate the summary score even in the absence of important content." ></td>
	<td class="line x" title="84:180	This is particularly likely if we were to employ partial matching for dependencies." ></td>
	<td class="line x" title="85:180	Partial matching (indicated in the result tables with the tag pm) splits each predicate dependency into two, replacing one or the other element with a variable, e.g. for the dependency subject(resign, John) we would obtain two partial dependencies subject(resign, x) and subject(x, John)." ></td>
	<td class="line x" title="86:180	This process helps circumvent some of the syntactic and lexical variation between a candidate and a reference, and it proved very useful in MT evaluation (Owczarzak, 2008)." ></td>
	<td class="line x" title="87:180	In summary evaluation, as will be shown in Section 5, it leads to higher correlations with human judgments only in the case of human-produced model summaries, because almost any variation between two model summaries is legal, i.e. either a paraphrase or another, but equally relevant, piece of information." ></td>
	<td class="line x" title="88:180	For automatic summaries, which are of relatively poor quality, partial matching lowers our methods ability to reflect human judgment, because it results in overly generous matching in situations where the examined information is neither a paraphrase nor relevant." ></td>
	<td class="line x" title="89:180	Similarly, evaluating a summary against the union of all references, as we do in the baseline version of our method, increases the pool of possible matches, but may also produce score inflation through matching repetitive information across models." ></td>
	<td class="line x" title="90:180	To deal with this, we produce a version of the score (marked in the result tables with the tag one) that counts only one hit for every dependency match, independent of how many instances of a given dependency are present in the comparison." ></td>
	<td class="line x" title="91:180	The use of WordNet1 module (Rennie, 2000) did not provide a great advantage (see results tagged with wn), and sometimes even lowered our correlations, especially in evaluation of automatic systems." ></td>
	<td class="line x" title="92:180	This makes sense if we take into consideration that WordNet lists all possible synonyms for all possible senses of a word, and so, given a great number of cross-sentence comparisons in multi-sentence summaries, there is an increased risk of spurious matches between words which, despite being potentially synonymous in certain contexts, are not equivalent in the text." ></td>
	<td class="line x" title="93:180	Another area of concern was the potential noise introduced by the parser and the annotation process." ></td>
	<td class="line x" title="94:180	Due to parsing errors, two otherwise equivalent expressions might be encoded as differing sets of dependencies." ></td>
	<td class="line x" title="95:180	In MT evaluation, the dependency-based method can alleviate parser 1http://wordnet.princeton.edu/ 193 noise by comparing n-best parses for the candidate andthereference(Owczarzaketal.,2007),butthis is not an efficient solution for comparing multisentence summaries." ></td>
	<td class="line x" title="96:180	We have therefore attempted to at least partially counteract this issue by removing relation labels from the dependencies (i.e. producing dependencies of the form (resign, John) instead of subject(resign, John)), which did provide someimprovement(seeresultstaggedwithnorel)." ></td>
	<td class="line x" title="97:180	Finally, we experimented with a predicate-only version of the evaluation, where only the predicate dependencies participate in the comparison, excluding dependencies that provide purely grammatical information such as person, tense, or number (tagged in the results table as pred)." ></td>
	<td class="line x" title="98:180	This move proved beneficial only in the case of system summaries, perhaps by decreasing the number of trivial matches, but decreased the methods correlation for model summaries, where such detailed information might be necessary to assess the degree of similarity between two human summaries." ></td>
	<td class="line x" title="99:180	5 Experimental results The first question we have to ask is: which of the manual evaluation categories do we want our metric to imitate?" ></td>
	<td class="line x" title="100:180	It is unlikely that a single automatic measure will be able to correctly reflect both Readability and Content Responsiveness, as form and content are separate qualities and need different measures." ></td>
	<td class="line x" title="101:180	Content seems to be the more important aspect, especially given that Readability can be partially derived from Responsiveness (a summary high in content cannot be very low in readability, although some very readable summaries can have little relevant content)." ></td>
	<td class="line x" title="102:180	Content Responsiveness was provided in DUC 2007 data, but not in TAC 2008, where the extrinsic Pyramid measure was used to evaluate content." ></td>
	<td class="line x" title="103:180	It is, in fact, preferable to compare our metric against thePyramidscoreratherthanContentResponsiveness, because both the Pyramid and our method aim to measure the degree of similarity between a candidate and a model, whereas Content Responsiveness is a direct assessment of whether the summarys content is adequate given a topic and a source text." ></td>
	<td class="line x" title="104:180	The Pyramid is, at the same time, a costly manual evaluation method, so an automatic metric that successfully emulates it would be a useful replacement." ></td>
	<td class="line x" title="105:180	Another question is whether we focus on system-level or summary-level evaluation." ></td>
	<td class="line x" title="106:180	The correlation values at the summary-level are generally much lower than on the system-level, which means the metrics are better at evaluating system performance than the quality of individual summaries." ></td>
	<td class="line x" title="107:180	System-level evaluations are essential to shared summarization tasks; summary-level assessment might be useful to developers who want to test the effect of particular improvements in their system." ></td>
	<td class="line x" title="108:180	Of course, the ideal evaluation metric would show high correlations with human judgment on both levels." ></td>
	<td class="line x" title="109:180	We used the data from the TAC 2008 and DUC 2007 Summarization tracks." ></td>
	<td class="line x" title="110:180	The first set comprised 58 system submissions and 4 humanproduced model summaries for each of the 96 subtopics (there were 48 topics, each of which required two summaries: a main and an update summary), as well as human-produced Overall Responsiveness and Pyramid scores for each summary." ></td>
	<td class="line x" title="111:180	The second set included 32 system submissions and 4 human models for each of the 45 topics." ></td>
	<td class="line x" title="112:180	For fair comparison of models and systems, we used jackknifing: while each model was evaluated against the remaining three models, each system summary was evaluated four times, each time against a different set of three models, and the four scores were averaged." ></td>
	<td class="line x" title="113:180	5.1 System-level correlations Table 1 presents system-level Pearsons correlations between the scores provided by our dependency-based metric DEPEVAL(summ), as well as the automatic metrics ROUGE-2, ROUGE-SU4, and BE-HM used in the TAC evaluation, and the manual Pyramid scores, which measured the content quality of the systems." ></td>
	<td class="line x" title="114:180	It also includes correlations with the manual Overall Responsiveness score, which reflected both content and linguistic quality." ></td>
	<td class="line x" title="115:180	Table 3 shows the correlations with Content Responsiveness for DUC 2007 data for ROUGE, BE, and those few select versions of DEPEVAL(summ) which achieve optimal results on TAC 2008 data (for a more detailed discussion of the selection see Section 6)." ></td>
	<td class="line x" title="116:180	The correlations are listed for the following versions of our method: pm partial matching for dependencies; wn WordNet; pred matching predicate-only dependencies; norel ignoring dependency relation label; one counting a match only once irrespective of how many instances of 194 TAC 2008 Pyramid Overall Responsiveness Metric models systems models systems DEPEVAL(summ): Variations base 0.653 0.931 0.883 0.862 pm 0.690 0.811 0.943 0.740 wn 0.687 0.929 0.888 0.860 pred 0.415 0.946 0.706 0.909 norel 0.676 0.929 0.880 0.861 one 0.585 0.958* 0.858 0.900 DEPEVAL(summ): Combinations pm wn 0.694 0.903 0.952* 0.839 pm pred 0.534 0.880 0.898 0.831 pm norel 0.722 0.907 0.936 0.835 pm one 0.611 0.950 0.876 0.895 wn pred 0.374 0.946 0.716 0.912 wn norel 0.405 0.941 0.752 0.905 wn one 0.611 0.952 0.856 0.897 pred norel 0.415 0.945 0.735 0.905 pred one 0.415 0.953 0.721 0.921* norel one 0.600 0.958* 0.863 0.900 pm wn pred 0.527 0.870 0.905 0.821 pm wn norel 0.738 0.897 0.931 0.826 pm wn one 0.634 0.936 0.887 0.881 pm pred norel 0.642 0.876 0.946 0.815 pm pred one 0.504 0.948 0.817 0.907 pm norel one 0.725 0.941 0.905 0.880 wn pred norel 0.433 0.944 0.764 0.906 wn pred one 0.385 0.950 0.722 0.919 wn norel one 0.632 0.954 0.872 0.896 pred norel one 0.452 0.955 0.756 0.919 pm wn pred norel 0.643 0.861 0.940 0.800 pm wn pred one 0.486 0.932 0.809 0.890 pm pred norel one 0.711 0.939 0.881 0.891 pm wn norel one 0.743* 0.930 0.902 0.870 wn pred norel one 0.467 0.950 0.767 0.918 pm wn pred norel one 0.712 0.927 0.887 0.880 Other metrics ROUGE-2 0.277 0.946 0.725 0.894 ROUGE-SU4 0.457 0.928 0.866 0.874 BE-HM 0.423 0.949 0.656 0.911 Table 1: System-level Pearsons correlation between automatic and manual evaluation metrics for TAC 2008 data." ></td>
	<td class="line x" title="117:180	a particular dependency are present in the candidate and reference." ></td>
	<td class="line x" title="118:180	For each of the metrics, including ROUGE and BE, we present the correlationsforrecall." ></td>
	<td class="line x" title="119:180	Thehighestresultineachcategory is marked by an asterisk." ></td>
	<td class="line x" title="120:180	The background gradient indicates whether DEPEVAL(summ) correlation is higher than all three competitors ROUGE2, ROUGE-SU4, andBE(darkestgrey), twoofthe three (medium grey), one of the three (light grey), or none (white)." ></td>
	<td class="line x" title="121:180	The 95% confidence intervals are not included here for reasons of space, but their comparison suggests that none of the system-level differences in correlation levels are large enough to be significant." ></td>
	<td class="line x" title="122:180	This is because the intervals themselves are very wide, due to relatively small number of summarizers (58 automatic and 8 human for TAC; 32 automatic and 10 human for DUC) involved in the comparison." ></td>
	<td class="line x" title="123:180	5.2 Summary-level correlations Tables 2 and 4 present the same correlations, but this time on the level of individual summaries." ></td>
	<td class="line x" title="124:180	As before, the highest level in each category is marked by an asterisk." ></td>
	<td class="line x" title="125:180	Contrary to system-level, here some correlations obtained by DEPEVAL(summ) are significantly higher than those achieved by the three competing metrics, ROUGE-2, ROUGE-SU4, and BE-HM, as determined by the confidence intervals." ></td>
	<td class="line x" title="126:180	The letters in parenthesis indicate that a given DEPEVAL(summ) variant is significantly better at correlating with human judgment than ROUGE-2 (= R2), ROUGE-SU4 (= R4), or BE-HM (= B)." ></td>
	<td class="line x" title="127:180	6 Discussion and future work It is obvious that none of the versions performs best across the board; their different characteristics might render them better suited either for models or for automatic systems, but not for both at the same time." ></td>
	<td class="line x" title="128:180	This can be explained if we understand that evaluating human gold standard summaries and automatically generated summaries of poor-to-medium quality is, in a way, not the same task." ></td>
	<td class="line x" title="129:180	Given that human models are by default well-formed and relevant, relaxing any restraints on matching between them (i.e. allowing partial dependencies, removing the relation label, or adding synonyms) serves, in effect, to accept as correct either (1) the same conceptual information expressed in different ways (where the difference might be real or introduced by faulty parsing), or (2) other information, yet still relevant to the topic." ></td>
	<td class="line x" title="130:180	Accepting information of the former type as correct will ratchet up the score for the summaryandthecorrelationwiththesummarysPyramid score, which measures identity of information across summaries." ></td>
	<td class="line x" title="131:180	Accepting the first and second type of information will raise the score and the correlation with Responsiveness, which measures relevance of information to the particular topic." ></td>
	<td class="line x" title="132:180	However, inevaluating system summaries such relaxation of matching constraints will result in accepting irrelevant and ungrammatical information ascorrect,drivinguptheDEPEVAL(summ)score, but lowering its correlation with both Pyramid and Responsiveness." ></td>
	<td class="line x" title="133:180	Insimplewords,itisokaytogive a model summary the benefit of doubt, and accept its content as correct even if it is not matching other model summaries exactly, but the same strategy applied to a system summary might cause mass over-estimation of the summarys quality." ></td>
	<td class="line x" title="134:180	This substantial difference in the nature of human-generated models and system-produced summaries has impact on all automatic means of evaluation, as long as we are limited to methods that operate on more shallow levels than a full 195 TAC 2008 Pyramid Overall Responsiveness Metric models systems models systems DEPEVAL(summ): Variations base 0.436 (B) 0.595 (R2,R4,B) 0.186 0.373 (R2,B) pm 0.467 (B) 0.584 (R2,B) 0.183 0.368 (B) wn 0.448 (B) 0.592 (R2,B) 0.192 0.376 (R2,R4,B) pred 0.344 0.543 (B) 0.170 0.327 norel 0.437 (B) 0.596* (R2,R4,B) 0.186 0.373 (R2,B) one 0.396 0.587 (R2,B) 0.171 0.376 (R2,R4,B) DEPEVAL(summ): Combinations pm wn 0.474 (B) 0.577 (R2,B) 0.194* 0.371 (R2,B) pm pred 0.407 0.537 (B) 0.153 0.337 pm norel 0.483 (R2,B) 0.584 (R2,B) 0.168 0.362 pm one 0.402 0.577 (R2,B) 0.167 0.384 (R2,R4,B) wn pred 0.352 0.537 (B) 0.182 0.328 wn norel 0.364 0.541 (B) 0.187 0.329 wn one 0.411 0.581 (R2,B) 0.182 0.384 (R2,R4,B) pred norel 0.351 0.547 (B) 0.169 0.327 pred one 0.325 0.542 (B) 0.171 0.347 norel one 0.403 0.589 (R2,B) 0.176 0.377 (R2,R4,B) pm wn pred 0.415 0.526 (B) 0.167 0.337 pm wn norel 0.488* (R2,R4,B) 0.576 (R2,B) 0.168 0.366 (B) pm wn one 0.417 0.563 (B) 0.179 0.389* (R2,R4.B) pm pred norel 0.433 (B) 0.538 (B) 0.124 0.333 pm pred one 0.357 0.545 (B) 0.151 0.381 (R2,R4,B) pm norel one 0.437 (B) 0.567 (R2,B) 0.174 0.369 (B) wn pred norel 0.353 0.541 (B) 0.180 0.324 wn pred one 0.328 0.535 (B) 0.179 0.346 wn norel one 0.416 0.584 (R2,B) 0.185 0.385 (R2,R4,B) pred norel one 0.336 0.549 (B) 0.169 0.351 pm wn pred norel 0.428 (B) 0.524 (B) 0.120 0.334 pm wn pred one 0.363 0.525 (B) 0.164 0.380 (R2,R4,B) pm pred norel one 0.420 (B) 0.533 (B) 0.154 0.375 (R2,R4,B) pm wn norel one 0.452 (B) 0.558 (B) 0.179 0.376 (R2,R4,B) wn pred norel one 0.338 0.544 (B) 0.178 0.349 pm wn pred norel one 0.427 (B) 0.522 (B) 0.153 0.379 (R2,R4,B) Other metrics ROUGE-2 0.307 0.527 0.098 0.323 ROUGE-SU4 0.318 0.557 0.153 0.327 BE-HM 0.239 0.456 0.135 0.317 Table2: Summary-levelPearsonscorrelationbetweenautomaticandmanual evaluation metrics for TAC 2008 data." ></td>
	<td class="line x" title="135:180	DUC 2007 Content Responsiveness Metric models systems DEPEVAL(summ) 0.7341 0.8429 DEPEVAL(summ) wn 0.7355 0.8354 DEPEVAL(summ) norel 0.7394 0.8277 DEPEVAL(summ) one 0.7507 0.8634 ROUGE-2 0.4077 0.8772 ROUGE-SU4 0.2533 0.8297 BE-HM 0.5471 0.8608 Table 3: System-level Pearsons correlation between automatic metrics and Content Responsiveness for DUC 2007 data." ></td>
	<td class="line x" title="136:180	For model summaries, only DEPEVAL correlations are significant (the 95% confidence interval does not include zero)." ></td>
	<td class="line x" title="137:180	None of the differences between metrics are significant at the 95% level." ></td>
	<td class="line x" title="138:180	DUC 2007 Content Responsiveness Metric models systems DEPEVAL(summ) 0.2059 0.4150 DEPEVAL(summ) wn 0.2081 0.4178 DEPEVAL(summ) norel 0.2119 0.4185 DEPEVAL(summ) one 0.1999 0.4101 ROUGE-2 0.1501 0.3875 ROUGE-SU4 0.1397 0.4264 BE-HM 0.1330 0.3722 Table 4: Summary-level Pearsons correlation between automatic metrics and Content Responsiveness for DUC 2007 data." ></td>
	<td class="line x" title="139:180	ROUGE-SU4 and BE correlations for model summaries are not statistically significant." ></td>
	<td class="line x" title="140:180	None of the differences between metrics are significant at the 95% level." ></td>
	<td class="line x" title="141:180	semantic and pragmatic analysis against humanlevel world knowledge." ></td>
	<td class="line x" title="142:180	The problem is twofold: first, our automatic metrics measure identity rather than quality." ></td>
	<td class="line x" title="143:180	Similarity of content between a candidate summary and one or more references is acting as a proxy measure for the quality of the candidate summary; yet, we cannot forget that the relation between these two features is not purely linear." ></td>
	<td class="line x" title="144:180	A candidate highly similar to the reference will be, necessarily, of good quality, but a candidate which is dissimilar from a reference is not necessarily of low quality (vide the case of parallel model summaries, which almost always contain some non-overlapping information)." ></td>
	<td class="line x" title="145:180	The second problem is the extent to which our metrics are able to distinguish content through the veil of differing forms." ></td>
	<td class="line x" title="146:180	Synonyms, paraphrases, or pragmatic features such as the choice of topic and focus render simple string-matching techniques ineffective, especially in the area of summarization where the evaluation happens on a supra-sentential level." ></td>
	<td class="line x" title="147:180	As a result, then, a lot of effort was put into developing metrics that can identify similar content despite non-similar form, which naturally led to the application of linguistically-oriented approaches that look beyond surface word order." ></td>
	<td class="line x" title="148:180	Essentially, though, we are using imperfect measures of similarity as an imperfect stand-in for quality, and the accumulated noise often causes a divergence in our metrics performance with model and system summaries." ></td>
	<td class="line x" title="149:180	Much like the inverse relation of precision and recall, changes and additions that improve a metrics correlation with human scores for model summaries often weaken the correlation for system summaries, and vice versa." ></td>
	<td class="line x" title="150:180	Admittedly, we could just ignore this problem and focus on increasing correlations for automatic summaries only; after all, the whole point of creating evaluation metrics is to score and rank the output of systems." ></td>
	<td class="line x" title="151:180	Such a perspective can be rather short-sighted, though, given that we expect continuous improvement from the summarization systems to, ideally, human levels, so the same issueswhichnowpreventhighcorrelationsformodels will start surfacing in evaluation of systemproduced summaries as well." ></td>
	<td class="line x" title="152:180	Using metrics that only perform reliably for low-quality summaries might prevent us from noticing when those summaries become better." ></td>
	<td class="line x" title="153:180	Our goal should be, therefore, to develop a metric which obtains high correlations in both categories, with the assumption that such a metric will be more reliable in evaluating summaries of varying quality." ></td>
	<td class="line x" title="154:180	196 Since there is no single winner among all 32 variants of DEPEVAL(summ) on TAC 2008 data, wemustdecidewhichofthecategoriesismostimportant to a successful automatic evaluation metric." ></td>
	<td class="line x" title="155:180	Correlations with Overall Responsiveness are in general lower than those with the Pyramid score (except in the case of system-level models)." ></td>
	<td class="line x" title="156:180	This makes sense, if we rememeber that Overall Responsiveness judges content as well as linguistic quality, which are two different dimensions and so a single automatic metric is unlikely to reflect it well, and that it judges content in terms of its relevance to topic, which is also beyond the reach of contemporary metrics which can at most judge content similarity to a model." ></td>
	<td class="line x" title="157:180	This means that the Pyramid score makes for a more relevant metric to emulate." ></td>
	<td class="line x" title="158:180	The last dilemma is whether we choose to focus on systemor summary-level correlations." ></td>
	<td class="line x" title="159:180	This ties in with the purpose which the evaluation metric should serve." ></td>
	<td class="line x" title="160:180	In comparisons of multiple systems, such as in TAC 2008, the value is placed in the correct ordering of these systems; while summary-level assessment can give us important feedback and insight during the system development stage." ></td>
	<td class="line x" title="161:180	The final choice among all DEPEVAL(summ) versions hinges on all of these factors: we should prefer a variant which correlates highly with the Pyramid score rather than with Responsiveness, which minimizes the gap between model and automatic peer correlations while retaining relatively high values for both, and which fulfills these requirements similarly well on both summaryand system-levels." ></td>
	<td class="line x" title="162:180	Three such variants are the baseline DEPEVAL(summ), the WordNet version DEPEVAL(summ) wn, and the version with removed relation labels DEPEVAL(summ) norel." ></td>
	<td class="line x" title="163:180	Both the baselineandnorelversionsachievesignificantimprovement over ROUGE and BE in correlations with the Pyramid score for automatic summaries, and over BE for models, on the summary level." ></td>
	<td class="line x" title="164:180	In fact, almost in all categories they achieve higher correlations than ROUGE and BE." ></td>
	<td class="line x" title="165:180	The only exceptions are the correlations with Pyramid for systems at the system-level, but there the results are close and none of the differences in that category are significant." ></td>
	<td class="line x" title="166:180	To balance this exception, DEPEVAL(summ) achieves much higher correlations withthePyramidscoresformodelsummariesthan either ROUGE or BE on the system level." ></td>
	<td class="line x" title="167:180	In order to see whether the DEPEVAL(summ) advantage holds for other data, we examined the most optimal versions (baseline, wn, norel, as well as one, which is the closest counterpart to label-free BE-HM) on data from DUC 2007." ></td>
	<td class="line x" title="168:180	Because only a portion of the DUC 2007 data was evaluated with Pyramid, we chose to look rather at the Content Responsiveness scores." ></td>
	<td class="line x" title="169:180	As can be seen in Tables 3 and 4, the same patterns hold: decided advantage over ROUGE/BE when it comes to model summaries (especially at system-level), comparable results for automatic summaries." ></td>
	<td class="line x" title="170:180	Since DUC 2007 data consisted of fewer summaries (1,620 vs 5,952 at TAC) and fewer submissions (32 vs 57 at TAC), some results did not reach statistical significance." ></td>
	<td class="line x" title="171:180	In Table 3, in the models category, only DEPEVAL(summ) correlations are significant." ></td>
	<td class="line x" title="172:180	In Table 4, in the model category, only DEPEVAL(summ) and ROUGE-2 correlations are significant." ></td>
	<td class="line x" title="173:180	Note also that these correlations with Content Responsiveness are generally lower than those with Pyramid in previous tables, but in the case of summary-level comparison higher than the correlations with Overall Responsiveness." ></td>
	<td class="line x" title="174:180	This is to be expected given our earlier discussion of the differences in what these metrics measure." ></td>
	<td class="line x" title="175:180	As mentioned before, the dependency-based evaluation can be approached from different angles, leading to differences in performance." ></td>
	<td class="line x" title="176:180	This is exemplified in our experiment, where DEPEVAL(summ) outperforms BE, even though both these metrics rest on the same general idea." ></td>
	<td class="line x" title="177:180	The new implementation of BE presented at the TAC 2008 workshop (Tratz and Hovy, 2008) introduces transformations for dependencies in order to increasethenumberofmatchesamongelementsthat are semantically similar yet differ in terms of syntactic structure and/or lexical choices, and adds WordNet for synonym matching." ></td>
	<td class="line x" title="178:180	Its core modules were updated as well: Minipar was replaced with the Charniak-Johnson reranking parser (Charniak and Johnson, 2005), Named Entity identification was added, and the BE extraction is conducted usingasetofTregexrules(LevyandAndrew, 2006)." ></td>
	<td class="line x" title="179:180	Since our method, presented in this paper, also uses the reranking parser, as well as WordNet, it would be interesting to compare both methods directly in terms of the performance of the dependency extraction procedure." ></td>
	<td class="line x" title="180:180	197" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1034
Robust Machine Translation Evaluation with Entailment Features
Padó, Sebastian;Galley, Michel;Jurafsky, Daniel;Manning, Christopher D.;"></td>
	<td class="line x" title="1:300	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 297305, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:300	c2009 ACL and AFNLP Robust Machine Translation Evaluation with Entailment Features Sebastian Pado Stuttgart University pado@ims.uni-stuttgart.de Michel Galley, Dan Jurafsky, Chris Manning Stanford University {mgalley,jurafsky,manning}@stanford.edu Abstract Existing evaluation metrics for machine translation lack crucial robustness: their correlations with human quality judgments vary considerably across languages and genres." ></td>
	<td class="line x" title="3:300	We believe that the main reason istheirinabilitytoproperlycapture meaning:Agood translation candidate means the same thing as the reference translation, regardless of formulation." ></td>
	<td class="line x" title="4:300	We propose a metric that evaluates MT output based on a rich set of features motivated by textual entailment, such as lexical-semantic (in-)compatibility and argument structure overlap." ></td>
	<td class="line p" title="5:300	We compare this metric against a combination metric of four state-of-theart scores (BLEU, NIST, TER, and METEOR) in two different settings." ></td>
	<td class="line x" title="6:300	The combination metric outperforms the individual scores, but is bested by the entailment-based metric." ></td>
	<td class="line x" title="7:300	Combining the entailment and traditional features yields further improvements." ></td>
	<td class="line x" title="8:300	1 Introduction Constant evaluation is vital to the progress of machine translation (MT)." ></td>
	<td class="line x" title="9:300	Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002)." ></td>
	<td class="line x" title="10:300	BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations." ></td>
	<td class="line x" title="11:300	The resulting scores are cheap and objective." ></td>
	<td class="line x" title="12:300	However, studies such as Callison-Burch et al.(2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be gamed by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences." ></td>
	<td class="line x" title="14:300	This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM." ></td>
	<td class="line x" title="15:300	The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred." ></td>
	<td class="line x" title="16:300	This is problematic, but not surprising: The metrics treat any divergence from the reference as a negative,while(computational)linguisticshaslong dealt with linguistic variation that preserves the meaning, usually called paraphrase, such as: (1) HYP: However, this was declared terrorism by observers and witnesses." ></td>
	<td class="line x" title="17:300	REF: Nevertheless, commentators as well as eyewitnesses are terming it terrorism." ></td>
	<td class="line oc" title="18:300	A number of metrics have been designed to account forparaphrase,eitherbymakingthematchingmore intelligent (TER, Snover et al.(2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al.(2008); Liu and Gildea (2005))." ></td>
	<td class="line n" title="21:300	Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments." ></td>
	<td class="line x" title="22:300	Our paper proposes two strategies." ></td>
	<td class="line x" title="23:300	We first explore the combination of traditional scores into a more robust ensemble metric with linear regression." ></td>
	<td class="line x" title="24:300	Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses." ></td>
	<td class="line x" title="25:300	We operationalize meaning equivalence by bidirectional textual entailment (RTE, Dagan et al.(2005)), and thus predict the quality of MT hypotheses with a rich RTE feature set." ></td>
	<td class="line x" title="27:300	The entailment-based model goes beyond existing word-level semantic metrics such as METEOR by integrating phrasal and compositional aspects of meaning equivalence, such as multiword paraphrases, (in-)correct argument and modification relations, and (dis-)allowed phrase reorderings." ></td>
	<td class="line x" title="28:300	We demonstrate that the resulting metric beats both individual and combined traditional MT metrics." ></td>
	<td class="line x" title="29:300	The complementary features of both metric types can be combined into a joint, superior metric." ></td>
	<td class="line x" title="30:300	297 HYP: Three aid workers were kidnapped." ></td>
	<td class="line x" title="31:300	REF: Three aid workers were kidnapped by pirates." ></td>
	<td class="line x" title="32:300	no entailment entailment HYP: The virus did not infect anybody." ></td>
	<td class="line x" title="33:300	REF: No one was infected by the virus." ></td>
	<td class="line x" title="34:300	entailment entailment Figure 1: Entailment status between an MT system hypothesis and a reference translation for equivalent (top) and non-equivalent (bottom) translations." ></td>
	<td class="line x" title="35:300	2 Regression-based MT Quality Prediction CurrentMTmetricstendtofocusonasingledimension of linguistic information." ></td>
	<td class="line x" title="36:300	Since the importance of these dimensions tends not to be stable across language pairs, genres, and systems, performance of these metrics varies substantially." ></td>
	<td class="line x" title="37:300	A simple strategy to overcome this problem could be to combine the judgments of different metrics." ></td>
	<td class="line x" title="38:300	For example, Paul et al.(2007) train binary classifiers on a feature set formed by a number of MT metrics." ></td>
	<td class="line x" title="40:300	We follow a similar idea, but use a regularized linear regression to directly predict human ratings." ></td>
	<td class="line x" title="41:300	Feature combination via regression is a supervised approach that requires labeled data." ></td>
	<td class="line x" title="42:300	As we show in Section 5, this data is available, and the resulting model generalizes well from relatively small amounts of training data." ></td>
	<td class="line x" title="43:300	3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE)." ></td>
	<td class="line x" title="44:300	TE was introduced by Dagan et al.(2005) as a concept that corresponds more closely to common sense reasoning patterns than classical, strict logical entailment." ></td>
	<td class="line x" title="46:300	Textual entailment is defined informally as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true." ></td>
	<td class="line x" title="47:300	Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006)." ></td>
	<td class="line x" title="48:300	The relation between textual entailment and MT evaluation is shown in Figure 1." ></td>
	<td class="line x" title="49:300	Perfect MT output and the reference translation entail each other (top)." ></td>
	<td class="line x" title="50:300	Translation problems that impact semantic equivalence, e.g., deletion or addition of material, can break entailment in one or both directions (bottom)." ></td>
	<td class="line x" title="51:300	On the modelling level, there is common ground between RTE and MT evaluation: Both have to distinguish between valid and invalid variation to determine whether two texts convey the same information or not." ></td>
	<td class="line x" title="52:300	For example, to recognize the bidirectional entailment in Ex." ></td>
	<td class="line x" title="53:300	(1), RTE must account for the following reformulations: synonymy (However/Nevertheless), more general semantic relatedness (observers/commentators), phrasal replacements (and/as well as), and an active/passive alternation that implies structural change (is declared/are terming)." ></td>
	<td class="line x" title="54:300	This leads us to our main hypothesis: RTE features are designed to distinguish meaning-preserving variation from true divergence and are thus also good predictors in MT evaluation." ></td>
	<td class="line x" title="55:300	However, while the original RTE task is asymmetric, MT evaluation needs to determine meaning equivalence, which is a symmetric relation." ></td>
	<td class="line x" title="56:300	We do this by checking for entailment in both directions (see Figure 1)." ></td>
	<td class="line x" title="57:300	Operationally, this ensures we detect translations which either delete or insert material." ></td>
	<td class="line x" title="58:300	Clearly, there are also differences between the two tasks." ></td>
	<td class="line x" title="59:300	An important one is that RTE assumes the well-formedness of the two sentences." ></td>
	<td class="line x" title="60:300	This is not generally true in MT, and could lead to degraded linguistic analyses." ></td>
	<td class="line x" title="61:300	However, entailment relations are more sensitive to the contribution of individualwords(MacCartneyandManning,2008)." ></td>
	<td class="line x" title="62:300	In Example 2, the modal modifiers break the entailment between two otherwise identical sentences: (2) HYP: Peter is certainly from Lincolnshire." ></td>
	<td class="line x" title="63:300	REF: Peter is possibly from Lincolnshire." ></td>
	<td class="line x" title="64:300	This means that the prediction of TE hinges on correct semantic analysis and is sensitive to misanalyses." ></td>
	<td class="line x" title="65:300	In contrast, human MT judgments behave robustly." ></td>
	<td class="line x" title="66:300	Translations that involve individual errors, like (2), are judged lower than perfect ones, but usually not crucially so, since most aspects are still rendered correctly." ></td>
	<td class="line x" title="67:300	We thus expect even noisy RTE features to be predictive for translation quality." ></td>
	<td class="line x" title="68:300	This allows us to use an off-the-shelf RTE system to obtain features, and to combine them using a regression model as described in Section 2." ></td>
	<td class="line x" title="69:300	3.1 The Stanford Entailment Recognizer The Stanford Entailment Recognizer (MacCartney et al., 2006) is a stochastic model that computes match and mismatch features for each premisehypothesis pair." ></td>
	<td class="line x" title="70:300	The three stages of the system are shown in Figure 2." ></td>
	<td class="line x" title="71:300	The system first uses a robust broad-coverage PCFG parser and a deterministic constituent-dependency converter to construct linguistic representations of the premise and 298 Stage 3: Feature computation (w/ numbers of features) Premise: India buys 1,000 tanks." ></td>
	<td class="line x" title="72:300	Hypothesis: India acquires arms." ></td>
	<td class="line x" title="73:300	Stage 1: Linguistic analysis India buys 1,000 tanks subj dobj India acquires arms subj dobj Stage 2: Alignment India buys 1,000 tanks subj dobj India acquires arms subj dobj 0.9 1.0 0.7 Alignment (8): Semantic compatibility (34): Insertions and deletions (20): Preservation of reference (16): Structural alignment (28): Overall alignment quality Modality, Factivity, Polarity, Quantification, Lexical-semantic relatedness, Tense Felicity of appositions and adjuncts, Types of unaligned material Locations, Dates, Entities Alignment of main verbs and syntactically prominent words, Argument structure (mis-)matches Figure 2: The Stanford Entailment Recognizer the hypothesis." ></td>
	<td class="line x" title="74:300	The results are typed dependency graphs that contain a node for each word and labeled edges representing the grammatical relations between words." ></td>
	<td class="line x" title="75:300	Named entities are identified, and contiguous collocations grouped." ></td>
	<td class="line x" title="76:300	Next, it identifies the highest-scoring alignment from each node in the hypothesis graph to a single node in the premise graph, or to null." ></td>
	<td class="line x" title="77:300	It uses a locally decomposable scoring function: The score of an alignment is the sum of the local word and edge alignment scores." ></td>
	<td class="line x" title="78:300	The computation of these scores make extensive use of about ten lexical similarity resources, including WordNet, InfoMap, and Dekang Lins thesaurus." ></td>
	<td class="line x" title="79:300	Since the search space is exponential in the hypothesis length, the system uses stochastic (rather than exhaustive) search based on Gibbs sampling (see de Marneffe et al.(2007))." ></td>
	<td class="line x" title="81:300	Entailment features." ></td>
	<td class="line x" title="82:300	In the third stage, the system produces roughly 100 features for each aligned premise-hypothesis pair." ></td>
	<td class="line x" title="83:300	A small number of them are real-valued (mostly quality scores), but most are binary implementations of small linguistic theories whose activation indicates syntactic and semantic (mis-)matches of different types." ></td>
	<td class="line x" title="84:300	Figure 2 groups the features into five classes." ></td>
	<td class="line x" title="85:300	Alignment features measure the overall quality of the alignment as given by the lexical resources." ></td>
	<td class="line x" title="86:300	Semantic compatibility features check to what extent the aligned material has the same meaning and preserves semantic dimensions such as modality and factivity, taking a limited amount of context into account." ></td>
	<td class="line x" title="87:300	Insertion/deletion features explicitly address material that remains unaligned and assess its felicity." ></td>
	<td class="line x" title="88:300	Reference features ascertain that the two sentences actually refer to the same events and participants." ></td>
	<td class="line x" title="89:300	Finally, structural features add structural considerations by ensuring that argument structure is preserved in the translation." ></td>
	<td class="line x" title="90:300	See MacCartney et al.(2006) for details on the features, and Sections 5 and 6 for examples of feature firings." ></td>
	<td class="line x" title="92:300	Efficiency considerations." ></td>
	<td class="line x" title="93:300	The use of deep linguistic analysis makes our entailment-based metric considerably more heavyweight than traditional MT metrics." ></td>
	<td class="line x" title="94:300	The average total runtime per sentence pair is 5 seconds on an AMD 2.6GHz Opteron core  efficient enough to perform regular evaluations on development and test sets." ></td>
	<td class="line x" title="95:300	We are currently investigating caching and optimizations that will enable the use of our metric for MT parameter tuning in a Minimum Error Rate Training setup (Och, 2003)." ></td>
	<td class="line x" title="96:300	4 Experimental Evaluation 4.1 Experiments Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a fiveor seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al., 2008)." ></td>
	<td class="line x" title="97:300	An alternative that has been adopted by the yearly WMT evaluation shared tasks since 2008 is the collection of pairwise preference judgments between pairs of MT hypotheses which can be elicited (somewhat) more reliably." ></td>
	<td class="line x" title="98:300	We demonstrate that our approach works well for both types of annotation and different corpora." ></td>
	<td class="line x" title="99:300	Experiment 1 models absolute scores on Asian newswire, and Experiment 2 pairwise preferences on European speech and news data." ></td>
	<td class="line x" title="100:300	4.2 Evaluation We evaluate the output of our models both on the sentence and on the system level." ></td>
	<td class="line x" title="101:300	At the sentence level, we can correlate predictions in Experiment 1 directly with human judgments with Spearmans , 299 a non-parametric rank correlation coefficient appropriate for non-normally distributed data." ></td>
	<td class="line x" title="102:300	In Experiment 2, the predictions cannot be pooled between sentences." ></td>
	<td class="line x" title="103:300	Instead of correlation, we compute consistency (i.e., accuracy) with human preferences." ></td>
	<td class="line x" title="104:300	System-level predictions are computed in both experiments from sentence-level predictions, as the ratio of sentences for which each system provided the best translation (Callison-Burch et al., 2008)." ></td>
	<td class="line x" title="105:300	We extend this procedure slightly because realvalued predictions cannot predict ties, while human raters decide for a significant portion of sentences (as much as 80% in absolute score annotation) to tie two systems for first place." ></td>
	<td class="line x" title="106:300	To simulate this behavior, we compute tie-aware predictions as the percentage of sentences where the systems hypothesis was assigned a score better or at most  worse than the best system." ></td>
	<td class="line x" title="107:300	 is set to match the frequency of ties in the training data." ></td>
	<td class="line x" title="108:300	Finally, the predictions are again correlated with human judgments using Spearmans ." ></td>
	<td class="line x" title="109:300	Tie awareness makes a considerable practical difference, improving correlation figures by 510 points.1 4.3 Baseline Metrics We consider four baselines." ></td>
	<td class="line x" title="110:300	They are small regression models as described in Section 2 over component scores of four widely used MT metrics." ></td>
	<td class="line x" title="111:300	To alleviate possible nonlinearity, we add all features in linear and log space." ></td>
	<td class="line x" title="112:300	Each baselines carries the name of the underlying metric plus the suffix -R.2 BLEUR includes the following 18 sentence-level scores: BLEU-n and n-gram precision scores (1n4); BLEU brevity penalty (BP); BLEU score divided by BP." ></td>
	<td class="line x" title="113:300	To counteract BLEUs brittleness at the sentence level, we also smooth BLEU-n and n-gram precision as in Lin and Och (2004)." ></td>
	<td class="line x" title="114:300	NISTR consists of 16 features." ></td>
	<td class="line x" title="115:300	NIST-n scores (1n10) and information-weighted n-gram precision scores (1n4); NIST brevity penalty (BP); and NIST score divided by BP." ></td>
	<td class="line x" title="116:300	1Due to space constraints, we only show results for tieaware predictions." ></td>
	<td class="line x" title="117:300	See Pado et al.(2009) for a discussion." ></td>
	<td class="line x" title="119:300	2The regression models can simulate the behaviour of each component by setting the weights appropriately, but are strictly more powerful." ></td>
	<td class="line x" title="120:300	A possible danger is that the parameters overfit on the training set." ></td>
	<td class="line x" title="121:300	We therefore verified that the three non-trivial baseline regression models indeed confer a benefit over the default component combination scores: BLEU-1 (which outperformed BLEU-4 in the MetricsMATR 2008 evaluation), NIST-4, and TER (with all costs set to 1)." ></td>
	<td class="line x" title="122:300	We found higher robustness and improved correlations for the regression models." ></td>
	<td class="line x" title="123:300	An exception is BLEU-1 and NIST-4 on Expt." ></td>
	<td class="line x" title="124:300	1 (Ar, Ch), which perform 0.51 point better at the sentence level." ></td>
	<td class="line x" title="125:300	TERR includes 50 features." ></td>
	<td class="line x" title="126:300	We start with the standard TER score and the number of each of the four edit operations." ></td>
	<td class="line x" title="127:300	Since the default uniform cost does not always correlate well with human judgment,weduplicatethesefeaturesfor9non-uniform edit costs." ></td>
	<td class="line x" title="128:300	We find it effective to set insertion cost close to 0, as a way of enabling surface variation, and indeed the new TERp metric uses a similarly low default insertion cost (Snover et al., 2009)." ></td>
	<td class="line o" title="129:300	METEORR consists of METEOR v0.7." ></td>
	<td class="line x" title="130:300	4.4 Combination Metrics The following three regression models implement the methods discussed in Sections 2 and 3." ></td>
	<td class="line x" title="131:300	MTR combines the 85 features of the four baseline models." ></td>
	<td class="line x" title="132:300	It uses no entailment features." ></td>
	<td class="line x" title="133:300	RTER uses the 70 entailment features described in Section 3.1, but no MTR features." ></td>
	<td class="line x" title="134:300	MT+RTER uses all MTR and RTER features, combining matching and entailment evidence.3 5 Expt." ></td>
	<td class="line x" title="135:300	1: Predicting Absolute Scores Data." ></td>
	<td class="line x" title="136:300	Our first experiment evaluates the models we have proposed on a corpus with traditional annotation on a seven-point scale, namely the NIST OpenMT 2008 corpus.4 The corpus contains translations of newswire text into English from three source languages (Arabic (Ar), Chinese (Ch), Urdu (Ur))." ></td>
	<td class="line x" title="137:300	Each language consists of 15002800 sentence pairs produced by 715 MT systems." ></td>
	<td class="line x" title="138:300	We use a round robin scheme." ></td>
	<td class="line x" title="139:300	We optimize the weights of our regression models on two languages and then predict the human scores on the third language." ></td>
	<td class="line x" title="140:300	This gauges performance of our models when training and test data come from the same genre, but from different languages, which we believe to be a setup of practical interest." ></td>
	<td class="line x" title="141:300	For each test set, we set the system-level tie parameter  so that the relative frequency of ties was equal to the training set (6580%)." ></td>
	<td class="line x" title="142:300	Hypotheses generally had to receive scores within 0.30.5 points to tie." ></td>
	<td class="line x" title="143:300	Results." ></td>
	<td class="line x" title="144:300	Table 1 shows the results." ></td>
	<td class="line x" title="145:300	We first concentrate on the upper half (sentence-level results)." ></td>
	<td class="line x" title="146:300	The predictions of all models correlate highly significantly with human judgments, but we still see robustness issues for the individual MT metrics." ></td>
	<td class="line x" title="147:300	3Software for RTER and MT+RTER is available from http://nlp.stanford.edu/software/mteval.shtml." ></td>
	<td class="line o" title="148:300	4Available from http://www.nist.gov. 300 Evaluation Data Metrics train test BLEUR METEORR NISTR TERR MTR RTER MT+RTER Sentence-level Ar+Ch Ur 49.9 49.1 49.5 50.1 50.1 54.5 55.6 Ar+Ur Ch 53.9 61.1 53.1 50.3 57.3 58.0 62.7 Ch+Ur Ar 52.5 60.1 50.4 54.5 55.2 59.9 61.1 System-level Ar+Ch Ur 73.9 68.4 50.0 90.0 92.7 77.4 81.0 Ar+Ur Ch 38.5 44.3 40.0 59.0 51.8 47.7 57.3 Ch+Ur Ar 59.7 86.3 61.9 42.1 48.1 59.7 61.7 Table 1: Expt." ></td>
	<td class="line x" title="149:300	1: Spearmans  for correlation between human absolute scores and model predictions on NIST OpenMT 2008." ></td>
	<td class="line x" title="150:300	Sentence level: All correlations are highly significant." ></td>
	<td class="line x" title="151:300	System level: : p<0.05." ></td>
	<td class="line o" title="152:300	METEORR achieves the best correlation for Chinese and Arabic, but fails for Urdu, apparently the most difficult language." ></td>
	<td class="line o" title="153:300	TERR shows the best result for Urdu, but does worse than METEORR for Arabic and even worse than BLEUR for Chinese." ></td>
	<td class="line x" title="154:300	The MTR combination metric alleviates this problem to some extent by improving the worst-case performance on Urdu to the level of the best individual metric." ></td>
	<td class="line x" title="155:300	The entailment-based RTER system outperforms MTR on each language." ></td>
	<td class="line x" title="156:300	It particularly improves on MTRs correlation on Urdu." ></td>
	<td class="line o" title="157:300	Even though METEORR still does somewhat better than MTR and RTER, we consider this an important confirmation for the usefulness of entailment features in MT evaluation, and for their robustness.5 In addition, the combined model MT+RTER is best for all three languages, outperforming METEORR for each language pair." ></td>
	<td class="line x" title="158:300	It performs considerably better than either MTR or RTER." ></td>
	<td class="line x" title="159:300	This is a second result: the types of evidence provided by MTR and RTER appear to be complementary and can be combined into a superior model." ></td>
	<td class="line x" title="160:300	On the system level (bottom half of Table 1), there is high variance due to the small number of predictions per language, and many predictions are not significantly correlated with human judgments." ></td>
	<td class="line o" title="161:300	BLEUR, METEORR, and NISTR significantly predict one language each (all Arabic); TERR, MTR, and RTER predict two languages." ></td>
	<td class="line x" title="162:300	MT+RTER is the only model that shows significance for all three languages." ></td>
	<td class="line x" title="163:300	This result supports the conclusions we have drawn from the sentence-level analysis." ></td>
	<td class="line x" title="164:300	Further analysis." ></td>
	<td class="line x" title="165:300	We decided to conduct a thorough analysis of the Urdu dataset, the most difficult source language for all metrics." ></td>
	<td class="line x" title="166:300	We start with a fea5These results are substantially better than the performance our metric showed in the MetricsMATR 2008 challenge." ></td>
	<td class="line x" title="167:300	Beyond general enhancement of our model, we attribute the less good MetricsMATR 2008 results to an infelicitous choice of training data for the submission, coupled with the large amount of ASR output in the test data, whose disfluencies represent an additional layer of problems for deep approaches." ></td>
	<td class="line x" title="168:300	20 40 60 80 100 0.42 0.46 0.50 0.54 % Training data MT08 Ar+Ch Spearman's rho on MT 08 Ur a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 Metrics MtRteR RteR MtR MetR Figure 3: Experiment 1: Learning curve (Urdu)." ></td>
	<td class="line x" title="169:300	ture ablation study." ></td>
	<td class="line x" title="170:300	Removing any feature group from RTER results in drops in correlation of at least three points." ></td>
	<td class="line x" title="171:300	The largest drops occur for the structural ( =11) and insertion/deletion ( =8) features." ></td>
	<td class="line x" title="172:300	Thus, all feature groups appear to contribute to the good correlation of RTER." ></td>
	<td class="line x" title="173:300	However, there are big differences in the generality of the feature groups: in isolation, the insertion/deletion features achieve almost no correlation, and need to be complemented by more robust features." ></td>
	<td class="line x" title="174:300	Next, we analyze the role of training data." ></td>
	<td class="line x" title="175:300	Figure 3 shows Urdu average correlations for models trained on increasing subsets of the training data (10% increments, 10 random draws per step; Ar and Ch show similar patterns.)" ></td>
	<td class="line o" title="176:300	METEORR does not improve, which is to be expected given the model definition." ></td>
	<td class="line x" title="177:300	RTER has a rather flat learning curve that climbs to within 2 points of the final correlation value for 20% of the training set (about 400 sentence pairs)." ></td>
	<td class="line x" title="178:300	Apparently, entailment features do not require a large training set, presumably because most features of RTER are binary." ></td>
	<td class="line x" title="179:300	The remaining two models, MTR and MT+RTER, show clearer benefit from more data." ></td>
	<td class="line x" title="180:300	With 20% of the total data, they climb to within 5 points of their final performance, but keep slowly improving further." ></td>
	<td class="line x" title="181:300	301 REF: I shall face that fact today." ></td>
	<td class="line x" title="182:300	HYP: Today I will face this reality." ></td>
	<td class="line o" title="183:300	[doc WL-34-174270-7483871, sent 4, system1] Gold: 6 METEORR: 2.8 RTER: 6.1  Only function words unaligned (will, this)  Alignment fact/reality: hypernymy is ok in upward monotone context REF: What does BBCs Haroon Rasheed say after a visit to Lal Masjid Jamia Hafsa complex?" ></td>
	<td class="line x" title="184:300	There are no underground tunnels in Lal Masjid or Jamia Hafsa." ></td>
	<td class="line x" title="185:300	The presence of the foreigners could not be confirmed as well." ></td>
	<td class="line x" title="186:300	What became of the extremists like Abuzar?" ></td>
	<td class="line x" title="187:300	HYP: BBC Haroon Rasheed Lal Masjid, Jamia Hafsa after his visit to Auob Medical Complex says Lal Masjid and seminary in under a land mine, not also been confirmed the presence of foreigners could not be, such as Abu by the extremist?" ></td>
	<td class="line o" title="188:300	[doc WL-12-174261-7457007, sent 2, system2] Gold: 1 METEORR: 4.5 RTER: 1.2  Hypothesis root node unaligned  Missing alignments for subjects  Important entities in hypothesis cannot be aligned  Reference, hypothesis differ in polarity Table 2: Expt." ></td>
	<td class="line x" title="189:300	1: Reference translations and MT output (Urdu)." ></td>
	<td class="line x" title="190:300	Scores are out of 7 (higher is better)." ></td>
	<td class="line o" title="191:300	Finally, we provide a qualitative comparison of RTERs performance against the best baseline metric, METEORR." ></td>
	<td class="line p" title="192:300	Since the computation of RTER takes considerably more resources than METEORR, it is interesting to compare the predictions of RTER against METEORR." ></td>
	<td class="line x" title="193:300	Table 2 shows two classes of examples with apparent improvements." ></td>
	<td class="line n" title="194:300	The first example (top) shows a good translation that is erroneously assigned a low score by METEORR because (a) it cannot align fact and reality (METEORR aligns only synonyms) and (b) it punishes the change of word order through its penalty term." ></td>
	<td class="line x" title="195:300	RTER correctly assigns a high score." ></td>
	<td class="line x" title="196:300	The features show that this prediction results from two semantic judgments." ></td>
	<td class="line x" title="197:300	The first is that the lack of alignments for two function words is unproblematic; the second is that the alignment between fact and reality, which is established on the basis of WordNet similarity, is indeed licensed in the current context." ></td>
	<td class="line x" title="198:300	More generally, we find that RTER is able to account for more valid variation in good translations because (a) it judges the validity of alignments dependent on context; (b) it incorporates more semantic similarities; and (c) it weighs mismatches according to the words status." ></td>
	<td class="line n" title="199:300	The second example (bottom) shows a very bad translation that is scored highly by METEORR, sincealmostallofthereferencewordsappeareither literally or as synonyms in the hypothesis (marked in italics)." ></td>
	<td class="line o" title="200:300	In combination with METEORRs concentration on recall, this is sufficient to yield a moderately high score." ></td>
	<td class="line x" title="201:300	In the case of RTER, a number of mismatch features have fired." ></td>
	<td class="line x" title="202:300	They indicate problems with the structural well-formedness of the MT output as well as semantic incompatibility between hypothesis and reference (argument structure and reference mismatches)." ></td>
	<td class="line x" title="203:300	6 Expt." ></td>
	<td class="line x" title="204:300	2: Predicting Pairwise Preferences In this experiment, we predict human pairwise preference judgments (cf.Section 4)." ></td>
	<td class="line x" title="206:300	We reuse the linear regression framework from Section 2 and predict pairwise preferences by predicting two absolute scores (as before) and comparing them.6 Data." ></td>
	<td class="line x" title="207:300	This experiment uses the 20062008 corpora of the Workshop on Statistical Machine Translation (WMT).7 It consists of data from EUROPARL (Koehn, 2005) and various news commentaries, with five source languages (French, German, Spanish, Czech, and Hungarian)." ></td>
	<td class="line x" title="208:300	As training set, we use the portions of WMT 2006 and 2007 that are annotated with absolute scores on a fivepoint scale (around 14,000 sentences produced by 40 systems)." ></td>
	<td class="line x" title="209:300	The test set is formed by the WMT 2008 relative rank annotation task." ></td>
	<td class="line x" title="210:300	As in Experiment 1, we set  so that the incidence of ties in the training and test set is equal (60%)." ></td>
	<td class="line x" title="211:300	Results." ></td>
	<td class="line x" title="212:300	Table 4 shows the results." ></td>
	<td class="line x" title="213:300	The left result column shows consistency, i.e., the accuracy on human pairwise preference judgments.8 The pattern of results matches our observations in Expt." ></td>
	<td class="line p" title="214:300	1: Among individual metrics, METEORR and TERR do better than BLEUR and NISTR." ></td>
	<td class="line x" title="215:300	MTR and RTER outperform individual metrics." ></td>
	<td class="line x" title="216:300	The best result by a wide margin, 52.5%, is shown by MT+RTER." ></td>
	<td class="line x" title="217:300	6We also experimented with a logistic regression model that predicts binary preferences directly." ></td>
	<td class="line x" title="218:300	Its performance is comparable; see Pado et al.(2009) for details." ></td>
	<td class="line x" title="220:300	7Available from http://www.statmt.org/." ></td>
	<td class="line x" title="221:300	8The random baseline is not 50%, but, according to our experiments, 39.8%." ></td>
	<td class="line x" title="222:300	This has two reasons: (1) the judgments include contradictory and tie annotations that cannot be predicted correctly (raw inter-annotator agreement on WMT 2008 was 58%); (2) metrics have to submit a total order over the translations for each sentence, which introduces transitivity constraints." ></td>
	<td class="line x" title="223:300	For details, see Callison-Burch et al.(2008)." ></td>
	<td class="line x" title="225:300	302 Segment MTR RTER MT+RTER Gold REF: Scottish NHS boards need to improve criminal records checks for employees outside Europe, a watchdog has said." ></td>
	<td class="line x" title="226:300	HYP: The Scottish health ministry should improve the controls on extracommunity employees to check whether they have criminal precedents, said the monitoring committee." ></td>
	<td class="line x" title="227:300	[1357, lium-systran] Rank: 3 Rank: 1 Rank: 2 Rank: 1 REF: Arguments, bullying and fights between the pupils have extended to the relations between their parents." ></td>
	<td class="line x" title="228:300	HYP: Disputes, chicane and fights between the pupils transposed in relations between the parents." ></td>
	<td class="line x" title="229:300	[686, rbmt4] Rank: 5 Rank: 2 Rank: 4 Rank: 5 Table 3: Expt." ></td>
	<td class="line x" title="230:300	2: Reference translations and MT output (French)." ></td>
	<td class="line x" title="231:300	Ranks are out of five (smaller is better)." ></td>
	<td class="line o" title="232:300	Feature set Consistency(%) System-level correlation () BLEUR 49.6 69.3 METEORR 51.1 72.6 NISTR 50.2 70.4 TERR 51.2 72.5 MTR 51.5 73.1 RTER 51.8 78.3 MT+RTER 52.5 75.8 WMT 08 (worst) 44 37 WMT 08 (best) 56 83 Table 4: Expt." ></td>
	<td class="line x" title="233:300	2: Prediction of pairwise preferences on the WMT 2008 dataset." ></td>
	<td class="line x" title="234:300	The right column shows Spearmans  for the correlation between human judgments and tieaware system-level predictions." ></td>
	<td class="line x" title="235:300	All metrics predict system scores highly significantly, partly due to the larger number of systems compared (87 systems)." ></td>
	<td class="line o" title="236:300	Again, we see better results for METEORR and TERR than for BLEUR and NISTR, and the individual metrics do worse than the combination models." ></td>
	<td class="line x" title="237:300	Among the latter, the order is: MTR (worst), MT+RTER, and RTER (best at 78.3)." ></td>
	<td class="line x" title="238:300	WMT 2009." ></td>
	<td class="line x" title="239:300	We submitted the Expt." ></td>
	<td class="line x" title="240:300	2 RTER metric to the WMT 2009 shared MT evaluation task (Pado et al., 2009)." ></td>
	<td class="line x" title="241:300	The results provide further validation for our results and our general approach." ></td>
	<td class="line x" title="242:300	At the system level, RTER made third place (avg.correlation=0.79),trailingthetwotopmetrics closely ( =0.80,  =0.83) and making the best predictions for Hungarian." ></td>
	<td class="line x" title="243:300	It also obtained the second-best consistency score (53%, best: 54%)." ></td>
	<td class="line x" title="244:300	Metriccomparison." ></td>
	<td class="line x" title="245:300	The pairwise preference annotation of WMT 2008 gives us the opportunity to compare the MTR and RTER models by computing consistency separately on the top (highestranked) and bottom (lowest-ranked) hypotheses for each reference." ></td>
	<td class="line x" title="246:300	RTER performs about 1.5 percent better on the top than on the bottom hypotheses." ></td>
	<td class="line x" title="247:300	The MTR model shows the inverse behavior, performing 2 percent worse on the top hypotheses." ></td>
	<td class="line x" title="248:300	This matches well with our intuitions: We see some noise-induced degradation for the entailment features, but not much." ></td>
	<td class="line x" title="249:300	In contrast, surface-based features are better at detecting bad translations than at discriminating among good ones." ></td>
	<td class="line x" title="250:300	Table 3 further illustrates the difference between thetopmodelsontwoexamplesentences.Inthetop example, RTER makes a more accurate prediction than MTR." ></td>
	<td class="line x" title="251:300	The human raters favorite translation deviates considerably from the reference in lexical choice, syntactic structure, and word order, for which it is punished by MTR (rank 3/5)." ></td>
	<td class="line x" title="252:300	In contrast, RTER determines correctly that the propositional content of the reference is almost completely preserved (rank 1)." ></td>
	<td class="line x" title="253:300	In the bottom example, RTERs prediction is less accurate." ></td>
	<td class="line x" title="254:300	This sentence was rated as bad by the judge, presumably due to the inappropriate main verb translation." ></td>
	<td class="line x" title="255:300	Together with the subject mismatch, MTR correctly predicts a low score (rank 5/5)." ></td>
	<td class="line x" title="256:300	RTERs attention to semantic overlap leads to an incorrect high score (rank 2/5)." ></td>
	<td class="line x" title="257:300	Feature Weights." ></td>
	<td class="line x" title="258:300	Finally, we make two observations about feature weights in the RTER model." ></td>
	<td class="line x" title="259:300	First, the model has learned high weights not only for the overall alignment score (which behavesmostsimilarlytotraditionalmetrics),butalso for a number of binary syntacto-semantic match and mismatch features." ></td>
	<td class="line x" title="260:300	This confirms that these features systematically confer the benefit we have shown anecdotally in Table 2." ></td>
	<td class="line x" title="261:300	Features with a consistently negative effect include dropping adjuncts, unaligned or poorly aligned root nodes, incompatible modality between the main clauses, person and location mismatches (as opposed to general mismatches) and wrongly handled passives." ></td>
	<td class="line x" title="262:300	Con303 versely, higher scores result from factors such as high alignment score, matching embeddings under factive verbs, and matches between appositions." ></td>
	<td class="line x" title="263:300	Second, good MT evaluation feature weights are not good weights for RTE." ></td>
	<td class="line x" title="264:300	Some differences, particularly for structural features, are caused by the low grammaticality of MT data." ></td>
	<td class="line x" title="265:300	For example, the feature that fires for mismatches between dependents of predicates is unreliable on the WMT data." ></td>
	<td class="line x" title="266:300	Other differences do reflect more fundamental differences between the two tasks (cf.Section 3)." ></td>
	<td class="line x" title="268:300	For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation." ></td>
	<td class="line x" title="269:300	7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical." ></td>
	<td class="line oc" title="270:300	Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al.(2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases." ></td>
	<td class="line n" title="272:300	These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf.the bottom example in Table 2)." ></td>
	<td class="line x" title="274:300	Thus, incorporation of syntactic knowledge has been the focus of another line of research." ></td>
	<td class="line x" title="275:300	Amigo et al.(2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality." ></td>
	<td class="line x" title="277:300	Similar ideas have been applied by Owczarzak et al.(2008) to LFG parses, and by Liu and Gildea (2005) to features derived from phrase-structure tress." ></td>
	<td class="line x" title="279:300	This approach has also been successful for the related task of summarization evaluation (Hovy et al., 2006)." ></td>
	<td class="line x" title="280:300	The most comparable work to ours is Gimenez and Marquez (2008)." ></td>
	<td class="line x" title="281:300	Our results agree on the crucial point that the use of a wide range of linguistic knowledge in MT evaluation is desirable and important." ></td>
	<td class="line x" title="282:300	However, Gimenez and Marquez advocate the use of a bottom-up development process that builds on a set of heterogeneous, independent metrics each of which measures overlap with respect to one linguistic level." ></td>
	<td class="line x" title="283:300	In contrast, our aim is to provide a top-down, integrated motivation for the features we integrate through the textual entailment recognition paradigm." ></td>
	<td class="line x" title="284:300	8 Conclusion and Outlook In this paper, we have explored a strategy for the evaluation of MT output that aims at comprehensively assessing the meaning equivalence between reference and hypothesis." ></td>
	<td class="line x" title="285:300	To do so, we exploit the common ground between MT evaluation and the Recognition of Textual Entailment (RTE), both of which have to distinguish valid from invalid linguistic variation." ></td>
	<td class="line x" title="286:300	Conceputalizing MT evaluation as an entailment problem motivates the use of a rich feature set that covers, unlike almost all earlier metrics, a wide range of linguistic levels, including lexical, syntactic, and compositional phenomena." ></td>
	<td class="line x" title="287:300	We have used an off-the-shelf RTE system to compute these features, and demonstrated that a regression model over these features can outperform an ensemble of traditional MT metrics in two experiments on different datasets." ></td>
	<td class="line x" title="288:300	Even though the features build on deep linguistic analysis, they are robust enough to be used in a real-world setting, at least on written text." ></td>
	<td class="line x" title="289:300	A limited amount of training data is sufficient, and the weights generalize well." ></td>
	<td class="line x" title="290:300	Our data analysis has confirmed that each of the feature groups contributes to the overall success of the RTE metric, and that its gains come from its better success at abstracting away from valid variation (such as word order or lexical substitution), while still detecting major semantic divergences." ></td>
	<td class="line x" title="291:300	We have also clarified the relationship between MT evaluation and textual entailment: The majority of phenomena (but not all) that are relevant for RTE are also informative for MT evaluation." ></td>
	<td class="line x" title="292:300	The focus of this study was on the use of an existing RTE infrastructure for MT evaluation." ></td>
	<td class="line x" title="293:300	Future workwillhavetoassesstheeffectivenessofindividual features and investigate ways to customize RTE systems for the MT evaluation task." ></td>
	<td class="line x" title="294:300	An interesting aspect that we could not follow up on in this paper is that entailment features are linguistically interpretable (cf.Fig." ></td>
	<td class="line x" title="296:300	2) and may find use in uncovering systematic shortcomings of MT systems." ></td>
	<td class="line x" title="297:300	A limitation of our current metric is that it is language-dependent and relies on NLP tools in the target language that are still unavailable for many languages, such as reliable parsers." ></td>
	<td class="line x" title="298:300	To some extent, of course, this problem holds as well for state-of-the-art MT systems." ></td>
	<td class="line x" title="299:300	Nevertheless, it must be an important focus of future research to develop robust meaning-based metrics for other languages that can cash in the promise that we have shown for evaluating translation into English." ></td>
	<td class="line x" title="300:300	304" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="P09-1035
The Contribution of Linguistic Features to Automatic Machine Translation Evaluation
Amigó, Enrique;Giménez, Jesús;Gonzalo, Julio;Verdejo, Felisa;"></td>
	<td class="line x" title="1:202	Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306314, Suntec, Singapore, 2-7 August 2009." ></td>
	<td class="line x" title="2:202	c2009 ACL and AFNLP The Contribution of Linguistic Features to Automatic Machine Translation Evaluation Enrique Amigo1 Jesus Gimenez2 Julio Gonzalo 1 Felisa Verdejo1 1UNED, Madrid {enrique,julio,felisa}@lsi.uned.es 2UPC, Barcelona jgimenez@lsi.upc.edu Abstract A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested." ></td>
	<td class="line x" title="3:202	However, n-gram based metrics are still today the dominant approach." ></td>
	<td class="line x" title="4:202	The main reason is that the advantages of employing deeper linguistic information have not been clarified yet." ></td>
	<td class="line x" title="5:202	In this work, we propose a novel approach for meta-evaluation of MT evaluation metrics, since correlation cofficient against human judges do not reveal details about the advantages and disadvantages of particular metrics." ></td>
	<td class="line x" title="6:202	We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics." ></td>
	<td class="line x" title="7:202	Overall, our experiments show that (i) both lexical and linguistic metrics present complementary advantages and (ii) combining both kinds of metrics yields the most robust metaevaluation performance." ></td>
	<td class="line x" title="8:202	1 Introduction Automatic evaluation methods based on similarity to human references have substantially accelerated the development cycle of many NLP tasks, such as Machine Translation, Automatic Summarization, Sentence Compression and Language Generation." ></td>
	<td class="line x" title="9:202	These automatic evaluation metrics allow developers to optimize their systems without the need for expensive human assessments for each of their possible system configurations." ></td>
	<td class="line x" title="10:202	However, estimating the system output quality according to its similarity to human references is not a trivial task." ></td>
	<td class="line x" title="11:202	The main problem is that many NLP tasks are open/subjective; therefore, different humans may generate different outputs, all of them equally valid." ></td>
	<td class="line x" title="12:202	Thus, language variability is an issue." ></td>
	<td class="line x" title="13:202	In order to tackle language variability in the context of Machine Translation, a considerable effort has also been made to include deeper linguistic information in automatic evaluation metrics, both syntactic and semantic (see Section 2 for details)." ></td>
	<td class="line x" title="14:202	However, the most commonly used metrics are still based on n-gram matching." ></td>
	<td class="line x" title="15:202	The reason is that the advantages of employing higher linguistic processing levels have not been clarified yet." ></td>
	<td class="line x" title="16:202	The main goal of our work is to analyze to what extent deep linguistic features can contribute to the automatic evaluation of translation quality." ></td>
	<td class="line x" title="17:202	For that purpose, we compare  using four different test beds  the performance of 16 n-gram based metrics, 48 linguistic metrics and one combined metric from the state of the art." ></td>
	<td class="line x" title="18:202	Analyzing the reliability of evaluation metrics requires meta-evaluation criteria." ></td>
	<td class="line x" title="19:202	In this respect, we identify important drawbacks of the standard meta-evaluation methods based on correlation with human judgements." ></td>
	<td class="line x" title="20:202	In order to overcome these drawbacks, we then introduce six novel meta-evaluation criteria which represent different metric reliability dimensions." ></td>
	<td class="line x" title="21:202	Our analysis indicates that: (i) both lexical and linguistic metrics have complementary advantages and different drawbacks; (ii) combining both kinds of metrics is a more effective and robust evaluation method across all meta-evaluation criteria." ></td>
	<td class="line x" title="22:202	In addition, we also perform a qualitative analysis of one hundred sentences that were incorrectly evaluated by state-of-the-art metrics." ></td>
	<td class="line x" title="23:202	The analysis confirms that deep linguistic techniques are necessary to avoid the most common types of error." ></td>
	<td class="line x" title="24:202	Section 2 examines the state of the art Section 3 describes the test beds and metrics considered in our experiments." ></td>
	<td class="line x" title="25:202	In Section 4 the correlation between human assessors and metrics is computed, with a discussion of its drawbacks." ></td>
	<td class="line x" title="26:202	In Section 5 different quality aspects of metrics are analysed." ></td>
	<td class="line x" title="27:202	Conclusions are drawn in the last section." ></td>
	<td class="line x" title="28:202	306 2 Previous Work on Machine Translation Meta-Evaluation Insofar as automatic evaluation metrics for machine translation have been proposed, different meta-evaluation frameworks have been gradually introduced." ></td>
	<td class="line x" title="29:202	For instance, Papineni et al.(2001) introduced the BLEU metric and evaluated its reliability in terms of Pearson correlation with human assessments for adequacy and fluency judgements." ></td>
	<td class="line x" title="31:202	With the aim of overcoming some of the deficiencies of BLEU, Doddington (2002) introduced the NIST metric." ></td>
	<td class="line x" title="32:202	Metric reliability was also estimated in terms of correlation with human assessments, but over different document sources and for a varying number of references and segment sizes." ></td>
	<td class="line x" title="33:202	Melamed et al.(2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead." ></td>
	<td class="line x" title="35:202	Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nieen et al., 2000), PER (Tillmann et al., 1997), and variants of ROUGE, BLEU and GTM." ></td>
	<td class="line x" title="36:202	They computed both Pearson and Spearman correlation, obtaining similar results in both cases." ></td>
	<td class="line oc" title="37:202	In a different work, Banerjee and Lavie (2005) argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations." ></td>
	<td class="line o" title="38:202	In order to address this issue, they computed the translation-by-translation correlation with human judgements (i.e., correlation at the segment level)." ></td>
	<td class="line x" title="39:202	All that metrics were based on n-gram overlap." ></td>
	<td class="line x" title="40:202	But there is also extensive research focused on including linguistic knowledge in metrics (Owczarzak et al., 2006; Reeder et al., 2001; Liu and Gildea, 2005; Amigo et al., 2006; Mehay and Brew, 2007; Gimenez and M`arquez, 2007; Owczarzak et al., 2007; Popovic and Ney, 2007; Gimenez and M`arquez, 2008b) among others." ></td>
	<td class="line x" title="41:202	In all these cases, metrics were also evaluated by means of correlation with human judgements." ></td>
	<td class="line x" title="42:202	In a different research line, several authors have suggested approaching automatic evaluation through the combination of individual metric scores." ></td>
	<td class="line x" title="43:202	Among the most relevant let us cite research by Kulesza and Shieber (2004), Albrecht and Hwa (2007)." ></td>
	<td class="line x" title="44:202	But finding optimal metric combinations requires a meta-evaluation criterion." ></td>
	<td class="line x" title="45:202	Most approaches again rely on correlation with human judgements." ></td>
	<td class="line x" title="46:202	However, some of them measured the reliability of metric combinations in terms of their ability to discriminate between human translations and automatic ones (human likeness) (Amigo et al., 2005)." ></td>
	<td class="line x" title="47:202	In this work, we present a novel approach to meta-evaluation which is distinguished by the use of additional easily interpretable meta-evaluation criteria oriented to measure different aspects of metric reliability." ></td>
	<td class="line x" title="48:202	We then apply this approach to find out about the advantages and challenges of including linguistic features in meta-evaluation criteria." ></td>
	<td class="line x" title="49:202	3 Metrics and Test Beds 3.1 Metric Set For our study, we have compiled a rich set of metric variants at three linguistic levels: lexical, syntactic, and semantic." ></td>
	<td class="line x" title="50:202	In all cases, translation quality is measured by comparing automatic translations against a set of human references." ></td>
	<td class="line o" title="51:202	At the lexical level, we have included several standard metrics, based on different similarity assumptions: edit distance (WER, PER and TER), lexical precision (BLEU and NIST), lexical recall (ROUGE), and F-measure (GTM and METEOR)." ></td>
	<td class="line x" title="52:202	At the syntactic level, we have used several families of metrics based on dependency parsing (DP) and constituency trees (CP)." ></td>
	<td class="line x" title="53:202	At the semantic level, we have included three different families which operate using named entities (NE), semantic roles (SR), and discourse representations (DR)." ></td>
	<td class="line x" title="54:202	A detailed description of these metrics can be found in (Gimenez and M`arquez, 2007)." ></td>
	<td class="line x" title="55:202	Finally, we have also considered ULC, which is a very simple approach to metric combination based on the unnormalized arithmetic mean of metric scores, as described by Gimenez and M`arquez (2008a)." ></td>
	<td class="line x" title="56:202	ULC considers a subset of metrics which operate at several linguistic levels." ></td>
	<td class="line x" title="57:202	This approach has proven very effective in recent evaluation campaigns." ></td>
	<td class="line x" title="58:202	Metric computation has been carried out using the IQMT Framework for Automatic MT Evaluation (Gimenez, 2007)1." ></td>
	<td class="line x" title="59:202	The simplicity of this approach (with no training of the metric weighting scheme) ensures that the potential advantages detected in our experiments are not due to overfitting effects." ></td>
	<td class="line x" title="60:202	1http://www.lsi.upc.edu/nlp/IQMT 307 2004 2005 AE CE AE CE #references 5 5 5 4 #systemsassessed 5 10 5+1 5 #casesassessed 347 447 266 272 Table 1: NIST 2004/2005 MT Evaluation Campaigns." ></td>
	<td class="line x" title="61:202	Test bed description 3.2 Test Beds We use the test beds from the 2004 and 2005 NIST MT Evaluation Campaigns (Le and Przybocki, 2005)2." ></td>
	<td class="line x" title="62:202	Both campaigns include two different translations exercises: Arabic-to-English (AE) and Chinese-to-English (CE)." ></td>
	<td class="line x" title="63:202	Human assessments of adequacy and fluency, on a 1-5 scale, are available for a subset of sentences, each evaluated by two different human judges." ></td>
	<td class="line x" title="64:202	A brief numerical description of these test beds is available in Table 1." ></td>
	<td class="line x" title="65:202	The corpus AE05 includes, apart from five automatic systems, one human-aided system that is only used in our last experiment." ></td>
	<td class="line x" title="66:202	4 Correlation with Human Judgements 4.1 Correlation at the Segment vs. System Levels Let us first analyze the correlation with human judgements for linguistic vs. n-gram based metrics." ></td>
	<td class="line x" title="67:202	Figure 1 shows the correlation obtained by each automatic evaluation metric at system level (horizontal axis) versus segment level (vertical axis) in our test beds." ></td>
	<td class="line x" title="68:202	Linguistic metrics are represented by grey plots, and black plots represent metrics based on n-gram overlap." ></td>
	<td class="line x" title="69:202	The most remarkable aspect is that there exists a certain trade-off between correlation at segment versus system level." ></td>
	<td class="line x" title="70:202	In fact, this graph produces a negative Pearson correlation coefficient between system and segment levels of 0.44." ></td>
	<td class="line x" title="71:202	In other words, depending on how the correlation is computed, the relative predictive power of metrics can swap." ></td>
	<td class="line x" title="72:202	Therefore, we need additional meta-evaluation criteria in order to clarify the behavior of linguistic metrics as compared to n-gram based metrics." ></td>
	<td class="line x" title="73:202	However, there are some exceptions." ></td>
	<td class="line x" title="74:202	Some metrics achieve high correlation at both levels." ></td>
	<td class="line x" title="75:202	The first one is ULC (the circle in the plot), which combines both kind of metrics in a heuristic way (see Section 3.1)." ></td>
	<td class="line x" title="76:202	The metric nearest to ULC is 2http://www.nist.gov/speech/tests/mt Figure 1: Averaged Pearson correlation at system vs. segment level over all test beds." ></td>
	<td class="line x" title="77:202	DP-Or-star, which computes lexical overlapping but on dependency relationships." ></td>
	<td class="line x" title="78:202	These results are a first evidence of the advantages of combining metrics at several linguistic processing levels." ></td>
	<td class="line x" title="79:202	4.2 Drawbacks of Correlation-based Meta-evaluation Although correlation with human judgements is considered the standard meta-evaluation criterion, it presents serious drawbacks." ></td>
	<td class="line x" title="80:202	With respect to correlation at system level, the main problem is that the relative performance of different metrics changes almost randomly between testbeds." ></td>
	<td class="line x" title="81:202	One of the reasons is that the number of assessed systems per testbed is usually low, and then correlation has a small number of samples to be estimated with." ></td>
	<td class="line x" title="82:202	Usually, the correlation at system level is computed over no more than a few systems." ></td>
	<td class="line x" title="83:202	For instance, Table 2 shows the best 10 metrics in CE05 according to their correlation with human judges at the system level, and then the ranking they obtain in the AE05 testbed." ></td>
	<td class="line x" title="84:202	There are substantial swaps between both rankings." ></td>
	<td class="line x" title="85:202	Indeed, the Pearson correlation of both ranks is only 0.26." ></td>
	<td class="line pc" title="86:202	This result supports the intuition in (Banerjee and Lavie, 2005) that correlation at segment level is necessary to ensure the reliability of metrics in different situations." ></td>
	<td class="line x" title="87:202	However, the correlation values of metrics at segment level have also drawbacks related to their interpretability." ></td>
	<td class="line x" title="88:202	Most metrics achieve a Pearson coefficient lower than 0.5." ></td>
	<td class="line x" title="89:202	Figure 2 shows two possible relationships between human and metric 308 Table 2: Metrics rankings according to correlation with human judgements using CE05 vs. AE05 Figure 2: Human judgements and scores of two hypothetical metrics with Pearson correlation 0.5 produced scores." ></td>
	<td class="line x" title="90:202	Both hypothetical metrics A and B would achieve a 0.5 correlation." ></td>
	<td class="line x" title="91:202	In the case of Metric A, a high score implies a high human assessed quality, but not the reverse." ></td>
	<td class="line x" title="92:202	This is the tendency hypothesized by Culy and Riehemann (2003)." ></td>
	<td class="line x" title="93:202	In the case of Metric B, the high scored translations can achieve both low or high quality according to human judges but low scores ensure low quality." ></td>
	<td class="line x" title="94:202	Therefore, the same Pearson coefficient may hide very different behaviours." ></td>
	<td class="line x" title="95:202	In this work, we tackle these drawbacks by defining more specific meta-evaluation criteria." ></td>
	<td class="line x" title="96:202	5 Alternatives to Correlation-based Meta-evaluation We have seen that correlation with human judgements has serious limitations for metric evaluation." ></td>
	<td class="line x" title="97:202	Therefore, we have focused on other aspects of metric reliability that have revealed differences between n-gram and linguistic based metrics: 1." ></td>
	<td class="line x" title="98:202	Is the metric able to accurately reveal improvements between two systems?" ></td>
	<td class="line x" title="99:202	2." ></td>
	<td class="line x" title="100:202	Can we trust the metric when it says that a translation is very good or very bad?" ></td>
	<td class="line x" title="101:202	Figure 3: SIP versus SIR 3." ></td>
	<td class="line x" title="102:202	Are metrics able to identify good translations which are dissimilar from the models?" ></td>
	<td class="line x" title="103:202	We now discuss each of these aspects separately." ></td>
	<td class="line x" title="104:202	5.1 Ability of metrics to Reveal System Improvements We now investigate to what extent a significant system improvement according to the metric implies a significant improvement according to human assessors, and viceversa." ></td>
	<td class="line x" title="105:202	In other words: are the metrics able to detect any quality improvement?" ></td>
	<td class="line x" title="106:202	Is a metric score improvement a strong evidence of quality increase?" ></td>
	<td class="line x" title="107:202	Knowing that a metric has a 0.8 Pearson correlation at the system level or 0.5 at the segment level does not provide a direct answer to this question." ></td>
	<td class="line x" title="108:202	In order to tackle this issue, we compare metrics versus human assessments in terms of precision and recall over statistically significant improvements within all system pairs in the test beds." ></td>
	<td class="line x" title="109:202	First, Table 3 shows the amount of significant improvements over human judgements according to the Wilcoxon statistical significant test (  0.025)." ></td>
	<td class="line x" title="110:202	For instance, the testbed CE2004 consists of 10 systems, i.e. 45 system pairs; from these, in 40 cases (rightmost column) one of the systems significantly improves the other." ></td>
	<td class="line x" title="111:202	Now we would like to know, for every metric, if the pairs which are significantly different according to human judges are also the pairs which are significantly different according to the metric." ></td>
	<td class="line x" title="112:202	Based on these data, we define two metametrics: Significant Improvement Precision (SIP) and Significant Improvement Recall (SIR)." ></td>
	<td class="line x" title="113:202	SIP 309 Systems System pairs Sig." ></td>
	<td class="line x" title="114:202	imp." ></td>
	<td class="line x" title="115:202	CE2004 10 45 40 AE2004 5 10 8 CE2005 5 10 4 AE2005 5 10 6 Total 25 75 58 Table 3: System pairs with a significant difference according to human judgements (Wilcoxon test) (precision) represents the reliability of improvements detected by metrics." ></td>
	<td class="line x" title="116:202	SIR (recall) represents to what extent the metric is able to cover the significant improvements detected by humans." ></td>
	<td class="line x" title="117:202	Let Ih be the set of significant improvements detected by human assessors and Im the set detected by the metric m. Then: SIP = |IhIm||I m| SIR = |IhIm||I h| Figure 3 shows the SIR and SIP values obtained for each metric." ></td>
	<td class="line x" title="118:202	Linguistic metrics achieve higher precision values but at the cost of an important recall decrease." ></td>
	<td class="line x" title="119:202	Given that linguistic metrics require matching translation with references at additional linguistic levels, the significant improvements detected are more reliable (higher precision or SIP), but at the cost of recall over real significant improvements (lower SIR)." ></td>
	<td class="line x" title="120:202	This result supports the behaviour predicted in (Gimenez and M`arquez, 2009)." ></td>
	<td class="line x" title="121:202	Although linguistic metrics were motivated by the idea of modeling linguistic variability, the practical effect is that current linguistic metrics introduce additional restrictions (such as dependency tree overlap, for instance) for accepting automatic translations." ></td>
	<td class="line x" title="122:202	Then they reward precision at the cost of recall in the evaluation process, and this explains the high correlation with human judgements at system level with respect to segment level." ></td>
	<td class="line x" title="123:202	All n-gram based metrics achieve SIP and SIR values between 0.8 and 0.9." ></td>
	<td class="line x" title="124:202	This result suggests that n-gram based metrics are reasonably reliable for this purpose." ></td>
	<td class="line x" title="125:202	Note that the combined metric, ULC (the circle in the figure), achieves results comparable to n-gram based metrics with this test3." ></td>
	<td class="line x" title="126:202	That is, combining linguistic and ngram based metrics preserves the good behavior of n-gram based metrics in this test." ></td>
	<td class="line x" title="127:202	3Notice that we just have 75 significant improvement samples, so small differences in SIP or SIR have no relevance 5.2 Reliability of High and Low Metric Scores The issue tackled in this section is to what extent a very low or high score according to the metric is reliable for detecting extreme cases (very good or very bad translations)." ></td>
	<td class="line x" title="128:202	In particular, note that detecting wrong translations is crucial in order to analyze the system drawbacks." ></td>
	<td class="line x" title="129:202	In order to define an accuracy measure for the reliability of very low/high metric scores, it is necessary to define quality thresholds for both the human assessments and metric scales." ></td>
	<td class="line x" title="130:202	Defining thresholds for manual scores is immediate (e.g., lower than 4/10)." ></td>
	<td class="line x" title="131:202	However, each automatic evaluation metric has its own scale properties." ></td>
	<td class="line x" title="132:202	In order to solve scaling problems we will focus on equivalent rank positions: we associate the ith translation according to the metric ranking with the quality value manually assigned to the ith translation in the manual ranking." ></td>
	<td class="line x" title="133:202	Being Qh(t) and Qm(t) the human and metric assessed quality for the translation t, and being rankh(t) and rankm(t) the rank of the translation t according to humans and the metric, the normalized metric assessed quality is: QNm(t) = Qh(tprime)|(rankh(tprime) = rankm(t)) In order to analyze the reliability of metrics when identifying wrong or high quality translations, we look for contradictory results between the metric and the assessments." ></td>
	<td class="line x" title="134:202	In other words, we look for metric errors in which the quality estimated by the metric is low (QNm(t)3) but the quality assigned by assessors is high (Qh(t)5) or viceversa (QNm(t)7 and Qh(t)4)." ></td>
	<td class="line x" title="135:202	The vertical axis in Figure 4 represents the ratio of errors in the set of low scored translations according to a given metric." ></td>
	<td class="line x" title="136:202	The horizontal axis represents the ratio of errors over the set of high scored translations." ></td>
	<td class="line x" title="137:202	The first observation is that all metrics are less reliable when they assign low scores (which corresponds with the situation A described in Section 4.2)." ></td>
	<td class="line x" title="138:202	For instance, the best metric erroneously assigns a low score in more than 20% of the cases." ></td>
	<td class="line x" title="139:202	In general, the linguistic metrics do not improve the ability to capture wrong translations (horizontal axis in the figure)." ></td>
	<td class="line x" title="140:202	However, again, the combining metric ULC achieves the same reliability as the best n-gram based metric." ></td>
	<td class="line x" title="141:202	310 In order to check the robustness of these results, we computed the correlation of individual metric failures between test beds, obtaining 0.67 Pearson for the lowest correlated test bed pair (AE2004 and CE2005) and 0.88 for the highest correlated pair (AE2004 and CE2004)." ></td>
	<td class="line x" title="142:202	Figure 4: Counter sample ratio for high vs low metric scored translations 5.2.1 Analysis of Evaluation Samples In order to shed some light on the reasons for the automatic evaluation failures when assigning low scores, we have manually analyzed cases in which a metric score is low but the quality according to humans is high (QNm  3 and Qh  7)." ></td>
	<td class="line o" title="143:202	We have studied 100 sentence evaluation cases from representatives of each metric family including: 1PER, BLEU, DP-Or-star, GTM (e = 2), METEOR and ROUGEL." ></td>
	<td class="line x" title="144:202	The evaluation cases have been extracted from the four test beds." ></td>
	<td class="line x" title="145:202	We have identified four main (non exclusive) failure causes: Format issues, e.g. US  vs United States)." ></td>
	<td class="line x" title="146:202	Elements such as abbreviations, acronyms or numbers which do not match the manual translation." ></td>
	<td class="line x" title="147:202	Pseudo-synonym terms, e.g. US Scheduled the Release vs. US set to Release)." ></td>
	<td class="line x" title="148:202	) In most of these cases, synonymy can only be identified from the discourse context." ></td>
	<td class="line x" title="149:202	Therefore, terminological resources (e.g., WordNet) are not enough to tackle this problem." ></td>
	<td class="line x" title="150:202	Non relevant information omissions, e.g. Thank you vs. Thank you very much or dollar vs. US dollar))." ></td>
	<td class="line x" title="151:202	The translation system obviates some information which, in context, is not considered crucial by the human assessors." ></td>
	<td class="line x" title="152:202	This effect is specially important in short sentences." ></td>
	<td class="line x" title="153:202	Incorrect structures that change the meaning while maintaining the same idea (e.g., Bush Praises NASA s Mars Mission vs  Bush praises nasa of Mars mission )." ></td>
	<td class="line x" title="154:202	Note that all of these kinds of failure except formatting issues require deep linguistic processing while n-gram overlap or even synonyms extracted from a standard ontology are not enough to deal with them." ></td>
	<td class="line x" title="155:202	This conclusion motivates the incorporation of linguistic processing into automatic evaluation metrics." ></td>
	<td class="line x" title="156:202	5.3 Ability to Deal with Translations that are Dissimilar to References." ></td>
	<td class="line x" title="157:202	The results presented in Section 5.2 indicate that a high score in metrics tends to be highly related to truly good translations." ></td>
	<td class="line x" title="158:202	This is due to the fact that a high word overlapping with human references is a reliable evidence of quality." ></td>
	<td class="line x" title="159:202	However, in some cases the translations to be evaluated are not so similar to human references." ></td>
	<td class="line x" title="160:202	An example of this appears in the test bed NIST05AE which includes a human-aided system, LinearB (Callison-Burch, 2005)." ></td>
	<td class="line x" title="161:202	This system produces correct translations whose words do not necessarily overlap with references." ></td>
	<td class="line x" title="162:202	On the other hand, a statistics based system tends to produce incorrect translations with a high level of lexical overlapping with the set of human references." ></td>
	<td class="line x" title="163:202	This case was reported by Callison-Burch et al.(2006) and later studied by Gimenez and M`arquez (2007)." ></td>
	<td class="line x" title="165:202	They found out that lexical metrics fail to produce reliable evaluation scores." ></td>
	<td class="line x" title="166:202	They favor systems which share the expected reference sublanguage (e.g., statistical) and penalize those which do not (e.g., LinearB)." ></td>
	<td class="line x" title="167:202	We can find in our test bed many instances in which the statistical systems obtain a metric score similar to the assisted system while achieving a lower mark according to human assessors." ></td>
	<td class="line x" title="168:202	For instance, for the following translations, ROUGEL assigns a slightly higher score to the output of a statistical system which contains a lot of grammatical and syntactical failures." ></td>
	<td class="line x" title="169:202	Human assisted system: The Chinese President made unprecedented criticism of the leaders of Hong Kong after political failings in the former British colony on Monday . Human assessment=8.5." ></td>
	<td class="line x" title="170:202	Statistical system: Chinese President Hu Jintao today unprecedented criticism to the leaders of Hong Kong wake political and financial failure in the former British colony." ></td>
	<td class="line x" title="171:202	Human assessment=3." ></td>
	<td class="line x" title="172:202	311 Figure 5: Maximum translation quality decreasing over similarly scored translation pairs." ></td>
	<td class="line x" title="173:202	In order to check the metric resistance to be cheated by translations with high lexical overlapping, we estimate the quality decrease that we could cause if we optimized the human-aided translations according to the automatic metric." ></td>
	<td class="line x" title="174:202	For this, we consider in each translation case c, the worse automatic translation t that equals or improves the human-aided translation th according to the automatic metric m. Formally the averaged quality decrease is: Quality decrease(m) = Avgc(maxt(Qh(th)Qh(t)|Qm(th)Qm(t))) Figure 5 illustrates the results obtained." ></td>
	<td class="line x" title="175:202	All metrics are suitable to be cheated, assigning similar or higher scores to worse translations." ></td>
	<td class="line x" title="176:202	However, linguistic metrics are more resistant." ></td>
	<td class="line x" title="177:202	In addition, the combined metric ULC obtains the best results, better than both linguistic and n-gram based metrics." ></td>
	<td class="line x" title="178:202	Our conclusion is that including higher linguistic levels in metrics is relevant to prevent ungrammatical n-gram matching to achieve similar scores than grammatical constructions." ></td>
	<td class="line x" title="179:202	5.4 The Oracle System Test In order to obtain additional evidence about the usefulness of combining evaluation metrics at different processing levels, let us consider the following situation: given a set of reference translations we want to train a combined system that takes the most appropriate translation approach for each text segment." ></td>
	<td class="line x" title="180:202	We consider the set of translations system presented in each competition as the translation approaches pool." ></td>
	<td class="line x" title="181:202	Then, the upper bound on the quality of the combined system is given by the Metric OST maxOST 6.72 ULC 5.79 ROUGEW 5.71 DP-Or-star 5.70 CP-Oc-star 5.70 NIST 5.70 randOST 5.20 minOST 3.67 Table 4: Metrics ranked according to the Oracle System Test predictive power of the employed automatic evaluation metric." ></td>
	<td class="line x" title="182:202	This upper bound is obtained by selecting the highest scored translation t according to a specific metric m for each translation case c. The Oracle System Test (OST) consists of computing the averaged human assessed quality Qh of the selected translations according to human assessors across all cases." ></td>
	<td class="line x" title="183:202	Formally: OST(m) = Avgc(Qh(Argmaxt(Qm(t))|tc)) We use the sum of adequacy and fluency, both in a 1-5 scale, as a global quality measure." ></td>
	<td class="line x" title="184:202	Thus, OST scores are in a 2-10 range." ></td>
	<td class="line x" title="185:202	In summary, the OST represents the best combined system that could be trained according to a specific automatic evaluation metric." ></td>
	<td class="line x" title="186:202	Table 4 shows OST values obtained for the best metrics." ></td>
	<td class="line x" title="187:202	In the table we have also included a random, a maximum (always pick the best translation according to humans) and a minimum (always pick the worse translation according to human) OST for all 4." ></td>
	<td class="line x" title="188:202	The most remarkable result in Table 4 is that metrics are closer to the random baseline than to the upperbound (maximum OST)." ></td>
	<td class="line x" title="189:202	This result confirms the idea that an improvement on metric reliability could contribute considerably to the systems optimization process." ></td>
	<td class="line x" title="190:202	However, the key point is that the combined metric, ULC, improves all the others (5.79 vs. 5.71), indicating the importance of combining n-gram and linguistic features." ></td>
	<td class="line x" title="191:202	6 Conclusions Our experiments show that, on one hand, traditional n-gram based metrics are more or equally 4In all our experiments, the meta-metric values are computed over each test bed independently before averaging in order to assign equal relevance to the four possible contexts (test beds) 312 reliable for estimating the translation quality at the segment level, for predicting significant improvement between systems and for detecting poor and excellent translations." ></td>
	<td class="line x" title="192:202	On the other hand, linguistically motivated metrics improve n-gram metrics in two ways: (i) they achieve higher correlation with human judgements at system level and (ii) they are more resistant to reward poor translations with high word overlapping with references." ></td>
	<td class="line x" title="193:202	The underlying phenomenon is that, rather than managing the linguistics variability, linguistic based metrics introduce additional restrictions for assigning high scores." ></td>
	<td class="line x" title="194:202	This effect decreases the recall over significant system improvements achieved by n-gram based metrics and does not solve the problem of detecting wrong translations." ></td>
	<td class="line x" title="195:202	Linguistic metrics, however, are more difficult to cheat." ></td>
	<td class="line x" title="196:202	In general, the greatest pitfall of metrics is the low reliability of low metric values." ></td>
	<td class="line x" title="197:202	Our qualitative analysis of evaluated sentences has shown that deeper linguistic techniques are necessary to overcome the important surface differences between acceptable automatic translations and human references." ></td>
	<td class="line x" title="198:202	But our key finding is that combining both kinds of metrics gives top performance according to every meta-evaluation criteria." ></td>
	<td class="line x" title="199:202	In addition, our Combined System Test shows that, when training a combined translation system, using metrics at several linguistic processing levels improves substantially the use of individual metrics." ></td>
	<td class="line x" title="200:202	In summary, our results motivate: (i) working on new linguistic metrics for overcoming the barrier of linguistic variability and (ii) performing new metric combining schemes based on linear regression over human judgements (Kulesza and Shieber, 2004), training models over human/machine discrimination (Albrecht and Hwa, 2007) or non parametric methods based on reference to reference distances (Amigo et al., 2005)." ></td>
	<td class="line x" title="201:202	Acknowledgments This work has been partially supported by the Spanish Government, project INES/Text-Mess." ></td>
	<td class="line x" title="202:202	We are indebted to the three ACL anonymous reviewers which provided detailed suggestions to improve our work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0403
A Simple Automatic MT Evaluation Metric
Homola, Petr;Kuboň, Vladislav;Pecina, Pavel;"></td>
	<td class="line x" title="1:83	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 3336, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:83	c2009 Association for Computational Linguistics A Simple Automatic MT Evaluation Metric Petr Homola Charles University Prague, Czech Republic Vladislav Kubon Charles University Prague, Czech Republic {homola|vk|pecina}@ufal.mff.cuni.cz Pavel Pecina Charles University Prague, Czech Republic Abstract This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deficits of the standard BLEU metric from a slightly different angle." ></td>
	<td class="line x" title="3:83	It employes Levenshteins edit distance for establishing alignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages." ></td>
	<td class="line x" title="4:83	It also incorporates a very simple measure expressing the differences in the word order." ></td>
	<td class="line x" title="5:83	The paper also includes evaluation on the data from the previous SMT workshop for several language pairs." ></td>
	<td class="line x" title="6:83	1 Introduction The problem of finding a reliable machine translation metrics corresponding with a human judgment has recently returned to the centre of attention." ></td>
	<td class="line x" title="7:83	After a brief period following the introduction of generally accepted and widely used metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), when it seemed that this persistent problem has finally been solved, the researchers active in the field of machine translation (MT) started to express their worries that although these metrics are simple, fast and able to provide consistent results for a particular system during its development, they are not sufficiently reliable for the comparison of different systems or different language pairs." ></td>
	<td class="line x" title="8:83	The results of the NIST evaluation in 2005 (Le and Przybocki, 2005) have also strengthened the suspicion that the correlation between human judgment and the BLEU and NIST measures is not as strong as it was widely believed." ></td>
	<td class="line x" title="9:83	Both measures seem to favor the MT output created by systems based on n-gram architecture, they are unable to take into account certain factors which are very important for the human judges of translation quality." ></td>
	<td class="line x" title="10:83	The article (Callison-Burch et al., 2006) thoroughly discusses the deficits of the BLEU and similar metrics." ></td>
	<td class="line oc" title="11:83	The authors claim that the existing automatic metrics, including some of the new and seemingly more reliable ones as e.g. Meteor (cf.(Banerjee and Lavie, 2005)) ." ></td>
	<td class="line n" title="13:83	they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation. This claim is supported by a construction of translation variations which have identical BLEU score, but which are very different for a human judge." ></td>
	<td class="line x" title="14:83	The authors identify three prominent factors which contribute to the inadequacy of BLEU  the failure to deal with synonyms and paraphrases, no penalties for missing content, and the crudeness of the brevity penalty." ></td>
	<td class="line x" title="15:83	Let us add some more factors based on our experiments with languages typologically different than English, Arabic or Chinese, which are probably the languages most frequently used in recent shared-task MT evaluations." ></td>
	<td class="line x" title="16:83	The highly inflected languages and languages with a higher degree of word-order freedom may provide additional examples of sentences in which relatively small alterations of correct word forms may have a dire effect on the BLEU score while the sentence still remains understandable and acceptable for human evaluators." ></td>
	<td class="line x" title="17:83	The effect of rich inflection has been observed for example in (Tynovsky, 2007), where the author mentions the fact that the BLEU score used for measuring the improvements in his experimental Czech-German EBMT system penalized heavily all subtle errors in Czech morphology arising from an out-of-context combined partial translations taken from different examples." ></td>
	<td class="line x" title="18:83	The problem of the insensitivity of BLEU to the variations of the order of n-grams identified in reference translations has already been mentioned in 33 the paper (Callison-Burch et al., 2006)." ></td>
	<td class="line x" title="19:83	The authors showed examples where changing a good word order into an unacceptable one did not affect the BLEU score." ></td>
	<td class="line x" title="20:83	We may add a different example documenting the phenomenon that a pair of syntactically correct Czech sentences with the same word forms, differing only in the word order whose n-gram score for n = 2, 3, and 4 differs greatly." ></td>
	<td class="line x" title="21:83	Let us take one of the sentences from the 2008 SMT workshop and its reference translation: When Caligula appointed his horse to the Senate, the horse at least did not have blood on its hoofs." ></td>
	<td class="line x" title="22:83	 Kdyz Caligula zvolil do senatu sveho kone, nemel jeho kun aspon na kopytech krev." ></td>
	<td class="line x" title="23:83	If we modify the Czech reference sentence into Kdyz sveho kone do senatu zvolil Caligula, jeho kun aspon nemel na kopytech krev., we destroy 8 out of 15 bigrams, 11 out of 14 trigrams and 12 out of 13 quadrigrams while we still have sentence with almost identical meaning and probably very similar human evaluation." ></td>
	<td class="line x" title="24:83	The BLEU score of the modified sentence is, however, lower than it would be for the identical copy of the reference translation." ></td>
	<td class="line x" title="25:83	2 The description of the proposed metric There is one aspect of the problem of a MT quality metric which tends to be overlooked but which is very important from the practical point of view." ></td>
	<td class="line x" title="26:83	This aspect concerns the expected difficulties when post-editing the MT output." ></td>
	<td class="line x" title="27:83	It is very important for everybody who really wants to use the MT output and who faces the decision whether it is better to post-edit the MT output or whether a new translation made by human translators would be faster and more efficient way towards the desired quality." ></td>
	<td class="line x" title="28:83	It is no wonder that such a metric is mentioned only in connection with systems which really aim at practical exploitation, not with a majority of experimental MT system which will hardly ever reach the stage of industrial exploitation." ></td>
	<td class="line x" title="29:83	We have described one example of such practically oriented metric in (Hajic et al., 2003)." ></td>
	<td class="line x" title="30:83	The metric exploits the matching algorithm of Trados Translators Workbench for obtaining the percentage of differences between the MT output and the reference translation (created by post-editing the MT output)." ></td>
	<td class="line x" title="31:83	The advantage of this measure is its close connection to the real world of human translating by means of translation memory, the disadvantage concerns the use of a proprietary matching algorithm which has not been made public and which requires the actual use of the Trados software." ></td>
	<td class="line x" title="32:83	Nevertheless, the matching algorithm of Trados gives results which to a great extent correspond to a much simpler traditional metric, to the Levenshteins edit distance." ></td>
	<td class="line x" title="33:83	The use of this metric may help to refine a very strict treatment of wordform differences by BLEU." ></td>
	<td class="line x" title="34:83	A similar approach at the level of unigram matching has been used by the well-known METEOR metric (Agarwal and Lavie, 2008), which proved its qualities during the previous MT evaluation task in 2008 (CallisonBurch et al., 2008)." ></td>
	<td class="line x" title="35:83	Meteor uses Porter stemmer as one step in the word alignment algorithm." ></td>
	<td class="line x" title="36:83	It also relies on synonymy relations in WordNet." ></td>
	<td class="line x" title="37:83	When designing our metric, we have decided to follow two general strategies  to use as simple means as possible and to avoid using any language dependent tools or resources." ></td>
	<td class="line x" title="38:83	Levenshtein metric (or its modification for word-level edit distance) therefore seemed to be the best candidate for several aspects of the proposed measure." ></td>
	<td class="line x" title="39:83	The first aspect we have decided to include was the inflection." ></td>
	<td class="line x" title="40:83	The edit distance has one advantage over the language independent stemmer  it can uniformly handle the differences regardless of their position in the string." ></td>
	<td class="line x" title="41:83	The stemmer will probably face certain problems with changes inside the stem as e.g. in the Czech equivalent of the word house in different cases dum (nom.sg)  domu (gen., dat." ></td>
	<td class="line x" title="42:83	or loc." ></td>
	<td class="line x" title="43:83	sg.)" ></td>
	<td class="line x" title="44:83	or German Mann in different numbers der Mann (sg.)" ></td>
	<td class="line x" title="45:83	 die Manner (pl.), while the edit distance will treat them uniformly with the variation of prefixes, suffixes and infixes." ></td>
	<td class="line x" title="46:83	As mentioned above, we have also intended to aim at the treatment of the free word order in our metric." ></td>
	<td class="line x" title="47:83	However this seems to be one of the major flaws of the BLEU score, it turned out that the word order is extremely difficult if we stick to the use of simple and language independent means." ></td>
	<td class="line x" title="48:83	If we take Czech as an example of a language with relatively high degree of word-order freedom, we can still find certain restrictions (e.g. the sentencesecond position of clitics, their mutual order, the adjectives typically, but not always preceding the nouns they depend upon etc.) which will definitely influence the human judgment of the acceptability of a particular sentence." ></td>
	<td class="line x" title="49:83	These restrictions are language dependent (for example Polish, the 34 language very closely related to Czech, has different rules for congruent attributes, the adjectives stand much more often to the right of the governing noun) and they are also very difficult to capture algorithmically." ></td>
	<td class="line x" title="50:83	If the MT output is compared to a single reference translation only, there is, in fact, no way how the metric could account for the possible correct variations of the word order without exploiting very deep language dependent information." ></td>
	<td class="line x" title="51:83	If there are more reference translations, it is possible that they will provide the natural variations of the word order, but it, in fact, means that if we want to stick to the above mentioned requirements, we have to give up the hope that our metric will capture this important phenomenon." ></td>
	<td class="line x" title="52:83	2.1 Word alignment algorithm In order to capture the word form variations caused by the inflection, we have decided to employ the following alignment algorithm at the level of individual word forms." ></td>
	<td class="line x" title="53:83	Let us use the following notation: Let the reference translation R be a sequence of words ri, where i < 1,,n >." ></td>
	<td class="line x" title="54:83	Let the MT output T be a sequence of words tj, where j< 1,,m>." ></td>
	<td class="line x" title="55:83	Let us also set a threshold of similarity s < 0,1 >." ></td>
	<td class="line x" title="56:83	(s roughly expresses how different the forms of a lemma may be." ></td>
	<td class="line x" title="57:83	The idea behind this criterion is that a mistake in one morphological category (reflected mostly by a different ending of the corresponding word form) is not as serious as a completely different lexeme." ></td>
	<td class="line x" title="58:83	This holds especially for morphologically rich languages that can have tens or even hundreds of distinct word forms for a single lemma.)" ></td>
	<td class="line x" title="59:83	Starting from t1, let us find for each tj the best ri for i < 1,,n > such that the edit distance dj from tj to ri normalized by the length of tj is minimal and at the same time dj < s. If the ri is already aligned to some tk, k < j and the edit distance dk > dj, then align tj to ri and re-calculate the alignment for tk to its second best candidate, otherwise take the second best candidaterl conforming with the above mentioned conditions and align it to tj." ></td>
	<td class="line x" title="60:83	As a result of this process, we get the alignment score ATR from T to R. ATR = summationtext(1d i) m (for i < 1,,n >)where d i = 1 for those word forms ti which are not aligned to any of the word forms rj from R. Then we calculate the alignment score ART using the same algorithm and aligning the words from R to T. The similarity score S equals the minimum from ATR and ART . The way how the similarity score S is constructed ensures that the score takes into account a difference in length between T and R, therefore it is not necessary to include any brevity penalty into the metric." ></td>
	<td class="line x" title="61:83	2.2 A structural metric In order to express word-order difference between the MT output and the reference translation we have designed a structural part of the metric." ></td>
	<td class="line x" title="62:83	It is based on an algorithm similar to one of the standard sorting methods, an insert sort." ></td>
	<td class="line x" title="63:83	The reference translation R represents the desired word order and the algorithm counts the number of operations necessary for obtaining the correct word order from the word order of the MT output T by inserting the words ti to their desired positions rj (ti is aligned to rj)." ></td>
	<td class="line x" title="64:83	If a particular word ti is not aligned to any rj, a penalty of 1 is added to the number of operations." ></td>
	<td class="line x" title="65:83	2.3 A combination of both metrics The overall score is computed as a weighted average of both metrics mentioned above." ></td>
	<td class="line x" title="66:83	LetLbe the lexical similarity score and M the structural score based on a word mapping." ></td>
	<td class="line x" title="67:83	Then then overall score S can be obtained as follows: S = aL+bM The coefficients a and b must sum up to one." ></td>
	<td class="line x" title="68:83	They allow to capture the difference in the degree of word-order freedom among target languages." ></td>
	<td class="line x" title="69:83	The coefficient b should be set lower for the target languages with more free word-order." ></td>
	<td class="line x" title="70:83	Because both then partial measuresLandM have values in the interval < 0,1 >, the value of S will also fall into this interval." ></td>
	<td class="line x" title="71:83	3 The experiment We have performed a test of the proposed metric using the data from the last years SMT workshop.1 The parameters a, b, and s have been set to the same value for all evaluated language pairs, no language dependent alterations were tested in this experiment: Parameter Value s 0.15 a 0.9 b 0.1 1The data are available at http://www.statmt.org/wmt08." ></td>
	<td class="line x" title="72:83	35 The values for the parameters have been set up empirically with special attention being paid to Czech, the only language with really rich inflection among the languages being tested." ></td>
	<td class="line x" title="73:83	We have performed sentence-level and systemlevel evaluation using the Spearmans rank correlation coefficient which is defined as follows:  = 1 6 summationtextd2 i n(n21) wheredi = xiyi is the difference between the ranks of corresponding values Xi and Yi and n is the number of values in each data set." ></td>
	<td class="line x" title="74:83	The following scores express the correlation of our automatic metric and the human judgements for the language pairs English-Czech and EnglishGerman." ></td>
	<td class="line x" title="75:83	The sentence-level correlation sent is the average of Spearmans  across all sentences." ></td>
	<td class="line x" title="76:83	Language pair Metric sent sys English-Czech proposed 0.20 0.50 English-Czech BLEU 0.21 0.50 English-German proposed 0.91 0.37 English-German BLEU 0.90 0.20 3.1 Conclusions The metric presented in this paper attempts to combine some of the important factors which seem to be neglected by some generally accepted MT evaluation metrics." ></td>
	<td class="line x" title="77:83	Inspired by the fact that human judges tend to accept incorrect word-forms of corectly translated lemmas, it employs a similarity measure relaxing the requirements on identity (or similarity) of matching word forms in the MT output and the reference translation." ></td>
	<td class="line x" title="78:83	At the same time, it also incorporates a penalty for different length of the MT output and the reference translation." ></td>
	<td class="line x" title="79:83	The second component of the metric tackles the problem of incorrect word-order." ></td>
	<td class="line x" title="80:83	The constants used in the metric allow to set the weight of its two components with regard to the target language properties." ></td>
	<td class="line x" title="81:83	The experiments performed on the data from the previous shared evaluation task are promising." ></td>
	<td class="line x" title="82:83	They indicate that the first component of the metric succesfully replaces the strict unigram measure used in BLEU while the second component may require certain alteration in order to achieve a higher correlation with human judgement." ></td>
	<td class="line x" title="83:83	Acknowledgments The presented research has been supported by the grant No. 1ET100300517 of the GAAV CR and by Ministry of Education of the Czech Republic, project MSM 0021620838." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0404
Machine Translation Evaluation with Textual Entailment Features
Padó, Sebastian;Galley, Michel;Jurafsky, Daniel;Manning, Christopher D.;"></td>
	<td class="line x" title="1:155	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 3741, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:155	c2009 Association for Computational Linguistics Textual Entailment Features for Machine Translation Evaluation Sebastian Pado, Michel Galley, Dan Jurafsky, Christopher D. Manning Stanford University {pado,mgalley,jurafsky,manning}@stanford.edu Abstract We present two regression models for the prediction of pairwise preference judgments among MT hypotheses." ></td>
	<td class="line x" title="3:155	Both models are based on feature sets that are motivated by textual entailment and incorporate lexical similarity as well as local syntactic features and specific semantic phenomena." ></td>
	<td class="line x" title="4:155	One model predicts absolute scores; the other one direct pairwise judgments." ></td>
	<td class="line x" title="5:155	We find that both models are competitive with regression models built over the scores of established MT evaluation metrics." ></td>
	<td class="line x" title="6:155	Further data analysis clarifies the complementary behavior of the two feature sets." ></td>
	<td class="line x" title="7:155	1 Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics." ></td>
	<td class="line x" title="8:155	Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf.BLEU (Papineni et al., 2002), NIST (Doddington, 2002)." ></td>
	<td class="line x" title="10:155	These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation." ></td>
	<td class="line x" title="11:155	With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al.(2006))." ></td>
	<td class="line x" title="13:155	In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensedfromunwantedvariation(GimenezandM`arquez, 2008)." ></td>
	<td class="line oc" title="14:155	However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008)." ></td>
	<td class="line x" title="15:155	Other proposalsuse structural informationsuchasdependency edges (Owczarzak et al., 2007)." ></td>
	<td class="line x" title="16:155	In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM." ></td>
	<td class="line x" title="17:155	The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred HYP: Virus was infected." ></td>
	<td class="line x" title="18:155	REF: No one was infected by the virus." ></td>
	<td class="line x" title="19:155	no entailment no entailment HYP: The virus did not infect anybody." ></td>
	<td class="line x" title="20:155	REF: No one was infected by the virus." ></td>
	<td class="line x" title="21:155	entailment entailment Figure 1: Entailment status between an MT system hypothesis and a reference translation for good translations (above) and bad translations (below)." ></td>
	<td class="line x" title="22:155	suggests that the quality of an MT hypothesis should be predictable by a combination of lexical and structural features that model the matches and mismatches between system output and reference translation." ></td>
	<td class="line x" title="23:155	We use supervised regression models to combine these features and analyze feature weights to obtain further insights into the usefulness of different feature types." ></td>
	<td class="line x" title="24:155	2 Textual Entailment for MT Evaluation 2.1 Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al.(2005) as a concept that corresponds more closely to common sense reasoning than classical, categorical entailment." ></td>
	<td class="line x" title="26:155	Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true." ></td>
	<td class="line x" title="27:155	Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006)." ></td>
	<td class="line x" title="28:155	Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1." ></td>
	<td class="line x" title="29:155	Very good MT output should entail thereferencetranslation." ></td>
	<td class="line x" title="30:155	Incontrast, missinghypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions." ></td>
	<td class="line x" title="31:155	Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over 37 surface-based methods, provided that the linguistic analysis was sufficiently robust." ></td>
	<td class="line x" title="32:155	Thus, for RTE, deep matching outperforms surface matching." ></td>
	<td class="line x" title="33:155	The reason is that linguistic representation makes it considerably easier to distinguish admissible variation (i.e., paraphrase) from true, meaning-changing divergence." ></td>
	<td class="line x" title="34:155	Admissible variation may be lexical (synonymy), structural (word and phrase placement), or both (diathesis alternations)." ></td>
	<td class="line x" title="35:155	The working hypothesis of this paper is that the benefits of deeper analysis carry over to MT evaluation." ></td>
	<td class="line x" title="36:155	More specifically, we test whether the features that allow good performance on the RTE task can also predict human judgments for MT output." ></td>
	<td class="line x" title="37:155	Analogously to RTE, these features should help us to differentiate meaning preserving translation variants from bad translations." ></td>
	<td class="line x" title="38:155	Nevertheless, there are also substantial differences between TE and MT evaluation." ></td>
	<td class="line x" title="39:155	Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation." ></td>
	<td class="line x" title="40:155	Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output." ></td>
	<td class="line x" title="41:155	However, there is a second difference between the tasks that works to our advantage." ></td>
	<td class="line x" title="42:155	Due to its strict compositional nature, TErequiresanaccuratesemanticanalysisofallsentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (MacCartney and Manning, 2008)." ></td>
	<td class="line x" title="43:155	In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so." ></td>
	<td class="line x" title="44:155	We therefore expect that even noisy entailment features can be predictive in MT evaluation." ></td>
	<td class="line x" title="45:155	2.2 Entailment-based prediction of MT quality Regression-based prediction." ></td>
	<td class="line x" title="46:155	Experiences from the annotation of MT quality judgments show that human raters have difficulty in consistently assigning absolute scores to MT system output, due to the number of ways in which MT output can deviate." ></td>
	<td class="line x" title="47:155	Thus, the human annotation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al., 2008)." ></td>
	<td class="line x" title="48:155	This section presents two models for the prediction of pairwise preferences." ></td>
	<td class="line x" title="49:155	The first model (ABS) is a regularized linear regression model over entailment-motivated features (see below) that predicts an absolute score for each referencehypothesis pair." ></td>
	<td class="line x" title="50:155	Pairwise preferences are created simply by comparing the absolute predicted scores." ></td>
	<td class="line x" title="51:155	This model is more general, since it can also be used where absolute score predictions are desirable; furthermore, the model is efficient with a runtime linear in the number of systems and corpus size." ></td>
	<td class="line x" title="52:155	On the downside, this model is not optimized for the prediction of pairwise judgments." ></td>
	<td class="line x" title="53:155	The second model we consider is a regularized logistic regression model (PAIR) that is directly optimized to predict a weighted binary preference for each hypothesis pair." ></td>
	<td class="line x" title="54:155	This model is less efficient since its runtime is Alignment score(3) Unaligned material (10) Adjuncts (7) Apposition (2) Modality (5) Factives (8) Polarity (5) Quantors (4) Tense (2) Dates (6) Root (2) Semantic Relations (4) Semantic relatedness (7) Structural Match (5) Compatibility of locations and entities (4) Table 1: Entailment feature groups provided by the Stanford RTE system, with number of features quadratic in the number of systems." ></td>
	<td class="line x" title="55:155	On the other hand, it can be trained on more reliable pairwise preference judgments." ></td>
	<td class="line x" title="56:155	In a second step, we combine the individual decisions to compute the highest-likelihood total ordering of hypotheses." ></td>
	<td class="line x" title="57:155	The construction of an optimal ordering from weighted pairwise preferences is an NPhard problem (via reduction of CYCLIC-ORDERING; Barzilay and Elhadad, 2002), but a greedy search yields a close approximation (Cohen et al., 1999)." ></td>
	<td class="line x" title="58:155	Both models can be used to predict system-level scores from sentence-level scores." ></td>
	<td class="line x" title="59:155	Again, we have two method for doing this." ></td>
	<td class="line x" title="60:155	The basic method (BASIC) predicts the quality of each system directly as the percentage of sentences for which its output was rated best among all systems." ></td>
	<td class="line x" title="61:155	However, we noticed that the manual rankings for the WMT 2007 dataset show a tie for best system for almost 30% of sentences." ></td>
	<td class="line x" title="62:155	BASIC is systematically unable to account for these ties." ></td>
	<td class="line x" title="63:155	We therefore implemented a tie-aware prediction method (WITHTIES) that uses the same sentence-level output as BASIC, but computes system-level quality differently, as the percentage of sentences where the systems hypothesis was scored better or at most  worse than the best system, for some global tie interval ." ></td>
	<td class="line x" title="64:155	Features." ></td>
	<td class="line x" title="65:155	We use the Stanford RTE system (MacCartney et al., 2006) to generate a set of entailment features (RTE) for each pair of MT hypothesis and reference translation." ></td>
	<td class="line x" title="66:155	Features are generated in both directions to avoid biases towards short or long translations." ></td>
	<td class="line x" title="67:155	The Stanford RTE system uses a three-stage architecture." ></td>
	<td class="line x" title="68:155	It (a) constructs a robust, dependency-based linguistic analysis of the two sentences; (b) identifies the best alignment between the two dependency graphs given similarity scores from a range of lexical resources, using a Markov Chain Monte Carlo sampling strategy; and (c) computes roughly 75 features over the aligned pair of dependency graphs." ></td>
	<td class="line x" title="69:155	The different feature groups are shown in Table 1." ></td>
	<td class="line x" title="70:155	A small number features are real-valued, measuring different quality aspects of the alignment." ></td>
	<td class="line x" title="71:155	The other features are binary, indicating matches and mismatches of different types (e.g., alignment between predicates embedded under compatible or incompatible modals, respectively)." ></td>
	<td class="line x" title="72:155	To judge to what extent the entailment-based model delivers improvements that cannot be obtained with established methods, we also experiment with a feature set 38 formed from a set of established MT evaluation metrics (TRADMT)." ></td>
	<td class="line x" title="73:155	We combine different parametrization of (smoothed) BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006), to give a total of roughly 100 features." ></td>
	<td class="line x" title="74:155	Finally, we consider a combination of both feature sets (COMB)." ></td>
	<td class="line x" title="75:155	3 Experimental Evaluation Setup." ></td>
	<td class="line x" title="76:155	To assess and compare the performance of our models, we use corpora that were created by past instances of the WMT workshop." ></td>
	<td class="line x" title="77:155	We optimize the feature weights for the ABS models on the WMT 2006 and 2007 absolute score annotations, and correspondingly for the PAIR models on the WMT 2007 absolute score and ranking annotations." ></td>
	<td class="line x" title="78:155	All models are evaluated on WMT 2008 to compare against the published results." ></td>
	<td class="line x" title="79:155	Finally, we need to set the tie interval ." ></td>
	<td class="line x" title="80:155	Since we did not want to optimize , we simply assumed that the percentage of ties observed on WMT 2007 generalizes to test sets such as the 2008 dataset." ></td>
	<td class="line x" title="81:155	We set  so that there are ties for first place on 30% of the sentences, with good practical success (see below)." ></td>
	<td class="line x" title="82:155	Results." ></td>
	<td class="line x" title="83:155	Table 2 shows our results." ></td>
	<td class="line x" title="84:155	The first results column (Cons) shows consistency, i.e., accuracy in predicting human pairwise preference judgments." ></td>
	<td class="line x" title="85:155	Note that the performance of a random baseline is not at 50%, but substantially lower." ></td>
	<td class="line x" title="86:155	This is due to (a) the presence of contradictions and ties in the human judgments, which cannot be predicted; and (b) WMTs requirement to compute a total ordering of all translations for a given sentence (rather than independent binary judgments), which introduces transitivity constraints." ></td>
	<td class="line x" title="87:155	See CallisonBurch et al.(2008) for details." ></td>
	<td class="line x" title="89:155	Among our models, PAIR shows a somewhat better consistency than ABS, as can be expected from a model directly optimized on pairwise judgments." ></td>
	<td class="line x" title="90:155	Across feature sets, COMB works best with a consistency of 0.53, competitive with published WMT 2008 results." ></td>
	<td class="line x" title="91:155	The two final columns (BASIC and WITHTIES) show Spearmans  for the correlation between human judgments and the two types of system-level predictions." ></td>
	<td class="line x" title="92:155	For BASIC system-level predictions, we find that PAIR performs considerably worse than ABS, by a margin of up to  =0.1." ></td>
	<td class="line x" title="93:155	Recall that the system-level analysisconsidersonlythetop-rankedhypotheses; apparently, a model optimized on pairwise judgments has a harder time choosing the best among the top-ranked hypotheses." ></td>
	<td class="line x" title="94:155	This interpretation is supported by the large benefit that PAIR derives from explicit tie modeling." ></td>
	<td class="line x" title="95:155	ABS gains as well, although not as much, so that the correlation of the tie-aware predictions is similar for ABS and PAIR." ></td>
	<td class="line x" title="96:155	Comparing different feature sets, BASIC show a similar pattern to the consistency figures." ></td>
	<td class="line x" title="97:155	There is no clear winner between RTE and TRADMT." ></td>
	<td class="line x" title="98:155	The performance of TRADMT is considerably better than the performance of BLEU and TER in the WMT 2008 evaluation, where   0.55." ></td>
	<td class="line x" title="99:155	RTE is able to match the performance of an Model Feature set Cons (Acc.)" ></td>
	<td class="line x" title="100:155	BASIC () WITHTIES () ABS TRADMT 0.50 0.74 0.74 ABS RTE 0.51 0.72 0.78 ABS COMB 0.51 0.74 0.74 PAIR TRADMT 0.52 0.63 0.73 PAIR RTE 0.51 0.66 0.77 PAIR COMB 0.53 0.70 0.77 WMT 2008 (worst) 0.44 0.37 WMT 2008 (best) 0.56 0.83 Table 2: Evaluation on the WMT 2008 dataset for our regressionmodels, comparedtoresultsfromWMT2008 ensemble of state-of-the-art metrics, which validates our hope that linguistically motivated entailment features are sufficiently robust to make a positive contribution in MT evaluation." ></td>
	<td class="line x" title="101:155	Furthermore, the two individual feature sets are outperformed by the combined feature set COMB." ></td>
	<td class="line x" title="102:155	We interpret this as support for our regressionbased combination approach." ></td>
	<td class="line x" title="103:155	Moving to WITHTIES, we see the best results from the RTE model which improves by  =0.06 for ABS and  =0.11 for PAIR." ></td>
	<td class="line x" title="104:155	There is less improvement for theotherfeaturesets, inparticular COMB." ></td>
	<td class="line x" title="105:155	Wesubmitted the two overall best models, ABS-RTE and PAIR-RTE with tie-aware prediction, to the WMT 2009 challenge." ></td>
	<td class="line x" title="106:155	Data Analysis." ></td>
	<td class="line x" title="107:155	We analyzed at the models predictions to gain a better understanding of the differences in the behavior of TRADMT-based and RTE-based models." ></td>
	<td class="line x" title="108:155	As a first step, we computed consistency numbers for the set of top translations (hypotheses that were ranked highest for a given reference) and for the set of bottom translations (hypotheses that were ranked worst for a given reference)." ></td>
	<td class="line x" title="109:155	We found small but consistent differences between the models: RTE performs about 1.5 percent better on the top hypotheses than on the bottom translations." ></td>
	<td class="line x" title="110:155	We found the inverse effect for the TRADMT model, which performs 2 points worse on the top hypotheses than on the bottom hypotheses." ></td>
	<td class="line x" title="111:155	Revisiting our initial concern that the entailment features are too noisy for very bad translations, this finding indicates some ungrammaticality-induced degradation for the entailment features, but not much." ></td>
	<td class="line x" title="112:155	Conversely, these numbers also provide support for our initial hypothesis that surface-based features are good at detecting very deviant translations, but can have trouble dealing with legitimate linguistic variation." ></td>
	<td class="line x" title="113:155	Next, we analyzed the average size of the score differences between the best and second-best hypotheses for correct and incorrect predictions." ></td>
	<td class="line x" title="114:155	We found that the RTE-based model predicted on average almost twice the difference for correct predictions (= 0.30) than for incorrect predictions (=0.16), while the difference was considerably smaller for the TRADMT-based model (=0.17 for correct vs. =0.13 for incorrect)." ></td>
	<td class="line x" title="115:155	We believe it is this better discrimination on the top hypothe39 Segment TRADMT RTE COMB Gold REF: Scottish NHS boards need to improve criminal records checks for employees outside Europe, a watchdog has said." ></td>
	<td class="line x" title="116:155	HYP: The Scottish health ministry should improve the controls on extracommunity employees to check whether they have criminal precedents, said the monitoring committee." ></td>
	<td class="line x" title="117:155	[1357, lium-systran] Rank: 3 Rank: 1 Rank: 2 Rank: 1 REF: Arguments, bullying and fights between the pupils have extended to the relations between their parents." ></td>
	<td class="line x" title="118:155	HYP: Disputes, chicane and fights between the pupils transposed in relations between the parents." ></td>
	<td class="line x" title="119:155	[686, rbmt4] Rank: 5 Rank: 2 Rank: 4 Rank: 5 Table 3: Examples of reference translations and MT output from the WMT 2008 French-English News dataset." ></td>
	<td class="line x" title="120:155	Rank judgments are out of five (smaller is better)." ></td>
	<td class="line x" title="121:155	ses that explains the increased benefit the RTE-based model obtains from tie-aware predictions: if the best hypothesis is wrong, chances are much better than for the TRADMT-based model that counting the secondbest hypothesis as best is correct." ></td>
	<td class="line x" title="122:155	Unfortunately, this property is not shared by COMB to the same degree, and it does not improve as much as RTE." ></td>
	<td class="line x" title="123:155	Table 3 illustrates the difference between RTE and TRADMT." ></td>
	<td class="line x" title="124:155	In the first example, RTE makes a more accurate prediction than TRADMT." ></td>
	<td class="line x" title="125:155	The human raters favorite translation deviates considerably from the reference translation in lexical choice, syntactic structure, and word order, for which it is punished by TRADMT." ></td>
	<td class="line x" title="126:155	In contrast, RTE determines correctly that the propositional content of the reference is almost completely preserved." ></td>
	<td class="line x" title="127:155	The prediction of COMB is between the two extremes." ></td>
	<td class="line x" title="128:155	The second example shows a sentence where RTE provides a worse prediction." ></td>
	<td class="line x" title="129:155	This sentence was rated as bad by the judge, presumably due to the inappropriate translation of the main verb." ></td>
	<td class="line x" title="130:155	This problem, together with the reformulation of the subject, leads TRADMT to correctly predict a low score (rank 5/5)." ></td>
	<td class="line x" title="131:155	RTEs deeper analysis comes up with a high score (rank 2/5), based on the existing semantic overlap." ></td>
	<td class="line x" title="132:155	The combined model is closer to the truth, predicting rank 4." ></td>
	<td class="line x" title="133:155	Feature Weights." ></td>
	<td class="line x" title="134:155	Finally, we assessed the importance of the different entailment feature groups in the RTE model.1 Since the presence of correlated features makes the weights difficult to interpret, we restrict ourselves to two general observations." ></td>
	<td class="line x" title="135:155	First, we find high weights not only for the score of the alignment between hypothesis and reference, but also for a number of syntacto-semantic match and mismatch features." ></td>
	<td class="line x" title="136:155	This means that we do get an additional benefit from the presence of these features." ></td>
	<td class="line x" title="137:155	For example, featureswithanegativeeffectincludedroppingadjuncts, unaligned root nodes, incompatible modality between the main clauses, person and location mismatches (as opposed to general mismatches) and wrongly handled passives." ></td>
	<td class="line x" title="138:155	Conversely, some factors that increase the prediction are good alignment, matching embeddings under factive verbs, and matches between appositions." ></td>
	<td class="line x" title="139:155	1The feature weights are similar for the COMB model." ></td>
	<td class="line x" title="140:155	Second, we find clear differences in the usefulness of feature groups between MT evaluation and the RTE task." ></td>
	<td class="line x" title="141:155	Some of them, in particular structural features, can be linked to the generally lower grammaticality of MT hypotheses." ></td>
	<td class="line x" title="142:155	A case in point is a feature that fires for mismatches between dependents of predicates and which is too unreliable on the SMT data." ></td>
	<td class="line x" title="143:155	Other differences simply reflect that the two tasks have different profiles, as sketched in Section 2.1." ></td>
	<td class="line x" title="144:155	RTE exhibits high feature weights for quantifier and polarity features, both of which have the potential to influence entailment decisions, but are relatively unimportant for MT evaluation, at least at the current state of the art." ></td>
	<td class="line x" title="145:155	4 Conclusion In this paper, we have investigated an approach to MT evaluation that is inspired by the similarity between this task and textual entailment." ></td>
	<td class="line x" title="146:155	Our two models  one predicting absolute scores and one predicting pairwise preference judgments  use entailment features to predict the quality of MT hypotheses, thus replacing surface matching with syntacto-semantic matching." ></td>
	<td class="line x" title="147:155	Both models perform similarly, showing sufficient robustness and coverage to attain comparable performance to a committee of established MT evaluation metrics." ></td>
	<td class="line x" title="148:155	We have described two refinements: (1) combining the features into a superior joint model; and (2) adding a confidence interval around the best hypothesis to model ties for first place." ></td>
	<td class="line x" title="149:155	Both strategies improve correlation; however, unfortunately the benefits do not currently combine." ></td>
	<td class="line x" title="150:155	Our feature weight analysis indicates that syntacto-semantic features do play an important role in score prediction in the RTE model." ></td>
	<td class="line p" title="151:155	We plan to assess the additional benefit of the full entailment feature set against the TRADMT feature set extended by a proper lexical similarity metric, such as METEOR." ></td>
	<td class="line x" title="152:155	The computation of entailment features is more heavyweight than traditional MT evaluation metrics." ></td>
	<td class="line x" title="153:155	We found the speed (about 6 s per hypothesis on a current PC) to be sufficient for easily judging the quality of datasets of the size conventionally used for MT evaluation." ></td>
	<td class="line x" title="154:155	However, this may still be too expensive as part of an MT model that directly optimizes some performance measure, e.g., minimum error rate training (Och, 2003)." ></td>
	<td class="line x" title="155:155	40" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0405
Combining Multi-Engine Translations with Moses
Chen, Yu;Jellinghaus, Michael;Eisele, Andreas;Zhang, Yi;Hunsicker, Sabine;Theison, Silke;Federmann, Christian;Uszkoreit, Hans;"></td>
	<td class="line x" title="1:116	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 4246, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:116	c2009 Association for Computational Linguistics Combining Multi-Engine Translations with Moses Yu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2, Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,2 1: Universitat des Saarlandes, Saarbrucken, Germany 2: Deutsches Forschungszentrum fur Kunstliche Intelligenz GmbH, Saarbrucken, Germany {yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de {eisele,cfedermann,uszkoreit}@dfki.de Abstract We present a simple method for generating translations with the Moses toolkit (Koehn et al., 2007) from existing hypotheses produced by other translation engines." ></td>
	<td class="line x" title="3:116	As the structures underlying these translation engines are not known, an evaluationbased strategy is applied to select systems for combination." ></td>
	<td class="line x" title="4:116	The experiments show promising improvements in terms of BLEU." ></td>
	<td class="line x" title="5:116	1 Introduction With the wealth of machine translation systems available nowadays (many of them online and for free), it makes increasing sense to investigate clever ways of combining them." ></td>
	<td class="line x" title="6:116	Obviously, the main objective lies in finding out how to integrate the respective advantages of different approaches: Statistical machine translation (SMT) and rulebased machine translation (RBMT) systems often have complementary characteristics." ></td>
	<td class="line x" title="7:116	Previous work on building hybrid systems includes, among others, approaches using reranking, regeneration with an SMT decoder (Eisele et al., 2008; Chen et al., 2007), and confusion networks (Matusov et al., 2006; Rosti et al., 2007; He et al., 2008)." ></td>
	<td class="line x" title="8:116	The approach by (Eisele et al., 2008) aimed specifically at filling lexical gaps in an SMT system with information from a number of RBMT systems." ></td>
	<td class="line x" title="9:116	The output of the RBMT engines was word-aligned with the input, yielding a total of seven phrase tables which where simply concatenated to expand the phrase table constructed from the training corpus." ></td>
	<td class="line x" title="10:116	This approach differs from the confusion network approaches mainly in that the final hypotheses do not necessarily follow any of the input translations as the skeleton." ></td>
	<td class="line x" title="11:116	On the other hand, it emphasizes that the additional translations should be produced by RBMT systems with lexicons that cannot be learned from the data." ></td>
	<td class="line x" title="12:116	The present work continues on the same track as the paper mentioned above but implements a number of important changes, most prominently a relaxation of the restrictions on the number and type of input systems." ></td>
	<td class="line x" title="13:116	These differences are described in more detail in Section 2." ></td>
	<td class="line x" title="14:116	Section 3 explains the implementation of our system and Section 4 its application in a number of experiments." ></td>
	<td class="line x" title="15:116	Finally, Section 5 concludes this paper with a summary and some thoughts on future work." ></td>
	<td class="line x" title="16:116	2 Integrating Multiple Systems of Unknown Type and Quality When comparing (Eisele et al., 2008) to the present work, our proposal is more general in a way that the requirement for knowledge about the systems is minimum." ></td>
	<td class="line x" title="17:116	The types and the identities of the participated systems are assumed unknown." ></td>
	<td class="line x" title="18:116	Accordingly, we are not able to restrict ourselves to a certain class of systems as (Eisele et al., 2008) did." ></td>
	<td class="line x" title="19:116	We rely on a standard phrase-based SMT framework to extract the valuable pieces from the system outputs." ></td>
	<td class="line x" title="20:116	These extracted segments are also used to improve an existing SMT system that we have access to." ></td>
	<td class="line x" title="21:116	While (Eisele et al., 2008) included translations from all of a fixed number of RBMT systems and added one feature to the translation model for each system, integrating all given system outputs in this way in our case could expand the search space tremendously." ></td>
	<td class="line x" title="22:116	Meanwhile, we cannot rely on the assumption that all candidate systems actually have the potential to improve our baseline." ></td>
	<td class="line x" title="23:116	This implies the need for a first step of system selection where the best candidate systems are identified and a limited number of them is chosen to be included in the combination." ></td>
	<td class="line x" title="24:116	Our approach would not work without a small set of tuning data being available so that we can evaluate the systems for later selection and adjust the weights of our systems." ></td>
	<td class="line x" title="25:116	Such tuning data is included in this years 42 task." ></td>
	<td class="line x" title="26:116	In this paper, we use the Moses decoder to construct translations from the given system outputs." ></td>
	<td class="line x" title="27:116	We mainly propose two slightly different ways: One is to construct translation models solely from the given translations and the other is to extend an existing translation model with these additional translations." ></td>
	<td class="line x" title="28:116	3 Implementation Despite the fact that the output of current MT systems is usually not comparable in quality to human translations, the machine-generated translations are nevertheless parallel to the input so that it is straightforward to construct a translation model from data of this kind." ></td>
	<td class="line x" title="29:116	This is the spirit behind our method for combining multiple translations." ></td>
	<td class="line x" title="30:116	3.1 Direct combination Clearly, for the same source sentence, we expect to have different translations from different translation systems, just like we would expect from human translators." ></td>
	<td class="line x" title="31:116	Also, every system may have its own advantages." ></td>
	<td class="line x" title="32:116	We break these translations into smaller units and hope to be able to select the best ones and form them into a better translation." ></td>
	<td class="line x" title="33:116	One single translation of a few thousand sentences is normally inadequate for building a reliable general-purpose SMT system (data sparseness problem)." ></td>
	<td class="line x" title="34:116	However, in the system combination task, this is no longer an issue as the system only needs to translate sentences within the data set." ></td>
	<td class="line x" title="35:116	When more translation engines are available, the size of this set becomes larger." ></td>
	<td class="line x" title="36:116	Hence, we collect translations from all available systems and pair them with the corresponding input text, thus forming a medium-sized hypothesis corpus." ></td>
	<td class="line x" title="37:116	Our system starts processing this corpus with a standard phrase-based SMT setup, using the Moses toolkit (Koehn et al., 2007)." ></td>
	<td class="line x" title="38:116	The hypothesis corpus is first tokenized and lowercased." ></td>
	<td class="line x" title="39:116	Then, we run GIZA++ (Och and Ney, 2003) on the corpus to obtain word alignments in both directions." ></td>
	<td class="line x" title="40:116	The phrases are extracted from the intersection of the alignments with the grow heuristics." ></td>
	<td class="line x" title="41:116	In addition, we also generate a reordering model with the default configuration as included in the Moses toolkit." ></td>
	<td class="line x" title="42:116	This hypothesis translation model can already be used by the Moses decoder together with a language model to perform translations over the corresponding sentence set." ></td>
	<td class="line x" title="43:116	3.2 Integration into existing SMT system Sometimes, the goal of system combination is not only to produce a translation but also to improve one of the systems." ></td>
	<td class="line x" title="44:116	In this paper, we aim at incorporating the additional system outputs to improve an out-of-domain SMT system trained on the Europarl corpus (Koehn, 2005)." ></td>
	<td class="line x" title="45:116	Our hope is that the additional translation hypotheses could bring in new phrases or, more generally, new information that was not contained in the Europarl model." ></td>
	<td class="line x" title="46:116	In order to facilitate comparisons, we use in-domain LMs for all setups." ></td>
	<td class="line x" title="47:116	We investigate two alternative ways of integrating the additional phrases into the existing SMT system: One is to take the hypothesis translation model described in Section 3.1, the other is to construct system-specific models constructed with only translations from one system at a time." ></td>
	<td class="line x" title="48:116	Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model." ></td>
	<td class="line x" title="49:116	The method requires tuning on at least six more features, which expands the search space for the translation task unnecessarily." ></td>
	<td class="line x" title="50:116	We instead integrate the translation models from multiple sources by extending the phrase table." ></td>
	<td class="line x" title="51:116	In contrast to the prior approach presented in (Chen et al., 2007) and (Eisele et al., 2008) which concatenates the phrase tables and adds new features as system markers, our extension method avoids duplicate entries in the final combined table." ></td>
	<td class="line x" title="52:116	Given a set of hypothesis translation models (derived from an arbitrary number of system outputs) and an original large translation model to be improved, we first sort the models by quality (see Section 3.3), always assigning the highest priority to the original model." ></td>
	<td class="line x" title="53:116	The additional phrase tables are appended to the large model in sorted order such that only phrase pairs that were never seen before are included." ></td>
	<td class="line x" title="54:116	Lastly, we add new features (in the form of additional columns in the phrase table) to the translation model to indicate each pairs origin." ></td>
	<td class="line oc" title="55:116	3.3 System evaluation Since both the system translations and the reference translations are available for the tuning 43 set, we first compare each output to the reference translation using BLEU (Papineni et al., 2001) and METEOR (Banerjee and Lavie, 2005) and a combined scoring scheme provided by the ULC toolkit (Gimenez and Marquez, 2008)." ></td>
	<td class="line x" title="56:116	In our experiments, we selected a subset of 5 systems for the combination, in most cases, based on BLEU." ></td>
	<td class="line x" title="57:116	On the other hand, some systems may be designed in a way that they deliver interesting unique translation segments." ></td>
	<td class="line x" title="58:116	Therefore, we also measure the similarity among system outputs as shown in Table 2 in a given collection by calculating average similarity scores across every pair of outputs." ></td>
	<td class="line x" title="59:116	de-en fr-en es-en en-de en-fr en-es Num." ></td>
	<td class="line x" title="60:116	20 23 28 15 16 9 Median 19.87 26.55 22.50 13.78 24.76 23.70 Range 16.37 17.06 9.74 4.75 11.05 13.94 Top 5 de-en fr-en es-en en-de en-fr en-es Median 22.26 27.93 26.43 15.21 26.62 26.61 Range 4.31 4.76 5.71 1.71 0.68 5.56 Table 1: Statistics of system outputs BLEU scores The range of BLEU scores cannot indicate the similarity of the systems." ></td>
	<td class="line x" title="61:116	The direction with the most systems submitted is Spanish-English but their respective performances are very close to each other." ></td>
	<td class="line x" title="62:116	As for the selected subset, the EnglishFrench systems have the most similar performance in terms of BLEU scores." ></td>
	<td class="line x" title="63:116	The French-English translations have the largest range in BLEU but the similarity in this group is not the lowest." ></td>
	<td class="line x" title="64:116	de-en fr-en es-en en-de en-fr en-es All 34.09 46.48 61.83 31.74 44.95 38.11 Selected 36.65 56.16 56.06 33.92 52.78 57.25 Table 2: Similarity of the system outputs Ideally, we should select systems with highest quality scores and lowest similarity scores." ></td>
	<td class="line o" title="65:116	For German-English, we selected the three with the highest METEOR scores and another two with high METEOR scores but low similarity scores to the first three." ></td>
	<td class="line x" title="66:116	For the other language directions, we chose five systems from different institutions with the highest scores." ></td>
	<td class="line x" title="67:116	3.4 Language models We use a standard n-gram language model for each target language using the monolingual training data provided in the translation task." ></td>
	<td class="line x" title="68:116	These LMs are thus specific to the same domain as the input texts." ></td>
	<td class="line x" title="69:116	Moreover, we also generate hypothesis LMs solely based on the given system outputs, that is, LMs that model how the candidate systems convey information in the target language." ></td>
	<td class="line x" title="70:116	These LMs do not require any additional training data." ></td>
	<td class="line x" title="71:116	Therefore, we do not require any training data other than the given system outputs by using the hypothesis language model and the hypothesis translation model." ></td>
	<td class="line x" title="72:116	3.5 Tuning After building the models, it is essential to tune the SMT system to optimize the feature weights." ></td>
	<td class="line x" title="73:116	We use Minimal Error Rate Training (Och, 2003) to maximize BLEU on the complete development data." ></td>
	<td class="line x" title="74:116	Unlike the standard tuning procedure, we do not tune the final system directly." ></td>
	<td class="line x" title="75:116	Instead, we obtain the weights using models built from the tuning portion of the system outputs." ></td>
	<td class="line x" title="76:116	For each combination variant, we first train models on the provided outputs corresponding to the tuning set." ></td>
	<td class="line x" title="77:116	This system, called the tuning system, is also tuned on the tuning set." ></td>
	<td class="line x" title="78:116	The initial weights of any additional features not included in the standard setting are set to 0." ></td>
	<td class="line x" title="79:116	We then adapt the weights to the system built with translations corresponding to the test set." ></td>
	<td class="line x" title="80:116	The procedure and the settings for building this system must be identical to that of the tuning system." ></td>
	<td class="line x" title="81:116	4 Experiments The purpose of this exercise is to understand the nature of the system combination task in practice." ></td>
	<td class="line x" title="82:116	Therefore, we restrict ourselves to the training data and system translations provided by the shared task." ></td>
	<td class="line x" title="83:116	The types of the systems that produced the translations are assumed to be unknown." ></td>
	<td class="line x" title="84:116	We report results for six translation directions between four languages." ></td>
	<td class="line x" title="85:116	4.1 Data and baseline We build an SMT system from release v4 of the Europarl corpus (Koehn, 2005), following a standard routine using the Moses toolkit." ></td>
	<td class="line x" title="86:116	The system also includes 5-gram language models trained on in-domain corpora of the respective target languages using SRILM (Stolcke, 2002)." ></td>
	<td class="line x" title="87:116	The systems in this paper, including the baseline, are all tuned on the same 501-sentence tuning set." ></td>
	<td class="line x" title="88:116	Note also that the provided n-best outputs are excluded in our experiments." ></td>
	<td class="line x" title="89:116	44 4.2 Results The experiments include three different setups for direct system combination, involving only hypothesis translation models." ></td>
	<td class="line x" title="90:116	System S0, the baseline for this group, uses a hypothesis translation model built with all available system translations and a hypothesis LM (also from the machine-generated outputs)." ></td>
	<td class="line x" title="91:116	S1 differs from S0 in that the LM in S1 is generated from a large news corpus." ></td>
	<td class="line x" title="92:116	S2 consists of translation models built with only the five selected systems." ></td>
	<td class="line x" title="93:116	The BLEU scores of these systems are shown in Table 3." ></td>
	<td class="line x" title="94:116	de-en fr-en es-en en-de en-fr en-es Top 1 21.16 30.91 28.54 14.96 26.55 27.84 Mean 17.29 23.78 21.39 12.76 22.96 21.43 S0 20.46 27.50 23.35 13.95 27.29 25.59 S1 21.76 28.05 25.49 15.16 27.70 26.09 S2 21.71 24.98 27.26 15.62 24.28 25.22 Table 3: BLEU scores of direct system combination When all outputs are included, the combined system can always produce translations better than most of the systems." ></td>
	<td class="line x" title="95:116	When only a hypothesis LM is used, the BLEU scores are always higher than the average BLEU scores of the outputs." ></td>
	<td class="line x" title="96:116	It even outperforms the top system for English-French." ></td>
	<td class="line x" title="97:116	This simple setup (S0) is certainly a feasible solution when no additional data is available and no system evaluation is possible." ></td>
	<td class="line x" title="98:116	This approach appears to be more effective on typically difficult language pairs that involve German." ></td>
	<td class="line x" title="99:116	As for the systems with normal language models, neither of the systems ensure better translations." ></td>
	<td class="line x" title="100:116	The translation quality is not completely determined by the number of included translations and their quality." ></td>
	<td class="line x" title="101:116	On the other hand, the output set with higher diversity (Table 2) usually leads to better combination results." ></td>
	<td class="line x" title="102:116	This observation is consistent with the results from the system integration experiments shown in Table 4." ></td>
	<td class="line x" title="103:116	de-en fr-en es-en en-de en-fr en-es Bas 19.13 25.07 24.55 13.59 23.67 23.67 Med 17.99 24.56 20.70 13.19 24.19 22.12 All 21.40 28.00 27.75 15.21 27.20 26.41 Top5 21.70 26.01 28.53 15.52 27.87 27.92 Table 4: BLEU scores of integrated SMT systems (Bas: Baseline, Med: Median) There are two variants in our experiments on system integration." ></td>
	<td class="line x" title="104:116	All in Table 4 represents the system that integrates the complete hypothesis translation model with the Europarl model, while Top 5 refers to the system that incorporates the five system-specific models separately." ></td>
	<td class="line x" title="105:116	Both setups result in an improvement over the baseline Europarlbased SMT system." ></td>
	<td class="line x" title="106:116	BLEU scores increase by up to 4.25 points." ></td>
	<td class="line x" title="107:116	The integrated SMT system sometimes produces translations better than the best system (7 out of 12 cases)." ></td>
	<td class="line x" title="108:116	5 Conclusion This work uses the Moses toolkit to combine translations from multiple engines in a simple way." ></td>
	<td class="line x" title="109:116	The experiments on six translation directions show interesting results: The final translations are always better than the majority of the given systems, while the combination performs better than the best system in half the cases." ></td>
	<td class="line x" title="110:116	A similar approach was applied to improve an existing SMT system which was built in a domain different from the test task." ></td>
	<td class="line x" title="111:116	We achieved improvements in all cases." ></td>
	<td class="line x" title="112:116	There are many possible future directions to continue this work." ></td>
	<td class="line x" title="113:116	As we have shown, the quality of the combined system is more related to the diversity of the involved systems than to the number of the systems or their quality." ></td>
	<td class="line x" title="114:116	Hand-picked systems lead to better combinations than those selected by BLEU scores." ></td>
	<td class="line x" title="115:116	It would be interesting to develop a more comprehensive system selection strategy." ></td>
	<td class="line x" title="116:116	Acknowledgments This work was supported by the EuroMatrix project (IST-034291) which is funded by the European Community under the Sixth Framework Programme for Research and Technological Development." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0408
Machine Translation System Combination with Flexible Word Ordering
Heafield, Kenneth;Hanneman, Greg;Lavie, Alon;"></td>
	<td class="line x" title="1:137	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 5660, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:137	c2009 Association for Computational Linguistics Machine Translation System Combination with Flexible Word Ordering Kenneth Heafield, Greg Hanneman, Alon Lavie Language Technologies Institute, Carnegie Mellon University 5000 Forbes Avenue, Pittsburgh, PA 15213, USA {kheafiel,ghannema,alavie}@cs.cmu.edu Abstract We describe a synthetic method for combining machine translations produced by different systems given the same input." ></td>
	<td class="line x" title="3:137	One-best outputs are explicitly aligned to remove duplicate words." ></td>
	<td class="line x" title="4:137	Hypotheses follow system outputs in sentence order, switching between systems mid-sentence to produce a combined output." ></td>
	<td class="line x" title="5:137	Experiments with the WMT 2009 tuning data showed improvement of 2 BLEU and 1 METEOR point over the best HungarianEnglish system." ></td>
	<td class="line x" title="6:137	Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task." ></td>
	<td class="line x" title="7:137	1 Introduction Many systems for machine translation, with different underlying approaches, are of competitive quality." ></td>
	<td class="line x" title="8:137	Nonetheless these approaches and systems have different strengths and weaknesses." ></td>
	<td class="line x" title="9:137	By offsetting weaknesses with strengths of other systems, combination can produce higher quality than does any component system." ></td>
	<td class="line x" title="10:137	One approach to system combination uses confusion networks (Rosti et al., 2008; Karakos et al., 2008)." ></td>
	<td class="line x" title="11:137	In the most common form, a skeleton sentence is chosen from among the one-best system outputs." ></td>
	<td class="line x" title="12:137	This skeleton determines the ordering of the final combined sentence." ></td>
	<td class="line x" title="13:137	The remaining outputs are aligned with the skeleton, producing a list of alternatives for each word in the skeleton, which comprises a confusion network." ></td>
	<td class="line x" title="14:137	A decoder chooses from the original skeleton word and its alternatives to produce a final output sentence." ></td>
	<td class="line x" title="15:137	While there are a number of variations on this theme, our approach differs fundamentally in that the effective skeleton changes on a per-phrase basis." ></td>
	<td class="line x" title="16:137	Our system is an enhancement of our previous work (Jayaraman and Lavie, 2005)." ></td>
	<td class="line x" title="17:137	A hypothesis uses words from systems in order, switching between systems at phrase boundaries." ></td>
	<td class="line x" title="18:137	Alignments and a synchronization method merge meaningequivalent output from different systems." ></td>
	<td class="line x" title="19:137	Hypotheses are scored based on system confidence, alignment support, and a language model." ></td>
	<td class="line x" title="20:137	We contribute a few enhancements to this process." ></td>
	<td class="line x" title="21:137	First, we introduce an alignment-sensitive method for synchronizing available hypothesis extensions across systems." ></td>
	<td class="line x" title="22:137	Second, we pack similar partial hypotheses, which allows greater diversity in our beam search while maintaining the accuracy of n-best output." ></td>
	<td class="line x" title="23:137	Finally, we describe an improved model selection process that determined our submissions to the WMT 2009 shared system combination task." ></td>
	<td class="line x" title="24:137	The remainder of this paper is organized as follows." ></td>
	<td class="line x" title="25:137	Section 2 describes the system with emphasis on our modifications." ></td>
	<td class="line x" title="26:137	Tuning, our experimental setup, and submitted systems are described in Section 3." ></td>
	<td class="line x" title="27:137	Section 4 concludes." ></td>
	<td class="line x" title="28:137	2 System The system consists of alignment (Section 2.1) and phrase detection (Section 2.2) followed by decoding." ></td>
	<td class="line x" title="29:137	The decoder constructs hypothesis sentences one word at a time, starting from the left." ></td>
	<td class="line x" title="30:137	A partially constructed hypothesis comprises: Word The most recently decoded word." ></td>
	<td class="line x" title="31:137	Initially, this is the beginning of sentence marker." ></td>
	<td class="line x" title="32:137	Used The set of used words from each system." ></td>
	<td class="line x" title="33:137	Initially empty." ></td>
	<td class="line x" title="34:137	Phrase The current phrase constraint from Section 2.2, if any." ></td>
	<td class="line x" title="35:137	The initial hypothesis is not in a phrase." ></td>
	<td class="line x" title="36:137	Features Four feature values defined in Section 2.4 and used in Section 2.5 for beam search 56 and hypothesis ranking." ></td>
	<td class="line x" title="37:137	Initially, all features are 1." ></td>
	<td class="line x" title="38:137	Previous A set of preceding hypothesis pointers described in Section 2.5." ></td>
	<td class="line x" title="39:137	Initially empty." ></td>
	<td class="line x" title="40:137	The leftmost unused word from each system corresponds to a continuation of the partial hypothesis." ></td>
	<td class="line x" title="41:137	Therefore, for each system, we extend a partial hypothesis by appending that systems leftmost unused word, yielding several new hypotheses." ></td>
	<td class="line x" title="42:137	The appended word, and those aligned with it, are marked as used in the new hypothesis." ></td>
	<td class="line x" title="43:137	Since systems do not align perfectly, too few words may be marked as used, a problem addressed in Section 2.3." ></td>
	<td class="line x" title="44:137	As described in Section 2.4, hypotheses are scored using four features based on alignment, system confidence, and a language model." ></td>
	<td class="line x" title="45:137	Since the search space is quite large, we use these partial scores for a beam search, where the beam contains hypotheses of equal length." ></td>
	<td class="line x" title="46:137	This space contains hypotheses that extend in precisely the same way, which we exploit in Section 2.5 to increase diversity." ></td>
	<td class="line x" title="47:137	Finally, a hypothesis is complete when the end of sentence marker is appended." ></td>
	<td class="line oc" title="48:137	2.1 Alignment Sentences from different systems are aligned in pairs using a modified version of the METEOR (Banerjee and Lavie, 2005) matcher." ></td>
	<td class="line x" title="49:137	This identifies alignments in three phases: exact matches up to case, WordNet (Fellbaum, 1998) morphology matches, and shared WordNet synsets." ></td>
	<td class="line x" title="50:137	These sources of alignments are quite precise and unable to pick up on looser matches such as mentioned and said that legitimately appear in output from different systems." ></td>
	<td class="line x" title="51:137	Artificial alignments are intended to fill gaps by using surrounding alignments as clues." ></td>
	<td class="line x" title="52:137	If a word is not aligned to any word in some other sentence, we search left and right for words that are aligned into that sentence." ></td>
	<td class="line x" title="53:137	If these alignments are sufficiently close to each other in the other sentence, words between them are considered for artificial alignment." ></td>
	<td class="line x" title="54:137	An artificial alignment is added if a matching part of speech is found." ></td>
	<td class="line x" title="55:137	The algorithm is described fully by Jayaraman and Lavie (2005)." ></td>
	<td class="line x" title="56:137	2.2 Phrases Switching between systems is permitted outside phrases or at phrase boundaries." ></td>
	<td class="line x" title="57:137	We find phrases in two ways." ></td>
	<td class="line x" title="58:137	Alignment phrases are maximally long sequences of words which align, in the same order and without interruption, to a word sequence from at least one other system." ></td>
	<td class="line x" title="59:137	Punctuation phrases place punctuation in a phrase with the preceding word, if any." ></td>
	<td class="line x" title="60:137	When the decoder extends a hypothesis, it considers the longest phrase in which no word is used." ></td>
	<td class="line x" title="61:137	If a punctuation phrase is partially used, the decoder marks the entire phrase as used to avoid extraneous punctuation." ></td>
	<td class="line x" title="62:137	2.3 Synchronization While phrases address near-equal pieces of translation output, we must also deal with equally meaningful output that does not align." ></td>
	<td class="line x" title="63:137	The immediate effect of this issue is that too few words are marked as used by the decoder, leading to duplication in the combined output." ></td>
	<td class="line x" title="64:137	In addition, partially aligned system output results in lingering unused words between used words." ></td>
	<td class="line x" title="65:137	Often these are function words that, with language model scoring, make output unnecessarily verbose." ></td>
	<td class="line x" title="66:137	To deal with this problem, we expire lingering words by marking them as used." ></td>
	<td class="line x" title="67:137	Specifically, we consider the frontier of each system, which is the leftmost unused word." ></td>
	<td class="line x" title="68:137	If a frontier lags behind, words as used to advance the frontier." ></td>
	<td class="line x" title="69:137	Our two methods for synchronization differ in how frontiers are compared across systems and the tightness of the constraint." ></td>
	<td class="line x" title="70:137	Previously, we measured frontiers from the beginning of sentence." ></td>
	<td class="line x" title="71:137	Based on this measurement, the synchronization constraint requires that the frontiers of each system differ by at most s. Equivalently, a frontier is lagging if it is more than s words behind the rightmost frontier." ></td>
	<td class="line x" title="72:137	Lagging frontiers are advanced until the synchronization constraint becomes satisfied." ></td>
	<td class="line x" title="73:137	We found this method can cause problems in the presence of variable length output." ></td>
	<td class="line x" title="74:137	When the variability in output length exceeds s, proper synchronization requires distances between frontiers greater than s, which this constraint disallows." ></td>
	<td class="line x" title="75:137	Alignments indicate where words are synchronous." ></td>
	<td class="line x" title="76:137	Words near an alignment are also likely to be synchronous even without an explicit alignment." ></td>
	<td class="line x" title="77:137	For example, in the fragments even more serious, you and even worse, you from WMT 2008, serious and worse do not align but do share relative position from other alignments, suggesting these are synchronous." ></td>
	<td class="line x" title="78:137	We formalize this by measuring the relative position of frontiers from alignments on each side." ></td>
	<td class="line x" title="79:137	For example, 57 if the frontier itself is aligned then relative position is zero." ></td>
	<td class="line x" title="80:137	For each pair of systems, we check if these relative positions differ by at most s under an alignment on either side." ></td>
	<td class="line x" title="81:137	Confidence in a systems frontier is the sum of the systems own confidence plus confidence in systems for which the pair-wise constraint is satisfied." ></td>
	<td class="line x" title="82:137	If confidence in any frontier falls below 0.8, the least confident lagging frontier is advanced." ></td>
	<td class="line x" title="83:137	The process repeats until the constraint becomes satisfied." ></td>
	<td class="line x" title="84:137	2.4 Scores We score partial and complete hypotheses using system confidence, alignments, and a language model." ></td>
	<td class="line x" title="85:137	Specifically, we have four features which operate at the word level: Alignment Confidence in the system from which the word came plus confidence in systems to which the word aligns." ></td>
	<td class="line x" title="86:137	Language Model Score from a suffix array language model (Zhang and Vogel, 2006) trained on English from monolingual and French-English data provided by the contest." ></td>
	<td class="line x" title="87:137	N-Gram parenleftbig13parenrightbigorderngram using language model order and length of ngram found." ></td>
	<td class="line x" title="88:137	Overlap overlaporder1 where overlap is the length of intersection between the preceding and current n-grams." ></td>
	<td class="line x" title="89:137	The N-Gram and Overlap features are intended to improve fluency across phrase boundaries." ></td>
	<td class="line x" title="90:137	Features are combined using a log-linear model trained as discussed in Section 3." ></td>
	<td class="line x" title="91:137	Hypotheses are scored using the geometric average score of each word in the hypothesis." ></td>
	<td class="line x" title="92:137	2.5 Search Of note is that a words score is impacted only by its alignments and the n-gram found by the language model." ></td>
	<td class="line x" title="93:137	Therefore two partial hypotheses that differ only in words preceding the n-gram and in their average score are in some sense duplicates." ></td>
	<td class="line x" title="94:137	With the same set of used words and same phrase constraint, they extend in precisely the same way." ></td>
	<td class="line x" title="95:137	In particular, the highest scoring hypothesis will never use a lower scoring duplicate." ></td>
	<td class="line x" title="96:137	We use duplicate detecting beam search to explore our hypothesis space." ></td>
	<td class="line x" title="97:137	A beam contains partial hypotheses of the same length." ></td>
	<td class="line x" title="98:137	Duplicate hypotheses are detected on insertion and packed, with the combined hypothesis given the highest score of those packed." ></td>
	<td class="line x" title="99:137	Once a beam contains the top scoring partial hypotheses of length l, these hypotheses are extended to length l+1 and placed in another beam." ></td>
	<td class="line x" title="100:137	Those hypotheses reaching end of sentence are placed in a separate beam, which is equivalent to packing them into one final hypothesis." ></td>
	<td class="line x" title="101:137	Once we remove partial hypothesis that did not extend to the final hypothesis, the hypotheses are a lattice connected by parent pointers." ></td>
	<td class="line x" title="102:137	While we submitted only one-best hypotheses, accurate n-best hypotheses are important for training as explained in Section 3." ></td>
	<td class="line x" title="103:137	Unpacking the hypothesis lattice into n-best hypotheses is guided by scores stored in each hypothesis." ></td>
	<td class="line x" title="104:137	For this task, we use an n-best beam of paths from the end of sentence hypothesis to a partial hypothesis." ></td>
	<td class="line x" title="105:137	Paths are built by induction, starting with a zero-length path from the end of sentence hypothesis to itself." ></td>
	<td class="line x" title="106:137	The top scoring path is removed and its terminal hypothesis is examined." ></td>
	<td class="line x" title="107:137	If it is the beginning of sentence, the path is output as a complete hypothesis." ></td>
	<td class="line x" title="108:137	Otherwise, we extend the path to each parent hypothesis, adjusting each path score as necessary, and insert into the beam." ></td>
	<td class="line x" title="109:137	This process terminates with n complete hypotheses or an empty beam." ></td>
	<td class="line x" title="110:137	3 Tuning Given the 502 sentences made available for tuning by WMT 2009, we selected feature weights for scoring, a set of systems to combine, confidence in each selected system, and the type and distance s of synchronization." ></td>
	<td class="line x" title="111:137	Of these, only feature weights can be trained, for which we used minimum error rate training with version 1.04 of IBM-style BLEU (Papineni et al., 2002) in case-insensitive mode." ></td>
	<td class="line x" title="112:137	We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation." ></td>
	<td class="line x" title="113:137	This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences." ></td>
	<td class="line x" title="114:137	We chose to do only two folds due to limited computational time and a desire to test many models." ></td>
	<td class="line x" title="115:137	We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (Snover et al., 2006)." ></td>
	<td class="line x" title="116:137	For each source language, we ex58 In Sync s BLEU METE TER Systems and Confidences cz length 8 .236 .507 59.1 google .46 cu-bojar .27 uedin .27 cz align 5 .226 .499 57.8 google .50 cu-bojar .25 uedin .25 cz align 7 .211 .508 65.9 cu-bojar .60 google .20 uedin .20 cz .231 .504 57.8 google de length 7 .255 .531 54.2 google .40 uka .30 stuttgart .15 umd .15 de length 6 .260 .532 55.2 google .50 systran .25 umd .25 de align 9 .256 .533 55.5 google .40 uka .30 stuttgart .15 umd .15 de align 6 .200 .514 54.2 google .31 uedin .22 systran .18 umd .16 uka .14 de .244 .523 57.5 google es align 8 .297 .560 52.7 google .75 uedin .25 es length 5 .289 .548 52.1 google .50 talp-upc .17 uedin .17 rwth .17 es .297 .558 52.7 google fr align 6 .329 .574 49.9 google .70 lium1 .30 fr align 8 .314 .596 48.6 google .50 lium1 .30 limsi1 .20 fr length 8 .323 .570 48.5 google .50 lium1 .25 limsi1 .25 fr .324 .576 48.7 google hu length 5 .162 .403 69.2 umd .50 morpho .40 uedin .10 hu length 8 .158 .407 69.5 umd .50 morpho .40 uedin .10 hu align 7 .153 .392 68.0 umd .33 morpho .33 uedin .33 hu .141 .391 66.1 umd xx length 5 .326 .584 49.6 google-fr .61 google-es .39 xx align 4 .328 .580 49.5 google-fr .80 google-es .20 xx align 5 .324 .576 48.6 google-fr .61 google-es .39 xx align 7 .319 .587 51.1 google-fr .50 google-es .50 xx .324 .576 48.7 google-fr Table 1: Combination models used for submission to WMT 2009." ></td>
	<td class="line x" title="117:137	For each language, we list our primary combination, contrastive combinations, and a high-scoring system for comparison in italic." ></td>
	<td class="line x" title="118:137	All translations are into English." ></td>
	<td class="line x" title="119:137	The xx source language combines translations from different languages, in our case French and Spanish." ></td>
	<td class="line x" title="120:137	Scores from BLEU, METEOR, and TER are the average of two crossvalidation folds with 100 evaluation sentences each." ></td>
	<td class="line x" title="121:137	Numbers following system names indicate contrastive systems." ></td>
	<td class="line x" title="122:137	More evaluation, including human scores, will be published by WMT." ></td>
	<td class="line x" title="123:137	perimented with various sets of high-scoring systems to combine." ></td>
	<td class="line x" title="124:137	We also tried confidence values proportional to various powers of BLEU and METEOR scores, as well as hand-picked values." ></td>
	<td class="line x" title="125:137	Finally we tried both variants of synchronization with values of s ranging from 2 to 9." ></td>
	<td class="line x" title="126:137	In total, 405 distinct models were evaluated." ></td>
	<td class="line x" title="127:137	For each source source language, our primary system was chosen by performing well on all three metrics." ></td>
	<td class="line x" title="128:137	Models that scored well on individual metrics were submitted as contrastive systems." ></td>
	<td class="line x" title="129:137	In Table 1 we report the models underlying each submitted system." ></td>
	<td class="line x" title="130:137	4 Conclusion We found our combinations are quite sensitive to presence of and confidence in the underlying systems." ></td>
	<td class="line x" title="131:137	Further, we show the most improvement when these systems are close in quality, as is the case with our Hungarian-English system." ></td>
	<td class="line x" title="132:137	The two methods of synchronization were surprisingly competitive, a factor we attribute to short sentence length compared with WMT 2008 Europarl sentences." ></td>
	<td class="line x" title="133:137	Opportunities for further work include persentence system confidence, automatic training of more parameters, and different alignment models." ></td>
	<td class="line x" title="134:137	We look forward to evaluation results from WMT 2009." ></td>
	<td class="line x" title="135:137	Acknowledgments The authors wish to thank Jonathan Clark for training the language model and other assistance." ></td>
	<td class="line x" title="136:137	This work was supported in part by the DARPA GALE program and by a NSF Graduate Research Fellowship." ></td>
	<td class="line x" title="137:137	59" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0418
NICT@WMT09: Model Adaptation and Transliteration for Spanish-English SMT
Paul, Michael;Finch, Andrew;Sumita, Eiichiro;"></td>
	<td class="line x" title="1:82	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 105109, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:82	c2009 Association for Computational Linguistics NICT@WMT09: Model Adaptation and Transliteration for Spanish-English SMT Michael Paul, Andrew Finch and Eiichiro Sumita Language Translation Group MASTAR Project National Institute of Information and Communications Technology Michael.Paul@nict.go.jp Abstract This paper describes the NICT statistical machine translation (SMT) system used for the WMT 2009 Shared Task (WMT09) evaluation." ></td>
	<td class="line x" title="3:82	We participated in the Spanish-English translation task." ></td>
	<td class="line x" title="4:82	The focus of this years participation was to investigate model adaptation and transliteration techniques in order to improve the translation quality of the baseline phrasebased SMT system." ></td>
	<td class="line x" title="5:82	1 Introduction This paper describes the NICT statistical machine translation (SMT) system used for the shared task of the Fourth Workshop on Statistical Machine Translation." ></td>
	<td class="line x" title="6:82	We participated in the SpanishEnglish translation task under the Constrained Condition." ></td>
	<td class="line x" title="7:82	For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M parallel sentences extracted from the proceedings of the European Parliament, and the News Commentary (NC) corpus (Callison-Burch et al., 2008), which consists of 74K parallel sentences taken from major news outlets like BBC, Der Spiegel, and Le Monde." ></td>
	<td class="line x" title="8:82	In order to adapt SMT systems to a speci c domain, recent research focuses on model adaptation techniques that adjust their parameters based on information about the evaluation domain (Foster and Kuhn, 2007; Finch and Sumita, 2008a)." ></td>
	<td class="line x" title="9:82	Statistical models can be trained on in-domain and out-of-domain data sets and combined at run-time using probabilistic weighting between domain-speci c statistical models." ></td>
	<td class="line x" title="10:82	As the of cial WMT09 evaluation testset consists of documents taken from the news domain, we applied statistical model adaptation techniques to combine translation models (tm), language models (lm) and distortion models (dm) trained on (a) the in-domain NC corpus and (b) the out-of-domain EP corpus (cf.Section 2)." ></td>
	<td class="line x" title="12:82	One major problem in the given translation task was the large amount of out-of-vocabulary (OOV) words, i.e., source language words that do not occur in the training corpus." ></td>
	<td class="line x" title="13:82	For unknown words, no translation entry is available in the statistical translation model (phrase-table)." ></td>
	<td class="line x" title="14:82	As a result, these OOV words cannot be translated." ></td>
	<td class="line x" title="15:82	Dealing with languages with a rich morphology like Spanish and having a limited amount of bilingual resources make this problem even more severe." ></td>
	<td class="line x" title="16:82	There have been several efforts in dealing with OOV words to improve translation quality." ></td>
	<td class="line x" title="17:82	In addition to parallel text corpora, external bilingual dictionaries can be exploited to reduce the OOV problem (Okuma et al., 2007)." ></td>
	<td class="line x" title="18:82	However, these approaches depend on the coverage of the utilized external dictionaries." ></td>
	<td class="line x" title="19:82	Data sparseness problems due to in ectional variations were previously addressed by applying word transformations using stemming or lemmatization (Popovic and Ney, 2005; Gupta and Federico, 2006)." ></td>
	<td class="line x" title="20:82	A tight integration of morphosyntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation." ></td>
	<td class="line x" title="21:82	However, these approaches still suffer from the data sparseness problem, since lemmata and in ectional forms never seen in the training corpus cannot be translated." ></td>
	<td class="line x" title="22:82	In order to generate translations for unknown words, previous approaches focused on transliteration methods, where a sequence of characters is mapped from one writing system into another." ></td>
	<td class="line x" title="23:82	For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese 105 katakana1 words with phonetically equivalent English words." ></td>
	<td class="line x" title="24:82	More recently, (Finch and Sumita, 2008b) proposed a transliteration method that is based directly on techniques developed for phrasebased SMT, and transforms a character sequence from one language into another in a subwordlevel, character-based manner." ></td>
	<td class="line x" title="25:82	We extend this approach by exploiting the phrase-table of the baseline SMT system to train a phrase-based transliteration model that generates English translations of Spanish OOV words as described in Section 3." ></td>
	<td class="line x" title="26:82	The effects of the proposed techniques are investigated in detail in Section 4." ></td>
	<td class="line x" title="27:82	2 Model Adaptation Phrase-based statistical machine translation engines use multiple statistical models to generate a translation hypothesis in which (1) the translation model ensures that the source phrases and the selected target phrases are appropriate translations of each other, (2) the language model ensures that the target language is  uent, (3) the distortion model controls the reordering of the input sentence, and (4) the word penalty ensures that the translations do not become too long or too short." ></td>
	<td class="line x" title="28:82	During decoding, all model scores are weighted and combined to  nd the most likely translation hypothesis for a given input sentence (Koehn et al., 2007)." ></td>
	<td class="line x" title="29:82	In order to adapt SMT systems to a speci c domain, separate statistical models can be trained on parallel text corpora taken from the respective domain (in-domain) and additional out-ofdomain language resources." ></td>
	<td class="line x" title="30:82	The models are then combined using mixture modeling (Hastie et al., 2001), i.e., each model is weighted according to its  t with in-domain development data sets and the linear combination of the respective scores is used to  nd the best translation hypothesis during the decoding of unseen input sentences." ></td>
	<td class="line x" title="31:82	In this paper, the above model adaptation technique is applied to combine the NC and the EP language resources provided by the organizers for the Spanish-English translation task." ></td>
	<td class="line x" title="32:82	As the WMT09 evaluation testset consists of documents taken from the news domain, we used the NC corpus to train the in-domain models and the EP corpus to train the out-of-domain component models." ></td>
	<td class="line x" title="33:82	Using mixture modeling, the above mentioned statistical models are combined where each component model is optimized separately." ></td>
	<td class="line x" title="34:82	Weight opti1A special syllabary alphabet used to write down foreign names or loan words." ></td>
	<td class="line x" title="35:82	mization is carried out using a simple grid-search method." ></td>
	<td class="line x" title="36:82	At each point on the grid of weight parameter values, the translation quality of the combined weighted component models is evaluated for development data sets taken from (a) the NC corpus and (b) from the EP corpus." ></td>
	<td class="line x" title="37:82	3 Transliteration Source language input words that cannot be translated by the standard phrase-based SMT models are either left untranslated or simply removed from the translation output." ></td>
	<td class="line x" title="38:82	Common examples are named entities such as personal names or technical terms, but also include content words like common nouns or verbs that are not covered by the training data." ></td>
	<td class="line x" title="39:82	Such unknown occurrences could bene t from being transliterated into the MT systems output during translation of orthographically related languages like Spanish and English." ></td>
	<td class="line x" title="40:82	In this paper, we apply a phrase-based transliteration approach similar to the one proposed in (Finch and Sumita, 2008b)." ></td>
	<td class="line x" title="41:82	The transliteration method is based directly on techniques developed for phrase-based SMT and treats the task of transforming a character sequence from one language into another as a character-level translation process." ></td>
	<td class="line x" title="42:82	In contrast to (Finch and Sumita, 2008b) where external dictionaries and inter-language links in Wikipedia2 are utilized, the transliteration training examples used for the experiments in Section 4 are extracted directly from the phrasetable of the baseline SMT systems trained on the provided data sets." ></td>
	<td class="line x" title="43:82	For each phrase-table entry, corresponding word pairs are identi ed according to a string similarity measure based on the editdistance (Wagner, 1974) that is de ned as the sum of the costs of insertion, deletion, and substitution operations required to map one character sequence into the other and can be calculated by a dynamic programming technique (Cormen et al., 1989)." ></td>
	<td class="line x" title="44:82	In order to reduce noise in the training data, only word pairs whose word length and similarity are above a pre-de ned threshold are utilized for the training of the transliteration model." ></td>
	<td class="line x" title="45:82	The obtained transliteration model is applied as a post-process  lter to the SMT decoding process, i.e all source language words that could not be translated using the SMT engine are replaced with the corresponding transliterated word forms in order to obtain the  nal translation output." ></td>
	<td class="line x" title="46:82	2http://www.wikipedia.org 106 4 Experiments The effects of model adaptation and transliteration techniques were evaluated using the SpanishEnglish language resources summarized in Table 1." ></td>
	<td class="line x" title="47:82	In addition, the characteristics of this years testset are given in Table 2." ></td>
	<td class="line x" title="48:82	The sentence length is given as the average number of words per sentence." ></td>
	<td class="line x" title="49:82	The OOV word  gures give the percentage of words in the evaluation data set that do not appear in the NC/EP training data." ></td>
	<td class="line x" title="50:82	In order to get an idea how dif cult the translation task may be, we also calculated the language perplexity of the respective evaluation data sets according to 5-gram target language models trained on the NC/EP data sets." ></td>
	<td class="line x" title="51:82	Concerning the development sets, the newsdev2009 data taken from the same news sources as the evaluation set of the shared task was used for the tuning of the SMT engines, and the devtest2006 data taken from the EP corpus was used for system parameter optimization." ></td>
	<td class="line x" title="52:82	For the evaluation of the proposed methods, we used the testsets of the Second Workshop on SMT (nc-test2007 for NC and test2007 for EP)." ></td>
	<td class="line x" title="53:82	All data sets were casesensitive with punctuation marks tokenized." ></td>
	<td class="line x" title="54:82	The numbers in Table 1 indicate that the characteristics of this years testset differ largely from testsets of previous evaluation campaigns." ></td>
	<td class="line x" title="55:82	The NC devset (2,438/1,378 OOVs) contains twice as many untranslatable Spanish words as the NC evalset (1,168/73 OOVs) and the EP devset (912/63 OOVs)." ></td>
	<td class="line x" title="56:82	In addition, the high language perplexity  gures for this years testset show that the translation quality output for both baseline systems is expected to be much lower than those for the EP evaluation data sets." ></td>
	<td class="line oc" title="57:82	In this paper, translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations (Papineni et al., 2002), and (2) the METEOR metrics that calculates unigram overlaps between translations (Banerjee and Lavie, 2005)." ></td>
	<td class="line o" title="58:82	Scores of both metrics range between 0 (worst) and 1 (best) and are displayed in percent  gures." ></td>
	<td class="line x" title="59:82	4.1 Baseline Our baseline system is a fairly typical phrasebased machine translation system (Finch and Sumita, 2008a) built within the framework of a feature-based exponential model containing the following features: Table 1: Language Resources Corpus Train Dev Eval NC Spanish sentences 74K 2,001 2,007 words 2,048K 49,116 56,081 vocab 61K 9,047 8,638 length 27.6 24.5 27.9 OOV (%)  5.2 / 2.9 1.4 / 0.9 English sentences 74K 2,001 2,007 words 1,795K 46,524 49,693 vocab 47K 8,110 7,541 length 24.2 23.2 24.8 OOV (%)  5.2 / 2.9 1.2 / 0.9 perplexity  349 / 381 348 / 458 EP Spanish sentences 1,404K 1,861 2,000 words 41,003K 50,216 61,293 vocab 170K 7,422 8,251 length 29.2 27.0 30.6 OOV (%)  2.4 / 0.1 2.4 / 0.2 English sentences 1,404K 1,861 2,000 words 39,354K 48,663 59,145 vocab 121K 5,869 6,428 length 28.0 26.1 29.6 OOV (%)  1.8 / 0.1 1.9 / 0.1 perplexity  210 / 72 305 / 125 Table 2: Testset 2009 Corpus Test NC Spanish sentences 3,027 words 80,591 vocab 12,616 length 26.6  Source-target phrase translation probability  Inverse phrase translation probability  Source-target lexical weighting probability  Inverse lexical weighting probability  Phrase penalty  Language model probability  Lexical reordering probability  Simple distance-based distortion model  Word penalty For the training of the statistical models, standard word alignment (GIZA++ (Och and Ney, 2003)) and language modeling (SRILM (Stolcke, 2002)) tools were used." ></td>
	<td class="line x" title="60:82	We used 5-gram language models trained with modi ed Knesser-Ney smoothing." ></td>
	<td class="line x" title="61:82	The language models were trained on the target side of the provided training corpora." ></td>
	<td class="line x" title="62:82	Minimum error rate training (MERT) with respect to BLEU score was used to tune the decoders parameters, and performed using the technique proposed in (Och, 2003)." ></td>
	<td class="line x" title="63:82	For the translation, the inhouse multi-stack phrase-based decoder CleopATRa was used." ></td>
	<td class="line x" title="64:82	The automatic evaluation scores of the baseline systems trained on (a) only the NC corpus and (b) only on the EP corpus are summarized in Table 3." ></td>
	<td class="line o" title="65:82	107 Table 3: Baseline Performance NC Eval EP Eval BLEU METEOR BLEU METEOR baseline 17.56 40.52 33.00 56.50 4.2 Effects of Model Adaptation In order to investigate the effect of model adaptation, each model component was optimized separately using the method described in Section 2." ></td>
	<td class="line x" title="66:82	Table 4 summarizes the automatic evaluation results for various model combinations." ></td>
	<td class="line o" title="67:82	The combination of NC and EP models using equal weights achieves only a slight improvement for the NC task (BLEU: +0.4%, METEOR: +0.4%), but a large improvement for the EP task (BLEU: +1.0%, METEOR: +1.7%)." ></td>
	<td class="line x" title="68:82	Weight optimization further improves all translation tasks where the highest evaluation scores are achieved when the optimized weights for all statistical models are used." ></td>
	<td class="line o" title="69:82	In total, model adaptation gains 1.1% and 1.3% in BLEU and 0.8% and 1.8% in METEOR for the NC and EP translation tasks, respectively." ></td>
	<td class="line o" title="70:82	Table 4: Effects of Model Adaptation weight NC Eval EP Eval optimization BLEU METEOR BLEU METEOR  17.92 40.72 34.00 58.20 tm 18.13 40.95 34.05 58.23 tm+lm 18.25 41.23 34.12 58.22 tm+dm 18.36 41.06 34.24 58.34 tm+lm+dm 18.65 41.35 34.35 58.36 4.3 Effects of Transliteration In order to investigate the effects of transliteration, we trained three different transliteration using the phrase-table of the baseline systems trained on (a) only the NC corpus, (b) only the EP corpus, and (c) on the merged corpus (NC+EP)." ></td>
	<td class="line x" title="71:82	The performance of these phrase-based transliteration models is evaluated for 2000 randomly selected transliteration examples." ></td>
	<td class="line o" title="72:82	Table 5 summarizes the haracter-based automatic evaluation scores for the word error rate (WER) metrics, i.e., the edit distance between the system output and the closest reference translation (Niessen et al., 2000), as well as the BLEU and METEOR metrics." ></td>
	<td class="line x" title="73:82	The best performance is achieved when training examples from both domains are exploit to transliterate unknown Spanish words into English." ></td>
	<td class="line x" title="74:82	Therefore, the NC+EP transliteration model was applied to the translation outputs of all mixture models described in Section 4.2." ></td>
	<td class="line x" title="75:82	The effects of the transliteration post-process are summarized in Table 6." ></td>
	<td class="line o" title="76:82	Transliteration consisTable 5: Transliteration Performance Training character-based Data WER BLEU METEOR NC 13.10 83.62 86.74 EP 11.76 85.93 87.89 NC+EP 11.72 86.08 87.89 tently improves the translation quality of all mixture models, although the gains obtained for the NC task (BLEU: +1.3%, METEOR: +1.3%) are much larger than those for the EP task (BLEU: +0.1%, METEOR: +0.2%) which is due to the larger amount of untranslatable words in the NC evaluation data set." ></td>
	<td class="line x" title="77:82	Table 6: Effects of Transliteration weight NC Eval EP Eval optimization BLEU METEOR BLEU METEOR tm 19.14 42.39 34.11 58.46 tm+lm 19.46 42.65 34.16 58.44 tm+dm 19.77 42.35 34.38 58.57 tm+lm+dm 19.95 42.64 34.48 58.60 4.4 WMT09 Testset Results Based on the automatic evaluation results presented in the previous sections, we selected the SMT engine based on the tm+lm+dm weights optimized on the NC devset as the primary run for our testset run submission." ></td>
	<td class="line x" title="78:82	All other model weight combinations were submitted as contrastive runs." ></td>
	<td class="line x" title="79:82	The BLEU scores of these runs are listed in Table 7 and con rm the results obtained for the above experiments, i.e., the best performing system is the one based on the mixture models using separately optimized weights in combination with the transliteration of untranslatable Spanish words using the phrase-based transliteration model trained on all available language resources." ></td>
	<td class="line x" title="80:82	Table 7: Testset 2009 Performance weight NC Eval EP Eval optimization BLEU BLEU tm 21.07 20.81 tm+lm 20.95 20.59 tm+dm 21.45 21.32 tm+lm+dm 21.67 21.27 5 Conclusion The work for this years shared task focused on the task of effectively utilizing out-of-domain language resources and handling OOV words to improve translation quality." ></td>
	<td class="line o" title="81:82	Overall our experiments show that the incorporation of mixture models and phrase-based transliteration techniques largely out-performed standard phrase-based SMT engines gaining a total of 2.4% in BLEU and 2.1% in METEOR for the news domain." ></td>
	<td class="line x" title="82:82	108" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-0420
Experiments in Morphosyntactic Processing for Translating to and from German
Fraser, Alexander;"></td>
	<td class="line x" title="1:118	Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 115119, Athens, Greece, 30 March  31 March 2009." ></td>
	<td class="line x" title="2:118	c2009 Association for Computational Linguistics Experiments in morphosyntactic processing for translating to and from German Alexander Fraser Institute for Natural Language Processing University of Stuttgart fraser@ims.uni-stuttgart.de Abstract We describe two shared task systems and associated experiments." ></td>
	<td class="line x" title="3:118	The German to English system used reordering rules applied to parses and morphological splitting and stemming." ></td>
	<td class="line x" title="4:118	The English to German system used an additional translation step which recreated compound words and generated morphological inflection." ></td>
	<td class="line x" title="5:118	1 Introduction The Institute for Natural Language Processing (IfNLP), Stuttgart, participated in the WMT-2009 shared tasks for German to English and English to German translation with constrained systems which employed morphological and syntactic processing techniques." ></td>
	<td class="line x" title="6:118	The systems were based on the open source Moses docoder (Koehn et al., 2007)." ></td>
	<td class="line x" title="7:118	We combined IfNLP tools for syntactic and morphological analysis (which are publicly available and widely used) with preprocessing techniques that were successfully used by other groups in WMT-2008, and extended these." ></td>
	<td class="line x" title="8:118	For English to German translation, we additionally performed a step which recreated compound words and generated morphological inflection." ></td>
	<td class="line x" title="9:118	1.1 Baseline The baseline is the standard system supplied for the shared task." ></td>
	<td class="line x" title="10:118	We used the default parameters of the Moses toolkit, except for a small difference in the generation of the word alignments, see section 3." ></td>
	<td class="line x" title="11:118	2 Improvements 2.1 Character Normalization We normalize both the English and German by converting all characters to their nearest equivalent in Latin-1 (ISO 8859-1) encoding1, except for the euro sign, which is handled specially." ></td>
	<td class="line o" title="12:118	We did not modify the SGML files used for calculating BLEU and METEOR scores in any way." ></td>
	<td class="line x" title="13:118	2.2 German Writing Reform German underwent a writing reform from the alte Rechtschreibung (old spelling rules/orthography) to the neue Rechtschreibung (gloss: new spelling rules/orthography) recently." ></td>
	<td class="line x" title="14:118	Early Europarl data are written using the alte Rechtschreibung and hence need to be converted to the neue Rechtschreibung in order to match the news data, which is in the new form." ></td>
	<td class="line x" title="15:118	We began the process by mapping all cased variants of a particular word to a single class (such as by mapping two words which are written with ue and u, but are otherwise identical, to a single class)." ></td>
	<td class="line x" title="16:118	We then tried to automatically identify the correct variant under the writing reform for each class." ></td>
	<td class="line x" title="17:118	Initially we tried the linux tool aspell but found that its coverage (the recall of its lexicon) was poor." ></td>
	<td class="line x" title="18:118	We used a simple technique for finding the best variant." ></td>
	<td class="line x" title="19:118	We separated the Europarl corpus into portions written using the old and new forms." ></td>
	<td class="line x" title="20:118	We used the incidence of the word dass (the complementizer meaning that) and its old rules variant da." ></td>
	<td class="line x" title="21:118	We used a chunk size of 70 sentences to segment Europarl into old and new by counting whether there were more instances of da or dass, respectively, in each chunk." ></td>
	<td class="line x" title="22:118	We added the news corpora to the new portion." ></td>
	<td class="line x" title="23:118	For each variant we counted the number of times it occurred in the new data and subtracted the number of times it occurred in the old data; the variant with the highest adjusted count was selected." ></td>
	<td class="line x" title="24:118	1Latin-1 is an 8-bit encoding which has the common accented characters used in Western European languages." ></td>
	<td class="line x" title="25:118	A reviewer pointed out that ISO 8859-15 has superseded ISO 8859-1." ></td>
	<td class="line x" title="26:118	115 2.3 Reordering German German word order differs from English substantially." ></td>
	<td class="line x" title="27:118	Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Drabek and Yarowsky (2004), Collins et al.(2005), Popovic and Ney (2006), Wang et al.(2007) and many others." ></td>
	<td class="line x" title="30:118	To obtain a parse of each German sentence in the training, dev and test corpora, we employed the IfNLP BitPar probabilistic parser (Schmid, 2004), using models learned from the Tiger Treebank for German." ></td>
	<td class="line x" title="31:118	Dealing with morphological productivity is important in the syntactic parsing of German." ></td>
	<td class="line x" title="32:118	BitPar has been designed with this in mind." ></td>
	<td class="line x" title="33:118	IfNLPs SMOR analyzer is used for morphological analysis (Schmid et al., 2004)." ></td>
	<td class="line x" title="34:118	SMOR is run over a list of types in each German sentence, and outputs a list of analyses for each type, each of which corresponds to a POS tag." ></td>
	<td class="line x" title="35:118	BitPar is limited to choosing one of these POS tags for this type." ></td>
	<td class="line x" title="36:118	Words which SMOR fails to analyze are allowed to occur with any POS tag." ></td>
	<td class="line x" title="37:118	We reimplemented the syntactic preprocessing approach of Collins et al.(2005), with modifications." ></td>
	<td class="line x" title="39:118	Reordering rules are applied to a German parse tree (generated by BitPar), and focus on reordering the words in the German clause structure to more closely resemble English clause structure." ></td>
	<td class="line x" title="40:118	The rules are applied to both the training data for the SMT system, and the input (the dev and test sets)." ></td>
	<td class="line x" title="41:118	We previously performed an error analysis of this approach and for the work described here we addressed some of the shortcomings identified through the analysis." ></td>
	<td class="line x" title="42:118	The analysis was performed on the Europarl dev2006 set." ></td>
	<td class="line x" title="43:118	The first error that we noticed occurring frequently was that some large clausal units which were labeled as subjects were being moved forward in the sentence." ></td>
	<td class="line x" title="44:118	We modified the rule moving subjects forward to not apply to the constituents S, CS, VP and CVP." ></td>
	<td class="line x" title="45:118	See the first part of table 3 for an example." ></td>
	<td class="line x" title="46:118	The phrase dass der Balkan ist kein Gebiet is moved under the original rules, and with the modification is no longer moved2." ></td>
	<td class="line x" title="47:118	2Note that there is an unrelated reordering error at the end of the sentence for both BEFORE and AFTER, gibt (gloss: gives) should have moved to follow das (gloss: that)." ></td>
	<td class="line o" title="48:118	System BLEU METEOR LR no processing 18.91 49.50 1.0097 c+w 19.37 49.69 1.0067 c+w, s/s 19.18 51.13 1.0035 c+w, old reordering 19.61 50.44 1.0092 c+w, new reordering 19.91 50.84 1.0059 c+w, new reordering, s/s (submitted, bug) 19.65 51.57 1.0093 * c+w, new reordering, s/s 19.73 51.59 1.0062 as * IRSTLM quantized 19.52 51.33 1.0003 as * IRSTLM 19.75 51.61 1.0013 as * IRSTLM 21.2 quantized 19.52 51.51 1.0095 as * RANDLM 19.67 51.73 1.0067 as * RANDLM 21.2 21.03 51.96 1.0111 Table 1: German to English, dev-2009b (case sensitive), c+w = char+word normalization, s/s = splitting/stemming, 21.2 = larger LM System BLEU METEOR LR no processing 13.55 38.31 0.9910 c+w (no second step) 14.11 38.27 0.9991 c+w, s/s, second step (submitted, bug) 12.34 37.89 1.0338 c+w, s/s, second step 13.05 37.94 1.0157 Table 2: English to German, dev-2009b (case sensitive), c+w = char+word normalization, s/s = splitting/stemming The second error that we handled was that S-RC constituents which do not have a complementizer are reordered incorrectly." ></td>
	<td class="line x" title="49:118	We modified the original verb 2nd rule, so that if there is no complementizer in a S-RC constituent, then the head is moved to the second position, see the second part of table 3 for an example." ></td>
	<td class="line x" title="50:118	Using the original rules, the verb 2nd rule fails to fire, incorrectly leaving haben (gloss: have) at the end of the clause." ></td>
	<td class="line x" title="51:118	2.4 Morphological Decomposition We implemented the frequency-based word splitting approach of Koehn and Knight (2003), and made modifications, including some similar to those described by Stymne et al.(2008)." ></td>
	<td class="line x" title="53:118	This well-known technique splits compound words." ></td>
	<td class="line x" title="54:118	In addition, we performed simple suffix elimination, aimed at removing inflection marking features such as gender and case that are not necessary for translation to English." ></td>
	<td class="line x" title="55:118	We took the stem combination with the highest geometric mean of the frequencies of the stems, but following Stymne et al.(2008), we restricted stems to minimum length 4, and we allowed an extended list of infixes: s, n, en, nen, es, er and ien." ></td>
	<td class="line x" title="57:118	For suffixes, we allowed: e, en, n, es, s, em and er, which is more aggressive 116 INPUT Mir ist bewusst , dass der Balkan kein Gebiet ist , das Anlass zu Optimismus gibt . gloss me is clear , that the Balkans not area is , that opportunity for optimism gives . BEFORE Mir dass der Balkan ist kein Gebiet ist bewusst , , das Anlass zu Optimismus gibt . gloss me that the Balkans is not area is clear , that opportunity for optimism gives . AFTER Mir ist bewusst , dass der Balkan ist kein Gebiet , das Anlass zu Optimismus gibt . gloss me is clear , that the Balkans is not area , that opportunity for optimism gives . REF I am aware that the Balkans are not the most promising area for optimism . INPUT Am 23." ></td>
	<td class="line x" title="58:118	November 1999 hat ein Partnerschaftstag stattgefunden , an dem viele von uns teilgenommen haben . gloss on 23 November 1999 have a partnership-day took-place , in which many of us participated have . BEFORE Am 23." ></td>
	<td class="line x" title="59:118	November 1999 ein Partnerschaftstag hat stattgefunden , an dem teilgenommen viele von uns haben . gloss on 23 November 1999 a partnership-day have took-place , in which participated many of us have . AFTER Am 23." ></td>
	<td class="line x" title="60:118	November 1999 ein Partnerschaftstag hat stattgefunden , an dem viele von uns haben teilgenommen . gloss on 23 November 1999 a partnership-day have took-place , in which many of us have participated . REF A partnership day was held on 23 November 1999 , in which many of us participated . Table 3: Differences in reordering: BEFORE is reordering using rules in (Collins et al., 2005), AFTER is our modified reordering than used in previous work (and therefore generalizes more but at the same time causes some erroneous conflation)." ></td>
	<td class="line x" title="61:118	We stripped e, en and n from all stems (but remembered the most frequent variant, so that applying the procedure to Kirchturm results in Kirche Turm (gloss: church tower))." ></td>
	<td class="line x" title="62:118	We store an alignment from the original German to the simplified German which we will use in the next section." ></td>
	<td class="line x" title="63:118	2.5 Morphological Generation For translation from English to German, we first translated from English to the simplified German presented in the previous section, and then performed an independent translation step from simplified German to fully inflected German." ></td>
	<td class="line x" title="64:118	Two processes are handled by this step." ></td>
	<td class="line x" title="65:118	First, series of stems corresponding to compound words are recomposed (along with infixes which are not present in the simplified German form) into compound words." ></td>
	<td class="line x" title="66:118	Second, inflection is added (e.g., case and gender agreement is handled)." ></td>
	<td class="line x" title="67:118	Both of these processes are implemented using a Moses system trained on a parallel corpus where the source language is simplified German and the target language is fully inflected German." ></td>
	<td class="line x" title="68:118	The alignment is error-free as it was generated as a side effect of the splitting and stemming process described in the previous section." ></td>
	<td class="line x" title="69:118	In translation, reordering is not allowed, but we otherwise use standard Moses settings." ></td>
	<td class="line x" title="70:118	3 Experiments 3.1 German to English We trained our German to English system on the constrained parallel data." ></td>
	<td class="line x" title="71:118	The English data was processed using character normalization." ></td>
	<td class="line x" title="72:118	The German data was first processed using character and word (writing reform) normalization." ></td>
	<td class="line x" title="73:118	We then parsed the German data using BitPar and applied the modified reordering rules." ></td>
	<td class="line x" title="74:118	After this the splitting and stemming process was applied." ></td>
	<td class="line x" title="75:118	Finally, we lowercased the data." ></td>
	<td class="line x" title="76:118	Word alignments were generated using Model 4 (Brown et al., 1993) using the multi-threaded implementation of GIZA++ (Och and Ney, 2003; Gao and Vogel, 2008)." ></td>
	<td class="line x" title="77:118	We first trained Model 4 with English as the source language, and then with German as the source language, resulting in two Viterbi alignments3." ></td>
	<td class="line x" title="78:118	The resulting Viterbi alignments were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003)." ></td>
	<td class="line x" title="79:118	We estimated a standard Moses system using default settings." ></td>
	<td class="line x" title="80:118	MERT was run until convergence using dev-2009a (separately for each experiment)." ></td>
	<td class="line x" title="81:118	One limitation of our German to English system is that we were unable to scale to the full language modeling data using SRILM (Stolcke, 2002), 5grams and modified Kneser-Ney with no singleton deletion4." ></td>
	<td class="line x" title="82:118	The language model in our submitted system is based on all of the available English data, but news-train08 is truncated to the first 10193376 lines, meaning that we did not train on the remaining 11038787 lines, so we used a little less than half of the data." ></td>
	<td class="line x" title="83:118	We converted the lan3We used 5 iterations of Model 1, 4 iterations of HMM (Vogel et al., 1996) and 4 iterations of Model 4." ></td>
	<td class="line x" title="84:118	4SRILM failed when trained on the full data, even when a machine with 32 GB RAM and 48 GB swap was used." ></td>
	<td class="line x" title="85:118	117 guage model trained using SRILM to the binary format using IRSTLM." ></td>
	<td class="line oc" title="86:118	Experiments are presented in table 1, using BLEU (Papineni et al., 2001) and METEOR5 (Banerjee and Lavie, 2005), and we also show the length ratio (ratio of hypothesized tokens to reference tokens)." ></td>
	<td class="line p" title="87:118	For translation into English METEOR had superior correlation with human rankings to BLEU at WMT 2008 (Callison-Burch et al., 2008)." ></td>
	<td class="line x" title="88:118	Our submitted system had a bug where the environment variable LC ALL was set to en US when creating the binarized filtered lexicalized reordering table for the test set (and for the blindtest set, but not for the dev set used for MERT)." ></td>
	<td class="line x" title="89:118	This caused minor degradation, see the system marked (*) for the system with the bug corrected." ></td>
	<td class="line o" title="90:118	Each system increases in both BLEU and METEOR as improvements are added." ></td>
	<td class="line x" title="91:118	An exception is that splitting/stemming decreases BLEU somewhat." ></td>
	<td class="line p" title="92:118	However, we trust the METEOR results more due to their better correlation with human judgements." ></td>
	<td class="line x" title="93:118	We also compared using a different language model instead of the SRILM model (the bottom half of table 1)." ></td>
	<td class="line x" title="94:118	These used either the reduced English language modeling data or the full data (21.2 M segments, marked 21.2 in the results)." ></td>
	<td class="line x" title="95:118	RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system)." ></td>
	<td class="line x" title="96:118	IRSTLM (Federico and Cettolo, 2007) also performs well, but the quantized model on the 21.2 data did not improve over the smaller quantized model6." ></td>
	<td class="line x" title="97:118	IRSTLM uses an approximation of Witten-Bell smoothing, our results support that this is competitive." ></td>
	<td class="line x" title="98:118	3.2 English to German We trained our English to German system on the constrained parallel data." ></td>
	<td class="line x" title="99:118	The first SMT system translates from lowercased English to lowercased simplified German, which is then recased." ></td>
	<td class="line x" title="100:118	The syntactic reordering process is not used, but otherwise the German data is processed identically." ></td>
	<td class="line x" title="101:118	The alignment from simplified German to English is generated as described in the previous section." ></td>
	<td class="line o" title="102:118	We used all of the German data to train the language 5METEOR used default weights, stemming and Wordnet synsets." ></td>
	<td class="line x" title="103:118	6After speaking with the authors, we plan to try IRSTLM on the full data using memory mapping for binarization." ></td>
	<td class="line x" title="104:118	model on simplified German." ></td>
	<td class="line x" title="105:118	The second SMT system translates mixed case simplified German to mixed case unsimplified German." ></td>
	<td class="line x" title="106:118	The translation model is built only on the simplified German from the parallel text, and the language model is trained on all German data." ></td>
	<td class="line x" title="107:118	We present the results in table 2." ></td>
	<td class="line n" title="108:118	METEOR7 did not correlate as well as BLEU for translation out of English in WMT 2008." ></td>
	<td class="line x" title="109:118	The BLEU score of our final system is worse than the baseline." ></td>
	<td class="line x" title="110:118	We had chosen to submit this system as we found it more interesting than submitting a vanilla system." ></td>
	<td class="line x" title="111:118	In addition, the system of Stymne et al.(2008) received a good human evaluation despite having a relatively low BLEU score, and we hoped we were performing similar morphological generalization." ></td>
	<td class="line x" title="113:118	We expect to be able to improve this system through error analysis." ></td>
	<td class="line x" title="114:118	In an initial inspection we found case mismatching problems between step one and step two." ></td>
	<td class="line x" title="115:118	4 Conclusion We presented our German to English system which employed character normalization, compensated for problems caused by the German writing reform, used modified syntactic reordering rules (in combination with morphologically aware parsing), and employed substring-based morphological analysis." ></td>
	<td class="line o" title="116:118	Our best system improves by 2.46 METEOR and 1.12 BLEU over a standard Moses system." ></td>
	<td class="line x" title="117:118	Our English to German system used the same two normalizations and the substring-based morphological analysis, and additionally implemented a second translation step for recreating compound words and generating case and gender inflection." ></td>
	<td class="line x" title="118:118	We will improve this system in future work." ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2310
Coupling Hierarchical Word Reordering and Decoding in Phrase-Based Statistical Machine Translation
Khalilov, Maxim;Fonollosa, José A. R.;Dras, Mark;"></td>
	<td class="line x" title="1:153	Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 7886, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:153	c 2009 Association for Computational Linguistics Coupling hierarchical word reordering and decoding in phrase-based statistical machine translation Maxim Khalilov and Jos A.R. Fonollosa Universitat Politcnica de Catalunya Campus Nord UPC, 08034, Barcelona, Spain {khalilov,adrian}@gps.tsc.upc.edu Mark Dras Macquarie University North Ryde NSW 2109, Sydney, Australia madras@ics.mq.edu.au Abstract In this paper, we start with the existing idea of taking reordering rules automatically derived from syntactic representations, and applying them in a preprocessing step before translation to make the source sentence structurally more like the target; and we propose a new approach to hierarchically extracting these rules." ></td>
	<td class="line x" title="3:153	We evaluate this, combined with a lattice-based decoding, and show improvements over stateof-the-art distortion models." ></td>
	<td class="line x" title="4:153	1 Introduction One of the big challenges for the MT community is the problem of placing translated words in a natural order." ></td>
	<td class="line x" title="5:153	This issue originates from the fact that different languages are characterized by different word order requirements." ></td>
	<td class="line x" title="6:153	The problem is especially important if the distance between words which should be reordered is high (global reordering); in this case the reordering decision is very difficult to take based on statistical information due to dramatic expansion of the search space with the increase in number of words involved in the search process." ></td>
	<td class="line x" title="7:153	Classically, statistical machine translation (SMT) systems do not incorporate any linguistic analysis and work at the surface level of word forms." ></td>
	<td class="line x" title="8:153	However, more recently MT systems are moving towards including additional linguistic and syntactic informative sources (for example, sourceand/or targetside syntax) into word reordering process." ></td>
	<td class="line x" title="9:153	In this paper we propose using a syntactic reordering system operating with fully, partially and nonlexicalized reordering patterns, which are applied on the step prior to translation; the novel idea in this paper is in the derivation of these rules in a hierarchical manner, inspired by Imamura et al (2005)." ></td>
	<td class="line x" title="10:153	Furthermore, we propose generating a word lattice from the bilingual corpus with the reordered source side, extending the search space on the decoding step." ></td>
	<td class="line x" title="11:153	A thorough study of the combination of syntactical and word lattice reordering approaches is another novelty of the paper." ></td>
	<td class="line x" title="12:153	2 Related work Many reordering algorithms have appeared over the past few years." ></td>
	<td class="line x" title="13:153	Word class-based reordering was a part of Ochs Alignment Template system (Och et al., 2004); the main criticism of this approach is that it shows bad performance for the pair of languages with very distinct word order." ></td>
	<td class="line x" title="14:153	The state-of-the-art SMT system Moses implements a distance-based reordering model (Koehn et al., 2003) and a distortion model, operating with rewrite patterns extracted from a phrase alignment table (Tillman, 2004)." ></td>
	<td class="line x" title="15:153	Many SMT models implement the brute force approach, introducing several constrains for the reordering search as described in Kanthak et al.(2005) and Crego et al.(2005)." ></td>
	<td class="line x" title="18:153	The main criticism of such systems is that the constraints are not lexicalized." ></td>
	<td class="line x" title="19:153	Recently there has been interest in SMT exploiting non-monotonic decoding which allow for extension of the search space and linguistic information involvement." ></td>
	<td class="line x" title="20:153	The variety of such models includes a constrained distance-based reordering (Costa-juss et al., 2006); and a constrained version of distortion model where the reordering search problem is tackled through a set of linguistically motivated rules used during decoding (Crego and Mario, 2007)." ></td>
	<td class="line x" title="21:153	78 A quite popular class of reordering algorithms is a monotonization of the source part of the parallel corpus prior to translation." ></td>
	<td class="line x" title="22:153	The first work on this approach is described in Nieen and Ney (2004), where morpho-syntactic information was used to account for the reorderings needed." ></td>
	<td class="line x" title="23:153	A representative set of similar systems includes: a set of hand-crafted reordering patterns for German-to-English (Collins et al., 2005) and Chinese-English (Wang et al., 2007) translations, emphasizing the distinction between German/Chinese and English clause structure; and statistical machine reordering (SMR) technique where a monotonization of the source words sequence is performed by translating them into the reordered one using well established SMT mechanism (Costa-juss and Fonollosa, 2006)." ></td>
	<td class="line x" title="24:153	Coupling of SMR algorithm and the search space extension via generating a set of weighted reordering hypotheses has demonstrated a significant improvement, as shown in Costa-juss and Fonollosa (2008)." ></td>
	<td class="line x" title="25:153	The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for FrenchEnglish translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments." ></td>
	<td class="line x" title="26:153	We propose using a word distortion model not only to monotonize the source part of the corpus (using a different approach to rewrite rule organization from Xia and McCord), but also to extend the search space during decoding." ></td>
	<td class="line x" title="27:153	3 Baseline phrase-based SMT systems The reference system which was used as a translation mechanism is the state-of-the-art Moses-based SMT (Koehn et al., 2007)." ></td>
	<td class="line x" title="28:153	The training and weights tuning procedures can be found on the Moses web page1." ></td>
	<td class="line x" title="29:153	Classical phrase-based translation is considered as a three step algorithm: (1) the source sequence of words is segmented into phrases, (2) each phrase is translated into the target language using a translation table, (3) the target phrases are reordered to fit the target language." ></td>
	<td class="line x" title="30:153	The probabilities of the phrases are estimated by relative frequencies of their appearance in the training corpus." ></td>
	<td class="line x" title="31:153	1http://www.statmt.org/moses/ In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004)." ></td>
	<td class="line x" title="32:153	According to this model, monotonic or reordered local orientations enriched with probabilities are learned from training data." ></td>
	<td class="line x" title="33:153	During decoding, translation is viewed as a monotone block sequence generation process with the possibility to swap a pair of neighbor blocks." ></td>
	<td class="line x" title="34:153	4 Syntax-based reordering coupled with word graph Our syntax-based reordering system requires access to source and target language parse trees and word alignments intersections." ></td>
	<td class="line x" title="35:153	4.1 Notation Syntax-based reordering (SBR) operates with source and target parse trees that represent the syntactic structure of a string in source and target languages according to a Context-Free Grammar (CFG)." ></td>
	<td class="line x" title="36:153	We call this representation 'CFG form'." ></td>
	<td class="line x" title="37:153	We formally define a CFG in the usual way as G = N,T,R,S, where N is a set of nonterminal symbols (corresponding to source-side phrase and partof-speech tags); T is a set of source-side terminals (the lexicon), R is a set of production rules of the form   , with   N and , which is a sequence of terminal and nonterminal symbols; and S  N is the distinguished symbol." ></td>
	<td class="line x" title="38:153	The reordering rules then have the form 0@0k@k  d0@d0 dk@dk|Lexicon|p1 (1) where i  N for all 0  i  k; (do dk) is a permutation of (0k); Lexicon comes from the source-side set of words for each i; and p1 is a probability associated with the rule." ></td>
	<td class="line x" title="39:153	Figure 1 gives two examples of the rule format." ></td>
	<td class="line x" title="40:153	4.2 Rules extraction Concept." ></td>
	<td class="line x" title="41:153	Inspired by the ideas presented in Imamura et al.(2005), where monolingual correspondences of syntactic nodes are used during decoding, we extract a set of bilingual patterns allowing for reordering as described below: 79 (1) align the monotone bilingual corpus with GIZA++ (Och and Ney, 2003) and find the intersection of direct and inverse word alignments, resulting in the construction of the projection matrix P (see below)); (2) parse the source and the target parts of the parallel corpus; (3) extract reordering patterns from the parallel non-isomorphic CFG-trees based on the word alignment intersection." ></td>
	<td class="line x" title="43:153	Step 2 is straightforward; we explain aspects of Steps 1 and 3 in more detail below." ></td>
	<td class="line x" title="44:153	Figures 1 and 2 show an example of the extraction of two lexicalized rules for a parallel Arabic-English sentence: Arabic: English: h*A this hW is fndq your +k hotel We use this below in our explanations." ></td>
	<td class="line x" title="45:153	Figure 2: Example of subtree transfer and reordering rules extraction." ></td>
	<td class="line x" title="46:153	Projection matrix." ></td>
	<td class="line x" title="47:153	Bilingual content can be represented in the form of words or sequences of words depending on the syntactic role of the corresponding grammatical element (constituent or POS)." ></td>
	<td class="line x" title="48:153	Given two parse trees and a word alignment intersection, a projection matrix P is defined as an M N matrix such that M is the number of words in the target phrase; N is the number of words in the source phrase; and a cell (i,j) has a value based on the alignment intersection  this value is zero if word i and word j do not align, and is a unique non-zero link number if they do." ></td>
	<td class="line x" title="49:153	For the trees in Figure 2, P =    1 0 0 0 0 2 0 0 0 0 0 3 0 0 4 0    Unary chains." ></td>
	<td class="line x" title="50:153	Given an unary chain of the form X  Y , rules are extracted for each level in this chain." ></td>
	<td class="line x" title="51:153	For example given a rule NP@0ADVP@1  ADVP@1NP@0 and a unary chain 'ADVP  AD', a following equivalent rule will be generated NP@0AD@1  AD@1NP@0." ></td>
	<td class="line x" title="52:153	The role of target-side parse tree." ></td>
	<td class="line x" title="53:153	Although reordering is performed on the source side only, the target-side tree is of great importance: the reordering rules can be only extracted if the words covered by the rule are entirely covered by both a node in the source and in the target trees." ></td>
	<td class="line x" title="54:153	It allows the more accurate determination of the covering and limits of the extracted rules." ></td>
	<td class="line x" title="55:153	4.3 Rules organization Once the list of fully lexicalized reordering patterns is extracted, all the rules are progressively processed reducing the amount of lexical information." ></td>
	<td class="line x" title="56:153	These initial rules are iteratively expanded such that each element of the pattern is generalized until all the lexical elements of the rule are represented in the form of fully unlexicalized categories." ></td>
	<td class="line x" title="57:153	Hence, from each NN@0 NP@1  NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >>|p NN@0 NNP@1  NNP@1 NN@0 | NN@0 << fndq >> NNP@1 << +k >>|pprime Figure 1: Directly extracted rules." ></td>
	<td class="line x" title="58:153	80 initial pattern with N lexical elements, 2N 2 partially lexicalized rules and 1 general rule are generated." ></td>
	<td class="line x" title="59:153	An example of the process of delexicalization can be found in Figure 3." ></td>
	<td class="line x" title="60:153	Thus, finally three types of rules are available: (1) fully lexicalized (initial) rules, (2) partially lexicalized rules and (3) unlexicalized (general) rules." ></td>
	<td class="line x" title="61:153	On the next step, the sets are processed separately: patterns are pruned and ambiguous rules are removed." ></td>
	<td class="line x" title="62:153	All the rules from the fully lexicalized, partially lexicalized and general sets that appear fewer than k times are directly discarded (k is a shorthand for kful, kpart and kgener)." ></td>
	<td class="line x" title="63:153	The probability of a pattern is estimated based on relative frequency of their appearance in the training corpus." ></td>
	<td class="line x" title="64:153	Only one the most probable rule is stored." ></td>
	<td class="line x" title="65:153	Fully lexicalized rules are not pruned (kful = 0); partially lexicalized rules that have been seen only once were discarded (kpart = 1); the thresholds kgener was set to 3: it limits the number of general patterns capturing rare grammatical exceptions which can be easily found in any language." ></td>
	<td class="line x" title="66:153	Only the one-best reordering is used in other stages of the algorithm, so the rule output functioning as an input to the next rule can lead to situations reverting the change of word order that the previously applied rule made." ></td>
	<td class="line x" title="67:153	Therefore, the rules that can be ambiguous when applied sequentially during decoding are pruned according to the higher probability principle." ></td>
	<td class="line x" title="68:153	For example, for the pair of patterns with the same lexicon (which is empty for a general rule leading to a recurring contradiction NP@0 VP@1  VP@1 NP@0 p1, VP@0 NP@1  NP@1 VP@0 p2 ), the less probable rule is removed." ></td>
	<td class="line x" title="69:153	Finally, there are three resulting parameter tables analogous to the 'r-table' as stated in (Yamada and Knight, 2001), consisting of POSand constituentbased patterns allowing for reordering and monotone distortion (examples can be found in Table 5)." ></td>
	<td class="line x" title="70:153	4.4 Source-side monotonization Rule application is performed as a bottom-up parse tree traversal following two principles: (1) the longest possible rule is applied, i.e. among a set of nested rules, the rule with a longest left-side covering is selected." ></td>
	<td class="line x" title="71:153	For example, in the case of the appearance of an NN JJ RB sequence and presence of the two reordering rules NN@0 JJ@1   and NN@0 JJ@1 RB@2   the latter pattern will be applied." ></td>
	<td class="line x" title="72:153	(2) the rule containing the maximum lexical information is applied, i.e. in case there is more than one alternative pattern from different groups, the lexicalized rules have preference over the partially lexicalized, and partially lexicalized over general ones." ></td>
	<td class="line x" title="73:153	Figure 4: Reordered source-side parse tree." ></td>
	<td class="line x" title="74:153	Once the reordering of the training corpus is ready, it is realigned and new more monotonic alignment is passed to the SMT system." ></td>
	<td class="line x" title="75:153	In theory, the word links from the original alignment can be used, however, due to our experience, running GIZA++ again results in a better word alignment since it is easier to learn on the modified training example." ></td>
	<td class="line x" title="76:153	Example of correct local reordering done with the SBR model can be found in Figure 4." ></td>
	<td class="line x" title="77:153	Initial rule: NN@0 NP@1  NP@1 NN@0 | NN@0 << fndq >> NP@1 << +k >>|p1 Part." ></td>
	<td class="line x" title="78:153	lexic." ></td>
	<td class="line x" title="79:153	rules: NN@0 NP@1  NP@1 NN@0 | NN@0 << fndq >> NP@1 << >>|p2 NN@0 NP@1  NP@1 NN@0 | NN@0 << >> NP@1 << +k >>|p3 General rule: NN@0 NP@1  NP@1 NN@0 |p4 Figure 3: Example of a lexical rule expansion." ></td>
	<td class="line x" title="80:153	81 4.5 Coupling with decoding In order to improve reordering power of the translation system, we implemented an additional reordering as described in Crego and Mario (2006)." ></td>
	<td class="line x" title="81:153	Multiple word segmentations is encoded in a lattice, which is then passed to the input of the decoder, containing reordering alternatives consistent with the previously extracted rules." ></td>
	<td class="line x" title="82:153	The decoder takes the n-best reordering of a source sentence coded in the form of a word lattice." ></td>
	<td class="line x" title="83:153	This approach is in line with recent research tendencies in SMT, as described for example in (Hildebrand et al., 2008; Xu et al., 2005)." ></td>
	<td class="line x" title="84:153	Originally, word lattice algorithms do not involve syntax into reordering process, therefore their reordering power is limited at representing long-distance reordering." ></td>
	<td class="line x" title="85:153	Our approach is designed in the spirit of hybrid MT, integrating syntax transfer approach and statistical word lattice methods to achieve better MT performance on the basis of the standard state-of-the-art models." ></td>
	<td class="line x" title="86:153	During training a set of word permutation patterns is automatically learned following given word-toword alignment." ></td>
	<td class="line x" title="87:153	Since the original and monotonized (reordered) alignments may vary, different sets of reordering patterns are generated." ></td>
	<td class="line x" title="88:153	Note that no information about the syntax of the sentence is used: the reordering permutations are motivated by the crossed links found in the word alignment and, conS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 L>n +h +h >n mTEmmTEm *w Eryq >n *w Eryq tAryx mTEm Eryq *w Eryq tAryx *w *w Eryq mTEm tAryx Eryq *w S 1 2 3 4 5 6 7 8 9>n +h +h >n mTEmmTEm >n *w tAryx *w mTEm tAryx 10 L Eryq mTEm Eryq tAryx >n +h mTEm *w tAryx Eryq Word lattice, plain text: Word lattice, reordered text:>n +h mTEm *w Eryq tAryx (c) (b) S 1 2 3 4 5>n +h mTEm*w LtAryxEryq >n +h mTEm *w tAryx  EryqMonotonic search, plain text:(a) Figure 5: Comparative example of a monotone search (a), word lattice for a plain (b) and reordered (c) source sentences." ></td>
	<td class="line x" title="89:153	82 sequently, the generalization power of this framework is limited to local permutations." ></td>
	<td class="line x" title="90:153	On the step prior to decoding, the system generates word reordering graph for every source sentence, expressed in the form of a word lattice." ></td>
	<td class="line x" title="91:153	The decoder processes word lattice instead of only one input hypothesis, extending the monotonic search graph with alternative paths." ></td>
	<td class="line x" title="92:153	Original sentence in Arabic, the English gloss and reference translation are: Ar.: Gl.: >n +h this mTEm restaurant *w has Eryq history tAryx illustrious Ref: this restaurant has an illustrious history The monotonic search graph (a) is extended with a word lattice for the monotonic train set (b) and reordered train sets (c)." ></td>
	<td class="line x" title="93:153	Figure 5 shows an example of the input word graph expressed in the form of a word lattice." ></td>
	<td class="line x" title="94:153	Lattice (c) differ from the graph (b) in number of edges and provides more input options to the decoder." ></td>
	<td class="line x" title="95:153	The decision about final translation is taken during decoding considering all the possible paths, provided by the word lattice." ></td>
	<td class="line x" title="96:153	5 Experiments and results 5.1 Data The experiments were performed on two ArabicEnglish corpora: the BTEC08 corpus from the tourist domain and the 50K first-lines extraction from the corpus that was provided to the NIST08 evaluation campaign and belongs to the news domain (NIST50K)." ></td>
	<td class="line x" title="97:153	The corpora differ mainly in the average sentence length (ASL), which is the key corpus characteristic in global reordering studies." ></td>
	<td class="line x" title="98:153	A training set statistics can be found in Table 1." ></td>
	<td class="line x" title="99:153	BTEC NIST50K Ar En Ar En Sentences 24.9 K 24.9 K 50 K 50 K Words 225 K 210 K 1.2 M 1.35 M ASL 9.05 8.46 24.61 26.92 Voc 11.4 K 7.6 K 55.3 36.3 Table 1: Basic statistics of the BTEC training corpus." ></td>
	<td class="line x" title="100:153	The BTEC development dataset consists of 489 sentences and 3.8 K running words, with 6 humanmade reference translations per sentence; the dataset used to test the translation quality has 500 sentences, 4.1 K words and is also provided with 6 reference translations." ></td>
	<td class="line x" title="101:153	The NIST50K development set consists of 1353 sentences and 43 K words; the test data contains 1056 sentences and 33 K running words." ></td>
	<td class="line x" title="102:153	Both datasets have 4 reference translations per sentence." ></td>
	<td class="line x" title="103:153	5.2 Arabic data preprocessing We took a similar approach to that shown in Habash and Sadat (2006), using the MADA+TOKAN system for disambiguation and tokenization." ></td>
	<td class="line x" title="104:153	For disambiguation only diacritic unigram statistics were employed." ></td>
	<td class="line x" title="105:153	For tokenization we used the D3 scheme with -TAGBIES option." ></td>
	<td class="line x" title="106:153	The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics." ></td>
	<td class="line x" title="107:153	The -TAGBIES option produces Bies POS tags on all taggable tokens." ></td>
	<td class="line x" title="108:153	5.3 Experimental setup We used the Stanford Parser (Klein and Manning, 2003) for both languages, Penn English Treebank (Marcus et al., 1993) and Penn Arabic Treebank set (Kulick et al., 2006)." ></td>
	<td class="line x" title="109:153	The English Treebank is provided with 48 POS and 14 syntactic tags, the Arabic Treebank has 26 POS and 23 syntactic categories." ></td>
	<td class="line x" title="110:153	As mentioned above, specific rules are not pruned away due to a limited amount of training material we set the thresholds kpart and kgener to relatively low values, 1 and 3, respectively." ></td>
	<td class="line x" title="111:153	Evaluation conditions were case-insensitive and with punctuation marks considered." ></td>
	<td class="line x" title="112:153	The targetside 4-gram language model was estimated using the SRILM toolkit (Stolcke, 2002) and modified Kneser-Ney discounting with interpolation." ></td>
	<td class="line x" title="113:153	The highest BLEU score (Papineni et al., 2002) was chosen as the optimization criterion." ></td>
	<td class="line oc" title="114:153	Apart from BLEU, a standard automatic measure METEOR (Banerjee and Lavie, 2005) was used for evaluation." ></td>
	<td class="line o" title="115:153	5.4 Results The scores considered are: BLEU scores obtained for the development set as the final point of the MERT procedure (Dev), and BLEU and METEOR scores obtained on test dataset (Test)." ></td>
	<td class="line x" title="116:153	We present BTEC results (Tables 2), characterized by relatively short sentence length, and the re83 sults obtained on the NIST corpus (Tables 3) with much longer sentences and much need of global reordering." ></td>
	<td class="line o" title="117:153	Dev Test BLEU BLEU METEOR Plain 48.31 45.02 65.98 BL 48.46 47.10 68.10 SBR 48.75 47.52 67.33 SBR+lattice 48.90 48.78 68.85 Table 2: Summary of BTEC experimental results." ></td>
	<td class="line o" title="118:153	Dev Test BLEU BLEU METEOR Plain 41.83 43.80 62.03 BL 42.68 43.52 62.17 SBR 42.71 44.01 63.29 SBR+lattice 43.05 44.89 63.30 Table 3: Summary of NIST50K experimental results." ></td>
	<td class="line x" title="119:153	Four SMT systems are contrasted: BL refers to the Moses baseline system: the training data is not reordered, lexicalized reordering model (Tillman, 2004) is applied; SBR refers to the monotonic system configuration with reordered (SBR) source part; SBR+lattice is the run with reordered source part, on the translation step the input is represented as a word lattice." ></td>
	<td class="line x" title="120:153	We also compare the proposed approach with a monotonic system configuration (Plain)." ></td>
	<td class="line x" title="121:153	It shows the effect of source-reordering and lattice input, also decoded monotonically." ></td>
	<td class="line x" title="122:153	Automatic scores obtained on the test dataset evolve similarly when the SBR and word lattice representation applied to BTEC and NIST50K tasks." ></td>
	<td class="line x" title="123:153	The combined method coupling two reordering techniques was more effective than the techniques applied independently and shows an improvement in terms of BLEU for both corpora." ></td>
	<td class="line o" title="124:153	The METEOR score is only slightly better for the SBR configurations in case of BTEC task; in the case of NIST50K the METEOR improvement is more evident." ></td>
	<td class="line x" title="125:153	The general trend is that automatic scores evaluated on the test set increase with the reordering model complexity." ></td>
	<td class="line x" title="126:153	Application of the SBR algorithm only (without a word lattice decoding) does not allow achieving statistical significance threshold for a 95% confidence interval and 1000 resamples (Koehn, 2004) for either of considered corpora." ></td>
	<td class="line x" title="127:153	However, the SBR+lattice system configuration outperforms the BL by about 1.7 BLEU points (3.5%) for BTEC task and about 1.4 BLEU point (3.1%) for NIST task." ></td>
	<td class="line x" title="128:153	These differences is statistically significant." ></td>
	<td class="line x" title="129:153	Figure 6 demonstrates how two reordering techniques interact within a sentence with a need for both global and local word permutations." ></td>
	<td class="line x" title="130:153	5.5 Syntax-based rewrite rules As mentioned above, the SBR operates with three groups of reordering rules, which are the product of complete or partial delexicalization of the originally extracted patterns." ></td>
	<td class="line x" title="131:153	The groups are processed and pruned independently." ></td>
	<td class="line x" title="132:153	Basic rules statistics for both translation tasks can be found in Table 4." ></td>
	<td class="line x" title="133:153	The major part of reordering rules consists of two or three elements (for BTEC task there are no patterns including more than three nodes)." ></td>
	<td class="line x" title="134:153	For NIST50K there are a few rules with higher size in words of the move (up to 8)." ></td>
	<td class="line x" title="135:153	In addition, there are some long lexicalized rules (7-8), generating a high number of partially lexicalized patterns." ></td>
	<td class="line x" title="136:153	Table 5 shows the most frequent reordering rules with non-monotonic right part from each group." ></td>
	<td class="line x" title="137:153	Ar." ></td>
	<td class="line x" title="138:153	plain.: En." ></td>
	<td class="line x" title="139:153	gloss: AElnt announced Ajhzp press AlAElAm release l by bEvp mission AlAmm AlmtHdp nations united fy in syrAlywn sierra leone An that   En." ></td>
	<td class="line x" title="140:153	ref.: a press release by the united nations mission to sierra leone announced that  Ar." ></td>
	<td class="line x" title="141:153	reord.: Ajhzp AlAElAm l bEvp AlmtHdp AlAmm fy syrAlywn AElnt An  Figure 6: Example of SBR application (highlited bold) and local reordering error corrected with word lattice reordering (underlined)." ></td>
	<td class="line x" title="142:153	84 6 Conclusions In this study we have shown how the translation quality can be improved, coupling (1) SBR algorithm and (2) word alignment-based reordering framework applied during decoding." ></td>
	<td class="line x" title="143:153	The system automatically learns a set of syntactic reordering patterns that exploit systematic differences between word order of source and target languages." ></td>
	<td class="line x" title="144:153	Translation accuracy is clearly higher when allowing for SBR coupled with word lattice input representation than standard Moses SMT with existing (lexicalized) reordering models within the decoder and one input hypothesis condition." ></td>
	<td class="line x" title="145:153	We have also compared the reordering model a monotonic system." ></td>
	<td class="line x" title="146:153	The method was tested translating from Arabic to English." ></td>
	<td class="line x" title="147:153	Two corpora and tasks were considered: the BTEC task with much need of local reordering and the NIST50K task requiring long-distance permutations caused by longer sentences." ></td>
	<td class="line x" title="148:153	The reordering approach can be expanded for any other pair of languages with available parse tools." ></td>
	<td class="line x" title="149:153	We also expect that the method scale to a large training set, and that the improvement will still be kept, however, we plan to confirm this assumption experimentally in the near future." ></td>
	<td class="line x" title="150:153	Acknowledgments This work has been funded by the Spanish Government under grant TEC2006-13964-C03 (AVIVAVOZ project) and under a FPU grant." ></td>
	<td class="line x" title="151:153	Group # of rules Voc 2-element 3-element 4-element [5-8]-element BTEC experiments Specific rules 703 413 406 7 0 0 Partially lexicalized rules 1,306 432 382 50 0 0 General rules 259 5 259 0 0 0 NIST50K experiments Specific rules 517 399 193 109 72 25 Partially lexicalized rules 17,897 14,263 374 638 1,010 12,241 General rules 489 372 180 90 72 30 Table 4: Basic reordering rules statistics." ></td>
	<td class="line x" title="152:153	Specific rules NN@0 NP@1 -> NP@1 NN@0 | NN@0  Asm  NP@1  +y  | 0.0270 DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0  AlAmm DTJJ@1  AlmtHdp  | 0.0515 Partially lexicalized rules DTNN@0 DTJJ@1 -> DTJJ@1 DTNN@0 | DTNN@0  NON DTJJ@1  AlmtHdp  | 0.0017 NN@0 NNP@1 -> NNP@1 NN@0 | NN@0  NON NNP@1  $rm  | 0.0017 General rules PP@0 NP@1 -> PP@0 NP@1 | 0.0432 NN@0 DTNN@1 DTJJ@2 -> NN@0 DTJJ@2 DTNN@1 |0.0259 Table 5: Examples of Arabic-to-English reordering rules." ></td>
	<td class="line x" title="153:153	85" ></td>
</tr></table>
<table cellspacing="0" cellpadding="0" class="srcPaper"><tr>
<td class="line srcData" title="W09-2404
One Translation Per Discourse
Carpuat, Marine;"></td>
	<td class="line x" title="1:142	Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 1927, Boulder, Colorado, June 2009." ></td>
	<td class="line x" title="2:142	c 2009 Association for Computational Linguistics One Translation per Discourse Marine Carpuat Center for Computational Learning Systems Columbia University 475 Riverside Drive, New York, NY 10115 marine@ccls.columbia.edu Abstract We revisit the one sense per discourse hypothesis of Gale et al. in the context of machine translation." ></td>
	<td class="line x" title="3:142	Since a given sense can be lexicalized differently in translation, do we observe one translation per discourse?" ></td>
	<td class="line x" title="4:142	Analysis of manual translations reveals that the hypothesis still holds when using translations in parallel text as sense annotation, thus confirming that translational differences represent useful sense distinctions." ></td>
	<td class="line x" title="5:142	Analysis of Statistical Machine Translation (SMT) output showed that despite ignoring document structure, the one translation per discourse hypothesis is strongly supported in part because of the low variability in SMT lexical choice." ></td>
	<td class="line x" title="6:142	More interestingly, cases where the hypothesis does not hold can reveal lexical choice errors." ></td>
	<td class="line x" title="7:142	A preliminary study showed that enforcing the one translation per discourse constraint in SMT can potentially improve translation quality, and that SMT systems might benefit from translating sentences within their entire document context." ></td>
	<td class="line x" title="8:142	1 Introduction The one sense per discourse hypothesis formulated by Gale et al.(1992b) has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation (WSD) and related tasks (e.g., Yarowsky (1995); Agirre and Rigau The author was partially funded by GALE DARPA Contract No." ></td>
	<td class="line x" title="10:142	HR0011-06-C-0023." ></td>
	<td class="line x" title="11:142	Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency." ></td>
	<td class="line x" title="12:142	(1996))." ></td>
	<td class="line x" title="13:142	In this paper, we investigate its potential usefulness in the context of machine translation." ></td>
	<td class="line x" title="14:142	A growing body of work suggests that translational differences represent observable sense distinctions that are useful in applications." ></td>
	<td class="line x" title="15:142	In monolingual WSD, word alignments in parallel corpora have been successfully used as learning evidence (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003)." ></td>
	<td class="line x" title="16:142	In Statistical Machine Translation (SMT), recent work shows that WSD helps translation quality when the WSD system directly uses translation candidates as sense inventories (Carpuat and Wu, 2007; Chan et al., 2007; Gimenez and M`arquez, 2007)." ></td>
	<td class="line x" title="17:142	In this paper, we revisit the one sense per discourse hypothesis using word translations in parallel text as senses." ></td>
	<td class="line x" title="18:142	Our first goal is to empirically evaluate whether the one translation per document hypothesis holds on French-English reference corpora, thus verifying whether translations exhibit the same properties as monolingual senses." ></td>
	<td class="line x" title="19:142	Our second goal consists in evaluating whether the one translation per discourse hypothesis has the potential to be as useful to statistical machine translation as the one sense per discourse hypothesis to WSD." ></td>
	<td class="line x" title="20:142	Current Statistical Machine Translation (SMT) systems translate one sentence at a time, ignoring any document level information." ></td>
	<td class="line x" title="21:142	Implementing a one translation per document constraint might help provide consistency in translation for sentences drawn from the same document." ></td>
	<td class="line x" title="22:142	After briefly discussing related work, we will show that the one translation per discourse hypothesis holds on automatic word alignments of manually translated data." ></td>
	<td class="line x" title="23:142	Despite ignoring any information beyond the sentential level, automatic SMT out19 put also strongly exhibits the one translation per discourse property." ></td>
	<td class="line x" title="24:142	In addition, we will show that having more than one translation per discourse in SMT output often reveals lexical choice errors, and that enforcing the constraint might help improve overall consistency across sentences and translation quality throughout documents." ></td>
	<td class="line x" title="25:142	2 Related Work In the original one sense per discourse study, Gale et al.(1992b) considered a sample of 9 polysemous English words." ></td>
	<td class="line x" title="27:142	A total of 5 judges were showed pairs of concordance lines for these words taken from Groliers Encyclopedia and asked to identify whether they shared the same sense." ></td>
	<td class="line x" title="28:142	Results strongly support the one sense per discourse hypothesis: 94% of polysemous words drawn from the same document have the same sense." ></td>
	<td class="line x" title="29:142	The experiment was replicated with the same conclusion on the Brown corpus." ></td>
	<td class="line x" title="30:142	Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model." ></td>
	<td class="line x" title="31:142	A subsequent larger scale study of polysemy based on the WordNet sense inventory in the SEMCOR corpus does not support the hypothesis as strongly (Krovetz, 1998)." ></td>
	<td class="line x" title="32:142	Only 77% of ambiguous words have a single sense per discourse." ></td>
	<td class="line x" title="33:142	Analysis revealed that the one sense per discourse hypothesis is only supported for homonymous senses and not for finer-grained sense distinction." ></td>
	<td class="line x" title="34:142	In machine translation, discourse level information has only been indirectly used by adaptation of translation or language models to specific genre or topics (e.g., Foster and Kuhn (2007); Koehn and Schroeder (2007))." ></td>
	<td class="line x" title="35:142	While phrase-based SMT models incorporate the one sense per collocation hypothesis by attempting to translate phrases rather than single words (Koehn et al., 2007), the one sense per discourse hypothesis has not been explicitly used in SMT modeling." ></td>
	<td class="line x" title="36:142	Even the recent generation of SMT models that explicitly use WSD modeling to perform lexical choice rely on sentence context rather than wider document context and translate sentences in isolation (Carpuat and Wu, 2007; Chan et al., 2007; Gimenez and M`arquez, 2007; Stroppa et al., 2007; Specia et al., 2008)." ></td>
	<td class="line x" title="37:142	Other context-sensitive SMT approaches (Gimpel and Smith, 2008) and global lexical choice models (Bangalore etal., 2007) also translate sentences independently." ></td>
	<td class="line x" title="38:142	3 One translation per discourse in reference translations In this section we investigate whether the one sense per discourse hypothesis holds in translation." ></td>
	<td class="line x" title="39:142	Does one sense per discourse mean one translation per discourse?" ></td>
	<td class="line x" title="40:142	On the one hand, one translation per discourse might be too strict a constraint to allow for variations in lexicalization of a given sense." ></td>
	<td class="line x" title="41:142	While a WSD task produces a set of predefined sense labels, a single sense might be correctly translated in many different ways in a full sentence translation." ></td>
	<td class="line x" title="42:142	On the other hand, if the author of the source language text is assumed to consistently use one sense per word per document, translators might also prefer consistent translations of the same source language word throughout a document." ></td>
	<td class="line x" title="43:142	In addition, translated text tends to exhibit more regularities than original text, as shown by machine learning approches to discriminate between translationese and original texts (Baroni and Bernardini, 2006) although patterns of syntactic regularity seemed more informative than lexical choice for those experiments." ></td>
	<td class="line x" title="44:142	3.1 Manual translation data We will test the one translation per discourse hypothesis on a corpus of French and English translations, using standard freely available MT data sets and software." ></td>
	<td class="line x" title="45:142	We use a corpus of 90 French-English news articles made available for the WMT evaluations1." ></td>
	<td class="line x" title="46:142	All development data that contained article boundaries were used." ></td>
	<td class="line x" title="47:142	The data is split into two sets of about 27k words each as described in Table 1." ></td>
	<td class="line x" title="48:142	The articles cover topics ranging from international and local politics to sports and music." ></td>
	<td class="line x" title="49:142	They are drawn from a wide variety of newspapers and magazines originally published in various European languages." ></td>
	<td class="line x" title="50:142	As a result, even though only a single English reference translation is available, it was produced by several different interpreters." ></td>
	<td class="line x" title="51:142	It would have been interesting to perform this analysis with multiple references, but this is unfortunately not possible with 1http://www.statmt.org/wmt09/translation-task.html 20 Test set Language Sentences Tokens Types Singletons no. 1 French 1070 27440 5958 3727 English (ref) 1070 24544 5566 3342 English (SMT) 1070 24758 5075 2932 no. 2 French 1080 27924 6150 3839 English (ref) 1080 24825 5686 3414 English (SMT) 1080 25128 5240 3080 Table 1: Data statistics for the bilingual corpus, including the French side, the manually translated English side (ref) and the automatic English translations (SMT) the French-English data currently available." ></td>
	<td class="line x" title="52:142	Since golden word-alignments are not available, we automatically word align the corpus using standard SMT training techniques." ></td>
	<td class="line x" title="53:142	Using IBM-4 alignment models learned on the large WMT training corpus (see Section 4.1 for more details), we align GIZA++(Och and Ney, 2003) to obtain the IBM4 alignments in both translation directions, expand their intersection with additional links using the grow-diag-final-and heuristic (Koehn et al., 2007)." ></td>
	<td class="line x" title="54:142	This creates a total of 51660 alignment links, and about 89% of French tokens are aligned to at least one English token." ></td>
	<td class="line x" title="55:142	Note that all links involving stopwords are not considered for the rest of the study." ></td>
	<td class="line x" title="56:142	3.2 One translation per discourse holds For every French lemma that occurs more than once in a document, we compute the number of English translations." ></td>
	<td class="line x" title="57:142	In order to allow for morphological and syntactic variations, we compute those statistics using English lemmas obtained by running Treetagger (Schmid, 1994) with the standard French and English parameter settings2." ></td>
	<td class="line x" title="58:142	A higher level of generalization is introduced by conducting the same analysis using stems, which are simply defined as 4-letter prefixes." ></td>
	<td class="line x" title="59:142	We have a total of 2316 different lemma types and 6603 lemma-document pairs." ></td>
	<td class="line x" title="60:142	The scale of this empirical evaluation is much larger than in Gale et al.(1992a) where only 9 target words were considered and in Krovetz (1998) which used the entire SEMCOR corpus vocabulary." ></td>
	<td class="line x" title="62:142	The resulting distribution of number of English translations per French word-document pair is given in the first half of Table 2." ></td>
	<td class="line x" title="63:142	Remarkably, more than 2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ 98% of the French lemmas are aligned to no more than 2 English translations and 80% of French lemmas have a single translation per document." ></td>
	<td class="line x" title="64:142	While these numbers are not as high as the 94% agreement reported by Gale et al.(1992b) in their empirical study, they still strongly support the one translation per discourse hypothesis." ></td>
	<td class="line x" title="66:142	Generalizing from lemmas to stems yields a 4.3 point increase in the percentage of French lemmas with a single translation per document." ></td>
	<td class="line x" title="67:142	Note that using stems might yield to false positives since different words can share the same prefix, however, since we only compare words that align to the same French word in a given document, the amount of noise introduced should be small." ></td>
	<td class="line x" title="68:142	Manual inspection shows that this increase is often due to variations in the POS of the translation, more specifically variations between noun and verb forms which share the same 4-letter prefix as can be seen in the following examples: verb vs. noun conclude vs. conclusion, investigate vs. investigation, apply vs. application, inject vs. injection, establish vs. establishment, criticize vs. critism, recruit vs. recruitment, regulate vs. regulation 3.3 Exceptions: one sense but more than one translation per discourse We investigate what happens in the 15 to 20% of cases where a French word is not consistenly translated throughout a document." ></td>
	<td class="line x" title="69:142	Do these translation differences reflect sense ambiguity in French, or are they close variations in English lexical choice?" ></td>
	<td class="line x" title="70:142	For 21 reference SMT lemmas stems lemmas stems 1 80.82% 85.14% 83.03% 86.38% 2 17.88% 13.91% 15.43% 12.47% 3 01.12% 00.95% 01.25% 00.85% 4 00.18% 00.00% 00.17% 00.22% Table 2: Distribution of number of English translation per document using the word-aligned reference translations and the automatic SMT translations a given French word, how semantically similar are the various English translations?" ></td>
	<td class="line x" title="71:142	We measure semantic similarity using the shortest path length in WordNet (Fellbaum, 1998) as implemented in the WordNet Similarity package (Pedersen et al., 2004)." ></td>
	<td class="line x" title="72:142	The path length is defined as the number of WordNet nodes or synsets in a path between two words: words that belong to the same synset therefore have a shortest path length of 1, while words that are related via a common synonym, hypernym or hyponym have a shortest path length of 2." ></td>
	<td class="line x" title="73:142	Note that this similarity metric is only defined for two WordNet vocabulary words of the same POS." ></td>
	<td class="line x" title="74:142	For 57% of the French lemmas with multiple translations, those translations can be linked by a WordNet path of no more than 4 nodes." ></td>
	<td class="line x" title="75:142	In 19% of the cases, the translations belong to the same synset, another 19% are separated by a path of length 2 only." ></td>
	<td class="line x" title="76:142	Given that sense distinctions in WordNet are very fine-grained, these numbers show that the translations have very similar meanings." ></td>
	<td class="line x" title="77:142	In other words, while the one sense per translation hypothesis does not hold for those 57%, the one sense per discourse hypothesis still holds." ></td>
	<td class="line x" title="78:142	Examples of those synonymous translations are given below: synonyms with SPL = 1 adjust and adapt, earn and gain, movie and film, education and training, holiday and day synonyms with SPL = 2 travel and circulate, scientist and researcher, investigation and inquiry, leave and abandon, witness and eyewitness synonyms with SPL = 3 ratio and proportion, quiet and peace, plane and aircraft 3.4 Exceptions: more than one sense per discourse Among the words with a high WordNet path length or no existing path, we find translations that are not synonyms or semantically similar words, but related words sometimes with different POS." ></td>
	<td class="line x" title="79:142	They fall within two categories." ></td>
	<td class="line x" title="80:142	The first category is that of fine-grained sense distinctions for which the one sense per discourse hypothesis has been showed to break for monolingual WordNet sense distinctions Krovetz (1998)." ></td>
	<td class="line x" title="81:142	However, for those closely related words, it would be possible to write correct English translations that use the same English form throughout a document." ></td>
	<td class="line x" title="82:142	Nationality translation Tibet vs. Tibetan, French vs. France, Paris vs. Parisian, Europe vs. European, French vs. Frenchman Agent/entity policeman vs. police, alderman vs. city The second category of not identical but related translations is explained by a limitation of our experiment set-up: we are looking at single-word translations while the translation of a longer multiword phrase should be considered as a whole." ></td>
	<td class="line x" title="83:142	In the following example, the French word emission is aligned to both emission and greenhouse in the same document, because French does not repeat the long phrase emission de gaz `a effet de serre throughout the document, while the more concise English translationgreenhouse gas emissions is used throughout: Fr apr`es la periode de reduction des emissions [] la Hongrie a pris lengagement de reduire les emissions de gaz `a effet de serre de 6 pour cent [] En [] to cut greenhouse gas emissions after 2012 [] Hungary agreed to cut its greenhouse gas emissions by 6 percent [] Finally, there are a few rare instances where the different translations for a French word reflect a 22 sense distinction in French and could not be correctly translated in English with the same English word." ></td>
	<td class="line x" title="84:142	These are cases where both the one sense per discourse hypothesis and the one translation per discourse break, and where it is not possible to paraphrase the English sentences to fullfill either constraints." ></td>
	<td class="line x" title="85:142	In these instances, the French word is used in two different senses related by metonymy, but the metonymy relation does not translate into English and two non-synonym English words are used as a result." ></td>
	<td class="line x" title="86:142	For instance, the French word bureau translates to both office and desk in the same document, while retraite translates both to retirement and pension." ></td>
	<td class="line x" title="87:142	We found a single instance where two homonym senses of the French word coffre are in the same sentence." ></td>
	<td class="line x" title="88:142	This sentence seems to be a headline, which suggests that the author or translator deliberately used the ambiguous repetition to attract the attention of the reader." ></td>
	<td class="line x" title="89:142	Fr un coffre dans le coffre En a trunk in the boot 4 One translation per discourse in SMT We now turn to empirically testing the one translation per discourse hypothesis on automatically translated text." ></td>
	<td class="line x" title="90:142	While there is an implicit assumption that a wellwritten document produced by a human writer will not introduce unncessary ambiguities, most SMT systems translate one sentence at a time, without any model of discourse or document." ></td>
	<td class="line x" title="91:142	This might suggest that the one translation per discourse hypothesis will not be as strongly supported as by manual translations." ></td>
	<td class="line x" title="92:142	However, this effect might be compensated by the tendency of automatically translated text to exhibit little variety in lexical choice as MT systems tend to produce very literal word for word translations." ></td>
	<td class="line x" title="93:142	As can be seen in Table 1 the reference translations use a larger vocabulary than the automatic translations for the same text." ></td>
	<td class="line x" title="94:142	4.1 Automatically translated data We build a standard SMT system and automatically translate the data set described in Section 3.1." ></td>
	<td class="line x" title="95:142	We strictly follow the instructions for building a phrasebased SMT system that is close to the state-of-theart in the WMT evaluations3, using the large training sets of about 460M words from Europarl and news." ></td>
	<td class="line x" title="96:142	We use the Moses phrase-based statistical machine translation system (Koehn et al., 2007) and follow standard training, tuning and decoding strategies." ></td>
	<td class="line x" title="97:142	The translation model consists of a standard Moses phrase-table with lexicalized reordering." ></td>
	<td class="line x" title="98:142	Bidirectional GIZA++ word alignments are intersected using the grow-diag-final-and heuristic." ></td>
	<td class="line x" title="99:142	Translations of phrases of up to 7 words long are collected and scored with translation probilities and lexical weighting." ></td>
	<td class="line x" title="100:142	The English language model is a 4-gram model with Kneser-Ney smoothing, built with the SRI language modeling toolkit (Stolcke, 2002)." ></td>
	<td class="line x" title="101:142	The word alignment between French input sentences and English SMT output is easily obtained as a by-product of decoding." ></td>
	<td class="line x" title="102:142	We have a total of 56003 alignment links, and 96% of French tokens are linked to a least one English translation." ></td>
	<td class="line x" title="103:142	4.2 One translation per discourse holds We perform the same analysis as for the manual translations." ></td>
	<td class="line x" title="104:142	The distribution of the number of translations for a given French word that occurs repeatedly in a document still strongly supports the one translation per document hypothesis (Table 2)." ></td>
	<td class="line x" title="105:142	In fact, SMT lexical choice seems to be more regular than in manual translations." ></td>
	<td class="line x" title="106:142	4.3 Exceptions: where SMT and reference disagree Again, it is interesting to look at instances where the hypothesis is not verified." ></td>
	<td class="line x" title="107:142	We will not focus on the exceptions that fall in the categories previously observed in Section 3." ></td>
	<td class="line x" title="108:142	Instead, we take a closer look at cases where the reference consistently uses the same English translation, while SMT selects different translation candidates." ></td>
	<td class="line x" title="109:142	There are cases where the SMT system arbitrarily chooses different synonymous translation candidates for the same word in different sentences." ></td>
	<td class="line o" title="110:142	This is not incorrect but will affect translation quality as measured by automatic metrics which compare 3http://www.statmt.org/wmt09/baseline.html 23 Test set Decoding Input METEOR BLEU NIST no. 1 Moses 49.05 20.45 6.135 +postprocess (transprob) 48.73 19.93 6.064 +postprocess (bestmatch) 50.01 20.64 6.220 +decode (transprob) 49.04 20.44 6.128 +decode (bestmatch) 49.36 20.70 6.179 no. 2 Moses 49.60 21.10 6.211 +postprocess (transprob) 49.20 20.43 6.128 +postprocess (bestmatch) 50.56 21.19 6.291 +decode (transprob) 49.58 21.02 6.201 +decode (bestmatch) 50.60 21.21 6.243 Table 3: Enforcing one translation per discourse can help METEOR, BLEU and NIST scores when using the supervised sense disambiguation technique (bestmatch)." ></td>
	<td class="line x" title="111:142	Relying on the unsupervised context-independent SMT translation probabilities (transprob) does not help." ></td>
	<td class="line x" title="112:142	matches between SMT output and manually translated references." ></td>
	<td class="line x" title="113:142	For instance, in a single document, the French agents pathog`enes translates to both (1) pathogens and (2) disease-causing agents while the reference consistently translates to pathogens." ></td>
	<td class="line x" title="114:142	Similarly, the French phrase parmi les detenus is inconsistently translated to among detainees and among those arrested in the same document." ></td>
	<td class="line x" title="115:142	Synonym translations detainees vs. arrested, apartment vs. flat, good vs. beautiful, unit vs. cell However, the majority of differences in translation reflect lexical choice errors." ></td>
	<td class="line x" title="116:142	For instance, the French adjective biologique is incorrectly disambiguated as organic in the phrase fille biologique which should be translated as biological daughter." ></td>
	<td class="line x" title="117:142	SMT lexical choice errors conseiller: advisor vs. councillor, arrondissement: district vs. rounding-off, bal: ball vs. court, biologique: biological vs. organic, assurance: insurance vs. assurance, franchise: frankness vs. deductible While some of those translation distinctions can be explained by differences in topics, all of those French words occur in a large number of documents and cannot be disambiguated by topic alone." ></td>
	<td class="line x" title="118:142	This suggests that local sentential context is not sufficient to correctly disambiguate translation candidates." ></td>
	<td class="line x" title="119:142	5 Detecting SMT errors Based on the observations from the previous section, we further evaluate whether breaking the one translation per discourse hypothesis is indicative of a translation error." ></td>
	<td class="line x" title="120:142	For this purpose, we attempt to correct the translations provided by the Moses SMT system by enforcing the one translation per discourse constraint and evaluate the impact on translation quality." ></td>
	<td class="line x" title="121:142	5.1 Enforcing one translation per discourse In order to get a sense of the potential impact of the one translation per discourse constraint in SMT, we attempt to enforce it using two simple postprocessing techniques." ></td>
	<td class="line x" title="122:142	First, we select a set of French words which are not consistently translated to a single English words in a given document." ></td>
	<td class="line x" title="123:142	We apply a document frequency-based filter to select content words for each document." ></td>
	<td class="line x" title="124:142	This yields a set of 595 French target word types occurring in a total of 89 documents." ></td>
	<td class="line x" title="125:142	Second, we propose a single English translation for all the occurrences of the French target in a document." ></td>
	<td class="line x" title="126:142	We used two different strategies: (1) the fully unsupervised strategy consists in selecting the translation with highest probability among those produced by the baseline SMT system, and 24 Moses Young people under 25 years face various drawbacks when a contract with an assurance at an accessible price , as can be the low experience in the conduct and seniority of driving licences . +postprocess young people under 25 years against various drawbacks when a contract with an insurance at an accessible price , as can be the small experience in the conduct and seniority of driving licences . Moses drivers the most far-sighted can opt for insurance any risk with frankness , so that they get blankets insurance to any risk but at a price more accessible . +postprocess drivers the most far-sighted can opt for insurance any risk with exemption , so that they get blankets insurance to any risk but at a price more accessible . Moses  These ill are isolated , nurses puts gloves rubber and masks of protection and we have antibiotics adapted to treat them ,  said Tibor Nyulasi . +postprocess  These patient are isolated , personnel puts gloves rubber and masks of protection and we have antibiotics appropriate to treat them ,  say Tibor Nyulasi . Moses according to the Ministry of Defence , they also served to make known to the public the real aims of the presence of the army abroad . +postprocess according to the Ministry of Defence , they also use to make known to the public the real purpose of the presence of the army abroad . Moses the public authorities also prepare Christmas . +postprocess the public authorities also puritan Christmas . Table 4: Examples of translation improvement (bold) and degradation (italics) by enforcing the one translation per discourse constraint through postprocessing (2) the supervised strategy picks, among the baseline SMT translations, the one that matches the reference." ></td>
	<td class="line x" title="127:142	Note that the supervised strategy does not predict perfect translations, but an approximation of the golden translations: in addition to noise in word alignments due to phrasal translations, the translations selected are lemmas that might not be in the correctly inflected form for use in the full sentence translation." ></td>
	<td class="line x" title="128:142	Third, we integrate the selected translation candidates by (1) postprocessing the baseline SMT output the translations of the French target word are simply replaced by the recommended translation, and (2) encouraging the SMT system to choose the recommended translations by annotating SMT input using the xml input markup scheme again, this approach is not optimal as it introduces additional translation candidates without probability scores and forces single word translation to compete with phrasal translation even if they are consistent." ></td>
	<td class="line oc" title="129:142	5.2 Impact on translation quality As reported in Table 3, small increases in METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al., 2002) and NIST scores (Doddington, 2002) suggest that SMT output matches the references better after postprocessing or decoding with the suggested lemma translations." ></td>
	<td class="line x" title="130:142	Examples of both improved and degraded lexical choice are given in Table 4." ></td>
	<td class="line x" title="131:142	Since we are modifying translations for a limited set of single-words only, only 10% to 30% of the test set sentences are translated differently." ></td>
	<td class="line x" title="132:142	We manually inspected a random sample of 100 of those sentence pairs for two different systems: postprocess (bestmatch) and decode (bestmatch)." ></td>
	<td class="line x" title="133:142	For each sentence pair, we determined whether the one sense per discourse processing improved, degraded or made no difference in translation quality compared to the baseline Moses output." ></td>
	<td class="line x" title="134:142	Among the sentence pairs where a real change in translation quality was observed, the postprocessing heuristic yielded improvements in 62.5% (decode) and 64.5% (postprocess) of sentences considered." ></td>
	<td class="line x" title="135:142	For 41% (decode) and 57% (postprocess) of the sentences in the sam25 ple, changes only consisted of synonym substitution, morphological variations or local reorderings which did not impact translation quality." ></td>
	<td class="line x" title="136:142	Taken together, these results suggest that the one sense per discourse constraint should be useful to SMT and that it would be worthwile to integrate it directly into SMT modeling." ></td>
	<td class="line x" title="137:142	6 Conclusion We investigated the one sense per discourse hypothesis (Gale et al., 1992b) in the context of machine translation." ></td>
	<td class="line x" title="138:142	Analysis of manual translations showed that the hypothesis still holds when using translations in parallel text as sense annotation, thus confirming that translational differences represent useful sense distinctions." ></td>
	<td class="line x" title="139:142	Analysis of SMT output showed that despite ignoring document structure, the one translation per discourse hypothesis is strongly supported in part because of the low variability in SMT lexical choice." ></td>
	<td class="line x" title="140:142	More interestingly, cases where the hypothesis does not hold can reveal lexical choice errors in an unsupervised fashion." ></td>
	<td class="line x" title="141:142	A preliminary study showed that enforcing the one translation per discourse constraint in SMT can potentially improve translation quality, and that SMT systems might benefit from translating sentences within their entire document context." ></td>
	<td class="line x" title="142:142	In future work, we will (1) evaluate whether one translation per discourse holds for other language pairs such as Arabic-English and Chinese-English, which are not as closely related as French-English and for which multiple reference corpora are available, and (2) directly implement the one translation per discourse constraint within SMT." ></td>
</tr></table>
</div
</body></html>
